
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 180 papers. December 2024.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #7a30efcf;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: #7a30efcf;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #7a30ef17;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ", "Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹", "Ğ¼Ğ¸Ğ½ÑƒÑ‚"],
                hour: ["Ñ‡Ğ°Ñ", "Ñ‡Ğ°ÑĞ°", "Ñ‡Ğ°ÑĞ¾Ğ²"],
                day: ["Ğ´ĞµĞ½ÑŒ", "Ğ´Ğ½Ñ", "Ğ´Ğ½ĞµĞ¹"],
                justNow: "Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ñ‚Ğ¾",
                ago: "Ğ½Ğ°Ğ·Ğ°Ğ´"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["åˆ†é’Ÿ", "åˆ†é’Ÿ", "åˆ†é’Ÿ"],
                hour: ["å°æ—¶", "å°æ—¶", "å°æ—¶"],
                day: ["å¤©", "å¤©", "å¤©"],
                justNow: "åˆšåˆš",
                ago: "å‰"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            } else if (lastDigit === 1) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒÑ";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒĞ¸";
            } else {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ç¯‡è®ºæ–‡"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">ğŸ”º</h1><h1 class="title-text" id="doomgrad">hf monthly</h1></a>
            <p><span id="title-date">Ğ”ĞµĞºĞ°Ğ±Ñ€ÑŒ 2024</span> | <span id="title-articles-count">180 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/m/2024-11.html">â¬…ï¸ <span id="prev-date">11.2024</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/m/2025-01.html">â¡ï¸ <span id="next-date">01.2025</span></a></span>
            <span class="nav-item" id="nav-daily"><a href="https://hfday.ru">ğŸ“ˆ <span id='top-day-label'>Ğ”ĞµĞ½ÑŒ</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">ğŸ”€ <span id="sort-label-text">Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ</option>
                    <option value="pub_date">Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸</option>
                    <option value="issue_id">Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">ğŸ·ï¸ Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> AâˆªB</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> Aâˆ©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">ğŸ§¹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> âœ–ï¸ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': 'Ğ”ĞµĞºĞ°Ğ±Ñ€ÑŒ 2024', 'en': 'December 2024', 'zh': '12æœˆ2024å¹´'};
        let feedDateNext = {'ru': '01.2025', 'en': '01/2025', 'zh': '1æœˆ2025å¹´'};
        let feedDatePrev = {'ru': '11.2024', 'en': '11/2024', 'zh': '11æœˆ2024å¹´'};
        let filterLabel = {'ru': 'Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€', 'en': 'Topics', 'zh': 'ä¸»é¢˜ç­›é€‰'}
        let publishedLabel = {'ru': 'ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ñ‚ ', 'en': 'published on ', 'zh': 'å‘è¡¨äº'}
        let sortLabel = {'ru': 'Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾', 'en': 'Sort by', 'zh': 'æ’åºæ–¹å¼'}
        let paperLabel = {'ru': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ', 'en': 'Paper', 'zh': 'è®ºæ–‡'}
        let topMonthLabel = {'ru': 'ĞœĞµÑÑÑ†', 'en': 'Month', 'zh': 'æœˆåº¦è®ºæ–‡'}
        let topDayLabel = {'ru': 'Ğ”ĞµĞ½ÑŒ', 'en': 'Day', 'zh': 'æ—¥åº¦è®ºæ–‡'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf monthly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2412.06531', 'title': 'Unraveling the Complexity of Memory in RL Agents: an Approach for Classification and Evaluation', 'url': 'https://huggingface.co/papers/2412.06531', 'abstract': "The incorporation of memory into agents is essential for numerous tasks within the domain of Reinforcement Learning (RL). In particular, memory is paramount for tasks that require the utilization of past information, adaptation to novel environments, and improved sample efficiency. However, the term ``memory'' encompasses a wide range of concepts, which, coupled with the lack of a unified methodology for validating an agent's memory, leads to erroneous judgments about agents' memory capabilities and prevents objective comparison with other memory-enhanced agents. This paper aims to streamline the concept of memory in RL by providing practical precise definitions of agent memory types, such as long-term versus short-term memory and declarative versus procedural memory, inspired by cognitive science. Using these definitions, we categorize different classes of agent memory, propose a robust experimental methodology for evaluating the memory capabilities of RL agents, and standardize evaluations. Furthermore, we empirically demonstrate the importance of adhering to the proposed methodology when evaluating different types of agent memory by conducting experiments with different RL agents and what its violation leads to.", 'score': 53, 'issue_id': 1045, 'pub_date': '2024-12-09', 'pub_date_card': {'ru': '9 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 9', 'zh': '12æœˆ9æ—¥'}, 'hash': '7ddb66a515f8803e', 'authors': ['Egor Cherepanov', 'Nikita Kachaev', 'Artem Zholus', 'Alexey K. Kovalev', 'Aleksandr I. Panov'], 'affiliations': ['AIRI, Moscow, Russia', 'Chandar Research Lab', 'MIPT, Dolgoprudny, Russia', 'Mila Quebec AI Institute', 'Polytechnique MontrÃ©al'], 'pdf_title_img': 'assets/pdf/title_img/2412.06531.jpg', 'data': {'categories': ['#benchmark', '#rl', '#reasoning', '#agents', '#optimization'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¸ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ‡ĞµÑ‚ĞºĞ¸Ğµ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğµ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ½Ğ°ÑƒĞºĞ¾Ğ¹. ĞĞ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ RL-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¾Ğº. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Streamlining Memory Evaluation in Reinforcement Learning', 'desc': 'This paper focuses on the role of memory in Reinforcement Learning (RL) agents, highlighting its importance for tasks that need past information and adaptability. It clarifies the various types of memory, such as long-term and short-term, as well as declarative and procedural memory, drawing from cognitive science. The authors propose a standardized methodology for evaluating the memory capabilities of RL agents, which is currently lacking in the field. Through experiments, they show that following this methodology is crucial for accurate assessments of memory in RL agents.'}, 'zh': {'title': 'å¼ºåŒ–å­¦ä¹ ä¸­çš„è®°å¿†ï¼šå®šä¹‰ä¸è¯„ä¼°çš„é‡è¦æ€§', 'desc': 'åœ¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸­ï¼Œå°†è®°å¿†èå…¥æ™ºèƒ½ä½“æ˜¯è®¸å¤šä»»åŠ¡çš„å…³é”®ã€‚è®°å¿†å¯¹äºåˆ©ç”¨è¿‡å»ä¿¡æ¯ã€é€‚åº”æ–°ç¯å¢ƒå’Œæé«˜æ ·æœ¬æ•ˆç‡è‡³å…³é‡è¦ã€‚æœ¬æ–‡æ—¨åœ¨é€šè¿‡æä¾›æ™ºèƒ½ä½“è®°å¿†ç±»å‹çš„æ˜ç¡®å®šä¹‰ï¼Œç®€åŒ–RLä¸­è®°å¿†çš„æ¦‚å¿µï¼Œå¹¶æå‡ºä¸€ç§è¯„ä¼°æ™ºèƒ½ä½“è®°å¿†èƒ½åŠ›çš„å®éªŒæ–¹æ³•ã€‚æˆ‘ä»¬é€šè¿‡å®éªŒéªŒè¯äº†éµå¾ªè¯¥æ–¹æ³•çš„é‡è¦æ€§ï¼Œå¹¶å±•ç¤ºäº†ä¸éµå¾ªæ—¶å¯èƒ½å¯¼è‡´çš„é”™è¯¯åˆ¤æ–­ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.06559', 'title': 'ProcessBench: Identifying Process Errors in Mathematical Reasoning', 'url': 'https://huggingface.co/papers/2412.06559', 'abstract': 'As language models regularly make mistakes when solving math problems, automated identification of errors in the reasoning process becomes increasingly significant for their scalable oversight. In this paper, we introduce ProcessBench for measuring the ability to identify erroneous steps in mathematical reasoning. It consists of 3,400 test cases, primarily focused on competition- and Olympiad-level math problems. Each test case contains a step-by-step solution with error location annotated by human experts. Models are required to identify the earliest step that contains an error, or conclude that all steps are correct. We conduct extensive evaluation on ProcessBench, involving two types of models: process reward models (PRMs) and critic models, where for the latter we prompt general language models to critique each solution step by step. We draw two main observations: (1) Existing PRMs typically fail to generalize to more challenging math problems beyond GSM8K and MATH. They underperform both critic models (i.e., prompted general language models) and our own trained PRM that is straightforwardly fine-tuned on the PRM800K dataset. (2) The best open-source model, QwQ-32B-Preview, has demonstrated the critique capability competitive with the proprietary model GPT-4o, despite that it still lags behind the reasoning-specialized o1-mini. We hope ProcessBench can foster future research in reasoning process assessment, paving the way toward scalable oversight of language models.', 'score': 36, 'issue_id': 1039, 'pub_date': '2024-12-09', 'pub_date_card': {'ru': '9 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 9', 'zh': '12æœˆ9æ—¥'}, 'hash': '02f9abef0bc10297', 'authors': ['Chujie Zheng', 'Zhenru Zhang', 'Beichen Zhang', 'Runji Lin', 'Keming Lu', 'Bowen Yu', 'Dayiheng Liu', 'Jingren Zhou', 'Junyang Lin'], 'affiliations': ['Qwen Team, Alibaba Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2412.06559.jpg', 'data': {'categories': ['#dataset', '#reasoning', '#math', '#benchmark', '#open_source', '#training'], 'emoji': 'ğŸ§®', 'ru': {'title': 'ProcessBench: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ² Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ProcessBench - Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ² Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ…. ProcessBench ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 3400 Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ², Ğ² Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¼ Ğ¾Ğ»Ğ¸Ğ¼Ğ¿Ğ¸Ğ°Ğ´Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ, Ñ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¾Ñ‚ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ğ¾Ğµ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Process Reward Models (PRM) Ğ¸ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ PRM Ğ¿Ğ»Ğ¾Ñ…Ğ¾ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‚ÑÑ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ° Ğ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ QwQ-32B-Preview Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ñ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ğ¾Ğ¹ GPT-4.'}, 'en': {'title': 'Enhancing Error Detection in Math Reasoning with ProcessBench', 'desc': 'This paper presents ProcessBench, a benchmark designed to evaluate how well language models can identify errors in mathematical reasoning. It includes 3,400 test cases that focus on advanced math problems, with each case providing a detailed solution and expert-annotated error locations. The study compares the performance of process reward models (PRMs) and critic models, revealing that existing PRMs struggle with complex problems while critic models, particularly those based on general language models, perform better. The findings suggest that ProcessBench can enhance research on assessing reasoning processes in language models, contributing to their effective oversight.'}, 'zh': {'title': 'æå‡è¯­è¨€æ¨¡å‹æ•°å­¦æ¨ç†çš„é”™è¯¯è¯†åˆ«èƒ½åŠ›', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºProcessBenchçš„å·¥å…·ï¼Œç”¨äºè¯„ä¼°è¯­è¨€æ¨¡å‹åœ¨æ•°å­¦æ¨ç†ä¸­è¯†åˆ«é”™è¯¯æ­¥éª¤çš„èƒ½åŠ›ã€‚è¯¥å·¥å…·åŒ…å«3400ä¸ªæµ‹è¯•æ¡ˆä¾‹ï¼Œä¸»è¦é›†ä¸­åœ¨ç«èµ›å’Œå¥¥æ—åŒ¹å…‹çº§åˆ«çš„æ•°å­¦é—®é¢˜ä¸Šã€‚æ¯ä¸ªæ¡ˆä¾‹éƒ½æä¾›äº†é€æ­¥è§£å†³æ–¹æ¡ˆï¼Œå¹¶ç”±äººç±»ä¸“å®¶æ ‡æ³¨äº†é”™è¯¯ä½ç½®ã€‚ç ”ç©¶å‘ç°ï¼Œç°æœ‰çš„è¿‡ç¨‹å¥–åŠ±æ¨¡å‹åœ¨æ›´å…·æŒ‘æˆ˜æ€§çš„æ•°å­¦é—®é¢˜ä¸Šè¡¨ç°ä¸ä½³ï¼Œè€Œç»è¿‡å¾®è°ƒçš„PRMåœ¨è¯†åˆ«é”™è¯¯æ–¹é¢ä¼˜äºä¸€èˆ¬è¯­è¨€æ¨¡å‹çš„æ‰¹è¯„èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.06769', 'title': 'Training Large Language Models to Reason in a Continuous Latent Space', 'url': 'https://huggingface.co/papers/2412.06769', 'abstract': 'Large language models (LLMs) are restricted to reason in the "language space", where they typically express the reasoning process with a chain-of-thought (CoT) to solve a complex reasoning problem. However, we argue that language space may not always be optimal for reasoning. For example, most word tokens are primarily for textual coherence and not essential for reasoning, while some critical tokens require complex planning and pose huge challenges to LLMs. To explore the potential of LLM reasoning in an unrestricted latent space instead of using natural language, we introduce a new paradigm Coconut (Chain of Continuous Thought). We utilize the last hidden state of the LLM as a representation of the reasoning state (termed "continuous thought"). Rather than decoding this into a word token, we feed it back to the LLM as the subsequent input embedding directly in the continuous space. Experiments show that Coconut can effectively augment the LLM on several reasoning tasks. This novel latent reasoning paradigm leads to emergent advanced reasoning patterns: the continuous thought can encode multiple alternative next reasoning steps, allowing the model to perform a breadth-first search (BFS) to solve the problem, rather than prematurely committing to a single deterministic path like CoT. Coconut outperforms CoT in certain logical reasoning tasks that require substantial backtracking during planning, with fewer thinking tokens during inference. These findings demonstrate the promise of latent reasoning and offer valuable insights for future research.', 'score': 23, 'issue_id': 1039, 'pub_date': '2024-12-09', 'pub_date_card': {'ru': '9 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 9', 'zh': '12æœˆ9æ—¥'}, 'hash': '0ab7afee5f208244', 'authors': ['Shibo Hao', 'Sainbayar Sukhbaatar', 'DiJia Su', 'Xian Li', 'Zhiting Hu', 'Jason Weston', 'Yuandong Tian'], 'affiliations': ['FAIR at Meta', 'UC San Diego'], 'pdf_title_img': 'assets/pdf/title_img/2412.06769.jpg', 'data': {'categories': ['#architecture', '#reasoning', '#rl', '#training'], 'emoji': 'ğŸ¥¥', 'ru': {'title': 'Coconut: Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Coconut (Chain of Continuous Thought). Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°Ğ·Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğ¹ (CoT), Coconut Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ°Ğº Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ…Ğ¾Ğ´Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Coconut Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒÑĞ¸Ğ»Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¸Ğ¼ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ¸ÑĞº Ğ² ÑˆĞ¸Ñ€Ğ¸Ğ½Ñƒ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ´ĞµÑ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ÑƒÑ‚Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Coconut Ğ½Ğ°Ğ´ CoT Ğ² Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ±ÑĞºÑ‚Ñ€ĞµĞºĞ¸Ğ½Ğ³Ğ° Ğ¿Ñ€Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸.'}, 'en': {'title': 'Unlocking Reasoning Potential with Continuous Thought', 'desc': 'This paper introduces a new reasoning approach for large language models (LLMs) called Coconut, which operates in a continuous latent space rather than the traditional language space. The authors argue that the language space can limit reasoning capabilities, as many tokens are not essential for reasoning tasks. By using the last hidden state of the LLM as a representation of reasoning, Coconut allows for more flexible exploration of reasoning paths, enabling the model to consider multiple alternatives simultaneously. Experimental results show that Coconut outperforms the conventional chain-of-thought method in logical reasoning tasks that require backtracking, demonstrating the effectiveness of this novel paradigm.'}, 'zh': {'title': 'Coconutï¼šè¶…è¶Šè¯­è¨€ç©ºé—´çš„æ¨ç†æ–°èŒƒå¼', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šå¸¸åœ¨â€œè¯­è¨€ç©ºé—´â€ä¸­è¿›è¡Œæ¨ç†ï¼Œä½¿ç”¨é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¥è§£å†³å¤æ‚é—®é¢˜ã€‚ç„¶è€Œï¼Œè¯­è¨€ç©ºé—´å¹¶ä¸æ€»æ˜¯æœ€ä¼˜çš„æ¨ç†æ–¹å¼ï¼Œå› ä¸ºè®¸å¤šè¯æ±‡ä¸»è¦ç”¨äºæ–‡æœ¬è¿è´¯æ€§ï¼Œè€Œéæ¨ç†æœ¬èº«ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°èŒƒå¼Coconutï¼ˆè¿ç»­æ€ç»´é“¾ï¼‰ï¼Œåˆ©ç”¨LLMçš„æœ€åéšè—çŠ¶æ€ä½œä¸ºæ¨ç†çŠ¶æ€çš„è¡¨ç¤ºï¼Œå¹¶ç›´æ¥åœ¨è¿ç»­ç©ºé—´ä¸­è¿›è¡Œè¾“å…¥åµŒå…¥ã€‚å®éªŒè¡¨æ˜ï¼ŒCoconutåœ¨å¤šä¸ªæ¨ç†ä»»åŠ¡ä¸­æœ‰æ•ˆå¢å¼ºäº†LLMçš„è¡¨ç°ï¼Œå°¤å…¶åœ¨éœ€è¦å¤§é‡å›æº¯çš„é€»è¾‘æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºä¼ ç»Ÿçš„CoTã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.06781', 'title': 'Around the World in 80 Timesteps: A Generative Approach to Global Visual Geolocation', 'url': 'https://huggingface.co/papers/2412.06781', 'abstract': "Global visual geolocation predicts where an image was captured on Earth. Since images vary in how precisely they can be localized, this task inherently involves a significant degree of ambiguity. However, existing approaches are deterministic and overlook this aspect. In this paper, we aim to close the gap between traditional geolocalization and modern generative methods. We propose the first generative geolocation approach based on diffusion and Riemannian flow matching, where the denoising process operates directly on the Earth's surface. Our model achieves state-of-the-art performance on three visual geolocation benchmarks: OpenStreetView-5M, YFCC-100M, and iNat21. In addition, we introduce the task of probabilistic visual geolocation, where the model predicts a probability distribution over all possible locations instead of a single point. We introduce new metrics and baselines for this task, demonstrating the advantages of our diffusion-based approach. Codes and models will be made available.", 'score': 9, 'issue_id': 1047, 'pub_date': '2024-12-09', 'pub_date_card': {'ru': '9 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 9', 'zh': '12æœˆ9æ—¥'}, 'hash': '70c292a63437d52a', 'authors': ['Nicolas Dufour', 'David Picard', 'Vicky Kalogeiton', 'Loic Landrieu'], 'affiliations': ['LIGM, Ecole des Ponts, IP Paris, CNRS, UGE', 'LIX, Ecole Polytechnique, IP Paris'], 'pdf_title_img': 'assets/pdf/title_img/2412.06781.jpg', 'data': {'categories': ['#diffusion', '#benchmark', '#cv', '#games', '#dataset'], 'emoji': 'ğŸŒ', 'ru': {'title': 'Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ³ĞµĞ¾Ğ»Ğ¾ĞºĞ°Ñ†Ğ¸Ñ: Ğ¾Ñ‚ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğº Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ³ĞµĞ¾Ğ»Ğ¾ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿ĞµÑ€Ğ²ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ¾Ğ»Ğ¾ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ¸ Ñ€Ğ¸Ğ¼Ğ°Ğ½Ğ¾Ğ²ÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‰ÑƒÑ Ğ½ĞµĞ¿Ğ¾ÑÑ€ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ½Ğ° Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ—ĞµĞ¼Ğ»Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ñ‚Ñ€ĞµÑ… ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ³ĞµĞ¾Ğ»Ğ¾ĞºĞ°Ñ†Ğ¸Ğ¸. ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ° Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ½Ğ¾Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ³ĞµĞ¾Ğ»Ğ¾ĞºĞ°Ñ†Ğ¸Ğ¸, Ğ³Ğ´Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¿Ğ¾ Ğ²ÑĞµĞ¼ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ñ‹Ğ¼ Ğ¼ĞµÑÑ‚Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ÑĞ¼.'}, 'en': {'title': 'Revolutionizing Geolocation with Generative Models', 'desc': "This paper presents a novel approach to global visual geolocation, which determines where an image was taken on Earth. Unlike traditional methods that provide a single location, this research introduces a generative model that predicts a probability distribution over possible locations, addressing the inherent ambiguity in localization. The proposed method utilizes diffusion and Riemannian flow matching to enhance the denoising process directly on the Earth's surface. The model outperforms existing techniques on multiple benchmarks, showcasing the effectiveness of integrating generative methods into geolocation tasks."}, 'zh': {'title': 'ç”Ÿæˆæ€§åœ°ç†å®šä½ï¼šè¶…è¶Šä¼ ç»Ÿæ–¹æ³•çš„åˆ›æ–°', 'desc': 'å…¨çƒè§†è§‰åœ°ç†å®šä½æ—¨åœ¨é¢„æµ‹å›¾åƒåœ¨åœ°çƒä¸Šçš„æ‹æ‘„ä½ç½®ã€‚ç”±äºå›¾åƒçš„å®šä½ç²¾åº¦å„ä¸ç›¸åŒï¼Œè¿™ä¸€ä»»åŠ¡æœ¬è´¨ä¸Šå­˜åœ¨å¾ˆå¤§çš„æ¨¡ç³Šæ€§ã€‚ç°æœ‰çš„æ–¹æ³•å¾€å¾€æ˜¯ç¡®å®šæ€§çš„ï¼Œæœªèƒ½è€ƒè™‘è¿™ä¸€ç‚¹ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£å’Œé»æ›¼æµåŒ¹é…çš„ç”Ÿæˆæ€§åœ°ç†å®šä½æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨åœ°çƒè¡¨é¢ç›´æ¥è¿›è¡Œå»å™ªå¤„ç†ï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.04432', 'title': 'Divot: Diffusion Powers Video Tokenizer for Comprehension and Generation', 'url': 'https://huggingface.co/papers/2412.04432', 'abstract': 'In recent years, there has been a significant surge of interest in unifying image comprehension and generation within Large Language Models (LLMs). This growing interest has prompted us to explore extending this unification to videos. The core challenge lies in developing a versatile video tokenizer that captures both the spatial characteristics and temporal dynamics of videos to obtain representations for LLMs, and the representations can be further decoded into realistic video clips to enable video generation. In this work, we introduce Divot, a Diffusion-Powered Video Tokenizer, which leverages the diffusion process for self-supervised video representation learning. We posit that if a video diffusion model can effectively de-noise video clips by taking the features of a video tokenizer as the condition, then the tokenizer has successfully captured robust spatial and temporal information. Additionally, the video diffusion model inherently functions as a de-tokenizer, decoding videos from their representations. Building upon the Divot tokenizer, we present Divot-Vicuna through video-to-text autoregression and text-to-video generation by modeling the distributions of continuous-valued Divot features with a Gaussian Mixture Model. Experimental results demonstrate that our diffusion-based video tokenizer, when integrated with a pre-trained LLM, achieves competitive performance across various video comprehension and generation benchmarks. The instruction tuned Divot-Vicuna also excels in video storytelling, generating interleaved narratives and corresponding videos.', 'score': 9, 'issue_id': 1044, 'pub_date': '2024-12-05', 'pub_date_card': {'ru': '5 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 5', 'zh': '12æœˆ5æ—¥'}, 'hash': 'f3480cfa7666bb51', 'authors': ['Yuying Ge', 'Yizhuo Li', 'Yixiao Ge', 'Ying Shan'], 'affiliations': ['ARC Lab, Tencent PCG'], 'pdf_title_img': 'assets/pdf/title_img/2412.04432.jpg', 'data': {'categories': ['#benchmark', '#video', '#architecture', '#story_generation', '#multimodal', '#diffusion'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Divot - Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Divot Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾, ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Divot ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Divot-Vicuna, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ°Ñ Ğº Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ²-Ñ‚ĞµĞºÑÑ‚ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Divot Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'Unifying Video Understanding and Generation with Divot', 'desc': "This paper discusses the integration of image understanding and creation within Large Language Models (LLMs) and extends this concept to video processing. The authors introduce Divot, a video tokenizer that uses a diffusion process for self-supervised learning, capturing both spatial and temporal features of videos. By conditioning a video diffusion model on the tokenizer's features, they demonstrate effective video representation and generation. The results show that Divot, when combined with a pre-trained LLM, performs well in video comprehension and storytelling tasks, generating coherent narratives and videos."}, 'zh': {'title': 'è§†é¢‘ç†è§£ä¸ç”Ÿæˆçš„ç»Ÿä¸€æ–°çªç ´', 'desc': 'è¿‘å¹´æ¥ï¼Œå›¾åƒç†è§£ä¸ç”Ÿæˆåœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­çš„ç»Ÿä¸€å¼•èµ·äº†å¹¿æ³›å…³æ³¨ã€‚æˆ‘ä»¬æ¢ç´¢å°†è¿™ç§ç»Ÿä¸€æ‰©å±•åˆ°è§†é¢‘é¢†åŸŸï¼Œé¢ä¸´çš„æ ¸å¿ƒæŒ‘æˆ˜æ˜¯å¼€å‘ä¸€ä¸ªå¤šåŠŸèƒ½çš„è§†é¢‘æ ‡è®°å™¨ï¼Œä»¥æ•æ‰è§†é¢‘çš„ç©ºé—´ç‰¹å¾å’Œæ—¶é—´åŠ¨æ€ã€‚æˆ‘ä»¬æå‡ºäº†Divotï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæ‰©æ•£è¿‡ç¨‹çš„è§†é¢‘æ ‡è®°å™¨ï¼Œèƒ½å¤Ÿè¿›è¡Œè‡ªç›‘ç£çš„è§†é¢‘è¡¨ç¤ºå­¦ä¹ ã€‚é€šè¿‡å°†Divotä¸é¢„è®­ç»ƒçš„LLMç»“åˆï¼Œæˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è§†é¢‘ç†è§£å’Œç”Ÿæˆçš„å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.05939', 'title': 'Exploring Multi-Grained Concept Annotations for Multimodal Large Language Models', 'url': 'https://huggingface.co/papers/2412.05939', 'abstract': 'Multimodal Large Language Models (MLLMs) excel in vision--language tasks by pre-training solely on coarse-grained concept annotations (e.g., image captions). We hypothesize that integrating fine-grained concept annotations (e.g., object labels and object regions) will further improve performance, as both data granularities complement each other in terms of breadth and depth in concept representation. We introduce a new dataset featuring Multimodal Multi-Grained Concept annotations (MMGiC) for MLLMs. In constructing MMGiC, we explore the impact of different data recipes on multimodal comprehension and generation. Our analyses reveal that multi-grained concept annotations integrate and complement each other, under our structured template and a general MLLM framework. We clearly explore and demonstrate the potential of MMGiC to help MLLMs better locate and learn concepts, aligning vision and language at multiple granularities. We further validate our hypothesis by investigating the fair comparison and effective collaboration between MMGiC and image--caption data on 12 multimodal comprehension and generation benchmarks, e.g., their appropriate combination achieve 3.95% and 2.34% absolute improvements over image--caption data alone on POPE and SEED-Bench. Code, data and models will be available at https://github.com/LooperXX/MMGiC.', 'score': 8, 'issue_id': 1042, 'pub_date': '2024-12-08', 'pub_date_card': {'ru': '8 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 8', 'zh': '12æœˆ8æ—¥'}, 'hash': 'a036a456409ed492', 'authors': ['Xiao Xu', 'Tianhao Niu', 'Yuxi Xie', 'Libo Qin', 'Wanxiang Che', 'Min-Yen Kan'], 'affiliations': ['Central South University', 'Harbin Institute of Technology', 'National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2412.05939.jpg', 'data': {'categories': ['#data', '#multimodal', '#alignment', '#dataset', '#benchmark', '#open_source'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'ĞœĞ½Ğ¾Ğ³Ğ¾Ğ·ĞµÑ€Ğ½Ğ¸ÑÑ‚Ñ‹Ğµ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… MMGiC, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ·ĞµÑ€Ğ½Ğ¸ÑÑ‚Ñ‹Ğµ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ·ĞµÑ€Ğ½Ğ¸ÑÑ‚Ñ‹Ğµ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚Ğ¾Ğ² Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ÑÑÑ‚ Ğ´Ñ€ÑƒĞ³ Ğ´Ñ€ÑƒĞ³Ğ° Ğ¸ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ÑÑ‚ MLLM Ğ»ÑƒÑ‡ÑˆĞµ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸ Ğ¸Ğ·ÑƒÑ‡Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚Ñ‹, ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ° Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑƒÑ€Ğ¾Ğ²Ğ½ÑÑ… Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° 12 Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ MMGiC Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞµĞ¹ Ğº Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼.'}, 'en': {'title': 'Enhancing MLLMs with Multi-Grained Concept Annotations', 'desc': "This paper discusses the enhancement of Multimodal Large Language Models (MLLMs) by incorporating fine-grained concept annotations alongside coarse-grained annotations like image captions. The authors introduce a new dataset called MMGiC, which includes both types of annotations to improve the models' understanding and generation of multimodal content. Their experiments show that using multi-grained annotations leads to better alignment between vision and language, resulting in significant performance improvements on various benchmarks. The findings suggest that combining different levels of data granularity can effectively enhance the capabilities of MLLMs in vision-language tasks."}, 'zh': {'title': 'å¤šç²’åº¦æ¦‚å¿µæ³¨é‡Šæå‡å¤šæ¨¡æ€å­¦ä¹ æ•ˆæœ', 'desc': 'å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†è§‰-è¯­è¨€ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä¸»è¦ä¾èµ–äºç²—ç²’åº¦çš„æ¦‚å¿µæ³¨é‡Šï¼ˆå¦‚å›¾åƒæ ‡é¢˜ï¼‰ã€‚æˆ‘ä»¬å‡è®¾ï¼Œæ•´åˆç»†ç²’åº¦çš„æ¦‚å¿µæ³¨é‡Šï¼ˆå¦‚ç‰©ä½“æ ‡ç­¾å’Œç‰©ä½“åŒºåŸŸï¼‰å°†è¿›ä¸€æ­¥æå‡æ€§èƒ½ï¼Œå› ä¸ºè¿™ä¸¤ç§æ•°æ®ç²’åº¦åœ¨æ¦‚å¿µè¡¨ç¤ºçš„å¹¿åº¦å’Œæ·±åº¦ä¸Šç›¸è¾…ç›¸æˆã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†ï¼ŒåŒ…å«å¤šæ¨¡æ€å¤šç²’åº¦æ¦‚å¿µæ³¨é‡Šï¼ˆMMGiCï¼‰ï¼Œå¹¶æ¢è®¨ä¸åŒæ•°æ®ç»„åˆå¯¹å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆçš„å½±å“ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼ŒMMGiCèƒ½å¤Ÿå¸®åŠ©MLLMsæ›´å¥½åœ°å®šä½å’Œå­¦ä¹ æ¦‚å¿µï¼Œåœ¨å¤šä¸ªç²’åº¦ä¸Šå¯¹é½è§†è§‰å’Œè¯­è¨€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.06699', 'title': 'You See it, You Got it: Learning 3D Creation on Pose-Free Videos at Scale', 'url': 'https://huggingface.co/papers/2412.06699', 'abstract': "Recent 3D generation models typically rely on limited-scale 3D `gold-labels' or 2D diffusion priors for 3D content creation. However, their performance is upper-bounded by constrained 3D priors due to the lack of scalable learning paradigms. In this work, we present See3D, a visual-conditional multi-view diffusion model trained on large-scale Internet videos for open-world 3D creation. The model aims to Get 3D knowledge by solely Seeing the visual contents from the vast and rapidly growing video data -- You See it, You Got it. To achieve this, we first scale up the training data using a proposed data curation pipeline that automatically filters out multi-view inconsistencies and insufficient observations from source videos. This results in a high-quality, richly diverse, large-scale dataset of multi-view images, termed WebVi3D, containing 320M frames from 16M video clips. Nevertheless, learning generic 3D priors from videos without explicit 3D geometry or camera pose annotations is nontrivial, and annotating poses for web-scale videos is prohibitively expensive. To eliminate the need for pose conditions, we introduce an innovative visual-condition - a purely 2D-inductive visual signal generated by adding time-dependent noise to the masked video data. Finally, we introduce a novel visual-conditional 3D generation framework by integrating See3D into a warping-based pipeline for high-fidelity 3D generation. Our numerical and visual comparisons on single and sparse reconstruction benchmarks show that See3D, trained on cost-effective and scalable video data, achieves notable zero-shot and open-world generation capabilities, markedly outperforming models trained on costly and constrained 3D datasets. Please refer to our project page at: https://vision.baai.ac.cn/see3d", 'score': 7, 'issue_id': 1041, 'pub_date': '2024-12-09', 'pub_date_card': {'ru': '9 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 9', 'zh': '12æœˆ9æ—¥'}, 'hash': '1d5d0c1aa060a03f', 'authors': ['Baorui Ma', 'Huachen Gao', 'Haoge Deng', 'Zhengxiong Luo', 'Tiejun Huang', 'Lulu Tang', 'Xinlong Wang'], 'affiliations': ['Beijing Academy of Artificial Intelligence (BAAI)'], 'pdf_title_img': 'assets/pdf/title_img/2412.06699.jpg', 'data': {'categories': ['#dataset', '#open_source', '#3d', '#diffusion', '#data', '#benchmark'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Ğ’Ñ‹ Ğ²Ğ¸Ğ´Ğ¸Ñ‚Ğµ - Ğ²Ñ‹ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°ĞµÑ‚Ğµ 3D', 'desc': 'Ğ¡Ğ¼3D - ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ğ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚-Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ 3D-ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°. ĞĞ½Ğ° Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¾ 3D, Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ğµ Ğ¾Ğ³Ñ€Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞ¼Ğ¾Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ñ‹Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… WebVi3D, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 320 Ğ¼Ğ»Ğ½ ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ¸Ğ· 16 Ğ¼Ğ»Ğ½ Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ»Ğ¸Ğ¿Ğ¾Ğ². Ğ¡Ğ¼3D Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½Ğ¾Ğ²Ğ°Ñ‚Ğ¾Ñ€ÑĞºĞ¸Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ñ‹Ğ¹ ÑĞ¸Ğ³Ğ½Ğ°Ğ» Ğ¸ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ² ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´ĞµÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ 3D-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'You See It, You Got It: 3D Generation from Video Data', 'desc': 'This paper introduces See3D, a novel multi-view diffusion model designed for generating 3D content from large-scale Internet videos. Unlike traditional methods that depend on limited 3D labels or 2D priors, See3D leverages a vast dataset of multi-view images, called WebVi3D, which is curated from 320 million frames across 16 million video clips. The model innovatively uses a visual-condition approach that eliminates the need for explicit 3D geometry or camera pose annotations, allowing it to learn generic 3D priors effectively. The results demonstrate that See3D excels in zero-shot and open-world generation tasks, outperforming existing models that rely on expensive 3D datasets.'}, 'zh': {'title': 'é€šè¿‡è§†è§‰å†…å®¹å®ç°å¼€æ”¾ä¸–ç•Œ3Dç”Ÿæˆ', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºSee3Dçš„è§†è§‰æ¡ä»¶å¤šè§†è§’æ‰©æ•£æ¨¡å‹ï¼Œæ—¨åœ¨é€šè¿‡å¤§è§„æ¨¡äº’è”ç½‘è§†é¢‘è¿›è¡Œå¼€æ”¾ä¸–ç•Œçš„3Då†…å®¹ç”Ÿæˆã€‚è¯¥æ¨¡å‹é€šè¿‡è‡ªåŠ¨è¿‡æ»¤å¤šè§†è§’ä¸ä¸€è‡´æ€§å’Œä¸è¶³è§‚å¯Ÿï¼Œæ„å»ºäº†ä¸€ä¸ªåŒ…å«320Må¸§çš„é«˜è´¨é‡å¤šè§†è§’å›¾åƒæ•°æ®é›†WebVi3Dã€‚ä¸ºäº†æ¶ˆé™¤å¯¹ç›¸æœºå§¿æ€çš„ä¾èµ–ï¼ŒSee3Då¼•å…¥äº†ä¸€ç§åˆ›æ–°çš„è§†è§‰æ¡ä»¶ï¼Œé€šè¿‡åœ¨æ©è”½è§†é¢‘æ•°æ®ä¸­æ·»åŠ æ—¶é—´ç›¸å…³å™ªå£°ç”Ÿæˆçº¯2Dè¯±å¯¼è§†è§‰ä¿¡å·ã€‚æœ€ç»ˆï¼ŒSee3Dåœ¨å•ä¸€å’Œç¨€ç–é‡å»ºåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„é›¶æ ·æœ¬å’Œå¼€æ”¾ä¸–ç•Œç”Ÿæˆèƒ½åŠ›ï¼Œè¶…è¶Šäº†åŸºäºæ˜‚è´µ3Dæ•°æ®é›†è®­ç»ƒçš„æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.03123', 'title': 'Robust Multi-bit Text Watermark with LLM-based Paraphrasers', 'url': 'https://huggingface.co/papers/2412.03123', 'abstract': 'We propose an imperceptible multi-bit text watermark embedded by paraphrasing with LLMs. We fine-tune a pair of LLM paraphrasers that are designed to behave differently so that their paraphrasing difference reflected in the text semantics can be identified by a trained decoder. To embed our multi-bit watermark, we use two paraphrasers alternatively to encode the pre-defined binary code at the sentence level. Then we use a text classifier as the decoder to decode each bit of the watermark. Through extensive experiments, we show that our watermarks can achieve over 99.99\\% detection AUC with small (1.1B) text paraphrasers while keeping the semantic information of the original sentence. More importantly, our pipeline is robust under word substitution and sentence paraphrasing perturbations and generalizes well to out-of-distributional data. We also show the stealthiness of our watermark with LLM-based evaluation. We open-source the code: https://github.com/xiaojunxu/multi-bit-text-watermark.', 'score': 5, 'issue_id': 1040, 'pub_date': '2024-12-04', 'pub_date_card': {'ru': '4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 4', 'zh': '12æœˆ4æ—¥'}, 'hash': 'bfad93a82eaec473', 'authors': ['Xiaojun Xu', 'Jinghan Jia', 'Yuanshun Yao', 'Yang Liu', 'Hang Li'], 'affiliations': ['ByteDance Research', 'Michigan State University', 'University of California, Santa Cruz'], 'pdf_title_img': 'assets/pdf/title_img/2412.03123.jpg', 'data': {'categories': ['#data', '#multimodal', '#dataset', '#open_source', '#small_models'], 'emoji': 'ğŸ’§', 'ru': {'title': 'ĞĞµĞ²Ğ¸Ğ´Ğ¸Ğ¼Ñ‹Ğµ Ğ²Ğ¾Ğ´ÑĞ½Ñ‹Ğµ Ğ·Ğ½Ğ°ĞºĞ¸ Ğ² Ñ‚ĞµĞºÑÑ‚Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½ĞµĞ·Ğ°Ğ¼ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ±Ğ¸Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ´ÑĞ½Ğ¾Ğ³Ğ¾ Ğ·Ğ½Ğ°ĞºĞ° Ğ² Ñ‚ĞµĞºÑÑ‚ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿ĞµÑ€ĞµÑ„Ñ€Ğ°Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM). ĞĞ½Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ Ğ¿Ğ°Ñ€Ñƒ LLM-Ğ¿Ğ°Ñ€Ğ°Ñ„Ñ€Ğ°Ğ·ĞµÑ€Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ²ĞµĞ´ÑƒÑ‚ ÑĞµĞ±Ñ Ğ¿Ğ¾-Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¼Ñƒ, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¸Ñ… Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ² Ğ¿ĞµÑ€ĞµÑ„Ñ€Ğ°Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¼Ğ¾Ğ³Ğ»Ğ¸ Ğ±Ñ‹Ñ‚ÑŒ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ñ‹ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ¼. Ğ”Ğ»Ñ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾Ğ´ÑĞ½Ğ¾Ğ³Ğ¾ Ğ·Ğ½Ğ°ĞºĞ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ÑÑ Ğ´Ğ²Ğ° Ğ¿Ğ°Ñ€Ğ°Ñ„Ñ€Ğ°Ğ·ĞµÑ€Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ±Ğ¸Ğ½Ğ°Ñ€Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ´ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°, ĞµĞ³Ğ¾ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğº Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼ Ğ¸ Ñ…Ğ¾Ñ€Ğ¾ÑˆÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ.'}, 'en': {'title': 'Stealthy Multi-Bit Watermarking via Paraphrasing with LLMs', 'desc': "This paper presents a method for embedding a multi-bit watermark in text using paraphrasing techniques with large language models (LLMs). The authors fine-tune two distinct LLM paraphrasers that create different paraphrases, allowing a trained decoder to identify the semantic differences and extract the embedded watermark. The watermark is encoded at the sentence level by alternating between the two paraphrasers, achieving high detection accuracy while preserving the original text's meaning. The proposed method demonstrates robustness against various text perturbations and maintains effectiveness even with out-of-distribution data."}, 'zh': {'title': 'éšå½¢æ°´å°ï¼Œè¯­ä¹‰ä¿ç•™ï¼', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šè¿‡å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œçš„ä¸å¯å¯Ÿè§‰çš„å¤šæ¯”ç‰¹æ–‡æœ¬æ°´å°åµŒå…¥æ–¹æ³•ã€‚æˆ‘ä»¬å¾®è°ƒäº†ä¸€å¯¹è¡¨ç°ä¸åŒçš„LLMæ”¹å†™å™¨ï¼Œä»¥ä¾¿é€šè¿‡æ–‡æœ¬è¯­ä¹‰çš„å·®å¼‚æ¥è¯†åˆ«å…¶æ”¹å†™ç»“æœã€‚ä¸ºäº†åµŒå…¥å¤šæ¯”ç‰¹æ°´å°ï¼Œæˆ‘ä»¬äº¤æ›¿ä½¿ç”¨ä¸¤ä¸ªæ”¹å†™å™¨åœ¨å¥å­çº§åˆ«ç¼–ç é¢„å®šä¹‰çš„äºŒè¿›åˆ¶ä»£ç ï¼Œå¹¶ä½¿ç”¨æ–‡æœ¬åˆ†ç±»å™¨ä½œä¸ºè§£ç å™¨æ¥è§£ç æ¯ä¸€ä½æ°´å°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ°´å°åœ¨ä¿æŒåŸå¥è¯­ä¹‰ä¿¡æ¯çš„åŒæ—¶ï¼Œæ£€æµ‹AUCè¶…è¿‡99.99%ï¼Œå¹¶ä¸”åœ¨è¯è¯­æ›¿æ¢å’Œå¥å­æ”¹å†™æ‰°åŠ¨ä¸‹è¡¨ç°å‡ºè‰¯å¥½çš„é²æ£’æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.05600', 'title': 'Global and Dense Embeddings of Earth: Major TOM Floating in the Latent Space', 'url': 'https://huggingface.co/papers/2412.05600', 'abstract': "With the ever-increasing volumes of the Earth observation data present in the archives of large programmes such as Copernicus, there is a growing need for efficient vector representations of the underlying raw data. The approach of extracting feature representations from pretrained deep neural networks is a powerful approach that can provide semantic abstractions of the input data. However, the way this is done for imagery archives containing geospatial data has not yet been defined. In this work, an extension is proposed to an existing community project, Major TOM, focused on the provision and standardization of open and free AI-ready datasets for Earth observation. Furthermore, four global and dense embedding datasets are released openly and for free along with the publication of this manuscript, resulting in the most comprehensive global open dataset of geospatial visual embeddings in terms of covered Earth's surface.", 'score': 4, 'issue_id': 1046, 'pub_date': '2024-12-07', 'pub_date_card': {'ru': '7 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 7', 'zh': '12æœˆ7æ—¥'}, 'hash': 'a30334645404dda6', 'authors': ['Mikolaj Czerkawski', 'Marcin Kluczek', 'JÄ™drzej S. Bojanowski'], 'affiliations': ['CloudFerro, Warsaw, Poland', 'Î¦-lab, European Space Agency, Frascati, Italy'], 'pdf_title_img': 'assets/pdf/title_img/2412.05600.jpg', 'data': {'categories': ['#data', '#dataset', '#open_source'], 'emoji': 'ğŸ›°ï¸', 'ru': {'title': 'Ğ“Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° ÑĞ¿ÑƒÑ‚Ğ½Ğ¸ĞºĞ¾Ğ²Ñ‹Ñ… ÑĞ½Ğ¸Ğ¼ĞºĞ¾Ğ²', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ¸ÑÑ‚Ğ°Ğ½Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ğ¾Ğ½Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ—ĞµĞ¼Ğ»Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑÑÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚ Major TOM Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ñ Ğ—ĞµĞ¼Ğ»Ğ¸. ĞĞ½Ğ¸ Ğ²Ñ‹Ğ¿ÑƒÑĞºĞ°ÑÑ‚ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ², Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ· ÑĞ¿ÑƒÑ‚Ğ½Ğ¸ĞºĞ¾Ğ²Ñ‹Ñ… ÑĞ½Ğ¸Ğ¼ĞºĞ¾Ğ². Ğ­Ñ‚Ğ¾ ÑĞ°Ğ¼Ñ‹Ğ¹ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ³ĞµĞ¾Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ñ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ğ° Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ—ĞµĞ¼Ğ»Ğ¸.'}, 'en': {'title': 'Unlocking Earth Data: Semantic Embeddings for Geospatial Insights', 'desc': 'This paper addresses the challenge of efficiently representing large volumes of Earth observation data, particularly from programs like Copernicus. It proposes using pretrained deep neural networks to extract feature representations that provide semantic insights into geospatial imagery. The authors extend the Major TOM project, which aims to standardize open AI-ready datasets for Earth observation. Additionally, they release four global embedding datasets, creating a comprehensive resource for researchers working with geospatial visual embeddings.'}, 'zh': {'title': 'é«˜æ•ˆåœ°çƒè§‚æµ‹æ•°æ®è¡¨ç¤ºçš„æ–°æ–¹æ³•', 'desc': 'éšç€åœ°çƒè§‚æµ‹æ•°æ®é‡çš„ä¸æ–­å¢åŠ ï¼Œå¦‚ä½•æœ‰æ•ˆåœ°è¡¨ç¤ºè¿™äº›åŸå§‹æ•°æ®å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ä»é¢„è®­ç»ƒæ·±åº¦ç¥ç»ç½‘ç»œä¸­æå–ç‰¹å¾è¡¨ç¤ºçš„æ–¹æ³•ï¼Œå¯ä»¥ä¸ºè¾“å…¥æ•°æ®æä¾›è¯­ä¹‰æŠ½è±¡ã€‚é’ˆå¯¹åŒ…å«åœ°ç†ç©ºé—´æ•°æ®çš„å›¾åƒæ¡£æ¡ˆï¼Œæœ¬æ–‡å®šä¹‰äº†ä¸€ç§æ–°çš„å¤„ç†æ–¹å¼ï¼Œå¹¶æ‰©å±•äº†ç°æœ‰çš„ç¤¾åŒºé¡¹ç›®Major TOMã€‚æœ€åï¼Œæœ¬æ–‡å…¬å¼€å‘å¸ƒäº†å››ä¸ªå…¨çƒå¯†é›†åµŒå…¥æ•°æ®é›†ï¼Œæˆä¸ºè¦†ç›–åœ°çƒè¡¨é¢çš„æœ€å…¨é¢çš„å¼€æ”¾åœ°ç†ç©ºé—´è§†è§‰åµŒå…¥æ•°æ®é›†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.06767', 'title': 'MAtCha Gaussians: Atlas of Charts for High-Quality Geometry and Photorealism From Sparse Views', 'url': 'https://huggingface.co/papers/2412.06767', 'abstract': "We present a novel appearance model that simultaneously realizes explicit high-quality 3D surface mesh recovery and photorealistic novel view synthesis from sparse view samples. Our key idea is to model the underlying scene geometry Mesh as an Atlas of Charts which we render with 2D Gaussian surfels (MAtCha Gaussians). MAtCha distills high-frequency scene surface details from an off-the-shelf monocular depth estimator and refines it through Gaussian surfel rendering. The Gaussian surfels are attached to the charts on the fly, satisfying photorealism of neural volumetric rendering and crisp geometry of a mesh model, i.e., two seemingly contradicting goals in a single model. At the core of MAtCha lies a novel neural deformation model and a structure loss that preserve the fine surface details distilled from learned monocular depths while addressing their fundamental scale ambiguities. Results of extensive experimental validation demonstrate MAtCha's state-of-the-art quality of surface reconstruction and photorealism on-par with top contenders but with dramatic reduction in the number of input views and computational time. We believe MAtCha will serve as a foundational tool for any visual application in vision, graphics, and robotics that require explicit geometry in addition to photorealism. Our project page is the following: https://anttwo.github.io/matcha/", 'score': 3, 'issue_id': 1050, 'pub_date': '2024-12-09', 'pub_date_card': {'ru': '9 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 9', 'zh': '12æœˆ9æ—¥'}, 'hash': '3f286ad9a4744a4d', 'authors': ['Antoine GuÃ©don', 'Tomoki Ichikawa', 'Kohei Yamashita', 'Ko Nishino'], 'affiliations': ['Graduate School of Informatics, Kyoto University, Japan', 'LIGM, Ecole des Ponts, Univ Gustave Eiffel, CNRS, France'], 'pdf_title_img': 'assets/pdf/title_img/2412.06767.jpg', 'data': {'categories': ['#3d', '#cv', '#robotics'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ¤Ğ¾Ñ‚Ğ¾Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ°Ñ 3D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ğ¸Ğ· Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ³Ğ¾ Ñ‡Ğ¸ÑĞ»Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ Ğ²Ğ¸Ğ´Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ 3D-ÑĞµÑ‚ĞºÑƒ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€ÑƒĞµÑ‚ Ñ„Ğ¾Ñ‚Ğ¾Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ñ€Ğ°ĞºÑƒÑ€ÑÑ‹ Ğ¸Ğ· Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´Ğ¾Ğ². ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ ÑÑ†ĞµĞ½Ñ‹ ĞºĞ°Ğº Ğ°Ñ‚Ğ»Ğ°ÑĞ° Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾Ñ‚Ñ€Ğ¸ÑĞ¾Ğ²Ñ‹Ğ²Ğ°ÑÑ‚ÑÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ 2D Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹Ñ… ÑÑ‘Ñ€Ñ„ĞµĞ»Ğ¾Ğ² (MAtCha Gaussians). MAtCha Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ¸ ÑÑ†ĞµĞ½Ñ‹ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ñ‹ Ğ¸Ğ· Ğ¼Ğ¾Ğ½Ğ¾ĞºÑƒĞ»ÑÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ†ĞµĞ½Ñ‰Ğ¸ĞºĞ° Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ¸ ÑƒÑ‚Ğ¾Ñ‡Ğ½ÑĞµÑ‚ Ğ¸Ñ… Ñ‡ĞµÑ€ĞµĞ· Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³ Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹Ñ… ÑÑ‘Ñ€Ñ„ĞµĞ»Ğ¾Ğ². Ğ’ Ğ¾ÑĞ½Ğ¾Ğ²Ğµ MAtCha Ğ»ĞµĞ¶Ğ¸Ñ‚ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ğ°Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑÑ‚ Ğ¼ĞµĞ»ĞºĞ¸Ğµ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¸Ğ· Ğ¾Ñ†ĞµĞ½Ğ¾Ğº Ğ¼Ğ¾Ğ½Ğ¾ĞºÑƒĞ»ÑÑ€Ğ½Ğ¾Ğ¹ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹.'}, 'en': {'title': 'MAtCha: Merging Geometry and Photorealism in 3D Reconstruction', 'desc': 'This paper introduces MAtCha, a new model that effectively combines high-quality 3D surface mesh recovery with photorealistic image generation from limited view samples. The model uses an innovative approach called an Atlas of Charts, which employs 2D Gaussian surfels to enhance the visual quality of the rendered scenes. By integrating a neural deformation model and a structure loss, MAtCha preserves intricate surface details while resolving scale ambiguities from monocular depth estimations. Experimental results show that MAtCha achieves top-tier performance in both surface reconstruction and photorealism, requiring fewer input views and less computational power than existing methods.'}, 'zh': {'title': 'MAtChaï¼šé«˜æ•ˆçš„3Dè¡¨é¢é‡å»ºä¸é€¼çœŸæ¸²æŸ“ç»“åˆ', 'desc': 'æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¤–è§‚æ¨¡å‹ï¼Œèƒ½å¤ŸåŒæ—¶å®ç°é«˜è´¨é‡çš„3Dè¡¨é¢ç½‘æ ¼æ¢å¤å’Œé€¼çœŸçš„æ–°è§†å›¾åˆæˆã€‚æˆ‘ä»¬çš„å…³é”®æ€æƒ³æ˜¯å°†åœºæ™¯å‡ ä½•å»ºæ¨¡ä¸ºä¸€ä¸ªå›¾é›†ï¼Œé€šè¿‡2Dé«˜æ–¯è¡¨é¢ç‚¹è¿›è¡Œæ¸²æŸ“ã€‚MAtChaä»ç°æˆçš„å•ç›®æ·±åº¦ä¼°è®¡å™¨ä¸­æå–é«˜é¢‘åœºæ™¯è¡¨é¢ç»†èŠ‚ï¼Œå¹¶é€šè¿‡é«˜æ–¯è¡¨é¢ç‚¹æ¸²æŸ“è¿›è¡Œç²¾ç»†åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMAtChaåœ¨è¡¨é¢é‡å»ºå’Œé€¼çœŸåº¦æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„è´¨é‡ï¼ŒåŒæ—¶æ˜¾è‘—å‡å°‘äº†è¾“å…¥è§†å›¾æ•°é‡å’Œè®¡ç®—æ—¶é—´ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.06782', 'title': 'CARP: Visuomotor Policy Learning via Coarse-to-Fine Autoregressive Prediction', 'url': 'https://huggingface.co/papers/2412.06782', 'abstract': 'In robotic visuomotor policy learning, diffusion-based models have achieved significant success in improving the accuracy of action trajectory generation compared to traditional autoregressive models. However, they suffer from inefficiency due to multiple denoising steps and limited flexibility from complex constraints. In this paper, we introduce Coarse-to-Fine AutoRegressive Policy (CARP), a novel paradigm for visuomotor policy learning that redefines the autoregressive action generation process as a coarse-to-fine, next-scale approach. CARP decouples action generation into two stages: first, an action autoencoder learns multi-scale representations of the entire action sequence; then, a GPT-style transformer refines the sequence prediction through a coarse-to-fine autoregressive process. This straightforward and intuitive approach produces highly accurate and smooth actions, matching or even surpassing the performance of diffusion-based policies while maintaining efficiency on par with autoregressive policies. We conduct extensive evaluations across diverse settings, including single-task and multi-task scenarios on state-based and image-based simulation benchmarks, as well as real-world tasks. CARP achieves competitive success rates, with up to a 10% improvement, and delivers 10x faster inference compared to state-of-the-art policies, establishing a high-performance, efficient, and flexible paradigm for action generation in robotic tasks.', 'score': 3, 'issue_id': 1039, 'pub_date': '2024-12-09', 'pub_date_card': {'ru': '9 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 9', 'zh': '12æœˆ9æ—¥'}, 'hash': '584dec780be05e2d', 'authors': ['Zhefei Gong', 'Pengxiang Ding', 'Shangke Lyu', 'Siteng Huang', 'Mingyang Sun', 'Wei Zhao', 'Zhaoxin Fan', 'Donglin Wang'], 'affiliations': ['Beijing Advanced Innovation Center for Future Blockchain and Privacy Computing', 'Westlake University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2412.06782.jpg', 'data': {'categories': ['#optimization', '#diffusion', '#agents', '#training', '#robotics'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'CARP: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğµ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ¾Ğ¼Ğ¾Ñ‚Ğ¾Ñ€Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ², Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ CARP. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¼ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ Ğ² ÑÑ‚Ğ¸Ğ»Ğµ GPT ÑƒÑ‚Ğ¾Ñ‡Ğ½ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ. CARP Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¿Ğ»Ğ°Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼ÑƒÑ Ğ¸Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‰ÑƒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»Ğ¸ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ² 10 Ñ€Ğ°Ğ· Ğ±Ğ¾Ğ»ĞµĞµ Ğ±Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¹ Ğ²Ñ‹Ğ²Ğ¾Ğ´ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Efficient and Accurate Action Generation with CARP', 'desc': 'This paper presents the Coarse-to-Fine AutoRegressive Policy (CARP), a new method for robotic visuomotor policy learning that enhances action trajectory generation. CARP improves upon traditional autoregressive models by breaking down the action generation into two stages: first, it uses an action autoencoder to create multi-scale representations, and then a GPT-style transformer refines these predictions. This approach not only increases the accuracy and smoothness of actions but also maintains efficiency, achieving up to 10x faster inference than existing methods. Extensive evaluations show that CARP outperforms diffusion-based models and achieves competitive success rates in various robotic tasks.'}, 'zh': {'title': 'é«˜æ•ˆçµæ´»çš„æœºå™¨äººåŠ¨ä½œç”Ÿæˆæ–°èŒƒå¼', 'desc': 'åœ¨æœºå™¨äººè§†è§‰è¿åŠ¨ç­–ç•¥å­¦ä¹ ä¸­ï¼ŒåŸºäºæ‰©æ•£æ¨¡å‹çš„æŠ€æœ¯åœ¨åŠ¨ä½œè½¨è¿¹ç”Ÿæˆçš„å‡†ç¡®æ€§ä¸Šå–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†ç”±äºå¤šæ¬¡å»å™ªæ­¥éª¤å’Œå¤æ‚çº¦æŸï¼Œæ•ˆç‡è¾ƒä½ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„ç²—åˆ°ç»†è‡ªå›å½’ç­–ç•¥ï¼ˆCARPï¼‰ï¼Œå°†è‡ªå›å½’åŠ¨ä½œç”Ÿæˆè¿‡ç¨‹é‡æ–°å®šä¹‰ä¸ºç²—åˆ°ç»†çš„ä¸‹ä¸€å°ºåº¦æ–¹æ³•ã€‚CARPå°†åŠ¨ä½œç”Ÿæˆåˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šé¦–å…ˆï¼ŒåŠ¨ä½œè‡ªç¼–ç å™¨å­¦ä¹ æ•´ä¸ªåŠ¨ä½œåºåˆ—çš„å¤šå°ºåº¦è¡¨ç¤ºï¼›ç„¶åï¼ŒGPTé£æ ¼çš„å˜æ¢å™¨é€šè¿‡ç²—åˆ°ç»†çš„è‡ªå›å½’è¿‡ç¨‹ç²¾ç‚¼åºåˆ—é¢„æµ‹ã€‚è¯¥æ–¹æ³•åœ¨æ•ˆç‡ä¸Šä¸è‡ªå›å½’ç­–ç•¥ç›¸å½“ï¼ŒåŒæ—¶åœ¨å‡†ç¡®æ€§å’Œæµç•…æ€§ä¸Šä¸åŸºäºæ‰©æ•£çš„ç­–ç•¥ç›¸åŒ¹é…æˆ–è¶…è¶Šï¼Œå±•ç¤ºäº†é«˜æ€§èƒ½ã€é«˜æ•ˆå’Œçµæ´»çš„æœºå™¨äººä»»åŠ¡åŠ¨ä½œç”Ÿæˆæ–°èŒƒå¼ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.04470', 'title': 'Turbo3D: Ultra-fast Text-to-3D Generation', 'url': 'https://huggingface.co/papers/2412.04470', 'abstract': "We present Turbo3D, an ultra-fast text-to-3D system capable of generating high-quality Gaussian splatting assets in under one second. Turbo3D employs a rapid 4-step, 4-view diffusion generator and an efficient feed-forward Gaussian reconstructor, both operating in latent space. The 4-step, 4-view generator is a student model distilled through a novel Dual-Teacher approach, which encourages the student to learn view consistency from a multi-view teacher and photo-realism from a single-view teacher. By shifting the Gaussian reconstructor's inputs from pixel space to latent space, we eliminate the extra image decoding time and halve the transformer sequence length for maximum efficiency. Our method demonstrates superior 3D generation results compared to previous baselines, while operating in a fraction of their runtime.", 'score': 1, 'issue_id': 1053, 'pub_date': '2024-12-05', 'pub_date_card': {'ru': '5 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 5', 'zh': '12æœˆ5æ—¥'}, 'hash': '426824b2e09af1a0', 'authors': ['Hanzhe Hu', 'Tianwei Yin', 'Fujun Luan', 'Yiwei Hu', 'Hao Tan', 'Zexiang Xu', 'Sai Bi', 'Shubham Tulsiani', 'Kai Zhang'], 'affiliations': ['Adobe Research', 'Carnegie Mellon University', 'Massachusetts Institute of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2412.04470.jpg', 'data': {'categories': ['#3d', '#architecture', '#diffusion', '#optimization'], 'emoji': 'ğŸš€', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² 3D-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸: Ğ¾Ñ‚ Ñ‚ĞµĞºÑÑ‚Ğ° Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ·Ğ° Ğ´Ğ¾Ğ»Ğ¸ ÑĞµĞºÑƒĞ½Ğ´Ñ‹', 'desc': 'Turbo3D - ÑÑ‚Ğ¾ ÑĞ²ĞµÑ€Ñ…Ğ±Ñ‹ÑÑ‚Ñ€Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ, ÑĞ¾Ğ·Ğ´Ğ°ÑÑ‰Ğ°Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ğ° ÑĞ¿Ğ»Ğ°Ñ‚Ñ‚Ğ¸Ğ½Ğ³Ğ° Ğ¼ĞµĞ½ĞµĞµ Ñ‡ĞµĞ¼ Ğ·Ğ° ÑĞµĞºÑƒĞ½Ğ´Ñƒ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ±Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¹ 4-ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğ¹, 4-Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ñ‹Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ‚Ğ¾Ñ€ Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹Ñ… ÑĞ¿Ğ»Ğ°Ñ‚Ğ¾Ğ², Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‰Ğ¸Ğµ Ğ² Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ. Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Dual-Teacher, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¾Ñ‰Ñ€ÑĞµÑ‚ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ² Ğ¸ Ñ„Ğ¾Ñ‚Ğ¾Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ. Ğ‘Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑÑƒ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ‚Ğ¾Ñ€Ğ° Ğ² Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾, Turbo3D Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ².'}, 'en': {'title': 'Turbo3D: Lightning-Fast Text-to-3D Generation!', 'desc': 'Turbo3D is a cutting-edge system that quickly converts text descriptions into 3D models using Gaussian splatting techniques. It utilizes a unique 4-step, 4-view diffusion generator, which is trained through a Dual-Teacher method to ensure both view consistency and photo-realism. By processing data in latent space instead of pixel space, Turbo3D significantly reduces the time needed for image decoding and optimizes the transformer sequence length. This innovative approach results in faster and higher-quality 3D asset generation compared to existing methods.'}, 'zh': {'title': 'Turbo3Dï¼šè¶…å¿«é€Ÿæ–‡æœ¬åˆ°3Dç”Ÿæˆçš„é©å‘½', 'desc': 'Turbo3Dæ˜¯ä¸€ç§è¶…å¿«é€Ÿçš„æ–‡æœ¬åˆ°3Dç”Ÿæˆç³»ç»Ÿï¼Œèƒ½å¤Ÿåœ¨ä¸åˆ°ä¸€ç§’çš„æ—¶é—´å†…ç”Ÿæˆé«˜è´¨é‡çš„é«˜æ–¯ç‚¹äº‘èµ„äº§ã€‚è¯¥ç³»ç»Ÿé‡‡ç”¨å¿«é€Ÿçš„å››æ­¥ã€å››è§†å›¾æ‰©æ•£ç”Ÿæˆå™¨å’Œé«˜æ•ˆçš„å‰é¦ˆé«˜æ–¯é‡æ„å™¨ï¼Œå‡åœ¨æ½œåœ¨ç©ºé—´ä¸­è¿è¡Œã€‚é€šè¿‡æ–°é¢–çš„åŒæ•™å¸ˆæ–¹æ³•ï¼Œå››æ­¥ã€å››è§†å›¾ç”Ÿæˆå™¨çš„å­¦ç”Ÿæ¨¡å‹èƒ½å¤Ÿä»å¤šè§†å›¾æ•™å¸ˆé‚£é‡Œå­¦ä¹ è§†å›¾ä¸€è‡´æ€§ï¼Œå¹¶ä»å•è§†å›¾æ•™å¸ˆé‚£é‡Œå­¦ä¹ ç…§ç‰‡çœŸå®æ„Ÿã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼ŒTurbo3Dåœ¨3Dç”Ÿæˆç»“æœä¸Šä¼˜äºä¹‹å‰çš„åŸºçº¿ï¼ŒåŒæ—¶è¿è¡Œæ—¶é—´ä»…ä¸ºå…¶ä¸€å°éƒ¨åˆ†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.04144', 'title': "If You Can't Use Them, Recycle Them: Optimizing Merging at Scale Mitigates Performance Tradeoffs", 'url': 'https://huggingface.co/papers/2412.04144', 'abstract': "Model merging has shown great promise at combining expert models, but the benefit of merging is unclear when merging ``generalist'' models trained on many tasks. We explore merging in the context of large (sim100B) models, by recycling checkpoints that exhibit tradeoffs among different tasks. Such checkpoints are often created in the process of developing a frontier model, and many suboptimal ones are usually discarded. Given a pool of model checkpoints obtained from different training runs (e.g., different stages, objectives, hyperparameters, and data mixtures), which naturally show tradeoffs across different language capabilities (e.g., instruction following vs. code generation), we investigate whether merging can recycle such suboptimal models into a Pareto-optimal one. Our optimization algorithm tunes the weight of each checkpoint in a linear combination, resulting in a Pareto-optimal models that outperforms both individual models and merge-based baselines. Further analysis shows that good merges tend to include almost all checkpoints with with non-zero weights, indicating that even seemingly bad initial checkpoints can contribute to good final merges.", 'score': 1, 'issue_id': 1049, 'pub_date': '2024-12-05', 'pub_date_card': {'ru': '5 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 5', 'zh': '12æœˆ5æ—¥'}, 'hash': '405fa76c78968872', 'authors': ['Muhammad Khalifa', 'Yi-Chern Tan', 'Arash Ahmadian', 'Tom Hosking', 'Honglak Lee', 'Lu Wang', 'Ahmet ÃœstÃ¼n', 'Tom Sherborne', 'Matthias GallÃ©'], 'affiliations': ['Cohere', 'Cohere For AI', 'University of Michigan'], 'pdf_title_img': 'assets/pdf/title_img/2412.04144.jpg', 'data': {'categories': ['#training', '#architecture', '#optimization'], 'emoji': 'ğŸ§©', 'ru': {'title': 'ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ĞŸĞ°Ñ€ĞµÑ‚Ğ¾-Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (Ğ¾ĞºĞ¾Ğ»Ğ¾ 100 Ğ¼Ğ»Ñ€Ğ´ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²ĞµÑĞ° Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ‡ĞµĞºĞ¿Ğ¾Ğ¹Ğ½Ñ‚Ğ¾Ğ² Ğ² Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ ĞºĞ°Ğº Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‚Ğ°Ğº Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ¸Ğ·Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ½ĞµĞ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‡ĞµĞºĞ¿Ğ¾Ğ¹Ğ½Ñ‚Ñ‹ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ²Ğ½ĞµÑÑ‚Ğ¸ Ğ²ĞºĞ»Ğ°Ğ´ Ğ² Ğ¸Ñ‚Ğ¾Ğ³Ğ¾Ğ²Ğ¾Ğµ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾Ğµ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ.'}, 'en': {'title': 'Unlocking Potential: Merging Suboptimal Models for Optimal Performance', 'desc': 'This paper investigates the process of merging large language models, specifically those with around 100 billion parameters, to enhance their performance across various tasks. The authors focus on recycling suboptimal model checkpoints, which are often discarded during model development, to create a new model that is Pareto-optimal. By employing an optimization algorithm that adjusts the weights of these checkpoints, the resulting merged model surpasses both individual models and traditional merging methods. The findings suggest that even checkpoints that appear to be poor can play a significant role in achieving better overall performance when merged effectively.'}, 'zh': {'title': 'åˆå¹¶æ¨¡å‹ï¼Œä¼˜åŒ–æ€§èƒ½çš„å…³é”®', 'desc': 'æ¨¡å‹åˆå¹¶åœ¨ç»“åˆä¸“å®¶æ¨¡å‹æ–¹é¢æ˜¾ç¤ºå‡ºå¾ˆå¤§æ½œåŠ›ï¼Œä½†åœ¨åˆå¹¶è®­ç»ƒäº†å¤šç§ä»»åŠ¡çš„â€œé€šç”¨â€æ¨¡å‹æ—¶ï¼Œå…¶å¥½å¤„å¹¶ä¸æ˜ç¡®ã€‚æˆ‘ä»¬åœ¨å¤§å‹æ¨¡å‹ï¼ˆå¦‚1000äº¿å‚æ•°ï¼‰ä¸­æ¢ç´¢åˆå¹¶ï¼Œé€šè¿‡å›æ”¶åœ¨ä¸åŒä»»åŠ¡ä¹‹é—´å­˜åœ¨æƒè¡¡çš„æ£€æŸ¥ç‚¹ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡è°ƒæ•´æ¯ä¸ªæ£€æŸ¥ç‚¹çš„æƒé‡ï¼Œç”Ÿæˆä¸€ä¸ªå¸•ç´¯æ‰˜æœ€ä¼˜æ¨¡å‹ï¼Œè¶…è¶Šäº†å•ä¸ªæ¨¡å‹å’ŒåŸºäºåˆå¹¶çš„åŸºå‡†ã€‚è¿›ä¸€æ­¥åˆ†æè¡¨æ˜ï¼Œå¥½çš„åˆå¹¶é€šå¸¸åŒ…æ‹¬å‡ ä¹æ‰€æœ‰å…·æœ‰éé›¶æƒé‡çš„æ£€æŸ¥ç‚¹ï¼Œè¡¨æ˜å³ä½¿æ˜¯çœ‹ä¼¼ä¸ä½³çš„åˆå§‹æ£€æŸ¥ç‚¹ä¹Ÿèƒ½ä¸ºæœ€ç»ˆåˆå¹¶åšå‡ºè´¡çŒ®ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.05355', 'title': 'MotionShop: Zero-Shot Motion Transfer in Video Diffusion Models with Mixture of Score Guidance', 'url': 'https://huggingface.co/papers/2412.05355', 'abstract': 'In this work, we propose the first motion transfer approach in diffusion transformer through Mixture of Score Guidance (MSG), a theoretically-grounded framework for motion transfer in diffusion models. Our key theoretical contribution lies in reformulating conditional score to decompose motion score and content score in diffusion models. By formulating motion transfer as a mixture of potential energies, MSG naturally preserves scene composition and enables creative scene transformations while maintaining the integrity of transferred motion patterns. This novel sampling operates directly on pre-trained video diffusion models without additional training or fine-tuning. Through extensive experiments, MSG demonstrates successful handling of diverse scenarios including single object, multiple objects, and cross-object motion transfer as well as complex camera motion transfer. Additionally, we introduce MotionBench, the first motion transfer dataset consisting of 200 source videos and 1000 transferred motions, covering single/multi-object transfers, and complex camera motions.', 'score': 0, 'issue_id': 1047, 'pub_date': '2024-12-06', 'pub_date_card': {'ru': '6 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 6', 'zh': '12æœˆ6æ—¥'}, 'hash': 'fd2b5f6636c2d2af', 'authors': ['Hidir Yesiltepe', 'Tuna Han Salih Meral', 'Connor Dunlop', 'Pinar Yanardag'], 'affiliations': ['Virginia Tech'], 'pdf_title_img': 'assets/pdf/title_img/2412.05355.jpg', 'data': {'categories': ['#diffusion', '#benchmark', '#video', '#dataset'], 'emoji': 'ğŸ­', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑÑƒ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑÑƒ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°Ñ…, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Mixture of Score Guidance (MSG). ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ²ĞºĞ»Ğ°Ğ´ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²ĞºĞµ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑĞºĞ¾Ñ€Ğ° Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ ÑĞºĞ¾Ñ€Ğ° Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. MSG Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ‚ÑŒ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ ÑÑ†ĞµĞ½Ñ‹ Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ‚Ğ²Ğ¾Ñ€Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑÑ†ĞµĞ½Ñ‹, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ñ†ĞµĞ»Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿ĞµÑ€ĞµĞ½ĞµÑĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ² Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Revolutionizing Motion Transfer with Mixture of Score Guidance', 'desc': 'This paper introduces a new method called Mixture of Score Guidance (MSG) for transferring motion in video using diffusion transformers. The authors reformulate the conditional score to separate motion and content scores, allowing for better control over how motion is applied to scenes. MSG enables creative transformations while keeping the original motion patterns intact, and it works with existing pre-trained video diffusion models without needing extra training. The paper also presents MotionBench, a dataset designed for evaluating motion transfer techniques, featuring a variety of scenarios including single and multiple object transfers and complex camera movements.'}, 'zh': {'title': 'åˆ›æ–°è¿åŠ¨è½¬ç§»ï¼šæ··åˆè¯„åˆ†å¼•å¯¼çš„åº”ç”¨', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åœ¨æ‰©æ•£å˜æ¢å™¨ä¸­è¿›è¡Œè¿åŠ¨è½¬ç§»çš„é¦–ä¸ªæ–¹æ³•ï¼Œç§°ä¸ºæ··åˆè¯„åˆ†å¼•å¯¼ï¼ˆMSGï¼‰ã€‚æˆ‘ä»¬é€šè¿‡é‡æ–°æ„å»ºæ¡ä»¶è¯„åˆ†ï¼Œå°†è¿åŠ¨è¯„åˆ†å’Œå†…å®¹è¯„åˆ†åˆ†è§£ï¼Œä»è€Œä¸ºæ‰©æ•£æ¨¡å‹ä¸­çš„è¿åŠ¨è½¬ç§»æä¾›äº†ç†è®ºåŸºç¡€ã€‚MSGæ–¹æ³•èƒ½å¤Ÿè‡ªç„¶åœ°ä¿æŒåœºæ™¯æ„å›¾ï¼Œå¹¶åœ¨ä¿æŒè½¬ç§»è¿åŠ¨æ¨¡å¼å®Œæ•´æ€§çš„åŒæ—¶ï¼Œå®ç°åˆ›æ„åœºæ™¯å˜æ¢ã€‚é€šè¿‡å¤§é‡å®éªŒï¼ŒMSGæˆåŠŸå¤„ç†äº†å¤šç§åœºæ™¯ï¼ŒåŒ…æ‹¬å•ä¸ªç‰©ä½“ã€å¤šç‰©ä½“å’Œå¤æ‚ç›¸æœºè¿åŠ¨è½¬ç§»ï¼Œå¹¶å¼•å…¥äº†MotionBenchæ•°æ®é›†ï¼ŒåŒ…å«200ä¸ªæºè§†é¢‘å’Œ1000ä¸ªè½¬ç§»è¿åŠ¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.07760', 'title': 'SynCamMaster: Synchronizing Multi-Camera Video Generation from Diverse Viewpoints', 'url': 'https://huggingface.co/papers/2412.07760', 'abstract': 'Recent advancements in video diffusion models have shown exceptional abilities in simulating real-world dynamics and maintaining 3D consistency. This progress inspires us to investigate the potential of these models to ensure dynamic consistency across various viewpoints, a highly desirable feature for applications such as virtual filming. Unlike existing methods focused on multi-view generation of single objects for 4D reconstruction, our interest lies in generating open-world videos from arbitrary viewpoints, incorporating 6 DoF camera poses. To achieve this, we propose a plug-and-play module that enhances a pre-trained text-to-video model for multi-camera video generation, ensuring consistent content across different viewpoints. Specifically, we introduce a multi-view synchronization module to maintain appearance and geometry consistency across these viewpoints. Given the scarcity of high-quality training data, we design a hybrid training scheme that leverages multi-camera images and monocular videos to supplement Unreal Engine-rendered multi-camera videos. Furthermore, our method enables intriguing extensions, such as re-rendering a video from novel viewpoints. We also release a multi-view synchronized video dataset, named SynCamVideo-Dataset. Project page: https://jianhongbai.github.io/SynCamMaster/.', 'score': 34, 'issue_id': 1081, 'pub_date': '2024-12-10', 'pub_date_card': {'ru': '10 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 10', 'zh': '12æœˆ10æ—¥'}, 'hash': '5ac69027d8ae0669', 'authors': ['Jianhong Bai', 'Menghan Xia', 'Xintao Wang', 'Ziyang Yuan', 'Xiao Fu', 'Zuozhu Liu', 'Haoji Hu', 'Pengfei Wan', 'Di Zhang'], 'affiliations': ['CUHK', 'Kuaishou Technology', 'Tsinghua University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2412.07760.jpg', 'data': {'categories': ['#diffusion', '#dataset', '#3d', '#open_source', '#video'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Ğ¡Ğ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ° Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ text-to-video Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ‚Ğ¾Ñ‡ĞµĞº Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ°. ĞĞ½Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½ÑƒÑ ÑÑ…ĞµĞ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ñ‹Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¼Ğ¾Ğ½Ğ¾ĞºÑƒĞ»ÑÑ€Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ğº Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ°Ğ¼ Ğ¸Ğ· Unreal Engine. ĞœĞµÑ‚Ğ¾Ğ´ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿ĞµÑ€ĞµÑ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ñ‚ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ² Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ²Ñ‹Ğ¿ÑƒÑĞº Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° SynCamVideo-Dataset.'}, 'en': {'title': 'Dynamic Consistency in Multi-View Video Generation', 'desc': "This paper explores the use of video diffusion models to create videos that maintain dynamic consistency from multiple viewpoints, which is important for applications like virtual filming. The authors propose a new module that enhances existing text-to-video models, allowing them to generate videos that are consistent in appearance and geometry across different camera angles. They introduce a multi-view synchronization module to ensure that the content remains coherent, even when viewed from various perspectives. Additionally, they present a hybrid training approach that combines different types of video data to improve the model's performance and release a new dataset for multi-view synchronized videos."}, 'zh': {'title': 'å®ç°å¤šè§†è§’è§†é¢‘çš„ä¸€è‡´æ€§', 'desc': 'æœ€è¿‘è§†é¢‘æ‰©æ•£æ¨¡å‹çš„è¿›å±•æ˜¾ç¤ºå‡ºåœ¨æ¨¡æ‹Ÿç°å®ä¸–ç•ŒåŠ¨æ€å’Œä¿æŒä¸‰ç»´ä¸€è‡´æ€§æ–¹é¢çš„å“è¶Šèƒ½åŠ›ã€‚æˆ‘ä»¬ç ”ç©¶è¿™äº›æ¨¡å‹åœ¨ä¸åŒè§†è§’ä¸‹ç¡®ä¿åŠ¨æ€ä¸€è‡´æ€§çš„æ½œåŠ›ï¼Œè¿™å¯¹äºè™šæ‹Ÿæ‹æ‘„ç­‰åº”ç”¨éå¸¸é‡è¦ã€‚ä¸ç°æœ‰æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬å…³æ³¨çš„æ˜¯ä»ä»»æ„è§†è§’ç”Ÿæˆå¼€æ”¾ä¸–ç•Œè§†é¢‘ï¼Œå¹¶å¼•å…¥å…­è‡ªç”±åº¦ç›¸æœºå§¿æ€ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå¯æ’æ‹”æ¨¡å—ï¼Œå¢å¼ºäº†é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°è§†é¢‘æ¨¡å‹ï¼Œä»¥å®ç°å¤šç›¸æœºè§†é¢‘ç”Ÿæˆï¼Œå¹¶ç¡®ä¿ä¸åŒè§†è§’ä¸‹å†…å®¹çš„ä¸€è‡´æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.08580', 'title': 'LAION-SG: An Enhanced Large-Scale Dataset for Training Complex Image-Text Models with Structural Annotations', 'url': 'https://huggingface.co/papers/2412.08580', 'abstract': 'Recent advances in text-to-image (T2I) generation have shown remarkable success in producing high-quality images from text. However, existing T2I models show decayed performance in compositional image generation involving multiple objects and intricate relationships. We attribute this problem to limitations in existing datasets of image-text pairs, which lack precise inter-object relationship annotations with prompts only. To address this problem, we construct LAION-SG, a large-scale dataset with high-quality structural annotations of scene graphs (SG), which precisely describe attributes and relationships of multiple objects, effectively representing the semantic structure in complex scenes. Based on LAION-SG, we train a new foundation model SDXL-SG to incorporate structural annotation information into the generation process. Extensive experiments show advanced models trained on our LAION-SG boast significant performance improvements in complex scene generation over models on existing datasets. We also introduce CompSG-Bench, a benchmark that evaluates models on compositional image generation, establishing a new standard for this domain.', 'score': 26, 'issue_id': 1081, 'pub_date': '2024-12-11', 'pub_date_card': {'ru': '11 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 11', 'zh': '12æœˆ11æ—¥'}, 'hash': '07b05e5ae44a52c7', 'authors': ['Zejian Li', 'Chenye Meng', 'Yize Li', 'Ling Yang', 'Shengyuan Zhang', 'Jiarui Ma', 'Jiayi Li', 'Guang Yang', 'Changyuan Yang', 'Zhiyuan Yang', 'Jinxiong Chang', 'Lingyun Sun'], 'affiliations': ['Alibaba Group', 'Ant Group', 'Jiangnan University', 'Peking University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2412.08580.jpg', 'data': {'categories': ['#dataset', '#cv', '#synthetic', '#benchmark', '#games'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'ĞĞ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ²', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸. ĞĞ½Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ LAION-SG Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ² Ğ²Ğ¸Ğ´Ğµ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ² ÑÑ†ĞµĞ½. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ğ±Ñ‹Ğ»Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ SDXL-SG, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº CompSG-Bench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Enhancing Text-to-Image Generation with Structured Scene Graphs', 'desc': 'This paper addresses the challenges faced by text-to-image (T2I) models in generating complex images with multiple objects and their relationships. The authors identify that existing datasets lack detailed annotations for inter-object relationships, which hampers model performance. To overcome this, they introduce LAION-SG, a new dataset that includes comprehensive scene graph annotations, enhancing the understanding of object attributes and relationships. They also present a new model, SDXL-SG, trained on this dataset, which shows significant improvements in generating intricate scenes, along with a new benchmark, CompSG-Bench, for evaluating compositional image generation.'}, 'zh': {'title': 'æ„å»ºé«˜è´¨é‡æ•°æ®é›†ï¼Œæå‡å›¾åƒç”Ÿæˆèƒ½åŠ›', 'desc': 'æœ€è¿‘åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆï¼ˆT2Iï¼‰æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œèƒ½å¤Ÿä»æ–‡æœ¬ç”Ÿæˆé«˜è´¨é‡å›¾åƒã€‚ç„¶è€Œï¼Œç°æœ‰çš„T2Iæ¨¡å‹åœ¨ç”ŸæˆåŒ…å«å¤šä¸ªå¯¹è±¡å’Œå¤æ‚å…³ç³»çš„å›¾åƒæ—¶è¡¨ç°ä¸ä½³ã€‚æˆ‘ä»¬è®¤ä¸ºè¿™ä¸ªé—®é¢˜æºäºç°æœ‰å›¾åƒ-æ–‡æœ¬å¯¹æ•°æ®é›†çš„å±€é™æ€§ï¼Œè¿™äº›æ•°æ®é›†ç¼ºä¹ç²¾ç¡®çš„å¯¹è±¡é—´å…³ç³»æ³¨é‡Šã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æ„å»ºäº†LAION-SGï¼Œè¿™æ˜¯ä¸€ä¸ªå…·æœ‰é«˜è´¨é‡ç»“æ„æ³¨é‡Šçš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œèƒ½å¤Ÿæœ‰æ•ˆè¡¨ç¤ºå¤æ‚åœºæ™¯ä¸­çš„è¯­ä¹‰ç»“æ„ï¼Œå¹¶åŸºäºæ­¤è®­ç»ƒäº†æ–°çš„åŸºç¡€æ¨¡å‹SDXL-SGã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.08443', 'title': 'POINTS1.5: Building a Vision-Language Model towards Real World Applications', 'url': 'https://huggingface.co/papers/2412.08443', 'abstract': 'Vision-language models have made significant strides recently, demonstrating superior performance across a range of tasks, e.g. optical character recognition and complex diagram analysis. Building on this trend, we introduce a new vision-language model, POINTS1.5, designed to excel in various real-world applications. POINTS1.5 is an enhancement of POINTS1.0 and incorporates several key innovations: i) We replace the original CLIP vision encoder, which had a fixed image resolution, with a NaViT-style vision encoder that supports native dynamic high resolution. This allows POINTS1.5 to process images of any resolution without needing to split them into tiles. ii) We add bilingual support to POINTS1.5, significantly enhancing its capability in Chinese. Due to the scarcity of open-source Chinese datasets for vision-language models, we collect numerous images from the Internet and annotate them using a combination of manual and automatic methods. iii) We propose a set of rigorous filtering methods for visual instruction tuning datasets. We comprehensively evaluate all these filtering methods, and choose the most effective ones to obtain the final visual instruction tuning set. Thanks to these innovations, POINTS1.5 significantly outperforms POINTS1.0 and demonstrates strong performance across a range of real-world applications. Notably, POINTS1.5-7B is trained on fewer than 4 billion tokens and ranks first on the OpenCompass leaderboard among models with fewer than 10 billion parameters', 'score': 25, 'issue_id': 1083, 'pub_date': '2024-12-11', 'pub_date_card': {'ru': '11 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 11', 'zh': '12æœˆ11æ—¥'}, 'hash': '02dbe9638e613a10', 'authors': ['Yuan Liu', 'Le Tian', 'Xiao Zhou', 'Xinyu Gao', 'Kavio Yu', 'Yang Yu', 'Jie Zhou'], 'affiliations': ['Pattern Recognition Center, WeChat AI, Tencent Inc, China'], 'pdf_title_img': 'assets/pdf/title_img/2412.08443.jpg', 'data': {'categories': ['#dataset', '#cv', '#low_resource', '#architecture', '#data', '#multilingual', '#training', '#open_source'], 'emoji': 'ğŸ”', 'ru': {'title': 'POINTS1.5: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ POINTS1.5, ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½ÑƒÑ Ğ²ĞµÑ€ÑĞ¸Ñ POINTS1.0. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ Ğ·Ğ°Ğ¼ĞµĞ½Ñƒ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° NaViT Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ, Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ²ÑƒÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¸ (Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¸Ğ¹ Ğ¸ ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¸Ğ¹) Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ ÑÑ‚Ñ€Ğ¾Ğ³Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ POINTS1.5 Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ÑÑ…, Ğ½ĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¼ĞµĞ½ĞµĞµ Ñ‡ĞµĞ¼ 4 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². POINTS1.5-7B Ğ·Ğ°Ğ½Ğ¸Ğ¼Ğ°ĞµÑ‚ Ğ¿ĞµÑ€Ğ²Ğ¾Ğµ Ğ¼ĞµÑÑ‚Ğ¾ Ğ² Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ğµ OpenCompass ÑÑ€ĞµĞ´Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¼ĞµĞ½ĞµĞµ Ñ‡ĞµĞ¼ 10 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ².'}, 'en': {'title': 'POINTS1.5: Elevating Vision-Language Models with Dynamic Resolution and Bilingual Support', 'desc': 'The paper presents POINTS1.5, an advanced vision-language model that improves upon its predecessor, POINTS1.0. It features a NaViT-style vision encoder that allows for dynamic high-resolution image processing, eliminating the need for image tiling. Additionally, POINTS1.5 introduces bilingual support, particularly enhancing its performance in Chinese by utilizing a newly curated dataset. The model also employs rigorous filtering methods for visual instruction tuning datasets, leading to superior performance in various real-world applications and achieving top rankings in benchmark evaluations.'}, 'zh': {'title': 'POINTS1.5ï¼šè§†è§‰è¯­è¨€æ¨¡å‹çš„æ–°çªç ´', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„è§†è§‰è¯­è¨€æ¨¡å‹POINTS1.5ï¼Œè¯¥æ¨¡å‹åœ¨å¤šä¸ªå®é™…åº”ç”¨ä¸­è¡¨ç°ä¼˜å¼‚ã€‚ä¸ä¹‹å‰çš„ç‰ˆæœ¬POINTS1.0ç›¸æ¯”ï¼ŒPOINTS1.5è¿›è¡Œäº†å¤šé¡¹é‡è¦æ”¹è¿›ï¼ŒåŒ…æ‹¬ä½¿ç”¨æ”¯æŒåŠ¨æ€é«˜åˆ†è¾¨ç‡çš„NaViTé£æ ¼è§†è§‰ç¼–ç å™¨ï¼Œèƒ½å¤Ÿå¤„ç†ä»»æ„åˆ†è¾¨ç‡çš„å›¾åƒã€‚è¯¥æ¨¡å‹è¿˜å¢åŠ äº†å¯¹ä¸­æ–‡çš„åŒè¯­æ”¯æŒï¼Œé€šè¿‡æ”¶é›†å’Œæ³¨é‡Šå¤§é‡å›¾åƒæ¥æå‡å…¶ä¸­æ–‡èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€å¥—ä¸¥æ ¼çš„è§†è§‰æŒ‡ä»¤è°ƒä¼˜æ•°æ®é›†è¿‡æ»¤æ–¹æ³•ï¼Œç¡®ä¿æœ€ç»ˆæ•°æ®é›†çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.08486', 'title': 'Learning Flow Fields in Attention for Controllable Person Image Generation', 'url': 'https://huggingface.co/papers/2412.08486', 'abstract': "Controllable person image generation aims to generate a person image conditioned on reference images, allowing precise control over the person's appearance or pose. However, prior methods often distort fine-grained textural details from the reference image, despite achieving high overall image quality. We attribute these distortions to inadequate attention to corresponding regions in the reference image. To address this, we thereby propose learning flow fields in attention (Leffa), which explicitly guides the target query to attend to the correct reference key in the attention layer during training. Specifically, it is realized via a regularization loss on top of the attention map within a diffusion-based baseline. Our extensive experiments show that Leffa achieves state-of-the-art performance in controlling appearance (virtual try-on) and pose (pose transfer), significantly reducing fine-grained detail distortion while maintaining high image quality. Additionally, we show that our loss is model-agnostic and can be used to improve the performance of other diffusion models.", 'score': 14, 'issue_id': 1086, 'pub_date': '2024-12-11', 'pub_date_card': {'ru': '11 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 11', 'zh': '12æœˆ11æ—¥'}, 'hash': 'ff329acbd2056afe', 'authors': ['Zijian Zhou', 'Shikun Liu', 'Xiao Han', 'Haozhe Liu', 'Kam Woh Ng', 'Tian Xie', 'Yuren Cong', 'Hang Li', 'Mengmeng Xu', 'Juan-Manuel PÃ©rez-RÃºa', 'Aditya Patel', 'Tao Xiang', 'Miaojing Shi', 'Sen He'], 'affiliations': ['Kings College London', 'Meta AI', 'Tongji University'], 'pdf_title_img': 'assets/pdf/title_img/2412.08486.jpg', 'data': {'categories': ['#diffusion', '#optimization', '#training', '#cv'], 'emoji': 'ğŸ‘¤', 'ru': {'title': 'Ğ¢Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ»ÑĞ´ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Leffa Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ»ÑĞ´ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ»ĞµĞ¹ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ° Ğ² ÑĞ»Ğ¾ÑÑ… Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ñ‚Ğ¾Ñ‡Ğ½ĞµĞµ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸ Ğ¸Ğ· Ñ€ĞµÑ„ĞµÑ€ĞµĞ½ÑĞ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. Leffa Ñ€ĞµĞ°Ğ»Ğ¸Ğ·ÑƒĞµÑ‚ÑÑ Ñ‡ĞµÑ€ĞµĞ· Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ… ĞºĞ°Ñ€Ñ‚Ñ‹ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Leffa Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğµ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ Ğ²Ğ¸Ğ´Ğ° Ğ¸ Ğ¿Ğ¾Ğ·Ñ‹, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°Ñ Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ»ĞºĞ¸Ñ… Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹.'}, 'en': {'title': 'Enhancing Image Generation with Targeted Attention', 'desc': 'This paper presents a method called Learning Flow Fields in Attention (Leffa) for controllable person image generation. The goal is to create images of people that accurately reflect the appearance and pose of reference images without losing important details. Previous methods struggled with distorting fine textures, which this approach aims to fix by improving how the model focuses on specific areas of the reference image. The authors demonstrate that Leffa not only enhances the quality of generated images but is also adaptable to other diffusion models, making it a versatile solution in the field.'}, 'zh': {'title': 'ç²¾ç¡®æ§åˆ¶äººç‰©å›¾åƒç”Ÿæˆçš„å…³é”®', 'desc': 'å¯æ§çš„äººç‰©å›¾åƒç”Ÿæˆæ—¨åœ¨æ ¹æ®å‚è€ƒå›¾åƒç”Ÿæˆç‰¹å®šå¤–è§‚æˆ–å§¿åŠ¿çš„äººç‰©å›¾åƒã€‚ä»¥å¾€çš„æ–¹æ³•è™½ç„¶åœ¨æ•´ä½“å›¾åƒè´¨é‡ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†å¸¸å¸¸ä¼šæ‰­æ›²å‚è€ƒå›¾åƒä¸­çš„ç»†èŠ‚çº¹ç†ã€‚æˆ‘ä»¬è®¤ä¸ºè¿™ç§æ‰­æ›²æ˜¯ç”±äºå¯¹å‚è€ƒå›¾åƒä¸­ç›¸åº”åŒºåŸŸå…³æ³¨ä¸è¶³é€ æˆçš„ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åœ¨æ³¨æ„åŠ›æœºåˆ¶ä¸­å­¦ä¹ æµåœºçš„æ–¹æ³•ï¼ˆLeffaï¼‰ï¼Œé€šè¿‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¼•å¯¼ç›®æ ‡æŸ¥è¯¢å…³æ³¨æ­£ç¡®çš„å‚è€ƒå…³é”®ç‚¹ï¼Œä»è€Œæ˜¾è‘—å‡å°‘ç»†èŠ‚æ‰­æ›²ï¼ŒåŒæ—¶ä¿æŒé«˜å›¾åƒè´¨é‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.07744', 'title': 'StyleMaster: Stylize Your Video with Artistic Generation and Translation', 'url': 'https://huggingface.co/papers/2412.07744', 'abstract': 'Style control has been popular in video generation models. Existing methods often generate videos far from the given style, cause content leakage, and struggle to transfer one video to the desired style. Our first observation is that the style extraction stage matters, whereas existing methods emphasize global style but ignore local textures. In order to bring texture features while preventing content leakage, we filter content-related patches while retaining style ones based on prompt-patch similarity; for global style extraction, we generate a paired style dataset through model illusion to facilitate contrastive learning, which greatly enhances the absolute style consistency. Moreover, to fill in the image-to-video gap, we train a lightweight motion adapter on still videos, which implicitly enhances stylization extent, and enables our image-trained model to be seamlessly applied to videos. Benefited from these efforts, our approach, StyleMaster, not only achieves significant improvement in both style resemblance and temporal coherence, but also can easily generalize to video style transfer with a gray tile ControlNet. Extensive experiments and visualizations demonstrate that StyleMaster significantly outperforms competitors, effectively generating high-quality stylized videos that align with textual content and closely resemble the style of reference images. Our project page is at https://zixuan-ye.github.io/stylemaster', 'score': 12, 'issue_id': 1084, 'pub_date': '2024-12-10', 'pub_date_card': {'ru': '10 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 10', 'zh': '12æœˆ10æ—¥'}, 'hash': '1eac9c28026939fc', 'authors': ['Zixuan Ye', 'Huijuan Huang', 'Xintao Wang', 'Pengfei Wan', 'Di Zhang', 'Wenhan Luo'], 'affiliations': ['Hong Kong University of Science and Technology', 'KuaiShou Technology'], 'pdf_title_img': 'assets/pdf/title_img/2412.07744.jpg', 'data': {'categories': ['#optimization', '#style_transfer', '#video', '#multimodal'], 'emoji': 'ğŸ¨', 'ru': {'title': 'StyleMaster: Ğ¡Ğ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑ‚Ğ¸Ğ»ĞµĞ²Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'StyleMaster - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑÑ‚Ğ¸Ğ»ĞµĞ²Ğ¾Ğ¼Ñƒ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞœĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ¸Ğ»Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ°Ñ‚Ñ‡ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ° Ñ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ¼ Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¿Ğ°Ñ€Ğ½Ñ‹Ñ… ÑÑ‚Ğ¸Ğ»ĞµĞ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ›ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…, Ğº Ğ²Ğ¸Ğ´ĞµĞ¾. StyleMaster Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ ÑÑ‚Ğ¸Ğ»Ñ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'StyleMaster: Elevating Video Style Transfer with Texture and Consistency', 'desc': 'This paper presents StyleMaster, a novel approach to video style transfer that addresses common issues in existing methods, such as content leakage and poor style adherence. The authors emphasize the importance of local texture features in addition to global style, proposing a method that filters content-related patches while preserving style-related ones. They introduce a paired style dataset for contrastive learning to enhance style consistency and develop a lightweight motion adapter to bridge the gap between image and video stylization. Extensive experiments show that StyleMaster significantly improves style resemblance and temporal coherence, outperforming existing models in generating high-quality stylized videos.'}, 'zh': {'title': 'StyleMasterï¼šæå‡è§†é¢‘é£æ ¼è½¬ç§»çš„åˆ›æ–°æ–¹æ³•', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºStyleMasterçš„è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œæ—¨åœ¨æ”¹å–„è§†é¢‘çš„é£æ ¼è½¬ç§»æ•ˆæœã€‚æˆ‘ä»¬å‘ç°ï¼Œç°æœ‰æ–¹æ³•åœ¨é£æ ¼æå–é˜¶æ®µå¾€å¾€å¿½è§†å±€éƒ¨çº¹ç†ï¼Œå¯¼è‡´ç”Ÿæˆçš„è§†é¢‘ä¸ç›®æ ‡é£æ ¼ä¸ç¬¦ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬é€šè¿‡è¿‡æ»¤å†…å®¹ç›¸å…³çš„å›¾åƒå—æ¥ä¿ç•™é£æ ¼ç‰¹å¾ï¼Œå¹¶åˆ©ç”¨å¯¹æ¯”å­¦ä¹ å¢å¼ºå…¨å±€é£æ ¼çš„ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è®­ç»ƒäº†ä¸€ä¸ªè½»é‡çº§çš„è¿åŠ¨é€‚é…å™¨ï¼Œä½¿å¾—å›¾åƒè®­ç»ƒçš„æ¨¡å‹èƒ½å¤Ÿæ— ç¼åº”ç”¨äºè§†é¢‘ç”Ÿæˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.08646', 'title': 'StreamChat: Chatting with Streaming Video', 'url': 'https://huggingface.co/papers/2412.08646', 'abstract': 'This paper presents StreamChat, a novel approach that enhances the interaction capabilities of Large Multimodal Models (LMMs) with streaming video content. In streaming interaction scenarios, existing methods rely solely on visual information available at the moment a question is posed, resulting in significant delays as the model remains unaware of subsequent changes in the streaming video. StreamChat addresses this limitation by innovatively updating the visual context at each decoding step, ensuring that the model utilizes up-to-date video content throughout the decoding process. Additionally, we introduce a flexible and efficient crossattention-based architecture to process dynamic streaming inputs while maintaining inference efficiency for streaming interactions. Furthermore, we construct a new dense instruction dataset to facilitate the training of streaming interaction models, complemented by a parallel 3D-RoPE mechanism that encodes the relative temporal information of visual and text tokens. Experimental results demonstrate that StreamChat achieves competitive performance on established image and video benchmarks and exhibits superior capabilities in streaming interaction scenarios compared to state-of-the-art video LMM.', 'score': 11, 'issue_id': 1086, 'pub_date': '2024-12-11', 'pub_date_card': {'ru': '11 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 11', 'zh': '12æœˆ11æ—¥'}, 'hash': '6d48f15bab7c3545', 'authors': ['Jihao Liu', 'Zhiding Yu', 'Shiyi Lan', 'Shihao Wang', 'Rongyao Fang', 'Jan Kautz', 'Hongsheng Li', 'Jose M. Alvare'], 'affiliations': ['CPII under InnoHK', 'CUHK MMLab', 'NVIDIA', 'Shanghai AI Laboratory', 'The Hong Kong Polytechnic University'], 'pdf_title_img': 'assets/pdf/title_img/2412.08646.jpg', 'data': {'categories': ['#video', '#multimodal', '#dataset', '#architecture', '#benchmark'], 'emoji': 'ğŸ¥', 'ru': {'title': 'StreamChat: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ¾Ğ¼ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¸ Ñ Ğ˜Ğ˜', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ StreamChat - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LMM) Ñ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ²Ğ¸Ğ´ĞµĞ¾. StreamChat Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ ÑˆĞ°Ğ³Ğµ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ°ĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞºÑ€Ğ¾ÑÑ-Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ñ‹Ñ… Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ StreamChat Ğ² ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ LMM.'}, 'en': {'title': 'StreamChat: Real-Time Interaction with Streaming Video', 'desc': 'This paper introduces StreamChat, a new method that improves how Large Multimodal Models (LMMs) interact with live video. Traditional approaches only use the visual information available at the time a question is asked, which can cause delays as they miss changes in the video. StreamChat solves this by updating the visual context continuously during the decoding process, allowing the model to respond with the most current video content. It also features a cross-attention architecture for efficient processing of dynamic inputs and a new dataset for training, leading to better performance in streaming interactions compared to existing models.'}, 'zh': {'title': 'StreamChatï¼šå®æ—¶è§†é¢‘äº¤äº’çš„æ–°çªç ´', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•StreamChatï¼Œæ—¨åœ¨å¢å¼ºå¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMï¼‰ä¸æµåª’ä½“è§†é¢‘å†…å®¹çš„äº¤äº’èƒ½åŠ›ã€‚åœ¨æµåª’ä½“äº¤äº’åœºæ™¯ä¸­ï¼Œç°æœ‰æ–¹æ³•ä»…ä¾èµ–äºæé—®æ—¶å¯ç”¨çš„è§†è§‰ä¿¡æ¯ï¼Œå¯¼è‡´æ¨¡å‹æ— æ³•åŠæ—¶è·å–è§†é¢‘ä¸­çš„åç»­å˜åŒ–ï¼Œä»è€Œäº§ç”Ÿæ˜¾è‘—å»¶è¿Ÿã€‚StreamChaté€šè¿‡åœ¨æ¯ä¸ªè§£ç æ­¥éª¤ä¸­åˆ›æ–°æ€§åœ°æ›´æ–°è§†è§‰ä¸Šä¸‹æ–‡ï¼Œç¡®ä¿æ¨¡å‹åœ¨è§£ç è¿‡ç¨‹ä¸­åˆ©ç”¨æœ€æ–°çš„è§†é¢‘å†…å®¹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§çµæ´»é«˜æ•ˆçš„åŸºäºäº¤å‰æ³¨æ„åŠ›çš„æ¶æ„ï¼Œä»¥å¤„ç†åŠ¨æ€æµåª’ä½“è¾“å…¥ï¼ŒåŒæ—¶ä¿æŒæµåª’ä½“äº¤äº’çš„æ¨ç†æ•ˆç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.07825', 'title': '3DSRBench: A Comprehensive 3D Spatial Reasoning Benchmark', 'url': 'https://huggingface.co/papers/2412.07825', 'abstract': '3D spatial reasoning is the ability to analyze and interpret the positions, orientations, and spatial relationships of objects within the 3D space. This allows models to develop a comprehensive understanding of the 3D scene, enabling their applicability to a broader range of areas, such as autonomous navigation, robotics, and AR/VR. While large multi-modal models (LMMs) have achieved remarkable progress in a wide range of image and video understanding tasks, their capabilities to perform 3D spatial reasoning on diverse natural images are less studied. In this work we present the first comprehensive 3D spatial reasoning benchmark, 3DSRBench, with 2,772 manually annotated visual question-answer pairs across 12 question types. We conduct robust and thorough evaluation of 3D spatial reasoning capabilities by balancing the data distribution and adopting a novel FlipEval strategy. To further study the robustness of 3D spatial reasoning w.r.t. camera 3D viewpoints, our 3DSRBench includes two subsets with 3D spatial reasoning questions on paired images with common and uncommon viewpoints. We benchmark a wide range of open-sourced and proprietary LMMs, uncovering their limitations in various aspects of 3D awareness, such as height, orientation, location, and multi-object reasoning, as well as their degraded performance on images with uncommon camera viewpoints. Our 3DSRBench provide valuable findings and insights about the future development of LMMs with strong 3D reasoning capabilities. Our project page and dataset is available https://3dsrbench.github.io.', 'score': 11, 'issue_id': 1082, 'pub_date': '2024-12-10', 'pub_date_card': {'ru': '10 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 10', 'zh': '12æœˆ10æ—¥'}, 'hash': '91db0c60d08d4efd', 'authors': ['Wufei Ma', 'Haoyu Chen', 'Guofeng Zhang', 'Celso M de Melo', 'Alan Yuille', 'Jieneng Chen'], 'affiliations': ['Carnegie Mellon University', 'DEVCOM Army Research Laboratory', 'Johns Hopkins University'], 'pdf_title_img': 'assets/pdf/title_img/2412.07825.jpg', 'data': {'categories': ['#open_source', '#benchmark', '#3d', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': '3DSRBench: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ 3D Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ñƒ LMM', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ benchmark Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LMM) Ğº 3D Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ 3DSRBench, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 2772 Ğ²Ñ€ÑƒÑ‡Ğ½ÑƒÑ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ-Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ¿Ğ¾ 12 Ñ‚Ğ¸Ğ¿Ğ°Ğ¼ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ², ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ 3D Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ñ€Ğ¾Ğ±Ğ°ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ñ€Ğ°ĞºÑƒÑ€ÑĞ°Ğ¼ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ FlipEval Ğ´Ğ»Ñ Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… LMM Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ°ÑĞ¿ĞµĞºÑ‚Ğ°Ñ… 3D Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ²Ñ‹ÑĞ¾Ñ‚Ğ°, Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ, Ñ€Ğ°ÑĞ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¾ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ñ….'}, 'en': {'title': 'Enhancing 3D Spatial Reasoning in AI Models', 'desc': 'This paper introduces 3DSRBench, a new benchmark designed to evaluate 3D spatial reasoning in large multi-modal models (LMMs). It includes 2,772 annotated visual question-answer pairs that cover various question types related to spatial relationships in 3D environments. The study highlights the limitations of current LMMs in understanding aspects like height, orientation, and location, especially when dealing with images taken from uncommon viewpoints. By providing a structured evaluation framework, this work aims to enhance the development of models with improved 3D reasoning capabilities.'}, 'zh': {'title': 'æ¨åŠ¨3Dç©ºé—´æ¨ç†çš„æœªæ¥å‘å±•', 'desc': '3Dç©ºé—´æ¨ç†æ˜¯åˆ†æå’Œç†è§£ä¸‰ç»´ç©ºé—´ä¸­ç‰©ä½“ä½ç½®ã€æ–¹å‘å’Œç©ºé—´å…³ç³»çš„èƒ½åŠ›ã€‚æœ¬æ–‡æå‡ºäº†ç¬¬ä¸€ä¸ªå…¨é¢çš„3Dç©ºé—´æ¨ç†åŸºå‡†ï¼Œ3DSRBenchï¼ŒåŒ…å«2772ä¸ªæ‰‹åŠ¨æ ‡æ³¨çš„è§†è§‰é—®ç­”å¯¹ï¼Œæ¶µç›–12ç§é—®é¢˜ç±»å‹ã€‚æˆ‘ä»¬é€šè¿‡å¹³è¡¡æ•°æ®åˆ†å¸ƒå’Œé‡‡ç”¨æ–°é¢–çš„FlipEvalç­–ç•¥ï¼Œå¯¹3Dç©ºé—´æ¨ç†èƒ½åŠ›è¿›è¡Œäº†å…¨é¢è¯„ä¼°ã€‚ç ”ç©¶ç»“æœæ­ç¤ºäº†ç°æœ‰å¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨3Dæ„è¯†æ–¹é¢çš„å±€é™æ€§ï¼Œå¹¶ä¸ºæœªæ¥çš„æ¨¡å‹å‘å±•æä¾›äº†é‡è¦è§è§£ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.06234', 'title': 'Generative Densification: Learning to Densify Gaussians for High-Fidelity Generalizable 3D Reconstruction', 'url': 'https://huggingface.co/papers/2412.06234', 'abstract': 'Generalized feed-forward Gaussian models have achieved significant progress in sparse-view 3D reconstruction by leveraging prior knowledge from large multi-view datasets. However, these models often struggle to represent high-frequency details due to the limited number of Gaussians. While the densification strategy used in per-scene 3D Gaussian splatting (3D-GS) optimization can be adapted to the feed-forward models, it may not be ideally suited for generalized scenarios. In this paper, we propose Generative Densification, an efficient and generalizable method to densify Gaussians generated by feed-forward models. Unlike the 3D-GS densification strategy, which iteratively splits and clones raw Gaussian parameters, our method up-samples feature representations from the feed-forward models and generates their corresponding fine Gaussians in a single forward pass, leveraging the embedded prior knowledge for enhanced generalization. Experimental results on both object-level and scene-level reconstruction tasks demonstrate that our method outperforms state-of-the-art approaches with comparable or smaller model sizes, achieving notable improvements in representing fine details.', 'score': 9, 'issue_id': 1083, 'pub_date': '2024-12-09', 'pub_date_card': {'ru': '9 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 9', 'zh': '12æœˆ9æ—¥'}, 'hash': '1da969b562c1f280', 'authors': ['Seungtae Nam', 'Xiangyu Sun', 'Gyeongjin Kang', 'Younggeun Lee', 'Seungjun Oh', 'Eunbyung Park'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2412.06234.jpg', 'data': {'categories': ['#training', '#3d', '#dataset'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑƒĞ¿Ğ»Ğ¾Ñ‚Ğ½ĞµĞ½Ğ¸Ğµ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑˆĞ°Ğ³ Ğ² 3D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğ¼ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸ĞµĞ¼', 'desc': "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ 'Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑƒĞ¿Ğ»Ğ¾Ñ‚Ğ½ĞµĞ½Ğ¸Ğµ' Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğµ Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ². Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒĞ¿Ğ»Ğ¾Ñ‚Ğ½ÑÑ‚ÑŒ Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ, ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², 'Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑƒĞ¿Ğ»Ğ¾Ñ‚Ğ½ĞµĞ½Ğ¸Ğµ' Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ğµ Ğ°Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ĞµĞ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ ĞºĞ°Ğº Ğ´Ğ»Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ñ‚Ğ°Ğº Ğ¸ Ğ´Ğ»Ñ ÑÑ†ĞµĞ½."}, 'en': {'title': 'Enhancing 3D Reconstruction with Generative Densification', 'desc': 'This paper introduces Generative Densification, a novel method aimed at improving the representation of high-frequency details in sparse-view 3D reconstruction using feed-forward Gaussian models. Traditional methods struggle with detail due to a limited number of Gaussians, but our approach efficiently up-samples feature representations in a single forward pass. By leveraging prior knowledge from large datasets, this method enhances generalization and performance in both object-level and scene-level tasks. Experimental results show that Generative Densification outperforms existing techniques while maintaining comparable or smaller model sizes.'}, 'zh': {'title': 'ç”Ÿæˆç¨ å¯†åŒ–ï¼šæå‡3Dé‡å»ºç»†èŠ‚çš„é«˜æ•ˆæ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºç”Ÿæˆç¨ å¯†åŒ–çš„é«˜æ•ˆæ–¹æ³•ï¼Œç”¨äºå¢å¼ºå‰é¦ˆé«˜æ–¯æ¨¡å‹åœ¨ç¨€ç–è§†å›¾3Dé‡å»ºä¸­çš„è¡¨ç°ã€‚ä¸ä¼ ç»Ÿçš„3Dé«˜æ–¯ç¨ å¯†åŒ–ç­–ç•¥ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡å•æ¬¡å‰å‘ä¼ æ’­ä»å‰é¦ˆæ¨¡å‹ä¸­ä¸Šé‡‡æ ·ç‰¹å¾è¡¨ç¤ºï¼Œç”Ÿæˆç›¸åº”çš„ç»†èŠ‚é«˜æ–¯ã€‚è¯¥æ–¹æ³•åˆ©ç”¨åµŒå…¥çš„å…ˆéªŒçŸ¥è¯†ï¼Œæå‡äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°è¡¨ç¤ºé«˜é¢‘ç»†èŠ‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç‰©ä½“çº§å’Œåœºæ™¯çº§é‡å»ºä»»åŠ¡ä¸­å‡ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œä¸”æ¨¡å‹è§„æ¨¡ç›¸å¯¹è¾ƒå°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.05467', 'title': 'The BrowserGym Ecosystem for Web Agent Research', 'url': 'https://huggingface.co/papers/2412.05467', 'abstract': "The BrowserGym ecosystem addresses the growing need for efficient evaluation and benchmarking of web agents, particularly those leveraging automation and Large Language Models (LLMs) for web interaction tasks. Many existing benchmarks suffer from fragmentation and inconsistent evaluation methodologies, making it challenging to achieve reliable comparisons and reproducible results. BrowserGym aims to solve this by providing a unified, gym-like environment with well-defined observation and action spaces, facilitating standardized evaluation across diverse benchmarks. Combined with AgentLab, a complementary framework that aids in agent creation, testing, and analysis, BrowserGym offers flexibility for integrating new benchmarks while ensuring consistent evaluation and comprehensive experiment management. This standardized approach seeks to reduce the time and complexity of developing web agents, supporting more reliable comparisons and facilitating in-depth analysis of agent behaviors, and could result in more adaptable, capable agents, ultimately accelerating innovation in LLM-driven automation. As a supporting evidence, we conduct the first large-scale, multi-benchmark web agent experiment and compare the performance of 6 state-of-the-art LLMs across all benchmarks currently available in BrowserGym. Among other findings, our results highlight a large discrepancy between OpenAI and Anthropic's latests models, with Claude-3.5-Sonnet leading the way on almost all benchmarks, except on vision-related tasks where GPT-4o is superior. Despite these advancements, our results emphasize that building robust and efficient web agents remains a significant challenge, due to the inherent complexity of real-world web environments and the limitations of current models.", 'score': 8, 'issue_id': 1096, 'pub_date': '2024-12-06', 'pub_date_card': {'ru': '6 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 6', 'zh': '12æœˆ6æ—¥'}, 'hash': '6e35afb20d47b1a6', 'authors': ['Thibault Le Sellier De Chezelles', 'Maxime Gasse', 'Alexandre Drouin', 'Massimo Caccia', 'LÃ©o Boisvert', 'Megh Thakkar', 'Tom Marty', 'Rim Assouel', 'Sahar Omidi Shayegan', 'Lawrence Keunho Jang', 'Xing Han LÃ¹', 'Ori Yoran', 'Dehan Kong', 'Frank F. Xu', 'Siva Reddy', 'Quentin Cappart', 'Graham Neubig', 'Ruslan Salakhutdinov', 'Nicolas Chapados', 'Alexandre Lacoste'], 'affiliations': ['Carnegie Mellon University', 'McGill University', 'Mila', 'Polytechnique MontrÃ©al', 'ServiceNow Research', 'Tel Aviv University', 'UniversitÃ© de MontrÃ©al', 'iMean AI'], 'pdf_title_img': 'assets/pdf/title_img/2412.05467.jpg', 'data': {'categories': ['#agi', '#interpretability', '#benchmark', '#games', '#agents'], 'emoji': 'ğŸŒ', 'ru': {'title': 'BrowserGym: ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ€ĞµĞ´Ğ° Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²ĞµĞ±-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLM', 'desc': 'BrowserGym Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ ÑĞºĞ¾ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ğ²ĞµĞ±-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ñ… Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ğ´Ğ»Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ğ²ĞµĞ±-Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°Ğ¼Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑÑ€ĞµĞ´Ñƒ Ñ Ñ‡ĞµÑ‚ĞºĞ¾ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°Ğ¼Ğ¸ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ¿Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ğ¼. Ğ’ ÑĞ¾Ñ‡ĞµÑ‚Ğ°Ğ½Ğ¸Ğ¸ Ñ AgentLab, BrowserGym ÑƒĞ¿Ñ€Ğ¾Ñ‰Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ²ĞµĞ±-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğµ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¸Ñ… Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ³Ğ¾ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ° Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Claude-3.5-Sonnet Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ·Ğ° Ğ¸ÑĞºĞ»ÑÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ñ‹Ğ¼ Ğ·Ñ€ĞµĞ½Ğ¸ĞµĞ¼, Ğ³Ğ´Ğµ Ğ»Ğ¸Ğ´Ğ¸Ñ€ÑƒĞµÑ‚ GPT-4o.'}, 'en': {'title': 'Streamlining Web Agent Evaluation with BrowserGym', 'desc': 'The BrowserGym ecosystem provides a standardized platform for evaluating web agents that use automation and Large Language Models (LLMs). It addresses issues of fragmentation and inconsistent methodologies in existing benchmarks, allowing for reliable comparisons and reproducible results. By offering a gym-like environment with clear observation and action spaces, BrowserGym simplifies the development and testing of web agents. The complementary AgentLab framework enhances this by supporting agent creation and analysis, ultimately aiming to accelerate innovation in LLM-driven automation.'}, 'zh': {'title': 'BrowserGymï¼šæå‡ç½‘ç»œä»£ç†è¯„ä¼°çš„æ ‡å‡†åŒ–ä¸æ•ˆç‡', 'desc': 'BrowserGymç”Ÿæ€ç³»ç»Ÿæ—¨åœ¨é«˜æ•ˆè¯„ä¼°å’ŒåŸºå‡†æµ‹è¯•ç½‘ç»œä»£ç†ï¼Œç‰¹åˆ«æ˜¯åˆ©ç”¨è‡ªåŠ¨åŒ–å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œç½‘ç»œäº¤äº’çš„ä»£ç†ã€‚ç°æœ‰çš„åŸºå‡†æµ‹è¯•å¾€å¾€å­˜åœ¨ç¢ç‰‡åŒ–å’Œè¯„ä¼°æ–¹æ³•ä¸ä¸€è‡´çš„é—®é¢˜ï¼Œå¯¼è‡´éš¾ä»¥å®ç°å¯é çš„æ¯”è¾ƒå’Œå¯é‡å¤çš„ç»“æœã€‚BrowserGymé€šè¿‡æä¾›ç»Ÿä¸€çš„ã€ç±»ä¼¼äºå¥èº«æˆ¿çš„ç¯å¢ƒï¼Œå®šä¹‰æ˜ç¡®çš„è§‚å¯Ÿå’Œè¡ŒåŠ¨ç©ºé—´ï¼Œä¿ƒè¿›äº†ä¸åŒåŸºå‡†æµ‹è¯•ä¹‹é—´çš„æ ‡å‡†åŒ–è¯„ä¼°ã€‚ç»“åˆAgentLabæ¡†æ¶ï¼ŒBrowserGymä¸ä»…æ”¯æŒæ–°åŸºå‡†çš„é›†æˆï¼Œè¿˜ç¡®ä¿äº†ä¸€è‡´çš„è¯„ä¼°å’Œå…¨é¢çš„å®éªŒç®¡ç†ï¼Œä»è€ŒåŠ é€Ÿäº†åŸºäºLLMçš„è‡ªåŠ¨åŒ–åˆ›æ–°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.08629', 'title': 'FlowEdit: Inversion-Free Text-Based Editing Using Pre-Trained Flow Models', 'url': 'https://huggingface.co/papers/2412.08629', 'abstract': "Editing real images using a pre-trained text-to-image (T2I) diffusion/flow model often involves inverting the image into its corresponding noise map. However, inversion by itself is typically insufficient for obtaining satisfactory results, and therefore many methods additionally intervene in the sampling process. Such methods achieve improved results but are not seamlessly transferable between model architectures. Here, we introduce FlowEdit, a text-based editing method for pre-trained T2I flow models, which is inversion-free, optimization-free and model agnostic. Our method constructs an ODE that directly maps between the source and target distributions (corresponding to the source and target text prompts) and achieves a lower transport cost than the inversion approach. This leads to state-of-the-art results, as we illustrate with Stable Diffusion 3 and FLUX. Code and examples are available on the project's webpage.", 'score': 7, 'issue_id': 1088, 'pub_date': '2024-12-11', 'pub_date_card': {'ru': '11 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 11', 'zh': '12æœˆ11æ—¥'}, 'hash': 'b08b9bb78c9561f4', 'authors': ['Vladimir Kulikov', 'Matan Kleiner', 'Inbar Huberman-Spiegelglas', 'Tomer Michaeli'], 'affiliations': ['Technion Israel Institute of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2412.08629.jpg', 'data': {'categories': ['#optimization', '#architecture', '#cv', '#open_source', '#diffusion', '#multimodal'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'FlowEdit: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ¸Ğ½Ğ²ĞµÑ€ÑĞ¸Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ FlowEdit - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ text-to-image. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², FlowEdit Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¸Ğ½Ğ²ĞµÑ€ÑĞ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼ Ğº Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ğ¸ ODE, Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶Ğ°ÑÑ‰ĞµĞ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². FlowEdit Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ½Ğ° Ğ¸Ğ½Ğ²ĞµÑ€ÑĞ¸Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¾ Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Stable Diffusion 3 Ğ¸ FLUX.'}, 'en': {'title': 'Seamless Image Editing with FlowEdit: No Inversion Needed!', 'desc': 'This paper presents FlowEdit, a novel method for editing images using pre-trained text-to-image (T2I) flow models without the need for inversion or optimization. Traditional methods often require converting images into noise maps, which can be inefficient and model-specific. FlowEdit instead utilizes an ordinary differential equation (ODE) to directly connect the source and target distributions based on text prompts, resulting in a more efficient editing process. The approach demonstrates superior performance compared to existing methods, as shown with models like Stable Diffusion 3 and FLUX.'}, 'zh': {'title': 'æ— åæ¼”çš„æ–‡æœ¬å›¾åƒç¼–è¾‘æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºFlowEditçš„æ–‡æœ¬ç¼–è¾‘æ–¹æ³•ï¼Œä¸“ä¸ºé¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æµæ¨¡å‹è®¾è®¡ã€‚ä¸ä¼ ç»Ÿçš„å›¾åƒåæ¼”æ–¹æ³•ä¸åŒï¼ŒFlowEditä¸éœ€è¦åæ¼”å’Œä¼˜åŒ–ï¼Œä¸”å¯¹æ¨¡å‹æ¶æ„ä¸æ•æ„Ÿã€‚è¯¥æ–¹æ³•é€šè¿‡æ„å»ºä¸€ä¸ªå¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆODEï¼‰ï¼Œç›´æ¥åœ¨æºåˆ†å¸ƒå’Œç›®æ ‡åˆ†å¸ƒä¹‹é—´è¿›è¡Œæ˜ å°„ï¼Œä»è€Œé™ä½äº†ä¼ è¾“æˆæœ¬ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFlowEditåœ¨Stable Diffusion 3å’ŒFLUXä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ•ˆæœã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.07797', 'title': 'Mogo: RQ Hierarchical Causal Transformer for High-Quality 3D Human Motion Generation', 'url': 'https://huggingface.co/papers/2412.07797', 'abstract': 'In the field of text-to-motion generation, Bert-type Masked Models (MoMask, MMM) currently produce higher-quality outputs compared to GPT-type autoregressive models (T2M-GPT). However, these Bert-type models often lack the streaming output capability required for applications in video game and multimedia environments, a feature inherent to GPT-type models. Additionally, they demonstrate weaker performance in out-of-distribution generation. To surpass the quality of BERT-type models while leveraging a GPT-type structure, without adding extra refinement models that complicate scaling data, we propose a novel architecture, Mogo (Motion Only Generate Once), which generates high-quality lifelike 3D human motions by training a single transformer model. Mogo consists of only two main components: 1) RVQ-VAE, a hierarchical residual vector quantization variational autoencoder, which discretizes continuous motion sequences with high precision; 2) Hierarchical Causal Transformer, responsible for generating the base motion sequences in an autoregressive manner while simultaneously inferring residuals across different layers. Experimental results demonstrate that Mogo can generate continuous and cyclic motion sequences up to 260 frames (13 seconds), surpassing the 196 frames (10 seconds) length limitation of existing datasets like HumanML3D. On the HumanML3D test set, Mogo achieves a FID score of 0.079, outperforming both the GPT-type model T2M-GPT (FID = 0.116), AttT2M (FID = 0.112) and the BERT-type model MMM (FID = 0.080). Furthermore, our model achieves the best quantitative performance in out-of-distribution generation.', 'score': 7, 'issue_id': 1080, 'pub_date': '2024-12-05', 'pub_date_card': {'ru': '5 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 5', 'zh': '12æœˆ5æ—¥'}, 'hash': 'ff9bb8b603f9d972', 'authors': ['Dongjie Fu'], 'affiliations': ['Mogo AI'], 'pdf_title_img': 'assets/pdf/title_img/2412.07797.jpg', 'data': {'categories': ['#optimization', '#3d', '#games', '#architecture'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Mogo: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Mogo Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ°. Mogo Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ RVQ-VAE Ğ´Ğ»Ñ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ñ‹Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Mogo Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ GPT-Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ñ‹Ğµ Ğ¸ BERT-Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ½Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Mogo: Revolutionizing Text-to-Motion with High-Quality 3D Generation', 'desc': 'This paper introduces Mogo, a new architecture for generating high-quality 3D human motions from text. Mogo combines a hierarchical residual vector quantization variational autoencoder (RVQ-VAE) with a hierarchical causal transformer to produce continuous and cyclic motion sequences efficiently. Unlike existing Bert-type models, Mogo maintains the streaming output capability of GPT-type models while improving performance in out-of-distribution scenarios. Experimental results show that Mogo not only generates longer motion sequences but also achieves superior quality metrics compared to both GPT-type and Bert-type models.'}, 'zh': {'title': 'Mogoï¼šé«˜æ•ˆç”Ÿæˆé«˜è´¨é‡3Däººç±»åŠ¨ä½œçš„åˆ›æ–°æ¶æ„', 'desc': 'åœ¨æ–‡æœ¬åˆ°åŠ¨ä½œç”Ÿæˆé¢†åŸŸï¼ŒBertç±»å‹çš„æ¨¡å‹ï¼ˆå¦‚MoMask, MMMï¼‰è™½ç„¶è¾“å‡ºè´¨é‡è¾ƒé«˜ï¼Œä½†ç¼ºä¹æµå¼è¾“å‡ºèƒ½åŠ›ï¼Œæ— æ³•æ»¡è¶³è§†é¢‘æ¸¸æˆå’Œå¤šåª’ä½“ç¯å¢ƒçš„éœ€æ±‚ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒGPTç±»å‹çš„è‡ªå›å½’æ¨¡å‹ï¼ˆå¦‚T2M-GPTï¼‰å…·å¤‡è¿™ä¸€ç‰¹æ€§ï¼Œä½†åœ¨ç”Ÿæˆè´¨é‡ä¸Šç¨é€Šä¸€ç­¹ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ¶æ„Mogoï¼ˆMotion Only Generate Onceï¼‰ï¼Œå®ƒé€šè¿‡è®­ç»ƒå•ä¸€çš„å˜æ¢å™¨æ¨¡å‹ç”Ÿæˆé«˜è´¨é‡çš„3Däººç±»åŠ¨ä½œã€‚Mogoç»“åˆäº†é«˜ç²¾åº¦çš„å±‚æ¬¡æ®‹å·®å‘é‡é‡åŒ–å˜åˆ†è‡ªç¼–ç å™¨å’Œå±‚æ¬¡å› æœå˜æ¢å™¨ï¼Œèƒ½å¤Ÿç”Ÿæˆè¿ç»­ä¸”å¾ªç¯çš„åŠ¨ä½œåºåˆ—ï¼Œè¶…è¶Šäº†ç°æœ‰æ•°æ®é›†çš„é™åˆ¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.06016', 'title': 'Track4Gen: Teaching Video Diffusion Models to Track Points Improves Video Generation', 'url': 'https://huggingface.co/papers/2412.06016', 'abstract': 'While recent foundational video generators produce visually rich output, they still struggle with appearance drift, where objects gradually degrade or change inconsistently across frames, breaking visual coherence. We hypothesize that this is because there is no explicit supervision in terms of spatial tracking at the feature level. We propose Track4Gen, a spatially aware video generator that combines video diffusion loss with point tracking across frames, providing enhanced spatial supervision on the diffusion features. Track4Gen merges the video generation and point tracking tasks into a single network by making minimal changes to existing video generation architectures. Using Stable Video Diffusion as a backbone, Track4Gen demonstrates that it is possible to unify video generation and point tracking, which are typically handled as separate tasks. Our extensive evaluations show that Track4Gen effectively reduces appearance drift, resulting in temporally stable and visually coherent video generation. Project page: hyeonho99.github.io/track4gen', 'score': 5, 'issue_id': 1097, 'pub_date': '2024-12-08', 'pub_date_card': {'ru': '8 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 8', 'zh': '12æœˆ8æ—¥'}, 'hash': 'dd5fae1f1367dc71', 'authors': ['Hyeonho Jeong', 'Chun-Hao Paul Huang', 'Jong Chul Ye', 'Niloy Mitra', 'Duygu Ceylan'], 'affiliations': ['Adobe Research', 'KAIST', 'University College London'], 'pdf_title_img': 'assets/pdf/title_img/2412.06016.jpg', 'data': {'categories': ['#video', '#architecture', '#diffusion', '#games'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Ğ¡Ñ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ‚Ğ¾Ñ‡ĞµĞº', 'desc': 'Track4Gen - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ‚Ğ¾Ñ‡ĞµĞº Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ğ´Ñ€Ğ°Ğ¼Ğ¸. ĞĞ½ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ´Ñ€ĞµĞ¹Ñ„Ğ° Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ Ğ²Ğ¸Ğ´Ğ° Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ². Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Stable Video Diffusion Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¾ÑĞ½Ğ¾Ğ²Ñ‹, Track4Gen Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡ĞµĞº. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ĞµÑ‚ Ğ´Ñ€ĞµĞ¹Ñ„ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ Ğ²Ğ¸Ğ´Ğ°, ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'Unifying Video Generation and Tracking for Coherent Visuals', 'desc': 'This paper introduces Track4Gen, a novel video generator that addresses the issue of appearance drift in generated videos. Appearance drift occurs when objects in a video change inconsistently over time, leading to a lack of visual coherence. The authors propose a solution that integrates spatial tracking with video generation by using a combination of video diffusion loss and point tracking across frames. By merging these tasks into a single network, Track4Gen enhances the stability and coherence of generated videos, demonstrating significant improvements in visual quality.'}, 'zh': {'title': 'Track4Genï¼šç»Ÿä¸€è§†é¢‘ç”Ÿæˆä¸ç‚¹è·Ÿè¸ªï¼Œæå‡è§†è§‰è¿è´¯æ€§', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹Track4Genï¼Œæ—¨åœ¨è§£å†³è§†é¢‘ç”Ÿæˆä¸­çš„å¤–è§‚æ¼‚ç§»é—®é¢˜ã€‚å¤–è§‚æ¼‚ç§»æ˜¯æŒ‡åœ¨è§†é¢‘å¸§ä¸­ï¼Œç‰©ä½“çš„å¤–è§‚é€æ¸é€€åŒ–æˆ–ä¸ä¸€è‡´ï¼Œå½±å“è§†è§‰è¿è´¯æ€§ã€‚Track4Gené€šè¿‡ç»“åˆè§†é¢‘æ‰©æ•£æŸå¤±å’Œç‚¹è·Ÿè¸ªï¼Œæä¾›äº†æ›´å¼ºçš„ç©ºé—´ç›‘ç£ï¼Œä»è€Œæ”¹å–„äº†ç”Ÿæˆè§†é¢‘çš„è´¨é‡ã€‚è¯¥æ¨¡å‹å°†è§†é¢‘ç”Ÿæˆå’Œç‚¹è·Ÿè¸ªä»»åŠ¡åˆå¹¶ä¸ºä¸€ä¸ªç½‘ç»œï¼Œå±•ç¤ºäº†è¿™ä¸¤è€…å¯ä»¥æœ‰æ•ˆç»Ÿä¸€ï¼Œæ˜¾è‘—å‡å°‘å¤–è§‚æ¼‚ç§»ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.06071', 'title': 'KaSA: Knowledge-Aware Singular-Value Adaptation of Large Language Models', 'url': 'https://huggingface.co/papers/2412.06071', 'abstract': "The increasing sizes of large language models (LLMs) result in significant computational overhead and memory usage when adapting these models to specific tasks or domains. Various parameter-efficient fine-tuning (PEFT) methods have been devised to mitigate these challenges by training a small set of parameters for the task-specific updates of the model weights. Among PEFT methods, LoRA stands out for its simplicity and efficiency, inspiring the development of a series of variants. However, LoRA and its successors disregard the knowledge that is noisy or irrelevant to the targeted task, detrimentally impacting model performance and leading to suboptimality. To address this limitation, we introduce Knowledge-aware Singular-value Adaptation (KaSA), a PEFT method that leverages singular value decomposition (SVD) with knowledge-aware singular values to dynamically activate knowledge based on its relevance to the task at hand. We conduct extensive experiments across a range of LLMs on tasks spanning natural language understanding (NLU), generation (NLG), instruction following, and commonsense reasoning. The experimental results demonstrate that KaSA consistently outperforms FFT and 14 popular PEFT baselines across 16 benchmarks and 4 synthetic datasets, underscoring our method's efficacy and adaptability. The source code of our method is available at https://github.com/juyongjiang/KaSA.", 'score': 4, 'issue_id': 1089, 'pub_date': '2024-12-08', 'pub_date_card': {'ru': '8 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 8', 'zh': '12æœˆ8æ—¥'}, 'hash': 'eec0cadc18c298d0', 'authors': ['Fan Wang', 'Juyong Jiang', 'Chansung Park', 'Sunghun Kim', 'Jing Tang'], 'affiliations': ['Electronics and Telecommunications Research Institute', 'The Hong Kong University of Science and Technology', 'The Hong Kong University of Science and Technology (Guangzhou)'], 'pdf_title_img': 'assets/pdf/title_img/2412.06071.jpg', 'data': {'categories': ['#synthetic', '#transfer_learning', '#benchmark', '#optimization', '#dataset', '#training', '#open_source'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'KaSA: Ğ£Ğ¼Ğ½Ğ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ KaSA (Knowledge-aware Singular-value Adaptation). Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¸Ğ½Ğ³ÑƒĞ»ÑÑ€Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. KaSA Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°ĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº LoRA, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¸Ğ³Ğ½Ğ¾Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑˆÑƒĞ¼Ğ½Ñ‹Ğµ Ğ¸Ğ»Ğ¸ Ğ½ĞµÑ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ KaSA Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ 14 Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² PEFT Ğ½Ğ° 16 Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¸ 4 ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°, ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼ Ğ¸ Ğ·Ğ´Ñ€Ğ°Ğ²Ğ¾Ğ³Ğ¾ ÑĞ¼Ñ‹ÑĞ»Ğ°.'}, 'en': {'title': 'Enhancing Task Adaptation in LLMs with Knowledge-aware Singular-value Adaptation', 'desc': 'This paper addresses the challenges of adapting large language models (LLMs) to specific tasks while managing computational and memory costs. It introduces a new parameter-efficient fine-tuning (PEFT) method called Knowledge-aware Singular-value Adaptation (KaSA), which utilizes singular value decomposition (SVD) to focus on relevant knowledge for task performance. Unlike previous methods like LoRA, KaSA dynamically activates knowledge based on its importance, improving model efficiency and effectiveness. Experimental results show that KaSA outperforms existing PEFT techniques across various natural language tasks, demonstrating its superior adaptability and performance.'}, 'zh': {'title': 'çŸ¥è¯†æ„ŸçŸ¥å¾®è°ƒï¼Œæå‡æ¨¡å‹æ€§èƒ½ï¼', 'desc': 'éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è§„æ¨¡çš„å¢åŠ ï¼Œé€‚åº”ç‰¹å®šä»»åŠ¡æ—¶ä¼šå¯¼è‡´æ˜¾è‘—çš„è®¡ç®—å¼€é”€å’Œå†…å­˜ä½¿ç”¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæå‡ºäº†å¤šç§å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•ï¼Œé€šè¿‡è®­ç»ƒå°‘é‡å‚æ•°æ¥æ›´æ–°æ¨¡å‹æƒé‡ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„PEFTæ–¹æ³•â€”â€”çŸ¥è¯†æ„ŸçŸ¥å¥‡å¼‚å€¼é€‚åº”ï¼ˆKaSAï¼‰ï¼Œå®ƒåˆ©ç”¨å¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰åŠ¨æ€æ¿€æ´»ä¸ä»»åŠ¡ç›¸å…³çš„çŸ¥è¯†ï¼Œä»è€Œæé«˜æ¨¡å‹æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒKaSAåœ¨å¤šä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ä¸Šè¡¨ç°ä¼˜äº14ç§æµè¡Œçš„PEFTåŸºçº¿ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§å’Œé€‚åº”æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.08503', 'title': 'StyleStudio: Text-Driven Style Transfer with Selective Control of Style Elements', 'url': 'https://huggingface.co/papers/2412.08503', 'abstract': 'Text-driven style transfer aims to merge the style of a reference image with content described by a text prompt. Recent advancements in text-to-image models have improved the nuance of style transformations, yet significant challenges remain, particularly with overfitting to reference styles, limiting stylistic control, and misaligning with textual content. In this paper, we propose three complementary strategies to address these issues. First, we introduce a cross-modal Adaptive Instance Normalization (AdaIN) mechanism for better integration of style and text features, enhancing alignment. Second, we develop a Style-based Classifier-Free Guidance (SCFG) approach that enables selective control over stylistic elements, reducing irrelevant influences. Finally, we incorporate a teacher model during early generation stages to stabilize spatial layouts and mitigate artifacts. Our extensive evaluations demonstrate significant improvements in style transfer quality and alignment with textual prompts. Furthermore, our approach can be integrated into existing style transfer frameworks without fine-tuning.', 'score': 2, 'issue_id': 1088, 'pub_date': '2024-12-11', 'pub_date_card': {'ru': '11 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 11', 'zh': '12æœˆ11æ—¥'}, 'hash': '7b217a36b7d6db44', 'authors': ['Mingkun Lei', 'Xue Song', 'Beier Zhu', 'Hao Wang', 'Chi Zhang'], 'affiliations': ['AGI Lab, Westlake University', 'Fudan University', 'Nanyang Technological University', 'The Hong Kong University of Science and Technology (Guangzhou)'], 'pdf_title_img': 'assets/pdf/title_img/2412.08503.jpg', 'data': {'categories': ['#cv', '#multimodal'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿ĞµÑ€ĞµĞ½Ğ¾Ñ ÑÑ‚Ğ¸Ğ»Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑÑƒ ÑÑ‚Ğ¸Ğ»Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ñ‚Ñ€Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸: ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ AdaIN Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ ÑÑ‚Ğ¸Ğ»Ñ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Style-based Classifier-Free Guidance Ğ´Ğ»Ñ ÑĞµĞ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ ÑÑ‚Ğ¸Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½Ğ¾Ğ²ĞºĞ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ° ÑÑ‚Ğ¸Ğ»Ñ Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ°Ğ¼. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ Ğ² ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ¸ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Enhancing Text-Driven Style Transfer with Adaptive Techniques', 'desc': 'This paper focuses on improving text-driven style transfer, which combines the style of an image with content from a text description. The authors identify challenges such as overfitting to styles and misalignment with text, and propose three strategies to overcome these issues. They introduce a cross-modal Adaptive Instance Normalization (AdaIN) for better feature integration, a Style-based Classifier-Free Guidance (SCFG) for selective stylistic control, and a teacher model to stabilize outputs. The results show enhanced style transfer quality and better alignment with text, and the methods can be easily integrated into existing frameworks.'}, 'zh': {'title': 'æå‡æ–‡æœ¬é©±åŠ¨é£æ ¼è¿ç§»çš„è´¨é‡ä¸å¯¹é½æ€§', 'desc': 'æœ¬æ–‡ç ”ç©¶äº†æ–‡æœ¬é©±åŠ¨çš„é£æ ¼è¿ç§»ï¼Œæ—¨åœ¨å°†å‚è€ƒå›¾åƒçš„é£æ ¼ä¸æ–‡æœ¬æç¤ºæè¿°çš„å†…å®¹ç›¸ç»“åˆã€‚å°½ç®¡æœ€è¿‘çš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹åœ¨é£æ ¼è½¬æ¢çš„ç»†å¾®å·®åˆ«ä¸Šå–å¾—äº†è¿›å±•ï¼Œä½†ä»é¢ä¸´è¿‡æ‹Ÿåˆã€é£æ ¼æ§åˆ¶æœ‰é™å’Œæ–‡æœ¬å†…å®¹ä¸å¯¹é½ç­‰æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸‰ç§äº’è¡¥ç­–ç•¥ï¼ŒåŒ…æ‹¬è·¨æ¨¡æ€è‡ªé€‚åº”å®ä¾‹å½’ä¸€åŒ–æœºåˆ¶ã€åŸºäºé£æ ¼çš„æ— åˆ†ç±»å™¨å¼•å¯¼æ–¹æ³•ä»¥åŠåœ¨ç”Ÿæˆæ—©æœŸé˜¶æ®µå¼•å…¥æ•™å¸ˆæ¨¡å‹ã€‚æˆ‘ä»¬çš„è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œæ‰€ææ–¹æ³•åœ¨é£æ ¼è¿ç§»è´¨é‡å’Œä¸æ–‡æœ¬æç¤ºçš„å¯¹é½æ€§ä¸Šæœ‰æ˜¾è‘—æå‡ï¼Œå¹¶ä¸”å¯ä»¥æ— ç¼é›†æˆåˆ°ç°æœ‰çš„é£æ ¼è¿ç§»æ¡†æ¶ä¸­ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.07147', 'title': 'MIT-10M: A Large Scale Parallel Corpus of Multilingual Image Translation', 'url': 'https://huggingface.co/papers/2412.07147', 'abstract': 'Image Translation (IT) holds immense potential across diverse domains, enabling the translation of textual content within images into various languages. However, existing datasets often suffer from limitations in scale, diversity, and quality, hindering the development and evaluation of IT models. To address this issue, we introduce MIT-10M, a large-scale parallel corpus of multilingual image translation with over 10M image-text pairs derived from real-world data, which has undergone extensive data cleaning and multilingual translation validation. It contains 840K images in three sizes, 28 categories, tasks with three levels of difficulty and 14 languages image-text pairs, which is a considerable improvement on existing datasets. We conduct extensive experiments to evaluate and train models on MIT-10M. The experimental results clearly indicate that our dataset has higher adaptability when it comes to evaluating the performance of the models in tackling challenging and complex image translation tasks in the real world. Moreover, the performance of the model fine-tuned with MIT-10M has tripled compared to the baseline model, further confirming its superiority.', 'score': 1, 'issue_id': 1087, 'pub_date': '2024-12-10', 'pub_date_card': {'ru': '10 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 10', 'zh': '12æœˆ10æ—¥'}, 'hash': 'cf26c855c674b8aa', 'authors': ['Bo Li', 'Shaolin Zhu', 'Lijie Wen'], 'affiliations': ['Baidu Inc., Beijing, China', 'College of Intelligence and Computing, Tianjin University, Tianjin, China', 'School of Software, Tsinghua University, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2412.07147.jpg', 'data': {'categories': ['#machine_translation', '#training', '#multilingual', '#benchmark', '#dataset', '#data', '#synthetic'], 'emoji': 'ğŸŒ', 'ru': {'title': 'MIT-10M: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ´Ğ»Ñ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° Ñ‚ĞµĞºÑÑ‚Ğ° Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… MIT-10M Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° Ñ‚ĞµĞºÑÑ‚Ğ° Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…. Ğ­Ñ‚Ğ¾Ñ‚ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ±Ğ¾Ğ»ĞµĞµ 10 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¿Ğ°Ñ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ-Ñ‚ĞµĞºÑÑ‚ Ğ½Ğ° 14 ÑĞ·Ñ‹ĞºĞ°Ñ…, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… 28 ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¹ Ğ¸ Ñ‚Ñ€Ğ¸ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸. MIT-10M Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ñƒ, Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° MIT-10M, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ñ‚Ñ€ĞµÑ…ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸.'}, 'en': {'title': 'Unlocking Multilingual Image Translation with MIT-10M', 'desc': 'This paper presents MIT-10M, a new large-scale dataset designed for multilingual image translation (IT). It consists of over 10 million image-text pairs, which are carefully curated to enhance diversity and quality, addressing the limitations of existing datasets. The dataset includes images across 28 categories and supports 14 languages, making it suitable for various IT tasks of different complexities. Experimental results show that models trained on MIT-10M significantly outperform baseline models, demonstrating its effectiveness in real-world image translation scenarios.'}, 'zh': {'title': 'MIT-10Mï¼šæå‡å›¾åƒç¿»è¯‘çš„å¤šè¯­è¨€æ•°æ®é›†', 'desc': 'æœ¬æ–‡ä»‹ç»äº†MIT-10Mï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„å¤šè¯­è¨€å›¾åƒç¿»è¯‘å¹³è¡Œè¯­æ–™åº“ï¼ŒåŒ…å«è¶…è¿‡1000ä¸‡å¯¹å›¾åƒå’Œæ–‡æœ¬ã€‚è¯¥æ•°æ®é›†ç»è¿‡ä¸¥æ ¼çš„æ•°æ®æ¸…ç†å’Œå¤šè¯­è¨€ç¿»è¯‘éªŒè¯ï¼Œè§£å†³äº†ç°æœ‰æ•°æ®é›†åœ¨è§„æ¨¡ã€å¤šæ ·æ€§å’Œè´¨é‡ä¸Šçš„ä¸è¶³ã€‚MIT-10MåŒ…å«840Kå¼ å›¾åƒï¼Œæ¶µç›–28ä¸ªç±»åˆ«å’Œ14ç§è¯­è¨€ï¼Œæä¾›äº†ä¸åŒéš¾åº¦çº§åˆ«çš„ä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨MIT-10Må¾®è°ƒçš„æ¨¡å‹åœ¨å¤„ç†å¤æ‚çš„å›¾åƒç¿»è¯‘ä»»åŠ¡æ—¶ï¼Œæ€§èƒ½æ˜¾è‘—æå‡ï¼Œè¾¾åˆ°äº†åŸºçº¿æ¨¡å‹çš„ä¸‰å€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.06676', 'title': "I Don't Know: Explicit Modeling of Uncertainty with an [IDK] Token", 'url': 'https://huggingface.co/papers/2412.06676', 'abstract': 'Large Language Models are known to capture real-world knowledge, allowing them to excel in many downstream tasks. Despite recent advances, these models are still prone to what are commonly known as hallucinations, causing them to emit unwanted and factually incorrect text. In this work, we propose a novel calibration method that can be used to combat hallucinations. We add a special [IDK] ("I don\'t know") token to the model\'s vocabulary and introduce an objective function that shifts probability mass to the [IDK] token for incorrect predictions. This approach allows the model to express uncertainty in its output explicitly. We evaluate our proposed method across multiple model architectures and factual downstream tasks. We find that models trained with our method are able to express uncertainty in places where they would previously make mistakes while suffering only a small loss of encoded knowledge. We further perform extensive ablation studies of multiple variations of our approach and provide a detailed analysis of the precision-recall tradeoff of our method.', 'score': 0, 'issue_id': 1096, 'pub_date': '2024-12-09', 'pub_date_card': {'ru': '9 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 9', 'zh': '12æœˆ9æ—¥'}, 'hash': 'c40c597f8ab0b2f9', 'authors': ['Roi Cohen', 'Konstantin Dobler', 'Eden Biran', 'Gerard de Melo'], 'affiliations': ['HPI / University of Potsdam', 'Tel Aviv University'], 'pdf_title_img': 'assets/pdf/title_img/2412.06676.jpg', 'data': {'categories': ['#training', '#hallucinations', '#architecture'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞĞ°ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ˜Ğ˜ Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ¸Ñ‚ÑŒ Â«Ğ¯ Ğ½Ğµ Ğ·Ğ½Ğ°ÑÂ»', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ´Ğ»Ñ Ğ±Ğ¾Ñ€ÑŒĞ±Ñ‹ Ñ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½ [IDK] (Â«Ğ¯ Ğ½Ğµ Ğ·Ğ½Ğ°ÑÂ») Ğ² ÑĞ»Ğ¾Ğ²Ğ°Ñ€ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ñ†ĞµĞ»ĞµĞ²ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ¼ĞµÑ‰Ğ°ĞµÑ‚ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ½ÑƒÑ Ğ¼Ğ°ÑÑÑƒ Ğº ÑÑ‚Ğ¾Ğ¼Ñƒ Ñ‚Ğ¾ĞºĞµĞ½Ñƒ Ğ´Ğ»Ñ Ğ½ĞµĞ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ²Ğ½Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ¶Ğ°Ñ‚ÑŒ Ğ½ĞµÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² ÑĞ²Ğ¾Ğ¸Ñ… Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ‚Ğ°ĞºĞ¸Ğ¼ Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹ Ğ²Ñ‹Ñ€Ğ°Ğ¶Ğ°Ñ‚ÑŒ Ğ½ĞµÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¼ĞµÑÑ‚Ğ°Ñ…, Ğ³Ğ´Ğµ Ñ€Ğ°Ğ½ÑŒÑˆĞµ Ğ´ĞµĞ»Ğ°Ğ»Ğ¸ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ñ‚ĞµÑ€ÑÑ Ğ»Ğ¸ÑˆÑŒ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆÑƒÑ Ñ‡Ğ°ÑÑ‚ÑŒ Ğ·Ğ°ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹.'}, 'en': {'title': "Empowering Models to Say 'I Don't Know'", 'desc': "This paper addresses the issue of hallucinations in Large Language Models (LLMs), where the models generate incorrect information. The authors introduce a calibration method that incorporates an [IDK] token, allowing the model to indicate uncertainty in its predictions. By adjusting the model's objective function, they shift some probability away from incorrect outputs towards the [IDK] token. The results show that this method helps models to better express uncertainty while maintaining most of their factual knowledge."}, 'zh': {'title': 'é€šè¿‡[IDK]æ ‡è®°æå‡æ¨¡å‹çš„ä¸ç¡®å®šæ€§è¡¨è¾¾', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿæ•æ‰ç°å®ä¸–ç•Œçš„çŸ¥è¯†ï¼Œå› æ­¤åœ¨è®¸å¤šä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹ä»ç„¶å®¹æ˜“å‡ºç°å¹»è§‰ï¼Œå¯¼è‡´ç”Ÿæˆä¸å‡†ç¡®çš„æ–‡æœ¬ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ ¡å‡†æ–¹æ³•ï¼Œé€šè¿‡åœ¨æ¨¡å‹è¯æ±‡ä¸­æ·»åŠ ç‰¹æ®Šçš„[IDK]ï¼ˆ"æˆ‘ä¸çŸ¥é“"ï¼‰æ ‡è®°ï¼Œå¹¶å¼•å…¥ä¸€ä¸ªç›®æ ‡å‡½æ•°ï¼Œå°†æ¦‚ç‡åˆ†é…è½¬ç§»åˆ°[IDK]æ ‡è®°ä¸Šï¼Œä»è€Œæœ‰æ•ˆåº”å¯¹å¹»è§‰é—®é¢˜ã€‚ç»è¿‡è¯„ä¼°ï¼Œæˆ‘ä»¬å‘ç°ä½¿ç”¨è¯¥æ–¹æ³•è®­ç»ƒçš„æ¨¡å‹èƒ½å¤Ÿåœ¨ä»¥å‰å‡ºé”™çš„åœ°æ–¹æ˜ç¡®è¡¨è¾¾ä¸ç¡®å®šæ€§ï¼ŒåŒæ—¶ä»…æœ‰å°‘é‡çŸ¥è¯†æŸå¤±ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.08467', 'title': 'Bootstrapping Language-Guided Navigation Learning with Self-Refining Data Flywheel', 'url': 'https://huggingface.co/papers/2412.08467', 'abstract': 'Creating high-quality data for training robust language-instructed agents is a long-lasting challenge in embodied AI. In this paper, we introduce a Self-Refining Data Flywheel (SRDF) that generates high-quality and large-scale navigational instruction-trajectory pairs by iteratively refining the data pool through the collaboration between two models, the instruction generator and the navigator, without any human-in-the-loop annotation. Specifically, SRDF starts with using a base generator to create an initial data pool for training a base navigator, followed by applying the trained navigator to filter the data pool. This leads to higher-fidelity data to train a better generator, which can, in turn, produce higher-quality data for training the next-round navigator. Such a flywheel establishes a data self-refining process, yielding a continuously improved and highly effective dataset for large-scale language-guided navigation learning. Our experiments demonstrate that after several flywheel rounds, the navigator elevates the performance boundary from 70% to 78% SPL on the classic R2R test set, surpassing human performance (76%) for the first time. Meanwhile, this process results in a superior generator, evidenced by a SPICE increase from 23.5 to 26.2, better than all previous VLN instruction generation methods. Finally, we demonstrate the scalability of our method through increasing environment and instruction diversity, and the generalization ability of our pre-trained navigator across various downstream navigation tasks, surpassing state-of-the-art methods by a large margin in all cases.', 'score': 0, 'issue_id': 1090, 'pub_date': '2024-12-11', 'pub_date_card': {'ru': '11 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 11', 'zh': '12æœˆ11æ—¥'}, 'hash': 'f49d4b05298275a6', 'authors': ['Zun Wang', 'Jialu Li', 'Yicong Hong', 'Songze Li', 'Kunchang Li', 'Shoubin Yu', 'Yi Wang', 'Yu Qiao', 'Yali Wang', 'Mohit Bansal', 'Limin Wang'], 'affiliations': ['Adobe Research', 'Nanjing University', 'Shanghai AI Laboratory', 'UNC Chapel Hill'], 'pdf_title_img': 'assets/pdf/title_img/2412.08467.jpg', 'data': {'categories': ['#dataset', '#transfer_learning', '#optimization', '#agents', '#training'], 'emoji': 'ğŸ§­', 'ru': {'title': 'Ğ¡Ğ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¹ÑÑ Ñ†Ğ¸ĞºĞ» Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Self-Refining Data Flywheel (SRDF) Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼Ğ¸ Ğ² Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ˜Ğ˜. SRDF Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ, Ğ² ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ Ğ´Ğ²Ğ° Ğ¼Ğ¾Ğ´ÑƒĞ»Ñ - Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ¸ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ‚Ğ¾Ñ€ - ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸Ñ‡Ğ°ÑÑ‚ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ±ĞµĞ· ÑƒÑ‡Ğ°ÑÑ‚Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾ÑĞ»Ğµ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ‚Ğ¾Ñ€ Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾ÑˆĞµĞ» Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ R2R, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ½ÑƒĞ² 78% SPL. ĞœĞµÑ‚Ğ¾Ğ´ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'Self-Refining Data Flywheel: Elevating Language-Guided Navigation Performance', 'desc': "This paper presents a novel approach called the Self-Refining Data Flywheel (SRDF) to enhance the quality of data used for training language-instructed agents in embodied AI. The SRDF operates by iteratively refining a dataset of navigational instruction-trajectory pairs through the collaboration of an instruction generator and a navigator, eliminating the need for human annotation. By using a base generator to create initial data and then filtering it with a trained navigator, the process continuously improves the dataset, leading to better training outcomes. The results show significant performance improvements, with the navigator achieving a success rate of 78% on the R2R test set, surpassing human performance, and demonstrating the method's scalability and generalization across various tasks."}, 'zh': {'title': 'è‡ªæˆ‘ç²¾ç‚¼æ•°æ®é£è½®ï¼šæå‡å¯¼èˆªæ€§èƒ½çš„æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªæˆ‘ç²¾ç‚¼æ•°æ®é£è½®ï¼ˆSRDFï¼‰ï¼Œæ—¨åœ¨ä¸ºè¯­è¨€æŒ‡å¯¼çš„å¯¼èˆªä»£ç†ç”Ÿæˆé«˜è´¨é‡çš„å¤§è§„æ¨¡æ•°æ®ã€‚SRDFé€šè¿‡æŒ‡ä»¤ç”Ÿæˆå™¨å’Œå¯¼èˆªå™¨ä¸¤ä¸ªæ¨¡å‹çš„åä½œï¼Œè¿­ä»£åœ°ç²¾ç‚¼æ•°æ®æ± ï¼Œä»è€Œæ— éœ€äººå·¥æ ‡æ³¨ã€‚è¯¥æ–¹æ³•ä»åŸºç¡€ç”Ÿæˆå™¨å¼€å§‹ï¼Œåˆ›å»ºåˆå§‹æ•°æ®æ± ï¼Œç„¶ååˆ©ç”¨è®­ç»ƒå¥½çš„å¯¼èˆªå™¨è¿‡æ»¤æ•°æ®ï¼Œå½¢æˆé«˜ä¿çœŸæ•°æ®ä»¥è®­ç»ƒæ›´å¥½çš„ç”Ÿæˆå™¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç»è¿‡å¤šè½®é£è½®è¿­ä»£ï¼Œå¯¼èˆªå™¨çš„æ€§èƒ½ä»70%æå‡è‡³78%ï¼Œé¦–æ¬¡è¶…è¶Šäººç±»è¡¨ç°ï¼ŒåŒæ—¶ç”Ÿæˆå™¨çš„æ€§èƒ½ä¹Ÿæ˜¾è‘—æé«˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.04467', 'title': 'VisionZip: Longer is Better but Not Necessary in Vision Language Models', 'url': 'https://huggingface.co/papers/2412.04467', 'abstract': 'Recent advancements in vision-language models have enhanced performance by increasing the length of visual tokens, making them much longer than text tokens and significantly raising computational costs. However, we observe that the visual tokens generated by popular vision encoders, such as CLIP and SigLIP, contain significant redundancy. To address this, we introduce VisionZip, a simple yet effective method that selects a set of informative tokens for input to the language model, reducing visual token redundancy and improving efficiency while maintaining model performance. The proposed VisionZip can be widely applied to image and video understanding tasks and is well-suited for multi-turn dialogues in real-world scenarios, where previous methods tend to underperform. Experimental results show that VisionZip outperforms the previous state-of-the-art method by at least 5% performance gains across nearly all settings. Moreover, our method significantly enhances model inference speed, improving the prefilling time by 8x and enabling the LLaVA-Next 13B model to infer faster than the LLaVA-Next 7B model while achieving better results. Furthermore, we analyze the causes of this redundancy and encourage the community to focus on extracting better visual features rather than merely increasing token length. Our code is available at https://github.com/dvlab-research/VisionZip .', 'score': 80, 'issue_id': 980, 'pub_date': '2024-12-05', 'pub_date_card': {'ru': '5 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 5', 'zh': '12æœˆ5æ—¥'}, 'hash': '5539efbb0d3e8e80', 'authors': ['Senqiao Yang', 'Yukang Chen', 'Zhuotao Tian', 'Chengyao Wang', 'Jingyao Li', 'Bei Yu', 'Jiaya Jia'], 'affiliations': ['CUHK', 'HITSZ', 'HKUST'], 'pdf_title_img': 'assets/pdf/title_img/2412.04467.jpg', 'data': {'categories': ['#inference', '#interpretability', '#multimodal', '#optimization', '#cv'], 'emoji': 'ğŸ—œï¸', 'ru': {'title': 'VisionZip: Ğ¡Ğ¶Ğ¸Ğ¼Ğ°ĞµĞ¼ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹, ÑƒÑĞºĞ¾Ñ€ÑĞµĞ¼ Ğ˜Ğ˜', 'desc': 'VisionZip - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞĞ½ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ°Ğ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ñ‚Ğ°ĞºĞ¸Ğ¼Ğ¸ ĞºĞ°Ğº CLIP Ğ¸ SigLIP. VisionZip Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ğ¾ ÑĞ¾ÑÑ€ĞµĞ´Ğ¾Ñ‚Ğ¾Ñ‡Ğ¸Ñ‚ÑŒÑÑ Ğ½Ğ° Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ², Ğ° Ğ½Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ Ğ½Ğ° ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ².'}, 'en': {'title': 'Streamlining Visual Tokens for Enhanced Efficiency in Vision-Language Models', 'desc': 'This paper presents VisionZip, a method designed to reduce redundancy in visual tokens used in vision-language models. By selecting only the most informative tokens, VisionZip improves computational efficiency without sacrificing performance. The method shows significant improvements in both model inference speed and overall accuracy, outperforming previous state-of-the-art techniques by at least 5%. The authors emphasize the importance of optimizing visual feature extraction rather than simply increasing token length.'}, 'zh': {'title': 'VisionZipï¼šé«˜æ•ˆå‡å°‘è§†è§‰æ ‡è®°å†—ä½™çš„åˆ›æ–°æ–¹æ³•', 'desc': 'æœ€è¿‘ï¼Œè§†è§‰è¯­è¨€æ¨¡å‹çš„è¿›å±•é€šè¿‡å¢åŠ è§†è§‰æ ‡è®°çš„é•¿åº¦æ¥æé«˜æ€§èƒ½ï¼Œä½†è¿™ä¹Ÿæ˜¾è‘—å¢åŠ äº†è®¡ç®—æˆæœ¬ã€‚æˆ‘ä»¬å‘ç°ï¼Œæµè¡Œçš„è§†è§‰ç¼–ç å™¨ç”Ÿæˆçš„è§†è§‰æ ‡è®°å­˜åœ¨æ˜¾è‘—çš„å†—ä½™ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†VisionZipï¼Œè¿™æ˜¯ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ–¹æ³•ï¼Œå¯ä»¥é€‰æ‹©ä¸€ç»„ä¿¡æ¯ä¸°å¯Œçš„æ ‡è®°è¾“å…¥åˆ°è¯­è¨€æ¨¡å‹ä¸­ï¼Œä»è€Œå‡å°‘è§†è§‰æ ‡è®°çš„å†—ä½™ï¼Œæé«˜æ•ˆç‡ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVisionZipåœ¨å‡ ä¹æ‰€æœ‰è®¾ç½®ä¸­æ¯”ä¹‹å‰çš„æœ€å…ˆè¿›æ–¹æ³•æé«˜äº†è‡³å°‘5%çš„æ€§èƒ½ï¼Œå¹¶æ˜¾è‘—æå‡äº†æ¨¡å‹æ¨ç†é€Ÿåº¦ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.04424', 'title': 'Florence-VL: Enhancing Vision-Language Models with Generative Vision Encoder and Depth-Breadth Fusion', 'url': 'https://huggingface.co/papers/2412.04424', 'abstract': 'We present Florence-VL, a new family of multimodal large language models (MLLMs) with enriched visual representations produced by Florence-2, a generative vision foundation model. Unlike the widely used CLIP-style vision transformer trained by contrastive learning, Florence-2 can capture different levels and aspects of visual features, which are more versatile to be adapted to diverse downstream tasks. We propose a novel feature-fusion architecture and an innovative training recipe that effectively integrates Florence-2\'s visual features into pretrained LLMs, such as Phi 3.5 and LLama 3. In particular, we propose "depth-breath fusion (DBFusion)" to fuse the visual features extracted from different depths and under multiple prompts. Our model training is composed of end-to-end pretraining of the whole model followed by finetuning of the projection layer and the LLM, on a carefully designed recipe of diverse open-source datasets that include high-quality image captions and instruction-tuning pairs. Our quantitative analysis and visualization of Florence-VL\'s visual features show its advantages over popular vision encoders on vision-language alignment, where the enriched depth and breath play important roles. Florence-VL achieves significant improvements over existing state-of-the-art MLLMs across various multi-modal and vision-centric benchmarks covering general VQA, perception, hallucination, OCR, Chart, knowledge-intensive understanding, etc. To facilitate future research, our models and the complete training recipe are open-sourced. https://github.com/JiuhaiChen/Florence-VL', 'score': 42, 'issue_id': 981, 'pub_date': '2024-12-05', 'pub_date_card': {'ru': '5 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 5', 'zh': '12æœˆ5æ—¥'}, 'hash': '18a7fac6be50215d', 'authors': ['Jiuhai Chen', 'Jianwei Yang', 'Haiping Wu', 'Dianqi Li', 'Jianfeng Gao', 'Tianyi Zhou', 'Bin Xiao'], 'affiliations': ['Microsoft Research', 'University of Maryland'], 'pdf_title_img': 'assets/pdf/title_img/2412.04424.jpg', 'data': {'categories': ['#alignment', '#training', '#benchmark', '#hallucinations', '#open_source', '#architecture', '#multimodal'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'Florence-VL: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Florence-VL Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ½Ğ¾Ğ²Ğ¾Ğµ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Florence-2 - Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² CLIP, Florence-2 ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑƒÑ€Ğ¾Ğ²Ğ½Ğ¸ Ğ¸ Ğ°ÑĞ¿ĞµĞºÑ‚Ñ‹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸Ğº. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¸ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ€ĞµÑ†ĞµĞ¿Ñ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ Florence-2 Ğ² Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Florence-VL Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ MLLM Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ·Ñ€ĞµĞ½Ğ¸Ğµ Ñ‚ĞµÑÑ‚Ğ°Ñ….'}, 'en': {'title': 'Florence-VL: Bridging Vision and Language with Depth-Breath Fusion', 'desc': 'Florence-VL is a new type of multimodal large language model that enhances visual understanding using advanced features from the Florence-2 vision model. Unlike traditional models that rely on contrastive learning, Florence-2 captures a wider range of visual details, making it adaptable for various tasks. The model employs a unique feature-fusion method called depth-breath fusion (DBFusion) to combine visual information from different levels and prompts effectively. As a result, Florence-VL outperforms existing models in multiple benchmarks related to vision and language tasks, and its training methods are available for further research.'}, 'zh': {'title': 'Florence-VLï¼šå¤šæ¨¡æ€è¯­è¨€æ¨¡å‹çš„æ–°çªç ´', 'desc': 'Florence-VLæ˜¯ä¸€ç§æ–°å‹çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œç»“åˆäº†Florence-2ç”Ÿæˆçš„ä¸°å¯Œè§†è§‰è¡¨ç¤ºã€‚ä¸ä¼ ç»Ÿçš„å¯¹æ¯”å­¦ä¹ è®­ç»ƒçš„CLIPé£æ ¼è§†è§‰å˜æ¢å™¨ä¸åŒï¼ŒFlorence-2èƒ½å¤Ÿæ•æ‰ä¸åŒå±‚æ¬¡å’Œæ–¹é¢çš„è§†è§‰ç‰¹å¾ï¼Œæ›´åŠ çµæ´»åœ°é€‚åº”å„ç§ä¸‹æ¸¸ä»»åŠ¡ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„ç‰¹å¾èåˆæ¶æ„å’Œåˆ›æ–°çš„è®­ç»ƒæ–¹æ¡ˆï¼Œæœ‰æ•ˆåœ°å°†Florence-2çš„è§†è§‰ç‰¹å¾æ•´åˆåˆ°é¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹ä¸­ã€‚Florence-VLåœ¨å¤šç§å¤šæ¨¡æ€å’Œè§†è§‰ä¸­å¿ƒåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº†æ€§èƒ½ï¼Œå±•ç¤ºäº†å…¶åœ¨è§†è§‰-è¯­è¨€å¯¹é½æ–¹é¢çš„ä¼˜åŠ¿ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.04468', 'title': 'NVILA: Efficient Frontier Visual Language Models', 'url': 'https://huggingface.co/papers/2412.04468', 'abstract': 'Visual language models (VLMs) have made significant advances in accuracy in recent years. However, their efficiency has received much less attention. This paper introduces NVILA, a family of open VLMs designed to optimize both efficiency and accuracy. Building on top of VILA, we improve its model architecture by first scaling up the spatial and temporal resolutions, and then compressing visual tokens. This "scale-then-compress" approach enables NVILA to efficiently process high-resolution images and long videos. We also conduct a systematic investigation to enhance the efficiency of NVILA throughout its entire lifecycle, from training and fine-tuning to deployment. NVILA matches or surpasses the accuracy of many leading open and proprietary VLMs across a wide range of image and video benchmarks. At the same time, it reduces training costs by 4.5X, fine-tuning memory usage by 3.4X, pre-filling latency by 1.6-2.2X, and decoding latency by 1.2-2.8X. We will soon make our code and models available to facilitate reproducibility.', 'score': 35, 'issue_id': 998, 'pub_date': '2024-12-05', 'pub_date_card': {'ru': '5 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 5', 'zh': '12æœˆ5æ—¥'}, 'hash': '56c438ada1bc3480', 'authors': ['Zhijian Liu', 'Ligeng Zhu', 'Baifeng Shi', 'Zhuoyang Zhang', 'Yuming Lou', 'Shang Yang', 'Haocheng Xi', 'Shiyi Cao', 'Yuxian Gu', 'Dacheng Li', 'Xiuyu Li', 'Yunhao Fang', 'Yukang Chen', 'Cheng-Yu Hsieh', 'De-An Huang', 'An-Chieh Cheng', 'Vishwesh Nath', 'Jinyi Hu', 'Sifei Liu', 'Ranjay Krishna', 'Daguang Xu', 'Xiaolong Wang', 'Pavlo Molchanov', 'Jan Kautz', 'Hongxu Yin', 'Song Han', 'Yao Lu'], 'affiliations': ['MIT', 'NVIDIA', 'Tsinghua University', 'UC Berkeley', 'UC San Diego', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2412.04468.jpg', 'data': {'categories': ['#architecture', '#training', '#benchmark', '#video', '#open_source', '#optimization', '#cv'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'NVILA: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¾ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ NVILA, Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ğ»Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ VILA, ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ ÑĞ¶Ğ°Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹. NVILA Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞŸÑ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ñ„Ğ°Ğ¹Ğ½-Ñ‚ÑĞ½Ğ¸Ğ½Ğ³Ğµ Ğ¸ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞµ.'}, 'en': {'title': 'NVILA: Efficiency Meets Accuracy in Visual Language Models', 'desc': "This paper presents NVILA, a new family of visual language models (VLMs) that focus on improving both efficiency and accuracy. The authors enhance the existing VILA architecture by increasing spatial and temporal resolutions before compressing visual tokens, which allows NVILA to handle high-resolution images and long videos more effectively. They also perform a comprehensive analysis to boost NVILA's efficiency throughout its lifecycle, including training, fine-tuning, and deployment. As a result, NVILA achieves competitive accuracy while significantly reducing training and operational costs compared to other leading VLMs."}, 'zh': {'title': 'é«˜æ•ˆä¸å‡†ç¡®å¹¶é‡çš„è§†è§‰è¯­è¨€æ¨¡å‹', 'desc': 'è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰è¿‘å¹´æ¥åœ¨å‡†ç¡®æ€§æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å…¶æ•ˆç‡å´å—åˆ°è¾ƒå°‘å…³æ³¨ã€‚æœ¬æ–‡ä»‹ç»äº†NVILAï¼Œè¿™æ˜¯ä¸€ç³»åˆ—æ—¨åœ¨ä¼˜åŒ–æ•ˆç‡å’Œå‡†ç¡®æ€§çš„å¼€æ”¾å¼VLMã€‚æˆ‘ä»¬é€šè¿‡å…ˆæé«˜ç©ºé—´å’Œæ—¶é—´åˆ†è¾¨ç‡ï¼Œç„¶åå‹ç¼©è§†è§‰æ ‡è®°ï¼Œæ”¹è¿›äº†VILAçš„æ¨¡å‹æ¶æ„ã€‚è¿™ç§â€œå…ˆæ‰©å±•åå‹ç¼©â€çš„æ–¹æ³•ä½¿NVILAèƒ½å¤Ÿé«˜æ•ˆå¤„ç†é«˜åˆ†è¾¨ç‡å›¾åƒå’Œé•¿è§†é¢‘ï¼ŒåŒæ—¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ä¸é¢†å…ˆçš„VLMç›¸æ¯”ï¼Œå‡†ç¡®æ€§ç›¸å½“æˆ–æ›´é«˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.04455', 'title': 'Code-as-Monitor: Constraint-aware Visual Programming for Reactive and Proactive Robotic Failure Detection', 'url': 'https://huggingface.co/papers/2412.04455', 'abstract': 'Automatic detection and prevention of open-set failures are crucial in closed-loop robotic systems. Recent studies often struggle to simultaneously identify unexpected failures reactively after they occur and prevent foreseeable ones proactively. To this end, we propose Code-as-Monitor (CaM), a novel paradigm leveraging the vision-language model (VLM) for both open-set reactive and proactive failure detection. The core of our method is to formulate both tasks as a unified set of spatio-temporal constraint satisfaction problems and use VLM-generated code to evaluate them for real-time monitoring. To enhance the accuracy and efficiency of monitoring, we further introduce constraint elements that abstract constraint-related entities or their parts into compact geometric elements. This approach offers greater generality, simplifies tracking, and facilitates constraint-aware visual programming by leveraging these elements as visual prompts. Experiments show that CaM achieves a 28.7% higher success rate and reduces execution time by 31.8% under severe disturbances compared to baselines across three simulators and a real-world setting. Moreover, CaM can be integrated with open-loop control policies to form closed-loop systems, enabling long-horizon tasks in cluttered scenes with dynamic environments.', 'score': 32, 'issue_id': 981, 'pub_date': '2024-12-05', 'pub_date_card': {'ru': '5 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 5', 'zh': '12æœˆ5æ—¥'}, 'hash': 'a00b427eb116e4bf', 'authors': ['Enshen Zhou', 'Qi Su', 'Cheng Chi', 'Zhizheng Zhang', 'Zhongyuan Wang', 'Tiejun Huang', 'Lu Sheng', 'He Wang'], 'affiliations': ['Beijing Academy of Artificial Intelligence', 'School of Computer Science, Peking University', 'School of Software, Beihang University'], 'pdf_title_img': 'assets/pdf/title_img/2412.04455.jpg', 'data': {'categories': ['#agents', '#robotics', '#optimization', '#cv', '#security'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞšĞ¾Ğ´ ĞºĞ°Ğº Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€: ÑƒĞ¼Ğ½Ğ¾Ğµ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ²', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Code-as-Monitor (CaM), Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ² Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…. CaM Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ĞºĞ°Ğº Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ´ Ğ´Ğ»Ñ Ğ¸Ñ… Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ² Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ°Ğ±ÑÑ‚Ñ€Ğ°Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚ĞµĞ¹, Ñ‡Ñ‚Ğ¾ ÑƒĞ¿Ñ€Ğ¾Ñ‰Ğ°ĞµÑ‚ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ¾Ğ±Ğ»ĞµĞ³Ñ‡Ğ°ĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ CaM Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ….'}, 'en': {'title': 'Revolutionizing Robotic Failure Detection with Code-as-Monitor', 'desc': 'This paper presents Code-as-Monitor (CaM), a new method for detecting and preventing failures in robotic systems that operate in unpredictable environments. CaM uses a vision-language model (VLM) to handle both reactive and proactive failure detection by treating these tasks as spatio-temporal constraint satisfaction problems. The method improves monitoring accuracy and efficiency by introducing compact geometric elements that represent constraints, making it easier to track and manage them visually. Experimental results demonstrate that CaM significantly outperforms existing methods, achieving higher success rates and faster execution times in both simulated and real-world scenarios.'}, 'zh': {'title': 'æ™ºèƒ½ç›‘æ§ï¼šæå‡æœºå™¨äººç³»ç»Ÿçš„æ•…éšœæ£€æµ‹ä¸é¢„é˜²', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºCode-as-Monitorï¼ˆCaMï¼‰çš„æ–°æ–¹æ³•ï¼Œç”¨äºè‡ªåŠ¨æ£€æµ‹å’Œé¢„é˜²é—­ç¯æœºå™¨äººç³»ç»Ÿä¸­çš„å¼€æ”¾é›†æ•…éšœã€‚è¯¥æ–¹æ³•åˆ©ç”¨è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å°†ååº”æ€§å’Œä¸»åŠ¨æ€§æ•…éšœæ£€æµ‹ä»»åŠ¡ç»Ÿä¸€ä¸ºæ—¶ç©ºçº¦æŸæ»¡è¶³é—®é¢˜ã€‚é€šè¿‡ç”Ÿæˆçš„ä»£ç è¿›è¡Œå®æ—¶ç›‘æ§ï¼ŒCaMåœ¨å‡†ç¡®æ€§å’Œæ•ˆç‡ä¸Šéƒ½æœ‰æ˜¾è‘—æå‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCaMåœ¨ä¸¥é‡å¹²æ‰°ä¸‹çš„æˆåŠŸç‡æé«˜äº†28.7%ï¼Œæ‰§è¡Œæ—¶é—´å‡å°‘äº†31.8%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.04454', 'title': 'Aguvis: Unified Pure Vision Agents for Autonomous GUI Interaction', 'url': 'https://huggingface.co/papers/2412.04454', 'abstract': 'Graphical User Interfaces (GUIs) are critical to human-computer interaction, yet automating GUI tasks remains challenging due to the complexity and variability of visual environments. Existing approaches often rely on textual representations of GUIs, which introduce limitations in generalization, efficiency, and scalability. In this paper, we introduce Aguvis, a unified pure vision-based framework for autonomous GUI agents that operates across various platforms. Our approach leverages image-based observations, and grounding instructions in natural language to visual elements, and employs a consistent action space to ensure cross-platform generalization. To address the limitations of previous work, we integrate explicit planning and reasoning within the model, enhancing its ability to autonomously navigate and interact with complex digital environments. We construct a large-scale dataset of GUI agent trajectories, incorporating multimodal reasoning and grounding, and employ a two-stage training pipeline that first focuses on general GUI grounding, followed by planning and reasoning. Through comprehensive experiments, we demonstrate that Aguvis surpasses previous state-of-the-art methods in both offline and real-world online scenarios, achieving, to our knowledge, the first fully autonomous pure vision GUI agent capable of performing tasks independently without collaboration with external closed-source models. We open-sourced all datasets, models, and training recipes to facilitate future research at https://aguvis-project.github.io/.', 'score': 32, 'issue_id': 980, 'pub_date': '2024-12-05', 'pub_date_card': {'ru': '5 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 5', 'zh': '12æœˆ5æ—¥'}, 'hash': 'a088657cce2c618c', 'authors': ['Yiheng Xu', 'Zekun Wang', 'Junli Wang', 'Dunjie Lu', 'Tianbao Xie', 'Amrita Saha', 'Doyen Sahoo', 'Tao Yu', 'Caiming Xiong'], 'affiliations': ['Salesforce Research', 'University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2412.04454.jpg', 'data': {'categories': ['#open_source', '#games', '#multimodal', '#agents', '#training', '#reasoning', '#dataset'], 'emoji': 'ğŸ–¥ï¸', 'ru': {'title': 'Aguvis: ĞĞ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ğ¹ Ğ“Ğ˜ĞŸ-Ğ°Ğ³ĞµĞ½Ñ‚ Ğ½Ğ° Ñ‡Ğ¸ÑÑ‚Ğ¾Ğ¼ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ¼ Ğ·Ñ€ĞµĞ½Ğ¸Ğ¸', 'desc': 'ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Aguvis - ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ° Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ (Ğ“Ğ˜ĞŸ), Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‰ÑƒÑ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºÑƒ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ Ğº Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºÑ€Ğ¾ÑÑ-Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼ĞµĞ½Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ. Aguvis Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑĞ²Ğ½Ğ¾Ğµ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ†Ğ¸Ñ„Ñ€Ğ¾Ğ²Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Aguvis Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² Ğ¾Ñ„Ğ»Ğ°Ğ¹Ğ½ Ğ¸ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Aguvis: Revolutionizing Autonomous GUI Interaction with Pure Vision', 'desc': 'This paper presents Aguvis, a novel framework designed for autonomous GUI agents that operates purely on visual inputs. Unlike previous methods that depend on textual representations, Aguvis utilizes image-based observations and natural language instructions to interact with GUI elements. The framework incorporates explicit planning and reasoning, allowing it to effectively navigate and perform tasks in complex digital environments. Through extensive testing, Aguvis demonstrates superior performance compared to existing methods, marking a significant advancement in the development of fully autonomous GUI agents.'}, 'zh': {'title': 'Aguvisï¼šå®Œå…¨è‡ªä¸»çš„è§†è§‰GUIä»£ç†', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºAguvisçš„æ¡†æ¶ï¼Œæ—¨åœ¨è‡ªåŠ¨åŒ–å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»»åŠ¡ã€‚è¯¥æ¡†æ¶åŸºäºçº¯è§†è§‰çš„æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨ä¸åŒå¹³å°ä¸Šæ“ä½œï¼Œå…‹æœäº†ä¼ ç»Ÿæ–¹æ³•çš„å±€é™æ€§ã€‚Aguvisé€šè¿‡å›¾åƒè§‚å¯Ÿå’Œè‡ªç„¶è¯­è¨€æŒ‡ä»¤çš„ç»“åˆï¼Œå¢å¼ºäº†æ¨¡å‹çš„è§„åˆ’å’Œæ¨ç†èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿç‹¬ç«‹å¯¼èˆªå’Œä¸å¤æ‚æ•°å­—ç¯å¢ƒäº’åŠ¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAguvisåœ¨ç¦»çº¿å’Œåœ¨çº¿åœºæ™¯ä¸­å‡è¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œæˆä¸ºé¦–ä¸ªå®Œå…¨è‡ªä¸»çš„çº¯è§†è§‰GUIä»£ç†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.03679', 'title': 'Evaluating Language Models as Synthetic Data Generators', 'url': 'https://huggingface.co/papers/2412.03679', 'abstract': "Given the increasing use of synthetic data in language model (LM) post-training, an LM's ability to generate high-quality data has become nearly as crucial as its ability to solve problems directly. While prior works have focused on developing effective data generation methods, they lack systematic comparison of different LMs as data generators in a unified setting. To address this gap, we propose AgoraBench, a benchmark that provides standardized settings and metrics to evaluate LMs' data generation abilities. Through synthesizing 1.26 million training instances using 6 LMs and training 99 student models, we uncover key insights about LMs' data generation capabilities. First, we observe that LMs exhibit distinct strengths. For instance, GPT-4o excels at generating new problems, while Claude-3.5-Sonnet performs better at enhancing existing ones. Furthermore, our analysis reveals that an LM's data generation ability doesn't necessarily correlate with its problem-solving ability. Instead, multiple intrinsic features of data quality-including response quality, perplexity, and instruction difficulty-collectively serve as better indicators. Finally, we demonstrate that strategic choices in output format and cost-conscious model selection significantly impact data generation effectiveness.", 'score': 30, 'issue_id': 980, 'pub_date': '2024-12-04', 'pub_date_card': {'ru': '4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 4', 'zh': '12æœˆ4æ—¥'}, 'hash': '2b80634d3f712590', 'authors': ['Seungone Kim', 'Juyoung Suk', 'Xiang Yue', 'Vijay Viswanathan', 'Seongyun Lee', 'Yizhong Wang', 'Kiril Gashteovski', 'Carolin Lawrence', 'Sean Welleck', 'Graham Neubig'], 'affiliations': ['Carnegie Mellon University', 'KAIST AI', 'NEC Laboratories Europe', 'Ss. Cyril and Methodius University of Skopje', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2412.03679.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#synthetic', '#data'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'AgoraBench: ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ AgoraBench - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚, ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ² 1,26 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ 6 Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¯Ğœ Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ¸Ğ² 99 ÑÑ‚ÑƒĞ´ĞµĞ½Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ¯Ğœ Ğ¸Ğ¼ĞµÑÑ‚ ÑĞ²Ğ¾Ğ¸ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñ‹ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¸ ÑÑ‚Ğ° ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğµ Ğ²ÑĞµĞ³Ğ´Ğ° ĞºĞ¾Ñ€Ñ€ĞµĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ñ Ğ½Ğ°Ğ²Ñ‹ĞºĞ°Ğ¼Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ñ‹, Ğ²Ğ»Ğ¸ÑÑÑ‰Ğ¸Ğµ Ğ½Ğ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Unlocking the Power of Language Models in Data Generation', 'desc': "This paper introduces AgoraBench, a benchmark designed to evaluate the data generation capabilities of various language models (LMs). It highlights the importance of synthetic data in enhancing the performance of LMs, especially in post-training scenarios. The study synthesizes a large dataset using six different LMs and trains 99 student models to analyze their data generation strengths and weaknesses. Key findings indicate that while some LMs excel in creating new problems, others are better at refining existing ones, and that data generation quality is influenced by several intrinsic features rather than just the LM's problem-solving skills."}, 'zh': {'title': 'è¯„ä¼°è¯­è¨€æ¨¡å‹æ•°æ®ç”Ÿæˆèƒ½åŠ›çš„æ–°åŸºå‡†', 'desc': 'éšç€åˆæˆæ•°æ®åœ¨è¯­è¨€æ¨¡å‹åæœŸè®­ç»ƒä¸­çš„ä½¿ç”¨å¢åŠ ï¼Œè¯­è¨€æ¨¡å‹ç”Ÿæˆé«˜è´¨é‡æ•°æ®çš„èƒ½åŠ›å˜å¾—ä¸ç›´æ¥è§£å†³é—®é¢˜çš„èƒ½åŠ›åŒæ ·é‡è¦ã€‚ä»¥å¾€çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨å¼€å‘æœ‰æ•ˆçš„æ•°æ®ç”Ÿæˆæ–¹æ³•ï¼Œä½†ç¼ºä¹å¯¹ä¸åŒè¯­è¨€æ¨¡å‹ä½œä¸ºæ•°æ®ç”Ÿæˆå™¨çš„ç³»ç»Ÿæ¯”è¾ƒã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†AgoraBenchï¼Œä¸€ä¸ªæä¾›æ ‡å‡†åŒ–è®¾ç½®å’ŒæŒ‡æ ‡çš„åŸºå‡†ï¼Œç”¨äºè¯„ä¼°è¯­è¨€æ¨¡å‹çš„æ•°æ®ç”Ÿæˆèƒ½åŠ›ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œä¸åŒè¯­è¨€æ¨¡å‹åœ¨æ•°æ®ç”Ÿæˆæ–¹é¢å…·æœ‰ç‹¬ç‰¹çš„ä¼˜åŠ¿ï¼Œå¹¶ä¸”æ•°æ®ç”Ÿæˆèƒ½åŠ›ä¸è§£å†³é—®é¢˜çš„èƒ½åŠ›å¹¶ä¸æ€»æ˜¯ç›¸å…³ï¼Œè€Œæ˜¯ä¸æ•°æ®è´¨é‡çš„å¤šä¸ªå†…åœ¨ç‰¹å¾æœ‰å…³ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.03895', 'title': 'A Noise is Worth Diffusion Guidance', 'url': 'https://huggingface.co/papers/2412.03895', 'abstract': "Diffusion models excel in generating high-quality images. However, current diffusion models struggle to produce reliable images without guidance methods, such as classifier-free guidance (CFG). Are guidance methods truly necessary? Observing that noise obtained via diffusion inversion can reconstruct high-quality images without guidance, we focus on the initial noise of the denoising pipeline. By mapping Gaussian noise to `guidance-free noise', we uncover that small low-magnitude low-frequency components significantly enhance the denoising process, removing the need for guidance and thus improving both inference throughput and memory. Expanding on this, we propose \\ours, a novel method that replaces guidance methods with a single refinement of the initial noise. This refined noise enables high-quality image generation without guidance, within the same diffusion pipeline. Our noise-refining model leverages efficient noise-space learning, achieving rapid convergence and strong performance with just 50K text-image pairs. We validate its effectiveness across diverse metrics and analyze how refined noise can eliminate the need for guidance. See our project page: https://cvlab-kaist.github.io/NoiseRefine/.", 'score': 25, 'issue_id': 985, 'pub_date': '2024-12-05', 'pub_date_card': {'ru': '5 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 5', 'zh': '12æœˆ5æ—¥'}, 'hash': '1e3bc318f7457d8c', 'authors': ['Donghoon Ahn', 'Jiwon Kang', 'Sanghyun Lee', 'Jaewon Min', 'Minjae Kim', 'Wooseok Jang', 'Hyoungwon Cho', 'Sayak Paul', 'SeonHwa Kim', 'Eunju Cha', 'Kyong Hwan Jin', 'Seungryong Kim'], 'affiliations': ['Hugging Face', 'KAIST', 'Korea University', 'Sookmyung Womens University'], 'pdf_title_img': 'assets/pdf/title_img/2412.03895.jpg', 'data': {'categories': ['#cv', '#optimization', '#diffusion', '#inference'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ½Ğ°Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµÑ…Ğ½Ğ¸Ğº Ğ½Ğ°Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ NoiseRefine, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ·Ğ°Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ°Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ¾Ğ´Ğ½Ğ¾ĞºÑ€Ğ°Ñ‚Ğ½Ñ‹Ğ¼ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑˆÑƒĞ¼Ğ°. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°Ñ‚ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ğ½Ğ°Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ‚Ğ¾Ñ‚ Ğ¶Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ ÑˆÑƒĞ¼Ğ° ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ ÑˆÑƒĞ¼Ğ¾Ğ², Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ¹ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ²ÑĞµĞ³Ğ¾ Ğ½Ğ° 50 Ñ‚Ñ‹ÑÑÑ‡Ğ°Ñ… Ğ¿Ğ°Ñ€ Ñ‚ĞµĞºÑÑ‚-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ.'}, 'en': {'title': 'Guidance-Free Image Generation with Noise Refinement', 'desc': 'This paper investigates the necessity of guidance methods in diffusion models for image generation. It reveals that high-quality images can be produced by refining the initial noise in the denoising process, eliminating the reliance on techniques like classifier-free guidance. The authors introduce a new method called \textit{NoiseRefine}, which enhances the denoising process by focusing on low-magnitude low-frequency components of noise. Their approach demonstrates improved efficiency in image generation, achieving strong results with fewer training samples and without the need for additional guidance methods.'}, 'zh': {'title': 'æ— æŒ‡å¯¼ç”Ÿæˆé«˜è´¨é‡å›¾åƒçš„æ–°æ–¹æ³•', 'desc': 'æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆé«˜è´¨é‡å›¾åƒæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†ç›®å‰çš„æ‰©æ•£æ¨¡å‹åœ¨æ²¡æœ‰æŒ‡å¯¼æ–¹æ³•çš„æƒ…å†µä¸‹éš¾ä»¥ç”Ÿæˆå¯é çš„å›¾åƒã€‚æˆ‘ä»¬å‘ç°ï¼Œé€šè¿‡æ‰©æ•£åæ¼”è·å¾—çš„å™ªå£°å¯ä»¥åœ¨æ²¡æœ‰æŒ‡å¯¼çš„æƒ…å†µä¸‹é‡å»ºé«˜è´¨é‡å›¾åƒï¼Œå› æ­¤æˆ‘ä»¬å…³æ³¨å»å™ªæµç¨‹ä¸­çš„åˆå§‹å™ªå£°ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•\textit{ours}ï¼Œé€šè¿‡å¯¹åˆå§‹å™ªå£°è¿›è¡Œå•æ¬¡ç²¾ç‚¼ï¼Œæ›¿ä»£äº†ä¼ ç»Ÿçš„æŒ‡å¯¼æ–¹æ³•ï¼Œä»è€Œåœ¨åŒä¸€æ‰©æ•£æµç¨‹ä¸­å®ç°é«˜è´¨é‡å›¾åƒç”Ÿæˆã€‚æˆ‘ä»¬çš„æ¨¡å‹åˆ©ç”¨é«˜æ•ˆçš„å™ªå£°ç©ºé—´å­¦ä¹ ï¼Œå¿«é€Ÿæ”¶æ•›å¹¶åœ¨ä»…ä½¿ç”¨50Kæ–‡æœ¬-å›¾åƒå¯¹çš„æƒ…å†µä¸‹å–å¾—äº†è‰¯å¥½çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.01506', 'title': 'Structured 3D Latents for Scalable and Versatile 3D Generation', 'url': 'https://huggingface.co/papers/2412.01506', 'abstract': 'We introduce a novel 3D generation method for versatile and high-quality 3D asset creation. The cornerstone is a unified Structured LATent (SLAT) representation which allows decoding to different output formats, such as Radiance Fields, 3D Gaussians, and meshes. This is achieved by integrating a sparsely-populated 3D grid with dense multiview visual features extracted from a powerful vision foundation model, comprehensively capturing both structural (geometry) and textural (appearance) information while maintaining flexibility during decoding. We employ rectified flow transformers tailored for SLAT as our 3D generation models and train models with up to 2 billion parameters on a large 3D asset dataset of 500K diverse objects. Our model generates high-quality results with text or image conditions, significantly surpassing existing methods, including recent ones at similar scales. We showcase flexible output format selection and local 3D editing capabilities which were not offered by previous models. Code, model, and data will be released.', 'score': 22, 'issue_id': 980, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 2', 'zh': '12æœˆ2æ—¥'}, 'hash': 'c35e5586d464b27c', 'authors': ['Jianfeng Xiang', 'Zelong Lv', 'Sicheng Xu', 'Yu Deng', 'Ruicheng Wang', 'Bowen Zhang', 'Dong Chen', 'Xin Tong', 'Jiaolong Yang'], 'affiliations': ['Microsoft Research', 'Tsinghua University', 'USTC'], 'pdf_title_img': 'assets/pdf/title_img/2412.01506.jpg', 'data': {'categories': ['#3d', '#dataset', '#training', '#open_source'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. ĞÑĞ½Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° ÑĞ²Ğ»ÑĞµÑ‚ÑÑ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ (SLAT), Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰ĞµĞµ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ñ‹. SLAT Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½ÑƒÑ 3D-ÑĞµÑ‚ĞºÑƒ Ñ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ°Ğ¼Ğ¸, Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ· Ğ¼Ğ¾Ñ‰Ğ½Ğ¾Ğ¹ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ. Ğ”Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ÑÑ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ñ‹ Ñ Ğ²Ñ‹Ğ¿Ñ€ÑĞ¼Ğ»ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ¼, Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ SLAT, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· 500 Ñ‚Ñ‹ÑÑÑ‡ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ².'}, 'en': {'title': 'Revolutionizing 3D Asset Creation with SLAT!', 'desc': 'This paper presents a new method for creating high-quality 3D assets using a unified Structured LATent (SLAT) representation. The SLAT allows for flexible decoding into various formats like Radiance Fields, 3D Gaussians, and meshes by combining a sparse 3D grid with detailed visual features from a vision model. The authors utilize rectified flow transformers designed for SLAT, training models with up to 2 billion parameters on a large dataset of 500,000 diverse 3D objects. The results show significant improvements in quality and versatility, enabling local 3D editing and output format selection that previous models could not achieve.'}, 'zh': {'title': 'çµæ´»é«˜æ•ˆçš„3Dèµ„äº§ç”Ÿæˆæ–°æ–¹æ³•', 'desc': 'æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„3Dç”Ÿæˆæ–¹æ³•ï¼Œç”¨äºå¤šåŠŸèƒ½å’Œé«˜è´¨é‡çš„3Dèµ„äº§åˆ›å»ºã€‚è¯¥æ–¹æ³•çš„æ ¸å¿ƒæ˜¯ç»Ÿä¸€çš„ç»“æ„åŒ–æ½œåœ¨(SLAT)è¡¨ç¤ºï¼Œèƒ½å¤Ÿè§£ç ä¸ºä¸åŒçš„è¾“å‡ºæ ¼å¼ï¼Œå¦‚è¾å°„åœºã€3Dé«˜æ–¯å’Œç½‘æ ¼ã€‚é€šè¿‡å°†ç¨€ç–çš„3Dç½‘æ ¼ä¸ä»å¼ºå¤§çš„è§†è§‰åŸºç¡€æ¨¡å‹ä¸­æå–çš„å¯†é›†å¤šè§†è§’è§†è§‰ç‰¹å¾ç›¸ç»“åˆï¼Œæˆ‘ä»¬å…¨é¢æ•æ‰äº†ç»“æ„ï¼ˆå‡ ä½•ï¼‰å’Œçº¹ç†ï¼ˆå¤–è§‚ï¼‰ä¿¡æ¯ï¼ŒåŒæ—¶åœ¨è§£ç è¿‡ç¨‹ä¸­ä¿æŒçµæ´»æ€§ã€‚æˆ‘ä»¬çš„æ¨¡å‹ä½¿ç”¨é’ˆå¯¹SLATçš„æ•´æµæµå˜æ¢å™¨è¿›è¡Œ3Dç”Ÿæˆï¼Œè®­ç»ƒäº†é«˜è¾¾20äº¿å‚æ•°çš„æ¨¡å‹ï¼Œç”Ÿæˆçš„ç»“æœåœ¨æ–‡æœ¬æˆ–å›¾åƒæ¡ä»¶ä¸‹çš„è´¨é‡æ˜¾è‘—è¶…è¿‡ç°æœ‰æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.01339', 'title': 'Negative Token Merging: Image-based Adversarial Feature Guidance', 'url': 'https://huggingface.co/papers/2412.01339', 'abstract': 'Text-based adversarial guidance using a negative prompt has emerged as a widely adopted approach to push the output features away from undesired concepts. While useful, performing adversarial guidance using text alone can be insufficient to capture complex visual concepts and avoid undesired visual elements like copyrighted characters. In this paper, for the first time we explore an alternate modality in this direction by performing adversarial guidance directly using visual features from a reference image or other images in a batch. In particular, we introduce negative token merging (NegToMe), a simple but effective training-free approach which performs adversarial guidance by selectively pushing apart matching semantic features (between reference and output generation) during the reverse diffusion process. When used w.r.t. other images in the same batch, we observe that NegToMe significantly increases output diversity (racial, gender, visual) without sacrificing output image quality. Similarly, when used w.r.t. a reference copyrighted asset, NegToMe helps reduce visual similarity with copyrighted content by 34.57%. NegToMe is simple to implement using just few-lines of code, uses only marginally higher (<4%) inference times and generalizes to different diffusion architectures like Flux, which do not natively support the use of a separate negative prompt. Code is available at https://negtome.github.io', 'score': 21, 'issue_id': 982, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 2', 'zh': '12æœˆ2æ—¥'}, 'hash': '2cfa8e0ec05b9ae1', 'authors': ['Jaskirat Singh', 'Lindsey Li', 'Weijia Shi', 'Ranjay Krishna', 'Yejin Choi', 'Pang Wei Koh', 'Michael F. Cohen', 'Stephen Gould', 'Liang Zheng', 'Luke Zettlemoyer'], 'affiliations': ['Allen Institute for AI', 'Australian National University', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2412.01339.jpg', 'data': {'categories': ['#training', '#cv', '#ethics', '#multimodal', '#security', '#diffusion'], 'emoji': 'ğŸ¨', 'ru': {'title': 'NegToMe: Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ´Ğ»Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ”Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ NegToMe (negative token merging) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ½ĞµĞ³Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ², NegToMe Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ñ€ĞµÑ„ĞµÑ€ĞµĞ½ÑĞ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ¸Ñ‚ÑŒ Ğ¸Ñ… ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ñ Ğ·Ğ°Ñ‰Ğ¸Ñ‰ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€ÑĞºĞ¸Ğ¼ Ğ¿Ñ€Ğ°Ğ²Ğ¾Ğ¼ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ°Ğ¼Ğ¸. NegToMe Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ»ĞµĞ³ĞºĞ¾ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ğ¼Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ².'}, 'en': {'title': 'Enhancing Image Diversity and Copyright Safety with NegToMe', 'desc': 'This paper introduces a new method called Negative Token Merging (NegToMe) for adversarial guidance in image generation. Unlike traditional methods that rely solely on text prompts, NegToMe uses visual features from reference images to steer the output away from unwanted concepts. The approach selectively separates matching semantic features during the reverse diffusion process, enhancing the diversity of generated images while maintaining quality. Additionally, it effectively reduces visual similarity to copyrighted content, making it a practical tool for creators.'}, 'zh': {'title': 'é€šè¿‡è§†è§‰ç‰¹å¾å®ç°å¯¹æŠ—å¼•å¯¼çš„åˆ›æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¯¹æŠ—å¼•å¯¼æ–¹æ³•ï¼Œç§°ä¸ºè´Ÿä»¤ç‰Œåˆå¹¶ï¼ˆNegToMeï¼‰ï¼Œé€šè¿‡ç›´æ¥ä½¿ç”¨å‚è€ƒå›¾åƒçš„è§†è§‰ç‰¹å¾æ¥æ¨åŠ¨è¾“å‡ºç‰¹å¾è¿œç¦»ä¸å¸Œæœ›çš„æ¦‚å¿µã€‚ä¸ä»…ä½¿ç”¨æ–‡æœ¬è¿›è¡Œå¯¹æŠ—å¼•å¯¼ç›¸æ¯”ï¼Œè¿™ç§æ–¹æ³•èƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰å¤æ‚çš„è§†è§‰æ¦‚å¿µï¼Œå¹¶æœ‰æ•ˆé¿å…ç‰ˆæƒè§’è‰²ç­‰ä¸å¸Œæœ›çš„è§†è§‰å…ƒç´ ã€‚å®éªŒè¡¨æ˜ï¼ŒNegToMeæ˜¾è‘—æé«˜äº†è¾“å‡ºçš„å¤šæ ·æ€§ï¼ŒåŒæ—¶ä¿æŒäº†å›¾åƒè´¨é‡ï¼Œå¹¶ä¸”åœ¨å‡å°‘ä¸ç‰ˆæƒå†…å®¹çš„è§†è§‰ç›¸ä¼¼æ€§æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚è¯¥æ–¹æ³•å®ç°ç®€å•ï¼Œä»£ç é‡å°‘ï¼Œä¸”å¯¹ä¸åŒçš„æ‰©æ•£æ¶æ„å…·æœ‰è‰¯å¥½çš„é€šç”¨æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.04146', 'title': 'AnyDressing: Customizable Multi-Garment Virtual Dressing via Latent Diffusion Models', 'url': 'https://huggingface.co/papers/2412.04146', 'abstract': 'Recent advances in garment-centric image generation from text and image prompts based on diffusion models are impressive. However, existing methods lack support for various combinations of attire, and struggle to preserve the garment details while maintaining faithfulness to the text prompts, limiting their performance across diverse scenarios. In this paper, we focus on a new task, i.e., Multi-Garment Virtual Dressing, and we propose a novel AnyDressing method for customizing characters conditioned on any combination of garments and any personalized text prompts. AnyDressing comprises two primary networks named GarmentsNet and DressingNet, which are respectively dedicated to extracting detailed clothing features and generating customized images. Specifically, we propose an efficient and scalable module called Garment-Specific Feature Extractor in GarmentsNet to individually encode garment textures in parallel. This design prevents garment confusion while ensuring network efficiency. Meanwhile, we design an adaptive Dressing-Attention mechanism and a novel Instance-Level Garment Localization Learning strategy in DressingNet to accurately inject multi-garment features into their corresponding regions. This approach efficiently integrates multi-garment texture cues into generated images and further enhances text-image consistency. Additionally, we introduce a Garment-Enhanced Texture Learning strategy to improve the fine-grained texture details of garments. Thanks to our well-craft design, AnyDressing can serve as a plug-in module to easily integrate with any community control extensions for diffusion models, improving the diversity and controllability of synthesized images. Extensive experiments show that AnyDressing achieves state-of-the-art results.', 'score': 16, 'issue_id': 989, 'pub_date': '2024-12-05', 'pub_date_card': {'ru': '5 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 5', 'zh': '12æœˆ5æ—¥'}, 'hash': 'ed058640f1a96005', 'authors': ['Xinghui Li', 'Qichao Sun', 'Pengze Zhang', 'Fulong Ye', 'Zhichao Liao', 'Wanquan Feng', 'Songtao Zhao', 'Qian He'], 'affiliations': ['ByteDance', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2412.04146.jpg', 'data': {'categories': ['#diffusion', '#games', '#cv', '#multimodal'], 'emoji': 'ğŸ‘š', 'ru': {'title': 'AnyDressing: Ğ’Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€ĞºĞ° Ğ»ÑĞ±Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ´ĞµĞ¶Ğ´Ñ‹ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ AnyDressing Ğ´Ğ»Ñ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€ĞºĞ¸ Ğ¾Ğ´ĞµĞ¶Ğ´Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ğ´Ğ²ÑƒÑ… Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹: GarmentsNet Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹ Ğ¾Ğ´ĞµĞ¶Ğ´Ñ‹ Ğ¸ DressingNet Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ’ GarmentsNet Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Garment-Specific Feature Extractor Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€ Ğ¾Ğ´ĞµĞ¶Ğ´Ñ‹. DressingNet Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Dressing-Attention Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Instance-Level Garment Localization Learning Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¾Ğ´ĞµĞ¶Ğ´Ñ‹.'}, 'en': {'title': 'AnyDressing: Revolutionizing Multi-Garment Image Generation', 'desc': 'This paper introduces AnyDressing, a novel method for Multi-Garment Virtual Dressing that enhances image generation from text and image prompts. It features two main networks: GarmentsNet, which extracts detailed clothing features, and DressingNet, which generates customized images while maintaining text-image consistency. The method includes a Garment-Specific Feature Extractor to efficiently encode garment textures and an adaptive Dressing-Attention mechanism to accurately place multi-garment features. Overall, AnyDressing improves the diversity and controllability of synthesized images, achieving state-of-the-art performance in garment-centric image generation.'}, 'zh': {'title': 'AnyDressingï¼šå¤šæœè£…è™šæ‹Ÿç©¿è¡£çš„æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¤šæœè£…è™šæ‹Ÿç©¿è¡£ä»»åŠ¡ï¼Œå¹¶æå‡ºäº†AnyDressingæ–¹æ³•ï¼Œä»¥æ”¯æŒä»»æ„ç»„åˆçš„æœè£…å’Œä¸ªæ€§åŒ–æ–‡æœ¬æç¤ºã€‚AnyDressingåŒ…å«ä¸¤ä¸ªä¸»è¦ç½‘ç»œï¼šGarmentsNetç”¨äºæå–æœè£…ç»†èŠ‚ç‰¹å¾ï¼ŒDressingNetç”¨äºç”Ÿæˆå®šåˆ¶å›¾åƒã€‚æˆ‘ä»¬è®¾è®¡äº†é«˜æ•ˆçš„æœè£…ç‰¹å¾æå–æ¨¡å—ï¼Œé¿å…äº†æœè£…æ··æ·†ï¼ŒåŒæ—¶æé«˜äº†ç½‘ç»œæ•ˆç‡ã€‚æ­¤å¤–ï¼ŒDressingNetä¸­çš„è‡ªé€‚åº”æ³¨æ„æœºåˆ¶å’Œå®ä¾‹çº§æœè£…å®šä½å­¦ä¹ ç­–ç•¥ï¼Œç¡®ä¿äº†å¤šæœè£…ç‰¹å¾çš„å‡†ç¡®æ³¨å…¥ï¼Œä»è€Œæå‡äº†ç”Ÿæˆå›¾åƒçš„å¤šæ ·æ€§å’Œä¸€è‡´æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.03632', 'title': 'MV-Adapter: Multi-view Consistent Image Generation Made Easy', 'url': 'https://huggingface.co/papers/2412.03632', 'abstract': 'Existing multi-view image generation methods often make invasive modifications to pre-trained text-to-image (T2I) models and require full fine-tuning, leading to (1) high computational costs, especially with large base models and high-resolution images, and (2) degradation in image quality due to optimization difficulties and scarce high-quality 3D data. In this paper, we propose the first adapter-based solution for multi-view image generation, and introduce MV-Adapter, a versatile plug-and-play adapter that enhances T2I models and their derivatives without altering the original network structure or feature space. By updating fewer parameters, MV-Adapter enables efficient training and preserves the prior knowledge embedded in pre-trained models, mitigating overfitting risks. To efficiently model the 3D geometric knowledge within the adapter, we introduce innovative designs that include duplicated self-attention layers and parallel attention architecture, enabling the adapter to inherit the powerful priors of the pre-trained models to model the novel 3D knowledge. Moreover, we present a unified condition encoder that seamlessly integrates camera parameters and geometric information, facilitating applications such as text- and image-based 3D generation and texturing. MV-Adapter achieves multi-view generation at 768 resolution on Stable Diffusion XL (SDXL), and demonstrates adaptability and versatility. It can also be extended to arbitrary view generation, enabling broader applications. We demonstrate that MV-Adapter sets a new quality standard for multi-view image generation, and opens up new possibilities due to its efficiency, adaptability and versatility.', 'score': 15, 'issue_id': 981, 'pub_date': '2024-12-04', 'pub_date_card': {'ru': '4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 4', 'zh': '12æœˆ4æ—¥'}, 'hash': 'fe0891c026484abc', 'authors': ['Zehuan Huang', 'Yuan-Chen Guo', 'Haoran Wang', 'Ran Yi', 'Lizhuang Ma', 'Yan-Pei Cao', 'Lu Sheng'], 'affiliations': ['School of Software, Beihang University', 'Shanghai Jiao Tong University', 'VAST'], 'pdf_title_img': 'assets/pdf/title_img/2412.03632.jpg', 'data': {'categories': ['#training', '#synthetic', '#optimization', '#architecture', '#cv', '#3d'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'MV-Adapter: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MV-Adapter - Ğ¿ĞµÑ€Ğ²Ğ¾Ğµ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ğ½Ğ¾Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ»Ğ°Ğ³Ğ¸Ğ½ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‚ĞµĞºÑÑ‚-Ğ²-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ±ĞµĞ· Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¸Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞ½Ğ¸Ğ¶Ğ°Ñ Ñ€Ğ¸ÑĞº Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. MV-Adapter Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ´ÑƒĞ±Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞ»Ğ¾Ğ¸ ÑĞ°Ğ¼Ğ¾Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ 3D-Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸. Ğ ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸ĞµĞ¼ 768 Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Stable Diffusion XL.'}, 'en': {'title': 'Efficient Multi-View Image Generation with MV-Adapter', 'desc': 'This paper introduces MV-Adapter, a novel adapter-based approach for multi-view image generation that avoids invasive changes to pre-trained text-to-image (T2I) models. By updating fewer parameters, MV-Adapter enhances the efficiency of training while preserving the knowledge from pre-trained models, thus reducing the risk of overfitting. The design incorporates advanced features like duplicated self-attention layers and a unified condition encoder to effectively integrate 3D geometric information. MV-Adapter not only achieves high-quality multi-view generation but also allows for broader applications in arbitrary view generation.'}, 'zh': {'title': 'é«˜æ•ˆå¤šè§†è§’å›¾åƒç”Ÿæˆçš„æ–°æ ‡å‡†', 'desc': 'ç°æœ‰çš„å¤šè§†è§’å›¾åƒç”Ÿæˆæ–¹æ³•é€šå¸¸éœ€è¦å¯¹é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹è¿›è¡Œå¤§å¹…ä¿®æ”¹ï¼Œå¹¶ä¸”éœ€è¦å®Œå…¨å¾®è°ƒï¼Œè¿™å¯¼è‡´äº†é«˜è®¡ç®—æˆæœ¬å’Œå›¾åƒè´¨é‡ä¸‹é™ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºé€‚é…å™¨çš„å¤šè§†è§’å›¾åƒç”Ÿæˆè§£å†³æ–¹æ¡ˆï¼ŒMV-Adapteræ˜¯ä¸€ç§å¯æ’æ‹”çš„é€‚é…å™¨ï¼Œå¯ä»¥å¢å¼ºæ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ï¼Œè€Œæ— éœ€æ”¹å˜åŸå§‹ç½‘ç»œç»“æ„ã€‚é€šè¿‡æ›´æ–°æ›´å°‘çš„å‚æ•°ï¼ŒMV-Adapterå®ç°äº†é«˜æ•ˆè®­ç»ƒï¼Œå¹¶ä¿ç•™äº†é¢„è®­ç»ƒæ¨¡å‹ä¸­çš„å…ˆéªŒçŸ¥è¯†ï¼Œé™ä½äº†è¿‡æ‹Ÿåˆé£é™©ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ç»Ÿä¸€çš„æ¡ä»¶ç¼–ç å™¨ï¼Œèƒ½å¤Ÿæ— ç¼æ•´åˆç›¸æœºå‚æ•°å’Œå‡ ä½•ä¿¡æ¯ï¼Œä¿ƒè¿›æ–‡æœ¬å’Œå›¾åƒåŸºç¡€çš„3Dç”Ÿæˆå’Œçº¹ç†å¤„ç†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.04431', 'title': 'Infinity: Scaling Bitwise AutoRegressive Modeling for High-Resolution Image Synthesis', 'url': 'https://huggingface.co/papers/2412.04431', 'abstract': 'We present Infinity, a Bitwise Visual AutoRegressive Modeling capable of generating high-resolution, photorealistic images following language instruction. Infinity redefines visual autoregressive model under a bitwise token prediction framework with an infinite-vocabulary tokenizer & classifier and bitwise self-correction mechanism, remarkably improving the generation capacity and details. By theoretically scaling the tokenizer vocabulary size to infinity and concurrently scaling the transformer size, our method significantly unleashes powerful scaling capabilities compared to vanilla VAR. Infinity sets a new record for autoregressive text-to-image models, outperforming top-tier diffusion models like SD3-Medium and SDXL. Notably, Infinity surpasses SD3-Medium by improving the GenEval benchmark score from 0.62 to 0.73 and the ImageReward benchmark score from 0.87 to 0.96, achieving a win rate of 66%. Without extra optimization, Infinity generates a high-quality 1024x1024 image in 0.8 seconds, making it 2.6x faster than SD3-Medium and establishing it as the fastest text-to-image model. Models and codes will be released to promote further exploration of Infinity for visual generation and unified tokenizer modeling.', 'score': 14, 'issue_id': 980, 'pub_date': '2024-12-05', 'pub_date_card': {'ru': '5 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 5', 'zh': '12æœˆ5æ—¥'}, 'hash': 'f297f288187d4cc4', 'authors': ['Jian Han', 'Jinlai Liu', 'Yi Jiang', 'Bin Yan', 'Yuqi Zhang', 'Zehuan Yuan', 'Bingyue Peng', 'Xiaobing Liu'], 'affiliations': ['ByteDance'], 'pdf_title_img': 'assets/pdf/title_img/2412.04431.jpg', 'data': {'categories': ['#open_source', '#diffusion', '#multimodal', '#cv', '#benchmark', '#architecture'], 'emoji': 'ğŸ”„', 'ru': {'title': 'Infinity: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ±ĞµÑĞºĞ¾Ğ½ĞµÑ‡Ğ½Ñ‹Ğ¼ ÑĞ»Ğ¾Ğ²Ğ°Ñ€ĞµĞ¼', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Infinity - Ğ±Ğ¸Ñ‚Ğ¾Ğ²Ğ°Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ„Ğ¾Ñ‚Ğ¾Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ. Infinity Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ±ĞµÑĞºĞ¾Ğ½ĞµÑ‡Ğ½Ñ‹Ğ¹ ÑĞ»Ğ¾Ğ²Ğ°Ñ€ÑŒ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ±Ğ¸Ñ‚Ğ¾Ğ²Ğ¾Ğ¹ ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ Ñ€ÑĞ´Ñƒ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ GenEval Ğ¸ ImageReward. Infinity Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ 1024x1024 Ğ·Ğ° 0.8 ÑĞµĞºÑƒĞ½Ğ´Ñ‹, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ ĞµĞµ ÑĞ°Ğ¼Ğ¾Ğ¹ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ text-to-image Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚.'}, 'en': {'title': 'Infinity: Redefining Text-to-Image Generation with Infinite Vocabulary', 'desc': 'Infinity is a novel Bitwise Visual AutoRegressive Model designed to create high-resolution, photorealistic images based on language instructions. It introduces an infinite-vocabulary tokenizer and classifier, along with a bitwise self-correction mechanism, which enhances the detail and quality of generated images. By scaling both the tokenizer and transformer sizes, Infinity significantly improves upon traditional visual autoregressive models. It sets new performance records in text-to-image generation, outperforming leading diffusion models and achieving faster generation times without additional optimization.'}, 'zh': {'title': 'Infinityï¼šæ— é™å¯èƒ½çš„è§†è§‰ç”Ÿæˆæ¨¡å‹', 'desc': 'æœ¬æ–‡ä»‹ç»äº†Infinityï¼Œä¸€ç§åŸºäºä½çš„è§†è§‰è‡ªå›å½’æ¨¡å‹ï¼Œèƒ½å¤Ÿæ ¹æ®è¯­è¨€æŒ‡ä»¤ç”Ÿæˆé«˜åˆ†è¾¨ç‡ã€é€¼çœŸçš„å›¾åƒã€‚Infinityåœ¨ä½å…ƒä»¤ç‰Œé¢„æµ‹æ¡†æ¶ä¸‹é‡æ–°å®šä¹‰äº†è§†è§‰è‡ªå›å½’æ¨¡å‹ï¼Œé‡‡ç”¨æ— é™è¯æ±‡é‡çš„ä»¤ç‰Œå™¨å’Œåˆ†ç±»å™¨ï¼Œä»¥åŠä½å…ƒè‡ªæˆ‘æ ¡æ­£æœºåˆ¶ï¼Œæ˜¾è‘—æå‡äº†ç”Ÿæˆèƒ½åŠ›å’Œç»†èŠ‚è¡¨ç°ã€‚é€šè¿‡ç†è®ºä¸Šå°†ä»¤ç‰Œå™¨çš„è¯æ±‡é‡æ‰©å±•åˆ°æ— é™ï¼Œå¹¶åŒæ—¶æ‰©å±•å˜æ¢å™¨çš„è§„æ¨¡ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç›¸æ¯”ä¼ ç»Ÿçš„VARæ¨¡å‹é‡Šæ”¾äº†å¼ºå¤§çš„æ‰©å±•èƒ½åŠ›ã€‚Infinityåœ¨è‡ªå›å½’æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ä¸­åˆ›ä¸‹æ–°çºªå½•ï¼Œè¶…è¶Šäº†é¡¶çº§æ‰©æ•£æ¨¡å‹ï¼Œå¦‚SD3-Mediumå’ŒSDXLã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.04315', 'title': 'Densing Law of LLMs', 'url': 'https://huggingface.co/papers/2412.04315', 'abstract': "Large Language Models (LLMs) have emerged as a milestone in artificial intelligence, and their performance can improve as the model size increases. However, this scaling brings great challenges to training and inference efficiency, particularly for deploying LLMs in resource-constrained environments, and the scaling trend is becoming increasingly unsustainable. This paper introduces the concept of ``capacity density'' as a new metric to evaluate the quality of the LLMs across different scales and describes the trend of LLMs in terms of both effectiveness and efficiency. To calculate the capacity density of a given target LLM, we first introduce a set of reference models and develop a scaling law to predict the downstream performance of these reference models based on their parameter sizes. We then define the effective parameter size of the target LLM as the parameter size required by a reference model to achieve equivalent performance, and formalize the capacity density as the ratio of the effective parameter size to the actual parameter size of the target LLM. Capacity density provides a unified framework for assessing both model effectiveness and efficiency. Our further analysis of recent open-source base LLMs reveals an empirical law (the densing law)that the capacity density of LLMs grows exponentially over time. More specifically, using some widely used benchmarks for evaluation, the capacity density of LLMs doubles approximately every three months. The law provides new perspectives to guide future LLM development, emphasizing the importance of improving capacity density to achieve optimal results with minimal computational overhead.", 'score': 13, 'issue_id': 981, 'pub_date': '2024-12-05', 'pub_date_card': {'ru': '5 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 5', 'zh': '12æœˆ5æ—¥'}, 'hash': '4a5189762d711a19', 'authors': ['Chaojun Xiao', 'Jie Cai', 'Weilin Zhao', 'Guoyang Zeng', 'Xu Han', 'Zhiyuan Liu', 'Maosong Sun'], 'affiliations': ['ModelBest Inc.', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2412.04315.jpg', 'data': {'categories': ['#training', '#benchmark', '#open_source', '#optimization', '#architecture', '#inference'], 'emoji': 'ğŸ“ˆ', 'ru': {'title': 'ĞŸĞ»Ğ¾Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ñ‰Ğ½Ğ¾ÑÑ‚Ğ¸: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿ÑƒÑ‚ÑŒ Ğº ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼', 'desc': "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ 'Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ñ‰Ğ½Ğ¾ÑÑ‚Ğ¸' Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¿Ğ¾Ğ½ÑÑ‚Ğ¸Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ñ‰Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ°Ğº Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğº Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ. ĞĞ½Ğ°Ğ»Ğ¸Ğ· ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… LLM Ğ²Ñ‹ÑĞ²Ğ¸Ğ» ÑĞ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ·Ğ°ĞºĞ¾Ğ½ ÑĞºÑĞ¿Ğ¾Ğ½ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ¾ÑÑ‚Ğ° Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ñ‰Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½ĞµĞ¼. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ğµ LLM, Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°Ñ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ñ‰Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ…."}, 'en': {'title': 'Maximizing Performance with Minimal Resources: The Capacity Density Revolution', 'desc': "This paper discusses the challenges of training and using large language models (LLMs) as they grow in size. It introduces a new metric called 'capacity density' to evaluate LLMs based on their effectiveness and efficiency. The authors develop a scaling law to predict how well different models perform relative to their size and define capacity density as the ratio of effective parameter size to actual parameter size. Their findings suggest that the capacity density of LLMs is increasing rapidly, which could guide future improvements in model design to optimize performance while reducing resource use."}, 'zh': {'title': 'æå‡å®¹é‡å¯†åº¦ï¼Œä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹çš„æ•ˆç‡ä¸æ•ˆæœ', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨äººå·¥æ™ºèƒ½é¢†åŸŸå–å¾—äº†é‡è¦è¿›å±•ï¼Œä½†éšç€æ¨¡å‹è§„æ¨¡çš„å¢åŠ ï¼Œè®­ç»ƒå’Œæ¨ç†çš„æ•ˆç‡é¢ä¸´å·¨å¤§æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†â€œå®¹é‡å¯†åº¦â€è¿™ä¸€æ–°æŒ‡æ ‡ï¼Œç”¨äºè¯„ä¼°ä¸åŒè§„æ¨¡LLMsçš„è´¨é‡ï¼Œå¹¶æè¿°äº†LLMsåœ¨æœ‰æ•ˆæ€§å’Œæ•ˆç‡æ–¹é¢çš„è¶‹åŠ¿ã€‚é€šè¿‡å¼•å…¥ä¸€ç»„å‚è€ƒæ¨¡å‹å¹¶å¼€å‘ç¼©æ”¾æ³•åˆ™ï¼Œæœ¬æ–‡è®¡ç®—äº†ç›®æ ‡LLMçš„æœ‰æ•ˆå‚æ•°å¤§å°ï¼Œå¹¶å°†å®¹é‡å¯†åº¦å®šä¹‰ä¸ºæœ‰æ•ˆå‚æ•°å¤§å°ä¸å®é™…å‚æ•°å¤§å°çš„æ¯”ç‡ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼ŒLLMsçš„å®¹é‡å¯†åº¦æ¯ä¸‰ä¸ªæœˆå¤§çº¦ç¿»å€ï¼Œä¸ºæœªæ¥çš„LLMå¼€å‘æä¾›äº†æ–°çš„è§†è§’ï¼Œå¼ºè°ƒäº†æé«˜å®¹é‡å¯†åº¦çš„é‡è¦æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.04280', 'title': 'HumanEdit: A High-Quality Human-Rewarded Dataset for Instruction-based Image Editing', 'url': 'https://huggingface.co/papers/2412.04280', 'abstract': 'We present HumanEdit, a high-quality, human-rewarded dataset specifically designed for instruction-guided image editing, enabling precise and diverse image manipulations through open-form language instructions. Previous large-scale editing datasets often incorporate minimal human feedback, leading to challenges in aligning datasets with human preferences. HumanEdit bridges this gap by employing human annotators to construct data pairs and administrators to provide feedback. With meticulously curation, HumanEdit comprises 5,751 images and requires more than 2,500 hours of human effort across four stages, ensuring both accuracy and reliability for a wide range of image editing tasks. The dataset includes six distinct types of editing instructions: Action, Add, Counting, Relation, Remove, and Replace, encompassing a broad spectrum of real-world scenarios. All images in the dataset are accompanied by masks, and for a subset of the data, we ensure that the instructions are sufficiently detailed to support mask-free editing. Furthermore, HumanEdit offers comprehensive diversity and high-resolution 1024 times 1024 content sourced from various domains, setting a new versatile benchmark for instructional image editing datasets. With the aim of advancing future research and establishing evaluation benchmarks in the field of image editing, we release HumanEdit at https://huggingface.co/datasets/BryanW/HumanEdit.', 'score': 12, 'issue_id': 980, 'pub_date': '2024-12-05', 'pub_date_card': {'ru': '5 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 5', 'zh': '12æœˆ5æ—¥'}, 'hash': '4467ff5ceea9cb1d', 'authors': ['Jinbin Bai', 'Wei Chow', 'Ling Yang', 'Xiangtai Li', 'Juncheng Li', 'Hanwang Zhang', 'Shuicheng Yan'], 'affiliations': ['Nanyang Technological University', 'National University of Singapore', 'Peking University', 'Skywork AI'], 'pdf_title_img': 'assets/pdf/title_img/2412.04280.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#data'], 'emoji': 'ğŸ¨', 'ru': {'title': 'HumanEdit: Ğ ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ¼', 'desc': 'HumanEdit - ÑÑ‚Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ ÑƒÑ‡Ğ°ÑÑ‚Ğ¸ĞµĞ¼ Ğ»ÑĞ´ĞµĞ¹. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 5751 Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ñ Ğ¼Ğ°ÑĞºĞ°Ğ¼Ğ¸ Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼Ğ¸ ÑˆĞµÑÑ‚Ğ¸ Ñ‚Ğ¸Ğ¿Ğ¾Ğ²: Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ, Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ, Ğ¿Ğ¾Ğ´ÑÑ‡ĞµÑ‚, Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğµ, ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ·Ğ°Ğ¼ĞµĞ½Ğ°. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ°ĞµÑ‚ÑÑ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸ĞµĞ¼, Ñ‚Ğ°Ğº ĞºĞ°Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ğ»ÑÑ Ñ Ğ¿Ñ€Ğ¸Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ¸ Ğ°Ğ´Ğ¼Ğ¸Ğ½Ğ¸ÑÑ‚Ñ€Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ². HumanEdit Ğ¿Ñ€ĞµĞ´Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'HumanEdit: Elevating Image Editing with Human-Centric Instructions', 'desc': 'HumanEdit is a newly created dataset aimed at improving instruction-guided image editing by incorporating significant human feedback. Unlike previous datasets that lacked sufficient human input, HumanEdit utilizes human annotators to create data pairs and provide valuable feedback, ensuring alignment with human preferences. The dataset consists of 5,751 high-resolution images and includes six types of editing instructions, allowing for a wide range of image manipulation tasks. By offering detailed instructions and masks, HumanEdit sets a new standard for versatility and accuracy in image editing research.'}, 'zh': {'title': 'HumanEditï¼šç²¾å‡†å¤šæ ·çš„å›¾åƒç¼–è¾‘æ•°æ®é›†', 'desc': 'HumanEditæ˜¯ä¸€ä¸ªé«˜è´¨é‡çš„äººç±»å¥–åŠ±æ•°æ®é›†ï¼Œä¸“é—¨ç”¨äºæŒ‡å¯¼å›¾åƒç¼–è¾‘ã€‚ä¸ä»¥å¾€çš„å¤§è§„æ¨¡ç¼–è¾‘æ•°æ®é›†ä¸åŒï¼ŒHumanEdité€šè¿‡äººç±»æ³¨é‡Šè€…æ„å»ºæ•°æ®å¯¹ï¼Œå¹¶ç”±ç®¡ç†å‘˜æä¾›åé¦ˆï¼Œç¡®ä¿æ•°æ®ä¸äººç±»åå¥½çš„å¯¹é½ã€‚è¯¥æ•°æ®é›†åŒ…å«5751å¼ å›¾åƒï¼Œç»è¿‡2500å¤šä¸ªå°æ—¶çš„äººå·¥åŠªåŠ›ï¼Œæ¶µç›–å…­ç§ä¸åŒç±»å‹çš„ç¼–è¾‘æŒ‡ä»¤ï¼Œæ”¯æŒå¤šæ ·åŒ–çš„å›¾åƒç¼–è¾‘ä»»åŠ¡ã€‚HumanEditä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†ä¸€ä¸ªæ–°çš„åŸºå‡†ï¼Œæ¨åŠ¨å›¾åƒç¼–è¾‘é¢†åŸŸçš„å‘å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.02142', 'title': 'Personalized Multimodal Large Language Models: A Survey', 'url': 'https://huggingface.co/papers/2412.02142', 'abstract': 'Multimodal Large Language Models (MLLMs) have become increasingly important due to their state-of-the-art performance and ability to integrate multiple data modalities, such as text, images, and audio, to perform complex tasks with high accuracy. This paper presents a comprehensive survey on personalized multimodal large language models, focusing on their architecture, training methods, and applications. We propose an intuitive taxonomy for categorizing the techniques used to personalize MLLMs to individual users, and discuss the techniques accordingly. Furthermore, we discuss how such techniques can be combined or adapted when appropriate, highlighting their advantages and underlying rationale. We also provide a succinct summary of personalization tasks investigated in existing research, along with the evaluation metrics commonly used. Additionally, we summarize the datasets that are useful for benchmarking personalized MLLMs. Finally, we outline critical open challenges. This survey aims to serve as a valuable resource for researchers and practitioners seeking to understand and advance the development of personalized multimodal large language models.', 'score': 11, 'issue_id': 982, 'pub_date': '2024-12-03', 'pub_date_card': {'ru': '3 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 3', 'zh': '12æœˆ3æ—¥'}, 'hash': 'a34ccd5a86ab7840', 'authors': ['Junda Wu', 'Hanjia Lyu', 'Yu Xia', 'Zhehao Zhang', 'Joe Barrow', 'Ishita Kumar', 'Mehrnoosh Mirtaheri', 'Hongjie Chen', 'Ryan A. Rossi', 'Franck Dernoncourt', 'Tong Yu', 'Ruiyi Zhang', 'Jiuxiang Gu', 'Nesreen K. Ahmed', 'Yu Wang', 'Xiang Chen', 'Hanieh Deilamsalehy', 'Namyong Park', 'Sungchul Kim', 'Huanrui Yang', 'Subrata Mitra', 'Zhengmian Hu', 'Nedim Lipka', 'Dang Nguyen', 'Yue Zhao', 'Jiebo Luo', 'Julian McAuley'], 'affiliations': ['Adobe Research', 'Cisco Research', 'Dartmouth College', 'Meta AI', 'University of Arizona', 'University of California, San Diego', 'University of Maryland', 'University of Massachusetts at Amherst', 'University of Oregon', 'University of Rochester', 'University of Southern California', 'Virginia Tech'], 'pdf_title_img': 'assets/pdf/title_img/2412.02142.jpg', 'data': {'categories': ['#training', '#survey', '#multimodal', '#architecture', '#dataset', '#benchmark'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞŸĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜: Ğ¾Ñ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ´Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (ĞœLLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ĞœLLM Ğ¸ Ğ¾Ğ±ÑÑƒĞ¶Ğ´Ğ°ÑÑ‚ Ğ¸Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ, Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ. Ğ Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ĞœLLM, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¸Ñ… Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ’ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ğµ Ğ¾Ğ±Ğ¾Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ñ‹ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸.'}, 'en': {'title': 'Personalizing Multimodal Language Models for Enhanced User Experience', 'desc': 'This paper surveys personalized multimodal large language models (MLLMs), which integrate various data types like text, images, and audio for improved task performance. It introduces a taxonomy to categorize personalization techniques, detailing their architectures and training methods. The authors also discuss how these techniques can be adapted or combined, emphasizing their benefits and rationale. Additionally, the paper reviews existing personalization tasks, evaluation metrics, and relevant datasets, while identifying key challenges in the field.'}, 'zh': {'title': 'ä¸ªæ€§åŒ–å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„æœªæ¥ä¹‹è·¯', 'desc': 'å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨æ•´åˆæ–‡æœ¬ã€å›¾åƒå’ŒéŸ³é¢‘ç­‰å¤šç§æ•°æ®æ¨¡æ€æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿé«˜æ•ˆå®Œæˆå¤æ‚ä»»åŠ¡ã€‚æœ¬æ–‡å¯¹ä¸ªæ€§åŒ–å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œäº†å…¨é¢çš„è°ƒæŸ¥ï¼Œé‡ç‚¹ä»‹ç»äº†å…¶æ¶æ„ã€è®­ç»ƒæ–¹æ³•å’Œåº”ç”¨ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç›´è§‚çš„åˆ†ç±»æ³•ï¼Œç”¨äºå¯¹ä¸ªæ€§åŒ–MLLMsçš„æŠ€æœ¯è¿›è¡Œåˆ†ç±»ï¼Œå¹¶è®¨è®ºäº†ç›¸åº”çš„æŠ€æœ¯ã€‚æœ€åï¼Œæˆ‘ä»¬æ€»ç»“äº†ç°æœ‰ç ”ç©¶ä¸­è°ƒæŸ¥çš„ä¸ªæ€§åŒ–ä»»åŠ¡åŠå…¶è¯„ä¼°æŒ‡æ ‡ï¼Œå¹¶æŒ‡å‡ºäº†æœªæ¥çš„æŒ‘æˆ˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.03304', 'title': 'Global MMLU: Understanding and Addressing Cultural and Linguistic Biases in Multilingual Evaluation', 'url': 'https://huggingface.co/papers/2412.03304', 'abstract': 'Cultural biases in multilingual datasets pose significant challenges for their effectiveness as global benchmarks. These biases stem not only from language but also from the cultural knowledge required to interpret questions, reducing the practical utility of translated datasets like MMLU. Furthermore, translation often introduces artifacts that can distort the meaning or clarity of questions in the target language. A common practice in multilingual evaluation is to rely on machine-translated evaluation sets, but simply translating a dataset is insufficient to address these challenges. In this work, we trace the impact of both of these issues on multilingual evaluations and ensuing model performances. Our large-scale evaluation of state-of-the-art open and proprietary models illustrates that progress on MMLU depends heavily on learning Western-centric concepts, with 28% of all questions requiring culturally sensitive knowledge. Moreover, for questions requiring geographic knowledge, an astounding 84.9% focus on either North American or European regions. Rankings of model evaluations change depending on whether they are evaluated on the full portion or the subset of questions annotated as culturally sensitive, showing the distortion to model rankings when blindly relying on translated MMLU. We release Global-MMLU, an improved MMLU with evaluation coverage across 42 languages -- with improved overall quality by engaging with compensated professional and community annotators to verify translation quality while also rigorously evaluating cultural biases present in the original dataset. This comprehensive Global-MMLU set also includes designated subsets labeled as culturally sensitive and culturally agnostic to allow for more holistic, complete evaluation.', 'score': 10, 'issue_id': 988, 'pub_date': '2024-12-04', 'pub_date_card': {'ru': '4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 4', 'zh': '12æœˆ4æ—¥'}, 'hash': 'f9677fd07f780c7b', 'authors': ['Shivalika Singh', 'Angelika Romanou', 'ClÃ©mentine Fourrier', 'David I. Adelani', 'Jian Gang Ngui', 'Daniel Vila-Suero', 'Peerat Limkonchotiwat', 'Kelly Marchisio', 'Wei Qi Leong', 'Yosephine Susanto', 'Raymond Ng', 'Shayne Longpre', 'Wei-Yin Ko', 'Madeline Smith', 'Antoine Bosselut', 'Alice Oh', 'Andre F. T. Martins', 'Leshem Choshen', 'Daphne Ippolito', 'Enzo Ferrante', 'Marzieh Fadaee', 'Beyza Ermis', 'Sara Hooker'], 'affiliations': ['AI Singapore', 'CONICET & Universidad de Buenos Aires', 'Carnegie Mellon University', 'Cohere', 'Cohere For AI', 'EPFL', 'Hugging Face', 'Instituto Superior TÃ©cnico, Universidade de Lisboa', 'Instituto de TelecomunicaÃ§Ãµes', 'KAIST', 'MIT', 'MIT, MIT-IBM Watson AI Lab', 'Mila, McGill University & Canada CIFAR AI Chair', 'National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2412.03304.jpg', 'data': {'categories': ['#ethics', '#dataset', '#multilingual', '#low_resource', '#machine_translation'], 'emoji': 'ğŸŒ', 'ru': {'title': 'ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ñ‹Ñ… Ğ±Ğ°Ñ€ÑŒĞµÑ€Ğ¾Ğ² Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ ÑÑ‚Ğ¸Ñ… Ğ¿Ñ€ĞµĞ´ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² ĞºĞ°Ğº Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ², Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ¿ĞµÑ€ĞµĞ²ĞµĞ´ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… MMLU. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ 28% Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ² MMLU Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‚ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ğ¾-ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, Ğ° 84.9% Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¿Ğ¾ Ğ³ĞµĞ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ğ¸ ÑĞ¾ÑÑ€ĞµĞ´Ğ¾Ñ‚Ğ¾Ñ‡ĞµĞ½Ñ‹ Ğ½Ğ° Ğ¡ĞµĞ²ĞµÑ€Ğ½Ğ¾Ğ¹ ĞĞ¼ĞµÑ€Ğ¸ĞºĞµ Ğ¸ Ğ•Ğ²Ñ€Ğ¾Ğ¿Ğµ. Ğ’ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ±Ñ‹Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Global-MMLU, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ 42 ÑĞ·Ñ‹ĞºĞ° Ğ¸ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Bridging Cultural Gaps in Multilingual Machine Learning', 'desc': 'This paper addresses the cultural biases found in multilingual datasets, which hinder their effectiveness as global benchmarks for machine learning models. It highlights how these biases arise not only from language differences but also from the cultural context needed to understand questions, particularly in datasets like MMLU. The authors demonstrate that relying solely on machine translation can lead to significant distortions in model performance and evaluation rankings, especially for questions requiring cultural or geographic knowledge. To combat these issues, they introduce Global-MMLU, a refined dataset that includes culturally sensitive annotations and improved translation quality, enabling more accurate evaluations across 42 languages.'}, 'zh': {'title': 'è§£å†³å¤šè¯­è¨€æ•°æ®é›†ä¸­çš„æ–‡åŒ–åè§', 'desc': 'è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¤šè¯­è¨€æ•°æ®é›†ä¸­å­˜åœ¨çš„æ–‡åŒ–åè§å¯¹å…¨çƒåŸºå‡†çš„å½±å“ã€‚è¿™äº›åè§ä¸ä»…æºäºè¯­è¨€ï¼Œè¿˜æ¶‰åŠåˆ°ç†è§£é—®é¢˜æ‰€éœ€çš„æ–‡åŒ–çŸ¥è¯†ï¼Œé™ä½äº†ç¿»è¯‘æ•°æ®é›†çš„å®ç”¨æ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè®¸å¤šé—®é¢˜éœ€è¦æ–‡åŒ–æ•æ„Ÿçš„çŸ¥è¯†ï¼Œå°¤å…¶æ˜¯å…³äºåœ°ç†çŸ¥è¯†çš„é—®é¢˜ï¼Œä¸»è¦é›†ä¸­åœ¨åŒ—ç¾å’Œæ¬§æ´²åœ°åŒºã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œä½œè€…å‘å¸ƒäº†Global-MMLUï¼Œä¸€ä¸ªæ”¹è¿›çš„å¤šè¯­è¨€è¯„ä¼°é›†ï¼Œæ¶µç›–42ç§è¯­è¨€ï¼Œå¹¶é€šè¿‡ä¸“ä¸šå’Œç¤¾åŒºæ³¨é‡Šè€…éªŒè¯ç¿»è¯‘è´¨é‡ï¼Œè¯„ä¼°æ–‡åŒ–åè§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.04378', 'title': 'Discriminative Fine-tuning of LVLMs', 'url': 'https://huggingface.co/papers/2412.04378', 'abstract': 'Contrastively-trained Vision-Language Models (VLMs) like CLIP have become the de facto approach for discriminative vision-language representation learning. However, these models have limited language understanding, often exhibiting a "bag of words" behavior. At the same time, Large Vision-Language Models (LVLMs), which combine vision encoders with LLMs, have been shown capable of detailed vision-language reasoning, yet their autoregressive nature renders them less suitable for discriminative tasks.   In this work, we propose to combine "the best of both worlds": a new training approach for discriminative fine-tuning of LVLMs that results in strong discriminative and compositional capabilities. Essentially, our approach converts a generative LVLM into a discriminative one, unlocking its capability for powerful image-text discrimination combined with enhanced language understanding.   Our contributions include: (1) A carefully designed training/optimization framework that utilizes image-text pairs of variable length and granularity for training the model with both contrastive and next-token prediction losses. This is accompanied by ablation studies that justify the necessity of our framework\'s components. (2) A parameter-efficient adaptation method using a combination of soft prompting and LoRA adapters. (3) Significant improvements over state-of-the-art CLIP-like models of similar size, including standard image-text retrieval benchmarks and notable gains in compositionality.', 'score': 9, 'issue_id': 989, 'pub_date': '2024-12-05', 'pub_date_card': {'ru': '5 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 5', 'zh': '12æœˆ5æ—¥'}, 'hash': '6d05f8d3eedf64ed', 'authors': ['Yassine Ouali', 'Adrian Bulat', 'Alexandros Xenos', 'Anestis Zaganidis', 'Ioannis Maniadis Metaxas', 'Georgios Tzimiropoulos', 'Brais Martinez'], 'affiliations': ['Queen Mary University of London', 'Samsung AI Cambridge', 'Technical University of Iasi'], 'pdf_title_img': 'assets/pdf/title_img/2412.04378.jpg', 'data': {'categories': ['#architecture', '#cv', '#reasoning', '#benchmark', '#optimization', '#multimodal', '#training'], 'emoji': 'ğŸ”', 'ru': {'title': 'ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ Ğ¸Ğ· Ğ´Ğ²ÑƒÑ… Ğ¼Ğ¸Ñ€Ğ¾Ğ²: Ğ´Ğ¸ÑĞºÑ€Ğ¸Ğ¼Ğ¸Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ LVLM Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ ÑĞ·Ñ‹ĞºĞ°', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LVLM) Ğ´Ğ»Ñ Ğ´Ğ¸ÑĞºÑ€Ğ¸Ğ¼Ğ¸Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒÑÑ‚ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñ‹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‚Ğ¸Ğ¿Ğ° CLIP Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… LVLM, ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ ÑĞ·Ñ‹ĞºĞ° Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğº ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑÑ…ĞµĞ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¸ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑĞ¼Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ñ‚Ğ¸Ğ¿Ğ° CLIP Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Unlocking Discriminative Power in Vision-Language Models', 'desc': 'This paper introduces a new training method for Large Vision-Language Models (LVLMs) that enhances their ability to understand and discriminate between images and text. By combining contrastive learning with next-token prediction, the model achieves better performance in tasks requiring detailed vision-language reasoning. The authors also present a parameter-efficient adaptation technique that utilizes soft prompting and LoRA adapters, making the model more efficient. Overall, this work demonstrates significant improvements over existing models like CLIP, particularly in image-text retrieval and compositional understanding.'}, 'zh': {'title': 'ç»“åˆä¼˜åŠ¿ï¼Œæå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„åŒºåˆ†èƒ½åŠ›', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è®­ç»ƒæ–¹æ³•ï¼Œç”¨äºå¯¹å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰è¿›è¡ŒåŒºåˆ†æ€§å¾®è°ƒï¼Œä»è€Œæé«˜å…¶åœ¨å›¾åƒ-æ–‡æœ¬ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†å¯¹æ¯”å­¦ä¹ å’Œä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹æŸå¤±ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£è¯­è¨€å¹¶è¿›è¡Œå›¾åƒ-æ–‡æœ¬çš„åŒºåˆ†ã€‚é€šè¿‡ä½¿ç”¨å¯å˜é•¿åº¦å’Œç²’åº¦çš„å›¾åƒ-æ–‡æœ¬å¯¹è¿›è¡Œè®­ç»ƒï¼Œæˆ‘ä»¬çš„æ¡†æ¶å±•ç¤ºäº†å…¶å„ä¸ªç»„æˆéƒ¨åˆ†çš„å¿…è¦æ€§ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ ‡å‡†çš„å›¾åƒ-æ–‡æœ¬æ£€ç´¢åŸºå‡†ä¸Šæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„CLIPç±»æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.01169', 'title': 'OmniFlow: Any-to-Any Generation with Multi-Modal Rectified Flows', 'url': 'https://huggingface.co/papers/2412.01169', 'abstract': 'We introduce OmniFlow, a novel generative model designed for any-to-any generation tasks such as text-to-image, text-to-audio, and audio-to-image synthesis. OmniFlow advances the rectified flow (RF) framework used in text-to-image models to handle the joint distribution of multiple modalities. It outperforms previous any-to-any models on a wide range of tasks, such as text-to-image and text-to-audio synthesis. Our work offers three key contributions: First, we extend RF to a multi-modal setting and introduce a novel guidance mechanism, enabling users to flexibly control the alignment between different modalities in the generated outputs. Second, we propose a novel architecture that extends the text-to-image MMDiT architecture of Stable Diffusion 3 and enables audio and text generation. The extended modules can be efficiently pretrained individually and merged with the vanilla text-to-image MMDiT for fine-tuning. Lastly, we conduct a comprehensive study on the design choices of rectified flow transformers for large-scale audio and text generation, providing valuable insights into optimizing performance across diverse modalities. The Code will be available at https://github.com/jacklishufan/OmniFlows.', 'score': 8, 'issue_id': 980, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 2', 'zh': '12æœˆ2æ—¥'}, 'hash': '7f3df6f7d4733664', 'authors': ['Shufan Li', 'Konstantinos Kallidromitis', 'Akash Gokul', 'Zichun Liao', 'Yusuke Kato', 'Kazuki Kozuka', 'Aditya Grover'], 'affiliations': ['Panasonic AI Research', 'Salesforce AI Research', 'UCLA'], 'pdf_title_img': 'assets/pdf/title_img/2412.01169.jpg', 'data': {'categories': ['#diffusion', '#multimodal', '#training', '#optimization', '#architecture'], 'emoji': 'ğŸ”„', 'ru': {'title': 'OmniFlow: ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸', 'desc': 'OmniFlow - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ»ÑĞ±Ğ¾Ğ³Ğ¾ Ñ‚Ğ¸Ğ¿Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ğ»ÑĞ±Ğ¾Ğ¹ Ğ´Ñ€ÑƒĞ³Ğ¾Ğ¹ Ñ‚Ğ¸Ğ¿, Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€ Ñ‚ĞµĞºÑÑ‚-Ğ²-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¸Ğ»Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾-Ğ²-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ²Ñ‹Ğ¿Ñ€ÑĞ¼Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ° (rectified flow) Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ñ‹Ğ¼ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. OmniFlow Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾Ğ¼ ÑĞ¿ĞµĞºÑ‚Ñ€Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ MMDiT Ğ¸Ğ· Stable Diffusion 3, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¸Ğ·ÑƒÑ‡Ğ°ÑÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ñ Ğ²Ñ‹Ğ¿Ñ€ÑĞ¼Ğ»ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ¼ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°.'}, 'en': {'title': 'OmniFlow: Bridging Modalities for Any-to-Any Generation', 'desc': 'OmniFlow is a new generative model that can create outputs across different types of data, like turning text into images or audio. It builds on the rectified flow (RF) framework, allowing it to understand and generate multiple types of data together. This model not only improves performance on tasks like text-to-image and text-to-audio synthesis but also introduces a way for users to control how different data types relate to each other in the generated results. Additionally, it features a unique architecture that enhances existing models and provides insights into optimizing generative tasks across various modalities.'}, 'zh': {'title': 'OmniFlowï¼šå¤šæ¨¡æ€ç”Ÿæˆçš„çµæ´»è§£å†³æ–¹æ¡ˆ', 'desc': 'OmniFlowæ˜¯ä¸€ç§æ–°å‹ç”Ÿæˆæ¨¡å‹ï¼Œæ—¨åœ¨å¤„ç†ä»»æ„åˆ°ä»»æ„çš„ç”Ÿæˆä»»åŠ¡ï¼Œå¦‚æ–‡æœ¬åˆ°å›¾åƒã€æ–‡æœ¬åˆ°éŸ³é¢‘å’ŒéŸ³é¢‘åˆ°å›¾åƒçš„åˆæˆã€‚å®ƒåœ¨æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ä¸­æ”¹è¿›äº†ä¿®æ­£æµï¼ˆRFï¼‰æ¡†æ¶ï¼Œä»¥å¤„ç†å¤šç§æ¨¡æ€çš„è”åˆåˆ†å¸ƒã€‚OmniFlowåœ¨å¤šç§ä»»åŠ¡ä¸Šè¶…è¶Šäº†ä¹‹å‰çš„ä»»æ„åˆ°ä»»æ„æ¨¡å‹ï¼Œæä¾›äº†çµæ´»çš„æ¨¡æ€å¯¹é½æ§åˆ¶æœºåˆ¶ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¿˜æ¢è®¨äº†ä¿®æ­£æµå˜æ¢å™¨åœ¨å¤§è§„æ¨¡éŸ³é¢‘å’Œæ–‡æœ¬ç”Ÿæˆä¸­çš„è®¾è®¡é€‰æ‹©ï¼Œä¸ºä¼˜åŒ–ä¸åŒæ¨¡æ€çš„æ€§èƒ½æä¾›äº†å®è´µçš„è§è§£ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.01820', 'title': 'Towards Universal Soccer Video Understanding', 'url': 'https://huggingface.co/papers/2412.01820', 'abstract': 'As a globally celebrated sport, soccer has attracted widespread interest from fans all over the world. This paper aims to develop a comprehensive multi-modal framework for soccer video understanding. Specifically, we make the following contributions in this paper: (i) we introduce SoccerReplay-1988, the largest multi-modal soccer dataset to date, featuring videos and detailed annotations from 1,988 complete matches, with an automated annotation pipeline; (ii) we present the first visual-language foundation model in the soccer domain, MatchVision, which leverages spatiotemporal information across soccer videos and excels in various downstream tasks; (iii) we conduct extensive experiments and ablation studies on event classification, commentary generation, and multi-view foul recognition. MatchVision demonstrates state-of-the-art performance on all of them, substantially outperforming existing models, which highlights the superiority of our proposed data and model. We believe that this work will offer a standard paradigm for sports understanding research.', 'score': 8, 'issue_id': 980, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 2', 'zh': '12æœˆ2æ—¥'}, 'hash': 'fa9f7e4132ee5026', 'authors': ['Jiayuan Rao', 'Haoning Wu', 'Hao Jiang', 'Ya Zhang', 'Yanfeng Wang', 'Weidi Xie'], 'affiliations': ['Alibaba Group, China', 'CMIC, Shanghai Jiao Tong University, China', 'School of Artificial Intelligence, Shanghai Jiao Tong University, China'], 'pdf_title_img': 'assets/pdf/title_img/2412.01820.jpg', 'data': {'categories': ['#dataset', '#architecture', '#multimodal', '#video'], 'emoji': 'âš½', 'ru': {'title': 'MatchVision: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ¼ Ğ·Ñ€ĞµĞ½Ğ¸Ğ¸ Ğ´Ğ»Ñ Ñ„ÑƒÑ‚Ğ±Ğ¾Ğ»Ğ°', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ MatchVision Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ñ„ÑƒÑ‚Ğ±Ğ¾Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½ĞµĞ¹ÑˆĞ¸Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ SoccerReplay-1988, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸ 1988 Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ñ… Ğ¼Ğ°Ñ‚Ñ‡ĞµĞ¹. MatchVision Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ· Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ¼Ğ¼ĞµĞ½Ñ‚Ğ°Ñ€Ğ¸ĞµĞ² Ğ¸ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ°Ñ€ÑƒÑˆĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Revolutionizing Soccer Video Analysis with MatchVision', 'desc': 'This paper presents a new framework for understanding soccer videos using machine learning. It introduces SoccerReplay-1988, a large dataset containing videos and annotations from nearly 2,000 soccer matches, which helps in training models. The authors also develop MatchVision, a visual-language model that processes spatiotemporal data from soccer videos and performs well in tasks like event classification and commentary generation. The results show that MatchVision outperforms existing models, setting a new standard for research in sports video analysis.'}, 'zh': {'title': 'è¶³çƒè§†é¢‘ç†è§£çš„æ–°æ ‡å‡†', 'desc': 'è¿™ç¯‡è®ºæ–‡æ—¨åœ¨å¼€å‘ä¸€ä¸ªå…¨é¢çš„å¤šæ¨¡æ€æ¡†æ¶ï¼Œç”¨äºç†è§£è¶³çƒè§†é¢‘ã€‚æˆ‘ä»¬å¼•å…¥äº†SoccerReplay-1988ï¼Œè¿™æ˜¯è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„å¤šæ¨¡æ€è¶³çƒæ•°æ®é›†ï¼ŒåŒ…å«1988åœºå®Œæ•´æ¯”èµ›çš„è§†é¢‘å’Œè¯¦ç»†æ³¨é‡Šã€‚æˆ‘ä»¬è¿˜æå‡ºäº†è¶³çƒé¢†åŸŸçš„é¦–ä¸ªè§†è§‰-è¯­è¨€åŸºç¡€æ¨¡å‹MatchVisionï¼Œèƒ½å¤Ÿåˆ©ç”¨è¶³çƒè§†é¢‘ä¸­çš„æ—¶ç©ºä¿¡æ¯ï¼Œå¹¶åœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒå’Œæ¶ˆèç ”ç©¶ï¼ŒMatchVisionåœ¨äº‹ä»¶åˆ†ç±»ã€è¯„è®ºç”Ÿæˆå’Œå¤šè§†è§’çŠ¯è§„è¯†åˆ«ç­‰ä»»åŠ¡ä¸Šå‡è¡¨ç°å‡ºæœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ˜¾ç¤ºäº†æˆ‘ä»¬æå‡ºçš„æ•°æ®å’Œæ¨¡å‹çš„ä¼˜è¶Šæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.04062', 'title': 'ZipAR: Accelerating Autoregressive Image Generation through Spatial Locality', 'url': 'https://huggingface.co/papers/2412.04062', 'abstract': "In this paper, we propose ZipAR, a training-free, plug-and-play parallel decoding framework for accelerating auto-regressive (AR) visual generation. The motivation stems from the observation that images exhibit local structures, and spatially distant regions tend to have minimal interdependence. Given a partially decoded set of visual tokens, in addition to the original next-token prediction scheme in the row dimension, the tokens corresponding to spatially adjacent regions in the column dimension can be decoded in parallel, enabling the ``next-set prediction'' paradigm. By decoding multiple tokens simultaneously in a single forward pass, the number of forward passes required to generate an image is significantly reduced, resulting in a substantial improvement in generation efficiency. Experiments demonstrate that ZipAR can reduce the number of model forward passes by up to 91% on the Emu3-Gen model without requiring any additional retraining.", 'score': 7, 'issue_id': 981, 'pub_date': '2024-12-05', 'pub_date_card': {'ru': '5 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 5', 'zh': '12æœˆ5æ—¥'}, 'hash': '44d216e3fa2fb345', 'authors': ['Yefei He', 'Feng Chen', 'Yuanyu He', 'Shaoxuan He', 'Hong Zhou', 'Kaipeng Zhang', 'Bohan Zhuang'], 'affiliations': ['Shanghai AI Laboratory, China', 'The University of Adelaide, Australia', 'Zhejiang University, China'], 'pdf_title_img': 'assets/pdf/title_img/2412.04062.jpg', 'data': {'categories': ['#optimization', '#training', '#cv'], 'emoji': 'ğŸš€', 'ru': {'title': 'ZipAR: Ğ£ÑĞºĞ¾Ñ€ÑĞµĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼', 'desc': 'ZipAR - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸Ğ¼ĞµÑÑ‚ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑŒ. ZipAR Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹, ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ ÑĞ¼ĞµĞ¶Ğ½Ñ‹Ğ¼ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑĞ¼, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€ÑĞ¼Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ZipAR Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ¸Ñ‚ÑŒ Ñ‡Ğ¸ÑĞ»Ğ¾ Ğ¿Ñ€ÑĞ¼Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ½Ğ° 91% Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Emu3-Gen Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Accelerate Image Generation with Parallel Decoding!', 'desc': "This paper introduces ZipAR, a new framework designed to speed up the process of generating images using auto-regressive models without the need for retraining. It leverages the observation that images have local structures, allowing for the parallel decoding of visual tokens in adjacent regions. By implementing a 'next-set prediction' approach, ZipAR can decode multiple tokens at once, significantly cutting down the number of forward passes needed to create an image. Experiments show that this method can reduce forward passes by up to 91%, greatly enhancing generation efficiency."}, 'zh': {'title': 'ZipARï¼šåŠ é€Ÿè‡ªå›å½’è§†è§‰ç”Ÿæˆçš„é«˜æ•ˆè§£ç æ¡†æ¶', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºZipARçš„æ¡†æ¶ï¼Œæ—¨åœ¨åŠ é€Ÿè‡ªå›å½’è§†è§‰ç”Ÿæˆã€‚è¯¥æ¡†æ¶ä¸éœ€è¦é‡æ–°è®­ç»ƒï¼Œèƒ½å¤Ÿå®ç°å¹¶è¡Œè§£ç ï¼Œåˆ©ç”¨å›¾åƒçš„å±€éƒ¨ç»“æ„ç‰¹æ€§ã€‚é€šè¿‡åœ¨åˆ—ç»´åº¦ä¸Šå¹¶è¡Œè§£ç ç©ºé—´ç›¸é‚»åŒºåŸŸçš„è§†è§‰æ ‡è®°ï¼ŒZipARæ˜¾è‘—å‡å°‘äº†ç”Ÿæˆå›¾åƒæ‰€éœ€çš„å‰å‘ä¼ æ’­æ¬¡æ•°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒZipARåœ¨Emu3-Genæ¨¡å‹ä¸Šå¯ä»¥å°†å‰å‘ä¼ æ’­æ¬¡æ•°å‡å°‘å¤šè¾¾91%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.04462', 'title': '4Real-Video: Learning Generalizable Photo-Realistic 4D Video Diffusion', 'url': 'https://huggingface.co/papers/2412.04462', 'abstract': 'We propose 4Real-Video, a novel framework for generating 4D videos, organized as a grid of video frames with both time and viewpoint axes. In this grid, each row contains frames sharing the same timestep, while each column contains frames from the same viewpoint. We propose a novel two-stream architecture. One stream performs viewpoint updates on columns, and the other stream performs temporal updates on rows. After each diffusion transformer layer, a synchronization layer exchanges information between the two token streams. We propose two implementations of the synchronization layer, using either hard or soft synchronization. This feedforward architecture improves upon previous work in three ways: higher inference speed, enhanced visual quality (measured by FVD, CLIP, and VideoScore), and improved temporal and viewpoint consistency (measured by VideoScore and Dust3R-Confidence).', 'score': 6, 'issue_id': 996, 'pub_date': '2024-12-05', 'pub_date_card': {'ru': '5 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 5', 'zh': '12æœˆ5æ—¥'}, 'hash': '2046f12235c1227f', 'authors': ['Chaoyang Wang', 'Peiye Zhuang', 'Tuan Duc Ngo', 'Willi Menapace', 'Aliaksandr Siarohin', 'Michael Vasilkovsky', 'Ivan Skorokhodov', 'Sergey Tulyakov', 'Peter Wonka', 'Hsin-Ying Lee'], 'affiliations': ['KAUST', 'Snap Inc', 'Umass Amherst'], 'pdf_title_img': 'assets/pdf/title_img/2412.04462.jpg', 'data': {'categories': ['#video', '#architecture'], 'emoji': 'ğŸ¥', 'ru': {'title': '4Real-Video: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 4D-Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': '4Real-Video - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 4D-Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ğ²Ğ¸Ğ´Ğµ ÑĞµÑ‚ĞºĞ¸ ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ñ Ğ¾ÑÑĞ¼Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¸ Ñ€Ğ°ĞºÑƒÑ€ÑĞ°. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ´Ğ²ÑƒÑ…Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°: Ğ¾Ğ´Ğ¸Ğ½ Ğ¿Ğ¾Ñ‚Ğ¾Ğº Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑĞµÑ‚ Ñ€Ğ°ĞºÑƒÑ€ÑÑ‹ Ğ¿Ğ¾ ÑÑ‚Ğ¾Ğ»Ğ±Ñ†Ğ°Ğ¼, Ğ´Ñ€ÑƒĞ³Ğ¾Ğ¹ - Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ ÑÑ‚Ñ€Ğ¾ĞºĞ°Ğ¼. ĞŸĞ¾ÑĞ»Ğµ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ÑĞ»Ğ¾Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° Ğ¿Ñ€Ğ¾Ğ¸ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ°Ğ¼Ğ¸. Ğ­Ñ‚Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ¿Ğ¾ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°, Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¸ Ñ€Ğ°ĞºÑƒÑ€ÑĞ°Ğ¼.'}, 'en': {'title': 'Revolutionizing 4D Video Generation with 4Real-Video', 'desc': 'The paper introduces 4Real-Video, a new framework designed to create 4D videos that are structured in a grid format, incorporating both time and viewpoint dimensions. It utilizes a two-stream architecture where one stream focuses on updating the viewpoint across columns, while the other stream handles temporal updates along the rows. A synchronization layer is implemented after each diffusion transformer layer to facilitate communication between the two streams, with options for hard or soft synchronization. This innovative approach results in faster inference, better visual quality, and improved consistency in both temporal and viewpoint aspects compared to previous methods.'}, 'zh': {'title': '4Real-Videoï¼šé«˜æ•ˆç”Ÿæˆ4Dè§†é¢‘çš„æ–°æ¡†æ¶', 'desc': 'æˆ‘ä»¬æå‡ºäº†4Real-Videoï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„4Dè§†é¢‘ç”Ÿæˆæ¡†æ¶ï¼Œè§†é¢‘å¸§ä»¥æ—¶é—´å’Œè§†è§’ä¸ºè½´ç»„ç»‡æˆç½‘æ ¼ã€‚åœ¨è¿™ä¸ªç½‘æ ¼ä¸­ï¼Œæ¯ä¸€è¡ŒåŒ…å«ç›¸åŒæ—¶é—´æ­¥çš„å¸§ï¼Œè€Œæ¯ä¸€åˆ—åŒ…å«ç›¸åŒè§†è§’çš„å¸§ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æ–°é¢–çš„åŒæµæ¶æ„ï¼Œä¸€æ¡æµåœ¨åˆ—ä¸Šè¿›è¡Œè§†è§’æ›´æ–°ï¼Œå¦ä¸€æ¡æµåœ¨è¡Œä¸Šè¿›è¡Œæ—¶é—´æ›´æ–°ã€‚é€šè¿‡åŒæ­¥å±‚åœ¨ä¸¤ä¸ªæµä¹‹é—´äº¤æ¢ä¿¡æ¯ï¼Œæˆ‘ä»¬çš„æ¶æ„åœ¨æ¨ç†é€Ÿåº¦ã€è§†è§‰è´¨é‡å’Œæ—¶é—´ä¸è§†è§’ä¸€è‡´æ€§æ–¹é¢éƒ½æœ‰æ˜¾è‘—æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.19574', 'title': 'KV Shifting Attention Enhances Language Modeling', 'url': 'https://huggingface.co/papers/2411.19574', 'abstract': "The current large language models are mainly based on decode-only structure transformers, which have great in-context learning (ICL) capabilities. It is generally believed that the important foundation of its ICL capability is the induction heads mechanism, which requires at least two layers attention. In order to more efficiently implement the ability of the model's induction, we revisit the induction heads mechanism and proposed a KV shifting attention. We theoretically prove that the KV shifting attention reducing the model's requirements for the depth and width of the induction heads mechanism. Our experimental results demonstrate that KV shifting attention is beneficial to learning induction heads and language modeling, which lead to better performance or faster convergence from toy models to the pre-training models with more than 10 B parameters.", 'score': 6, 'issue_id': 987, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 29', 'zh': '11æœˆ29æ—¥'}, 'hash': 'feb0a592d740fe9b', 'authors': ['Mingyu Xu', 'Wei Cheng', 'Bingning Wang', 'Weipeng Chen'], 'affiliations': ['Baichuan Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2411.19574.jpg', 'data': {'categories': ['#optimization', '#architecture', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞŸĞ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ½Ğ´ÑƒĞºÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¸Ğ½Ğ´ÑƒĞºÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğº Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ KV shifting attention Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ½Ğ´ÑƒĞºÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ¢ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğº Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğµ Ğ¸ ÑˆĞ¸Ñ€Ğ¸Ğ½Ğµ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° Ğ¸Ğ½Ğ´ÑƒĞºÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğº. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ KV shifting attention ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸Ğ½Ğ´ÑƒĞºÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Ğ³Ğ¾Ğ»Ğ¾Ğ²ĞºĞ°Ğ¼ Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ñ Ğº Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ»Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ¹ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Enhancing Induction Heads with KV Shifting Attention', 'desc': "This paper discusses improvements in large language models that use a decode-only transformer structure, focusing on their in-context learning (ICL) abilities. The authors highlight the importance of the induction heads mechanism, which traditionally requires multiple layers of attention. They introduce a new approach called KV shifting attention, which simplifies the induction heads mechanism by reducing the model's depth and width requirements. Experimental results show that this new attention method enhances the learning of induction heads and improves language modeling performance, leading to faster convergence in models with over 10 billion parameters."}, 'zh': {'title': 'æå‡å½’çº³èƒ½åŠ›çš„KVä½ç§»æ³¨æ„åŠ›æœºåˆ¶', 'desc': 'å½“å‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ä¸»è¦åŸºäºè§£ç ç»“æ„çš„å˜æ¢å™¨ï¼Œå…·æœ‰å¾ˆå¼ºçš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ã€‚äººä»¬æ™®éè®¤ä¸ºï¼Œè¿™ç§èƒ½åŠ›çš„é‡è¦åŸºç¡€æ˜¯å½’çº³å¤´æœºåˆ¶ï¼Œè€Œè¯¥æœºåˆ¶è‡³å°‘éœ€è¦ä¸¤å±‚æ³¨æ„åŠ›ã€‚ä¸ºäº†æ›´æœ‰æ•ˆåœ°å®ç°æ¨¡å‹çš„å½’çº³èƒ½åŠ›ï¼Œæˆ‘ä»¬é‡æ–°å®¡è§†äº†å½’çº³å¤´æœºåˆ¶ï¼Œå¹¶æå‡ºäº†ä¸€ç§KVä½ç§»æ³¨æ„åŠ›ã€‚æˆ‘ä»¬çš„ç†è®ºè¯æ˜è¡¨æ˜ï¼ŒKVä½ç§»æ³¨æ„åŠ›é™ä½äº†æ¨¡å‹å¯¹å½’çº³å¤´æœºåˆ¶çš„æ·±åº¦å’Œå®½åº¦è¦æ±‚ï¼Œå®éªŒç»“æœæ˜¾ç¤ºå…¶åœ¨å­¦ä¹ å½’çº³å¤´å’Œè¯­è¨€å»ºæ¨¡æ–¹é¢å…·æœ‰ä¼˜åŠ¿ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.04003', 'title': 'Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement', 'url': 'https://huggingface.co/papers/2412.04003', 'abstract': 'Large Language Models (LLMs) have achieved remarkable progress in recent years; however, their excellent performance is still largely limited to major world languages, primarily English. Many LLMs continue to face challenges with multilingual tasks, especially when it comes to low-resource languages. To address this issue, we introduced Marco-LLM: Massive multilingual training for cross-lingual enhancement LLM. We have collected a substantial amount of multilingual data for several low-resource languages and conducted extensive continual pre-training using the Qwen2 models. This effort has resulted in a multilingual LLM named Marco-LLM. Through comprehensive evaluations on various multilingual benchmarks, including MMMLU, AGIEval, Belebele, Flores-200, XCOPA and many others, Marco-LLM has demonstrated substantial improvements over state-of-the-art LLMs. Furthermore, Marco-LLM achieved substantial enhancements in any-to-any machine translation tasks, showing the effectiveness of our multilingual LLM. Marco-LLM is a pioneering multilingual LLM designed to not only perform exceptionally well in multilingual tasks, including low-resource languages, but also maintain strong performance in English and other major languages, closing the performance gap between high- and low-resource language capabilities. By bridging languages, this effort demonstrates our dedication to ensuring LLMs work accurately across various languages.', 'score': 6, 'issue_id': 985, 'pub_date': '2024-12-05', 'pub_date_card': {'ru': '5 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 5', 'zh': '12æœˆ5æ—¥'}, 'hash': '435ec5749dcb2e12', 'authors': ['Lingfeng Ming', 'Bo Zeng', 'Chenyang Lyu', 'Tianqi Shi', 'Yu Zhao', 'Xue Yang', 'Yefeng Liu', 'Yiyu Wang', 'Linlong Xu', 'Yangyang Liu', 'Xiaohu Zhao', 'Hao Wang', 'Heng Liu', 'Hao Zhou', 'Huifeng Yin', 'Zifu Shang', 'Haijun Li', 'Longyue Wang', 'Weihua Luo', 'Kaifu Zhang'], 'affiliations': ['MarcoPolo Team, Alibaba International Digital Commerce'], 'pdf_title_img': 'assets/pdf/title_img/2412.04003.jpg', 'data': {'categories': ['#training', '#machine_translation', '#dataset', '#low_resource', '#multilingual'], 'emoji': 'ğŸŒ', 'ru': {'title': 'Marco-LLM: ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ±Ğ°Ñ€ÑŒĞµÑ€Ğ° Ğ² Ğ¼Ğ¸Ñ€Ğµ Ğ˜Ğ˜', 'desc': 'Marco-LLM - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM), Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€ĞµÑÑƒÑ€ÑĞ½Ñ‹Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ°Ğ¼Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ±Ñ‹Ğ»Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ Ğ¾Ğ±ÑŠĞµĞ¼Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Qwen2. Marco-LLM Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ LLM Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ MMMLU, AGIEval, Belebele Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ. ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ° ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ»ÑĞ±Ñ‹Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ°Ğ¼Ğ¸.'}, 'en': {'title': 'Bridging Language Gaps with Marco-LLM', 'desc': 'This paper presents Marco-LLM, a large language model designed to improve performance in multilingual tasks, particularly for low-resource languages. The authors collected extensive multilingual data and performed continual pre-training using Qwen2 models to enhance cross-lingual capabilities. Evaluations on multiple benchmarks showed that Marco-LLM outperforms existing state-of-the-art models, especially in any-to-any machine translation tasks. The model aims to bridge the performance gap between high-resource and low-resource languages, ensuring effective communication across diverse linguistic backgrounds.'}, 'zh': {'title': 'Marco-LLMï¼šæ‰“ç ´è¯­è¨€å£å’çš„å¤šè¯­è¨€æ¨¡å‹', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è¿‘å¹´æ¥å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å…¶ä¼˜ç§€è¡¨ç°ä¸»è¦é›†ä¸­åœ¨ä¸»è¦ä¸–ç•Œè¯­è¨€ä¸Šï¼Œå°¤å…¶æ˜¯è‹±è¯­ã€‚ä¸ºäº†æ”¹å–„ä½èµ„æºè¯­è¨€çš„å¤šè¯­è¨€ä»»åŠ¡è¡¨ç°ï¼Œæˆ‘ä»¬æå‡ºäº†Marco-LLMï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹è·¨è¯­è¨€å¢å¼ºçš„å¤§è§„æ¨¡å¤šè¯­è¨€è®­ç»ƒæ¨¡å‹ã€‚æˆ‘ä»¬æ”¶é›†äº†å¤§é‡ä½èµ„æºè¯­è¨€çš„å¤šè¯­è¨€æ•°æ®ï¼Œå¹¶ä½¿ç”¨Qwen2æ¨¡å‹è¿›è¡Œäº†å¹¿æ³›çš„æŒç»­é¢„è®­ç»ƒã€‚Marco-LLMåœ¨å¤šé¡¹å¤šè¯­è¨€åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„æ”¹è¿›ï¼Œå°¤å…¶åœ¨ä»»æ„å¯¹ä»»æ„çš„æœºå™¨ç¿»è¯‘ä»»åŠ¡ä¸­ï¼Œå±•ç¤ºäº†å…¶å¤šè¯­è¨€æ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.04139', 'title': 'Monet: Mixture of Monosemantic Experts for Transformers', 'url': 'https://huggingface.co/papers/2412.04139', 'abstract': 'Understanding the internal computations of large language models (LLMs) is crucial for aligning them with human values and preventing undesirable behaviors like toxic content generation. However, mechanistic interpretability is hindered by polysemanticity -- where individual neurons respond to multiple, unrelated concepts. While Sparse Autoencoders (SAEs) have attempted to disentangle these features through sparse dictionary learning, they have compromised LLM performance due to reliance on post-hoc reconstruction loss. To address this issue, we introduce Mixture of Monosemantic Experts for Transformers (Monet) architecture, which incorporates sparse dictionary learning directly into end-to-end Mixture-of-Experts pretraining. Our novel expert decomposition method enables scaling the expert count to 262,144 per layer while total parameters scale proportionally to the square root of the number of experts. Our analyses demonstrate mutual exclusivity of knowledge across experts and showcase the parametric knowledge encapsulated within individual experts. Moreover, Monet allows knowledge manipulation over domains, languages, and toxicity mitigation without degrading general performance. Our pursuit of transparent LLMs highlights the potential of scaling expert counts to enhance} mechanistic interpretability and directly resect the internal knowledge to fundamentally adjust} model behavior. The source code and pretrained checkpoints are available at https://github.com/dmis-lab/Monet.', 'score': 6, 'issue_id': 982, 'pub_date': '2024-12-05', 'pub_date_card': {'ru': '5 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 5', 'zh': '12æœˆ5æ—¥'}, 'hash': '9853123764b31006', 'authors': ['Jungwoo Park', 'Young Jin Ahn', 'Kee-Eung Kim', 'Jaewoo Kang'], 'affiliations': ['AIGEN Sciences', 'KAIST', 'Korea University'], 'pdf_title_img': 'assets/pdf/title_img/2412.04139.jpg', 'data': {'categories': ['#training', '#interpretability', '#optimization', '#open_source', '#architecture', '#alignment'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞŸÑ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ñ‹Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: Ñ€Ğ°ÑÑˆĞ¸Ñ„Ñ€Ğ¾Ğ²ĞºĞ° Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ¾Ğ½Ğ¾ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Mixture of Monosemantic Experts for Transformers (Monet) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Monet Ğ²Ğ½ĞµĞ´Ñ€ÑĞµÑ‚ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ Ğ½ĞµĞ¿Ğ¾ÑÑ€ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ² Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‚Ğ¸Ğ¿Ğ° Mixture-of-Experts. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ´Ğ¾ 262,144 Ğ½Ğ° ÑĞ»Ğ¾Ğ¹, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¾Ğ±Ñ‰ĞµĞµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ñ€Ğ°ÑÑ‚ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¿Ğ¾Ñ€Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ ĞºĞ²Ğ°Ğ´Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¼Ñƒ ĞºĞ¾Ñ€Ğ½Ñ Ñ‡Ğ¸ÑĞ»Ğ° ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ². ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ¸ÑĞºĞ»ÑÑ‡Ğ°ÑÑ‰ĞµĞµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ğ¼Ğ¸ Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ, Ğ¸Ğ½ĞºĞ°Ğ¿ÑÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ² Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ñ….'}, 'en': {'title': 'Unlocking LLMs: Expert Knowledge for Safer AI', 'desc': "This paper addresses the challenge of understanding how large language models (LLMs) work internally, particularly to align them with human values and reduce harmful outputs. The authors introduce a new architecture called Mixture of Monosemantic Experts for Transformers (Monet), which improves upon previous methods by integrating sparse dictionary learning directly into the model's training process. Monet allows for a large number of specialized experts, enhancing the model's ability to manage different types of knowledge while maintaining overall performance. The findings suggest that this approach not only improves interpretability but also enables better control over the model's behavior regarding various domains and toxicity levels."}, 'zh': {'title': 'æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„å¯è§£é‡Šæ€§ä¸æ€§èƒ½', 'desc': 'ç†è§£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å†…éƒ¨è®¡ç®—å¯¹äºä½¿å…¶ä¸äººç±»ä»·å€¼è§‚å¯¹é½è‡³å…³é‡è¦ï¼Œå¹¶é˜²æ­¢ç”Ÿæˆæœ‰å®³å†…å®¹ã€‚ç„¶è€Œï¼Œæœºåˆ¶å¯è§£é‡Šæ€§å—åˆ°å¤šä¹‰æ€§çš„é˜»ç¢ï¼Œå³å•ä¸ªç¥ç»å…ƒå¯¹å¤šä¸ªæ— å…³æ¦‚å¿µçš„å“åº”ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ¶æ„ï¼Œç§°ä¸ºå•ä¹‰ä¸“å®¶æ··åˆæ¨¡å‹ï¼ˆMonetï¼‰ï¼Œå®ƒå°†ç¨€ç–å­—å…¸å­¦ä¹ ç›´æ¥èå…¥ç«¯åˆ°ç«¯çš„ä¸“å®¶é¢„è®­ç»ƒä¸­ï¼Œä»è€Œæé«˜äº†æ¨¡å‹çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œä¸“å®¶ä¹‹é—´çš„çŸ¥è¯†æ˜¯ç›¸äº’ç‹¬ç«‹çš„ï¼Œå¹¶ä¸”Monetå…è®¸åœ¨ä¸åŒé¢†åŸŸã€è¯­è¨€å’Œæ¯’æ€§å‡è½»æ–¹é¢è¿›è¡ŒçŸ¥è¯†æ“ä½œï¼Œè€Œä¸ä¼šé™ä½æ•´ä½“æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.04449', 'title': 'p-MoD: Building Mixture-of-Depths MLLMs via Progressive Ratio Decay', 'url': 'https://huggingface.co/papers/2412.04449', 'abstract': 'Despite the remarkable performance of multimodal large language models (MLLMs) across diverse tasks, the substantial training and inference costs impede their advancement. The majority of computation stems from the overwhelming volume of vision tokens processed by the transformer decoder. In this paper, we propose to build efficient MLLMs by leveraging the Mixture-of-Depths (MoD) mechanism, where each transformer decoder layer selects essential vision tokens to process while skipping redundant ones. However, integrating MoD into MLLMs is non-trivial. To address the challenges of training and inference stability as well as limited training data, we adapt the MoD module with two novel designs: tanh-gated weight normalization (TanhNorm) and symmetric token reweighting (STRing). Moreover, we observe that vision tokens exhibit higher redundancy in deeper layer and thus design a progressive ratio decay (PRD) strategy, which gradually reduces the token retention ratio layer by layer, employing a shifted cosine schedule. This crucial design fully unleashes the potential of MoD, significantly boosting the efficiency and performance of our models. To validate the effectiveness of our approach, we conduct extensive experiments with two baseline models across 14 benchmarks. Our model, p-MoD, matches or even surpasses the performance of the baseline models, with only 55.6% TFLOPs and 53.8% KV cache storage during inference, and 77.7% GPU hours during training.', 'score': 5, 'issue_id': 996, 'pub_date': '2024-12-05', 'pub_date_card': {'ru': '5 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 5', 'zh': '12æœˆ5æ—¥'}, 'hash': 'a68320734b5346db', 'authors': ['Jun Zhang', 'Desen Meng', 'Ji Qi', 'Zhenpeng Huang', 'Tao Wu', 'Limin Wang'], 'affiliations': ['China Mobile (Suzhou) Software Technology Co., Ltd.', 'Shanghai AI Lab', 'State Key Laboratory for Novel Software Technology, Nanjing University'], 'pdf_title_img': 'assets/pdf/title_img/2412.04449.jpg', 'data': {'categories': ['#optimization', '#inference', '#training', '#benchmark', '#multimodal'], 'emoji': 'ğŸš€', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: Ğ¼ĞµĞ½ÑŒÑˆĞµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° Mixture-of-Depths (MoD). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ MoD Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ²ÑƒÑ… Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ‚ĞµÑ…Ğ½Ğ¸Ğº: Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²ĞµÑĞ¾Ğ² Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ³Ğ¸Ğ¿ĞµÑ€Ğ±Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ‚Ğ°Ğ½Ğ³ĞµĞ½ÑĞ° (TanhNorm) Ğ¸ ÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ²Ğ·Ğ²ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² (STRing). Ğ¢Ğ°ĞºĞ¶Ğµ Ğ²Ğ²ĞµĞ´ĞµĞ½Ğ° ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ ÑĞ¾Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ñ (PRD), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¾Ñ‚ ÑĞ»Ğ¾Ñ Ğº ÑĞ»Ğ¾Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ p-MoD Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¸Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ².'}, 'en': {'title': 'Efficient Multimodal Learning with Selective Token Processing', 'desc': 'This paper introduces a method to enhance the efficiency of multimodal large language models (MLLMs) by using a Mixture-of-Depths (MoD) mechanism. The MoD mechanism allows the model to selectively process important vision tokens while ignoring redundant ones, reducing computational costs. To improve training and inference stability, the authors implement two innovative designs: tanh-gated weight normalization (TanhNorm) and symmetric token reweighting (STRing). Their experiments show that the proposed model, p-MoD, achieves comparable or superior performance to baseline models while significantly lowering resource usage during both training and inference.'}, 'zh': {'title': 'é«˜æ•ˆå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„åˆ›æ–°è®¾è®¡', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œé€šè¿‡æ··åˆæ·±åº¦ï¼ˆMoDï¼‰æœºåˆ¶æ¥ä¼˜åŒ–è®¡ç®—æ•ˆç‡ã€‚è¯¥æœºåˆ¶å…è®¸æ¯ä¸ªå˜æ¢å™¨è§£ç å™¨å±‚é€‰æ‹©é‡è¦çš„è§†è§‰æ ‡è®°è¿›è¡Œå¤„ç†ï¼ŒåŒæ—¶è·³è¿‡å†—ä½™çš„æ ‡è®°ã€‚ä¸ºäº†è§£å†³è®­ç»ƒå’Œæ¨ç†çš„ç¨³å®šæ€§é—®é¢˜ï¼Œæœ¬æ–‡å¼•å…¥äº†ä¸¤ç§æ–°è®¾è®¡ï¼štanhé—¨æ§æƒé‡å½’ä¸€åŒ–ï¼ˆTanhNormï¼‰å’Œå¯¹ç§°æ ‡è®°é‡åŠ æƒï¼ˆSTRingï¼‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„p-MoDæ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—é™ä½äº†è®¡ç®—èµ„æºçš„æ¶ˆè€—ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.03704', 'title': 'Scaling Inference-Time Search with Vision Value Model for Improved Visual Comprehension', 'url': 'https://huggingface.co/papers/2412.03704', 'abstract': "Despite significant advancements in vision-language models (VLMs), there lacks effective approaches to enhance response quality by scaling inference-time computation. This capability is known to be a core step towards the self-improving models in recent large language model studies. In this paper, we present Vision Value Model (VisVM) that can guide VLM inference-time search to generate responses with better visual comprehension. Specifically, VisVM not only evaluates the generated sentence quality in the current search step, but also anticipates the quality of subsequent sentences that may result from the current step, thus providing a long-term value. In this way, VisVM steers VLMs away from generating sentences prone to hallucinations or insufficient detail, thereby producing higher quality responses. Experimental results demonstrate that VisVM-guided search significantly enhances VLMs' ability to generate descriptive captions with richer visual details and fewer hallucinations, compared with greedy decoding and search methods with other visual reward signals. Furthermore, we find that self-training the model with the VisVM-guided captions improve VLM's performance across a wide range of multimodal benchmarks, indicating the potential for developing self-improving VLMs. Our value model and code are available at https://github.com/si0wang/VisVM.", 'score': 5, 'issue_id': 993, 'pub_date': '2024-12-04', 'pub_date_card': {'ru': '4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 4', 'zh': '12æœˆ4æ—¥'}, 'hash': 'ec4d70e89a11baa5', 'authors': ['Wang Xiyao', 'Yang Zhengyuan', 'Li Linjie', 'Lu Hongjin', 'Xu Yuancheng', 'Lin Chung-Ching Lin', 'Lin Kevin', 'Huang Furong', 'Wang Lijuan'], 'affiliations': ['Microsoft', 'University of Maryland, College Park'], 'pdf_title_img': 'assets/pdf/title_img/2412.03704.jpg', 'data': {'categories': ['#optimization', '#inference', '#cv', '#hallucinations', '#training', '#multimodal'], 'emoji': 'ğŸ”', 'ru': {'title': 'VisVM: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ VLM Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Vision Value Model (VisVM) - Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‰ÑƒÑ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° (VLM) Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. VisVM Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ñ…, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ¸ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ VisVM Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ VLM Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ñ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº. Ğ‘Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ğ³Ğ¾, ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ VisVM ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ VLM Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ….'}, 'en': {'title': 'Enhancing Visual Comprehension in VLMs with Vision Value Model', 'desc': 'This paper introduces the Vision Value Model (VisVM), which enhances the response quality of vision-language models (VLMs) during inference. VisVM evaluates not only the quality of the current generated sentence but also predicts the quality of future sentences, providing a long-term value assessment. By guiding the search process, VisVM helps VLMs avoid generating sentences that lack detail or contain inaccuracies, leading to more descriptive and accurate outputs. Experimental results show that using VisVM significantly improves the generation of captions with richer visual details and reduces hallucinations, paving the way for self-improving VLMs through self-training.'}, 'zh': {'title': 'æå‡è§†è§‰è¯­è¨€æ¨¡å‹å“åº”è´¨é‡çš„å…³é”®', 'desc': 'å°½ç®¡è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨æ¨ç†æ—¶è®¡ç®—èƒ½åŠ›çš„æå‡æ–¹é¢ä»ç¼ºä¹æœ‰æ•ˆçš„æ–¹æ³•ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§è§†è§‰ä»·å€¼æ¨¡å‹ï¼ˆVisVMï¼‰ï¼Œå®ƒå¯ä»¥æŒ‡å¯¼VLMåœ¨æ¨ç†æ—¶çš„æœç´¢ï¼Œä»¥ç”Ÿæˆæ›´å…·è§†è§‰ç†è§£çš„å“åº”ã€‚VisVMä¸ä»…è¯„ä¼°å½“å‰æœç´¢æ­¥éª¤ç”Ÿæˆå¥å­çš„è´¨é‡ï¼Œè¿˜é¢„æµ‹åç»­å¥å­çš„è´¨é‡ï¼Œä»è€Œæä¾›é•¿æœŸä»·å€¼ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVisVMå¼•å¯¼çš„æœç´¢æ˜¾è‘—æé«˜äº†VLMç”Ÿæˆæè¿°æ€§æ ‡é¢˜çš„èƒ½åŠ›ï¼Œå‡å°‘äº†å¹»è§‰ç°è±¡ï¼Œæ˜¾ç¤ºå‡ºè‡ªæˆ‘æå‡VLMçš„æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.04448', 'title': 'MEMO: Memory-Guided Diffusion for Expressive Talking Video Generation', 'url': 'https://huggingface.co/papers/2412.04448', 'abstract': 'Recent advances in video diffusion models have unlocked new potential for realistic audio-driven talking video generation. However, achieving seamless audio-lip synchronization, maintaining long-term identity consistency, and producing natural, audio-aligned expressions in generated talking videos remain significant challenges. To address these challenges, we propose Memory-guided EMOtion-aware diffusion (MEMO), an end-to-end audio-driven portrait animation approach to generate identity-consistent and expressive talking videos. Our approach is built around two key modules: (1) a memory-guided temporal module, which enhances long-term identity consistency and motion smoothness by developing memory states to store information from a longer past context to guide temporal modeling via linear attention; and (2) an emotion-aware audio module, which replaces traditional cross attention with multi-modal attention to enhance audio-video interaction, while detecting emotions from audio to refine facial expressions via emotion adaptive layer norm. Extensive quantitative and qualitative results demonstrate that MEMO generates more realistic talking videos across diverse image and audio types, outperforming state-of-the-art methods in overall quality, audio-lip synchronization, identity consistency, and expression-emotion alignment.', 'score': 5, 'issue_id': 993, 'pub_date': '2024-12-05', 'pub_date_card': {'ru': '5 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 5', 'zh': '12æœˆ5æ—¥'}, 'hash': 'cb01c778a4be31bd', 'authors': ['Longtao Zheng', 'Yifan Zhang', 'Hanzhong Guo', 'Jiachun Pan', 'Zhenxiong Tan', 'Jiahao Lu', 'Chuanxin Tang', 'Bo An', 'Shuicheng Yan'], 'affiliations': ['Nanyang Technological University', 'National University of Singapore', 'Skywork AI'], 'pdf_title_img': 'assets/pdf/title_img/2412.04448.jpg', 'data': {'categories': ['#long_context', '#multimodal', '#video', '#diffusion'], 'emoji': 'ğŸ­', 'ru': {'title': 'MEMO: Ğ ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ°Ñ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ñ€Ñ‚Ñ€ĞµÑ‚Ğ¾Ğ² Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ñ€Ñ‚Ñ€ĞµÑ‚Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ MEMO (Memory-guided EMOtion-aware diffusion). ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹ MEMO Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ»Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ğ»Ğ°Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾-Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. MEMO Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ MEMO Ğ½Ğ°Ğ´ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ² Ğ¾Ğ±Ñ‰ĞµĞ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ, ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ³ÑƒĞ± Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾, ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ»Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¸ Ğ²Ñ‹Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ ÑĞ¼Ğ¾Ñ†Ğ¸ÑĞ¼.'}, 'en': {'title': 'MEMO: Revolutionizing Audio-Driven Talking Video Generation', 'desc': 'This paper introduces Memory-guided EMOtion-aware diffusion (MEMO), a novel approach for generating talking videos that are synchronized with audio. MEMO addresses key challenges such as audio-lip synchronization and maintaining consistent identities over time. It utilizes a memory-guided temporal module to enhance motion smoothness and identity consistency by leveraging past information. Additionally, an emotion-aware audio module improves the interaction between audio and video, ensuring that facial expressions align with detected emotions, resulting in high-quality, expressive talking videos.'}, 'zh': {'title': 'è®°å¿†å¼•å¯¼çš„æƒ…æ„Ÿæ„ŸçŸ¥æ‰©æ•£ï¼šç”Ÿæˆæ›´çœŸå®çš„å¯¹è¯è§†é¢‘', 'desc': 'æœ€è¿‘è§†é¢‘æ‰©æ•£æ¨¡å‹çš„è¿›å±•ä¸ºåŸºäºéŸ³é¢‘çš„çœŸå®æ„Ÿå¯¹è¯è§†é¢‘ç”Ÿæˆå¼€è¾Ÿäº†æ–°æ½œåŠ›ã€‚ç„¶è€Œï¼Œå®ç°æ— ç¼çš„éŸ³é¢‘ä¸å˜´å”‡åŒæ­¥ã€ä¿æŒé•¿æœŸèº«ä»½ä¸€è‡´æ€§ä»¥åŠç”Ÿæˆè‡ªç„¶çš„éŸ³é¢‘å¯¹é½è¡¨æƒ…ä»ç„¶æ˜¯é‡å¤§æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†è®°å¿†å¼•å¯¼çš„æƒ…æ„Ÿæ„ŸçŸ¥æ‰©æ•£ï¼ˆMEMOï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç«¯åˆ°ç«¯çš„éŸ³é¢‘é©±åŠ¨è‚–åƒåŠ¨ç”»æ–¹æ³•ï¼Œæ—¨åœ¨ç”Ÿæˆèº«ä»½ä¸€è‡´ä¸”å¯Œæœ‰è¡¨ç°åŠ›çš„å¯¹è¯è§†é¢‘ã€‚æˆ‘ä»¬çš„æ–¹æ¡ˆå›´ç»•ä¸¤ä¸ªå…³é”®æ¨¡å—æ„å»ºï¼šè®°å¿†å¼•å¯¼çš„æ—¶é—´æ¨¡å—å’Œæƒ…æ„Ÿæ„ŸçŸ¥éŸ³é¢‘æ¨¡å—ï¼Œæ˜¾è‘—æå‡äº†è§†é¢‘ç”Ÿæˆçš„è´¨é‡å’Œä¸€è‡´æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.04106', 'title': 'MRGen: Diffusion-based Controllable Data Engine for MRI Segmentation towards Unannotated Modalities', 'url': 'https://huggingface.co/papers/2412.04106', 'abstract': 'Medical image segmentation has recently demonstrated impressive progress with deep neural networks, yet the heterogeneous modalities and scarcity of mask annotations limit the development of segmentation models on unannotated modalities. This paper investigates a new paradigm for leveraging generative models in medical applications: controllably synthesizing data for unannotated modalities, without requiring registered data pairs. Specifically, we make the following contributions in this paper: (i) we collect and curate a large-scale radiology image-text dataset, MedGen-1M, comprising modality labels, attributes, region, and organ information, along with a subset of organ mask annotations, to support research in controllable medical image generation; (ii) we propose a diffusion-based data engine, termed MRGen, which enables generation conditioned on text prompts and masks, synthesizing MR images for diverse modalities lacking mask annotations, to train segmentation models on unannotated modalities; (iii) we conduct extensive experiments across various modalities, illustrating that our data engine can effectively synthesize training samples and extend MRI segmentation towards unannotated modalities.', 'score': 5, 'issue_id': 980, 'pub_date': '2024-12-04', 'pub_date_card': {'ru': '4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 4', 'zh': '12æœˆ4æ—¥'}, 'hash': '7b2720f9a6c27027', 'authors': ['Haoning Wu', 'Ziheng Zhao', 'Ya Zhang', 'Weidi Xie', 'Yanfeng Wang'], 'affiliations': ['CMIC, Shanghai Jiao Tong University, China', 'School of Artificial Intelligence, Shanghai Jiao Tong University, China', 'Shanghai AI Laboratory, China'], 'pdf_title_img': 'assets/pdf/title_img/2412.04106.jpg', 'data': {'categories': ['#diffusion', '#healthcare', '#synthetic', '#cv', '#data', '#dataset'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ĞœĞ Ğ¢ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ±ĞµĞ· Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¸Ğ½Ñ‚ĞµĞ·Ñƒ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ½ĞµĞ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ MedGen-1M, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ Ñ€Ğ°Ğ´Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¾Ğ¹ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¾Ğ². ĞĞ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ MRGen Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞœĞ Ğ¢-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº Ğ¸ Ğ¼Ğ°ÑĞ¾Ğº. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ½ĞµĞ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑÑ… ĞœĞ Ğ¢.'}, 'en': {'title': 'Generating Synthetic Data for Medical Image Segmentation', 'desc': 'This paper presents a novel approach to medical image segmentation by using generative models to create synthetic data for modalities that lack mask annotations. The authors introduce a large dataset called MedGen-1M, which includes radiology images with modality labels and some organ mask annotations, facilitating research in medical image generation. They propose a diffusion-based data engine named MRGen that generates MR images based on text prompts and available masks, allowing for the training of segmentation models on unannotated data. Extensive experiments demonstrate the effectiveness of MRGen in synthesizing training samples and improving segmentation performance across various MRI modalities.'}, 'zh': {'title': 'åˆ©ç”¨ç”Ÿæˆæ¨¡å‹æ¨åŠ¨åŒ»å­¦å›¾åƒåˆ†å‰²çš„åˆ›æ–°', 'desc': 'è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­ä½¿ç”¨ç”Ÿæˆæ¨¡å‹çš„æ–°æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹æœªæ ‡æ³¨æ¨¡æ€çš„æ•°æ®åˆæˆã€‚ç ”ç©¶è€…ä»¬æ”¶é›†å¹¶æ•´ç†äº†ä¸€ä¸ªå¤§å‹çš„æ”¾å°„å­¦å›¾åƒ-æ–‡æœ¬æ•°æ®é›†MedGen-1Mï¼ŒåŒ…å«æ¨¡æ€æ ‡ç­¾ã€å±æ€§ã€åŒºåŸŸå’Œå™¨å®˜ä¿¡æ¯ï¼Œä»¥åŠéƒ¨åˆ†å™¨å®˜çš„æ©è†œæ³¨é‡Šã€‚è®ºæ–‡ä¸­æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£çš„ç”Ÿæˆæ•°æ®å¼•æ“MRGenï¼Œå¯ä»¥æ ¹æ®æ–‡æœ¬æç¤ºå’Œæ©è†œç”Ÿæˆç¼ºä¹æ©è†œæ³¨é‡Šçš„MRå›¾åƒï¼Œä»è€Œè®­ç»ƒæœªæ ‡æ³¨æ¨¡æ€çš„åˆ†å‰²æ¨¡å‹ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒï¼Œç»“æœè¡¨æ˜è¯¥æ•°æ®å¼•æ“èƒ½å¤Ÿæœ‰æ•ˆåˆæˆè®­ç»ƒæ ·æœ¬ï¼Œæ¨åŠ¨MRIåˆ†å‰²å‘æœªæ ‡æ³¨æ¨¡æ€çš„æ‰©å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.04403', 'title': 'Establishing Task Scaling Laws via Compute-Efficient Model Ladders', 'url': 'https://huggingface.co/papers/2412.04403', 'abstract': 'We develop task scaling laws and model ladders to predict the individual task performance of pretrained language models (LMs) in the overtrained setting. Standard power laws for language modeling loss cannot accurately model task performance. Therefore, we leverage a two-step prediction approach: first use model and data size to predict a task-specific loss, and then use this task loss to predict task performance. We train a set of small-scale "ladder" models, collect data points to fit the parameterized functions of the two prediction steps, and make predictions for two target models: a 7B model trained to 4T tokens and a 13B model trained to 5T tokens. Training the ladder models only costs 1% of the compute used for the target models. On four multiple-choice tasks written in ranked classification format, we can predict the accuracy of both target models within 2 points of absolute error. We have higher prediction error on four other tasks (average absolute error 6.9) and find that these are often tasks with higher variance in task metrics. We also find that using less compute to train fewer ladder models tends to deteriorate predictions. Finally, we empirically show that our design choices and the two-step approach lead to superior performance in establishing scaling laws.', 'score': 2, 'issue_id': 1001, 'pub_date': '2024-12-05', 'pub_date_card': {'ru': '5 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 5', 'zh': '12æœˆ5æ—¥'}, 'hash': '799212928bdda07f', 'authors': ['Akshita Bhagia', 'Jiacheng Liu', 'Alexander Wettig', 'David Heineman', 'Oyvind Tafjord', 'Ananya Harsh Jha', 'Luca Soldaini', 'Noah A. Smith', 'Dirk Groeneveld', 'Pang Wei Koh', 'Jesse Dodge', 'Hannaneh Hajishirzi'], 'affiliations': ['Allen Institute for Artificial Intelligence', 'Paul G. Allen School of Computer Science & Engineering, University of Washington', 'Princeton University'], 'pdf_title_img': 'assets/pdf/title_img/2412.04403.jpg', 'data': {'categories': ['#small_models', '#optimization', '#dataset', '#training'], 'emoji': 'ğŸ“ˆ', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. ĞĞ½Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½ÑƒÑ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ½Ğ° ĞµĞµ Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ¸Ñ‚Ğ¾Ğ³Ğ¾Ğ²ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ. Ğ”Ğ»Ñ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… 'Ğ»ĞµÑÑ‚Ğ½Ğ¸Ñ‡Ğ½Ñ‹Ñ…' Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ñ‡Ñ‚Ğ¾ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ²ÑĞµĞ³Ğ¾ 1% Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğ¼Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. ĞĞ° Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ° Ğ² Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ°Ñ… 2 Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿ÑƒĞ½ĞºÑ‚Ğ¾Ğ²."}, 'en': {'title': 'Predicting Performance with Model Ladders and Scaling Laws', 'desc': "This paper introduces a method to predict how well pretrained language models will perform on specific tasks, especially when they are overtrained. The authors find that traditional power laws do not effectively predict task performance, so they propose a two-step approach: first estimating task-specific loss based on model and data size, and then using that loss to predict actual task performance. They create smaller 'ladder' models to gather data and refine their predictions, achieving high accuracy in predicting the performance of larger models with significantly less computational cost. The study also highlights that while their method works well for some tasks, it struggles with others that have more variability in results, indicating the need for careful model training and selection."}, 'zh': {'title': 'ä»»åŠ¡è¡¨ç°é¢„æµ‹çš„æ–°æ–¹æ³•', 'desc': 'æˆ‘ä»¬å¼€å‘äº†ä»»åŠ¡ç¼©æ”¾æ³•åˆ™å’Œæ¨¡å‹é˜¶æ¢¯ï¼Œä»¥é¢„æµ‹é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹åœ¨è¿‡åº¦è®­ç»ƒæƒ…å†µä¸‹çš„å•ä¸ªä»»åŠ¡è¡¨ç°ã€‚æ ‡å‡†çš„è¯­è¨€å»ºæ¨¡æŸå¤±å¹‚å¾‹æ— æ³•å‡†ç¡®å»ºæ¨¡ä»»åŠ¡è¡¨ç°ï¼Œå› æ­¤æˆ‘ä»¬é‡‡ç”¨äº†ä¸¤æ­¥é¢„æµ‹æ–¹æ³•ï¼šé¦–å…ˆä½¿ç”¨æ¨¡å‹å’Œæ•°æ®è§„æ¨¡é¢„æµ‹ç‰¹å®šä»»åŠ¡çš„æŸå¤±ï¼Œç„¶ååˆ©ç”¨è¯¥ä»»åŠ¡æŸå¤±é¢„æµ‹ä»»åŠ¡è¡¨ç°ã€‚æˆ‘ä»¬è®­ç»ƒäº†ä¸€ç»„å°è§„æ¨¡çš„â€œé˜¶æ¢¯â€æ¨¡å‹ï¼Œæ”¶é›†æ•°æ®ç‚¹ä»¥æ‹Ÿåˆè¿™ä¸¤æ­¥é¢„æµ‹çš„å‚æ•°åŒ–å‡½æ•°ï¼Œå¹¶å¯¹ä¸¤ä¸ªç›®æ ‡æ¨¡å‹è¿›è¡Œé¢„æµ‹ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œè¿™ç§è®¾è®¡é€‰æ‹©å’Œä¸¤æ­¥æ–¹æ³•åœ¨å»ºç«‹ç¼©æ”¾æ³•åˆ™æ–¹é¢è¡¨ç°ä¼˜è¶Šã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.04262', 'title': 'SynFinTabs: A Dataset of Synthetic Financial Tables for Information and Table Extraction', 'url': 'https://huggingface.co/papers/2412.04262', 'abstract': 'Table extraction from document images is a challenging AI problem, and labelled data for many content domains is difficult to come by. Existing table extraction datasets often focus on scientific tables due to the vast amount of academic articles that are readily available, along with their source code. However, there are significant layout and typographical differences between tables found across scientific, financial, and other domains. Current datasets often lack the words, and their positions, contained within the tables, instead relying on unreliable OCR to extract these features for training modern machine learning models on natural language processing tasks. Therefore, there is a need for a more general method of obtaining labelled data. We present SynFinTabs, a large-scale, labelled dataset of synthetic financial tables. Our hope is that our method of generating these synthetic tables is transferable to other domains. To demonstrate the effectiveness of our dataset in training models to extract information from table images, we create FinTabQA, a layout large language model trained on an extractive question-answering task. We test our model using real-world financial tables and compare it to a state-of-the-art generative model and discuss the results. We make the dataset, model, and dataset generation code publicly available.', 'score': 2, 'issue_id': 994, 'pub_date': '2024-12-05', 'pub_date_card': {'ru': '5 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 5', 'zh': '12æœˆ5æ—¥'}, 'hash': '3fef2d0f6d8b7c5c', 'authors': ['Ethan Bradley', 'Muhammad Roman', 'Karen Rafferty', 'Barry Devereux'], 'affiliations': ['School of Electronics, Electrical Engineering and Computer Science, Queens University Belfast, Belfast, UK'], 'pdf_title_img': 'assets/pdf/title_img/2412.04262.jpg', 'data': {'categories': ['#open_source', '#data', '#dataset', '#synthetic', '#transfer_learning', '#benchmark'], 'emoji': 'ğŸ“Š', 'ru': {'title': 'Ğ¡Ğ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ñ… Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ SynFinTabs - ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ñ… Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½ Ğ¸ Ğ² Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ FinTabQA - ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ· Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ°ÑÑŒ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ñ… Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ğ°Ñ… Ğ¸ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ»Ğ°ÑÑŒ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚, Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸ ĞºĞ¾Ğ´ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ñ…Ğ¾Ğ´ÑÑ‚ÑÑ Ğ² Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¼ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğµ.'}, 'en': {'title': 'Unlocking Financial Insights with Synthetic Table Data', 'desc': 'This paper addresses the challenge of table extraction from document images, particularly in the financial domain where labeled data is scarce. It introduces SynFinTabs, a large-scale dataset of synthetic financial tables designed to improve the training of machine learning models for this task. The authors also present FinTabQA, a layout large language model specifically trained for extractive question-answering on table images. By comparing their model with existing state-of-the-art generative models, they demonstrate the effectiveness of their synthetic dataset and make all resources publicly available for further research.'}, 'zh': {'title': 'åˆæˆé‡‘èè¡¨æ ¼æ•°æ®é›†çš„åˆ›æ–°åº”ç”¨', 'desc': 'æœ¬æ–‡è®¨è®ºäº†ä»æ–‡æ¡£å›¾åƒä¸­æå–è¡¨æ ¼çš„æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨ç¼ºä¹æ ‡æ³¨æ•°æ®çš„æƒ…å†µä¸‹ã€‚ç°æœ‰çš„æ•°æ®é›†ä¸»è¦é›†ä¸­åœ¨ç§‘å­¦è¡¨æ ¼ä¸Šï¼Œå¿½è§†äº†é‡‘èç­‰å…¶ä»–é¢†åŸŸçš„è¡¨æ ¼å¸ƒå±€å’Œæ’ç‰ˆå·®å¼‚ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½œè€…æå‡ºäº†SynFinTabsï¼Œä¸€ä¸ªå¤§è§„æ¨¡çš„åˆæˆé‡‘èè¡¨æ ¼æ ‡æ³¨æ•°æ®é›†ï¼Œæ—¨åœ¨ä¸ºä¸åŒé¢†åŸŸæä¾›å¯è½¬ç§»çš„æ ‡æ³¨æ•°æ®ç”Ÿæˆæ–¹æ³•ã€‚é€šè¿‡åˆ›å»ºFinTabQAæ¨¡å‹ï¼Œä½œè€…å±•ç¤ºäº†è¯¥æ•°æ®é›†åœ¨è®­ç»ƒä¿¡æ¯æå–æ¨¡å‹æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œå¹¶ä¸æœ€å…ˆè¿›çš„ç”Ÿæˆæ¨¡å‹è¿›è¡Œäº†æ¯”è¾ƒã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.04363', 'title': 'Challenges in Trustworthy Human Evaluation of Chatbots', 'url': 'https://huggingface.co/papers/2412.04363', 'abstract': 'Open community-driven platforms like Chatbot Arena that collect user preference data from site visitors have gained a reputation as one of the most trustworthy publicly available benchmarks for LLM performance. While now standard, it is tricky to implement effective guardrails to collect high-quality annotations from humans. In this paper, we demonstrate that three sources of bad annotations, both malicious and otherwise, can corrupt the reliability of open leaderboard rankings. In particular, we show that only 10\\% of poor quality votes by apathetic (site visitors not appropriately incentivized to give correct votes) or adversarial (bad actors seeking to inflate the ranking of a target model) annotators can change the rankings of models by up to 5 places on the leaderboard. Finally, we discuss open challenges in ensuring high-quality human annotations.', 'score': 2, 'issue_id': 992, 'pub_date': '2024-12-05', 'pub_date_card': {'ru': '5 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 5', 'zh': '12æœˆ5æ—¥'}, 'hash': '4fa16cf75a2af540', 'authors': ['Wenting Zhao', 'Alexander M. Rush', 'Tanya Goyal'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2412.04363.jpg', 'data': {'categories': ['#ethics', '#rlhf', '#hallucinations', '#alignment', '#benchmark'], 'emoji': 'ğŸ­', 'ru': {'title': 'ĞÑ…Ğ¸Ğ»Ğ»ĞµÑĞ¾Ğ²Ğ° Ğ¿ÑÑ‚Ğ° ĞºÑ€Ğ°ÑƒĞ´ÑĞ¾Ñ€ÑĞ¸Ğ½Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ² Ğ˜Ğ˜', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Chatbot Arena. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ»ÑÑÑ‚ Ñ‚Ñ€Ğ¸ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ° Ğ½ĞµĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… Ğ¸ÑĞºĞ°Ğ·Ğ¸Ñ‚ÑŒ Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞŸĞ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ²ÑĞµĞ³Ğ¾ 10% Ğ½ĞµĞ´Ğ¾Ğ±Ñ€Ğ¾ÑĞ¾Ğ²ĞµÑÑ‚Ğ½Ñ‹Ñ… Ğ¾Ñ†ĞµĞ½Ğ¾Ğº Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¸Ğ·Ğ¼ĞµĞ½Ğ¸Ñ‚ÑŒ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ğµ Ğ½Ğ° 5 Ğ¼ĞµÑÑ‚. ĞĞ±ÑÑƒĞ¶Ğ´Ğ°ÑÑ‚ÑÑ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ»ÑĞ´ÑŒĞ¼Ğ¸.'}, 'en': {'title': 'Guarding Against Bad Votes: Ensuring Trustworthy LLM Rankings', 'desc': 'This paper examines the challenges of maintaining high-quality annotations in community-driven platforms that evaluate large language models (LLMs). It identifies three main sources of poor annotations: apathetic users who lack motivation to provide accurate feedback, adversarial users who intentionally skew results, and other unspecified factors. The authors reveal that even a small percentage of low-quality votes can significantly impact the rankings of models on leaderboards. The paper concludes by highlighting the ongoing challenges in ensuring reliable human annotations for accurate model evaluation.'}, 'zh': {'title': 'ç¡®ä¿é«˜è´¨é‡æ³¨é‡Šï¼Œæå‡æ¨¡å‹æ’åçš„ä¿¡ä»»åº¦', 'desc': 'æœ¬æ–‡æ¢è®¨äº†å¼€æ”¾ç¤¾åŒºå¹³å°ï¼ˆå¦‚Chatbot Arenaï¼‰åœ¨æ”¶é›†ç”¨æˆ·åå¥½æ•°æ®æ—¶çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬å‘ç°ï¼Œä½è´¨é‡çš„æ³¨é‡Šæ¥æºä¸»è¦æœ‰ä¸‰ç§ï¼ŒåŒ…æ‹¬å†·æ¼ çš„ç”¨æˆ·å’Œæ¶æ„è¡Œä¸ºè€…ï¼Œè¿™äº›éƒ½å¯èƒ½å½±å“å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ’åã€‚ç ”ç©¶è¡¨æ˜ï¼Œä»…10%çš„ä½è´¨é‡æŠ•ç¥¨å°±èƒ½ä½¿æ¨¡å‹åœ¨æ’è¡Œæ¦œä¸Šå˜åŒ–å¤šè¾¾5ä¸ªåæ¬¡ã€‚æœ€åï¼Œæ–‡ç« è®¨è®ºäº†ç¡®ä¿é«˜è´¨é‡äººç±»æ³¨é‡Šçš„å¼€æ”¾æ€§æŒ‘æˆ˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.01824', 'title': 'X-Prompt: Towards Universal In-Context Image Generation in Auto-Regressive Vision Language Foundation Models', 'url': 'https://huggingface.co/papers/2412.01824', 'abstract': "In-context generation is a key component of large language models' (LLMs) open-task generalization capability. By leveraging a few examples as context, LLMs can perform both in-domain and out-of-domain tasks. Recent advancements in auto-regressive vision-language models (VLMs) built upon LLMs have showcased impressive performance in text-to-image generation. However, the potential of in-context learning for general image generation tasks remains largely unexplored. To address this, we introduce X-Prompt, a purely auto-regressive large-vision language model designed to deliver competitive performance across a wide range of both seen and unseen image generation tasks, all within a unified in-context learning framework. X-Prompt incorporates a specialized design that efficiently compresses valuable features from in-context examples, supporting longer in-context token sequences and improving its ability to generalize to unseen tasks. A unified training task for both text and image prediction enables X-Prompt to handle general image generation with enhanced task awareness from in-context examples. Extensive experiments validate the model's performance across diverse seen image generation tasks and its capacity to generalize to previously unseen tasks.", 'score': 37, 'issue_id': 911, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 2', 'zh': '12æœˆ2æ—¥'}, 'hash': '6b7c2d6555d8df8d', 'authors': ['Zeyi Sun', 'Ziyang Chu', 'Pan Zhang', 'Tong Wu', 'Xiaoyi Dong', 'Yuhang Zang', 'Yuanjun Xiong', 'Dahua Lin', 'Jiaqi Wang'], 'affiliations': ['CPII under InnoHK', 'MThreads AI', 'Shanghai AI Laboratory', 'Shanghai Jiao Tong University', 'The Chinese University of Hong Kong', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2412.01824.jpg', 'data': {'categories': ['#agi', '#cv', '#games', '#optimization', '#training', '#multimodal'], 'emoji': 'ğŸ¨', 'ru': {'title': 'X-Prompt: ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ X-Prompt - Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ĞµĞ¹ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ ĞºĞ°Ğº Ğ·Ğ½Ğ°ĞºĞ¾Ğ¼Ñ‹Ğµ, Ñ‚Ğ°Ğº Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. X-Prompt ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑĞ¶Ğ¸Ğ¼Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ¸Ğ· ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ€Ğ°Ğ½ĞµĞµ Ğ½Ğµ Ğ²ÑÑ‚Ñ€ĞµÑ‡Ğ°Ğ²ÑˆĞ¸ĞµÑÑ.'}, 'en': {'title': 'X-Prompt: Unlocking Image Generation with In-Context Learning', 'desc': 'This paper presents X-Prompt, an advanced auto-regressive vision-language model that enhances image generation through in-context learning. By utilizing a few examples as context, X-Prompt can effectively generate images for both familiar and novel tasks. The model is designed to compress important features from these examples, allowing it to manage longer sequences of context and improve generalization. Extensive testing shows that X-Prompt performs well on a variety of image generation tasks, demonstrating its ability to adapt to new challenges.'}, 'zh': {'title': 'X-Promptï¼šæå‡å›¾åƒç”Ÿæˆçš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºX-Promptçš„è‡ªå›å½’å¤§è§„æ¨¡è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨æå‡å›¾åƒç”Ÿæˆä»»åŠ¡çš„è¡¨ç°ã€‚X-Prompté€šè¿‡åˆ©ç”¨ä¸Šä¸‹æ–‡ä¸­çš„ç¤ºä¾‹ï¼Œèƒ½å¤Ÿåœ¨å·²çŸ¥å’ŒæœªçŸ¥çš„å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­å®ç°ç«äº‰åŠ›çš„æ€§èƒ½ã€‚è¯¥æ¨¡å‹é‡‡ç”¨äº†ä¸“é—¨çš„è®¾è®¡ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå‹ç¼©ä¸Šä¸‹æ–‡ç¤ºä¾‹ä¸­çš„é‡è¦ç‰¹å¾ï¼Œä»è€Œæ”¯æŒæ›´é•¿çš„ä¸Šä¸‹æ–‡åºåˆ—å¹¶æé«˜å¯¹æœªçŸ¥ä»»åŠ¡çš„æ³›åŒ–èƒ½åŠ›ã€‚é€šè¿‡ç»Ÿä¸€çš„è®­ç»ƒä»»åŠ¡ï¼ŒX-Promptåœ¨æ–‡æœ¬å’Œå›¾åƒé¢„æµ‹æ–¹é¢è¡¨ç°å‡ºè‰²ï¼ŒéªŒè¯äº†å…¶åœ¨å¤šæ ·åŒ–å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.00131', 'title': 'Open-Sora Plan: Open-Source Large Video Generation Model', 'url': 'https://huggingface.co/papers/2412.00131', 'abstract': 'We introduce Open-Sora Plan, an open-source project that aims to contribute a large generation model for generating desired high-resolution videos with long durations based on various user inputs. Our project comprises multiple components for the entire video generation process, including a Wavelet-Flow Variational Autoencoder, a Joint Image-Video Skiparse Denoiser, and various condition controllers. Moreover, many assistant strategies for efficient training and inference are designed, and a multi-dimensional data curation pipeline is proposed for obtaining desired high-quality data. Benefiting from efficient thoughts, our Open-Sora Plan achieves impressive video generation results in both qualitative and quantitative evaluations. We hope our careful design and practical experience can inspire the video generation research community. All our codes and model weights are publicly available at https://github.com/PKU-YuanGroup/Open-Sora-Plan.', 'score': 19, 'issue_id': 911, 'pub_date': '2024-11-28', 'pub_date_card': {'ru': '28 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 28', 'zh': '11æœˆ28æ—¥'}, 'hash': 'fa9b0009de797ca2', 'authors': ['Bin Lin', 'Yunyang Ge', 'Xinhua Cheng', 'Zongjian Li', 'Bin Zhu', 'Shaodong Wang', 'Xianyi He', 'Yang Ye', 'Shenghai Yuan', 'Liuhan Chen', 'Tanghui Jia', 'Junwu Zhang', 'Zhenyu Tang', 'Yatian Pang', 'Bin She', 'Cen Yan', 'Zhiheng Hu', 'Xiaoyi Dong', 'Lin Chen', 'Zhang Pan', 'Xing Zhou', 'Shaoling Dong', 'Yonghong Tian', 'Li Yuan'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2412.00131.jpg', 'data': {'categories': ['#open_source', '#data', '#inference', '#training', '#video'], 'emoji': 'ğŸ¬', 'ru': {'title': 'ĞÑ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'ĞŸÑ€Ğ¾ĞµĞºÑ‚ Open-Sora Plan Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ğ²ĞµĞ¹Ğ²Ğ»ĞµÑ‚-Ğ¿Ğ¾Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€, ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ñ‹Ğ¹ Ğ´ĞµĞ½Ğ¾Ğ¹Ğ·ĞµÑ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ»ĞµÑ€Ñ‹ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğ¹. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ñ‹ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼ĞµÑ€Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ ĞºÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞŸÑ€Ğ¾ĞµĞºÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³ Ğ²Ğ¿ĞµÑ‡Ğ°Ñ‚Ğ»ÑÑÑ‰Ğ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ ĞºĞ°Ğº Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ² ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ñ†ĞµĞ½ĞºĞ°Ñ….'}, 'en': {'title': 'Empowering High-Resolution Video Generation with Open-Sora Plan', 'desc': 'The Open-Sora Plan is an open-source initiative focused on creating a large-scale generative model for producing high-resolution videos that can last for extended periods, tailored to user specifications. It integrates several advanced components, including a Wavelet-Flow Variational Autoencoder and a Joint Image-Video Skiparse Denoiser, to enhance the video generation process. The project also introduces various condition controllers and efficient training strategies, along with a multi-dimensional data curation pipeline to ensure high-quality output. Overall, the Open-Sora Plan demonstrates significant advancements in video generation, aiming to inspire further research in the field.'}, 'zh': {'title': 'å¼€æ”¾æºä»£ç ï¼Œç”Ÿæˆé«˜è´¨é‡è§†é¢‘çš„æœªæ¥', 'desc': 'Open-Soraè®¡åˆ’æ˜¯ä¸€ä¸ªå¼€æºé¡¹ç›®ï¼Œæ—¨åœ¨åŸºäºç”¨æˆ·è¾“å…¥ç”Ÿæˆé«˜åˆ†è¾¨ç‡çš„é•¿æ—¶è§†é¢‘ã€‚è¯¥é¡¹ç›®åŒ…å«å¤šä¸ªç»„ä»¶ï¼Œå¦‚å°æ³¢æµå˜åˆ†è‡ªç¼–ç å™¨å’Œè”åˆå›¾åƒ-è§†é¢‘å»å™ªå™¨ï¼Œæ”¯æŒæ•´ä¸ªè§†é¢‘ç”Ÿæˆè¿‡ç¨‹ã€‚æˆ‘ä»¬è¿˜è®¾è®¡äº†å¤šç§é«˜æ•ˆçš„è®­ç»ƒå’Œæ¨ç†ç­–ç•¥ï¼Œå¹¶æå‡ºäº†å¤šç»´æ•°æ®ç­–åˆ’ç®¡é“ï¼Œä»¥è·å–é«˜è´¨é‡æ•°æ®ã€‚é€šè¿‡è¿™äº›é«˜æ•ˆçš„è®¾è®¡ï¼ŒOpen-Soraè®¡åˆ’åœ¨è§†é¢‘ç”Ÿæˆçš„å®šæ€§å’Œå®šé‡è¯„ä¼°ä¸­å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.00927', 'title': 'VISTA: Enhancing Long-Duration and High-Resolution Video Understanding by Video Spatiotemporal Augmentation', 'url': 'https://huggingface.co/papers/2412.00927', 'abstract': 'Current large multimodal models (LMMs) face significant challenges in processing and comprehending long-duration or high-resolution videos, which is mainly due to the lack of high-quality datasets. To address this issue from a data-centric perspective, we propose VISTA, a simple yet effective Video Spatiotemporal Augmentation framework that synthesizes long-duration and high-resolution video instruction-following pairs from existing video-caption datasets. VISTA spatially and temporally combines videos to create new synthetic videos with extended durations and enhanced resolutions, and subsequently produces question-answer pairs pertaining to these newly synthesized videos. Based on this paradigm, we develop seven video augmentation methods and curate VISTA-400K, a video instruction-following dataset aimed at enhancing long-duration and high-resolution video understanding. Finetuning various video LMMs on our data resulted in an average improvement of 3.3% across four challenging benchmarks for long-video understanding. Furthermore, we introduce the first comprehensive high-resolution video understanding benchmark HRVideoBench, on which our finetuned models achieve a 6.5% performance gain. These results highlight the effectiveness of our framework.', 'score': 17, 'issue_id': 912, 'pub_date': '2024-12-01', 'pub_date_card': {'ru': '1 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 1', 'zh': '12æœˆ1æ—¥'}, 'hash': 'b4065e6e554dea31', 'authors': ['Weiming Ren', 'Huan Yang', 'Jie Min', 'Cong Wei', 'Wenhu Chen'], 'affiliations': ['University of Waterloo', 'Vector Institute'], 'pdf_title_img': 'assets/pdf/title_img/2412.00927.jpg', 'data': {'categories': ['#multimodal', '#video', '#dataset', '#data', '#long_context', '#benchmark', '#synthetic'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Ğ¡Ğ¸Ğ½Ñ‚ĞµĞ· Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ˜Ğ˜ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'VISTA - ÑÑ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. ĞĞ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒÑ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾, Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ€Ñ‹ Ğº Ğ½Ğ¸Ğ¼. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ VISTA ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ VISTA-400K, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ» ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° 3.3% Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº HRVideoBench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Enhancing Video Understanding with VISTA: A Data-Centric Approach', 'desc': "This paper introduces VISTA, a framework designed to improve the understanding of long-duration and high-resolution videos by creating synthetic video instruction-following pairs. VISTA utilizes existing video-caption datasets to spatially and temporally augment videos, generating new high-quality video data. The authors developed seven augmentation methods and created the VISTA-400K dataset, which significantly enhances the training of large multimodal models (LMMs). The results show that finetuning LMMs on this dataset leads to notable performance improvements on various benchmarks, demonstrating the framework's effectiveness in video comprehension tasks."}, 'zh': {'title': 'VISTAï¼šæå‡è§†é¢‘ç†è§£çš„æ–°æ–¹æ³•', 'desc': 'å½“å‰çš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨å¤„ç†é•¿æ—¶é•¿æˆ–é«˜åˆ†è¾¨ç‡è§†é¢‘æ—¶é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼Œä¸»è¦æ˜¯ç”±äºç¼ºä¹é«˜è´¨é‡çš„æ•°æ®é›†ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†VISTAï¼Œä¸€ä¸ªç®€å•è€Œæœ‰æ•ˆçš„è§†é¢‘æ—¶ç©ºå¢å¼ºæ¡†æ¶ï¼Œèƒ½å¤Ÿä»ç°æœ‰çš„è§†é¢‘-å­—å¹•æ•°æ®é›†ä¸­åˆæˆé•¿æ—¶é•¿å’Œé«˜åˆ†è¾¨ç‡çš„è§†é¢‘æŒ‡ä»¤å¯¹ã€‚VISTAé€šè¿‡ç©ºé—´å’Œæ—¶é—´çš„ç»„åˆï¼Œåˆ›å»ºæ–°çš„åˆæˆè§†é¢‘ï¼Œå¹¶ç”Ÿæˆä¸è¿™äº›æ–°åˆæˆè§†é¢‘ç›¸å…³çš„é—®é¢˜-ç­”æ¡ˆå¯¹ã€‚é€šè¿‡åœ¨æˆ‘ä»¬çš„æ•°æ®ä¸Šå¾®è°ƒå„ç§è§†é¢‘å¤šæ¨¡æ€æ¨¡å‹ï¼Œå¹³å‡æé«˜äº†3.3%çš„æ€§èƒ½ï¼Œè¿›ä¸€æ­¥éªŒè¯äº†æˆ‘ä»¬æ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.18499', 'title': 'GATE OpenING: A Comprehensive Benchmark for Judging Open-ended Interleaved Image-Text Generation', 'url': 'https://huggingface.co/papers/2411.18499', 'abstract': 'Multimodal Large Language Models (MLLMs) have made significant strides in visual understanding and generation tasks. However, generating interleaved image-text content remains a challenge, which requires integrated multimodal understanding and generation abilities. While the progress in unified models offers new solutions, existing benchmarks are insufficient for evaluating these methods due to data size and diversity limitations. To bridge this gap, we introduce GATE OpenING (OpenING), a comprehensive benchmark comprising 5,400 high-quality human-annotated instances across 56 real-world tasks. OpenING covers diverse daily scenarios such as travel guide, design, and brainstorming, offering a robust platform for challenging interleaved generation methods. In addition, we present IntJudge, a judge model for evaluating open-ended multimodal generation methods. Trained with a novel data pipeline, our IntJudge achieves an agreement rate of 82. 42% with human judgments, outperforming GPT-based evaluators by 11.34%. Extensive experiments on OpenING reveal that current interleaved generation methods still have substantial room for improvement. Key findings on interleaved image-text generation are further presented to guide the development of next-generation models. The OpenING is open-sourced at https://opening.github.io.', 'score': 16, 'issue_id': 914, 'pub_date': '2024-11-27', 'pub_date_card': {'ru': '27 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 27', 'zh': '11æœˆ27æ—¥'}, 'hash': 'bec1bf0079934886', 'authors': ['Pengfei Zhou', 'Xiaopeng Peng', 'Jiajun Song', 'Chuanhao Li', 'Zhaopan Xu', 'Yue Yang', 'Ziyao Guo', 'Hao Zhang', 'Yuqi Lin', 'Yefei He', 'Lirui Zhao', 'Shuo Liu', 'Tianhua Li', 'Yuxuan Xie', 'Xiaojun Chang', 'Yu Qiao', 'Wenqi Shao', 'Kaipeng Zhang'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2411.18499.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#open_source', '#dataset', '#games'], 'emoji': 'ğŸ”€', 'ru': {'title': 'ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… GATE OpenING Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ÑÑ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾-Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°. OpenING Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 5400 Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¸Ğ· 56 Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ IntJudge Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ Ñ‚Ğ¸Ğ¿Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ GPT-based Ğ¾Ñ†ĞµĞ½Ñ‰Ğ¸ĞºĞ¾Ğ² Ğ½Ğ° 11.34%. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ÑÑ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ¸Ğ¼ĞµÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Bridging the Gap in Multimodal Generation with OpenING!', 'desc': 'This paper discusses the advancements in Multimodal Large Language Models (MLLMs) for tasks that involve both images and text. It highlights the challenges of generating content that combines these two modalities effectively, which requires a deep understanding of both. To address the limitations of existing benchmarks, the authors introduce GATE OpenING, a new dataset with 5,400 annotated examples across 56 tasks that reflect real-world scenarios. Additionally, they present IntJudge, a model designed to evaluate multimodal generation methods, which shows improved agreement with human judgments compared to previous evaluators.'}, 'zh': {'title': 'æ¨åŠ¨å¤šæ¨¡æ€ç”Ÿæˆçš„åŸºå‡†ä¸è¯„ä¼°', 'desc': 'å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†è§‰ç†è§£å’Œç”Ÿæˆä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œç”Ÿæˆäº¤é”™çš„å›¾åƒ-æ–‡æœ¬å†…å®¹ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œè¿™éœ€è¦ç»¼åˆçš„å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†GATE OpenINGï¼ˆOpenINGï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«5400ä¸ªé«˜è´¨é‡äººç±»æ ‡æ³¨å®ä¾‹çš„ç»¼åˆåŸºå‡†ï¼Œæ¶µç›–56ä¸ªçœŸå®ä¸–ç•Œä»»åŠ¡ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¿˜æå‡ºäº†IntJudgeï¼Œä¸€ä¸ªç”¨äºè¯„ä¼°å¼€æ”¾å¼å¤šæ¨¡æ€ç”Ÿæˆæ–¹æ³•çš„è¯„ä¼°æ¨¡å‹ï¼Œæ˜¾ç¤ºå‡ºä¸äººç±»åˆ¤æ–­çš„é«˜ä¸€è‡´æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.01819', 'title': 'Switti: Designing Scale-Wise Transformers for Text-to-Image Synthesis', 'url': 'https://huggingface.co/papers/2412.01819', 'abstract': 'This work presents Switti, a scale-wise transformer for text-to-image generation. Starting from existing next-scale prediction AR models, we first explore them for T2I generation and propose architectural modifications to improve their convergence and overall performance. We then observe that self-attention maps of our pretrained scale-wise AR model exhibit weak dependence on preceding scales. Based on this insight, we propose a non-AR counterpart facilitating {sim}11% faster sampling and lower memory usage while also achieving slightly better generation quality.Furthermore, we reveal that classifier-free guidance at high-resolution scales is often unnecessary and can even degrade performance. %may be not only unnecessary but potentially detrimental. By disabling guidance at these scales, we achieve an additional sampling acceleration of {sim}20% and improve the generation of fine-grained details. Extensive human preference studies and automated evaluations show that Switti outperforms existing T2I AR models and competes with state-of-the-art T2I diffusion models while being up to 7{times} faster.', 'score': 15, 'issue_id': 914, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 2', 'zh': '12æœˆ2æ—¥'}, 'hash': '25cf4512f592aea4', 'authors': ['Anton Voronov', 'Denis Kuznedelev', 'Mikhail Khoroshikh', 'Valentin Khrulkov', 'Dmitry Baranchuk'], 'affiliations': ['HSE University', 'MIPT', 'Skoltech', 'Yandex Research'], 'pdf_title_img': 'assets/pdf/title_img/2412.01819.jpg', 'data': {'categories': ['#diffusion', '#architecture', '#video', '#cv', '#optimization'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'Switti: Ğ±Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¹ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Switti - Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ñ Ğ¾ ÑĞ»Ğ°Ğ±Ğ¾Ğ¹ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ ĞºĞ°Ñ€Ñ‚ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¾Ğ², Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° Ğ½ĞµĞ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ²ĞµÑ€ÑĞ¸Ñ, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‰Ğ°Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¸ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ñ‚ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ guidance Ğ½Ğ° Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸ÑÑ… Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ.'}, 'en': {'title': 'Switti: Accelerating Text-to-Image Generation with Scale-Wise Transformers', 'desc': 'This paper introduces Switti, a new transformer model designed for generating images from text. It builds on existing autoregressive (AR) models by making architectural changes that enhance performance and speed. The authors find that the self-attention mechanisms in their model do not rely heavily on previous scales, allowing for a non-autoregressive approach that speeds up sampling and reduces memory usage. Additionally, they demonstrate that removing classifier-free guidance at higher resolutions can improve detail generation and further accelerate the process, leading to a model that is significantly faster than current state-of-the-art methods.'}, 'zh': {'title': 'Swittiï¼šåŠ é€Ÿæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„å˜æ¢å™¨', 'desc': 'æœ¬æ–‡ä»‹ç»äº†Swittiï¼Œä¸€ç§ç”¨äºæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„å°ºåº¦å˜æ¢å™¨ã€‚æˆ‘ä»¬ä»ç°æœ‰çš„ä¸‹ä¸€å°ºåº¦é¢„æµ‹è‡ªå›å½’æ¨¡å‹å‡ºå‘ï¼Œæ¢ç´¢å…¶åœ¨T2Iç”Ÿæˆä¸­çš„åº”ç”¨ï¼Œå¹¶æå‡ºæ¶æ„ä¿®æ”¹ä»¥æé«˜æ”¶æ•›æ€§å’Œæ•´ä½“æ€§èƒ½ã€‚ç ”ç©¶å‘ç°ï¼Œæˆ‘ä»¬çš„é¢„è®­ç»ƒå°ºåº¦è‡ªå›å½’æ¨¡å‹çš„è‡ªæ³¨æ„åŠ›å›¾å¯¹å‰ä¸€å°ºåº¦çš„ä¾èµ–æ€§è¾ƒå¼±ï¼Œå› æ­¤æˆ‘ä»¬æå‡ºäº†ä¸€ç§éè‡ªå›å½’çš„æ›¿ä»£æ–¹æ¡ˆï¼Œèƒ½å¤Ÿå®ç°çº¦11%çš„é‡‡æ ·åŠ é€Ÿå’Œæ›´ä½çš„å†…å­˜ä½¿ç”¨ï¼ŒåŒæ—¶ç”Ÿæˆè´¨é‡ç•¥æœ‰æå‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°é«˜åˆ†è¾¨ç‡å°ºåº¦ä¸‹çš„æ— åˆ†ç±»å™¨å¼•å¯¼é€šå¸¸æ˜¯ä¸å¿…è¦çš„ï¼Œç”šè‡³å¯èƒ½ä¼šé™ä½æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.00154', 'title': 'o1-Coder: an o1 Replication for Coding', 'url': 'https://huggingface.co/papers/2412.00154', 'abstract': "The technical report introduces O1-CODER, an attempt to replicate OpenAI's o1 model with a focus on coding tasks. It integrates reinforcement learning (RL) and Monte Carlo Tree Search (MCTS) to enhance the model's System-2 thinking capabilities. The framework includes training a Test Case Generator (TCG) for standardized code testing, using MCTS to generate code data with reasoning processes, and iteratively fine-tuning the policy model to initially produce pseudocode, followed by the generation of the full code. The report also addresses the opportunities and challenges in deploying o1-like models in real-world applications, suggesting transitioning to the System-2 paradigm and highlighting the imperative for environment state updates. Updated model progress and experimental results will be reported in subsequent versions. All source code, curated datasets, as well as the derived models will be disclosed at https://github.com/ADaM-BJTU/O1-CODER .", 'score': 14, 'issue_id': 912, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 29', 'zh': '11æœˆ29æ—¥'}, 'hash': '4a9b72e0b9a6ba64', 'authors': ['Yuxiang Zhang', 'Shangxi Wu', 'Yuqi Yang', 'Jiangming Shu', 'Jinlin Xiao', 'Chao Kong', 'Jitao Sang'], 'affiliations': ['School of Computer Science and Technology, Beijing Jiaotong University, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2412.00154.jpg', 'data': {'categories': ['#dataset', '#training', '#open_source', '#optimization', '#reasoning', '#rl'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'O1-CODER: Ğ£ÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ Ğ˜Ğ˜ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ñƒ-2 Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¢ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¾Ñ‚Ñ‡ĞµÑ‚ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ O1-CODER - Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚ĞºÑƒ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ o1 Ğ¾Ñ‚ OpenAI Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ ĞœĞ¾Ğ½Ñ‚Ğµ-ĞšĞ°Ñ€Ğ»Ğ¾ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ñ‹-2. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ° Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ÑĞ»ÑƒÑ‡Ğ°ĞµĞ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ MCTS Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ° Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°Ğ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¸ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ´Ğ¾Ğ²Ğ¾Ğ´ĞºÑƒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸. ĞÑ‚Ñ‡ĞµÑ‚ Ñ‚Ğ°ĞºĞ¶Ğµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ñ‹Ñ… o1 Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ÑÑ….'}, 'en': {'title': 'Enhancing Coding with O1-CODER: A New Approach to System-2 Thinking', 'desc': "The paper presents O1-CODER, a model designed to replicate OpenAI's o1 specifically for coding tasks. It combines reinforcement learning (RL) with Monte Carlo Tree Search (MCTS) to improve the model's ability to think critically and reason through problems. The framework includes a Test Case Generator (TCG) for consistent code testing and uses MCTS to create code data that incorporates reasoning steps. The report discusses the potential and challenges of applying o1-like models in practical scenarios, emphasizing the need for updates in the environment state during the learning process."}, 'zh': {'title': 'O1-CODERï¼šæå‡ç¼–ç ä»»åŠ¡çš„æ™ºèƒ½æ¨¡å‹', 'desc': 'æœ¬æ–‡ä»‹ç»äº†O1-CODERï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨å¤åˆ¶OpenAIçš„o1æ¨¡å‹ï¼Œä¸“æ³¨äºç¼–ç ä»»åŠ¡çš„æŠ€æœ¯æŠ¥å‘Šã€‚è¯¥æ¨¡å‹ç»“åˆäº†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å’Œè’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰ï¼Œä»¥å¢å¼ºå…¶ç³»ç»Ÿ2æ€ç»´èƒ½åŠ›ã€‚æ¡†æ¶ä¸­åŒ…æ‹¬è®­ç»ƒæµ‹è¯•ç”¨ä¾‹ç”Ÿæˆå™¨ï¼ˆTCGï¼‰ä»¥è¿›è¡Œæ ‡å‡†åŒ–ä»£ç æµ‹è¯•ï¼Œåˆ©ç”¨MCTSç”Ÿæˆå¸¦æœ‰æ¨ç†è¿‡ç¨‹çš„ä»£ç æ•°æ®ï¼Œå¹¶è¿­ä»£å¾®è°ƒç­–ç•¥æ¨¡å‹ï¼Œåˆæ­¥ç”Ÿæˆä¼ªä»£ç ï¼Œéšåç”Ÿæˆå®Œæ•´ä»£ç ã€‚æŠ¥å‘Šè¿˜è®¨è®ºäº†åœ¨å®é™…åº”ç”¨ä¸­éƒ¨ç½²ç±»ä¼¼o1æ¨¡å‹çš„æœºé‡å’ŒæŒ‘æˆ˜ï¼Œå»ºè®®è½¬å‘ç³»ç»Ÿ2èŒƒå¼ï¼Œå¹¶å¼ºè°ƒç¯å¢ƒçŠ¶æ€æ›´æ–°çš„é‡è¦æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.18671', 'title': 'TAPTRv3: Spatial and Temporal Context Foster Robust Tracking of Any Point in Long Video', 'url': 'https://huggingface.co/papers/2411.18671', 'abstract': 'In this paper, we present TAPTRv3, which is built upon TAPTRv2 to improve its point tracking robustness in long videos. TAPTRv2 is a simple DETR-like framework that can accurately track any point in real-world videos without requiring cost-volume. TAPTRv3 improves TAPTRv2 by addressing its shortage in querying high quality features from long videos, where the target tracking points normally undergo increasing variation over time. In TAPTRv3, we propose to utilize both spatial and temporal context to bring better feature querying along the spatial and temporal dimensions for more robust tracking in long videos. For better spatial feature querying, we present Context-aware Cross-Attention (CCA), which leverages surrounding spatial context to enhance the quality of attention scores when querying image features. For better temporal feature querying, we introduce Visibility-aware Long-Temporal Attention (VLTA) to conduct temporal attention to all past frames while considering their corresponding visibilities, which effectively addresses the feature drifting problem in TAPTRv2 brought by its RNN-like long-temporal modeling. TAPTRv3 surpasses TAPTRv2 by a large margin on most of the challenging datasets and obtains state-of-the-art performance. Even when compared with methods trained with large-scale extra internal data, TAPTRv3 is still competitive.', 'score': 13, 'issue_id': 909, 'pub_date': '2024-11-27', 'pub_date_card': {'ru': '27 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 27', 'zh': '11æœˆ27æ—¥'}, 'hash': 'f99e1015e9222dc6', 'authors': ['Jinyuan Qu', 'Hongyang Li', 'Shilong Liu', 'Tianhe Ren', 'Zhaoyang Zeng', 'Lei Zhang'], 'affiliations': ['International Digital Economy Academy (IDEA)', 'South China University of Technology', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2411.18671.jpg', 'data': {'categories': ['#long_context', '#benchmark', '#video', '#cv', '#optimization'], 'emoji': 'ğŸ‘ï¸', 'ru': {'title': 'ĞšĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡ĞµĞº Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'TAPTRv3 - ÑÑ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ°Ñ Ğ²ĞµÑ€ÑĞ¸Ñ TAPTRv2 Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡ĞµĞº Ğ² Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ° Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ². Ğ’Ğ²ĞµĞ´ĞµĞ½Ñ‹ Ğ´Ğ²Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ°: Context-aware Cross-Attention (CCA) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ° Ğ¸ Visibility-aware Long-Temporal Attention (VLTA) Ğ´Ğ»Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°. TAPTRv3 Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰ÑƒÑ Ğ²ĞµÑ€ÑĞ¸Ñ Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Enhancing Video Point Tracking with TAPTRv3', 'desc': 'TAPTRv3 is an advanced framework designed to enhance point tracking in lengthy videos, building on the previous version, TAPTRv2. It improves the ability to query high-quality features by incorporating both spatial and temporal contexts, which helps in maintaining robust tracking despite variations over time. The paper introduces Context-aware Cross-Attention (CCA) for better spatial feature querying and Visibility-aware Long-Temporal Attention (VLTA) for improved temporal feature querying, effectively addressing issues like feature drifting. TAPTRv3 demonstrates significant performance improvements over TAPTRv2 and competes well against other state-of-the-art methods, even those trained on larger datasets.'}, 'zh': {'title': 'TAPTRv3ï¼šé•¿è§†é¢‘ç‚¹è·Ÿè¸ªçš„æ–°çªç ´', 'desc': 'æœ¬æ–‡ä»‹ç»äº†TAPTRv3ï¼Œè¿™æ˜¯åœ¨TAPTRv2åŸºç¡€ä¸Šå¼€å‘çš„ï¼Œæ—¨åœ¨æé«˜é•¿è§†é¢‘ä¸­çš„ç‚¹è·Ÿè¸ªé²æ£’æ€§ã€‚TAPTRv2æ˜¯ä¸€ä¸ªç®€å•çš„ç±»ä¼¼DETRçš„æ¡†æ¶ï¼Œå¯ä»¥å‡†ç¡®è·Ÿè¸ªç°å®è§†é¢‘ä¸­çš„ä»»æ„ç‚¹ï¼Œè€Œæ— éœ€æˆæœ¬ä½“ç§¯ã€‚TAPTRv3é€šè¿‡åˆ©ç”¨ç©ºé—´å’Œæ—¶é—´ä¸Šä¸‹æ–‡æ¥æ”¹å–„ç‰¹å¾æŸ¥è¯¢ï¼Œä»è€Œåœ¨é•¿è§†é¢‘ä¸­å®ç°æ›´ç¨³å¥çš„è·Ÿè¸ªã€‚æˆ‘ä»¬æå‡ºäº†ä¸Šä¸‹æ–‡æ„ŸçŸ¥äº¤å‰æ³¨æ„åŠ›ï¼ˆCCAï¼‰å’Œå¯è§æ€§æ„ŸçŸ¥é•¿æ—¶é—´æ³¨æ„åŠ›ï¼ˆVLTAï¼‰ï¼Œæ˜¾è‘—æå‡äº†ç‰¹å¾æŸ¥è¯¢çš„è´¨é‡ï¼Œè¶…è¶Šäº†TAPTRv2ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.00100', 'title': 'Steering Rectified Flow Models in the Vector Field for Controlled Image Generation', 'url': 'https://huggingface.co/papers/2412.00100', 'abstract': 'Diffusion models (DMs) excel in photorealism, image editing, and solving inverse problems, aided by classifier-free guidance and image inversion techniques. However, rectified flow models (RFMs) remain underexplored for these tasks. Existing DM-based methods often require additional training, lack generalization to pretrained latent models, underperform, and demand significant computational resources due to extensive backpropagation through ODE solvers and inversion processes. In this work, we first develop a theoretical and empirical understanding of the vector field dynamics of RFMs in efficiently guiding the denoising trajectory. Our findings reveal that we can navigate the vector field in a deterministic and gradient-free manner. Utilizing this property, we propose FlowChef, which leverages the vector field to steer the denoising trajectory for controlled image generation tasks, facilitated by gradient skipping. FlowChef is a unified framework for controlled image generation that, for the first time, simultaneously addresses classifier guidance, linear inverse problems, and image editing without the need for extra training, inversion, or intensive backpropagation. Finally, we perform extensive evaluations and show that FlowChef significantly outperforms baselines in terms of performance, memory, and time requirements, achieving new state-of-the-art results. Project Page: https://flowchef.github.io.', 'score': 12, 'issue_id': 911, 'pub_date': '2024-11-27', 'pub_date_card': {'ru': '27 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 27', 'zh': '11æœˆ27æ—¥'}, 'hash': '5fceae277a0e3d85', 'authors': ['Maitreya Patel', 'Song Wen', 'Dimitris N. Metaxas', 'Yezhou Yang'], 'affiliations': ['Arizona State University', 'Rutgers University'], 'pdf_title_img': 'assets/pdf/title_img/2412.00100.jpg', 'data': {'categories': ['#dataset', '#cv', '#optimization', '#diffusion', '#benchmark'], 'emoji': 'ğŸŒŠ', 'ru': {'title': 'FlowChef: Ğ£Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ»ĞµĞ¼ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ FlowChef. ĞĞ½ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ»Ñ Ğ² Ñ€ĞµĞºÑ‚Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (RFM) Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ĞµĞ¹ ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ. FlowChef Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ñ… Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ»Ğ¸ Ğ¸Ğ½Ğ²ĞµÑ€ÑĞ¸Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ FlowChef Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑĞ¼ Ğº Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'FlowChef: Revolutionizing Controlled Image Generation with Efficiency', 'desc': 'This paper introduces FlowChef, a new framework that enhances controlled image generation using rectified flow models (RFMs). Unlike traditional diffusion models, FlowChef efficiently guides the denoising process without requiring additional training or extensive computational resources. It utilizes a unique property of RFMs to navigate the vector field in a deterministic way, allowing for effective classifier guidance and image editing. The results demonstrate that FlowChef outperforms existing methods in performance, memory usage, and processing time, setting new benchmarks in the field.'}, 'zh': {'title': 'FlowChefï¼šé«˜æ•ˆçš„å—æ§å›¾åƒç”Ÿæˆæ–°æ–¹æ³•', 'desc': 'æ‰©æ•£æ¨¡å‹åœ¨ç”ŸæˆçœŸå®æ„Ÿå›¾åƒã€å›¾åƒç¼–è¾‘å’Œè§£å†³é€†é—®é¢˜æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†ä¿®æ­£æµæ¨¡å‹åœ¨è¿™äº›ä»»åŠ¡ä¸­ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ç°æœ‰çš„åŸºäºæ‰©æ•£æ¨¡å‹çš„æ–¹æ³•é€šå¸¸éœ€è¦é¢å¤–çš„è®­ç»ƒï¼Œç¼ºä¹å¯¹é¢„è®­ç»ƒæ½œåœ¨æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶ä¸”åœ¨æ€§èƒ½ä¸Šè¡¨ç°ä¸ä½³ï¼Œè®¡ç®—èµ„æºæ¶ˆè€—å¤§ã€‚æœ¬æ–‡æå‡ºFlowChefï¼Œé€šè¿‡æœ‰æ•ˆå¼•å¯¼å»å™ªè½¨è¿¹ï¼Œåˆ©ç”¨å‘é‡åœºçš„ç‰¹æ€§ï¼Œå®ç°äº†å—æ§å›¾åƒç”Ÿæˆä»»åŠ¡ï¼Œä¸”æ— éœ€é¢å¤–è®­ç»ƒæˆ–å¤æ‚çš„åå‘ä¼ æ’­ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFlowChefåœ¨æ€§èƒ½ã€å†…å­˜å’Œæ—¶é—´éœ€æ±‚ä¸Šæ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œè¾¾åˆ°äº†æ–°çš„æœ€å…ˆè¿›ç»“æœã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.01199', 'title': 'TinyFusion: Diffusion Transformers Learned Shallow', 'url': 'https://huggingface.co/papers/2412.01199', 'abstract': 'Diffusion Transformers have demonstrated remarkable capabilities in image generation but often come with excessive parameterization, resulting in considerable inference overhead in real-world applications. In this work, we present TinyFusion, a depth pruning method designed to remove redundant layers from diffusion transformers via end-to-end learning. The core principle of our approach is to create a pruned model with high recoverability, allowing it to regain strong performance after fine-tuning. To accomplish this, we introduce a differentiable sampling technique to make pruning learnable, paired with a co-optimized parameter to simulate future fine-tuning. While prior works focus on minimizing loss or error after pruning, our method explicitly models and optimizes the post-fine-tuning performance of pruned models. Experimental results indicate that this learnable paradigm offers substantial benefits for layer pruning of diffusion transformers, surpassing existing importance-based and error-based methods. Additionally, TinyFusion exhibits strong generalization across diverse architectures, such as DiTs, MARs, and SiTs. Experiments with DiT-XL show that TinyFusion can craft a shallow diffusion transformer at less than 7% of the pre-training cost, achieving a 2times speedup with an FID score of 2.86, outperforming competitors with comparable efficiency. Code is available at https://github.com/VainF/TinyFusion.', 'score': 11, 'issue_id': 912, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 2', 'zh': '12æœˆ2æ—¥'}, 'hash': '7d1de7010c3fabd7', 'authors': ['Gongfan Fang', 'Kunjun Li', 'Xinyin Ma', 'Xinchao Wang'], 'affiliations': ['National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2412.01199.jpg', 'data': {'categories': ['#inference', '#diffusion', '#training', '#optimization', '#architecture'], 'emoji': 'âœ‚ï¸', 'ru': {'title': 'TinyFusion: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¾Ğ±Ñ€ĞµĞ·ĞºĞ° Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°', 'desc': 'TinyFusion - ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±Ñ€ĞµĞ·ĞºĞ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ´Ğ»Ñ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°Ñ…. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ñ€ÑƒĞµĞ¼ÑƒÑ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ğ¾Ğ¹ Ğ¾Ğ±Ñ€ĞµĞ·ĞºĞ¸ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ÑĞ»Ğµ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. TinyFusion Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ğ±Ñ€ĞµĞ·ĞºĞ¸ Ğ¸ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ Ñ Ğ´Ğ²ÑƒĞºÑ€Ğ°Ñ‚Ğ½Ñ‹Ğ¼ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'TinyFusion: Efficient Layer Pruning for Fast and Effective Diffusion Transformers', 'desc': 'This paper introduces TinyFusion, a method for optimizing diffusion transformers by removing unnecessary layers through a process called depth pruning. The approach focuses on maintaining high performance after the model is pruned, using a learnable technique that allows the model to adapt during fine-tuning. Unlike previous methods that only aim to reduce loss or error, TinyFusion specifically targets the performance of the pruned model post-fine-tuning. Experimental results show that TinyFusion significantly improves layer pruning efficiency and generalization across various architectures, achieving faster inference times and better performance metrics.'}, 'zh': {'title': 'TinyFusionï¼šé«˜æ•ˆå‰ªæï¼Œæå‡æ‰©æ•£å˜æ¢å™¨æ€§èƒ½', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºTinyFusionçš„æ·±åº¦å‰ªææ–¹æ³•ï¼Œæ—¨åœ¨å‡å°‘æ‰©æ•£å˜æ¢å™¨ä¸­çš„å†—ä½™å±‚ï¼Œä»è€Œé™ä½æ¨ç†å¼€é”€ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡ç«¯åˆ°ç«¯å­¦ä¹ å®ç°å‰ªæï¼Œå¹¶ç¡®ä¿å‰ªæåçš„æ¨¡å‹åœ¨å¾®è°ƒåèƒ½å¤Ÿæ¢å¤å¼ºå¤§çš„æ€§èƒ½ã€‚TinyFusionå¼•å…¥äº†ä¸€ç§å¯å¾®åˆ†é‡‡æ ·æŠ€æœ¯ï¼Œä½¿å¾—å‰ªæè¿‡ç¨‹å¯å­¦ä¹ ï¼Œå¹¶ä¸å…±åŒä¼˜åŒ–çš„å‚æ•°ç»“åˆï¼Œä»¥æ¨¡æ‹Ÿæœªæ¥çš„å¾®è°ƒæ•ˆæœã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTinyFusionåœ¨æ‰©æ•£å˜æ¢å™¨çš„å±‚å‰ªææ–¹é¢ä¼˜äºç°æœ‰çš„æ–¹æ³•ï¼Œå±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.00568', 'title': 'The Well: a Large-Scale Collection of Diverse Physics Simulations for Machine Learning', 'url': 'https://huggingface.co/papers/2412.00568', 'abstract': 'Machine learning based surrogate models offer researchers powerful tools for accelerating simulation-based workflows. However, as standard datasets in this space often cover small classes of physical behavior, it can be difficult to evaluate the efficacy of new approaches. To address this gap, we introduce the Well: a large-scale collection of datasets containing numerical simulations of a wide variety of spatiotemporal physical systems. The Well draws from domain experts and numerical software developers to provide 15TB of data across 16 datasets covering diverse domains such as biological systems, fluid dynamics, acoustic scattering, as well as magneto-hydrodynamic simulations of extra-galactic fluids or supernova explosions. These datasets can be used individually or as part of a broader benchmark suite. To facilitate usage of the Well, we provide a unified PyTorch interface for training and evaluating models. We demonstrate the function of this library by introducing example baselines that highlight the new challenges posed by the complex dynamics of the Well. The code and data is available at https://github.com/PolymathicAI/the_well.', 'score': 10, 'issue_id': 923, 'pub_date': '2024-11-30', 'pub_date_card': {'ru': '30 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 30', 'zh': '11æœˆ30æ—¥'}, 'hash': '1352e219e54ac56e', 'authors': ['Ruben Ohana', 'Michael McCabe', 'Lucas Meyer', 'Rudy Morel', 'Fruzsina J. Agocs', 'Miguel Beneitez', 'Marsha Berger', 'Blakesley Burkhart', 'Stuart B. Dalziel', 'Drummond B. Fielding', 'Daniel Fortunato', 'Jared A. Goldberg', 'Keiya Hirashima', 'Yan-Fei Jiang', 'Rich R. Kerswell', 'Suryanarayana Maddu', 'Jonah Miller', 'Payel Mukhopadhyay', 'Stefan S. Nixon', 'Jeff Shen', 'Romain Watteaux', 'Bruno RÃ©galdo-Saint Blancard', 'FranÃ§ois Rozet', 'Liam H. Parker', 'Miles Cranmer', 'Shirley Ho'], 'affiliations': ['CEA DAM', 'Cornell University', 'Flatiron Institute', 'Los Alamos National Laboratory', 'New York University', 'Polymathic AI', 'Princeton University', 'Rutgers University', 'University of California, Berkeley', 'University of Cambridge', 'University of Colorado, Boulder', 'University of LiÃ¨ge', 'University of Tokyo'], 'pdf_title_img': 'assets/pdf/title_img/2412.00568.jpg', 'data': {'categories': ['#data', '#dataset', '#benchmark'], 'emoji': 'ğŸŒŠ', 'ru': {'title': 'Well: Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ ÑÑƒÑ€Ñ€Ğ¾Ğ³Ğ°Ñ‚Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸', 'desc': "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ 'Well' Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑƒÑ€Ñ€Ğ¾Ğ³Ğ°Ñ‚Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. 'Well' ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 15 Ğ¢Ğ‘ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· 16 Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ², Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ñ„Ğ¸Ğ·Ğ¸ĞºĞ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ±Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹, Ğ³Ğ¸Ğ´Ñ€Ğ¾Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºÑƒ Ğ¸ Ğ°ÑÑ‚Ñ€Ğ¾Ñ„Ğ¸Ğ·Ğ¸ĞºÑƒ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹Ñ PyTorch Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° ÑÑ‚Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ²Ğ°Ğ½ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‡ÑŒ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼ Ğ² Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ², Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸ÑÑ…."}, 'en': {'title': 'Unlocking Complex Simulations with the Well: A New Dataset for Machine Learning', 'desc': 'This paper presents a large-scale collection of datasets called the Well, designed to enhance machine learning surrogate models for simulation-based workflows. The Well contains 15TB of data across 16 diverse datasets, covering various physical systems such as fluid dynamics and biological systems. By providing a unified PyTorch interface, the Well facilitates the training and evaluation of machine learning models on these complex datasets. The authors also introduce example baselines to demonstrate the unique challenges posed by the intricate dynamics represented in the Well.'}, 'zh': {'title': 'æœºå™¨å­¦ä¹ åŠ é€Ÿä»¿çœŸï¼šæ¢ç´¢"Well"æ•°æ®é›†', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºæœºå™¨å­¦ä¹ çš„æ›¿ä»£æ¨¡å‹ï¼Œæ—¨åœ¨åŠ é€ŸåŸºäºä»¿çœŸçš„å·¥ä½œæµç¨‹ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåä¸º"Well"çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼ŒåŒ…å«å¤šç§æ—¶ç©ºç‰©ç†ç³»ç»Ÿçš„æ•°å€¼ä»¿çœŸæ•°æ®ï¼Œæ€»è®¡15TBï¼Œæ¶µç›–ç”Ÿç‰©ç³»ç»Ÿã€æµä½“åŠ¨åŠ›å­¦ã€å£°æ•£å°„ç­‰å¤šä¸ªé¢†åŸŸã€‚è¯¥æ•°æ®é›†ä¸ºç ”ç©¶äººå‘˜æä¾›äº†ä¸°å¯Œçš„èµ„æºï¼Œä»¥è¯„ä¼°æ–°æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå¹¶å¯å•ç‹¬ä½¿ç”¨æˆ–ä½œä¸ºæ›´å¹¿æ³›åŸºå‡†å¥—ä»¶çš„ä¸€éƒ¨åˆ†ã€‚ä¸ºäº†æ–¹ä¾¿ä½¿ç”¨ï¼Œæˆ‘ä»¬æä¾›äº†ç»Ÿä¸€çš„PyTorchæ¥å£ï¼Œå¸®åŠ©è®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹ï¼Œå¹¶å±•ç¤ºäº†æ–°çš„æŒ‘æˆ˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.00174', 'title': 'SOLAMI: Social Vision-Language-Action Modeling for Immersive Interaction with 3D Autonomous Characters', 'url': 'https://huggingface.co/papers/2412.00174', 'abstract': "Human beings are social animals. How to equip 3D autonomous characters with similar social intelligence that can perceive, understand and interact with humans remains an open yet foundamental problem. In this paper, we introduce SOLAMI, the first end-to-end Social vision-Language-Action (VLA) Modeling framework for Immersive interaction with 3D autonomous characters. Specifically, SOLAMI builds 3D autonomous characters from three aspects: (1) Social VLA Architecture: We propose a unified social VLA framework to generate multimodal response (speech and motion) based on the user's multimodal input to drive the character for social interaction. (2) Interactive Multimodal Data: We present SynMSI, a synthetic multimodal social interaction dataset generated by an automatic pipeline using only existing motion datasets to address the issue of data scarcity. (3) Immersive VR Interface: We develop a VR interface that enables users to immersively interact with these characters driven by various architectures. Extensive quantitative experiments and user studies demonstrate that our framework leads to more precise and natural character responses (in both speech and motion) that align with user expectations with lower latency.", 'score': 10, 'issue_id': 913, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 29', 'zh': '11æœˆ29æ—¥'}, 'hash': '5a30d9c535229ab0', 'authors': ['Jianping Jiang', 'Weiye Xiao', 'Zhengyu Lin', 'Huaizhong Zhang', 'Tianxiang Ren', 'Yang Gao', 'Zhiqian Lin', 'Zhongang Cai', 'Lei Yang', 'Ziwei Liu'], 'affiliations': ['S-Lab, Nanyang Technological University', 'SenseTime Research'], 'pdf_title_img': 'assets/pdf/title_img/2412.00174.jpg', 'data': {'categories': ['#synthetic', '#games', '#dataset', '#agents', '#3d', '#multimodal'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ ÑƒĞ¼Ğ½Ñ‹Ñ… 3D Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹ Ğ´Ğ»Ñ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ SOLAMI - Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ ÑĞºĞ²Ğ¾Ğ·Ğ½Ğ¾Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ·Ñ€ĞµĞ½Ğ¸Ñ-ÑĞ·Ñ‹ĞºĞ°-Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ (VLA) Ğ´Ğ»Ñ Ğ¸Ğ¼Ğ¼ĞµÑ€ÑĞ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ 3D Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶Ğ°Ğ¼Ğ¸. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ VLA Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ³Ğ¾ Ğ²Ğ²Ğ¾Ğ´Ğ°. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ½ĞµÑ…Ğ²Ğ°Ñ‚ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… SynMSI, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸. Ğ¢Ğ°ĞºĞ¶Ğµ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ VR-Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹Ñ Ğ´Ğ»Ñ Ğ¸Ğ¼Ğ¼ĞµÑ€ÑĞ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Empowering 3D Characters with Social Intelligence through SOLAMI', 'desc': 'This paper presents SOLAMI, a novel framework designed to enhance the social intelligence of 3D autonomous characters. It integrates a Social Vision-Language-Action (VLA) architecture that allows characters to respond to user inputs with both speech and motion, facilitating more natural interactions. To support this, the authors introduce SynMSI, a synthetic dataset that helps overcome the challenge of limited training data for social interactions. Additionally, a virtual reality interface is developed to provide users with an immersive experience when interacting with these characters, resulting in improved response accuracy and reduced latency.'}, 'zh': {'title': 'èµ‹äºˆ3Dè§’è‰²ç¤¾äº¤æ™ºèƒ½çš„åˆ›æ–°æ¡†æ¶', 'desc': 'æœ¬æ–‡ä»‹ç»äº†SOLAMIï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç«¯åˆ°ç«¯çš„ç¤¾ä¼šè§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰å»ºæ¨¡æ¡†æ¶ï¼Œæ—¨åœ¨ä¸3Dè‡ªä¸»è§’è‰²è¿›è¡Œæ²‰æµ¸å¼äº’åŠ¨ã€‚SOLAMIä»ä¸‰ä¸ªæ–¹é¢æ„å»º3Dè‡ªä¸»è§’è‰²ï¼šé¦–å…ˆï¼Œæå‡ºäº†ç»Ÿä¸€çš„ç¤¾ä¼šVLAæ¶æ„ï¼Œæ ¹æ®ç”¨æˆ·çš„å¤šæ¨¡æ€è¾“å…¥ç”Ÿæˆå¤šæ¨¡æ€å“åº”ï¼ˆè¯­éŸ³å’ŒåŠ¨ä½œï¼‰ï¼Œä»¥é©±åŠ¨è§’è‰²è¿›è¡Œç¤¾äº¤äº’åŠ¨ã€‚å…¶æ¬¡ï¼Œä»‹ç»äº†SynMSIï¼Œè¿™æ˜¯ä¸€ä¸ªåˆæˆçš„å¤šæ¨¡æ€ç¤¾äº¤äº’åŠ¨æ•°æ®é›†ï¼Œé€šè¿‡è‡ªåŠ¨åŒ–æµç¨‹ç”Ÿæˆï¼Œè§£å†³äº†æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ã€‚æœ€åï¼Œå¼€å‘äº†ä¸€ä¸ªè™šæ‹Ÿç°å®æ¥å£ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿä¸è¿™äº›è§’è‰²è¿›è¡Œæ²‰æµ¸å¼äº’åŠ¨ï¼Œå®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿæä¾›æ›´ç²¾ç¡®å’Œè‡ªç„¶çš„è§’è‰²å“åº”ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.01822', 'title': 'VLsI: Verbalized Layers-to-Interactions from Large to Small Vision Language Models', 'url': 'https://huggingface.co/papers/2412.01822', 'abstract': 'The recent surge in high-quality visual instruction tuning samples from closed-source vision-language models (VLMs) such as GPT-4V has accelerated the release of open-source VLMs across various model sizes. However, scaling VLMs to improve performance using larger models brings significant computational challenges, especially for deployment on resource-constrained devices like mobile platforms and robots. To address this, we propose VLsI: Verbalized Layers-to-Interactions, a new VLM family in 2B and 7B model sizes, which prioritizes efficiency without compromising accuracy. VLsI leverages a unique, layer-wise distillation process, introducing intermediate "verbalizers" that map features from each layer to natural language space, allowing smaller VLMs to flexibly align with the reasoning processes of larger VLMs. This approach mitigates the training instability often encountered in output imitation and goes beyond typical final-layer tuning by aligning the small VLMs\' layer-wise progression with that of the large ones. We validate VLsI across ten challenging vision-language benchmarks, achieving notable performance gains (11.0% for 2B and 17.4% for 7B) over GPT-4V without the need for model scaling, merging, or architectural changes.', 'score': 9, 'issue_id': 916, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 2', 'zh': '12æœˆ2æ—¥'}, 'hash': '0c3ac9e6ce3f4cd2', 'authors': ['Byung-Kwan Lee', 'Ryo Hachiuma', 'Yu-Chiang Frank Wang', 'Yong Man Ro', 'Yueh-Hua Wu'], 'affiliations': ['KAIST', 'NVIDIA'], 'pdf_title_img': 'assets/pdf/title_img/2412.01822.jpg', 'data': {'categories': ['#dataset', '#small_models', '#architecture', '#transfer_learning', '#optimization', '#training', '#benchmark', '#open_source'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ğ¾ÑĞ»Ğ¾Ğ¹Ğ½ÑƒÑ Ğ²ĞµÑ€Ğ±Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ', 'desc': "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ğ¾Ğµ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ VLsI, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±ĞµĞ· ÑƒÑ‰ĞµÑ€Ğ±Ğ° Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. VLsI Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¿Ğ¾ÑĞ»Ğ¾Ğ¹Ğ½Ğ¾Ğ¹ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸, Ğ²Ğ²Ğ¾Ğ´Ñ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ 'Ğ²ĞµÑ€Ğ±Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ñ‹', ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶Ğ°ÑÑ‚ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ¸Ğ· ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ÑĞ»Ğ¾Ñ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ³Ğ¸Ğ±ĞºĞ¾ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ñ‹Ğ²Ğ°Ñ‚ÑŒÑÑ Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°Ğ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ VLsI Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ GPT-4V Ğ½Ğ° Ğ´ĞµÑÑÑ‚Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸Ğ»Ğ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹."}, 'en': {'title': 'Efficient Vision-Language Models with Layer-wise Distillation', 'desc': "This paper introduces VLsI, a new family of vision-language models designed to enhance efficiency while maintaining accuracy. It employs a layer-wise distillation technique that uses intermediate 'verbalizers' to translate features from each layer into natural language, enabling smaller models to better mimic the reasoning of larger models. This method addresses the common issue of training instability seen in traditional output imitation approaches. The results show significant performance improvements on various benchmarks, demonstrating that smaller models can achieve competitive results without needing to increase their size or alter their architecture."}, 'zh': {'title': 'é«˜æ•ˆè§†è§‰è¯­è¨€æ¨¡å‹çš„åˆ›æ–°ä¹‹è·¯', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å®¶æ—ï¼Œç§°ä¸ºVLsIï¼Œæ—¨åœ¨æé«˜æ¨¡å‹çš„æ•ˆç‡è€Œä¸ç‰ºç‰²å‡†ç¡®æ€§ã€‚VLsIé‡‡ç”¨äº†ä¸€ç§ç‹¬ç‰¹çš„å±‚çº§è’¸é¦è¿‡ç¨‹ï¼Œé€šè¿‡å¼•å…¥ä¸­é—´çš„â€œè¯­è¨€åŒ–å™¨â€ï¼Œå°†æ¯ä¸€å±‚çš„ç‰¹å¾æ˜ å°„åˆ°è‡ªç„¶è¯­è¨€ç©ºé—´ï¼Œä½¿å¾—è¾ƒå°çš„VLMèƒ½å¤Ÿçµæ´»åœ°ä¸è¾ƒå¤§VLMçš„æ¨ç†è¿‡ç¨‹å¯¹é½ã€‚è¯¥æ–¹æ³•æœ‰æ•ˆç¼“è§£äº†è¾“å‡ºæ¨¡ä»¿ä¸­å¸¸è§çš„è®­ç»ƒä¸ç¨³å®šæ€§ï¼Œå¹¶é€šè¿‡å¯¹é½å°å‹VLMçš„å±‚çº§è¿›å±•ä¸å¤§å‹VLMçš„å±‚çº§è¿›å±•ï¼Œè¶…è¶Šäº†å…¸å‹çš„æœ€ç»ˆå±‚è°ƒä¼˜ã€‚æˆ‘ä»¬åœ¨åä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„è§†è§‰è¯­è¨€åŸºå‡†ä¸ŠéªŒè¯äº†VLsIï¼Œå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.01064', 'title': 'FLOAT: Generative Motion Latent Flow Matching for Audio-driven Talking Portrait', 'url': 'https://huggingface.co/papers/2412.01064', 'abstract': 'With the rapid advancement of diffusion-based generative models, portrait image animation has achieved remarkable results. However, it still faces challenges in temporally consistent video generation and fast sampling due to its iterative sampling nature. This paper presents FLOAT, an audio-driven talking portrait video generation method based on flow matching generative model. We shift the generative modeling from the pixel-based latent space to a learned motion latent space, enabling efficient design of temporally consistent motion. To achieve this, we introduce a transformer-based vector field predictor with a simple yet effective frame-wise conditioning mechanism. Additionally, our method supports speech-driven emotion enhancement, enabling a natural incorporation of expressive motions. Extensive experiments demonstrate that our method outperforms state-of-the-art audio-driven talking portrait methods in terms of visual quality, motion fidelity, and efficiency.', 'score': 8, 'issue_id': 912, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 2', 'zh': '12æœˆ2æ—¥'}, 'hash': '1978e3ecf42cac0c', 'authors': ['Taekyung Ki', 'Dongchan Min', 'Gyoungsu Chae'], 'affiliations': ['DeepBrain AI Inc.', 'Graduate School of AI, KAIST'], 'pdf_title_img': 'assets/pdf/title_img/2412.01064.jpg', 'data': {'categories': ['#audio', '#diffusion', '#multimodal', '#video', '#architecture'], 'emoji': 'ğŸ—£ï¸', 'ru': {'title': 'Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰Ğ¸Ğ¼ Ğ¿Ğ¾Ñ€Ñ‚Ñ€ĞµÑ‚Ğ¾Ğ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ FLOAT - Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰Ğ¸Ğ¼ Ğ¿Ğ¾Ñ€Ñ‚Ñ€ĞµÑ‚Ğ¾Ğ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°ÑƒĞ´Ğ¸Ğ¾, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑÑÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ· Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¸ĞºÑ‚Ğ¾Ñ€ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ»Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° Ñ Ğ¿Ğ¾ĞºĞ°Ğ´Ñ€Ğ¾Ğ²Ñ‹Ğ¼ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑĞ»Ğ¾Ğ²Ğ»Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ. FLOAT Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€ĞµÑ‡Ğ¸, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑÑ‚ÑŒ Ğ²Ñ‹Ñ€Ğ°Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'FLOAT: Revolutionizing Audio-Driven Talking Portraits with Motion Consistency', 'desc': 'This paper introduces FLOAT, a novel method for generating talking portrait videos driven by audio. It addresses the challenges of creating temporally consistent animations and speeding up the sampling process by utilizing a flow matching generative model. By transitioning from pixel-based latent spaces to learned motion latent spaces, FLOAT enhances the design of motion consistency. The method also incorporates a transformer-based vector field predictor that allows for effective frame-wise conditioning and supports emotion enhancement based on speech, leading to improved visual quality and motion fidelity.'}, 'zh': {'title': 'FLOATï¼šé«˜æ•ˆéŸ³é¢‘é©±åŠ¨çš„äººåƒè§†é¢‘ç”Ÿæˆ', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºFLOATçš„éŸ³é¢‘é©±åŠ¨äººåƒè§†é¢‘ç”Ÿæˆæ–¹æ³•ï¼ŒåŸºäºæµåŒ¹é…ç”Ÿæˆæ¨¡å‹ã€‚æˆ‘ä»¬å°†ç”Ÿæˆå»ºæ¨¡ä»åŸºäºåƒç´ çš„æ½œåœ¨ç©ºé—´è½¬ç§»åˆ°å­¦ä¹ çš„è¿åŠ¨æ½œåœ¨ç©ºé—´ï¼Œä»è€Œå®ç°æ—¶é—´ä¸€è‡´çš„è¿åŠ¨è®¾è®¡ã€‚è¯¥æ–¹æ³•å¼•å…¥äº†åŸºäºå˜æ¢å™¨çš„å‘é‡åœºé¢„æµ‹å™¨ï¼Œå¹¶é‡‡ç”¨ç®€å•æœ‰æ•ˆçš„é€å¸§æ¡ä»¶æœºåˆ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è§†è§‰è´¨é‡ã€è¿åŠ¨ä¿çœŸåº¦å’Œæ•ˆç‡æ–¹é¢ä¼˜äºç°æœ‰çš„éŸ³é¢‘é©±åŠ¨äººåƒç”Ÿæˆæ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.18933', 'title': 'Efficient Track Anything', 'url': 'https://huggingface.co/papers/2411.18933', 'abstract': 'Segment Anything Model 2 (SAM 2) has emerged as a powerful tool for video object segmentation and tracking anything. Key components of SAM 2 that drive the impressive video object segmentation performance include a large multistage image encoder for frame feature extraction and a memory mechanism that stores memory contexts from past frames to help current frame segmentation. The high computation complexity of multistage image encoder and memory module has limited its applications in real-world tasks, e.g., video object segmentation on mobile devices. To address this limitation, we propose EfficientTAMs, lightweight track anything models that produce high-quality results with low latency and model size. Our idea is based on revisiting the plain, nonhierarchical Vision Transformer (ViT) as an image encoder for video object segmentation, and introducing an efficient memory module, which reduces the complexity for both frame feature extraction and memory computation for current frame segmentation. We take vanilla lightweight ViTs and efficient memory module to build EfficientTAMs, and train the models on SA-1B and SA-V datasets for video object segmentation and track anything tasks. We evaluate on multiple video segmentation benchmarks including semi-supervised VOS and promptable video segmentation, and find that our proposed EfficientTAM with vanilla ViT perform comparably to SAM 2 model (HieraB+SAM 2) with ~2x speedup on A100 and ~2.4x parameter reduction. On segment anything image tasks, our EfficientTAMs also perform favorably over original SAM with ~20x speedup on A100 and ~20x parameter reduction. On mobile devices such as iPhone 15 Pro Max, our EfficientTAMs can run at ~10 FPS for performing video object segmentation with reasonable quality, highlighting the capability of small models for on-device video object segmentation applications.', 'score': 8, 'issue_id': 912, 'pub_date': '2024-11-28', 'pub_date_card': {'ru': '28 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 28', 'zh': '11æœˆ28æ—¥'}, 'hash': '86ff7f63f69fa104', 'authors': ['Yunyang Xiong', 'Chong Zhou', 'Xiaoyu Xiang', 'Lemeng Wu', 'Chenchen Zhu', 'Zechun Liu', 'Saksham Suri', 'Balakrishnan Varadarajan', 'Ramya Akula', 'Forrest Iandola', 'Raghuraman Krishnamoorthi', 'Bilge Soran', 'Vikas Chandra'], 'affiliations': ['Meta AI', 'Nanyang Technological University'], 'pdf_title_img': 'assets/pdf/title_img/2411.18933.jpg', 'data': {'categories': ['#video', '#small_models', '#benchmark', '#optimization', '#architecture'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ…', 'desc': 'EfficientTAMs - ÑÑ‚Ğ¾ Ğ¾Ğ±Ğ»ĞµĞ³Ñ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ½Ğ¸ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ñ‹ Ğ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ³Ğ¾ Vision Transformer (ViT) Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ÑƒĞ»Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. EfficientTAMs Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ñ‹Ğµ Ñ SAM 2, Ğ½Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ Ğ² 2 Ñ€Ğ°Ğ·Ğ° Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ Ğ¸ Ğ¸Ğ¼ĞµÑÑ‚ Ğ² 2,4 Ñ€Ğ°Ğ·Ğ° Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞĞ° Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ…, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº iPhone 15 Pro Max, EfficientTAMs Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ ÑĞ¾ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒÑ Ğ¾ĞºĞ¾Ğ»Ğ¾ 10 ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ² ÑĞµĞºÑƒĞ½Ğ´Ñƒ.'}, 'en': {'title': 'EfficientTAMs: Lightweight Video Segmentation for Mobile Devices', 'desc': 'The Segment Anything Model 2 (SAM 2) is a sophisticated tool for video object segmentation, but its complexity limits its use on mobile devices. To overcome this, the authors introduce EfficientTAMs, which are lightweight models designed to maintain high-quality segmentation while reducing latency and model size. They utilize a simple Vision Transformer (ViT) for feature extraction and an efficient memory module to streamline the segmentation process. The results show that EfficientTAMs can achieve comparable performance to SAM 2 with significant improvements in speed and resource efficiency, making them suitable for real-time applications on mobile platforms.'}, 'zh': {'title': 'é«˜æ•ˆè§†é¢‘ç‰©ä½“åˆ†å‰²ï¼Œè½»é‡åŒ–æ¨¡å‹æ–°é€‰æ‹©', 'desc': 'Segment Anything Model 2ï¼ˆSAM 2ï¼‰æ˜¯ä¸€ç§å¼ºå¤§çš„è§†é¢‘ç‰©ä½“åˆ†å‰²å’Œè·Ÿè¸ªå·¥å…·ã€‚ä¸ºäº†æé«˜æ€§èƒ½ï¼ŒSAM 2ä½¿ç”¨äº†å¤šé˜¶æ®µå›¾åƒç¼–ç å™¨å’Œè®°å¿†æœºåˆ¶ï¼Œä½†å…¶è®¡ç®—å¤æ‚æ€§é™åˆ¶äº†åœ¨ç§»åŠ¨è®¾å¤‡ä¸Šçš„åº”ç”¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†é«˜æ•ˆçš„è·Ÿè¸ªæ¨¡å‹EfficientTAMsï¼Œå®ƒä½¿ç”¨è½»é‡çº§çš„è§†è§‰å˜æ¢å™¨ï¼ˆViTï¼‰ä½œä¸ºå›¾åƒç¼–ç å™¨ï¼Œå¹¶å¼•å…¥é«˜æ•ˆçš„è®°å¿†æ¨¡å—ï¼Œä»è€Œé™ä½äº†è®¡ç®—å¤æ‚æ€§ã€‚æˆ‘ä»¬çš„EfficientTAMsåœ¨å¤šä¸ªè§†é¢‘åˆ†å‰²åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿåœ¨ç§»åŠ¨è®¾å¤‡ä¸Šä»¥åˆç†çš„è´¨é‡è¿›è¡Œè§†é¢‘ç‰©ä½“åˆ†å‰²ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.17459', 'title': 'WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent Video Diffusion Model', 'url': 'https://huggingface.co/papers/2411.17459', 'abstract': 'Video Variational Autoencoder (VAE) encodes videos into a low-dimensional latent space, becoming a key component of most Latent Video Diffusion Models (LVDMs) to reduce model training costs. However, as the resolution and duration of generated videos increase, the encoding cost of Video VAEs becomes a limiting bottleneck in training LVDMs. Moreover, the block-wise inference method adopted by most LVDMs can lead to discontinuities of latent space when processing long-duration videos. The key to addressing the computational bottleneck lies in decomposing videos into distinct components and efficiently encoding the critical information. Wavelet transform can decompose videos into multiple frequency-domain components and improve the efficiency significantly, we thus propose Wavelet Flow VAE (WF-VAE), an autoencoder that leverages multi-level wavelet transform to facilitate low-frequency energy flow into latent representation. Furthermore, we introduce a method called Causal Cache, which maintains the integrity of latent space during block-wise inference. Compared to state-of-the-art video VAEs, WF-VAE demonstrates superior performance in both PSNR and LPIPS metrics, achieving 2x higher throughput and 4x lower memory consumption while maintaining competitive reconstruction quality. Our code and models are available at https://github.com/PKU-YuanGroup/WF-VAE.', 'score': 7, 'issue_id': 909, 'pub_date': '2024-11-26', 'pub_date_card': {'ru': '26 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 26', 'zh': '11æœˆ26æ—¥'}, 'hash': '9b68162718865b81', 'authors': ['Zongjian Li', 'Bin Lin', 'Yang Ye', 'Liuhan Chen', 'Xinhua Cheng', 'Shenghai Yuan', 'Li Yuan'], 'affiliations': ['Peking University', 'Peng Cheng Laboratory', 'Rabbitpre Intelligence'], 'pdf_title_img': 'assets/pdf/title_img/2411.17459.jpg', 'data': {'categories': ['#diffusion', '#data', '#video', '#architecture', '#open_source', '#optimization', '#training'], 'emoji': 'ğŸŒŠ', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ²ĞµĞ¹Ğ²Ğ»ĞµÑ‚Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Wavelet Flow VAE (WF-VAE). Ğ­Ñ‚Ğ¾Ñ‚ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ¾Ğµ Ğ²ĞµĞ¹Ğ²Ğ»ĞµÑ‚-Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾. WF-VAE Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LVDM). ĞœĞµÑ‚Ğ¾Ğ´ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ Causal Cache Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ñ†ĞµĞ»Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ¿Ñ€Ğ¸ Ğ¿Ğ¾Ğ±Ğ»Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ.'}, 'en': {'title': 'Efficient Video Encoding with Wavelet Flow VAE', 'desc': 'The paper introduces Wavelet Flow VAE (WF-VAE), a novel approach to encoding videos into a low-dimensional latent space using wavelet transforms. This method addresses the computational bottleneck of traditional Video VAEs, especially when generating high-resolution and long-duration videos. By decomposing videos into frequency-domain components, WF-VAE enhances the efficiency of encoding critical information. Additionally, the proposed Causal Cache method ensures continuity in the latent space during block-wise inference, resulting in improved performance metrics compared to existing video VAEs.'}, 'zh': {'title': 'å°æ³¢æµVAEï¼šé«˜æ•ˆè§†é¢‘ç¼–ç çš„æ–°æ–¹æ³•', 'desc': 'è§†é¢‘å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰å°†è§†é¢‘ç¼–ç ä¸ºä½ç»´æ½œåœ¨ç©ºé—´ï¼Œæ˜¯å¤§å¤šæ•°æ½œåœ¨è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆLVDMsï¼‰çš„å…³é”®ç»„æˆéƒ¨åˆ†ï¼Œèƒ½å¤Ÿé™ä½æ¨¡å‹è®­ç»ƒæˆæœ¬ã€‚ç„¶è€Œï¼Œéšç€ç”Ÿæˆè§†é¢‘çš„åˆ†è¾¨ç‡å’Œæ—¶é•¿å¢åŠ ï¼Œè§†é¢‘VAEçš„ç¼–ç æˆæœ¬æˆä¸ºè®­ç»ƒLVDMsçš„ç“¶é¢ˆã€‚æ­¤å¤–ï¼Œå¤§å¤šæ•°LVDMsé‡‡ç”¨çš„å—çŠ¶æ¨ç†æ–¹æ³•åœ¨å¤„ç†é•¿æ—¶é•¿è§†é¢‘æ—¶å¯èƒ½å¯¼è‡´æ½œåœ¨ç©ºé—´çš„ä¸è¿ç»­æ€§ã€‚ä¸ºäº†è§£å†³è®¡ç®—ç“¶é¢ˆï¼Œæˆ‘ä»¬æå‡ºäº†å°æ³¢æµVAEï¼ˆWF-VAEï¼‰ï¼Œé€šè¿‡å¤šçº§å°æ³¢å˜æ¢æœ‰æ•ˆç¼–ç è§†é¢‘çš„å…³é”®ä¿¡æ¯ï¼Œå¹¶å¼•å…¥å› æœç¼“å­˜æ–¹æ³•ä»¥ä¿æŒæ½œåœ¨ç©ºé—´çš„å®Œæ•´æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.01316', 'title': 'Long Video Diffusion Generation with Segmented Cross-Attention and Content-Rich Video Data Curation', 'url': 'https://huggingface.co/papers/2412.01316', 'abstract': 'We introduce Presto, a novel video diffusion model designed to generate 15-second videos with long-range coherence and rich content. Extending video generation methods to maintain scenario diversity over long durations presents significant challenges. To address this, we propose a Segmented Cross-Attention (SCA) strategy, which splits hidden states into segments along the temporal dimension, allowing each segment to cross-attend to a corresponding sub-caption. SCA requires no additional parameters, enabling seamless incorporation into current DiT-based architectures. To facilitate high-quality long video generation, we build the LongTake-HD dataset, consisting of 261k content-rich videos with scenario coherence, annotated with an overall video caption and five progressive sub-captions. Experiments show that our Presto achieves 78.5% on the VBench Semantic Score and 100% on the Dynamic Degree, outperforming existing state-of-the-art video generation methods. This demonstrates that our proposed Presto significantly enhances content richness, maintains long-range coherence, and captures intricate textual details. More details are displayed on our project page: https://presto-video.github.io/.', 'score': 6, 'issue_id': 910, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 2', 'zh': '12æœˆ2æ—¥'}, 'hash': '5bca597a08ec0a14', 'authors': ['Xin Yan', 'Yuxuan Cai', 'Qiuyue Wang', 'Yuan Zhou', 'Wenhao Huang', 'Huan Yang'], 'affiliations': ['01.AI'], 'pdf_title_img': 'assets/pdf/title_img/2412.01316.jpg', 'data': {'categories': ['#video', '#dataset', '#architecture', '#diffusion', '#long_context'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Presto: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜', 'desc': 'Presto - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 15-ÑĞµĞºÑƒĞ½Ğ´Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ñ€Ğ¾Ğ»Ğ¸ĞºĞ¾Ğ² Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ğ±Ğ¾Ğ³Ğ°Ñ‚Ñ‹Ğ¼ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸ĞµĞ¼. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Segmented Cross-Attention (SCA), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ½Ğ° ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ²Ğ´Ğ¾Ğ»ÑŒ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼Ñƒ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ñƒ Ğ¿ĞµÑ€ĞµĞºÑ€ĞµÑÑ‚Ğ½Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ñ‰Ğ°Ñ‚ÑŒÑÑ Ğº ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰ĞµĞ¹ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞ¸. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±Ñ‹Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ LongTake-HD, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 261 Ñ‚Ñ‹ÑÑÑ‡Ñƒ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ±Ğ¾Ğ³Ğ°Ñ‚Ñ‹Ğ¼ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒÑ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Presto Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑĞ¼ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ ÑÑ‚ĞµĞ¿ĞµĞ½Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Presto: Revolutionizing Video Generation with Long-Range Coherence', 'desc': 'Presto is a new video diffusion model that generates 15-second videos while ensuring long-range coherence and rich content. It introduces a Segmented Cross-Attention (SCA) strategy that divides hidden states into segments, allowing each segment to focus on specific sub-captions without needing extra parameters. To support this model, the LongTake-HD dataset was created, containing 261,000 videos with coherent scenarios and detailed captions. Experiments show that Presto outperforms existing methods, achieving high scores in semantic understanding and dynamic content generation.'}, 'zh': {'title': 'Prestoï¼šç”Ÿæˆé•¿æ—¶é—´ä¸€è‡´æ€§è§†é¢‘çš„æ–°æ–¹æ³•', 'desc': 'æˆ‘ä»¬ä»‹ç»äº†ä¸€ç§æ–°çš„è§†é¢‘æ‰©æ•£æ¨¡å‹Prestoï¼Œæ—¨åœ¨ç”Ÿæˆå…·æœ‰é•¿æ—¶é—´ä¸€è‡´æ€§å’Œä¸°å¯Œå†…å®¹çš„15ç§’è§†é¢‘ã€‚ä¸ºäº†è§£å†³åœ¨é•¿æ—¶é—´å†…ä¿æŒåœºæ™¯å¤šæ ·æ€§çš„æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ†æ®µäº¤å‰æ³¨æ„åŠ›(SCA)ç­–ç•¥ï¼Œè¯¥ç­–ç•¥å°†éšè—çŠ¶æ€æ²¿æ—¶é—´ç»´åº¦åˆ†æ®µï¼Œä½¿æ¯ä¸ªæ®µèƒ½å¤Ÿä¸ç›¸åº”çš„å­æ ‡é¢˜è¿›è¡Œäº¤å‰å…³æ³¨ã€‚SCAä¸éœ€è¦é¢å¤–çš„å‚æ•°ï¼Œå¯ä»¥æ— ç¼é›†æˆåˆ°ç°æœ‰çš„åŸºäºDiTçš„æ¶æ„ä¸­ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒPrestoåœ¨è§†é¢‘ç”Ÿæˆæ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œæå‡äº†å†…å®¹ä¸°å¯Œæ€§å’Œé•¿è·ç¦»ä¸€è‡´æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.01800', 'title': 'PhysGame: Uncovering Physical Commonsense Violations in Gameplay Videos', 'url': 'https://huggingface.co/papers/2412.01800', 'abstract': 'Recent advancements in video-based large language models (Video LLMs) have witnessed the emergence of diverse capabilities to reason and interpret dynamic visual content. Among them, gameplay videos stand out as a distinctive data source, often containing glitches that defy physics commonsense. This characteristic renders them an effective benchmark for assessing the under-explored capability of physical commonsense understanding in video LLMs. In this paper, we propose PhysGame as a pioneering benchmark to evaluate physical commonsense violations in gameplay videos. PhysGame comprises 880 videos associated with glitches spanning four fundamental domains (i.e., mechanics, kinematics, optics, and material properties) and across 12 distinct physical commonsense. Through extensively evaluating various state-ofthe-art video LLMs, our findings reveal that the performance of current open-source video LLMs significantly lags behind that of proprietary counterparts. To bridge this gap, we curate an instruction tuning dataset PhysInstruct with 140,057 question-answering pairs to facilitate physical commonsense learning. In addition, we also propose a preference optimization dataset PhysDPO with 34,358 training pairs, where the dis-preferred responses are generated conditioned on misleading titles (i.e., meta information hacking), fewer frames (i.e., temporal hacking) and lower spatial resolutions (i.e., spatial hacking). Based on the suite of datasets, we propose PhysVLM as a physical knowledge-enhanced video LLM. Extensive experiments on both physical-oriented benchmark PhysGame and general video understanding benchmarks demonstrate the state-ofthe-art performance of PhysVLM.', 'score': 4, 'issue_id': 917, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 2', 'zh': '12æœˆ2æ—¥'}, 'hash': '43cc035146493599', 'authors': ['Meng Cao', 'Haoran Tang', 'Haoze Zhao', 'Hangyu Guo', 'Jiaheng Liu', 'Ge Zhang', 'Ruyang Liu', 'Qiang Sun', 'Ian Reid', 'Xiaodan Liang'], 'affiliations': ['Alibaba Group', 'Mohamed bin Zayed University of Artificial Intelligence', 'Peking University', 'Sun Yat-sen University', 'University of Toronto', 'University of Waterloo'], 'pdf_title_img': 'assets/pdf/title_img/2412.01800.jpg', 'data': {'categories': ['#dataset', '#video', '#open_source', '#optimization', '#benchmark', '#reasoning', '#multimodal', '#interpretability'], 'emoji': 'ğŸ®', 'ru': {'title': 'PhysVLM: ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ·Ğ´Ñ€Ğ°Ğ²Ğ¾Ğ¼Ñƒ ÑĞ¼Ñ‹ÑĞ»Ñƒ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ³Ñ€Ğ¾Ğ²Ñ‹Ğµ Ğ³Ğ»Ğ¸Ñ‚Ñ‡Ğ¸', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº PhysGame Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (Video LLM) Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ³ĞµĞ¹Ğ¼Ğ¿Ğ»ĞµĞ¹Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ³Ğ»Ğ¸Ñ‚Ñ‡Ğ°Ğ¼Ğ¸. ĞĞ½Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… PhysInstruct Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ·Ğ´Ñ€Ğ°Ğ²Ğ¾Ğ¼Ñƒ ÑĞ¼Ñ‹ÑĞ»Ñƒ Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ PhysDPO Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹. ĞĞ° Ğ±Ğ°Ğ·Ğµ ÑÑ‚Ğ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ±Ñ‹Ğ»Ğ° Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ PhysVLM, Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ²ÑˆĞ°Ñ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ĞºĞ°Ğº Ğ½Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ PhysGame, Ñ‚Ğ°Ğº Ğ¸ Ğ½Ğ° Ğ¾Ğ±Ñ‰Ğ¸Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'Enhancing Video LLMs with Physical Commonsense Understanding', 'desc': 'This paper introduces PhysGame, a new benchmark designed to evaluate how well video-based large language models (Video LLMs) understand physical commonsense by analyzing glitches in gameplay videos. The benchmark includes 880 videos that highlight violations of physical laws across four domains: mechanics, kinematics, optics, and material properties. To improve the performance of Video LLMs, the authors created two additional datasets: PhysInstruct for instruction tuning and PhysDPO for preference optimization, which help models learn from common misconceptions and misleading information. The proposed PhysVLM model, enhanced with physical knowledge, shows superior performance on both the PhysGame benchmark and general video understanding tasks compared to existing models.'}, 'zh': {'title': 'æå‡è§†é¢‘æ¨¡å‹çš„ç‰©ç†å¸¸è¯†ç†è§£èƒ½åŠ›', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°é¢–çš„åŸºå‡†æµ‹è¯•PhysGameï¼Œç”¨äºè¯„ä¼°è§†é¢‘å¤§è¯­è¨€æ¨¡å‹åœ¨æ¸¸æˆè§†é¢‘ä¸­å¯¹ç‰©ç†å¸¸è¯†çš„ç†è§£èƒ½åŠ›ã€‚æ¸¸æˆè§†é¢‘ä¸­å¸¸å¸¸å‡ºç°è¿åç‰©ç†å¸¸è¯†çš„æ•…éšœï¼Œè¿™ä½¿å¾—å®ƒä»¬æˆä¸ºè¯„ä¼°æ¨¡å‹èƒ½åŠ›çš„æœ‰æ•ˆæ•°æ®æºã€‚æˆ‘ä»¬è¿˜åˆ›å»ºäº†PhysInstructå’ŒPhysDPOä¸¤ä¸ªæ•°æ®é›†ï¼Œä»¥å¸®åŠ©æ¨¡å‹å­¦ä¹ ç‰©ç†å¸¸è¯†å¹¶ä¼˜åŒ–å…¶åå¥½ã€‚é€šè¿‡è¿™äº›æ•°æ®é›†ï¼Œæˆ‘ä»¬æå‡ºäº†PhysVLMï¼Œä¸€ä¸ªå¢å¼ºç‰©ç†çŸ¥è¯†çš„è§†é¢‘å¤§è¯­è¨€æ¨¡å‹ï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å±•ç¤ºäº†å…¶ä¼˜è¶Šçš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.19939', 'title': 'VLSBench: Unveiling Visual Leakage in Multimodal Safety', 'url': 'https://huggingface.co/papers/2411.19939', 'abstract': 'Safety concerns of Multimodal large language models (MLLMs) have gradually become an important problem in various applications. Surprisingly, previous works indicate a counter-intuitive phenomenon that using textual unlearning to align MLLMs achieves comparable safety performances with MLLMs trained with image-text pairs. To explain such a counter-intuitive phenomenon, we discover a visual safety information leakage (VSIL) problem in existing multimodal safety benchmarks, i.e., the potentially risky and sensitive content in the image has been revealed in the textual query. In this way, MLLMs can easily refuse these sensitive text-image queries according to textual queries. However, image-text pairs without VSIL are common in real-world scenarios and are overlooked by existing multimodal safety benchmarks. To this end, we construct multimodal visual leakless safety benchmark (VLSBench) preventing visual safety leakage from image to textual query with 2.4k image-text pairs. Experimental results indicate that VLSBench poses a significant challenge to both open-source and close-source MLLMs, including LLaVA, Qwen2-VL, Llama3.2-Vision, and GPT-4o. This study demonstrates that textual alignment is enough for multimodal safety scenarios with VSIL, while multimodal alignment is a more promising solution for multimodal safety scenarios without VSIL. Please see our code and data at: http://hxhcreate.github.io/VLSBench', 'score': 4, 'issue_id': 914, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 29', 'zh': '11æœˆ29æ—¥'}, 'hash': '03d8c636cf8c9486', 'authors': ['Xuhao Hu', 'Dongrui Liu', 'Hao Li', 'Xuanjing Huang', 'Jing Shao'], 'affiliations': ['Beihang University', 'Fudan University', 'Shanghai Artificial Intelligence Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2411.19939.jpg', 'data': {'categories': ['#ethics', '#multimodal', '#benchmark', '#leakage', '#open_source', '#dataset'], 'emoji': 'ğŸ”', 'ru': {'title': 'ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸ Ñ„ĞµĞ½Ğ¾Ğ¼ĞµĞ½ ÑƒÑ‚ĞµÑ‡ĞºĞ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ (VSIL) Ğ² ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¾Ğ½Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… VLSBench, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 2400 Ğ¿Ğ°Ñ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ-Ñ‚ĞµĞºÑÑ‚ Ğ±ĞµĞ· VSIL. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ VLSBench Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ»Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… MLLM Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ´Ğ»Ñ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ² Ğ±ĞµĞ· VSIL.'}, 'en': {'title': 'Unveiling Safety in Multimodal Models: The VSIL Challenge', 'desc': 'This paper addresses safety issues in Multimodal Large Language Models (MLLMs) by highlighting a problem called Visual Safety Information Leakage (VSIL). It shows that using textual unlearning can achieve safety performance similar to training with image-text pairs, which is surprising. The authors create a new benchmark, VLSBench, to test MLLMs without the influence of VSIL, using 2.4k image-text pairs. The findings suggest that while textual alignment can ensure safety in scenarios with VSIL, multimodal alignment is necessary for scenarios without it.'}, 'zh': {'title': 'å¤šæ¨¡æ€å®‰å…¨æ€§çš„æ–°æŒ‘æˆ˜ä¸è§£å†³æ–¹æ¡ˆ', 'desc': 'è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å®‰å…¨æ€§æ–¹é¢çš„æŒ‘æˆ˜ã€‚ç ”ç©¶å‘ç°ï¼Œæ–‡æœ¬å»å­¦ä¹ å¯ä»¥ä¸ä½¿ç”¨å›¾åƒ-æ–‡æœ¬å¯¹è®­ç»ƒçš„æ¨¡å‹åœ¨å®‰å…¨æ€§è¡¨ç°ä¸Šç›¸å½“ã€‚ä½œè€…æŒ‡å‡ºï¼Œç°æœ‰çš„å¤šæ¨¡æ€å®‰å…¨åŸºå‡†å­˜åœ¨è§†è§‰å®‰å…¨ä¿¡æ¯æ³„æ¼ï¼ˆVSILï¼‰é—®é¢˜ï¼Œè¿™ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿè½»æ˜“æ‹’ç»æ•æ„Ÿçš„æ–‡æœ¬-å›¾åƒæŸ¥è¯¢ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œç ”ç©¶è€…æ„å»ºäº†ä¸€ä¸ªæ–°çš„å¤šæ¨¡æ€è§†è§‰æ— æ³„æ¼å®‰å…¨åŸºå‡†ï¼ˆVLSBenchï¼‰ï¼Œä»¥æ›´å¥½åœ°è¯„ä¼°æ¨¡å‹åœ¨æ²¡æœ‰VSILæƒ…å†µä¸‹çš„å®‰å…¨æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.00947', 'title': 'VisOnlyQA: Large Vision Language Models Still Struggle with Visual Perception of Geometric Information', 'url': 'https://huggingface.co/papers/2412.00947', 'abstract': 'Errors in understanding visual information in images (i.e., visual perception errors) remain a major source of mistakes in Large Vision Language Models (LVLMs). While further analysis is essential, there is a deficiency in datasets for evaluating the visual perception of LVLMs. In this work, we introduce VisOnlyQA, a new dataset designed to directly evaluate the visual perception capabilities of LVLMs on questions about geometric and numerical information in scientific figures. Our dataset enables us to analyze the visual perception of LVLMs for fine-grained visual information, independent of other capabilities such as reasoning. The evaluation set of VisOnlyQA includes 1,200 multiple-choice questions in 12 tasks on four categories of figures. We also provide synthetic training data consisting of 70k instances. Our experiments on VisOnlyQA highlight the following findings: (i) 20 LVLMs we evaluate, including GPT-4o and Gemini 1.5 Pro, work poorly on the visual perception tasks in VisOnlyQA, while human performance is nearly perfect. (ii) Fine-tuning on synthetic training data demonstrates the potential for enhancing the visual perception of LVLMs, but observed improvements are limited to certain tasks and specific models. (iii) Stronger language models improve the visual perception of LVLMs. In summary, our experiments suggest that both training data and model architectures should be improved to enhance the visual perception capabilities of LVLMs. The datasets, code, and model responses are provided at https://github.com/psunlpgroup/VisOnlyQA.', 'score': 4, 'issue_id': 911, 'pub_date': '2024-12-01', 'pub_date_card': {'ru': '1 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 1', 'zh': '12æœˆ1æ—¥'}, 'hash': '7135f8936b0d3ee8', 'authors': ['Ryo Kamoi', 'Yusen Zhang', 'Sarkar Snigdha Sarathi Das', 'Ranran Haoran Zhang', 'Rui Zhang'], 'affiliations': ['Penn State University'], 'pdf_title_img': 'assets/pdf/title_img/2412.00947.jpg', 'data': {'categories': ['#dataset', '#interpretability', '#cv', '#synthetic', '#training'], 'emoji': 'ğŸ‘ï¸', 'ru': {'title': 'VisOnlyQA: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿ÑƒÑ‚ÑŒ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ AI', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ VisOnlyQA Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LVLM). Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¸ Ñ‡Ğ¸ÑĞ»Ğ¾Ğ²Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¼Ñƒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ LVLM, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº GPT-4o Ğ¸ Gemini 1.5 Pro, Ğ¿Ğ»Ğ¾Ñ…Ğ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ VisOnlyQA, Ğ² Ñ‚Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ĞºĞ°Ğº Ğ»ÑĞ´Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸ Ğ¸Ğ´ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ğº Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ LVLM.'}, 'en': {'title': 'Enhancing Visual Perception in LVLMs with VisOnlyQA', 'desc': 'This paper addresses the issue of visual perception errors in Large Vision Language Models (LVLMs), which can lead to mistakes when interpreting images. The authors introduce a new dataset called VisOnlyQA, specifically designed to evaluate LVLMs on geometric and numerical questions related to scientific figures. The dataset includes 1,200 multiple-choice questions across various tasks and provides synthetic training data to improve model performance. The findings reveal that while LVLMs struggle with visual perception tasks, fine-tuning with synthetic data can enhance their capabilities, particularly when using stronger language models.'}, 'zh': {'title': 'æå‡è§†è§‰æ„ŸçŸ¥ï¼ŒåŠ©åŠ›å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹', 'desc': 'æœ¬ç ”ç©¶ä»‹ç»äº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†VisOnlyQAï¼Œæ—¨åœ¨ç›´æ¥è¯„ä¼°å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨ç§‘å­¦å›¾å½¢ä¸­å‡ ä½•å’Œæ•°å€¼ä¿¡æ¯é—®é¢˜ä¸Šçš„è§†è§‰æ„ŸçŸ¥èƒ½åŠ›ã€‚è¯¥æ•°æ®é›†åŒ…å«1200ä¸ªå¤šé¡¹é€‰æ‹©é¢˜ï¼Œæ¶µç›–å››ç±»å›¾å½¢çš„12ä¸ªä»»åŠ¡ï¼Œå¸®åŠ©åˆ†æLVLMså¯¹ç»†ç²’åº¦è§†è§‰ä¿¡æ¯çš„ç†è§£ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯„ä¼°çš„20ä¸ªLVLMsåœ¨è§†è§‰æ„ŸçŸ¥ä»»åŠ¡ä¸Šçš„è¡¨ç°è¾ƒå·®ï¼Œè€Œäººç±»çš„è¡¨ç°å‡ ä¹å®Œç¾ã€‚é€šè¿‡åˆæˆè®­ç»ƒæ•°æ®è¿›è¡Œå¾®è°ƒå¯ä»¥æå‡LVLMsçš„è§†è§‰æ„ŸçŸ¥èƒ½åŠ›ï¼Œä½†æ”¹è¿›æ•ˆæœæœ‰é™ï¼Œä¸”æ›´å¼ºçš„è¯­è¨€æ¨¡å‹èƒ½å¤Ÿæé«˜è§†è§‰æ„ŸçŸ¥èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.00176', 'title': 'Art-Free Generative Models: Art Creation Without Graphic Art Knowledge', 'url': 'https://huggingface.co/papers/2412.00176', 'abstract': 'We explore the question: "How much prior art knowledge is needed to create art?" To investigate this, we propose a text-to-image generation model trained without access to art-related content. We then introduce a simple yet effective method to learn an art adapter using only a few examples of selected artistic styles. Our experiments show that art generated using our method is perceived by users as comparable to art produced by models trained on large, art-rich datasets. Finally, through data attribution techniques, we illustrate how examples from both artistic and non-artistic datasets contributed to the creation of new artistic styles.', 'score': 3, 'issue_id': 921, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 29', 'zh': '11æœˆ29æ—¥'}, 'hash': '41f372dd1c030fbd', 'authors': ['Hui Ren', 'Joanna Materzynska', 'Rohit Gandikota', 'David Bau', 'Antonio Torralba'], 'affiliations': ['MIT', 'Northeastern University', 'ShanghaiTech University'], 'pdf_title_img': 'assets/pdf/title_img/2412.00176.jpg', 'data': {'categories': ['#cv', '#dataset', '#synthetic', '#games', '#multimodal'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ˜ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ Ñ‚Ğ²Ğ¾Ñ€Ğ¸Ñ‚ ÑˆĞµĞ´ĞµĞ²Ñ€Ñ‹ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¸Ğ·ÑƒÑ‡Ğ°ÑÑ‚ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ Ğ¾ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾Ğ¼ Ğ¾Ğ±ÑŠĞµĞ¼Ğµ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¾Ğ± Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²Ğµ Ğ´Ğ»Ñ ĞµĞ³Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ. ĞĞ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ±ĞµĞ· Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ° Ğº ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ñƒ, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ğ¾Ğ¼Ñƒ Ñ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²Ğ¾Ğ¼. Ğ—Ğ°Ñ‚ĞµĞ¼ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ¹, Ğ½Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ğ° Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²Ğ°, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ»Ğ¸ÑˆÑŒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ…ÑƒĞ´Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… ÑÑ‚Ğ¸Ğ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²Ğ¾, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ğ¾Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°, Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°ĞµÑ‚ÑÑ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ¾Ğ¼ Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ±Ğ¾Ğ³Ğ°Ñ‚Ñ‹Ñ… Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²Ğ¾Ğ¼.'}, 'en': {'title': 'Creating Art with Minimal Prior Knowledge', 'desc': "This paper investigates the necessity of prior art knowledge in generating art through a text-to-image model. The authors propose a novel approach that trains the model without any art-related content, relying instead on a small number of examples from specific artistic styles to create an 'art adapter'. Their experiments reveal that the art produced by this method is perceived similarly to that generated by models trained on extensive art datasets. Additionally, they employ data attribution techniques to demonstrate how both artistic and non-artistic examples influence the development of new artistic styles."}, 'zh': {'title': 'åˆ›é€ è‰ºæœ¯æ— éœ€ä¸°å¯Œçš„è‰ºæœ¯çŸ¥è¯†', 'desc': 'æˆ‘ä»¬æ¢è®¨äº†åˆ›é€ è‰ºæœ¯éœ€è¦å¤šå°‘å…ˆå‰çš„è‰ºæœ¯çŸ¥è¯†ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨æ²¡æœ‰è‰ºæœ¯ç›¸å…³å†…å®¹çš„æƒ…å†µä¸‹è¿›è¡Œè®­ç»ƒã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„æ–¹æ³•ï¼Œä»…ä½¿ç”¨å°‘é‡é€‰å®šè‰ºæœ¯é£æ ¼çš„ç¤ºä¾‹æ¥å­¦ä¹ è‰ºæœ¯é€‚é…å™¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨æˆ‘ä»¬çš„æ–¹æ³•ç”Ÿæˆçš„è‰ºæœ¯ä½œå“åœ¨ç”¨æˆ·çœ¼ä¸­ä¸åœ¨å¤§å‹è‰ºæœ¯ä¸°å¯Œæ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¨¡å‹ç”Ÿæˆçš„è‰ºæœ¯ä½œå“ç›¸å½“ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.01250', 'title': 'Collaborative Instance Navigation: Leveraging Agent Self-Dialogue to Minimize User Input', 'url': 'https://huggingface.co/papers/2412.01250', 'abstract': 'Existing embodied instance goal navigation tasks, driven by natural language, assume human users to provide complete and nuanced instance descriptions prior to the navigation, which can be impractical in the real world as human instructions might be brief and ambiguous. To bridge this gap, we propose a new task, Collaborative Instance Navigation (CoIN), with dynamic agent-human interaction during navigation to actively resolve uncertainties about the target instance in natural, template-free, open-ended dialogues. To address CoIN, we propose a novel method, Agent-user Interaction with UncerTainty Awareness (AIUTA), leveraging the perception capability of Vision Language Models (VLMs) and the capability of Large Language Models (LLMs). First, upon object detection, a Self-Questioner model initiates a self-dialogue to obtain a complete and accurate observation description, while a novel uncertainty estimation technique mitigates inaccurate VLM perception. Then, an Interaction Trigger module determines whether to ask a question to the user, continue or halt navigation, minimizing user input. For evaluation, we introduce CoIN-Bench, a benchmark supporting both real and simulated humans. AIUTA achieves competitive performance in instance navigation against state-of-the-art methods, demonstrating great flexibility in handling user inputs.', 'score': 3, 'issue_id': 915, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 2', 'zh': '12æœˆ2æ—¥'}, 'hash': '8b768de35e4c5114', 'authors': ['Francesco Taioli', 'Edoardo Zorzi', 'Gianni Franchi', 'Alberto Castellini', 'Alessandro Farinelli', 'Marco Cristani', 'Yiming Wang'], 'affiliations': ['Fondazione Bruno Kessler', 'Polytechnic of Turin', 'U2IS, ENSTA Paris, Institut Polytechnique de Paris', 'University of Verona'], 'pdf_title_img': 'assets/pdf/title_img/2412.01250.jpg', 'data': {'categories': ['#benchmark', '#interpretability', '#multimodal', '#reasoning', '#agents'], 'emoji': 'ğŸ—ºï¸', 'ru': {'title': 'Ğ˜Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ° Ñ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ñ†ĞµĞ»Ğ¸ Ñƒ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ Ğ¸Ğ½ÑÑ‚Ğ°Ğ½ÑĞ°Ğ¼ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² - Collaborative Instance Navigation (CoIN), Ğ³Ğ´Ğµ Ğ°Ğ³ĞµĞ½Ñ‚ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²ÑƒĞµÑ‚ Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¼ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ AIUTA, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Vision Language Models Ğ¸ Large Language Models Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². AIUTA Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»Ğ¸ Self-Questioner Ğ´Ğ»Ñ ÑĞ°Ğ¼Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Interaction Trigger Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‚ÑŒ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº CoIN-Bench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼.'}, 'en': {'title': 'Navigating Together: Enhancing Instance Navigation with Human-AI Collaboration', 'desc': 'This paper introduces a new task called Collaborative Instance Navigation (CoIN), which allows agents to interact with humans during navigation to clarify ambiguous instructions. The proposed method, Agent-user Interaction with UncerTainty Awareness (AIUTA), uses Vision Language Models (VLMs) and Large Language Models (LLMs) to enhance the navigation process. It includes a Self-Questioner model that helps the agent gather detailed information about the target instance and an Interaction Trigger module that decides when to engage the user for input. The authors also present CoIN-Bench, a benchmark for evaluating the performance of navigation systems in both real and simulated environments, showing that AIUTA performs well compared to existing methods.'}, 'zh': {'title': 'åä½œå®ä¾‹å¯¼èˆªï¼šè®©æœºå™¨æ›´æ‡‚äººç±»æŒ‡ä»¤', 'desc': 'ç°æœ‰çš„å®ä¾‹ç›®æ ‡å¯¼èˆªä»»åŠ¡é€šå¸¸éœ€è¦ç”¨æˆ·æä¾›è¯¦ç»†çš„æè¿°ï¼Œä½†åœ¨ç°å®ä¸­ï¼Œè¿™ç§è¦æ±‚å¾€å¾€ä¸åˆ‡å®é™…ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„ä»»åŠ¡ï¼Œç§°ä¸ºåä½œå®ä¾‹å¯¼èˆªï¼ˆCoINï¼‰ï¼Œé€šè¿‡åŠ¨æ€çš„ä»£ç†-ç”¨æˆ·äº’åŠ¨æ¥è§£å†³å¯¼èˆªä¸­çš„ä¸ç¡®å®šæ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•ï¼Œä»£ç†-ç”¨æˆ·äº’åŠ¨ä¸ä¸ç¡®å®šæ€§æ„è¯†ï¼ˆAIUTAï¼‰ï¼Œç»“åˆäº†è§†è§‰è¯­è¨€æ¨¡å‹å’Œå¤§å‹è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨å¯¼èˆªè¿‡ç¨‹ä¸­ä¸»åŠ¨ä¸ç”¨æˆ·å¯¹è¯ã€‚é€šè¿‡å¼•å…¥CoIN-BenchåŸºå‡†ï¼Œæˆ‘ä»¬çš„AIUTAæ–¹æ³•åœ¨å®ä¾‹å¯¼èˆªä¸­è¡¨ç°å‡ºè‰²ï¼Œå±•ç¤ºäº†å¤„ç†ç”¨æˆ·è¾“å…¥çš„çµæ´»æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.19799', 'title': 'INCLUDE: Evaluating Multilingual Language Understanding with Regional Knowledge', 'url': 'https://huggingface.co/papers/2411.19799', 'abstract': 'The performance differential of large language models (LLM) between languages hinders their effective deployment in many regions, inhibiting the potential economic and societal value of generative AI tools in many communities. However, the development of functional LLMs in many languages (\\ie, multilingual LLMs) is bottlenecked by the lack of high-quality evaluation resources in languages other than English. Moreover, current practices in multilingual benchmark construction often translate English resources, ignoring the regional and cultural knowledge of the environments in which multilingual systems would be used. In this work, we construct an evaluation suite of 197,243 QA pairs from local exam sources to measure the capabilities of multilingual LLMs in a variety of regional contexts. Our novel resource, INCLUDE, is a comprehensive knowledge- and reasoning-centric benchmark across 44 written languages that evaluates multilingual LLMs for performance in the actual language environments where they would be deployed.', 'score': 3, 'issue_id': 914, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 29', 'zh': '11æœˆ29æ—¥'}, 'hash': '7b0cbdd28adecf2c', 'authors': ['Angelika Romanou', 'Negar Foroutan', 'Anna Sotnikova', 'Zeming Chen', 'Sree Harsha Nelaturu', 'Shivalika Singh', 'Rishabh Maheshwary', 'Micol Altomare', 'Mohamed A. Haggag', 'Snegha A', 'Alfonso Amayuelas', 'Azril Hafizi Amirudin', 'Viraat Aryabumi', 'Danylo Boiko', 'Michael Chang', 'Jenny Chim', 'Gal Cohen', 'Aditya Kumar Dalmia', 'Abraham Diress', 'Sharad Duwal', 'Daniil Dzenhaliou', 'Daniel Fernando Erazo Florez', 'Fabian Farestam', 'Joseph Marvin Imperial', 'Shayekh Bin Islam', 'Perttu Isotalo', 'Maral Jabbarishiviari', 'BÃ¶rje F. Karlsson', 'Eldar Khalilov', 'Christopher Klamm', 'Fajri Koto', 'Dominik KrzemiÅ„ski', 'Gabriel Adriano de Melo', 'Syrielle Montariol', 'Yiyang Nan', 'Joel Niklaus', 'Jekaterina Novikova', 'Johan Samir Obando Ceron', 'Debjit Paul', 'Esther Ploeger', 'Jebish Purbey', 'Swati Rajwal', 'Selvan Sunitha Ravi', 'Sara Rydell', 'Roshan Santhosh', 'Drishti Sharma', 'Marjana Prifti Skenduli', 'Arshia Soltani Moakhar', 'Bardia Soltani Moakhar', 'Ran Tamir', 'Ayush Kumar Tarun', 'Azmine Toushik Wasi', 'Thenuka Ovin Weerasinghe', 'Serhan Yilmaz', 'Mike Zhang', 'Imanol Schlag', 'Marzieh Fadaee', 'Sara Hooker', 'Antoine Bosselut'], 'affiliations': ['Cohere For AI', 'Cohere For AI Community', 'EPFL', 'ETH Zurich', 'Swiss AI Initiative'], 'pdf_title_img': 'assets/pdf/title_img/2411.19799.jpg', 'data': {'categories': ['#multilingual', '#low_resource', '#benchmark', '#reasoning'], 'emoji': 'ğŸŒ', 'ru': {'title': 'Ğ“Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ñ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğµ Ğ½ĞµÑ€Ğ°Ğ²Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… INCLUDE Ğ¸Ğ· 197,243 Ğ¿Ğ°Ñ€ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° 44 ÑĞ·Ñ‹ĞºĞ°Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… LLM. Ğ­Ñ‚Ğ¾Ñ‚ Ñ€ĞµÑÑƒÑ€Ñ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ¼ĞµÑÑ‚Ğ½Ñ‹Ñ… ÑĞºĞ·Ğ°Ğ¼ĞµĞ½Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ°Ñ… Ğ¸ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ñ‹. INCLUDE Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… LLM Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ… Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Bridging Language Gaps with INCLUDE: A Multilingual Benchmark for LLMs', 'desc': 'This paper addresses the performance gap of large language models (LLMs) across different languages, which limits their use in various regions. It highlights the challenge of developing multilingual LLMs due to the scarcity of high-quality evaluation resources in non-English languages. The authors propose a new evaluation suite called INCLUDE, consisting of 197,243 question-and-answer pairs sourced from local exams. This benchmark is designed to assess the capabilities of multilingual LLMs in real-world contexts, taking into account regional and cultural knowledge.'}, 'zh': {'title': 'æå‡å¤šè¯­è¨€æ¨¡å‹çš„å®é™…åº”ç”¨èƒ½åŠ›', 'desc': 'è¿™ç¯‡è®ºæ–‡è®¨è®ºäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä¸åŒè¯­è¨€ä¹‹é—´çš„æ€§èƒ½å·®å¼‚ï¼Œè¿™å½±å“äº†å®ƒä»¬åœ¨è®¸å¤šåœ°åŒºçš„æœ‰æ•ˆåº”ç”¨ã€‚ä¸ºäº†å…‹æœå¤šè¯­è¨€LLMå¼€å‘ä¸­çš„ç“¶é¢ˆï¼Œä½œè€…æ„å»ºäº†ä¸€ä¸ªåŒ…å«197,243ä¸ªé—®ç­”å¯¹çš„è¯„ä¼°å¥—ä»¶ï¼Œæ¥æºäºå½“åœ°è€ƒè¯•ææ–™ã€‚è¿™ä¸ªæ–°èµ„æºINCLUDEæ˜¯ä¸€ä¸ªå…¨é¢çš„çŸ¥è¯†å’Œæ¨ç†ä¸­å¿ƒåŸºå‡†ï¼Œæ¶µç›–44ç§ä¹¦é¢è¯­è¨€ï¼Œæ—¨åœ¨è¯„ä¼°å¤šè¯­è¨€LLMåœ¨å®é™…è¯­è¨€ç¯å¢ƒä¸­çš„è¡¨ç°ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œç ”ç©¶å¸Œæœ›æå‡ç”Ÿæˆæ€§äººå·¥æ™ºèƒ½å·¥å…·åœ¨ä¸åŒç¤¾åŒºçš„ç»æµå’Œç¤¾ä¼šä»·å€¼ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.01821', 'title': 'World-consistent Video Diffusion with Explicit 3D Modeling', 'url': 'https://huggingface.co/papers/2412.01821', 'abstract': 'Recent advancements in diffusion models have set new benchmarks in image and video generation, enabling realistic visual synthesis across single- and multi-frame contexts. However, these models still struggle with efficiently and explicitly generating 3D-consistent content. To address this, we propose World-consistent Video Diffusion (WVD), a novel framework that incorporates explicit 3D supervision using XYZ images, which encode global 3D coordinates for each image pixel. More specifically, we train a diffusion transformer to learn the joint distribution of RGB and XYZ frames. This approach supports multi-task adaptability via a flexible inpainting strategy. For example, WVD can estimate XYZ frames from ground-truth RGB or generate novel RGB frames using XYZ projections along a specified camera trajectory. In doing so, WVD unifies tasks like single-image-to-3D generation, multi-view stereo, and camera-controlled video generation. Our approach demonstrates competitive performance across multiple benchmarks, providing a scalable solution for 3D-consistent video and image generation with a single pretrained model.', 'score': 2, 'issue_id': 922, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 2', 'zh': '12æœˆ2æ—¥'}, 'hash': '843891601e85de06', 'authors': ['Qihang Zhang', 'Shuangfei Zhai', 'Miguel Angel Bautista', 'Kevin Miao', 'Alexander Toshev', 'Joshua Susskind', 'Jiatao Gu'], 'affiliations': ['Apple', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2412.01821.jpg', 'data': {'categories': ['#3d', '#benchmark', '#diffusion', '#video'], 'emoji': 'ğŸ¥', 'ru': {'title': '3D-ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ World-consistent Video Diffusion (WVD) Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D-ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°. WVD Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ XYZ-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ²Ğ½Ğ¾Ğ³Ğ¾ 3D-ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ RGB Ğ¸ XYZ ĞºĞ°Ğ´Ñ€Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ 3D Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼ ĞºĞ°Ğ¼ĞµÑ€Ñ‹. WVD Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ….'}, 'en': {'title': 'Unifying 3D Consistency in Video Generation with WVD', 'desc': 'This paper introduces World-consistent Video Diffusion (WVD), a new framework that enhances video and image generation by incorporating 3D supervision. WVD uses XYZ images to provide global 3D coordinates for each pixel, allowing for more accurate and consistent 3D content generation. The framework employs a diffusion transformer to learn the relationship between RGB and XYZ frames, enabling tasks like generating new RGB frames from 3D data. Overall, WVD offers a versatile solution for various tasks, achieving strong performance across benchmarks with a single pretrained model.'}, 'zh': {'title': 'ç»Ÿä¸€3Dä¸€è‡´æ€§çš„è§†é¢‘ç”Ÿæˆæ–°æ¡†æ¶', 'desc': 'æœ€è¿‘ï¼Œæ‰©æ•£æ¨¡å‹åœ¨å›¾åƒå’Œè§†é¢‘ç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œèƒ½å¤Ÿåœ¨å•å¸§å’Œå¤šå¸§ä¸Šä¸‹æ–‡ä¸­å®ç°é€¼çœŸçš„è§†è§‰åˆæˆã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨é«˜æ•ˆä¸”æ˜ç¡®åœ°ç”Ÿæˆ3Dä¸€è‡´å†…å®¹æ–¹é¢ä»ç„¶å­˜åœ¨æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸–ç•Œä¸€è‡´è§†é¢‘æ‰©æ•£ï¼ˆWVDï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œåˆ©ç”¨XYZå›¾åƒè¿›è¡Œæ˜ç¡®çš„3Dç›‘ç£ï¼Œç¼–ç æ¯ä¸ªå›¾åƒåƒç´ çš„å…¨å±€3Dåæ ‡ã€‚WVDé€šè¿‡çµæ´»çš„ä¿®è¡¥ç­–ç•¥æ”¯æŒå¤šä»»åŠ¡é€‚åº”æ€§ï¼Œèƒ½å¤Ÿä»çœŸå®çš„RGBä¼°è®¡XYZå¸§ï¼Œæˆ–æ²¿æŒ‡å®šçš„ç›¸æœºè½¨è¿¹ç”Ÿæˆæ–°çš„RGBå¸§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.00319', 'title': 'Improving speaker verification robustness with synthetic emotional utterances', 'url': 'https://huggingface.co/papers/2412.00319', 'abstract': 'A speaker verification (SV) system offers an authentication service designed to confirm whether a given speech sample originates from a specific speaker. This technology has paved the way for various personalized applications that cater to individual preferences. A noteworthy challenge faced by SV systems is their ability to perform consistently across a range of emotional spectra. Most existing models exhibit high error rates when dealing with emotional utterances compared to neutral ones. Consequently, this phenomenon often leads to missing out on speech of interest. This issue primarily stems from the limited availability of labeled emotional speech data, impeding the development of robust speaker representations that encompass diverse emotional states.   To address this concern, we propose a novel approach employing the CycleGAN framework to serve as a data augmentation method. This technique synthesizes emotional speech segments for each specific speaker while preserving the unique vocal identity. Our experimental findings underscore the effectiveness of incorporating synthetic emotional data into the training process. The models trained using this augmented dataset consistently outperform the baseline models on the task of verifying speakers in emotional speech scenarios, reducing equal error rate by as much as 3.64% relative.', 'score': 1, 'issue_id': 926, 'pub_date': '2024-11-30', 'pub_date_card': {'ru': '30 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 30', 'zh': '11æœˆ30æ—¥'}, 'hash': 'e9e71338ed876bc0', 'authors': ['Nikhil Kumar Koditala', 'Chelsea Jui-Ting Ju', 'Ruirui Li', 'Minho Jin', 'Aman Chadha', 'Andreas Stolcke'], 'affiliations': ['Amazon Alexa AI, USA', 'Amazon Business, USA', 'Amazon GenAI, USA', 'Amazon Search Experience Science, USA', 'Amazon Web Services, USA'], 'pdf_title_img': 'assets/pdf/title_img/2412.00319.jpg', 'data': {'categories': ['#training', '#dataset', '#synthetic', '#audio'], 'emoji': 'ğŸ­', 'ru': {'title': 'CycleGAN Ğ´Ğ»Ñ Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ€ĞµÑ‡Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰ĞµĞ³Ğ¾', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰ĞµĞ³Ğ¾ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ CycleGAN Ğ´Ğ»Ñ Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ€ĞµÑ‡Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€ÑƒÑÑ‚ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµÑ‡ĞµĞ²Ñ‹Ğµ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¸ĞºÑ‚Ğ¾Ñ€Ğ°, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ²ÑƒÑ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰Ğ¸Ñ… Ğ² ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ñ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ€ĞµÑ‡ÑŒÑ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ñ€Ğ°Ğ²Ğ½ÑƒÑ Ğ¾ÑˆĞ¸Ğ±ĞºÑƒ (EER) Ğ´Ğ¾ 3.64% Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ»Ğ¸Ğ½Ğ¸Ğ¸.'}, 'en': {'title': 'Enhancing Speaker Verification with Synthetic Emotional Data', 'desc': "This paper discusses a speaker verification (SV) system that authenticates speakers based on their voice. A major challenge for these systems is accurately verifying speakers when they express different emotions, as most existing models struggle with emotional speech. The authors propose using a CycleGAN framework to create synthetic emotional speech data, which helps improve the training of SV models. Their results show that incorporating this augmented data significantly enhances the models' performance, reducing error rates in emotional speech verification."}, 'zh': {'title': 'æƒ…æ„Ÿè¯­éŸ³éªŒè¯çš„æ–°çªç ´', 'desc': 'æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§è¯´è¯äººéªŒè¯ç³»ç»Ÿï¼Œæ—¨åœ¨ç¡®è®¤ç‰¹å®šè¯­éŸ³æ ·æœ¬æ˜¯å¦æ¥è‡ªç‰¹å®šè¯´è¯è€…ã€‚è¯¥ç³»ç»Ÿé¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜æ˜¯å¦‚ä½•åœ¨ä¸åŒæƒ…æ„ŸçŠ¶æ€ä¸‹ä¿æŒä¸€è‡´çš„æ€§èƒ½ã€‚ä¸ºäº†å…‹æœè¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬é‡‡ç”¨CycleGANæ¡†æ¶è¿›è¡Œæ•°æ®å¢å¼ºï¼Œåˆæˆæ¯ä¸ªè¯´è¯è€…çš„æƒ…æ„Ÿè¯­éŸ³ç‰‡æ®µï¼ŒåŒæ—¶ä¿ç•™å…¶ç‹¬ç‰¹çš„å£°éŸ³ç‰¹å¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨åˆæˆæƒ…æ„Ÿæ•°æ®è®­ç»ƒçš„æ¨¡å‹åœ¨æƒ…æ„Ÿè¯­éŸ³éªŒè¯ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œé”™è¯¯ç‡é™ä½äº†3.64%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.00869', 'title': 'Exploring the Abilities of Large Language Models to Solve Proportional Analogies via Knowledge-Enhanced Prompting', 'url': 'https://huggingface.co/papers/2412.00869', 'abstract': 'Making analogies is fundamental to cognition. Proportional analogies, which consist of four terms, are often used to assess linguistic and cognitive abilities. For instance, completing analogies like "Oxygen is to Gas as <blank> is to <blank>" requires identifying the semantic relationship (e.g., "type of") between the first pair of terms ("Oxygen" and "Gas") and finding a second pair that shares the same relationship (e.g., "Aluminum" and "Metal"). In this work, we introduce a 15K Multiple-Choice Question Answering (MCQA) dataset for proportional analogy completion and evaluate the performance of contemporary Large Language Models (LLMs) in various knowledge-enhanced prompt settings. Specifically, we augment prompts with three types of knowledge: exemplar, structured, and targeted. Our results show that despite extensive training data, solving proportional analogies remains challenging for current LLMs, with the best model achieving an accuracy of 55%. Notably, we find that providing targeted knowledge can better assist models in completing proportional analogies compared to providing exemplars or collections of structured knowledge.', 'score': 1, 'issue_id': 926, 'pub_date': '2024-12-01', 'pub_date_card': {'ru': '1 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 1', 'zh': '12æœˆ1æ—¥'}, 'hash': '6816796631155a8c', 'authors': ['Thilini Wijesiriwardene', 'Ruwan Wickramarachchi', 'Sreeram Vennam', 'Vinija Jain', 'Aman Chadha', 'Amitava Das', 'Ponnurangam Kumaraguru', 'Amit Sheth'], 'affiliations': ['AI Institute, University of South Carolina, USA', 'Amazon GenAI, USA', 'IIIT Hyderabad, India', 'Meta, USA', 'Stanford University, USA'], 'pdf_title_img': 'assets/pdf/title_img/2412.00869.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#reasoning', '#data'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ‘Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²ÑĞµ ĞµÑ‰Ğµ Ğ½Ğµ Ğ¼Ğ°ÑÑ‚ĞµÑ€Ğ° Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ğ¹', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ñ€ĞµÑˆĞ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¿Ğ¾Ñ€Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· 15 000 Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ğ¼ Ğ´Ğ»Ñ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ñ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ğ¹. ĞĞ½Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… LLM Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ°Ñ… Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ»Ğ¸ÑˆÑŒ 55%, Ğ¸ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ÑÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ, Ñ‡ĞµĞ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ğ¸Ğ»Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ.'}, 'en': {'title': 'Enhancing Analogy Completion in LLMs with Targeted Knowledge', 'desc': 'This paper focuses on the challenge of solving proportional analogies, which are important for assessing cognitive abilities. The authors present a new dataset containing 15,000 multiple-choice questions designed for this purpose. They evaluate how well current Large Language Models (LLMs) perform on these analogies, especially when given different types of knowledge prompts. The findings reveal that while LLMs struggle with these tasks, targeted knowledge prompts significantly improve their performance compared to other types of knowledge.'}, 'zh': {'title': 'æå‡ç±»æ¯”èƒ½åŠ›ï¼ŒçŸ¥è¯†æ˜¯å…³é”®ï¼', 'desc': 'ç±»æ¯”æ˜¯è®¤çŸ¥çš„é‡è¦éƒ¨åˆ†ï¼Œæ¯”ä¾‹ç±»æ¯”ç”±å››ä¸ªæœ¯è¯­ç»„æˆï¼Œå¸¸ç”¨äºè¯„ä¼°è¯­è¨€å’Œè®¤çŸ¥èƒ½åŠ›ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªåŒ…å«15,000ä¸ªå¤šé¡¹é€‰æ‹©é¢˜çš„æ¯”ä¾‹ç±»æ¯”å®Œæˆæ•°æ®é›†ï¼Œå¹¶è¯„ä¼°äº†å½“å‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä¸åŒçŸ¥è¯†å¢å¼ºæç¤ºè®¾ç½®ä¸‹çš„è¡¨ç°ã€‚ç ”ç©¶å‘ç°ï¼Œå°½ç®¡æ¨¡å‹ç»è¿‡å¤§é‡è®­ç»ƒï¼Œè§£å†³æ¯”ä¾‹ç±»æ¯”ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œæœ€ä½³æ¨¡å‹çš„å‡†ç¡®ç‡ä»…ä¸º55%ã€‚ç‰¹åˆ«æ˜¯ï¼Œæä¾›é’ˆå¯¹æ€§çš„çŸ¥è¯†æ¯”æä¾›ç¤ºä¾‹æˆ–ç»“æ„åŒ–çŸ¥è¯†æ›´èƒ½å¸®åŠ©æ¨¡å‹å®Œæˆæ¯”ä¾‹ç±»æ¯”ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.01408', 'title': 'Towards Cross-Lingual Audio Abuse Detection in Low-Resource Settings with Few-Shot Learning', 'url': 'https://huggingface.co/papers/2412.01408', 'abstract': 'Online abusive content detection, particularly in low-resource settings and within the audio modality, remains underexplored. We investigate the potential of pre-trained audio representations for detecting abusive language in low-resource languages, in this case, in Indian languages using Few Shot Learning (FSL). Leveraging powerful representations from models such as Wav2Vec and Whisper, we explore cross-lingual abuse detection using the ADIMA dataset with FSL. Our approach integrates these representations within the Model-Agnostic Meta-Learning (MAML) framework to classify abusive language in 10 languages. We experiment with various shot sizes (50-200) evaluating the impact of limited data on performance. Additionally, a feature visualization study was conducted to better understand model behaviour. This study highlights the generalization ability of pre-trained models in low-resource scenarios and offers valuable insights into detecting abusive language in multilingual contexts.', 'score': 1, 'issue_id': 918, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 2', 'zh': '12æœˆ2æ—¥'}, 'hash': 'f4c5e5e0485d3969', 'authors': ['Aditya Narayan Sankaran', 'Reza Farahbaksh', 'Noel Crespi'], 'affiliations': ['SAMOVAR, TÃ©lÃ©com SudParis Institut Polytechnique de Paris 91120 Palaiseau, France'], 'pdf_title_img': 'assets/pdf/title_img/2412.01408.jpg', 'data': {'categories': ['#low_resource', '#interpretability', '#dataset', '#multilingual', '#audio', '#training'], 'emoji': 'ğŸ™ï¸', 'ru': {'title': 'Ğ‘Ğ¾Ñ€ÑŒĞ±Ğ° Ñ Ğ¾ÑĞºĞ¾Ñ€Ğ±Ğ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ² Ğ°ÑƒĞ´Ğ¸Ğ¾: Ğ¼Ğ°Ğ»Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¼Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ¾Ğ²', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¾ÑĞºĞ¾Ñ€Ğ±Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ² Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ½Ğ° ÑĞ·Ñ‹ĞºĞ°Ñ… Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµÑÑƒÑ€ÑĞ°Ğ¼Ğ¸, Ğ² Ñ‡Ğ°ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ¸Ğ½Ğ´Ğ¸Ğ¹ÑĞºĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ°Ñ…, Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Few Shot Learning (FSL). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ°ÑƒĞ´Ğ¸Ğ¾-Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ· Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Wav2Vec Ğ¸ Whisper Ğ² Ñ€Ğ°Ğ¼ĞºĞ°Ñ… Model-Agnostic Meta-Learning (MAML) Ğ´Ğ»Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¾ÑĞºĞ¾Ñ€Ğ±Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° Ğ² 10 ÑĞ·Ñ‹ĞºĞ°Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ğ»Ğ¸ÑÑŒ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°Ğ¼Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ğº (50-200) Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ†ĞµĞ½Ğ½Ñ‹Ğµ insights Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¾ÑĞºĞ¾Ñ€Ğ±Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ.'}, 'en': {'title': 'Empowering Low-Resource Languages: Detecting Abuse in Audio with Few Shot Learning', 'desc': 'This paper focuses on detecting abusive language in audio, especially in languages with limited resources, like many Indian languages. It uses Few Shot Learning (FSL) techniques with pre-trained audio models such as Wav2Vec and Whisper to improve detection accuracy. The authors apply the Model-Agnostic Meta-Learning (MAML) framework to classify abusive content across 10 different languages, even with minimal training data. Their findings demonstrate how well pre-trained models can generalize in low-resource settings, providing insights for better abuse detection in multilingual environments.'}, 'zh': {'title': 'åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹æå‡ä½èµ„æºç¯å¢ƒä¸‹çš„è¾±éª‚å†…å®¹æ£€æµ‹', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†åœ¨ä½èµ„æºç¯å¢ƒä¸­ï¼Œç‰¹åˆ«æ˜¯éŸ³é¢‘æ¨¡å¼ä¸‹ï¼Œæ£€æµ‹åœ¨çº¿è¾±éª‚å†…å®¹çš„æ½œåŠ›ã€‚æˆ‘ä»¬ä½¿ç”¨é¢„è®­ç»ƒçš„éŸ³é¢‘è¡¨ç¤ºï¼Œç»“åˆå°‘é‡å­¦ä¹ ï¼ˆFew Shot Learningï¼‰ï¼Œåœ¨å°åº¦è¯­è¨€ä¸­è¿›è¡Œè¾±éª‚è¯­è¨€çš„æ£€æµ‹ã€‚é€šè¿‡åˆ©ç”¨Wav2Vecå’ŒWhisperç­‰æ¨¡å‹çš„å¼ºå¤§è¡¨ç¤ºï¼Œæˆ‘ä»¬åœ¨ADIMAæ•°æ®é›†ä¸Šè¿›è¡Œè·¨è¯­è¨€çš„è¾±éª‚æ£€æµ‹ã€‚ç ”ç©¶è¡¨æ˜ï¼Œé¢„è®­ç»ƒæ¨¡å‹åœ¨ä½èµ„æºåœºæ™¯ä¸­çš„æ³›åŒ–èƒ½åŠ›ï¼Œä¸ºå¤šè¯­è¨€ç¯å¢ƒä¸­çš„è¾±éª‚è¯­è¨€æ£€æµ‹æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.19477', 'title': 'A Simple and Provable Scaling Law for the Test-Time Compute of Large Language Models', 'url': 'https://huggingface.co/papers/2411.19477', 'abstract': 'We propose a general two-stage algorithm that enjoys a provable scaling law for the test-time compute of large language models (LLMs). Given an input problem, the proposed algorithm first generates N candidate solutions, and then chooses the best one via a multiple-round knockout tournament where each pair of candidates are compared for K times and only the winners move on to the next round. In a minimalistic implementation, both stages can be executed with a black-box LLM alone and nothing else (e.g., no external verifier or reward model), and a total of N times (K + 1) highly parallelizable LLM calls are needed for solving an input problem. Assuming that a generated candidate solution is correct with probability p_{gen} > 0 and a comparison between a pair of correct and incorrect solutions identifies the right winner with probability p_{comp} > 0.5 (i.e., better than a random guess), we prove theoretically that the failure probability of the proposed algorithm decays to zero exponentially with respect to N and K: $P(final output is incorrect) le (1 - p_{gen})^N + lceil log_2 N rceil e^{-2 K (p_{comp} - 0.5)^2}.$ Our empirical results with the challenging MMLU-Pro benchmark validate the technical assumptions, as well as the efficacy of the proposed algorithm and the gains from scaling up its test-time compute.', 'score': 1, 'issue_id': 912, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 29', 'zh': '11æœˆ29æ—¥'}, 'hash': '6057902d5fcbe3f4', 'authors': ['Yanxi Chen', 'Xuchen Pan', 'Yaliang Li', 'Bolin Ding', 'Jingren Zhou'], 'affiliations': ['Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2411.19477.jpg', 'data': {'categories': ['#training', '#benchmark', '#architecture', '#optimization'], 'emoji': 'ğŸ†', 'ru': {'title': 'ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ Ğ·Ğ°ĞºĞ¾Ğ½ Ğ´Ğ»Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ N ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ¾Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµĞµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ÑƒĞ½Ğ´Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ‚ÑƒÑ€Ğ½Ğ¸Ñ€Ğ° Ğ½Ğ° Ğ²Ñ‹Ğ±Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğµ. Ğ¢ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ° ÑĞºÑĞ¿Ğ¾Ğ½ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ĞµÑ‚ÑÑ Ñ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸ĞµĞ¼ N Ğ¸ K. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¼ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ MMLU-Pro Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°.'}, 'en': {'title': 'Efficient Solution Selection for Large Language Models', 'desc': 'This paper introduces a two-stage algorithm designed to improve the efficiency of large language models (LLMs) during test time. The first stage generates multiple candidate solutions for a given problem, while the second stage employs a knockout tournament to select the best solution through repeated comparisons. The algorithm can operate solely with a black-box LLM, requiring a manageable number of parallel calls to generate and evaluate candidates. The authors provide theoretical proof that the likelihood of incorrect outputs decreases exponentially as the number of candidates and comparisons increases, supported by empirical results from the MMLU-Pro benchmark.'}, 'zh': {'title': 'é«˜æ•ˆé€‰æ‹©ï¼šä¸¤é˜¶æ®µç®—æ³•ä¼˜åŒ–å¤§è¯­è¨€æ¨¡å‹è®¡ç®—', 'desc': 'æˆ‘ä»¬æå‡ºäº†ä¸€ç§é€šç”¨çš„ä¸¤é˜¶æ®µç®—æ³•ï¼Œèƒ½å¤Ÿåœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æµ‹è¯•æ—¶é—´è®¡ç®—ä¸­å®ç°å¯è¯æ˜çš„æ‰©å±•è§„å¾‹ã€‚è¯¥ç®—æ³•é¦–å…ˆç”ŸæˆNä¸ªå€™é€‰è§£ï¼Œç„¶åé€šè¿‡å¤šè½®æ·˜æ±°èµ›é€‰æ‹©æœ€ä½³è§£ï¼Œæ¯å¯¹å€™é€‰è§£æ¯”è¾ƒKæ¬¡ï¼Œåªæœ‰èƒœè€…è¿›å…¥ä¸‹ä¸€è½®ã€‚è¯¥ç®—æ³•çš„æœ€ç®€å®ç°ä»…éœ€ä½¿ç”¨é»‘ç®±LLMï¼Œæ— éœ€å¤–éƒ¨éªŒè¯å™¨æˆ–å¥–åŠ±æ¨¡å‹ï¼Œæ€»å…±éœ€è¦Næ¬¡(K + 1)é«˜åº¦å¯å¹¶è¡Œçš„LLMè°ƒç”¨æ¥è§£å†³è¾“å…¥é—®é¢˜ã€‚ç†è®ºè¯æ˜è¡¨æ˜ï¼Œå‡è®¾ç”Ÿæˆçš„å€™é€‰è§£æ­£ç¡®çš„æ¦‚ç‡ä¸ºp_{gen} > 0ï¼Œä¸”æ­£ç¡®ä¸é”™è¯¯è§£çš„æ¯”è¾ƒèƒ½ä»¥æ¦‚ç‡p_{comp} > 0.5è¯†åˆ«å‡ºæ­£ç¡®çš„èƒœè€…ï¼Œåˆ™è¯¥ç®—æ³•çš„å¤±è´¥æ¦‚ç‡éšç€Nå’ŒKçš„å¢åŠ å‘ˆæŒ‡æ•°çº§ä¸‹é™ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.07730', 'title': 'STIV: Scalable Text and Image Conditioned Video Generation', 'url': 'https://huggingface.co/papers/2412.07730', 'abstract': 'The field of video generation has made remarkable advancements, yet there remains a pressing need for a clear, systematic recipe that can guide the development of robust and scalable models. In this work, we present a comprehensive study that systematically explores the interplay of model architectures, training recipes, and data curation strategies, culminating in a simple and scalable text-image-conditioned video generation method, named STIV. Our framework integrates image condition into a Diffusion Transformer (DiT) through frame replacement, while incorporating text conditioning via a joint image-text conditional classifier-free guidance. This design enables STIV to perform both text-to-video (T2V) and text-image-to-video (TI2V) tasks simultaneously. Additionally, STIV can be easily extended to various applications, such as video prediction, frame interpolation, multi-view generation, and long video generation, etc. With comprehensive ablation studies on T2I, T2V, and TI2V, STIV demonstrate strong performance, despite its simple design. An 8.7B model with 512 resolution achieves 83.1 on VBench T2V, surpassing both leading open and closed-source models like CogVideoX-5B, Pika, Kling, and Gen-3. The same-sized model also achieves a state-of-the-art result of 90.1 on VBench I2V task at 512 resolution. By providing a transparent and extensible recipe for building cutting-edge video generation models, we aim to empower future research and accelerate progress toward more versatile and reliable video generation solutions.', 'score': 48, 'issue_id': 1061, 'pub_date': '2024-12-10', 'pub_date_card': {'ru': '10 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 10', 'zh': '12æœˆ10æ—¥'}, 'hash': 'a43bca3bdca1a7ba', 'authors': ['Zongyu Lin', 'Wei Liu', 'Chen Chen', 'Jiasen Lu', 'Wenze Hu', 'Tsu-Jui Fu', 'Jesse Allardice', 'Zhengfeng Lai', 'Liangchen Song', 'Bowen Zhang', 'Cha Chen', 'Yiran Fei', 'Yifan Jiang', 'Lezhi Li', 'Yizhou Sun', 'Kai-Wei Chang', 'Yinfei Yang'], 'affiliations': ['Apple', 'University of California, Los Angeles'], 'pdf_title_img': 'assets/pdf/title_img/2412.07730.jpg', 'data': {'categories': ['#architecture', '#games', '#diffusion', '#training', '#video'], 'emoji': 'ğŸ¬', 'ru': {'title': 'STIV: ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ€ĞµÑ†ĞµĞ¿Ñ‚ Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´ STIV. STIV Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Diffusion Transformer Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğµ Ğ±ĞµĞ·ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ STIV Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ĞºĞ°Ğº Ñ‚ĞµĞºÑÑ‚-Ğ²-Ğ²Ğ¸Ğ´ĞµĞ¾ (T2V), Ñ‚Ğ°Ğº Ğ¸ Ñ‚ĞµĞºÑÑ‚-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ-Ğ²-Ğ²Ğ¸Ğ´ĞµĞ¾ (TI2V). ĞœĞ¾Ğ´ĞµĞ»ÑŒ STIV Ğ¾Ğ±ÑŠĞµĞ¼Ğ¾Ğ¼ 8.7B Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… T2V Ğ¸ I2V, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¸ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'STIV: Simplifying Video Generation with Text and Image Conditioning', 'desc': 'This paper introduces STIV, a novel framework for video generation that effectively combines text and image inputs to create videos. It utilizes a Diffusion Transformer architecture, enhancing its capabilities through a method called frame replacement and a joint image-text conditional classifier-free guidance. STIV is designed to handle both text-to-video (T2V) and text-image-to-video (TI2V) tasks, making it versatile for various applications like video prediction and frame interpolation. The model demonstrates impressive performance metrics, outperforming existing models while maintaining a straightforward design, thus providing a clear pathway for future advancements in video generation.'}, 'zh': {'title': 'è§†é¢‘ç”Ÿæˆçš„ç®€å•ä¸å¼ºå¤§', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†è§†é¢‘ç”Ÿæˆæ¨¡å‹çš„æ¶æ„ã€è®­ç»ƒæ–¹æ³•å’Œæ•°æ®ç­–åˆ’ç­–ç•¥ä¹‹é—´çš„å…³ç³»ï¼Œæå‡ºäº†ä¸€ç§ç®€å•ä¸”å¯æ‰©å±•çš„è§†é¢‘ç”Ÿæˆæ–¹æ³•STIVã€‚STIVé€šè¿‡å¸§æ›¿æ¢å°†å›¾åƒæ¡ä»¶é›†æˆåˆ°æ‰©æ•£å˜æ¢å™¨ä¸­ï¼ŒåŒæ—¶åˆ©ç”¨æ— æ¡ä»¶åˆ†ç±»å™¨å¼•å¯¼å®ç°æ–‡æœ¬æ¡ä»¶ã€‚è¯¥æ¡†æ¶èƒ½å¤ŸåŒæ—¶æ‰§è¡Œæ–‡æœ¬åˆ°è§†é¢‘ï¼ˆT2Vï¼‰å’Œæ–‡æœ¬-å›¾åƒåˆ°è§†é¢‘ï¼ˆTI2Vï¼‰ä»»åŠ¡ï¼Œå¹¶ä¸”å¯ä»¥æ‰©å±•åˆ°è§†é¢‘é¢„æµ‹ã€å¸§æ’å€¼ç­‰å¤šç§åº”ç”¨ã€‚é€šè¿‡å…¨é¢çš„æ¶ˆèç ”ç©¶ï¼ŒSTIVåœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå±•ç¤ºäº†å…¶å¼ºå¤§çš„æ€§èƒ½å’Œç®€å•çš„è®¾è®¡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.05210', 'title': 'Evaluating and Aligning CodeLLMs on Human Preference', 'url': 'https://huggingface.co/papers/2412.05210', 'abstract': 'Code large language models (codeLLMs) have made significant strides in code generation. Most previous code-related benchmarks, which consist of various programming exercises along with the corresponding test cases, are used as a common measure to evaluate the performance and capabilities of code LLMs. However, the current code LLMs focus on synthesizing the correct code snippet, ignoring the alignment with human preferences, where the query should be sampled from the practical application scenarios and the model-generated responses should satisfy the human preference. To bridge the gap between the model-generated response and human preference, we present a rigorous human-curated benchmark CodeArena to emulate the complexity and diversity of real-world coding tasks, where 397 high-quality samples spanning 40 categories and 44 programming languages, carefully curated from user queries. Further, we propose a diverse synthetic instruction corpus SynCode-Instruct (nearly 20B tokens) by scaling instructions from the website to verify the effectiveness of the large-scale synthetic instruction fine-tuning, where Qwen2.5-SynCoder totally trained on synthetic instruction data can achieve top-tier performance of open-source code LLMs. The results find performance differences between execution-based benchmarks and CodeArena. Our systematic experiments of CodeArena on 40+ LLMs reveal a notable performance gap between open SOTA code LLMs (e.g. Qwen2.5-Coder) and proprietary LLMs (e.g., OpenAI o1), underscoring the importance of the human preference alignment.\\url{https://codearenaeval.github.io/ }', 'score': 40, 'issue_id': 1060, 'pub_date': '2024-12-06', 'pub_date_card': {'ru': '6 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 6', 'zh': '12æœˆ6æ—¥'}, 'hash': '0232aabe01d37826', 'authors': ['Jian Yang', 'Jiaxi Yang', 'Ke Jin', 'Yibo Miao', 'Lei Zhang', 'Liqun Yang', 'Zeyu Cui', 'Yichang Zhang', 'Binyuan Hui', 'Junyang Lin'], 'affiliations': ['Alibaba Group', 'Shanghai Jiao Tong University', 'Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2412.05210.jpg', 'data': {'categories': ['#plp', '#alignment', '#benchmark', '#open_source', '#synthetic', '#training'], 'emoji': 'ğŸ†', 'ru': {'title': 'CodeArena: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ˜Ğ˜-Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ½Ğ¸ĞºĞ¾Ğ² Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸ÑÑ‚Ğ°', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº CodeArena Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ´Ğ° Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¸Ğ· 397 Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° 44 ÑĞ·Ñ‹ĞºĞ°Ñ…. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ±Ñ‹Ğ» Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ SynCode-Instruct Ğ¾Ğ±ÑŠĞµĞ¼Ğ¾Ğ¼ Ğ¾ĞºĞ¾Ğ»Ğ¾ 20 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´Ğ»Ñ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ ĞºĞ¾Ğ´Ğ°, Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°Ñ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°.'}, 'en': {'title': 'Bridging Code Generation and Human Preferences with CodeArena', 'desc': 'This paper discusses the advancements in code large language models (codeLLMs) for code generation tasks. It highlights the limitations of existing benchmarks that primarily focus on code correctness without considering human preferences in real-world applications. To address this, the authors introduce CodeArena, a human-curated benchmark that includes diverse coding tasks across multiple programming languages. Additionally, they present SynCode-Instruct, a large synthetic instruction dataset that enhances the training of codeLLMs, revealing significant performance differences between various models when evaluated against human-aligned benchmarks.'}, 'zh': {'title': 'æå‡ä»£ç ç”Ÿæˆæ¨¡å‹ä¸äººç±»åå¥½çš„å¯¹é½', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„åŸºå‡†æµ‹è¯•å·¥å…·CodeArenaï¼Œç”¨äºè¯„ä¼°ä»£ç ç”Ÿæˆå¤§è¯­è¨€æ¨¡å‹ï¼ˆcode LLMsï¼‰çš„æ€§èƒ½ã€‚ç°æœ‰çš„åŸºå‡†æµ‹è¯•ä¸»è¦å…³æ³¨ä»£ç ç‰‡æ®µçš„æ­£ç¡®æ€§ï¼Œè€Œå¿½è§†äº†ä¸äººç±»åå¥½çš„å¯¹é½ã€‚CodeArenaé€šè¿‡æä¾›397ä¸ªé«˜è´¨é‡æ ·æœ¬ï¼Œæ¶µç›–40ä¸ªç±»åˆ«å’Œ44ç§ç¼–ç¨‹è¯­è¨€ï¼Œæ¨¡æ‹ŸçœŸå®ç¼–ç ä»»åŠ¡çš„å¤æ‚æ€§å’Œå¤šæ ·æ€§ã€‚ç ”ç©¶è¿˜æå‡ºäº†ä¸€ä¸ªå¤šæ ·åŒ–çš„åˆæˆæŒ‡ä»¤è¯­æ–™åº“SynCode-Instructï¼Œä»¥éªŒè¯å¤§è§„æ¨¡åˆæˆæŒ‡ä»¤å¾®è°ƒçš„æœ‰æ•ˆæ€§ï¼Œç»“æœæ˜¾ç¤ºå¼€æºä»£ç LLMsä¸ä¸“æœ‰LLMsä¹‹é—´å­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½å·®è·ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.07589', 'title': 'DiffSensei: Bridging Multi-Modal LLMs and Diffusion Models for Customized Manga Generation', 'url': 'https://huggingface.co/papers/2412.07589', 'abstract': 'Story visualization, the task of creating visual narratives from textual descriptions, has seen progress with text-to-image generation models. However, these models often lack effective control over character appearances and interactions, particularly in multi-character scenes. To address these limitations, we propose a new task: customized manga generation and introduce DiffSensei, an innovative framework specifically designed for generating manga with dynamic multi-character control. DiffSensei integrates a diffusion-based image generator with a multimodal large language model (MLLM) that acts as a text-compatible identity adapter. Our approach employs masked cross-attention to seamlessly incorporate character features, enabling precise layout control without direct pixel transfer. Additionally, the MLLM-based adapter adjusts character features to align with panel-specific text cues, allowing flexible adjustments in character expressions, poses, and actions. We also introduce MangaZero, a large-scale dataset tailored to this task, containing 43,264 manga pages and 427,147 annotated panels, supporting the visualization of varied character interactions and movements across sequential frames. Extensive experiments demonstrate that DiffSensei outperforms existing models, marking a significant advancement in manga generation by enabling text-adaptable character customization. The project page is https://jianzongwu.github.io/projects/diffsensei/.', 'score': 26, 'issue_id': 1061, 'pub_date': '2024-12-10', 'pub_date_card': {'ru': '10 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 10', 'zh': '12æœˆ10æ—¥'}, 'hash': '8a1bb8ed9ae040f2', 'authors': ['Jianzong Wu', 'Chao Tang', 'Jingbo Wang', 'Yanhong Zeng', 'Xiangtai Li', 'Yunhai Tong'], 'affiliations': ['Bytedance Seed Project', 'Nanyang Technological University', 'Peking University', 'Shanghai AI Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2412.07589.jpg', 'data': {'categories': ['#multimodal', '#diffusion', '#story_generation', '#dataset', '#cv'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'DiffSensei: Ğ˜Ğ˜-Ñ…ÑƒĞ´Ğ¾Ğ¶Ğ½Ğ¸Ğº Ğ¼Ğ°Ğ½Ğ³Ğ¸ Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ DiffSensei - Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ°Ğ½Ğ³Ğ¸ Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹. DiffSensei Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ ĞºÑ€Ğ¾ÑÑ-Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½Ğ¾Ğ²ĞºĞ¸ Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³Ğ¸Ğ±ĞºĞ¾ Ğ½Ğ°ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ²Ñ‹Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ğ·Ñ‹ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… MangaZero Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Dynamic Manga Generation with Character Control', 'desc': 'This paper presents DiffSensei, a novel framework for generating manga that allows for dynamic control of multiple characters based on textual descriptions. It combines a diffusion-based image generator with a multimodal large language model (MLLM) to effectively manage character identities and interactions in complex scenes. The framework utilizes masked cross-attention to integrate character features, enabling precise layout control and adjustments in expressions, poses, and actions according to panel-specific text cues. Additionally, the authors introduce MangaZero, a comprehensive dataset designed for this task, which significantly enhances the training and evaluation of manga generation models.'}, 'zh': {'title': 'å®šåˆ¶æ¼«ç”»ç”Ÿæˆçš„æ–°çªç ´', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„ä»»åŠ¡ï¼šå®šåˆ¶æ¼«ç”»ç”Ÿæˆï¼Œå¹¶ä»‹ç»äº†DiffSenseiæ¡†æ¶ï¼Œæ—¨åœ¨å®ç°åŠ¨æ€å¤šè§’è‰²æ§åˆ¶çš„æ¼«ç”»ç”Ÿæˆã€‚è¯¥æ¡†æ¶ç»“åˆäº†åŸºäºæ‰©æ•£çš„å›¾åƒç”Ÿæˆå™¨å’Œå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œé€šè¿‡æ©è”½äº¤å‰æ³¨æ„åŠ›æœºåˆ¶æœ‰æ•ˆæ•´åˆè§’è‰²ç‰¹å¾ã€‚DiffSenseièƒ½å¤Ÿæ ¹æ®é¢æ¿ç‰¹å®šçš„æ–‡æœ¬æç¤ºçµæ´»è°ƒæ•´è§’è‰²çš„è¡¨æƒ…ã€å§¿åŠ¿å’ŒåŠ¨ä½œï¼Œä»è€Œå®ç°ç²¾ç¡®çš„å¸ƒå±€æ§åˆ¶ã€‚æˆ‘ä»¬è¿˜æ¨å‡ºäº†MangaZeroæ•°æ®é›†ï¼ŒåŒ…å«43,264é¡µæ¼«ç”»å’Œ427,147ä¸ªæ³¨é‡Šé¢æ¿ï¼Œæ”¯æŒå¤šæ ·åŒ–è§’è‰²äº¤äº’å’ŒåŠ¨ä½œçš„å¯è§†åŒ–ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.04653', 'title': 'Hidden in the Noise: Two-Stage Robust Watermarking for Images', 'url': 'https://huggingface.co/papers/2412.04653', 'abstract': "As the quality of image generators continues to improve, deepfakes become a topic of considerable societal debate. Image watermarking allows responsible model owners to detect and label their AI-generated content, which can mitigate the harm. Yet, current state-of-the-art methods in image watermarking remain vulnerable to forgery and removal attacks. This vulnerability occurs in part because watermarks distort the distribution of generated images, unintentionally revealing information about the watermarking techniques.   In this work, we first demonstrate a distortion-free watermarking method for images, based on a diffusion model's initial noise. However, detecting the watermark requires comparing the initial noise reconstructed for an image to all previously used initial noises. To mitigate these issues, we propose a two-stage watermarking framework for efficient detection. During generation, we augment the initial noise with generated Fourier patterns to embed information about the group of initial noises we used. For detection, we (i) retrieve the relevant group of noises, and (ii) search within the given group for an initial noise that might match our image. This watermarking approach achieves state-of-the-art robustness to forgery and removal against a large battery of attacks.", 'score': 20, 'issue_id': 1060, 'pub_date': '2024-12-05', 'pub_date_card': {'ru': '5 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 5', 'zh': '12æœˆ5æ—¥'}, 'hash': '8032f89319a70b88', 'authors': ['Kasra Arabi', 'Benjamin Feuer', 'R. Teal Witter', 'Chinmay Hegde', 'Niv Cohen'], 'affiliations': ['New York University'], 'pdf_title_img': 'assets/pdf/title_img/2412.04653.jpg', 'data': {'categories': ['#diffusion', '#security', '#rag', '#cv'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'ĞĞµĞ²Ğ¸Ğ´Ğ¸Ğ¼Ñ‹Ğµ Ğ¸ Ğ½ĞµÑƒÑĞ·Ğ²Ğ¸Ğ¼Ñ‹Ğµ: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğµ AI-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ²Ğ¾Ğ´ÑĞ½Ñ‹Ñ… Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ¾Ğ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑˆÑƒĞ¼Ğ° Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ² Ğ¤ÑƒÑ€ÑŒĞµ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¾Ğ´ÑĞ½Ñ‹Ğµ Ğ·Ğ½Ğ°ĞºĞ¸ Ğ±ĞµĞ· Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğº Ğ°Ñ‚Ğ°ĞºĞ°Ğ¼ Ğ¿Ğ¾Ğ´Ğ´ĞµĞ»ĞºĞ¸ Ğ¸ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹.'}, 'en': {'title': 'Robust Watermarking for AI-Generated Images', 'desc': 'This paper presents a novel approach to image watermarking that aims to enhance the security of AI-generated content against forgery and removal attacks. The authors introduce a distortion-free watermarking method that utilizes the initial noise from a diffusion model, which helps to avoid revealing watermarking techniques. They propose a two-stage framework that first augments the initial noise with Fourier patterns to embed watermark information, and then efficiently detects the watermark by comparing it to a group of initial noises. This method demonstrates significant robustness against various attacks, setting a new standard in the field of image watermarking.'}, 'zh': {'title': 'æ— å¤±çœŸæ°´å°ï¼Œä¿æŠ¤AIç”Ÿæˆå†…å®¹çš„æœªæ¥', 'desc': 'éšç€å›¾åƒç”ŸæˆæŠ€æœ¯çš„ä¸æ–­è¿›æ­¥ï¼Œæ·±åº¦ä¼ªé€ æˆä¸ºç¤¾ä¼šè®¨è®ºçš„çƒ­ç‚¹ã€‚å›¾åƒæ°´å°æŠ€æœ¯å¯ä»¥å¸®åŠ©æ¨¡å‹æ‹¥æœ‰è€…æ£€æµ‹å’Œæ ‡è®°ä»–ä»¬ç”Ÿæˆçš„å†…å®¹ï¼Œä»è€Œå‡å°‘æ½œåœ¨çš„å±å®³ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ°´å°æ–¹æ³•åœ¨é¢å¯¹ä¼ªé€ å’Œå»é™¤æ”»å‡»æ—¶ä»ç„¶å­˜åœ¨è„†å¼±æ€§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ— å¤±çœŸæ°´å°æ–¹æ³•ï¼Œå¹¶é€šè¿‡ä¸¤é˜¶æ®µæ¡†æ¶æé«˜äº†æ°´å°çš„æ£€æµ‹æ•ˆç‡ï¼Œæ˜¾è‘—å¢å¼ºäº†å¯¹å„ç§æ”»å‡»çš„é²æ£’æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.07674', 'title': 'FiVA: Fine-grained Visual Attribute Dataset for Text-to-Image Diffusion Models', 'url': 'https://huggingface.co/papers/2412.07674', 'abstract': 'Recent advances in text-to-image generation have enabled the creation of high-quality images with diverse applications. However, accurately describing desired visual attributes can be challenging, especially for non-experts in art and photography. An intuitive solution involves adopting favorable attributes from the source images. Current methods attempt to distill identity and style from source images. However, "style" is a broad concept that includes texture, color, and artistic elements, but does not cover other important attributes such as lighting and dynamics. Additionally, a simplified "style" adaptation prevents combining multiple attributes from different sources into one generated image. In this work, we formulate a more effective approach to decompose the aesthetics of a picture into specific visual attributes, allowing users to apply characteristics such as lighting, texture, and dynamics from different images. To achieve this goal, we constructed the first fine-grained visual attributes dataset (FiVA) to the best of our knowledge. This FiVA dataset features a well-organized taxonomy for visual attributes and includes around 1 M high-quality generated images with visual attribute annotations. Leveraging this dataset, we propose a fine-grained visual attribute adaptation framework (FiVA-Adapter), which decouples and adapts visual attributes from one or more source images into a generated one. This approach enhances user-friendly customization, allowing users to selectively apply desired attributes to create images that meet their unique preferences and specific content requirements.', 'score': 17, 'issue_id': 1061, 'pub_date': '2024-12-10', 'pub_date_card': {'ru': '10 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 10', 'zh': '12æœˆ10æ—¥'}, 'hash': '339e4e8d1664472e', 'authors': ['Tong Wu', 'Yinghao Xu', 'Ryan Po', 'Mengchen Zhang', 'Guandao Yang', 'Jiaqi Wang', 'Ziwei Liu', 'Dahua Lin', 'Gordon Wetzstein'], 'affiliations': ['CPII under InnoHK', 'S-Lab, NTU', 'Shanghai Artificial Intelligence Laboratory', 'Stanford University', 'The Chinese University of Hong Kong', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2412.07674.jpg', 'data': {'categories': ['#dataset', '#cv', '#synthetic'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ¢Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ¾Ğ² Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ FiVA Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¾Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ¾Ğ² Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº FiVA-Adapter Ğ´Ğ»Ñ Ğ¸Ñ… Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‚Ğ°ĞºĞ¸Ğµ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ĞºĞ°Ğº Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ğµ, Ñ‚ĞµĞºÑÑ‚ÑƒÑ€Ğ° Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ°. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°ÑÑ‚Ğ¾Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ´Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ñ‹ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ².'}, 'en': {'title': 'Customize Your Images with Fine-Grained Visual Attributes!', 'desc': 'This paper presents a new method for text-to-image generation that allows users to customize images by selecting specific visual attributes from different source images. The authors introduce the FiVA dataset, which contains around 1 million high-quality images annotated with fine-grained visual attributes like lighting, texture, and dynamics. Their proposed framework, FiVA-Adapter, effectively decouples these attributes, enabling users to combine them in a single generated image. This approach improves the flexibility and user-friendliness of image generation, making it easier for non-experts to achieve their desired visual outcomes.'}, 'zh': {'title': 'ç»†ç²’åº¦è§†è§‰å±æ€§é€‚é…ï¼Œå®šåˆ¶ä½ çš„å›¾åƒï¼', 'desc': 'æœ€è¿‘ï¼Œæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„æŠ€æœ¯å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œèƒ½å¤Ÿåˆ›å»ºé«˜è´¨é‡çš„å›¾åƒï¼Œåº”ç”¨å¹¿æ³›ã€‚ç„¶è€Œï¼Œå‡†ç¡®æè¿°æ‰€éœ€çš„è§†è§‰å±æ€§å¯¹éä¸“ä¸šäººå£«æ¥è¯´å¯èƒ½å¾ˆå›°éš¾ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œå°†å›¾åƒçš„ç¾å­¦åˆ†è§£ä¸ºå…·ä½“çš„è§†è§‰å±æ€§ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿä»ä¸åŒçš„å›¾åƒä¸­åº”ç”¨ç‰¹å¾ï¼Œå¦‚å…‰ç…§ã€çº¹ç†å’ŒåŠ¨æ€ã€‚æˆ‘ä»¬æ„å»ºäº†ç¬¬ä¸€ä¸ªç»†ç²’åº¦è§†è§‰å±æ€§æ•°æ®é›†ï¼ˆFiVAï¼‰ï¼Œå¹¶æå‡ºäº†FiVA-Adapteræ¡†æ¶ï¼Œä»¥ä¾¿ç”¨æˆ·èƒ½å¤Ÿæ ¹æ®ä¸ªäººåå¥½å®šåˆ¶ç”Ÿæˆçš„å›¾åƒã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.07759', 'title': '3DTrajMaster: Mastering 3D Trajectory for Multi-Entity Motion in Video Generation', 'url': 'https://huggingface.co/papers/2412.07759', 'abstract': 'This paper aims to manipulate multi-entity 3D motions in video generation. Previous methods on controllable video generation primarily leverage 2D control signals to manipulate object motions and have achieved remarkable synthesis results. However, 2D control signals are inherently limited in expressing the 3D nature of object motions. To overcome this problem, we introduce 3DTrajMaster, a robust controller that regulates multi-entity dynamics in 3D space, given user-desired 6DoF pose (location and rotation) sequences of entities. At the core of our approach is a plug-and-play 3D-motion grounded object injector that fuses multiple input entities with their respective 3D trajectories through a gated self-attention mechanism. In addition, we exploit an injector architecture to preserve the video diffusion prior, which is crucial for generalization ability. To mitigate video quality degradation, we introduce a domain adaptor during training and employ an annealed sampling strategy during inference. To address the lack of suitable training data, we construct a 360-Motion Dataset, which first correlates collected 3D human and animal assets with GPT-generated trajectory and then captures their motion with 12 evenly-surround cameras on diverse 3D UE platforms. Extensive experiments show that 3DTrajMaster sets a new state-of-the-art in both accuracy and generalization for controlling multi-entity 3D motions. Project page: http://fuxiao0719.github.io/projects/3dtrajmaster', 'score': 16, 'issue_id': 1064, 'pub_date': '2024-12-10', 'pub_date_card': {'ru': '10 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 10', 'zh': '12æœˆ10æ—¥'}, 'hash': '0e2f96c50d396f1d', 'authors': ['Xiao Fu', 'Xian Liu', 'Xintao Wang', 'Sida Peng', 'Menghan Xia', 'Xiaoyu Shi', 'Ziyang Yuan', 'Pengfei Wan', 'Di Zhang', 'Dahua Lin'], 'affiliations': ['Kuaishou Technology', 'The Chinese University of Hong Kong', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2412.07759.jpg', 'data': {'categories': ['#video', '#3d', '#dataset'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Ğ£Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ 3D-Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ĞµĞ¼ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ 3DTrajMaster - ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ»ĞµÑ€ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¾Ğ¹ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ° Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² 3D-Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚ - Ğ¸Ğ½Ğ¶ĞµĞºÑ‚Ğ¾Ñ€ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ 3D-Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ³ĞµĞ¹Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ°Ğ¼Ğ¾Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. Ğ”Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ÑÑ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€ Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ñ‚Ğ¶Ğ¸Ğ³Ğ° Ğ¿Ñ€Ğ¸ ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ 360-Motion Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Mastering 3D Motion Control in Video Generation', 'desc': 'This paper presents 3DTrajMaster, a novel approach for controlling multi-entity motions in 3D video generation. Unlike previous methods that rely on 2D control signals, 3DTrajMaster utilizes 6DoF pose sequences to accurately represent the 3D dynamics of objects. The method incorporates a gated self-attention mechanism to integrate multiple entities with their 3D trajectories, enhancing the realism of generated videos. Additionally, the authors introduce a 360-Motion Dataset to improve training data quality, leading to superior performance in both accuracy and generalization for 3D motion control.'}, 'zh': {'title': 'æŒæ§ä¸‰ç»´è¿åŠ¨ï¼Œé‡å¡‘è§†é¢‘ç”Ÿæˆ', 'desc': 'æœ¬æ–‡æ—¨åœ¨æ“æ§è§†é¢‘ç”Ÿæˆä¸­çš„å¤šå®ä½“ä¸‰ç»´è¿åŠ¨ã€‚ä»¥å¾€çš„å¯æ§è§†é¢‘ç”Ÿæˆæ–¹æ³•ä¸»è¦ä¾èµ–äºŒç»´æ§åˆ¶ä¿¡å·æ¥æ“æ§ç‰©ä½“è¿åŠ¨ï¼Œä½†äºŒç»´ä¿¡å·åœ¨è¡¨è¾¾ä¸‰ç»´è¿åŠ¨ç‰¹æ€§æ–¹é¢å­˜åœ¨å±€é™ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†3DTrajMasterï¼Œè¿™æ˜¯ä¸€ç§å¼ºå¤§çš„æ§åˆ¶å™¨ï¼Œå¯ä»¥æ ¹æ®ç”¨æˆ·æœŸæœ›çš„å…­è‡ªç”±åº¦å§¿æ€åºåˆ—è°ƒèŠ‚ä¸‰ç»´ç©ºé—´ä¸­çš„å¤šå®ä½“åŠ¨æ€ã€‚æˆ‘ä»¬çš„ç ”ç©¶é€šè¿‡ä¸€ä¸ªæ’ä»¶å¼çš„ä¸‰ç»´è¿åŠ¨åŸºç¡€ç‰©ä½“æ³¨å…¥å™¨ï¼Œç»“åˆå¤šä¸ªè¾“å…¥å®ä½“åŠå…¶ç›¸åº”çš„ä¸‰ç»´è½¨è¿¹ï¼Œåˆ©ç”¨é—¨æ§è‡ªæ³¨æ„åŠ›æœºåˆ¶å®ç°äº†è¿™ä¸€ç›®æ ‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.07774', 'title': 'UniReal: Universal Image Generation and Editing via Learning Real-world Dynamics', 'url': 'https://huggingface.co/papers/2412.07774', 'abstract': 'We introduce UniReal, a unified framework designed to address various image generation and editing tasks. Existing solutions often vary by tasks, yet share fundamental principles: preserving consistency between inputs and outputs while capturing visual variations. Inspired by recent video generation models that effectively balance consistency and variation across frames, we propose a unifying approach that treats image-level tasks as discontinuous video generation. Specifically, we treat varying numbers of input and output images as frames, enabling seamless support for tasks such as image generation, editing, customization, composition, etc. Although designed for image-level tasks, we leverage videos as a scalable source for universal supervision. UniReal learns world dynamics from large-scale videos, demonstrating advanced capability in handling shadows, reflections, pose variation, and object interaction, while also exhibiting emergent capability for novel applications.', 'score': 16, 'issue_id': 1058, 'pub_date': '2024-12-10', 'pub_date_card': {'ru': '10 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 10', 'zh': '12æœˆ10æ—¥'}, 'hash': '64e24bea8dffc31d', 'authors': ['Xi Chen', 'Zhifei Zhang', 'He Zhang', 'Yuqian Zhou', 'Soo Ye Kim', 'Qing Liu', 'Yijun Li', 'Jianming Zhang', 'Nanxuan Zhao', 'Yilin Wang', 'Hui Ding', 'Zhe Lin', 'Hengshuang Zhao'], 'affiliations': ['Adobe Research', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2412.07774.jpg', 'data': {'categories': ['#cv', '#multimodal', '#video'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…', 'desc': 'UniReal - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ½Ğ° Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ ĞºĞ°Ğº Ğ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ¸ÑÑ‚ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ¸ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ ĞºĞ°Ğ´Ñ€Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ĞµĞ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ñ‚ĞµĞ½Ğ¸, Ğ¾Ñ‚Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ· Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². UniReal Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'UniReal: Unifying Image Generation and Editing through Video Dynamics', 'desc': 'UniReal is a comprehensive framework that simplifies various image generation and editing tasks by treating them as a form of video generation. It focuses on maintaining consistency between input and output images while allowing for visual variations, similar to how video frames work. By using videos as a source of universal supervision, UniReal learns complex world dynamics, enabling it to manage challenges like shadows, reflections, and object interactions effectively. This approach not only enhances traditional image tasks but also opens up new possibilities for innovative applications in image processing.'}, 'zh': {'title': 'ç»Ÿä¸€æ¡†æ¶ï¼Œå›¾åƒç”Ÿæˆä¸ç¼–è¾‘çš„æœªæ¥', 'desc': 'UniRealæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å„ç§å›¾åƒç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡ã€‚è¯¥æ¡†æ¶é€šè¿‡å°†å›¾åƒä»»åŠ¡è§†ä¸ºä¸è¿ç»­çš„è§†é¢‘ç”Ÿæˆï¼Œæ¥ä¿æŒè¾“å…¥å’Œè¾“å‡ºä¹‹é—´çš„ä¸€è‡´æ€§ï¼ŒåŒæ—¶æ•æ‰è§†è§‰å˜åŒ–ã€‚UniRealåˆ©ç”¨å¤§è§„æ¨¡è§†é¢‘ä½œä¸ºé€šç”¨ç›‘ç£æºï¼Œå­¦ä¹ ä¸–ç•ŒåŠ¨æ€ï¼Œä»è€Œåœ¨å¤„ç†é˜´å½±ã€åå°„ã€å§¿æ€å˜åŒ–å’Œç‰©ä½“äº¤äº’æ–¹é¢å±•ç°å‡ºå…ˆè¿›çš„èƒ½åŠ›ã€‚è¯¥æ–¹æ³•ä¸ä»…é€‚ç”¨äºå›¾åƒä»»åŠ¡ï¼Œè¿˜å±•ç°å‡ºå¯¹æ–°åº”ç”¨çš„æ½œåœ¨èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.07583', 'title': 'Mobile Video Diffusion', 'url': 'https://huggingface.co/papers/2412.07583', 'abstract': 'Video diffusion models have achieved impressive realism and controllability but are limited by high computational demands, restricting their use on mobile devices. This paper introduces the first mobile-optimized video diffusion model. Starting from a spatio-temporal UNet from Stable Video Diffusion (SVD), we reduce memory and computational cost by reducing the frame resolution, incorporating multi-scale temporal representations, and introducing two novel pruning schema to reduce the number of channels and temporal blocks. Furthermore, we employ adversarial finetuning to reduce the denoising to a single step. Our model, coined as MobileVD, is 523x more efficient (1817.2 vs. 4.34 TFLOPs) with a slight quality drop (FVD 149 vs. 171), generating latents for a 14x512x256 px clip in 1.7 seconds on a Xiaomi-14 Pro. Our results are available at https://qualcomm-ai-research.github.io/mobile-video-diffusion/', 'score': 15, 'issue_id': 1067, 'pub_date': '2024-12-10', 'pub_date_card': {'ru': '10 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 10', 'zh': '12æœˆ10æ—¥'}, 'hash': '0e1944ce23519ab1', 'authors': ['Haitam Ben Yahia', 'Denis Korzhenkov', 'Ioannis Lelekas', 'Amir Ghodrati', 'Amirhossein Habibian'], 'affiliations': ['Qualcomm AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2412.07583.jpg', 'data': {'categories': ['#inference', '#training', '#optimization', '#small_models', '#video', '#diffusion'], 'emoji': 'ğŸ“±', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ…', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ MobileVD - Ğ¿ĞµÑ€Ğ²ÑƒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ½Ğ¸ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Stable Video Diffusion, ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ¸Ğ² Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ğ´Ñ€Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ² Ğ½Ğ¾Ğ²Ñ‹Ğµ ÑÑ…ĞµĞ¼Ñ‹ Ğ¿Ñ€ÑƒĞ½Ğ¸Ğ½Ğ³Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ´Ğ½Ğ¾ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğµ ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ¾ÑÑ‚ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ´Ğ¾Ğ²Ğ¾Ğ´ĞºĞ¸. MobileVD Ğ² 523 Ñ€Ğ°Ğ·Ğ° ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ĞµĞµ Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸ Ğ½ĞµĞ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ»Ğ¸Ğ¿Ğ° 14x512x256 Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹ Ğ·Ğ° 1,7 ÑĞµĞºÑƒĞ½Ğ´Ñ‹ Ğ½Ğ° Xiaomi-14 Pro.'}, 'en': {'title': 'MobileVD: Efficient Video Diffusion for Mobile Devices', 'desc': 'This paper presents MobileVD, a mobile-optimized video diffusion model that enhances efficiency while maintaining quality. It builds on the spatio-temporal UNet architecture from Stable Video Diffusion and reduces computational demands by lowering frame resolution and using multi-scale temporal representations. The authors introduce two innovative pruning techniques to minimize the number of channels and temporal blocks, significantly cutting down memory usage. Additionally, adversarial finetuning is applied to streamline the denoising process, achieving a remarkable 523x efficiency improvement with only a minor quality reduction.'}, 'zh': {'title': 'ç§»åŠ¨è®¾å¤‡ä¸Šçš„é«˜æ•ˆè§†é¢‘æ‰©æ•£æ¨¡å‹', 'desc': 'è§†é¢‘æ‰©æ•£æ¨¡å‹åœ¨çœŸå®æ„Ÿå’Œå¯æ§æ€§æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†ç”±äºè®¡ç®—éœ€æ±‚é«˜ï¼Œé™åˆ¶äº†å…¶åœ¨ç§»åŠ¨è®¾å¤‡ä¸Šçš„åº”ç”¨ã€‚æœ¬æ–‡ä»‹ç»äº†é¦–ä¸ªé’ˆå¯¹ç§»åŠ¨è®¾å¤‡ä¼˜åŒ–çš„è§†é¢‘æ‰©æ•£æ¨¡å‹ã€‚æˆ‘ä»¬é€šè¿‡é™ä½å¸§åˆ†è¾¨ç‡ã€ç»“åˆå¤šå°ºåº¦æ—¶é—´è¡¨ç¤ºä»¥åŠå¼•å…¥ä¸¤ç§æ–°é¢–çš„å‰ªææ–¹æ¡ˆæ¥å‡å°‘å†…å­˜å’Œè®¡ç®—æˆæœ¬ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é‡‡ç”¨å¯¹æŠ—å¾®è°ƒå°†å»å™ªè¿‡ç¨‹ç®€åŒ–ä¸ºå•æ­¥æ“ä½œã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.07626', 'title': 'OmniDocBench: Benchmarking Diverse PDF Document Parsing with Comprehensive Annotations', 'url': 'https://huggingface.co/papers/2412.07626', 'abstract': 'Document content extraction is crucial in computer vision, especially for meeting the high-quality data needs of large language models (LLMs) and retrieval-augmented generation (RAG) technologies. However, current document parsing methods suffer from significant limitations in terms of diversity and comprehensive evaluation. To address these challenges, we introduce OmniDocBench, a novel multi-source benchmark designed to advance automated document content extraction. OmniDocBench includes a meticulously curated and annotated high-quality evaluation dataset comprising nine diverse document types, such as academic papers, textbooks, slides, among others. Our benchmark provides a flexible and comprehensive evaluation framework with 19 layout category labels and 14 attribute labels, enabling multi-level assessments across entire datasets, individual modules, or specific data types. Using OmniDocBench, we perform an exhaustive comparative analysis of existing modular pipelines and multimodal end-to-end methods, highlighting their limitations in handling document diversity and ensuring fair evaluation. OmniDocBench establishes a robust, diverse, and fair evaluation standard for the document content extraction field, offering crucial insights for future advancements and fostering the development of document parsing technologies. The codes and dataset is available in https://github.com/opendatalab/OmniDocBench.', 'score': 15, 'issue_id': 1065, 'pub_date': '2024-12-10', 'pub_date_card': {'ru': '10 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 10', 'zh': '12æœˆ10æ—¥'}, 'hash': '4bba9d3934addbcc', 'authors': ['Linke Ouyang', 'Yuan Qu', 'Hongbin Zhou', 'Jiawei Zhu', 'Rui Zhang', 'Qunshu Lin', 'Bin Wang', 'Zhiyuan Zhao', 'Man Jiang', 'Xiaomeng Zhao', 'Jin Shi', 'Fan Wu', 'Pei Chu', 'Minghao Liu', 'Zhenxiang Li', 'Chao Xu', 'Bo Zhang', 'Botian Shi', 'Zhongying Tu', 'Conghui He'], 'affiliations': ['Abaka AI', 'Shanghai AI Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2412.07626.jpg', 'data': {'categories': ['#multimodal', '#dataset', '#benchmark'], 'emoji': 'ğŸ“„', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ¸Ğ· Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'OmniDocBench - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ¸Ğ· Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Ğ´ĞµĞ²ÑÑ‚Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ³Ğ¸Ğ±ĞºÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ 19 Ğ¼ĞµÑ‚ĞºĞ°Ğ¼Ğ¸ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¹ Ğ¼Ğ°ĞºĞµÑ‚Ğ° Ğ¸ 14 Ğ¼ĞµÑ‚ĞºĞ°Ğ¼Ğ¸ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ¾Ğ². Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ OmniDocBench, Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½Ğ¾Ğ² Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… end-to-end Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ².'}, 'en': {'title': 'OmniDocBench: Elevating Document Content Extraction Standards', 'desc': 'This paper introduces OmniDocBench, a new benchmark aimed at improving document content extraction in computer vision. It addresses the limitations of current parsing methods by providing a diverse and comprehensive evaluation framework with a curated dataset of nine document types. The benchmark includes 19 layout category labels and 14 attribute labels, allowing for detailed assessments of various extraction methods. By conducting a thorough analysis of existing techniques, OmniDocBench sets a new standard for evaluating document parsing technologies, promoting advancements in the field.'}, 'zh': {'title': 'OmniDocBenchï¼šæ–‡æ¡£æå–çš„æ–°æ ‡å‡†', 'desc': 'æ–‡æ¡£å†…å®¹æå–åœ¨è®¡ç®—æœºè§†è§‰ä¸­è‡³å…³é‡è¦ï¼Œå°¤å…¶æ˜¯æ»¡è¶³å¤§å‹è¯­è¨€æ¨¡å‹å’Œå¢å¼ºæ£€ç´¢ç”ŸæˆæŠ€æœ¯çš„é«˜è´¨é‡æ•°æ®éœ€æ±‚ã€‚ç„¶è€Œï¼Œç›®å‰çš„æ–‡æ¡£è§£ææ–¹æ³•åœ¨å¤šæ ·æ€§å’Œå…¨é¢è¯„ä¼°æ–¹é¢å­˜åœ¨æ˜¾è‘—å±€é™ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†OmniDocBenchï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„å¤šæºåŸºå‡†ï¼Œæ—¨åœ¨æ¨åŠ¨è‡ªåŠ¨åŒ–æ–‡æ¡£å†…å®¹æå–çš„å‘å±•ã€‚OmniDocBenchæä¾›äº†ä¸€ä¸ªçµæ´»ä¸”å…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼Œèƒ½å¤Ÿå¯¹æ–‡æ¡£å¤šæ ·æ€§è¿›è¡Œæ·±å…¥åˆ†æï¼Œå¹¶ä¸ºæœªæ¥çš„æ–‡æ¡£è§£ææŠ€æœ¯å‘å±•æä¾›é‡è¦è§è§£ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.07724', 'title': 'Granite Guardian', 'url': 'https://huggingface.co/papers/2412.07724', 'abstract': 'We introduce the Granite Guardian models, a suite of safeguards designed to provide risk detection for prompts and responses, enabling safe and responsible use in combination with any large language model (LLM). These models offer comprehensive coverage across multiple risk dimensions, including social bias, profanity, violence, sexual content, unethical behavior, jailbreaking, and hallucination-related risks such as context relevance, groundedness, and answer relevance for retrieval-augmented generation (RAG). Trained on a unique dataset combining human annotations from diverse sources and synthetic data, Granite Guardian models address risks typically overlooked by traditional risk detection models, such as jailbreaks and RAG-specific issues. With AUC scores of 0.871 and 0.854 on harmful content and RAG-hallucination-related benchmarks respectively, Granite Guardian is the most generalizable and competitive model available in the space. Released as open-source, Granite Guardian aims to promote responsible AI development across the community.   https://github.com/ibm-granite/granite-guardian', 'score': 14, 'issue_id': 1058, 'pub_date': '2024-12-10', 'pub_date_card': {'ru': '10 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 10', 'zh': '12æœˆ10æ—¥'}, 'hash': '9d3367b124b8b792', 'authors': ['Inkit Padhi', 'Manish Nagireddy', 'Giandomenico Cornacchia', 'Subhajit Chaudhury', 'Tejaswini Pedapati', 'Pierre Dognin', 'Keerthiram Murugesan', 'Erik Miehling', 'MartÃ­n SantillÃ¡n Cooper', 'Kieran Fraser', 'Giulio Zizzo', 'Muhammad Zaid Hameed', 'Mark Purcell', 'Michael Desmond', 'Qian Pan', 'Inge Vejsbjerg', 'Elizabeth M. Daly', 'Michael Hind', 'Werner Geyer', 'Ambrish Rawat', 'Kush R. Varshney', 'Prasanna Sattigeri'], 'affiliations': ['IBM Research'], 'pdf_title_img': 'assets/pdf/title_img/2412.07724.jpg', 'data': {'categories': ['#rag', '#hallucinations', '#dataset', '#open_source', '#benchmark', '#ethics', '#synthetic', '#multimodal'], 'emoji': 'ğŸ›¡ï¸', 'ru': {'title': 'Ğ—Ğ°Ñ‰Ğ¸Ñ‚Ğ½Ğ¸Ğº Ğ˜Ğ˜: Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Granite Guardian - Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ñ€Ğ¸ÑĞºĞ¾Ğ² Ğ² Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ñ… Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). Ğ­Ñ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ°ÑĞ¿ĞµĞºÑ‚Ñ‹ Ñ€Ğ¸ÑĞºĞ°, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ½ĞµĞ½Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ»ĞµĞºÑĞ¸ĞºÑƒ, Ğ½Ğ°ÑĞ¸Ğ»Ğ¸Ğµ, ÑĞµĞºÑÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚, Ğ½ĞµÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ¸ Ñ€Ğ¸ÑĞºĞ¸, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰ĞµĞ¼ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğµ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Granite Guardian Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¸ Ñ€Ğ¸ÑĞºĞ¾Ğ². ĞŸÑ€Ğ¾ĞµĞºÑ‚ Ğ²Ñ‹Ğ¿ÑƒÑ‰ĞµĞ½ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°.'}, 'en': {'title': 'Granite Guardian: Safeguarding AI with Comprehensive Risk Detection', 'desc': 'The Granite Guardian models are designed to enhance the safety of large language models (LLMs) by detecting various risks in prompts and responses. They cover a wide range of risk factors, including social bias, profanity, and hallucination-related issues, which are often missed by traditional models. These models are trained on a unique dataset that combines human annotations and synthetic data, making them effective at identifying risks like jailbreaking and retrieval-augmented generation (RAG) problems. With high AUC scores, Granite Guardian represents a significant advancement in responsible AI development and is available as open-source for community use.'}, 'zh': {'title': 'Granite Guardianï¼šå®‰å…¨ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„å®ˆæŠ¤è€…', 'desc': 'Granite Guardianæ¨¡å‹æ˜¯ä¸€å¥—æ—¨åœ¨æä¾›é£é™©æ£€æµ‹çš„å®‰å…¨ä¿éšœå·¥å…·ï¼Œé€‚ç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å®‰å…¨å’Œè´Ÿè´£ä»»ä½¿ç”¨ã€‚è¿™äº›æ¨¡å‹è¦†ç›–å¤šä¸ªé£é™©ç»´åº¦ï¼ŒåŒ…æ‹¬ç¤¾ä¼šåè§ã€ç²—ä¿—è¯­è¨€ã€æš´åŠ›ã€æ€§å†…å®¹ã€ä¸é“å¾·è¡Œä¸ºã€è¶Šç‹±å’Œå¹»è§‰ç›¸å…³é£é™©ã€‚Granite Guardianæ¨¡å‹é€šè¿‡ç»“åˆæ¥è‡ªå¤šç§æ¥æºçš„äººç±»æ³¨é‡Šå’Œåˆæˆæ•°æ®è¿›è¡Œè®­ç»ƒï¼Œè§£å†³äº†ä¼ ç»Ÿé£é™©æ£€æµ‹æ¨¡å‹é€šå¸¸å¿½è§†çš„é£é™©ã€‚ä½œä¸ºå¼€æºé¡¹ç›®ï¼ŒGranite Guardianæ—¨åœ¨ä¿ƒè¿›ç¤¾åŒºå†…è´Ÿè´£ä»»çš„äººå·¥æ™ºèƒ½å‘å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.06578', 'title': 'MoViE: Mobile Diffusion for Video Editing', 'url': 'https://huggingface.co/papers/2412.06578', 'abstract': 'Recent progress in diffusion-based video editing has shown remarkable potential for practical applications. However, these methods remain prohibitively expensive and challenging to deploy on mobile devices. In this study, we introduce a series of optimizations that render mobile video editing feasible. Building upon the existing image editing model, we first optimize its architecture and incorporate a lightweight autoencoder. Subsequently, we extend classifier-free guidance distillation to multiple modalities, resulting in a threefold on-device speedup. Finally, we reduce the number of sampling steps to one by introducing a novel adversarial distillation scheme which preserves the controllability of the editing process. Collectively, these optimizations enable video editing at 12 frames per second on mobile devices, while maintaining high quality. Our results are available at https://qualcomm-ai-research.github.io/mobile-video-editing/', 'score': 12, 'issue_id': 1067, 'pub_date': '2024-12-09', 'pub_date_card': {'ru': '9 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 9', 'zh': '12æœˆ9æ—¥'}, 'hash': 'cd0edf1a976f2b03', 'authors': ['Adil Karjauv', 'Noor Fathima', 'Ioannis Lelekas', 'Fatih Porikli', 'Amir Ghodrati', 'Amirhossein Habibian'], 'affiliations': ['Qualcomm AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2412.06578.jpg', 'data': {'categories': ['#inference', '#optimization', '#diffusion', '#video', '#architecture'], 'emoji': 'ğŸ“±', 'ru': {'title': 'Ğ‘Ñ‹ÑÑ‚Ñ€Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ñ‹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ… Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ½ĞµĞ´Ñ€Ğ¸Ğ»Ğ¸ Ğ»ĞµĞ³ĞºĞ¸Ğ¹ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€. ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ñ€Ğ°ÑÑˆĞ¸Ñ€Ğ¸Ğ»Ğ¸ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ±ĞµĞ·ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ Ğ²Ğ²ĞµĞ»Ğ¸ Ğ½Ğ¾Ğ²ÑƒÑ ÑÑ…ĞµĞ¼Ñƒ ÑĞ¾ÑÑ‚ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸. Ğ­Ñ‚Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ 12 ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ² ÑĞµĞºÑƒĞ½Ğ´Ñƒ Ğ½Ğ° Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ… Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°.'}, 'en': {'title': 'Optimizing Mobile Video Editing for Speed and Quality', 'desc': 'This paper presents advancements in diffusion-based video editing specifically tailored for mobile devices. The authors optimize the existing image editing model by enhancing its architecture and integrating a lightweight autoencoder. They also apply classifier-free guidance distillation across multiple modalities, achieving a threefold increase in processing speed on mobile. Additionally, a novel adversarial distillation method is introduced to reduce sampling steps to one, ensuring high-quality video editing at 12 frames per second while maintaining user control.'}, 'zh': {'title': 'ç§»åŠ¨è§†é¢‘ç¼–è¾‘çš„é«˜æ•ˆä¼˜åŒ–æ–¹æ¡ˆ', 'desc': 'æœ¬ç ”ç©¶é’ˆå¯¹åŸºäºæ‰©æ•£çš„è§†é¢‘ç¼–è¾‘æ–¹æ³•è¿›è¡Œäº†ä¼˜åŒ–ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨ç§»åŠ¨è®¾å¤‡ä¸Šé«˜æ•ˆè¿è¡Œã€‚æˆ‘ä»¬é¦–å…ˆä¼˜åŒ–äº†ç°æœ‰çš„å›¾åƒç¼–è¾‘æ¨¡å‹ï¼Œå¹¶å¼•å…¥äº†è½»é‡çº§çš„è‡ªç¼–ç å™¨ã€‚æ¥ç€ï¼Œæˆ‘ä»¬å°†æ— åˆ†ç±»å™¨å¼•å¯¼è’¸é¦æ‰©å±•åˆ°å¤šç§æ¨¡æ€ï¼Œå®ç°äº†è®¾å¤‡ä¸Šçš„ä¸‰å€é€Ÿåº¦æå‡ã€‚æœ€åï¼Œé€šè¿‡å¼•å…¥æ–°é¢–çš„å¯¹æŠ—è’¸é¦æ–¹æ¡ˆï¼Œæˆ‘ä»¬å°†é‡‡æ ·æ­¥éª¤å‡å°‘åˆ°ä¸€ä¸ªï¼ŒåŒæ—¶ä¿æŒäº†ç¼–è¾‘è¿‡ç¨‹çš„å¯æ§æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.07776', 'title': 'Video Motion Transfer with Diffusion Transformers', 'url': 'https://huggingface.co/papers/2412.07776', 'abstract': 'We propose DiTFlow, a method for transferring the motion of a reference video to a newly synthesized one, designed specifically for Diffusion Transformers (DiT). We first process the reference video with a pre-trained DiT to analyze cross-frame attention maps and extract a patch-wise motion signal called the Attention Motion Flow (AMF). We guide the latent denoising process in an optimization-based, training-free, manner by optimizing latents with our AMF loss to generate videos reproducing the motion of the reference one. We also apply our optimization strategy to transformer positional embeddings, granting us a boost in zero-shot motion transfer capabilities. We evaluate DiTFlow against recently published methods, outperforming all across multiple metrics and human evaluation.', 'score': 12, 'issue_id': 1062, 'pub_date': '2024-12-10', 'pub_date_card': {'ru': '10 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 10', 'zh': '12æœˆ10æ—¥'}, 'hash': '57a229123b5b2c38', 'authors': ['Alexander Pondaven', 'Aliaksandr Siarohin', 'Sergey Tulyakov', 'Philip Torr', 'Fabio Pizzati'], 'affiliations': ['MBZUAI', 'Snap Inc.', 'University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2412.07776.jpg', 'data': {'categories': ['#training', '#video', '#optimization', '#diffusion', '#transfer_learning', '#multimodal', '#architecture'], 'emoji': 'ğŸ¬', 'ru': {'title': 'DiTFlow: ĞŸĞµÑ€ĞµĞ½Ğ¾Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ²', 'desc': 'DiTFlow - ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ° Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸Ğ· ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ğ½Ğ¾Ğ²Ğ¾Ğµ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğ¹ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ´Ğ»Ñ Ğ”Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¢Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² (DiT). ĞœĞµÑ‚Ğ¾Ğ´ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ°Ñ€Ñ‚Ñ‹ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ğ´Ñ€Ğ°Ğ¼Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ DiT Ğ¸ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ ÑĞ¸Ğ³Ğ½Ğ°Ğ» Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ğ°Ñ‚Ñ‡ĞµĞ¹, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Attention Motion Flow (AMF). DiTFlow Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ AMF, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµÑÑ‚Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¸Ğ· ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞœĞµÑ‚Ğ¾Ğ´ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğº Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ°Ğ¼ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°, ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ° Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Seamlessly Transfer Motion with DiTFlow!', 'desc': 'DiTFlow is a novel approach for transferring motion from a reference video to a newly created video using Diffusion Transformers (DiT). The method involves analyzing the reference video to extract Attention Motion Flow (AMF), which captures motion signals across frames. By optimizing the latent space with the AMF loss, DiTFlow effectively guides the video generation process without requiring additional training. The technique also enhances zero-shot motion transfer by optimizing transformer positional embeddings, demonstrating superior performance compared to existing methods in various evaluations.'}, 'zh': {'title': 'DiTFlowï¼šé«˜æ•ˆçš„è§†é¢‘è¿åŠ¨è½¬ç§»æ–¹æ³•', 'desc': 'æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºDiTFlowçš„æ–¹æ³•ï¼Œç”¨äºå°†å‚è€ƒè§†é¢‘çš„è¿åŠ¨è½¬ç§»åˆ°æ–°åˆæˆçš„è§†é¢‘ä¸Šï¼Œç‰¹åˆ«ä¸ºæ‰©æ•£å˜æ¢å™¨ï¼ˆDiTï¼‰è®¾è®¡ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä½¿ç”¨é¢„è®­ç»ƒçš„DiTå¤„ç†å‚è€ƒè§†é¢‘ï¼Œåˆ†æè·¨å¸§æ³¨æ„åŠ›å›¾ï¼Œå¹¶æå–ç§°ä¸ºæ³¨æ„åŠ›è¿åŠ¨æµï¼ˆAMFï¼‰çš„è¡¥ä¸çº§è¿åŠ¨ä¿¡å·ã€‚é€šè¿‡ä¼˜åŒ–AMFæŸå¤±ï¼Œæˆ‘ä»¬ä»¥æ— è®­ç»ƒçš„ä¼˜åŒ–æ–¹å¼å¼•å¯¼æ½œåœ¨å»å™ªè¿‡ç¨‹ï¼Œä»è€Œç”Ÿæˆå†ç°å‚è€ƒè§†é¢‘è¿åŠ¨çš„è§†é¢‘ã€‚æˆ‘ä»¬è¿˜å°†ä¼˜åŒ–ç­–ç•¥åº”ç”¨äºå˜æ¢å™¨çš„ä½ç½®åµŒå…¥ï¼Œæ˜¾è‘—æå‡äº†é›¶æ ·æœ¬è¿åŠ¨è½¬ç§»çš„èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.07334', 'title': 'Frame Representation Hypothesis: Multi-Token LLM Interpretability and Concept-Guided Text Generation', 'url': 'https://huggingface.co/papers/2412.07334', 'abstract': "Interpretability is a key challenge in fostering trust for Large Language Models (LLMs), which stems from the complexity of extracting reasoning from model's parameters. We present the Frame Representation Hypothesis, a theoretically robust framework grounded in the Linear Representation Hypothesis (LRH) to interpret and control LLMs by modeling multi-token words. Prior research explored LRH to connect LLM representations with linguistic concepts, but was limited to single token analysis. As most words are composed of several tokens, we extend LRH to multi-token words, thereby enabling usage on any textual data with thousands of concepts. To this end, we propose words can be interpreted as frames, ordered sequences of vectors that better capture token-word relationships. Then, concepts can be represented as the average of word frames sharing a common concept. We showcase these tools through Top-k Concept-Guided Decoding, which can intuitively steer text generation using concepts of choice. We verify said ideas on Llama 3.1, Gemma 2, and Phi 3 families, demonstrating gender and language biases, exposing harmful content, but also potential to remediate them, leading to safer and more transparent LLMs. Code is available at https://github.com/phvv-me/frame-representation-hypothesis.git", 'score': 11, 'issue_id': 1062, 'pub_date': '2024-12-10', 'pub_date_card': {'ru': '10 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 10', 'zh': '12æœˆ10æ—¥'}, 'hash': 'ed0e99c50709fbaf', 'authors': ['Pedro H. V. Valois', 'Lincon S. Souza', 'Erica K. Shimomoto', 'Kazuhiro Fukui'], 'affiliations': ['National Institute of Advanced Industrial Science and Technology (AIST)', 'University of Tsukuba'], 'pdf_title_img': 'assets/pdf/title_img/2412.07334.jpg', 'data': {'categories': ['#training', '#ethics', '#data', '#interpretability', '#multimodal', '#alignment', '#architecture'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ¾Ğ²Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ÑĞ»Ğ¾Ğ²: ĞºĞ»ÑÑ‡ Ğº Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ³Ğ¸Ğ¿Ğ¾Ñ‚ĞµĞ·Ñƒ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑÑÑ‚ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½ÑƒÑ Ğ³Ğ¸Ğ¿Ğ¾Ñ‚ĞµĞ·Ñƒ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ‚Ğ¾ĞºĞµĞ½Ğ½Ñ‹Ñ… ÑĞ»Ğ¾Ğ², Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒÑ Ğ¸Ñ… ĞºĞ°Ğº ÑƒĞ¿Ğ¾Ñ€ÑĞ´Ğ¾Ñ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ². ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ½Ñ‚ÑƒĞ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ñ‚ĞµĞºÑÑ‚Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½Ğ´ĞµÑ€Ğ½Ñ‹Ñ… Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… LLM.'}, 'en': {'title': 'Unlocking LLMs: Interpreting Words as Frames for Safer AI', 'desc': 'This paper addresses the challenge of interpreting Large Language Models (LLMs) by introducing the Frame Representation Hypothesis, which builds on the Linear Representation Hypothesis (LRH). It extends the analysis from single-token to multi-token words, allowing for a more comprehensive understanding of how words are represented in LLMs. By modeling words as framesâ€”ordered sequences of vectorsâ€”the authors provide a method to capture the relationships between tokens and concepts more effectively. The proposed Top-k Concept-Guided Decoding technique enables controlled text generation based on selected concepts, revealing biases and harmful content while also offering pathways for remediation.'}, 'zh': {'title': 'æå‡å¤§å‹è¯­è¨€æ¨¡å‹å¯è§£é‡Šæ€§çš„æ¡†æ¶è¡¨ç¤ºå‡è®¾', 'desc': 'æœ¬æ–‡æå‡ºäº†æ¡†æ¶è¡¨ç¤ºå‡è®¾ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¯è§£é‡Šæ€§å’Œå¯æ§æ€§ã€‚æˆ‘ä»¬æ‰©å±•äº†çº¿æ€§è¡¨ç¤ºå‡è®¾ï¼ˆLRHï¼‰ï¼Œå°†å…¶åº”ç”¨äºå¤šæ ‡è®°è¯çš„åˆ†æï¼Œä»¥æ›´å¥½åœ°ç†è§£å’Œæ§åˆ¶æ¨¡å‹çš„è¾“å‡ºã€‚é€šè¿‡å°†è¯è¯­è§†ä¸ºæ¡†æ¶ï¼Œåˆ©ç”¨å‘é‡åºåˆ—æ•æ‰æ ‡è®°ä¸è¯ä¹‹é—´çš„å…³ç³»ï¼Œæˆ‘ä»¬èƒ½å¤Ÿåœ¨æ–‡æœ¬ç”Ÿæˆä¸­å¼•å…¥æ¦‚å¿µæŒ‡å¯¼ã€‚æˆ‘ä»¬çš„ç ”ç©¶åœ¨å¤šä¸ªæ¨¡å‹ä¸ŠéªŒè¯äº†è¿™äº›æ–¹æ³•ï¼Œæ­ç¤ºäº†æ€§åˆ«å’Œè¯­è¨€åè§ï¼ŒåŒæ—¶å±•ç¤ºäº†æ”¹å–„è¿™äº›é—®é¢˜çš„æ½œåŠ›ï¼Œä»è€Œæ¨åŠ¨æ›´å®‰å…¨å’Œé€æ˜çš„LLMsã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.06674', 'title': 'EMOv2: Pushing 5M Vision Model Frontier', 'url': 'https://huggingface.co/papers/2412.06674', 'abstract': 'This work focuses on developing parameter-efficient and lightweight models for dense predictions while trading off parameters, FLOPs, and performance. Our goal is to set up the new frontier of the 5M magnitude lightweight model on various downstream tasks. Inverted Residual Block (IRB) serves as the infrastructure for lightweight CNNs, but no counterparts have been recognized by attention-based design. Our work rethinks the lightweight infrastructure of efficient IRB and practical components in Transformer from a unified perspective, extending CNN-based IRB to attention-based models and abstracting a one-residual Meta Mobile Block (MMBlock) for lightweight model design. Following neat but effective design criterion, we deduce a modern Improved Inverted Residual Mobile Block (i2RMB) and improve a hierarchical Efficient MOdel (EMOv2) with no elaborate complex structures. Considering the imperceptible latency for mobile users when downloading models under 4G/5G bandwidth and ensuring model performance, we investigate the performance upper limit of lightweight models with a magnitude of 5M. Extensive experiments on various vision recognition, dense prediction, and image generation tasks demonstrate the superiority of our EMOv2 over state-of-the-art methods, e.g., EMOv2-1M/2M/5M achieve 72.3, 75.8, and 79.4 Top-1 that surpass equal-order CNN-/Attention-based models significantly. At the same time, EMOv2-5M equipped RetinaNet achieves 41.5 mAP for object detection tasks that surpasses the previous EMO-5M by +2.6. When employing the more robust training recipe, our EMOv2-5M eventually achieves 82.9 Top-1 accuracy, which elevates the performance of 5M magnitude models to a new level. Code is available at https://github.com/zhangzjn/EMOv2.', 'score': 10, 'issue_id': 1061, 'pub_date': '2024-12-09', 'pub_date_card': {'ru': '9 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 9', 'zh': '12æœˆ9æ—¥'}, 'hash': 'a890d1e09015a7a6', 'authors': ['Jiangning Zhang', 'Teng Hu', 'Haoyang He', 'Zhucun Xue', 'Yabiao Wang', 'Chengjie Wang', 'Yong Liu', 'Xiangtai Li', 'Dacheng Tao'], 'affiliations': ['Institute of Cyber-Systems and Control, Zhejiang University, Hangzhou, China', 'Nanyang Technological University, Singapore', 'Shanghai Jiao Tong University, Shanghai, China', 'Youtu Lab, Tencent, China'], 'pdf_title_img': 'assets/pdf/title_img/2412.06674.jpg', 'data': {'categories': ['#small_models', '#architecture', '#optimization', '#training', '#cv'], 'emoji': 'ğŸš€', 'ru': {'title': 'EMOv2: ĞĞ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ´Ğ»Ñ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ­Ñ‚Ğ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ğ¾ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼ Ğ¸ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¹, Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€ÑƒÑ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², FLOP Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±Ğ»Ğ¾Ğº i2RMB, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¸Ğ½Ğ²ĞµÑ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¼ Ğ±Ğ»Ğ¾ĞºĞµ (IRB) Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°Ñ… Transformer. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ±Ğ»Ğ¾ĞºĞ° ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ° Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ EMOv2, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ EMOv2 Ñ 5 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ 82.9% Top-1 Ğ½Ğ° ImageNet, ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°Ñ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ€ĞµĞºĞ¾Ñ€Ğ´ Ğ´Ğ»Ñ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Lightweight Models, Heavyweight Performance!', 'desc': 'This paper presents a new approach to creating lightweight models for dense prediction tasks in machine learning, focusing on efficiency in terms of parameters and computational cost (FLOPs). The authors introduce a novel architecture called the Improved Inverted Residual Mobile Block (i2RMB) that extends the Inverted Residual Block (IRB) concept to attention-based models. They demonstrate that their Efficient Model version 2 (EMOv2) outperforms existing models in various tasks, achieving impressive accuracy with only 5 million parameters. The results indicate that lightweight models can achieve high performance without complex structures, making them suitable for mobile applications.'}, 'zh': {'title': 'è½»é‡çº§æ¨¡å‹çš„æ–°çªç ´ï¼š5Mé‡çº§çš„é«˜æ•ˆè®¾è®¡', 'desc': 'æœ¬ç ”ç©¶è‡´åŠ›äºå¼€å‘å‚æ•°é«˜æ•ˆä¸”è½»é‡çº§çš„æ¨¡å‹ï¼Œä»¥å®ç°å¯†é›†é¢„æµ‹ï¼ŒåŒæ—¶åœ¨å‚æ•°ã€FLOPså’Œæ€§èƒ½ä¹‹é—´è¿›è¡Œæƒè¡¡ã€‚æˆ‘ä»¬æ—¨åœ¨ä¸ºå„ç§ä¸‹æ¸¸ä»»åŠ¡å»ºç«‹5Mé‡çº§è½»é‡çº§æ¨¡å‹çš„æ–°å‰æ²¿ã€‚æˆ‘ä»¬é‡æ–°æ€è€ƒäº†é«˜æ•ˆçš„åå‘æ®‹å·®å—ï¼ˆIRBï¼‰å’ŒTransformerä¸­çš„å®ç”¨ç»„ä»¶ï¼Œä»ç»Ÿä¸€çš„è§’åº¦æ‰©å±•äº†åŸºäºCNNçš„IRBåˆ°åŸºäºæ³¨æ„åŠ›çš„æ¨¡å‹ï¼Œå¹¶æŠ½è±¡å‡ºä¸€ç§è½»é‡çº§æ¨¡å‹è®¾è®¡çš„å…ƒæ®‹å·®ç§»åŠ¨å—ï¼ˆMMBlockï¼‰ã€‚é€šè¿‡ç®€æ´è€Œæœ‰æ•ˆçš„è®¾è®¡æ ‡å‡†ï¼Œæˆ‘ä»¬æ¨å¯¼å‡ºä¸€ç§ç°ä»£åŒ–çš„æ”¹è¿›åå‘æ®‹å·®ç§»åŠ¨å—ï¼ˆi2RMBï¼‰ï¼Œå¹¶åœ¨æ²¡æœ‰å¤æ‚ç»“æ„çš„æƒ…å†µä¸‹æ”¹è¿›äº†åˆ†å±‚é«˜æ•ˆæ¨¡å‹ï¼ˆEMOv2ï¼‰ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.03548', 'title': 'Perception Tokens Enhance Visual Reasoning in Multimodal Language Models', 'url': 'https://huggingface.co/papers/2412.03548', 'abstract': "Multimodal language models (MLMs) still face challenges in fundamental visual perception tasks where specialized models excel. Tasks requiring reasoning about 3D structures benefit from depth estimation, and reasoning about 2D object instances benefits from object detection. Yet, MLMs can not produce intermediate depth or boxes to reason over. Finetuning MLMs on relevant data doesn't generalize well and outsourcing computation to specialized vision tools is too compute-intensive and memory-inefficient. To address this, we introduce Perception Tokens, intrinsic image representations designed to assist reasoning tasks where language is insufficient. Perception tokens act as auxiliary reasoning tokens, akin to chain-of-thought prompts in language models. For example, in a depth-related task, an MLM augmented with perception tokens can reason by generating a depth map as tokens, enabling it to solve the problem effectively. We propose AURORA, a training method that augments MLMs with perception tokens for improved reasoning over visual inputs. AURORA leverages a VQVAE to transform intermediate image representations, such as depth maps into a tokenized format and bounding box tokens, which is then used in a multi-task training framework. AURORA achieves notable improvements across counting benchmarks: +10.8% on BLINK, +11.3% on CVBench, and +8.3% on SEED-Bench, outperforming finetuning approaches in generalization across datasets. It also improves on relative depth: over +6% on BLINK. With perception tokens, AURORA expands the scope of MLMs beyond language-based reasoning, paving the way for more effective visual reasoning capabilities.", 'score': 10, 'issue_id': 1060, 'pub_date': '2024-12-04', 'pub_date_card': {'ru': '4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 4', 'zh': '12æœˆ4æ—¥'}, 'hash': 'b1047666846bb684', 'authors': ['Mahtab Bigverdi', 'Zelun Luo', 'Cheng-Yu Hsieh', 'Ethan Shen', 'Dongping Chen', 'Linda G. Shapiro', 'Ranjay Krishna'], 'affiliations': ['Google Research', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2412.03548.jpg', 'data': {'categories': ['#multimodal', '#cv', '#reasoning', '#training', '#optimization'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ¢Ğ¾ĞºĞµĞ½Ñ‹ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLM) Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ 'Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ' - Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ°Ğ¼ Ğ² Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ AURORA Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ VQVAE Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¿Ğ¾Ğ´ÑÑ‡ĞµÑ‚Ğ° Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹."}, 'en': {'title': 'Enhancing Visual Reasoning in MLMs with Perception Tokens', 'desc': 'This paper introduces Perception Tokens to enhance multimodal language models (MLMs) in visual reasoning tasks. Traditional MLMs struggle with tasks like depth estimation and object detection because they cannot generate necessary intermediate representations. The proposed AURORA method integrates these perception tokens into MLMs, allowing them to produce depth maps and bounding boxes as tokens for better reasoning. AURORA shows significant performance improvements on various benchmarks, demonstrating its effectiveness in expanding the reasoning capabilities of MLMs beyond just language.'}, 'zh': {'title': 'æ„ŸçŸ¥æ ‡è®°ï¼šæå‡å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹çš„è§†è§‰æ¨ç†èƒ½åŠ›', 'desc': 'å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼ˆMLMsï¼‰åœ¨åŸºæœ¬è§†è§‰æ„ŸçŸ¥ä»»åŠ¡ä¸­ä»é¢ä¸´æŒ‘æˆ˜ï¼Œä¸“é—¨æ¨¡å‹åœ¨è¿™äº›ä»»åŠ¡ä¸­è¡¨ç°æ›´å¥½ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†æ„ŸçŸ¥æ ‡è®°ï¼ˆPerception Tokensï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å†…åœ¨çš„å›¾åƒè¡¨ç¤ºï¼Œæ—¨åœ¨è¾…åŠ©è¯­è¨€ä¸è¶³çš„æ¨ç†ä»»åŠ¡ã€‚é€šè¿‡å¼•å…¥æ„ŸçŸ¥æ ‡è®°ï¼ŒMLMsèƒ½å¤Ÿç”Ÿæˆæ·±åº¦å›¾ç­‰ä¸­é—´è¡¨ç¤ºï¼Œä»è€Œæœ‰æ•ˆè§£å†³ä¸æ·±åº¦ç›¸å…³çš„é—®é¢˜ã€‚æˆ‘ä»¬æå‡ºçš„AURORAè®­ç»ƒæ–¹æ³•é€šè¿‡å°†æ„ŸçŸ¥æ ‡è®°ä¸MLMsç»“åˆï¼Œæ˜¾è‘—æé«˜äº†è§†è§‰è¾“å…¥çš„æ¨ç†èƒ½åŠ›ï¼Œå°¤å…¶åœ¨è®¡æ•°åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.05148', 'title': 'LoRA.rar: Learning to Merge LoRAs via Hypernetworks for Subject-Style Conditioned Image Generation', 'url': 'https://huggingface.co/papers/2412.05148', 'abstract': 'Recent advancements in image generation models have enabled personalized image creation with both user-defined subjects (content) and styles. Prior works achieved personalization by merging corresponding low-rank adaptation parameters (LoRAs) through optimization-based methods, which are computationally demanding and unsuitable for real-time use on resource-constrained devices like smartphones. To address this, we introduce LoRA.rar, a method that not only improves image quality but also achieves a remarkable speedup of over 4000times in the merging process. LoRA.rar pre-trains a hypernetwork on a diverse set of content-style LoRA pairs, learning an efficient merging strategy that generalizes to new, unseen content-style pairs, enabling fast, high-quality personalization. Moreover, we identify limitations in existing evaluation metrics for content-style quality and propose a new protocol using multimodal large language models (MLLM) for more accurate assessment. Our method significantly outperforms the current state of the art in both content and style fidelity, as validated by MLLM assessments and human evaluations.', 'score': 8, 'issue_id': 1066, 'pub_date': '2024-12-06', 'pub_date_card': {'ru': '6 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 6', 'zh': '12æœˆ6æ—¥'}, 'hash': 'ef1bd7ea8522423b', 'authors': ['Donald Shenaj', 'Ondrej Bohdal', 'Mete Ozay', 'Pietro Zanuttigh', 'Umberto Michieli'], 'affiliations': ['Samsung R&D Institute (SRUK)', 'University of Padova'], 'pdf_title_img': 'assets/pdf/title_img/2412.05148.jpg', 'data': {'categories': ['#multimodal', '#optimization', '#synthetic', '#benchmark', '#dataset', '#cv'], 'emoji': 'ğŸ¨', 'ru': {'title': 'LoRA.rar: Ğ¡Ğ²ĞµÑ€Ñ…Ğ±Ñ‹ÑÑ‚Ñ€Ğ°Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ LoRA.rar Ğ´Ğ»Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ³Ğ¸Ğ¿ĞµÑ€ÑĞµÑ‚ÑŒ Ğ´Ğ»Ñ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² LoRA Ğ´Ğ»Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ¸ ÑÑ‚Ğ¸Ğ»Ñ, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ» Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ¸ ÑÑ‚Ğ¸Ğ»Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM). Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ LoRA.rar Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ¸ ÑÑ‚Ğ¸Ğ»Ñ.'}, 'en': {'title': 'Revolutionizing Image Personalization with Lightning Speed', 'desc': 'This paper presents LoRA.rar, a novel approach for personalized image generation that significantly enhances both speed and quality. Unlike previous methods that relied on computationally intensive optimization to merge low-rank adaptation parameters (LoRAs), LoRA.rar achieves over 4000 times faster merging by utilizing a pre-trained hypernetwork. This hypernetwork learns to efficiently combine content and style from diverse LoRA pairs, allowing for quick adaptation to new combinations. Additionally, the authors propose a new evaluation protocol using multimodal large language models to better assess the quality of generated images, demonstrating that their method surpasses existing techniques in both content and style fidelity.'}, 'zh': {'title': 'ä¸ªæ€§åŒ–å›¾åƒç”Ÿæˆçš„å¿«é€Ÿè§£å†³æ–¹æ¡ˆ', 'desc': 'æœ€è¿‘ï¼Œå›¾åƒç”Ÿæˆæ¨¡å‹çš„è¿›æ­¥ä½¿å¾—ä¸ªæ€§åŒ–å›¾åƒåˆ›å»ºæˆä¸ºå¯èƒ½ï¼Œç”¨æˆ·å¯ä»¥å®šä¹‰å›¾åƒçš„å†…å®¹å’Œé£æ ¼ã€‚ä»¥å¾€çš„ä¸ªæ€§åŒ–æ–¹æ³•é€šè¿‡ä¼˜åŒ–åˆå¹¶ä½ç§©é€‚åº”å‚æ•°ï¼ˆLoRAsï¼‰ï¼Œä½†è¿™ç§æ–¹æ³•è®¡ç®—é‡å¤§ï¼Œä¸é€‚åˆåœ¨èµ„æºæœ‰é™çš„è®¾å¤‡ä¸Šå®æ—¶ä½¿ç”¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†LoRA.rarï¼Œè¿™ç§æ–¹æ³•ä¸ä»…æé«˜äº†å›¾åƒè´¨é‡ï¼Œè¿˜åœ¨åˆå¹¶è¿‡ç¨‹ä¸­å®ç°äº†è¶…è¿‡4000å€çš„é€Ÿåº¦æå‡ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§æ–°çš„è¯„ä¼°åè®®ï¼Œåˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰æ¥æ›´å‡†ç¡®åœ°è¯„ä¼°å†…å®¹å’Œé£æ ¼çš„è´¨é‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.06673', 'title': 'ILLUME: Illuminating Your LLMs to See, Draw, and Self-Enhance', 'url': 'https://huggingface.co/papers/2412.06673', 'abstract': 'In this paper, we introduce ILLUME, a unified multimodal large language model (MLLM) that seamlessly integrates multimodal understanding and generation capabilities within a single large language model through a unified next-token prediction formulation. To address the large dataset size typically required for image-text alignment, we propose to enhance data efficiency through the design of a vision tokenizer that incorporates semantic information and a progressive multi-stage training procedure. This approach reduces the dataset size to just 15M for pretraining -- over four times fewer than what is typically needed -- while achieving competitive or even superior performance with existing unified MLLMs, such as Janus. Additionally, to promote synergistic enhancement between understanding and generation capabilities, which is under-explored in previous works, we introduce a novel self-enhancing multimodal alignment scheme. This scheme supervises the MLLM to self-assess the consistency between text descriptions and self-generated images, facilitating the model to interpret images more accurately and avoid unrealistic and incorrect predictions caused by misalignment in image generation. Based on extensive experiments, our proposed ILLUME stands out and competes with state-of-the-art unified MLLMs and specialized models across various benchmarks for multimodal understanding, generation, and editing.', 'score': 8, 'issue_id': 1058, 'pub_date': '2024-12-09', 'pub_date_card': {'ru': '9 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 9', 'zh': '12æœˆ9æ—¥'}, 'hash': 'a8071141959ac48a', 'authors': ['Chunwei Wang', 'Guansong Lu', 'Junwei Yang', 'Runhui Huang', 'Jianhua Han', 'Lu Hou', 'Wei Zhang', 'Hang Xu'], 'affiliations': ['Huawei Noahs Ark Lab'], 'pdf_title_img': 'assets/pdf/title_img/2412.06673.jpg', 'data': {'categories': ['#agi', '#dataset', '#benchmark', '#interpretability', '#training', '#optimization', '#multimodal'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ILLUME: Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'ILLUME - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ°Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ñ€Ğ°Ğ¼ĞºĞ°Ñ… ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¼ĞµĞ½ÑŒÑˆĞµĞ¼ Ğ¾Ğ±ÑŠĞµĞ¼Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ’Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ° ÑÑ…ĞµĞ¼Ğ° ÑĞ°Ğ¼Ğ¾ÑƒÑĞ¸Ğ»Ğ¸Ğ²Ğ°ÑÑ‰ĞµĞ³Ğ¾ÑÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸. ILLUME Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'ILLUME: Efficient Multimodal Mastery in One Model', 'desc': 'The paper presents ILLUME, a unified multimodal large language model (MLLM) that combines understanding and generation of both text and images. It introduces a vision tokenizer that uses semantic information to improve data efficiency, allowing for effective training with a smaller dataset of only 15 million samples. The model employs a self-enhancing multimodal alignment scheme to ensure that the generated images accurately reflect the text descriptions, reducing errors in image generation. Through extensive testing, ILLUME demonstrates competitive performance against existing models in multimodal tasks.'}, 'zh': {'title': 'ILLUMEï¼šå¤šæ¨¡æ€ç†è§£ä¸ç”Ÿæˆçš„ç»Ÿä¸€æ¨¡å‹', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ILLUMEï¼Œè¿™æ˜¯ä¸€ç§ç»Ÿä¸€çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œå®ƒé€šè¿‡ç»Ÿä¸€çš„ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹å…¬å¼ï¼Œå°†å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆèƒ½åŠ›æ— ç¼é›†æˆåœ¨ä¸€ä¸ªæ¨¡å‹ä¸­ã€‚ä¸ºäº†åº”å¯¹é€šå¸¸éœ€è¦çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§è§†è§‰æ ‡è®°å™¨ï¼Œç»“åˆäº†è¯­ä¹‰ä¿¡æ¯ï¼Œå¹¶é‡‡ç”¨æ¸è¿›å¼å¤šé˜¶æ®µè®­ç»ƒç¨‹åºï¼Œä»è€Œå°†é¢„è®­ç»ƒæ‰€éœ€çš„æ•°æ®é›†å¤§å°å‡å°‘åˆ°ä»…1500ä¸‡ï¼Œè¿œä½äºé€šå¸¸æ‰€éœ€çš„æ•°é‡ï¼ŒåŒæ—¶åœ¨æ€§èƒ½ä¸Šä¸ç°æœ‰çš„ç»Ÿä¸€MLLMï¼ˆå¦‚Janusï¼‰ç«äº‰æˆ–æ›´ä¼˜ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„è‡ªæˆ‘å¢å¼ºå¤šæ¨¡æ€å¯¹é½æ–¹æ¡ˆï¼Œç›‘ç£æ¨¡å‹è‡ªæˆ‘è¯„ä¼°æ–‡æœ¬æè¿°ä¸è‡ªç”Ÿæˆå›¾åƒä¹‹é—´çš„ä¸€è‡´æ€§ï¼Œä»è€Œæé«˜å›¾åƒç†è§£çš„å‡†ç¡®æ€§ï¼Œé¿å…å› ç”Ÿæˆå›¾åƒä¸ä¸€è‡´è€Œå¯¼è‡´çš„ä¸ç°å®å’Œé”™è¯¯é¢„æµ‹ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒï¼ŒILLUMEåœ¨å¤šæ¨¡æ€ç†è§£ã€ç”Ÿæˆå’Œç¼–è¾‘çš„å„ç±»åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°çªå‡ºï¼Œèƒ½å¤Ÿä¸æœ€å…ˆè¿›çš„ç»Ÿä¸€MLLMå’Œä¸“ä¸šæ¨¡å‹ç«äº‰ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.07721', 'title': 'ObjCtrl-2.5D: Training-free Object Control with Camera Poses', 'url': 'https://huggingface.co/papers/2412.07721', 'abstract': "This study aims to achieve more precise and versatile object control in image-to-video (I2V) generation. Current methods typically represent the spatial movement of target objects with 2D trajectories, which often fail to capture user intention and frequently produce unnatural results. To enhance control, we present ObjCtrl-2.5D, a training-free object control approach that uses a 3D trajectory, extended from a 2D trajectory with depth information, as a control signal. By modeling object movement as camera movement, ObjCtrl-2.5D represents the 3D trajectory as a sequence of camera poses, enabling object motion control using an existing camera motion control I2V generation model (CMC-I2V) without training. To adapt the CMC-I2V model originally designed for global motion control to handle local object motion, we introduce a module to isolate the target object from the background, enabling independent local control. In addition, we devise an effective way to achieve more accurate object control by sharing low-frequency warped latent within the object's region across frames. Extensive experiments demonstrate that ObjCtrl-2.5D significantly improves object control accuracy compared to training-free methods and offers more diverse control capabilities than training-based approaches using 2D trajectories, enabling complex effects like object rotation. Code and results are available at https://wzhouxiff.github.io/projects/ObjCtrl-2.5D/.", 'score': 7, 'issue_id': 1061, 'pub_date': '2024-12-10', 'pub_date_card': {'ru': '10 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 10', 'zh': '12æœˆ10æ—¥'}, 'hash': 'de32794ee8780d29', 'authors': ['Zhouxia Wang', 'Yushi Lan', 'Shangchen Zhou', 'Chen Change Loy'], 'affiliations': ['S-Lab, Nanyang Technological University'], 'pdf_title_img': 'assets/pdf/title_img/2412.07721.jpg', 'data': {'categories': ['#3d', '#video'], 'emoji': 'ğŸ¥', 'ru': {'title': '3D Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ ObjCtrl-2.5D. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ 3D Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ñ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ 2D Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². ObjCtrl-2.5D Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ° ĞºĞ°Ğº Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ ĞºĞ°Ğ¼ĞµÑ€Ñ‹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ° Ğ¾Ñ‚ Ñ„Ğ¾Ğ½Ğ° Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ° Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ.'}, 'en': {'title': 'Revolutionizing Object Control in I2V with 3D Trajectories', 'desc': 'This paper introduces ObjCtrl-2.5D, a novel approach for enhancing object control in image-to-video (I2V) generation. Unlike traditional methods that rely on 2D trajectories, ObjCtrl-2.5D utilizes 3D trajectories, incorporating depth information to better reflect user intentions. By treating object movement as camera movement, it allows for precise control without the need for additional training. The method also includes a module for isolating the target object from the background, enabling more accurate and diverse control over object motion, including complex effects like rotation.'}, 'zh': {'title': 'æå‡å›¾åƒåˆ°è§†é¢‘ç”Ÿæˆä¸­çš„ç‰©ä½“æ§åˆ¶ç²¾åº¦ä¸å¤šæ ·æ€§', 'desc': 'æœ¬ç ”ç©¶æ—¨åœ¨æé«˜å›¾åƒåˆ°è§†é¢‘ç”Ÿæˆä¸­çš„ç‰©ä½“æ§åˆ¶ç²¾åº¦å’Œå¤šæ ·æ€§ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä½¿ç”¨äºŒç»´è½¨è¿¹è¡¨ç¤ºç›®æ ‡ç‰©ä½“çš„ç©ºé—´è¿åŠ¨ï¼Œéš¾ä»¥æ•æ‰ç”¨æˆ·æ„å›¾ï¼Œä¸”å¸¸å¸¸äº§ç”Ÿä¸è‡ªç„¶çš„ç»“æœã€‚æˆ‘ä»¬æå‡ºäº†ObjCtrl-2.5Dï¼Œè¿™æ˜¯ä¸€ç§æ— è®­ç»ƒçš„ç‰©ä½“æ§åˆ¶æ–¹æ³•ï¼Œåˆ©ç”¨å¸¦æ·±åº¦ä¿¡æ¯çš„ä¸‰ç»´è½¨è¿¹ä½œä¸ºæ§åˆ¶ä¿¡å·ã€‚é€šè¿‡å°†ç‰©ä½“è¿åŠ¨å»ºæ¨¡ä¸ºç›¸æœºè¿åŠ¨ï¼ŒObjCtrl-2.5Dèƒ½å¤Ÿåœ¨ä¸è¿›è¡Œè®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œä½¿ç”¨ç°æœ‰çš„ç›¸æœºè¿åŠ¨æ§åˆ¶æ¨¡å‹å®ç°ç‰©ä½“è¿åŠ¨æ§åˆ¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.06845', 'title': 'Fully Open Source Moxin-7B Technical Report', 'url': 'https://huggingface.co/papers/2412.06845', 'abstract': 'Recently, Large Language Models (LLMs) have undergone a significant transformation, marked by a rapid rise in both their popularity and capabilities. Leading this evolution are proprietary LLMs like GPT-4 and GPT-o1, which have captured widespread attention in the AI community due to their remarkable performance and versatility. Simultaneously, open-source LLMs, such as LLaMA and Mistral, have made great contributions to the ever-increasing popularity of LLMs due to the ease to customize and deploy the models across diverse applications. Although open-source LLMs present unprecedented opportunities for innovation and research, the commercialization of LLMs has raised concerns about transparency, reproducibility, and safety. Many open-source LLMs fail to meet fundamental transparency requirements by withholding essential components like training code and data, and some use restrictive licenses whilst claiming to be "open-source," which may hinder further innovations on LLMs. To mitigate this issue, we introduce Moxin 7B, a fully open-source LLM developed in accordance with the Model Openness Framework (MOF), a ranked classification system that evaluates AI models based on model completeness and openness, adhering to principles of open science, open source, open data, and open access. Our model achieves the highest MOF classification level of "open science" through the comprehensive release of pre-training code and configurations, training and fine-tuning datasets, and intermediate and final checkpoints. Experiments show that our model achieves superior performance in zero-shot evaluation compared with popular 7B models and performs competitively in few-shot evaluation.', 'score': 7, 'issue_id': 1058, 'pub_date': '2024-12-08', 'pub_date_card': {'ru': '8 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 8', 'zh': '12æœˆ8æ—¥'}, 'hash': '410471f06d9e6883', 'authors': ['Pu Zhao', 'Xuan Shen', 'Zhenglun Kong', 'Yixin Shen', 'Sung-En Chang', 'Timothy Rupprecht', 'Lei Lu', 'Enfu Nan', 'Changdi Yang', 'Yumei He', 'Xingchen Xu', 'Yu Huang', 'Wei Wang', 'Yue Chen', 'Yong He', 'Yanzhi Wang'], 'affiliations': ['AIBAO LLC', 'Cornell University', 'Futurewei Technologies', 'Harvard University', 'Northeastern University', 'Roboraction.ai', 'Tulane University', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2412.06845.jpg', 'data': {'categories': ['#data', '#dataset', '#open_source', '#ethics', '#training', '#multimodal'], 'emoji': 'ğŸ”“', 'ru': {'title': 'Moxin 7B: ĞÑ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ LLM Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ñ‹Ñ… Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ğ˜Ğ˜', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Moxin 7B - Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ (LLM), Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ Ğ² ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¸ Ñ ĞœĞ¾Ğ´ĞµĞ»ÑŒÑ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾ÑÑ‚Ğ¸ (MOF). Moxin 7B Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑÑˆĞµĞ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ MOF Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑĞºÑ€Ñ‹Ñ‚Ğ¸Ñ ĞºĞ¾Ğ´Ğ°, ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¹, Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ğµ 7B-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² zero-shot Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ¸ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ² few-shot Ğ¾Ñ†ĞµĞ½ĞºĞµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ğ¸ LLM Ğ´Ğ»Ñ ÑÑ‚Ğ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¹ Ğ¸ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹.'}, 'en': {'title': 'Moxin 7B: Leading the Way in Open-Source Language Models', 'desc': 'This paper discusses the evolution of Large Language Models (LLMs), highlighting the contrast between proprietary models like GPT-4 and open-source alternatives such as LLaMA. It emphasizes the importance of transparency and reproducibility in AI, noting that many open-source models do not fully disclose their training processes or data. To address these issues, the authors introduce Moxin 7B, an open-source LLM that adheres to the Model Openness Framework (MOF), ensuring comprehensive access to its training code and datasets. The results demonstrate that Moxin 7B outperforms other 7B models in zero-shot tasks and remains competitive in few-shot scenarios, showcasing the potential of fully open-source LLMs.'}, 'zh': {'title': 'Moxin 7Bï¼šå¼€æºè¯­è¨€æ¨¡å‹çš„æ–°æ ‡æ†', 'desc': 'æœ€è¿‘ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç»å†äº†æ˜¾è‘—çš„å˜é©ï¼Œå—åˆ°äº†å¹¿æ³›å…³æ³¨ã€‚ä»¥GPT-4å’ŒGPT-o1ä¸ºä»£è¡¨çš„ä¸“æœ‰LLMså±•ç°äº†å“è¶Šçš„æ€§èƒ½å’Œå¤šæ ·æ€§ï¼ŒåŒæ—¶å¼€æºLLMså¦‚LLaMAå’ŒMistralä¹Ÿå› å…¶æ˜“äºå®šåˆ¶å’Œéƒ¨ç½²è€Œå—åˆ°é’çã€‚å°½ç®¡å¼€æºLLMsä¸ºåˆ›æ–°å’Œç ”ç©¶æä¾›äº†å‰æ‰€æœªæœ‰çš„æœºä¼šï¼Œä½†å…¶å•†ä¸šåŒ–å¸¦æ¥äº†é€æ˜æ€§ã€å¯é‡å¤æ€§å’Œå®‰å…¨æ€§æ–¹é¢çš„æ‹…å¿§ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†Moxin 7Bï¼Œè¿™æ˜¯ä¸€ä¸ªå®Œå…¨å¼€æºçš„LLMï¼Œéµå¾ªæ¨¡å‹å¼€æ”¾æ¡†æ¶ï¼ˆMOFï¼‰ï¼Œå¹¶åœ¨é€æ˜æ€§å’Œå¼€æ”¾æ€§æ–¹é¢è¾¾åˆ°äº†æœ€é«˜æ ‡å‡†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.05983', 'title': 'Chimera: Improving Generalist Model with Domain-Specific Experts', 'url': 'https://huggingface.co/papers/2412.05983', 'abstract': 'Recent advancements in Large Multi-modal Models (LMMs) underscore the importance of scaling by increasing image-text paired data, achieving impressive performance on general tasks. Despite their effectiveness in broad applications, generalist models are primarily trained on web-scale datasets dominated by natural images, resulting in the sacrifice of specialized capabilities for domain-specific tasks that require extensive domain prior knowledge. Moreover, directly integrating expert models tailored for specific domains is challenging due to the representational gap and imbalanced optimization between the generalist model and experts. To address these challenges, we introduce Chimera, a scalable and low-cost multi-modal pipeline designed to boost the ability of existing LMMs with domain-specific experts. Specifically, we design a progressive training strategy to integrate features from expert models into the input of a generalist LMM. To address the imbalanced optimization caused by the well-aligned general visual encoder, we introduce a novel Generalist-Specialist Collaboration Masking (GSCM) mechanism. This results in a versatile model that excels across the chart, table, math, and document domains, achieving state-of-the-art performance on multi-modal reasoning and visual content extraction tasks, both of which are challenging tasks for assessing existing LMMs.', 'score': 5, 'issue_id': 1064, 'pub_date': '2024-12-08', 'pub_date_card': {'ru': '8 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 8', 'zh': '12æœˆ8æ—¥'}, 'hash': '991103a0d9d85ad4', 'authors': ['Tianshuo Peng', 'Mingsheng Li', 'Hongbin Zhou', 'Renqiu Xia', 'Renrui Zhang', 'Lei Bai', 'Song Mao', 'Bin Wang', 'Conghui He', 'Aojun Zhou', 'Botian Shi', 'Tao Chen', 'Bo Zhang', 'Xiangyu Yue'], 'affiliations': ['Fudan University', 'MMLab, The Chinese University of Hong Kong', 'Shanghai Artificial Intelligence Laboratory', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2412.05983.jpg', 'data': {'categories': ['#transfer_learning', '#multimodal', '#cv', '#reasoning', '#training', '#optimization'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Chimera: Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ LMM Ñ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Chimera - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LMM) Ğ´Ğ»Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ½ÑƒÑ LMM. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ½ĞµÑĞ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (GSCM). Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ¼ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‰Ğ°Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ² ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ….'}, 'en': {'title': 'Chimera: Bridging Generalist and Specialist Models for Enhanced Multi-modal Learning', 'desc': "This paper presents Chimera, a new approach to enhance Large Multi-modal Models (LMMs) by integrating domain-specific expert models. The authors highlight that while LMMs perform well on general tasks, they struggle with specialized tasks due to their training on broad datasets. To improve this, Chimera employs a progressive training strategy that incorporates expert features into the generalist model's input. Additionally, the Generalist-Specialist Collaboration Masking (GSCM) mechanism is introduced to balance optimization between the generalist and specialist models, leading to superior performance in multi-modal reasoning and visual content extraction tasks."}, 'zh': {'title': 'Chimeraï¼šæå‡å¤šæ¨¡æ€æ¨¡å‹çš„é¢†åŸŸä¸“é•¿', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºChimeraçš„å¤šæ¨¡æ€æ¨¡å‹ï¼Œæ—¨åœ¨æå‡ç°æœ‰å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨ç‰¹å®šé¢†åŸŸçš„èƒ½åŠ›ã€‚é€šè¿‡å¼•å…¥é¢†åŸŸä¸“å®¶æ¨¡å‹çš„ç‰¹å¾ï¼ŒChimeraé‡‡ç”¨æ¸è¿›å¼è®­ç»ƒç­–ç•¥ï¼Œè§£å†³äº†é€šç”¨æ¨¡å‹ä¸ä¸“å®¶æ¨¡å‹ä¹‹é—´çš„è¡¨ç¤ºå·®è·å’Œä¼˜åŒ–ä¸å¹³è¡¡é—®é¢˜ã€‚ç‰¹åˆ«åœ°ï¼Œæ–‡ç« æå‡ºäº†ä¸€ç§æ–°çš„é€šç”¨-ä¸“å®¶åä½œæ©ç æœºåˆ¶ï¼ˆGSCMï¼‰ï¼Œä»¥ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ã€‚æœ€ç»ˆï¼ŒChimeraåœ¨å›¾è¡¨ã€è¡¨æ ¼ã€æ•°å­¦å’Œæ–‡æ¡£ç­‰é¢†åŸŸçš„å¤šæ¨¡æ€æ¨ç†å’Œè§†è§‰å†…å®¹æå–ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.07282', 'title': 'HARP: Hesitation-Aware Reframing in Transformer Inference Pass', 'url': 'https://huggingface.co/papers/2412.07282', 'abstract': 'This paper aims to improve the performance of large language models by addressing the variable computational demands in inference steps, where some tokens require more computational resources than others. We present HARP, a simple modification to "off-the-shelf" Transformer forward pass. Drawing from hesitation and the framing effect in decision-making, HARP selectively applies additional computation when the model encounters uncertainty during token generation. Our method mimics human cognitive processes by pausing at difficult decision points and reframing inputs for a different perspective. Unlike other approaches, HARP is model-agnostic, training-free, and easy to implement. We thoroughly evaluate our method across various downstream tasks and model sizes, demonstrating performance improvements up to +5.16%. Notably, HARP achieves these gains while maintaining inference times twice faster than beam search. Simple and yet with significant gains, HARP offers a practical solution for enhancing the performance of Transformer-based language models with minimal computational impact.', 'score': 4, 'issue_id': 1067, 'pub_date': '2024-12-10', 'pub_date_card': {'ru': '10 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 10', 'zh': '12æœˆ10æ—¥'}, 'hash': 'e2c6a5ca5a1b3139', 'authors': ['Romain StoraÃ¯', 'Seung-won Hwang'], 'affiliations': ['Computer Science and Engineering, Seoul National University'], 'pdf_title_img': 'assets/pdf/title_img/2412.07282.jpg', 'data': {'categories': ['#architecture', '#training', '#inference', '#optimization'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'HARP: ĞŸĞ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ LLM Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ¼Ğ¸Ñ‚Ğ°Ñ†Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ HARP Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). HARP ÑĞµĞ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ğ³Ğ´Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑÑ‚Ğ°Ğ»ĞºĞ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ñ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ñ€ÑƒÑ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğµ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ½Ğµ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ñ‚ Ğ¾Ñ‚ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ»ĞµĞ³ĞºĞ¾ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·ÑƒĞµÑ‚ÑÑ. HARP Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¾ 5.16% Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ¾ÑÑ‚Ğ°Ğ²Ğ°ÑÑÑŒ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ² Ğ´Ğ²Ğ° Ñ€Ğ°Ğ·Ğ° Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¿Ğ¾ Ğ»ÑƒÑ‡Ñƒ.'}, 'en': {'title': 'HARP: Enhancing Language Models with Smart Computation', 'desc': 'This paper introduces HARP, a method designed to enhance the efficiency of large language models during inference by addressing the varying computational needs of different tokens. HARP selectively allocates additional computational resources when the model faces uncertainty, mimicking human decision-making processes. The approach is model-agnostic, meaning it can be applied to any Transformer model without requiring retraining. Evaluation shows that HARP can improve performance by up to 5.16% while also speeding up inference times compared to traditional methods like beam search.'}, 'zh': {'title': 'HARPï¼šæå‡è¯­è¨€æ¨¡å‹æ€§èƒ½çš„ç®€å•è§£å†³æ–¹æ¡ˆ', 'desc': 'æœ¬æ–‡æ—¨åœ¨é€šè¿‡è§£å†³æ¨ç†æ­¥éª¤ä¸­ä¸åŒä»¤ç‰Œçš„è®¡ç®—éœ€æ±‚ï¼Œæ¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚æˆ‘ä»¬æå‡ºäº†HARPï¼Œè¿™æ˜¯ä¸€ç§å¯¹ç°æœ‰Transformerå‰å‘ä¼ æ’­çš„ç®€å•ä¿®æ”¹ã€‚HARPåœ¨æ¨¡å‹ç”Ÿæˆä»¤ç‰Œæ—¶é‡åˆ°ä¸ç¡®å®šæ€§æ—¶ï¼Œé€‰æ‹©æ€§åœ°åº”ç”¨é¢å¤–çš„è®¡ç®—ï¼Œæ¨¡ä»¿äººç±»åœ¨å›°éš¾å†³ç­–ç‚¹çš„è®¤çŸ¥è¿‡ç¨‹ã€‚ä¸å…¶ä»–æ–¹æ³•ä¸åŒï¼ŒHARPä¸ä¾èµ–äºç‰¹å®šæ¨¡å‹ï¼Œä¸”æ— éœ€è®­ç»ƒï¼Œæ˜“äºå®ç°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.06089', 'title': 'GraPE: A Generate-Plan-Edit Framework for Compositional T2I Synthesis', 'url': 'https://huggingface.co/papers/2412.06089', 'abstract': 'Text-to-image (T2I) generation has seen significant progress with diffusion models, enabling generation of photo-realistic images from text prompts. Despite this progress, existing methods still face challenges in following complex text prompts, especially those requiring compositional and multi-step reasoning. Given such complex instructions, SOTA models often make mistakes in faithfully modeling object attributes, and relationships among them. In this work, we present an alternate paradigm for T2I synthesis, decomposing the task of complex multi-step generation into three steps, (a) Generate: we first generate an image using existing diffusion models (b) Plan: we make use of Multi-Modal LLMs (MLLMs) to identify the mistakes in the generated image expressed in terms of individual objects and their properties, and produce a sequence of corrective steps required in the form of an edit-plan. (c) Edit: we make use of an existing text-guided image editing models to sequentially execute our edit-plan over the generated image to get the desired image which is faithful to the original instruction. Our approach derives its strength from the fact that it is modular in nature, is training free, and can be applied over any combination of image generation and editing models. As an added contribution, we also develop a model capable of compositional editing, which further helps improve the overall accuracy of our proposed approach. Our method flexibly trades inference time compute with performance on compositional text prompts. We perform extensive experimental evaluation across 3 benchmarks and 10 T2I models including DALLE-3 and the latest -- SD-3.5-Large. Our approach not only improves the performance of the SOTA models, by upto 3 points, it also reduces the performance gap between weaker and stronger models. https://dair-iitd.github.io/GraPE/{https://dair-iitd.github.io/GraPE/}', 'score': 2, 'issue_id': 1068, 'pub_date': '2024-12-08', 'pub_date_card': {'ru': '8 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 8', 'zh': '12æœˆ8æ—¥'}, 'hash': '3e4dc7918727c4ec', 'authors': ['Ashish Goswami', 'Satyam Kumar Modi', 'Santhosh Rishi Deshineni', 'Harman Singh', 'Prathosh A. P', 'Parag Singla'], 'affiliations': ['IISc Bangalore', 'IIT-Delhi'], 'pdf_title_img': 'assets/pdf/title_img/2412.06089.jpg', 'data': {'categories': ['#diffusion', '#benchmark', '#reasoning', '#multimodal', '#cv'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼: Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸', 'desc': 'Ğ”Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ½Ğ° Ñ‚Ñ€Ğ¸ ÑÑ‚Ğ°Ğ¿Ğ°: Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ, Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ² ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¸ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ»Ğ°Ğ½Ğ° ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸. Ğ—Ğ°Ñ‚ĞµĞ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ÑÑ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ°. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ñ‹Ğ¼, Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ‚ÑŒÑÑ Ğº Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸ÑĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Modular Approach to Enhance Text-to-Image Generation', 'desc': 'This paper introduces a new method for text-to-image (T2I) generation that addresses the challenges of complex text prompts. The proposed approach breaks down the generation process into three steps: generating an initial image, planning corrections using Multi-Modal LLMs, and editing the image based on a corrective plan. This modular framework allows for training-free application across various image generation and editing models, enhancing the accuracy of the output. The authors demonstrate that their method improves performance on complex prompts and narrows the gap between different model capabilities through extensive evaluations.'}, 'zh': {'title': 'æ¨¡å—åŒ–çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆï¼ˆT2Iï¼‰æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ¨¡å‹åœ¨å¤„ç†å¤æ‚æ–‡æœ¬æç¤ºæ—¶çš„ä¸è¶³ã€‚æˆ‘ä»¬å°†å¤æ‚çš„å¤šæ­¥éª¤ç”Ÿæˆä»»åŠ¡åˆ†è§£ä¸ºä¸‰ä¸ªæ­¥éª¤ï¼šé¦–å…ˆç”Ÿæˆå›¾åƒï¼Œç„¶ååˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰è¯†åˆ«ç”Ÿæˆå›¾åƒä¸­çš„é”™è¯¯ï¼Œæœ€åé€šè¿‡æ–‡æœ¬å¼•å¯¼çš„å›¾åƒç¼–è¾‘æ¨¡å‹é€æ­¥æ‰§è¡Œç¼–è¾‘è®¡åˆ’ã€‚è¯¥æ–¹æ³•å…·æœ‰æ¨¡å—åŒ–ã€æ— è®­ç»ƒéœ€æ±‚çš„ç‰¹ç‚¹ï¼Œèƒ½å¤Ÿçµæ´»åº”ç”¨äºå„ç§å›¾åƒç”Ÿæˆå’Œç¼–è¾‘æ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†ç°æœ‰æ¨¡å‹çš„æ€§èƒ½ï¼Œå¹¶ç¼©å°äº†å¼ºå¼±æ¨¡å‹ä¹‹é—´çš„æ€§èƒ½å·®è·ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.07338', 'title': 'Contextualized Counterspeech: Strategies for Adaptation, Personalization, and Evaluation', 'url': 'https://huggingface.co/papers/2412.07338', 'abstract': 'AI-generated counterspeech offers a promising and scalable strategy to curb online toxicity through direct replies that promote civil discourse. However, current counterspeech is one-size-fits-all, lacking adaptation to the moderation context and the users involved. We propose and evaluate multiple strategies for generating tailored counterspeech that is adapted to the moderation context and personalized for the moderated user. We instruct an LLaMA2-13B model to generate counterspeech, experimenting with various configurations based on different contextual information and fine-tuning strategies. We identify the configurations that generate persuasive counterspeech through a combination of quantitative indicators and human evaluations collected via a pre-registered mixed-design crowdsourcing experiment. Results show that contextualized counterspeech can significantly outperform state-of-the-art generic counterspeech in adequacy and persuasiveness, without compromising other characteristics. Our findings also reveal a poor correlation between quantitative indicators and human evaluations, suggesting that these methods assess different aspects and highlighting the need for nuanced evaluation methodologies. The effectiveness of contextualized AI-generated counterspeech and the divergence between human and algorithmic evaluations underscore the importance of increased human-AI collaboration in content moderation.', 'score': 2, 'issue_id': 1066, 'pub_date': '2024-12-10', 'pub_date_card': {'ru': '10 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 10', 'zh': '12æœˆ10æ—¥'}, 'hash': '8b3e1c2da99f56d4', 'authors': ['Lorenzo Cima', 'Alessio Miaschi', 'Amaury Trujillo', 'Marco Avvenuti', "Felice Dell'Orletta", 'Stefano Cresci'], 'affiliations': ['IIT-CNR', 'ILC-CNR', 'University of Pisa'], 'pdf_title_img': 'assets/pdf/title_img/2412.07338.jpg', 'data': {'categories': ['#rlhf', '#training', '#ethics', '#multimodal', '#alignment'], 'emoji': 'ğŸ—¨ï¸', 'ru': {'title': 'Ğ˜Ğ˜ Ğ½Ğ° ÑÑ‚Ñ€Ğ°Ğ¶Ğµ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-ÑÑ‚Ğ¸ĞºĞµÑ‚Ğ°: Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ² Ñ‚Ğ¾ĞºÑĞ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸', 'desc': 'Ğ”Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ÑƒĞ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ñ‚Ğ¾ĞºÑĞ¸Ñ‡Ğ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¼ĞµĞ½Ñ‚Ğ°Ñ€Ğ¸Ğ¸ Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ LLaMA2-13B. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ÑƒĞ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ñ‹ Ğ¿Ğ¾ Ğ°Ğ´ĞµĞºĞ²Ğ°Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑƒĞ±ĞµĞ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ ÑĞ»Ğ°Ğ±ÑƒÑ ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑĞ¼Ğ¸ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼Ğ¸ Ğ»ÑĞ´ĞµĞ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞ¸.'}, 'en': {'title': 'Tailored AI Counterspeech: A New Era in Online Discourse', 'desc': 'This paper discusses a new approach to generating counterspeech using AI to combat online toxicity. The authors focus on creating personalized and context-aware responses rather than generic replies, which can be more effective in promoting civil discourse. They utilize the LLaMA2-13B model and test various configurations to enhance the persuasiveness of the generated counterspeech. The study finds that tailored responses significantly outperform standard methods, highlighting the need for better evaluation techniques that consider both quantitative metrics and human feedback.'}, 'zh': {'title': 'ä¸ªæ€§åŒ–åè¨€è®ºï¼šæå‡åœ¨çº¿äº¤æµçš„æœ‰æ•ˆæ€§', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºäººå·¥æ™ºèƒ½çš„åè¨€è®ºç­–ç•¥ï¼Œæ—¨åœ¨é€šè¿‡ç›´æ¥å›å¤æ¥å‡å°‘åœ¨çº¿æ¯’æ€§ã€‚å½“å‰çš„åè¨€è®ºæ–¹æ³•ç¼ºä¹é’ˆå¯¹æ€§ï¼Œæ— æ³•é€‚åº”ä¸åŒçš„ç®¡ç†ç¯å¢ƒå’Œç”¨æˆ·éœ€æ±‚ã€‚æˆ‘ä»¬ä½¿ç”¨LLaMA2-13Bæ¨¡å‹ç”Ÿæˆä¸ªæ€§åŒ–çš„åè¨€è®ºï¼Œå¹¶é€šè¿‡å¤šç§é…ç½®è¿›è¡Œå®éªŒï¼Œä»¥è¯„ä¼°å…¶æœ‰æ•ˆæ€§ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œé’ˆå¯¹ç‰¹å®šä¸Šä¸‹æ–‡çš„åè¨€è®ºåœ¨é€‚å½“æ€§å’Œè¯´æœåŠ›ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„é€šç”¨åè¨€è®ºï¼Œå¼ºè°ƒäº†äººæœºåä½œåœ¨å†…å®¹ç®¡ç†ä¸­çš„é‡è¦æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.04835', 'title': 'Maximizing Alignment with Minimal Feedback: Efficiently Learning Rewards for Visuomotor Robot Policy Alignment', 'url': 'https://huggingface.co/papers/2412.04835', 'abstract': "Visuomotor robot policies, increasingly pre-trained on large-scale datasets, promise significant advancements across robotics domains. However, aligning these policies with end-user preferences remains a challenge, particularly when the preferences are hard to specify. While reinforcement learning from human feedback (RLHF) has become the predominant mechanism for alignment in non-embodied domains like large language models, it has not seen the same success in aligning visuomotor policies due to the prohibitive amount of human feedback required to learn visual reward functions. To address this limitation, we propose Representation-Aligned Preference-based Learning (RAPL), an observation-only method for learning visual rewards from significantly less human preference feedback. Unlike traditional RLHF, RAPL focuses human feedback on fine-tuning pre-trained vision encoders to align with the end-user's visual representation and then constructs a dense visual reward via feature matching in this aligned representation space. We first validate RAPL through simulation experiments in the X-Magical benchmark and Franka Panda robotic manipulation, demonstrating that it can learn rewards aligned with human preferences, more efficiently uses preference data, and generalizes across robot embodiments. Finally, our hardware experiments align pre-trained Diffusion Policies for three object manipulation tasks. We find that RAPL can fine-tune these policies with 5x less real human preference data, taking the first step towards minimizing human feedback while maximizing visuomotor robot policy alignment.", 'score': 2, 'issue_id': 1064, 'pub_date': '2024-12-06', 'pub_date_card': {'ru': '6 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 6', 'zh': '12æœˆ6æ—¥'}, 'hash': '1fe129433c4c71a5', 'authors': ['Ran Tian', 'Yilin Wu', 'Chenfeng Xu', 'Masayoshi Tomizuka', 'Jitendra Malik', 'Andrea Bajcsy'], 'affiliations': ['Carnegie Mellon University', 'UC Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2412.04835.jpg', 'data': {'categories': ['#training', '#robotics', '#alignment', '#rlhf'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ÑƒÑ‡Ğ°ÑÑ‚Ğ¸ĞµĞ¼ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ RAPL Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼ĞµĞ½ÑŒÑˆĞµĞ³Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ¾Ñ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° (RLHF), RAPL Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ² Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ ĞºĞ¾Ğ½ĞµÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ» ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ñ… Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ¼. RAPL Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ½Ğ°ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ² 5 Ñ€Ğ°Ğ· Ğ¼ĞµĞ½ÑŒÑˆĞµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑÑ… Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°.'}, 'en': {'title': 'Efficiently Aligning Robot Policies with Minimal Human Feedback', 'desc': 'This paper introduces Representation-Aligned Preference-based Learning (RAPL), a novel approach to align visuomotor robot policies with human preferences using minimal feedback. RAPL improves upon traditional reinforcement learning from human feedback (RLHF) by focusing on fine-tuning pre-trained vision encoders, allowing for the construction of dense visual rewards through feature matching. The method is validated through simulations and hardware experiments, showing that it can effectively learn rewards that align with human preferences while requiring significantly less human input. Overall, RAPL represents a significant step towards efficient alignment of robot policies in various manipulation tasks.'}, 'zh': {'title': 'å‡å°‘äººç±»åé¦ˆï¼Œå®ç°æœºå™¨äººç­–ç•¥å¯¹é½çš„çªç ´', 'desc': 'è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œç§°ä¸ºåŸºäºè¡¨ç¤ºå¯¹é½çš„åå¥½å­¦ä¹ ï¼ˆRAPLï¼‰ï¼Œæ—¨åœ¨å‡å°‘å¯¹äººç±»åé¦ˆçš„éœ€æ±‚ï¼Œä»¥ä¾¿æ›´å¥½åœ°å¯¹é½è§†è§‰è¿åŠ¨æœºå™¨äººç­–ç•¥ã€‚ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•éœ€è¦å¤§é‡çš„äººç±»åé¦ˆæ¥å­¦ä¹ è§†è§‰å¥–åŠ±å‡½æ•°ï¼Œè€ŒRAPLé€šè¿‡ä¼˜åŒ–é¢„è®­ç»ƒçš„è§†è§‰ç¼–ç å™¨æ¥å®ç°è¿™ä¸€ç›®æ ‡ã€‚è¯¥æ–¹æ³•é€šè¿‡ç‰¹å¾åŒ¹é…åœ¨å¯¹é½çš„è¡¨ç¤ºç©ºé—´ä¸­æ„å»ºå¯†é›†çš„è§†è§‰å¥–åŠ±ï¼Œä»è€Œæé«˜äº†å­¦ä¹ æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRAPLèƒ½å¤Ÿåœ¨å‡å°‘äººç±»åå¥½æ•°æ®çš„æƒ…å†µä¸‹ï¼Œæœ‰æ•ˆåœ°å¯¹é½æœºå™¨äººç­–ç•¥ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.07720', 'title': 'ACDiT: Interpolating Autoregressive Conditional Modeling and Diffusion Transformer', 'url': 'https://huggingface.co/papers/2412.07720', 'abstract': 'The recent surge of interest in comprehensive multimodal models has necessitated the unification of diverse modalities. However, the unification suffers from disparate methodologies. Continuous visual generation necessitates the full-sequence diffusion-based approach, despite its divergence from the autoregressive modeling in the text domain. We posit that autoregressive modeling, i.e., predicting the future based on past deterministic experience, remains crucial in developing both a visual generation model and a potential unified multimodal model. In this paper, we explore an interpolation between the autoregressive modeling and full-parameters diffusion to model visual information. At its core, we present ACDiT, an Autoregressive blockwise Conditional Diffusion Transformer, where the block size of diffusion, i.e., the size of autoregressive units, can be flexibly adjusted to interpolate between token-wise autoregression and full-sequence diffusion. ACDiT is easy to implement, as simple as creating a Skip-Causal Attention Mask (SCAM) during training. During inference, the process iterates between diffusion denoising and autoregressive decoding that can make full use of KV-Cache. We verify the effectiveness of ACDiT on image and video generation tasks. We also demonstrate that benefitted from autoregressive modeling, ACDiT can be seamlessly used in visual understanding tasks despite being trained on the diffusion objective. The analysis of the trade-off between autoregressive modeling and diffusion demonstrates the potential of ACDiT to be used in long-horizon visual generation tasks. These strengths make it promising as the backbone of future unified models.', 'score': 1, 'issue_id': 1072, 'pub_date': '2024-12-10', 'pub_date_card': {'ru': '10 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 10', 'zh': '12æœˆ10æ—¥'}, 'hash': 'a51fac86380d3065', 'authors': ['Jinyi Hu', 'Shengding Hu', 'Yuxuan Song', 'Yufei Huang', 'Mingxuan Wang', 'Hao Zhou', 'Zhiyuan Liu', 'Wei-Ying Ma', 'Maosong Sun'], 'affiliations': ['ByteDance', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2412.07720.jpg', 'data': {'categories': ['#architecture', '#multimodal', '#video', '#games', '#cv', '#diffusion'], 'emoji': 'ğŸï¸', 'ru': {'title': 'ACDiT: ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¸ Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ACDiT (Autoregressive blockwise Conditional Diffusion Transformer). Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ³Ğ¸Ğ±ĞºĞ¾ Ğ½Ğ°ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ Ğ±Ğ»Ğ¾ĞºĞ° Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸. ACDiT Ğ»ĞµĞ³ĞºĞ¾ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·ÑƒĞµÑ‚ÑÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ°ÑĞºĞ¸ Skip-Causal Attention Mask (SCAM) Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'ACDiT: Bridging Autoregression and Diffusion for Visual Generation', 'desc': 'This paper introduces ACDiT, a novel model that combines autoregressive modeling with diffusion techniques for visual generation. The model allows for flexible adjustment between token-wise autoregression and full-sequence diffusion, enhancing its adaptability. ACDiT is designed to efficiently generate images and videos while also being applicable to visual understanding tasks. The authors demonstrate that this approach can effectively balance the strengths of both methodologies, making it a strong candidate for future multimodal models.'}, 'zh': {'title': 'ACDiTï¼šè‡ªå›å½’ä¸æ‰©æ•£çš„å®Œç¾ç»“åˆ', 'desc': 'è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¤šæ¨¡æ€æ¨¡å‹çš„ç»Ÿä¸€é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨è§†è§‰ç”Ÿæˆæ–¹é¢ã€‚ä½œè€…æå‡ºäº†ä¸€ç§æ–°çš„æ¨¡å‹ACDiTï¼Œå®ƒç»“åˆäº†è‡ªå›å½’å»ºæ¨¡å’Œå…¨åºåˆ—æ‰©æ•£çš„æ–¹æ³•ã€‚ACDiTçš„è®¾è®¡å…è®¸çµæ´»è°ƒæ•´æ‰©æ•£çš„å—å¤§å°ï¼Œä»è€Œåœ¨æ ‡è®°çº§è‡ªå›å½’å’Œå…¨åºåˆ—æ‰©æ•£ä¹‹é—´è¿›è¡Œæ’å€¼ã€‚é€šè¿‡åœ¨å›¾åƒå’Œè§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸Šçš„éªŒè¯ï¼ŒACDiTå±•ç¤ºäº†å…¶åœ¨è§†è§‰ç†è§£ä»»åŠ¡ä¸­çš„æ½œåŠ›ï¼Œè¡¨æ˜å®ƒå¯ä»¥ä½œä¸ºæœªæ¥ç»Ÿä¸€æ¨¡å‹çš„åŸºç¡€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.07187', 'title': 'A New Federated Learning Framework Against Gradient Inversion Attacks', 'url': 'https://huggingface.co/papers/2412.07187', 'abstract': "Federated Learning (FL) aims to protect data privacy by enabling clients to collectively train machine learning models without sharing their raw data. However, recent studies demonstrate that information exchanged during FL is subject to Gradient Inversion Attacks (GIA) and, consequently, a variety of privacy-preserving methods have been integrated into FL to thwart such attacks, such as Secure Multi-party Computing (SMC), Homomorphic Encryption (HE), and Differential Privacy (DP). Despite their ability to protect data privacy, these approaches inherently involve substantial privacy-utility trade-offs. By revisiting the key to privacy exposure in FL under GIA, which lies in the frequent sharing of model gradients that contain private data, we take a new perspective by designing a novel privacy preserve FL framework that effectively ``breaks the direct connection'' between the shared parameters and the local private data to defend against GIA. Specifically, we propose a Hypernetwork Federated Learning (HyperFL) framework that utilizes hypernetworks to generate the parameters of the local model and only the hypernetwork parameters are uploaded to the server for aggregation. Theoretical analyses demonstrate the convergence rate of the proposed HyperFL, while extensive experimental results show the privacy-preserving capability and comparable performance of HyperFL. Code is available at https://github.com/Pengxin-Guo/HyperFL.", 'score': 1, 'issue_id': 1061, 'pub_date': '2024-12-10', 'pub_date_card': {'ru': '10 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 10', 'zh': '12æœˆ10æ—¥'}, 'hash': '772d62408694e82b', 'authors': ['Pengxin Guo', 'Shuang Zeng', 'Wenhao Chen', 'Xiaodan Zhang', 'Weihong Ren', 'Yuyin Zhou', 'Liangqiong Qu'], 'affiliations': ['College of Computer Science, Beijing University of Technology', 'Department of Computer Science and Engineering, UC Santa Cruz', 'Department of Mathematics, The University of Hong Kong', 'School of Computing and Data Science, The University of Hong Kong', 'School of Mechanical Engineering and Automation, Harbin Institute of Technology, Shenzhen'], 'pdf_title_img': 'assets/pdf/title_img/2412.07187.jpg', 'data': {'categories': ['#data', '#healthcare', '#ethics', '#security', '#open_source', '#training'], 'emoji': 'ğŸ›¡ï¸', 'ru': {'title': 'Ğ—Ğ°Ñ‰Ğ¸Ñ‚Ğ° ĞºĞ¾Ğ½Ñ„Ğ¸Ğ´ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ñ„ĞµĞ´ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ³Ğ¸Ğ¿ĞµÑ€ÑĞµÑ‚ĞµĞ¹', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ„ĞµĞ´ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ (FL), Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñƒ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ´ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚ Ğ°Ñ‚Ğ°Ğº Ğ¸Ğ½Ğ²ĞµÑ€ÑĞ¸Ğ¸ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ° (GIA). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº HyperFL, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ³Ğ¸Ğ¿ĞµÑ€ÑĞµÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ½Ğ° ÑĞµÑ€Ğ²ĞµÑ€ Ğ´Ğ»Ñ Ğ°Ğ³Ñ€ĞµĞ³Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ³Ğ¸Ğ¿ĞµÑ€ÑĞµÑ‚Ğ¸. Ğ¢ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° HyperFL. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ HyperFL ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ‚ÑŒ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ´ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Revolutionizing Privacy in Federated Learning with Hypernetworks', 'desc': 'Federated Learning (FL) allows multiple clients to train machine learning models while keeping their data private. However, it is vulnerable to Gradient Inversion Attacks (GIA), which can expose sensitive information through shared model gradients. To address this, the paper introduces a new framework called Hypernetwork Federated Learning (HyperFL), which uses hypernetworks to generate model parameters, ensuring that only hypernetwork parameters are shared. This approach effectively reduces the risk of privacy breaches while maintaining strong performance and convergence rates in training.'}, 'zh': {'title': 'è¶…ç½‘ç»œè”é‚¦å­¦ä¹ ï¼šä¿æŠ¤éšç§çš„æ–°æ–¹æ³•', 'desc': 'è”é‚¦å­¦ä¹ ï¼ˆFLï¼‰æ—¨åœ¨é€šè¿‡è®©å®¢æˆ·ç«¯å…±åŒè®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹è€Œä¸å…±äº«åŸå§‹æ•°æ®æ¥ä¿æŠ¤æ•°æ®éšç§ã€‚ç„¶è€Œï¼Œæœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œåœ¨FLè¿‡ç¨‹ä¸­äº¤æ¢çš„ä¿¡æ¯å¯èƒ½ä¼šå—åˆ°æ¢¯åº¦åæ¼”æ”»å‡»ï¼ˆGIAï¼‰çš„å¨èƒï¼Œå› æ­¤è®¸å¤šéšç§ä¿æŠ¤æ–¹æ³•è¢«æ•´åˆè¿›FLä¸­ä»¥æŠµå¾¡è¿™äº›æ”»å‡»ã€‚è¿™äº›æ–¹æ³•å¦‚å®‰å…¨å¤šæ–¹è®¡ç®—ï¼ˆSMCï¼‰ã€åŒæ€åŠ å¯†ï¼ˆHEï¼‰å’Œå·®åˆ†éšç§ï¼ˆDPï¼‰è™½ç„¶èƒ½ä¿æŠ¤æ•°æ®éšç§ï¼Œä½†é€šå¸¸ä¼šæ¶‰åŠæ˜¾è‘—çš„éšç§ä¸æ•ˆç”¨ä¹‹é—´çš„æƒè¡¡ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„éšç§ä¿æŠ¤FLæ¡†æ¶â€”â€”è¶…ç½‘ç»œè”é‚¦å­¦ä¹ ï¼ˆHyperFLï¼‰ï¼Œé€šè¿‡è®¾è®¡è¶…ç½‘ç»œç”Ÿæˆæœ¬åœ°æ¨¡å‹å‚æ•°ï¼Œä»è€Œæœ‰æ•ˆâ€œæ‰“ç ´â€å…±äº«å‚æ•°ä¸æœ¬åœ°ç§æœ‰æ•°æ®ä¹‹é—´çš„ç›´æ¥è”ç³»ï¼Œä»¥æŠµå¾¡GIAã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.02259', 'title': 'VideoGen-of-Thought: A Collaborative Framework for Multi-Shot Video Generation', 'url': 'https://huggingface.co/papers/2412.02259', 'abstract': 'Current video generation models excel at generating short clips but still struggle with creating multi-shot, movie-like videos. Existing models trained on large-scale data on the back of rich computational resources are unsurprisingly inadequate for maintaining a logical storyline and visual consistency across multiple shots of a cohesive script since they are often trained with a single-shot objective. To this end, we propose VideoGen-of-Thought (VGoT), a collaborative and training-free architecture designed specifically for multi-shot video generation. VGoT is designed with three goals in mind as follows. Multi-Shot Video Generation: We divide the video generation process into a structured, modular sequence, including (1) Script Generation, which translates a curt story into detailed prompts for each shot; (2) Keyframe Generation, responsible for creating visually consistent keyframes faithful to character portrayals; and (3) Shot-Level Video Generation, which transforms information from scripts and keyframes into shots; (4) Smoothing Mechanism that ensures a consistent multi-shot output. Reasonable Narrative Design: Inspired by cinematic scriptwriting, our prompt generation approach spans five key domains, ensuring logical consistency, character development, and narrative flow across the entire video. Cross-Shot Consistency: We ensure temporal and identity consistency by leveraging identity-preserving (IP) embeddings across shots, which are automatically created from the narrative. Additionally, we incorporate a cross-shot smoothing mechanism, which integrates a reset boundary that effectively combines latent features from adjacent shots, resulting in smooth transitions and maintaining visual coherence throughout the video. Our experiments demonstrate that VGoT surpasses existing video generation methods in producing high-quality, coherent, multi-shot videos.', 'score': 43, 'issue_id': 933, 'pub_date': '2024-12-03', 'pub_date_card': {'ru': '3 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 3', 'zh': '12æœˆ3æ—¥'}, 'hash': '5a007f38be3e3ba7', 'authors': ['Mingzhe Zheng', 'Yongqi Xu', 'Haojian Huang', 'Xuran Ma', 'Yexin Liu', 'Wenjie Shu', 'Yatian Pang', 'Feilong Tang', 'Qifeng Chen', 'Harry Yang', 'Ser-Nam Lim'], 'affiliations': ['Everlyn AI', 'Hong Kong University of Science and Technology', 'National University of Singapore', 'Peking University', 'University of Central Florida', 'University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2412.02259.jpg', 'data': {'categories': ['#video', '#story_generation', '#games'], 'emoji': 'ğŸ¬', 'ru': {'title': 'VGoT: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ĞºĞ°Ğ´Ñ€Ğ¾Ğ²Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ»Ğ¾Ğ³Ğ¸ĞºĞ¸ Ğ¿Ğ¾Ğ²ĞµÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ĞºĞ°Ğ´Ñ€Ğ¾Ğ²Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ VideoGen-of-Thought (VGoT). Ğ­Ñ‚Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ÑÑ‚Ğ°Ğ¿Ğ¾Ğ²: Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ñ, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ², Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑÑ†ĞµĞ½ Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ÑĞ³Ğ»Ğ°Ğ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ. VGoT Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑÑ†ĞµĞ½Ğ°Ğ¼Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¼ĞµĞ¶ĞºĞ°Ğ´Ñ€Ğ¾Ğ²Ğ¾Ğµ ÑĞ³Ğ»Ğ°Ğ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ VGoT Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸ ÑĞ²ÑĞ·Ğ½Ñ‹Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾ĞºĞ°Ğ´Ñ€Ğ¾Ğ²Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'Revolutionizing Multi-Shot Video Generation with VGoT', 'desc': 'The paper introduces VideoGen-of-Thought (VGoT), a novel architecture aimed at improving multi-shot video generation. Unlike traditional models that focus on single-shot outputs, VGoT employs a structured approach that includes script generation, keyframe creation, and shot-level video generation, ensuring a cohesive narrative. It emphasizes reasonable narrative design by incorporating principles from cinematic scriptwriting, which enhances character development and logical flow. Additionally, VGoT utilizes identity-preserving embeddings and a cross-shot smoothing mechanism to maintain visual consistency and smooth transitions across multiple shots.'}, 'zh': {'title': 'å¤šé•œå¤´è§†é¢‘ç”Ÿæˆçš„æ–°çªç ´', 'desc': 'å½“å‰çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹åœ¨ç”ŸæˆçŸ­ç‰‡æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨åˆ›å»ºå¤šé•œå¤´ã€ç”µå½±èˆ¬çš„è§†é¢‘æ—¶ä»ç„¶å­˜åœ¨å›°éš¾ã€‚ç°æœ‰æ¨¡å‹é€šå¸¸åªé’ˆå¯¹å•é•œå¤´ç›®æ ‡è¿›è¡Œè®­ç»ƒï¼Œå› æ­¤åœ¨ä¿æŒé€»è¾‘æ•…äº‹çº¿å’Œè§†è§‰ä¸€è‡´æ€§æ–¹é¢æ˜¾å¾—ä¸è¶³ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†VideoGen-of-Thoughtï¼ˆVGoTï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ä¸“é—¨ä¸ºå¤šé•œå¤´è§†é¢‘ç”Ÿæˆè®¾è®¡çš„åä½œå’Œæ— è®­ç»ƒæ¶æ„ã€‚VGoTé€šè¿‡è„šæœ¬ç”Ÿæˆã€å…³é”®å¸§ç”Ÿæˆå’Œé•œå¤´çº§è§†é¢‘ç”Ÿæˆç­‰æ¨¡å—åŒ–æ­¥éª¤ï¼Œç¡®ä¿äº†åˆç†çš„å™äº‹è®¾è®¡å’Œè·¨é•œå¤´çš„ä¸€è‡´æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.19943', 'title': "Critical Tokens Matter: Token-Level Contrastive Estimation Enhence LLM's Reasoning Capability", 'url': 'https://huggingface.co/papers/2411.19943', 'abstract': "Large Language Models (LLMs) have exhibited remarkable performance on reasoning tasks. They utilize autoregressive token generation to construct reasoning trajectories, enabling the development of a coherent chain of thought. In this work, we explore the impact of individual tokens on the final outcomes of reasoning tasks. We identify the existence of ``critical tokens'' that lead to incorrect reasoning trajectories in LLMs. Specifically, we find that LLMs tend to produce positive outcomes when forced to decode other tokens instead of critical tokens. Motivated by this observation, we propose a novel approach - cDPO - designed to automatically recognize and conduct token-level rewards for the critical tokens during the alignment process. Specifically, we develop a contrastive estimation approach to automatically identify critical tokens. It is achieved by comparing the generation likelihood of positive and negative models. To achieve this, we separately fine-tune the positive and negative models on various reasoning trajectories, consequently, they are capable of identifying identify critical tokens within incorrect trajectories that contribute to erroneous outcomes. Moreover, to further align the model with the critical token information during the alignment process, we extend the conventional DPO algorithms to token-level DPO and utilize the differential likelihood from the aforementioned positive and negative model as important weight for token-level DPO learning.Experimental results on GSM8K and MATH500 benchmarks with two-widely used models Llama-3 (8B and 70B) and deepseek-math (7B) demonstrate the effectiveness of the propsoed approach cDPO.", 'score': 33, 'issue_id': 933, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 29', 'zh': '11æœˆ29æ—¥'}, 'hash': 'aaf523f6bd9412e3', 'authors': ['Zicheng Lin', 'Tian Liang', 'Jiahao Xu', 'Xing Wang', 'Ruilin Luo', 'Chufan Shi', 'Siheng Li', 'Yujiu Yang', 'Zhaopeng Tu'], 'affiliations': ['Tsinghua University', 'Tencent AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2411.19943.jpg', 'data': {'categories': ['#math', '#training', '#rlhf', '#reasoning', '#benchmark', '#alignment'], 'emoji': 'ğŸ”', 'ru': {'title': 'ĞŸĞ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ LLM Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²', 'desc': "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ½Ğ° Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ 'ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²', ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´ÑÑ‚ Ğº Ğ½ĞµĞ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¼ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ÑĞ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ cDPO Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… GSM8K Ğ¸ MATH500 Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Llama-3 Ğ¸ deepseek-math Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°."}, 'en': {'title': 'Enhancing Reasoning in LLMs by Identifying Critical Tokens', 'desc': "This paper investigates how individual tokens in Large Language Models (LLMs) affect reasoning outcomes. It identifies 'critical tokens' that can lead to incorrect reasoning paths, suggesting that LLMs perform better when they focus on non-critical tokens. The authors introduce a new method called cDPO, which uses contrastive estimation to automatically detect these critical tokens during the model's alignment process. Experimental results show that cDPO improves reasoning performance on benchmark datasets by effectively managing token-level rewards."}, 'zh': {'title': 'è¯†åˆ«å…³é”®tokenï¼Œæå‡æ¨ç†å‡†ç¡®æ€§', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿé€šè¿‡è‡ªå›å½’çš„æ–¹å¼ç”Ÿæˆæ¨ç†è¿‡ç¨‹ã€‚æœ¬æ–‡æ¢è®¨äº†å•ä¸ªtokenå¯¹æ¨ç†ä»»åŠ¡æœ€ç»ˆç»“æœçš„å½±å“ï¼Œå‘ç°å­˜åœ¨â€œå…³é”®tokenâ€ï¼Œè¿™äº›tokenä¼šå¯¼è‡´é”™è¯¯çš„æ¨ç†è½¨è¿¹ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•cDPOï¼Œæ—¨åœ¨è‡ªåŠ¨è¯†åˆ«å…³é”®tokenå¹¶åœ¨å¯¹é½è¿‡ç¨‹ä¸­è¿›è¡Œtokençº§å¥–åŠ±ã€‚é€šè¿‡å¯¹æ¯”æ­£è´Ÿæ¨¡å‹çš„ç”Ÿæˆå¯èƒ½æ€§ï¼Œæˆ‘ä»¬èƒ½å¤Ÿè¯†åˆ«å‡ºåœ¨é”™è¯¯è½¨è¿¹ä¸­å¯¼è‡´é”™è¯¯ç»“æœçš„å…³é”®tokenï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.01928', 'title': 'MALT: Improving Reasoning with Multi-Agent LLM Training', 'url': 'https://huggingface.co/papers/2412.01928', 'abstract': 'Enabling effective collaboration among LLMs is a crucial step toward developing autonomous systems capable of solving complex problems. While LLMs are typically used as single-model generators, where humans critique and refine their outputs, the potential for jointly-trained collaborative models remains largely unexplored. Despite promising results in multi-agent communication and debate settings, little progress has been made in training models to work together on tasks. In this paper, we present a first step toward "Multi-agent LLM training" (MALT) on reasoning problems. Our approach employs a sequential multi-agent setup with heterogeneous LLMs assigned specialized roles: a generator, verifier, and refinement model iteratively solving problems. We propose a trajectory-expansion-based synthetic data generation process and a credit assignment strategy driven by joint outcome based rewards. This enables our post-training setup to utilize both positive and negative trajectories to autonomously improve each model\'s specialized capabilities as part of a joint sequential system. We evaluate our approach across MATH, GSM8k, and CQA, where MALT on Llama 3.1 8B models achieves relative improvements of 14.14%, 7.12%, and 9.40% respectively over the same baseline model. This demonstrates an early advance in multi-agent cooperative capabilities for performance on mathematical and common sense reasoning questions. More generally, our work provides a concrete direction for research around multi-agent LLM training approaches.', 'score': 19, 'issue_id': 943, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 2', 'zh': '12æœˆ2æ—¥'}, 'hash': '980ef49924f6a484', 'authors': ['Sumeet Ramesh Motwani', 'Chandler Smith', 'Rocktim Jyoti Das', 'Markian Rybchuk', 'Philip H. S. Torr', 'Ivan Laptev', 'Fabio Pizzati', 'Ronald Clark', 'Christian Schroeder de Witt'], 'affiliations': ['University of Oxford', 'Cooperative AI Foundation', 'MBZUAI', 'UC Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2412.01928.jpg', 'data': {'categories': ['#math', '#training', '#synthetic', '#agents', '#reasoning'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ¡Ğ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ LLM Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ', 'desc': "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ´Ğ»Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ½Ğ°Ğ´ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ 'ĞœÑƒĞ»ÑŒÑ‚Ğ¸-Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ LLM' (MALT), Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ setup Ñ Ğ³ĞµÑ‚ĞµÑ€Ğ¾Ğ³ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ LLM Ğ² ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ¾Ğ»ÑÑ…: Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€, Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ. ĞŸÑ€Ğ¾Ñ†ĞµÑÑ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ ĞºÑ€ĞµĞ´Ğ¸Ñ‚Ğ°, ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼ÑƒÑ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¸ Ğ·Ğ´Ñ€Ğ°Ğ²Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸."}, 'en': {'title': 'Unlocking Collaborative Intelligence in LLMs', 'desc': 'This paper introduces a novel approach called Multi-agent LLM training (MALT) aimed at enhancing collaboration among large language models (LLMs) for solving complex reasoning tasks. The authors propose a structured setup where different LLMs take on specialized rolesâ€”such as generator, verifier, and refinement modelâ€”to iteratively tackle problems. They implement a synthetic data generation process and a credit assignment strategy that rewards models based on their joint performance, allowing them to learn from both successful and unsuccessful attempts. The results show significant performance improvements on various reasoning benchmarks, highlighting the potential of cooperative multi-agent systems in advancing LLM capabilities.'}, 'zh': {'title': 'å¤šæ™ºèƒ½ä½“åä½œè®­ç»ƒï¼Œæå‡æ¨ç†èƒ½åŠ›ï¼', 'desc': 'æœ¬æ–‡æ¢è®¨äº†å¤šæ™ºèƒ½ä½“å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è®­ç»ƒçš„æ½œåŠ›ï¼Œæ—¨åœ¨æé«˜æ¨¡å‹åœ¨å¤æ‚é—®é¢˜ä¸Šçš„åä½œèƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§å¤šæ™ºèƒ½ä½“è®¾ç½®ï¼Œæ¨¡å‹åˆ†ä¸ºç”Ÿæˆå™¨ã€éªŒè¯å™¨å’Œç²¾ç‚¼æ¨¡å‹ï¼ŒååŒè§£å†³æ¨ç†é—®é¢˜ã€‚é€šè¿‡è½¨è¿¹æ‰©å±•çš„åˆæˆæ•°æ®ç”Ÿæˆå’ŒåŸºäºè”åˆç»“æœçš„å¥–åŠ±ç­–ç•¥ï¼Œæˆ‘ä»¬çš„æ¨¡å‹èƒ½å¤Ÿè‡ªä¸»æå‡å„è‡ªçš„ä¸“ä¸šèƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨MALTæ–¹æ³•çš„Llama 3.1 8Bæ¨¡å‹åœ¨æ•°å­¦å’Œå¸¸è¯†æ¨ç†ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.01981', 'title': 'Free Process Rewards without Process Labels', 'url': 'https://huggingface.co/papers/2412.01981', 'abstract': "Different from its counterpart outcome reward models (ORMs), which evaluate the entire responses, a process reward model (PRM) scores a reasoning trajectory step by step, providing denser and more fine grained rewards. However, training a PRM requires labels annotated at every intermediate step, presenting significant challenges for both manual and automatic data collection. This paper aims to address this challenge. Both theoretically and empirically, we show that an implicit PRM can be obtained at no additional cost, by simply training an ORM on the cheaper response-level labels. The only assumption is to parameterize the outcome reward as the log-likelihood ratios of the policy and reference models, which can be optimized regardless of the specific choice of loss objectives. In experiments, we instantiate our implicit PRMs with various objectives and evaluate their performance on MATH. We show that our implicit PRM outperforms a strong MCTS-based baseline \\'a la Math-Shepherd using less than 1/38 of the training data. Its performance can be further improved with majority voting. We further find that scaling up instructions and responses benefits our implicit PRM, and the latter brings a larger gain. Particularly, we find that our implicit PRM, when instantiated with the cross-entropy (CE) loss, is more data-efficient and can keep improving generation models even when trained with only one response per instruction, the setup that suffers from extreme data scarcity and imbalance. Further, instructions should be relevant to downstream tasks while the diversity of responses does not bring gains. Surprisingly, training on extra Math-Shepherd step labels brings no further improvements to our implicit PRM trained on only outcome data. We hope that our work will encourage a rethinking of PRM training approaches and contribute to making training PRMs more accessible.", 'score': 18, 'issue_id': 933, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 2', 'zh': '12æœˆ2æ—¥'}, 'hash': '13434e4f301a0d88', 'authors': ['Lifan Yuan', 'Wendi Li', 'Huayu Chen', 'Ganqu Cui', 'Ning Ding', 'Kaiyan Zhang', 'Bowen Zhou', 'Zhiyuan Liu', 'Hao Peng'], 'affiliations': ['Huazhong University of Science and Technology', 'Tsinghua University', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2412.01981.jpg', 'data': {'categories': ['#optimization', '#math', '#training', '#reasoning', '#data', '#low_resource'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ PRM Ğ±ĞµĞ· Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ (PRM) Ğ² Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ½ĞµÑĞ²Ğ½ÑƒÑ PRM Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚, Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ·Ğ° Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ (ORM) Ğ½Ğ° Ğ±Ğ¾Ğ»ĞµĞµ Ğ´ĞµÑˆĞµĞ²Ñ‹Ñ… Ğ¼ĞµÑ‚ĞºĞ°Ñ… ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ MATH Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ±ĞµĞ¹Ğ·Ğ»Ğ°Ğ¹Ğ½ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ MCTS, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼ĞµĞ½ĞµĞµ 1/38 Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½ĞµÑĞ²Ğ½Ğ¾Ğ¹ PRM.'}, 'en': {'title': 'Unlocking Efficient Training for Process Reward Models', 'desc': 'This paper introduces a novel approach to training process reward models (PRMs) that score reasoning steps individually, as opposed to outcome reward models (ORMs) which evaluate entire responses. The authors propose that an implicit PRM can be derived from an ORM trained on simpler response-level labels, thus avoiding the need for detailed step-by-step annotations. Through theoretical and empirical analysis, they demonstrate that this implicit PRM can achieve superior performance on tasks like MATH, using significantly less training data than traditional methods. The findings suggest that optimizing the outcome reward as log-likelihood ratios enhances data efficiency and model performance, even in scenarios with limited training examples.'}, 'zh': {'title': 'éšå¼è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼šé«˜æ•ˆè®­ç»ƒçš„æ–°æ€è·¯', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰ï¼Œä¸ä¼ ç»Ÿçš„ç»“æœå¥–åŠ±æ¨¡å‹ï¼ˆORMï¼‰ä¸åŒï¼ŒPRMèƒ½å¤Ÿé€æ­¥è¯„ä¼°æ¨ç†è¿‡ç¨‹ï¼Œæä¾›æ›´ç»†è‡´çš„å¥–åŠ±ã€‚ç„¶è€Œï¼Œè®­ç»ƒPRMéœ€è¦åœ¨æ¯ä¸ªä¸­é—´æ­¥éª¤éƒ½æœ‰æ ‡æ³¨ï¼Œè¿™åœ¨æ•°æ®æ”¶é›†ä¸Šé¢ä¸´æŒ‘æˆ˜ã€‚æˆ‘ä»¬å±•ç¤ºäº†é€šè¿‡è®­ç»ƒORMå¹¶ä½¿ç”¨å“åº”çº§åˆ«çš„æ ‡ç­¾ï¼Œå¯ä»¥åœ¨æ²¡æœ‰é¢å¤–æˆæœ¬çš„æƒ…å†µä¸‹è·å¾—éšå¼PRMã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œéšå¼PRMåœ¨æ•°æ®æ•ˆç‡å’Œæ€§èƒ½ä¸Šä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œå°¤å…¶æ˜¯åœ¨æ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.02611', 'title': 'AV-Odyssey Bench: Can Your Multimodal LLMs Really Understand Audio-Visual Information?', 'url': 'https://huggingface.co/papers/2412.02611', 'abstract': 'Recently, multimodal large language models (MLLMs), such as GPT-4o, Gemini 1.5 Pro, and Reka Core, have expanded their capabilities to include vision and audio modalities. While these models demonstrate impressive performance across a wide range of audio-visual applications, our proposed DeafTest reveals that MLLMs often struggle with simple tasks humans find trivial: 1) determining which of two sounds is louder, and 2) determining which of two sounds has a higher pitch. Motivated by these observations, we introduce AV-Odyssey Bench, a comprehensive audio-visual benchmark designed to assess whether those MLLMs can truly understand the audio-visual information. This benchmark encompasses 4,555 carefully crafted problems, each incorporating text, visual, and audio components. To successfully infer answers, models must effectively leverage clues from both visual and audio inputs. To ensure precise and objective evaluation of MLLM responses, we have structured the questions as multiple-choice, eliminating the need for human evaluation or LLM-assisted assessment. We benchmark a series of closed-source and open-source models and summarize the observations. By revealing the limitations of current models, we aim to provide useful insight for future dataset collection and model development.', 'score': 17, 'issue_id': 933, 'pub_date': '2024-12-03', 'pub_date_card': {'ru': '3 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 3', 'zh': '12æœˆ3æ—¥'}, 'hash': 'f63565048b4948b4', 'authors': ['Kaixiong Gong', 'Kaituo Feng', 'Bohao Li', 'Yibing Wang', 'Mofan Cheng', 'Shijia Yang', 'Jiaming Han', 'Benyou Wang', 'Yutong Bai', 'Zhuoran Yang', 'Xiangyu Yue'], 'affiliations': ['CUHK (SZ)', 'CUHK MMLab', 'Stanford University', 'UC Berkeley', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2412.02611.jpg', 'data': {'categories': ['#open_source', '#benchmark', '#interpretability', '#multimodal', '#games'], 'emoji': 'ğŸ§', 'ru': {'title': 'Ğ¡Ğ»Ñ‹ÑˆĞ°Ñ‚ Ğ»Ğ¸ Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‚Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ²Ğ¸Ğ´ÑÑ‚?', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚ DeafTest Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº AV-Odyssey Bench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ°ÑƒĞ´Ğ¸Ğ¾-Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ MLLM Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ½Ğµ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ³Ñ€Ğ¾Ğ¼ĞºĞ¾ÑÑ‚Ğ¸ Ğ¸ Ğ²Ñ‹ÑĞ¾Ñ‚Ñ‹ Ğ·Ğ²ÑƒĞºĞ¾Ğ². Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 4,555 Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼, Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±Ğ¾Ğ¸Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€ÑĞ´Ğ° Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ²Ñ‹ÑĞ²Ğ¸Ğ² Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ´Ğ»Ñ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞµĞ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Unveiling the Limits of Multimodal Models with AV-Odyssey Bench', 'desc': 'This paper introduces DeafTest, a benchmark that highlights the limitations of multimodal large language models (MLLMs) in understanding basic audio tasks that humans find easy. The authors present AV-Odyssey Bench, which consists of 4,555 problems that require models to analyze and integrate audio, visual, and text information. The benchmark is designed to objectively evaluate MLLM performance through multiple-choice questions, avoiding reliance on human judgment. By assessing various models, the study aims to shed light on the shortcomings of current MLLMs and guide future improvements in model training and dataset creation.'}, 'zh': {'title': 'æ­ç¤ºå¤šæ¨¡æ€æ¨¡å‹çš„å±€é™æ€§', 'desc': 'æœ€è¿‘ï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å¦‚GPT-4oã€Gemini 1.5 Proå’ŒReka Coreï¼Œæ‰©å±•äº†å…¶åœ¨è§†è§‰å’ŒéŸ³é¢‘æ–¹é¢çš„èƒ½åŠ›ã€‚å°½ç®¡è¿™äº›æ¨¡å‹åœ¨å¤šç§éŸ³é¢‘-è§†è§‰åº”ç”¨ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†æˆ‘ä»¬çš„DeafTestæ˜¾ç¤ºï¼ŒMLLMsåœ¨ä¸€äº›äººç±»è®¤ä¸ºç®€å•çš„ä»»åŠ¡ä¸Šå¸¸å¸¸è¡¨ç°ä¸ä½³ï¼Œä¾‹å¦‚åˆ¤æ–­ä¸¤ä¸ªå£°éŸ³å“ªä¸ªæ›´å“å’Œå“ªä¸ªéŸ³è°ƒæ›´é«˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†AV-Odyssey Benchï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„éŸ³é¢‘-è§†è§‰åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°è¿™äº›MLLMsæ˜¯å¦çœŸæ­£ç†è§£éŸ³é¢‘-è§†è§‰ä¿¡æ¯ã€‚è¯¥åŸºå‡†åŒ…å«4555ä¸ªç²¾å¿ƒè®¾è®¡çš„é—®é¢˜ï¼Œè¦æ±‚æ¨¡å‹æœ‰æ•ˆåˆ©ç”¨è§†è§‰å’ŒéŸ³é¢‘è¾“å…¥ä¸­çš„çº¿ç´¢ï¼Œä»¥å‡†ç¡®æ¨æ–­ç­”æ¡ˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.02114', 'title': 'OmniCreator: Self-Supervised Unified Generation with Universal Editing', 'url': 'https://huggingface.co/papers/2412.02114', 'abstract': 'We introduce OmniCreator, a novel framework that can conduct text-prompted unified (image+video) generation as well as editing all in one place. OmniCreator acquires generative and universal editing capabilities in a self-supervised manner, taking original text-video pairs as conditions while utilizing the same video as a denoising target to learn the semantic correspondence between video and text. During inference, when presented with a text prompt and a video, OmniCreator is capable of generating a target that is faithful to both, achieving a universal editing effect that is unconstrained as opposed to existing editing work that primarily focuses on certain editing types or relies on additional controls (e.g., structural conditions, attention features, or DDIM inversion). On the other hand, when presented with a text prompt only, OmniCreator becomes generative, producing high-quality video as a result of the semantic correspondence learned. Importantly, we found that the same capabilities extend to images as is, making OmniCreator a truly unified framework. Further, due to the lack of existing generative video editing benchmarks, we introduce the OmniBench-99 dataset, designed to evaluate the performance of generative video editing models comprehensively. Extensive experiments demonstrate that OmniCreator exhibits substantial superiority over all other models.', 'score': 12, 'issue_id': 939, 'pub_date': '2024-12-03', 'pub_date_card': {'ru': '3 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 3', 'zh': '12æœˆ3æ—¥'}, 'hash': '62bf26709baf7f97', 'authors': ['Haodong Chen', 'Lan Wang', 'Harry Yang', 'Ser-Nam Lim'], 'affiliations': ['Everlyn AI', 'HKUST', 'MSU', 'UCF'], 'pdf_title_img': 'assets/pdf/title_img/2412.02114.jpg', 'data': {'categories': ['#diffusion', '#multimodal', '#benchmark', '#dataset', '#video', '#optimization'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¾Ñ€ Ğ¼ĞµĞ´Ğ¸Ğ° Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑƒ', 'desc': 'OmniCreator - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². ĞĞ½Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¿Ğ°Ñ€Ñ‹ Ñ‚ĞµĞºÑÑ‚-Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¸ ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ²ÑĞ·Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼. OmniCreator Ğ¼Ğ¾Ğ¶ĞµÑ‚ ĞºĞ°Ğº Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ, Ñ‚Ğ°Ğº Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… OmniBench-99 Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'OmniCreator: Unified Text-Prompted Image and Video Generation and Editing', 'desc': 'OmniCreator is a new framework that allows for the generation and editing of both images and videos using text prompts. It learns to understand the relationship between text and video through self-supervised learning, using original text-video pairs as input. During use, it can either generate new videos based on text prompts or edit existing videos while maintaining fidelity to the provided text. Additionally, the framework is evaluated using a new dataset called OmniBench-99, which helps assess the performance of generative video editing models.'}, 'zh': {'title': 'OmniCreatorï¼šç»Ÿä¸€çš„å›¾åƒä¸è§†é¢‘ç”Ÿæˆä¸ç¼–è¾‘æ¡†æ¶', 'desc': 'OmniCreatoræ˜¯ä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨ä¸€ä¸ªå¹³å°ä¸Šè¿›è¡Œæ–‡æœ¬æç¤ºçš„ç»Ÿä¸€ç”Ÿæˆï¼ˆå›¾åƒ+è§†é¢‘ï¼‰å’Œç¼–è¾‘ã€‚å®ƒé€šè¿‡è‡ªç›‘ç£å­¦ä¹ ï¼Œåˆ©ç”¨åŸå§‹çš„æ–‡æœ¬-è§†é¢‘å¯¹ä½œä¸ºæ¡ä»¶ï¼ŒåŒæ—¶ä½¿ç”¨ç›¸åŒçš„è§†é¢‘ä½œä¸ºå»å™ªç›®æ ‡ï¼Œå­¦ä¹ è§†é¢‘ä¸æ–‡æœ¬ä¹‹é—´çš„è¯­ä¹‰å¯¹åº”å…³ç³»ã€‚åœ¨æ¨ç†é˜¶æ®µï¼ŒOmniCreatorèƒ½å¤Ÿæ ¹æ®æ–‡æœ¬æç¤ºå’Œè§†é¢‘ç”Ÿæˆå¿ å®äºä¸¤è€…çš„ç›®æ ‡ï¼Œå®ç°æ— çº¦æŸçš„é€šç”¨ç¼–è¾‘æ•ˆæœã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†OmniBench-99æ•°æ®é›†ï¼Œä»¥å…¨é¢è¯„ä¼°ç”Ÿæˆè§†é¢‘ç¼–è¾‘æ¨¡å‹çš„æ€§èƒ½ï¼Œå®éªŒç»“æœè¡¨æ˜OmniCreatoråœ¨æ‰€æœ‰æ¨¡å‹ä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„ä¼˜åŠ¿ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.19655', 'title': 'Truth or Mirage? Towards End-to-End Factuality Evaluation with LLM-OASIS', 'url': 'https://huggingface.co/papers/2411.19655', 'abstract': 'After the introduction of Large Language Models (LLMs), there have been substantial improvements in the performance of Natural Language Generation (NLG) tasks, including Text Summarization and Machine Translation. However, LLMs still produce outputs containing hallucinations, that is, content not grounded in factual information. Therefore, developing methods to assess the factuality of LLMs has become urgent.   Indeed, resources for factuality evaluation have recently emerged. Although challenging, these resources face one or more of the following limitations: (i) they are tailored to a specific task or domain; (ii) they are limited in size, thereby preventing the training of new factuality evaluators; (iii) they are designed for simpler verification tasks, such as claim verification.   To address these issues, we introduce LLM-Oasis, to the best of our knowledge the largest resource for training end-to-end factuality evaluators. LLM-Oasis is constructed by extracting claims from Wikipedia, falsifying a subset of these claims, and generating pairs of factual and unfactual texts. We then rely on human annotators to both validate the quality of our dataset and to create a gold standard test set for benchmarking factuality evaluation systems.   Our experiments demonstrate that LLM-Oasis presents a significant challenge for state-of-the-art LLMs, with GPT-4o achieving up to 60% accuracy in our proposed end-to-end factuality evaluation task, highlighting its potential to drive future research in the field.', 'score': 11, 'issue_id': 947, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 29', 'zh': '11æœˆ29æ—¥'}, 'hash': 'd1255811f49c640e', 'authors': ['Alessandro ScirÃ¨', 'Andrei Stefan Bejgu', 'Simone Tedeschi', 'Karim Ghonim', 'Federico Martelli', 'Roberto Navigli'], 'affiliations': ['Babelscape, Italy', 'Sapienza University of Rome'], 'pdf_title_img': 'assets/pdf/title_img/2411.19655.jpg', 'data': {'categories': ['#hallucinations', '#benchmark', '#multimodal', '#machine_translation', '#dataset'], 'emoji': 'ğŸ”', 'ru': {'title': 'LLM-Oasis: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ„Ğ°ĞºÑ‚Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ LLM-Oasis - ĞºÑ€ÑƒĞ¿Ğ½ĞµĞ¹ÑˆĞ¸Ğ¹ Ñ€ĞµÑÑƒÑ€Ñ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¾Ñ†ĞµĞ½Ñ‰Ğ¸ĞºĞ¾Ğ² Ñ„Ğ°ĞºÑ‚Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). LLM-Oasis ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ· Ğ’Ğ¸ĞºĞ¸Ğ¿ĞµĞ´Ğ¸Ğ¸, Ñ„Ğ°Ğ»ÑŒÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ñ‡Ğ°ÑÑ‚Ğ¸ ÑÑ‚Ğ¸Ñ… ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ°Ñ€ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ Ğ½ĞµÑ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ LLM-Oasis Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ»Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… LLM, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ GPT-4 Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¾ 60% Ğ² Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ„Ğ°ĞºÑ‚Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ ĞµÑÑƒÑ€Ñ Ğ¸Ğ¼ĞµĞµÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ´Ğ»Ñ ÑÑ‚Ğ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸.'}, 'en': {'title': 'LLM-Oasis: A New Frontier for Factuality Evaluation in Language Models', 'desc': 'This paper discusses the advancements in Natural Language Generation (NLG) tasks due to Large Language Models (LLMs), while also addressing the issue of hallucinations, which are inaccuracies in generated content. It introduces LLM-Oasis, a comprehensive resource designed to train evaluators for assessing the factuality of LLM outputs. LLM-Oasis is created by extracting claims from Wikipedia, falsifying some, and generating pairs of factual and unfactual texts, validated by human annotators. The experiments show that LLM-Oasis poses a significant challenge to current LLMs, indicating its potential to enhance future research in factuality evaluation.'}, 'zh': {'title': 'æ¨åŠ¨äº‹å®æ€§è¯„ä¼°çš„æœªæ¥ç ”ç©¶', 'desc': 'æœ¬æ–‡ä»‹ç»äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€ç”Ÿæˆä»»åŠ¡ä¸­çš„åº”ç”¨ï¼Œå°¤å…¶æ˜¯åœ¨æ–‡æœ¬æ‘˜è¦å’Œæœºå™¨ç¿»è¯‘æ–¹é¢çš„è¿›å±•ã€‚ç„¶è€Œï¼ŒLLMs ä»ç„¶ä¼šäº§ç”Ÿè™šå‡ä¿¡æ¯ï¼Œå³ä¸äº‹å®ä¸ç¬¦çš„å†…å®¹ï¼Œå› æ­¤è¯„ä¼° LLMs çš„äº‹å®æ€§å˜å¾—éå¸¸é‡è¦ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº† LLM-Oasisï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè®­ç»ƒç«¯åˆ°ç«¯äº‹å®æ€§è¯„ä¼°å™¨çš„æœ€å¤§èµ„æºã€‚é€šè¿‡ä»ç»´åŸºç™¾ç§‘æå–å£°æ˜å¹¶ç”ŸæˆçœŸå®ä¸è™šå‡çš„æ–‡æœ¬å¯¹ï¼ŒLLM-Oasis ä¸ºäº‹å®æ€§è¯„ä¼°ç³»ç»Ÿçš„åŸºå‡†æµ‹è¯•æä¾›äº†é‡è¦çš„æ•°æ®é›†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.02632', 'title': 'Scaling Image Tokenizers with Grouped Spherical Quantization', 'url': 'https://huggingface.co/papers/2412.02632', 'abstract': 'Vision tokenizers have gained a lot of attraction due to their scalability and compactness; previous works depend on old-school GAN-based hyperparameters, biased comparisons, and a lack of comprehensive analysis of the scaling behaviours. To tackle those issues, we introduce Grouped Spherical Quantization (GSQ), featuring spherical codebook initialization and lookup regularization to constrain codebook latent to a spherical surface. Our empirical analysis of image tokenizer training strategies demonstrates that GSQ-GAN achieves superior reconstruction quality over state-of-the-art methods with fewer training iterations, providing a solid foundation for scaling studies. Building on this, we systematically examine the scaling behaviours of GSQ, specifically in latent dimensionality, codebook size, and compression ratios, and their impact on model performance. Our findings reveal distinct behaviours at high and low spatial compression levels, underscoring challenges in representing high-dimensional latent spaces. We show that GSQ can restructure high-dimensional latent into compact, low-dimensional spaces, thus enabling efficient scaling with improved quality. As a result, GSQ-GAN achieves a 16x down-sampling with a reconstruction FID (rFID) of 0.50.', 'score': 9, 'issue_id': 933, 'pub_date': '2024-12-03', 'pub_date_card': {'ru': '3 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 3', 'zh': '12æœˆ3æ—¥'}, 'hash': '60eda94a31cded90', 'authors': ['Jiangtao Wang', 'Zhen Qin', 'Yifan Zhang', 'Vincent Tao Hu', 'BjÃ¶rn Ommer', 'Rania Briq', 'Stefan Kesselheim'], 'affiliations': ['CompVis @ LMU Munich', 'JÃ¼lich Supercomputing Centre', 'TapTap', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2412.02632.jpg', 'data': {'categories': ['#training', '#inference', '#cv', '#data'], 'emoji': 'ğŸ”', 'ru': {'title': 'GSQ: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° ÑÑ„ĞµÑ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ¸', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ - Ğ“Ñ€ÑƒĞ¿Ğ¿Ğ¾Ğ²Ğ°Ñ Ğ¡Ñ„ĞµÑ€Ğ¸Ñ‡ĞµÑĞºĞ°Ñ ĞšĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ (GSQ). GSQ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ„ĞµÑ€Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ĞºĞ¾Ğ´Ğ¾Ğ²Ğ¾Ğ¹ ĞºĞ½Ğ¸Ğ³Ğ¸ Ğ¸ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ´Ğ»Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° ĞºĞ¾Ğ´Ğ¾Ğ²Ğ¾Ğ¹ ĞºĞ½Ğ¸Ğ³Ğ¸ ÑÑ„ĞµÑ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚ÑŒÑ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ ÑĞ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ GSQ-GAN Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞµĞ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ GSQ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ğ¸ Ğ½Ğ¸Ğ·ĞºĞ¸Ñ… ÑƒÑ€Ğ¾Ğ²Ğ½ÑÑ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ, Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ².'}, 'en': {'title': 'Efficient Image Processing with Grouped Spherical Quantization', 'desc': "This paper introduces a new method called Grouped Spherical Quantization (GSQ) for improving vision tokenizers, which are tools used to process images efficiently. GSQ uses a special technique to initialize and regularize a spherical codebook, helping to keep the data organized on a spherical surface. The authors demonstrate that GSQ-GAN, a model based on this method, can reconstruct images with high quality while requiring fewer training iterations compared to existing methods. Their analysis also reveals how different factors like latent dimensionality and codebook size affect the model's performance, particularly in handling high-dimensional data efficiently."}, 'zh': {'title': 'åˆ†ç»„çƒé¢é‡åŒ–ï¼šé«˜æ•ˆçš„è§†è§‰æ ‡è®°å™¨æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„è§†è§‰æ ‡è®°å™¨æ–¹æ³•ï¼Œç§°ä¸ºåˆ†ç»„çƒé¢é‡åŒ–ï¼ˆGSQï¼‰ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿæ–¹æ³•ä¸­çš„ä¸€äº›é—®é¢˜ã€‚GSQé€šè¿‡çƒé¢ä»£ç æœ¬åˆå§‹åŒ–å’ŒæŸ¥æ‰¾æ­£åˆ™åŒ–ï¼Œé™åˆ¶äº†ä»£ç æœ¬æ½œåœ¨ç©ºé—´åœ¨çƒé¢ä¸Šçš„åˆ†å¸ƒã€‚æˆ‘ä»¬çš„å®è¯åˆ†æè¡¨æ˜ï¼ŒGSQ-GANåœ¨é‡å»ºè´¨é‡ä¸Šä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œå¹¶ä¸”è®­ç»ƒè¿­ä»£æ¬¡æ•°æ›´å°‘ã€‚ç ”ç©¶è¿˜ç³»ç»Ÿåœ°è€ƒå¯Ÿäº†GSQåœ¨æ½œåœ¨ç»´åº¦ã€ä»£ç æœ¬å¤§å°å’Œå‹ç¼©æ¯”ç­‰æ–¹é¢çš„æ‰©å±•è¡Œä¸ºï¼Œæ­ç¤ºäº†é«˜ç»´æ½œåœ¨ç©ºé—´è¡¨ç¤ºçš„æŒ‘æˆ˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.02592', 'title': 'OCR Hinders RAG: Evaluating the Cascading Impact of OCR on Retrieval-Augmented Generation', 'url': 'https://huggingface.co/papers/2412.02592', 'abstract': "Retrieval-augmented Generation (RAG) enhances Large Language Models (LLMs) by integrating external knowledge to reduce hallucinations and incorporate up-to-date information without retraining. As an essential part of RAG, external knowledge bases are commonly built by extracting structured data from unstructured PDF documents using Optical Character Recognition (OCR). However, given the imperfect prediction of OCR and the inherent non-uniform representation of structured data, knowledge bases inevitably contain various OCR noises. In this paper, we introduce OHRBench, the first benchmark for understanding the cascading impact of OCR on RAG systems. OHRBench includes 350 carefully selected unstructured PDF documents from six real-world RAG application domains, along with Q&As derived from multimodal elements in documents, challenging existing OCR solutions used for RAG To better understand OCR's impact on RAG systems, we identify two primary types of OCR noise: Semantic Noise and Formatting Noise and apply perturbation to generate a set of structured data with varying degrees of each OCR noise. Using OHRBench, we first conduct a comprehensive evaluation of current OCR solutions and reveal that none is competent for constructing high-quality knowledge bases for RAG systems. We then systematically evaluate the impact of these two noise types and demonstrate the vulnerability of RAG systems. Furthermore, we discuss the potential of employing Vision-Language Models (VLMs) without OCR in RAG systems. Code: https://github.com/opendatalab/OHR-Bench", 'score': 8, 'issue_id': 937, 'pub_date': '2024-12-03', 'pub_date_card': {'ru': '3 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 3', 'zh': '12æœˆ3æ—¥'}, 'hash': '91dbac114744b1e9', 'authors': ['Junyuan Zhang', 'Qintong Zhang', 'Bin Wang', 'Linke Ouyang', 'Zichen Wen', 'Ying Li', 'Ka-Ho Chow', 'Conghui He', 'Wentao Zhang'], 'affiliations': ['Beihang University', 'Peking University', 'Shanghai AI Laboratory', 'Shanghai Jiaotong University', 'The University of HongKong'], 'pdf_title_img': 'assets/pdf/title_img/2412.02592.jpg', 'data': {'categories': ['#hallucinations', '#dataset', '#multimodal', '#benchmark', '#rag', '#optimization', '#survey'], 'emoji': 'ğŸ”', 'ru': {'title': 'OHRBench: Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°Ñ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ OCR Ğ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ RAG', 'desc': 'OHRBench - ÑÑ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ² (OCR) Ğ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ (RAG). ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 350 Ğ½ĞµÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… PDF-Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸Ğ· ÑˆĞµÑÑ‚Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ RAG, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ²Ñ‹Ğ´ĞµĞ»ÑÑÑ‚ Ğ´Ğ²Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ° ÑˆÑƒĞ¼Ğ° OCR: ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¸ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ½Ñ‹Ğ¹, Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ Ğ²Ğ¾Ğ·Ğ¼ÑƒÑ‰ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ ÑÑ‚ĞµĞ¿ĞµĞ½ÑŒÑ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ñ‚Ğ¸Ğ¿Ğ° ÑˆÑƒĞ¼Ğ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ ÑĞ¸ÑÑ‚ĞµĞ¼ RAG Ğº Ğ¾ÑˆĞ¸Ğ±ĞºĞ°Ğ¼ OCR Ğ¸ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (VLM) Ğ±ĞµĞ· OCR Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… RAG.'}, 'en': {'title': "Enhancing RAG: Understanding OCR's Impact with OHRBench", 'desc': 'This paper presents OHRBench, a benchmark designed to evaluate the effects of Optical Character Recognition (OCR) errors on Retrieval-Augmented Generation (RAG) systems. It identifies two main types of OCR noise: Semantic Noise, which affects the meaning of the extracted data, and Formatting Noise, which impacts the structure and presentation. The study reveals that current OCR solutions are inadequate for creating high-quality knowledge bases necessary for effective RAG applications. Additionally, it explores the potential of using Vision-Language Models (VLMs) as an alternative to traditional OCR methods in RAG systems.'}, 'zh': {'title': 'æ­ç¤ºOCRå¯¹RAGç³»ç»Ÿçš„å½±å“', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†OHRBenchï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç”¨äºç†è§£å…‰å­¦å­—ç¬¦è¯†åˆ«ï¼ˆOCRï¼‰å¯¹æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿå½±å“çš„åŸºå‡†ã€‚ç ”ç©¶å‘ç°ï¼ŒOCRåœ¨å¤„ç†éç»“æ„åŒ–PDFæ–‡æ¡£æ—¶ä¼šå¼•å…¥è¯­ä¹‰å™ªå£°å’Œæ ¼å¼å™ªå£°ï¼Œå¯¼è‡´çŸ¥è¯†åº“è´¨é‡ä¸‹é™ã€‚é€šè¿‡å¯¹350ä¸ªçœŸå®ä¸–ç•Œåº”ç”¨é¢†åŸŸçš„æ–‡æ¡£è¿›è¡Œè¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºç°æœ‰çš„OCRè§£å†³æ–¹æ¡ˆæ— æ³•æœ‰æ•ˆæ„å»ºé«˜è´¨é‡çš„çŸ¥è¯†åº“ã€‚æœ€åï¼Œè®ºæ–‡æ¢è®¨äº†åœ¨RAGç³»ç»Ÿä¸­ä½¿ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰è€Œä¸ä¾èµ–OCRçš„æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.01292', 'title': 'LSceneLLM: Enhancing Large 3D Scene Understanding Using Adaptive Visual Preferences', 'url': 'https://huggingface.co/papers/2412.01292', 'abstract': "Research on 3D Vision-Language Models (3D-VLMs) is gaining increasing attention, which is crucial for developing embodied AI within 3D scenes, such as visual navigation and embodied question answering. Due to the high density of visual features, especially in large 3D scenes, accurately locating task-relevant visual information is challenging. Existing works attempt to segment all objects and consider their features as scene representations. However, these task-agnostic object features include much redundant information and missing details for the task-relevant area. To tackle these problems, we propose LSceneLLM, an adaptive framework that automatically identifies task-relevant areas by leveraging LLM's visual preference for different tasks, followed by a plug-and-play scene magnifier module to capture fine-grained details in focused areas. Specifically, a dense token selector examines the attention map of LLM to identify visual preferences for the instruction input. It then magnifies fine-grained details of the focusing area. An adaptive self-attention module is leveraged to fuse the coarse-grained and selected fine-grained visual information. To comprehensively evaluate the large scene understanding ability of 3D-VLMs, we further introduce a cross-room understanding benchmark, XR-Scene, which contains a series of large scene understanding tasks including XR-QA, XR-EmbodiedPlanning, and XR-SceneCaption. Experiments show that our method surpasses existing methods on both large scene understanding and existing scene understanding benchmarks. Plunging our scene magnifier module into the existing 3D-VLMs also brings significant improvement.", 'score': 7, 'issue_id': 933, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 2', 'zh': '12æœˆ2æ—¥'}, 'hash': 'e8f8ddd05e13e9ef', 'authors': ['Hongyan Zhi', 'Peihao Chen', 'Junyan Li', 'Shuailei Ma', 'Xinyu Sun', 'Tianhang Xiang', 'Yinjie Lei', 'Mingkui Tan', 'Chuang Gan'], 'affiliations': ['MIT-IBM Watson AI Lab', 'Northeastern University', 'Pazhou Laboratory', 'Sichuan University', 'South China University of Technology', 'Tencent Robotics X', 'UMass Amherst'], 'pdf_title_img': 'assets/pdf/title_img/2412.01292.jpg', 'data': {'categories': ['#architecture', '#benchmark', '#3d', '#multimodal'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ 3D-ÑÑ†ĞµĞ½ Ñ LSceneLLM', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ 3D Vision-Language Models (3D-VLMs) Ğ²Ğ°Ğ¶Ğ½Ğ¾ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ AI Ğ² 3D-ÑÑ†ĞµĞ½Ğ°Ñ…, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹. Ğ˜Ğ·-Ğ·Ğ° Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… 3D-ÑÑ†ĞµĞ½Ğ°Ñ… ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸Ñ‚ÑŒ Ğ²Ğ°Ğ¶Ğ½ÑƒÑ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ. ĞŸÑ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° LSceneLLM, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ LLM Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ½Ğ°Ñˆ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑÑ†ĞµĞ½ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ñ€Ğ¸ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ² ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ 3D-VLMs.'}, 'en': {'title': 'Enhancing 3D Scene Understanding with LSceneLLM', 'desc': 'This paper introduces LSceneLLM, a novel framework designed to enhance 3D Vision-Language Models (3D-VLMs) for better understanding of large 3D scenes. It addresses the challenge of identifying task-relevant visual information amidst the dense features present in these scenes. By utilizing a dense token selector and an adaptive self-attention module, the framework effectively magnifies important details while reducing redundant information. The authors also present a new benchmark, XR-Scene, to evaluate the performance of 3D-VLMs on various large scene understanding tasks, demonstrating that their approach significantly outperforms existing methods.'}, 'zh': {'title': 'è‡ªé€‚åº”3Dè§†è§‰è¯­è¨€æ¨¡å‹ï¼Œæå‡åœºæ™¯ç†è§£èƒ½åŠ›', 'desc': '3Dè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆ3D-VLMsï¼‰çš„ç ”ç©¶è¶Šæ¥è¶Šå—åˆ°å…³æ³¨ï¼Œè¿™å¯¹åœ¨3Dåœºæ™¯ä¸­å‘å±•å…·èº«äººå·¥æ™ºèƒ½è‡³å…³é‡è¦ï¼Œå¦‚è§†è§‰å¯¼èˆªå’Œå…·èº«é—®ç­”ã€‚ç”±äº3Dåœºæ™¯ä¸­è§†è§‰ç‰¹å¾çš„é«˜å¯†åº¦ï¼Œå‡†ç¡®å®šä½ä¸ä»»åŠ¡ç›¸å…³çš„è§†è§‰ä¿¡æ¯å˜å¾—å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ç°æœ‰æ–¹æ³•å°è¯•å¯¹æ‰€æœ‰å¯¹è±¡è¿›è¡Œåˆ†å‰²ï¼Œå¹¶å°†å…¶ç‰¹å¾è§†ä¸ºåœºæ™¯è¡¨ç¤ºï¼Œä½†è¿™äº›ä¸ä»»åŠ¡æ— å…³çš„å¯¹è±¡ç‰¹å¾åŒ…å«å¤§é‡å†—ä½™ä¿¡æ¯å’Œç¼ºå¤±çš„ç»†èŠ‚ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†LSceneLLMï¼Œä¸€ä¸ªè‡ªé€‚åº”æ¡†æ¶ï¼Œé€šè¿‡åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯¹ä¸åŒä»»åŠ¡çš„è§†è§‰åå¥½ï¼Œè‡ªåŠ¨è¯†åˆ«ä¸ä»»åŠ¡ç›¸å…³çš„åŒºåŸŸï¼Œå¹¶é€šè¿‡å¯æ’æ‹”çš„åœºæ™¯æ”¾å¤§æ¨¡å—æ•æ‰èšç„¦åŒºåŸŸçš„ç»†ç²’åº¦ç»†èŠ‚ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.02700', 'title': 'Motion Prompting: Controlling Video Generation with Motion Trajectories', 'url': 'https://huggingface.co/papers/2412.02700', 'abstract': 'Motion control is crucial for generating expressive and compelling video content; however, most existing video generation models rely mainly on text prompts for control, which struggle to capture the nuances of dynamic actions and temporal compositions. To this end, we train a video generation model conditioned on spatio-temporally sparse or dense motion trajectories. In contrast to prior motion conditioning work, this flexible representation can encode any number of trajectories, object-specific or global scene motion, and temporally sparse motion; due to its flexibility we refer to this conditioning as motion prompts. While users may directly specify sparse trajectories, we also show how to translate high-level user requests into detailed, semi-dense motion prompts, a process we term motion prompt expansion. We demonstrate the versatility of our approach through various applications, including camera and object motion control, "interacting" with an image, motion transfer, and image editing. Our results showcase emergent behaviors, such as realistic physics, suggesting the potential of motion prompts for probing video models and interacting with future generative world models. Finally, we evaluate quantitatively, conduct a human study, and demonstrate strong performance. Video results are available on our webpage: https://motion-prompting.github.io/', 'score': 5, 'issue_id': 949, 'pub_date': '2024-12-03', 'pub_date_card': {'ru': '3 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 3', 'zh': '12æœˆ3æ—¥'}, 'hash': '6adffbc375f9f4f5', 'authors': ['Daniel Geng', 'Charles Herrmann', 'Junhwa Hur', 'Forrester Cole', 'Serena Zhang', 'Tobias Pfaff', 'Tatiana Lopez-Guevara', 'Carl Doersch', 'Yusuf Aytar', 'Michael Rubinstein', 'Chen Sun', 'Oliver Wang', 'Andrew Owens', 'Deqing Sun'], 'affiliations': ['Brown University', 'Google DeepMind', 'University of Michigan'], 'pdf_title_img': 'assets/pdf/title_img/2412.02700.jpg', 'data': {'categories': ['#video', '#optimization', '#multimodal', '#games'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Ğ£Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ĞµĞ¼ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ñ‹ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ĞµĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ»Ğ¸ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ, Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ñ… 'motion prompts'. Ğ­Ñ‚Ğ° Ğ³Ğ¸Ğ±ĞºĞ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ ĞºĞ°Ğ¼ĞµÑ€Ñ‹, Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ Ğ¾Ğ±Ñ‰ÑƒÑ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºÑƒ ÑÑ†ĞµĞ½Ñ‹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ¿ĞµÑ‡Ğ°Ñ‚Ğ»ÑÑÑ‰Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ÑÑ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¿ĞµÑ€ĞµĞ½Ğ¾Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€Ğ¾ÑĞ²Ğ»ÑĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ñ„Ğ¸Ğ·Ğ¸ĞºĞµ."}, 'en': {'title': 'Empowering Video Generation with Flexible Motion Prompts', 'desc': 'This paper presents a novel approach to video generation by using motion prompts, which are flexible representations of motion trajectories. Unlike traditional models that rely solely on text prompts, this method allows for the encoding of various types of motion, including object-specific and global scene movements. The authors introduce a technique called motion prompt expansion, enabling users to convert high-level requests into detailed motion trajectories. The results indicate that this approach not only enhances video generation but also allows for realistic interactions and behaviors within the generated content.'}, 'zh': {'title': 'è¿åŠ¨æç¤ºï¼šè§†é¢‘ç”Ÿæˆçš„æ–°æ–¹å¼', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œåˆ©ç”¨è¿åŠ¨è½¨è¿¹ä½œä¸ºæ§åˆ¶æ‰‹æ®µï¼Œå…‹æœäº†ä¼ ç»Ÿæ–‡æœ¬æç¤ºåœ¨åŠ¨æ€åŠ¨ä½œå’Œæ—¶é—´ç»„åˆä¸Šçš„å±€é™æ€§ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§çµæ´»çš„è¿åŠ¨æç¤ºè¡¨ç¤ºï¼Œå¯ä»¥ç¼–ç ä»»æ„æ•°é‡çš„è½¨è¿¹ï¼ŒåŒ…æ‹¬ç‰¹å®šç‰©ä½“æˆ–å…¨å±€åœºæ™¯çš„è¿åŠ¨ã€‚ç”¨æˆ·å¯ä»¥ç›´æ¥æŒ‡å®šç¨€ç–è½¨è¿¹ï¼Œæˆ‘ä»¬è¿˜å±•ç¤ºäº†å¦‚ä½•å°†é«˜å±‚æ¬¡çš„ç”¨æˆ·è¯·æ±‚è½¬åŒ–ä¸ºè¯¦ç»†çš„åŠç¨€ç–è¿åŠ¨æç¤ºã€‚é€šè¿‡å¤šç§åº”ç”¨å±•ç¤ºäº†æˆ‘ä»¬æ–¹æ³•çš„å¤šæ ·æ€§ï¼ŒåŒ…æ‹¬ç›¸æœºå’Œç‰©ä½“è¿åŠ¨æ§åˆ¶ã€ä¸å›¾åƒçš„äº¤äº’ã€è¿åŠ¨è½¬ç§»å’Œå›¾åƒç¼–è¾‘ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.19542', 'title': 'A dynamic parallel method for performance optimization on hybrid CPUs', 'url': 'https://huggingface.co/papers/2411.19542', 'abstract': 'The AIPC concept is gaining popularity, and more and more hybrid CPUs will be running AI models on client devices. However, the current AI inference framework overlooks the imbalanced hardware capability of hybrid CPUs, leading to low inference performance. To address this issue, we have introduced a dynamic parallel method for hybrid CPUs, which significantly increases LLM inference performance by balancing the workload for each core of a hybrid CPU before the parallel work starts. This method has enabled Neural Speed to achieve more than 90% (on average) of memory bandwidth on two hybrid Intel CPUs.', 'score': 5, 'issue_id': 936, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 29', 'zh': '11æœˆ29æ—¥'}, 'hash': '27226211eddf71d4', 'authors': ['Luo Yu', 'Liu Yucheng', 'Shen Haihao'], 'affiliations': ['Intel Corporation'], 'pdf_title_img': 'assets/pdf/title_img/2411.19542.jpg', 'data': {'categories': ['#inference', '#architecture', '#optimization'], 'emoji': 'ğŸš€', 'ru': {'title': 'Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ğ˜Ğ˜ Ğ½Ğ° Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ñ… CPU: Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ´Ğ»Ñ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ñ€Ğ¾Ğ², Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ¸ Ğ´Ğ»Ñ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ğ˜Ğ˜ Ğ½Ğµ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½ĞµÑ€Ğ°Ğ²Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ´ĞµÑ€ Ğ² Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ñ… CPU, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€ÑƒĞµÑ‚ Ğ½Ğ°Ğ³Ñ€ÑƒĞ·ĞºÑƒ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ´Ñ€Ğ°Ğ¼Ğ¸ Ğ¿ĞµÑ€ĞµĞ´ Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ¾Ğ¼ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° LLM. Ğ’ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğµ, Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Neural Speed Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³ Ğ±Ğ¾Ğ»ĞµĞµ 90% Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ½Ğ¾Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ½Ğ° Ğ´Ğ²ÑƒÑ… Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ñ€Ğ°Ñ… Intel.'}, 'en': {'title': 'Boosting AI Inference with Dynamic Workload Balancing on Hybrid CPUs', 'desc': 'The paper discusses the growing trend of using hybrid CPUs for running AI models on client devices. It highlights a problem where existing AI inference frameworks do not effectively utilize the varying hardware capabilities of these hybrid CPUs, resulting in suboptimal performance. To solve this, the authors propose a dynamic parallel method that balances the workload across CPU cores before starting parallel processing. This approach has led to significant improvements in inference performance, achieving over 90% memory bandwidth utilization on hybrid Intel CPUs.'}, 'zh': {'title': 'åŠ¨æ€å¹³è¡¡ï¼Œæå‡AIæ¨ç†æ€§èƒ½ï¼', 'desc': 'AIPCæ¦‚å¿µè¶Šæ¥è¶Šå—æ¬¢è¿ï¼Œè¶Šæ¥è¶Šå¤šçš„æ··åˆCPUå°†åœ¨å®¢æˆ·ç«¯è®¾å¤‡ä¸Šè¿è¡ŒAIæ¨¡å‹ã€‚ç„¶è€Œï¼Œç›®å‰çš„AIæ¨ç†æ¡†æ¶å¿½è§†äº†æ··åˆCPUçš„ä¸å¹³è¡¡ç¡¬ä»¶èƒ½åŠ›ï¼Œå¯¼è‡´æ¨ç†æ€§èƒ½ä½ä¸‹ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŠ¨æ€å¹¶è¡Œæ–¹æ³•ï¼Œæ˜¾è‘—æé«˜äº†æ··åˆCPUçš„LLMæ¨ç†æ€§èƒ½ï¼Œé€šè¿‡åœ¨å¹¶è¡Œå·¥ä½œå¼€å§‹ä¹‹å‰å¹³è¡¡æ¯ä¸ªæ ¸å¿ƒçš„å·¥ä½œè´Ÿè½½ã€‚è¯¥æ–¹æ³•ä½¿Neural Speedåœ¨ä¸¤æ¬¾æ··åˆIntel CPUä¸Šå®ç°äº†è¶…è¿‡90%çš„å†…å­˜å¸¦å®½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.19067', 'title': 'MaskRIS: Semantic Distortion-aware Data Augmentation for Referring Image Segmentation', 'url': 'https://huggingface.co/papers/2411.19067', 'abstract': "Referring Image Segmentation (RIS) is an advanced vision-language task that involves identifying and segmenting objects within an image as described by free-form text descriptions. While previous studies focused on aligning visual and language features, exploring training techniques, such as data augmentation, remains underexplored. In this work, we explore effective data augmentation for RIS and propose a novel training framework called Masked Referring Image Segmentation (MaskRIS). We observe that the conventional image augmentations fall short of RIS, leading to performance degradation, while simple random masking significantly enhances the performance of RIS. MaskRIS uses both image and text masking, followed by Distortion-aware Contextual Learning (DCL) to fully exploit the benefits of the masking strategy. This approach can improve the model's robustness to occlusions, incomplete information, and various linguistic complexities, resulting in a significant performance improvement. Experiments demonstrate that MaskRIS can easily be applied to various RIS models, outperforming existing methods in both fully supervised and weakly supervised settings. Finally, MaskRIS achieves new state-of-the-art performance on RefCOCO, RefCOCO+, and RefCOCOg datasets. Code is available at https://github.com/naver-ai/maskris.", 'score': 5, 'issue_id': 934, 'pub_date': '2024-11-28', 'pub_date_card': {'ru': '28 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 28', 'zh': '11æœˆ28æ—¥'}, 'hash': '74d4a17af3574a5d', 'authors': ['Minhyun Lee', 'Seungho Lee', 'Song Park', 'Dongyoon Han', 'Byeongho Heo', 'Hyunjung Shim'], 'affiliations': ['KAIST AI', 'NAVER AI Lab', 'Yonsei University'], 'pdf_title_img': 'assets/pdf/title_img/2411.19067.jpg', 'data': {'categories': ['#optimization', '#cv', '#survey', '#dataset', '#training'], 'emoji': 'ğŸ­', 'ru': {'title': 'ĞœĞ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ (RIS). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ MaskRIS, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº Ğ¾ĞºĞºĞ»ÑĞ·Ğ¸ÑĞ¼, Ğ½ĞµĞ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ MaskRIS Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ĞºĞ°Ğº Ğ¿Ñ€Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¼, Ñ‚Ğ°Ğº Ğ¸ Ğ¿Ñ€Ğ¸ ÑĞ»Ğ°Ğ±Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸.'}, 'en': {'title': 'Enhancing Referring Image Segmentation with Masking Techniques', 'desc': 'Referring Image Segmentation (RIS) is a task that combines visual and language understanding to identify and segment objects in images based on text descriptions. This paper introduces a new training framework called Masked Referring Image Segmentation (MaskRIS), which focuses on effective data augmentation techniques for RIS. The authors found that traditional image augmentations were inadequate, while their method of random masking significantly improved performance. MaskRIS enhances model robustness against occlusions and linguistic variations, achieving state-of-the-art results on several benchmark datasets.'}, 'zh': {'title': 'Masked Referring Image Segmentationï¼šæå‡å›¾åƒåˆ†å‰²æ€§èƒ½çš„æ–°æ–¹æ³•', 'desc': 'å¼•ç”¨å›¾åƒåˆ†å‰²ï¼ˆRISï¼‰æ˜¯ä¸€ç§å…ˆè¿›çš„è§†è§‰-è¯­è¨€ä»»åŠ¡ï¼Œæ—¨åœ¨æ ¹æ®è‡ªç”±å½¢å¼çš„æ–‡æœ¬æè¿°è¯†åˆ«å’Œåˆ†å‰²å›¾åƒä¸­çš„å¯¹è±¡ã€‚æœ¬æ–‡æ¢è®¨äº†æœ‰æ•ˆçš„æ•°æ®å¢å¼ºæŠ€æœ¯ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°çš„è®­ç»ƒæ¡†æ¶ï¼Œç§°ä¸ºMasked Referring Image Segmentationï¼ˆMaskRISï¼‰ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä¼ ç»Ÿçš„å›¾åƒå¢å¼ºæ–¹æ³•åœ¨RISä¸­æ•ˆæœä¸ä½³ï¼Œè€Œç®€å•çš„éšæœºé®ç½©æ˜¾è‘—æå‡äº†RISçš„æ€§èƒ½ã€‚MaskRISç»“åˆäº†å›¾åƒå’Œæ–‡æœ¬é®ç½©ï¼Œå¹¶é‡‡ç”¨äº†å¤±çœŸæ„ŸçŸ¥ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆDCLï¼‰ï¼Œä»è€Œæé«˜äº†æ¨¡å‹å¯¹é®æŒ¡ã€ä¸å®Œæ•´ä¿¡æ¯å’Œè¯­è¨€å¤æ‚æ€§çš„é²æ£’æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.00239', 'title': 'Generating a Low-code Complete Workflow via Task Decomposition and RAG', 'url': 'https://huggingface.co/papers/2412.00239', 'abstract': 'AI technologies are moving rapidly from research to production. With the popularity of Foundation Models (FMs) that generate text, images, and video, AI-based systems are increasing their complexity. Compared to traditional AI-based software, systems employing FMs, or GenAI-based systems, are more difficult to design due to their scale and versatility. This makes it necessary to document best practices, known as design patterns in software engineering, that can be used across GenAI applications. Our first contribution is to formalize two techniques, Task Decomposition and Retrieval-Augmented Generation (RAG), as design patterns for GenAI-based systems. We discuss their trade-offs in terms of software quality attributes and comment on alternative approaches. We recommend to AI practitioners to consider these techniques not only from a scientific perspective but also from the standpoint of desired engineering properties such as flexibility, maintainability, safety, and security. As a second contribution, we describe our industry experience applying Task Decomposition and RAG to build a complex real-world GenAI application for enterprise users: Workflow Generation. The task of generating workflows entails generating a specific plan using data from the system environment, taking as input a user requirement. As these two patterns affect the entire AI development cycle, we explain how they impacted the dataset creation, model training, model evaluation, and deployment phases.', 'score': 2, 'issue_id': 948, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 29', 'zh': '11æœˆ29æ—¥'}, 'hash': '44bf29d0fbeafbc3', 'authors': ['Orlando Marquez Ayala', 'Patrice BÃ©chard'], 'affiliations': ['ServiceNow'], 'pdf_title_img': 'assets/pdf/title_img/2412.00239.jpg', 'data': {'categories': ['#optimization', '#training', '#rag', '#dataset', '#security', '#benchmark'], 'emoji': 'ğŸ§©', 'ru': {'title': 'ĞŸĞ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜: Ğ¾Ñ‚ Ñ‚ĞµĞ¾Ñ€Ğ¸Ğ¸ Ğº Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸ĞºĞµ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ²ÑƒÑ… Ñ‚ĞµÑ…Ğ½Ğ¸Ğº - Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ (RAG) - ĞºĞ°Ğº Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ² Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±ÑÑƒĞ¶Ğ´Ğ°ÑÑ‚ ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑÑ‹ ÑÑ‚Ğ¸Ñ… Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ² Ñ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ¾Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´ÑƒÑÑ‚ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ñ… Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ, Ğ½Ğ¾ Ğ¸ Ñ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ¶ĞµĞ»Ğ°ĞµĞ¼Ñ‹Ñ… Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ½Ñ‹Ñ… ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ². Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ÑÑ Ğ¾Ğ¿Ñ‹Ñ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¸Ñ… Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ² Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜ Ğ´Ğ»Ñ ĞºĞ¾Ñ€Ğ¿Ğ¾Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ - Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±ÑŠÑÑĞ½ÑÑÑ‚, ĞºĞ°Ğº ÑÑ‚Ğ¸ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ğ¿Ğ¾Ğ²Ğ»Ğ¸ÑĞ»Ğ¸ Ğ½Ğ° Ğ²ĞµÑÑŒ Ñ†Ğ¸ĞºĞ» Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ˜Ğ˜, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ², Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğµ.'}, 'en': {'title': 'Streamlining GenAI Development with Design Patterns', 'desc': 'This paper discusses the challenges of designing AI systems that use Foundation Models (FMs) due to their complexity and versatility. It introduces two design patterns, Task Decomposition and Retrieval-Augmented Generation (RAG), which can help streamline the development of GenAI applications. The authors analyze the trade-offs of these techniques in relation to software quality attributes like flexibility and security. Additionally, they share their practical experience in applying these patterns to create a Workflow Generation application for enterprise users, highlighting their influence on various stages of the AI development cycle.'}, 'zh': {'title': 'è®¾è®¡æ¨¡å¼åŠ©åŠ›ç”ŸæˆAIç³»ç»Ÿçš„å¼€å‘', 'desc': 'éšç€åŸºç¡€æ¨¡å‹ï¼ˆFMsï¼‰åœ¨æ–‡æœ¬ã€å›¾åƒå’Œè§†é¢‘ç”Ÿæˆä¸­çš„æ™®åŠï¼ŒAIç³»ç»Ÿçš„å¤æ‚æ€§ä¸æ–­å¢åŠ ã€‚ä¸ä¼ ç»Ÿçš„AIè½¯ä»¶ç›¸æ¯”ï¼ŒåŸºäºç”ŸæˆAIï¼ˆGenAIï¼‰çš„ç³»ç»Ÿåœ¨è®¾è®¡ä¸Šæ›´å…·æŒ‘æˆ˜æ€§ï¼Œå› æ­¤éœ€è¦è®°å½•æœ€ä½³å®è·µï¼Œç§°ä¸ºè®¾è®¡æ¨¡å¼ã€‚æœ¬æ–‡é¦–æ¬¡å°†ä»»åŠ¡åˆ†è§£å’Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å½¢å¼åŒ–ä¸ºGenAIç³»ç»Ÿçš„è®¾è®¡æ¨¡å¼ï¼Œå¹¶è®¨è®ºå®ƒä»¬åœ¨è½¯ä»¶è´¨é‡å±æ€§æ–¹é¢çš„æƒè¡¡ã€‚æˆ‘ä»¬è¿˜åˆ†äº«äº†åœ¨ä¼ä¸šç”¨æˆ·çš„å¤æ‚GenAIåº”ç”¨â€”â€”å·¥ä½œæµç”Ÿæˆä¸­çš„å®é™…ç»éªŒï¼Œè¯´æ˜è¿™äº›æ¨¡å¼å¦‚ä½•å½±å“æ•´ä¸ªAIå¼€å‘å‘¨æœŸã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.01558', 'title': 'VideoLights: Feature Refinement and Cross-Task Alignment Transformer for Joint Video Highlight Detection and Moment Retrieval', 'url': 'https://huggingface.co/papers/2412.01558', 'abstract': 'Video Highlight Detection and Moment Retrieval (HD/MR) are essential in video analysis. Recent joint prediction transformer models often overlook their cross-task dynamics and video-text alignment and refinement. Moreover, most models typically use limited, uni-directional attention mechanisms, resulting in weakly integrated representations and suboptimal performance in capturing the interdependence between video and text modalities. Although large-language and vision-language models (LLM/LVLMs) have gained prominence across various domains, their application in this field remains relatively underexplored. Here we propose VideoLights, a novel HD/MR framework addressing these limitations through (i) Convolutional Projection and Feature Refinement modules with an alignment loss for better video-text feature alignment, (ii) Bi-Directional Cross-Modal Fusion network for strongly coupled query-aware clip representations, and (iii) Uni-directional joint-task feedback mechanism enhancing both tasks through correlation. In addition, (iv) we introduce hard positive/negative losses for adaptive error penalization and improved learning, and (v) leverage LVLMs like BLIP-2 for enhanced multimodal feature integration and intelligent pretraining using synthetic data generated from LVLMs. Comprehensive experiments on QVHighlights, TVSum, and Charades-STA benchmarks demonstrate state-of-the-art performance. Codes and models are available at https://github.com/dpaul06/VideoLights .', 'score': 2, 'issue_id': 935, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 2', 'zh': '12æœˆ2æ—¥'}, 'hash': '12235c4ebf26fe4a', 'authors': ['Dhiman Paul', 'Md Rizwan Parvez', 'Nabeel Mohammed', 'Shafin Rahman'], 'affiliations': ['Department of Electrical and Computer Engineering, North South University, Dhaka, Bangladesh', 'Qatar Computing Research Institute (QCRI), Qatar'], 'pdf_title_img': 'assets/pdf/title_img/2412.01558.jpg', 'data': {'categories': ['#video', '#games', '#synthetic', '#architecture', '#benchmark', '#multimodal'], 'emoji': 'ğŸ¥', 'ru': {'title': 'VideoLights: ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ²Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ VideoLights - Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¿Ğ¾ Ğ½Ğ¸Ğ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ´Ğ²ÑƒĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğµ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸. ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ VideoLights Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Enhancing Video-Text Integration with VideoLights', 'desc': 'This paper presents VideoLights, a new framework for Video Highlight Detection and Moment Retrieval that improves the integration of video and text data. It introduces several innovative components, including Convolutional Projection and Feature Refinement modules to enhance video-text alignment, and a Bi-Directional Cross-Modal Fusion network for better representation of clips. The framework also employs a Uni-directional joint-task feedback mechanism to strengthen the relationship between the two tasks and introduces hard positive/negative losses for more effective learning. The results show that VideoLights achieves state-of-the-art performance on multiple benchmarks, demonstrating its effectiveness in video analysis.'}, 'zh': {'title': 'VideoLightsï¼šæå‡è§†é¢‘ä¸æ–‡æœ¬åˆ†æçš„å…¨æ–°æ¡†æ¶', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºVideoLightsçš„è§†é¢‘é«˜äº®æ£€æµ‹å’Œæ—¶åˆ»æ£€ç´¢æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ¨¡å‹åœ¨è§†é¢‘ä¸æ–‡æœ¬å¯¹é½å’Œè·¨ä»»åŠ¡åŠ¨æ€æ–¹é¢çš„ä¸è¶³ã€‚æˆ‘ä»¬å¼•å…¥äº†å·ç§¯æŠ•å½±å’Œç‰¹å¾ç²¾ç‚¼æ¨¡å—ï¼Œä»¥æé«˜è§†é¢‘å’Œæ–‡æœ¬ç‰¹å¾çš„å¯¹é½æ•ˆæœï¼Œå¹¶é‡‡ç”¨åŒå‘è·¨æ¨¡æ€èåˆç½‘ç»œæ¥å¢å¼ºæŸ¥è¯¢æ„ŸçŸ¥çš„ç‰‡æ®µè¡¨ç¤ºã€‚é€šè¿‡å•å‘è”åˆä»»åŠ¡åé¦ˆæœºåˆ¶ï¼Œæˆ‘ä»¬èƒ½å¤Ÿæå‡ä¸¤ä¸ªä»»åŠ¡ä¹‹é—´çš„ç›¸å…³æ€§ï¼ŒåŒæ—¶å¼•å…¥ç¡¬æ­£è´ŸæŸå¤±ä»¥æ”¹å–„å­¦ä¹ æ•ˆæœã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVideoLightsåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.18478', 'title': 'Beyond Examples: High-level Automated Reasoning Paradigm in In-Context Learning via MCTS', 'url': 'https://huggingface.co/papers/2411.18478', 'abstract': "In-context Learning (ICL) enables large language models (LLMs) to tackle downstream tasks through sophisticated prompting and high-quality demonstrations. However, this traditional ICL paradigm shows limitations when facing complex mathematical reasoning tasks, primarily due to its heavy dependence on example quality and the necessity for human intervention in challenging scenarios. To address these limitations, this paper presents HiAR-ICL, a High-level Automated Reasoning paradigm in ICL that shifts focus from specific examples to abstract thinking patterns, extending the conventional concept of context in ICL. HiAR-ICL introduces five atomic reasoning actions as fundamental components for constructing chain-structured patterns. Using Monte Carlo Tree Search, we explore reasoning paths and construct thought cards to guide subsequent inference. We then develop a cognitive complexity framework that dynamically matches problems with appropriate thought cards. Experimental results demonstrate HiAR-ICL's effectiveness, achieving state-of-the-art accuracy (79.6%) on the MATH benchmark with Qwen2.5-7B-Instruct, surpassing GPT-4o (76.6%) and Claude 3.5 (71.1%).", 'score': 21, 'issue_id': 890, 'pub_date': '2024-11-27', 'pub_date_card': {'ru': '27 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 27', 'zh': '11æœˆ27æ—¥'}, 'hash': '05890d0739faa85c', 'authors': ['Jinyang Wu', 'Mingkuan Feng', 'Shuai Zhang', 'Feihu Che', 'Zengqi Wen', 'Jianhua Tao'], 'affiliations': ['Department of Automation, Tsinghua University', 'Beijing National Research Center for Information Science and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2411.18478.jpg', 'data': {'categories': ['#training', '#inference', '#reasoning', '#math'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'HiAR-ICL: ĞĞ±ÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': "HiAR-ICL - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ°Ñ… Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿ÑÑ‚ÑŒ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ. ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ÑÑ Ğ¿Ğ¾Ğ¸ÑĞº ĞœĞ¾Ğ½Ñ‚Ğµ-ĞšĞ°Ñ€Ğ»Ğ¾, HiAR-ICL ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ 'ĞºĞ°Ñ€Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ¼Ñ‹ÑĞ»ĞµĞ¹' Ğ´Ğ»Ñ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ° Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ HiAR-ICL Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ MATH, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ GPT-4 Ğ¸ Claude 3.5."}, 'en': {'title': 'Revolutionizing Mathematical Reasoning with HiAR-ICL', 'desc': "This paper introduces HiAR-ICL, a new approach to In-context Learning (ICL) that enhances large language models' ability to perform complex mathematical reasoning tasks. Traditional ICL relies heavily on the quality of examples and often requires human input, which can be limiting. HiAR-ICL shifts the focus from specific examples to abstract reasoning patterns, utilizing five atomic reasoning actions to create structured reasoning chains. The method employs Monte Carlo Tree Search to explore reasoning paths and develop a cognitive complexity framework that matches problems with suitable thought cards, achieving superior performance on the MATH benchmark."}, 'zh': {'title': 'é«˜å±‚æ¬¡è‡ªåŠ¨æ¨ç†ï¼šè¶…è¶Šä¼ ç»Ÿä¸Šä¸‹æ–‡å­¦ä¹ çš„å±€é™æ€§', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„é«˜å±‚æ¬¡è‡ªåŠ¨æ¨ç†èŒƒå¼HiAR-ICLï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿä¸Šä¸‹æ–‡å­¦ä¹ åœ¨å¤æ‚æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­çš„å±€é™æ€§ã€‚HiAR-ICLé€šè¿‡å¼•å…¥äº”ç§åŸºæœ¬æ¨ç†åŠ¨ä½œï¼Œè½¬å˜äº†å¯¹å…·ä½“ç¤ºä¾‹çš„ä¾èµ–ï¼Œå¼ºè°ƒæŠ½è±¡æ€ç»´æ¨¡å¼çš„é‡è¦æ€§ã€‚è¯¥æ–¹æ³•åˆ©ç”¨è’™ç‰¹å¡æ´›æ ‘æœç´¢æ¢ç´¢æ¨ç†è·¯å¾„ï¼Œå¹¶æ„å»ºæ€ç»´å¡ç‰‡ä»¥æŒ‡å¯¼åç»­æ¨ç†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒHiAR-ICLåœ¨MATHåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†79.6%çš„å‡†ç¡®ç‡ï¼Œè¶…è¶Šäº†å…¶ä»–å…ˆè¿›æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.19930', 'title': 'On Domain-Specific Post-Training for Multimodal Large Language Models', 'url': 'https://huggingface.co/papers/2411.19930', 'abstract': 'Recent years have witnessed the rapid development of general multimodal large language models (MLLMs). However, adapting general MLLMs to specific domains, such as scientific fields and industrial applications, remains less explored. This paper systematically investigates domain adaptation of MLLMs through post-training, focusing on data synthesis, training pipelines, and task evaluation. (1) Data Synthesis: Using open-source models, we develop a visual instruction synthesizer that effectively generates diverse visual instruction tasks from domain-specific image-caption pairs. Our synthetic tasks surpass those generated by manual rules, GPT-4, and GPT-4V in enhancing the domain-specific performance of MLLMs. (2) Training Pipeline: While the two-stage training--initially on image-caption pairs followed by visual instruction tasks--is commonly adopted for developing general MLLMs, we apply a single-stage training pipeline to enhance task diversity for domain-specific post-training. (3) Task Evaluation: We conduct experiments in two domains, biomedicine and food, by post-training MLLMs of different sources and scales (e.g., Qwen2-VL-2B, LLaVA-v1.6-8B, Llama-3.2-11B), and then evaluating MLLM performance on various domain-specific tasks. To support further research in MLLM domain adaptation, we will open-source our implementations.', 'score': 18, 'issue_id': 885, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 29', 'zh': '11æœˆ29æ—¥'}, 'hash': '5d14749b38f15e60', 'authors': ['Daixuan Cheng', 'Shaohan Huang', 'Ziyu Zhu', 'Xintong Zhang', 'Wayne Xin Zhao', 'Zhongzhi Luan', 'Bo Dai', 'Zhenliang Zhang'], 'affiliations': ['Beihang University', 'Beijing Institute of Technology', 'Renmin University of China', 'State Key Laboratory of General Artificial Intelligence, BIGAI', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2411.19930.jpg', 'data': {'categories': ['#transfer_learning', '#dataset', '#training', '#open_source', '#multimodal', '#synthetic'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'ĞĞ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜ Ğº ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑĞ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğº ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ğ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ°Ñ‚Ğ¾Ñ€ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ€Ñ‹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ-Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ½Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ»Ğ¸ Ğ¾Ğ´Ğ½Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ñ€Ğ¸ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ğ»Ğ¸ÑÑŒ Ğ² Ğ±Ğ¸Ğ¾Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ¹ Ğ¸ Ğ¿Ğ¸Ñ‰ĞµĞ²Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ… Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… MLLM, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°.'}, 'en': {'title': 'Enhancing MLLMs for Specific Domains through Innovative Adaptation Techniques', 'desc': 'This paper explores how to adapt general multimodal large language models (MLLMs) for specific fields like biomedicine and food. It introduces a method for data synthesis that creates diverse visual instruction tasks from image-caption pairs, outperforming previous methods. The authors propose a single-stage training pipeline to improve task diversity during post-training, rather than the usual two-stage approach. Finally, they evaluate the performance of various MLLMs on domain-specific tasks and plan to share their findings to aid future research in this area.'}, 'zh': {'title': 'æå‡å¤šæ¨¡æ€æ¨¡å‹çš„é¢†åŸŸé€‚åº”æ€§', 'desc': 'è¿‘å¹´æ¥ï¼Œé€šç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰è¿…é€Ÿå‘å±•ã€‚ç„¶è€Œï¼Œå°†é€šç”¨MLLMsé€‚åº”äºç‰¹å®šé¢†åŸŸï¼Œå¦‚ç§‘å­¦å’Œå·¥ä¸šåº”ç”¨ï¼Œä»ç„¶è¾ƒå°‘è¢«æ¢ç´¢ã€‚æœ¬æ–‡ç³»ç»Ÿç ”ç©¶äº†MLLMsçš„é¢†åŸŸé€‚åº”æ€§ï¼Œé‡ç‚¹åœ¨äºæ•°æ®åˆæˆã€è®­ç»ƒæµç¨‹å’Œä»»åŠ¡è¯„ä¼°ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ç§è§†è§‰æŒ‡ä»¤åˆæˆå™¨ï¼Œèƒ½å¤Ÿæœ‰æ•ˆç”Ÿæˆå¤šæ ·åŒ–çš„è§†è§‰æŒ‡ä»¤ä»»åŠ¡ï¼Œä»è€Œæå‡MLLMsåœ¨ç‰¹å®šé¢†åŸŸçš„è¡¨ç°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.19189', 'title': 'Video Depth without Video Models', 'url': 'https://huggingface.co/papers/2411.19189', 'abstract': 'Video depth estimation lifts monocular video clips to 3D by inferring dense depth at every frame. Recent advances in single-image depth estimation, brought about by the rise of large foundation models and the use of synthetic training data, have fueled a renewed interest in video depth. However, naively applying a single-image depth estimator to every frame of a video disregards temporal continuity, which not only leads to flickering but may also break when camera motion causes sudden changes in depth range. An obvious and principled solution would be to build on top of video foundation models, but these come with their own limitations; including expensive training and inference, imperfect 3D consistency, and stitching routines for the fixed-length (short) outputs. We take a step back and demonstrate how to turn a single-image latent diffusion model (LDM) into a state-of-the-art video depth estimator. Our model, which we call RollingDepth, has two main ingredients: (i) a multi-frame depth estimator that is derived from a single-image LDM and maps very short video snippets (typically frame triplets) to depth snippets. (ii) a robust, optimization-based registration algorithm that optimally assembles depth snippets sampled at various different frame rates back into a consistent video. RollingDepth is able to efficiently handle long videos with hundreds of frames and delivers more accurate depth videos than both dedicated video depth estimators and high-performing single-frame models. Project page: rollingdepth.github.io.', 'score': 16, 'issue_id': 889, 'pub_date': '2024-11-28', 'pub_date_card': {'ru': '28 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 28', 'zh': '11æœˆ28æ—¥'}, 'hash': '1fc611a9a44595a1', 'authors': ['Bingxin Ke', 'Dominik Narnhofer', 'Shengyu Huang', 'Lei Ke', 'Torben Peters', 'Katerina Fragkiadaki', 'Anton Obukhov', 'Konrad Schindler'], 'affiliations': ['Carnegie Mellon University', 'ETH Zurich'], 'pdf_title_img': 'assets/pdf/title_img/2411.19189.jpg', 'data': {'categories': ['#3d', '#diffusion', '#video', '#optimization'], 'emoji': 'ğŸ¥', 'ru': {'title': 'RollingDepth: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ LDM', 'desc': 'Ğ”Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ RollingDepth. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ½Ğ° Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LDM) Ğ´Ğ»Ñ Ğ¾Ğ´Ğ¸Ğ½Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´Ğ²Ğ° ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°: Ğ¾Ñ†ĞµĞ½Ñ‰Ğ¸Ğº Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ´Ğ»Ñ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ñ€ĞµĞ³Ğ¸ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑĞ±Ğ¾Ñ€ĞºĞ¸ Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ñ†ĞµĞ»Ğ¾ÑÑ‚Ğ½Ğ¾Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾. RollingDepth ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ°Ğº ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾Ñ†ĞµĞ½Ñ‰Ğ¸ĞºĞ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ²Ğ¸Ğ´ĞµĞ¾, Ñ‚Ğ°Ğº Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ². Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ¸Ğ°Ğ¿Ğ°Ğ·Ğ¾Ğ½Ğ° Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ¿Ñ€Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ğ¼ĞµÑ€Ñ‹.'}, 'en': {'title': 'Transforming Monocular Videos into Accurate 3D Depth with RollingDepth', 'desc': 'This paper presents RollingDepth, a novel approach for estimating depth in videos by leveraging a single-image latent diffusion model (LDM). The method addresses the challenge of temporal continuity in video depth estimation, which often leads to flickering and inconsistencies when using traditional single-image models. RollingDepth utilizes a multi-frame depth estimator to analyze short video snippets and a robust registration algorithm to combine these depth estimates into a coherent video output. The results show that RollingDepth outperforms existing video depth estimators and single-frame models, providing accurate depth information for long video sequences.'}, 'zh': {'title': 'å°†å•å›¾åƒæ·±åº¦ä¼°è®¡æå‡ä¸ºè§†é¢‘æ·±åº¦ä¼°è®¡çš„åˆ›æ–°ä¹‹è·¯', 'desc': 'è§†é¢‘æ·±åº¦ä¼°è®¡é€šè¿‡æ¨æ–­æ¯å¸§çš„å¯†é›†æ·±åº¦ï¼Œå°†å•ç›®è§†é¢‘ç‰‡æ®µæå‡ä¸º3Dã€‚æœ€è¿‘ï¼Œå•å›¾åƒæ·±åº¦ä¼°è®¡çš„è¿›å±•æ¿€å‘äº†å¯¹è§†é¢‘æ·±åº¦çš„å…³æ³¨ï¼Œä½†ç®€å•åœ°å°†å•å›¾åƒæ·±åº¦ä¼°è®¡å™¨åº”ç”¨äºæ¯å¸§ä¼šå¿½ç•¥æ—¶é—´è¿ç»­æ€§ï¼Œå¯¼è‡´é—ªçƒå’Œæ·±åº¦èŒƒå›´çš„çªç„¶å˜åŒ–ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºRollingDepthçš„æ¨¡å‹ï¼Œå®ƒç»“åˆäº†å¤šå¸§æ·±åº¦ä¼°è®¡å’Œä¼˜åŒ–çš„æ³¨å†Œç®—æ³•ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå¤„ç†é•¿è§†é¢‘å¹¶æä¾›æ›´å‡†ç¡®çš„æ·±åº¦è§†é¢‘ã€‚è¯¥æ¨¡å‹åœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†ä¸“ç”¨è§†é¢‘æ·±åº¦ä¼°è®¡å™¨å’Œé«˜æ€§èƒ½å•å¸§æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.19146', 'title': 'Puzzle: Distillation-Based NAS for Inference-Optimized LLMs', 'url': 'https://huggingface.co/papers/2411.19146', 'abstract': "Large language models (LLMs) have demonstrated remarkable capabilities, but their adoption is limited by high computational costs during inference. While increasing parameter counts enhances accuracy, it also widens the gap between state-of-the-art capabilities and practical deployability. We present Puzzle, a framework to accelerate LLM inference on specific hardware while preserving their capabilities. Through an innovative application of neural architecture search (NAS) at an unprecedented scale, Puzzle systematically optimizes models with tens of billions of parameters under hardware constraints. Our approach utilizes blockwise local knowledge distillation (BLD) for parallel architecture exploration and employs mixed-integer programming for precise constraint optimization.   We demonstrate the real-world impact of our framework through Llama-3.1-Nemotron-51B-Instruct (Nemotron-51B), a publicly available model derived from Llama-3.1-70B-Instruct. Nemotron-51B achieves a 2.17x inference throughput speedup, fitting on a single NVIDIA H100 GPU while preserving 98.4% of the original model's capabilities. Nemotron-51B currently stands as the most accurate language model capable of inference on a single GPU with large batch sizes. Remarkably, this transformation required just 45B training tokens, compared to over 15T tokens used for the 70B model it was derived from. This establishes a new paradigm where powerful models can be optimized for efficient deployment with only negligible compromise of their capabilities, demonstrating that inference performance, not parameter count alone, should guide model selection. With the release of Nemotron-51B and the presentation of the Puzzle framework, we provide practitioners immediate access to state-of-the-art language modeling capabilities at significantly reduced computational costs.", 'score': 9, 'issue_id': 886, 'pub_date': '2024-11-28', 'pub_date_card': {'ru': '28 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 28', 'zh': '11æœˆ28æ—¥'}, 'hash': 'b33bb17742a81e99', 'authors': ['Akhiad Bercovich', 'Tomer Ronen', 'Talor Abramovich', 'Nir Ailon', 'Nave Assaf', 'Mohammad Dabbah', 'Ido Galil', 'Amnon Geifman', 'Yonatan Geifman', 'Izhak Golan', 'Netanel Haber', 'Ehud Karpas', 'Itay Levy', 'Shahar Mor', 'Zach Moshe', 'Najeeb Nabwani', 'Omri Puny', 'Ran Rubin', 'Itamar Schen', 'Ido Shahaf', 'Oren Tropp', 'Omer Ullman Argov', 'Ran Zilberstein', 'Ran El-Yaniv'], 'affiliations': ['NVIDIA'], 'pdf_title_img': 'assets/pdf/title_img/2411.19146.jpg', 'data': {'categories': ['#architecture', '#optimization', '#training', '#inference'], 'emoji': 'ğŸ§©', 'ru': {'title': 'Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ LLM Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ˜Ğ˜', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Puzzle Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ½Ğ° ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¼ Ğ¾Ğ±Ğ¾Ñ€ÑƒĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¸Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº (NAS) Ğ¸ Ğ±Ğ»Ğ¾Ñ‡Ğ½ÑƒÑ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ (BLD), Puzzle Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ´ĞµÑÑÑ‚ĞºĞ°Ğ¼Ğ¸ Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¿Ğ¾Ğ´ Ğ°Ğ¿Ğ¿Ğ°Ñ€Ğ°Ñ‚Ğ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ. ĞĞ° Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Nemotron-51B, Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ· Llama-3.1-70B-Instruct, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ 2.17-ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğµ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ 98.4% Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ°, Ğ° Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ´Ğ¾Ğ»Ğ¶Ğ½Ğ° Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑ‚ÑŒ Ğ²Ñ‹Ğ±Ğ¾Ñ€ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Optimizing Large Language Models for Efficient Inference', 'desc': "This paper introduces Puzzle, a framework designed to enhance the inference speed of large language models (LLMs) while maintaining their performance. It employs neural architecture search (NAS) to optimize models with billions of parameters specifically for certain hardware, addressing the challenge of high computational costs. The framework utilizes blockwise local knowledge distillation (BLD) and mixed-integer programming to efficiently explore and optimize model architectures. The resulting model, Nemotron-51B, achieves a significant speedup in inference on a single GPU while retaining most of the original model's capabilities, showcasing a new approach to deploying powerful LLMs more efficiently."}, 'zh': {'title': 'é«˜æ•ˆæ¨ç†ï¼Œå¼ºå¤§æ¨¡å‹çš„æ–°èŒƒå¼', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†é«˜è®¡ç®—æˆæœ¬é™åˆ¶äº†å®ƒä»¬çš„åº”ç”¨ã€‚æˆ‘ä»¬æå‡ºäº†Puzzleæ¡†æ¶ï¼Œé€šè¿‡ç¥ç»æ¶æ„æœç´¢ï¼ˆNASï¼‰åœ¨ç‰¹å®šç¡¬ä»¶ä¸ŠåŠ é€ŸLLMæ¨ç†ï¼ŒåŒæ—¶ä¿æŒå…¶èƒ½åŠ›ã€‚è¯¥æ¡†æ¶åˆ©ç”¨å—çŠ¶å±€éƒ¨çŸ¥è¯†è’¸é¦ï¼ˆBLDï¼‰è¿›è¡Œå¹¶è¡Œæ¶æ„æ¢ç´¢ï¼Œå¹¶é‡‡ç”¨æ··åˆæ•´æ•°è§„åˆ’è¿›è¡Œç²¾ç¡®çº¦æŸä¼˜åŒ–ã€‚é€šè¿‡Nemotron-51Bæ¨¡å‹ï¼Œæˆ‘ä»¬å±•ç¤ºäº†åœ¨å•ä¸ªNVIDIA H100 GPUä¸Šå®ç°2.17å€æ¨ç†ååé‡æå‡çš„å®é™…æ•ˆæœï¼ŒåŒæ—¶ä¿ç•™äº†98.4%çš„åŸå§‹æ¨¡å‹èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.19108', 'title': "Timestep Embedding Tells: It's Time to Cache for Video Diffusion Model", 'url': 'https://huggingface.co/papers/2411.19108', 'abstract': 'As a fundamental backbone for video generation, diffusion models are challenged by low inference speed due to the sequential nature of denoising. Previous methods speed up the models by caching and reusing model outputs at uniformly selected timesteps. However, such a strategy neglects the fact that differences among model outputs are not uniform across timesteps, which hinders selecting the appropriate model outputs to cache, leading to a poor balance between inference efficiency and visual quality. In this study, we introduce Timestep Embedding Aware Cache (TeaCache), a training-free caching approach that estimates and leverages the fluctuating differences among model outputs across timesteps. Rather than directly using the time-consuming model outputs, TeaCache focuses on model inputs, which have a strong correlation with the modeloutputs while incurring negligible computational cost. TeaCache first modulates the noisy inputs using the timestep embeddings to ensure their differences better approximating those of model outputs. TeaCache then introduces a rescaling strategy to refine the estimated differences and utilizes them to indicate output caching. Experiments show that TeaCache achieves up to 4.41x acceleration over Open-Sora-Plan with negligible (-0.07% Vbench score) degradation of visual quality.', 'score': 9, 'issue_id': 886, 'pub_date': '2024-11-28', 'pub_date_card': {'ru': '28 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 28', 'zh': '11æœˆ28æ—¥'}, 'hash': '02a6c2edf156e9d3', 'authors': ['Feng Liu', 'Shiwei Zhang', 'Xiaofeng Wang', 'Yujie Wei', 'Haonan Qiu', 'Yuzhong Zhao', 'Yingya Zhang', 'Qixiang Ye', 'Fang Wan'], 'affiliations': ['Alibaba Group', 'Fudan University', 'Institute of Automation, Chinese Academy of Sciences', 'Nanyang Technological University', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2411.19108.jpg', 'data': {'categories': ['#diffusion', '#optimization', '#video', '#inference'], 'emoji': 'â±ï¸', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ğ¾Ğµ ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ TeaCache. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ°Ñ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ². TeaCache Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ Ğ¾Ñ†ĞµĞ½Ğ¾Ğº Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ğ¹ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ñ… Ğ´Ğ»Ñ ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ TeaCache Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ´Ğ¾ 4,41 Ñ€Ğ°Ğ·Ğ° Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Open-Sora-Plan Ğ¿Ñ€Ğ¸ Ğ½ĞµĞ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'Accelerating Video Generation with Smart Caching', 'desc': 'This paper presents TeaCache, a novel caching method designed to enhance the efficiency of diffusion models in video generation. Traditional approaches cache model outputs at fixed timesteps, which can lead to suboptimal performance due to the uneven differences in outputs across timesteps. TeaCache addresses this by focusing on modulating noisy inputs with timestep embeddings, allowing for a more accurate estimation of output differences without the heavy computational cost. The results demonstrate that TeaCache significantly accelerates inference speed while maintaining high visual quality, achieving a notable performance improvement over existing methods.'}, 'zh': {'title': 'æå‡è§†é¢‘ç”Ÿæˆé€Ÿåº¦çš„æ–°æ–¹æ³•ï¼šTeaCache', 'desc': 'æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„ç¼“å­˜æ–¹æ³•ï¼Œç§°ä¸ºæ—¶é—´æ­¥åµŒå…¥æ„ŸçŸ¥ç¼“å­˜ï¼ˆTeaCacheï¼‰ï¼Œæ—¨åœ¨æé«˜è§†é¢‘ç”Ÿæˆä¸­çš„æ‰©æ•£æ¨¡å‹çš„æ¨ç†é€Ÿåº¦ã€‚ä¼ ç»Ÿæ–¹æ³•é€šè¿‡åœ¨å‡åŒ€é€‰æ‹©çš„æ—¶é—´æ­¥ç¼“å­˜æ¨¡å‹è¾“å‡ºï¼Œä½†å¿½ç•¥äº†ä¸åŒæ—¶é—´æ­¥ä¹‹é—´è¾“å‡ºå·®å¼‚çš„ä¸å‡åŒ€æ€§ã€‚TeaCacheé€šè¿‡è°ƒèŠ‚å™ªå£°è¾“å…¥ï¼Œåˆ©ç”¨æ—¶é—´æ­¥åµŒå…¥æ¥æ›´å¥½åœ°è¿‘ä¼¼æ¨¡å‹è¾“å‡ºçš„å·®å¼‚ï¼Œä»è€Œä¼˜åŒ–ç¼“å­˜é€‰æ‹©ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTeaCacheåœ¨ä¿æŒè§†è§‰è´¨é‡çš„åŒæ—¶ï¼Œæ¨ç†é€Ÿåº¦æé«˜äº†4.41å€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.19324', 'title': 'Trajectory Attention for Fine-grained Video Motion Control', 'url': 'https://huggingface.co/papers/2411.19324', 'abstract': 'Recent advancements in video generation have been greatly driven by video diffusion models, with camera motion control emerging as a crucial challenge in creating view-customized visual content. This paper introduces trajectory attention, a novel approach that performs attention along available pixel trajectories for fine-grained camera motion control. Unlike existing methods that often yield imprecise outputs or neglect temporal correlations, our approach possesses a stronger inductive bias that seamlessly injects trajectory information into the video generation process. Importantly, our approach models trajectory attention as an auxiliary branch alongside traditional temporal attention. This design enables the original temporal attention and the trajectory attention to work in synergy, ensuring both precise motion control and new content generation capability, which is critical when the trajectory is only partially available. Experiments on camera motion control for images and videos demonstrate significant improvements in precision and long-range consistency while maintaining high-quality generation. Furthermore, we show that our approach can be extended to other video motion control tasks, such as first-frame-guided video editing, where it excels in maintaining content consistency over large spatial and temporal ranges.', 'score': 9, 'issue_id': 885, 'pub_date': '2024-11-28', 'pub_date_card': {'ru': '28 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 28', 'zh': '11æœˆ28æ—¥'}, 'hash': '02a266f597ae69e7', 'authors': ['Zeqi Xiao', 'Wenqi Ouyang', 'Yifan Zhou', 'Shuai Yang', 'Lei Yang', 'Jianlou Si', 'Xingang Pan'], 'affiliations': ['S-Lab, Nanyang Technological University', 'Sensetime Research', 'Wangxuan Institute of Computer Technology, Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2411.19324.jpg', 'data': {'categories': ['#diffusion', '#video'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Ğ¢Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ trajectory attention', 'desc': "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ 'trajectory attention' Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ´Ğ¾Ğ»ÑŒ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ²Ğ½ĞµĞ´Ñ€ÑÑ‚ÑŒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. Trajectory attention Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ ĞºĞ°Ğº Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ²ĞµÑ‚Ğ²ÑŒ Ğ½Ğ°Ñ€ÑĞ´Ñƒ Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ ĞºĞ°Ğº Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ, Ñ‚Ğ°Ğº Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ² Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ñ€Ğ°ÑÑÑ‚Ğ¾ÑĞ½Ğ¸ÑÑ… Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸."}, 'en': {'title': 'Enhancing Video Generation with Trajectory Attention', 'desc': 'This paper presents a new method called trajectory attention for improving video generation, particularly in controlling camera motion. It enhances the traditional temporal attention mechanism by incorporating pixel trajectories, allowing for more precise and consistent video outputs. The method effectively combines trajectory information with temporal attention, addressing challenges in generating view-customized content. Experiments show that this approach not only improves motion control but also maintains high-quality video generation across various tasks.'}, 'zh': {'title': 'è½¨è¿¹æ³¨æ„åŠ›ï¼šç²¾ç¡®æ§åˆ¶è§†é¢‘ç”Ÿæˆä¸­çš„ç›¸æœºè¿åŠ¨', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°é¢–çš„è½¨è¿¹æ³¨æ„åŠ›æœºåˆ¶ï¼Œç”¨äºè§†é¢‘ç”Ÿæˆä¸­çš„ç›¸æœºè¿åŠ¨æ§åˆ¶ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿæ›´ç²¾ç¡®åœ°å¤„ç†è¿åŠ¨æ§åˆ¶ï¼Œå¹¶æœ‰æ•ˆåœ°ç»“åˆäº†è½¨è¿¹ä¿¡æ¯ã€‚é€šè¿‡å°†è½¨è¿¹æ³¨æ„åŠ›ä½œä¸ºè¾…åŠ©åˆ†æ”¯ä¸ä¼ ç»Ÿæ—¶é—´æ³¨æ„åŠ›ç»“åˆï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç”Ÿæˆæ–°å†…å®¹çš„åŒæ—¶ï¼Œç¡®ä¿äº†è¿åŠ¨æ§åˆ¶çš„ç²¾ç¡®æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å›¾åƒå’Œè§†é¢‘çš„ç›¸æœºè¿åŠ¨æ§åˆ¶ä¸­æ˜¾è‘—æé«˜äº†ç²¾åº¦å’Œé•¿è·ç¦»ä¸€è‡´æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.18552', 'title': 'FAM Diffusion: Frequency and Attention Modulation for High-Resolution Image Generation with Stable Diffusion', 'url': 'https://huggingface.co/papers/2411.18552', 'abstract': 'Diffusion models are proficient at generating high-quality images. They are however effective only when operating at the resolution used during training. Inference at a scaled resolution leads to repetitive patterns and structural distortions. Retraining at higher resolutions quickly becomes prohibitive. Thus, methods enabling pre-existing diffusion models to operate at flexible test-time resolutions are highly desirable. Previous works suffer from frequent artifacts and often introduce large latency overheads. We propose two simple modules that combine to solve these issues. We introduce a Frequency Modulation (FM) module that leverages the Fourier domain to improve the global structure consistency, and an Attention Modulation (AM) module which improves the consistency of local texture patterns, a problem largely ignored in prior works. Our method, coined Fam diffusion, can seamlessly integrate into any latent diffusion model and requires no additional training. Extensive qualitative results highlight the effectiveness of our method in addressing structural and local artifacts, while quantitative results show state-of-the-art performance. Also, our method avoids redundant inference tricks for improved consistency such as patch-based or progressive generation, leading to negligible latency overheads.', 'score': 8, 'issue_id': 892, 'pub_date': '2024-11-27', 'pub_date_card': {'ru': '27 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 27', 'zh': '11æœˆ27æ—¥'}, 'hash': 'dd1bf99b66f1b34d', 'authors': ['Haosen Yang', 'Adrian Bulat', 'Isma Hadji', 'Hai X. Pham', 'Xiatian Zhu', 'Georgios Tzimiropoulos', 'Brais Martinez'], 'affiliations': ['Queen Mary University, UK', 'Samsung AI Center, Cambridge, UK', 'University of Surrey, UK'], 'pdf_title_img': 'assets/pdf/title_img/2411.18552.jpg', 'data': {'categories': ['#cv', '#optimization', '#diffusion', '#inference'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'Ğ“Ğ¸Ğ±ĞºĞ¾Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Fam diffusion Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ²Ğ° Ğ¼Ğ¾Ğ´ÑƒĞ»Ñ: Frequency Modulation (FM) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Attention Modulation (AM) Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚ÑƒÑ€Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ»ĞµĞ³ĞºĞ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ² Ğ»ÑĞ±ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ¸ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ² ÑƒÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ñ… Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ğ¾Ğ², Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ»ÑƒÑ‡ÑˆÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Flexible Image Generation with Fam Diffusion', 'desc': 'This paper presents a novel approach to enhance diffusion models for image generation, allowing them to work effectively at various resolutions without retraining. The proposed Fam diffusion method introduces two key modules: Frequency Modulation (FM) for improving global structure consistency and Attention Modulation (AM) for refining local texture patterns. These modules address common issues like repetitive patterns and structural distortions that occur when using scaled resolutions. The method integrates seamlessly into existing latent diffusion models, demonstrating state-of-the-art performance with minimal latency overheads and improved image quality.'}, 'zh': {'title': 'çµæ´»åˆ†è¾¨ç‡ä¸‹çš„é«˜è´¨é‡å›¾åƒç”Ÿæˆ', 'desc': 'æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆé«˜è´¨é‡å›¾åƒæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†ä»…åœ¨è®­ç»ƒæ—¶ä½¿ç”¨çš„åˆ†è¾¨ç‡ä¸‹æœ‰æ•ˆã€‚åœ¨ä¸åŒçš„åˆ†è¾¨ç‡ä¸‹æ¨ç†ä¼šå¯¼è‡´é‡å¤æ¨¡å¼å’Œç»“æ„å¤±çœŸã€‚æˆ‘ä»¬æå‡ºäº†ä¸¤ä¸ªç®€å•çš„æ¨¡å—ï¼Œé¢‘ç‡è°ƒåˆ¶ï¼ˆFMï¼‰æ¨¡å—å’Œæ³¨æ„åŠ›è°ƒåˆ¶ï¼ˆAMï¼‰æ¨¡å—ï¼Œæ¥è§£å†³è¿™äº›é—®é¢˜ã€‚æˆ‘ä»¬çš„Famæ‰©æ•£æ–¹æ³•å¯ä»¥æ— ç¼é›†æˆåˆ°ä»»ä½•æ½œåœ¨æ‰©æ•£æ¨¡å‹ä¸­ï¼Œæ— éœ€é¢å¤–è®­ç»ƒï¼Œå¹¶ä¸”åœ¨å¤„ç†ç»“æ„å’Œå±€éƒ¨ä¼ªå½±æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.19527', 'title': 'DisCoRD: Discrete Tokens to Continuous Motion via Rectified Flow Decoding', 'url': 'https://huggingface.co/papers/2411.19527', 'abstract': 'Human motion, inherently continuous and dynamic, presents significant challenges for generative models. Despite their dominance, discrete quantization methods, such as VQ-VAEs, suffer from inherent limitations, including restricted expressiveness and frame-wise noise artifacts. Continuous approaches, while producing smoother and more natural motions, often falter due to high-dimensional complexity and limited training data. To resolve this "discord" between discrete and continuous representations, we introduce DisCoRD: Discrete Tokens to Continuous Motion via Rectified Flow Decoding, a novel method that decodes discrete motion tokens into continuous motion through rectified flow. By employing an iterative refinement process in the continuous space, DisCoRD captures fine-grained dynamics and ensures smoother and more natural motions. Compatible with any discrete-based framework, our method enhances naturalness without compromising faithfulness to the conditioning signals. Extensive evaluations demonstrate that DisCoRD achieves state-of-the-art performance, with FID of 0.032 on HumanML3D and 0.169 on KIT-ML. These results solidify DisCoRD as a robust solution for bridging the divide between discrete efficiency and continuous realism. Our project page is available at: https://whwjdqls.github.io/discord.github.io/.', 'score': 8, 'issue_id': 891, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 29', 'zh': '11æœˆ29æ—¥'}, 'hash': 'b1fc0d8f7ba13620', 'authors': ['Jungbin Cho', 'Junwan Kim', 'Jisoo Kim', 'Minseo Kim', 'Mingu Kang', 'Sungeun Hong', 'Tae-Hyun Oh', 'Youngjae Yu'], 'affiliations': ['POSTECH', 'Sungkyunkwan University', 'Yonsei University'], 'pdf_title_img': 'assets/pdf/title_img/2411.19527.jpg', 'data': {'categories': ['#video', '#dataset', '#training'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'DisCoRD: ĞœĞ¾ÑÑ‚ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğ¼ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ¼Ğ¾Ğ¼ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ DisCoRD Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ¸ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ¸ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğµ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ñ‹Ğ¿Ñ€ÑĞ¼Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ñ‚Ğ¾Ğº. DisCoRD Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ğµ Ğ² Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ´Ğ»Ñ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° Ñ‚Ğ¾Ğ½ĞºĞ¸Ñ… Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸Ğº Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞµ FID Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ… HumanML3D Ğ¸ KIT-ML.'}, 'en': {'title': 'Bridging the Gap: DisCoRD for Smooth Human Motion Generation', 'desc': 'This paper addresses the challenges of generating human motion using machine learning models, particularly the limitations of discrete quantization methods like VQ-VAEs. It introduces a new method called DisCoRD, which stands for Discrete Tokens to Continuous Motion via Rectified Flow Decoding. DisCoRD effectively converts discrete motion tokens into smooth continuous motion by using an iterative refinement process in the continuous space. The results show that DisCoRD outperforms existing methods, achieving state-of-the-art performance metrics on benchmark datasets, thus providing a solution that balances discrete efficiency with continuous realism.'}, 'zh': {'title': 'æ‰“ç ´ç¦»æ•£ä¸è¿ç»­çš„ç•Œé™ï¼Œæå‡è¿åŠ¨ç”Ÿæˆè‡ªç„¶æ€§', 'desc': 'äººç±»è¿åŠ¨æ˜¯è¿ç»­å’ŒåŠ¨æ€çš„ï¼Œè¿™ç»™ç”Ÿæˆæ¨¡å‹å¸¦æ¥äº†å¾ˆå¤§æŒ‘æˆ˜ã€‚å°½ç®¡ç¦»æ•£é‡åŒ–æ–¹æ³•ï¼ˆå¦‚VQ-VAEsï¼‰å ä¸»å¯¼åœ°ä½ï¼Œä½†å®ƒä»¬åœ¨è¡¨è¾¾èƒ½åŠ›å’Œå¸§å™ªå£°æ–¹é¢å­˜åœ¨å±€é™ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•DisCoRDï¼Œé€šè¿‡ä¿®æ­£æµè§£ç å°†ç¦»æ•£è¿åŠ¨æ ‡è®°è§£ç ä¸ºè¿ç»­è¿åŠ¨ï¼Œè§£å†³äº†ç¦»æ•£å’Œè¿ç»­è¡¨ç¤ºä¹‹é—´çš„çŸ›ç›¾ã€‚DisCoRDåœ¨è¿ç»­ç©ºé—´ä¸­è¿›è¡Œè¿­ä»£ä¼˜åŒ–ï¼Œæ•æ‰ç»†å¾®åŠ¨æ€ï¼Œç¡®ä¿è¿åŠ¨æ›´åŠ å¹³æ»‘è‡ªç„¶ï¼Œä¸”ä¸ä»»ä½•åŸºäºç¦»æ•£çš„æ¡†æ¶å…¼å®¹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.19950', 'title': 'AlphaTablets: A Generic Plane Representation for 3D Planar Reconstruction from Monocular Videos', 'url': 'https://huggingface.co/papers/2411.19950', 'abstract': 'We introduce AlphaTablets, a novel and generic representation of 3D planes that features continuous 3D surface and precise boundary delineation. By representing 3D planes as rectangles with alpha channels, AlphaTablets combine the advantages of current 2D and 3D plane representations, enabling accurate, consistent and flexible modeling of 3D planes. We derive differentiable rasterization on top of AlphaTablets to efficiently render 3D planes into images, and propose a novel bottom-up pipeline for 3D planar reconstruction from monocular videos. Starting with 2D superpixels and geometric cues from pre-trained models, we initialize 3D planes as AlphaTablets and optimize them via differentiable rendering. An effective merging scheme is introduced to facilitate the growth and refinement of AlphaTablets. Through iterative optimization and merging, we reconstruct complete and accurate 3D planes with solid surfaces and clear boundaries. Extensive experiments on the ScanNet dataset demonstrate state-of-the-art performance in 3D planar reconstruction, underscoring the great potential of AlphaTablets as a generic 3D plane representation for various applications. Project page is available at: https://hyzcluster.github.io/alphatablets', 'score': 5, 'issue_id': 888, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 29', 'zh': '11æœˆ29æ—¥'}, 'hash': '9f7d2daec9cb311d', 'authors': ['Yuze He', 'Wang Zhao', 'Shaohui Liu', 'Yubin Hu', 'Yushi Bai', 'Yu-Hui Wen', 'Yong-Jin Liu'], 'affiliations': ['Beijing Jiaotong University', 'ETH Zurich', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2411.19950.jpg', 'data': {'categories': ['#3d'], 'emoji': 'ğŸ“', 'ru': {'title': 'AlphaTablets: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ 3D Ğ¿Ğ»Ğ¾ÑĞºĞ¾ÑÑ‚ĞµĞ¹', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ AlphaTablets - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ¿Ğ»Ğ¾ÑĞºĞ¾ÑÑ‚ĞµĞ¹ Ğ² Ğ²Ğ¸Ğ´Ğµ Ğ¿Ñ€ÑĞ¼Ğ¾ÑƒĞ³Ğ¾Ğ»ÑŒĞ½Ğ¸ĞºĞ¾Ğ² Ñ Ğ°Ğ»ÑŒÑ„Ğ°-ĞºĞ°Ğ½Ğ°Ğ»Ğ°Ğ¼Ğ¸. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ñ‡ĞµÑ‚Ğ°Ñ‚ÑŒ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… 2D Ğ¸ 3D Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ¸ Ğ³Ğ¸Ğ±ĞºĞ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ 3D Ğ¿Ğ»Ğ¾ÑĞºĞ¾ÑÑ‚ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ñ€ÑƒĞµĞ¼ÑƒÑ Ñ€Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ 3D Ğ¿Ğ»Ğ¾ÑĞºĞ¾ÑÑ‚ĞµĞ¹ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ 3D Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ»Ğ¾ÑĞºĞ¾ÑÑ‚ĞµĞ¹ Ğ¸Ğ· Ğ¼Ğ¾Ğ½Ğ¾ĞºÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ScanNet Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ 3D Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ»Ğ¾ÑĞºĞ¾ÑÑ‚ĞµĞ¹.'}, 'en': {'title': 'AlphaTablets: Revolutionizing 3D Plane Representation', 'desc': 'AlphaTablets is a new way to represent 3D planes that combines the benefits of both 2D and 3D models. It uses rectangles with alpha channels to create smooth surfaces and clear edges for accurate modeling. The paper introduces a method for rendering these planes efficiently and a pipeline for reconstructing 3D planes from single videos. By optimizing the representation through merging and iterative processes, AlphaTablets achieves high-quality 3D reconstructions, as shown in tests on the ScanNet dataset.'}, 'zh': {'title': 'AlphaTabletsï¼š3Då¹³é¢é‡å»ºçš„æ–°æ–¹æ³•', 'desc': 'æˆ‘ä»¬ä»‹ç»äº†AlphaTabletsï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–ä¸”é€šç”¨çš„3Då¹³é¢è¡¨ç¤ºæ–¹æ³•ï¼Œå…·æœ‰è¿ç»­çš„3Dè¡¨é¢å’Œç²¾ç¡®çš„è¾¹ç•Œåˆ’åˆ†ã€‚é€šè¿‡å°†3Då¹³é¢è¡¨ç¤ºä¸ºå¸¦æœ‰alphaé€šé“çš„çŸ©å½¢ï¼ŒAlphaTabletsç»“åˆäº†å½“å‰2Då’Œ3Då¹³é¢è¡¨ç¤ºçš„ä¼˜ç‚¹ï¼Œå®ç°äº†3Då¹³é¢çš„å‡†ç¡®ã€ä¸€è‡´å’Œçµæ´»å»ºæ¨¡ã€‚æˆ‘ä»¬åœ¨AlphaTabletsçš„åŸºç¡€ä¸Šæ¨å¯¼å‡ºå¯å¾®åˆ†å…‰æ …åŒ–æŠ€æœ¯ï¼Œä»¥é«˜æ•ˆåœ°å°†3Då¹³é¢æ¸²æŸ“ä¸ºå›¾åƒï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°é¢–çš„è‡ªä¸‹è€Œä¸Šçš„å•ç›®è§†é¢‘3Då¹³é¢é‡å»ºç®¡é“ã€‚é€šè¿‡è¿­ä»£ä¼˜åŒ–å’Œåˆå¹¶ï¼Œæˆ‘ä»¬èƒ½å¤Ÿé‡å»ºå‡ºå®Œæ•´ä¸”å‡†ç¡®çš„3Då¹³é¢ï¼Œå…·æœ‰åšå®çš„è¡¨é¢å’Œæ¸…æ™°çš„è¾¹ç•Œã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.19460', 'title': 'Look Every Frame All at Once: Video-Ma$^2$mba for Efficient Long-form Video Understanding with Multi-Axis Gradient Checkpointing', 'url': 'https://huggingface.co/papers/2411.19460', 'abstract': 'With the growing scale and complexity of video data, efficiently processing long video sequences poses significant challenges due to the quadratic increase in memory and computational demands associated with existing transformer-based Large Multi-modal Models (LMMs). To address these issues, we introduce Video-Ma^2mba, a novel architecture that incorporates State Space Models (SSMs) within the Mamba-2 framework, replacing the attention mechanisms. This allows the LMMs to scale linearly in terms of time and memory requirements, making it feasible to handle long-duration video content. Furthermore, we enhance the memory efficiency introducing the Multi-Axis Gradient Checkpointing (MA-GC) method, which strategically manages memory by retaining only essential activations across multiple computational axes. Our approach significantly reduces the memory footprint compared to standard gradient checkpointing. Empirical analyses show that Video-Ma^2mba can process extensive video sequences-equivalent to millions of tokens or over two hours of continuous sequences at 1 FPS-on a single GPU. By maintaining a detailed capture of temporal dynamics, our model improves the accuracy and relevance of responses in long video understanding tasks, demonstrating substantial advantages over existing frameworks.', 'score': 5, 'issue_id': 886, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 29', 'zh': '11æœˆ29æ—¥'}, 'hash': 'b96751a3db484750', 'authors': ['Hosu Lee', 'Junho Kim', 'Hyunjun Kim', 'Yong Man Ro'], 'affiliations': ['Integrated Vision and Language Lab, KAIST, South Korea'], 'pdf_title_img': 'assets/pdf/title_img/2411.19460.jpg', 'data': {'categories': ['#architecture', '#long_context', '#video', '#optimization'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Video-Ma^2mba', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Video-Ma^2mba - Ğ½Ğ¾Ğ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LMM) Ğ¿Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¸ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¾ÑĞµĞ²Ğ¾Ğ³Ğ¾ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ‡ĞµĞºĞ¿Ğ¾Ğ¹Ğ½Ñ‚Ğ¸Ğ½Ğ³Ğ° (MA-GC) Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Video-Ma^2mba Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¼ GPU, ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'Revolutionizing Long Video Processing with Linear Scalability', 'desc': 'The paper presents Video-Ma^2mba, a new architecture designed to efficiently process long video sequences by integrating State Space Models (SSMs) into the Mamba-2 framework. This innovative approach replaces traditional attention mechanisms, allowing the model to scale linearly in memory and computational requirements, which is crucial for handling extensive video data. Additionally, the introduction of Multi-Axis Gradient Checkpointing (MA-GC) optimizes memory usage by retaining only necessary activations, significantly reducing the memory footprint. Empirical results indicate that Video-Ma^2mba can effectively manage video sequences equivalent to millions of tokens, enhancing the accuracy of long video understanding tasks compared to existing models.'}, 'zh': {'title': 'é«˜æ•ˆå¤„ç†é•¿è§†é¢‘åºåˆ—çš„æ–°æ–¹æ³•', 'desc': 'éšç€è§†é¢‘æ•°æ®è§„æ¨¡å’Œå¤æ‚æ€§çš„å¢åŠ ï¼Œå¤„ç†é•¿è§†é¢‘åºåˆ—é¢ä¸´ç€æ˜¾è‘—çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ¶æ„Video-Ma^2mbaï¼Œå®ƒåœ¨Mamba-2æ¡†æ¶ä¸­å¼•å…¥äº†çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMsï¼‰ï¼Œæ›¿ä»£äº†ä¼ ç»Ÿçš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œä»è€Œä½¿å¾—å¤§è§„æ¨¡å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨æ—¶é—´å’Œå†…å­˜éœ€æ±‚ä¸Šå®ç°çº¿æ€§æ‰©å±•ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†å¤šè½´æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆMA-GCï¼‰æ–¹æ³•ï¼Œä¼˜åŒ–å†…å­˜ç®¡ç†ï¼Œä»…ä¿ç•™å¿…è¦çš„æ¿€æ´»ä¿¡æ¯ï¼Œæ˜¾è‘—é™ä½äº†å†…å­˜å ç”¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVideo-Ma^2mbaèƒ½å¤Ÿåœ¨å•ä¸ªGPUä¸Šå¤„ç†ç›¸å½“äºæ•°ç™¾ä¸‡ä¸ªæ ‡è®°æˆ–è¶…è¿‡ä¸¤å°æ—¶çš„è¿ç»­è§†é¢‘åºåˆ—ï¼Œæå‡äº†é•¿è§†é¢‘ç†è§£ä»»åŠ¡çš„å‡†ç¡®æ€§å’Œç›¸å…³æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.19865', 'title': 'Reverse Thinking Makes LLMs Stronger Reasoners', 'url': 'https://huggingface.co/papers/2411.19865', 'abstract': "Reverse thinking plays a crucial role in human reasoning. Humans can reason not only from a problem to a solution but also in reverse, i.e., start from the solution and reason towards the problem. This often enhances overall reasoning performance as it enables consistency checks between their forward and backward thinking. To enable Large Language Models (LLMs) to perform reverse thinking, we introduce Reverse-Enhanced Thinking (RevThink), a framework composed of data augmentation and learning objectives. In RevThink, we augment the dataset by collecting structured forward-backward reasoning from a teacher model, consisting of: (1) the original question, (2) forward reasoning, (3) backward question, and (4) backward reasoning. We then employ three objectives to train a smaller student model in a multi-task learning fashion: (a) generate forward reasoning from a question, (b) generate a backward question from a question, and (c) generate backward reasoning from the backward question. Experiments across 12 datasets covering commonsense, math, and logical reasoning show an average 13.53% improvement over the student model's zero-shot performance and a 6.84% improvement over the strongest knowledge distillation baselines. Moreover, our method demonstrates sample efficiency -- using only 10% of the correct forward reasoning from the training data, it outperforms a standard fine-tuning method trained on 10x more forward reasoning. RevThink also exhibits strong generalization to out-of-distribution held-out datasets.", 'score': 4, 'issue_id': 899, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 29', 'zh': '11æœˆ29æ—¥'}, 'hash': '8f066f57ddff0ae8', 'authors': ['Justin Chih-Yao Chen', 'Zifeng Wang', 'Hamid Palangi', 'Rujun Han', 'Sayna Ebrahimi', 'Long Le', 'Vincent Perot', 'Swaroop Mishra', 'Mohit Bansal', 'Chen-Yu Lee', 'Tomas Pfister'], 'affiliations': ['Google Cloud AI Research', 'Google DeepMind', 'UNC Chapel Hill'], 'pdf_title_img': 'assets/pdf/title_img/2411.19865.jpg', 'data': {'categories': ['#data', '#training', '#transfer_learning', '#small_models', '#dataset', '#reasoning'], 'emoji': 'ğŸ”„', 'ru': {'title': 'RevThink: Ğ£ÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ LLM Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ñ‹Ğ¼ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Reverse-Enhanced Thinking (RevThink) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). RevThink Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ, Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ÑÑ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€ÑĞ¼Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¾Ñ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸-ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ: Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€ÑĞ¼Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Empowering LLMs with Reverse Reasoning for Enhanced Performance', 'desc': 'This paper introduces Reverse-Enhanced Thinking (RevThink), a framework designed to improve Large Language Models (LLMs) by enabling them to perform reverse reasoning. RevThink enhances reasoning performance by augmenting datasets with structured forward and backward reasoning examples, allowing models to learn from both directions. The framework employs multi-task learning objectives to train a smaller student model, focusing on generating forward reasoning, backward questions, and backward reasoning. Experimental results show significant improvements in reasoning tasks, demonstrating the effectiveness and efficiency of RevThink in enhancing model performance with limited data.'}, 'zh': {'title': 'åå‘æ€ç»´ï¼šæå‡æ¨ç†èƒ½åŠ›çš„æ–°æ–¹æ³•', 'desc': 'åå‘æ€ç»´åœ¨äººçš„æ¨ç†ä¸­èµ·ç€é‡è¦ä½œç”¨ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºåå‘å¢å¼ºæ€ç»´ï¼ˆRevThinkï¼‰çš„æ¡†æ¶ï¼Œæ—¨åœ¨ä½¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰èƒ½å¤Ÿè¿›è¡Œåå‘æ¨ç†ã€‚è¯¥æ¡†æ¶é€šè¿‡æ•°æ®å¢å¼ºå’Œå­¦ä¹ ç›®æ ‡æ¥å®ç°ï¼Œæ”¶é›†ç»“æ„åŒ–çš„å‰å‘å’Œåå‘æ¨ç†æ•°æ®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRevThinkåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„æ¨ç†æ€§èƒ½ï¼Œå¹¶å±•ç¤ºäº†è‰¯å¥½çš„æ ·æœ¬æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.19638', 'title': 'LLM Teacher-Student Framework for Text Classification With No Manually Annotated Data: A Case Study in IPTC News Topic Classification', 'url': 'https://huggingface.co/papers/2411.19638', 'abstract': "With the ever-increasing number of news stories available online, classifying them by topic, regardless of the language they are written in, has become crucial for enhancing readers' access to relevant content. To address this challenge, we propose a teacher-student framework based on large language models (LLMs) for developing multilingual news classification models of reasonable size with no need for manual data annotation. The framework employs a Generative Pretrained Transformer (GPT) model as the teacher model to develop an IPTC Media Topic training dataset through automatic annotation of news articles in Slovenian, Croatian, Greek, and Catalan. The teacher model exhibits a high zero-shot performance on all four languages. Its agreement with human annotators is comparable to that between the human annotators themselves. To mitigate the computational limitations associated with the requirement of processing millions of texts daily, smaller BERT-like student models are fine-tuned on the GPT-annotated dataset. These student models achieve high performance comparable to the teacher model. Furthermore, we explore the impact of the training data size on the performance of the student models and investigate their monolingual, multilingual and zero-shot cross-lingual capabilities. The findings indicate that student models can achieve high performance with a relatively small number of training instances, and demonstrate strong zero-shot cross-lingual abilities. Finally, we publish the best-performing news topic classifier, enabling multilingual classification with the top-level categories of the IPTC Media Topic schema.", 'score': 4, 'issue_id': 889, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 29', 'zh': '11æœˆ29æ—¥'}, 'hash': '98bf5f113194343b', 'authors': ['Taja Kuzman', 'Nikola LjubeÅ¡iÄ‡'], 'affiliations': ['Department of Knowledge Technologies, JoÅ¾ef Stefan Institute, 1000 Ljubljana, Slovenia', 'JoÅ¾ef Stefan International Postgraduate School, 1000 Ljubljana, Slovenia', 'University of Ljubljana, 1000 Ljubljana, Slovenia'], 'pdf_title_img': 'assets/pdf/title_img/2411.19638.jpg', 'data': {'categories': ['#machine_translation', '#training', '#low_resource', '#multilingual', '#dataset'], 'emoji': 'ğŸ“°', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ°Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚ĞµĞ¹ Ğ±ĞµĞ· Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸', 'desc': "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ¹ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚ĞµĞ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ 'ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒ-ÑƒÑ‡ĞµĞ½Ğ¸Ğº'. Ğ‘Ğ¾Ğ»ÑŒÑˆĞ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ GPT Ğ²Ñ‹ÑÑ‚ÑƒĞ¿Ğ°ĞµÑ‚ Ğ² Ñ€Ğ¾Ğ»Ğ¸ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ, Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€ÑƒÑ Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚Ğ½Ñ‹Ğµ ÑÑ‚Ğ°Ñ‚ÑŒĞ¸ Ğ½Ğ° Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ… ÑĞ·Ñ‹ĞºĞ°Ñ…. ĞœĞ¾Ğ´ĞµĞ»Ğ¸-ÑƒÑ‡ĞµĞ½Ğ¸ĞºĞ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ BERT Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ÑÑ Ğ½Ğ° ÑÑ‚Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ¹ Ğ¸ ĞºÑ€Ğ¾ÑÑ-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚ĞµĞ¹."}, 'en': {'title': 'Empowering Multilingual News Classification with Teacher-Student LLMs', 'desc': 'This paper presents a teacher-student framework utilizing large language models (LLMs) for multilingual news classification without manual data annotation. The teacher model, a Generative Pretrained Transformer (GPT), automatically annotates news articles in multiple languages, achieving high zero-shot performance and agreement with human annotators. Smaller BERT-like student models are then fine-tuned on this annotated dataset, demonstrating comparable performance to the teacher model while being computationally efficient. The study also highlights the effectiveness of the student models in multilingual and zero-shot cross-lingual tasks, ultimately providing a robust news topic classifier for diverse languages.'}, 'zh': {'title': 'å¤šè¯­è¨€æ–°é—»åˆ†ç±»çš„æ–°æ–¹æ³•', 'desc': 'éšç€åœ¨çº¿æ–°é—»æ•°é‡çš„ä¸æ–­å¢åŠ ï¼ŒæŒ‰ä¸»é¢˜å¯¹æ–°é—»è¿›è¡Œåˆ†ç±»å˜å¾—è‡³å…³é‡è¦ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ•™å¸ˆ-å­¦ç”Ÿæ¡†æ¶ï¼Œç”¨äºå¼€å‘å¤šè¯­è¨€æ–°é—»åˆ†ç±»æ¨¡å‹ï¼Œä¸”æ— éœ€æ‰‹åŠ¨æ•°æ®æ ‡æ³¨ã€‚æ•™å¸ˆæ¨¡å‹ä½¿ç”¨ç”Ÿæˆé¢„è®­ç»ƒå˜æ¢å™¨ï¼ˆGPTï¼‰è‡ªåŠ¨æ ‡æ³¨æ–°é—»æ–‡ç« ï¼Œå±•ç¤ºå‡ºåœ¨å¤šç§è¯­è¨€ä¸Šçš„é«˜é›¶æ ·æœ¬æ€§èƒ½ã€‚é€šè¿‡å¾®è°ƒè¾ƒå°çš„BERTç±»å­¦ç”Ÿæ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹åœ¨ç›¸å¯¹è¾ƒå°‘çš„è®­ç»ƒå®ä¾‹ä¸‹ä¹Ÿèƒ½è¾¾åˆ°ä¸æ•™å¸ˆæ¨¡å‹ç›¸å½“çš„é«˜æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.18673', 'title': 'AC3D: Analyzing and Improving 3D Camera Control in Video Diffusion Transformers', 'url': 'https://huggingface.co/papers/2411.18673', 'abstract': 'Numerous works have recently integrated 3D camera control into foundational text-to-video models, but the resulting camera control is often imprecise, and video generation quality suffers. In this work, we analyze camera motion from a first principles perspective, uncovering insights that enable precise 3D camera manipulation without compromising synthesis quality. First, we determine that motion induced by camera movements in videos is low-frequency in nature. This motivates us to adjust train and test pose conditioning schedules, accelerating training convergence while improving visual and motion quality. Then, by probing the representations of an unconditional video diffusion transformer, we observe that they implicitly perform camera pose estimation under the hood, and only a sub-portion of their layers contain the camera information. This suggested us to limit the injection of camera conditioning to a subset of the architecture to prevent interference with other video features, leading to 4x reduction of training parameters, improved training speed and 10% higher visual quality. Finally, we complement the typical dataset for camera control learning with a curated dataset of 20K diverse dynamic videos with stationary cameras. This helps the model disambiguate the difference between camera and scene motion, and improves the dynamics of generated pose-conditioned videos. We compound these findings to design the Advanced 3D Camera Control (AC3D) architecture, the new state-of-the-art model for generative video modeling with camera control.', 'score': 4, 'issue_id': 886, 'pub_date': '2024-11-27', 'pub_date_card': {'ru': '27 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 27', 'zh': '11æœˆ27æ—¥'}, 'hash': '1ea35d3552a278a3', 'authors': ['Sherwin Bahmani', 'Ivan Skorokhodov', 'Guocheng Qian', 'Aliaksandr Siarohin', 'Willi Menapace', 'Andrea Tagliasacchi', 'David B. Lindell', 'Sergey Tulyakov'], 'affiliations': ['SFU', 'Snap Inc.', 'University of Toronto', 'Vector Institute'], 'pdf_title_img': 'assets/pdf/title_img/2411.18673.jpg', 'data': {'categories': ['#dataset', '#architecture', '#diffusion', '#games', '#3d', '#training', '#optimization', '#video'], 'emoji': 'ğŸ¥', 'ru': {'title': 'ĞŸÑ€ĞµÑ†Ğ¸Ğ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ 3D-ĞºĞ°Ğ¼ĞµÑ€Ğ¾Ğ¹ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ 3D-ĞºĞ°Ğ¼ĞµÑ€Ğ¾Ğ¹ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ñ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¾Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ñ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğ¹ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ² Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ»Ğ¾Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ¼ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Advanced 3D Camera Control (AC3D), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ĞºĞ°Ğ¼ĞµÑ€Ğ¾Ğ¹ Ğ±ĞµĞ· ÑƒÑ‰ĞµÑ€Ğ±Ğ° Ğ´Ğ»Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾. ĞœĞ¾Ğ´ĞµĞ»ÑŒ AC3D Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ ĞºĞ°Ğ¼ĞµÑ€Ğ¾Ğ¹.'}, 'en': {'title': 'Precision in 3D Camera Control for Enhanced Video Generation', 'desc': 'This paper presents a novel approach to improve 3D camera control in text-to-video models, addressing issues of imprecision and video quality. By analyzing camera motion, the authors discover that it is primarily low-frequency, which leads to adjustments in training and testing schedules that enhance convergence and visual fidelity. They also find that only certain layers of a video diffusion transformer contain relevant camera information, allowing for a more efficient architecture that reduces training parameters while boosting quality. Finally, the introduction of a curated dataset of dynamic videos aids the model in distinguishing between camera and scene motion, culminating in the development of the Advanced 3D Camera Control (AC3D) model, which sets a new standard in generative video modeling.'}, 'zh': {'title': 'å…ˆè¿›çš„3Dç›¸æœºæ§åˆ¶ï¼Œæå‡è§†é¢‘ç”Ÿæˆè´¨é‡', 'desc': 'æœ¬ç ”ç©¶åˆ†æäº†3Dç›¸æœºæ§åˆ¶åœ¨æ–‡æœ¬åˆ°è§†é¢‘æ¨¡å‹ä¸­çš„åº”ç”¨ï¼Œå‘ç°ç›¸æœºè¿åŠ¨å¯¹è§†é¢‘ç”Ÿæˆè´¨é‡æœ‰æ˜¾è‘—å½±å“ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„è®­ç»ƒå’Œæµ‹è¯•å§¿æ€è°ƒèŠ‚ç­–ç•¥ï¼Œä»¥æé«˜è®­ç»ƒæ”¶æ•›é€Ÿåº¦å’Œè§†é¢‘çš„è§†è§‰è´¨é‡ã€‚é€šè¿‡å¯¹æ— æ¡ä»¶è§†é¢‘æ‰©æ•£å˜æ¢å™¨çš„è¡¨ç¤ºè¿›è¡Œæ¢æµ‹ï¼Œæˆ‘ä»¬å‘ç°ç›¸æœºå§¿æ€ä¼°è®¡åœ¨æ¨¡å‹å†…éƒ¨éšå¼æ‰§è¡Œï¼Œå› æ­¤æˆ‘ä»¬é™åˆ¶äº†ç›¸æœºæ¡ä»¶çš„æ³¨å…¥ï¼Œä»¥å‡å°‘å¯¹å…¶ä»–è§†é¢‘ç‰¹å¾çš„å¹²æ‰°ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬è®¾è®¡äº†å…ˆè¿›çš„3Dç›¸æœºæ§åˆ¶æ¶æ„ï¼ˆAC3Dï¼‰ï¼Œæˆä¸ºå…·æœ‰ç›¸æœºæ§åˆ¶çš„ç”Ÿæˆè§†é¢‘å»ºæ¨¡çš„æ–°ä¸€ä»£æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.19842', 'title': 'Scaling Transformers for Low-Bitrate High-Quality Speech Coding', 'url': 'https://huggingface.co/papers/2411.19842', 'abstract': 'The tokenization of speech with neural audio codec models is a vital part of modern AI pipelines for the generation or understanding of speech, alone or in a multimodal context. Traditionally such tokenization models have concentrated on low parameter-count architectures using only components with strong inductive biases. In this work we show that by scaling a transformer architecture with large parameter count to this problem, and applying a flexible Finite Scalar Quantization (FSQ) based bottleneck, it is possible to reach state-of-the-art speech quality at extremely low bit-rates of 400 or 700 bits-per-second. The trained models strongly out-perform existing baselines in both objective and subjective tests.', 'score': 3, 'issue_id': 900, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 29', 'zh': '11æœˆ29æ—¥'}, 'hash': '23e49aedef71b878', 'authors': ['Julian D Parker', 'Anton Smirnov', 'Jordi Pons', 'CJ Carr', 'Zack Zukowski', 'Zach Evans', 'Xubo Liu'], 'affiliations': ['Stability AI'], 'pdf_title_img': 'assets/pdf/title_img/2411.19842.jpg', 'data': {'categories': ['#multimodal', '#architecture', '#audio', '#training'], 'emoji': 'ğŸ—£ï¸', 'ru': {'title': 'Ğ¢Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ñ‹ Ğ¿Ğ¾ĞºĞ¾Ñ€ÑÑÑ‚ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸', 'desc': 'Ğ”Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµÑ‡Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ°ÑƒĞ´Ğ¸Ğ¾ĞºĞ¾Ğ´ĞµĞºĞ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ³Ğ¸Ğ±ĞºĞ¾Ğ³Ğ¾ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ FSQ. Ğ­Ñ‚Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€ĞµÑ‡Ğ¸ Ğ¿Ñ€Ğ¸ ĞºÑ€Ğ°Ğ¹Ğ½Ğµ Ğ½Ğ¸Ğ·ĞºĞ¸Ñ… Ğ±Ğ¸Ñ‚Ñ€ĞµĞ¹Ñ‚Ğ°Ñ… Ğ² 400-700 Ğ±Ğ¸Ñ‚/Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ°Ğº Ğ² Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ² ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ….'}, 'en': {'title': 'Transforming Speech Quality with Scalable Neural Models', 'desc': 'This paper discusses the importance of tokenizing speech using neural audio codec models in AI systems. It highlights a new approach that utilizes a large-scale transformer architecture combined with Finite Scalar Quantization (FSQ) to improve speech quality. The proposed method achieves impressive results, delivering high-quality speech at very low bit-rates of 400 or 700 bits-per-second. The models developed in this study significantly outperform existing methods in both objective measures and subjective evaluations.'}, 'zh': {'title': 'é€šè¿‡æ‰©å±•å˜æ¢å™¨å®ç°é«˜è´¨é‡ä½æ¯”ç‰¹ç‡è¯­éŸ³æ ‡è®°åŒ–', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†ä½¿ç”¨ç¥ç»éŸ³é¢‘ç¼–è§£ç æ¨¡å‹å¯¹è¯­éŸ³è¿›è¡Œæ ‡è®°åŒ–çš„é‡è¦æ€§ã€‚ä¼ ç»Ÿçš„æ ‡è®°åŒ–æ¨¡å‹é€šå¸¸é‡‡ç”¨ä½å‚æ•°é‡çš„æ¶æ„ï¼Œä¾èµ–äºå¼ºçš„å½’çº³åç½®ã€‚æˆ‘ä»¬å±•ç¤ºäº†é€šè¿‡æ‰©å±•å˜æ¢å™¨æ¶æ„å¹¶åº”ç”¨çµæ´»çš„æœ‰é™æ ‡é‡é‡åŒ–ï¼ˆFSQï¼‰ç“¶é¢ˆï¼Œå¯ä»¥åœ¨æä½çš„æ¯”ç‰¹ç‡ä¸‹å®ç°æœ€å…ˆè¿›çš„è¯­éŸ³è´¨é‡ã€‚è®­ç»ƒåçš„æ¨¡å‹åœ¨å®¢è§‚å’Œä¸»è§‚æµ‹è¯•ä¸­å‡æ˜¾è‘—è¶…è¶Šäº†ç°æœ‰åŸºå‡†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.18664', 'title': 'Spatiotemporal Skip Guidance for Enhanced Video Diffusion Sampling', 'url': 'https://huggingface.co/papers/2411.18664', 'abstract': 'Diffusion models have emerged as a powerful tool for generating high-quality images, videos, and 3D content. While sampling guidance techniques like CFG improve quality, they reduce diversity and motion. Autoguidance mitigates these issues but demands extra weak model training, limiting its practicality for large-scale models. In this work, we introduce Spatiotemporal Skip Guidance (STG), a simple training-free sampling guidance method for enhancing transformer-based video diffusion models. STG employs an implicit weak model via self-perturbation, avoiding the need for external models or additional training. By selectively skipping spatiotemporal layers, STG produces an aligned, degraded version of the original model to boost sample quality without compromising diversity or dynamic degree. Our contributions include: (1) introducing STG as an efficient, high-performing guidance technique for video diffusion models, (2) eliminating the need for auxiliary models by simulating a weak model through layer skipping, and (3) ensuring quality-enhanced guidance without compromising sample diversity or dynamics unlike CFG. For additional results, visit https://junhahyung.github.io/STGuidance.', 'score': 2, 'issue_id': 900, 'pub_date': '2024-11-27', 'pub_date_card': {'ru': '27 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 27', 'zh': '11æœˆ27æ—¥'}, 'hash': '3576f88bbf3e4567', 'authors': ['Junha Hyung', 'Kinam Kim', 'Susung Hong', 'Min-Jung Kim', 'Jaegul Choo'], 'affiliations': ['KAIST AI', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2411.18664.jpg', 'data': {'categories': ['#diffusion', '#architecture', '#3d', '#training', '#video'], 'emoji': 'ğŸ¬', 'ru': {'title': 'STG: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ - Spatiotemporal Skip Guidance (STG). STG Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ°Ğ¼Ğ¾Ğ²Ğ¾Ğ·Ğ¼ÑƒÑ‰ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ»Ğ°Ğ±Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸Ğ·Ğ±Ğ¸Ñ€Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ»Ğ¾Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑÑĞ¼Ğ¿Ğ»Ğ¾Ğ² Ğ±ĞµĞ· ÑƒÑ‰ĞµÑ€Ğ±Ğ° Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸. STG Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº CFG Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ³Ğ°Ğ¹Ğ´ĞµĞ½Ñ, Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸Ğ»Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Enhancing Video Diffusion with Spatiotemporal Skip Guidance', 'desc': 'This paper presents Spatiotemporal Skip Guidance (STG), a novel method for improving video diffusion models without requiring additional training or external models. STG enhances the quality of generated videos by using a self-perturbation technique that simulates a weak model through selective skipping of spatiotemporal layers. This approach allows for better sample quality while maintaining diversity and dynamic motion, addressing the limitations of existing methods like CFG. The authors demonstrate that STG is an efficient and effective guidance technique that enhances the performance of transformer-based video diffusion models.'}, 'zh': {'title': 'æ—¶ç©ºè·³è·ƒå¼•å¯¼ï¼šæå‡è§†é¢‘æ‰©æ•£æ¨¡å‹çš„é«˜æ•ˆæ–¹æ³•', 'desc': 'æ‰©æ•£æ¨¡å‹å·²æˆä¸ºç”Ÿæˆé«˜è´¨é‡å›¾åƒã€è§†é¢‘å’Œ3Då†…å®¹çš„å¼ºå¤§å·¥å…·ã€‚è™½ç„¶é‡‡æ ·å¼•å¯¼æŠ€æœ¯å¦‚CFGå¯ä»¥æé«˜è´¨é‡ï¼Œä½†ä¼šé™ä½å¤šæ ·æ€§å’ŒåŠ¨æ€æ€§ã€‚è‡ªå¼•å¯¼æ–¹æ³•è™½ç„¶å¯ä»¥ç¼“è§£è¿™äº›é—®é¢˜ï¼Œä½†éœ€è¦é¢å¤–çš„å¼±æ¨¡å‹è®­ç»ƒï¼Œé™åˆ¶äº†å…¶åœ¨å¤§è§„æ¨¡æ¨¡å‹ä¸­çš„å®ç”¨æ€§ã€‚æˆ‘ä»¬æå‡ºäº†æ—¶ç©ºè·³è·ƒå¼•å¯¼ï¼ˆSTGï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç®€å•çš„æ— è®­ç»ƒé‡‡æ ·å¼•å¯¼æ–¹æ³•ï¼Œæ—¨åœ¨å¢å¼ºåŸºäºå˜æ¢å™¨çš„è§†é¢‘æ‰©æ•£æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.18092', 'title': 'Training Noise Token Pruning', 'url': 'https://huggingface.co/papers/2411.18092', 'abstract': "In the present work we present Training Noise Token (TNT) Pruning for vision transformers. Our method relaxes the discrete token dropping condition to continuous additive noise, providing smooth optimization in training, while retaining discrete dropping computational gains in deployment settings. We provide theoretical connections to Rate-Distortion literature, and empirical evaluations on the ImageNet dataset using ViT and DeiT architectures demonstrating TNT's advantages over previous pruning methods.", 'score': 1, 'issue_id': 905, 'pub_date': '2024-11-27', 'pub_date_card': {'ru': '27 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 27', 'zh': '11æœˆ27æ—¥'}, 'hash': '570d01745d1c7f3d', 'authors': ['Mingxing Rao', 'Bohan Jiang', 'Daniel Moyer'], 'affiliations': ['Vanderbilt University, Nashville, TN 37235, USA'], 'pdf_title_img': 'assets/pdf/title_img/2411.18092.jpg', 'data': {'categories': ['#optimization', '#training', '#inference', '#cv', '#architecture'], 'emoji': 'âœ‚ï¸', 'ru': {'title': 'ĞŸĞ»Ğ°Ğ²Ğ½Ğ°Ñ Ğ¾Ğ±Ñ€ĞµĞ·ĞºĞ° Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑˆÑƒĞ¼Ğ°', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±Ñ€ĞµĞ·ĞºĞ¸ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ğ´Ğ»Ñ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Training Noise Token (TNT). ĞœĞµÑ‚Ğ¾Ğ´ Ğ·Ğ°Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğµ Ğ¾Ñ‚Ğ±Ñ€Ğ°ÑÑ‹Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ½Ğ° Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑˆÑƒĞ¼Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ğ»Ğ°Ğ²Ğ½ÑƒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ±Ñ€Ğ°ÑÑ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´ÑÑ‚ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ²ÑĞ·Ğ¸ Ñ Ğ»Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ğ¾Ğ¹ Ğ¿Ğ¾ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸-Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ImageNet Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€ ViT Ğ¸ DeiT. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° TNT Ğ¿ĞµÑ€ĞµĞ´ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¾Ğ±Ñ€ĞµĞ·ĞºĞ¸.'}, 'en': {'title': 'Smooth Optimization with TNT Pruning for Vision Transformers', 'desc': 'This paper introduces a novel approach called Training Noise Token (TNT) Pruning for vision transformers, which enhances the training process by allowing continuous additive noise instead of strictly dropping tokens. This method enables smoother optimization during training while still benefiting from the computational efficiency of discrete token dropping during deployment. The authors establish theoretical links to Rate-Distortion theory, which helps to understand the trade-offs involved in token pruning. Empirical results on the ImageNet dataset show that TNT Pruning outperforms existing pruning techniques when applied to ViT and DeiT architectures.'}, 'zh': {'title': 'è®­ç»ƒå™ªå£°æ ‡è®°å‰ªæï¼šä¼˜åŒ–ä¸æ•ˆç‡çš„ç»“åˆ', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºè§†è§‰å˜æ¢å™¨çš„è®­ç»ƒå™ªå£°æ ‡è®°ï¼ˆTNTï¼‰å‰ªææ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†ç¦»æ•£çš„æ ‡è®°ä¸¢å¼ƒæ¡ä»¶æ”¾å®½ä¸ºè¿ç»­çš„åŠ æ€§å™ªå£°ï¼Œä»è€Œåœ¨è®­ç»ƒä¸­å®ç°å¹³æ»‘ä¼˜åŒ–ï¼ŒåŒæ—¶åœ¨éƒ¨ç½²ç¯å¢ƒä¸­ä¿ç•™ç¦»æ•£ä¸¢å¼ƒçš„è®¡ç®—ä¼˜åŠ¿ã€‚æˆ‘ä»¬è¿˜æä¾›äº†ä¸ç‡å¤±çœŸæ–‡çŒ®çš„ç†è®ºè”ç³»ï¼Œå¹¶åœ¨ImageNetæ•°æ®é›†ä¸Šä½¿ç”¨ViTå’ŒDeiTæ¶æ„è¿›è¡Œäº†å®è¯è¯„ä¼°ï¼Œå±•ç¤ºäº†TNTç›¸è¾ƒäºä¹‹å‰å‰ªææ–¹æ³•çš„ä¼˜åŠ¿ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.18665', 'title': 'SpotLight: Shadow-Guided Object Relighting via Diffusion', 'url': 'https://huggingface.co/papers/2411.18665', 'abstract': 'Recent work has shown that diffusion models can be used as powerful neural rendering engines that can be leveraged for inserting virtual objects into images. Unlike typical physics-based renderers, however, neural rendering engines are limited by the lack of manual control over the lighting setup, which is often essential for improving or personalizing the desired image outcome. In this paper, we show that precise lighting control can be achieved for object relighting simply by specifying the desired shadows of the object. Rather surprisingly, we show that injecting only the shadow of the object into a pre-trained diffusion-based neural renderer enables it to accurately shade the object according to the desired light position, while properly harmonizing the object (and its shadow) within the target background image. Our method, SpotLight, leverages existing neural rendering approaches and achieves controllable relighting results with no additional training. Specifically, we demonstrate its use with two neural renderers from the recent literature. We show that SpotLight achieves superior object compositing results, both quantitatively and perceptually, as confirmed by a user study, outperforming existing diffusion-based models specifically designed for relighting.', 'score': 1, 'issue_id': 903, 'pub_date': '2024-11-27', 'pub_date_card': {'ru': '27 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 27', 'zh': '11æœˆ27æ—¥'}, 'hash': '1b31caf705bc142d', 'authors': ['FrÃ©dÃ©ric Fortier-Chouinard', 'Zitian Zhang', 'Louis-Etienne Messier', 'Mathieu Garon', 'Anand Bhattad', 'Jean-FranÃ§ois Lalonde'], 'affiliations': ['Depix Technologies', 'Toyota Technological Institute at Chicago', 'Universite Laval'], 'pdf_title_img': 'assets/pdf/title_img/2411.18665.jpg', 'data': {'categories': ['#3d', '#cv', '#diffusion'], 'emoji': 'ğŸ’¡', 'ru': {'title': 'ĞšĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ² Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¼ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğµ Ñ‡ĞµÑ€ĞµĞ· Ñ‚ĞµĞ½Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ SpotLight Ğ´Ğ»Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¼ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğµ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‚ĞµĞ½Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ° Ğ² Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ€ĞµĞ½Ğ´ĞµÑ€ĞµÑ€ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ·Ğ°Ñ‚ĞµĞ½ÑÑ‚ÑŒ Ğ¾Ğ±ÑŠĞµĞºÑ‚ Ğ² ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¸ Ñ Ğ¶ĞµĞ»Ğ°ĞµĞ¼Ñ‹Ğ¼ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ° ÑĞ²ĞµÑ‚Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ SpotLight Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµĞ½Ğ´ĞµÑ€ĞµÑ€Ğ°Ğ¼Ğ¸. Ğ¡Ğ¾Ğ³Ğ»Ğ°ÑĞ½Ğ¾ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, SpotLight Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¿ĞµÑ€ĞµÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ¸ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'SpotLight: Control Lighting with Shadows in Neural Rendering', 'desc': "This paper introduces SpotLight, a method that enhances neural rendering by allowing precise control over lighting for virtual objects in images. It achieves this by enabling users to specify the desired shadows of the object, which the diffusion model uses to accurately shade the object based on the light's position. SpotLight integrates seamlessly with existing pre-trained diffusion-based neural renderers, requiring no additional training. The results demonstrate significant improvements in object compositing, both in quantitative metrics and user perception, compared to traditional diffusion models designed for relighting."}, 'zh': {'title': 'SpotLightï¼šç²¾å‡†æ§åˆ¶è™šæ‹Ÿç‰©ä½“å…‰ç…§çš„åˆ›æ–°æ–¹æ³•', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºSpotLightçš„æ–¹æ³•ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹è¿›è¡Œè™šæ‹Ÿç‰©ä½“çš„é‡å…‰ç…§ã€‚ä¸ä¼ ç»Ÿçš„ç‰©ç†æ¸²æŸ“å™¨ä¸åŒï¼ŒSpotLighté€šè¿‡ä»…æŒ‡å®šç‰©ä½“çš„é˜´å½±æ¥å®ç°ç²¾ç¡®çš„å…‰ç…§æ§åˆ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿåœ¨ä¸éœ€è¦é¢å¤–è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œå‡†ç¡®åœ°æ ¹æ®æ‰€éœ€çš„å…‰æºä½ç½®ä¸ºç‰©ä½“ä¸Šè‰²ï¼Œå¹¶ä¸èƒŒæ™¯å›¾åƒå’Œè°èåˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSpotLightåœ¨ç‰©ä½“åˆæˆæ•ˆæœä¸Šä¼˜äºç°æœ‰çš„æ‰©æ•£æ¨¡å‹ï¼Œå¾—åˆ°äº†ç”¨æˆ·ç ”ç©¶çš„æ”¯æŒã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.05271', 'title': 'Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling', 'url': 'https://huggingface.co/papers/2412.05271', 'abstract': 'We introduce InternVL 2.5, an advanced multimodal large language model (MLLM) series that builds upon InternVL 2.0, maintaining its core model architecture while introducing significant enhancements in training and testing strategies as well as data quality. In this work, we delve into the relationship between model scaling and performance, systematically exploring the performance trends in vision encoders, language models, dataset sizes, and test-time configurations. Through extensive evaluations on a wide range of benchmarks, including multi-discipline reasoning, document understanding, multi-image / video understanding, real-world comprehension, multimodal hallucination detection, visual grounding, multilingual capabilities, and pure language processing, InternVL 2.5 exhibits competitive performance, rivaling leading commercial models such as GPT-4o and Claude-3.5-Sonnet. Notably, our model is the first open-source MLLMs to surpass 70% on the MMMU benchmark, achieving a 3.7-point improvement through Chain-of-Thought (CoT) reasoning and showcasing strong potential for test-time scaling. We hope this model contributes to the open-source community by setting new standards for developing and applying multimodal AI systems. HuggingFace demo see https://huggingface.co/spaces/OpenGVLab/InternVL', 'score': 56, 'issue_id': 1013, 'pub_date': '2024-12-06', 'pub_date_card': {'ru': '6 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 6', 'zh': '12æœˆ6æ—¥'}, 'hash': '81590cc90bda9173', 'authors': ['Zhe Chen', 'Weiyun Wang', 'Yue Cao', 'Yangzhou Liu', 'Zhangwei Gao', 'Erfei Cui', 'Jinguo Zhu', 'Shenglong Ye', 'Hao Tian', 'Zhaoyang Liu', 'Lixin Gu', 'Xuehui Wang', 'Qingyun Li', 'Yimin Ren', 'Zixuan Chen', 'Jiapeng Luo', 'Jiahao Wang', 'Tan Jiang', 'Bo Wang', 'Conghui He', 'Botian Shi', 'Xingcheng Zhang', 'Han Lv', 'Yi Wang', 'Wenqi Shao', 'Pei Chu', 'Zhongying Tu', 'Tong He', 'Zhiyong Wu', 'Huipeng Deng', 'Jiaye Ge', 'Kai Chen', 'Min Dou', 'Lewei Lu', 'Xizhou Zhu', 'Tong Lu', 'Dahua Lin', 'Yu Qiao', 'Jifeng Dai', 'Wenhai Wang'], 'affiliations': ['Fudan University', 'Nanjing University', 'SenseTime Research', 'Shanghai AI Laboratory', 'Shanghai Jiao Tong University', 'The Chinese University of Hong Kong', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2412.05271.jpg', 'data': {'categories': ['#open_source', '#hallucinations', '#reasoning', '#training', '#multimodal', '#benchmark', '#dataset'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ˜Ğ˜: InternVL 2.5 ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ñ‹', 'desc': 'InternVL 2.5 - ÑÑ‚Ğ¾ ÑƒÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ (MLLM), Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°ÑÑ‰Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ InternVL 2.0. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¸Ğ·ÑƒÑ‡Ğ¸Ğ»Ğ¸ ÑĞ²ÑĞ·ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ ĞµÑ‘ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ, Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ Ñ‚Ñ€ĞµĞ½Ğ´Ñ‹ Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ°Ñ…, ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ° ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ´Ğ¸ÑÑ†Ğ¸Ğ¿Ğ»Ğ¸Ğ½Ğ°Ñ€Ğ½Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². InternVL 2.5 ÑÑ‚Ğ°Ğ»Ğ° Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¹ MLLM, Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²ÑˆĞµĞ¹ Ğ¿Ğ¾Ñ€Ğ¾Ğ³ Ğ² 70% Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ MMMU, Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ² Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'InternVL 2.5: Setting New Standards in Multimodal AI', 'desc': 'InternVL 2.5 is a state-of-the-art multimodal large language model that enhances its predecessor, InternVL 2.0, by improving training methods and data quality. The paper investigates how increasing the model size affects its performance across various tasks, including reasoning, document understanding, and multimodal comprehension. Extensive testing shows that InternVL 2.5 competes effectively with top commercial models, achieving significant benchmarks like surpassing 70% on the MMMU benchmark. This model aims to advance the open-source community by establishing new benchmarks for multimodal AI applications.'}, 'zh': {'title': 'å¼€åˆ›å¤šæ¨¡æ€AIæ–°æ ‡å‡†çš„InternVL 2.5', 'desc': 'æˆ‘ä»¬ä»‹ç»äº†InternVL 2.5ï¼Œè¿™æ˜¯ä¸€ä¸ªå…ˆè¿›çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ç³»åˆ—ï¼ŒåŸºäºInternVL 2.0è¿›è¡Œæ”¹è¿›ã€‚è¯¥æ¨¡å‹åœ¨è®­ç»ƒå’Œæµ‹è¯•ç­–ç•¥ä»¥åŠæ•°æ®è´¨é‡ä¸Šè¿›è¡Œäº†æ˜¾è‘—å¢å¼ºï¼Œå¹¶ç³»ç»Ÿåœ°æ¢è®¨äº†æ¨¡å‹è§„æ¨¡ä¸æ€§èƒ½ä¹‹é—´çš„å…³ç³»ã€‚é€šè¿‡åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›è¯„ä¼°ï¼ŒInternVL 2.5å±•ç°äº†ä¸é¢†å…ˆå•†ä¸šæ¨¡å‹å¦‚GPT-4oå’ŒClaude-3.5-Sonnetç›¸åª²ç¾çš„ç«äº‰æ€§èƒ½ã€‚æˆ‘ä»¬çš„æ¨¡å‹é¦–æ¬¡åœ¨MMMUåŸºå‡†ä¸Šè¶…è¿‡70%ï¼Œå¹¶é€šè¿‡é“¾å¼æ€ç»´æ¨ç†å®ç°äº†3.7ç‚¹çš„æå‡ï¼Œå±•ç¤ºäº†åœ¨æµ‹è¯•æ—¶æ‰©å±•çš„å¼ºå¤§æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.04814', 'title': 'LiFT: Leveraging Human Feedback for Text-to-Video Model Alignment', 'url': 'https://huggingface.co/papers/2412.04814', 'abstract': 'Recent advancements in text-to-video (T2V) generative models have shown impressive capabilities. However, these models are still inadequate in aligning synthesized videos with human preferences (e.g., accurately reflecting text descriptions), which is particularly difficult to address, as human preferences are inherently subjective and challenging to formalize as objective functions. Therefore, this paper proposes LiFT, a novel fine-tuning method leveraging human feedback for T2V model alignment. Specifically, we first construct a Human Rating Annotation dataset, LiFT-HRA, consisting of approximately 10k human annotations, each including a score and its corresponding rationale. Based on this, we train a reward model LiFT-Critic to learn reward function effectively, which serves as a proxy for human judgment, measuring the alignment between given videos and human expectations. Lastly, we leverage the learned reward function to align the T2V model by maximizing the reward-weighted likelihood. As a case study, we apply our pipeline to CogVideoX-2B, showing that the fine-tuned model outperforms the CogVideoX-5B across all 16 metrics, highlighting the potential of human feedback in improving the alignment and quality of synthesized videos.', 'score': 32, 'issue_id': 1013, 'pub_date': '2024-12-06', 'pub_date_card': {'ru': '6 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 6', 'zh': '12æœˆ6æ—¥'}, 'hash': '3ac10cfceba4368e', 'authors': ['Yibin Wang', 'Zhiyu Tan', 'Junyan Wang', 'Xiaomeng Yang', 'Cheng Jin', 'Hao Li'], 'affiliations': ['Australian Institute for Machine Learning, The University of Adelaide', 'Fudan University', 'Shanghai Academy of Artificial Intelligence for Science'], 'pdf_title_img': 'assets/pdf/title_img/2412.04814.jpg', 'data': {'categories': ['#rlhf', '#dataset', '#training', '#alignment', '#video'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ñ†ĞµĞ½Ğ¾Ğº', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ LiFT - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ¾Ñ‚ Ğ»ÑĞ´ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ LiFT-HRA Ñ 10 Ñ‚Ñ‹ÑÑÑ‡Ğ°Ğ¼Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ñ†ĞµĞ½Ğ¾Ğº Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¸Ñ… Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ±Ñ‹Ğ»Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ-ĞºÑ€Ğ¸Ñ‚Ğ¸Ğº LiFT-Critic, Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‰Ğ°Ñ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¾Ğ¶Ğ¸Ğ´Ğ°Ğ½Ğ¸ÑĞ¼ Ğ»ÑĞ´ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ±Ñ‹Ğ» Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½ Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ CogVideoX-2B, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾Ğ¹Ñ‚Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½ÑƒÑ CogVideoX-5B Ğ¿Ğ¾ 16 Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼.'}, 'en': {'title': 'Aligning Videos with Human Preferences Using Feedback', 'desc': 'This paper introduces LiFT, a new method for improving text-to-video (T2V) generative models by incorporating human feedback. The authors create a dataset called LiFT-HRA, which contains around 10,000 human annotations that provide scores and rationales for video quality. They develop a reward model, LiFT-Critic, to quantify how well generated videos align with human expectations, effectively serving as a stand-in for human judgment. By maximizing the reward-weighted likelihood using this model, they demonstrate that their approach significantly enhances the performance of T2V models, as shown in their case study with CogVideoX-2B.'}, 'zh': {'title': 'åˆ©ç”¨äººç±»åé¦ˆæå‡æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆæ¨¡å‹çš„å¯¹é½æ€§', 'desc': 'æœ€è¿‘ï¼Œæ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼ˆT2Vï¼‰å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨å°†ç”Ÿæˆçš„è§†é¢‘ä¸äººç±»åå¥½å¯¹é½æ–¹é¢ä»ç„¶å­˜åœ¨ä¸è¶³ã€‚ç”±äºäººç±»åå¥½å…·æœ‰ä¸»è§‚æ€§ï¼Œéš¾ä»¥å½¢å¼åŒ–ä¸ºå®¢è§‚å‡½æ•°ï¼Œå› æ­¤æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¾®è°ƒæ–¹æ³•LiFTï¼Œåˆ©ç”¨äººç±»åé¦ˆæ¥æ”¹å–„T2Væ¨¡å‹çš„å¯¹é½ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåŒ…å«çº¦1ä¸‡æ¡äººç±»è¯„åˆ†åŠå…¶ç†ç”±çš„æ³¨é‡Šæ•°æ®é›†LiFT-HRAï¼Œå¹¶åŸºäºæ­¤è®­ç»ƒäº†ä¸€ä¸ªå¥–åŠ±æ¨¡å‹LiFT-Criticï¼Œä»¥æœ‰æ•ˆå­¦ä¹ å¥–åŠ±å‡½æ•°ã€‚é€šè¿‡æœ€å¤§åŒ–å¥–åŠ±åŠ æƒçš„ä¼¼ç„¶æ€§ï¼Œæˆ‘ä»¬æˆåŠŸåœ°å°†T2Væ¨¡å‹ä¸äººç±»æœŸæœ›å¯¹é½ï¼Œå®éªŒç»“æœè¡¨æ˜ï¼Œå¾®è°ƒåçš„æ¨¡å‹åœ¨å„é¡¹æŒ‡æ ‡ä¸Šå‡ä¼˜äºåŸæ¨¡å‹ï¼Œå±•ç¤ºäº†äººç±»åé¦ˆåœ¨æå‡ç”Ÿæˆè§†é¢‘è´¨é‡æ–¹é¢çš„æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.05237', 'title': 'MAmmoTH-VL: Eliciting Multimodal Reasoning with Instruction Tuning at Scale', 'url': 'https://huggingface.co/papers/2412.05237', 'abstract': 'Open-source multimodal large language models (MLLMs) have shown significant potential in a broad range of multimodal tasks. However, their reasoning capabilities remain constrained by existing instruction-tuning datasets, which were predominately repurposed from academic datasets such as VQA, AI2D, and ChartQA. These datasets target simplistic tasks, and only provide phrase-level answers without any intermediate rationales. To address these challenges, we introduce a scalable and cost-effective method to construct a large-scale multimodal instruction-tuning dataset with rich intermediate rationales designed to elicit CoT reasoning. Using only open models, we create a dataset containing 12M instruction-response pairs to cover diverse, reasoning-intensive tasks with detailed and faithful rationales. Experiments demonstrate that training MLLMs on this dataset significantly improves reasoning capabilities, achieving state-of-the-art performance on benchmarks such as MathVerse (+8.1%), MMMU-Pro (+7%), and MuirBench (+13.3%). Additionally, the model demonstrates notable improvements of up to 4% on non-reasoning-based benchmarks. Ablation studies further highlight the importance of key components, such as rewriting and self-filtering, in the dataset construction process.', 'score': 30, 'issue_id': 1012, 'pub_date': '2024-12-06', 'pub_date_card': {'ru': '6 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 6', 'zh': '12æœˆ6æ—¥'}, 'hash': '1e3d5645afd61cf2', 'authors': ['Jarvis Guo', 'Tuney Zheng', 'Yuelin Bai', 'Bo Li', 'Yubo Wang', 'King Zhu', 'Yizhi Li', 'Graham Neubig', 'Wenhu Chen', 'Xiang Yue'], 'affiliations': ['Carnegie Mellon University', 'Nanyang Technological University', 'The University of Manchester', 'University of Waterloo'], 'pdf_title_img': 'assets/pdf/title_img/2412.05237.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#open_source', '#reasoning', '#dataset', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°Ğ¼Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ ÑĞ¿Ğ¾ÑĞ¾Ğ±ÑÑ‚Ğ²ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· 12 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¿Ğ°Ñ€ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ², Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğµ Ğ¸Ğ½Ñ‚ĞµĞ½ÑĞ¸Ğ²Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ….'}, 'en': {'title': 'Enhancing Reasoning in MLLMs with Rich Rationales', 'desc': "This paper presents a new method for creating a large-scale multimodal instruction-tuning dataset aimed at improving the reasoning abilities of multimodal large language models (MLLMs). The authors identify that existing datasets are limited as they mainly provide simple answers without detailed reasoning. To overcome this, they developed a dataset with 12 million instruction-response pairs that include rich intermediate rationales, promoting chain-of-thought (CoT) reasoning. Experiments show that training on this dataset enhances MLLMs' reasoning performance significantly, achieving state-of-the-art results on various benchmarks."}, 'zh': {'title': 'æå‡å¤šæ¨¡æ€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„æ–°æ–¹æ³•', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ„å»ºå¤§è§„æ¨¡å¤šæ¨¡æ€æŒ‡ä»¤è°ƒä¼˜æ•°æ®é›†çš„æ–¹æ³•ï¼Œä»¥æé«˜å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚ç°æœ‰çš„æ•°æ®é›†ä¸»è¦æ¥è‡ªå­¦æœ¯ç ”ç©¶ï¼Œä»»åŠ¡ç®€å•ï¼Œç¼ºä¹ä¸­é—´æ¨ç†è¿‡ç¨‹çš„è¯¦ç»†è§£é‡Šã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ›å»ºäº†ä¸€ä¸ªåŒ…å«1200ä¸‡å¯¹æŒ‡ä»¤å’Œå“åº”çš„æ•°æ®é›†ï¼Œæ¶µç›–äº†å¤šæ ·åŒ–ä¸”éœ€è¦æ¨ç†çš„ä»»åŠ¡ï¼Œå¹¶æä¾›äº†ä¸°å¯Œçš„æ¨ç†ä¾æ®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨è¯¥æ•°æ®é›†è®­ç»ƒçš„MLLMsåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†æ¨ç†èƒ½åŠ›ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.04862', 'title': 'EXAONE 3.5: Series of Large Language Models for Real-world Use Cases', 'url': 'https://huggingface.co/papers/2412.04862', 'abstract': 'This technical report introduces the EXAONE 3.5 instruction-tuned language models, developed and released by LG AI Research. The EXAONE 3.5 language models are offered in three configurations: 32B, 7.8B, and 2.4B. These models feature several standout capabilities: 1) exceptional instruction following capabilities in real-world scenarios, achieving the highest scores across seven benchmarks, 2) outstanding long-context comprehension, attaining the top performance in four benchmarks, and 3) competitive results compared to state-of-the-art open models of similar sizes across nine general benchmarks. The EXAONE 3.5 language models are open to anyone for research purposes and can be downloaded from https://huggingface.co/LGAI-EXAONE. For commercial use, please reach out to the official contact point of LG AI Research: contact_us@lgresearch.ai.', 'score': 29, 'issue_id': 1011, 'pub_date': '2024-12-06', 'pub_date_card': {'ru': '6 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 6', 'zh': '12æœˆ6æ—¥'}, 'hash': '83e6f957e42ebb5e', 'authors': ['LG AI Research', 'Soyoung An', 'Kyunghoon Bae', 'Eunbi Choi', 'Kibong Choi', 'Stanley Jungkyu Choi', 'Seokhee Hong', 'Junwon Hwang', 'Hyojin Jeon', 'Gerrard Jeongwon Jo', 'Hyunjik Jo', 'Jiyeon Jung', 'Yountae Jung', 'Hyosang Kim', 'Joonkee Kim', 'Seonghwan Kim', 'Soyeon Kim', 'Sunkyoung Kim', 'Yireun Kim', 'Yongil Kim', 'Youchul Kim', 'Edward Hwayoung Lee', 'Haeju Lee', 'Honglak Lee', 'Jinsik Lee', 'Kyungmin Lee', 'Woohyung Lim', 'Sangha Park', 'Sooyoun Park', 'Yongmin Park', 'Sihoon Yang', 'Heuiyeen Yeen', 'Hyeongu Yun'], 'affiliations': ['LG AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2412.04862.jpg', 'data': {'categories': ['#architecture', '#training', '#open_source', '#small_models', '#long_context', '#benchmark'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'EXAONE 3.5: ĞĞ¾Ğ²Ğ¾Ğµ ÑĞ»Ğ¾Ğ²Ğ¾ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¾Ñ‚ LG AI Research', 'desc': 'ĞšĞ¾Ğ¼Ğ¿Ğ°Ğ½Ğ¸Ñ LG AI Research Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ EXAONE 3.5, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹ Ğ² Ñ‚Ñ€ĞµÑ… ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸ÑÑ…: 32B, 7.8B Ğ¸ 2.4B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞĞ½Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¸ÑĞºĞ»ÑÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼, Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ EXAONE 3.5 Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹ Ğ´Ğ»Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ñ†ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ñ‹ Ñ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ñ‹ Hugging Face.'}, 'en': {'title': 'EXAONE 3.5: Leading the Way in Instruction-Tuned Language Models', 'desc': 'The EXAONE 3.5 language models, created by LG AI Research, are advanced instruction-tuned models available in three sizes: 32B, 7.8B, and 2.4B. They excel in following instructions in real-world applications, achieving top scores on seven different benchmarks. Additionally, these models demonstrate superior long-context understanding, ranking first in four benchmarks. They are accessible for research purposes and show competitive performance against leading open models of similar sizes across nine general benchmarks.'}, 'zh': {'title': 'EXAONE 3.5ï¼šæŒ‡ä»¤è·Ÿéšä¸é•¿ä¸Šä¸‹æ–‡ç†è§£çš„å…ˆé”‹', 'desc': 'EXAONE 3.5 æ˜¯ç”± LG AI ç ”ç©¶æ‰€å¼€å‘çš„æŒ‡ä»¤è°ƒä¼˜è¯­è¨€æ¨¡å‹ï¼Œæä¾›ä¸‰ç§é…ç½®ï¼š32Bã€7.8B å’Œ 2.4Bã€‚è¿™äº›æ¨¡å‹åœ¨å®é™…åœºæ™¯ä¸­å…·æœ‰å“è¶Šçš„æŒ‡ä»¤è·Ÿéšèƒ½åŠ›ï¼Œåœ¨ä¸ƒä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€é«˜åˆ†ã€‚å®ƒä»¬åœ¨é•¿ä¸Šä¸‹æ–‡ç†è§£æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œåœ¨å››ä¸ªåŸºå‡†æµ‹è¯•ä¸­ä¹Ÿè¾¾åˆ°äº†æœ€ä½³æ€§èƒ½ã€‚æ­¤å¤–ï¼Œä¸åŒç±»å¼€æºæ¨¡å‹ç›¸æ¯”ï¼ŒEXAONE 3.5 åœ¨ä¹ä¸ªé€šç”¨åŸºå‡†æµ‹è¯•ä¸­ä¹Ÿå±•ç°äº†ç«äº‰åŠ›çš„ç»“æœã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.05270', 'title': 'APOLLO: SGD-like Memory, AdamW-level Performance', 'url': 'https://huggingface.co/papers/2412.05270', 'abstract': "Large language models (LLMs) are notoriously memory-intensive during training, particularly with the popular AdamW optimizer. This memory burden necessitates using more or higher-end GPUs or reducing batch sizes, limiting training scalability and throughput. To address this, various memory-efficient optimizers have been proposed to reduce optimizer memory usage. However, they face critical challenges: (i) reliance on costly SVD operations; (ii) significant performance trade-offs compared to AdamW; and (iii) still substantial optimizer memory overhead to maintain competitive performance.   In this work, we identify that AdamW's learning rate adaptation rule can be effectively coarsened as a structured learning rate update. Based on this insight, we propose Approximated Gradient Scaling for Memory-Efficient LLM Optimization (APOLLO), which approximates learning rate scaling using an auxiliary low-rank optimizer state based on pure random projection. This structured learning rate update rule makes APOLLO highly tolerant to further memory reductions while delivering comparable pre-training performance. Even its rank-1 variant, APOLLO-Mini, achieves superior pre-training performance compared to AdamW with SGD-level memory costs.   Extensive experiments demonstrate that the APOLLO series performs on-par with or better than AdamW, while achieving greater memory savings by nearly eliminating the optimization states of AdamW. These savings provide significant system-level benefits: (1) Enhanced Throughput: 3x throughput on an 8xA100-80GB setup compared to AdamW by supporting 4x larger batch sizes. (2) Improved Model Scalability: Pre-training LLaMA-13B with naive DDP on A100-80GB GPUs without system-level optimizations. (3) Low-End GPU Friendly Pre-training: Pre-training LLaMA-7B on a single GPU using less than 12 GB of memory with weight quantization.", 'score': 23, 'issue_id': 1012, 'pub_date': '2024-12-06', 'pub_date_card': {'ru': '6 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 6', 'zh': '12æœˆ6æ—¥'}, 'hash': '14bb480f5fe29bae', 'authors': ['Hanqing Zhu', 'Zhenyu Zhang', 'Wenyan Cong', 'Xi Liu', 'Sem Park', 'Vikas Chandra', 'Bo Long', 'David Z. Pan', 'Zhangyang Wang', 'Jinwon Lee'], 'affiliations': ['Department of Electrical and Computer Engineering, The University of Texas at Austin', 'Meta'], 'pdf_title_img': 'assets/pdf/title_img/2412.05270.jpg', 'data': {'categories': ['#inference', '#training', '#optimization'], 'emoji': 'ğŸš€', 'ru': {'title': 'APOLLO: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ LLM Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ APOLLO Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). APOLLO Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ½Ğ° ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ĞµÑ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ·Ğ¸Ñ‚ÑŒ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ğ¼ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ AdamW. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ APOLLO Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ğ¾Ğ¹ Ğ¸Ğ»Ğ¸ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ñ‡ĞµĞ¼ AdamW, Ğ¿Ñ€Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ğ¸ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ½ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ´ĞµĞ»Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ñ‹Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ LLM Ğ´Ğ°Ğ¶Ğµ Ğ½Ğ° GPU ÑÑ€ĞµĞ´Ğ½ĞµĞ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ.'}, 'en': {'title': 'APOLLO: Memory-Efficient Optimization for Large Language Models', 'desc': 'This paper addresses the high memory requirements of training large language models (LLMs) using the AdamW optimizer. It introduces a new method called APOLLO, which simplifies the learning rate adaptation process to reduce memory usage while maintaining performance. APOLLO utilizes a low-rank optimizer state based on random projection, allowing for significant memory savings without sacrificing training efficiency. The results show that APOLLO can achieve comparable or better performance than AdamW, enabling larger batch sizes and making LLM training feasible on lower-end GPUs.'}, 'zh': {'title': 'APOLLOï¼šé«˜æ•ˆå†…å­˜ä¼˜åŒ–çš„æœªæ¥', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¯¹å†…å­˜çš„éœ€æ±‚éå¸¸é«˜ï¼Œå°¤å…¶æ˜¯ä½¿ç”¨æµè¡Œçš„AdamWä¼˜åŒ–å™¨æ—¶ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œç ”ç©¶è€…ä»¬æå‡ºäº†å¤šç§å†…å­˜é«˜æ•ˆçš„ä¼˜åŒ–å™¨ï¼Œä½†å®ƒä»¬é¢ä¸´ç€ä¾èµ–æ˜‚è´µçš„SVDæ“ä½œå’Œæ€§èƒ½æŠ˜è¡·ç­‰æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºAPOLLOçš„ä¼˜åŒ–æ–¹æ³•ï¼Œé€šè¿‡è¿‘ä¼¼å­¦ä¹ ç‡ç¼©æ”¾æ¥å‡å°‘å†…å­˜ä½¿ç”¨ï¼ŒåŒæ—¶ä¿æŒä¸AdamWç›¸å½“çš„é¢„è®­ç»ƒæ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAPOLLOç³»åˆ—åœ¨å†…å­˜èŠ‚çœæ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œèƒ½å¤Ÿåœ¨è¾ƒä½çš„å†…å­˜æˆæœ¬ä¸‹å®ç°æ›´é«˜çš„è®­ç»ƒååé‡å’Œæ¨¡å‹å¯æ‰©å±•æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.04301', 'title': 'SwiftEdit: Lightning Fast Text-Guided Image Editing via One-Step Diffusion', 'url': 'https://huggingface.co/papers/2412.04301', 'abstract': 'Recent advances in text-guided image editing enable users to perform image edits through simple text inputs, leveraging the extensive priors of multi-step diffusion-based text-to-image models. However, these methods often fall short of the speed demands required for real-world and on-device applications due to the costly multi-step inversion and sampling process involved. In response to this, we introduce SwiftEdit, a simple yet highly efficient editing tool that achieve instant text-guided image editing (in 0.23s). The advancement of SwiftEdit lies in its two novel contributions: a one-step inversion framework that enables one-step image reconstruction via inversion and a mask-guided editing technique with our proposed attention rescaling mechanism to perform localized image editing. Extensive experiments are provided to demonstrate the effectiveness and efficiency of SwiftEdit. In particular, SwiftEdit enables instant text-guided image editing, which is extremely faster than previous multi-step methods (at least 50 times faster) while maintain a competitive performance in editing results. Our project page is at: https://swift-edit.github.io/', 'score': 18, 'issue_id': 1011, 'pub_date': '2024-12-05', 'pub_date_card': {'ru': '5 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 5', 'zh': '12æœˆ5æ—¥'}, 'hash': 'a4cea89a59a9a3c0', 'authors': ['Trong-Tung Nguyen', 'Quang Nguyen', 'Khoi Nguyen', 'Anh Tran', 'Cuong Pham'], 'affiliations': ['Posts & Telecom. Inst. of Tech., Vietnam', 'VinAI Research'], 'pdf_title_img': 'assets/pdf/title_img/2412.04301.jpg', 'data': {'categories': ['#multimodal', '#diffusion', '#cv'], 'emoji': 'ğŸš€', 'ru': {'title': 'ĞœĞ³Ğ½Ğ¾Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼', 'desc': 'SwiftEdit - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², SwiftEdit Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ´Ğ½Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²ÑƒÑ Ğ¸Ğ½Ğ²ĞµÑ€ÑĞ¸Ñ Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¼Ğ³Ğ½Ğ¾Ğ²ĞµĞ½Ğ½Ğ¾ - Ğ·Ğ° 0.23 ÑĞµĞºÑƒĞ½Ğ´Ñ‹. SwiftEdit Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ ĞºĞ°Ğº Ğ¼Ğ¸Ğ½Ğ¸Ğ¼ÑƒĞ¼ Ğ² 50 Ñ€Ğ°Ğ· Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ², ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'SwiftEdit: Instant Text-Guided Image Editing at Lightning Speed!', 'desc': 'This paper presents SwiftEdit, a new tool for fast text-guided image editing that significantly improves the speed of image modifications. Traditional methods rely on multi-step diffusion processes, which can be slow and inefficient for real-time applications. SwiftEdit introduces a one-step inversion framework that allows for quick image reconstruction and a mask-guided editing technique that uses attention rescaling for precise edits. The results show that SwiftEdit is at least 50 times faster than previous methods while still delivering high-quality editing outcomes.'}, 'zh': {'title': 'SwiftEditï¼šç¬æ—¶æ–‡æœ¬å¼•å¯¼å›¾åƒç¼–è¾‘çš„é©å‘½', 'desc': 'æœ€è¿‘çš„æ–‡æœ¬å¼•å¯¼å›¾åƒç¼–è¾‘æŠ€æœ¯ä½¿ç”¨æˆ·èƒ½å¤Ÿé€šè¿‡ç®€å•çš„æ–‡æœ¬è¾“å…¥è¿›è¡Œå›¾åƒç¼–è¾‘ï¼Œåˆ©ç”¨äº†å¤šæ­¥æ‰©æ•£æ¨¡å‹çš„ä¸°å¯Œå…ˆéªŒçŸ¥è¯†ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨å®é™…åº”ç”¨ä¸­å¾€å¾€é€Ÿåº¦è¾ƒæ…¢ï¼Œæ— æ³•æ»¡è¶³å®æ—¶å’Œè®¾å¤‡ç«¯çš„éœ€æ±‚ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†SwiftEditï¼Œè¿™æ˜¯ä¸€ç§ç®€å•è€Œé«˜æ•ˆçš„ç¼–è¾‘å·¥å…·ï¼Œå¯ä»¥å®ç°ç¬æ—¶çš„æ–‡æœ¬å¼•å¯¼å›¾åƒç¼–è¾‘ï¼Œé€Ÿåº¦è¾¾åˆ°0.23ç§’ã€‚SwiftEditçš„åˆ›æ–°åœ¨äºå…¶ä¸€ä½“åŒ–çš„åæ¼”æ¡†æ¶å’ŒåŸºäºæ©ç çš„ç¼–è¾‘æŠ€æœ¯ï¼Œèƒ½å¤Ÿå¿«é€Ÿè¿›è¡Œå±€éƒ¨å›¾åƒç¼–è¾‘ï¼ŒåŒæ—¶ä¿æŒä¸ä¼ ç»Ÿå¤šæ­¥æ–¹æ³•ç›¸å½“çš„ç¼–è¾‘æ•ˆæœã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.04445', 'title': 'Moto: Latent Motion Token as the Bridging Language for Robot Manipulation', 'url': 'https://huggingface.co/papers/2412.04445', 'abstract': 'Recent developments in Large Language Models pre-trained on extensive corpora have shown significant success in various natural language processing tasks with minimal fine-tuning. This success offers new promise for robotics, which has long been constrained by the high cost of action-labeled data. We ask: given the abundant video data containing interaction-related knowledge available as a rich "corpus", can a similar generative pre-training approach be effectively applied to enhance robot learning? The key challenge is to identify an effective representation for autoregressive pre-training that benefits robot manipulation tasks. Inspired by the way humans learn new skills through observing dynamic environments, we propose that effective robotic learning should emphasize motion-related knowledge, which is closely tied to low-level actions and is hardware-agnostic, facilitating the transfer of learned motions to actual robot actions. To this end, we introduce Moto, which converts video content into latent Motion Token sequences by a Latent Motion Tokenizer, learning a bridging "language" of motion from videos in an unsupervised manner. We pre-train Moto-GPT through motion token autoregression, enabling it to capture diverse visual motion knowledge. After pre-training, Moto-GPT demonstrates the promising ability to produce semantically interpretable motion tokens, predict plausible motion trajectories, and assess trajectory rationality through output likelihood. To transfer learned motion priors to real robot actions, we implement a co-fine-tuning strategy that seamlessly bridges latent motion token prediction and real robot control. Extensive experiments show that the fine-tuned Moto-GPT exhibits superior robustness and efficiency on robot manipulation benchmarks, underscoring its effectiveness in transferring knowledge from video data to downstream visual manipulation tasks.', 'score': 18, 'issue_id': 1011, 'pub_date': '2024-12-05', 'pub_date_card': {'ru': '5 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 5', 'zh': '12æœˆ5æ—¥'}, 'hash': 'a5ac6d786500ef9f', 'authors': ['Yi Chen', 'Yuying Ge', 'Yizhuo Li', 'Yixiao Ge', 'Mingyu Ding', 'Ying Shan', 'Xihui Liu'], 'affiliations': ['ARC Lab, Tencent PCG', 'The University of Hong Kong', 'University of California, Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2412.04445.jpg', 'data': {'categories': ['#training', '#transfer_learning', '#games', '#robotics', '#multimodal', '#video'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Moto, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚ Ğ² Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Latent Motion Tokenizer. Moto-GPT Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ĞµĞ¹ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸. ĞŸĞ¾ÑĞ»Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Moto-GPT Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Bridging Video Knowledge to Robot Actions with Moto-GPT', 'desc': 'This paper explores the application of Large Language Models to enhance robot learning by leveraging abundant video data. It introduces Moto, a system that converts video content into Motion Token sequences, allowing robots to learn motion-related knowledge in an unsupervised manner. The approach emphasizes the importance of motion understanding for effective robotic manipulation, enabling the transfer of learned skills to real-world actions. Through extensive experiments, Moto-GPT demonstrates improved performance in robot manipulation tasks, showcasing its ability to bridge the gap between video knowledge and practical robot control.'}, 'zh': {'title': 'åˆ©ç”¨è§†é¢‘æ•°æ®æå‡æœºå™¨äººå­¦ä¹ èƒ½åŠ›', 'desc': 'è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¦‚ä½•åˆ©ç”¨å¤§è§„æ¨¡è§†é¢‘æ•°æ®æ¥æå‡æœºå™¨äººå­¦ä¹ èƒ½åŠ›ã€‚ç ”ç©¶è€…æå‡ºäº†ä¸€ç§åä¸ºMotoçš„æ¨¡å‹ï¼Œé€šè¿‡å°†è§†é¢‘å†…å®¹è½¬æ¢ä¸ºæ½œåœ¨çš„è¿åŠ¨æ ‡è®°åºåˆ—ï¼Œæ¥å­¦ä¹ è¿åŠ¨çŸ¥è¯†ã€‚Moto-GPTç»è¿‡é¢„è®­ç»ƒåï¼Œèƒ½å¤Ÿç”Ÿæˆå¯è§£é‡Šçš„è¿åŠ¨æ ‡è®°ï¼Œå¹¶é¢„æµ‹åˆç†çš„è¿åŠ¨è½¨è¿¹ã€‚æœ€ç»ˆï¼Œç ”ç©¶è¡¨æ˜ï¼Œç»è¿‡å¾®è°ƒçš„Moto-GPTåœ¨æœºå™¨äººæ“ä½œåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¯æ˜äº†ä»è§†é¢‘æ•°æ®è½¬ç§»çŸ¥è¯†çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.04440', 'title': 'GenMAC: Compositional Text-to-Video Generation with Multi-Agent Collaboration', 'url': 'https://huggingface.co/papers/2412.04440', 'abstract': 'Text-to-video generation models have shown significant progress in the recent years. However, they still struggle with generating complex dynamic scenes based on compositional text prompts, such as attribute binding for multiple objects, temporal dynamics associated with different objects, and interactions between objects. Our key motivation is that complex tasks can be decomposed into simpler ones, each handled by a role-specialized MLLM agent. Multiple agents can collaborate together to achieve collective intelligence for complex goals. We propose GenMAC, an iterative, multi-agent framework that enables compositional text-to-video generation. The collaborative workflow includes three stages: Design, Generation, and Redesign, with an iterative loop between the Generation and Redesign stages to progressively verify and refine the generated videos. The Redesign stage is the most challenging stage that aims to verify the generated videos, suggest corrections, and redesign the text prompts, frame-wise layouts, and guidance scales for the next iteration of generation. To avoid hallucination of a single MLLM agent, we decompose this stage to four sequentially-executed MLLM-based agents: verification agent, suggestion agent, correction agent, and output structuring agent. Furthermore, to tackle diverse scenarios of compositional text-to-video generation, we design a self-routing mechanism to adaptively select the proper correction agent from a collection of correction agents each specialized for one scenario. Extensive experiments demonstrate the effectiveness of GenMAC, achieving state-of-the art performance in compositional text-to-video generation.', 'score': 13, 'issue_id': 1011, 'pub_date': '2024-12-05', 'pub_date_card': {'ru': '5 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 5', 'zh': '12æœˆ5æ—¥'}, 'hash': '28dc2191ba71c4ea', 'authors': ['Kaiyi Huang', 'Yukun Huang', 'Xuefei Ning', 'Zinan Lin', 'Yu Wang', 'Xihui Liu'], 'affiliations': ['Microsoft Research', 'The University of Hong Kong', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2412.04440.jpg', 'data': {'categories': ['#hallucinations', '#agents', '#games', '#multimodal', '#video'], 'emoji': 'ğŸ¬', 'ru': {'title': 'ĞšĞ¾Ğ»Ğ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ GenMAC - Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğ´Ğ»Ñ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞŸÑ€Ğ¾Ñ†ĞµÑÑ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑÑ‚Ğ°Ğ¿Ñ‹ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿ĞµÑ€ĞµĞ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ñ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ñ†Ğ¸ĞºĞ»Ğ¾Ğ¼ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¸ Ğ¿ĞµÑ€ĞµĞ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¿Ğ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°. GenMAC Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'Collaborative Intelligence for Text-to-Video Mastery', 'desc': 'This paper introduces GenMAC, a multi-agent framework designed to improve text-to-video generation by breaking down complex tasks into simpler components. It utilizes specialized machine learning agents that collaborate through three main stages: Design, Generation, and Redesign, with an iterative process to refine video outputs. The Redesign stage is particularly crucial as it involves multiple agents that verify, suggest corrections, and restructure prompts to enhance the generated videos. The framework also includes a self-routing mechanism to select the most suitable correction agent for various scenarios, leading to significant advancements in generating dynamic scenes from text prompts.'}, 'zh': {'title': 'GenMACï¼šåä½œç”Ÿæˆå¤æ‚è§†é¢‘çš„æ™ºèƒ½æ¡†æ¶', 'desc': 'æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆæ¨¡å‹è¿‘å¹´æ¥å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨ç”Ÿæˆå¤æ‚åŠ¨æ€åœºæ™¯æ—¶ä»é¢ä¸´æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºGenMACçš„å¤šä»£ç†æ¡†æ¶ï¼Œé€šè¿‡å°†å¤æ‚ä»»åŠ¡åˆ†è§£ä¸ºç®€å•ä»»åŠ¡æ¥å®ç°åä½œç”Ÿæˆã€‚è¯¥æ¡†æ¶åŒ…æ‹¬è®¾è®¡ã€ç”Ÿæˆå’Œé‡æ–°è®¾è®¡ä¸‰ä¸ªé˜¶æ®µï¼Œç”Ÿæˆå’Œé‡æ–°è®¾è®¡ä¹‹é—´å­˜åœ¨è¿­ä»£å¾ªç¯ï¼Œä»¥é€æ­¥éªŒè¯å’Œä¼˜åŒ–ç”Ÿæˆçš„è§†é¢‘ã€‚é€šè¿‡è‡ªé€‚åº”é€‰æ‹©ä¸“é—¨åŒ–çš„ä¿®æ­£ä»£ç†ï¼ŒGenMACèƒ½å¤Ÿæœ‰æ•ˆåº”å¯¹å¤šæ ·åŒ–çš„æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆåœºæ™¯ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.04887', 'title': 'Momentum-GS: Momentum Gaussian Self-Distillation for High-Quality Large Scene Reconstruction', 'url': 'https://huggingface.co/papers/2412.04887', 'abstract': "3D Gaussian Splatting has demonstrated notable success in large-scale scene reconstruction, but challenges persist due to high training memory consumption and storage overhead. Hybrid representations that integrate implicit and explicit features offer a way to mitigate these limitations. However, when applied in parallelized block-wise training, two critical issues arise since reconstruction accuracy deteriorates due to reduced data diversity when training each block independently, and parallel training restricts the number of divided blocks to the available number of GPUs. To address these issues, we propose Momentum-GS, a novel approach that leverages momentum-based self-distillation to promote consistency and accuracy across the blocks while decoupling the number of blocks from the physical GPU count. Our method maintains a teacher Gaussian decoder updated with momentum, ensuring a stable reference during training. This teacher provides each block with global guidance in a self-distillation manner, promoting spatial consistency in reconstruction. To further ensure consistency across the blocks, we incorporate block weighting, dynamically adjusting each block's weight according to its reconstruction accuracy. Extensive experiments on large-scale scenes show that our method consistently outperforms existing techniques, achieving a 12.8% improvement in LPIPS over CityGaussian with much fewer divided blocks and establishing a new state of the art. Project page: https://jixuan-fan.github.io/Momentum-GS_Page/", 'score': 10, 'issue_id': 1025, 'pub_date': '2024-12-06', 'pub_date_card': {'ru': '6 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 6', 'zh': '12æœˆ6æ—¥'}, 'hash': 'a52f45b90e5831fe', 'authors': ['Jixuan Fan', 'Wanhua Li', 'Yifei Han', 'Yansong Tang'], 'affiliations': ['Harvard University', 'Tsinghua Shenzhen International Graduate School, Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2412.04887.jpg', 'data': {'categories': ['#training', '#optimization', '#3d'], 'emoji': 'ğŸ™ï¸', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² 3D Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸: Momentum-GS Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Momentum-GS Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ 3D Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑÑ†ĞµĞ½ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ³Ğ°ÑƒÑÑĞ¾Ğ²ÑĞºĞ¾Ğ³Ğ¾ ÑĞ¿Ğ»Ğ°Ñ‚Ñ‚Ğ¸Ğ½Ğ³Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ°Ğ¼Ğ¾Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚Ğ° Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ±Ğ»Ğ¾ĞºĞ°Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒ-Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€ Ğ³Ğ°ÑƒÑÑĞ¸Ğ°Ğ½, Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑĞµĞ¼Ñ‹Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ±Ğ»Ğ¾ĞºĞ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Momentum-GS Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ° 12.8% Ğ¿Ğ¾ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞµ LPIPS Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ CityGaussian Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞµĞ³Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ±Ğ»Ğ¾ĞºĞ¾Ğ².'}, 'en': {'title': 'Enhancing 3D Reconstruction with Momentum-GS', 'desc': 'This paper introduces Momentum-GS, a new method for improving 3D scene reconstruction using Gaussian splatting. It addresses the challenges of high memory usage and limited data diversity in parallelized training by employing momentum-based self-distillation. This technique allows for consistent and accurate reconstruction across multiple blocks, independent of the number of available GPUs. The results show significant improvements in reconstruction quality, outperforming existing methods and setting a new benchmark in the field.'}, 'zh': {'title': 'åŠ¨é‡è‡ªè’¸é¦ï¼Œæå‡3Dé‡å»ºä¸€è‡´æ€§', 'desc': '3Dé«˜æ–¯ç‚¹äº‘æŠ€æœ¯åœ¨å¤§è§„æ¨¡åœºæ™¯é‡å»ºä¸­å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†ä»é¢ä¸´é«˜è®­ç»ƒå†…å­˜æ¶ˆè€—å’Œå­˜å‚¨å¼€é”€çš„é—®é¢˜ã€‚æ··åˆè¡¨ç¤ºæ³•ç»“åˆäº†éšå¼å’Œæ˜¾å¼ç‰¹å¾ï¼Œèƒ½å¤Ÿç¼“è§£è¿™äº›é™åˆ¶ã€‚ç„¶è€Œï¼Œåœ¨å¹¶è¡Œå—è®­ç»ƒä¸­ï¼Œç”±äºæ¯ä¸ªå—ç‹¬ç«‹è®­ç»ƒå¯¼è‡´æ•°æ®å¤šæ ·æ€§é™ä½ï¼Œé‡å»ºç²¾åº¦ä¸‹é™ï¼ŒåŒæ—¶å¹¶è¡Œè®­ç»ƒé™åˆ¶äº†å¯ç”¨GPUæ•°é‡ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Momentum-GSæ–¹æ³•ï¼Œé€šè¿‡åŠ¨é‡è‡ªè’¸é¦ä¿ƒè¿›å—é—´çš„ä¸€è‡´æ€§å’Œå‡†ç¡®æ€§ï¼ŒåŒæ—¶å°†å—çš„æ•°é‡ä¸ç‰©ç†GPUæ•°é‡è§£è€¦ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.05243', 'title': 'CompCap: Improving Multimodal Large Language Models with Composite Captions', 'url': 'https://huggingface.co/papers/2412.05243', 'abstract': "How well can Multimodal Large Language Models (MLLMs) understand composite images? Composite images (CIs) are synthetic visuals created by merging multiple visual elements, such as charts, posters, or screenshots, rather than being captured directly by a camera. While CIs are prevalent in real-world applications, recent MLLM developments have primarily focused on interpreting natural images (NIs). Our research reveals that current MLLMs face significant challenges in accurately understanding CIs, often struggling to extract information or perform complex reasoning based on these images. We find that existing training data for CIs are mostly formatted for question-answer tasks (e.g., in datasets like ChartQA and ScienceQA), while high-quality image-caption datasets, critical for robust vision-language alignment, are only available for NIs. To bridge this gap, we introduce Composite Captions (CompCap), a flexible framework that leverages Large Language Models (LLMs) and automation tools to synthesize CIs with accurate and detailed captions. Using CompCap, we curate CompCap-118K, a dataset containing 118K image-caption pairs across six CI types. We validate the effectiveness of CompCap-118K by supervised fine-tuning MLLMs of three sizes: xGen-MM-inst.-4B and LLaVA-NeXT-Vicuna-7B/13B. Empirical results show that CompCap-118K significantly enhances MLLMs' understanding of CIs, yielding average gains of 1.7%, 2.0%, and 2.9% across eleven benchmarks, respectively.", 'score': 9, 'issue_id': 1027, 'pub_date': '2024-12-06', 'pub_date_card': {'ru': '6 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 6', 'zh': '12æœˆ6æ—¥'}, 'hash': '718a80687afd1e64', 'authors': ['Xiaohui Chen', 'Satya Narayan Shukla', 'Mahmoud Azab', 'Aashu Singh', 'Qifan Wang', 'David Yang', 'ShengYun Peng', 'Hanchao Yu', 'Shen Yan', 'Xuewen Zhang', 'Baosheng He'], 'affiliations': ['Georgia Tech', 'Meta', 'Tufts University'], 'pdf_title_img': 'assets/pdf/title_img/2412.05243.jpg', 'data': {'categories': ['#data', '#dataset', '#optimization', '#synthetic', '#multimodal', '#training'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (MLLM) Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ CompCap - Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ñ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ÑĞ¼Ğ¸. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ CompCap, Ğ¾Ğ½Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ CompCap-118K Ğ¸Ğ· 118 Ñ‚Ñ‹ÑÑÑ‡ Ğ¿Ğ°Ñ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ-Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ. Ğ”Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ MLLM Ğ½Ğ° ÑÑ‚Ğ¾Ğ¼ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ğ»Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¿Ğ¾Ğ²Ñ‹ÑĞ¸Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° 1.7-2.9% Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ….'}, 'en': {'title': 'Enhancing MLLM Understanding of Composite Images with CompCap', 'desc': "This paper investigates how well Multimodal Large Language Models (MLLMs) can interpret composite images, which are created by combining various visual elements. The authors highlight that MLLMs have primarily been trained on natural images, leading to difficulties in understanding composite images due to a lack of suitable training data. To address this issue, they propose a new framework called Composite Captions (CompCap) that generates detailed captions for composite images, thus improving the training process. The study introduces a dataset, CompCap-118K, which significantly enhances MLLMs' performance on composite image understanding tasks, demonstrating measurable improvements across multiple benchmarks."}, 'zh': {'title': 'æå‡å¤šæ¨¡æ€æ¨¡å‹å¯¹å¤åˆå›¾åƒçš„ç†è§£èƒ½åŠ›', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å¯¹å¤åˆå›¾åƒï¼ˆCIsï¼‰çš„ç†è§£èƒ½åŠ›ã€‚å¤åˆå›¾åƒæ˜¯é€šè¿‡åˆæˆå¤šä¸ªè§†è§‰å…ƒç´ è€Œåˆ›å»ºçš„å›¾åƒï¼Œå½“å‰çš„MLLMsåœ¨ç†è§£è¿™äº›å›¾åƒæ—¶é¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†å¤åˆæ ‡é¢˜ï¼ˆCompCapï¼‰æ¡†æ¶ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å’Œè‡ªåŠ¨åŒ–å·¥å…·ç”Ÿæˆå‡†ç¡®çš„å›¾åƒæ ‡é¢˜ï¼Œå¹¶åˆ›å»ºäº†åŒ…å«118Kå›¾åƒ-æ ‡é¢˜å¯¹çš„æ•°æ®é›†CompCap-118Kã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCompCap-118Kæ˜¾è‘—æå‡äº†MLLMså¯¹å¤åˆå›¾åƒçš„ç†è§£èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.05263', 'title': 'Mind the Time: Temporally-Controlled Multi-Event Video Generation', 'url': 'https://huggingface.co/papers/2412.05263', 'abstract': 'Real-world videos consist of sequences of events. Generating such sequences with precise temporal control is infeasible with existing video generators that rely on a single paragraph of text as input. When tasked with generating multiple events described using a single prompt, such methods often ignore some of the events or fail to arrange them in the correct order. To address this limitation, we present MinT, a multi-event video generator with temporal control. Our key insight is to bind each event to a specific period in the generated video, which allows the model to focus on one event at a time. To enable time-aware interactions between event captions and video tokens, we design a time-based positional encoding method, dubbed ReRoPE. This encoding helps to guide the cross-attention operation. By fine-tuning a pre-trained video diffusion transformer on temporally grounded data, our approach produces coherent videos with smoothly connected events. For the first time in the literature, our model offers control over the timing of events in generated videos. Extensive experiments demonstrate that MinT outperforms existing open-source models by a large margin.', 'score': 6, 'issue_id': 1014, 'pub_date': '2024-12-06', 'pub_date_card': {'ru': '6 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 6', 'zh': '12æœˆ6æ—¥'}, 'hash': '826bb588770d5c27', 'authors': ['Ziyi Wu', 'Aliaksandr Siarohin', 'Willi Menapace', 'Ivan Skorokhodov', 'Yuwei Fang', 'Varnith Chordia', 'Igor Gilitschenski', 'Sergey Tulyakov'], 'affiliations': ['Snap Research', 'University of Toronto', 'Vector Institute'], 'pdf_title_img': 'assets/pdf/title_img/2412.05263.jpg', 'data': {'categories': ['#open_source', '#training', '#architecture', '#diffusion', '#video', '#multimodal'], 'emoji': 'â±ï¸', 'ru': {'title': 'Ğ¢Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸ÑĞ¼Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MinT - Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾Ğ¼ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·Ñ‹Ğ²Ğ°ĞµÑ‚ ĞºĞ°Ğ¶Ğ´Ğ¾Ğµ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğµ Ğº Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¿ĞµÑ€Ğ¸Ğ¾Ğ´Ñƒ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¼ Ğ²Ğ¸Ğ´ĞµĞ¾, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¼ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¸ Ğ·Ğ° Ñ€Ğ°Ğ·. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ReRoPE Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ĞµĞ¼ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ÑĞ¼Ğ¸ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ, Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° Ñ‚ĞµĞ¼Ğ¿Ğ¾Ñ€Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ»Ğ°Ğ²Ğ½Ğ¾ ÑĞ¾ĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸ÑĞ¼Ğ¸, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'MinT: Mastering Multi-Event Video Generation with Temporal Precision', 'desc': "This paper introduces MinT, a novel multi-event video generator that allows for precise temporal control over the events depicted in generated videos. Unlike traditional models that struggle with sequencing multiple events from a single text prompt, MinT binds each event to a specific time period, ensuring that all events are accurately represented and ordered. The authors implement a time-based positional encoding method called ReRoPE, which enhances the model's ability to manage interactions between event descriptions and video frames. By fine-tuning a pre-trained video diffusion transformer on data with temporal grounding, MinT achieves superior performance in generating coherent videos with well-timed events."}, 'zh': {'title': 'MinTï¼šç²¾å‡†æ§åˆ¶è§†é¢‘äº‹ä»¶æ—¶åºçš„ç”Ÿæˆå™¨', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºMinTçš„å¤šäº‹ä»¶è§†é¢‘ç”Ÿæˆå™¨ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰è§†é¢‘ç”Ÿæˆå™¨åœ¨ç”Ÿæˆå¤šä¸ªäº‹ä»¶æ—¶çš„æ—¶é—´æ§åˆ¶é—®é¢˜ã€‚é€šè¿‡å°†æ¯ä¸ªäº‹ä»¶ç»‘å®šåˆ°ç”Ÿæˆè§†é¢‘çš„ç‰¹å®šæ—¶é—´æ®µï¼ŒMinTèƒ½å¤Ÿé€ä¸ªå…³æ³¨äº‹ä»¶ï¼Œä»è€Œæé«˜ç”Ÿæˆè§†é¢‘çš„è¿è´¯æ€§ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ç§åŸºäºæ—¶é—´çš„ä½ç½®ä¿¡æ¯ç¼–ç æ–¹æ³•ReRoPEï¼Œä»¥å¢å¼ºäº‹ä»¶æè¿°ä¸è§†é¢‘å¸§ä¹‹é—´çš„äº¤äº’ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMinTåœ¨ç”Ÿæˆè§†é¢‘çš„æ—¶é—´æ§åˆ¶æ–¹é¢ä¼˜äºç°æœ‰çš„å¼€æºæ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.03428', 'title': '2DGS-Room: Seed-Guided 2D Gaussian Splatting with Geometric Constrains for High-Fidelity Indoor Scene Reconstruction', 'url': 'https://huggingface.co/papers/2412.03428', 'abstract': 'The reconstruction of indoor scenes remains challenging due to the inherent complexity of spatial structures and the prevalence of textureless regions. Recent advancements in 3D Gaussian Splatting have improved novel view synthesis with accelerated processing but have yet to deliver comparable performance in surface reconstruction. In this paper, we introduce 2DGS-Room, a novel method leveraging 2D Gaussian Splatting for high-fidelity indoor scene reconstruction. Specifically, we employ a seed-guided mechanism to control the distribution of 2D Gaussians, with the density of seed points dynamically optimized through adaptive growth and pruning mechanisms. To further improve geometric accuracy, we incorporate monocular depth and normal priors to provide constraints for details and textureless regions respectively. Additionally, multi-view consistency constraints are employed to mitigate artifacts and further enhance reconstruction quality. Extensive experiments on ScanNet and ScanNet++ datasets demonstrate that our method achieves state-of-the-art performance in indoor scene reconstruction.', 'score': 5, 'issue_id': 1013, 'pub_date': '2024-12-04', 'pub_date_card': {'ru': '4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 4', 'zh': '12æœˆ4æ—¥'}, 'hash': 'ead3f67b9be4d52b', 'authors': ['Wanting Zhang', 'Haodong Xiang', 'Zhichao Liao', 'Xiansong Lai', 'Xinghui Li', 'Long Zeng'], 'affiliations': ['Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2412.03428.jpg', 'data': {'categories': ['#3d'], 'emoji': 'ğŸ ', 'ru': {'title': '2DGS-Room: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€ÑŒĞµÑ€Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ 2D Ğ³Ğ°ÑƒÑÑĞ¾Ğ²ÑĞºĞ¾Ğ³Ğ¾ ÑĞ¿Ğ»Ğ°Ñ‚Ñ‚Ğ¸Ğ½Ğ³Ğ°', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ 2DGS-Room Ğ´Ğ»Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€ÑŒĞµÑ€Ğ¾Ğ² Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ 2D Ğ³Ğ°ÑƒÑÑĞ¾Ğ²ÑĞºĞ¾Ğ³Ğ¾ ÑĞ¿Ğ»Ğ°Ñ‚Ñ‚Ğ¸Ğ½Ğ³Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸ĞµĞ¼ 2D Ğ³Ğ°ÑƒÑÑĞ¸Ğ°Ğ½Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞµĞ¼ÑĞ½, Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ Ğ¸Ñ… Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ñ€Ğ¾ÑÑ‚ Ğ¸ Ğ¾Ñ‚ÑĞµÑ‡ĞµĞ½Ğ¸Ğµ. Ğ”Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ÑÑ Ğ¼Ğ¾Ğ½Ğ¾ĞºÑƒĞ»ÑÑ€Ğ½Ñ‹Ğµ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ¸ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ñ‹, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ScanNet Ğ¸ ScanNet++ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€ÑŒĞµÑ€Ğ¾Ğ².'}, 'en': {'title': 'Revolutionizing Indoor Scene Reconstruction with 2D Gaussian Splatting', 'desc': 'This paper presents 2DGS-Room, a new approach for reconstructing indoor scenes using 2D Gaussian Splatting. The method introduces a seed-guided mechanism that optimizes the distribution of 2D Gaussians, enhancing the reconstruction process. By incorporating monocular depth and normal priors, the approach improves geometric accuracy, especially in areas lacking texture. The use of multi-view consistency constraints helps reduce artifacts, leading to high-fidelity results in indoor scene reconstruction, as demonstrated by experiments on ScanNet and ScanNet++ datasets.'}, 'zh': {'title': 'é«˜ä¿çœŸå®¤å†…åœºæ™¯é‡å»ºçš„æ–°æ–¹æ³•', 'desc': 'å®¤å†…åœºæ™¯çš„é‡å»ºå› ç©ºé—´ç»“æ„å¤æ‚å’Œæ— çº¹ç†åŒºåŸŸçš„æ™®éå­˜åœ¨è€Œå…·æœ‰æŒ‘æˆ˜æ€§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•2DGS-Roomï¼Œåˆ©ç”¨2Dé«˜æ–¯ç‚¹äº‘å®ç°é«˜ä¿çœŸåº¦çš„å®¤å†…åœºæ™¯é‡å»ºã€‚æˆ‘ä»¬é‡‡ç”¨ç§å­å¼•å¯¼æœºåˆ¶æ¥æ§åˆ¶2Dé«˜æ–¯çš„åˆ†å¸ƒï¼Œå¹¶é€šè¿‡è‡ªé€‚åº”ç”Ÿé•¿å’Œä¿®å‰ªæœºåˆ¶åŠ¨æ€ä¼˜åŒ–ç§å­ç‚¹çš„å¯†åº¦ã€‚é€šè¿‡ç»“åˆå•ç›®æ·±åº¦å’Œæ³•çº¿å…ˆéªŒï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æé«˜äº†å‡ ä½•ç²¾åº¦ï¼Œå¹¶ä½¿ç”¨å¤šè§†å›¾ä¸€è‡´æ€§çº¦æŸæ¥å‡å°‘ä¼ªå½±ï¼Œæå‡é‡å»ºè´¨é‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.04827', 'title': 'PanoDreamer: 3D Panorama Synthesis from a Single Image', 'url': 'https://huggingface.co/papers/2412.04827', 'abstract': 'In this paper, we present PanoDreamer, a novel method for producing a coherent 360^circ 3D scene from a single input image. Unlike existing methods that generate the scene sequentially, we frame the problem as single-image panorama and depth estimation. Once the coherent panoramic image and its corresponding depth are obtained, the scene can be reconstructed by inpainting the small occluded regions and projecting them into 3D space. Our key contribution is formulating single-image panorama and depth estimation as two optimization tasks and introducing alternating minimization strategies to effectively solve their objectives. We demonstrate that our approach outperforms existing techniques in single-image 360^circ scene reconstruction in terms of consistency and overall quality.', 'score': 3, 'issue_id': 1026, 'pub_date': '2024-12-06', 'pub_date_card': {'ru': '6 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 6', 'zh': '12æœˆ6æ—¥'}, 'hash': 'db76a410fcf3c53c', 'authors': ['Avinash Paliwal', 'Xilong Zhou', 'Andrii Tsarov', 'Nima Khademi Kalantari'], 'affiliations': ['Leia Inc.', 'Max Planck Institute for Informatics', 'Texas A&M University'], 'pdf_title_img': 'assets/pdf/title_img/2412.04827.jpg', 'data': {'categories': ['#cv', '#optimization', '#3d'], 'emoji': 'ğŸŒ', 'ru': {'title': 'PanoDreamer: Ğ¦ĞµĞ»Ğ¾ÑÑ‚Ğ½Ğ°Ñ 360Â° 3D-ÑÑ†ĞµĞ½Ğ° Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ', 'desc': 'PanoDreamer - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ†ĞµĞ»Ğ¾ÑÑ‚Ğ½Ğ¾Ğ¹ 360-Ğ³Ñ€Ğ°Ğ´ÑƒÑĞ½Ğ¾Ğ¹ 3D-ÑÑ†ĞµĞ½Ñ‹ Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ñ… ÑÑ†ĞµĞ½Ñƒ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾, Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ ĞºĞ°Ğº Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ¿Ğ°Ğ½Ğ¾Ñ€Ğ°Ğ¼Ñ‹ Ğ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ¿Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ñƒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ²ĞºĞ»Ğ°Ğ´ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²ĞºĞµ ÑÑ‚Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ĞºĞ°Ğº Ğ´Ğ²ÑƒÑ… Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ²Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ Ğ¿Ğ¾Ğ¾Ñ‡ĞµÑ€ĞµĞ´Ğ½Ğ¾Ğ¹ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¸Ñ… ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ 360-Ğ³Ñ€Ğ°Ğ´ÑƒÑĞ½Ñ‹Ñ… ÑÑ†ĞµĞ½ Ğ¿Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ñƒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ñ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°.'}, 'en': {'title': 'Revolutionizing 360-Degree Scene Reconstruction from a Single Image', 'desc': 'PanoDreamer is a new method that creates a complete 360-degree 3D scene from just one image. Instead of building the scene step by step, it treats the task as estimating a panoramic view and its depth simultaneously. The method fills in missing parts of the image and converts them into 3D space. By using optimization techniques, PanoDreamer achieves better consistency and quality compared to previous methods for 360-degree scene reconstruction.'}, 'zh': {'title': 'å•å›¾ç”Ÿæˆ360åº¦3Dåœºæ™¯çš„æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•PanoDreamerï¼Œå¯ä»¥ä»å•å¼ è¾“å…¥å›¾åƒç”Ÿæˆä¸€è‡´çš„360åº¦3Dåœºæ™¯ã€‚ä¸ç°æœ‰æ–¹æ³•æŒ‰é¡ºåºç”Ÿæˆåœºæ™¯ä¸åŒï¼Œæˆ‘ä»¬å°†é—®é¢˜æ¡†å®šä¸ºå•å›¾å…¨æ™¯å’Œæ·±åº¦ä¼°è®¡ã€‚é€šè¿‡è·å–ä¸€è‡´çš„å…¨æ™¯å›¾åƒåŠå…¶å¯¹åº”çš„æ·±åº¦ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ä¿®å¤å°çš„é®æŒ¡åŒºåŸŸå¹¶å°†å…¶æŠ•å½±åˆ°3Dç©ºé—´ä¸­æ¥é‡å»ºåœºæ™¯ã€‚æˆ‘ä»¬çš„ä¸»è¦è´¡çŒ®æ˜¯å°†å•å›¾å…¨æ™¯å’Œæ·±åº¦ä¼°è®¡å½¢å¼åŒ–ä¸ºä¸¤ä¸ªä¼˜åŒ–ä»»åŠ¡ï¼Œå¹¶å¼•å…¥äº¤æ›¿æœ€å°åŒ–ç­–ç•¥æ¥æœ‰æ•ˆè§£å†³è¿™äº›ç›®æ ‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.04905', 'title': 'DEMO: Reframing Dialogue Interaction with Fine-grained Element Modeling', 'url': 'https://huggingface.co/papers/2412.04905', 'abstract': 'Large language models (LLMs) have made dialogue one of the central modes of human-machine interaction, leading to the accumulation of vast amounts of conversation logs and increasing demand for dialogue generation. A conversational life-cycle spans from the Prelude through the Interlocution to the Epilogue, encompassing various elements. Despite the existence of numerous dialogue-related studies, there is a lack of benchmarks that encompass comprehensive dialogue elements, hindering precise modeling and systematic evaluation. To bridge this gap, we introduce an innovative research task Dialogue Element MOdeling, including Element Awareness and Dialogue Agent Interaction, and propose a novel benchmark, DEMO, designed for a comprehensive dialogue modeling and assessment. Inspired by imitation learning, we further build the agent which possesses the adept ability to model dialogue elements based on the DEMO benchmark. Extensive experiments indicate that existing LLMs still exhibit considerable potential for enhancement, and our DEMO agent has superior performance in both in-domain and out-of-domain tasks.', 'score': 3, 'issue_id': 1017, 'pub_date': '2024-12-06', 'pub_date_card': {'ru': '6 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 6', 'zh': '12æœˆ6æ—¥'}, 'hash': '4b12d357252b671f', 'authors': ['Minzheng Wang', 'Xinghua Zhang', 'Kun Chen', 'Nan Xu', 'Haiyang Yu', 'Fei Huang', 'Wenji Mao', 'Yongbin Li'], 'affiliations': ['MAIS, Institute of Automation, Chinese Academy of Sciences', 'School of Artificial Intelligence, University of Chinese Academy of Sciences', 'Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2412.04905.jpg', 'data': {'categories': ['#benchmark', '#dialogue_generation', '#agents'], 'emoji': 'ğŸ—£ï¸', 'ru': {'title': 'DEMO: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ´Ğ»Ñ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼', 'desc': "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ 'ĞœĞ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°' Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº DEMO. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ‚Ğ¼ĞµÑ‡Ğ°ÑÑ‚ Ğ½ĞµÑ…Ğ²Ğ°Ñ‚ĞºÑƒ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ğ²ÑĞµ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ñ‹ Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ°. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ñ‹ Ğ¸Ğ¼Ğ¸Ñ‚Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ DEMO, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾Ğ³Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ğ²ÑĞµ ĞµÑ‰Ğµ Ğ¸Ğ¼ĞµÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸."}, 'en': {'title': 'Enhancing Dialogue Generation with Comprehensive Element Modeling', 'desc': 'This paper addresses the growing need for effective dialogue generation in human-machine interactions, particularly with large language models (LLMs). It identifies a gap in existing research due to the lack of comprehensive benchmarks that cover all aspects of dialogue, which hampers accurate modeling and evaluation. To tackle this issue, the authors introduce a new task called Dialogue Element MOdeling (DEMO), which focuses on understanding dialogue elements and how agents interact within conversations. Their experiments show that while current LLMs have room for improvement, the proposed DEMO agent outperforms them in various tasks, demonstrating its effectiveness in dialogue modeling.'}, 'zh': {'title': 'å…¨é¢å¯¹è¯å»ºæ¨¡çš„æ–°åŸºå‡†DEMO', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä½¿å¯¹è¯æˆä¸ºäººæœºäº¤äº’çš„ä¸»è¦æ–¹å¼ï¼Œå¯¼è‡´å¤§é‡å¯¹è¯æ—¥å¿—çš„ç§¯ç´¯å’Œå¯¹å¯¹è¯ç”Ÿæˆçš„éœ€æ±‚å¢åŠ ã€‚å¯¹è¯çš„ç”Ÿå‘½å‘¨æœŸåŒ…æ‹¬å‰å¥ã€å¯¹è¯å’Œç»“å°¾ï¼Œæ¶µç›–äº†å¤šä¸ªå…ƒç´ ã€‚å°½ç®¡å·²æœ‰è®¸å¤šä¸å¯¹è¯ç›¸å…³çš„ç ”ç©¶ï¼Œä½†ç¼ºä¹å…¨é¢çš„åŸºå‡†ï¼Œé™åˆ¶äº†ç²¾ç¡®å»ºæ¨¡å’Œç³»ç»Ÿè¯„ä¼°ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å¯¹è¯å…ƒç´ å»ºæ¨¡çš„æ–°ä»»åŠ¡ï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªæ–°çš„åŸºå‡†DEMOï¼Œä»¥ä¾¿è¿›è¡Œå…¨é¢çš„å¯¹è¯å»ºæ¨¡å’Œè¯„ä¼°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.04626', 'title': 'BigDocs: An Open and Permissively-Licensed Dataset for Training Multimodal Models on Document and Code Tasks', 'url': 'https://huggingface.co/papers/2412.04626', 'abstract': 'Multimodal AI has the potential to significantly enhance document-understanding tasks, such as processing receipts, understanding workflows, extracting data from documents, and summarizing reports. Code generation tasks that require long-structured outputs can also be enhanced by multimodality. Despite this, their use in commercial applications is often limited due to limited access to training data and restrictive licensing, which hinders open access. To address these limitations, we introduce BigDocs-7.5M, a high-quality, open-access dataset comprising 7.5 million multimodal documents across 30 tasks. We use an efficient data curation process to ensure our data is high-quality and license-permissive. Our process emphasizes accountability, responsibility, and transparency through filtering rules, traceable metadata, and careful content analysis. Additionally, we introduce BigDocs-Bench, a benchmark suite with 10 novel tasks where we create datasets that reflect real-world use cases involving reasoning over Graphical User Interfaces (GUI) and code generation from images. Our experiments show that training with BigDocs-Bench improves average performance up to 25.8% over closed-source GPT-4o in document reasoning and structured output tasks such as Screenshot2HTML or Image2Latex generation. Finally, human evaluations showed a preference for outputs from models trained on BigDocs over GPT-4o. This suggests that BigDocs can help both academics and the open-source community utilize and improve AI tools to enhance multimodal capabilities and document reasoning. The project is hosted at https://bigdocs.github.io .', 'score': 1, 'issue_id': 1030, 'pub_date': '2024-12-05', 'pub_date_card': {'ru': '5 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 5', 'zh': '12æœˆ5æ—¥'}, 'hash': '32747c6004865ffe', 'authors': ['Juan Rodriguez', 'Xiangru Jian', 'Siba Smarak Panigrahi', 'Tianyu Zhang', 'Aarash Feizi', 'Abhay Puri', 'Akshay Kalkunte', 'FranÃ§ois Savard', 'Ahmed Masry', 'Shravan Nayak', 'Rabiul Awal', 'Mahsa Massoud', 'Amirhossein Abaskohi', 'Zichao Li', 'Suyuchen Wang', 'Pierre-AndrÃ© NoÃ«l', 'Mats Leon Richter', 'Saverio Vadacchino', 'Shubbam Agarwal', 'Sanket Biswas', 'Sara Shanian', 'Ying Zhang', 'Noah Bolger', 'Kurt MacDonald', 'Simon Fauvel', 'Sathwik Tejaswi', 'Srinivas Sunkara', 'Joao Monteiro', 'Krishnamurthy DJ Dvijotham', 'Torsten Scholak', 'Nicolas Chapados', 'Sepideh Kharagani', 'Sean Hughes', 'M. Ã–zsu', 'Siva Reddy', 'Marco Pedersoli', 'Yoshua Bengio', 'Christopher Pal', 'Issam Laradji', 'Spandanna Gella', 'Perouz Taslakian', 'David Vazquez', 'Sai Rajeswar'], 'affiliations': ['CIFAR AI Chair', 'McGill University', 'Mila', 'Polytechnique MontrÃ©al', 'ServiceNow', 'Universitat AutÃ²noma de Barcelona', 'University of British Columbia', 'University of Waterloo', 'UniversitÃ© de MontrÃ©al', 'York University', 'Ã‰cole de Technologie SupÃ©rieure'], 'pdf_title_img': 'assets/pdf/title_img/2412.04626.jpg', 'data': {'categories': ['#data', '#open_source', '#reasoning', '#multimodal', '#games', '#graphs', '#dataset', '#benchmark'], 'emoji': 'ğŸ“„', 'ru': {'title': 'BigDocs: ĞÑ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ¿ÑƒÑ‚ÑŒ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ BigDocs-7.5M - ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğ¹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸Ğ· 7,5 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ 30 Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ BigDocs-Bench - Ğ½Ğ°Ğ±Ğ¾Ñ€ Ñ‚ĞµÑÑ‚Ğ¾Ğ² Ñ 10 Ğ½Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ GUI Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ° Ğ¸Ğ· Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° BigDocs-Bench ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑÑ€ĞµĞ´Ğ½ÑÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ¾ 25,8% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ GPT-4o Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼ Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. Ğ§ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ½Ğ° BigDocs, Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ GPT-4o.'}, 'en': {'title': 'Unlocking Multimodal AI with BigDocs-7.5M', 'desc': 'This paper presents BigDocs-7.5M, a large open-access dataset designed to improve multimodal AI applications in document understanding and code generation. The dataset includes 7.5 million multimodal documents across 30 tasks, ensuring high quality and permissive licensing through a careful curation process. Additionally, the authors introduce BigDocs-Bench, a benchmark suite with 10 new tasks that simulate real-world scenarios involving reasoning over graphical user interfaces and generating code from images. Experiments demonstrate that models trained on this dataset outperform existing closed-source models, indicating its potential to advance both academic research and open-source AI development.'}, 'zh': {'title': 'å¤šæ¨¡æ€AIåŠ©åŠ›æ–‡æ¡£ç†è§£çš„æœªæ¥', 'desc': 'å¤šæ¨¡æ€äººå·¥æ™ºèƒ½å¯ä»¥æ˜¾è‘—æå‡æ–‡æ¡£ç†è§£ä»»åŠ¡çš„æ•ˆæœï¼Œä¾‹å¦‚å¤„ç†æ”¶æ®ã€ç†è§£å·¥ä½œæµç¨‹ã€ä»æ–‡æ¡£ä¸­æå–æ•°æ®å’Œæ€»ç»“æŠ¥å‘Šã€‚æˆ‘ä»¬æå‡ºçš„BigDocs-7.5Mæ˜¯ä¸€ä¸ªé«˜è´¨é‡ã€å¼€æ”¾è®¿é—®çš„æ•°æ®é›†ï¼ŒåŒ…å«750ä¸‡ä»½å¤šæ¨¡æ€æ–‡æ¡£ï¼Œæ¶µç›–30ä¸ªä»»åŠ¡ã€‚é€šè¿‡é«˜æ•ˆçš„æ•°æ®æ•´ç†è¿‡ç¨‹ï¼Œæˆ‘ä»¬ç¡®ä¿æ•°æ®çš„é«˜è´¨é‡å’Œè®¸å¯å‹å¥½æ€§ï¼Œå¹¶å¼ºè°ƒè´£ä»»å’Œé€æ˜åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨BigDocs-Benchè¿›è¡Œè®­ç»ƒå¯ä»¥åœ¨æ–‡æ¡£æ¨ç†å’Œç»“æ„åŒ–è¾“å‡ºä»»åŠ¡ä¸Šæé«˜å¹³å‡æ€§èƒ½ï¼Œå¸®åŠ©å­¦æœ¯ç•Œå’Œå¼€æºç¤¾åŒºæ›´å¥½åœ°åˆ©ç”¨å’Œæ”¹è¿›AIå·¥å…·ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.03555', 'title': 'PaliGemma 2: A Family of Versatile VLMs for Transfer', 'url': 'https://huggingface.co/papers/2412.03555', 'abstract': 'PaliGemma 2 is an upgrade of the PaliGemma open Vision-Language Model (VLM) based on the Gemma 2 family of language models. We combine the SigLIP-So400m vision encoder that was also used by PaliGemma with the whole range of Gemma 2 models, from the 2B one all the way up to the 27B model. We train these models at three resolutions (224px, 448px, and 896px) in multiple stages to equip them with broad knowledge for transfer via fine-tuning. The resulting family of base models covering different model sizes and resolutions allows us to investigate factors impacting transfer performance (such as learning rate) and to analyze the interplay between the type of task, model size, and resolution. We further increase the number and breadth of transfer tasks beyond the scope of PaliGemma including different OCR-related tasks such as table structure recognition, molecular structure recognition, music score recognition, as well as long fine-grained captioning and radiography report generation, on which PaliGemma 2 obtains state-of-the-art results.', 'score': 55, 'issue_id': 964, 'pub_date': '2024-12-04', 'pub_date_card': {'ru': '4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 4', 'zh': '12æœˆ4æ—¥'}, 'hash': '12d0d9bcc8060099', 'authors': ['Andreas Steiner', 'AndrÃ© Susano Pinto', 'Michael Tschannen', 'Daniel Keysers', 'Xiao Wang', 'Yonatan Bitton', 'Alexey Gritsenko', 'Matthias Minderer', 'Anthony Sherbondy', 'Shangbang Long', 'Siyang Qin', 'Reeve Ingle', 'Emanuele Bugliarello', 'Sahar Kazemzadeh', 'Thomas Mesnard', 'Ibrahim Alabdulmohsin', 'Lucas Beyer', 'Xiaohua Zhai'], 'affiliations': ['Google DeepMind'], 'pdf_title_img': 'assets/pdf/title_img/2412.03555.jpg', 'data': {'categories': ['#multimodal', '#optimization', '#training', '#cv', '#transfer_learning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'PaliGemma 2: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜', 'desc': 'PaliGemma 2 - ÑÑ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ°Ñ Ğ²ĞµÑ€ÑĞ¸Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ PaliGemma, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Gemma 2. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ SigLIP-So400m Ñ Ñ€ÑĞ´Ğ¾Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Gemma 2 Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ², Ğ¾Ñ‚ 2B Ğ´Ğ¾ 27B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ğ»Ğ¾ÑÑŒ Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ… Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ (224px, 448px Ğ¸ 896px) Ğ² Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ÑÑ‚Ğ°Ğ¿Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ¾Ğ±Ñ€ĞµÑ‚ĞµĞ½Ğ¸Ñ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. PaliGemma 2 Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†, Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€, Ğ½Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ€Ğ°Ğ´Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ¾Ğ².'}, 'en': {'title': 'PaliGemma 2: Advancing Vision-Language Understanding', 'desc': 'PaliGemma 2 is an enhanced Vision-Language Model (VLM) that builds on the original PaliGemma framework by integrating the SigLIP-So400m vision encoder with various sizes of the Gemma 2 language models. The models are trained at three different image resolutions to improve their ability to transfer knowledge through fine-tuning. This upgrade allows researchers to explore how different factors, like learning rates and model sizes, affect performance on various tasks. PaliGemma 2 also expands its capabilities to include a wider range of tasks, achieving state-of-the-art results in areas such as optical character recognition and detailed captioning.'}, 'zh': {'title': 'PaliGemma 2ï¼šè§†è§‰ä¸è¯­è¨€çš„å®Œç¾ç»“åˆ', 'desc': 'PaliGemma 2 æ˜¯åŸºäº Gemma 2 è¯­è¨€æ¨¡å‹å®¶æ—çš„ PaliGemma å¼€æ”¾è§†è§‰è¯­è¨€æ¨¡å‹çš„å‡çº§ç‰ˆã€‚æˆ‘ä»¬ç»“åˆäº† SigLIP-So400m è§†è§‰ç¼–ç å™¨å’Œä¸åŒè§„æ¨¡çš„ Gemma 2 æ¨¡å‹ï¼Œè¿›è¡Œå¤šé˜¶æ®µè®­ç»ƒï¼Œä»¥æé«˜æ¨¡å‹çš„çŸ¥è¯†è¿ç§»èƒ½åŠ›ã€‚é€šè¿‡åœ¨ä¸‰ç§åˆ†è¾¨ç‡ä¸‹è®­ç»ƒï¼Œæˆ‘ä»¬èƒ½å¤Ÿç ”ç©¶å½±å“è¿ç§»æ€§èƒ½çš„å› ç´ ï¼Œå¦‚å­¦ä¹ ç‡ï¼Œå¹¶åˆ†æä»»åŠ¡ç±»å‹ã€æ¨¡å‹å¤§å°å’Œåˆ†è¾¨ç‡ä¹‹é—´çš„å…³ç³»ã€‚PaliGemma 2 æ‰©å±•äº†è¿ç§»ä»»åŠ¡çš„æ•°é‡å’ŒèŒƒå›´ï¼Œæ¶µç›–äº†å¤šç§å…‰å­¦å­—ç¬¦è¯†åˆ«ç›¸å…³ä»»åŠ¡ï¼Œå¹¶åœ¨è¿™äº›ä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.02687', 'title': 'SNOOPI: Supercharged One-step Diffusion Distillation with Proper Guidance', 'url': 'https://huggingface.co/papers/2412.02687', 'abstract': "Recent approaches have yielded promising results in distilling multi-step text-to-image diffusion models into one-step ones. The state-of-the-art efficient distillation technique, i.e., SwiftBrushv2 (SBv2), even surpasses the teacher model's performance with limited resources. However, our study reveals its instability when handling different diffusion model backbones due to using a fixed guidance scale within the Variational Score Distillation (VSD) loss. Another weakness of the existing one-step diffusion models is the missing support for negative prompt guidance, which is crucial in practical image generation. This paper presents SNOOPI, a novel framework designed to address these limitations by enhancing the guidance in one-step diffusion models during both training and inference. First, we effectively enhance training stability through Proper Guidance-SwiftBrush (PG-SB), which employs a random-scale classifier-free guidance approach. By varying the guidance scale of both teacher models, we broaden their output distributions, resulting in a more robust VSD loss that enables SB to perform effectively across diverse backbones while maintaining competitive performance. Second, we propose a training-free method called Negative-Away Steer Attention (NASA), which integrates negative prompts into one-step diffusion models via cross-attention to suppress undesired elements in generated images. Our experimental results show that our proposed methods significantly improve baseline models across various metrics. Remarkably, we achieve an HPSv2 score of 31.08, setting a new state-of-the-art benchmark for one-step diffusion models.", 'score': 55, 'issue_id': 961, 'pub_date': '2024-12-03', 'pub_date_card': {'ru': '3 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 3', 'zh': '12æœˆ3æ—¥'}, 'hash': 'd766bad745d5f322', 'authors': ['Viet Nguyen', 'Anh Nguyen', 'Trung Dao', 'Khoi Nguyen', 'Cuong Pham', 'Toan Tran', 'Anh Tran'], 'affiliations': ['Posts & Telecom. Inst. of Tech.', 'VinAI Research'], 'pdf_title_img': 'assets/pdf/title_img/2412.02687.jpg', 'data': {'categories': ['#cv', '#dataset', '#optimization', '#inference', '#training', '#benchmark', '#diffusion'], 'emoji': 'ğŸ¨', 'ru': {'title': 'ĞŸĞ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ³Ğ¸Ğ±ĞºĞ¾ÑÑ‚Ğ¸ Ğ¾Ğ´Ğ½Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ SNOOPI - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ´Ğ½Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ PG-SB Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ° Ğ±ĞµÑĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ°. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ NASA Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½ĞµĞ³Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· ĞºÑ€Ğ¾ÑÑ-Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ€ĞµĞºĞ¾Ñ€Ğ´Ğ° HPSv2 Ğ² 31.08 Ğ´Ğ»Ñ Ğ¾Ğ´Ğ½Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'SNOOPI: Enhancing One-Step Diffusion Models with Robust Guidance', 'desc': 'This paper introduces SNOOPI, a new framework that improves one-step text-to-image diffusion models by addressing issues with guidance stability and negative prompt support. The authors enhance training stability using Proper Guidance-SwiftBrush (PG-SB), which applies a random-scale classifier-free guidance method to diversify output distributions. Additionally, they present Negative-Away Steer Attention (NASA), a training-free technique that incorporates negative prompts to eliminate unwanted elements in generated images. Experimental results demonstrate that SNOOPI outperforms existing models, achieving a new state-of-the-art HPSv2 score of 31.08.'}, 'zh': {'title': 'SNOOPIï¼šæå‡ä¸€æ­¥æ‰©æ•£æ¨¡å‹çš„ç¨³å®šæ€§ä¸ç”Ÿæˆè´¨é‡', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°æ¡†æ¶SNOOPIï¼Œæ—¨åœ¨è§£å†³ç°æœ‰ä¸€æ­¥æ‰©æ•£æ¨¡å‹çš„å±€é™æ€§ã€‚æˆ‘ä»¬é€šè¿‡Proper Guidance-SwiftBrush (PG-SB)æ–¹æ³•å¢å¼ºäº†è®­ç»ƒçš„ç¨³å®šæ€§ï¼Œé‡‡ç”¨éšæœºå°ºåº¦çš„æ— åˆ†ç±»å™¨å¼•å¯¼ç­–ç•¥ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§æ— è®­ç»ƒçš„æ–¹æ³•Negative-Away Steer Attention (NASA)ï¼Œé€šè¿‡äº¤å‰æ³¨æ„åŠ›å°†è´Ÿæç¤ºé›†æˆåˆ°ä¸€æ­¥æ‰©æ•£æ¨¡å‹ä¸­ï¼Œä»¥æŠ‘åˆ¶ç”Ÿæˆå›¾åƒä¸­çš„ä¸å¿…è¦å…ƒç´ ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šæ˜¾è‘—æé«˜äº†åŸºçº¿æ¨¡å‹çš„æ€§èƒ½ï¼Œåˆ›é€ äº†ä¸€æ­¥æ‰©æ•£æ¨¡å‹çš„æ–°æ ‡æ†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.03552', 'title': 'Imagine360: Immersive 360 Video Generation from Perspective Anchor', 'url': 'https://huggingface.co/papers/2412.03552', 'abstract': '360^circ videos offer a hyper-immersive experience that allows the viewers to explore a dynamic scene from full 360 degrees. To achieve more user-friendly and personalized content creation in 360^circ video format, we seek to lift standard perspective videos into 360^circ equirectangular videos. To this end, we introduce Imagine360, the first perspective-to-360^circ video generation framework that creates high-quality 360^circ videos with rich and diverse motion patterns from video anchors. Imagine360 learns fine-grained spherical visual and motion patterns from limited 360^circ video data with several key designs. 1) Firstly we adopt the dual-branch design, including a perspective and a panorama video denoising branch to provide local and global constraints for 360^circ video generation, with motion module and spatial LoRA layers fine-tuned on extended web 360^circ videos. 2) Additionally, an antipodal mask is devised to capture long-range motion dependencies, enhancing the reversed camera motion between antipodal pixels across hemispheres. 3) To handle diverse perspective video inputs, we propose elevation-aware designs that adapt to varying video masking due to changing elevations across frames. Extensive experiments show Imagine360 achieves superior graphics quality and motion coherence among state-of-the-art 360^circ video generation methods. We believe Imagine360 holds promise for advancing personalized, immersive 360^circ video creation.', 'score': 23, 'issue_id': 958, 'pub_date': '2024-12-04', 'pub_date_card': {'ru': '4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 4', 'zh': '12æœˆ4æ—¥'}, 'hash': '90dc986cabb575af', 'authors': ['Jing Tan', 'Shuai Yang', 'Tong Wu', 'Jingwen He', 'Yuwei Guo', 'Ziwei Liu', 'Dahua Lin'], 'affiliations': ['Nanyang Technological University', 'Shanghai AI Laboratory', 'Shanghai Jiao Tong University', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2412.03552.jpg', 'data': {'categories': ['#cv', '#video', '#multimodal'], 'emoji': 'ğŸŒ', 'ru': {'title': 'ĞŸĞ¾Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ² 360Â°: Ğ¾Ñ‚ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğº Ğ¿Ğ°Ğ½Ğ¾Ñ€Ğ°Ğ¼Ğ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ñ‹Ñ‚Ñƒ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Imagine360 - Ğ¿ĞµÑ€Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 360-Ğ³Ñ€Ğ°Ğ´ÑƒÑĞ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…Ğ²ĞµÑ‚Ğ²ĞµĞ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ñ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑĞ¼Ğ¸ ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ Ğ¿Ğ°Ğ½Ğ¾Ñ€Ğ°Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ°Ğ½Ñ‚Ğ¸Ğ¿Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ°ÑĞºÑƒ Ğ´Ğ»Ñ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° Ğ´Ğ°Ğ»ÑŒĞ½Ğ¸Ñ… Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ñ‹ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğº Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸ÑĞ¼ ÑƒĞ³Ğ»Ğ° Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ° Ğ²Ğ¾ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¸ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Transforming Perspective Videos into Immersive 360Â° Experiences', 'desc': 'The paper presents Imagine360, a novel framework for converting standard perspective videos into immersive 360-degree equirectangular videos. It employs a dual-branch architecture that integrates local and global constraints to enhance video quality and motion coherence. Key innovations include an antipodal mask for capturing long-range motion dependencies and elevation-aware designs to adapt to varying perspectives. Extensive experiments demonstrate that Imagine360 outperforms existing methods in generating high-quality, dynamic 360-degree videos.'}, 'zh': {'title': 'Imagine360ï¼šä¸ªæ€§åŒ–æ²‰æµ¸å¼360åº¦è§†é¢‘åˆ›ä½œçš„æœªæ¥', 'desc': '360åº¦è§†é¢‘æä¾›äº†ä¸€ç§è¶…æ²‰æµ¸å¼ä½“éªŒï¼Œè®©è§‚ä¼—å¯ä»¥ä»å…¨æ–¹ä½æ¢ç´¢åŠ¨æ€åœºæ™¯ã€‚ä¸ºå®ç°æ›´å‹å¥½å’Œä¸ªæ€§åŒ–çš„360åº¦è§†é¢‘å†…å®¹åˆ›ä½œï¼Œæˆ‘ä»¬æå‡ºäº†Imagine360ï¼Œè¿™æ˜¯é¦–ä¸ªå°†æ ‡å‡†è§†è§’è§†é¢‘è½¬æ¢ä¸º360åº¦è§†é¢‘çš„æ¡†æ¶ã€‚Imagine360é€šè¿‡æœ‰é™çš„360åº¦è§†é¢‘æ•°æ®å­¦ä¹ ç»†è‡´çš„çƒé¢è§†è§‰å’Œè¿åŠ¨æ¨¡å¼ï¼Œé‡‡ç”¨åŒåˆ†æ”¯è®¾è®¡æ¥æä¾›å±€éƒ¨å’Œå…¨å±€çº¦æŸã€‚å®éªŒè¡¨æ˜ï¼ŒImagine360åœ¨å›¾å½¢è´¨é‡å’Œè¿åŠ¨ä¸€è‡´æ€§æ–¹é¢ä¼˜äºç°æœ‰çš„360åº¦è§†é¢‘ç”Ÿæˆæ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.03515', 'title': 'Distilling Diffusion Models to Efficient 3D LiDAR Scene Completion', 'url': 'https://huggingface.co/papers/2412.03515', 'abstract': 'Diffusion models have been applied to 3D LiDAR scene completion due to their strong training stability and high completion quality. However, the slow sampling speed limits the practical application of diffusion-based scene completion models since autonomous vehicles require an efficient perception of surrounding environments. This paper proposes a novel distillation method tailored for 3D LiDAR scene completion models, dubbed ScoreLiDAR, which achieves efficient yet high-quality scene completion. ScoreLiDAR enables the distilled model to sample in significantly fewer steps after distillation. To improve completion quality, we also introduce a novel Structural Loss, which encourages the distilled model to capture the geometric structure of the 3D LiDAR scene. The loss contains a scene-wise term constraining the holistic structure and a point-wise term constraining the key landmark points and their relative configuration. Extensive experiments demonstrate that ScoreLiDAR significantly accelerates the completion time from 30.55 to 5.37 seconds per frame (>5times) on SemanticKITTI and achieves superior performance compared to state-of-the-art 3D LiDAR scene completion models. Our code is publicly available at https://github.com/happyw1nd/ScoreLiDAR.', 'score': 21, 'issue_id': 957, 'pub_date': '2024-12-04', 'pub_date_card': {'ru': '4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 4', 'zh': '12æœˆ4æ—¥'}, 'hash': '6e733cf9c0a1b851', 'authors': ['Shengyuan Zhang', 'An Zhao', 'Ling Yang', 'Zejian Li', 'Chenye Meng', 'Haoran Xu', 'Tianrun Chen', 'AnYang Wei', 'Perry Pengyun GU', 'Lingyun Sun'], 'affiliations': ['Peking University', 'Zhejiang Green Zhixing Technology co., ltd', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2412.03515.jpg', 'data': {'categories': ['#optimization', '#diffusion', '#training', '#3d', '#open_source'], 'emoji': 'ğŸš—', 'ru': {'title': 'Ğ‘Ñ‹ÑÑ‚Ñ€Ğ¾Ğµ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ğµ 3D LiDAR-ÑÑ†ĞµĞ½ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑĞ¿Ğ¾Ñ€Ñ‚Ğ½Ñ‹Ñ… ÑÑ€ĞµĞ´ÑÑ‚Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ñ 3D LiDAR-ÑÑ†ĞµĞ½ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ScoreLiDAR. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒÑĞºĞ¾Ñ€Ğ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ ÑĞµĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ñ ÑÑ†ĞµĞ½Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¡Ñ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½ÑƒÑ ĞŸĞ¾Ñ‚ĞµÑ€Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ»ÑƒÑ‡ÑˆĞµ ÑƒĞ»Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ 3D LiDAR-ÑÑ†ĞµĞ½Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ScoreLiDAR ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ğ²Ñ€ĞµĞ¼Ñ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ğ² 5 Ñ€Ğ°Ğ· Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ñ 3D LiDAR-ÑÑ†ĞµĞ½.'}, 'en': {'title': 'Accelerating 3D LiDAR Scene Completion with ScoreLiDAR', 'desc': "This paper introduces ScoreLiDAR, a new method for improving the efficiency of 3D LiDAR scene completion using diffusion models. The proposed distillation technique allows the model to generate high-quality scene completions in significantly fewer sampling steps, making it faster and more practical for real-time applications like autonomous vehicles. Additionally, a novel Structural Loss is introduced to enhance the model's ability to understand and replicate the geometric structure of the 3D scenes. Experimental results show that ScoreLiDAR reduces completion time dramatically while outperforming existing state-of-the-art models in quality."}, 'zh': {'title': 'é«˜æ•ˆ3D LiDARåœºæ™¯è¡¥å…¨çš„æ–°æ–¹æ³•', 'desc': 'æ‰©æ•£æ¨¡å‹å› å…¶å¼ºå¤§çš„è®­ç»ƒç¨³å®šæ€§å’Œé«˜è´¨é‡çš„åœºæ™¯è¡¥å…¨è€Œè¢«åº”ç”¨äº3D LiDARåœºæ™¯è¡¥å…¨ã€‚ç„¶è€Œï¼Œæ…¢é€Ÿé‡‡æ ·é€Ÿåº¦é™åˆ¶äº†åŸºäºæ‰©æ•£çš„åœºæ™¯è¡¥å…¨æ¨¡å‹çš„å®é™…åº”ç”¨ï¼Œå› ä¸ºè‡ªåŠ¨é©¾é©¶è½¦è¾†éœ€è¦é«˜æ•ˆæ„ŸçŸ¥å‘¨å›´ç¯å¢ƒã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„è’¸é¦æ–¹æ³•ï¼Œç§°ä¸ºScoreLiDARï¼Œæ—¨åœ¨å®ç°é«˜æ•ˆä¸”é«˜è´¨é‡çš„åœºæ™¯è¡¥å…¨ã€‚é€šè¿‡å¼•å…¥ç»“æ„æŸå¤±ï¼ŒScoreLiDARèƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰3D LiDARåœºæ™¯çš„å‡ ä½•ç»“æ„ï¼ŒåŒæ—¶æ˜¾è‘—åŠ å¿«äº†æ¯å¸§çš„è¡¥å…¨æ—¶é—´ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.03069', 'title': 'TokenFlow: Unified Image Tokenizer for Multimodal Understanding and Generation', 'url': 'https://huggingface.co/papers/2412.03069', 'abstract': "We present TokenFlow, a novel unified image tokenizer that bridges the long-standing gap between multimodal understanding and generation. Prior research attempt to employ a single reconstruction-targeted Vector Quantization (VQ) encoder for unifying these two tasks. We observe that understanding and generation require fundamentally different granularities of visual information. This leads to a critical trade-off, particularly compromising performance in multimodal understanding tasks. TokenFlow addresses this challenge through an innovative dual-codebook architecture that decouples semantic and pixel-level feature learning while maintaining their alignment via a shared mapping mechanism. This design enables direct access to both high-level semantic representations crucial for understanding tasks and fine-grained visual features essential for generation through shared indices. Our extensive experiments demonstrate TokenFlow's superiority across multiple dimensions. Leveraging TokenFlow, we demonstrate for the first time that discrete visual input can surpass LLaVA-1.5 13B in understanding performance, achieving a 7.2\\% average improvement. For image reconstruction, we achieve a strong FID score of 0.63 at 384*384 resolution. Moreover, TokenFlow establishes state-of-the-art performance in autoregressive image generation with a GenEval score of 0.55 at 256*256 resolution, achieving comparable results to SDXL.", 'score': 18, 'issue_id': 957, 'pub_date': '2024-12-04', 'pub_date_card': {'ru': '4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 4', 'zh': '12æœˆ4æ—¥'}, 'hash': '820e62e1bd498d55', 'authors': ['Liao Qu', 'Huichao Zhang', 'Yiheng Liu', 'Xu Wang', 'Yi Jiang', 'Yiming Gao', 'Hu Ye', 'Daniel K. Du', 'Zehuan Yuan', 'Xinglong Wu'], 'affiliations': ['ByteDance'], 'pdf_title_img': 'assets/pdf/title_img/2412.03069.jpg', 'data': {'categories': ['#multimodal', '#cv', '#architecture'], 'emoji': 'ğŸ”€', 'ru': {'title': 'TokenFlow: ĞµĞ´Ğ¸Ğ½Ñ‹Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'TokenFlow - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ñ Ğ´Ğ²Ğ¾Ğ¹Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ±ÑƒĞºĞ¾Ğ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ², ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¸Ñ… Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±Ñ‰Ğ¸Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. TokenFlow Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ° 7.2% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ LLaVA-1.5 13B. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'TokenFlow: Bridging Understanding and Generation in Image Processing', 'desc': 'TokenFlow is a new image tokenizer that improves how machines understand and generate images by using a dual-codebook architecture. This approach separates the learning of high-level semantic features from fine-grained pixel-level details, allowing for better performance in both understanding and generation tasks. By aligning these two types of information through a shared mapping, TokenFlow can effectively utilize both granularities of visual data. The results show that TokenFlow outperforms previous models in understanding and image generation, achieving significant improvements in performance metrics.'}, 'zh': {'title': 'TokenFlowï¼šå¤šæ¨¡æ€ç†è§£ä¸ç”Ÿæˆçš„æ¡¥æ¢', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°é¢–çš„å›¾åƒæ ‡è®°å™¨TokenFlowï¼Œå®ƒå¼¥åˆäº†å¤šæ¨¡æ€ç†è§£ä¸ç”Ÿæˆä¹‹é—´çš„å·®è·ã€‚ç ”ç©¶è¡¨æ˜ï¼Œç†è§£å’Œç”Ÿæˆä»»åŠ¡éœ€è¦ä¸åŒç²’åº¦çš„è§†è§‰ä¿¡æ¯ï¼Œä¼ ç»Ÿçš„å•ä¸€é‡å»ºç›®æ ‡å‘é‡é‡åŒ–ç¼–ç å™¨æ— æ³•æœ‰æ•ˆå¤„ç†è¿™ä¸€é—®é¢˜ã€‚TokenFlowé€šè¿‡åˆ›æ–°çš„åŒä»£ç æœ¬æ¶æ„ï¼Œè§£è€¦äº†è¯­ä¹‰å’Œåƒç´ çº§ç‰¹å¾å­¦ä¹ ï¼ŒåŒæ—¶é€šè¿‡å…±äº«æ˜ å°„æœºåˆ¶ä¿æŒå®ƒä»¬çš„å¯¹é½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTokenFlowåœ¨å¤šé¡¹ä»»åŠ¡ä¸­è¡¨ç°ä¼˜è¶Šï¼Œé¦–æ¬¡è¯æ˜ç¦»æ•£è§†è§‰è¾“å…¥åœ¨ç†è§£æ€§èƒ½ä¸Šè¶…è¶Šäº†LLaVA-1.5 13Bã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.00493', 'title': 'Video-3D LLM: Learning Position-Aware Video Representation for 3D Scene Understanding', 'url': 'https://huggingface.co/papers/2412.00493', 'abstract': "The rapid advancement of Multimodal Large Language Models (MLLMs) has significantly impacted various multimodal tasks. However, these models face challenges in tasks that require spatial understanding within 3D environments. Efforts to enhance MLLMs, such as incorporating point cloud features, have been made, yet a considerable gap remains between the models' learned representations and the inherent complexity of 3D scenes. This discrepancy largely stems from the training of MLLMs on predominantly 2D data, which restricts their effectiveness in comprehending 3D spaces. To address this issue, in this paper, we propose a novel generalist model, i.e., Video-3D LLM, for 3D scene understanding. By treating 3D scenes as dynamic videos and incorporating 3D position encoding into these representations, our Video-3D LLM aligns video representations with real-world spatial contexts more accurately. Additionally, we have implemented a maximum coverage sampling technique to optimize the balance between computational costs and performance efficiency. Extensive experiments demonstrate that our model achieves state-of-the-art performance on several 3D scene understanding benchmarks, including ScanRefer, Multi3DRefer, Scan2Cap, ScanQA, and SQA3D.", 'score': 14, 'issue_id': 964, 'pub_date': '2024-11-30', 'pub_date_card': {'ru': '30 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 30', 'zh': '11æœˆ30æ—¥'}, 'hash': '10c214b548697656', 'authors': ['Duo Zheng', 'Shijia Huang', 'Liwei Wang'], 'affiliations': ['The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2412.00493.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#3d', '#optimization', '#training'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Video-3D LLM: ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Video-3D LLM Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ 3D-ÑÑ†ĞµĞ½Ñ‹ ĞºĞ°Ğº Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ 3D-Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ğ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ»Ğ¸ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ Ñ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¿Ğ¾ĞºÑ€Ñ‹Ñ‚Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ğ¼Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ¿Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ 3D-ÑÑ†ĞµĞ½.'}, 'en': {'title': 'Revolutionizing 3D Scene Understanding with Video-3D LLM', 'desc': 'This paper introduces a new model called Video-3D LLM, designed to improve understanding of 3D scenes by treating them like dynamic videos. The model incorporates 3D position encoding to better align video representations with real-world spatial contexts, addressing the limitations of existing Multimodal Large Language Models (MLLMs) that primarily learn from 2D data. To enhance efficiency, a maximum coverage sampling technique is used, balancing computational costs with performance. The results show that Video-3D LLM achieves state-of-the-art performance on multiple benchmarks for 3D scene understanding.'}, 'zh': {'title': 'æå‡3Dåœºæ™¯ç†è§£çš„åˆ›æ–°æ¨¡å‹', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œç§°ä¸ºVideo-3D LLMï¼Œæ—¨åœ¨æé«˜3Dåœºæ™¯ç†è§£èƒ½åŠ›ã€‚ä¼ ç»Ÿçš„MLLMä¸»è¦åŸºäº2Dæ•°æ®è®­ç»ƒï¼Œå¯¼è‡´å®ƒä»¬åœ¨å¤„ç†3Dç¯å¢ƒæ—¶å­˜åœ¨å±€é™æ€§ã€‚é€šè¿‡å°†3Dåœºæ™¯è§†ä¸ºåŠ¨æ€è§†é¢‘ï¼Œå¹¶å¼•å…¥3Dä½ç½®ç¼–ç ï¼ŒVideo-3D LLMèƒ½å¤Ÿæ›´å‡†ç¡®åœ°å¯¹é½è§†é¢‘è¡¨ç¤ºä¸ç°å®ä¸–ç•Œçš„ç©ºé—´ä¸Šä¸‹æ–‡ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æå‡ºäº†ä¸€ç§æœ€å¤§è¦†ç›–é‡‡æ ·æŠ€æœ¯ï¼Œä»¥ä¼˜åŒ–è®¡ç®—æˆæœ¬å’Œæ€§èƒ½æ•ˆç‡ä¹‹é—´çš„å¹³è¡¡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.03205', 'title': 'U-MATH: A University-Level Benchmark for Evaluating Mathematical Skills in LLMs', 'url': 'https://huggingface.co/papers/2412.03205', 'abstract': "The current evaluation of mathematical skills in LLMs is limited, as existing benchmarks are either relatively small, primarily focus on elementary and high-school problems, or lack diversity in topics. Additionally, the inclusion of visual elements in tasks remains largely under-explored.   To address these gaps, we introduce U-MATH, a novel benchmark of 1,100 unpublished open-ended university-level problems sourced from teaching materials. It is balanced across six core subjects, with 20% of multimodal problems. Given the open-ended nature of U-MATH problems, we employ an LLM to judge the correctness of generated solutions. To this end, we release mu-MATH, a dataset to evaluate the LLMs' capabilities in judging solutions.   The evaluation of general domain, math-specific, and multimodal LLMs highlights the challenges presented by U-MATH. Our findings reveal that LLMs achieve a maximum accuracy of only 63% on text-based tasks, with even lower 45% on visual problems. The solution assessment proves challenging for LLMs, with the best LLM judge having an F1-score of 80% on mu-MATH.", 'score': 13, 'issue_id': 968, 'pub_date': '2024-12-04', 'pub_date_card': {'ru': '4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 4', 'zh': '12æœˆ4æ—¥'}, 'hash': '8df63a02d444d462', 'authors': ['Konstantin Chernyshev', 'Vitaliy Polshkov', 'Ekaterina Artemova', 'Alex Myasnikov', 'Vlad Stepanov', 'Alexei Miasnikov', 'Sergei Tilga'], 'affiliations': ['Gradarius', 'Stevens Institute of Technology', 'Toloka AI'], 'pdf_title_img': 'assets/pdf/title_img/2412.03205.jpg', 'data': {'categories': ['#multimodal', '#dataset', '#math', '#science', '#benchmark'], 'emoji': 'ğŸ§®', 'ru': {'title': 'U-MATH: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ˜Ğ˜', 'desc': 'ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº U-MATH Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ½ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 1100 Ğ·Ğ°Ğ´Ğ°Ñ‡ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ¸Ñ‚ĞµÑ‚ÑĞºĞ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¿Ğ¾ ÑˆĞµÑÑ‚Ğ¸ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğ¼ Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ°Ğ¼, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ 20% Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ”Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ´Ğ»Ñ Ñ‡ĞµĞ³Ğ¾ Ğ±Ñ‹Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ mu-MATH. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ LLM Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ»Ğ¸ÑˆÑŒ 63% Ğ½Ğ° Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¸ 45% Ğ½Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… U-MATH, Ğ° Ğ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ-Ğ¾Ñ†ĞµĞ½Ñ‰Ğ¸Ğº Ğ¸Ğ¼ĞµĞµÑ‚ F1-Ğ¼ĞµÑ€Ñƒ 80% Ğ½Ğ° mu-MATH.'}, 'en': {'title': 'U-MATH: Elevating Math Evaluation for LLMs', 'desc': 'This paper presents U-MATH, a new benchmark designed to evaluate the mathematical skills of large language models (LLMs) using 1,100 open-ended university-level problems. The benchmark covers six core subjects and includes multimodal tasks, which incorporate visual elements, addressing the limitations of existing evaluations. To assess the performance of LLMs on these problems, the authors introduce mu-MATH, a dataset specifically for evaluating the correctness of solutions generated by LLMs. The results indicate that LLMs struggle with these tasks, achieving only 63% accuracy on text-based problems and 45% on visual ones, highlighting the need for further advancements in LLM capabilities.'}, 'zh': {'title': 'U-MATHï¼šæå‡LLMsæ•°å­¦èƒ½åŠ›è¯„ä¼°çš„æ–°åŸºå‡†', 'desc': 'ç›®å‰å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ•°å­¦æŠ€èƒ½çš„è¯„ä¼°å­˜åœ¨å±€é™æ€§ï¼Œç°æœ‰åŸºå‡†æµ‹è¯•ç›¸å¯¹è¾ƒå°ï¼Œä¸»è¦é›†ä¸­åœ¨åŸºç¡€å’Œé«˜ä¸­é—®é¢˜ä¸Šï¼Œä¸”ç¼ºä¹ä¸»é¢˜å¤šæ ·æ€§ã€‚æ­¤å¤–ï¼Œä»»åŠ¡ä¸­è§†è§‰å…ƒç´ çš„åŒ…å«ä»ç„¶æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†U-MATHï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«1100ä¸ªæœªå‘è¡¨çš„å¼€æ”¾å¼å¤§å­¦çº§é—®é¢˜çš„æ–°åŸºå‡†ï¼Œæ¶µç›–å…­ä¸ªæ ¸å¿ƒå­¦ç§‘ï¼Œå…¶ä¸­20%çš„é—®é¢˜ä¸ºå¤šæ¨¡æ€é—®é¢˜ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼ŒLLMsåœ¨æ–‡æœ¬ä»»åŠ¡ä¸Šçš„æœ€é«˜å‡†ç¡®ç‡ä»…ä¸º63%ï¼Œè€Œåœ¨è§†è§‰é—®é¢˜ä¸Šçš„å‡†ç¡®ç‡æ›´ä½ï¼Œä»…ä¸º45%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.03517', 'title': 'NVComposer: Boosting Generative Novel View Synthesis with Multiple Sparse and Unposed Images', 'url': 'https://huggingface.co/papers/2412.03517', 'abstract': 'Recent advancements in generative models have significantly improved novel view synthesis (NVS) from multi-view data. However, existing methods depend on external multi-view alignment processes, such as explicit pose estimation or pre-reconstruction, which limits their flexibility and accessibility, especially when alignment is unstable due to insufficient overlap or occlusions between views. In this paper, we propose NVComposer, a novel approach that eliminates the need for explicit external alignment. NVComposer enables the generative model to implicitly infer spatial and geometric relationships between multiple conditional views by introducing two key components: 1) an image-pose dual-stream diffusion model that simultaneously generates target novel views and condition camera poses, and 2) a geometry-aware feature alignment module that distills geometric priors from dense stereo models during training. Extensive experiments demonstrate that NVComposer achieves state-of-the-art performance in generative multi-view NVS tasks, removing the reliance on external alignment and thus improving model accessibility. Our approach shows substantial improvements in synthesis quality as the number of unposed input views increases, highlighting its potential for more flexible and accessible generative NVS systems.', 'score': 13, 'issue_id': 960, 'pub_date': '2024-12-04', 'pub_date_card': {'ru': '4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 4', 'zh': '12æœˆ4æ—¥'}, 'hash': '9d51bf0b60be344b', 'authors': ['Lingen Li', 'Zhaoyang Zhang', 'Yaowei Li', 'Jiale Xu', 'Xiaoyu Li', 'Wenbo Hu', 'Weihao Cheng', 'Jinwei Gu', 'Tianfan Xue', 'Ying Shan'], 'affiliations': ['ARC Lab, Tencent PCG', 'Peking University', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2412.03517.jpg', 'data': {'categories': ['#optimization', '#3d', '#diffusion', '#cv'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Ğ¡Ğ¸Ğ½Ñ‚ĞµĞ· Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ² Ğ±ĞµĞ· ÑĞ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´Ğ¾Ğ²', 'desc': 'NVComposer - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¸Ğ½Ñ‚ĞµĞ·Ñƒ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¾ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ¼ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ğ¸Ğ´Ğ¾Ğ². ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…Ğ¿Ğ¾Ñ‚Ğ¾Ñ‡Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ² Ğ¸ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¹ ĞºĞ°Ğ¼ĞµÑ€. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ· Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ñ… ÑÑ‚ĞµÑ€ĞµĞ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ NVComposer Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ğ¾Ğ³Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ²Ğ¸Ğ´Ğ¾Ğ².'}, 'en': {'title': 'NVComposer: Generating Novel Views Without External Alignment', 'desc': 'This paper introduces NVComposer, a new method for generating novel views from multiple images without needing external alignment processes like pose estimation. NVComposer uses a dual-stream diffusion model that generates new views while also predicting camera poses, allowing for a more integrated approach. Additionally, it incorporates a geometry-aware feature alignment module that learns geometric information from stereo models during training. The results show that NVComposer outperforms existing methods, especially when there are many unaligned input views, making it a more flexible solution for novel view synthesis.'}, 'zh': {'title': 'NVComposerï¼šæ— é¡»å¤–éƒ¨å¯¹é½çš„ç”Ÿæˆæ–°è§†å›¾åˆæˆ', 'desc': 'æœ€è¿‘ç”Ÿæˆæ¨¡å‹çš„è¿›å±•æ˜¾è‘—æå‡äº†å¤šè§†å›¾æ•°æ®çš„æ–°çš„è§†å›¾åˆæˆï¼ˆNVSï¼‰èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä¾èµ–äºå¤–éƒ¨çš„å¤šè§†å›¾å¯¹é½è¿‡ç¨‹ï¼Œå¦‚æ˜¾å¼çš„å§¿æ€ä¼°è®¡æˆ–é¢„é‡å»ºï¼Œè¿™é™åˆ¶äº†å®ƒä»¬çš„çµæ´»æ€§å’Œå¯è®¿é—®æ€§ï¼Œå°¤å…¶æ˜¯åœ¨è§†å›¾ä¹‹é—´é‡å ä¸è¶³æˆ–é®æŒ¡æ—¶å¯¹é½ä¸ç¨³å®šçš„æƒ…å†µä¸‹ã€‚æœ¬æ–‡æå‡ºäº†NVComposerï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œæ¶ˆé™¤äº†å¯¹æ˜¾å¼å¤–éƒ¨å¯¹é½çš„éœ€æ±‚ã€‚NVComposeré€šè¿‡å¼•å…¥ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼Œä½¿ç”Ÿæˆæ¨¡å‹èƒ½å¤Ÿéšå¼æ¨æ–­å¤šä¸ªæ¡ä»¶è§†å›¾ä¹‹é—´çš„ç©ºé—´å’Œå‡ ä½•å…³ç³»ï¼Œä»è€Œåœ¨ç”Ÿæˆå¤šè§†å›¾NVSä»»åŠ¡ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.01106', 'title': 'One Shot, One Talk: Whole-body Talking Avatar from a Single Image', 'url': 'https://huggingface.co/papers/2412.01106', 'abstract': 'Building realistic and animatable avatars still requires minutes of multi-view or monocular self-rotating videos, and most methods lack precise control over gestures and expressions. To push this boundary, we address the challenge of constructing a whole-body talking avatar from a single image. We propose a novel pipeline that tackles two critical issues: 1) complex dynamic modeling and 2) generalization to novel gestures and expressions. To achieve seamless generalization, we leverage recent pose-guided image-to-video diffusion models to generate imperfect video frames as pseudo-labels. To overcome the dynamic modeling challenge posed by inconsistent and noisy pseudo-videos, we introduce a tightly coupled 3DGS-mesh hybrid avatar representation and apply several key regularizations to mitigate inconsistencies caused by imperfect labels. Extensive experiments on diverse subjects demonstrate that our method enables the creation of a photorealistic, precisely animatable, and expressive whole-body talking avatar from just a single image.', 'score': 12, 'issue_id': 957, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 2', 'zh': '12æœˆ2æ—¥'}, 'hash': '13d96f9bb346e344', 'authors': ['Jun Xiang', 'Yudong Guo', 'Leipeng Hu', 'Boyang Guo', 'Yancheng Yuan', 'Juyong Zhang'], 'affiliations': ['The Hong Kong Polytechnic University', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2412.01106.jpg', 'data': {'categories': ['#cv', '#optimization', '#multimodal', '#diffusion', '#3d'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰Ğ¸Ğ¹ Ğ°Ğ²Ğ°Ñ‚Ğ°Ñ€ Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ñ„Ğ¾Ñ‚Ğ¾', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ°Ğ²Ğ°Ñ‚Ğ°Ñ€Ğ¾Ğ², ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ¸Ñ‚ÑŒ Ğ¸ Ğ´Ğ²Ğ¸Ğ³Ğ°Ñ‚ÑŒÑÑ, Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²ÑĞµĞ³Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿ÑĞµĞ²Ğ´Ğ¾-Ğ²Ğ¸Ğ´ĞµĞ¾ ĞºĞ°Ğ´Ñ€Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑĞ»ÑƒĞ¶Ğ°Ñ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸. ĞĞ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ°Ğ²Ğ°Ñ‚Ğ°Ñ€Ğ°, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰ĞµĞµ 3D Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹ ÑĞ¿Ğ»Ğ°Ñ‚Ñ‹ Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ğ³Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑĞµÑ‚ĞºÑƒ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¶ĞµÑÑ‚Ñ‹ Ğ¸ Ğ¼Ğ¸Ğ¼Ğ¸ĞºÑƒ Ğ°Ğ²Ğ°Ñ‚Ğ°Ñ€Ğ°, Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ².'}, 'en': {'title': 'From One Image to a Lifelike Talking Avatar!', 'desc': "This paper presents a new method for creating realistic and animatable whole-body talking avatars using only a single image. The authors address two main challenges: modeling complex movements and ensuring the avatar can perform new gestures and expressions. They utilize pose-guided image-to-video diffusion models to generate video frames that serve as training data, despite being imperfect. To improve the quality of the avatar's animations, they introduce a hybrid representation that combines 3D mesh structures with regularization techniques to handle inconsistencies in the generated video frames."}, 'zh': {'title': 'ä»å•å¼ å›¾åƒç”Ÿæˆå…¨èº«ä¼šè¯´è¯çš„è™šæ‹Ÿå¤´åƒ', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§ä»å•å¼ å›¾åƒæ„å»ºå…¨èº«ä¼šè¯´è¯çš„è™šæ‹Ÿå¤´åƒçš„æ–¹æ³•ã€‚æˆ‘ä»¬è§£å†³äº†å¤æ‚åŠ¨æ€å»ºæ¨¡å’Œå¯¹æ–°æ‰‹åŠ¿ä¸è¡¨æƒ…çš„æ³›åŒ–è¿™ä¸¤ä¸ªå…³é”®é—®é¢˜ã€‚é€šè¿‡ä½¿ç”¨å§¿æ€å¼•å¯¼çš„å›¾åƒåˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œæˆ‘ä»¬ç”Ÿæˆäº†ä¸å®Œç¾çš„è§†é¢‘å¸§ä½œä¸ºä¼ªæ ‡ç­¾ï¼Œä»¥å®ç°æ— ç¼æ³›åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿä»å•å¼ å›¾åƒåˆ›å»ºå‡ºé€¼çœŸã€å¯ç²¾ç¡®åŠ¨ç”»å’Œå¯Œæœ‰è¡¨ç°åŠ›çš„å…¨èº«è™šæ‹Ÿå¤´åƒã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.02030', 'title': 'NitroFusion: High-Fidelity Single-Step Diffusion through Dynamic Adversarial Training', 'url': 'https://huggingface.co/papers/2412.02030', 'abstract': 'We introduce NitroFusion, a fundamentally different approach to single-step diffusion that achieves high-quality generation through a dynamic adversarial framework. While one-step methods offer dramatic speed advantages, they typically suffer from quality degradation compared to their multi-step counterparts. Just as a panel of art critics provides comprehensive feedback by specializing in different aspects like composition, color, and technique, our approach maintains a large pool of specialized discriminator heads that collectively guide the generation process. Each discriminator group develops expertise in specific quality aspects at different noise levels, providing diverse feedback that enables high-fidelity one-step generation. Our framework combines: (i) a dynamic discriminator pool with specialized discriminator groups to improve generation quality, (ii) strategic refresh mechanisms to prevent discriminator overfitting, and (iii) global-local discriminator heads for multi-scale quality assessment, and unconditional/conditional training for balanced generation. Additionally, our framework uniquely supports flexible deployment through bottom-up refinement, allowing users to dynamically choose between 1-4 denoising steps with the same model for direct quality-speed trade-offs. Through comprehensive experiments, we demonstrate that NitroFusion significantly outperforms existing single-step methods across multiple evaluation metrics, particularly excelling in preserving fine details and global consistency.', 'score': 11, 'issue_id': 966, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 2', 'zh': '12æœˆ2æ—¥'}, 'hash': '4c749ff913210111', 'authors': ['Dar-Yen Chen', 'Hmrishav Bandyopadhyay', 'Kai Zou', 'Yi-Zhe Song'], 'affiliations': ['NetMind.AI', 'SketchX, CVSSP, University of Surrey'], 'pdf_title_img': 'assets/pdf/title_img/2412.02030.jpg', 'data': {'categories': ['#diffusion', '#optimization', '#training', '#architecture', '#benchmark'], 'emoji': 'ğŸ¨', 'ru': {'title': 'NitroFusion: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ğ´Ğ½Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'NitroFusion - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ´Ğ½Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ¾ÑÑ‚ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ¿ÑƒĞ» ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ¸ÑĞºÑ€Ğ¸Ğ¼Ğ¸Ğ½Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ², ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ğ¸Ğ· ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ°ÑĞ¿ĞµĞºÑ‚Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑƒÑ€Ğ¾Ğ²Ğ½ÑÑ… ÑˆÑƒĞ¼Ğ°. NitroFusion Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ¸ÑĞºÑ€Ğ¸Ğ¼Ğ¸Ğ½Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾-Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ³Ğ¾Ğ»Ğ¾Ğ²ĞºĞ¸ Ğ´Ğ¸ÑĞºÑ€Ğ¸Ğ¼Ğ¸Ğ½Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³Ğ¸Ğ±ĞºĞ¾ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°Ñ‚ÑŒ Ğ¾Ñ‚ 1 Ğ´Ğ¾ 4 ÑˆĞ°Ğ³Ğ¾Ğ² Ğ´ĞµĞ½Ğ¾Ğ¹Ğ·Ğ¸Ğ½Ğ³Ğ°, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒÑ.'}, 'en': {'title': 'NitroFusion: Fast and High-Quality Image Generation with Dynamic Discriminators', 'desc': 'NitroFusion is a new method for generating images quickly while maintaining high quality. It uses a dynamic adversarial framework with multiple specialized discriminator heads that focus on different quality aspects, similar to art critics. This approach allows for better feedback during the generation process, leading to improved fidelity in one-step generation. Additionally, NitroFusion offers flexible options for users to balance speed and quality by adjusting the number of denoising steps used.'}, 'zh': {'title': 'NitroFusionï¼šé«˜æ•ˆä¸é«˜è´¨é‡ç”Ÿæˆçš„å®Œç¾ç»“åˆ', 'desc': 'NitroFusionæ˜¯ä¸€ç§å…¨æ–°çš„å•æ­¥æ‰©æ•£ç”Ÿæˆæ–¹æ³•ï¼Œé€šè¿‡åŠ¨æ€å¯¹æŠ—æ¡†æ¶å®ç°é«˜è´¨é‡ç”Ÿæˆã€‚ä¸ä¼ ç»Ÿçš„å•æ­¥æ–¹æ³•ç›¸æ¯”ï¼ŒNitroFusionåœ¨ç”Ÿæˆè´¨é‡ä¸Šæœ‰æ˜¾è‘—æå‡ï¼Œå°½ç®¡å•æ­¥æ–¹æ³•åœ¨é€Ÿåº¦ä¸Šå…·æœ‰ä¼˜åŠ¿ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å¤šä¸ªä¸“ä¸šçš„åˆ¤åˆ«å™¨ç»„ï¼Œé’ˆå¯¹ä¸åŒçš„å™ªå£°æ°´å¹³æä¾›å¤šæ ·åŒ–çš„åé¦ˆï¼Œä»è€Œæé«˜ç”Ÿæˆçš„ä¿çœŸåº¦ã€‚é€šè¿‡çµæ´»çš„éƒ¨ç½²æœºåˆ¶ï¼Œç”¨æˆ·å¯ä»¥æ ¹æ®éœ€è¦åœ¨1åˆ°4ä¸ªå»å™ªæ­¥éª¤ä¹‹é—´åŠ¨æ€é€‰æ‹©ï¼Œå®ç°è´¨é‡ä¸é€Ÿåº¦çš„å¹³è¡¡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.19103', 'title': 'VARCO-VISION: Expanding Frontiers in Korean Vision-Language Models', 'url': 'https://huggingface.co/papers/2411.19103', 'abstract': "In this paper, we introduce an open-source Korean-English vision-language model (VLM), VARCO-VISION. We incorporate a step-by-step training strategy that allows a model learn both linguistic and visual information while preserving the backbone model's knowledge. Our model demonstrates outstanding performance in diverse settings requiring bilingual image-text understanding and generation abilities compared to models of similar size. VARCO-VISION is also capable of grounding, referring, and OCR, expanding its usage and potential applications for real-world scenarios. In addition to the model, we release five Korean evaluation datasets, including four closed-set and one openset benchmarks. We anticipate that our milestone will broaden the opportunities for AI researchers aiming to train VLMs. VARCO-VISION is available at https://huggingface.co/NCSOFT/VARCO-VISION-14B.", 'score': 11, 'issue_id': 964, 'pub_date': '2024-11-28', 'pub_date_card': {'ru': '28 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 28', 'zh': '11æœˆ28æ—¥'}, 'hash': '4507a3a2ac0bc8b5', 'authors': ['Jeongho Ju', 'Daeyoung Kim', 'SunYoung Park', 'Youngjune Kim'], 'affiliations': ['NC Research, NCSOFT'], 'pdf_title_img': 'assets/pdf/title_img/2411.19103.jpg', 'data': {'categories': ['#open_source', '#dataset', '#multimodal', '#training', '#low_resource'], 'emoji': 'ğŸŒ', 'ru': {'title': 'VARCO-VISION: ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ´Ğ²ÑƒÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ¼ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ¼ Ğ·Ñ€ĞµĞ½Ğ¸Ğ¸', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸ÑĞ·Ñ‹Ñ‡Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ VARCO-VISION Ğ´Ğ»Ñ ĞºĞ¾Ñ€ĞµĞ¹ÑĞºĞ¾Ğ³Ğ¾ Ğ¸ Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ»Ğ¸ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑƒÑĞ²Ğ°Ğ¸Ğ²Ğ°Ñ‚ÑŒ ĞºĞ°Ğº Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ, Ñ‚Ğ°Ğº Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ. VARCO-VISION Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ´Ğ²ÑƒÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ñ€ĞµÑ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ ĞµĞµ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ….'}, 'en': {'title': 'VARCO-VISION: Bridging Korean and English through Vision-Language Learning', 'desc': 'This paper presents VARCO-VISION, an open-source vision-language model designed for Korean-English tasks. It employs a step-by-step training approach that effectively integrates linguistic and visual information while maintaining the foundational knowledge of the backbone model. The model excels in bilingual image-text understanding and generation, outperforming similar-sized models in various applications. Additionally, VARCO-VISION supports grounding, referring, and optical character recognition (OCR), and the authors provide five Korean evaluation datasets to facilitate further research in this area.'}, 'zh': {'title': 'VARCO-VISIONï¼šåŒè¯­è§†è§‰è¯­è¨€æ¨¡å‹çš„æ–°é‡Œç¨‹ç¢‘', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§å¼€æºçš„éŸ©è‹±è§†è§‰è¯­è¨€æ¨¡å‹VARCO-VISIONã€‚æˆ‘ä»¬é‡‡ç”¨é€æ­¥è®­ç»ƒç­–ç•¥ï¼Œä½¿æ¨¡å‹èƒ½å¤ŸåŒæ—¶å­¦ä¹ è¯­è¨€å’Œè§†è§‰ä¿¡æ¯ï¼ŒåŒæ—¶ä¿ç•™åŸºç¡€æ¨¡å‹çš„çŸ¥è¯†ã€‚ä¸åŒç±»æ¨¡å‹ç›¸æ¯”ï¼ŒVARCO-VISIONåœ¨åŒè¯­å›¾åƒæ–‡æœ¬ç†è§£å’Œç”Ÿæˆèƒ½åŠ›æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚è¯¥æ¨¡å‹è¿˜å…·å¤‡å®šä½ã€å¼•ç”¨å’Œå…‰å­¦å­—ç¬¦è¯†åˆ«ï¼ˆOCRï¼‰åŠŸèƒ½ï¼Œæ‰©å±•äº†å…¶åœ¨ç°å®åœºæ™¯ä¸­çš„åº”ç”¨æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.03558', 'title': 'MIDI: Multi-Instance Diffusion for Single Image to 3D Scene Generation', 'url': 'https://huggingface.co/papers/2412.03558', 'abstract': 'This paper introduces MIDI, a novel paradigm for compositional 3D scene generation from a single image. Unlike existing methods that rely on reconstruction or retrieval techniques or recent approaches that employ multi-stage object-by-object generation, MIDI extends pre-trained image-to-3D object generation models to multi-instance diffusion models, enabling the simultaneous generation of multiple 3D instances with accurate spatial relationships and high generalizability. At its core, MIDI incorporates a novel multi-instance attention mechanism, that effectively captures inter-object interactions and spatial coherence directly within the generation process, without the need for complex multi-step processes. The method utilizes partial object images and global scene context as inputs, directly modeling object completion during 3D generation. During training, we effectively supervise the interactions between 3D instances using a limited amount of scene-level data, while incorporating single-object data for regularization, thereby maintaining the pre-trained generalization ability. MIDI demonstrates state-of-the-art performance in image-to-scene generation, validated through evaluations on synthetic data, real-world scene data, and stylized scene images generated by text-to-image diffusion models.', 'score': 10, 'issue_id': 957, 'pub_date': '2024-12-04', 'pub_date_card': {'ru': '4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 4', 'zh': '12æœˆ4æ—¥'}, 'hash': '5e1a4c1e1017e7af', 'authors': ['Zehuan Huang', 'Yuan-Chen Guo', 'Xingqiao An', 'Yunhan Yang', 'Yangguang Li', 'Zi-Xin Zou', 'Ding Liang', 'Xihui Liu', 'Yan-Pei Cao', 'Lu Sheng'], 'affiliations': ['Beihang University', 'The University of Hong Kong', 'Tsinghua University', 'VAST'], 'pdf_title_img': 'assets/pdf/title_img/2412.03558.jpg', 'data': {'categories': ['#cv', '#synthetic', '#diffusion', '#training', '#3d'], 'emoji': 'ğŸ™ï¸', 'ru': {'title': 'MIDI: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D-ÑÑ†ĞµĞ½ Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MIDI - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½ Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. MIDI Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ Ğ¸Ñ… Ğ´Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞºĞ·ĞµĞ¼Ğ¿Ğ»ÑÑ€Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸ÑĞ¼Ğ¸. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒÑ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞºĞ·ĞµĞ¼Ğ¿Ğ»ÑÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½ĞµĞ¿Ğ¾ÑÑ€ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. MIDI Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑÑ†ĞµĞ½ Ğ¸Ğ· Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ĞµÑ‚ÑÑ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼Ğ¸ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ… Ğ¸ ÑÑ‚Ğ¸Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ….'}, 'en': {'title': 'MIDI: Revolutionizing 3D Scene Generation from Single Images', 'desc': 'This paper presents MIDI, a new approach for creating 3D scenes from a single image. It improves upon traditional methods by using multi-instance diffusion models, allowing for the generation of multiple 3D objects at once while maintaining their spatial relationships. MIDI features a unique multi-instance attention mechanism that captures how objects interact and fit together in space, simplifying the generation process. The method is trained with a combination of scene-level and single-object data, ensuring high performance and generalization across various types of scenes.'}, 'zh': {'title': 'MIDIï¼šä»å•å›¾åƒç”Ÿæˆ3Dåœºæ™¯çš„æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºMIDIçš„æ–°æ–¹æ³•ï¼Œç”¨äºä»å•å¼ å›¾åƒç”Ÿæˆç»„åˆ3Dåœºæ™¯ã€‚ä¸ç°æœ‰ä¾èµ–é‡å»ºæˆ–æ£€ç´¢æŠ€æœ¯çš„æ–¹æ³•ä¸åŒï¼ŒMIDIæ‰©å±•äº†é¢„è®­ç»ƒçš„å›¾åƒåˆ°3Då¯¹è±¡ç”Ÿæˆæ¨¡å‹ï¼Œé‡‡ç”¨å¤šå®ä¾‹æ‰©æ•£æ¨¡å‹ï¼Œå®ç°äº†å¤šä¸ª3Då®ä¾‹çš„åŒæ—¶ç”Ÿæˆã€‚MIDIçš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªæ–°é¢–çš„å¤šå®ä¾‹æ³¨æ„æœºåˆ¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ•æ‰å¯¹è±¡é—´çš„äº¤äº’å’Œç©ºé—´ä¸€è‡´æ€§ï¼Œç®€åŒ–äº†ç”Ÿæˆè¿‡ç¨‹ã€‚è¯¥æ–¹æ³•åœ¨å›¾åƒåˆ°åœºæ™¯ç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œç»è¿‡åˆæˆæ•°æ®ã€çœŸå®åœºæ™¯æ•°æ®å’Œæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„é£æ ¼åŒ–åœºæ™¯å›¾åƒçš„è¯„ä¼°éªŒè¯ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.03439', 'title': 'CleanDIFT: Diffusion Features without Noise', 'url': 'https://huggingface.co/papers/2412.03439', 'abstract': 'Internal features from large-scale pre-trained diffusion models have recently been established as powerful semantic descriptors for a wide range of downstream tasks. Works that use these features generally need to add noise to images before passing them through the model to obtain the semantic features, as the models do not offer the most useful features when given images with little to no noise. We show that this noise has a critical impact on the usefulness of these features that cannot be remedied by ensembling with different random noises. We address this issue by introducing a lightweight, unsupervised fine-tuning method that enables diffusion backbones to provide high-quality, noise-free semantic features. We show that these features readily outperform previous diffusion features by a wide margin in a wide variety of extraction setups and downstream tasks, offering better performance than even ensemble-based methods at a fraction of the cost.', 'score': 9, 'issue_id': 963, 'pub_date': '2024-12-04', 'pub_date_card': {'ru': '4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 4', 'zh': '12æœˆ4æ—¥'}, 'hash': 'cd474064bf17503a', 'authors': ['Nick Stracke', 'Stefan Andreas Baumann', 'Kolja Bauer', 'Frank Fundel', 'BjÃ¶rn Ommer'], 'affiliations': ['CompVis @ LMU Munich'], 'pdf_title_img': 'assets/pdf/title_img/2412.03439.jpg', 'data': {'categories': ['#data', '#cv', '#diffusion', '#training', '#optimization'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· ÑˆÑƒĞ¼Ğ°', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸, Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµĞ¼Ñ‹Ğµ Ğ¸Ğ· Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ÑĞ²Ğ»ÑÑÑ‚ÑÑ Ğ¼Ğ¾Ñ‰Ğ½Ñ‹Ğ¼Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ´ĞµÑĞºÑ€Ğ¸Ğ¿Ñ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ´Ğ½Ğ°ĞºĞ¾ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ÑˆÑƒĞ¼Ğ° Ğº Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼ Ğ¿ĞµÑ€ĞµĞ´ Ğ¸Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ğ»Ğ¸ÑĞµÑ‚ Ğ½Ğ° Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ÑÑ‚ÑŒ ÑÑ‚Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ğ¾Ğ¹ Ğ½ĞµĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ´Ğ¾Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°Ñ‚ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ±ĞµĞ· ÑˆÑƒĞ¼Ğ°. Ğ­Ñ‚Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ñ€Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ….'}, 'en': {'title': 'Unlocking Noise-Free Semantic Features from Diffusion Models', 'desc': 'This paper discusses how internal features from large pre-trained diffusion models can be used as effective semantic descriptors for various tasks. It highlights the problem that these models require added noise to generate useful features, which limits their effectiveness. The authors propose a new, lightweight, unsupervised fine-tuning method that allows these models to produce high-quality semantic features without the need for noise. Their approach significantly improves performance across multiple tasks compared to previous methods, including ensemble techniques, while being more efficient.'}, 'zh': {'title': 'æ— å™ªå£°çš„é«˜è´¨é‡è¯­ä¹‰ç‰¹å¾æå–', 'desc': 'æœ€è¿‘ï¼Œå¤§è§„æ¨¡é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„å†…éƒ¨ç‰¹å¾è¢«ç¡®ç«‹ä¸ºå¼ºå¤§çš„è¯­ä¹‰æè¿°ç¬¦ï¼Œé€‚ç”¨äºå¤šç§ä¸‹æ¸¸ä»»åŠ¡ã€‚é€šå¸¸ï¼Œè¿™äº›ç‰¹å¾éœ€è¦åœ¨å›¾åƒä¸­æ·»åŠ å™ªå£°åæ‰èƒ½æå–ï¼Œå› ä¸ºæ¨¡å‹åœ¨å¤„ç†å‡ ä¹æ²¡æœ‰å™ªå£°çš„å›¾åƒæ—¶ï¼Œæä¾›çš„ç‰¹å¾æ•ˆæœä¸ä½³ã€‚æˆ‘ä»¬å‘ç°å™ªå£°å¯¹ç‰¹å¾çš„æœ‰æ•ˆæ€§æœ‰é‡è¦å½±å“ï¼Œä¸”é€šè¿‡ä¸åŒéšæœºå™ªå£°çš„é›†æˆæ— æ³•è§£å†³è¿™ä¸ªé—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è½»é‡çº§çš„æ— ç›‘ç£å¾®è°ƒæ–¹æ³•ï¼Œä½¿æ‰©æ•£æ¨¡å‹èƒ½å¤Ÿæä¾›é«˜è´¨é‡ã€æ— å™ªå£°çš„è¯­ä¹‰ç‰¹å¾ï¼Œæ˜¾è‘—è¶…è¶Šäº†ä¹‹å‰çš„æ‰©æ•£ç‰¹å¾ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.03085', 'title': 'Mimir: Improving Video Diffusion Models for Precise Text Understanding', 'url': 'https://huggingface.co/papers/2412.03085', 'abstract': 'Text serves as the key control signal in video generation due to its narrative nature. To render text descriptions into video clips, current video diffusion models borrow features from text encoders yet struggle with limited text comprehension. The recent success of large language models (LLMs) showcases the power of decoder-only transformers, which offers three clear benefits for text-to-video (T2V) generation, namely, precise text understanding resulting from the superior scalability, imagination beyond the input text enabled by next token prediction, and flexibility to prioritize user interests through instruction tuning. Nevertheless, the feature distribution gap emerging from the two different text modeling paradigms hinders the direct use of LLMs in established T2V models. This work addresses this challenge with Mimir, an end-to-end training framework featuring a carefully tailored token fuser to harmonize the outputs from text encoders and LLMs. Such a design allows the T2V model to fully leverage learned video priors while capitalizing on the text-related capability of LLMs. Extensive quantitative and qualitative results demonstrate the effectiveness of Mimir in generating high-quality videos with excellent text comprehension, especially when processing short captions and managing shifting motions. Project page: https://lucaria-academy.github.io/Mimir/', 'score': 7, 'issue_id': 957, 'pub_date': '2024-12-04', 'pub_date_card': {'ru': '4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 4', 'zh': '12æœˆ4æ—¥'}, 'hash': 'a065164e5fdadf2c', 'authors': ['Shuai Tan', 'Biao Gong', 'Yutong Feng', 'Kecheng Zheng', 'Dandan Zheng', 'Shuwei Shi', 'Yujun Shen', 'Jingdong Chen', 'Ming Yang'], 'affiliations': ['Ant Group', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2412.03085.jpg', 'data': {'categories': ['#video', '#optimization', '#diffusion', '#multimodal', '#training'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Mimir: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Mimir - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ²Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚ Mimir - ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ 'token fuser', ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ñ‹ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ² Ğ¸ LLM. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Mimir ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²ĞµĞ½ Ğ² ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¸Ğ¼ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ´Ğ»Ñ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½."}, 'en': {'title': 'Mimir: Bridging Text Understanding and Video Generation', 'desc': 'This paper introduces Mimir, a new framework for text-to-video (T2V) generation that combines the strengths of text encoders and large language models (LLMs). It addresses the challenge of feature distribution gaps between these two text modeling approaches, which can hinder effective video generation. Mimir utilizes a specialized token fuser to integrate outputs from both models, enhancing text comprehension and video quality. The results show that Mimir excels in generating videos from short captions and effectively managing dynamic movements.'}, 'zh': {'title': 'Mimirï¼šæå‡æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆçš„æ™ºèƒ½', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºMimirçš„ç«¯åˆ°ç«¯è®­ç»ƒæ¡†æ¶ï¼Œç”¨äºæ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆï¼ˆT2Vï¼‰ã€‚è¯¥æ¡†æ¶é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„ä»¤ç‰Œèåˆå™¨ï¼Œè§£å†³äº†æ–‡æœ¬ç¼–ç å™¨ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¹‹é—´çš„ç‰¹å¾åˆ†å¸ƒå·®è·ã€‚Mimirèƒ½å¤Ÿå……åˆ†åˆ©ç”¨å­¦ä¹ åˆ°çš„è§†é¢‘å…ˆéªŒï¼ŒåŒæ—¶å¢å¼ºLLMsåœ¨æ–‡æœ¬ç†è§£æ–¹é¢çš„èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMimiråœ¨ç”Ÿæˆé«˜è´¨é‡è§†é¢‘æ—¶ï¼Œå°¤å…¶åœ¨å¤„ç†çŸ­æ–‡æœ¬å’ŒåŠ¨æ€å˜åŒ–æ—¶ï¼Œè¡¨ç°å‡ºè‰²ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.03565', 'title': 'Inst-IT: Boosting Multimodal Instance Understanding via Explicit Visual Prompt Instruction Tuning', 'url': 'https://huggingface.co/papers/2412.03565', 'abstract': 'Large Multimodal Models (LMMs) have made significant breakthroughs with the advancement of instruction tuning. However, while existing models can understand images and videos at a holistic level, they still struggle with instance-level understanding that requires a more nuanced comprehension and alignment. Instance-level understanding is crucial, as it focuses on the specific elements that we are most interested in. Excitingly, existing works find that the state-of-the-art LMMs exhibit strong instance understanding capabilities when provided with explicit visual cues. Motivated by this, we introduce an automated annotation pipeline assisted by GPT-4o to extract instance-level information from images and videos through explicit visual prompting for instance guidance. Building upon this pipeline, we proposed Inst-IT, a solution to enhance LMMs in Instance understanding via explicit visual prompt Instruction Tuning. Inst-IT consists of a benchmark to diagnose multimodal instance-level understanding, a large-scale instruction-tuning dataset, and a continuous instruction-tuning training paradigm to effectively enhance spatial-temporal instance understanding capabilities of existing LMMs. Experimental results show that, with the boost of Inst-IT, our models not only achieve outstanding performance on Inst-IT Bench but also demonstrate significant improvements across various generic image and video understanding benchmarks. This highlights that our dataset not only boosts instance-level understanding but also strengthens the overall capabilities of generic image and video comprehension.', 'score': 5, 'issue_id': 967, 'pub_date': '2024-12-04', 'pub_date_card': {'ru': '4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 4', 'zh': '12æœˆ4æ—¥'}, 'hash': '72af31b504d0aac1', 'authors': ['Wujian Peng', 'Lingchen Meng', 'Yitong Chen', 'Yiweng Xie', 'Yang Liu', 'Tao Gui', 'Hang Xu', 'Xipeng Qiu', 'Zuxuan Wu', 'Yu-Gang Jiang'], 'affiliations': ['Huawei Noahs Ark Lab', 'School of Computer Science, Fudan University', 'Shanghai Innovation Institute'], 'pdf_title_img': 'assets/pdf/title_img/2412.03565.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#multimodal', '#alignment', '#dataset', '#training'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑĞºĞ·ĞµĞ¼Ğ¿Ğ»ÑÑ€Ğ¾Ğ² Ğ² LMM Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ²Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Inst-IT - Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑĞºĞ·ĞµĞ¼Ğ¿Ğ»ÑÑ€Ğ¾Ğ² Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LMM) Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ²Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ GPT-4 Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑĞºĞ·ĞµĞ¼Ğ¿Ğ»ÑÑ€Ğ¾Ğ² Ğ¸Ğ· Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. Inst-IT Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚ Ğ´Ğ»Ñ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑĞºĞ·ĞµĞ¼Ğ¿Ğ»ÑÑ€Ğ¾Ğ², Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ğº Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ ÑĞºĞ·ĞµĞ¼Ğ¿Ğ»ÑÑ€Ğ¾Ğ², Ñ‚Ğ°Ğº Ğ¸ Ğ² Ğ¾Ğ±Ñ‰ĞµĞ¼ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'Enhancing Instance Understanding in Multimodal Models with Inst-IT', 'desc': "This paper discusses the limitations of Large Multimodal Models (LMMs) in understanding specific instances within images and videos, despite their overall comprehension abilities. It introduces a new automated annotation pipeline that uses GPT-4o to extract detailed instance-level information through explicit visual prompts. The authors propose a method called Inst-IT, which enhances LMMs' instance understanding by utilizing instruction tuning with a focus on spatial-temporal elements. Experimental results indicate that Inst-IT significantly improves both instance-level understanding and general image and video comprehension across various benchmarks."}, 'zh': {'title': 'æå‡å®ä¾‹ç†è§£èƒ½åŠ›çš„åˆ›æ–°æ–¹æ³•', 'desc': 'å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨æŒ‡ä»¤è°ƒä¼˜æ–¹é¢å–å¾—äº†æ˜¾è‘—çªç ´ï¼Œä½†åœ¨å®ä¾‹çº§ç†è§£ä¸Šä»ç„¶å­˜åœ¨æŒ‘æˆ˜ã€‚å®ä¾‹çº§ç†è§£å…³æ³¨ç‰¹å®šå…ƒç´ ï¼Œè¿™å¯¹äºæ·±å…¥ç†è§£å›¾åƒå’Œè§†é¢‘è‡³å…³é‡è¦ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§è‡ªåŠ¨æ³¨é‡Šç®¡é“ï¼Œåˆ©ç”¨GPT-4oé€šè¿‡æ˜ç¡®çš„è§†è§‰æç¤ºæå–å®ä¾‹çº§ä¿¡æ¯ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬å¼€å‘äº†Inst-ITï¼Œé€šè¿‡æ˜ç¡®çš„è§†è§‰æç¤ºæŒ‡ä»¤è°ƒä¼˜æ¥å¢å¼ºLMMsçš„å®ä¾‹ç†è§£èƒ½åŠ›ï¼Œå®éªŒç»“æœè¡¨æ˜ï¼ŒInst-ITæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨å¤šç§å›¾åƒå’Œè§†é¢‘ç†è§£åŸºå‡†ä¸Šçš„è¡¨ç°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.02980', 'title': 'Surveying the Effects of Quality, Diversity, and Complexity in Synthetic Data From Large Language Models', 'url': 'https://huggingface.co/papers/2412.02980', 'abstract': 'Synthetic data generation with Large Language Models is a promising paradigm for augmenting natural data over a nearly infinite range of tasks. Given this variety, direct comparisons among synthetic data generation algorithms are scarce, making it difficult to understand where improvement comes from and what bottlenecks exist. We propose to evaluate algorithms via the makeup of synthetic data generated by each algorithm in terms of data quality, diversity, and complexity. We choose these three characteristics for their significance in open-ended processes and the impact each has on the capabilities of downstream models. We find quality to be essential for in-distribution model generalization, diversity to be essential for out-of-distribution generalization, and complexity to be beneficial for both. Further, we emphasize the existence of Quality-Diversity trade-offs in training data and the downstream effects on model performance. We then examine the effect of various components in the synthetic data pipeline on each data characteristic. This examination allows us to taxonomize and compare synthetic data generation algorithms through the components they utilize and the resulting effects on data QDC composition. This analysis extends into a discussion on the importance of balancing QDC in synthetic data for efficient reinforcement learning and self-improvement algorithms. Analogous to the QD trade-offs in training data, often there exist trade-offs between model output quality and output diversity which impact the composition of synthetic data. We observe that many models are currently evaluated and optimized only for output quality, thereby limiting output diversity and the potential for self-improvement. We argue that balancing these trade-offs is essential to the development of future self-improvement algorithms and highlight a number of works making progress in this direction.', 'score': 4, 'issue_id': 968, 'pub_date': '2024-12-04', 'pub_date_card': {'ru': '4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 4', 'zh': '12æœˆ4æ—¥'}, 'hash': '8055d4be8211be80', 'authors': ['Alex Havrilla', 'Andrew Dai', "Laura O'Mahony", 'Koen Oostermeijer', 'Vera Zisler', 'Alon Albalak', 'Fabrizio Milo', 'Sharath Chandra Raparthy', 'Kanishk Gandhi', 'Baber Abbasi', 'Duy Phung', 'Maia Iyer', 'Dakota Mahan', 'Chase Blagden', 'Srishti Gureja', 'Mohammed Hamdy', 'Wen-Ding Li', 'Giovanni Paolini', 'Pawan Sasanka Ammanamanchi', 'Elliot Meyerson'], 'affiliations': ['Aleph Alpha @ IPAI', 'Cognizant AI Labs', 'Cohere for AI Community', 'Cornell University', 'Eleuther AI', 'Georgia Tech', 'IBM', 'Independent', 'Reka AI', 'Sakana AI', 'Stanford University', 'SynthLabs', 'University of Bologna', 'University of Limerick'], 'pdf_title_img': 'assets/pdf/title_img/2412.02980.jpg', 'data': {'categories': ['#synthetic', '#optimization', '#training', '#data', '#dataset', '#rl', '#benchmark'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ‘Ğ°Ğ»Ğ°Ğ½Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ² ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ˜Ğ˜', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ñ‹ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ, Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° ÑÑ‚Ğ¸Ñ… Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸Ğº Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ² ÑĞ°Ğ¼Ğ¾ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ‚Ğ¼ĞµÑ‡Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°Ñ Ğ¸Ñ… Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ¸ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» ÑĞ°Ğ¼Ğ¾ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Balancing Quality, Diversity, and Complexity in Synthetic Data Generation', 'desc': 'This paper discusses the generation of synthetic data using Large Language Models (LLMs) and its importance in enhancing natural data for various tasks. It evaluates synthetic data generation algorithms based on three key characteristics: quality, diversity, and complexity, which are crucial for the performance of downstream models. The authors highlight the trade-offs between these characteristics, noting that while quality is vital for in-distribution generalization, diversity is necessary for out-of-distribution generalization. The paper emphasizes the need for a balanced approach to these trade-offs to improve the effectiveness of reinforcement learning and self-improvement algorithms.'}, 'zh': {'title': 'åˆæˆæ•°æ®ç”Ÿæˆï¼šå¹³è¡¡è´¨é‡ä¸å¤šæ ·æ€§', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆåˆæˆæ•°æ®çš„æ½œåŠ›ï¼Œå¼ºè°ƒäº†åˆæˆæ•°æ®åœ¨è‡ªç„¶æ•°æ®å¢å¼ºä¸­çš„é‡è¦æ€§ã€‚æˆ‘ä»¬æå‡ºé€šè¿‡æ•°æ®è´¨é‡ã€å¤šæ ·æ€§å’Œå¤æ‚æ€§æ¥è¯„ä¼°åˆæˆæ•°æ®ç”Ÿæˆç®—æ³•ï¼Œè¿™ä¸‰è€…å¯¹ä¸‹æ¸¸æ¨¡å‹çš„èƒ½åŠ›æœ‰æ˜¾è‘—å½±å“ã€‚ç ”ç©¶å‘ç°ï¼Œæ•°æ®è´¨é‡å¯¹æ¨¡å‹çš„åˆ†å¸ƒå†…æ³›åŒ–è‡³å…³é‡è¦ï¼Œè€Œå¤šæ ·æ€§åˆ™å¯¹åˆ†å¸ƒå¤–æ³›åŒ–è‡³å…³é‡è¦ï¼Œå¤æ‚æ€§å¯¹ä¸¤è€…éƒ½æœ‰ç›Šã€‚æˆ‘ä»¬è¿˜å¼ºè°ƒäº†è®­ç»ƒæ•°æ®ä¸­çš„è´¨é‡-å¤šæ ·æ€§æƒè¡¡åŠå…¶å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ï¼Œè®¤ä¸ºåœ¨æœªæ¥çš„è‡ªæˆ‘æ”¹è¿›ç®—æ³•ä¸­å¹³è¡¡è¿™äº›æƒè¡¡æ˜¯è‡³å…³é‡è¦çš„ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.03187', 'title': 'Weighted-Reward Preference Optimization for Implicit Model Fusion', 'url': 'https://huggingface.co/papers/2412.03187', 'abstract': 'While fusing heterogeneous open-source LLMs with varying architectures and sizes can potentially integrate the strengths of different models, existing fusion methods face significant challenges, such as vocabulary alignment and merging distribution matrices. These procedures are not only complex but also prone to introducing noise and errors. In this paper, we propose an implicit fusion method, Weighted-Reward Preference Optimization (WRPO), which leverages preference optimization between the source LLMs and the target LLM to transfer their capabilities effectively. WRPO eliminates the need for vocabulary alignment and matrix fusion and can be efficiently scaled to accommodate various LLMs. To address distributional deviations between the source and target LLMs, WRPO introduces a progressive adaptation strategy that gradually shifts reliance on preferred examples from the target LLM to the source LLMs. Extensive experiments on the MT-Bench, AlpacaEval-2, and Arena-Hard benchmarks demonstrate that WRPO consistently outperforms existing knowledge fusion methods and various fine-tuning baselines. When applied to LLaMA3-8B-Instruct as the target model, WRPO achieves a length-controlled win rate of 55.9% against GPT-4-Preview-1106 on AlpacaEval-2 and a win rate of 46.2% against GPT-4-0314 on Arena-Hard. Our code is available at https://github.com/SLIT-AI/WRPO.', 'score': 4, 'issue_id': 961, 'pub_date': '2024-12-04', 'pub_date_card': {'ru': '4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 4', 'zh': '12æœˆ4æ—¥'}, 'hash': '6da11fbf4e1ea7d9', 'authors': ['Ziyi Yang', 'Fanqi Wan', 'Longguang Zhong', 'Tianyuan Shi', 'Xiaojun Quan'], 'affiliations': ['School of Computer Science and Engineering, Sun Yat-sen University, China'], 'pdf_title_img': 'assets/pdf/title_img/2412.03187.jpg', 'data': {'categories': ['#optimization', '#transfer_learning', '#open_source', '#architecture', '#training', '#benchmark'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'WRPO: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ¿Ñ€ÑĞ¼Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼ - Weighted-Reward Preference Optimization (WRPO). WRPO Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ° Ğ¸Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹, ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑÑ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ² Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ²Ğ°Ñ€ĞµĞ¹ Ğ¸ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğ¸ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ† Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ğ¹ Ğ² Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸ÑÑ… Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ WRPO Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğº Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ….'}, 'en': {'title': 'Effortless Fusion of LLMs with WRPO!', 'desc': 'This paper introduces a new method called Weighted-Reward Preference Optimization (WRPO) for fusing different open-source large language models (LLMs). WRPO simplifies the fusion process by avoiding complex tasks like vocabulary alignment and distribution matrix merging, which often introduce errors. Instead, it uses a preference optimization approach to effectively transfer capabilities from source LLMs to a target LLM. The method also includes a progressive adaptation strategy to manage differences in distributions between models, leading to improved performance on various benchmarks compared to existing methods.'}, 'zh': {'title': 'åŠ æƒå¥–åŠ±åå¥½ä¼˜åŒ–ï¼šé«˜æ•ˆèåˆå¤šç§å¤§è¯­è¨€æ¨¡å‹', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§éšå¼èåˆæ–¹æ³•ï¼Œç§°ä¸ºåŠ æƒå¥–åŠ±åå¥½ä¼˜åŒ–ï¼ˆWRPOï¼‰ï¼Œæ—¨åœ¨æœ‰æ•ˆæ•´åˆä¸åŒæ¶æ„å’Œè§„æ¨¡çš„å¼€æºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ã€‚WRPOé€šè¿‡ä¼˜åŒ–æºæ¨¡å‹ä¸ç›®æ ‡æ¨¡å‹ä¹‹é—´çš„åå¥½ï¼Œé¿å…äº†è¯æ±‡å¯¹é½å’ŒçŸ©é˜µèåˆçš„å¤æ‚æ€§ã€‚è¯¥æ–¹æ³•å¼•å…¥äº†æ¸è¿›é€‚åº”ç­–ç•¥ï¼Œé€æ­¥è°ƒæ•´å¯¹ç›®æ ‡æ¨¡å‹å’Œæºæ¨¡å‹çš„ä¾èµ–ï¼Œä»è€Œè§£å†³äº†åˆ†å¸ƒåå·®é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒWRPOåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºç°æœ‰çš„çŸ¥è¯†èåˆæ–¹æ³•å’Œå¾®è°ƒåŸºçº¿ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.00177', 'title': 'LumiNet: Latent Intrinsics Meets Diffusion Models for Indoor Scene Relighting', 'url': 'https://huggingface.co/papers/2412.00177', 'abstract': "We introduce LumiNet, a novel architecture that leverages generative models and latent intrinsic representations for effective lighting transfer. Given a source image and a target lighting image, LumiNet synthesizes a relit version of the source scene that captures the target's lighting. Our approach makes two key contributions: a data curation strategy from the StyleGAN-based relighting model for our training, and a modified diffusion-based ControlNet that processes both latent intrinsic properties from the source image and latent extrinsic properties from the target image. We further improve lighting transfer through a learned adaptor (MLP) that injects the target's latent extrinsic properties via cross-attention and fine-tuning.   Unlike traditional ControlNet, which generates images with conditional maps from a single scene, LumiNet processes latent representations from two different images - preserving geometry and albedo from the source while transferring lighting characteristics from the target. Experiments demonstrate that our method successfully transfers complex lighting phenomena including specular highlights and indirect illumination across scenes with varying spatial layouts and materials, outperforming existing approaches on challenging indoor scenes using only images as input.", 'score': 2, 'issue_id': 969, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 29', 'zh': '11æœˆ29æ—¥'}, 'hash': '210b042d1a430116', 'authors': ['Xiaoyan Xing', 'Konrad Groh', 'Sezer Karaoglu', 'Theo Gevers', 'Anand Bhattad'], 'affiliations': ['BCAI-Bosch', 'Toyota Technological Institute at Chicago', 'UvA-Bosch Delta Lab'], 'pdf_title_img': 'assets/pdf/title_img/2412.00177.jpg', 'data': {'categories': ['#data', '#optimization', '#cv', '#diffusion', '#architecture'], 'emoji': 'ğŸ’¡', 'ru': {'title': 'LumiNet: Ğ ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ Ğ¿ĞµÑ€ĞµĞ½Ğ¾Ñ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹', 'desc': 'LumiNet - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ° Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ. ĞĞ½Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ StyleGAN Ğ¸ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ²ĞµÑ€ÑĞ¸Ñ ControlNet, Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰ÑƒÑ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ° Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. LumiNet Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ½Ğ¾Ñ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ğ°, Ğ²Ğ½ĞµĞ´Ñ€ÑÑÑ‰ĞµĞ³Ğ¾ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğµ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ° Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ¸Ñ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ ÑĞ²ĞµÑ‚Ğ¾Ğ²Ñ‹Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ñ‹ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑÑ†ĞµĞ½Ğ°Ğ¼Ğ¸ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸ĞµĞ¹ Ğ¸ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'LumiNet: Mastering Lighting Transfer with Generative Models', 'desc': 'LumiNet is a new machine learning architecture designed for transferring lighting effects from one image to another. It uses generative models to create a relit version of a source image by applying the lighting characteristics of a target image. The model incorporates a unique data curation strategy and a modified diffusion-based ControlNet that processes both intrinsic and extrinsic properties of the images. By utilizing cross-attention and fine-tuning, LumiNet effectively captures complex lighting phenomena, outperforming traditional methods in challenging indoor environments.'}, 'zh': {'title': 'LumiNetï¼šé«˜æ•ˆå…‰ç…§è½¬ç§»çš„æ–°æ–¹æ³•', 'desc': 'LumiNetæ˜¯ä¸€ç§æ–°é¢–çš„æ¶æ„ï¼Œåˆ©ç”¨ç”Ÿæˆæ¨¡å‹å’Œæ½œåœ¨å†…åœ¨è¡¨ç¤ºæ¥å®ç°æœ‰æ•ˆçš„å…‰ç…§è½¬ç§»ã€‚è¯¥æ–¹æ³•é€šè¿‡è¾“å…¥æºå›¾åƒå’Œç›®æ ‡å…‰ç…§å›¾åƒï¼Œåˆæˆå‡ºä¸€ä¸ªæ•æ‰ç›®æ ‡å…‰ç…§çš„é‡æ–°ç…§æ˜ç‰ˆæœ¬ã€‚LumiNetçš„ä¸¤ä¸ªå…³é”®è´¡çŒ®åŒ…æ‹¬åŸºäºStyleGANçš„é‡æ–°ç…§æ˜æ¨¡å‹çš„æ•°æ®æ•´ç†ç­–ç•¥ï¼Œä»¥åŠå¤„ç†æºå›¾åƒçš„æ½œåœ¨å†…åœ¨å±æ€§å’Œç›®æ ‡å›¾åƒçš„æ½œåœ¨å¤–åœ¨å±æ€§çš„æ”¹è¿›æ‰©æ•£æ§åˆ¶ç½‘ç»œã€‚é€šè¿‡äº¤å‰æ³¨æ„åŠ›å’Œå¾®è°ƒï¼ŒLumiNetè¿›ä¸€æ­¥é€šè¿‡å­¦ä¹ é€‚é…å™¨ï¼ˆMLPï¼‰æ³¨å…¥ç›®æ ‡çš„æ½œåœ¨å¤–åœ¨å±æ€§ï¼Œä»è€Œæ”¹å–„å…‰ç…§è½¬ç§»æ•ˆæœã€‚'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (27)', '#agents (11)', '#agi (3)', '#alignment (11)', '#architecture (52)', '#audio (4)', '#benchmark (64)', '#cv (51)', '#data (26)', '#dataset (69)', '#diffusion (51)', '#ethics (9)', '#games (21)', '#graphs (1)', '#hallucinations (9)', '#healthcare (2)', '#inference (19)', '#interpretability (10)', '#leakage (1)', '#long_context (6)', '#low_resource (8)', '#machine_translation (5)', '#math (6)', '#multilingual (7)', '#multimodal (69)', '#open_source (39)', '#optimization (75)', '#plp (1)', '#rag (4)', '#reasoning (22)', '#rl (4)', '#rlhf (5)', '#robotics (5)', '#science (1)', '#security (5)', '#small_models (8)', '#story_generation (3)', '#survey (3)', '#synthetic (22)', '#training (89)', '#transfer_learning (11)', '#video (45)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            ğŸ”º ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'ğŸ”„ ' + getTimeDiff('2024-12-12 21:10',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ",
                    pub_date: "Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸",
                    issue_id: "Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "è¯„åˆ†",
                    pub_date: "å‘å¸ƒæ—¥æœŸ",
                    issue_id: "HFä¸Šä¼ æ—¥æœŸ"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2024-12-12 21:10')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2024-12-12 21:10')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('monthly'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    