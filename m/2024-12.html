
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 55 papers. December 2024.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #7a30efcf;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: #7a30efcf;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #7a30ef17;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ", "Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹", "Ğ¼Ğ¸Ğ½ÑƒÑ‚"],
                hour: ["Ñ‡Ğ°Ñ", "Ñ‡Ğ°ÑĞ°", "Ñ‡Ğ°ÑĞ¾Ğ²"],
                day: ["Ğ´ĞµĞ½ÑŒ", "Ğ´Ğ½Ñ", "Ğ´Ğ½ĞµĞ¹"],
                justNow: "Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ñ‚Ğ¾",
                ago: "Ğ½Ğ°Ğ·Ğ°Ğ´"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["åˆ†é’Ÿ", "åˆ†é’Ÿ", "åˆ†é’Ÿ"],
                hour: ["å°æ—¶", "å°æ—¶", "å°æ—¶"],
                day: ["å¤©", "å¤©", "å¤©"],
                justNow: "åˆšåˆš",
                ago: "å‰"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            } else if (lastDigit === 1) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒÑ";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒĞ¸";
            } else {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ç¯‡è®ºæ–‡"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">ğŸ”º</h1><h1 class="title-text" id="doomgrad">hf monthly</h1></a>
            <p><span id="title-date">Ğ”ĞµĞºĞ°Ğ±Ñ€ÑŒ 2024</span> | <span id="title-articles-count">55 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/m/2024-11.html">â¬…ï¸ <span id="prev-date">11.2024</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/m/2025-01.html">â¡ï¸ <span id="next-date">01.2025</span></a></span>
            <span class="nav-item" id="nav-daily"><a href="https://hfday.ru">ğŸ“ˆ <span id='top-day-label'>Ğ”ĞµĞ½ÑŒ</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">ğŸ”€ <span id="sort-label-text">Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ</option>
                    <option value="pub_date">Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸</option>
                    <option value="issue_id">Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">ğŸ·ï¸ Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> AâˆªB</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> Aâˆ©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">ğŸ§¹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> âœ–ï¸ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': 'Ğ”ĞµĞºĞ°Ğ±Ñ€ÑŒ 2024', 'en': 'December 2024', 'zh': '12æœˆ2024å¹´'};
        let feedDateNext = {'ru': '01.2025', 'en': '01/2025', 'zh': '1æœˆ2025å¹´'};
        let feedDatePrev = {'ru': '11.2024', 'en': '11/2024', 'zh': '11æœˆ2024å¹´'};
        let filterLabel = {'ru': 'Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€', 'en': 'Topics', 'zh': 'ä¸»é¢˜ç­›é€‰'}
        let publishedLabel = {'ru': 'ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ñ‚ ', 'en': 'published on ', 'zh': 'å‘è¡¨äº'}
        let sortLabel = {'ru': 'Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾', 'en': 'Sort by', 'zh': 'æ’åºæ–¹å¼'}
        let paperLabel = {'ru': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ', 'en': 'Paper', 'zh': 'è®ºæ–‡'}
        let topMonthLabel = {'ru': 'ĞœĞµÑÑÑ†', 'en': 'Month', 'zh': 'æœˆåº¦è®ºæ–‡'}
        let topDayLabel = {'ru': 'Ğ”ĞµĞ½ÑŒ', 'en': 'Day', 'zh': 'æ—¥åº¦è®ºæ–‡'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf monthly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2412.01824', 'title': 'X-Prompt: Towards Universal In-Context Image Generation in Auto-Regressive Vision Language Foundation Models', 'url': 'https://huggingface.co/papers/2412.01824', 'abstract': "In-context generation is a key component of large language models' (LLMs) open-task generalization capability. By leveraging a few examples as context, LLMs can perform both in-domain and out-of-domain tasks. Recent advancements in auto-regressive vision-language models (VLMs) built upon LLMs have showcased impressive performance in text-to-image generation. However, the potential of in-context learning for general image generation tasks remains largely unexplored. To address this, we introduce X-Prompt, a purely auto-regressive large-vision language model designed to deliver competitive performance across a wide range of both seen and unseen image generation tasks, all within a unified in-context learning framework. X-Prompt incorporates a specialized design that efficiently compresses valuable features from in-context examples, supporting longer in-context token sequences and improving its ability to generalize to unseen tasks. A unified training task for both text and image prediction enables X-Prompt to handle general image generation with enhanced task awareness from in-context examples. Extensive experiments validate the model's performance across diverse seen image generation tasks and its capacity to generalize to previously unseen tasks.", 'score': 37, 'issue_id': 911, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 2', 'zh': '12æœˆ2æ—¥'}, 'hash': '6b7c2d6555d8df8d', 'authors': ['Zeyi Sun', 'Ziyang Chu', 'Pan Zhang', 'Tong Wu', 'Xiaoyi Dong', 'Yuhang Zang', 'Yuanjun Xiong', 'Dahua Lin', 'Jiaqi Wang'], 'affiliations': ['CPII under InnoHK', 'MThreads AI', 'Shanghai AI Laboratory', 'Shanghai Jiao Tong University', 'The Chinese University of Hong Kong', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2412.01824.jpg', 'data': {'categories': ['#agi', '#cv', '#games', '#optimization', '#training', '#multimodal'], 'emoji': 'ğŸ¨', 'ru': {'title': 'X-Prompt: ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ X-Prompt - Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ĞµĞ¹ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ ĞºĞ°Ğº Ğ·Ğ½Ğ°ĞºĞ¾Ğ¼Ñ‹Ğµ, Ñ‚Ğ°Ğº Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. X-Prompt ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑĞ¶Ğ¸Ğ¼Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ¸Ğ· ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ€Ğ°Ğ½ĞµĞµ Ğ½Ğµ Ğ²ÑÑ‚Ñ€ĞµÑ‡Ğ°Ğ²ÑˆĞ¸ĞµÑÑ.'}, 'en': {'title': 'X-Prompt: Unlocking Image Generation with In-Context Learning', 'desc': 'This paper presents X-Prompt, an advanced auto-regressive vision-language model that enhances image generation through in-context learning. By utilizing a few examples as context, X-Prompt can effectively generate images for both familiar and novel tasks. The model is designed to compress important features from these examples, allowing it to manage longer sequences of context and improve generalization. Extensive testing shows that X-Prompt performs well on a variety of image generation tasks, demonstrating its ability to adapt to new challenges.'}, 'zh': {'title': 'X-Promptï¼šæå‡å›¾åƒç”Ÿæˆçš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºX-Promptçš„è‡ªå›å½’å¤§è§„æ¨¡è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨æå‡å›¾åƒç”Ÿæˆä»»åŠ¡çš„è¡¨ç°ã€‚X-Prompté€šè¿‡åˆ©ç”¨ä¸Šä¸‹æ–‡ä¸­çš„ç¤ºä¾‹ï¼Œèƒ½å¤Ÿåœ¨å·²çŸ¥å’ŒæœªçŸ¥çš„å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­å®ç°ç«äº‰åŠ›çš„æ€§èƒ½ã€‚è¯¥æ¨¡å‹é‡‡ç”¨äº†ä¸“é—¨çš„è®¾è®¡ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå‹ç¼©ä¸Šä¸‹æ–‡ç¤ºä¾‹ä¸­çš„é‡è¦ç‰¹å¾ï¼Œä»è€Œæ”¯æŒæ›´é•¿çš„ä¸Šä¸‹æ–‡åºåˆ—å¹¶æé«˜å¯¹æœªçŸ¥ä»»åŠ¡çš„æ³›åŒ–èƒ½åŠ›ã€‚é€šè¿‡ç»Ÿä¸€çš„è®­ç»ƒä»»åŠ¡ï¼ŒX-Promptåœ¨æ–‡æœ¬å’Œå›¾åƒé¢„æµ‹æ–¹é¢è¡¨ç°å‡ºè‰²ï¼ŒéªŒè¯äº†å…¶åœ¨å¤šæ ·åŒ–å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.00131', 'title': 'Open-Sora Plan: Open-Source Large Video Generation Model', 'url': 'https://huggingface.co/papers/2412.00131', 'abstract': 'We introduce Open-Sora Plan, an open-source project that aims to contribute a large generation model for generating desired high-resolution videos with long durations based on various user inputs. Our project comprises multiple components for the entire video generation process, including a Wavelet-Flow Variational Autoencoder, a Joint Image-Video Skiparse Denoiser, and various condition controllers. Moreover, many assistant strategies for efficient training and inference are designed, and a multi-dimensional data curation pipeline is proposed for obtaining desired high-quality data. Benefiting from efficient thoughts, our Open-Sora Plan achieves impressive video generation results in both qualitative and quantitative evaluations. We hope our careful design and practical experience can inspire the video generation research community. All our codes and model weights are publicly available at https://github.com/PKU-YuanGroup/Open-Sora-Plan.', 'score': 19, 'issue_id': 911, 'pub_date': '2024-11-28', 'pub_date_card': {'ru': '28 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 28', 'zh': '11æœˆ28æ—¥'}, 'hash': 'fa9b0009de797ca2', 'authors': ['Bin Lin', 'Yunyang Ge', 'Xinhua Cheng', 'Zongjian Li', 'Bin Zhu', 'Shaodong Wang', 'Xianyi He', 'Yang Ye', 'Shenghai Yuan', 'Liuhan Chen', 'Tanghui Jia', 'Junwu Zhang', 'Zhenyu Tang', 'Yatian Pang', 'Bin She', 'Cen Yan', 'Zhiheng Hu', 'Xiaoyi Dong', 'Lin Chen', 'Zhang Pan', 'Xing Zhou', 'Shaoling Dong', 'Yonghong Tian', 'Li Yuan'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2412.00131.jpg', 'data': {'categories': ['#open_source', '#data', '#inference', '#training', '#video'], 'emoji': 'ğŸ¬', 'ru': {'title': 'ĞÑ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'ĞŸÑ€Ğ¾ĞµĞºÑ‚ Open-Sora Plan Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ğ²ĞµĞ¹Ğ²Ğ»ĞµÑ‚-Ğ¿Ğ¾Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€, ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ñ‹Ğ¹ Ğ´ĞµĞ½Ğ¾Ğ¹Ğ·ĞµÑ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ»ĞµÑ€Ñ‹ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğ¹. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ñ‹ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼ĞµÑ€Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ ĞºÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞŸÑ€Ğ¾ĞµĞºÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³ Ğ²Ğ¿ĞµÑ‡Ğ°Ñ‚Ğ»ÑÑÑ‰Ğ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ ĞºĞ°Ğº Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ² ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ñ†ĞµĞ½ĞºĞ°Ñ….'}, 'en': {'title': 'Empowering High-Resolution Video Generation with Open-Sora Plan', 'desc': 'The Open-Sora Plan is an open-source initiative focused on creating a large-scale generative model for producing high-resolution videos that can last for extended periods, tailored to user specifications. It integrates several advanced components, including a Wavelet-Flow Variational Autoencoder and a Joint Image-Video Skiparse Denoiser, to enhance the video generation process. The project also introduces various condition controllers and efficient training strategies, along with a multi-dimensional data curation pipeline to ensure high-quality output. Overall, the Open-Sora Plan demonstrates significant advancements in video generation, aiming to inspire further research in the field.'}, 'zh': {'title': 'å¼€æ”¾æºä»£ç ï¼Œç”Ÿæˆé«˜è´¨é‡è§†é¢‘çš„æœªæ¥', 'desc': 'Open-Soraè®¡åˆ’æ˜¯ä¸€ä¸ªå¼€æºé¡¹ç›®ï¼Œæ—¨åœ¨åŸºäºç”¨æˆ·è¾“å…¥ç”Ÿæˆé«˜åˆ†è¾¨ç‡çš„é•¿æ—¶è§†é¢‘ã€‚è¯¥é¡¹ç›®åŒ…å«å¤šä¸ªç»„ä»¶ï¼Œå¦‚å°æ³¢æµå˜åˆ†è‡ªç¼–ç å™¨å’Œè”åˆå›¾åƒ-è§†é¢‘å»å™ªå™¨ï¼Œæ”¯æŒæ•´ä¸ªè§†é¢‘ç”Ÿæˆè¿‡ç¨‹ã€‚æˆ‘ä»¬è¿˜è®¾è®¡äº†å¤šç§é«˜æ•ˆçš„è®­ç»ƒå’Œæ¨ç†ç­–ç•¥ï¼Œå¹¶æå‡ºäº†å¤šç»´æ•°æ®ç­–åˆ’ç®¡é“ï¼Œä»¥è·å–é«˜è´¨é‡æ•°æ®ã€‚é€šè¿‡è¿™äº›é«˜æ•ˆçš„è®¾è®¡ï¼ŒOpen-Soraè®¡åˆ’åœ¨è§†é¢‘ç”Ÿæˆçš„å®šæ€§å’Œå®šé‡è¯„ä¼°ä¸­å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.00927', 'title': 'VISTA: Enhancing Long-Duration and High-Resolution Video Understanding by Video Spatiotemporal Augmentation', 'url': 'https://huggingface.co/papers/2412.00927', 'abstract': 'Current large multimodal models (LMMs) face significant challenges in processing and comprehending long-duration or high-resolution videos, which is mainly due to the lack of high-quality datasets. To address this issue from a data-centric perspective, we propose VISTA, a simple yet effective Video Spatiotemporal Augmentation framework that synthesizes long-duration and high-resolution video instruction-following pairs from existing video-caption datasets. VISTA spatially and temporally combines videos to create new synthetic videos with extended durations and enhanced resolutions, and subsequently produces question-answer pairs pertaining to these newly synthesized videos. Based on this paradigm, we develop seven video augmentation methods and curate VISTA-400K, a video instruction-following dataset aimed at enhancing long-duration and high-resolution video understanding. Finetuning various video LMMs on our data resulted in an average improvement of 3.3% across four challenging benchmarks for long-video understanding. Furthermore, we introduce the first comprehensive high-resolution video understanding benchmark HRVideoBench, on which our finetuned models achieve a 6.5% performance gain. These results highlight the effectiveness of our framework.', 'score': 17, 'issue_id': 912, 'pub_date': '2024-12-01', 'pub_date_card': {'ru': '1 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 1', 'zh': '12æœˆ1æ—¥'}, 'hash': 'b4065e6e554dea31', 'authors': ['Weiming Ren', 'Huan Yang', 'Jie Min', 'Cong Wei', 'Wenhu Chen'], 'affiliations': ['University of Waterloo', 'Vector Institute'], 'pdf_title_img': 'assets/pdf/title_img/2412.00927.jpg', 'data': {'categories': ['#multimodal', '#video', '#dataset', '#data', '#long_context', '#benchmark', '#synthetic'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Ğ¡Ğ¸Ğ½Ñ‚ĞµĞ· Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ˜Ğ˜ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'VISTA - ÑÑ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. ĞĞ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒÑ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾, Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ€Ñ‹ Ğº Ğ½Ğ¸Ğ¼. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ VISTA ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ VISTA-400K, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ» ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° 3.3% Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº HRVideoBench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Enhancing Video Understanding with VISTA: A Data-Centric Approach', 'desc': "This paper introduces VISTA, a framework designed to improve the understanding of long-duration and high-resolution videos by creating synthetic video instruction-following pairs. VISTA utilizes existing video-caption datasets to spatially and temporally augment videos, generating new high-quality video data. The authors developed seven augmentation methods and created the VISTA-400K dataset, which significantly enhances the training of large multimodal models (LMMs). The results show that finetuning LMMs on this dataset leads to notable performance improvements on various benchmarks, demonstrating the framework's effectiveness in video comprehension tasks."}, 'zh': {'title': 'VISTAï¼šæå‡è§†é¢‘ç†è§£çš„æ–°æ–¹æ³•', 'desc': 'å½“å‰çš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨å¤„ç†é•¿æ—¶é•¿æˆ–é«˜åˆ†è¾¨ç‡è§†é¢‘æ—¶é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼Œä¸»è¦æ˜¯ç”±äºç¼ºä¹é«˜è´¨é‡çš„æ•°æ®é›†ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†VISTAï¼Œä¸€ä¸ªç®€å•è€Œæœ‰æ•ˆçš„è§†é¢‘æ—¶ç©ºå¢å¼ºæ¡†æ¶ï¼Œèƒ½å¤Ÿä»ç°æœ‰çš„è§†é¢‘-å­—å¹•æ•°æ®é›†ä¸­åˆæˆé•¿æ—¶é•¿å’Œé«˜åˆ†è¾¨ç‡çš„è§†é¢‘æŒ‡ä»¤å¯¹ã€‚VISTAé€šè¿‡ç©ºé—´å’Œæ—¶é—´çš„ç»„åˆï¼Œåˆ›å»ºæ–°çš„åˆæˆè§†é¢‘ï¼Œå¹¶ç”Ÿæˆä¸è¿™äº›æ–°åˆæˆè§†é¢‘ç›¸å…³çš„é—®é¢˜-ç­”æ¡ˆå¯¹ã€‚é€šè¿‡åœ¨æˆ‘ä»¬çš„æ•°æ®ä¸Šå¾®è°ƒå„ç§è§†é¢‘å¤šæ¨¡æ€æ¨¡å‹ï¼Œå¹³å‡æé«˜äº†3.3%çš„æ€§èƒ½ï¼Œè¿›ä¸€æ­¥éªŒè¯äº†æˆ‘ä»¬æ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.18499', 'title': 'GATE OpenING: A Comprehensive Benchmark for Judging Open-ended Interleaved Image-Text Generation', 'url': 'https://huggingface.co/papers/2411.18499', 'abstract': 'Multimodal Large Language Models (MLLMs) have made significant strides in visual understanding and generation tasks. However, generating interleaved image-text content remains a challenge, which requires integrated multimodal understanding and generation abilities. While the progress in unified models offers new solutions, existing benchmarks are insufficient for evaluating these methods due to data size and diversity limitations. To bridge this gap, we introduce GATE OpenING (OpenING), a comprehensive benchmark comprising 5,400 high-quality human-annotated instances across 56 real-world tasks. OpenING covers diverse daily scenarios such as travel guide, design, and brainstorming, offering a robust platform for challenging interleaved generation methods. In addition, we present IntJudge, a judge model for evaluating open-ended multimodal generation methods. Trained with a novel data pipeline, our IntJudge achieves an agreement rate of 82. 42% with human judgments, outperforming GPT-based evaluators by 11.34%. Extensive experiments on OpenING reveal that current interleaved generation methods still have substantial room for improvement. Key findings on interleaved image-text generation are further presented to guide the development of next-generation models. The OpenING is open-sourced at https://opening.github.io.', 'score': 16, 'issue_id': 914, 'pub_date': '2024-11-27', 'pub_date_card': {'ru': '27 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 27', 'zh': '11æœˆ27æ—¥'}, 'hash': 'bec1bf0079934886', 'authors': ['Pengfei Zhou', 'Xiaopeng Peng', 'Jiajun Song', 'Chuanhao Li', 'Zhaopan Xu', 'Yue Yang', 'Ziyao Guo', 'Hao Zhang', 'Yuqi Lin', 'Yefei He', 'Lirui Zhao', 'Shuo Liu', 'Tianhua Li', 'Yuxuan Xie', 'Xiaojun Chang', 'Yu Qiao', 'Wenqi Shao', 'Kaipeng Zhang'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2411.18499.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#open_source', '#dataset', '#games'], 'emoji': 'ğŸ”€', 'ru': {'title': 'ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… GATE OpenING Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ÑÑ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾-Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°. OpenING Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 5400 Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¸Ğ· 56 Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ IntJudge Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ Ñ‚Ğ¸Ğ¿Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ GPT-based Ğ¾Ñ†ĞµĞ½Ñ‰Ğ¸ĞºĞ¾Ğ² Ğ½Ğ° 11.34%. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ÑÑ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ¸Ğ¼ĞµÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Bridging the Gap in Multimodal Generation with OpenING!', 'desc': 'This paper discusses the advancements in Multimodal Large Language Models (MLLMs) for tasks that involve both images and text. It highlights the challenges of generating content that combines these two modalities effectively, which requires a deep understanding of both. To address the limitations of existing benchmarks, the authors introduce GATE OpenING, a new dataset with 5,400 annotated examples across 56 tasks that reflect real-world scenarios. Additionally, they present IntJudge, a model designed to evaluate multimodal generation methods, which shows improved agreement with human judgments compared to previous evaluators.'}, 'zh': {'title': 'æ¨åŠ¨å¤šæ¨¡æ€ç”Ÿæˆçš„åŸºå‡†ä¸è¯„ä¼°', 'desc': 'å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†è§‰ç†è§£å’Œç”Ÿæˆä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œç”Ÿæˆäº¤é”™çš„å›¾åƒ-æ–‡æœ¬å†…å®¹ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œè¿™éœ€è¦ç»¼åˆçš„å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†GATE OpenINGï¼ˆOpenINGï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«5400ä¸ªé«˜è´¨é‡äººç±»æ ‡æ³¨å®ä¾‹çš„ç»¼åˆåŸºå‡†ï¼Œæ¶µç›–56ä¸ªçœŸå®ä¸–ç•Œä»»åŠ¡ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¿˜æå‡ºäº†IntJudgeï¼Œä¸€ä¸ªç”¨äºè¯„ä¼°å¼€æ”¾å¼å¤šæ¨¡æ€ç”Ÿæˆæ–¹æ³•çš„è¯„ä¼°æ¨¡å‹ï¼Œæ˜¾ç¤ºå‡ºä¸äººç±»åˆ¤æ–­çš„é«˜ä¸€è‡´æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.01819', 'title': 'Switti: Designing Scale-Wise Transformers for Text-to-Image Synthesis', 'url': 'https://huggingface.co/papers/2412.01819', 'abstract': 'This work presents Switti, a scale-wise transformer for text-to-image generation. Starting from existing next-scale prediction AR models, we first explore them for T2I generation and propose architectural modifications to improve their convergence and overall performance. We then observe that self-attention maps of our pretrained scale-wise AR model exhibit weak dependence on preceding scales. Based on this insight, we propose a non-AR counterpart facilitating {sim}11% faster sampling and lower memory usage while also achieving slightly better generation quality.Furthermore, we reveal that classifier-free guidance at high-resolution scales is often unnecessary and can even degrade performance. %may be not only unnecessary but potentially detrimental. By disabling guidance at these scales, we achieve an additional sampling acceleration of {sim}20% and improve the generation of fine-grained details. Extensive human preference studies and automated evaluations show that Switti outperforms existing T2I AR models and competes with state-of-the-art T2I diffusion models while being up to 7{times} faster.', 'score': 15, 'issue_id': 914, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 2', 'zh': '12æœˆ2æ—¥'}, 'hash': '25cf4512f592aea4', 'authors': ['Anton Voronov', 'Denis Kuznedelev', 'Mikhail Khoroshikh', 'Valentin Khrulkov', 'Dmitry Baranchuk'], 'affiliations': ['HSE University', 'MIPT', 'Skoltech', 'Yandex Research'], 'pdf_title_img': 'assets/pdf/title_img/2412.01819.jpg', 'data': {'categories': ['#diffusion', '#architecture', '#video', '#cv', '#optimization'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'Switti: Ğ±Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¹ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Switti - Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ñ Ğ¾ ÑĞ»Ğ°Ğ±Ğ¾Ğ¹ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ ĞºĞ°Ñ€Ñ‚ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¾Ğ², Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° Ğ½ĞµĞ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ²ĞµÑ€ÑĞ¸Ñ, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‰Ğ°Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¸ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ñ‚ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ guidance Ğ½Ğ° Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸ÑÑ… Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ.'}, 'en': {'title': 'Switti: Accelerating Text-to-Image Generation with Scale-Wise Transformers', 'desc': 'This paper introduces Switti, a new transformer model designed for generating images from text. It builds on existing autoregressive (AR) models by making architectural changes that enhance performance and speed. The authors find that the self-attention mechanisms in their model do not rely heavily on previous scales, allowing for a non-autoregressive approach that speeds up sampling and reduces memory usage. Additionally, they demonstrate that removing classifier-free guidance at higher resolutions can improve detail generation and further accelerate the process, leading to a model that is significantly faster than current state-of-the-art methods.'}, 'zh': {'title': 'Swittiï¼šåŠ é€Ÿæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„å˜æ¢å™¨', 'desc': 'æœ¬æ–‡ä»‹ç»äº†Swittiï¼Œä¸€ç§ç”¨äºæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„å°ºåº¦å˜æ¢å™¨ã€‚æˆ‘ä»¬ä»ç°æœ‰çš„ä¸‹ä¸€å°ºåº¦é¢„æµ‹è‡ªå›å½’æ¨¡å‹å‡ºå‘ï¼Œæ¢ç´¢å…¶åœ¨T2Iç”Ÿæˆä¸­çš„åº”ç”¨ï¼Œå¹¶æå‡ºæ¶æ„ä¿®æ”¹ä»¥æé«˜æ”¶æ•›æ€§å’Œæ•´ä½“æ€§èƒ½ã€‚ç ”ç©¶å‘ç°ï¼Œæˆ‘ä»¬çš„é¢„è®­ç»ƒå°ºåº¦è‡ªå›å½’æ¨¡å‹çš„è‡ªæ³¨æ„åŠ›å›¾å¯¹å‰ä¸€å°ºåº¦çš„ä¾èµ–æ€§è¾ƒå¼±ï¼Œå› æ­¤æˆ‘ä»¬æå‡ºäº†ä¸€ç§éè‡ªå›å½’çš„æ›¿ä»£æ–¹æ¡ˆï¼Œèƒ½å¤Ÿå®ç°çº¦11%çš„é‡‡æ ·åŠ é€Ÿå’Œæ›´ä½çš„å†…å­˜ä½¿ç”¨ï¼ŒåŒæ—¶ç”Ÿæˆè´¨é‡ç•¥æœ‰æå‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°é«˜åˆ†è¾¨ç‡å°ºåº¦ä¸‹çš„æ— åˆ†ç±»å™¨å¼•å¯¼é€šå¸¸æ˜¯ä¸å¿…è¦çš„ï¼Œç”šè‡³å¯èƒ½ä¼šé™ä½æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.00154', 'title': 'o1-Coder: an o1 Replication for Coding', 'url': 'https://huggingface.co/papers/2412.00154', 'abstract': "The technical report introduces O1-CODER, an attempt to replicate OpenAI's o1 model with a focus on coding tasks. It integrates reinforcement learning (RL) and Monte Carlo Tree Search (MCTS) to enhance the model's System-2 thinking capabilities. The framework includes training a Test Case Generator (TCG) for standardized code testing, using MCTS to generate code data with reasoning processes, and iteratively fine-tuning the policy model to initially produce pseudocode, followed by the generation of the full code. The report also addresses the opportunities and challenges in deploying o1-like models in real-world applications, suggesting transitioning to the System-2 paradigm and highlighting the imperative for environment state updates. Updated model progress and experimental results will be reported in subsequent versions. All source code, curated datasets, as well as the derived models will be disclosed at https://github.com/ADaM-BJTU/O1-CODER .", 'score': 14, 'issue_id': 912, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 29', 'zh': '11æœˆ29æ—¥'}, 'hash': '4a9b72e0b9a6ba64', 'authors': ['Yuxiang Zhang', 'Shangxi Wu', 'Yuqi Yang', 'Jiangming Shu', 'Jinlin Xiao', 'Chao Kong', 'Jitao Sang'], 'affiliations': ['School of Computer Science and Technology, Beijing Jiaotong University, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2412.00154.jpg', 'data': {'categories': ['#dataset', '#training', '#open_source', '#optimization', '#reasoning', '#rl'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'O1-CODER: Ğ£ÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ Ğ˜Ğ˜ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ñƒ-2 Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¢ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¾Ñ‚Ñ‡ĞµÑ‚ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ O1-CODER - Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚ĞºÑƒ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ o1 Ğ¾Ñ‚ OpenAI Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ ĞœĞ¾Ğ½Ñ‚Ğµ-ĞšĞ°Ñ€Ğ»Ğ¾ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ñ‹-2. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ° Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ÑĞ»ÑƒÑ‡Ğ°ĞµĞ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ MCTS Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ° Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°Ğ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¸ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ´Ğ¾Ğ²Ğ¾Ğ´ĞºÑƒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸. ĞÑ‚Ñ‡ĞµÑ‚ Ñ‚Ğ°ĞºĞ¶Ğµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ñ‹Ñ… o1 Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ÑÑ….'}, 'en': {'title': 'Enhancing Coding with O1-CODER: A New Approach to System-2 Thinking', 'desc': "The paper presents O1-CODER, a model designed to replicate OpenAI's o1 specifically for coding tasks. It combines reinforcement learning (RL) with Monte Carlo Tree Search (MCTS) to improve the model's ability to think critically and reason through problems. The framework includes a Test Case Generator (TCG) for consistent code testing and uses MCTS to create code data that incorporates reasoning steps. The report discusses the potential and challenges of applying o1-like models in practical scenarios, emphasizing the need for updates in the environment state during the learning process."}, 'zh': {'title': 'O1-CODERï¼šæå‡ç¼–ç ä»»åŠ¡çš„æ™ºèƒ½æ¨¡å‹', 'desc': 'æœ¬æ–‡ä»‹ç»äº†O1-CODERï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨å¤åˆ¶OpenAIçš„o1æ¨¡å‹ï¼Œä¸“æ³¨äºç¼–ç ä»»åŠ¡çš„æŠ€æœ¯æŠ¥å‘Šã€‚è¯¥æ¨¡å‹ç»“åˆäº†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å’Œè’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰ï¼Œä»¥å¢å¼ºå…¶ç³»ç»Ÿ2æ€ç»´èƒ½åŠ›ã€‚æ¡†æ¶ä¸­åŒ…æ‹¬è®­ç»ƒæµ‹è¯•ç”¨ä¾‹ç”Ÿæˆå™¨ï¼ˆTCGï¼‰ä»¥è¿›è¡Œæ ‡å‡†åŒ–ä»£ç æµ‹è¯•ï¼Œåˆ©ç”¨MCTSç”Ÿæˆå¸¦æœ‰æ¨ç†è¿‡ç¨‹çš„ä»£ç æ•°æ®ï¼Œå¹¶è¿­ä»£å¾®è°ƒç­–ç•¥æ¨¡å‹ï¼Œåˆæ­¥ç”Ÿæˆä¼ªä»£ç ï¼Œéšåç”Ÿæˆå®Œæ•´ä»£ç ã€‚æŠ¥å‘Šè¿˜è®¨è®ºäº†åœ¨å®é™…åº”ç”¨ä¸­éƒ¨ç½²ç±»ä¼¼o1æ¨¡å‹çš„æœºé‡å’ŒæŒ‘æˆ˜ï¼Œå»ºè®®è½¬å‘ç³»ç»Ÿ2èŒƒå¼ï¼Œå¹¶å¼ºè°ƒç¯å¢ƒçŠ¶æ€æ›´æ–°çš„é‡è¦æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.18671', 'title': 'TAPTRv3: Spatial and Temporal Context Foster Robust Tracking of Any Point in Long Video', 'url': 'https://huggingface.co/papers/2411.18671', 'abstract': 'In this paper, we present TAPTRv3, which is built upon TAPTRv2 to improve its point tracking robustness in long videos. TAPTRv2 is a simple DETR-like framework that can accurately track any point in real-world videos without requiring cost-volume. TAPTRv3 improves TAPTRv2 by addressing its shortage in querying high quality features from long videos, where the target tracking points normally undergo increasing variation over time. In TAPTRv3, we propose to utilize both spatial and temporal context to bring better feature querying along the spatial and temporal dimensions for more robust tracking in long videos. For better spatial feature querying, we present Context-aware Cross-Attention (CCA), which leverages surrounding spatial context to enhance the quality of attention scores when querying image features. For better temporal feature querying, we introduce Visibility-aware Long-Temporal Attention (VLTA) to conduct temporal attention to all past frames while considering their corresponding visibilities, which effectively addresses the feature drifting problem in TAPTRv2 brought by its RNN-like long-temporal modeling. TAPTRv3 surpasses TAPTRv2 by a large margin on most of the challenging datasets and obtains state-of-the-art performance. Even when compared with methods trained with large-scale extra internal data, TAPTRv3 is still competitive.', 'score': 13, 'issue_id': 909, 'pub_date': '2024-11-27', 'pub_date_card': {'ru': '27 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 27', 'zh': '11æœˆ27æ—¥'}, 'hash': 'f99e1015e9222dc6', 'authors': ['Jinyuan Qu', 'Hongyang Li', 'Shilong Liu', 'Tianhe Ren', 'Zhaoyang Zeng', 'Lei Zhang'], 'affiliations': ['International Digital Economy Academy (IDEA)', 'South China University of Technology', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2411.18671.jpg', 'data': {'categories': ['#long_context', '#benchmark', '#video', '#cv', '#optimization'], 'emoji': 'ğŸ‘ï¸', 'ru': {'title': 'ĞšĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡ĞµĞº Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'TAPTRv3 - ÑÑ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ°Ñ Ğ²ĞµÑ€ÑĞ¸Ñ TAPTRv2 Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡ĞµĞº Ğ² Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ° Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ². Ğ’Ğ²ĞµĞ´ĞµĞ½Ñ‹ Ğ´Ğ²Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ°: Context-aware Cross-Attention (CCA) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ° Ğ¸ Visibility-aware Long-Temporal Attention (VLTA) Ğ´Ğ»Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°. TAPTRv3 Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰ÑƒÑ Ğ²ĞµÑ€ÑĞ¸Ñ Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Enhancing Video Point Tracking with TAPTRv3', 'desc': 'TAPTRv3 is an advanced framework designed to enhance point tracking in lengthy videos, building on the previous version, TAPTRv2. It improves the ability to query high-quality features by incorporating both spatial and temporal contexts, which helps in maintaining robust tracking despite variations over time. The paper introduces Context-aware Cross-Attention (CCA) for better spatial feature querying and Visibility-aware Long-Temporal Attention (VLTA) for improved temporal feature querying, effectively addressing issues like feature drifting. TAPTRv3 demonstrates significant performance improvements over TAPTRv2 and competes well against other state-of-the-art methods, even those trained on larger datasets.'}, 'zh': {'title': 'TAPTRv3ï¼šé•¿è§†é¢‘ç‚¹è·Ÿè¸ªçš„æ–°çªç ´', 'desc': 'æœ¬æ–‡ä»‹ç»äº†TAPTRv3ï¼Œè¿™æ˜¯åœ¨TAPTRv2åŸºç¡€ä¸Šå¼€å‘çš„ï¼Œæ—¨åœ¨æé«˜é•¿è§†é¢‘ä¸­çš„ç‚¹è·Ÿè¸ªé²æ£’æ€§ã€‚TAPTRv2æ˜¯ä¸€ä¸ªç®€å•çš„ç±»ä¼¼DETRçš„æ¡†æ¶ï¼Œå¯ä»¥å‡†ç¡®è·Ÿè¸ªç°å®è§†é¢‘ä¸­çš„ä»»æ„ç‚¹ï¼Œè€Œæ— éœ€æˆæœ¬ä½“ç§¯ã€‚TAPTRv3é€šè¿‡åˆ©ç”¨ç©ºé—´å’Œæ—¶é—´ä¸Šä¸‹æ–‡æ¥æ”¹å–„ç‰¹å¾æŸ¥è¯¢ï¼Œä»è€Œåœ¨é•¿è§†é¢‘ä¸­å®ç°æ›´ç¨³å¥çš„è·Ÿè¸ªã€‚æˆ‘ä»¬æå‡ºäº†ä¸Šä¸‹æ–‡æ„ŸçŸ¥äº¤å‰æ³¨æ„åŠ›ï¼ˆCCAï¼‰å’Œå¯è§æ€§æ„ŸçŸ¥é•¿æ—¶é—´æ³¨æ„åŠ›ï¼ˆVLTAï¼‰ï¼Œæ˜¾è‘—æå‡äº†ç‰¹å¾æŸ¥è¯¢çš„è´¨é‡ï¼Œè¶…è¶Šäº†TAPTRv2ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.00100', 'title': 'Steering Rectified Flow Models in the Vector Field for Controlled Image Generation', 'url': 'https://huggingface.co/papers/2412.00100', 'abstract': 'Diffusion models (DMs) excel in photorealism, image editing, and solving inverse problems, aided by classifier-free guidance and image inversion techniques. However, rectified flow models (RFMs) remain underexplored for these tasks. Existing DM-based methods often require additional training, lack generalization to pretrained latent models, underperform, and demand significant computational resources due to extensive backpropagation through ODE solvers and inversion processes. In this work, we first develop a theoretical and empirical understanding of the vector field dynamics of RFMs in efficiently guiding the denoising trajectory. Our findings reveal that we can navigate the vector field in a deterministic and gradient-free manner. Utilizing this property, we propose FlowChef, which leverages the vector field to steer the denoising trajectory for controlled image generation tasks, facilitated by gradient skipping. FlowChef is a unified framework for controlled image generation that, for the first time, simultaneously addresses classifier guidance, linear inverse problems, and image editing without the need for extra training, inversion, or intensive backpropagation. Finally, we perform extensive evaluations and show that FlowChef significantly outperforms baselines in terms of performance, memory, and time requirements, achieving new state-of-the-art results. Project Page: https://flowchef.github.io.', 'score': 12, 'issue_id': 911, 'pub_date': '2024-11-27', 'pub_date_card': {'ru': '27 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 27', 'zh': '11æœˆ27æ—¥'}, 'hash': '5fceae277a0e3d85', 'authors': ['Maitreya Patel', 'Song Wen', 'Dimitris N. Metaxas', 'Yezhou Yang'], 'affiliations': ['Arizona State University', 'Rutgers University'], 'pdf_title_img': 'assets/pdf/title_img/2412.00100.jpg', 'data': {'categories': ['#dataset', '#cv', '#optimization', '#diffusion', '#benchmark'], 'emoji': 'ğŸŒŠ', 'ru': {'title': 'FlowChef: Ğ£Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ»ĞµĞ¼ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ FlowChef. ĞĞ½ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ»Ñ Ğ² Ñ€ĞµĞºÑ‚Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (RFM) Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ĞµĞ¹ ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ. FlowChef Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ñ… Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ»Ğ¸ Ğ¸Ğ½Ğ²ĞµÑ€ÑĞ¸Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ FlowChef Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑĞ¼ Ğº Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'FlowChef: Revolutionizing Controlled Image Generation with Efficiency', 'desc': 'This paper introduces FlowChef, a new framework that enhances controlled image generation using rectified flow models (RFMs). Unlike traditional diffusion models, FlowChef efficiently guides the denoising process without requiring additional training or extensive computational resources. It utilizes a unique property of RFMs to navigate the vector field in a deterministic way, allowing for effective classifier guidance and image editing. The results demonstrate that FlowChef outperforms existing methods in performance, memory usage, and processing time, setting new benchmarks in the field.'}, 'zh': {'title': 'FlowChefï¼šé«˜æ•ˆçš„å—æ§å›¾åƒç”Ÿæˆæ–°æ–¹æ³•', 'desc': 'æ‰©æ•£æ¨¡å‹åœ¨ç”ŸæˆçœŸå®æ„Ÿå›¾åƒã€å›¾åƒç¼–è¾‘å’Œè§£å†³é€†é—®é¢˜æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†ä¿®æ­£æµæ¨¡å‹åœ¨è¿™äº›ä»»åŠ¡ä¸­ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ç°æœ‰çš„åŸºäºæ‰©æ•£æ¨¡å‹çš„æ–¹æ³•é€šå¸¸éœ€è¦é¢å¤–çš„è®­ç»ƒï¼Œç¼ºä¹å¯¹é¢„è®­ç»ƒæ½œåœ¨æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶ä¸”åœ¨æ€§èƒ½ä¸Šè¡¨ç°ä¸ä½³ï¼Œè®¡ç®—èµ„æºæ¶ˆè€—å¤§ã€‚æœ¬æ–‡æå‡ºFlowChefï¼Œé€šè¿‡æœ‰æ•ˆå¼•å¯¼å»å™ªè½¨è¿¹ï¼Œåˆ©ç”¨å‘é‡åœºçš„ç‰¹æ€§ï¼Œå®ç°äº†å—æ§å›¾åƒç”Ÿæˆä»»åŠ¡ï¼Œä¸”æ— éœ€é¢å¤–è®­ç»ƒæˆ–å¤æ‚çš„åå‘ä¼ æ’­ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFlowChefåœ¨æ€§èƒ½ã€å†…å­˜å’Œæ—¶é—´éœ€æ±‚ä¸Šæ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œè¾¾åˆ°äº†æ–°çš„æœ€å…ˆè¿›ç»“æœã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.01199', 'title': 'TinyFusion: Diffusion Transformers Learned Shallow', 'url': 'https://huggingface.co/papers/2412.01199', 'abstract': 'Diffusion Transformers have demonstrated remarkable capabilities in image generation but often come with excessive parameterization, resulting in considerable inference overhead in real-world applications. In this work, we present TinyFusion, a depth pruning method designed to remove redundant layers from diffusion transformers via end-to-end learning. The core principle of our approach is to create a pruned model with high recoverability, allowing it to regain strong performance after fine-tuning. To accomplish this, we introduce a differentiable sampling technique to make pruning learnable, paired with a co-optimized parameter to simulate future fine-tuning. While prior works focus on minimizing loss or error after pruning, our method explicitly models and optimizes the post-fine-tuning performance of pruned models. Experimental results indicate that this learnable paradigm offers substantial benefits for layer pruning of diffusion transformers, surpassing existing importance-based and error-based methods. Additionally, TinyFusion exhibits strong generalization across diverse architectures, such as DiTs, MARs, and SiTs. Experiments with DiT-XL show that TinyFusion can craft a shallow diffusion transformer at less than 7% of the pre-training cost, achieving a 2times speedup with an FID score of 2.86, outperforming competitors with comparable efficiency. Code is available at https://github.com/VainF/TinyFusion.', 'score': 11, 'issue_id': 912, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 2', 'zh': '12æœˆ2æ—¥'}, 'hash': '7d1de7010c3fabd7', 'authors': ['Gongfan Fang', 'Kunjun Li', 'Xinyin Ma', 'Xinchao Wang'], 'affiliations': ['National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2412.01199.jpg', 'data': {'categories': ['#inference', '#diffusion', '#training', '#optimization', '#architecture'], 'emoji': 'âœ‚ï¸', 'ru': {'title': 'TinyFusion: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¾Ğ±Ñ€ĞµĞ·ĞºĞ° Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°', 'desc': 'TinyFusion - ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±Ñ€ĞµĞ·ĞºĞ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ´Ğ»Ñ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°Ñ…. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ñ€ÑƒĞµĞ¼ÑƒÑ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ğ¾Ğ¹ Ğ¾Ğ±Ñ€ĞµĞ·ĞºĞ¸ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ÑĞ»Ğµ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. TinyFusion Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ğ±Ñ€ĞµĞ·ĞºĞ¸ Ğ¸ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ Ñ Ğ´Ğ²ÑƒĞºÑ€Ğ°Ñ‚Ğ½Ñ‹Ğ¼ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'TinyFusion: Efficient Layer Pruning for Fast and Effective Diffusion Transformers', 'desc': 'This paper introduces TinyFusion, a method for optimizing diffusion transformers by removing unnecessary layers through a process called depth pruning. The approach focuses on maintaining high performance after the model is pruned, using a learnable technique that allows the model to adapt during fine-tuning. Unlike previous methods that only aim to reduce loss or error, TinyFusion specifically targets the performance of the pruned model post-fine-tuning. Experimental results show that TinyFusion significantly improves layer pruning efficiency and generalization across various architectures, achieving faster inference times and better performance metrics.'}, 'zh': {'title': 'TinyFusionï¼šé«˜æ•ˆå‰ªæï¼Œæå‡æ‰©æ•£å˜æ¢å™¨æ€§èƒ½', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºTinyFusionçš„æ·±åº¦å‰ªææ–¹æ³•ï¼Œæ—¨åœ¨å‡å°‘æ‰©æ•£å˜æ¢å™¨ä¸­çš„å†—ä½™å±‚ï¼Œä»è€Œé™ä½æ¨ç†å¼€é”€ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡ç«¯åˆ°ç«¯å­¦ä¹ å®ç°å‰ªæï¼Œå¹¶ç¡®ä¿å‰ªæåçš„æ¨¡å‹åœ¨å¾®è°ƒåèƒ½å¤Ÿæ¢å¤å¼ºå¤§çš„æ€§èƒ½ã€‚TinyFusionå¼•å…¥äº†ä¸€ç§å¯å¾®åˆ†é‡‡æ ·æŠ€æœ¯ï¼Œä½¿å¾—å‰ªæè¿‡ç¨‹å¯å­¦ä¹ ï¼Œå¹¶ä¸å…±åŒä¼˜åŒ–çš„å‚æ•°ç»“åˆï¼Œä»¥æ¨¡æ‹Ÿæœªæ¥çš„å¾®è°ƒæ•ˆæœã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTinyFusionåœ¨æ‰©æ•£å˜æ¢å™¨çš„å±‚å‰ªææ–¹é¢ä¼˜äºç°æœ‰çš„æ–¹æ³•ï¼Œå±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.00568', 'title': 'The Well: a Large-Scale Collection of Diverse Physics Simulations for Machine Learning', 'url': 'https://huggingface.co/papers/2412.00568', 'abstract': 'Machine learning based surrogate models offer researchers powerful tools for accelerating simulation-based workflows. However, as standard datasets in this space often cover small classes of physical behavior, it can be difficult to evaluate the efficacy of new approaches. To address this gap, we introduce the Well: a large-scale collection of datasets containing numerical simulations of a wide variety of spatiotemporal physical systems. The Well draws from domain experts and numerical software developers to provide 15TB of data across 16 datasets covering diverse domains such as biological systems, fluid dynamics, acoustic scattering, as well as magneto-hydrodynamic simulations of extra-galactic fluids or supernova explosions. These datasets can be used individually or as part of a broader benchmark suite. To facilitate usage of the Well, we provide a unified PyTorch interface for training and evaluating models. We demonstrate the function of this library by introducing example baselines that highlight the new challenges posed by the complex dynamics of the Well. The code and data is available at https://github.com/PolymathicAI/the_well.', 'score': 10, 'issue_id': 923, 'pub_date': '2024-11-30', 'pub_date_card': {'ru': '30 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 30', 'zh': '11æœˆ30æ—¥'}, 'hash': '1352e219e54ac56e', 'authors': ['Ruben Ohana', 'Michael McCabe', 'Lucas Meyer', 'Rudy Morel', 'Fruzsina J. Agocs', 'Miguel Beneitez', 'Marsha Berger', 'Blakesley Burkhart', 'Stuart B. Dalziel', 'Drummond B. Fielding', 'Daniel Fortunato', 'Jared A. Goldberg', 'Keiya Hirashima', 'Yan-Fei Jiang', 'Rich R. Kerswell', 'Suryanarayana Maddu', 'Jonah Miller', 'Payel Mukhopadhyay', 'Stefan S. Nixon', 'Jeff Shen', 'Romain Watteaux', 'Bruno RÃ©galdo-Saint Blancard', 'FranÃ§ois Rozet', 'Liam H. Parker', 'Miles Cranmer', 'Shirley Ho'], 'affiliations': ['CEA DAM', 'Cornell University', 'Flatiron Institute', 'Los Alamos National Laboratory', 'New York University', 'Polymathic AI', 'Princeton University', 'Rutgers University', 'University of California, Berkeley', 'University of Cambridge', 'University of Colorado, Boulder', 'University of LiÃ¨ge', 'University of Tokyo'], 'pdf_title_img': 'assets/pdf/title_img/2412.00568.jpg', 'data': {'categories': ['#data', '#dataset', '#benchmark'], 'emoji': 'ğŸŒŠ', 'ru': {'title': 'Well: Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ ÑÑƒÑ€Ñ€Ğ¾Ğ³Ğ°Ñ‚Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸', 'desc': "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ 'Well' Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑƒÑ€Ñ€Ğ¾Ğ³Ğ°Ñ‚Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. 'Well' ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 15 Ğ¢Ğ‘ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· 16 Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ², Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ñ„Ğ¸Ğ·Ğ¸ĞºĞ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ±Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹, Ğ³Ğ¸Ğ´Ñ€Ğ¾Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºÑƒ Ğ¸ Ğ°ÑÑ‚Ñ€Ğ¾Ñ„Ğ¸Ğ·Ğ¸ĞºÑƒ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹Ñ PyTorch Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° ÑÑ‚Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ²Ğ°Ğ½ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‡ÑŒ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼ Ğ² Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ², Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸ÑÑ…."}, 'en': {'title': 'Unlocking Complex Simulations with the Well: A New Dataset for Machine Learning', 'desc': 'This paper presents a large-scale collection of datasets called the Well, designed to enhance machine learning surrogate models for simulation-based workflows. The Well contains 15TB of data across 16 diverse datasets, covering various physical systems such as fluid dynamics and biological systems. By providing a unified PyTorch interface, the Well facilitates the training and evaluation of machine learning models on these complex datasets. The authors also introduce example baselines to demonstrate the unique challenges posed by the intricate dynamics represented in the Well.'}, 'zh': {'title': 'æœºå™¨å­¦ä¹ åŠ é€Ÿä»¿çœŸï¼šæ¢ç´¢"Well"æ•°æ®é›†', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºæœºå™¨å­¦ä¹ çš„æ›¿ä»£æ¨¡å‹ï¼Œæ—¨åœ¨åŠ é€ŸåŸºäºä»¿çœŸçš„å·¥ä½œæµç¨‹ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåä¸º"Well"çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼ŒåŒ…å«å¤šç§æ—¶ç©ºç‰©ç†ç³»ç»Ÿçš„æ•°å€¼ä»¿çœŸæ•°æ®ï¼Œæ€»è®¡15TBï¼Œæ¶µç›–ç”Ÿç‰©ç³»ç»Ÿã€æµä½“åŠ¨åŠ›å­¦ã€å£°æ•£å°„ç­‰å¤šä¸ªé¢†åŸŸã€‚è¯¥æ•°æ®é›†ä¸ºç ”ç©¶äººå‘˜æä¾›äº†ä¸°å¯Œçš„èµ„æºï¼Œä»¥è¯„ä¼°æ–°æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå¹¶å¯å•ç‹¬ä½¿ç”¨æˆ–ä½œä¸ºæ›´å¹¿æ³›åŸºå‡†å¥—ä»¶çš„ä¸€éƒ¨åˆ†ã€‚ä¸ºäº†æ–¹ä¾¿ä½¿ç”¨ï¼Œæˆ‘ä»¬æä¾›äº†ç»Ÿä¸€çš„PyTorchæ¥å£ï¼Œå¸®åŠ©è®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹ï¼Œå¹¶å±•ç¤ºäº†æ–°çš„æŒ‘æˆ˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.00174', 'title': 'SOLAMI: Social Vision-Language-Action Modeling for Immersive Interaction with 3D Autonomous Characters', 'url': 'https://huggingface.co/papers/2412.00174', 'abstract': "Human beings are social animals. How to equip 3D autonomous characters with similar social intelligence that can perceive, understand and interact with humans remains an open yet foundamental problem. In this paper, we introduce SOLAMI, the first end-to-end Social vision-Language-Action (VLA) Modeling framework for Immersive interaction with 3D autonomous characters. Specifically, SOLAMI builds 3D autonomous characters from three aspects: (1) Social VLA Architecture: We propose a unified social VLA framework to generate multimodal response (speech and motion) based on the user's multimodal input to drive the character for social interaction. (2) Interactive Multimodal Data: We present SynMSI, a synthetic multimodal social interaction dataset generated by an automatic pipeline using only existing motion datasets to address the issue of data scarcity. (3) Immersive VR Interface: We develop a VR interface that enables users to immersively interact with these characters driven by various architectures. Extensive quantitative experiments and user studies demonstrate that our framework leads to more precise and natural character responses (in both speech and motion) that align with user expectations with lower latency.", 'score': 10, 'issue_id': 913, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 29', 'zh': '11æœˆ29æ—¥'}, 'hash': '5a30d9c535229ab0', 'authors': ['Jianping Jiang', 'Weiye Xiao', 'Zhengyu Lin', 'Huaizhong Zhang', 'Tianxiang Ren', 'Yang Gao', 'Zhiqian Lin', 'Zhongang Cai', 'Lei Yang', 'Ziwei Liu'], 'affiliations': ['S-Lab, Nanyang Technological University', 'SenseTime Research'], 'pdf_title_img': 'assets/pdf/title_img/2412.00174.jpg', 'data': {'categories': ['#synthetic', '#games', '#dataset', '#agents', '#3d', '#multimodal'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ ÑƒĞ¼Ğ½Ñ‹Ñ… 3D Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹ Ğ´Ğ»Ñ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ SOLAMI - Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ ÑĞºĞ²Ğ¾Ğ·Ğ½Ğ¾Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ·Ñ€ĞµĞ½Ğ¸Ñ-ÑĞ·Ñ‹ĞºĞ°-Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ (VLA) Ğ´Ğ»Ñ Ğ¸Ğ¼Ğ¼ĞµÑ€ÑĞ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ 3D Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶Ğ°Ğ¼Ğ¸. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ VLA Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ³Ğ¾ Ğ²Ğ²Ğ¾Ğ´Ğ°. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ½ĞµÑ…Ğ²Ğ°Ñ‚ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… SynMSI, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸. Ğ¢Ğ°ĞºĞ¶Ğµ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ VR-Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹Ñ Ğ´Ğ»Ñ Ğ¸Ğ¼Ğ¼ĞµÑ€ÑĞ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Empowering 3D Characters with Social Intelligence through SOLAMI', 'desc': 'This paper presents SOLAMI, a novel framework designed to enhance the social intelligence of 3D autonomous characters. It integrates a Social Vision-Language-Action (VLA) architecture that allows characters to respond to user inputs with both speech and motion, facilitating more natural interactions. To support this, the authors introduce SynMSI, a synthetic dataset that helps overcome the challenge of limited training data for social interactions. Additionally, a virtual reality interface is developed to provide users with an immersive experience when interacting with these characters, resulting in improved response accuracy and reduced latency.'}, 'zh': {'title': 'èµ‹äºˆ3Dè§’è‰²ç¤¾äº¤æ™ºèƒ½çš„åˆ›æ–°æ¡†æ¶', 'desc': 'æœ¬æ–‡ä»‹ç»äº†SOLAMIï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç«¯åˆ°ç«¯çš„ç¤¾ä¼šè§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰å»ºæ¨¡æ¡†æ¶ï¼Œæ—¨åœ¨ä¸3Dè‡ªä¸»è§’è‰²è¿›è¡Œæ²‰æµ¸å¼äº’åŠ¨ã€‚SOLAMIä»ä¸‰ä¸ªæ–¹é¢æ„å»º3Dè‡ªä¸»è§’è‰²ï¼šé¦–å…ˆï¼Œæå‡ºäº†ç»Ÿä¸€çš„ç¤¾ä¼šVLAæ¶æ„ï¼Œæ ¹æ®ç”¨æˆ·çš„å¤šæ¨¡æ€è¾“å…¥ç”Ÿæˆå¤šæ¨¡æ€å“åº”ï¼ˆè¯­éŸ³å’ŒåŠ¨ä½œï¼‰ï¼Œä»¥é©±åŠ¨è§’è‰²è¿›è¡Œç¤¾äº¤äº’åŠ¨ã€‚å…¶æ¬¡ï¼Œä»‹ç»äº†SynMSIï¼Œè¿™æ˜¯ä¸€ä¸ªåˆæˆçš„å¤šæ¨¡æ€ç¤¾äº¤äº’åŠ¨æ•°æ®é›†ï¼Œé€šè¿‡è‡ªåŠ¨åŒ–æµç¨‹ç”Ÿæˆï¼Œè§£å†³äº†æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ã€‚æœ€åï¼Œå¼€å‘äº†ä¸€ä¸ªè™šæ‹Ÿç°å®æ¥å£ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿä¸è¿™äº›è§’è‰²è¿›è¡Œæ²‰æµ¸å¼äº’åŠ¨ï¼Œå®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿæä¾›æ›´ç²¾ç¡®å’Œè‡ªç„¶çš„è§’è‰²å“åº”ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.01822', 'title': 'VLsI: Verbalized Layers-to-Interactions from Large to Small Vision Language Models', 'url': 'https://huggingface.co/papers/2412.01822', 'abstract': 'The recent surge in high-quality visual instruction tuning samples from closed-source vision-language models (VLMs) such as GPT-4V has accelerated the release of open-source VLMs across various model sizes. However, scaling VLMs to improve performance using larger models brings significant computational challenges, especially for deployment on resource-constrained devices like mobile platforms and robots. To address this, we propose VLsI: Verbalized Layers-to-Interactions, a new VLM family in 2B and 7B model sizes, which prioritizes efficiency without compromising accuracy. VLsI leverages a unique, layer-wise distillation process, introducing intermediate "verbalizers" that map features from each layer to natural language space, allowing smaller VLMs to flexibly align with the reasoning processes of larger VLMs. This approach mitigates the training instability often encountered in output imitation and goes beyond typical final-layer tuning by aligning the small VLMs\' layer-wise progression with that of the large ones. We validate VLsI across ten challenging vision-language benchmarks, achieving notable performance gains (11.0% for 2B and 17.4% for 7B) over GPT-4V without the need for model scaling, merging, or architectural changes.', 'score': 9, 'issue_id': 916, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 2', 'zh': '12æœˆ2æ—¥'}, 'hash': '0c3ac9e6ce3f4cd2', 'authors': ['Byung-Kwan Lee', 'Ryo Hachiuma', 'Yu-Chiang Frank Wang', 'Yong Man Ro', 'Yueh-Hua Wu'], 'affiliations': ['KAIST', 'NVIDIA'], 'pdf_title_img': 'assets/pdf/title_img/2412.01822.jpg', 'data': {'categories': ['#dataset', '#small_models', '#architecture', '#transfer_learning', '#optimization', '#training', '#benchmark', '#open_source'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ğ¾ÑĞ»Ğ¾Ğ¹Ğ½ÑƒÑ Ğ²ĞµÑ€Ğ±Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ', 'desc': "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ğ¾Ğµ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ VLsI, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±ĞµĞ· ÑƒÑ‰ĞµÑ€Ğ±Ğ° Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. VLsI Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¿Ğ¾ÑĞ»Ğ¾Ğ¹Ğ½Ğ¾Ğ¹ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸, Ğ²Ğ²Ğ¾Ğ´Ñ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ 'Ğ²ĞµÑ€Ğ±Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ñ‹', ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶Ğ°ÑÑ‚ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ¸Ğ· ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ÑĞ»Ğ¾Ñ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ³Ğ¸Ğ±ĞºĞ¾ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ñ‹Ğ²Ğ°Ñ‚ÑŒÑÑ Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°Ğ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ VLsI Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ GPT-4V Ğ½Ğ° Ğ´ĞµÑÑÑ‚Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸Ğ»Ğ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹."}, 'en': {'title': 'Efficient Vision-Language Models with Layer-wise Distillation', 'desc': "This paper introduces VLsI, a new family of vision-language models designed to enhance efficiency while maintaining accuracy. It employs a layer-wise distillation technique that uses intermediate 'verbalizers' to translate features from each layer into natural language, enabling smaller models to better mimic the reasoning of larger models. This method addresses the common issue of training instability seen in traditional output imitation approaches. The results show significant performance improvements on various benchmarks, demonstrating that smaller models can achieve competitive results without needing to increase their size or alter their architecture."}, 'zh': {'title': 'é«˜æ•ˆè§†è§‰è¯­è¨€æ¨¡å‹çš„åˆ›æ–°ä¹‹è·¯', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å®¶æ—ï¼Œç§°ä¸ºVLsIï¼Œæ—¨åœ¨æé«˜æ¨¡å‹çš„æ•ˆç‡è€Œä¸ç‰ºç‰²å‡†ç¡®æ€§ã€‚VLsIé‡‡ç”¨äº†ä¸€ç§ç‹¬ç‰¹çš„å±‚çº§è’¸é¦è¿‡ç¨‹ï¼Œé€šè¿‡å¼•å…¥ä¸­é—´çš„â€œè¯­è¨€åŒ–å™¨â€ï¼Œå°†æ¯ä¸€å±‚çš„ç‰¹å¾æ˜ å°„åˆ°è‡ªç„¶è¯­è¨€ç©ºé—´ï¼Œä½¿å¾—è¾ƒå°çš„VLMèƒ½å¤Ÿçµæ´»åœ°ä¸è¾ƒå¤§VLMçš„æ¨ç†è¿‡ç¨‹å¯¹é½ã€‚è¯¥æ–¹æ³•æœ‰æ•ˆç¼“è§£äº†è¾“å‡ºæ¨¡ä»¿ä¸­å¸¸è§çš„è®­ç»ƒä¸ç¨³å®šæ€§ï¼Œå¹¶é€šè¿‡å¯¹é½å°å‹VLMçš„å±‚çº§è¿›å±•ä¸å¤§å‹VLMçš„å±‚çº§è¿›å±•ï¼Œè¶…è¶Šäº†å…¸å‹çš„æœ€ç»ˆå±‚è°ƒä¼˜ã€‚æˆ‘ä»¬åœ¨åä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„è§†è§‰è¯­è¨€åŸºå‡†ä¸ŠéªŒè¯äº†VLsIï¼Œå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.01064', 'title': 'FLOAT: Generative Motion Latent Flow Matching for Audio-driven Talking Portrait', 'url': 'https://huggingface.co/papers/2412.01064', 'abstract': 'With the rapid advancement of diffusion-based generative models, portrait image animation has achieved remarkable results. However, it still faces challenges in temporally consistent video generation and fast sampling due to its iterative sampling nature. This paper presents FLOAT, an audio-driven talking portrait video generation method based on flow matching generative model. We shift the generative modeling from the pixel-based latent space to a learned motion latent space, enabling efficient design of temporally consistent motion. To achieve this, we introduce a transformer-based vector field predictor with a simple yet effective frame-wise conditioning mechanism. Additionally, our method supports speech-driven emotion enhancement, enabling a natural incorporation of expressive motions. Extensive experiments demonstrate that our method outperforms state-of-the-art audio-driven talking portrait methods in terms of visual quality, motion fidelity, and efficiency.', 'score': 8, 'issue_id': 912, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 2', 'zh': '12æœˆ2æ—¥'}, 'hash': '1978e3ecf42cac0c', 'authors': ['Taekyung Ki', 'Dongchan Min', 'Gyoungsu Chae'], 'affiliations': ['DeepBrain AI Inc.', 'Graduate School of AI, KAIST'], 'pdf_title_img': 'assets/pdf/title_img/2412.01064.jpg', 'data': {'categories': ['#audio', '#diffusion', '#multimodal', '#video', '#architecture'], 'emoji': 'ğŸ—£ï¸', 'ru': {'title': 'Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰Ğ¸Ğ¼ Ğ¿Ğ¾Ñ€Ñ‚Ñ€ĞµÑ‚Ğ¾Ğ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ FLOAT - Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰Ğ¸Ğ¼ Ğ¿Ğ¾Ñ€Ñ‚Ñ€ĞµÑ‚Ğ¾Ğ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°ÑƒĞ´Ğ¸Ğ¾, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑÑÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ· Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¸ĞºÑ‚Ğ¾Ñ€ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ»Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° Ñ Ğ¿Ğ¾ĞºĞ°Ğ´Ñ€Ğ¾Ğ²Ñ‹Ğ¼ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑĞ»Ğ¾Ğ²Ğ»Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ. FLOAT Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€ĞµÑ‡Ğ¸, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑÑ‚ÑŒ Ğ²Ñ‹Ñ€Ğ°Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'FLOAT: Revolutionizing Audio-Driven Talking Portraits with Motion Consistency', 'desc': 'This paper introduces FLOAT, a novel method for generating talking portrait videos driven by audio. It addresses the challenges of creating temporally consistent animations and speeding up the sampling process by utilizing a flow matching generative model. By transitioning from pixel-based latent spaces to learned motion latent spaces, FLOAT enhances the design of motion consistency. The method also incorporates a transformer-based vector field predictor that allows for effective frame-wise conditioning and supports emotion enhancement based on speech, leading to improved visual quality and motion fidelity.'}, 'zh': {'title': 'FLOATï¼šé«˜æ•ˆéŸ³é¢‘é©±åŠ¨çš„äººåƒè§†é¢‘ç”Ÿæˆ', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºFLOATçš„éŸ³é¢‘é©±åŠ¨äººåƒè§†é¢‘ç”Ÿæˆæ–¹æ³•ï¼ŒåŸºäºæµåŒ¹é…ç”Ÿæˆæ¨¡å‹ã€‚æˆ‘ä»¬å°†ç”Ÿæˆå»ºæ¨¡ä»åŸºäºåƒç´ çš„æ½œåœ¨ç©ºé—´è½¬ç§»åˆ°å­¦ä¹ çš„è¿åŠ¨æ½œåœ¨ç©ºé—´ï¼Œä»è€Œå®ç°æ—¶é—´ä¸€è‡´çš„è¿åŠ¨è®¾è®¡ã€‚è¯¥æ–¹æ³•å¼•å…¥äº†åŸºäºå˜æ¢å™¨çš„å‘é‡åœºé¢„æµ‹å™¨ï¼Œå¹¶é‡‡ç”¨ç®€å•æœ‰æ•ˆçš„é€å¸§æ¡ä»¶æœºåˆ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è§†è§‰è´¨é‡ã€è¿åŠ¨ä¿çœŸåº¦å’Œæ•ˆç‡æ–¹é¢ä¼˜äºç°æœ‰çš„éŸ³é¢‘é©±åŠ¨äººåƒç”Ÿæˆæ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.18933', 'title': 'Efficient Track Anything', 'url': 'https://huggingface.co/papers/2411.18933', 'abstract': 'Segment Anything Model 2 (SAM 2) has emerged as a powerful tool for video object segmentation and tracking anything. Key components of SAM 2 that drive the impressive video object segmentation performance include a large multistage image encoder for frame feature extraction and a memory mechanism that stores memory contexts from past frames to help current frame segmentation. The high computation complexity of multistage image encoder and memory module has limited its applications in real-world tasks, e.g., video object segmentation on mobile devices. To address this limitation, we propose EfficientTAMs, lightweight track anything models that produce high-quality results with low latency and model size. Our idea is based on revisiting the plain, nonhierarchical Vision Transformer (ViT) as an image encoder for video object segmentation, and introducing an efficient memory module, which reduces the complexity for both frame feature extraction and memory computation for current frame segmentation. We take vanilla lightweight ViTs and efficient memory module to build EfficientTAMs, and train the models on SA-1B and SA-V datasets for video object segmentation and track anything tasks. We evaluate on multiple video segmentation benchmarks including semi-supervised VOS and promptable video segmentation, and find that our proposed EfficientTAM with vanilla ViT perform comparably to SAM 2 model (HieraB+SAM 2) with ~2x speedup on A100 and ~2.4x parameter reduction. On segment anything image tasks, our EfficientTAMs also perform favorably over original SAM with ~20x speedup on A100 and ~20x parameter reduction. On mobile devices such as iPhone 15 Pro Max, our EfficientTAMs can run at ~10 FPS for performing video object segmentation with reasonable quality, highlighting the capability of small models for on-device video object segmentation applications.', 'score': 8, 'issue_id': 912, 'pub_date': '2024-11-28', 'pub_date_card': {'ru': '28 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 28', 'zh': '11æœˆ28æ—¥'}, 'hash': '86ff7f63f69fa104', 'authors': ['Yunyang Xiong', 'Chong Zhou', 'Xiaoyu Xiang', 'Lemeng Wu', 'Chenchen Zhu', 'Zechun Liu', 'Saksham Suri', 'Balakrishnan Varadarajan', 'Ramya Akula', 'Forrest Iandola', 'Raghuraman Krishnamoorthi', 'Bilge Soran', 'Vikas Chandra'], 'affiliations': ['Meta AI', 'Nanyang Technological University'], 'pdf_title_img': 'assets/pdf/title_img/2411.18933.jpg', 'data': {'categories': ['#video', '#small_models', '#benchmark', '#optimization', '#architecture'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ…', 'desc': 'EfficientTAMs - ÑÑ‚Ğ¾ Ğ¾Ğ±Ğ»ĞµĞ³Ñ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ½Ğ¸ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ñ‹ Ğ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ³Ğ¾ Vision Transformer (ViT) Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ÑƒĞ»Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. EfficientTAMs Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ñ‹Ğµ Ñ SAM 2, Ğ½Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ Ğ² 2 Ñ€Ğ°Ğ·Ğ° Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ Ğ¸ Ğ¸Ğ¼ĞµÑÑ‚ Ğ² 2,4 Ñ€Ğ°Ğ·Ğ° Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞĞ° Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ…, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº iPhone 15 Pro Max, EfficientTAMs Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ ÑĞ¾ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒÑ Ğ¾ĞºĞ¾Ğ»Ğ¾ 10 ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ² ÑĞµĞºÑƒĞ½Ğ´Ñƒ.'}, 'en': {'title': 'EfficientTAMs: Lightweight Video Segmentation for Mobile Devices', 'desc': 'The Segment Anything Model 2 (SAM 2) is a sophisticated tool for video object segmentation, but its complexity limits its use on mobile devices. To overcome this, the authors introduce EfficientTAMs, which are lightweight models designed to maintain high-quality segmentation while reducing latency and model size. They utilize a simple Vision Transformer (ViT) for feature extraction and an efficient memory module to streamline the segmentation process. The results show that EfficientTAMs can achieve comparable performance to SAM 2 with significant improvements in speed and resource efficiency, making them suitable for real-time applications on mobile platforms.'}, 'zh': {'title': 'é«˜æ•ˆè§†é¢‘ç‰©ä½“åˆ†å‰²ï¼Œè½»é‡åŒ–æ¨¡å‹æ–°é€‰æ‹©', 'desc': 'Segment Anything Model 2ï¼ˆSAM 2ï¼‰æ˜¯ä¸€ç§å¼ºå¤§çš„è§†é¢‘ç‰©ä½“åˆ†å‰²å’Œè·Ÿè¸ªå·¥å…·ã€‚ä¸ºäº†æé«˜æ€§èƒ½ï¼ŒSAM 2ä½¿ç”¨äº†å¤šé˜¶æ®µå›¾åƒç¼–ç å™¨å’Œè®°å¿†æœºåˆ¶ï¼Œä½†å…¶è®¡ç®—å¤æ‚æ€§é™åˆ¶äº†åœ¨ç§»åŠ¨è®¾å¤‡ä¸Šçš„åº”ç”¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†é«˜æ•ˆçš„è·Ÿè¸ªæ¨¡å‹EfficientTAMsï¼Œå®ƒä½¿ç”¨è½»é‡çº§çš„è§†è§‰å˜æ¢å™¨ï¼ˆViTï¼‰ä½œä¸ºå›¾åƒç¼–ç å™¨ï¼Œå¹¶å¼•å…¥é«˜æ•ˆçš„è®°å¿†æ¨¡å—ï¼Œä»è€Œé™ä½äº†è®¡ç®—å¤æ‚æ€§ã€‚æˆ‘ä»¬çš„EfficientTAMsåœ¨å¤šä¸ªè§†é¢‘åˆ†å‰²åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿåœ¨ç§»åŠ¨è®¾å¤‡ä¸Šä»¥åˆç†çš„è´¨é‡è¿›è¡Œè§†é¢‘ç‰©ä½“åˆ†å‰²ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.17459', 'title': 'WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent Video Diffusion Model', 'url': 'https://huggingface.co/papers/2411.17459', 'abstract': 'Video Variational Autoencoder (VAE) encodes videos into a low-dimensional latent space, becoming a key component of most Latent Video Diffusion Models (LVDMs) to reduce model training costs. However, as the resolution and duration of generated videos increase, the encoding cost of Video VAEs becomes a limiting bottleneck in training LVDMs. Moreover, the block-wise inference method adopted by most LVDMs can lead to discontinuities of latent space when processing long-duration videos. The key to addressing the computational bottleneck lies in decomposing videos into distinct components and efficiently encoding the critical information. Wavelet transform can decompose videos into multiple frequency-domain components and improve the efficiency significantly, we thus propose Wavelet Flow VAE (WF-VAE), an autoencoder that leverages multi-level wavelet transform to facilitate low-frequency energy flow into latent representation. Furthermore, we introduce a method called Causal Cache, which maintains the integrity of latent space during block-wise inference. Compared to state-of-the-art video VAEs, WF-VAE demonstrates superior performance in both PSNR and LPIPS metrics, achieving 2x higher throughput and 4x lower memory consumption while maintaining competitive reconstruction quality. Our code and models are available at https://github.com/PKU-YuanGroup/WF-VAE.', 'score': 7, 'issue_id': 909, 'pub_date': '2024-11-26', 'pub_date_card': {'ru': '26 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 26', 'zh': '11æœˆ26æ—¥'}, 'hash': '9b68162718865b81', 'authors': ['Zongjian Li', 'Bin Lin', 'Yang Ye', 'Liuhan Chen', 'Xinhua Cheng', 'Shenghai Yuan', 'Li Yuan'], 'affiliations': ['Peking University', 'Peng Cheng Laboratory', 'Rabbitpre Intelligence'], 'pdf_title_img': 'assets/pdf/title_img/2411.17459.jpg', 'data': {'categories': ['#diffusion', '#data', '#video', '#architecture', '#open_source', '#optimization', '#training'], 'emoji': 'ğŸŒŠ', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ²ĞµĞ¹Ğ²Ğ»ĞµÑ‚Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Wavelet Flow VAE (WF-VAE). Ğ­Ñ‚Ğ¾Ñ‚ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ¾Ğµ Ğ²ĞµĞ¹Ğ²Ğ»ĞµÑ‚-Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾. WF-VAE Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LVDM). ĞœĞµÑ‚Ğ¾Ğ´ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ Causal Cache Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ñ†ĞµĞ»Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ¿Ñ€Ğ¸ Ğ¿Ğ¾Ğ±Ğ»Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ.'}, 'en': {'title': 'Efficient Video Encoding with Wavelet Flow VAE', 'desc': 'The paper introduces Wavelet Flow VAE (WF-VAE), a novel approach to encoding videos into a low-dimensional latent space using wavelet transforms. This method addresses the computational bottleneck of traditional Video VAEs, especially when generating high-resolution and long-duration videos. By decomposing videos into frequency-domain components, WF-VAE enhances the efficiency of encoding critical information. Additionally, the proposed Causal Cache method ensures continuity in the latent space during block-wise inference, resulting in improved performance metrics compared to existing video VAEs.'}, 'zh': {'title': 'å°æ³¢æµVAEï¼šé«˜æ•ˆè§†é¢‘ç¼–ç çš„æ–°æ–¹æ³•', 'desc': 'è§†é¢‘å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰å°†è§†é¢‘ç¼–ç ä¸ºä½ç»´æ½œåœ¨ç©ºé—´ï¼Œæ˜¯å¤§å¤šæ•°æ½œåœ¨è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆLVDMsï¼‰çš„å…³é”®ç»„æˆéƒ¨åˆ†ï¼Œèƒ½å¤Ÿé™ä½æ¨¡å‹è®­ç»ƒæˆæœ¬ã€‚ç„¶è€Œï¼Œéšç€ç”Ÿæˆè§†é¢‘çš„åˆ†è¾¨ç‡å’Œæ—¶é•¿å¢åŠ ï¼Œè§†é¢‘VAEçš„ç¼–ç æˆæœ¬æˆä¸ºè®­ç»ƒLVDMsçš„ç“¶é¢ˆã€‚æ­¤å¤–ï¼Œå¤§å¤šæ•°LVDMsé‡‡ç”¨çš„å—çŠ¶æ¨ç†æ–¹æ³•åœ¨å¤„ç†é•¿æ—¶é•¿è§†é¢‘æ—¶å¯èƒ½å¯¼è‡´æ½œåœ¨ç©ºé—´çš„ä¸è¿ç»­æ€§ã€‚ä¸ºäº†è§£å†³è®¡ç®—ç“¶é¢ˆï¼Œæˆ‘ä»¬æå‡ºäº†å°æ³¢æµVAEï¼ˆWF-VAEï¼‰ï¼Œé€šè¿‡å¤šçº§å°æ³¢å˜æ¢æœ‰æ•ˆç¼–ç è§†é¢‘çš„å…³é”®ä¿¡æ¯ï¼Œå¹¶å¼•å…¥å› æœç¼“å­˜æ–¹æ³•ä»¥ä¿æŒæ½œåœ¨ç©ºé—´çš„å®Œæ•´æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.01316', 'title': 'Long Video Diffusion Generation with Segmented Cross-Attention and Content-Rich Video Data Curation', 'url': 'https://huggingface.co/papers/2412.01316', 'abstract': 'We introduce Presto, a novel video diffusion model designed to generate 15-second videos with long-range coherence and rich content. Extending video generation methods to maintain scenario diversity over long durations presents significant challenges. To address this, we propose a Segmented Cross-Attention (SCA) strategy, which splits hidden states into segments along the temporal dimension, allowing each segment to cross-attend to a corresponding sub-caption. SCA requires no additional parameters, enabling seamless incorporation into current DiT-based architectures. To facilitate high-quality long video generation, we build the LongTake-HD dataset, consisting of 261k content-rich videos with scenario coherence, annotated with an overall video caption and five progressive sub-captions. Experiments show that our Presto achieves 78.5% on the VBench Semantic Score and 100% on the Dynamic Degree, outperforming existing state-of-the-art video generation methods. This demonstrates that our proposed Presto significantly enhances content richness, maintains long-range coherence, and captures intricate textual details. More details are displayed on our project page: https://presto-video.github.io/.', 'score': 6, 'issue_id': 910, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 2', 'zh': '12æœˆ2æ—¥'}, 'hash': '5bca597a08ec0a14', 'authors': ['Xin Yan', 'Yuxuan Cai', 'Qiuyue Wang', 'Yuan Zhou', 'Wenhao Huang', 'Huan Yang'], 'affiliations': ['01.AI'], 'pdf_title_img': 'assets/pdf/title_img/2412.01316.jpg', 'data': {'categories': ['#video', '#dataset', '#architecture', '#diffusion', '#long_context'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Presto: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜', 'desc': 'Presto - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 15-ÑĞµĞºÑƒĞ½Ğ´Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ñ€Ğ¾Ğ»Ğ¸ĞºĞ¾Ğ² Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ğ±Ğ¾Ğ³Ğ°Ñ‚Ñ‹Ğ¼ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸ĞµĞ¼. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Segmented Cross-Attention (SCA), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ½Ğ° ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ²Ğ´Ğ¾Ğ»ÑŒ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼Ñƒ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ñƒ Ğ¿ĞµÑ€ĞµĞºÑ€ĞµÑÑ‚Ğ½Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ñ‰Ğ°Ñ‚ÑŒÑÑ Ğº ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰ĞµĞ¹ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞ¸. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±Ñ‹Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ LongTake-HD, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 261 Ñ‚Ñ‹ÑÑÑ‡Ñƒ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ±Ğ¾Ğ³Ğ°Ñ‚Ñ‹Ğ¼ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒÑ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Presto Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑĞ¼ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ ÑÑ‚ĞµĞ¿ĞµĞ½Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Presto: Revolutionizing Video Generation with Long-Range Coherence', 'desc': 'Presto is a new video diffusion model that generates 15-second videos while ensuring long-range coherence and rich content. It introduces a Segmented Cross-Attention (SCA) strategy that divides hidden states into segments, allowing each segment to focus on specific sub-captions without needing extra parameters. To support this model, the LongTake-HD dataset was created, containing 261,000 videos with coherent scenarios and detailed captions. Experiments show that Presto outperforms existing methods, achieving high scores in semantic understanding and dynamic content generation.'}, 'zh': {'title': 'Prestoï¼šç”Ÿæˆé•¿æ—¶é—´ä¸€è‡´æ€§è§†é¢‘çš„æ–°æ–¹æ³•', 'desc': 'æˆ‘ä»¬ä»‹ç»äº†ä¸€ç§æ–°çš„è§†é¢‘æ‰©æ•£æ¨¡å‹Prestoï¼Œæ—¨åœ¨ç”Ÿæˆå…·æœ‰é•¿æ—¶é—´ä¸€è‡´æ€§å’Œä¸°å¯Œå†…å®¹çš„15ç§’è§†é¢‘ã€‚ä¸ºäº†è§£å†³åœ¨é•¿æ—¶é—´å†…ä¿æŒåœºæ™¯å¤šæ ·æ€§çš„æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ†æ®µäº¤å‰æ³¨æ„åŠ›(SCA)ç­–ç•¥ï¼Œè¯¥ç­–ç•¥å°†éšè—çŠ¶æ€æ²¿æ—¶é—´ç»´åº¦åˆ†æ®µï¼Œä½¿æ¯ä¸ªæ®µèƒ½å¤Ÿä¸ç›¸åº”çš„å­æ ‡é¢˜è¿›è¡Œäº¤å‰å…³æ³¨ã€‚SCAä¸éœ€è¦é¢å¤–çš„å‚æ•°ï¼Œå¯ä»¥æ— ç¼é›†æˆåˆ°ç°æœ‰çš„åŸºäºDiTçš„æ¶æ„ä¸­ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒPrestoåœ¨è§†é¢‘ç”Ÿæˆæ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œæå‡äº†å†…å®¹ä¸°å¯Œæ€§å’Œé•¿è·ç¦»ä¸€è‡´æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.01800', 'title': 'PhysGame: Uncovering Physical Commonsense Violations in Gameplay Videos', 'url': 'https://huggingface.co/papers/2412.01800', 'abstract': 'Recent advancements in video-based large language models (Video LLMs) have witnessed the emergence of diverse capabilities to reason and interpret dynamic visual content. Among them, gameplay videos stand out as a distinctive data source, often containing glitches that defy physics commonsense. This characteristic renders them an effective benchmark for assessing the under-explored capability of physical commonsense understanding in video LLMs. In this paper, we propose PhysGame as a pioneering benchmark to evaluate physical commonsense violations in gameplay videos. PhysGame comprises 880 videos associated with glitches spanning four fundamental domains (i.e., mechanics, kinematics, optics, and material properties) and across 12 distinct physical commonsense. Through extensively evaluating various state-ofthe-art video LLMs, our findings reveal that the performance of current open-source video LLMs significantly lags behind that of proprietary counterparts. To bridge this gap, we curate an instruction tuning dataset PhysInstruct with 140,057 question-answering pairs to facilitate physical commonsense learning. In addition, we also propose a preference optimization dataset PhysDPO with 34,358 training pairs, where the dis-preferred responses are generated conditioned on misleading titles (i.e., meta information hacking), fewer frames (i.e., temporal hacking) and lower spatial resolutions (i.e., spatial hacking). Based on the suite of datasets, we propose PhysVLM as a physical knowledge-enhanced video LLM. Extensive experiments on both physical-oriented benchmark PhysGame and general video understanding benchmarks demonstrate the state-ofthe-art performance of PhysVLM.', 'score': 4, 'issue_id': 917, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 2', 'zh': '12æœˆ2æ—¥'}, 'hash': '43cc035146493599', 'authors': ['Meng Cao', 'Haoran Tang', 'Haoze Zhao', 'Hangyu Guo', 'Jiaheng Liu', 'Ge Zhang', 'Ruyang Liu', 'Qiang Sun', 'Ian Reid', 'Xiaodan Liang'], 'affiliations': ['Alibaba Group', 'Mohamed bin Zayed University of Artificial Intelligence', 'Peking University', 'Sun Yat-sen University', 'University of Toronto', 'University of Waterloo'], 'pdf_title_img': 'assets/pdf/title_img/2412.01800.jpg', 'data': {'categories': ['#dataset', '#video', '#open_source', '#optimization', '#benchmark', '#reasoning', '#multimodal', '#interpretability'], 'emoji': 'ğŸ®', 'ru': {'title': 'PhysVLM: ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ·Ğ´Ñ€Ğ°Ğ²Ğ¾Ğ¼Ñƒ ÑĞ¼Ñ‹ÑĞ»Ñƒ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ³Ñ€Ğ¾Ğ²Ñ‹Ğµ Ğ³Ğ»Ğ¸Ñ‚Ñ‡Ğ¸', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº PhysGame Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (Video LLM) Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ³ĞµĞ¹Ğ¼Ğ¿Ğ»ĞµĞ¹Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ³Ğ»Ğ¸Ñ‚Ñ‡Ğ°Ğ¼Ğ¸. ĞĞ½Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… PhysInstruct Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ·Ğ´Ñ€Ğ°Ğ²Ğ¾Ğ¼Ñƒ ÑĞ¼Ñ‹ÑĞ»Ñƒ Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ PhysDPO Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹. ĞĞ° Ğ±Ğ°Ğ·Ğµ ÑÑ‚Ğ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ±Ñ‹Ğ»Ğ° Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ PhysVLM, Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ²ÑˆĞ°Ñ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ĞºĞ°Ğº Ğ½Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ PhysGame, Ñ‚Ğ°Ğº Ğ¸ Ğ½Ğ° Ğ¾Ğ±Ñ‰Ğ¸Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'Enhancing Video LLMs with Physical Commonsense Understanding', 'desc': 'This paper introduces PhysGame, a new benchmark designed to evaluate how well video-based large language models (Video LLMs) understand physical commonsense by analyzing glitches in gameplay videos. The benchmark includes 880 videos that highlight violations of physical laws across four domains: mechanics, kinematics, optics, and material properties. To improve the performance of Video LLMs, the authors created two additional datasets: PhysInstruct for instruction tuning and PhysDPO for preference optimization, which help models learn from common misconceptions and misleading information. The proposed PhysVLM model, enhanced with physical knowledge, shows superior performance on both the PhysGame benchmark and general video understanding tasks compared to existing models.'}, 'zh': {'title': 'æå‡è§†é¢‘æ¨¡å‹çš„ç‰©ç†å¸¸è¯†ç†è§£èƒ½åŠ›', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°é¢–çš„åŸºå‡†æµ‹è¯•PhysGameï¼Œç”¨äºè¯„ä¼°è§†é¢‘å¤§è¯­è¨€æ¨¡å‹åœ¨æ¸¸æˆè§†é¢‘ä¸­å¯¹ç‰©ç†å¸¸è¯†çš„ç†è§£èƒ½åŠ›ã€‚æ¸¸æˆè§†é¢‘ä¸­å¸¸å¸¸å‡ºç°è¿åç‰©ç†å¸¸è¯†çš„æ•…éšœï¼Œè¿™ä½¿å¾—å®ƒä»¬æˆä¸ºè¯„ä¼°æ¨¡å‹èƒ½åŠ›çš„æœ‰æ•ˆæ•°æ®æºã€‚æˆ‘ä»¬è¿˜åˆ›å»ºäº†PhysInstructå’ŒPhysDPOä¸¤ä¸ªæ•°æ®é›†ï¼Œä»¥å¸®åŠ©æ¨¡å‹å­¦ä¹ ç‰©ç†å¸¸è¯†å¹¶ä¼˜åŒ–å…¶åå¥½ã€‚é€šè¿‡è¿™äº›æ•°æ®é›†ï¼Œæˆ‘ä»¬æå‡ºäº†PhysVLMï¼Œä¸€ä¸ªå¢å¼ºç‰©ç†çŸ¥è¯†çš„è§†é¢‘å¤§è¯­è¨€æ¨¡å‹ï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å±•ç¤ºäº†å…¶ä¼˜è¶Šçš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.19939', 'title': 'VLSBench: Unveiling Visual Leakage in Multimodal Safety', 'url': 'https://huggingface.co/papers/2411.19939', 'abstract': 'Safety concerns of Multimodal large language models (MLLMs) have gradually become an important problem in various applications. Surprisingly, previous works indicate a counter-intuitive phenomenon that using textual unlearning to align MLLMs achieves comparable safety performances with MLLMs trained with image-text pairs. To explain such a counter-intuitive phenomenon, we discover a visual safety information leakage (VSIL) problem in existing multimodal safety benchmarks, i.e., the potentially risky and sensitive content in the image has been revealed in the textual query. In this way, MLLMs can easily refuse these sensitive text-image queries according to textual queries. However, image-text pairs without VSIL are common in real-world scenarios and are overlooked by existing multimodal safety benchmarks. To this end, we construct multimodal visual leakless safety benchmark (VLSBench) preventing visual safety leakage from image to textual query with 2.4k image-text pairs. Experimental results indicate that VLSBench poses a significant challenge to both open-source and close-source MLLMs, including LLaVA, Qwen2-VL, Llama3.2-Vision, and GPT-4o. This study demonstrates that textual alignment is enough for multimodal safety scenarios with VSIL, while multimodal alignment is a more promising solution for multimodal safety scenarios without VSIL. Please see our code and data at: http://hxhcreate.github.io/VLSBench', 'score': 4, 'issue_id': 914, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 29', 'zh': '11æœˆ29æ—¥'}, 'hash': '03d8c636cf8c9486', 'authors': ['Xuhao Hu', 'Dongrui Liu', 'Hao Li', 'Xuanjing Huang', 'Jing Shao'], 'affiliations': ['Beihang University', 'Fudan University', 'Shanghai Artificial Intelligence Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2411.19939.jpg', 'data': {'categories': ['#ethics', '#multimodal', '#benchmark', '#leakage', '#open_source', '#dataset'], 'emoji': 'ğŸ”', 'ru': {'title': 'ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸ Ñ„ĞµĞ½Ğ¾Ğ¼ĞµĞ½ ÑƒÑ‚ĞµÑ‡ĞºĞ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ (VSIL) Ğ² ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¾Ğ½Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… VLSBench, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 2400 Ğ¿Ğ°Ñ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ-Ñ‚ĞµĞºÑÑ‚ Ğ±ĞµĞ· VSIL. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ VLSBench Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ»Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… MLLM Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ´Ğ»Ñ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ² Ğ±ĞµĞ· VSIL.'}, 'en': {'title': 'Unveiling Safety in Multimodal Models: The VSIL Challenge', 'desc': 'This paper addresses safety issues in Multimodal Large Language Models (MLLMs) by highlighting a problem called Visual Safety Information Leakage (VSIL). It shows that using textual unlearning can achieve safety performance similar to training with image-text pairs, which is surprising. The authors create a new benchmark, VLSBench, to test MLLMs without the influence of VSIL, using 2.4k image-text pairs. The findings suggest that while textual alignment can ensure safety in scenarios with VSIL, multimodal alignment is necessary for scenarios without it.'}, 'zh': {'title': 'å¤šæ¨¡æ€å®‰å…¨æ€§çš„æ–°æŒ‘æˆ˜ä¸è§£å†³æ–¹æ¡ˆ', 'desc': 'è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å®‰å…¨æ€§æ–¹é¢çš„æŒ‘æˆ˜ã€‚ç ”ç©¶å‘ç°ï¼Œæ–‡æœ¬å»å­¦ä¹ å¯ä»¥ä¸ä½¿ç”¨å›¾åƒ-æ–‡æœ¬å¯¹è®­ç»ƒçš„æ¨¡å‹åœ¨å®‰å…¨æ€§è¡¨ç°ä¸Šç›¸å½“ã€‚ä½œè€…æŒ‡å‡ºï¼Œç°æœ‰çš„å¤šæ¨¡æ€å®‰å…¨åŸºå‡†å­˜åœ¨è§†è§‰å®‰å…¨ä¿¡æ¯æ³„æ¼ï¼ˆVSILï¼‰é—®é¢˜ï¼Œè¿™ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿè½»æ˜“æ‹’ç»æ•æ„Ÿçš„æ–‡æœ¬-å›¾åƒæŸ¥è¯¢ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œç ”ç©¶è€…æ„å»ºäº†ä¸€ä¸ªæ–°çš„å¤šæ¨¡æ€è§†è§‰æ— æ³„æ¼å®‰å…¨åŸºå‡†ï¼ˆVLSBenchï¼‰ï¼Œä»¥æ›´å¥½åœ°è¯„ä¼°æ¨¡å‹åœ¨æ²¡æœ‰VSILæƒ…å†µä¸‹çš„å®‰å…¨æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.00947', 'title': 'VisOnlyQA: Large Vision Language Models Still Struggle with Visual Perception of Geometric Information', 'url': 'https://huggingface.co/papers/2412.00947', 'abstract': 'Errors in understanding visual information in images (i.e., visual perception errors) remain a major source of mistakes in Large Vision Language Models (LVLMs). While further analysis is essential, there is a deficiency in datasets for evaluating the visual perception of LVLMs. In this work, we introduce VisOnlyQA, a new dataset designed to directly evaluate the visual perception capabilities of LVLMs on questions about geometric and numerical information in scientific figures. Our dataset enables us to analyze the visual perception of LVLMs for fine-grained visual information, independent of other capabilities such as reasoning. The evaluation set of VisOnlyQA includes 1,200 multiple-choice questions in 12 tasks on four categories of figures. We also provide synthetic training data consisting of 70k instances. Our experiments on VisOnlyQA highlight the following findings: (i) 20 LVLMs we evaluate, including GPT-4o and Gemini 1.5 Pro, work poorly on the visual perception tasks in VisOnlyQA, while human performance is nearly perfect. (ii) Fine-tuning on synthetic training data demonstrates the potential for enhancing the visual perception of LVLMs, but observed improvements are limited to certain tasks and specific models. (iii) Stronger language models improve the visual perception of LVLMs. In summary, our experiments suggest that both training data and model architectures should be improved to enhance the visual perception capabilities of LVLMs. The datasets, code, and model responses are provided at https://github.com/psunlpgroup/VisOnlyQA.', 'score': 4, 'issue_id': 911, 'pub_date': '2024-12-01', 'pub_date_card': {'ru': '1 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 1', 'zh': '12æœˆ1æ—¥'}, 'hash': '7135f8936b0d3ee8', 'authors': ['Ryo Kamoi', 'Yusen Zhang', 'Sarkar Snigdha Sarathi Das', 'Ranran Haoran Zhang', 'Rui Zhang'], 'affiliations': ['Penn State University'], 'pdf_title_img': 'assets/pdf/title_img/2412.00947.jpg', 'data': {'categories': ['#dataset', '#interpretability', '#cv', '#synthetic', '#training'], 'emoji': 'ğŸ‘ï¸', 'ru': {'title': 'VisOnlyQA: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿ÑƒÑ‚ÑŒ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ AI', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ VisOnlyQA Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LVLM). Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¸ Ñ‡Ğ¸ÑĞ»Ğ¾Ğ²Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¼Ñƒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ LVLM, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº GPT-4o Ğ¸ Gemini 1.5 Pro, Ğ¿Ğ»Ğ¾Ñ…Ğ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ VisOnlyQA, Ğ² Ñ‚Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ĞºĞ°Ğº Ğ»ÑĞ´Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸ Ğ¸Ğ´ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ğº Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ LVLM.'}, 'en': {'title': 'Enhancing Visual Perception in LVLMs with VisOnlyQA', 'desc': 'This paper addresses the issue of visual perception errors in Large Vision Language Models (LVLMs), which can lead to mistakes when interpreting images. The authors introduce a new dataset called VisOnlyQA, specifically designed to evaluate LVLMs on geometric and numerical questions related to scientific figures. The dataset includes 1,200 multiple-choice questions across various tasks and provides synthetic training data to improve model performance. The findings reveal that while LVLMs struggle with visual perception tasks, fine-tuning with synthetic data can enhance their capabilities, particularly when using stronger language models.'}, 'zh': {'title': 'æå‡è§†è§‰æ„ŸçŸ¥ï¼ŒåŠ©åŠ›å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹', 'desc': 'æœ¬ç ”ç©¶ä»‹ç»äº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†VisOnlyQAï¼Œæ—¨åœ¨ç›´æ¥è¯„ä¼°å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨ç§‘å­¦å›¾å½¢ä¸­å‡ ä½•å’Œæ•°å€¼ä¿¡æ¯é—®é¢˜ä¸Šçš„è§†è§‰æ„ŸçŸ¥èƒ½åŠ›ã€‚è¯¥æ•°æ®é›†åŒ…å«1200ä¸ªå¤šé¡¹é€‰æ‹©é¢˜ï¼Œæ¶µç›–å››ç±»å›¾å½¢çš„12ä¸ªä»»åŠ¡ï¼Œå¸®åŠ©åˆ†æLVLMså¯¹ç»†ç²’åº¦è§†è§‰ä¿¡æ¯çš„ç†è§£ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯„ä¼°çš„20ä¸ªLVLMsåœ¨è§†è§‰æ„ŸçŸ¥ä»»åŠ¡ä¸Šçš„è¡¨ç°è¾ƒå·®ï¼Œè€Œäººç±»çš„è¡¨ç°å‡ ä¹å®Œç¾ã€‚é€šè¿‡åˆæˆè®­ç»ƒæ•°æ®è¿›è¡Œå¾®è°ƒå¯ä»¥æå‡LVLMsçš„è§†è§‰æ„ŸçŸ¥èƒ½åŠ›ï¼Œä½†æ”¹è¿›æ•ˆæœæœ‰é™ï¼Œä¸”æ›´å¼ºçš„è¯­è¨€æ¨¡å‹èƒ½å¤Ÿæé«˜è§†è§‰æ„ŸçŸ¥èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.00176', 'title': 'Art-Free Generative Models: Art Creation Without Graphic Art Knowledge', 'url': 'https://huggingface.co/papers/2412.00176', 'abstract': 'We explore the question: "How much prior art knowledge is needed to create art?" To investigate this, we propose a text-to-image generation model trained without access to art-related content. We then introduce a simple yet effective method to learn an art adapter using only a few examples of selected artistic styles. Our experiments show that art generated using our method is perceived by users as comparable to art produced by models trained on large, art-rich datasets. Finally, through data attribution techniques, we illustrate how examples from both artistic and non-artistic datasets contributed to the creation of new artistic styles.', 'score': 3, 'issue_id': 921, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 29', 'zh': '11æœˆ29æ—¥'}, 'hash': '41f372dd1c030fbd', 'authors': ['Hui Ren', 'Joanna Materzynska', 'Rohit Gandikota', 'David Bau', 'Antonio Torralba'], 'affiliations': ['MIT', 'Northeastern University', 'ShanghaiTech University'], 'pdf_title_img': 'assets/pdf/title_img/2412.00176.jpg', 'data': {'categories': ['#cv', '#dataset', '#synthetic', '#games', '#multimodal'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ˜ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ Ñ‚Ğ²Ğ¾Ñ€Ğ¸Ñ‚ ÑˆĞµĞ´ĞµĞ²Ñ€Ñ‹ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¸Ğ·ÑƒÑ‡Ğ°ÑÑ‚ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ Ğ¾ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾Ğ¼ Ğ¾Ğ±ÑŠĞµĞ¼Ğµ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¾Ğ± Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²Ğµ Ğ´Ğ»Ñ ĞµĞ³Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ. ĞĞ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ±ĞµĞ· Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ° Ğº ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ñƒ, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ğ¾Ğ¼Ñƒ Ñ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²Ğ¾Ğ¼. Ğ—Ğ°Ñ‚ĞµĞ¼ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ¹, Ğ½Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ğ° Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²Ğ°, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ»Ğ¸ÑˆÑŒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ…ÑƒĞ´Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… ÑÑ‚Ğ¸Ğ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²Ğ¾, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ğ¾Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°, Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°ĞµÑ‚ÑÑ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ¾Ğ¼ Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ±Ğ¾Ğ³Ğ°Ñ‚Ñ‹Ñ… Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²Ğ¾Ğ¼.'}, 'en': {'title': 'Creating Art with Minimal Prior Knowledge', 'desc': "This paper investigates the necessity of prior art knowledge in generating art through a text-to-image model. The authors propose a novel approach that trains the model without any art-related content, relying instead on a small number of examples from specific artistic styles to create an 'art adapter'. Their experiments reveal that the art produced by this method is perceived similarly to that generated by models trained on extensive art datasets. Additionally, they employ data attribution techniques to demonstrate how both artistic and non-artistic examples influence the development of new artistic styles."}, 'zh': {'title': 'åˆ›é€ è‰ºæœ¯æ— éœ€ä¸°å¯Œçš„è‰ºæœ¯çŸ¥è¯†', 'desc': 'æˆ‘ä»¬æ¢è®¨äº†åˆ›é€ è‰ºæœ¯éœ€è¦å¤šå°‘å…ˆå‰çš„è‰ºæœ¯çŸ¥è¯†ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨æ²¡æœ‰è‰ºæœ¯ç›¸å…³å†…å®¹çš„æƒ…å†µä¸‹è¿›è¡Œè®­ç»ƒã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„æ–¹æ³•ï¼Œä»…ä½¿ç”¨å°‘é‡é€‰å®šè‰ºæœ¯é£æ ¼çš„ç¤ºä¾‹æ¥å­¦ä¹ è‰ºæœ¯é€‚é…å™¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨æˆ‘ä»¬çš„æ–¹æ³•ç”Ÿæˆçš„è‰ºæœ¯ä½œå“åœ¨ç”¨æˆ·çœ¼ä¸­ä¸åœ¨å¤§å‹è‰ºæœ¯ä¸°å¯Œæ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¨¡å‹ç”Ÿæˆçš„è‰ºæœ¯ä½œå“ç›¸å½“ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.01250', 'title': 'Collaborative Instance Navigation: Leveraging Agent Self-Dialogue to Minimize User Input', 'url': 'https://huggingface.co/papers/2412.01250', 'abstract': 'Existing embodied instance goal navigation tasks, driven by natural language, assume human users to provide complete and nuanced instance descriptions prior to the navigation, which can be impractical in the real world as human instructions might be brief and ambiguous. To bridge this gap, we propose a new task, Collaborative Instance Navigation (CoIN), with dynamic agent-human interaction during navigation to actively resolve uncertainties about the target instance in natural, template-free, open-ended dialogues. To address CoIN, we propose a novel method, Agent-user Interaction with UncerTainty Awareness (AIUTA), leveraging the perception capability of Vision Language Models (VLMs) and the capability of Large Language Models (LLMs). First, upon object detection, a Self-Questioner model initiates a self-dialogue to obtain a complete and accurate observation description, while a novel uncertainty estimation technique mitigates inaccurate VLM perception. Then, an Interaction Trigger module determines whether to ask a question to the user, continue or halt navigation, minimizing user input. For evaluation, we introduce CoIN-Bench, a benchmark supporting both real and simulated humans. AIUTA achieves competitive performance in instance navigation against state-of-the-art methods, demonstrating great flexibility in handling user inputs.', 'score': 3, 'issue_id': 915, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 2', 'zh': '12æœˆ2æ—¥'}, 'hash': '8b768de35e4c5114', 'authors': ['Francesco Taioli', 'Edoardo Zorzi', 'Gianni Franchi', 'Alberto Castellini', 'Alessandro Farinelli', 'Marco Cristani', 'Yiming Wang'], 'affiliations': ['Fondazione Bruno Kessler', 'Polytechnic of Turin', 'U2IS, ENSTA Paris, Institut Polytechnique de Paris', 'University of Verona'], 'pdf_title_img': 'assets/pdf/title_img/2412.01250.jpg', 'data': {'categories': ['#benchmark', '#interpretability', '#multimodal', '#reasoning', '#agents'], 'emoji': 'ğŸ—ºï¸', 'ru': {'title': 'Ğ˜Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ° Ñ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ñ†ĞµĞ»Ğ¸ Ñƒ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ Ğ¸Ğ½ÑÑ‚Ğ°Ğ½ÑĞ°Ğ¼ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² - Collaborative Instance Navigation (CoIN), Ğ³Ğ´Ğµ Ğ°Ğ³ĞµĞ½Ñ‚ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²ÑƒĞµÑ‚ Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¼ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ AIUTA, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Vision Language Models Ğ¸ Large Language Models Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². AIUTA Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»Ğ¸ Self-Questioner Ğ´Ğ»Ñ ÑĞ°Ğ¼Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Interaction Trigger Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‚ÑŒ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº CoIN-Bench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼.'}, 'en': {'title': 'Navigating Together: Enhancing Instance Navigation with Human-AI Collaboration', 'desc': 'This paper introduces a new task called Collaborative Instance Navigation (CoIN), which allows agents to interact with humans during navigation to clarify ambiguous instructions. The proposed method, Agent-user Interaction with UncerTainty Awareness (AIUTA), uses Vision Language Models (VLMs) and Large Language Models (LLMs) to enhance the navigation process. It includes a Self-Questioner model that helps the agent gather detailed information about the target instance and an Interaction Trigger module that decides when to engage the user for input. The authors also present CoIN-Bench, a benchmark for evaluating the performance of navigation systems in both real and simulated environments, showing that AIUTA performs well compared to existing methods.'}, 'zh': {'title': 'åä½œå®ä¾‹å¯¼èˆªï¼šè®©æœºå™¨æ›´æ‡‚äººç±»æŒ‡ä»¤', 'desc': 'ç°æœ‰çš„å®ä¾‹ç›®æ ‡å¯¼èˆªä»»åŠ¡é€šå¸¸éœ€è¦ç”¨æˆ·æä¾›è¯¦ç»†çš„æè¿°ï¼Œä½†åœ¨ç°å®ä¸­ï¼Œè¿™ç§è¦æ±‚å¾€å¾€ä¸åˆ‡å®é™…ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„ä»»åŠ¡ï¼Œç§°ä¸ºåä½œå®ä¾‹å¯¼èˆªï¼ˆCoINï¼‰ï¼Œé€šè¿‡åŠ¨æ€çš„ä»£ç†-ç”¨æˆ·äº’åŠ¨æ¥è§£å†³å¯¼èˆªä¸­çš„ä¸ç¡®å®šæ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•ï¼Œä»£ç†-ç”¨æˆ·äº’åŠ¨ä¸ä¸ç¡®å®šæ€§æ„è¯†ï¼ˆAIUTAï¼‰ï¼Œç»“åˆäº†è§†è§‰è¯­è¨€æ¨¡å‹å’Œå¤§å‹è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨å¯¼èˆªè¿‡ç¨‹ä¸­ä¸»åŠ¨ä¸ç”¨æˆ·å¯¹è¯ã€‚é€šè¿‡å¼•å…¥CoIN-BenchåŸºå‡†ï¼Œæˆ‘ä»¬çš„AIUTAæ–¹æ³•åœ¨å®ä¾‹å¯¼èˆªä¸­è¡¨ç°å‡ºè‰²ï¼Œå±•ç¤ºäº†å¤„ç†ç”¨æˆ·è¾“å…¥çš„çµæ´»æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.19799', 'title': 'INCLUDE: Evaluating Multilingual Language Understanding with Regional Knowledge', 'url': 'https://huggingface.co/papers/2411.19799', 'abstract': 'The performance differential of large language models (LLM) between languages hinders their effective deployment in many regions, inhibiting the potential economic and societal value of generative AI tools in many communities. However, the development of functional LLMs in many languages (\\ie, multilingual LLMs) is bottlenecked by the lack of high-quality evaluation resources in languages other than English. Moreover, current practices in multilingual benchmark construction often translate English resources, ignoring the regional and cultural knowledge of the environments in which multilingual systems would be used. In this work, we construct an evaluation suite of 197,243 QA pairs from local exam sources to measure the capabilities of multilingual LLMs in a variety of regional contexts. Our novel resource, INCLUDE, is a comprehensive knowledge- and reasoning-centric benchmark across 44 written languages that evaluates multilingual LLMs for performance in the actual language environments where they would be deployed.', 'score': 3, 'issue_id': 914, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 29', 'zh': '11æœˆ29æ—¥'}, 'hash': '7b0cbdd28adecf2c', 'authors': ['Angelika Romanou', 'Negar Foroutan', 'Anna Sotnikova', 'Zeming Chen', 'Sree Harsha Nelaturu', 'Shivalika Singh', 'Rishabh Maheshwary', 'Micol Altomare', 'Mohamed A. Haggag', 'Snegha A', 'Alfonso Amayuelas', 'Azril Hafizi Amirudin', 'Viraat Aryabumi', 'Danylo Boiko', 'Michael Chang', 'Jenny Chim', 'Gal Cohen', 'Aditya Kumar Dalmia', 'Abraham Diress', 'Sharad Duwal', 'Daniil Dzenhaliou', 'Daniel Fernando Erazo Florez', 'Fabian Farestam', 'Joseph Marvin Imperial', 'Shayekh Bin Islam', 'Perttu Isotalo', 'Maral Jabbarishiviari', 'BÃ¶rje F. Karlsson', 'Eldar Khalilov', 'Christopher Klamm', 'Fajri Koto', 'Dominik KrzemiÅ„ski', 'Gabriel Adriano de Melo', 'Syrielle Montariol', 'Yiyang Nan', 'Joel Niklaus', 'Jekaterina Novikova', 'Johan Samir Obando Ceron', 'Debjit Paul', 'Esther Ploeger', 'Jebish Purbey', 'Swati Rajwal', 'Selvan Sunitha Ravi', 'Sara Rydell', 'Roshan Santhosh', 'Drishti Sharma', 'Marjana Prifti Skenduli', 'Arshia Soltani Moakhar', 'Bardia Soltani Moakhar', 'Ran Tamir', 'Ayush Kumar Tarun', 'Azmine Toushik Wasi', 'Thenuka Ovin Weerasinghe', 'Serhan Yilmaz', 'Mike Zhang', 'Imanol Schlag', 'Marzieh Fadaee', 'Sara Hooker', 'Antoine Bosselut'], 'affiliations': ['Cohere For AI', 'Cohere For AI Community', 'EPFL', 'ETH Zurich', 'Swiss AI Initiative'], 'pdf_title_img': 'assets/pdf/title_img/2411.19799.jpg', 'data': {'categories': ['#multilingual', '#low_resource', '#benchmark', '#reasoning'], 'emoji': 'ğŸŒ', 'ru': {'title': 'Ğ“Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ñ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğµ Ğ½ĞµÑ€Ğ°Ğ²Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… INCLUDE Ğ¸Ğ· 197,243 Ğ¿Ğ°Ñ€ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° 44 ÑĞ·Ñ‹ĞºĞ°Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… LLM. Ğ­Ñ‚Ğ¾Ñ‚ Ñ€ĞµÑÑƒÑ€Ñ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ¼ĞµÑÑ‚Ğ½Ñ‹Ñ… ÑĞºĞ·Ğ°Ğ¼ĞµĞ½Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ°Ñ… Ğ¸ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ñ‹. INCLUDE Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… LLM Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ… Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Bridging Language Gaps with INCLUDE: A Multilingual Benchmark for LLMs', 'desc': 'This paper addresses the performance gap of large language models (LLMs) across different languages, which limits their use in various regions. It highlights the challenge of developing multilingual LLMs due to the scarcity of high-quality evaluation resources in non-English languages. The authors propose a new evaluation suite called INCLUDE, consisting of 197,243 question-and-answer pairs sourced from local exams. This benchmark is designed to assess the capabilities of multilingual LLMs in real-world contexts, taking into account regional and cultural knowledge.'}, 'zh': {'title': 'æå‡å¤šè¯­è¨€æ¨¡å‹çš„å®é™…åº”ç”¨èƒ½åŠ›', 'desc': 'è¿™ç¯‡è®ºæ–‡è®¨è®ºäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä¸åŒè¯­è¨€ä¹‹é—´çš„æ€§èƒ½å·®å¼‚ï¼Œè¿™å½±å“äº†å®ƒä»¬åœ¨è®¸å¤šåœ°åŒºçš„æœ‰æ•ˆåº”ç”¨ã€‚ä¸ºäº†å…‹æœå¤šè¯­è¨€LLMå¼€å‘ä¸­çš„ç“¶é¢ˆï¼Œä½œè€…æ„å»ºäº†ä¸€ä¸ªåŒ…å«197,243ä¸ªé—®ç­”å¯¹çš„è¯„ä¼°å¥—ä»¶ï¼Œæ¥æºäºå½“åœ°è€ƒè¯•ææ–™ã€‚è¿™ä¸ªæ–°èµ„æºINCLUDEæ˜¯ä¸€ä¸ªå…¨é¢çš„çŸ¥è¯†å’Œæ¨ç†ä¸­å¿ƒåŸºå‡†ï¼Œæ¶µç›–44ç§ä¹¦é¢è¯­è¨€ï¼Œæ—¨åœ¨è¯„ä¼°å¤šè¯­è¨€LLMåœ¨å®é™…è¯­è¨€ç¯å¢ƒä¸­çš„è¡¨ç°ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œç ”ç©¶å¸Œæœ›æå‡ç”Ÿæˆæ€§äººå·¥æ™ºèƒ½å·¥å…·åœ¨ä¸åŒç¤¾åŒºçš„ç»æµå’Œç¤¾ä¼šä»·å€¼ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.01821', 'title': 'World-consistent Video Diffusion with Explicit 3D Modeling', 'url': 'https://huggingface.co/papers/2412.01821', 'abstract': 'Recent advancements in diffusion models have set new benchmarks in image and video generation, enabling realistic visual synthesis across single- and multi-frame contexts. However, these models still struggle with efficiently and explicitly generating 3D-consistent content. To address this, we propose World-consistent Video Diffusion (WVD), a novel framework that incorporates explicit 3D supervision using XYZ images, which encode global 3D coordinates for each image pixel. More specifically, we train a diffusion transformer to learn the joint distribution of RGB and XYZ frames. This approach supports multi-task adaptability via a flexible inpainting strategy. For example, WVD can estimate XYZ frames from ground-truth RGB or generate novel RGB frames using XYZ projections along a specified camera trajectory. In doing so, WVD unifies tasks like single-image-to-3D generation, multi-view stereo, and camera-controlled video generation. Our approach demonstrates competitive performance across multiple benchmarks, providing a scalable solution for 3D-consistent video and image generation with a single pretrained model.', 'score': 2, 'issue_id': 922, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 2', 'zh': '12æœˆ2æ—¥'}, 'hash': '843891601e85de06', 'authors': ['Qihang Zhang', 'Shuangfei Zhai', 'Miguel Angel Bautista', 'Kevin Miao', 'Alexander Toshev', 'Joshua Susskind', 'Jiatao Gu'], 'affiliations': ['Apple', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2412.01821.jpg', 'data': {'categories': ['#3d', '#benchmark', '#diffusion', '#video'], 'emoji': 'ğŸ¥', 'ru': {'title': '3D-ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ World-consistent Video Diffusion (WVD) Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D-ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°. WVD Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ XYZ-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ²Ğ½Ğ¾Ğ³Ğ¾ 3D-ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ RGB Ğ¸ XYZ ĞºĞ°Ğ´Ñ€Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ 3D Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼ ĞºĞ°Ğ¼ĞµÑ€Ñ‹. WVD Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ….'}, 'en': {'title': 'Unifying 3D Consistency in Video Generation with WVD', 'desc': 'This paper introduces World-consistent Video Diffusion (WVD), a new framework that enhances video and image generation by incorporating 3D supervision. WVD uses XYZ images to provide global 3D coordinates for each pixel, allowing for more accurate and consistent 3D content generation. The framework employs a diffusion transformer to learn the relationship between RGB and XYZ frames, enabling tasks like generating new RGB frames from 3D data. Overall, WVD offers a versatile solution for various tasks, achieving strong performance across benchmarks with a single pretrained model.'}, 'zh': {'title': 'ç»Ÿä¸€3Dä¸€è‡´æ€§çš„è§†é¢‘ç”Ÿæˆæ–°æ¡†æ¶', 'desc': 'æœ€è¿‘ï¼Œæ‰©æ•£æ¨¡å‹åœ¨å›¾åƒå’Œè§†é¢‘ç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œèƒ½å¤Ÿåœ¨å•å¸§å’Œå¤šå¸§ä¸Šä¸‹æ–‡ä¸­å®ç°é€¼çœŸçš„è§†è§‰åˆæˆã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨é«˜æ•ˆä¸”æ˜ç¡®åœ°ç”Ÿæˆ3Dä¸€è‡´å†…å®¹æ–¹é¢ä»ç„¶å­˜åœ¨æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸–ç•Œä¸€è‡´è§†é¢‘æ‰©æ•£ï¼ˆWVDï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œåˆ©ç”¨XYZå›¾åƒè¿›è¡Œæ˜ç¡®çš„3Dç›‘ç£ï¼Œç¼–ç æ¯ä¸ªå›¾åƒåƒç´ çš„å…¨å±€3Dåæ ‡ã€‚WVDé€šè¿‡çµæ´»çš„ä¿®è¡¥ç­–ç•¥æ”¯æŒå¤šä»»åŠ¡é€‚åº”æ€§ï¼Œèƒ½å¤Ÿä»çœŸå®çš„RGBä¼°è®¡XYZå¸§ï¼Œæˆ–æ²¿æŒ‡å®šçš„ç›¸æœºè½¨è¿¹ç”Ÿæˆæ–°çš„RGBå¸§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.00319', 'title': 'Improving speaker verification robustness with synthetic emotional utterances', 'url': 'https://huggingface.co/papers/2412.00319', 'abstract': 'A speaker verification (SV) system offers an authentication service designed to confirm whether a given speech sample originates from a specific speaker. This technology has paved the way for various personalized applications that cater to individual preferences. A noteworthy challenge faced by SV systems is their ability to perform consistently across a range of emotional spectra. Most existing models exhibit high error rates when dealing with emotional utterances compared to neutral ones. Consequently, this phenomenon often leads to missing out on speech of interest. This issue primarily stems from the limited availability of labeled emotional speech data, impeding the development of robust speaker representations that encompass diverse emotional states.   To address this concern, we propose a novel approach employing the CycleGAN framework to serve as a data augmentation method. This technique synthesizes emotional speech segments for each specific speaker while preserving the unique vocal identity. Our experimental findings underscore the effectiveness of incorporating synthetic emotional data into the training process. The models trained using this augmented dataset consistently outperform the baseline models on the task of verifying speakers in emotional speech scenarios, reducing equal error rate by as much as 3.64% relative.', 'score': 1, 'issue_id': 926, 'pub_date': '2024-11-30', 'pub_date_card': {'ru': '30 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 30', 'zh': '11æœˆ30æ—¥'}, 'hash': 'e9e71338ed876bc0', 'authors': ['Nikhil Kumar Koditala', 'Chelsea Jui-Ting Ju', 'Ruirui Li', 'Minho Jin', 'Aman Chadha', 'Andreas Stolcke'], 'affiliations': ['Amazon Alexa AI, USA', 'Amazon Business, USA', 'Amazon GenAI, USA', 'Amazon Search Experience Science, USA', 'Amazon Web Services, USA'], 'pdf_title_img': 'assets/pdf/title_img/2412.00319.jpg', 'data': {'categories': ['#training', '#dataset', '#synthetic', '#audio'], 'emoji': 'ğŸ­', 'ru': {'title': 'CycleGAN Ğ´Ğ»Ñ Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ€ĞµÑ‡Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰ĞµĞ³Ğ¾', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰ĞµĞ³Ğ¾ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ CycleGAN Ğ´Ğ»Ñ Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ€ĞµÑ‡Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€ÑƒÑÑ‚ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµÑ‡ĞµĞ²Ñ‹Ğµ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¸ĞºÑ‚Ğ¾Ñ€Ğ°, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ²ÑƒÑ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰Ğ¸Ñ… Ğ² ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ñ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ€ĞµÑ‡ÑŒÑ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ñ€Ğ°Ğ²Ğ½ÑƒÑ Ğ¾ÑˆĞ¸Ğ±ĞºÑƒ (EER) Ğ´Ğ¾ 3.64% Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ»Ğ¸Ğ½Ğ¸Ğ¸.'}, 'en': {'title': 'Enhancing Speaker Verification with Synthetic Emotional Data', 'desc': "This paper discusses a speaker verification (SV) system that authenticates speakers based on their voice. A major challenge for these systems is accurately verifying speakers when they express different emotions, as most existing models struggle with emotional speech. The authors propose using a CycleGAN framework to create synthetic emotional speech data, which helps improve the training of SV models. Their results show that incorporating this augmented data significantly enhances the models' performance, reducing error rates in emotional speech verification."}, 'zh': {'title': 'æƒ…æ„Ÿè¯­éŸ³éªŒè¯çš„æ–°çªç ´', 'desc': 'æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§è¯´è¯äººéªŒè¯ç³»ç»Ÿï¼Œæ—¨åœ¨ç¡®è®¤ç‰¹å®šè¯­éŸ³æ ·æœ¬æ˜¯å¦æ¥è‡ªç‰¹å®šè¯´è¯è€…ã€‚è¯¥ç³»ç»Ÿé¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜æ˜¯å¦‚ä½•åœ¨ä¸åŒæƒ…æ„ŸçŠ¶æ€ä¸‹ä¿æŒä¸€è‡´çš„æ€§èƒ½ã€‚ä¸ºäº†å…‹æœè¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬é‡‡ç”¨CycleGANæ¡†æ¶è¿›è¡Œæ•°æ®å¢å¼ºï¼Œåˆæˆæ¯ä¸ªè¯´è¯è€…çš„æƒ…æ„Ÿè¯­éŸ³ç‰‡æ®µï¼ŒåŒæ—¶ä¿ç•™å…¶ç‹¬ç‰¹çš„å£°éŸ³ç‰¹å¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨åˆæˆæƒ…æ„Ÿæ•°æ®è®­ç»ƒçš„æ¨¡å‹åœ¨æƒ…æ„Ÿè¯­éŸ³éªŒè¯ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œé”™è¯¯ç‡é™ä½äº†3.64%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.00869', 'title': 'Exploring the Abilities of Large Language Models to Solve Proportional Analogies via Knowledge-Enhanced Prompting', 'url': 'https://huggingface.co/papers/2412.00869', 'abstract': 'Making analogies is fundamental to cognition. Proportional analogies, which consist of four terms, are often used to assess linguistic and cognitive abilities. For instance, completing analogies like "Oxygen is to Gas as <blank> is to <blank>" requires identifying the semantic relationship (e.g., "type of") between the first pair of terms ("Oxygen" and "Gas") and finding a second pair that shares the same relationship (e.g., "Aluminum" and "Metal"). In this work, we introduce a 15K Multiple-Choice Question Answering (MCQA) dataset for proportional analogy completion and evaluate the performance of contemporary Large Language Models (LLMs) in various knowledge-enhanced prompt settings. Specifically, we augment prompts with three types of knowledge: exemplar, structured, and targeted. Our results show that despite extensive training data, solving proportional analogies remains challenging for current LLMs, with the best model achieving an accuracy of 55%. Notably, we find that providing targeted knowledge can better assist models in completing proportional analogies compared to providing exemplars or collections of structured knowledge.', 'score': 1, 'issue_id': 926, 'pub_date': '2024-12-01', 'pub_date_card': {'ru': '1 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 1', 'zh': '12æœˆ1æ—¥'}, 'hash': '6816796631155a8c', 'authors': ['Thilini Wijesiriwardene', 'Ruwan Wickramarachchi', 'Sreeram Vennam', 'Vinija Jain', 'Aman Chadha', 'Amitava Das', 'Ponnurangam Kumaraguru', 'Amit Sheth'], 'affiliations': ['AI Institute, University of South Carolina, USA', 'Amazon GenAI, USA', 'IIIT Hyderabad, India', 'Meta, USA', 'Stanford University, USA'], 'pdf_title_img': 'assets/pdf/title_img/2412.00869.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#reasoning', '#data'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ‘Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²ÑĞµ ĞµÑ‰Ğµ Ğ½Ğµ Ğ¼Ğ°ÑÑ‚ĞµÑ€Ğ° Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ğ¹', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ñ€ĞµÑˆĞ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¿Ğ¾Ñ€Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· 15 000 Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ğ¼ Ğ´Ğ»Ñ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ñ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ğ¹. ĞĞ½Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… LLM Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ°Ñ… Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ»Ğ¸ÑˆÑŒ 55%, Ğ¸ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ÑÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ, Ñ‡ĞµĞ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ğ¸Ğ»Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ.'}, 'en': {'title': 'Enhancing Analogy Completion in LLMs with Targeted Knowledge', 'desc': 'This paper focuses on the challenge of solving proportional analogies, which are important for assessing cognitive abilities. The authors present a new dataset containing 15,000 multiple-choice questions designed for this purpose. They evaluate how well current Large Language Models (LLMs) perform on these analogies, especially when given different types of knowledge prompts. The findings reveal that while LLMs struggle with these tasks, targeted knowledge prompts significantly improve their performance compared to other types of knowledge.'}, 'zh': {'title': 'æå‡ç±»æ¯”èƒ½åŠ›ï¼ŒçŸ¥è¯†æ˜¯å…³é”®ï¼', 'desc': 'ç±»æ¯”æ˜¯è®¤çŸ¥çš„é‡è¦éƒ¨åˆ†ï¼Œæ¯”ä¾‹ç±»æ¯”ç”±å››ä¸ªæœ¯è¯­ç»„æˆï¼Œå¸¸ç”¨äºè¯„ä¼°è¯­è¨€å’Œè®¤çŸ¥èƒ½åŠ›ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªåŒ…å«15,000ä¸ªå¤šé¡¹é€‰æ‹©é¢˜çš„æ¯”ä¾‹ç±»æ¯”å®Œæˆæ•°æ®é›†ï¼Œå¹¶è¯„ä¼°äº†å½“å‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä¸åŒçŸ¥è¯†å¢å¼ºæç¤ºè®¾ç½®ä¸‹çš„è¡¨ç°ã€‚ç ”ç©¶å‘ç°ï¼Œå°½ç®¡æ¨¡å‹ç»è¿‡å¤§é‡è®­ç»ƒï¼Œè§£å†³æ¯”ä¾‹ç±»æ¯”ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œæœ€ä½³æ¨¡å‹çš„å‡†ç¡®ç‡ä»…ä¸º55%ã€‚ç‰¹åˆ«æ˜¯ï¼Œæä¾›é’ˆå¯¹æ€§çš„çŸ¥è¯†æ¯”æä¾›ç¤ºä¾‹æˆ–ç»“æ„åŒ–çŸ¥è¯†æ›´èƒ½å¸®åŠ©æ¨¡å‹å®Œæˆæ¯”ä¾‹ç±»æ¯”ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.01408', 'title': 'Towards Cross-Lingual Audio Abuse Detection in Low-Resource Settings with Few-Shot Learning', 'url': 'https://huggingface.co/papers/2412.01408', 'abstract': 'Online abusive content detection, particularly in low-resource settings and within the audio modality, remains underexplored. We investigate the potential of pre-trained audio representations for detecting abusive language in low-resource languages, in this case, in Indian languages using Few Shot Learning (FSL). Leveraging powerful representations from models such as Wav2Vec and Whisper, we explore cross-lingual abuse detection using the ADIMA dataset with FSL. Our approach integrates these representations within the Model-Agnostic Meta-Learning (MAML) framework to classify abusive language in 10 languages. We experiment with various shot sizes (50-200) evaluating the impact of limited data on performance. Additionally, a feature visualization study was conducted to better understand model behaviour. This study highlights the generalization ability of pre-trained models in low-resource scenarios and offers valuable insights into detecting abusive language in multilingual contexts.', 'score': 1, 'issue_id': 918, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 2', 'zh': '12æœˆ2æ—¥'}, 'hash': 'f4c5e5e0485d3969', 'authors': ['Aditya Narayan Sankaran', 'Reza Farahbaksh', 'Noel Crespi'], 'affiliations': ['SAMOVAR, TÃ©lÃ©com SudParis Institut Polytechnique de Paris 91120 Palaiseau, France'], 'pdf_title_img': 'assets/pdf/title_img/2412.01408.jpg', 'data': {'categories': ['#low_resource', '#interpretability', '#dataset', '#multilingual', '#audio', '#training'], 'emoji': 'ğŸ™ï¸', 'ru': {'title': 'Ğ‘Ğ¾Ñ€ÑŒĞ±Ğ° Ñ Ğ¾ÑĞºĞ¾Ñ€Ğ±Ğ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ² Ğ°ÑƒĞ´Ğ¸Ğ¾: Ğ¼Ğ°Ğ»Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¼Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ¾Ğ²', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¾ÑĞºĞ¾Ñ€Ğ±Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ² Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ½Ğ° ÑĞ·Ñ‹ĞºĞ°Ñ… Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµÑÑƒÑ€ÑĞ°Ğ¼Ğ¸, Ğ² Ñ‡Ğ°ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ¸Ğ½Ğ´Ğ¸Ğ¹ÑĞºĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ°Ñ…, Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Few Shot Learning (FSL). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ°ÑƒĞ´Ğ¸Ğ¾-Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ· Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Wav2Vec Ğ¸ Whisper Ğ² Ñ€Ğ°Ğ¼ĞºĞ°Ñ… Model-Agnostic Meta-Learning (MAML) Ğ´Ğ»Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¾ÑĞºĞ¾Ñ€Ğ±Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° Ğ² 10 ÑĞ·Ñ‹ĞºĞ°Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ğ»Ğ¸ÑÑŒ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°Ğ¼Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ğº (50-200) Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ†ĞµĞ½Ğ½Ñ‹Ğµ insights Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¾ÑĞºĞ¾Ñ€Ğ±Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ.'}, 'en': {'title': 'Empowering Low-Resource Languages: Detecting Abuse in Audio with Few Shot Learning', 'desc': 'This paper focuses on detecting abusive language in audio, especially in languages with limited resources, like many Indian languages. It uses Few Shot Learning (FSL) techniques with pre-trained audio models such as Wav2Vec and Whisper to improve detection accuracy. The authors apply the Model-Agnostic Meta-Learning (MAML) framework to classify abusive content across 10 different languages, even with minimal training data. Their findings demonstrate how well pre-trained models can generalize in low-resource settings, providing insights for better abuse detection in multilingual environments.'}, 'zh': {'title': 'åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹æå‡ä½èµ„æºç¯å¢ƒä¸‹çš„è¾±éª‚å†…å®¹æ£€æµ‹', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†åœ¨ä½èµ„æºç¯å¢ƒä¸­ï¼Œç‰¹åˆ«æ˜¯éŸ³é¢‘æ¨¡å¼ä¸‹ï¼Œæ£€æµ‹åœ¨çº¿è¾±éª‚å†…å®¹çš„æ½œåŠ›ã€‚æˆ‘ä»¬ä½¿ç”¨é¢„è®­ç»ƒçš„éŸ³é¢‘è¡¨ç¤ºï¼Œç»“åˆå°‘é‡å­¦ä¹ ï¼ˆFew Shot Learningï¼‰ï¼Œåœ¨å°åº¦è¯­è¨€ä¸­è¿›è¡Œè¾±éª‚è¯­è¨€çš„æ£€æµ‹ã€‚é€šè¿‡åˆ©ç”¨Wav2Vecå’ŒWhisperç­‰æ¨¡å‹çš„å¼ºå¤§è¡¨ç¤ºï¼Œæˆ‘ä»¬åœ¨ADIMAæ•°æ®é›†ä¸Šè¿›è¡Œè·¨è¯­è¨€çš„è¾±éª‚æ£€æµ‹ã€‚ç ”ç©¶è¡¨æ˜ï¼Œé¢„è®­ç»ƒæ¨¡å‹åœ¨ä½èµ„æºåœºæ™¯ä¸­çš„æ³›åŒ–èƒ½åŠ›ï¼Œä¸ºå¤šè¯­è¨€ç¯å¢ƒä¸­çš„è¾±éª‚è¯­è¨€æ£€æµ‹æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.19477', 'title': 'A Simple and Provable Scaling Law for the Test-Time Compute of Large Language Models', 'url': 'https://huggingface.co/papers/2411.19477', 'abstract': 'We propose a general two-stage algorithm that enjoys a provable scaling law for the test-time compute of large language models (LLMs). Given an input problem, the proposed algorithm first generates N candidate solutions, and then chooses the best one via a multiple-round knockout tournament where each pair of candidates are compared for K times and only the winners move on to the next round. In a minimalistic implementation, both stages can be executed with a black-box LLM alone and nothing else (e.g., no external verifier or reward model), and a total of N times (K + 1) highly parallelizable LLM calls are needed for solving an input problem. Assuming that a generated candidate solution is correct with probability p_{gen} > 0 and a comparison between a pair of correct and incorrect solutions identifies the right winner with probability p_{comp} > 0.5 (i.e., better than a random guess), we prove theoretically that the failure probability of the proposed algorithm decays to zero exponentially with respect to N and K: $P(final output is incorrect) le (1 - p_{gen})^N + lceil log_2 N rceil e^{-2 K (p_{comp} - 0.5)^2}.$ Our empirical results with the challenging MMLU-Pro benchmark validate the technical assumptions, as well as the efficacy of the proposed algorithm and the gains from scaling up its test-time compute.', 'score': 1, 'issue_id': 912, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 29', 'zh': '11æœˆ29æ—¥'}, 'hash': '6057902d5fcbe3f4', 'authors': ['Yanxi Chen', 'Xuchen Pan', 'Yaliang Li', 'Bolin Ding', 'Jingren Zhou'], 'affiliations': ['Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2411.19477.jpg', 'data': {'categories': ['#training', '#benchmark', '#architecture', '#optimization'], 'emoji': 'ğŸ†', 'ru': {'title': 'ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ Ğ·Ğ°ĞºĞ¾Ğ½ Ğ´Ğ»Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ N ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ¾Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµĞµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ÑƒĞ½Ğ´Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ‚ÑƒÑ€Ğ½Ğ¸Ñ€Ğ° Ğ½Ğ° Ğ²Ñ‹Ğ±Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğµ. Ğ¢ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ° ÑĞºÑĞ¿Ğ¾Ğ½ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ĞµÑ‚ÑÑ Ñ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸ĞµĞ¼ N Ğ¸ K. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¼ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ MMLU-Pro Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°.'}, 'en': {'title': 'Efficient Solution Selection for Large Language Models', 'desc': 'This paper introduces a two-stage algorithm designed to improve the efficiency of large language models (LLMs) during test time. The first stage generates multiple candidate solutions for a given problem, while the second stage employs a knockout tournament to select the best solution through repeated comparisons. The algorithm can operate solely with a black-box LLM, requiring a manageable number of parallel calls to generate and evaluate candidates. The authors provide theoretical proof that the likelihood of incorrect outputs decreases exponentially as the number of candidates and comparisons increases, supported by empirical results from the MMLU-Pro benchmark.'}, 'zh': {'title': 'é«˜æ•ˆé€‰æ‹©ï¼šä¸¤é˜¶æ®µç®—æ³•ä¼˜åŒ–å¤§è¯­è¨€æ¨¡å‹è®¡ç®—', 'desc': 'æˆ‘ä»¬æå‡ºäº†ä¸€ç§é€šç”¨çš„ä¸¤é˜¶æ®µç®—æ³•ï¼Œèƒ½å¤Ÿåœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æµ‹è¯•æ—¶é—´è®¡ç®—ä¸­å®ç°å¯è¯æ˜çš„æ‰©å±•è§„å¾‹ã€‚è¯¥ç®—æ³•é¦–å…ˆç”ŸæˆNä¸ªå€™é€‰è§£ï¼Œç„¶åé€šè¿‡å¤šè½®æ·˜æ±°èµ›é€‰æ‹©æœ€ä½³è§£ï¼Œæ¯å¯¹å€™é€‰è§£æ¯”è¾ƒKæ¬¡ï¼Œåªæœ‰èƒœè€…è¿›å…¥ä¸‹ä¸€è½®ã€‚è¯¥ç®—æ³•çš„æœ€ç®€å®ç°ä»…éœ€ä½¿ç”¨é»‘ç®±LLMï¼Œæ— éœ€å¤–éƒ¨éªŒè¯å™¨æˆ–å¥–åŠ±æ¨¡å‹ï¼Œæ€»å…±éœ€è¦Næ¬¡(K + 1)é«˜åº¦å¯å¹¶è¡Œçš„LLMè°ƒç”¨æ¥è§£å†³è¾“å…¥é—®é¢˜ã€‚ç†è®ºè¯æ˜è¡¨æ˜ï¼Œå‡è®¾ç”Ÿæˆçš„å€™é€‰è§£æ­£ç¡®çš„æ¦‚ç‡ä¸ºp_{gen} > 0ï¼Œä¸”æ­£ç¡®ä¸é”™è¯¯è§£çš„æ¯”è¾ƒèƒ½ä»¥æ¦‚ç‡p_{comp} > 0.5è¯†åˆ«å‡ºæ­£ç¡®çš„èƒœè€…ï¼Œåˆ™è¯¥ç®—æ³•çš„å¤±è´¥æ¦‚ç‡éšç€Nå’ŒKçš„å¢åŠ å‘ˆæŒ‡æ•°çº§ä¸‹é™ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.02259', 'title': 'VideoGen-of-Thought: A Collaborative Framework for Multi-Shot Video Generation', 'url': 'https://huggingface.co/papers/2412.02259', 'abstract': 'Current video generation models excel at generating short clips but still struggle with creating multi-shot, movie-like videos. Existing models trained on large-scale data on the back of rich computational resources are unsurprisingly inadequate for maintaining a logical storyline and visual consistency across multiple shots of a cohesive script since they are often trained with a single-shot objective. To this end, we propose VideoGen-of-Thought (VGoT), a collaborative and training-free architecture designed specifically for multi-shot video generation. VGoT is designed with three goals in mind as follows. Multi-Shot Video Generation: We divide the video generation process into a structured, modular sequence, including (1) Script Generation, which translates a curt story into detailed prompts for each shot; (2) Keyframe Generation, responsible for creating visually consistent keyframes faithful to character portrayals; and (3) Shot-Level Video Generation, which transforms information from scripts and keyframes into shots; (4) Smoothing Mechanism that ensures a consistent multi-shot output. Reasonable Narrative Design: Inspired by cinematic scriptwriting, our prompt generation approach spans five key domains, ensuring logical consistency, character development, and narrative flow across the entire video. Cross-Shot Consistency: We ensure temporal and identity consistency by leveraging identity-preserving (IP) embeddings across shots, which are automatically created from the narrative. Additionally, we incorporate a cross-shot smoothing mechanism, which integrates a reset boundary that effectively combines latent features from adjacent shots, resulting in smooth transitions and maintaining visual coherence throughout the video. Our experiments demonstrate that VGoT surpasses existing video generation methods in producing high-quality, coherent, multi-shot videos.', 'score': 21, 'issue_id': 933, 'pub_date': '2024-12-03', 'pub_date_card': {'ru': '3 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 3', 'zh': '12æœˆ3æ—¥'}, 'hash': '5a007f38be3e3ba7', 'authors': ['Mingzhe Zheng', 'Yongqi Xu', 'Haojian Huang', 'Xuran Ma', 'Yexin Liu', 'Wenjie Shu', 'Yatian Pang', 'Feilong Tang', 'Qifeng Chen', 'Harry Yang', 'Ser-Nam Lim'], 'affiliations': ['Everlyn AI', 'Hong Kong University of Science and Technology', 'National University of Singapore', 'Peking University', 'University of Central Florida', 'University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2412.02259.jpg', 'data': {'categories': ['#video', '#story_generation', '#games'], 'emoji': 'ğŸ¬', 'ru': {'title': 'VGoT: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ĞºĞ°Ğ´Ñ€Ğ¾Ğ²Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ»Ğ¾Ğ³Ğ¸ĞºĞ¸ Ğ¿Ğ¾Ğ²ĞµÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ĞºĞ°Ğ´Ñ€Ğ¾Ğ²Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ VideoGen-of-Thought (VGoT). Ğ­Ñ‚Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ÑÑ‚Ğ°Ğ¿Ğ¾Ğ²: Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ñ, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ², Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑÑ†ĞµĞ½ Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ÑĞ³Ğ»Ğ°Ğ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ. VGoT Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑÑ†ĞµĞ½Ğ°Ğ¼Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¼ĞµĞ¶ĞºĞ°Ğ´Ñ€Ğ¾Ğ²Ğ¾Ğµ ÑĞ³Ğ»Ğ°Ğ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ VGoT Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸ ÑĞ²ÑĞ·Ğ½Ñ‹Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾ĞºĞ°Ğ´Ñ€Ğ¾Ğ²Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'Revolutionizing Multi-Shot Video Generation with VGoT', 'desc': 'The paper introduces VideoGen-of-Thought (VGoT), a novel architecture aimed at improving multi-shot video generation. Unlike traditional models that focus on single-shot outputs, VGoT employs a structured approach that includes script generation, keyframe creation, and shot-level video generation, ensuring a cohesive narrative. It emphasizes reasonable narrative design by incorporating principles from cinematic scriptwriting, which enhances character development and logical flow. Additionally, VGoT utilizes identity-preserving embeddings and a cross-shot smoothing mechanism to maintain visual consistency and smooth transitions across multiple shots.'}, 'zh': {'title': 'å¤šé•œå¤´è§†é¢‘ç”Ÿæˆçš„æ–°çªç ´', 'desc': 'å½“å‰çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹åœ¨ç”ŸæˆçŸ­ç‰‡æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨åˆ›å»ºå¤šé•œå¤´ã€ç”µå½±èˆ¬çš„è§†é¢‘æ—¶ä»ç„¶å­˜åœ¨å›°éš¾ã€‚ç°æœ‰æ¨¡å‹é€šå¸¸åªé’ˆå¯¹å•é•œå¤´ç›®æ ‡è¿›è¡Œè®­ç»ƒï¼Œå› æ­¤åœ¨ä¿æŒé€»è¾‘æ•…äº‹çº¿å’Œè§†è§‰ä¸€è‡´æ€§æ–¹é¢æ˜¾å¾—ä¸è¶³ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†VideoGen-of-Thoughtï¼ˆVGoTï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ä¸“é—¨ä¸ºå¤šé•œå¤´è§†é¢‘ç”Ÿæˆè®¾è®¡çš„åä½œå’Œæ— è®­ç»ƒæ¶æ„ã€‚VGoTé€šè¿‡è„šæœ¬ç”Ÿæˆã€å…³é”®å¸§ç”Ÿæˆå’Œé•œå¤´çº§è§†é¢‘ç”Ÿæˆç­‰æ¨¡å—åŒ–æ­¥éª¤ï¼Œç¡®ä¿äº†åˆç†çš„å™äº‹è®¾è®¡å’Œè·¨é•œå¤´çš„ä¸€è‡´æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.19943', 'title': "Critical Tokens Matter: Token-Level Contrastive Estimation Enhence LLM's Reasoning Capability", 'url': 'https://huggingface.co/papers/2411.19943', 'abstract': "Large Language Models (LLMs) have exhibited remarkable performance on reasoning tasks. They utilize autoregressive token generation to construct reasoning trajectories, enabling the development of a coherent chain of thought. In this work, we explore the impact of individual tokens on the final outcomes of reasoning tasks. We identify the existence of ``critical tokens'' that lead to incorrect reasoning trajectories in LLMs. Specifically, we find that LLMs tend to produce positive outcomes when forced to decode other tokens instead of critical tokens. Motivated by this observation, we propose a novel approach - cDPO - designed to automatically recognize and conduct token-level rewards for the critical tokens during the alignment process. Specifically, we develop a contrastive estimation approach to automatically identify critical tokens. It is achieved by comparing the generation likelihood of positive and negative models. To achieve this, we separately fine-tune the positive and negative models on various reasoning trajectories, consequently, they are capable of identifying identify critical tokens within incorrect trajectories that contribute to erroneous outcomes. Moreover, to further align the model with the critical token information during the alignment process, we extend the conventional DPO algorithms to token-level DPO and utilize the differential likelihood from the aforementioned positive and negative model as important weight for token-level DPO learning.Experimental results on GSM8K and MATH500 benchmarks with two-widely used models Llama-3 (8B and 70B) and deepseek-math (7B) demonstrate the effectiveness of the propsoed approach cDPO.", 'score': 21, 'issue_id': 933, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 29', 'zh': '11æœˆ29æ—¥'}, 'hash': 'aaf523f6bd9412e3', 'authors': ['Zicheng Lin', 'Tian Liang', 'Jiahao Xu', 'Xing Wang', 'Ruilin Luo', 'Chufan Shi', 'Siheng Li', 'Yujiu Yang', 'Zhaopeng Tu'], 'affiliations': ['Tsinghua University', 'Tencent AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2411.19943.jpg', 'data': {'categories': ['#math', '#training', '#rlhf', '#reasoning', '#benchmark', '#alignment'], 'emoji': 'ğŸ”', 'ru': {'title': 'ĞŸĞ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ LLM Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²', 'desc': "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ½Ğ° Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ 'ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²', ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´ÑÑ‚ Ğº Ğ½ĞµĞ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¼ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ÑĞ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ cDPO Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… GSM8K Ğ¸ MATH500 Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Llama-3 Ğ¸ deepseek-math Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°."}, 'en': {'title': 'Enhancing Reasoning in LLMs by Identifying Critical Tokens', 'desc': "This paper investigates how individual tokens in Large Language Models (LLMs) affect reasoning outcomes. It identifies 'critical tokens' that can lead to incorrect reasoning paths, suggesting that LLMs perform better when they focus on non-critical tokens. The authors introduce a new method called cDPO, which uses contrastive estimation to automatically detect these critical tokens during the model's alignment process. Experimental results show that cDPO improves reasoning performance on benchmark datasets by effectively managing token-level rewards."}, 'zh': {'title': 'è¯†åˆ«å…³é”®tokenï¼Œæå‡æ¨ç†å‡†ç¡®æ€§', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿé€šè¿‡è‡ªå›å½’çš„æ–¹å¼ç”Ÿæˆæ¨ç†è¿‡ç¨‹ã€‚æœ¬æ–‡æ¢è®¨äº†å•ä¸ªtokenå¯¹æ¨ç†ä»»åŠ¡æœ€ç»ˆç»“æœçš„å½±å“ï¼Œå‘ç°å­˜åœ¨â€œå…³é”®tokenâ€ï¼Œè¿™äº›tokenä¼šå¯¼è‡´é”™è¯¯çš„æ¨ç†è½¨è¿¹ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•cDPOï¼Œæ—¨åœ¨è‡ªåŠ¨è¯†åˆ«å…³é”®tokenå¹¶åœ¨å¯¹é½è¿‡ç¨‹ä¸­è¿›è¡Œtokençº§å¥–åŠ±ã€‚é€šè¿‡å¯¹æ¯”æ­£è´Ÿæ¨¡å‹çš„ç”Ÿæˆå¯èƒ½æ€§ï¼Œæˆ‘ä»¬èƒ½å¤Ÿè¯†åˆ«å‡ºåœ¨é”™è¯¯è½¨è¿¹ä¸­å¯¼è‡´é”™è¯¯ç»“æœçš„å…³é”®tokenï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.01981', 'title': 'Free Process Rewards without Process Labels', 'url': 'https://huggingface.co/papers/2412.01981', 'abstract': "Different from its counterpart outcome reward models (ORMs), which evaluate the entire responses, a process reward model (PRM) scores a reasoning trajectory step by step, providing denser and more fine grained rewards. However, training a PRM requires labels annotated at every intermediate step, presenting significant challenges for both manual and automatic data collection. This paper aims to address this challenge. Both theoretically and empirically, we show that an implicit PRM can be obtained at no additional cost, by simply training an ORM on the cheaper response-level labels. The only assumption is to parameterize the outcome reward as the log-likelihood ratios of the policy and reference models, which can be optimized regardless of the specific choice of loss objectives. In experiments, we instantiate our implicit PRMs with various objectives and evaluate their performance on MATH. We show that our implicit PRM outperforms a strong MCTS-based baseline \\'a la Math-Shepherd using less than 1/38 of the training data. Its performance can be further improved with majority voting. We further find that scaling up instructions and responses benefits our implicit PRM, and the latter brings a larger gain. Particularly, we find that our implicit PRM, when instantiated with the cross-entropy (CE) loss, is more data-efficient and can keep improving generation models even when trained with only one response per instruction, the setup that suffers from extreme data scarcity and imbalance. Further, instructions should be relevant to downstream tasks while the diversity of responses does not bring gains. Surprisingly, training on extra Math-Shepherd step labels brings no further improvements to our implicit PRM trained on only outcome data. We hope that our work will encourage a rethinking of PRM training approaches and contribute to making training PRMs more accessible.", 'score': 15, 'issue_id': 933, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 2', 'zh': '12æœˆ2æ—¥'}, 'hash': '13434e4f301a0d88', 'authors': ['Lifan Yuan', 'Wendi Li', 'Huayu Chen', 'Ganqu Cui', 'Ning Ding', 'Kaiyan Zhang', 'Bowen Zhou', 'Zhiyuan Liu', 'Hao Peng'], 'affiliations': ['Huazhong University of Science and Technology', 'Tsinghua University', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2412.01981.jpg', 'data': {'categories': ['#optimization', '#math', '#training', '#reasoning', '#data', '#low_resource'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ PRM Ğ±ĞµĞ· Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ (PRM) Ğ² Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ½ĞµÑĞ²Ğ½ÑƒÑ PRM Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚, Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ·Ğ° Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ (ORM) Ğ½Ğ° Ğ±Ğ¾Ğ»ĞµĞµ Ğ´ĞµÑˆĞµĞ²Ñ‹Ñ… Ğ¼ĞµÑ‚ĞºĞ°Ñ… ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ MATH Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ±ĞµĞ¹Ğ·Ğ»Ğ°Ğ¹Ğ½ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ MCTS, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼ĞµĞ½ĞµĞµ 1/38 Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½ĞµÑĞ²Ğ½Ğ¾Ğ¹ PRM.'}, 'en': {'title': 'Unlocking Efficient Training for Process Reward Models', 'desc': 'This paper introduces a novel approach to training process reward models (PRMs) that score reasoning steps individually, as opposed to outcome reward models (ORMs) which evaluate entire responses. The authors propose that an implicit PRM can be derived from an ORM trained on simpler response-level labels, thus avoiding the need for detailed step-by-step annotations. Through theoretical and empirical analysis, they demonstrate that this implicit PRM can achieve superior performance on tasks like MATH, using significantly less training data than traditional methods. The findings suggest that optimizing the outcome reward as log-likelihood ratios enhances data efficiency and model performance, even in scenarios with limited training examples.'}, 'zh': {'title': 'éšå¼è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼šé«˜æ•ˆè®­ç»ƒçš„æ–°æ€è·¯', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰ï¼Œä¸ä¼ ç»Ÿçš„ç»“æœå¥–åŠ±æ¨¡å‹ï¼ˆORMï¼‰ä¸åŒï¼ŒPRMèƒ½å¤Ÿé€æ­¥è¯„ä¼°æ¨ç†è¿‡ç¨‹ï¼Œæä¾›æ›´ç»†è‡´çš„å¥–åŠ±ã€‚ç„¶è€Œï¼Œè®­ç»ƒPRMéœ€è¦åœ¨æ¯ä¸ªä¸­é—´æ­¥éª¤éƒ½æœ‰æ ‡æ³¨ï¼Œè¿™åœ¨æ•°æ®æ”¶é›†ä¸Šé¢ä¸´æŒ‘æˆ˜ã€‚æˆ‘ä»¬å±•ç¤ºäº†é€šè¿‡è®­ç»ƒORMå¹¶ä½¿ç”¨å“åº”çº§åˆ«çš„æ ‡ç­¾ï¼Œå¯ä»¥åœ¨æ²¡æœ‰é¢å¤–æˆæœ¬çš„æƒ…å†µä¸‹è·å¾—éšå¼PRMã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œéšå¼PRMåœ¨æ•°æ®æ•ˆç‡å’Œæ€§èƒ½ä¸Šä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œå°¤å…¶æ˜¯åœ¨æ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.02611', 'title': 'AV-Odyssey Bench: Can Your Multimodal LLMs Really Understand Audio-Visual Information?', 'url': 'https://huggingface.co/papers/2412.02611', 'abstract': 'Recently, multimodal large language models (MLLMs), such as GPT-4o, Gemini 1.5 Pro, and Reka Core, have expanded their capabilities to include vision and audio modalities. While these models demonstrate impressive performance across a wide range of audio-visual applications, our proposed DeafTest reveals that MLLMs often struggle with simple tasks humans find trivial: 1) determining which of two sounds is louder, and 2) determining which of two sounds has a higher pitch. Motivated by these observations, we introduce AV-Odyssey Bench, a comprehensive audio-visual benchmark designed to assess whether those MLLMs can truly understand the audio-visual information. This benchmark encompasses 4,555 carefully crafted problems, each incorporating text, visual, and audio components. To successfully infer answers, models must effectively leverage clues from both visual and audio inputs. To ensure precise and objective evaluation of MLLM responses, we have structured the questions as multiple-choice, eliminating the need for human evaluation or LLM-assisted assessment. We benchmark a series of closed-source and open-source models and summarize the observations. By revealing the limitations of current models, we aim to provide useful insight for future dataset collection and model development.', 'score': 11, 'issue_id': 933, 'pub_date': '2024-12-03', 'pub_date_card': {'ru': '3 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 3', 'zh': '12æœˆ3æ—¥'}, 'hash': 'f63565048b4948b4', 'authors': ['Kaixiong Gong', 'Kaituo Feng', 'Bohao Li', 'Yibing Wang', 'Mofan Cheng', 'Shijia Yang', 'Jiaming Han', 'Benyou Wang', 'Yutong Bai', 'Zhuoran Yang', 'Xiangyu Yue'], 'affiliations': ['CUHK (SZ)', 'CUHK MMLab', 'Stanford University', 'UC Berkeley', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2412.02611.jpg', 'data': {'categories': ['#open_source', '#benchmark', '#interpretability', '#multimodal', '#games'], 'emoji': 'ğŸ§', 'ru': {'title': 'Ğ¡Ğ»Ñ‹ÑˆĞ°Ñ‚ Ğ»Ğ¸ Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‚Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ²Ğ¸Ğ´ÑÑ‚?', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚ DeafTest Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº AV-Odyssey Bench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ°ÑƒĞ´Ğ¸Ğ¾-Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ MLLM Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ½Ğµ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ³Ñ€Ğ¾Ğ¼ĞºĞ¾ÑÑ‚Ğ¸ Ğ¸ Ğ²Ñ‹ÑĞ¾Ñ‚Ñ‹ Ğ·Ğ²ÑƒĞºĞ¾Ğ². Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 4,555 Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼, Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±Ğ¾Ğ¸Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€ÑĞ´Ğ° Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ²Ñ‹ÑĞ²Ğ¸Ğ² Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ´Ğ»Ñ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞµĞ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Unveiling the Limits of Multimodal Models with AV-Odyssey Bench', 'desc': 'This paper introduces DeafTest, a benchmark that highlights the limitations of multimodal large language models (MLLMs) in understanding basic audio tasks that humans find easy. The authors present AV-Odyssey Bench, which consists of 4,555 problems that require models to analyze and integrate audio, visual, and text information. The benchmark is designed to objectively evaluate MLLM performance through multiple-choice questions, avoiding reliance on human judgment. By assessing various models, the study aims to shed light on the shortcomings of current MLLMs and guide future improvements in model training and dataset creation.'}, 'zh': {'title': 'æ­ç¤ºå¤šæ¨¡æ€æ¨¡å‹çš„å±€é™æ€§', 'desc': 'æœ€è¿‘ï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å¦‚GPT-4oã€Gemini 1.5 Proå’ŒReka Coreï¼Œæ‰©å±•äº†å…¶åœ¨è§†è§‰å’ŒéŸ³é¢‘æ–¹é¢çš„èƒ½åŠ›ã€‚å°½ç®¡è¿™äº›æ¨¡å‹åœ¨å¤šç§éŸ³é¢‘-è§†è§‰åº”ç”¨ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†æˆ‘ä»¬çš„DeafTestæ˜¾ç¤ºï¼ŒMLLMsåœ¨ä¸€äº›äººç±»è®¤ä¸ºç®€å•çš„ä»»åŠ¡ä¸Šå¸¸å¸¸è¡¨ç°ä¸ä½³ï¼Œä¾‹å¦‚åˆ¤æ–­ä¸¤ä¸ªå£°éŸ³å“ªä¸ªæ›´å“å’Œå“ªä¸ªéŸ³è°ƒæ›´é«˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†AV-Odyssey Benchï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„éŸ³é¢‘-è§†è§‰åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°è¿™äº›MLLMsæ˜¯å¦çœŸæ­£ç†è§£éŸ³é¢‘-è§†è§‰ä¿¡æ¯ã€‚è¯¥åŸºå‡†åŒ…å«4555ä¸ªç²¾å¿ƒè®¾è®¡çš„é—®é¢˜ï¼Œè¦æ±‚æ¨¡å‹æœ‰æ•ˆåˆ©ç”¨è§†è§‰å’ŒéŸ³é¢‘è¾“å…¥ä¸­çš„çº¿ç´¢ï¼Œä»¥å‡†ç¡®æ¨æ–­ç­”æ¡ˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.02114', 'title': 'OmniCreator: Self-Supervised Unified Generation with Universal Editing', 'url': 'https://huggingface.co/papers/2412.02114', 'abstract': 'We introduce OmniCreator, a novel framework that can conduct text-prompted unified (image+video) generation as well as editing all in one place. OmniCreator acquires generative and universal editing capabilities in a self-supervised manner, taking original text-video pairs as conditions while utilizing the same video as a denoising target to learn the semantic correspondence between video and text. During inference, when presented with a text prompt and a video, OmniCreator is capable of generating a target that is faithful to both, achieving a universal editing effect that is unconstrained as opposed to existing editing work that primarily focuses on certain editing types or relies on additional controls (e.g., structural conditions, attention features, or DDIM inversion). On the other hand, when presented with a text prompt only, OmniCreator becomes generative, producing high-quality video as a result of the semantic correspondence learned. Importantly, we found that the same capabilities extend to images as is, making OmniCreator a truly unified framework. Further, due to the lack of existing generative video editing benchmarks, we introduce the OmniBench-99 dataset, designed to evaluate the performance of generative video editing models comprehensively. Extensive experiments demonstrate that OmniCreator exhibits substantial superiority over all other models.', 'score': 7, 'issue_id': 939, 'pub_date': '2024-12-03', 'pub_date_card': {'ru': '3 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 3', 'zh': '12æœˆ3æ—¥'}, 'hash': '62bf26709baf7f97', 'authors': ['Haodong Chen', 'Lan Wang', 'Harry Yang', 'Ser-Nam Lim'], 'affiliations': ['Everlyn AI', 'HKUST', 'MSU', 'UCF'], 'pdf_title_img': 'assets/pdf/title_img/2412.02114.jpg', 'data': {'categories': ['#diffusion', '#multimodal', '#benchmark', '#dataset', '#video', '#optimization'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¾Ñ€ Ğ¼ĞµĞ´Ğ¸Ğ° Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑƒ', 'desc': 'OmniCreator - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². ĞĞ½Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¿Ğ°Ñ€Ñ‹ Ñ‚ĞµĞºÑÑ‚-Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¸ ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ²ÑĞ·Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼. OmniCreator Ğ¼Ğ¾Ğ¶ĞµÑ‚ ĞºĞ°Ğº Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ, Ñ‚Ğ°Ğº Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… OmniBench-99 Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'OmniCreator: Unified Text-Prompted Image and Video Generation and Editing', 'desc': 'OmniCreator is a new framework that allows for the generation and editing of both images and videos using text prompts. It learns to understand the relationship between text and video through self-supervised learning, using original text-video pairs as input. During use, it can either generate new videos based on text prompts or edit existing videos while maintaining fidelity to the provided text. Additionally, the framework is evaluated using a new dataset called OmniBench-99, which helps assess the performance of generative video editing models.'}, 'zh': {'title': 'OmniCreatorï¼šç»Ÿä¸€çš„å›¾åƒä¸è§†é¢‘ç”Ÿæˆä¸ç¼–è¾‘æ¡†æ¶', 'desc': 'OmniCreatoræ˜¯ä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨ä¸€ä¸ªå¹³å°ä¸Šè¿›è¡Œæ–‡æœ¬æç¤ºçš„ç»Ÿä¸€ç”Ÿæˆï¼ˆå›¾åƒ+è§†é¢‘ï¼‰å’Œç¼–è¾‘ã€‚å®ƒé€šè¿‡è‡ªç›‘ç£å­¦ä¹ ï¼Œåˆ©ç”¨åŸå§‹çš„æ–‡æœ¬-è§†é¢‘å¯¹ä½œä¸ºæ¡ä»¶ï¼ŒåŒæ—¶ä½¿ç”¨ç›¸åŒçš„è§†é¢‘ä½œä¸ºå»å™ªç›®æ ‡ï¼Œå­¦ä¹ è§†é¢‘ä¸æ–‡æœ¬ä¹‹é—´çš„è¯­ä¹‰å¯¹åº”å…³ç³»ã€‚åœ¨æ¨ç†é˜¶æ®µï¼ŒOmniCreatorèƒ½å¤Ÿæ ¹æ®æ–‡æœ¬æç¤ºå’Œè§†é¢‘ç”Ÿæˆå¿ å®äºä¸¤è€…çš„ç›®æ ‡ï¼Œå®ç°æ— çº¦æŸçš„é€šç”¨ç¼–è¾‘æ•ˆæœã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†OmniBench-99æ•°æ®é›†ï¼Œä»¥å…¨é¢è¯„ä¼°ç”Ÿæˆè§†é¢‘ç¼–è¾‘æ¨¡å‹çš„æ€§èƒ½ï¼Œå®éªŒç»“æœè¡¨æ˜OmniCreatoråœ¨æ‰€æœ‰æ¨¡å‹ä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„ä¼˜åŠ¿ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.02632', 'title': 'Scaling Image Tokenizers with Grouped Spherical Quantization', 'url': 'https://huggingface.co/papers/2412.02632', 'abstract': 'Vision tokenizers have gained a lot of attraction due to their scalability and compactness; previous works depend on old-school GAN-based hyperparameters, biased comparisons, and a lack of comprehensive analysis of the scaling behaviours. To tackle those issues, we introduce Grouped Spherical Quantization (GSQ), featuring spherical codebook initialization and lookup regularization to constrain codebook latent to a spherical surface. Our empirical analysis of image tokenizer training strategies demonstrates that GSQ-GAN achieves superior reconstruction quality over state-of-the-art methods with fewer training iterations, providing a solid foundation for scaling studies. Building on this, we systematically examine the scaling behaviours of GSQ, specifically in latent dimensionality, codebook size, and compression ratios, and their impact on model performance. Our findings reveal distinct behaviours at high and low spatial compression levels, underscoring challenges in representing high-dimensional latent spaces. We show that GSQ can restructure high-dimensional latent into compact, low-dimensional spaces, thus enabling efficient scaling with improved quality. As a result, GSQ-GAN achieves a 16x down-sampling with a reconstruction FID (rFID) of 0.50.', 'score': 5, 'issue_id': 933, 'pub_date': '2024-12-03', 'pub_date_card': {'ru': '3 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 3', 'zh': '12æœˆ3æ—¥'}, 'hash': '60eda94a31cded90', 'authors': ['Jiangtao Wang', 'Zhen Qin', 'Yifan Zhang', 'Vincent Tao Hu', 'BjÃ¶rn Ommer', 'Rania Briq', 'Stefan Kesselheim'], 'affiliations': ['CompVis @ LMU Munich', 'JÃ¼lich Supercomputing Centre', 'TapTap', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2412.02632.jpg', 'data': {'categories': ['#training', '#inference', '#cv', '#data'], 'emoji': 'ğŸ”', 'ru': {'title': 'GSQ: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° ÑÑ„ĞµÑ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ¸', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ - Ğ“Ñ€ÑƒĞ¿Ğ¿Ğ¾Ğ²Ğ°Ñ Ğ¡Ñ„ĞµÑ€Ğ¸Ñ‡ĞµÑĞºĞ°Ñ ĞšĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ (GSQ). GSQ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ„ĞµÑ€Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ĞºĞ¾Ğ´Ğ¾Ğ²Ğ¾Ğ¹ ĞºĞ½Ğ¸Ğ³Ğ¸ Ğ¸ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ´Ğ»Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° ĞºĞ¾Ğ´Ğ¾Ğ²Ğ¾Ğ¹ ĞºĞ½Ğ¸Ğ³Ğ¸ ÑÑ„ĞµÑ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚ÑŒÑ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ ÑĞ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ GSQ-GAN Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞµĞ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ GSQ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ğ¸ Ğ½Ğ¸Ğ·ĞºĞ¸Ñ… ÑƒÑ€Ğ¾Ğ²Ğ½ÑÑ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ, Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ².'}, 'en': {'title': 'Efficient Image Processing with Grouped Spherical Quantization', 'desc': "This paper introduces a new method called Grouped Spherical Quantization (GSQ) for improving vision tokenizers, which are tools used to process images efficiently. GSQ uses a special technique to initialize and regularize a spherical codebook, helping to keep the data organized on a spherical surface. The authors demonstrate that GSQ-GAN, a model based on this method, can reconstruct images with high quality while requiring fewer training iterations compared to existing methods. Their analysis also reveals how different factors like latent dimensionality and codebook size affect the model's performance, particularly in handling high-dimensional data efficiently."}, 'zh': {'title': 'åˆ†ç»„çƒé¢é‡åŒ–ï¼šé«˜æ•ˆçš„è§†è§‰æ ‡è®°å™¨æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„è§†è§‰æ ‡è®°å™¨æ–¹æ³•ï¼Œç§°ä¸ºåˆ†ç»„çƒé¢é‡åŒ–ï¼ˆGSQï¼‰ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿæ–¹æ³•ä¸­çš„ä¸€äº›é—®é¢˜ã€‚GSQé€šè¿‡çƒé¢ä»£ç æœ¬åˆå§‹åŒ–å’ŒæŸ¥æ‰¾æ­£åˆ™åŒ–ï¼Œé™åˆ¶äº†ä»£ç æœ¬æ½œåœ¨ç©ºé—´åœ¨çƒé¢ä¸Šçš„åˆ†å¸ƒã€‚æˆ‘ä»¬çš„å®è¯åˆ†æè¡¨æ˜ï¼ŒGSQ-GANåœ¨é‡å»ºè´¨é‡ä¸Šä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œå¹¶ä¸”è®­ç»ƒè¿­ä»£æ¬¡æ•°æ›´å°‘ã€‚ç ”ç©¶è¿˜ç³»ç»Ÿåœ°è€ƒå¯Ÿäº†GSQåœ¨æ½œåœ¨ç»´åº¦ã€ä»£ç æœ¬å¤§å°å’Œå‹ç¼©æ¯”ç­‰æ–¹é¢çš„æ‰©å±•è¡Œä¸ºï¼Œæ­ç¤ºäº†é«˜ç»´æ½œåœ¨ç©ºé—´è¡¨ç¤ºçš„æŒ‘æˆ˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.19067', 'title': 'MaskRIS: Semantic Distortion-aware Data Augmentation for Referring Image Segmentation', 'url': 'https://huggingface.co/papers/2411.19067', 'abstract': "Referring Image Segmentation (RIS) is an advanced vision-language task that involves identifying and segmenting objects within an image as described by free-form text descriptions. While previous studies focused on aligning visual and language features, exploring training techniques, such as data augmentation, remains underexplored. In this work, we explore effective data augmentation for RIS and propose a novel training framework called Masked Referring Image Segmentation (MaskRIS). We observe that the conventional image augmentations fall short of RIS, leading to performance degradation, while simple random masking significantly enhances the performance of RIS. MaskRIS uses both image and text masking, followed by Distortion-aware Contextual Learning (DCL) to fully exploit the benefits of the masking strategy. This approach can improve the model's robustness to occlusions, incomplete information, and various linguistic complexities, resulting in a significant performance improvement. Experiments demonstrate that MaskRIS can easily be applied to various RIS models, outperforming existing methods in both fully supervised and weakly supervised settings. Finally, MaskRIS achieves new state-of-the-art performance on RefCOCO, RefCOCO+, and RefCOCOg datasets. Code is available at https://github.com/naver-ai/maskris.", 'score': 4, 'issue_id': 934, 'pub_date': '2024-11-28', 'pub_date_card': {'ru': '28 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 28', 'zh': '11æœˆ28æ—¥'}, 'hash': '74d4a17af3574a5d', 'authors': ['Minhyun Lee', 'Seungho Lee', 'Song Park', 'Dongyoon Han', 'Byeongho Heo', 'Hyunjung Shim'], 'affiliations': ['KAIST AI', 'NAVER AI Lab', 'Yonsei University'], 'pdf_title_img': 'assets/pdf/title_img/2411.19067.jpg', 'data': {'categories': ['#optimization', '#cv', '#survey', '#dataset', '#training'], 'emoji': 'ğŸ­', 'ru': {'title': 'ĞœĞ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ (RIS). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ MaskRIS, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº Ğ¾ĞºĞºĞ»ÑĞ·Ğ¸ÑĞ¼, Ğ½ĞµĞ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ MaskRIS Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ĞºĞ°Ğº Ğ¿Ñ€Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¼, Ñ‚Ğ°Ğº Ğ¸ Ğ¿Ñ€Ğ¸ ÑĞ»Ğ°Ğ±Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸.'}, 'en': {'title': 'Enhancing Referring Image Segmentation with Masking Techniques', 'desc': 'Referring Image Segmentation (RIS) is a task that combines visual and language understanding to identify and segment objects in images based on text descriptions. This paper introduces a new training framework called Masked Referring Image Segmentation (MaskRIS), which focuses on effective data augmentation techniques for RIS. The authors found that traditional image augmentations were inadequate, while their method of random masking significantly improved performance. MaskRIS enhances model robustness against occlusions and linguistic variations, achieving state-of-the-art results on several benchmark datasets.'}, 'zh': {'title': 'Masked Referring Image Segmentationï¼šæå‡å›¾åƒåˆ†å‰²æ€§èƒ½çš„æ–°æ–¹æ³•', 'desc': 'å¼•ç”¨å›¾åƒåˆ†å‰²ï¼ˆRISï¼‰æ˜¯ä¸€ç§å…ˆè¿›çš„è§†è§‰-è¯­è¨€ä»»åŠ¡ï¼Œæ—¨åœ¨æ ¹æ®è‡ªç”±å½¢å¼çš„æ–‡æœ¬æè¿°è¯†åˆ«å’Œåˆ†å‰²å›¾åƒä¸­çš„å¯¹è±¡ã€‚æœ¬æ–‡æ¢è®¨äº†æœ‰æ•ˆçš„æ•°æ®å¢å¼ºæŠ€æœ¯ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°çš„è®­ç»ƒæ¡†æ¶ï¼Œç§°ä¸ºMasked Referring Image Segmentationï¼ˆMaskRISï¼‰ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä¼ ç»Ÿçš„å›¾åƒå¢å¼ºæ–¹æ³•åœ¨RISä¸­æ•ˆæœä¸ä½³ï¼Œè€Œç®€å•çš„éšæœºé®ç½©æ˜¾è‘—æå‡äº†RISçš„æ€§èƒ½ã€‚MaskRISç»“åˆäº†å›¾åƒå’Œæ–‡æœ¬é®ç½©ï¼Œå¹¶é‡‡ç”¨äº†å¤±çœŸæ„ŸçŸ¥ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆDCLï¼‰ï¼Œä»è€Œæé«˜äº†æ¨¡å‹å¯¹é®æŒ¡ã€ä¸å®Œæ•´ä¿¡æ¯å’Œè¯­è¨€å¤æ‚æ€§çš„é²æ£’æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.01292', 'title': 'LSceneLLM: Enhancing Large 3D Scene Understanding Using Adaptive Visual Preferences', 'url': 'https://huggingface.co/papers/2412.01292', 'abstract': "Research on 3D Vision-Language Models (3D-VLMs) is gaining increasing attention, which is crucial for developing embodied AI within 3D scenes, such as visual navigation and embodied question answering. Due to the high density of visual features, especially in large 3D scenes, accurately locating task-relevant visual information is challenging. Existing works attempt to segment all objects and consider their features as scene representations. However, these task-agnostic object features include much redundant information and missing details for the task-relevant area. To tackle these problems, we propose LSceneLLM, an adaptive framework that automatically identifies task-relevant areas by leveraging LLM's visual preference for different tasks, followed by a plug-and-play scene magnifier module to capture fine-grained details in focused areas. Specifically, a dense token selector examines the attention map of LLM to identify visual preferences for the instruction input. It then magnifies fine-grained details of the focusing area. An adaptive self-attention module is leveraged to fuse the coarse-grained and selected fine-grained visual information. To comprehensively evaluate the large scene understanding ability of 3D-VLMs, we further introduce a cross-room understanding benchmark, XR-Scene, which contains a series of large scene understanding tasks including XR-QA, XR-EmbodiedPlanning, and XR-SceneCaption. Experiments show that our method surpasses existing methods on both large scene understanding and existing scene understanding benchmarks. Plunging our scene magnifier module into the existing 3D-VLMs also brings significant improvement.", 'score': 4, 'issue_id': 933, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 2', 'zh': '12æœˆ2æ—¥'}, 'hash': 'e8f8ddd05e13e9ef', 'authors': ['Hongyan Zhi', 'Peihao Chen', 'Junyan Li', 'Shuailei Ma', 'Xinyu Sun', 'Tianhang Xiang', 'Yinjie Lei', 'Mingkui Tan', 'Chuang Gan'], 'affiliations': ['MIT-IBM Watson AI Lab', 'Northeastern University', 'Pazhou Laboratory', 'Sichuan University', 'South China University of Technology', 'Tencent Robotics X', 'UMass Amherst'], 'pdf_title_img': 'assets/pdf/title_img/2412.01292.jpg', 'data': {'categories': ['#architecture', '#benchmark', '#3d', '#multimodal'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ 3D-ÑÑ†ĞµĞ½ Ñ LSceneLLM', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ 3D Vision-Language Models (3D-VLMs) Ğ²Ğ°Ğ¶Ğ½Ğ¾ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ AI Ğ² 3D-ÑÑ†ĞµĞ½Ğ°Ñ…, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹. Ğ˜Ğ·-Ğ·Ğ° Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… 3D-ÑÑ†ĞµĞ½Ğ°Ñ… ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸Ñ‚ÑŒ Ğ²Ğ°Ğ¶Ğ½ÑƒÑ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ. ĞŸÑ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° LSceneLLM, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ LLM Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ½Ğ°Ñˆ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑÑ†ĞµĞ½ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ñ€Ğ¸ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ² ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ 3D-VLMs.'}, 'en': {'title': 'Enhancing 3D Scene Understanding with LSceneLLM', 'desc': 'This paper introduces LSceneLLM, a novel framework designed to enhance 3D Vision-Language Models (3D-VLMs) for better understanding of large 3D scenes. It addresses the challenge of identifying task-relevant visual information amidst the dense features present in these scenes. By utilizing a dense token selector and an adaptive self-attention module, the framework effectively magnifies important details while reducing redundant information. The authors also present a new benchmark, XR-Scene, to evaluate the performance of 3D-VLMs on various large scene understanding tasks, demonstrating that their approach significantly outperforms existing methods.'}, 'zh': {'title': 'è‡ªé€‚åº”3Dè§†è§‰è¯­è¨€æ¨¡å‹ï¼Œæå‡åœºæ™¯ç†è§£èƒ½åŠ›', 'desc': '3Dè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆ3D-VLMsï¼‰çš„ç ”ç©¶è¶Šæ¥è¶Šå—åˆ°å…³æ³¨ï¼Œè¿™å¯¹åœ¨3Dåœºæ™¯ä¸­å‘å±•å…·èº«äººå·¥æ™ºèƒ½è‡³å…³é‡è¦ï¼Œå¦‚è§†è§‰å¯¼èˆªå’Œå…·èº«é—®ç­”ã€‚ç”±äº3Dåœºæ™¯ä¸­è§†è§‰ç‰¹å¾çš„é«˜å¯†åº¦ï¼Œå‡†ç¡®å®šä½ä¸ä»»åŠ¡ç›¸å…³çš„è§†è§‰ä¿¡æ¯å˜å¾—å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ç°æœ‰æ–¹æ³•å°è¯•å¯¹æ‰€æœ‰å¯¹è±¡è¿›è¡Œåˆ†å‰²ï¼Œå¹¶å°†å…¶ç‰¹å¾è§†ä¸ºåœºæ™¯è¡¨ç¤ºï¼Œä½†è¿™äº›ä¸ä»»åŠ¡æ— å…³çš„å¯¹è±¡ç‰¹å¾åŒ…å«å¤§é‡å†—ä½™ä¿¡æ¯å’Œç¼ºå¤±çš„ç»†èŠ‚ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†LSceneLLMï¼Œä¸€ä¸ªè‡ªé€‚åº”æ¡†æ¶ï¼Œé€šè¿‡åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯¹ä¸åŒä»»åŠ¡çš„è§†è§‰åå¥½ï¼Œè‡ªåŠ¨è¯†åˆ«ä¸ä»»åŠ¡ç›¸å…³çš„åŒºåŸŸï¼Œå¹¶é€šè¿‡å¯æ’æ‹”çš„åœºæ™¯æ”¾å¤§æ¨¡å—æ•æ‰èšç„¦åŒºåŸŸçš„ç»†ç²’åº¦ç»†èŠ‚ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.02592', 'title': 'OCR Hinders RAG: Evaluating the Cascading Impact of OCR on Retrieval-Augmented Generation', 'url': 'https://huggingface.co/papers/2412.02592', 'abstract': "Retrieval-augmented Generation (RAG) enhances Large Language Models (LLMs) by integrating external knowledge to reduce hallucinations and incorporate up-to-date information without retraining. As an essential part of RAG, external knowledge bases are commonly built by extracting structured data from unstructured PDF documents using Optical Character Recognition (OCR). However, given the imperfect prediction of OCR and the inherent non-uniform representation of structured data, knowledge bases inevitably contain various OCR noises. In this paper, we introduce OHRBench, the first benchmark for understanding the cascading impact of OCR on RAG systems. OHRBench includes 350 carefully selected unstructured PDF documents from six real-world RAG application domains, along with Q&As derived from multimodal elements in documents, challenging existing OCR solutions used for RAG To better understand OCR's impact on RAG systems, we identify two primary types of OCR noise: Semantic Noise and Formatting Noise and apply perturbation to generate a set of structured data with varying degrees of each OCR noise. Using OHRBench, we first conduct a comprehensive evaluation of current OCR solutions and reveal that none is competent for constructing high-quality knowledge bases for RAG systems. We then systematically evaluate the impact of these two noise types and demonstrate the vulnerability of RAG systems. Furthermore, we discuss the potential of employing Vision-Language Models (VLMs) without OCR in RAG systems. Code: https://github.com/opendatalab/OHR-Bench", 'score': 3, 'issue_id': 937, 'pub_date': '2024-12-03', 'pub_date_card': {'ru': '3 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 3', 'zh': '12æœˆ3æ—¥'}, 'hash': '91dbac114744b1e9', 'authors': ['Junyuan Zhang', 'Qintong Zhang', 'Bin Wang', 'Linke Ouyang', 'Zichen Wen', 'Ying Li', 'Ka-Ho Chow', 'Conghui He', 'Wentao Zhang'], 'affiliations': ['Beihang University', 'Peking University', 'Shanghai AI Laboratory', 'Shanghai Jiaotong University', 'The University of HongKong'], 'pdf_title_img': 'assets/pdf/title_img/2412.02592.jpg', 'data': {'categories': ['#hallucinations', '#dataset', '#multimodal', '#benchmark', '#rag', '#optimization', '#survey'], 'emoji': 'ğŸ”', 'ru': {'title': 'OHRBench: Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°Ñ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ OCR Ğ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ RAG', 'desc': 'OHRBench - ÑÑ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ² (OCR) Ğ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ (RAG). ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 350 Ğ½ĞµÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… PDF-Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸Ğ· ÑˆĞµÑÑ‚Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ RAG, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ²Ñ‹Ğ´ĞµĞ»ÑÑÑ‚ Ğ´Ğ²Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ° ÑˆÑƒĞ¼Ğ° OCR: ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¸ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ½Ñ‹Ğ¹, Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ Ğ²Ğ¾Ğ·Ğ¼ÑƒÑ‰ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ ÑÑ‚ĞµĞ¿ĞµĞ½ÑŒÑ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ñ‚Ğ¸Ğ¿Ğ° ÑˆÑƒĞ¼Ğ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ ÑĞ¸ÑÑ‚ĞµĞ¼ RAG Ğº Ğ¾ÑˆĞ¸Ğ±ĞºĞ°Ğ¼ OCR Ğ¸ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (VLM) Ğ±ĞµĞ· OCR Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… RAG.'}, 'en': {'title': "Enhancing RAG: Understanding OCR's Impact with OHRBench", 'desc': 'This paper presents OHRBench, a benchmark designed to evaluate the effects of Optical Character Recognition (OCR) errors on Retrieval-Augmented Generation (RAG) systems. It identifies two main types of OCR noise: Semantic Noise, which affects the meaning of the extracted data, and Formatting Noise, which impacts the structure and presentation. The study reveals that current OCR solutions are inadequate for creating high-quality knowledge bases necessary for effective RAG applications. Additionally, it explores the potential of using Vision-Language Models (VLMs) as an alternative to traditional OCR methods in RAG systems.'}, 'zh': {'title': 'æ­ç¤ºOCRå¯¹RAGç³»ç»Ÿçš„å½±å“', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†OHRBenchï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç”¨äºç†è§£å…‰å­¦å­—ç¬¦è¯†åˆ«ï¼ˆOCRï¼‰å¯¹æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿå½±å“çš„åŸºå‡†ã€‚ç ”ç©¶å‘ç°ï¼ŒOCRåœ¨å¤„ç†éç»“æ„åŒ–PDFæ–‡æ¡£æ—¶ä¼šå¼•å…¥è¯­ä¹‰å™ªå£°å’Œæ ¼å¼å™ªå£°ï¼Œå¯¼è‡´çŸ¥è¯†åº“è´¨é‡ä¸‹é™ã€‚é€šè¿‡å¯¹350ä¸ªçœŸå®ä¸–ç•Œåº”ç”¨é¢†åŸŸçš„æ–‡æ¡£è¿›è¡Œè¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºç°æœ‰çš„OCRè§£å†³æ–¹æ¡ˆæ— æ³•æœ‰æ•ˆæ„å»ºé«˜è´¨é‡çš„çŸ¥è¯†åº“ã€‚æœ€åï¼Œè®ºæ–‡æ¢è®¨äº†åœ¨RAGç³»ç»Ÿä¸­ä½¿ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰è€Œä¸ä¾èµ–OCRçš„æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.19542', 'title': 'A dynamic parallel method for performance optimization on hybrid CPUs', 'url': 'https://huggingface.co/papers/2411.19542', 'abstract': 'The AIPC concept is gaining popularity, and more and more hybrid CPUs will be running AI models on client devices. However, the current AI inference framework overlooks the imbalanced hardware capability of hybrid CPUs, leading to low inference performance. To address this issue, we have introduced a dynamic parallel method for hybrid CPUs, which significantly increases LLM inference performance by balancing the workload for each core of a hybrid CPU before the parallel work starts. This method has enabled Neural Speed to achieve more than 90% (on average) of memory bandwidth on two hybrid Intel CPUs.', 'score': 3, 'issue_id': 936, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 29', 'zh': '11æœˆ29æ—¥'}, 'hash': '27226211eddf71d4', 'authors': ['Luo Yu', 'Liu Yucheng', 'Shen Haihao'], 'affiliations': ['Intel Corporation'], 'pdf_title_img': 'assets/pdf/title_img/2411.19542.jpg', 'data': {'categories': ['#inference', '#architecture', '#optimization'], 'emoji': 'ğŸš€', 'ru': {'title': 'Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ğ˜Ğ˜ Ğ½Ğ° Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ñ… CPU: Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ´Ğ»Ñ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ñ€Ğ¾Ğ², Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ¸ Ğ´Ğ»Ñ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ğ˜Ğ˜ Ğ½Ğµ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½ĞµÑ€Ğ°Ğ²Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ´ĞµÑ€ Ğ² Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ñ… CPU, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€ÑƒĞµÑ‚ Ğ½Ğ°Ğ³Ñ€ÑƒĞ·ĞºÑƒ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ´Ñ€Ğ°Ğ¼Ğ¸ Ğ¿ĞµÑ€ĞµĞ´ Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ¾Ğ¼ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° LLM. Ğ’ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğµ, Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Neural Speed Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³ Ğ±Ğ¾Ğ»ĞµĞµ 90% Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ½Ğ¾Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ½Ğ° Ğ´Ğ²ÑƒÑ… Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ñ€Ğ°Ñ… Intel.'}, 'en': {'title': 'Boosting AI Inference with Dynamic Workload Balancing on Hybrid CPUs', 'desc': 'The paper discusses the growing trend of using hybrid CPUs for running AI models on client devices. It highlights a problem where existing AI inference frameworks do not effectively utilize the varying hardware capabilities of these hybrid CPUs, resulting in suboptimal performance. To solve this, the authors propose a dynamic parallel method that balances the workload across CPU cores before starting parallel processing. This approach has led to significant improvements in inference performance, achieving over 90% memory bandwidth utilization on hybrid Intel CPUs.'}, 'zh': {'title': 'åŠ¨æ€å¹³è¡¡ï¼Œæå‡AIæ¨ç†æ€§èƒ½ï¼', 'desc': 'AIPCæ¦‚å¿µè¶Šæ¥è¶Šå—æ¬¢è¿ï¼Œè¶Šæ¥è¶Šå¤šçš„æ··åˆCPUå°†åœ¨å®¢æˆ·ç«¯è®¾å¤‡ä¸Šè¿è¡ŒAIæ¨¡å‹ã€‚ç„¶è€Œï¼Œç›®å‰çš„AIæ¨ç†æ¡†æ¶å¿½è§†äº†æ··åˆCPUçš„ä¸å¹³è¡¡ç¡¬ä»¶èƒ½åŠ›ï¼Œå¯¼è‡´æ¨ç†æ€§èƒ½ä½ä¸‹ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŠ¨æ€å¹¶è¡Œæ–¹æ³•ï¼Œæ˜¾è‘—æé«˜äº†æ··åˆCPUçš„LLMæ¨ç†æ€§èƒ½ï¼Œé€šè¿‡åœ¨å¹¶è¡Œå·¥ä½œå¼€å§‹ä¹‹å‰å¹³è¡¡æ¯ä¸ªæ ¸å¿ƒçš„å·¥ä½œè´Ÿè½½ã€‚è¯¥æ–¹æ³•ä½¿Neural Speedåœ¨ä¸¤æ¬¾æ··åˆIntel CPUä¸Šå®ç°äº†è¶…è¿‡90%çš„å†…å­˜å¸¦å®½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.01558', 'title': 'VideoLights: Feature Refinement and Cross-Task Alignment Transformer for Joint Video Highlight Detection and Moment Retrieval', 'url': 'https://huggingface.co/papers/2412.01558', 'abstract': 'Video Highlight Detection and Moment Retrieval (HD/MR) are essential in video analysis. Recent joint prediction transformer models often overlook their cross-task dynamics and video-text alignment and refinement. Moreover, most models typically use limited, uni-directional attention mechanisms, resulting in weakly integrated representations and suboptimal performance in capturing the interdependence between video and text modalities. Although large-language and vision-language models (LLM/LVLMs) have gained prominence across various domains, their application in this field remains relatively underexplored. Here we propose VideoLights, a novel HD/MR framework addressing these limitations through (i) Convolutional Projection and Feature Refinement modules with an alignment loss for better video-text feature alignment, (ii) Bi-Directional Cross-Modal Fusion network for strongly coupled query-aware clip representations, and (iii) Uni-directional joint-task feedback mechanism enhancing both tasks through correlation. In addition, (iv) we introduce hard positive/negative losses for adaptive error penalization and improved learning, and (v) leverage LVLMs like BLIP-2 for enhanced multimodal feature integration and intelligent pretraining using synthetic data generated from LVLMs. Comprehensive experiments on QVHighlights, TVSum, and Charades-STA benchmarks demonstrate state-of-the-art performance. Codes and models are available at https://github.com/dpaul06/VideoLights .', 'score': 1, 'issue_id': 935, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 2', 'zh': '12æœˆ2æ—¥'}, 'hash': '12235c4ebf26fe4a', 'authors': ['Dhiman Paul', 'Md Rizwan Parvez', 'Nabeel Mohammed', 'Shafin Rahman'], 'affiliations': ['Department of Electrical and Computer Engineering, North South University, Dhaka, Bangladesh', 'Qatar Computing Research Institute (QCRI), Qatar'], 'pdf_title_img': 'assets/pdf/title_img/2412.01558.jpg', 'data': {'categories': ['#video', '#games', '#synthetic', '#architecture', '#benchmark', '#multimodal'], 'emoji': 'ğŸ¥', 'ru': {'title': 'VideoLights: ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ²Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ VideoLights - Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¿Ğ¾ Ğ½Ğ¸Ğ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ´Ğ²ÑƒĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğµ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸. ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ VideoLights Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Enhancing Video-Text Integration with VideoLights', 'desc': 'This paper presents VideoLights, a new framework for Video Highlight Detection and Moment Retrieval that improves the integration of video and text data. It introduces several innovative components, including Convolutional Projection and Feature Refinement modules to enhance video-text alignment, and a Bi-Directional Cross-Modal Fusion network for better representation of clips. The framework also employs a Uni-directional joint-task feedback mechanism to strengthen the relationship between the two tasks and introduces hard positive/negative losses for more effective learning. The results show that VideoLights achieves state-of-the-art performance on multiple benchmarks, demonstrating its effectiveness in video analysis.'}, 'zh': {'title': 'VideoLightsï¼šæå‡è§†é¢‘ä¸æ–‡æœ¬åˆ†æçš„å…¨æ–°æ¡†æ¶', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºVideoLightsçš„è§†é¢‘é«˜äº®æ£€æµ‹å’Œæ—¶åˆ»æ£€ç´¢æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ¨¡å‹åœ¨è§†é¢‘ä¸æ–‡æœ¬å¯¹é½å’Œè·¨ä»»åŠ¡åŠ¨æ€æ–¹é¢çš„ä¸è¶³ã€‚æˆ‘ä»¬å¼•å…¥äº†å·ç§¯æŠ•å½±å’Œç‰¹å¾ç²¾ç‚¼æ¨¡å—ï¼Œä»¥æé«˜è§†é¢‘å’Œæ–‡æœ¬ç‰¹å¾çš„å¯¹é½æ•ˆæœï¼Œå¹¶é‡‡ç”¨åŒå‘è·¨æ¨¡æ€èåˆç½‘ç»œæ¥å¢å¼ºæŸ¥è¯¢æ„ŸçŸ¥çš„ç‰‡æ®µè¡¨ç¤ºã€‚é€šè¿‡å•å‘è”åˆä»»åŠ¡åé¦ˆæœºåˆ¶ï¼Œæˆ‘ä»¬èƒ½å¤Ÿæå‡ä¸¤ä¸ªä»»åŠ¡ä¹‹é—´çš„ç›¸å…³æ€§ï¼ŒåŒæ—¶å¼•å…¥ç¡¬æ­£è´ŸæŸå¤±ä»¥æ”¹å–„å­¦ä¹ æ•ˆæœã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVideoLightsåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.18478', 'title': 'Beyond Examples: High-level Automated Reasoning Paradigm in In-Context Learning via MCTS', 'url': 'https://huggingface.co/papers/2411.18478', 'abstract': "In-context Learning (ICL) enables large language models (LLMs) to tackle downstream tasks through sophisticated prompting and high-quality demonstrations. However, this traditional ICL paradigm shows limitations when facing complex mathematical reasoning tasks, primarily due to its heavy dependence on example quality and the necessity for human intervention in challenging scenarios. To address these limitations, this paper presents HiAR-ICL, a High-level Automated Reasoning paradigm in ICL that shifts focus from specific examples to abstract thinking patterns, extending the conventional concept of context in ICL. HiAR-ICL introduces five atomic reasoning actions as fundamental components for constructing chain-structured patterns. Using Monte Carlo Tree Search, we explore reasoning paths and construct thought cards to guide subsequent inference. We then develop a cognitive complexity framework that dynamically matches problems with appropriate thought cards. Experimental results demonstrate HiAR-ICL's effectiveness, achieving state-of-the-art accuracy (79.6%) on the MATH benchmark with Qwen2.5-7B-Instruct, surpassing GPT-4o (76.6%) and Claude 3.5 (71.1%).", 'score': 21, 'issue_id': 890, 'pub_date': '2024-11-27', 'pub_date_card': {'ru': '27 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 27', 'zh': '11æœˆ27æ—¥'}, 'hash': '05890d0739faa85c', 'authors': ['Jinyang Wu', 'Mingkuan Feng', 'Shuai Zhang', 'Feihu Che', 'Zengqi Wen', 'Jianhua Tao'], 'affiliations': ['Department of Automation, Tsinghua University', 'Beijing National Research Center for Information Science and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2411.18478.jpg', 'data': {'categories': ['#training', '#inference', '#reasoning', '#math'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'HiAR-ICL: ĞĞ±ÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': "HiAR-ICL - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ°Ñ… Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿ÑÑ‚ÑŒ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ. ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ÑÑ Ğ¿Ğ¾Ğ¸ÑĞº ĞœĞ¾Ğ½Ñ‚Ğµ-ĞšĞ°Ñ€Ğ»Ğ¾, HiAR-ICL ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ 'ĞºĞ°Ñ€Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ¼Ñ‹ÑĞ»ĞµĞ¹' Ğ´Ğ»Ñ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ° Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ HiAR-ICL Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ MATH, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ GPT-4 Ğ¸ Claude 3.5."}, 'en': {'title': 'Revolutionizing Mathematical Reasoning with HiAR-ICL', 'desc': "This paper introduces HiAR-ICL, a new approach to In-context Learning (ICL) that enhances large language models' ability to perform complex mathematical reasoning tasks. Traditional ICL relies heavily on the quality of examples and often requires human input, which can be limiting. HiAR-ICL shifts the focus from specific examples to abstract reasoning patterns, utilizing five atomic reasoning actions to create structured reasoning chains. The method employs Monte Carlo Tree Search to explore reasoning paths and develop a cognitive complexity framework that matches problems with suitable thought cards, achieving superior performance on the MATH benchmark."}, 'zh': {'title': 'é«˜å±‚æ¬¡è‡ªåŠ¨æ¨ç†ï¼šè¶…è¶Šä¼ ç»Ÿä¸Šä¸‹æ–‡å­¦ä¹ çš„å±€é™æ€§', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„é«˜å±‚æ¬¡è‡ªåŠ¨æ¨ç†èŒƒå¼HiAR-ICLï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿä¸Šä¸‹æ–‡å­¦ä¹ åœ¨å¤æ‚æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­çš„å±€é™æ€§ã€‚HiAR-ICLé€šè¿‡å¼•å…¥äº”ç§åŸºæœ¬æ¨ç†åŠ¨ä½œï¼Œè½¬å˜äº†å¯¹å…·ä½“ç¤ºä¾‹çš„ä¾èµ–ï¼Œå¼ºè°ƒæŠ½è±¡æ€ç»´æ¨¡å¼çš„é‡è¦æ€§ã€‚è¯¥æ–¹æ³•åˆ©ç”¨è’™ç‰¹å¡æ´›æ ‘æœç´¢æ¢ç´¢æ¨ç†è·¯å¾„ï¼Œå¹¶æ„å»ºæ€ç»´å¡ç‰‡ä»¥æŒ‡å¯¼åç»­æ¨ç†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒHiAR-ICLåœ¨MATHåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†79.6%çš„å‡†ç¡®ç‡ï¼Œè¶…è¶Šäº†å…¶ä»–å…ˆè¿›æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.19930', 'title': 'On Domain-Specific Post-Training for Multimodal Large Language Models', 'url': 'https://huggingface.co/papers/2411.19930', 'abstract': 'Recent years have witnessed the rapid development of general multimodal large language models (MLLMs). However, adapting general MLLMs to specific domains, such as scientific fields and industrial applications, remains less explored. This paper systematically investigates domain adaptation of MLLMs through post-training, focusing on data synthesis, training pipelines, and task evaluation. (1) Data Synthesis: Using open-source models, we develop a visual instruction synthesizer that effectively generates diverse visual instruction tasks from domain-specific image-caption pairs. Our synthetic tasks surpass those generated by manual rules, GPT-4, and GPT-4V in enhancing the domain-specific performance of MLLMs. (2) Training Pipeline: While the two-stage training--initially on image-caption pairs followed by visual instruction tasks--is commonly adopted for developing general MLLMs, we apply a single-stage training pipeline to enhance task diversity for domain-specific post-training. (3) Task Evaluation: We conduct experiments in two domains, biomedicine and food, by post-training MLLMs of different sources and scales (e.g., Qwen2-VL-2B, LLaVA-v1.6-8B, Llama-3.2-11B), and then evaluating MLLM performance on various domain-specific tasks. To support further research in MLLM domain adaptation, we will open-source our implementations.', 'score': 18, 'issue_id': 885, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 29', 'zh': '11æœˆ29æ—¥'}, 'hash': '5d14749b38f15e60', 'authors': ['Daixuan Cheng', 'Shaohan Huang', 'Ziyu Zhu', 'Xintong Zhang', 'Wayne Xin Zhao', 'Zhongzhi Luan', 'Bo Dai', 'Zhenliang Zhang'], 'affiliations': ['Beihang University', 'Beijing Institute of Technology', 'Renmin University of China', 'State Key Laboratory of General Artificial Intelligence, BIGAI', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2411.19930.jpg', 'data': {'categories': ['#transfer_learning', '#dataset', '#training', '#open_source', '#multimodal', '#synthetic'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'ĞĞ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜ Ğº ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑĞ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğº ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ğ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ°Ñ‚Ğ¾Ñ€ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ€Ñ‹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ-Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ½Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ»Ğ¸ Ğ¾Ğ´Ğ½Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ñ€Ğ¸ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ğ»Ğ¸ÑÑŒ Ğ² Ğ±Ğ¸Ğ¾Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ¹ Ğ¸ Ğ¿Ğ¸Ñ‰ĞµĞ²Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ… Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… MLLM, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°.'}, 'en': {'title': 'Enhancing MLLMs for Specific Domains through Innovative Adaptation Techniques', 'desc': 'This paper explores how to adapt general multimodal large language models (MLLMs) for specific fields like biomedicine and food. It introduces a method for data synthesis that creates diverse visual instruction tasks from image-caption pairs, outperforming previous methods. The authors propose a single-stage training pipeline to improve task diversity during post-training, rather than the usual two-stage approach. Finally, they evaluate the performance of various MLLMs on domain-specific tasks and plan to share their findings to aid future research in this area.'}, 'zh': {'title': 'æå‡å¤šæ¨¡æ€æ¨¡å‹çš„é¢†åŸŸé€‚åº”æ€§', 'desc': 'è¿‘å¹´æ¥ï¼Œé€šç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰è¿…é€Ÿå‘å±•ã€‚ç„¶è€Œï¼Œå°†é€šç”¨MLLMsé€‚åº”äºç‰¹å®šé¢†åŸŸï¼Œå¦‚ç§‘å­¦å’Œå·¥ä¸šåº”ç”¨ï¼Œä»ç„¶è¾ƒå°‘è¢«æ¢ç´¢ã€‚æœ¬æ–‡ç³»ç»Ÿç ”ç©¶äº†MLLMsçš„é¢†åŸŸé€‚åº”æ€§ï¼Œé‡ç‚¹åœ¨äºæ•°æ®åˆæˆã€è®­ç»ƒæµç¨‹å’Œä»»åŠ¡è¯„ä¼°ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ç§è§†è§‰æŒ‡ä»¤åˆæˆå™¨ï¼Œèƒ½å¤Ÿæœ‰æ•ˆç”Ÿæˆå¤šæ ·åŒ–çš„è§†è§‰æŒ‡ä»¤ä»»åŠ¡ï¼Œä»è€Œæå‡MLLMsåœ¨ç‰¹å®šé¢†åŸŸçš„è¡¨ç°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.19189', 'title': 'Video Depth without Video Models', 'url': 'https://huggingface.co/papers/2411.19189', 'abstract': 'Video depth estimation lifts monocular video clips to 3D by inferring dense depth at every frame. Recent advances in single-image depth estimation, brought about by the rise of large foundation models and the use of synthetic training data, have fueled a renewed interest in video depth. However, naively applying a single-image depth estimator to every frame of a video disregards temporal continuity, which not only leads to flickering but may also break when camera motion causes sudden changes in depth range. An obvious and principled solution would be to build on top of video foundation models, but these come with their own limitations; including expensive training and inference, imperfect 3D consistency, and stitching routines for the fixed-length (short) outputs. We take a step back and demonstrate how to turn a single-image latent diffusion model (LDM) into a state-of-the-art video depth estimator. Our model, which we call RollingDepth, has two main ingredients: (i) a multi-frame depth estimator that is derived from a single-image LDM and maps very short video snippets (typically frame triplets) to depth snippets. (ii) a robust, optimization-based registration algorithm that optimally assembles depth snippets sampled at various different frame rates back into a consistent video. RollingDepth is able to efficiently handle long videos with hundreds of frames and delivers more accurate depth videos than both dedicated video depth estimators and high-performing single-frame models. Project page: rollingdepth.github.io.', 'score': 16, 'issue_id': 889, 'pub_date': '2024-11-28', 'pub_date_card': {'ru': '28 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 28', 'zh': '11æœˆ28æ—¥'}, 'hash': '1fc611a9a44595a1', 'authors': ['Bingxin Ke', 'Dominik Narnhofer', 'Shengyu Huang', 'Lei Ke', 'Torben Peters', 'Katerina Fragkiadaki', 'Anton Obukhov', 'Konrad Schindler'], 'affiliations': ['Carnegie Mellon University', 'ETH Zurich'], 'pdf_title_img': 'assets/pdf/title_img/2411.19189.jpg', 'data': {'categories': ['#3d', '#diffusion', '#video', '#optimization'], 'emoji': 'ğŸ¥', 'ru': {'title': 'RollingDepth: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ LDM', 'desc': 'Ğ”Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ RollingDepth. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ½Ğ° Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LDM) Ğ´Ğ»Ñ Ğ¾Ğ´Ğ¸Ğ½Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´Ğ²Ğ° ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°: Ğ¾Ñ†ĞµĞ½Ñ‰Ğ¸Ğº Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ´Ğ»Ñ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ñ€ĞµĞ³Ğ¸ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑĞ±Ğ¾Ñ€ĞºĞ¸ Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ñ†ĞµĞ»Ğ¾ÑÑ‚Ğ½Ğ¾Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾. RollingDepth ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ°Ğº ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾Ñ†ĞµĞ½Ñ‰Ğ¸ĞºĞ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ²Ğ¸Ğ´ĞµĞ¾, Ñ‚Ğ°Ğº Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ². Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ¸Ğ°Ğ¿Ğ°Ğ·Ğ¾Ğ½Ğ° Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ¿Ñ€Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ğ¼ĞµÑ€Ñ‹.'}, 'en': {'title': 'Transforming Monocular Videos into Accurate 3D Depth with RollingDepth', 'desc': 'This paper presents RollingDepth, a novel approach for estimating depth in videos by leveraging a single-image latent diffusion model (LDM). The method addresses the challenge of temporal continuity in video depth estimation, which often leads to flickering and inconsistencies when using traditional single-image models. RollingDepth utilizes a multi-frame depth estimator to analyze short video snippets and a robust registration algorithm to combine these depth estimates into a coherent video output. The results show that RollingDepth outperforms existing video depth estimators and single-frame models, providing accurate depth information for long video sequences.'}, 'zh': {'title': 'å°†å•å›¾åƒæ·±åº¦ä¼°è®¡æå‡ä¸ºè§†é¢‘æ·±åº¦ä¼°è®¡çš„åˆ›æ–°ä¹‹è·¯', 'desc': 'è§†é¢‘æ·±åº¦ä¼°è®¡é€šè¿‡æ¨æ–­æ¯å¸§çš„å¯†é›†æ·±åº¦ï¼Œå°†å•ç›®è§†é¢‘ç‰‡æ®µæå‡ä¸º3Dã€‚æœ€è¿‘ï¼Œå•å›¾åƒæ·±åº¦ä¼°è®¡çš„è¿›å±•æ¿€å‘äº†å¯¹è§†é¢‘æ·±åº¦çš„å…³æ³¨ï¼Œä½†ç®€å•åœ°å°†å•å›¾åƒæ·±åº¦ä¼°è®¡å™¨åº”ç”¨äºæ¯å¸§ä¼šå¿½ç•¥æ—¶é—´è¿ç»­æ€§ï¼Œå¯¼è‡´é—ªçƒå’Œæ·±åº¦èŒƒå›´çš„çªç„¶å˜åŒ–ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºRollingDepthçš„æ¨¡å‹ï¼Œå®ƒç»“åˆäº†å¤šå¸§æ·±åº¦ä¼°è®¡å’Œä¼˜åŒ–çš„æ³¨å†Œç®—æ³•ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå¤„ç†é•¿è§†é¢‘å¹¶æä¾›æ›´å‡†ç¡®çš„æ·±åº¦è§†é¢‘ã€‚è¯¥æ¨¡å‹åœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†ä¸“ç”¨è§†é¢‘æ·±åº¦ä¼°è®¡å™¨å’Œé«˜æ€§èƒ½å•å¸§æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.19146', 'title': 'Puzzle: Distillation-Based NAS for Inference-Optimized LLMs', 'url': 'https://huggingface.co/papers/2411.19146', 'abstract': "Large language models (LLMs) have demonstrated remarkable capabilities, but their adoption is limited by high computational costs during inference. While increasing parameter counts enhances accuracy, it also widens the gap between state-of-the-art capabilities and practical deployability. We present Puzzle, a framework to accelerate LLM inference on specific hardware while preserving their capabilities. Through an innovative application of neural architecture search (NAS) at an unprecedented scale, Puzzle systematically optimizes models with tens of billions of parameters under hardware constraints. Our approach utilizes blockwise local knowledge distillation (BLD) for parallel architecture exploration and employs mixed-integer programming for precise constraint optimization.   We demonstrate the real-world impact of our framework through Llama-3.1-Nemotron-51B-Instruct (Nemotron-51B), a publicly available model derived from Llama-3.1-70B-Instruct. Nemotron-51B achieves a 2.17x inference throughput speedup, fitting on a single NVIDIA H100 GPU while preserving 98.4% of the original model's capabilities. Nemotron-51B currently stands as the most accurate language model capable of inference on a single GPU with large batch sizes. Remarkably, this transformation required just 45B training tokens, compared to over 15T tokens used for the 70B model it was derived from. This establishes a new paradigm where powerful models can be optimized for efficient deployment with only negligible compromise of their capabilities, demonstrating that inference performance, not parameter count alone, should guide model selection. With the release of Nemotron-51B and the presentation of the Puzzle framework, we provide practitioners immediate access to state-of-the-art language modeling capabilities at significantly reduced computational costs.", 'score': 9, 'issue_id': 886, 'pub_date': '2024-11-28', 'pub_date_card': {'ru': '28 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 28', 'zh': '11æœˆ28æ—¥'}, 'hash': 'b33bb17742a81e99', 'authors': ['Akhiad Bercovich', 'Tomer Ronen', 'Talor Abramovich', 'Nir Ailon', 'Nave Assaf', 'Mohammad Dabbah', 'Ido Galil', 'Amnon Geifman', 'Yonatan Geifman', 'Izhak Golan', 'Netanel Haber', 'Ehud Karpas', 'Itay Levy', 'Shahar Mor', 'Zach Moshe', 'Najeeb Nabwani', 'Omri Puny', 'Ran Rubin', 'Itamar Schen', 'Ido Shahaf', 'Oren Tropp', 'Omer Ullman Argov', 'Ran Zilberstein', 'Ran El-Yaniv'], 'affiliations': ['NVIDIA'], 'pdf_title_img': 'assets/pdf/title_img/2411.19146.jpg', 'data': {'categories': ['#architecture', '#optimization', '#training', '#inference'], 'emoji': 'ğŸ§©', 'ru': {'title': 'Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ LLM Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ˜Ğ˜', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Puzzle Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ½Ğ° ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¼ Ğ¾Ğ±Ğ¾Ñ€ÑƒĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¸Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº (NAS) Ğ¸ Ğ±Ğ»Ğ¾Ñ‡Ğ½ÑƒÑ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ (BLD), Puzzle Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ´ĞµÑÑÑ‚ĞºĞ°Ğ¼Ğ¸ Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¿Ğ¾Ğ´ Ğ°Ğ¿Ğ¿Ğ°Ñ€Ğ°Ñ‚Ğ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ. ĞĞ° Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Nemotron-51B, Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ· Llama-3.1-70B-Instruct, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ 2.17-ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğµ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ 98.4% Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ°, Ğ° Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ´Ğ¾Ğ»Ğ¶Ğ½Ğ° Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑ‚ÑŒ Ğ²Ñ‹Ğ±Ğ¾Ñ€ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Optimizing Large Language Models for Efficient Inference', 'desc': "This paper introduces Puzzle, a framework designed to enhance the inference speed of large language models (LLMs) while maintaining their performance. It employs neural architecture search (NAS) to optimize models with billions of parameters specifically for certain hardware, addressing the challenge of high computational costs. The framework utilizes blockwise local knowledge distillation (BLD) and mixed-integer programming to efficiently explore and optimize model architectures. The resulting model, Nemotron-51B, achieves a significant speedup in inference on a single GPU while retaining most of the original model's capabilities, showcasing a new approach to deploying powerful LLMs more efficiently."}, 'zh': {'title': 'é«˜æ•ˆæ¨ç†ï¼Œå¼ºå¤§æ¨¡å‹çš„æ–°èŒƒå¼', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†é«˜è®¡ç®—æˆæœ¬é™åˆ¶äº†å®ƒä»¬çš„åº”ç”¨ã€‚æˆ‘ä»¬æå‡ºäº†Puzzleæ¡†æ¶ï¼Œé€šè¿‡ç¥ç»æ¶æ„æœç´¢ï¼ˆNASï¼‰åœ¨ç‰¹å®šç¡¬ä»¶ä¸ŠåŠ é€ŸLLMæ¨ç†ï¼ŒåŒæ—¶ä¿æŒå…¶èƒ½åŠ›ã€‚è¯¥æ¡†æ¶åˆ©ç”¨å—çŠ¶å±€éƒ¨çŸ¥è¯†è’¸é¦ï¼ˆBLDï¼‰è¿›è¡Œå¹¶è¡Œæ¶æ„æ¢ç´¢ï¼Œå¹¶é‡‡ç”¨æ··åˆæ•´æ•°è§„åˆ’è¿›è¡Œç²¾ç¡®çº¦æŸä¼˜åŒ–ã€‚é€šè¿‡Nemotron-51Bæ¨¡å‹ï¼Œæˆ‘ä»¬å±•ç¤ºäº†åœ¨å•ä¸ªNVIDIA H100 GPUä¸Šå®ç°2.17å€æ¨ç†ååé‡æå‡çš„å®é™…æ•ˆæœï¼ŒåŒæ—¶ä¿ç•™äº†98.4%çš„åŸå§‹æ¨¡å‹èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.19108', 'title': "Timestep Embedding Tells: It's Time to Cache for Video Diffusion Model", 'url': 'https://huggingface.co/papers/2411.19108', 'abstract': 'As a fundamental backbone for video generation, diffusion models are challenged by low inference speed due to the sequential nature of denoising. Previous methods speed up the models by caching and reusing model outputs at uniformly selected timesteps. However, such a strategy neglects the fact that differences among model outputs are not uniform across timesteps, which hinders selecting the appropriate model outputs to cache, leading to a poor balance between inference efficiency and visual quality. In this study, we introduce Timestep Embedding Aware Cache (TeaCache), a training-free caching approach that estimates and leverages the fluctuating differences among model outputs across timesteps. Rather than directly using the time-consuming model outputs, TeaCache focuses on model inputs, which have a strong correlation with the modeloutputs while incurring negligible computational cost. TeaCache first modulates the noisy inputs using the timestep embeddings to ensure their differences better approximating those of model outputs. TeaCache then introduces a rescaling strategy to refine the estimated differences and utilizes them to indicate output caching. Experiments show that TeaCache achieves up to 4.41x acceleration over Open-Sora-Plan with negligible (-0.07% Vbench score) degradation of visual quality.', 'score': 9, 'issue_id': 886, 'pub_date': '2024-11-28', 'pub_date_card': {'ru': '28 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 28', 'zh': '11æœˆ28æ—¥'}, 'hash': '02a6c2edf156e9d3', 'authors': ['Feng Liu', 'Shiwei Zhang', 'Xiaofeng Wang', 'Yujie Wei', 'Haonan Qiu', 'Yuzhong Zhao', 'Yingya Zhang', 'Qixiang Ye', 'Fang Wan'], 'affiliations': ['Alibaba Group', 'Fudan University', 'Institute of Automation, Chinese Academy of Sciences', 'Nanyang Technological University', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2411.19108.jpg', 'data': {'categories': ['#diffusion', '#optimization', '#video', '#inference'], 'emoji': 'â±ï¸', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ğ¾Ğµ ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ TeaCache. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ°Ñ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ². TeaCache Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ Ğ¾Ñ†ĞµĞ½Ğ¾Ğº Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ğ¹ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ñ… Ğ´Ğ»Ñ ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ TeaCache Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ´Ğ¾ 4,41 Ñ€Ğ°Ğ·Ğ° Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Open-Sora-Plan Ğ¿Ñ€Ğ¸ Ğ½ĞµĞ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'Accelerating Video Generation with Smart Caching', 'desc': 'This paper presents TeaCache, a novel caching method designed to enhance the efficiency of diffusion models in video generation. Traditional approaches cache model outputs at fixed timesteps, which can lead to suboptimal performance due to the uneven differences in outputs across timesteps. TeaCache addresses this by focusing on modulating noisy inputs with timestep embeddings, allowing for a more accurate estimation of output differences without the heavy computational cost. The results demonstrate that TeaCache significantly accelerates inference speed while maintaining high visual quality, achieving a notable performance improvement over existing methods.'}, 'zh': {'title': 'æå‡è§†é¢‘ç”Ÿæˆé€Ÿåº¦çš„æ–°æ–¹æ³•ï¼šTeaCache', 'desc': 'æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„ç¼“å­˜æ–¹æ³•ï¼Œç§°ä¸ºæ—¶é—´æ­¥åµŒå…¥æ„ŸçŸ¥ç¼“å­˜ï¼ˆTeaCacheï¼‰ï¼Œæ—¨åœ¨æé«˜è§†é¢‘ç”Ÿæˆä¸­çš„æ‰©æ•£æ¨¡å‹çš„æ¨ç†é€Ÿåº¦ã€‚ä¼ ç»Ÿæ–¹æ³•é€šè¿‡åœ¨å‡åŒ€é€‰æ‹©çš„æ—¶é—´æ­¥ç¼“å­˜æ¨¡å‹è¾“å‡ºï¼Œä½†å¿½ç•¥äº†ä¸åŒæ—¶é—´æ­¥ä¹‹é—´è¾“å‡ºå·®å¼‚çš„ä¸å‡åŒ€æ€§ã€‚TeaCacheé€šè¿‡è°ƒèŠ‚å™ªå£°è¾“å…¥ï¼Œåˆ©ç”¨æ—¶é—´æ­¥åµŒå…¥æ¥æ›´å¥½åœ°è¿‘ä¼¼æ¨¡å‹è¾“å‡ºçš„å·®å¼‚ï¼Œä»è€Œä¼˜åŒ–ç¼“å­˜é€‰æ‹©ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTeaCacheåœ¨ä¿æŒè§†è§‰è´¨é‡çš„åŒæ—¶ï¼Œæ¨ç†é€Ÿåº¦æé«˜äº†4.41å€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.19324', 'title': 'Trajectory Attention for Fine-grained Video Motion Control', 'url': 'https://huggingface.co/papers/2411.19324', 'abstract': 'Recent advancements in video generation have been greatly driven by video diffusion models, with camera motion control emerging as a crucial challenge in creating view-customized visual content. This paper introduces trajectory attention, a novel approach that performs attention along available pixel trajectories for fine-grained camera motion control. Unlike existing methods that often yield imprecise outputs or neglect temporal correlations, our approach possesses a stronger inductive bias that seamlessly injects trajectory information into the video generation process. Importantly, our approach models trajectory attention as an auxiliary branch alongside traditional temporal attention. This design enables the original temporal attention and the trajectory attention to work in synergy, ensuring both precise motion control and new content generation capability, which is critical when the trajectory is only partially available. Experiments on camera motion control for images and videos demonstrate significant improvements in precision and long-range consistency while maintaining high-quality generation. Furthermore, we show that our approach can be extended to other video motion control tasks, such as first-frame-guided video editing, where it excels in maintaining content consistency over large spatial and temporal ranges.', 'score': 9, 'issue_id': 885, 'pub_date': '2024-11-28', 'pub_date_card': {'ru': '28 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 28', 'zh': '11æœˆ28æ—¥'}, 'hash': '02a266f597ae69e7', 'authors': ['Zeqi Xiao', 'Wenqi Ouyang', 'Yifan Zhou', 'Shuai Yang', 'Lei Yang', 'Jianlou Si', 'Xingang Pan'], 'affiliations': ['S-Lab, Nanyang Technological University', 'Sensetime Research', 'Wangxuan Institute of Computer Technology, Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2411.19324.jpg', 'data': {'categories': ['#diffusion', '#video'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Ğ¢Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ trajectory attention', 'desc': "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ 'trajectory attention' Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ´Ğ¾Ğ»ÑŒ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ²Ğ½ĞµĞ´Ñ€ÑÑ‚ÑŒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. Trajectory attention Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ ĞºĞ°Ğº Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ²ĞµÑ‚Ğ²ÑŒ Ğ½Ğ°Ñ€ÑĞ´Ñƒ Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ ĞºĞ°Ğº Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ, Ñ‚Ğ°Ğº Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ² Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ñ€Ğ°ÑÑÑ‚Ğ¾ÑĞ½Ğ¸ÑÑ… Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸."}, 'en': {'title': 'Enhancing Video Generation with Trajectory Attention', 'desc': 'This paper presents a new method called trajectory attention for improving video generation, particularly in controlling camera motion. It enhances the traditional temporal attention mechanism by incorporating pixel trajectories, allowing for more precise and consistent video outputs. The method effectively combines trajectory information with temporal attention, addressing challenges in generating view-customized content. Experiments show that this approach not only improves motion control but also maintains high-quality video generation across various tasks.'}, 'zh': {'title': 'è½¨è¿¹æ³¨æ„åŠ›ï¼šç²¾ç¡®æ§åˆ¶è§†é¢‘ç”Ÿæˆä¸­çš„ç›¸æœºè¿åŠ¨', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°é¢–çš„è½¨è¿¹æ³¨æ„åŠ›æœºåˆ¶ï¼Œç”¨äºè§†é¢‘ç”Ÿæˆä¸­çš„ç›¸æœºè¿åŠ¨æ§åˆ¶ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿæ›´ç²¾ç¡®åœ°å¤„ç†è¿åŠ¨æ§åˆ¶ï¼Œå¹¶æœ‰æ•ˆåœ°ç»“åˆäº†è½¨è¿¹ä¿¡æ¯ã€‚é€šè¿‡å°†è½¨è¿¹æ³¨æ„åŠ›ä½œä¸ºè¾…åŠ©åˆ†æ”¯ä¸ä¼ ç»Ÿæ—¶é—´æ³¨æ„åŠ›ç»“åˆï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç”Ÿæˆæ–°å†…å®¹çš„åŒæ—¶ï¼Œç¡®ä¿äº†è¿åŠ¨æ§åˆ¶çš„ç²¾ç¡®æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å›¾åƒå’Œè§†é¢‘çš„ç›¸æœºè¿åŠ¨æ§åˆ¶ä¸­æ˜¾è‘—æé«˜äº†ç²¾åº¦å’Œé•¿è·ç¦»ä¸€è‡´æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.18552', 'title': 'FAM Diffusion: Frequency and Attention Modulation for High-Resolution Image Generation with Stable Diffusion', 'url': 'https://huggingface.co/papers/2411.18552', 'abstract': 'Diffusion models are proficient at generating high-quality images. They are however effective only when operating at the resolution used during training. Inference at a scaled resolution leads to repetitive patterns and structural distortions. Retraining at higher resolutions quickly becomes prohibitive. Thus, methods enabling pre-existing diffusion models to operate at flexible test-time resolutions are highly desirable. Previous works suffer from frequent artifacts and often introduce large latency overheads. We propose two simple modules that combine to solve these issues. We introduce a Frequency Modulation (FM) module that leverages the Fourier domain to improve the global structure consistency, and an Attention Modulation (AM) module which improves the consistency of local texture patterns, a problem largely ignored in prior works. Our method, coined Fam diffusion, can seamlessly integrate into any latent diffusion model and requires no additional training. Extensive qualitative results highlight the effectiveness of our method in addressing structural and local artifacts, while quantitative results show state-of-the-art performance. Also, our method avoids redundant inference tricks for improved consistency such as patch-based or progressive generation, leading to negligible latency overheads.', 'score': 8, 'issue_id': 892, 'pub_date': '2024-11-27', 'pub_date_card': {'ru': '27 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 27', 'zh': '11æœˆ27æ—¥'}, 'hash': 'dd1bf99b66f1b34d', 'authors': ['Haosen Yang', 'Adrian Bulat', 'Isma Hadji', 'Hai X. Pham', 'Xiatian Zhu', 'Georgios Tzimiropoulos', 'Brais Martinez'], 'affiliations': ['Queen Mary University, UK', 'Samsung AI Center, Cambridge, UK', 'University of Surrey, UK'], 'pdf_title_img': 'assets/pdf/title_img/2411.18552.jpg', 'data': {'categories': ['#cv', '#optimization', '#diffusion', '#inference'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'Ğ“Ğ¸Ğ±ĞºĞ¾Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Fam diffusion Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ²Ğ° Ğ¼Ğ¾Ğ´ÑƒĞ»Ñ: Frequency Modulation (FM) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Attention Modulation (AM) Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚ÑƒÑ€Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ»ĞµĞ³ĞºĞ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ² Ğ»ÑĞ±ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ¸ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ² ÑƒÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ñ… Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ğ¾Ğ², Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ»ÑƒÑ‡ÑˆÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Flexible Image Generation with Fam Diffusion', 'desc': 'This paper presents a novel approach to enhance diffusion models for image generation, allowing them to work effectively at various resolutions without retraining. The proposed Fam diffusion method introduces two key modules: Frequency Modulation (FM) for improving global structure consistency and Attention Modulation (AM) for refining local texture patterns. These modules address common issues like repetitive patterns and structural distortions that occur when using scaled resolutions. The method integrates seamlessly into existing latent diffusion models, demonstrating state-of-the-art performance with minimal latency overheads and improved image quality.'}, 'zh': {'title': 'çµæ´»åˆ†è¾¨ç‡ä¸‹çš„é«˜è´¨é‡å›¾åƒç”Ÿæˆ', 'desc': 'æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆé«˜è´¨é‡å›¾åƒæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†ä»…åœ¨è®­ç»ƒæ—¶ä½¿ç”¨çš„åˆ†è¾¨ç‡ä¸‹æœ‰æ•ˆã€‚åœ¨ä¸åŒçš„åˆ†è¾¨ç‡ä¸‹æ¨ç†ä¼šå¯¼è‡´é‡å¤æ¨¡å¼å’Œç»“æ„å¤±çœŸã€‚æˆ‘ä»¬æå‡ºäº†ä¸¤ä¸ªç®€å•çš„æ¨¡å—ï¼Œé¢‘ç‡è°ƒåˆ¶ï¼ˆFMï¼‰æ¨¡å—å’Œæ³¨æ„åŠ›è°ƒåˆ¶ï¼ˆAMï¼‰æ¨¡å—ï¼Œæ¥è§£å†³è¿™äº›é—®é¢˜ã€‚æˆ‘ä»¬çš„Famæ‰©æ•£æ–¹æ³•å¯ä»¥æ— ç¼é›†æˆåˆ°ä»»ä½•æ½œåœ¨æ‰©æ•£æ¨¡å‹ä¸­ï¼Œæ— éœ€é¢å¤–è®­ç»ƒï¼Œå¹¶ä¸”åœ¨å¤„ç†ç»“æ„å’Œå±€éƒ¨ä¼ªå½±æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.19527', 'title': 'DisCoRD: Discrete Tokens to Continuous Motion via Rectified Flow Decoding', 'url': 'https://huggingface.co/papers/2411.19527', 'abstract': 'Human motion, inherently continuous and dynamic, presents significant challenges for generative models. Despite their dominance, discrete quantization methods, such as VQ-VAEs, suffer from inherent limitations, including restricted expressiveness and frame-wise noise artifacts. Continuous approaches, while producing smoother and more natural motions, often falter due to high-dimensional complexity and limited training data. To resolve this "discord" between discrete and continuous representations, we introduce DisCoRD: Discrete Tokens to Continuous Motion via Rectified Flow Decoding, a novel method that decodes discrete motion tokens into continuous motion through rectified flow. By employing an iterative refinement process in the continuous space, DisCoRD captures fine-grained dynamics and ensures smoother and more natural motions. Compatible with any discrete-based framework, our method enhances naturalness without compromising faithfulness to the conditioning signals. Extensive evaluations demonstrate that DisCoRD achieves state-of-the-art performance, with FID of 0.032 on HumanML3D and 0.169 on KIT-ML. These results solidify DisCoRD as a robust solution for bridging the divide between discrete efficiency and continuous realism. Our project page is available at: https://whwjdqls.github.io/discord.github.io/.', 'score': 8, 'issue_id': 891, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 29', 'zh': '11æœˆ29æ—¥'}, 'hash': 'b1fc0d8f7ba13620', 'authors': ['Jungbin Cho', 'Junwan Kim', 'Jisoo Kim', 'Minseo Kim', 'Mingu Kang', 'Sungeun Hong', 'Tae-Hyun Oh', 'Youngjae Yu'], 'affiliations': ['POSTECH', 'Sungkyunkwan University', 'Yonsei University'], 'pdf_title_img': 'assets/pdf/title_img/2411.19527.jpg', 'data': {'categories': ['#video', '#dataset', '#training'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'DisCoRD: ĞœĞ¾ÑÑ‚ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğ¼ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ¼Ğ¾Ğ¼ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ DisCoRD Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ¸ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ¸ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğµ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ñ‹Ğ¿Ñ€ÑĞ¼Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ñ‚Ğ¾Ğº. DisCoRD Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ğµ Ğ² Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ´Ğ»Ñ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° Ñ‚Ğ¾Ğ½ĞºĞ¸Ñ… Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸Ğº Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞµ FID Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ… HumanML3D Ğ¸ KIT-ML.'}, 'en': {'title': 'Bridging the Gap: DisCoRD for Smooth Human Motion Generation', 'desc': 'This paper addresses the challenges of generating human motion using machine learning models, particularly the limitations of discrete quantization methods like VQ-VAEs. It introduces a new method called DisCoRD, which stands for Discrete Tokens to Continuous Motion via Rectified Flow Decoding. DisCoRD effectively converts discrete motion tokens into smooth continuous motion by using an iterative refinement process in the continuous space. The results show that DisCoRD outperforms existing methods, achieving state-of-the-art performance metrics on benchmark datasets, thus providing a solution that balances discrete efficiency with continuous realism.'}, 'zh': {'title': 'æ‰“ç ´ç¦»æ•£ä¸è¿ç»­çš„ç•Œé™ï¼Œæå‡è¿åŠ¨ç”Ÿæˆè‡ªç„¶æ€§', 'desc': 'äººç±»è¿åŠ¨æ˜¯è¿ç»­å’ŒåŠ¨æ€çš„ï¼Œè¿™ç»™ç”Ÿæˆæ¨¡å‹å¸¦æ¥äº†å¾ˆå¤§æŒ‘æˆ˜ã€‚å°½ç®¡ç¦»æ•£é‡åŒ–æ–¹æ³•ï¼ˆå¦‚VQ-VAEsï¼‰å ä¸»å¯¼åœ°ä½ï¼Œä½†å®ƒä»¬åœ¨è¡¨è¾¾èƒ½åŠ›å’Œå¸§å™ªå£°æ–¹é¢å­˜åœ¨å±€é™ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•DisCoRDï¼Œé€šè¿‡ä¿®æ­£æµè§£ç å°†ç¦»æ•£è¿åŠ¨æ ‡è®°è§£ç ä¸ºè¿ç»­è¿åŠ¨ï¼Œè§£å†³äº†ç¦»æ•£å’Œè¿ç»­è¡¨ç¤ºä¹‹é—´çš„çŸ›ç›¾ã€‚DisCoRDåœ¨è¿ç»­ç©ºé—´ä¸­è¿›è¡Œè¿­ä»£ä¼˜åŒ–ï¼Œæ•æ‰ç»†å¾®åŠ¨æ€ï¼Œç¡®ä¿è¿åŠ¨æ›´åŠ å¹³æ»‘è‡ªç„¶ï¼Œä¸”ä¸ä»»ä½•åŸºäºç¦»æ•£çš„æ¡†æ¶å…¼å®¹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.19950', 'title': 'AlphaTablets: A Generic Plane Representation for 3D Planar Reconstruction from Monocular Videos', 'url': 'https://huggingface.co/papers/2411.19950', 'abstract': 'We introduce AlphaTablets, a novel and generic representation of 3D planes that features continuous 3D surface and precise boundary delineation. By representing 3D planes as rectangles with alpha channels, AlphaTablets combine the advantages of current 2D and 3D plane representations, enabling accurate, consistent and flexible modeling of 3D planes. We derive differentiable rasterization on top of AlphaTablets to efficiently render 3D planes into images, and propose a novel bottom-up pipeline for 3D planar reconstruction from monocular videos. Starting with 2D superpixels and geometric cues from pre-trained models, we initialize 3D planes as AlphaTablets and optimize them via differentiable rendering. An effective merging scheme is introduced to facilitate the growth and refinement of AlphaTablets. Through iterative optimization and merging, we reconstruct complete and accurate 3D planes with solid surfaces and clear boundaries. Extensive experiments on the ScanNet dataset demonstrate state-of-the-art performance in 3D planar reconstruction, underscoring the great potential of AlphaTablets as a generic 3D plane representation for various applications. Project page is available at: https://hyzcluster.github.io/alphatablets', 'score': 5, 'issue_id': 888, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 29', 'zh': '11æœˆ29æ—¥'}, 'hash': '9f7d2daec9cb311d', 'authors': ['Yuze He', 'Wang Zhao', 'Shaohui Liu', 'Yubin Hu', 'Yushi Bai', 'Yu-Hui Wen', 'Yong-Jin Liu'], 'affiliations': ['Beijing Jiaotong University', 'ETH Zurich', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2411.19950.jpg', 'data': {'categories': ['#3d'], 'emoji': 'ğŸ“', 'ru': {'title': 'AlphaTablets: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ 3D Ğ¿Ğ»Ğ¾ÑĞºĞ¾ÑÑ‚ĞµĞ¹', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ AlphaTablets - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ¿Ğ»Ğ¾ÑĞºĞ¾ÑÑ‚ĞµĞ¹ Ğ² Ğ²Ğ¸Ğ´Ğµ Ğ¿Ñ€ÑĞ¼Ğ¾ÑƒĞ³Ğ¾Ğ»ÑŒĞ½Ğ¸ĞºĞ¾Ğ² Ñ Ğ°Ğ»ÑŒÑ„Ğ°-ĞºĞ°Ğ½Ğ°Ğ»Ğ°Ğ¼Ğ¸. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ñ‡ĞµÑ‚Ğ°Ñ‚ÑŒ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… 2D Ğ¸ 3D Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ¸ Ğ³Ğ¸Ğ±ĞºĞ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ 3D Ğ¿Ğ»Ğ¾ÑĞºĞ¾ÑÑ‚ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ñ€ÑƒĞµĞ¼ÑƒÑ Ñ€Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ 3D Ğ¿Ğ»Ğ¾ÑĞºĞ¾ÑÑ‚ĞµĞ¹ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ 3D Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ»Ğ¾ÑĞºĞ¾ÑÑ‚ĞµĞ¹ Ğ¸Ğ· Ğ¼Ğ¾Ğ½Ğ¾ĞºÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ScanNet Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ 3D Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ»Ğ¾ÑĞºĞ¾ÑÑ‚ĞµĞ¹.'}, 'en': {'title': 'AlphaTablets: Revolutionizing 3D Plane Representation', 'desc': 'AlphaTablets is a new way to represent 3D planes that combines the benefits of both 2D and 3D models. It uses rectangles with alpha channels to create smooth surfaces and clear edges for accurate modeling. The paper introduces a method for rendering these planes efficiently and a pipeline for reconstructing 3D planes from single videos. By optimizing the representation through merging and iterative processes, AlphaTablets achieves high-quality 3D reconstructions, as shown in tests on the ScanNet dataset.'}, 'zh': {'title': 'AlphaTabletsï¼š3Då¹³é¢é‡å»ºçš„æ–°æ–¹æ³•', 'desc': 'æˆ‘ä»¬ä»‹ç»äº†AlphaTabletsï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–ä¸”é€šç”¨çš„3Då¹³é¢è¡¨ç¤ºæ–¹æ³•ï¼Œå…·æœ‰è¿ç»­çš„3Dè¡¨é¢å’Œç²¾ç¡®çš„è¾¹ç•Œåˆ’åˆ†ã€‚é€šè¿‡å°†3Då¹³é¢è¡¨ç¤ºä¸ºå¸¦æœ‰alphaé€šé“çš„çŸ©å½¢ï¼ŒAlphaTabletsç»“åˆäº†å½“å‰2Då’Œ3Då¹³é¢è¡¨ç¤ºçš„ä¼˜ç‚¹ï¼Œå®ç°äº†3Då¹³é¢çš„å‡†ç¡®ã€ä¸€è‡´å’Œçµæ´»å»ºæ¨¡ã€‚æˆ‘ä»¬åœ¨AlphaTabletsçš„åŸºç¡€ä¸Šæ¨å¯¼å‡ºå¯å¾®åˆ†å…‰æ …åŒ–æŠ€æœ¯ï¼Œä»¥é«˜æ•ˆåœ°å°†3Då¹³é¢æ¸²æŸ“ä¸ºå›¾åƒï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°é¢–çš„è‡ªä¸‹è€Œä¸Šçš„å•ç›®è§†é¢‘3Då¹³é¢é‡å»ºç®¡é“ã€‚é€šè¿‡è¿­ä»£ä¼˜åŒ–å’Œåˆå¹¶ï¼Œæˆ‘ä»¬èƒ½å¤Ÿé‡å»ºå‡ºå®Œæ•´ä¸”å‡†ç¡®çš„3Då¹³é¢ï¼Œå…·æœ‰åšå®çš„è¡¨é¢å’Œæ¸…æ™°çš„è¾¹ç•Œã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.19460', 'title': 'Look Every Frame All at Once: Video-Ma$^2$mba for Efficient Long-form Video Understanding with Multi-Axis Gradient Checkpointing', 'url': 'https://huggingface.co/papers/2411.19460', 'abstract': 'With the growing scale and complexity of video data, efficiently processing long video sequences poses significant challenges due to the quadratic increase in memory and computational demands associated with existing transformer-based Large Multi-modal Models (LMMs). To address these issues, we introduce Video-Ma^2mba, a novel architecture that incorporates State Space Models (SSMs) within the Mamba-2 framework, replacing the attention mechanisms. This allows the LMMs to scale linearly in terms of time and memory requirements, making it feasible to handle long-duration video content. Furthermore, we enhance the memory efficiency introducing the Multi-Axis Gradient Checkpointing (MA-GC) method, which strategically manages memory by retaining only essential activations across multiple computational axes. Our approach significantly reduces the memory footprint compared to standard gradient checkpointing. Empirical analyses show that Video-Ma^2mba can process extensive video sequences-equivalent to millions of tokens or over two hours of continuous sequences at 1 FPS-on a single GPU. By maintaining a detailed capture of temporal dynamics, our model improves the accuracy and relevance of responses in long video understanding tasks, demonstrating substantial advantages over existing frameworks.', 'score': 5, 'issue_id': 886, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 29', 'zh': '11æœˆ29æ—¥'}, 'hash': 'b96751a3db484750', 'authors': ['Hosu Lee', 'Junho Kim', 'Hyunjun Kim', 'Yong Man Ro'], 'affiliations': ['Integrated Vision and Language Lab, KAIST, South Korea'], 'pdf_title_img': 'assets/pdf/title_img/2411.19460.jpg', 'data': {'categories': ['#architecture', '#long_context', '#video', '#optimization'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Video-Ma^2mba', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Video-Ma^2mba - Ğ½Ğ¾Ğ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LMM) Ğ¿Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¸ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¾ÑĞµĞ²Ğ¾Ğ³Ğ¾ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ‡ĞµĞºĞ¿Ğ¾Ğ¹Ğ½Ñ‚Ğ¸Ğ½Ğ³Ğ° (MA-GC) Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Video-Ma^2mba Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¼ GPU, ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'Revolutionizing Long Video Processing with Linear Scalability', 'desc': 'The paper presents Video-Ma^2mba, a new architecture designed to efficiently process long video sequences by integrating State Space Models (SSMs) into the Mamba-2 framework. This innovative approach replaces traditional attention mechanisms, allowing the model to scale linearly in memory and computational requirements, which is crucial for handling extensive video data. Additionally, the introduction of Multi-Axis Gradient Checkpointing (MA-GC) optimizes memory usage by retaining only necessary activations, significantly reducing the memory footprint. Empirical results indicate that Video-Ma^2mba can effectively manage video sequences equivalent to millions of tokens, enhancing the accuracy of long video understanding tasks compared to existing models.'}, 'zh': {'title': 'é«˜æ•ˆå¤„ç†é•¿è§†é¢‘åºåˆ—çš„æ–°æ–¹æ³•', 'desc': 'éšç€è§†é¢‘æ•°æ®è§„æ¨¡å’Œå¤æ‚æ€§çš„å¢åŠ ï¼Œå¤„ç†é•¿è§†é¢‘åºåˆ—é¢ä¸´ç€æ˜¾è‘—çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ¶æ„Video-Ma^2mbaï¼Œå®ƒåœ¨Mamba-2æ¡†æ¶ä¸­å¼•å…¥äº†çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMsï¼‰ï¼Œæ›¿ä»£äº†ä¼ ç»Ÿçš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œä»è€Œä½¿å¾—å¤§è§„æ¨¡å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨æ—¶é—´å’Œå†…å­˜éœ€æ±‚ä¸Šå®ç°çº¿æ€§æ‰©å±•ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†å¤šè½´æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆMA-GCï¼‰æ–¹æ³•ï¼Œä¼˜åŒ–å†…å­˜ç®¡ç†ï¼Œä»…ä¿ç•™å¿…è¦çš„æ¿€æ´»ä¿¡æ¯ï¼Œæ˜¾è‘—é™ä½äº†å†…å­˜å ç”¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVideo-Ma^2mbaèƒ½å¤Ÿåœ¨å•ä¸ªGPUä¸Šå¤„ç†ç›¸å½“äºæ•°ç™¾ä¸‡ä¸ªæ ‡è®°æˆ–è¶…è¿‡ä¸¤å°æ—¶çš„è¿ç»­è§†é¢‘åºåˆ—ï¼Œæå‡äº†é•¿è§†é¢‘ç†è§£ä»»åŠ¡çš„å‡†ç¡®æ€§å’Œç›¸å…³æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.19865', 'title': 'Reverse Thinking Makes LLMs Stronger Reasoners', 'url': 'https://huggingface.co/papers/2411.19865', 'abstract': "Reverse thinking plays a crucial role in human reasoning. Humans can reason not only from a problem to a solution but also in reverse, i.e., start from the solution and reason towards the problem. This often enhances overall reasoning performance as it enables consistency checks between their forward and backward thinking. To enable Large Language Models (LLMs) to perform reverse thinking, we introduce Reverse-Enhanced Thinking (RevThink), a framework composed of data augmentation and learning objectives. In RevThink, we augment the dataset by collecting structured forward-backward reasoning from a teacher model, consisting of: (1) the original question, (2) forward reasoning, (3) backward question, and (4) backward reasoning. We then employ three objectives to train a smaller student model in a multi-task learning fashion: (a) generate forward reasoning from a question, (b) generate a backward question from a question, and (c) generate backward reasoning from the backward question. Experiments across 12 datasets covering commonsense, math, and logical reasoning show an average 13.53% improvement over the student model's zero-shot performance and a 6.84% improvement over the strongest knowledge distillation baselines. Moreover, our method demonstrates sample efficiency -- using only 10% of the correct forward reasoning from the training data, it outperforms a standard fine-tuning method trained on 10x more forward reasoning. RevThink also exhibits strong generalization to out-of-distribution held-out datasets.", 'score': 4, 'issue_id': 899, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 29', 'zh': '11æœˆ29æ—¥'}, 'hash': '8f066f57ddff0ae8', 'authors': ['Justin Chih-Yao Chen', 'Zifeng Wang', 'Hamid Palangi', 'Rujun Han', 'Sayna Ebrahimi', 'Long Le', 'Vincent Perot', 'Swaroop Mishra', 'Mohit Bansal', 'Chen-Yu Lee', 'Tomas Pfister'], 'affiliations': ['Google Cloud AI Research', 'Google DeepMind', 'UNC Chapel Hill'], 'pdf_title_img': 'assets/pdf/title_img/2411.19865.jpg', 'data': {'categories': ['#data', '#training', '#transfer_learning', '#small_models', '#dataset', '#reasoning'], 'emoji': 'ğŸ”„', 'ru': {'title': 'RevThink: Ğ£ÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ LLM Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ñ‹Ğ¼ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Reverse-Enhanced Thinking (RevThink) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). RevThink Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ, Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ÑÑ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€ÑĞ¼Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¾Ñ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸-ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ: Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€ÑĞ¼Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Empowering LLMs with Reverse Reasoning for Enhanced Performance', 'desc': 'This paper introduces Reverse-Enhanced Thinking (RevThink), a framework designed to improve Large Language Models (LLMs) by enabling them to perform reverse reasoning. RevThink enhances reasoning performance by augmenting datasets with structured forward and backward reasoning examples, allowing models to learn from both directions. The framework employs multi-task learning objectives to train a smaller student model, focusing on generating forward reasoning, backward questions, and backward reasoning. Experimental results show significant improvements in reasoning tasks, demonstrating the effectiveness and efficiency of RevThink in enhancing model performance with limited data.'}, 'zh': {'title': 'åå‘æ€ç»´ï¼šæå‡æ¨ç†èƒ½åŠ›çš„æ–°æ–¹æ³•', 'desc': 'åå‘æ€ç»´åœ¨äººçš„æ¨ç†ä¸­èµ·ç€é‡è¦ä½œç”¨ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºåå‘å¢å¼ºæ€ç»´ï¼ˆRevThinkï¼‰çš„æ¡†æ¶ï¼Œæ—¨åœ¨ä½¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰èƒ½å¤Ÿè¿›è¡Œåå‘æ¨ç†ã€‚è¯¥æ¡†æ¶é€šè¿‡æ•°æ®å¢å¼ºå’Œå­¦ä¹ ç›®æ ‡æ¥å®ç°ï¼Œæ”¶é›†ç»“æ„åŒ–çš„å‰å‘å’Œåå‘æ¨ç†æ•°æ®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRevThinkåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„æ¨ç†æ€§èƒ½ï¼Œå¹¶å±•ç¤ºäº†è‰¯å¥½çš„æ ·æœ¬æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.19638', 'title': 'LLM Teacher-Student Framework for Text Classification With No Manually Annotated Data: A Case Study in IPTC News Topic Classification', 'url': 'https://huggingface.co/papers/2411.19638', 'abstract': "With the ever-increasing number of news stories available online, classifying them by topic, regardless of the language they are written in, has become crucial for enhancing readers' access to relevant content. To address this challenge, we propose a teacher-student framework based on large language models (LLMs) for developing multilingual news classification models of reasonable size with no need for manual data annotation. The framework employs a Generative Pretrained Transformer (GPT) model as the teacher model to develop an IPTC Media Topic training dataset through automatic annotation of news articles in Slovenian, Croatian, Greek, and Catalan. The teacher model exhibits a high zero-shot performance on all four languages. Its agreement with human annotators is comparable to that between the human annotators themselves. To mitigate the computational limitations associated with the requirement of processing millions of texts daily, smaller BERT-like student models are fine-tuned on the GPT-annotated dataset. These student models achieve high performance comparable to the teacher model. Furthermore, we explore the impact of the training data size on the performance of the student models and investigate their monolingual, multilingual and zero-shot cross-lingual capabilities. The findings indicate that student models can achieve high performance with a relatively small number of training instances, and demonstrate strong zero-shot cross-lingual abilities. Finally, we publish the best-performing news topic classifier, enabling multilingual classification with the top-level categories of the IPTC Media Topic schema.", 'score': 4, 'issue_id': 889, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 29', 'zh': '11æœˆ29æ—¥'}, 'hash': '98bf5f113194343b', 'authors': ['Taja Kuzman', 'Nikola LjubeÅ¡iÄ‡'], 'affiliations': ['Department of Knowledge Technologies, JoÅ¾ef Stefan Institute, 1000 Ljubljana, Slovenia', 'JoÅ¾ef Stefan International Postgraduate School, 1000 Ljubljana, Slovenia', 'University of Ljubljana, 1000 Ljubljana, Slovenia'], 'pdf_title_img': 'assets/pdf/title_img/2411.19638.jpg', 'data': {'categories': ['#machine_translation', '#training', '#low_resource', '#multilingual', '#dataset'], 'emoji': 'ğŸ“°', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ°Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚ĞµĞ¹ Ğ±ĞµĞ· Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸', 'desc': "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ¹ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚ĞµĞ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ 'ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒ-ÑƒÑ‡ĞµĞ½Ğ¸Ğº'. Ğ‘Ğ¾Ğ»ÑŒÑˆĞ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ GPT Ğ²Ñ‹ÑÑ‚ÑƒĞ¿Ğ°ĞµÑ‚ Ğ² Ñ€Ğ¾Ğ»Ğ¸ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ, Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€ÑƒÑ Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚Ğ½Ñ‹Ğµ ÑÑ‚Ğ°Ñ‚ÑŒĞ¸ Ğ½Ğ° Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ… ÑĞ·Ñ‹ĞºĞ°Ñ…. ĞœĞ¾Ğ´ĞµĞ»Ğ¸-ÑƒÑ‡ĞµĞ½Ğ¸ĞºĞ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ BERT Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ÑÑ Ğ½Ğ° ÑÑ‚Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ¹ Ğ¸ ĞºÑ€Ğ¾ÑÑ-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚ĞµĞ¹."}, 'en': {'title': 'Empowering Multilingual News Classification with Teacher-Student LLMs', 'desc': 'This paper presents a teacher-student framework utilizing large language models (LLMs) for multilingual news classification without manual data annotation. The teacher model, a Generative Pretrained Transformer (GPT), automatically annotates news articles in multiple languages, achieving high zero-shot performance and agreement with human annotators. Smaller BERT-like student models are then fine-tuned on this annotated dataset, demonstrating comparable performance to the teacher model while being computationally efficient. The study also highlights the effectiveness of the student models in multilingual and zero-shot cross-lingual tasks, ultimately providing a robust news topic classifier for diverse languages.'}, 'zh': {'title': 'å¤šè¯­è¨€æ–°é—»åˆ†ç±»çš„æ–°æ–¹æ³•', 'desc': 'éšç€åœ¨çº¿æ–°é—»æ•°é‡çš„ä¸æ–­å¢åŠ ï¼ŒæŒ‰ä¸»é¢˜å¯¹æ–°é—»è¿›è¡Œåˆ†ç±»å˜å¾—è‡³å…³é‡è¦ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ•™å¸ˆ-å­¦ç”Ÿæ¡†æ¶ï¼Œç”¨äºå¼€å‘å¤šè¯­è¨€æ–°é—»åˆ†ç±»æ¨¡å‹ï¼Œä¸”æ— éœ€æ‰‹åŠ¨æ•°æ®æ ‡æ³¨ã€‚æ•™å¸ˆæ¨¡å‹ä½¿ç”¨ç”Ÿæˆé¢„è®­ç»ƒå˜æ¢å™¨ï¼ˆGPTï¼‰è‡ªåŠ¨æ ‡æ³¨æ–°é—»æ–‡ç« ï¼Œå±•ç¤ºå‡ºåœ¨å¤šç§è¯­è¨€ä¸Šçš„é«˜é›¶æ ·æœ¬æ€§èƒ½ã€‚é€šè¿‡å¾®è°ƒè¾ƒå°çš„BERTç±»å­¦ç”Ÿæ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹åœ¨ç›¸å¯¹è¾ƒå°‘çš„è®­ç»ƒå®ä¾‹ä¸‹ä¹Ÿèƒ½è¾¾åˆ°ä¸æ•™å¸ˆæ¨¡å‹ç›¸å½“çš„é«˜æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.18673', 'title': 'AC3D: Analyzing and Improving 3D Camera Control in Video Diffusion Transformers', 'url': 'https://huggingface.co/papers/2411.18673', 'abstract': 'Numerous works have recently integrated 3D camera control into foundational text-to-video models, but the resulting camera control is often imprecise, and video generation quality suffers. In this work, we analyze camera motion from a first principles perspective, uncovering insights that enable precise 3D camera manipulation without compromising synthesis quality. First, we determine that motion induced by camera movements in videos is low-frequency in nature. This motivates us to adjust train and test pose conditioning schedules, accelerating training convergence while improving visual and motion quality. Then, by probing the representations of an unconditional video diffusion transformer, we observe that they implicitly perform camera pose estimation under the hood, and only a sub-portion of their layers contain the camera information. This suggested us to limit the injection of camera conditioning to a subset of the architecture to prevent interference with other video features, leading to 4x reduction of training parameters, improved training speed and 10% higher visual quality. Finally, we complement the typical dataset for camera control learning with a curated dataset of 20K diverse dynamic videos with stationary cameras. This helps the model disambiguate the difference between camera and scene motion, and improves the dynamics of generated pose-conditioned videos. We compound these findings to design the Advanced 3D Camera Control (AC3D) architecture, the new state-of-the-art model for generative video modeling with camera control.', 'score': 4, 'issue_id': 886, 'pub_date': '2024-11-27', 'pub_date_card': {'ru': '27 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 27', 'zh': '11æœˆ27æ—¥'}, 'hash': '1ea35d3552a278a3', 'authors': ['Sherwin Bahmani', 'Ivan Skorokhodov', 'Guocheng Qian', 'Aliaksandr Siarohin', 'Willi Menapace', 'Andrea Tagliasacchi', 'David B. Lindell', 'Sergey Tulyakov'], 'affiliations': ['SFU', 'Snap Inc.', 'University of Toronto', 'Vector Institute'], 'pdf_title_img': 'assets/pdf/title_img/2411.18673.jpg', 'data': {'categories': ['#dataset', '#architecture', '#diffusion', '#games', '#3d', '#training', '#optimization', '#video'], 'emoji': 'ğŸ¥', 'ru': {'title': 'ĞŸÑ€ĞµÑ†Ğ¸Ğ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ 3D-ĞºĞ°Ğ¼ĞµÑ€Ğ¾Ğ¹ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ 3D-ĞºĞ°Ğ¼ĞµÑ€Ğ¾Ğ¹ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ñ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¾Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ñ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğ¹ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ² Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ»Ğ¾Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ¼ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Advanced 3D Camera Control (AC3D), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ĞºĞ°Ğ¼ĞµÑ€Ğ¾Ğ¹ Ğ±ĞµĞ· ÑƒÑ‰ĞµÑ€Ğ±Ğ° Ğ´Ğ»Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾. ĞœĞ¾Ğ´ĞµĞ»ÑŒ AC3D Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ ĞºĞ°Ğ¼ĞµÑ€Ğ¾Ğ¹.'}, 'en': {'title': 'Precision in 3D Camera Control for Enhanced Video Generation', 'desc': 'This paper presents a novel approach to improve 3D camera control in text-to-video models, addressing issues of imprecision and video quality. By analyzing camera motion, the authors discover that it is primarily low-frequency, which leads to adjustments in training and testing schedules that enhance convergence and visual fidelity. They also find that only certain layers of a video diffusion transformer contain relevant camera information, allowing for a more efficient architecture that reduces training parameters while boosting quality. Finally, the introduction of a curated dataset of dynamic videos aids the model in distinguishing between camera and scene motion, culminating in the development of the Advanced 3D Camera Control (AC3D) model, which sets a new standard in generative video modeling.'}, 'zh': {'title': 'å…ˆè¿›çš„3Dç›¸æœºæ§åˆ¶ï¼Œæå‡è§†é¢‘ç”Ÿæˆè´¨é‡', 'desc': 'æœ¬ç ”ç©¶åˆ†æäº†3Dç›¸æœºæ§åˆ¶åœ¨æ–‡æœ¬åˆ°è§†é¢‘æ¨¡å‹ä¸­çš„åº”ç”¨ï¼Œå‘ç°ç›¸æœºè¿åŠ¨å¯¹è§†é¢‘ç”Ÿæˆè´¨é‡æœ‰æ˜¾è‘—å½±å“ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„è®­ç»ƒå’Œæµ‹è¯•å§¿æ€è°ƒèŠ‚ç­–ç•¥ï¼Œä»¥æé«˜è®­ç»ƒæ”¶æ•›é€Ÿåº¦å’Œè§†é¢‘çš„è§†è§‰è´¨é‡ã€‚é€šè¿‡å¯¹æ— æ¡ä»¶è§†é¢‘æ‰©æ•£å˜æ¢å™¨çš„è¡¨ç¤ºè¿›è¡Œæ¢æµ‹ï¼Œæˆ‘ä»¬å‘ç°ç›¸æœºå§¿æ€ä¼°è®¡åœ¨æ¨¡å‹å†…éƒ¨éšå¼æ‰§è¡Œï¼Œå› æ­¤æˆ‘ä»¬é™åˆ¶äº†ç›¸æœºæ¡ä»¶çš„æ³¨å…¥ï¼Œä»¥å‡å°‘å¯¹å…¶ä»–è§†é¢‘ç‰¹å¾çš„å¹²æ‰°ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬è®¾è®¡äº†å…ˆè¿›çš„3Dç›¸æœºæ§åˆ¶æ¶æ„ï¼ˆAC3Dï¼‰ï¼Œæˆä¸ºå…·æœ‰ç›¸æœºæ§åˆ¶çš„ç”Ÿæˆè§†é¢‘å»ºæ¨¡çš„æ–°ä¸€ä»£æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.19842', 'title': 'Scaling Transformers for Low-Bitrate High-Quality Speech Coding', 'url': 'https://huggingface.co/papers/2411.19842', 'abstract': 'The tokenization of speech with neural audio codec models is a vital part of modern AI pipelines for the generation or understanding of speech, alone or in a multimodal context. Traditionally such tokenization models have concentrated on low parameter-count architectures using only components with strong inductive biases. In this work we show that by scaling a transformer architecture with large parameter count to this problem, and applying a flexible Finite Scalar Quantization (FSQ) based bottleneck, it is possible to reach state-of-the-art speech quality at extremely low bit-rates of 400 or 700 bits-per-second. The trained models strongly out-perform existing baselines in both objective and subjective tests.', 'score': 3, 'issue_id': 900, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 29', 'zh': '11æœˆ29æ—¥'}, 'hash': '23e49aedef71b878', 'authors': ['Julian D Parker', 'Anton Smirnov', 'Jordi Pons', 'CJ Carr', 'Zack Zukowski', 'Zach Evans', 'Xubo Liu'], 'affiliations': ['Stability AI'], 'pdf_title_img': 'assets/pdf/title_img/2411.19842.jpg', 'data': {'categories': ['#multimodal', '#architecture', '#audio', '#training'], 'emoji': 'ğŸ—£ï¸', 'ru': {'title': 'Ğ¢Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ñ‹ Ğ¿Ğ¾ĞºĞ¾Ñ€ÑÑÑ‚ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸', 'desc': 'Ğ”Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµÑ‡Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ°ÑƒĞ´Ğ¸Ğ¾ĞºĞ¾Ğ´ĞµĞºĞ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ³Ğ¸Ğ±ĞºĞ¾Ğ³Ğ¾ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ FSQ. Ğ­Ñ‚Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€ĞµÑ‡Ğ¸ Ğ¿Ñ€Ğ¸ ĞºÑ€Ğ°Ğ¹Ğ½Ğµ Ğ½Ğ¸Ğ·ĞºĞ¸Ñ… Ğ±Ğ¸Ñ‚Ñ€ĞµĞ¹Ñ‚Ğ°Ñ… Ğ² 400-700 Ğ±Ğ¸Ñ‚/Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ°Ğº Ğ² Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ² ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ….'}, 'en': {'title': 'Transforming Speech Quality with Scalable Neural Models', 'desc': 'This paper discusses the importance of tokenizing speech using neural audio codec models in AI systems. It highlights a new approach that utilizes a large-scale transformer architecture combined with Finite Scalar Quantization (FSQ) to improve speech quality. The proposed method achieves impressive results, delivering high-quality speech at very low bit-rates of 400 or 700 bits-per-second. The models developed in this study significantly outperform existing methods in both objective measures and subjective evaluations.'}, 'zh': {'title': 'é€šè¿‡æ‰©å±•å˜æ¢å™¨å®ç°é«˜è´¨é‡ä½æ¯”ç‰¹ç‡è¯­éŸ³æ ‡è®°åŒ–', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†ä½¿ç”¨ç¥ç»éŸ³é¢‘ç¼–è§£ç æ¨¡å‹å¯¹è¯­éŸ³è¿›è¡Œæ ‡è®°åŒ–çš„é‡è¦æ€§ã€‚ä¼ ç»Ÿçš„æ ‡è®°åŒ–æ¨¡å‹é€šå¸¸é‡‡ç”¨ä½å‚æ•°é‡çš„æ¶æ„ï¼Œä¾èµ–äºå¼ºçš„å½’çº³åç½®ã€‚æˆ‘ä»¬å±•ç¤ºäº†é€šè¿‡æ‰©å±•å˜æ¢å™¨æ¶æ„å¹¶åº”ç”¨çµæ´»çš„æœ‰é™æ ‡é‡é‡åŒ–ï¼ˆFSQï¼‰ç“¶é¢ˆï¼Œå¯ä»¥åœ¨æä½çš„æ¯”ç‰¹ç‡ä¸‹å®ç°æœ€å…ˆè¿›çš„è¯­éŸ³è´¨é‡ã€‚è®­ç»ƒåçš„æ¨¡å‹åœ¨å®¢è§‚å’Œä¸»è§‚æµ‹è¯•ä¸­å‡æ˜¾è‘—è¶…è¶Šäº†ç°æœ‰åŸºå‡†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.18664', 'title': 'Spatiotemporal Skip Guidance for Enhanced Video Diffusion Sampling', 'url': 'https://huggingface.co/papers/2411.18664', 'abstract': 'Diffusion models have emerged as a powerful tool for generating high-quality images, videos, and 3D content. While sampling guidance techniques like CFG improve quality, they reduce diversity and motion. Autoguidance mitigates these issues but demands extra weak model training, limiting its practicality for large-scale models. In this work, we introduce Spatiotemporal Skip Guidance (STG), a simple training-free sampling guidance method for enhancing transformer-based video diffusion models. STG employs an implicit weak model via self-perturbation, avoiding the need for external models or additional training. By selectively skipping spatiotemporal layers, STG produces an aligned, degraded version of the original model to boost sample quality without compromising diversity or dynamic degree. Our contributions include: (1) introducing STG as an efficient, high-performing guidance technique for video diffusion models, (2) eliminating the need for auxiliary models by simulating a weak model through layer skipping, and (3) ensuring quality-enhanced guidance without compromising sample diversity or dynamics unlike CFG. For additional results, visit https://junhahyung.github.io/STGuidance.', 'score': 2, 'issue_id': 900, 'pub_date': '2024-11-27', 'pub_date_card': {'ru': '27 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 27', 'zh': '11æœˆ27æ—¥'}, 'hash': '3576f88bbf3e4567', 'authors': ['Junha Hyung', 'Kinam Kim', 'Susung Hong', 'Min-Jung Kim', 'Jaegul Choo'], 'affiliations': ['KAIST AI', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2411.18664.jpg', 'data': {'categories': ['#diffusion', '#architecture', '#3d', '#training', '#video'], 'emoji': 'ğŸ¬', 'ru': {'title': 'STG: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ - Spatiotemporal Skip Guidance (STG). STG Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ°Ğ¼Ğ¾Ğ²Ğ¾Ğ·Ğ¼ÑƒÑ‰ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ»Ğ°Ğ±Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸Ğ·Ğ±Ğ¸Ñ€Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ»Ğ¾Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑÑĞ¼Ğ¿Ğ»Ğ¾Ğ² Ğ±ĞµĞ· ÑƒÑ‰ĞµÑ€Ğ±Ğ° Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸. STG Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº CFG Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ³Ğ°Ğ¹Ğ´ĞµĞ½Ñ, Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸Ğ»Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Enhancing Video Diffusion with Spatiotemporal Skip Guidance', 'desc': 'This paper presents Spatiotemporal Skip Guidance (STG), a novel method for improving video diffusion models without requiring additional training or external models. STG enhances the quality of generated videos by using a self-perturbation technique that simulates a weak model through selective skipping of spatiotemporal layers. This approach allows for better sample quality while maintaining diversity and dynamic motion, addressing the limitations of existing methods like CFG. The authors demonstrate that STG is an efficient and effective guidance technique that enhances the performance of transformer-based video diffusion models.'}, 'zh': {'title': 'æ—¶ç©ºè·³è·ƒå¼•å¯¼ï¼šæå‡è§†é¢‘æ‰©æ•£æ¨¡å‹çš„é«˜æ•ˆæ–¹æ³•', 'desc': 'æ‰©æ•£æ¨¡å‹å·²æˆä¸ºç”Ÿæˆé«˜è´¨é‡å›¾åƒã€è§†é¢‘å’Œ3Då†…å®¹çš„å¼ºå¤§å·¥å…·ã€‚è™½ç„¶é‡‡æ ·å¼•å¯¼æŠ€æœ¯å¦‚CFGå¯ä»¥æé«˜è´¨é‡ï¼Œä½†ä¼šé™ä½å¤šæ ·æ€§å’ŒåŠ¨æ€æ€§ã€‚è‡ªå¼•å¯¼æ–¹æ³•è™½ç„¶å¯ä»¥ç¼“è§£è¿™äº›é—®é¢˜ï¼Œä½†éœ€è¦é¢å¤–çš„å¼±æ¨¡å‹è®­ç»ƒï¼Œé™åˆ¶äº†å…¶åœ¨å¤§è§„æ¨¡æ¨¡å‹ä¸­çš„å®ç”¨æ€§ã€‚æˆ‘ä»¬æå‡ºäº†æ—¶ç©ºè·³è·ƒå¼•å¯¼ï¼ˆSTGï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç®€å•çš„æ— è®­ç»ƒé‡‡æ ·å¼•å¯¼æ–¹æ³•ï¼Œæ—¨åœ¨å¢å¼ºåŸºäºå˜æ¢å™¨çš„è§†é¢‘æ‰©æ•£æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.18092', 'title': 'Training Noise Token Pruning', 'url': 'https://huggingface.co/papers/2411.18092', 'abstract': "In the present work we present Training Noise Token (TNT) Pruning for vision transformers. Our method relaxes the discrete token dropping condition to continuous additive noise, providing smooth optimization in training, while retaining discrete dropping computational gains in deployment settings. We provide theoretical connections to Rate-Distortion literature, and empirical evaluations on the ImageNet dataset using ViT and DeiT architectures demonstrating TNT's advantages over previous pruning methods.", 'score': 1, 'issue_id': 905, 'pub_date': '2024-11-27', 'pub_date_card': {'ru': '27 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 27', 'zh': '11æœˆ27æ—¥'}, 'hash': '570d01745d1c7f3d', 'authors': ['Mingxing Rao', 'Bohan Jiang', 'Daniel Moyer'], 'affiliations': ['Vanderbilt University, Nashville, TN 37235, USA'], 'pdf_title_img': 'assets/pdf/title_img/2411.18092.jpg', 'data': {'categories': ['#optimization', '#training', '#inference', '#cv', '#architecture'], 'emoji': 'âœ‚ï¸', 'ru': {'title': 'ĞŸĞ»Ğ°Ğ²Ğ½Ğ°Ñ Ğ¾Ğ±Ñ€ĞµĞ·ĞºĞ° Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑˆÑƒĞ¼Ğ°', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±Ñ€ĞµĞ·ĞºĞ¸ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ğ´Ğ»Ñ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Training Noise Token (TNT). ĞœĞµÑ‚Ğ¾Ğ´ Ğ·Ğ°Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğµ Ğ¾Ñ‚Ğ±Ñ€Ğ°ÑÑ‹Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ½Ğ° Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑˆÑƒĞ¼Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ğ»Ğ°Ğ²Ğ½ÑƒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ±Ñ€Ğ°ÑÑ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´ÑÑ‚ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ²ÑĞ·Ğ¸ Ñ Ğ»Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ğ¾Ğ¹ Ğ¿Ğ¾ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸-Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ImageNet Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€ ViT Ğ¸ DeiT. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° TNT Ğ¿ĞµÑ€ĞµĞ´ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¾Ğ±Ñ€ĞµĞ·ĞºĞ¸.'}, 'en': {'title': 'Smooth Optimization with TNT Pruning for Vision Transformers', 'desc': 'This paper introduces a novel approach called Training Noise Token (TNT) Pruning for vision transformers, which enhances the training process by allowing continuous additive noise instead of strictly dropping tokens. This method enables smoother optimization during training while still benefiting from the computational efficiency of discrete token dropping during deployment. The authors establish theoretical links to Rate-Distortion theory, which helps to understand the trade-offs involved in token pruning. Empirical results on the ImageNet dataset show that TNT Pruning outperforms existing pruning techniques when applied to ViT and DeiT architectures.'}, 'zh': {'title': 'è®­ç»ƒå™ªå£°æ ‡è®°å‰ªæï¼šä¼˜åŒ–ä¸æ•ˆç‡çš„ç»“åˆ', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºè§†è§‰å˜æ¢å™¨çš„è®­ç»ƒå™ªå£°æ ‡è®°ï¼ˆTNTï¼‰å‰ªææ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†ç¦»æ•£çš„æ ‡è®°ä¸¢å¼ƒæ¡ä»¶æ”¾å®½ä¸ºè¿ç»­çš„åŠ æ€§å™ªå£°ï¼Œä»è€Œåœ¨è®­ç»ƒä¸­å®ç°å¹³æ»‘ä¼˜åŒ–ï¼ŒåŒæ—¶åœ¨éƒ¨ç½²ç¯å¢ƒä¸­ä¿ç•™ç¦»æ•£ä¸¢å¼ƒçš„è®¡ç®—ä¼˜åŠ¿ã€‚æˆ‘ä»¬è¿˜æä¾›äº†ä¸ç‡å¤±çœŸæ–‡çŒ®çš„ç†è®ºè”ç³»ï¼Œå¹¶åœ¨ImageNetæ•°æ®é›†ä¸Šä½¿ç”¨ViTå’ŒDeiTæ¶æ„è¿›è¡Œäº†å®è¯è¯„ä¼°ï¼Œå±•ç¤ºäº†TNTç›¸è¾ƒäºä¹‹å‰å‰ªææ–¹æ³•çš„ä¼˜åŠ¿ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.18665', 'title': 'SpotLight: Shadow-Guided Object Relighting via Diffusion', 'url': 'https://huggingface.co/papers/2411.18665', 'abstract': 'Recent work has shown that diffusion models can be used as powerful neural rendering engines that can be leveraged for inserting virtual objects into images. Unlike typical physics-based renderers, however, neural rendering engines are limited by the lack of manual control over the lighting setup, which is often essential for improving or personalizing the desired image outcome. In this paper, we show that precise lighting control can be achieved for object relighting simply by specifying the desired shadows of the object. Rather surprisingly, we show that injecting only the shadow of the object into a pre-trained diffusion-based neural renderer enables it to accurately shade the object according to the desired light position, while properly harmonizing the object (and its shadow) within the target background image. Our method, SpotLight, leverages existing neural rendering approaches and achieves controllable relighting results with no additional training. Specifically, we demonstrate its use with two neural renderers from the recent literature. We show that SpotLight achieves superior object compositing results, both quantitatively and perceptually, as confirmed by a user study, outperforming existing diffusion-based models specifically designed for relighting.', 'score': 1, 'issue_id': 903, 'pub_date': '2024-11-27', 'pub_date_card': {'ru': '27 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 27', 'zh': '11æœˆ27æ—¥'}, 'hash': '1b31caf705bc142d', 'authors': ['FrÃ©dÃ©ric Fortier-Chouinard', 'Zitian Zhang', 'Louis-Etienne Messier', 'Mathieu Garon', 'Anand Bhattad', 'Jean-FranÃ§ois Lalonde'], 'affiliations': ['Depix Technologies', 'Toyota Technological Institute at Chicago', 'Universite Laval'], 'pdf_title_img': 'assets/pdf/title_img/2411.18665.jpg', 'data': {'categories': ['#3d', '#cv', '#diffusion'], 'emoji': 'ğŸ’¡', 'ru': {'title': 'ĞšĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ² Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¼ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğµ Ñ‡ĞµÑ€ĞµĞ· Ñ‚ĞµĞ½Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ SpotLight Ğ´Ğ»Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¼ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğµ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‚ĞµĞ½Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ° Ğ² Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ€ĞµĞ½Ğ´ĞµÑ€ĞµÑ€ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ·Ğ°Ñ‚ĞµĞ½ÑÑ‚ÑŒ Ğ¾Ğ±ÑŠĞµĞºÑ‚ Ğ² ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¸ Ñ Ğ¶ĞµĞ»Ğ°ĞµĞ¼Ñ‹Ğ¼ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ° ÑĞ²ĞµÑ‚Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ SpotLight Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµĞ½Ğ´ĞµÑ€ĞµÑ€Ğ°Ğ¼Ğ¸. Ğ¡Ğ¾Ğ³Ğ»Ğ°ÑĞ½Ğ¾ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, SpotLight Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¿ĞµÑ€ĞµÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ¸ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'SpotLight: Control Lighting with Shadows in Neural Rendering', 'desc': "This paper introduces SpotLight, a method that enhances neural rendering by allowing precise control over lighting for virtual objects in images. It achieves this by enabling users to specify the desired shadows of the object, which the diffusion model uses to accurately shade the object based on the light's position. SpotLight integrates seamlessly with existing pre-trained diffusion-based neural renderers, requiring no additional training. The results demonstrate significant improvements in object compositing, both in quantitative metrics and user perception, compared to traditional diffusion models designed for relighting."}, 'zh': {'title': 'SpotLightï¼šç²¾å‡†æ§åˆ¶è™šæ‹Ÿç‰©ä½“å…‰ç…§çš„åˆ›æ–°æ–¹æ³•', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºSpotLightçš„æ–¹æ³•ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹è¿›è¡Œè™šæ‹Ÿç‰©ä½“çš„é‡å…‰ç…§ã€‚ä¸ä¼ ç»Ÿçš„ç‰©ç†æ¸²æŸ“å™¨ä¸åŒï¼ŒSpotLighté€šè¿‡ä»…æŒ‡å®šç‰©ä½“çš„é˜´å½±æ¥å®ç°ç²¾ç¡®çš„å…‰ç…§æ§åˆ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿåœ¨ä¸éœ€è¦é¢å¤–è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œå‡†ç¡®åœ°æ ¹æ®æ‰€éœ€çš„å…‰æºä½ç½®ä¸ºç‰©ä½“ä¸Šè‰²ï¼Œå¹¶ä¸èƒŒæ™¯å›¾åƒå’Œè°èåˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSpotLightåœ¨ç‰©ä½“åˆæˆæ•ˆæœä¸Šä¼˜äºç°æœ‰çš„æ‰©æ•£æ¨¡å‹ï¼Œå¾—åˆ°äº†ç”¨æˆ·ç ”ç©¶çš„æ”¯æŒã€‚'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (8)', '#agents (2)', '#agi (1)', '#alignment (1)', '#architecture (17)', '#audio (4)', '#benchmark (20)', '#cv (11)', '#data (8)', '#dataset (23)', '#diffusion (15)', '#ethics (1)', '#games (8)', '#graphs', '#hallucinations (1)', '#healthcare', '#inference (9)', '#interpretability (5)', '#leakage (1)', '#long_context (4)', '#low_resource (4)', '#machine_translation (1)', '#math (3)', '#multilingual (3)', '#multimodal (16)', '#open_source (9)', '#optimization (23)', '#plp', '#rag (1)', '#reasoning (9)', '#rl (1)', '#rlhf (1)', '#robotics', '#science', '#security', '#small_models (3)', '#story_generation (1)', '#survey (2)', '#synthetic (7)', '#training (24)', '#transfer_learning (3)', '#video (20)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            ğŸ”º ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'ğŸ”„ ' + getTimeDiff('2024-12-04 10:11',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ",
                    pub_date: "Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸",
                    issue_id: "Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "è¯„åˆ†",
                    pub_date: "å‘å¸ƒæ—¥æœŸ",
                    issue_id: "HFä¸Šä¼ æ—¥æœŸ"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2024-12-04 10:11')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2024-12-04 10:11')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('monthly'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    