
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 78 papers. December 2024.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #7a30efcf;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: #7a30efcf;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #7a30ef17;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["–º–∏–Ω—É—Ç—É", "–º–∏–Ω—É—Ç—ã", "–º–∏–Ω—É—Ç"],
                hour: ["—á–∞—Å", "—á–∞—Å–∞", "—á–∞—Å–æ–≤"],
                day: ["–¥–µ–Ω—å", "–¥–Ω—è", "–¥–Ω–µ–π"],
                justNow: "—Ç–æ–ª—å–∫–æ —á—Ç–æ",
                ago: "–Ω–∞–∑–∞–¥"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["ÂàÜÈíü", "ÂàÜÈíü", "ÂàÜÈíü"],
                hour: ["Â∞èÊó∂", "Â∞èÊó∂", "Â∞èÊó∂"],
                day: ["Â§©", "Â§©", "Â§©"],
                justNow: "ÂàöÂàö",
                ago: "Ââç"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "—Å—Ç–∞—Ç–µ–π";
            } else if (lastDigit === 1) {
                word = "—Å—Ç–∞—Ç—å—è";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "—Å—Ç–∞—Ç—å–∏";
            } else {
                word = "—Å—Ç–∞—Ç–µ–π";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ÁØáËÆ∫Êñá"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">üî∫</h1><h1 class="title-text" id="doomgrad">hf monthly</h1></a>
            <p><span id="title-date">–î–µ–∫–∞–±—Ä—å 2024</span> | <span id="title-articles-count">78 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/m/2024-11.html">‚¨ÖÔ∏è <span id="prev-date">11.2024</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/m/2025-01.html">‚û°Ô∏è <span id="next-date">01.2025</span></a></span>
            <span class="nav-item" id="nav-daily"><a href="https://hfday.ru">üìà <span id='top-day-label'>–î–µ–Ω—å</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">üîÄ <span id="sort-label-text">–°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">—Ä–µ–π—Ç–∏–Ω–≥—É</option>
                    <option value="pub_date">–¥–∞—Ç–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏</option>
                    <option value="issue_id">–¥–æ–±–∞–≤–ª–µ–Ω–∏—é –Ω–∞ HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">üè∑Ô∏è –§–∏–ª—å—Ç—Ä</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A‚à™B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A‚à©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">üßπ</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ‚úñÔ∏è <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '–î–µ–∫–∞–±—Ä—å 2024', 'en': 'December 2024', 'zh': '12Êúà2024Âπ¥'};
        let feedDateNext = {'ru': '01.2025', 'en': '01/2025', 'zh': '1Êúà2025Âπ¥'};
        let feedDatePrev = {'ru': '11.2024', 'en': '11/2024', 'zh': '11Êúà2024Âπ¥'};
        let filterLabel = {'ru': '–§–∏–ª—å—Ç—Ä', 'en': 'Topics', 'zh': '‰∏ªÈ¢òÁ≠õÈÄâ'}
        let publishedLabel = {'ru': '—Å—Ç–∞—Ç—å—è –æ—Ç ', 'en': 'published on ', 'zh': 'ÂèëË°®‰∫é'}
        let sortLabel = {'ru': '–°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ', 'en': 'Sort by', 'zh': 'ÊéíÂ∫èÊñπÂºè'}
        let paperLabel = {'ru': '–°—Ç–∞—Ç—å—è', 'en': 'Paper', 'zh': 'ËÆ∫Êñá'}
        let topMonthLabel = {'ru': '–ú–µ—Å—è—Ü', 'en': 'Month', 'zh': 'ÊúàÂ∫¶ËÆ∫Êñá'}
        let topDayLabel = {'ru': '–î–µ–Ω—å', 'en': 'Day', 'zh': 'Êó•Â∫¶ËÆ∫Êñá'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf monthly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2412.02687', 'title': 'SNOOPI: Supercharged One-step Diffusion Distillation with Proper Guidance', 'url': 'https://huggingface.co/papers/2412.02687', 'abstract': "Recent approaches have yielded promising results in distilling multi-step text-to-image diffusion models into one-step ones. The state-of-the-art efficient distillation technique, i.e., SwiftBrushv2 (SBv2), even surpasses the teacher model's performance with limited resources. However, our study reveals its instability when handling different diffusion model backbones due to using a fixed guidance scale within the Variational Score Distillation (VSD) loss. Another weakness of the existing one-step diffusion models is the missing support for negative prompt guidance, which is crucial in practical image generation. This paper presents SNOOPI, a novel framework designed to address these limitations by enhancing the guidance in one-step diffusion models during both training and inference. First, we effectively enhance training stability through Proper Guidance-SwiftBrush (PG-SB), which employs a random-scale classifier-free guidance approach. By varying the guidance scale of both teacher models, we broaden their output distributions, resulting in a more robust VSD loss that enables SB to perform effectively across diverse backbones while maintaining competitive performance. Second, we propose a training-free method called Negative-Away Steer Attention (NASA), which integrates negative prompts into one-step diffusion models via cross-attention to suppress undesired elements in generated images. Our experimental results show that our proposed methods significantly improve baseline models across various metrics. Remarkably, we achieve an HPSv2 score of 31.08, setting a new state-of-the-art benchmark for one-step diffusion models.", 'score': 71, 'issue_id': 961, 'pub_date': '2024-12-03', 'pub_date_card': {'ru': '3 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 3', 'zh': '12Êúà3Êó•'}, 'hash': 'd766bad745d5f322', 'authors': ['Viet Nguyen', 'Anh Nguyen', 'Trung Dao', 'Khoi Nguyen', 'Cuong Pham', 'Toan Tran', 'Anh Tran'], 'affiliations': ['Posts & Telecom. Inst. of Tech.', 'VinAI Research'], 'pdf_title_img': 'assets/pdf/title_img/2412.02687.jpg', 'data': {'categories': ['#cv', '#dataset', '#optimization', '#inference', '#training', '#benchmark', '#diffusion'], 'emoji': 'üé®', 'ru': {'title': '–ü–æ–≤—ã—à–µ–Ω–∏–µ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –∏ –≥–∏–±–∫–æ—Å—Ç–∏ –æ–¥–Ω–æ—à–∞–≥–æ–≤—ã—Ö –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç SNOOPI - –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–¥–Ω–æ—à–∞–≥–æ–≤—ã—Ö –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥ PG-SB –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è –ø—É—Ç–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Å–ª—É—á–∞–π–Ω–æ–≥–æ –º–∞—Å—à—Ç–∞–±–∞ –±–µ—Å–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–Ω–æ–≥–æ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–∞. –¢–∞–∫–∂–µ –≤–≤–æ–¥–∏—Ç—Å—è –º–µ—Ç–æ–¥ NASA –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö –ø—Ä–æ–º–ø—Ç–æ–≤ —á–µ—Ä–µ–∑ –∫—Ä–æ—Å—Å-–≤–Ω–∏–º–∞–Ω–∏–µ. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø–æ —Ä–∞–∑–ª–∏—á–Ω—ã–º –º–µ—Ç—Ä–∏–∫–∞–º, –¥–æ—Å—Ç–∏–≥–∞—è –Ω–æ–≤–æ–≥–æ —Ä–µ–∫–æ—Ä–¥–∞ HPSv2 –≤ 31.08 –¥–ª—è –æ–¥–Ω–æ—à–∞–≥–æ–≤—ã—Ö –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π.'}, 'en': {'title': 'SNOOPI: Enhancing One-Step Diffusion Models with Robust Guidance', 'desc': 'This paper introduces SNOOPI, a new framework that improves one-step text-to-image diffusion models by addressing issues with guidance stability and negative prompt support. The authors enhance training stability using Proper Guidance-SwiftBrush (PG-SB), which applies a random-scale classifier-free guidance method to diversify output distributions. Additionally, they present Negative-Away Steer Attention (NASA), a training-free technique that incorporates negative prompts to eliminate unwanted elements in generated images. Experimental results demonstrate that SNOOPI outperforms existing models, achieving a new state-of-the-art HPSv2 score of 31.08.'}, 'zh': {'title': 'SNOOPIÔºöÊèêÂçá‰∏ÄÊ≠•Êâ©Êï£Ê®°ÂûãÁöÑÁ®≥ÂÆöÊÄß‰∏éÁîüÊàêË¥®Èáè', 'desc': 'Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞Ê°ÜÊû∂SNOOPIÔºåÊó®Âú®Ëß£ÂÜ≥Áé∞Êúâ‰∏ÄÊ≠•Êâ©Êï£Ê®°ÂûãÁöÑÂ±ÄÈôêÊÄß„ÄÇÊàë‰ª¨ÈÄöËøáProper Guidance-SwiftBrush (PG-SB)ÊñπÊ≥ïÂ¢ûÂº∫‰∫ÜËÆ≠ÁªÉÁöÑÁ®≥ÂÆöÊÄßÔºåÈááÁî®ÈöèÊú∫Â∞∫Â∫¶ÁöÑÊó†ÂàÜÁ±ªÂô®ÂºïÂØºÁ≠ñÁï•„ÄÇÊàë‰ª¨ËøòÊèêÂá∫‰∫Ü‰∏ÄÁßçÊó†ËÆ≠ÁªÉÁöÑÊñπÊ≥ïNegative-Away Steer Attention (NASA)ÔºåÈÄöËøá‰∫§ÂèâÊ≥®ÊÑèÂäõÂ∞ÜË¥üÊèêÁ§∫ÈõÜÊàêÂà∞‰∏ÄÊ≠•Êâ©Êï£Ê®°Âûã‰∏≠Ôºå‰ª•ÊäëÂà∂ÁîüÊàêÂõæÂÉè‰∏≠ÁöÑ‰∏çÂøÖË¶ÅÂÖÉÁ¥†„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂú®Â§ö‰∏™ÊåáÊ†á‰∏äÊòæËëóÊèêÈ´ò‰∫ÜÂü∫Á∫øÊ®°ÂûãÁöÑÊÄßËÉΩÔºåÂàõÈÄ†‰∫Ü‰∏ÄÊ≠•Êâ©Êï£Ê®°ÂûãÁöÑÊñ∞Ê†áÊùÜ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2412.03555', 'title': 'PaliGemma 2: A Family of Versatile VLMs for Transfer', 'url': 'https://huggingface.co/papers/2412.03555', 'abstract': 'PaliGemma 2 is an upgrade of the PaliGemma open Vision-Language Model (VLM) based on the Gemma 2 family of language models. We combine the SigLIP-So400m vision encoder that was also used by PaliGemma with the whole range of Gemma 2 models, from the 2B one all the way up to the 27B model. We train these models at three resolutions (224px, 448px, and 896px) in multiple stages to equip them with broad knowledge for transfer via fine-tuning. The resulting family of base models covering different model sizes and resolutions allows us to investigate factors impacting transfer performance (such as learning rate) and to analyze the interplay between the type of task, model size, and resolution. We further increase the number and breadth of transfer tasks beyond the scope of PaliGemma including different OCR-related tasks such as table structure recognition, molecular structure recognition, music score recognition, as well as long fine-grained captioning and radiography report generation, on which PaliGemma 2 obtains state-of-the-art results.', 'score': 62, 'issue_id': 964, 'pub_date': '2024-12-04', 'pub_date_card': {'ru': '4 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 4', 'zh': '12Êúà4Êó•'}, 'hash': '12d0d9bcc8060099', 'authors': ['Andreas Steiner', 'Andr√© Susano Pinto', 'Michael Tschannen', 'Daniel Keysers', 'Xiao Wang', 'Yonatan Bitton', 'Alexey Gritsenko', 'Matthias Minderer', 'Anthony Sherbondy', 'Shangbang Long', 'Siyang Qin', 'Reeve Ingle', 'Emanuele Bugliarello', 'Sahar Kazemzadeh', 'Thomas Mesnard', 'Ibrahim Alabdulmohsin', 'Lucas Beyer', 'Xiaohua Zhai'], 'affiliations': ['Google DeepMind'], 'pdf_title_img': 'assets/pdf/title_img/2412.03555.jpg', 'data': {'categories': ['#multimodal', '#optimization', '#training', '#cv', '#transfer_learning'], 'emoji': 'üß†', 'ru': {'title': 'PaliGemma 2: –ù–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –ò–ò', 'desc': 'PaliGemma 2 - —ç—Ç–æ —É–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –æ—Ç–∫—Ä—ã—Ç–æ–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ PaliGemma, –æ—Å–Ω–æ–≤–∞–Ω–Ω–∞—è –Ω–∞ —Å–µ–º–µ–π—Å—Ç–≤–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π Gemma 2. –ú–æ–¥–µ–ª—å —Å–æ—á–µ—Ç–∞–µ—Ç –≤–∏–∑—É–∞–ª—å–Ω—ã–π —ç–Ω–∫–æ–¥–µ—Ä SigLIP-So400m —Å —Ä—è–¥–æ–º –º–æ–¥–µ–ª–µ–π Gemma 2 —Ä–∞–∑–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–æ–≤, –æ—Ç 2B –¥–æ 27B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤. –û–±—É—á–µ–Ω–∏–µ –ø—Ä–æ–≤–æ–¥–∏–ª–æ—Å—å –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö —Ä–∞–∑–Ω–æ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è (224px, 448px –∏ 896px) –≤ –Ω–µ—Å–∫–æ–ª—å–∫–æ —ç—Ç–∞–ø–æ–≤ –¥–ª—è –ø—Ä–∏–æ–±—Ä–µ—Ç–µ–Ω–∏—è —à–∏—Ä–æ–∫–∏—Ö –∑–Ω–∞–Ω–∏–π. PaliGemma 2 –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –æ—Ç–ª–∏—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö, –≤–∫–ª—é—á–∞—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Ç–∞–±–ª–∏—Ü, –º–æ–ª–µ–∫—É–ª—è—Ä–Ω—ã—Ö —Å—Ç—Ä—É–∫—Ç—É—Ä, –Ω–æ—Ç–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π, –∞ —Ç–∞–∫–∂–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –ø–æ–¥—Ä–æ–±–Ω—ã—Ö –æ–ø–∏—Å–∞–Ω–∏–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ —Ä–∞–¥–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –æ—Ç—á–µ—Ç–æ–≤.'}, 'en': {'title': 'PaliGemma 2: Advancing Vision-Language Understanding', 'desc': 'PaliGemma 2 is an enhanced Vision-Language Model (VLM) that builds on the original PaliGemma framework by integrating the SigLIP-So400m vision encoder with various sizes of the Gemma 2 language models. The models are trained at three different image resolutions to improve their ability to transfer knowledge through fine-tuning. This upgrade allows researchers to explore how different factors, like learning rates and model sizes, affect performance on various tasks. PaliGemma 2 also expands its capabilities to include a wider range of tasks, achieving state-of-the-art results in areas such as optical character recognition and detailed captioning.'}, 'zh': {'title': 'PaliGemma 2ÔºöËßÜËßâ‰∏éËØ≠Ë®ÄÁöÑÂÆåÁæéÁªìÂêà', 'desc': 'PaliGemma 2 ÊòØÂü∫‰∫é Gemma 2 ËØ≠Ë®ÄÊ®°ÂûãÂÆ∂ÊóèÁöÑ PaliGemma ÂºÄÊîæËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÁöÑÂçáÁ∫ßÁâà„ÄÇÊàë‰ª¨ÁªìÂêà‰∫Ü SigLIP-So400m ËßÜËßâÁºñÁ†ÅÂô®Âíå‰∏çÂêåËßÑÊ®°ÁöÑ Gemma 2 Ê®°ÂûãÔºåËøõË°åÂ§öÈò∂ÊÆµËÆ≠ÁªÉÔºå‰ª•ÊèêÈ´òÊ®°ÂûãÁöÑÁü•ËØÜËøÅÁßªËÉΩÂäõ„ÄÇÈÄöËøáÂú®‰∏âÁßçÂàÜËæ®Áéá‰∏ãËÆ≠ÁªÉÔºåÊàë‰ª¨ËÉΩÂ§üÁ†îÁ©∂ÂΩ±ÂìçËøÅÁßªÊÄßËÉΩÁöÑÂõ†Á¥†ÔºåÂ¶ÇÂ≠¶‰π†ÁéáÔºåÂπ∂ÂàÜÊûê‰ªªÂä°Á±ªÂûã„ÄÅÊ®°ÂûãÂ§ßÂ∞èÂíåÂàÜËæ®Áéá‰πãÈó¥ÁöÑÂÖ≥Á≥ª„ÄÇPaliGemma 2 Êâ©Â±ï‰∫ÜËøÅÁßª‰ªªÂä°ÁöÑÊï∞ÈáèÂíåËåÉÂõ¥ÔºåÊ∂µÁõñ‰∫ÜÂ§öÁßçÂÖâÂ≠¶Â≠óÁ¨¶ËØÜÂà´Áõ∏ÂÖ≥‰ªªÂä°ÔºåÂπ∂Âú®Ëøô‰∫õ‰ªªÂä°‰∏äÂèñÂæó‰∫ÜÊúÄÂÖàËøõÁöÑÁªìÊûú„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2412.03552', 'title': 'Imagine360: Immersive 360 Video Generation from Perspective Anchor', 'url': 'https://huggingface.co/papers/2412.03552', 'abstract': '360^circ videos offer a hyper-immersive experience that allows the viewers to explore a dynamic scene from full 360 degrees. To achieve more user-friendly and personalized content creation in 360^circ video format, we seek to lift standard perspective videos into 360^circ equirectangular videos. To this end, we introduce Imagine360, the first perspective-to-360^circ video generation framework that creates high-quality 360^circ videos with rich and diverse motion patterns from video anchors. Imagine360 learns fine-grained spherical visual and motion patterns from limited 360^circ video data with several key designs. 1) Firstly we adopt the dual-branch design, including a perspective and a panorama video denoising branch to provide local and global constraints for 360^circ video generation, with motion module and spatial LoRA layers fine-tuned on extended web 360^circ videos. 2) Additionally, an antipodal mask is devised to capture long-range motion dependencies, enhancing the reversed camera motion between antipodal pixels across hemispheres. 3) To handle diverse perspective video inputs, we propose elevation-aware designs that adapt to varying video masking due to changing elevations across frames. Extensive experiments show Imagine360 achieves superior graphics quality and motion coherence among state-of-the-art 360^circ video generation methods. We believe Imagine360 holds promise for advancing personalized, immersive 360^circ video creation.', 'score': 23, 'issue_id': 958, 'pub_date': '2024-12-04', 'pub_date_card': {'ru': '4 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 4', 'zh': '12Êúà4Êó•'}, 'hash': '90dc986cabb575af', 'authors': ['Jing Tan', 'Shuai Yang', 'Tong Wu', 'Jingwen He', 'Yuwei Guo', 'Ziwei Liu', 'Dahua Lin'], 'affiliations': ['Nanyang Technological University', 'Shanghai AI Laboratory', 'Shanghai Jiao Tong University', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2412.03552.jpg', 'data': {'categories': ['#cv', '#video', '#multimodal'], 'emoji': 'üåê', 'ru': {'title': '–ü–æ–≥—Ä—É–∂–µ–Ω–∏–µ –≤ 360¬∞: –æ—Ç –æ–±—ã—á–Ω–æ–≥–æ –≤–∏–¥–µ–æ –∫ –ø–∞–Ω–æ—Ä–∞–º–Ω–æ–º—É –æ–ø—ã—Ç—É', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Imagine360 - –ø–µ—Ä–≤—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ 360-–≥—Ä–∞–¥—É—Å–Ω—ã—Ö –≤–∏–¥–µ–æ –∏–∑ –æ–±—ã—á–Ω—ã—Ö –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–Ω—ã—Ö –≤–∏–¥–µ–æ. –°–∏—Å—Ç–µ–º–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–≤—É—Ö–≤–µ—Ç–≤–µ–≤—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É —Å –º–æ–¥—É–ª—è–º–∏ —à—É–º–æ–ø–æ–¥–∞–≤–ª–µ–Ω–∏—è –¥–ª—è –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –∏ –ø–∞–Ω–æ—Ä–∞–º–Ω–æ–≥–æ –≤–∏–¥–µ–æ, –∞ —Ç–∞–∫–∂–µ –∞–Ω—Ç–∏–ø–æ–¥–∞–ª—å–Ω—É—é –º–∞—Å–∫—É –¥–ª—è –∑–∞—Ö–≤–∞—Ç–∞ –¥–∞–ª—å–Ω–∏—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –¥–≤–∏–∂–µ–Ω–∏—è. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω—ã —Ä–µ—à–µ–Ω–∏—è –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –∫ –∏–∑–º–µ–Ω–µ–Ω–∏—è–º —É–≥–ª–∞ –æ–±–∑–æ—Ä–∞ –≤–æ –≤—Ö–æ–¥–Ω—ã—Ö –≤–∏–¥–µ–æ. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –≥—Ä–∞—Ñ–∏–∫–∏ –∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –¥–≤–∏–∂–µ–Ω–∏—è –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏.'}, 'en': {'title': 'Transforming Perspective Videos into Immersive 360¬∞ Experiences', 'desc': 'The paper presents Imagine360, a novel framework for converting standard perspective videos into immersive 360-degree equirectangular videos. It employs a dual-branch architecture that integrates local and global constraints to enhance video quality and motion coherence. Key innovations include an antipodal mask for capturing long-range motion dependencies and elevation-aware designs to adapt to varying perspectives. Extensive experiments demonstrate that Imagine360 outperforms existing methods in generating high-quality, dynamic 360-degree videos.'}, 'zh': {'title': 'Imagine360Ôºö‰∏™ÊÄßÂåñÊ≤âÊµ∏Âºè360Â∫¶ËßÜÈ¢ëÂàõ‰ΩúÁöÑÊú™Êù•', 'desc': '360Â∫¶ËßÜÈ¢ëÊèê‰æõ‰∫Ü‰∏ÄÁßçË∂ÖÊ≤âÊµ∏Âºè‰ΩìÈ™åÔºåËÆ©ËßÇ‰ºóÂèØ‰ª•‰ªéÂÖ®Êñπ‰ΩçÊé¢Á¥¢Âä®ÊÄÅÂú∫ÊôØ„ÄÇ‰∏∫ÂÆûÁé∞Êõ¥ÂèãÂ•ΩÂíå‰∏™ÊÄßÂåñÁöÑ360Â∫¶ËßÜÈ¢ëÂÜÖÂÆπÂàõ‰ΩúÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜImagine360ÔºåËøôÊòØÈ¶ñ‰∏™Â∞ÜÊ†áÂáÜËßÜËßíËßÜÈ¢ëËΩ¨Êç¢‰∏∫360Â∫¶ËßÜÈ¢ëÁöÑÊ°ÜÊû∂„ÄÇImagine360ÈÄöËøáÊúâÈôêÁöÑ360Â∫¶ËßÜÈ¢ëÊï∞ÊçÆÂ≠¶‰π†ÁªÜËá¥ÁöÑÁêÉÈù¢ËßÜËßâÂíåËøêÂä®Ê®°ÂºèÔºåÈááÁî®ÂèåÂàÜÊîØËÆæËÆ°Êù•Êèê‰æõÂ±ÄÈÉ®ÂíåÂÖ®Â±ÄÁ∫¶Êùü„ÄÇÂÆûÈ™åË°®ÊòéÔºåImagine360Âú®ÂõæÂΩ¢Ë¥®ÈáèÂíåËøêÂä®‰∏ÄËá¥ÊÄßÊñπÈù¢‰ºò‰∫éÁé∞ÊúâÁöÑ360Â∫¶ËßÜÈ¢ëÁîüÊàêÊñπÊ≥ï„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2412.03515', 'title': 'Distilling Diffusion Models to Efficient 3D LiDAR Scene Completion', 'url': 'https://huggingface.co/papers/2412.03515', 'abstract': 'Diffusion models have been applied to 3D LiDAR scene completion due to their strong training stability and high completion quality. However, the slow sampling speed limits the practical application of diffusion-based scene completion models since autonomous vehicles require an efficient perception of surrounding environments. This paper proposes a novel distillation method tailored for 3D LiDAR scene completion models, dubbed ScoreLiDAR, which achieves efficient yet high-quality scene completion. ScoreLiDAR enables the distilled model to sample in significantly fewer steps after distillation. To improve completion quality, we also introduce a novel Structural Loss, which encourages the distilled model to capture the geometric structure of the 3D LiDAR scene. The loss contains a scene-wise term constraining the holistic structure and a point-wise term constraining the key landmark points and their relative configuration. Extensive experiments demonstrate that ScoreLiDAR significantly accelerates the completion time from 30.55 to 5.37 seconds per frame (>5times) on SemanticKITTI and achieves superior performance compared to state-of-the-art 3D LiDAR scene completion models. Our code is publicly available at https://github.com/happyw1nd/ScoreLiDAR.', 'score': 21, 'issue_id': 957, 'pub_date': '2024-12-04', 'pub_date_card': {'ru': '4 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 4', 'zh': '12Êúà4Êó•'}, 'hash': '6e733cf9c0a1b851', 'authors': ['Shengyuan Zhang', 'An Zhao', 'Ling Yang', 'Zejian Li', 'Chenye Meng', 'Haoran Xu', 'Tianrun Chen', 'AnYang Wei', 'Perry Pengyun GU', 'Lingyun Sun'], 'affiliations': ['Peking University', 'Zhejiang Green Zhixing Technology co., ltd', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2412.03515.jpg', 'data': {'categories': ['#optimization', '#diffusion', '#training', '#3d', '#open_source'], 'emoji': 'üöó', 'ru': {'title': '–ë—ã—Å—Ç—Ä–æ–µ –∏ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ 3D LiDAR-—Å—Ü–µ–Ω –¥–ª—è –∞–≤—Ç–æ–Ω–æ–º–Ω—ã—Ö —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–Ω—ã—Ö —Å—Ä–µ–¥—Å—Ç–≤', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –¥–ª—è –º–æ–¥–µ–ª–µ–π –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è 3D LiDAR-—Å—Ü–µ–Ω –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º ScoreLiDAR. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É—Å–∫–æ—Ä–∏—Ç—å –ø—Ä–æ—Ü–µ—Å—Å —Å–µ–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è —Å—Ü–µ–Ω—ã. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ –≤–≤–æ–¥—è—Ç –Ω–æ–≤—É—é –°—Ç—Ä—É–∫—Ç—É—Ä–Ω—É—é –ü–æ—Ç–µ—Ä—é, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–º–æ–≥–∞–µ—Ç –¥–∏—Å—Ç–∏–ª–ª–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –ª—É—á—à–µ —É–ª–∞–≤–ª–∏–≤–∞—Ç—å –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É 3D LiDAR-—Å—Ü–µ–Ω—ã. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ ScoreLiDAR —É—Å–∫–æ—Ä—è–µ—Ç –≤—Ä–µ–º—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –±–æ–ª–µ–µ —á–µ–º –≤ 5 —Ä–∞–∑ –∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è 3D LiDAR-—Å—Ü–µ–Ω.'}, 'en': {'title': 'Accelerating 3D LiDAR Scene Completion with ScoreLiDAR', 'desc': "This paper introduces ScoreLiDAR, a new method for improving the efficiency of 3D LiDAR scene completion using diffusion models. The proposed distillation technique allows the model to generate high-quality scene completions in significantly fewer sampling steps, making it faster and more practical for real-time applications like autonomous vehicles. Additionally, a novel Structural Loss is introduced to enhance the model's ability to understand and replicate the geometric structure of the 3D scenes. Experimental results show that ScoreLiDAR reduces completion time dramatically while outperforming existing state-of-the-art models in quality."}, 'zh': {'title': 'È´òÊïà3D LiDARÂú∫ÊôØË°•ÂÖ®ÁöÑÊñ∞ÊñπÊ≥ï', 'desc': 'Êâ©Êï£Ê®°ÂûãÂõ†ÂÖ∂Âº∫Â§ßÁöÑËÆ≠ÁªÉÁ®≥ÂÆöÊÄßÂíåÈ´òË¥®ÈáèÁöÑÂú∫ÊôØË°•ÂÖ®ËÄåË¢´Â∫îÁî®‰∫é3D LiDARÂú∫ÊôØË°•ÂÖ®„ÄÇÁÑ∂ËÄåÔºåÊÖ¢ÈÄüÈááÊ†∑ÈÄüÂ∫¶ÈôêÂà∂‰∫ÜÂü∫‰∫éÊâ©Êï£ÁöÑÂú∫ÊôØË°•ÂÖ®Ê®°ÂûãÁöÑÂÆûÈôÖÂ∫îÁî®ÔºåÂõ†‰∏∫Ëá™Âä®È©æÈ©∂ËΩ¶ËæÜÈúÄË¶ÅÈ´òÊïàÊÑüÁü•Âë®Âõ¥ÁéØÂ¢É„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑËí∏È¶èÊñπÊ≥ïÔºåÁß∞‰∏∫ScoreLiDARÔºåÊó®Âú®ÂÆûÁé∞È´òÊïà‰∏îÈ´òË¥®ÈáèÁöÑÂú∫ÊôØË°•ÂÖ®„ÄÇÈÄöËøáÂºïÂÖ•ÁªìÊûÑÊçüÂ§±ÔºåScoreLiDARËÉΩÂ§üÊõ¥Â•ΩÂú∞ÊçïÊçâ3D LiDARÂú∫ÊôØÁöÑÂá†‰ΩïÁªìÊûÑÔºåÂêåÊó∂ÊòæËëóÂä†Âø´‰∫ÜÊØèÂ∏ßÁöÑË°•ÂÖ®Êó∂Èó¥„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2412.03069', 'title': 'TokenFlow: Unified Image Tokenizer for Multimodal Understanding and Generation', 'url': 'https://huggingface.co/papers/2412.03069', 'abstract': "We present TokenFlow, a novel unified image tokenizer that bridges the long-standing gap between multimodal understanding and generation. Prior research attempt to employ a single reconstruction-targeted Vector Quantization (VQ) encoder for unifying these two tasks. We observe that understanding and generation require fundamentally different granularities of visual information. This leads to a critical trade-off, particularly compromising performance in multimodal understanding tasks. TokenFlow addresses this challenge through an innovative dual-codebook architecture that decouples semantic and pixel-level feature learning while maintaining their alignment via a shared mapping mechanism. This design enables direct access to both high-level semantic representations crucial for understanding tasks and fine-grained visual features essential for generation through shared indices. Our extensive experiments demonstrate TokenFlow's superiority across multiple dimensions. Leveraging TokenFlow, we demonstrate for the first time that discrete visual input can surpass LLaVA-1.5 13B in understanding performance, achieving a 7.2\\% average improvement. For image reconstruction, we achieve a strong FID score of 0.63 at 384*384 resolution. Moreover, TokenFlow establishes state-of-the-art performance in autoregressive image generation with a GenEval score of 0.55 at 256*256 resolution, achieving comparable results to SDXL.", 'score': 18, 'issue_id': 957, 'pub_date': '2024-12-04', 'pub_date_card': {'ru': '4 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 4', 'zh': '12Êúà4Êó•'}, 'hash': '820e62e1bd498d55', 'authors': ['Liao Qu', 'Huichao Zhang', 'Yiheng Liu', 'Xu Wang', 'Yi Jiang', 'Yiming Gao', 'Hu Ye', 'Daniel K. Du', 'Zehuan Yuan', 'Xinglong Wu'], 'affiliations': ['ByteDance'], 'pdf_title_img': 'assets/pdf/title_img/2412.03069.jpg', 'data': {'categories': ['#multimodal', '#cv', '#architecture'], 'emoji': 'üîÄ', 'ru': {'title': 'TokenFlow: –µ–¥–∏–Ω—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π', 'desc': 'TokenFlow - —ç—Ç–æ –Ω–æ–≤—ã–π —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∏–π –∑–∞–¥–∞—á–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É —Å –¥–≤–æ–π–Ω—ã–º –∫–æ–¥–±—É–∫–æ–º, –∫–æ—Ç–æ—Ä–∞—è —Ä–∞–∑–¥–µ–ª—è–µ—Ç –æ–±—É—á–µ–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –∏ –ø–∏–∫—Å–µ–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, —Å–æ—Ö—Ä–∞–Ω—è—è –∏—Ö –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ —á–µ—Ä–µ–∑ –æ–±—â–∏–π –º–µ—Ö–∞–Ω–∏–∑–º –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è. TokenFlow –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–æ–¥–µ–ª–∏ –≤ –∑–∞–¥–∞—á–∞—Ö –ø–æ–Ω–∏–º–∞–Ω–∏—è, –¥–æ—Å—Ç–∏–≥–∞—è —É–ª—É—á—à–µ–Ω–∏—è –Ω–∞ 7.2% –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å LLaVA-1.5 13B. –ú–æ–¥–µ–ª—å —Ç–∞–∫–∂–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –≤—ã—Å–æ–∫–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, –¥–æ—Å—Ç–∏–≥–∞—è –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π –Ω–∞ —É—Ä–æ–≤–Ω–µ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π.'}, 'en': {'title': 'TokenFlow: Bridging Understanding and Generation in Image Processing', 'desc': 'TokenFlow is a new image tokenizer that improves how machines understand and generate images by using a dual-codebook architecture. This approach separates the learning of high-level semantic features from fine-grained pixel-level details, allowing for better performance in both understanding and generation tasks. By aligning these two types of information through a shared mapping, TokenFlow can effectively utilize both granularities of visual data. The results show that TokenFlow outperforms previous models in understanding and image generation, achieving significant improvements in performance metrics.'}, 'zh': {'title': 'TokenFlowÔºöÂ§öÊ®°ÊÄÅÁêÜËß£‰∏éÁîüÊàêÁöÑÊ°•Ê¢Å', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÂõæÂÉèÊ†áËÆ∞Âô®TokenFlowÔºåÂÆÉÂº•Âêà‰∫ÜÂ§öÊ®°ÊÄÅÁêÜËß£‰∏éÁîüÊàê‰πãÈó¥ÁöÑÂ∑ÆË∑ù„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÁêÜËß£ÂíåÁîüÊàê‰ªªÂä°ÈúÄË¶Å‰∏çÂêåÁ≤íÂ∫¶ÁöÑËßÜËßâ‰ø°ÊÅØÔºå‰º†ÁªüÁöÑÂçï‰∏ÄÈáçÂª∫ÁõÆÊ†áÂêëÈáèÈáèÂåñÁºñÁ†ÅÂô®Êó†Ê≥ïÊúâÊïàÂ§ÑÁêÜËøô‰∏ÄÈóÆÈ¢ò„ÄÇTokenFlowÈÄöËøáÂàõÊñ∞ÁöÑÂèå‰ª£Á†ÅÊú¨Êû∂ÊûÑÔºåËß£ËÄ¶‰∫ÜËØ≠‰πâÂíåÂÉèÁ¥†Á∫ßÁâπÂæÅÂ≠¶‰π†ÔºåÂêåÊó∂ÈÄöËøáÂÖ±‰∫´Êò†Â∞ÑÊú∫Âà∂‰øùÊåÅÂÆÉ‰ª¨ÁöÑÂØπÈΩê„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåTokenFlowÂú®Â§öÈ°π‰ªªÂä°‰∏≠Ë°®Áé∞‰ºòË∂äÔºåÈ¶ñÊ¨°ËØÅÊòéÁ¶ªÊï£ËßÜËßâËæìÂÖ•Âú®ÁêÜËß£ÊÄßËÉΩ‰∏äË∂ÖË∂ä‰∫ÜLLaVA-1.5 13B„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2412.00493', 'title': 'Video-3D LLM: Learning Position-Aware Video Representation for 3D Scene Understanding', 'url': 'https://huggingface.co/papers/2412.00493', 'abstract': "The rapid advancement of Multimodal Large Language Models (MLLMs) has significantly impacted various multimodal tasks. However, these models face challenges in tasks that require spatial understanding within 3D environments. Efforts to enhance MLLMs, such as incorporating point cloud features, have been made, yet a considerable gap remains between the models' learned representations and the inherent complexity of 3D scenes. This discrepancy largely stems from the training of MLLMs on predominantly 2D data, which restricts their effectiveness in comprehending 3D spaces. To address this issue, in this paper, we propose a novel generalist model, i.e., Video-3D LLM, for 3D scene understanding. By treating 3D scenes as dynamic videos and incorporating 3D position encoding into these representations, our Video-3D LLM aligns video representations with real-world spatial contexts more accurately. Additionally, we have implemented a maximum coverage sampling technique to optimize the balance between computational costs and performance efficiency. Extensive experiments demonstrate that our model achieves state-of-the-art performance on several 3D scene understanding benchmarks, including ScanRefer, Multi3DRefer, Scan2Cap, ScanQA, and SQA3D.", 'score': 14, 'issue_id': 964, 'pub_date': '2024-11-30', 'pub_date_card': {'ru': '30 –Ω–æ—è–±—Ä—è', 'en': 'November 30', 'zh': '11Êúà30Êó•'}, 'hash': '10c214b548697656', 'authors': ['Duo Zheng', 'Shijia Huang', 'Liwei Wang'], 'affiliations': ['The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2412.00493.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#3d', '#optimization', '#training'], 'emoji': 'üé•', 'ru': {'title': 'Video-3D LLM: –ü—Ä–æ—Ä—ã–≤ –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ —Ç—Ä–µ—Ö–º–µ—Ä–Ω—ã—Ö —Å—Ü–µ–Ω', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å Video-3D LLM –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è —Ç—Ä–µ—Ö–º–µ—Ä–Ω—ã—Ö —Å—Ü–µ–Ω. –ú–æ–¥–µ–ª—å —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç 3D-—Å—Ü–µ–Ω—ã –∫–∞–∫ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–µ –≤–∏–¥–µ–æ –∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç 3D-–ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –ª—É—á—à–µ–≥–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –≤–∏–¥–µ–æ–ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π —Ä–µ–∞–ª—å–Ω—ã–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞–º. –ê–≤—Ç–æ—Ä—ã –ø—Ä–∏–º–µ–Ω–∏–ª–∏ —Ç–µ—Ö–Ω–∏–∫—É –≤—ã–±–æ—Ä–∫–∏ —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º –ø–æ–∫—Ä—ã—Ç–∏–µ–º –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –±–∞–ª–∞–Ω—Å–∞ –º–µ–∂–¥—É –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–º–∏ –∑–∞—Ç—Ä–∞—Ç–∞–º–∏ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å—é. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –º–æ–¥–µ–ª—å –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –Ω–∞–∏–ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —ç—Ç–∞–ª–æ–Ω–Ω—ã—Ö —Ç–µ—Å—Ç–∞—Ö –ø–æ –ø–æ–Ω–∏–º–∞–Ω–∏—é 3D-—Å—Ü–µ–Ω.'}, 'en': {'title': 'Revolutionizing 3D Scene Understanding with Video-3D LLM', 'desc': 'This paper introduces a new model called Video-3D LLM, designed to improve understanding of 3D scenes by treating them like dynamic videos. The model incorporates 3D position encoding to better align video representations with real-world spatial contexts, addressing the limitations of existing Multimodal Large Language Models (MLLMs) that primarily learn from 2D data. To enhance efficiency, a maximum coverage sampling technique is used, balancing computational costs with performance. The results show that Video-3D LLM achieves state-of-the-art performance on multiple benchmarks for 3D scene understanding.'}, 'zh': {'title': 'ÊèêÂçá3DÂú∫ÊôØÁêÜËß£ÁöÑÂàõÊñ∞Ê®°Âûã', 'desc': 'ËøôÁØáËÆ∫Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞ÂûãÁöÑÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMÔºâÔºåÁß∞‰∏∫Video-3D LLMÔºåÊó®Âú®ÊèêÈ´ò3DÂú∫ÊôØÁêÜËß£ËÉΩÂäõ„ÄÇ‰º†ÁªüÁöÑMLLM‰∏ªË¶ÅÂü∫‰∫é2DÊï∞ÊçÆËÆ≠ÁªÉÔºåÂØºËá¥ÂÆÉ‰ª¨Âú®Â§ÑÁêÜ3DÁéØÂ¢ÉÊó∂Â≠òÂú®Â±ÄÈôêÊÄß„ÄÇÈÄöËøáÂ∞Ü3DÂú∫ÊôØËßÜ‰∏∫Âä®ÊÄÅËßÜÈ¢ëÔºåÂπ∂ÂºïÂÖ•3D‰ΩçÁΩÆÁºñÁ†ÅÔºåVideo-3D LLMËÉΩÂ§üÊõ¥ÂáÜÁ°ÆÂú∞ÂØπÈΩêËßÜÈ¢ëË°®Á§∫‰∏éÁé∞ÂÆû‰∏ñÁïåÁöÑÁ©∫Èó¥‰∏ä‰∏ãÊñá„ÄÇÊ≠§Â§ñÔºåËÆ∫ÊñáËøòÊèêÂá∫‰∫Ü‰∏ÄÁßçÊúÄÂ§ßË¶ÜÁõñÈááÊ†∑ÊäÄÊúØÔºå‰ª•‰ºòÂåñËÆ°ÁÆóÊàêÊú¨ÂíåÊÄßËÉΩÊïàÁéá‰πãÈó¥ÁöÑÂπ≥Ë°°„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2412.03205', 'title': 'U-MATH: A University-Level Benchmark for Evaluating Mathematical Skills in LLMs', 'url': 'https://huggingface.co/papers/2412.03205', 'abstract': "The current evaluation of mathematical skills in LLMs is limited, as existing benchmarks are either relatively small, primarily focus on elementary and high-school problems, or lack diversity in topics. Additionally, the inclusion of visual elements in tasks remains largely under-explored.   To address these gaps, we introduce U-MATH, a novel benchmark of 1,100 unpublished open-ended university-level problems sourced from teaching materials. It is balanced across six core subjects, with 20% of multimodal problems. Given the open-ended nature of U-MATH problems, we employ an LLM to judge the correctness of generated solutions. To this end, we release mu-MATH, a dataset to evaluate the LLMs' capabilities in judging solutions.   The evaluation of general domain, math-specific, and multimodal LLMs highlights the challenges presented by U-MATH. Our findings reveal that LLMs achieve a maximum accuracy of only 63% on text-based tasks, with even lower 45% on visual problems. The solution assessment proves challenging for LLMs, with the best LLM judge having an F1-score of 80% on mu-MATH.", 'score': 13, 'issue_id': 968, 'pub_date': '2024-12-04', 'pub_date_card': {'ru': '4 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 4', 'zh': '12Êúà4Êó•'}, 'hash': '8df63a02d444d462', 'authors': ['Konstantin Chernyshev', 'Vitaliy Polshkov', 'Ekaterina Artemova', 'Alex Myasnikov', 'Vlad Stepanov', 'Alexei Miasnikov', 'Sergei Tilga'], 'affiliations': ['Gradarius', 'Stevens Institute of Technology', 'Toloka AI'], 'pdf_title_img': 'assets/pdf/title_img/2412.03205.jpg', 'data': {'categories': ['#multimodal', '#dataset', '#math', '#science', '#benchmark'], 'emoji': 'üßÆ', 'ru': {'title': 'U-MATH: –Ω–æ–≤—ã–π —Ä—É–±–µ–∂ –≤ –æ—Ü–µ–Ω–∫–µ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –ò–ò', 'desc': '–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ U-MATH –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –û–Ω —Å–æ–¥–µ—Ä–∂–∏—Ç 1100 –∑–∞–¥–∞—á —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç—Å–∫–æ–≥–æ —É—Ä–æ–≤–Ω—è –ø–æ —à–µ—Å—Ç–∏ –æ—Å–Ω–æ–≤–Ω—ã–º –ø—Ä–µ–¥–º–µ—Ç–∞–º, –≤–∫–ª—é—á–∞—è 20% –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á. –î–ª—è –æ—Ü–µ–Ω–∫–∏ —Ä–µ—à–µ–Ω–∏–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å, –¥–ª—è —á–µ–≥–æ –±—ã–ª —Å–æ–∑–¥–∞–Ω –¥–∞—Ç–∞—Å–µ—Ç mu-MATH. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ LLM –¥–æ—Å—Ç–∏–≥–∞—é—Ç —Ç–æ—á–Ω–æ—Å—Ç–∏ –ª–∏—à—å 63% –Ω–∞ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∏ 45% –Ω–∞ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö U-MATH, –∞ –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å-–æ—Ü–µ–Ω—â–∏–∫ –∏–º–µ–µ—Ç F1-–º–µ—Ä—É 80% –Ω–∞ mu-MATH.'}, 'en': {'title': 'U-MATH: Elevating Math Evaluation for LLMs', 'desc': 'This paper presents U-MATH, a new benchmark designed to evaluate the mathematical skills of large language models (LLMs) using 1,100 open-ended university-level problems. The benchmark covers six core subjects and includes multimodal tasks, which incorporate visual elements, addressing the limitations of existing evaluations. To assess the performance of LLMs on these problems, the authors introduce mu-MATH, a dataset specifically for evaluating the correctness of solutions generated by LLMs. The results indicate that LLMs struggle with these tasks, achieving only 63% accuracy on text-based problems and 45% on visual ones, highlighting the need for further advancements in LLM capabilities.'}, 'zh': {'title': 'U-MATHÔºöÊèêÂçáLLMsÊï∞Â≠¶ËÉΩÂäõËØÑ‰º∞ÁöÑÊñ∞Âü∫ÂáÜ', 'desc': 'ÁõÆÂâçÂØπÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÊï∞Â≠¶ÊäÄËÉΩÁöÑËØÑ‰º∞Â≠òÂú®Â±ÄÈôêÊÄßÔºåÁé∞ÊúâÂü∫ÂáÜÊµãËØïÁõ∏ÂØπËæÉÂ∞èÔºå‰∏ªË¶ÅÈõÜ‰∏≠Âú®Âü∫Á°ÄÂíåÈ´ò‰∏≠ÈóÆÈ¢ò‰∏äÔºå‰∏îÁº∫‰πè‰∏ªÈ¢òÂ§öÊ†∑ÊÄß„ÄÇÊ≠§Â§ñÔºå‰ªªÂä°‰∏≠ËßÜËßâÂÖÉÁ¥†ÁöÑÂåÖÂê´‰ªçÁÑ∂Êú™ÂæóÂà∞ÂÖÖÂàÜÊé¢Á¥¢„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÊàë‰ª¨ÂºïÂÖ•‰∫ÜU-MATHÔºåËøôÊòØ‰∏Ä‰∏™ÂåÖÂê´1100‰∏™Êú™ÂèëË°®ÁöÑÂºÄÊîæÂºèÂ§ßÂ≠¶Á∫ßÈóÆÈ¢òÁöÑÊñ∞Âü∫ÂáÜÔºåÊ∂µÁõñÂÖ≠‰∏™Ê†∏ÂøÉÂ≠¶ÁßëÔºåÂÖ∂‰∏≠20%ÁöÑÈóÆÈ¢ò‰∏∫Â§öÊ®°ÊÄÅÈóÆÈ¢ò„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåLLMsÂú®ÊñáÊú¨‰ªªÂä°‰∏äÁöÑÊúÄÈ´òÂáÜÁ°ÆÁéá‰ªÖ‰∏∫63%ÔºåËÄåÂú®ËßÜËßâÈóÆÈ¢ò‰∏äÁöÑÂáÜÁ°ÆÁéáÊõ¥‰ΩéÔºå‰ªÖ‰∏∫45%„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2411.19103', 'title': 'VARCO-VISION: Expanding Frontiers in Korean Vision-Language Models', 'url': 'https://huggingface.co/papers/2411.19103', 'abstract': "In this paper, we introduce an open-source Korean-English vision-language model (VLM), VARCO-VISION. We incorporate a step-by-step training strategy that allows a model learn both linguistic and visual information while preserving the backbone model's knowledge. Our model demonstrates outstanding performance in diverse settings requiring bilingual image-text understanding and generation abilities compared to models of similar size. VARCO-VISION is also capable of grounding, referring, and OCR, expanding its usage and potential applications for real-world scenarios. In addition to the model, we release five Korean evaluation datasets, including four closed-set and one openset benchmarks. We anticipate that our milestone will broaden the opportunities for AI researchers aiming to train VLMs. VARCO-VISION is available at https://huggingface.co/NCSOFT/VARCO-VISION-14B.", 'score': 13, 'issue_id': 964, 'pub_date': '2024-11-28', 'pub_date_card': {'ru': '28 –Ω–æ—è–±—Ä—è', 'en': 'November 28', 'zh': '11Êúà28Êó•'}, 'hash': '4507a3a2ac0bc8b5', 'authors': ['Jeongho Ju', 'Daeyoung Kim', 'SunYoung Park', 'Youngjune Kim'], 'affiliations': ['NC Research, NCSOFT'], 'pdf_title_img': 'assets/pdf/title_img/2411.19103.jpg', 'data': {'categories': ['#open_source', '#dataset', '#multimodal', '#training', '#low_resource'], 'emoji': 'üåè', 'ru': {'title': 'VARCO-VISION: –ü—Ä–æ—Ä—ã–≤ –≤ –¥–≤—É—è–∑—ã—á–Ω–æ–º –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–º –∑—Ä–µ–Ω–∏–∏', 'desc': '–í —ç—Ç–æ–π —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–∞—è –º—É–ª—å—Ç–∏—è–∑—ã—á–Ω–∞—è –º–æ–¥–µ–ª—å –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è VARCO-VISION –¥–ª—è –∫–æ—Ä–µ–π—Å–∫–æ–≥–æ –∏ –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ —è–∑—ã–∫–æ–≤. –ê–≤—Ç–æ—Ä—ã –ø—Ä–∏–º–µ–Ω–∏–ª–∏ –ø–æ—à–∞–≥–æ–≤—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –æ–±—É—á–µ–Ω–∏—è, –ø–æ–∑–≤–æ–ª—è—é—â—É—é –º–æ–¥–µ–ª–∏ —É—Å–≤–∞–∏–≤–∞—Ç—å –∫–∞–∫ –ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫—É—é, —Ç–∞–∫ –∏ –≤–∏–∑—É–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é. VARCO-VISION –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –≤—ã—Å–æ–∫—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö, —Ç—Ä–µ–±—É—é—â–∏—Ö –¥–≤—É—è–∑—ã—á–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –ú–æ–¥–µ–ª—å —Ç–∞–∫–∂–µ —Å–ø–æ—Å–æ–±–Ω–∞ –≤—ã–ø–æ–ª–Ω—è—Ç—å –∑–∞–¥–∞—á–∏ –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤, —Ä–µ—Ñ–µ—Ä–µ–Ω—Ü–∏–∏ –∏ –æ–ø—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Å–∏–º–≤–æ–ª–æ–≤, —á—Ç–æ —Ä–∞—Å—à–∏—Ä—è–µ—Ç –µ–µ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö.'}, 'en': {'title': 'VARCO-VISION: Bridging Korean and English through Vision-Language Learning', 'desc': 'This paper presents VARCO-VISION, an open-source vision-language model designed for Korean-English tasks. It employs a step-by-step training approach that effectively integrates linguistic and visual information while maintaining the foundational knowledge of the backbone model. The model excels in bilingual image-text understanding and generation, outperforming similar-sized models in various applications. Additionally, VARCO-VISION supports grounding, referring, and optical character recognition (OCR), and the authors provide five Korean evaluation datasets to facilitate further research in this area.'}, 'zh': {'title': 'VARCO-VISIONÔºöÂèåËØ≠ËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÁöÑÊñ∞ÈáåÁ®ãÁ¢ë', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂºÄÊ∫êÁöÑÈü©Ëã±ËßÜËßâËØ≠Ë®ÄÊ®°ÂûãVARCO-VISION„ÄÇÊàë‰ª¨ÈááÁî®ÈÄêÊ≠•ËÆ≠ÁªÉÁ≠ñÁï•Ôºå‰ΩøÊ®°ÂûãËÉΩÂ§üÂêåÊó∂Â≠¶‰π†ËØ≠Ë®ÄÂíåËßÜËßâ‰ø°ÊÅØÔºåÂêåÊó∂‰øùÁïôÂü∫Á°ÄÊ®°ÂûãÁöÑÁü•ËØÜ„ÄÇ‰∏éÂêåÁ±ªÊ®°ÂûãÁõ∏ÊØîÔºåVARCO-VISIONÂú®ÂèåËØ≠ÂõæÂÉèÊñáÊú¨ÁêÜËß£ÂíåÁîüÊàêËÉΩÂäõÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤„ÄÇËØ•Ê®°ÂûãËøòÂÖ∑Â§áÂÆö‰Ωç„ÄÅÂºïÁî®ÂíåÂÖâÂ≠¶Â≠óÁ¨¶ËØÜÂà´ÔºàOCRÔºâÂäüËÉΩÔºåÊâ©Â±ï‰∫ÜÂÖ∂Âú®Áé∞ÂÆûÂú∫ÊôØ‰∏≠ÁöÑÂ∫îÁî®ÊΩúÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2412.03517', 'title': 'NVComposer: Boosting Generative Novel View Synthesis with Multiple Sparse and Unposed Images', 'url': 'https://huggingface.co/papers/2412.03517', 'abstract': 'Recent advancements in generative models have significantly improved novel view synthesis (NVS) from multi-view data. However, existing methods depend on external multi-view alignment processes, such as explicit pose estimation or pre-reconstruction, which limits their flexibility and accessibility, especially when alignment is unstable due to insufficient overlap or occlusions between views. In this paper, we propose NVComposer, a novel approach that eliminates the need for explicit external alignment. NVComposer enables the generative model to implicitly infer spatial and geometric relationships between multiple conditional views by introducing two key components: 1) an image-pose dual-stream diffusion model that simultaneously generates target novel views and condition camera poses, and 2) a geometry-aware feature alignment module that distills geometric priors from dense stereo models during training. Extensive experiments demonstrate that NVComposer achieves state-of-the-art performance in generative multi-view NVS tasks, removing the reliance on external alignment and thus improving model accessibility. Our approach shows substantial improvements in synthesis quality as the number of unposed input views increases, highlighting its potential for more flexible and accessible generative NVS systems.', 'score': 13, 'issue_id': 960, 'pub_date': '2024-12-04', 'pub_date_card': {'ru': '4 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 4', 'zh': '12Êúà4Êó•'}, 'hash': '9d51bf0b60be344b', 'authors': ['Lingen Li', 'Zhaoyang Zhang', 'Yaowei Li', 'Jiale Xu', 'Xiaoyu Li', 'Wenbo Hu', 'Weihao Cheng', 'Jinwei Gu', 'Tianfan Xue', 'Ying Shan'], 'affiliations': ['ARC Lab, Tencent PCG', 'Peking University', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2412.03517.jpg', 'data': {'categories': ['#optimization', '#3d', '#diffusion', '#cv'], 'emoji': 'üé•', 'ru': {'title': '–°–∏–Ω—Ç–µ–∑ –Ω–æ–≤—ã—Ö —Ä–∞–∫—É—Ä—Å–æ–≤ –±–µ–∑ —è–≤–Ω–æ–≥–æ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –≤–∏–¥–æ–≤', 'desc': 'NVComposer - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–∏–Ω—Ç–µ–∑—É –Ω–æ–≤—ã—Ö —Ä–∞–∫—É—Ä—Å–æ–≤, –∫–æ—Ç–æ—Ä—ã–π —É—Å—Ç—Ä–∞–Ω—è–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –≤–æ –≤–Ω–µ—à–Ω–µ–º –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–∏ –≤–∏–¥–æ–≤. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–≤—É—Ö–ø–æ—Ç–æ—á–Ω—É—é –º–æ–¥–µ–ª—å –¥–∏—Ñ—Ñ—É–∑–∏–∏ –¥–ª—è –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ü–µ–ª–µ–≤—ã—Ö —Ä–∞–∫—É—Ä—Å–æ–≤ –∏ –ø–æ–∑–∏—Ü–∏–π –∫–∞–º–µ—Ä. –ú–µ—Ç–æ–¥ –≤–∫–ª—é—á–∞–µ—Ç –º–æ–¥—É–ª—å –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å —É—á–µ—Ç–æ–º –≥–µ–æ–º–µ—Ç—Ä–∏–∏, –∫–æ—Ç–æ—Ä—ã–π –∏–∑–≤–ª–µ–∫–∞–µ—Ç –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–µ –∑–∞–∫–æ–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç–∏ –∏–∑ –ø–ª–æ—Ç–Ω—ã—Ö —Å—Ç–µ—Ä–µ–æ –º–æ–¥–µ–ª–µ–π –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ NVComposer –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –Ω–∞–∏–ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –∑–∞–¥–∞—á–∞—Ö –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –º–Ω–æ–≥–æ—Ä–∞–∫—É—Ä—Å–Ω–æ–≥–æ —Å–∏–Ω—Ç–µ–∑–∞ –Ω–æ–≤—ã—Ö –≤–∏–¥–æ–≤.'}, 'en': {'title': 'NVComposer: Generating Novel Views Without External Alignment', 'desc': 'This paper introduces NVComposer, a new method for generating novel views from multiple images without needing external alignment processes like pose estimation. NVComposer uses a dual-stream diffusion model that generates new views while also predicting camera poses, allowing for a more integrated approach. Additionally, it incorporates a geometry-aware feature alignment module that learns geometric information from stereo models during training. The results show that NVComposer outperforms existing methods, especially when there are many unaligned input views, making it a more flexible solution for novel view synthesis.'}, 'zh': {'title': 'NVComposerÔºöÊó†È°ªÂ§ñÈÉ®ÂØπÈΩêÁöÑÁîüÊàêÊñ∞ËßÜÂõæÂêàÊàê', 'desc': 'ÊúÄËøëÁîüÊàêÊ®°ÂûãÁöÑËøõÂ±ïÊòæËëóÊèêÂçá‰∫ÜÂ§öËßÜÂõæÊï∞ÊçÆÁöÑÊñ∞ÁöÑËßÜÂõæÂêàÊàêÔºàNVSÔºâËÉΩÂäõ„ÄÇÁÑ∂ËÄåÔºåÁé∞ÊúâÊñπÊ≥ï‰æùËµñ‰∫éÂ§ñÈÉ®ÁöÑÂ§öËßÜÂõæÂØπÈΩêËøáÁ®ãÔºåÂ¶ÇÊòæÂºèÁöÑÂßøÊÄÅ‰º∞ËÆ°ÊàñÈ¢ÑÈáçÂª∫ÔºåËøôÈôêÂà∂‰∫ÜÂÆÉ‰ª¨ÁöÑÁÅµÊ¥ªÊÄßÂíåÂèØËÆøÈóÆÊÄßÔºåÂ∞§ÂÖ∂ÊòØÂú®ËßÜÂõæ‰πãÈó¥ÈáçÂè†‰∏çË∂≥ÊàñÈÅÆÊå°Êó∂ÂØπÈΩê‰∏çÁ®≥ÂÆöÁöÑÊÉÖÂÜµ‰∏ã„ÄÇÊú¨ÊñáÊèêÂá∫‰∫ÜNVComposerÔºåËøôÊòØ‰∏ÄÁßçÊñ∞È¢ñÁöÑÊñπÊ≥ïÔºåÊ∂àÈô§‰∫ÜÂØπÊòæÂºèÂ§ñÈÉ®ÂØπÈΩêÁöÑÈúÄÊ±Ç„ÄÇNVComposerÈÄöËøáÂºïÂÖ•‰∏§‰∏™ÂÖ≥ÈîÆÁªÑ‰ª∂Ôºå‰ΩøÁîüÊàêÊ®°ÂûãËÉΩÂ§üÈöêÂºèÊé®Êñ≠Â§ö‰∏™Êù°‰ª∂ËßÜÂõæ‰πãÈó¥ÁöÑÁ©∫Èó¥ÂíåÂá†‰ΩïÂÖ≥Á≥ªÔºå‰ªéËÄåÂú®ÁîüÊàêÂ§öËßÜÂõæNVS‰ªªÂä°‰∏≠ÂÆûÁé∞‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2412.01106', 'title': 'One Shot, One Talk: Whole-body Talking Avatar from a Single Image', 'url': 'https://huggingface.co/papers/2412.01106', 'abstract': 'Building realistic and animatable avatars still requires minutes of multi-view or monocular self-rotating videos, and most methods lack precise control over gestures and expressions. To push this boundary, we address the challenge of constructing a whole-body talking avatar from a single image. We propose a novel pipeline that tackles two critical issues: 1) complex dynamic modeling and 2) generalization to novel gestures and expressions. To achieve seamless generalization, we leverage recent pose-guided image-to-video diffusion models to generate imperfect video frames as pseudo-labels. To overcome the dynamic modeling challenge posed by inconsistent and noisy pseudo-videos, we introduce a tightly coupled 3DGS-mesh hybrid avatar representation and apply several key regularizations to mitigate inconsistencies caused by imperfect labels. Extensive experiments on diverse subjects demonstrate that our method enables the creation of a photorealistic, precisely animatable, and expressive whole-body talking avatar from just a single image.', 'score': 13, 'issue_id': 957, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 2', 'zh': '12Êúà2Êó•'}, 'hash': '13d96f9bb346e344', 'authors': ['Jun Xiang', 'Yudong Guo', 'Leipeng Hu', 'Boyang Guo', 'Yancheng Yuan', 'Juyong Zhang'], 'affiliations': ['The Hong Kong Polytechnic University', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2412.01106.jpg', 'data': {'categories': ['#cv', '#optimization', '#multimodal', '#diffusion', '#3d'], 'emoji': 'ü§ñ', 'ru': {'title': '–†–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–π –≥–æ–≤–æ—Ä—è—â–∏–π –∞–≤–∞—Ç–∞—Ä –∏–∑ –æ–¥–Ω–æ–≥–æ —Ñ–æ—Ç–æ', 'desc': '–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —Å–æ–∑–¥–∞–Ω–∏—è —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö –∞–≤–∞—Ç–∞—Ä–æ–≤, —Å–ø–æ—Å–æ–±–Ω—ã—Ö –≥–æ–≤–æ—Ä–∏—Ç—å –∏ –¥–≤–∏–≥–∞—Ç—å—Å—è, –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—Å–µ–≥–æ –æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Å–µ–≤–¥–æ-–≤–∏–¥–µ–æ –∫–∞–¥—Ä–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ —Å–ª—É–∂–∞—Ç –æ–±—É—á–∞—é—â–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏. –û–Ω–∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –≥–∏–±—Ä–∏–¥–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –∞–≤–∞—Ç–∞—Ä–∞, —Å–æ—á–µ—Ç–∞—é—â–µ–µ 3D –≥–∞—É—Å—Å–æ–≤—ã —Å–ø–ª–∞—Ç—ã –∏ –ø–æ–ª–∏–≥–æ–Ω–∞–ª—å–Ω—É—é —Å–µ—Ç–∫—É. –ú–µ—Ç–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Ç–æ—á–Ω–æ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä–æ–≤–∞—Ç—å –∂–µ—Å—Ç—ã –∏ –º–∏–º–∏–∫—É –∞–≤–∞—Ç–∞—Ä–∞, –ø—Ä–µ–æ–¥–æ–ª–µ–≤–∞—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –ø–æ–¥—Ö–æ–¥–æ–≤.'}, 'en': {'title': 'From One Image to a Lifelike Talking Avatar!', 'desc': "This paper presents a new method for creating realistic and animatable whole-body talking avatars using only a single image. The authors address two main challenges: modeling complex movements and ensuring the avatar can perform new gestures and expressions. They utilize pose-guided image-to-video diffusion models to generate video frames that serve as training data, despite being imperfect. To improve the quality of the avatar's animations, they introduce a hybrid representation that combines 3D mesh structures with regularization techniques to handle inconsistencies in the generated video frames."}, 'zh': {'title': '‰ªéÂçïÂº†ÂõæÂÉèÁîüÊàêÂÖ®Ë∫´‰ºöËØ¥ËØùÁöÑËôöÊãüÂ§¥ÂÉè', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßç‰ªéÂçïÂº†ÂõæÂÉèÊûÑÂª∫ÂÖ®Ë∫´‰ºöËØ¥ËØùÁöÑËôöÊãüÂ§¥ÂÉèÁöÑÊñπÊ≥ï„ÄÇÊàë‰ª¨Ëß£ÂÜ≥‰∫ÜÂ§çÊùÇÂä®ÊÄÅÂª∫Ê®°ÂíåÂØπÊñ∞ÊâãÂäø‰∏éË°®ÊÉÖÁöÑÊ≥õÂåñËøô‰∏§‰∏™ÂÖ≥ÈîÆÈóÆÈ¢ò„ÄÇÈÄöËøá‰ΩøÁî®ÂßøÊÄÅÂºïÂØºÁöÑÂõæÂÉèÂà∞ËßÜÈ¢ëÊâ©Êï£Ê®°ÂûãÔºåÊàë‰ª¨ÁîüÊàê‰∫Ü‰∏çÂÆåÁæéÁöÑËßÜÈ¢ëÂ∏ß‰Ωú‰∏∫‰º™Ê†áÁ≠æÔºå‰ª•ÂÆûÁé∞Êó†ÁºùÊ≥õÂåñ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïËÉΩÂ§ü‰ªéÂçïÂº†ÂõæÂÉèÂàõÂª∫Âá∫ÈÄºÁúü„ÄÅÂèØÁ≤æÁ°ÆÂä®ÁîªÂíåÂØåÊúâË°®Áé∞ÂäõÁöÑÂÖ®Ë∫´ËôöÊãüÂ§¥ÂÉè„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2412.02030', 'title': 'NitroFusion: High-Fidelity Single-Step Diffusion through Dynamic Adversarial Training', 'url': 'https://huggingface.co/papers/2412.02030', 'abstract': 'We introduce NitroFusion, a fundamentally different approach to single-step diffusion that achieves high-quality generation through a dynamic adversarial framework. While one-step methods offer dramatic speed advantages, they typically suffer from quality degradation compared to their multi-step counterparts. Just as a panel of art critics provides comprehensive feedback by specializing in different aspects like composition, color, and technique, our approach maintains a large pool of specialized discriminator heads that collectively guide the generation process. Each discriminator group develops expertise in specific quality aspects at different noise levels, providing diverse feedback that enables high-fidelity one-step generation. Our framework combines: (i) a dynamic discriminator pool with specialized discriminator groups to improve generation quality, (ii) strategic refresh mechanisms to prevent discriminator overfitting, and (iii) global-local discriminator heads for multi-scale quality assessment, and unconditional/conditional training for balanced generation. Additionally, our framework uniquely supports flexible deployment through bottom-up refinement, allowing users to dynamically choose between 1-4 denoising steps with the same model for direct quality-speed trade-offs. Through comprehensive experiments, we demonstrate that NitroFusion significantly outperforms existing single-step methods across multiple evaluation metrics, particularly excelling in preserving fine details and global consistency.', 'score': 11, 'issue_id': 966, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 2', 'zh': '12Êúà2Êó•'}, 'hash': '4c749ff913210111', 'authors': ['Dar-Yen Chen', 'Hmrishav Bandyopadhyay', 'Kai Zou', 'Yi-Zhe Song'], 'affiliations': ['NetMind.AI', 'SketchX, CVSSP, University of Surrey'], 'pdf_title_img': 'assets/pdf/title_img/2412.02030.jpg', 'data': {'categories': ['#diffusion', '#optimization', '#training', '#architecture', '#benchmark'], 'emoji': 'üé®', 'ru': {'title': 'NitroFusion: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ–¥–Ω–æ—à–∞–≥–æ–≤–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π', 'desc': 'NitroFusion - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–¥–Ω–æ—à–∞–≥–æ–≤–æ–π –¥–∏—Ñ—Ñ—É–∑–∏–∏, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –¥–∏–Ω–∞–º–∏—á–µ—Å–∫—É—é —Å–æ—Å—Ç—è–∑–∞—Ç–µ–ª—å–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –ú–µ—Ç–æ–¥ –ø—Ä–∏–º–µ–Ω—è–µ—Ç –±–æ–ª—å—à–æ–π –ø—É–ª —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ç–æ—Ä–æ–≤, –∫–∞–∂–¥—ã–π –∏–∑ –∫–æ—Ç–æ—Ä—ã—Ö —Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è –Ω–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–º –∞—Å–ø–µ–∫—Ç–µ –∫–∞—á–µ—Å—Ç–≤–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —É—Ä–æ–≤–Ω—è—Ö —à—É–º–∞. NitroFusion –≤–∫–ª—é—á–∞–µ—Ç –º–µ—Ö–∞–Ω–∏–∑–º—ã –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ç–æ—Ä–æ–≤ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –∏ –≥–ª–æ–±–∞–ª—å–Ω–æ-–ª–æ–∫–∞–ª—å–Ω—ã–µ –≥–æ–ª–æ–≤–∫–∏ –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ç–æ—Ä–æ–≤ –¥–ª—è –º–Ω–æ–≥–æ–º–∞—Å—à—Ç–∞–±–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞. –ü–æ–¥—Ö–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –≥–∏–±–∫–æ –≤—ã–±–∏—Ä–∞—Ç—å –æ—Ç 1 –¥–æ 4 —à–∞–≥–æ–≤ –¥–µ–Ω–æ–π–∑–∏–Ω–≥–∞, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è –∫–æ–º–ø—Ä–æ–º–∏—Å—Å –º–µ–∂–¥—É –∫–∞—á–µ—Å—Ç–≤–æ–º –∏ —Å–∫–æ—Ä–æ—Å—Ç—å—é.'}, 'en': {'title': 'NitroFusion: Fast and High-Quality Image Generation with Dynamic Discriminators', 'desc': 'NitroFusion is a new method for generating images quickly while maintaining high quality. It uses a dynamic adversarial framework with multiple specialized discriminator heads that focus on different quality aspects, similar to art critics. This approach allows for better feedback during the generation process, leading to improved fidelity in one-step generation. Additionally, NitroFusion offers flexible options for users to balance speed and quality by adjusting the number of denoising steps used.'}, 'zh': {'title': 'NitroFusionÔºöÈ´òÊïà‰∏éÈ´òË¥®ÈáèÁîüÊàêÁöÑÂÆåÁæéÁªìÂêà', 'desc': 'NitroFusionÊòØ‰∏ÄÁßçÂÖ®Êñ∞ÁöÑÂçïÊ≠•Êâ©Êï£ÁîüÊàêÊñπÊ≥ïÔºåÈÄöËøáÂä®ÊÄÅÂØπÊäóÊ°ÜÊû∂ÂÆûÁé∞È´òË¥®ÈáèÁîüÊàê„ÄÇ‰∏é‰º†ÁªüÁöÑÂçïÊ≠•ÊñπÊ≥ïÁõ∏ÊØîÔºåNitroFusionÂú®ÁîüÊàêË¥®Èáè‰∏äÊúâÊòæËëóÊèêÂçáÔºåÂ∞ΩÁÆ°ÂçïÊ≠•ÊñπÊ≥ïÂú®ÈÄüÂ∫¶‰∏äÂÖ∑Êúâ‰ºòÂäø„ÄÇËØ•ÊñπÊ≥ïÂà©Áî®Â§ö‰∏™‰∏ì‰∏öÁöÑÂà§Âà´Âô®ÁªÑÔºåÈíàÂØπ‰∏çÂêåÁöÑÂô™Â£∞Ê∞¥Âπ≥Êèê‰æõÂ§öÊ†∑ÂåñÁöÑÂèçÈ¶àÔºå‰ªéËÄåÊèêÈ´òÁîüÊàêÁöÑ‰øùÁúüÂ∫¶„ÄÇÈÄöËøáÁÅµÊ¥ªÁöÑÈÉ®ÁΩ≤Êú∫Âà∂ÔºåÁî®Êà∑ÂèØ‰ª•Ê†πÊçÆÈúÄË¶ÅÂú®1Âà∞4‰∏™ÂéªÂô™Ê≠•È™§‰πãÈó¥Âä®ÊÄÅÈÄâÊã©ÔºåÂÆûÁé∞Ë¥®Èáè‰∏éÈÄüÂ∫¶ÁöÑÂπ≥Ë°°„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2412.03558', 'title': 'MIDI: Multi-Instance Diffusion for Single Image to 3D Scene Generation', 'url': 'https://huggingface.co/papers/2412.03558', 'abstract': 'This paper introduces MIDI, a novel paradigm for compositional 3D scene generation from a single image. Unlike existing methods that rely on reconstruction or retrieval techniques or recent approaches that employ multi-stage object-by-object generation, MIDI extends pre-trained image-to-3D object generation models to multi-instance diffusion models, enabling the simultaneous generation of multiple 3D instances with accurate spatial relationships and high generalizability. At its core, MIDI incorporates a novel multi-instance attention mechanism, that effectively captures inter-object interactions and spatial coherence directly within the generation process, without the need for complex multi-step processes. The method utilizes partial object images and global scene context as inputs, directly modeling object completion during 3D generation. During training, we effectively supervise the interactions between 3D instances using a limited amount of scene-level data, while incorporating single-object data for regularization, thereby maintaining the pre-trained generalization ability. MIDI demonstrates state-of-the-art performance in image-to-scene generation, validated through evaluations on synthetic data, real-world scene data, and stylized scene images generated by text-to-image diffusion models.', 'score': 10, 'issue_id': 957, 'pub_date': '2024-12-04', 'pub_date_card': {'ru': '4 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 4', 'zh': '12Êúà4Êó•'}, 'hash': '5e1a4c1e1017e7af', 'authors': ['Zehuan Huang', 'Yuan-Chen Guo', 'Xingqiao An', 'Yunhan Yang', 'Yangguang Li', 'Zi-Xin Zou', 'Ding Liang', 'Xihui Liu', 'Yan-Pei Cao', 'Lu Sheng'], 'affiliations': ['Beihang University', 'The University of Hong Kong', 'Tsinghua University', 'VAST'], 'pdf_title_img': 'assets/pdf/title_img/2412.03558.jpg', 'data': {'categories': ['#cv', '#synthetic', '#diffusion', '#training', '#3d'], 'emoji': 'üèôÔ∏è', 'ru': {'title': 'MIDI: –†–µ–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ 3D-—Å—Ü–µ–Ω –∏–∑ –æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç MIDI - –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç—Ä–µ—Ö–º–µ—Ä–Ω—ã—Ö —Å—Ü–µ–Ω –∏–∑ –æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è. MIDI –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ 3D-–æ–±—ä–µ–∫—Ç–æ–≤ –∏ —Ä–∞—Å—à–∏—Ä—è–µ—Ç –∏—Ö –¥–æ –º–Ω–æ–≥–æ—ç–∫–∑–µ–º–ø–ª—è—Ä–Ω—ã—Ö –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –ø–æ–∑–≤–æ–ª—è—è –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ 3D-–æ–±—ä–µ–∫—Ç–æ–≤ —Å —Ç–æ—á–Ω—ã–º–∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –æ—Ç–Ω–æ—à–µ–Ω–∏—è–º–∏. –ö–ª—é—á–µ–≤–æ–π –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç—å—é —è–≤–ª—è–µ—Ç—Å—è –Ω–æ–≤—ã–π –º–µ—Ö–∞–Ω–∏–∑–º –º–Ω–æ–≥–æ—ç–∫–∑–µ–º–ø–ª—è—Ä–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ —É—á–∏—Ç—ã–≤–∞–µ—Ç –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –º–µ–∂–¥—É –æ–±—ä–µ–∫—Ç–∞–º–∏ –∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—É—é —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –Ω–µ–ø–æ—Å—Ä–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏. MIDI –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø–µ—Ä–µ–¥–æ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å—Ü–µ–Ω –∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, —á—Ç–æ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç—Å—è –æ—Ü–µ–Ω–∫–∞–º–∏ –Ω–∞ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö, —Ä–µ–∞–ª—å–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ö –∏ —Å—Ç–∏–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö.'}, 'en': {'title': 'MIDI: Revolutionizing 3D Scene Generation from Single Images', 'desc': 'This paper presents MIDI, a new approach for creating 3D scenes from a single image. It improves upon traditional methods by using multi-instance diffusion models, allowing for the generation of multiple 3D objects at once while maintaining their spatial relationships. MIDI features a unique multi-instance attention mechanism that captures how objects interact and fit together in space, simplifying the generation process. The method is trained with a combination of scene-level and single-object data, ensuring high performance and generalization across various types of scenes.'}, 'zh': {'title': 'MIDIÔºö‰ªéÂçïÂõæÂÉèÁîüÊàê3DÂú∫ÊôØÁöÑÊñ∞ÊñπÊ≥ï', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂêç‰∏∫MIDIÁöÑÊñ∞ÊñπÊ≥ïÔºåÁî®‰∫é‰ªéÂçïÂº†ÂõæÂÉèÁîüÊàêÁªÑÂêà3DÂú∫ÊôØ„ÄÇ‰∏éÁé∞Êúâ‰æùËµñÈáçÂª∫ÊàñÊ£ÄÁ¥¢ÊäÄÊúØÁöÑÊñπÊ≥ï‰∏çÂêåÔºåMIDIÊâ©Â±ï‰∫ÜÈ¢ÑËÆ≠ÁªÉÁöÑÂõæÂÉèÂà∞3DÂØπË±°ÁîüÊàêÊ®°ÂûãÔºåÈááÁî®Â§öÂÆû‰æãÊâ©Êï£Ê®°ÂûãÔºåÂÆûÁé∞‰∫ÜÂ§ö‰∏™3DÂÆû‰æãÁöÑÂêåÊó∂ÁîüÊàê„ÄÇMIDIÁöÑÊ†∏ÂøÉÊòØ‰∏Ä‰∏™Êñ∞È¢ñÁöÑÂ§öÂÆû‰æãÊ≥®ÊÑèÊú∫Âà∂ÔºåËÉΩÂ§üÊúâÊïàÊçïÊçâÂØπË±°Èó¥ÁöÑ‰∫§‰∫íÂíåÁ©∫Èó¥‰∏ÄËá¥ÊÄßÔºåÁÆÄÂåñ‰∫ÜÁîüÊàêËøáÁ®ã„ÄÇËØ•ÊñπÊ≥ïÂú®ÂõæÂÉèÂà∞Âú∫ÊôØÁîüÊàêÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤ÔºåÁªèËøáÂêàÊàêÊï∞ÊçÆ„ÄÅÁúüÂÆûÂú∫ÊôØÊï∞ÊçÆÂíåÊñáÊú¨Âà∞ÂõæÂÉèÊâ©Êï£Ê®°ÂûãÁîüÊàêÁöÑÈ£éÊ†ºÂåñÂú∫ÊôØÂõæÂÉèÁöÑËØÑ‰º∞È™åËØÅ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2412.03439', 'title': 'CleanDIFT: Diffusion Features without Noise', 'url': 'https://huggingface.co/papers/2412.03439', 'abstract': 'Internal features from large-scale pre-trained diffusion models have recently been established as powerful semantic descriptors for a wide range of downstream tasks. Works that use these features generally need to add noise to images before passing them through the model to obtain the semantic features, as the models do not offer the most useful features when given images with little to no noise. We show that this noise has a critical impact on the usefulness of these features that cannot be remedied by ensembling with different random noises. We address this issue by introducing a lightweight, unsupervised fine-tuning method that enables diffusion backbones to provide high-quality, noise-free semantic features. We show that these features readily outperform previous diffusion features by a wide margin in a wide variety of extraction setups and downstream tasks, offering better performance than even ensemble-based methods at a fraction of the cost.', 'score': 9, 'issue_id': 963, 'pub_date': '2024-12-04', 'pub_date_card': {'ru': '4 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 4', 'zh': '12Êúà4Êó•'}, 'hash': 'cd474064bf17503a', 'authors': ['Nick Stracke', 'Stefan Andreas Baumann', 'Kolja Bauer', 'Frank Fundel', 'Bj√∂rn Ommer'], 'affiliations': ['CompVis @ LMU Munich'], 'pdf_title_img': 'assets/pdf/title_img/2412.03439.jpg', 'data': {'categories': ['#data', '#cv', '#diffusion', '#training', '#optimization'], 'emoji': 'üé®', 'ru': {'title': '–£–ª—É—á—à–µ–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –±–µ–∑ —à—É–º–∞', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏, –∏–∑–≤–ª–µ–∫–∞–µ–º—ã–µ –∏–∑ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, —è–≤–ª—è—é—Ç—Å—è –º–æ—â–Ω—ã–º–∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–º–∏ –¥–µ—Å–∫—Ä–∏–ø—Ç–æ—Ä–∞–º–∏ –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á. –û–¥–Ω–∞–∫–æ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ —à—É–º–∞ –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º –ø–µ—Ä–µ–¥ –∏—Ö –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –º–æ–¥–µ–ª—å—é –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–ª–∏—è–µ—Ç –Ω–∞ –ø–æ–ª–µ–∑–Ω–æ—Å—Ç—å —ç—Ç–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥ –ª–µ–≥–∫–æ–≤–µ—Å–Ω–æ–π –Ω–µ–∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–π –¥–æ–Ω–∞—Å—Ç—Ä–æ–π–∫–∏, –ø–æ–∑–≤–æ–ª—è—é—â–∏–π –ø–æ–ª—É—á–∞—Ç—å –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –±–µ–∑ —à—É–º–∞. –≠—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏–µ –ø–æ–¥—Ö–æ–¥—ã –ø–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö –ø—Ä–∏ –º–µ–Ω—å—à–∏—Ö –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–∞—Ç—Ä–∞—Ç–∞—Ö.'}, 'en': {'title': 'Unlocking Noise-Free Semantic Features from Diffusion Models', 'desc': 'This paper discusses how internal features from large pre-trained diffusion models can be used as effective semantic descriptors for various tasks. It highlights the problem that these models require added noise to generate useful features, which limits their effectiveness. The authors propose a new, lightweight, unsupervised fine-tuning method that allows these models to produce high-quality semantic features without the need for noise. Their approach significantly improves performance across multiple tasks compared to previous methods, including ensemble techniques, while being more efficient.'}, 'zh': {'title': 'Êó†Âô™Â£∞ÁöÑÈ´òË¥®ÈáèËØ≠‰πâÁâπÂæÅÊèêÂèñ', 'desc': 'ÊúÄËøëÔºåÂ§ßËßÑÊ®°È¢ÑËÆ≠ÁªÉÊâ©Êï£Ê®°ÂûãÁöÑÂÜÖÈÉ®ÁâπÂæÅË¢´Á°ÆÁ´ã‰∏∫Âº∫Â§ßÁöÑËØ≠‰πâÊèèËø∞Á¨¶ÔºåÈÄÇÁî®‰∫éÂ§öÁßç‰∏ãÊ∏∏‰ªªÂä°„ÄÇÈÄöÂ∏∏ÔºåËøô‰∫õÁâπÂæÅÈúÄË¶ÅÂú®ÂõæÂÉè‰∏≠Ê∑ªÂä†Âô™Â£∞ÂêéÊâçËÉΩÊèêÂèñÔºåÂõ†‰∏∫Ê®°ÂûãÂú®Â§ÑÁêÜÂá†‰πéÊ≤°ÊúâÂô™Â£∞ÁöÑÂõæÂÉèÊó∂ÔºåÊèê‰æõÁöÑÁâπÂæÅÊïàÊûú‰∏ç‰Ω≥„ÄÇÊàë‰ª¨ÂèëÁé∞Âô™Â£∞ÂØπÁâπÂæÅÁöÑÊúâÊïàÊÄßÊúâÈáçË¶ÅÂΩ±ÂìçÔºå‰∏îÈÄöËøá‰∏çÂêåÈöèÊú∫Âô™Â£∞ÁöÑÈõÜÊàêÊó†Ê≥ïËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢ò„ÄÇ‰∏∫Ê≠§ÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçËΩªÈáèÁ∫ßÁöÑÊó†ÁõëÁù£ÂæÆË∞ÉÊñπÊ≥ïÔºå‰ΩøÊâ©Êï£Ê®°ÂûãËÉΩÂ§üÊèê‰æõÈ´òË¥®Èáè„ÄÅÊó†Âô™Â£∞ÁöÑËØ≠‰πâÁâπÂæÅÔºåÊòæËëóË∂ÖË∂ä‰∫Ü‰πãÂâçÁöÑÊâ©Êï£ÁâπÂæÅ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2412.03085', 'title': 'Mimir: Improving Video Diffusion Models for Precise Text Understanding', 'url': 'https://huggingface.co/papers/2412.03085', 'abstract': 'Text serves as the key control signal in video generation due to its narrative nature. To render text descriptions into video clips, current video diffusion models borrow features from text encoders yet struggle with limited text comprehension. The recent success of large language models (LLMs) showcases the power of decoder-only transformers, which offers three clear benefits for text-to-video (T2V) generation, namely, precise text understanding resulting from the superior scalability, imagination beyond the input text enabled by next token prediction, and flexibility to prioritize user interests through instruction tuning. Nevertheless, the feature distribution gap emerging from the two different text modeling paradigms hinders the direct use of LLMs in established T2V models. This work addresses this challenge with Mimir, an end-to-end training framework featuring a carefully tailored token fuser to harmonize the outputs from text encoders and LLMs. Such a design allows the T2V model to fully leverage learned video priors while capitalizing on the text-related capability of LLMs. Extensive quantitative and qualitative results demonstrate the effectiveness of Mimir in generating high-quality videos with excellent text comprehension, especially when processing short captions and managing shifting motions. Project page: https://lucaria-academy.github.io/Mimir/', 'score': 7, 'issue_id': 957, 'pub_date': '2024-12-04', 'pub_date_card': {'ru': '4 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 4', 'zh': '12Êúà4Êó•'}, 'hash': 'a065164e5fdadf2c', 'authors': ['Shuai Tan', 'Biao Gong', 'Yutong Feng', 'Kecheng Zheng', 'Dandan Zheng', 'Shuwei Shi', 'Yujun Shen', 'Jingdong Chen', 'Ming Yang'], 'affiliations': ['Ant Group', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2412.03085.jpg', 'data': {'categories': ['#video', '#optimization', '#diffusion', '#multimodal', '#training'], 'emoji': 'üé¨', 'ru': {'title': 'Mimir: –£–ª—É—á—à–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Mimir - –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–∞. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (LLM) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–Ω–∏–º–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏ –≤–æ–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏. –ö–ª—é—á–µ–≤–æ–π —ç–ª–µ–º–µ–Ω—Ç Mimir - —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π 'token fuser', –∫–æ—Ç–æ—Ä—ã–π –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –≤—ã—Ö–æ–¥—ã —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —ç–Ω–∫–æ–¥–µ—Ä–æ–≤ –∏ LLM. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ Mimir —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–µ–Ω –≤ —Å–æ–∑–¥–∞–Ω–∏–∏ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ —Å —Ö–æ—Ä–æ—à–∏–º –ø–æ–Ω–∏–º–∞–Ω–∏–µ–º —Ç–µ–∫—Å—Ç–∞, –æ—Å–æ–±–µ–Ω–Ω–æ –¥–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö –æ–ø–∏—Å–∞–Ω–∏–π –∏ –¥–∏–Ω–∞–º–∏—á–Ω—ã—Ö —Å—Ü–µ–Ω."}, 'en': {'title': 'Mimir: Bridging Text Understanding and Video Generation', 'desc': 'This paper introduces Mimir, a new framework for text-to-video (T2V) generation that combines the strengths of text encoders and large language models (LLMs). It addresses the challenge of feature distribution gaps between these two text modeling approaches, which can hinder effective video generation. Mimir utilizes a specialized token fuser to integrate outputs from both models, enhancing text comprehension and video quality. The results show that Mimir excels in generating videos from short captions and effectively managing dynamic movements.'}, 'zh': {'title': 'MimirÔºöÊèêÂçáÊñáÊú¨Âà∞ËßÜÈ¢ëÁîüÊàêÁöÑÊô∫ËÉΩ', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫MimirÁöÑÁ´ØÂà∞Á´ØËÆ≠ÁªÉÊ°ÜÊû∂ÔºåÁî®‰∫éÊñáÊú¨Âà∞ËßÜÈ¢ëÁîüÊàêÔºàT2VÔºâ„ÄÇËØ•Ê°ÜÊû∂ÈÄöËøáÁ≤æÂøÉËÆæËÆ°ÁöÑ‰ª§ÁâåËûçÂêàÂô®ÔºåËß£ÂÜ≥‰∫ÜÊñáÊú¨ÁºñÁ†ÅÂô®‰∏éÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâ‰πãÈó¥ÁöÑÁâπÂæÅÂàÜÂ∏ÉÂ∑ÆË∑ù„ÄÇMimirËÉΩÂ§üÂÖÖÂàÜÂà©Áî®Â≠¶‰π†Âà∞ÁöÑËßÜÈ¢ëÂÖàÈ™åÔºåÂêåÊó∂Â¢ûÂº∫LLMsÂú®ÊñáÊú¨ÁêÜËß£ÊñπÈù¢ÁöÑËÉΩÂäõ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåMimirÂú®ÁîüÊàêÈ´òË¥®ÈáèËßÜÈ¢ëÊó∂ÔºåÂ∞§ÂÖ∂Âú®Â§ÑÁêÜÁü≠ÊñáÊú¨ÂíåÂä®ÊÄÅÂèòÂåñÊó∂ÔºåË°®Áé∞Âá∫Ëâ≤„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2412.02186', 'title': 'VideoICL: Confidence-based Iterative In-context Learning for Out-of-Distribution Video Understanding', 'url': 'https://huggingface.co/papers/2412.02186', 'abstract': 'Recent advancements in video large multimodal models (LMMs) have significantly improved their video understanding and reasoning capabilities. However, their performance drops on out-of-distribution (OOD) tasks that are underrepresented in training data. Traditional methods like fine-tuning on OOD datasets are impractical due to high computational costs. While In-context learning (ICL) with demonstration examples has shown promising generalization performance in language tasks and image-language tasks without fine-tuning, applying ICL to video-language tasks faces challenges due to the limited context length in Video LMMs, as videos require longer token lengths. To address these issues, we propose VideoICL, a novel video in-context learning framework for OOD tasks that introduces a similarity-based relevant example selection strategy and a confidence-based iterative inference approach. This allows to select the most relevant examples and rank them based on similarity, to be used for inference. If the generated response has low confidence, our framework selects new examples and performs inference again, iteratively refining the results until a high-confidence response is obtained. This approach improves OOD video understanding performance by extending effective context length without incurring high costs. The experimental results on multiple benchmarks demonstrate significant performance gains, especially in domain-specific scenarios, laying the groundwork for broader video comprehension applications. Code will be released at https://github.com/KangsanKim07/VideoICL', 'score': 6, 'issue_id': 979, 'pub_date': '2024-12-03', 'pub_date_card': {'ru': '3 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 3', 'zh': '12Êúà3Êó•'}, 'hash': '846ae2b4252c5290', 'authors': ['Kangsan Kim', 'Geon Park', 'Youngwan Lee', 'Woongyeong Yeo', 'Sung Ju Hwang'], 'affiliations': ['DeepAuto.ai', 'ETRI', 'KAIST'], 'pdf_title_img': 'assets/pdf/title_img/2412.02186.jpg', 'data': {'categories': ['#multimodal', '#training', '#benchmark', '#optimization', '#video', '#long_context', '#reasoning'], 'emoji': 'üé•', 'ru': {'title': 'VideoICL: –£–º–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –≤–∏–¥–µ–æ –Ω–∞ –ø—Ä–∏–º–µ—Ä–∞—Ö –¥–ª—è –Ω–æ–≤—ã—Ö –∑–∞–¥–∞—á', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç VideoICL - –Ω–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É –æ–±—É—á–µ–Ω–∏—è –≤–∏–¥–µ–æ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –¥–ª—è –∑–∞–¥–∞—á –≤–Ω–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è (OOD). –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –æ—Ç–±–æ—Ä —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ö–æ–¥—Å—Ç–≤–∞ –∏ –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω—ã–π –≤—ã–≤–æ–¥ –Ω–∞ –æ—Å–Ω–æ–≤–µ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –≤—ã–±–∏—Ä–∞—Ç—å –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ–¥—Ö–æ–¥—è—â–∏–µ –ø—Ä–∏–º–µ—Ä—ã –∏ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞—Ç—å –∏—Ö –ø–æ —Å—Ö–æ–¥—Å—Ç–≤—É –¥–ª—è –≤—ã–≤–æ–¥–∞, –ø–æ–≤—Ç–æ—Ä—è—è –ø—Ä–æ—Ü–µ—Å—Å –ø—Ä–∏ –Ω–∏–∑–∫–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏. –ü–æ–¥—Ö–æ–¥ —É–ª—É—á—à–∞–µ—Ç –ø–æ–Ω–∏–º–∞–Ω–∏–µ OOD-–≤–∏–¥–µ–æ, —Ä–∞—Å—à–∏—Ä—è—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—É—é –¥–ª–∏–Ω—É –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –±–µ–∑ –≤—ã—Å–æ–∫–∏—Ö –∑–∞—Ç—Ä–∞—Ç.'}, 'en': {'title': 'Enhancing Video Understanding with Iterative In-Context Learning', 'desc': 'This paper introduces VideoICL, a new framework designed to enhance video understanding in large multimodal models (LMMs) when faced with out-of-distribution (OOD) tasks. It addresses the limitations of traditional fine-tuning methods by employing in-context learning (ICL) strategies that utilize relevant example selection based on similarity. The framework iteratively refines inference results by selecting new examples if initial responses lack confidence, thereby improving the overall performance on OOD tasks. Experimental results show that VideoICL significantly boosts video comprehension, particularly in specialized domains, paving the way for more effective video analysis applications.'}, 'zh': {'title': 'ËßÜÈ¢ëÁêÜËß£ÁöÑÊñ∞Á™ÅÁ†¥ÔºöVideoICLÊ°ÜÊû∂', 'desc': 'ÊúÄËøëÔºåËßÜÈ¢ëÂ§ßÂûãÂ§öÊ®°ÊÄÅÊ®°ÂûãÔºàLMMsÔºâÁöÑËøõÂ±ïÊòæËëóÊèêÂçá‰∫ÜÂÖ∂ËßÜÈ¢ëÁêÜËß£ÂíåÊé®ÁêÜËÉΩÂäõ„ÄÇÁÑ∂ËÄåÔºåËøô‰∫õÊ®°ÂûãÂú®Â§ÑÁêÜËÆ≠ÁªÉÊï∞ÊçÆ‰∏≠‰ª£Ë°®ÊÄß‰∏çË∂≥ÁöÑÂàÜÂ∏ÉÂ§ñÔºàOODÔºâ‰ªªÂä°Êó∂Ë°®Áé∞‰∏ç‰Ω≥„ÄÇ‰º†ÁªüÁöÑÂæÆË∞ÉÊñπÊ≥ïÁî±‰∫éËÆ°ÁÆóÊàêÊú¨È´òËÄå‰∏çÂàáÂÆûÈôÖÔºåÂõ†Ê≠§Êàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑËßÜÈ¢ë‰∏ä‰∏ãÊñáÂ≠¶‰π†Ê°ÜÊû∂VideoICLÔºåÈááÁî®Âü∫‰∫éÁõ∏‰ººÊÄßÁöÑÁõ∏ÂÖ≥Á§∫‰æãÈÄâÊã©Á≠ñÁï•ÂíåÂü∫‰∫éÁΩÆ‰ø°Â∫¶ÁöÑËø≠‰ª£Êé®ÁêÜÊñπÊ≥ï„ÄÇËøôÁßçÊñπÊ≥ïÈÄöËøáÈÄâÊã©ÊúÄÁõ∏ÂÖ≥ÁöÑÁ§∫‰æãÂπ∂Ê†πÊçÆÁõ∏‰ººÊÄßËøõË°åÊéíÂêçÔºåÊòæËëóÊèêÈ´ò‰∫ÜOODËßÜÈ¢ëÁêÜËß£ÁöÑÊÄßËÉΩ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2412.03565', 'title': 'Inst-IT: Boosting Multimodal Instance Understanding via Explicit Visual Prompt Instruction Tuning', 'url': 'https://huggingface.co/papers/2412.03565', 'abstract': 'Large Multimodal Models (LMMs) have made significant breakthroughs with the advancement of instruction tuning. However, while existing models can understand images and videos at a holistic level, they still struggle with instance-level understanding that requires a more nuanced comprehension and alignment. Instance-level understanding is crucial, as it focuses on the specific elements that we are most interested in. Excitingly, existing works find that the state-of-the-art LMMs exhibit strong instance understanding capabilities when provided with explicit visual cues. Motivated by this, we introduce an automated annotation pipeline assisted by GPT-4o to extract instance-level information from images and videos through explicit visual prompting for instance guidance. Building upon this pipeline, we proposed Inst-IT, a solution to enhance LMMs in Instance understanding via explicit visual prompt Instruction Tuning. Inst-IT consists of a benchmark to diagnose multimodal instance-level understanding, a large-scale instruction-tuning dataset, and a continuous instruction-tuning training paradigm to effectively enhance spatial-temporal instance understanding capabilities of existing LMMs. Experimental results show that, with the boost of Inst-IT, our models not only achieve outstanding performance on Inst-IT Bench but also demonstrate significant improvements across various generic image and video understanding benchmarks. This highlights that our dataset not only boosts instance-level understanding but also strengthens the overall capabilities of generic image and video comprehension.', 'score': 6, 'issue_id': 967, 'pub_date': '2024-12-04', 'pub_date_card': {'ru': '4 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 4', 'zh': '12Êúà4Êó•'}, 'hash': '72af31b504d0aac1', 'authors': ['Wujian Peng', 'Lingchen Meng', 'Yitong Chen', 'Yiweng Xie', 'Yang Liu', 'Tao Gui', 'Hang Xu', 'Xipeng Qiu', 'Zuxuan Wu', 'Yu-Gang Jiang'], 'affiliations': ['Huawei Noahs Ark Lab', 'School of Computer Science, Fudan University', 'Shanghai Innovation Institute'], 'pdf_title_img': 'assets/pdf/title_img/2412.03565.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#multimodal', '#alignment', '#dataset', '#training'], 'emoji': 'üîç', 'ru': {'title': '–£–ª—É—á—à–µ–Ω–∏–µ –ø–æ–Ω–∏–º–∞–Ω–∏—è —ç–∫–∑–µ–º–ø–ª—è—Ä–æ–≤ –≤ LMM —Å –ø–æ–º–æ—â—å—é —è–≤–Ω—ã—Ö –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –ø–æ–¥—Å–∫–∞–∑–æ–∫', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Inst-IT - —Ä–µ—à–µ–Ω–∏–µ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–Ω–∏–º–∞–Ω–∏—è —ç–∫–∑–µ–º–ø–ª—è—Ä–æ–≤ –≤ –±–æ–ª—å—à–∏—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö (LMM) —Å –ø–æ–º–æ—â—å—é —è–≤–Ω—ã—Ö –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –ø–æ–¥—Å–∫–∞–∑–æ–∫. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω–≤–µ–π–µ—Ä –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º GPT-4 –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–∞ —É—Ä–æ–≤–Ω–µ —ç–∫–∑–µ–º–ø–ª—è—Ä–æ–≤ –∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –≤–∏–¥–µ–æ. Inst-IT –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è —ç—Ç–∞–ª–æ–Ω–Ω—ã–π —Ç–µ—Å—Ç –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –Ω–∞ —É—Ä–æ–≤–Ω–µ —ç–∫–∑–µ–º–ø–ª—è—Ä–æ–≤, –±–æ–ª—å—à–æ–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏ –∏ –ø–∞—Ä–∞–¥–∏–≥–º—É –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –∫–∞–∫ –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ —ç–∫–∑–µ–º–ø–ª—è—Ä–æ–≤, —Ç–∞–∫ –∏ –≤ –æ–±—â–µ–º –ø–æ–Ω–∏–º–∞–Ω–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –≤–∏–¥–µ–æ.'}, 'en': {'title': 'Enhancing Instance Understanding in Multimodal Models with Inst-IT', 'desc': "This paper discusses the limitations of Large Multimodal Models (LMMs) in understanding specific instances within images and videos, despite their overall comprehension abilities. It introduces a new automated annotation pipeline that uses GPT-4o to extract detailed instance-level information through explicit visual prompts. The authors propose a method called Inst-IT, which enhances LMMs' instance understanding by utilizing instruction tuning with a focus on spatial-temporal elements. Experimental results indicate that Inst-IT significantly improves both instance-level understanding and general image and video comprehension across various benchmarks."}, 'zh': {'title': 'ÊèêÂçáÂÆû‰æãÁêÜËß£ËÉΩÂäõÁöÑÂàõÊñ∞ÊñπÊ≥ï', 'desc': 'Â§ßÂûãÂ§öÊ®°ÊÄÅÊ®°ÂûãÔºàLMMsÔºâÂú®Êåá‰ª§Ë∞É‰ºòÊñπÈù¢ÂèñÂæó‰∫ÜÊòæËëóÁ™ÅÁ†¥Ôºå‰ΩÜÂú®ÂÆû‰æãÁ∫ßÁêÜËß£‰∏ä‰ªçÁÑ∂Â≠òÂú®ÊåëÊàò„ÄÇÂÆû‰æãÁ∫ßÁêÜËß£ÂÖ≥Ê≥®ÁâπÂÆöÂÖÉÁ¥†ÔºåËøôÂØπ‰∫éÊ∑±ÂÖ•ÁêÜËß£ÂõæÂÉèÂíåËßÜÈ¢ëËá≥ÂÖ≥ÈáçË¶Å„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçËá™Âä®Ê≥®ÈáäÁÆ°ÈÅìÔºåÂà©Áî®GPT-4oÈÄöËøáÊòéÁ°ÆÁöÑËßÜËßâÊèêÁ§∫ÊèêÂèñÂÆû‰æãÁ∫ß‰ø°ÊÅØ„ÄÇÂü∫‰∫éÊ≠§ÔºåÊàë‰ª¨ÂºÄÂèë‰∫ÜInst-ITÔºåÈÄöËøáÊòéÁ°ÆÁöÑËßÜËßâÊèêÁ§∫Êåá‰ª§Ë∞É‰ºòÊù•Â¢ûÂº∫LMMsÁöÑÂÆû‰æãÁêÜËß£ËÉΩÂäõÔºåÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåInst-ITÊòæËëóÊèêÂçá‰∫ÜÊ®°ÂûãÂú®Â§öÁßçÂõæÂÉèÂíåËßÜÈ¢ëÁêÜËß£Âü∫ÂáÜ‰∏äÁöÑË°®Áé∞„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2412.02980', 'title': 'Surveying the Effects of Quality, Diversity, and Complexity in Synthetic Data From Large Language Models', 'url': 'https://huggingface.co/papers/2412.02980', 'abstract': 'Synthetic data generation with Large Language Models is a promising paradigm for augmenting natural data over a nearly infinite range of tasks. Given this variety, direct comparisons among synthetic data generation algorithms are scarce, making it difficult to understand where improvement comes from and what bottlenecks exist. We propose to evaluate algorithms via the makeup of synthetic data generated by each algorithm in terms of data quality, diversity, and complexity. We choose these three characteristics for their significance in open-ended processes and the impact each has on the capabilities of downstream models. We find quality to be essential for in-distribution model generalization, diversity to be essential for out-of-distribution generalization, and complexity to be beneficial for both. Further, we emphasize the existence of Quality-Diversity trade-offs in training data and the downstream effects on model performance. We then examine the effect of various components in the synthetic data pipeline on each data characteristic. This examination allows us to taxonomize and compare synthetic data generation algorithms through the components they utilize and the resulting effects on data QDC composition. This analysis extends into a discussion on the importance of balancing QDC in synthetic data for efficient reinforcement learning and self-improvement algorithms. Analogous to the QD trade-offs in training data, often there exist trade-offs between model output quality and output diversity which impact the composition of synthetic data. We observe that many models are currently evaluated and optimized only for output quality, thereby limiting output diversity and the potential for self-improvement. We argue that balancing these trade-offs is essential to the development of future self-improvement algorithms and highlight a number of works making progress in this direction.', 'score': 4, 'issue_id': 968, 'pub_date': '2024-12-04', 'pub_date_card': {'ru': '4 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 4', 'zh': '12Êúà4Êó•'}, 'hash': '8055d4be8211be80', 'authors': ['Alex Havrilla', 'Andrew Dai', "Laura O'Mahony", 'Koen Oostermeijer', 'Vera Zisler', 'Alon Albalak', 'Fabrizio Milo', 'Sharath Chandra Raparthy', 'Kanishk Gandhi', 'Baber Abbasi', 'Duy Phung', 'Maia Iyer', 'Dakota Mahan', 'Chase Blagden', 'Srishti Gureja', 'Mohammed Hamdy', 'Wen-Ding Li', 'Giovanni Paolini', 'Pawan Sasanka Ammanamanchi', 'Elliot Meyerson'], 'affiliations': ['Aleph Alpha @ IPAI', 'Cognizant AI Labs', 'Cohere for AI Community', 'Cornell University', 'Eleuther AI', 'Georgia Tech', 'IBM', 'Independent', 'Reka AI', 'Sakana AI', 'Stanford University', 'SynthLabs', 'University of Bologna', 'University of Limerick'], 'pdf_title_img': 'assets/pdf/title_img/2412.02980.jpg', 'data': {'categories': ['#synthetic', '#optimization', '#training', '#data', '#dataset', '#rl', '#benchmark'], 'emoji': 'üß†', 'ru': {'title': '–ë–∞–ª–∞–Ω—Å –∫–∞—á–µ—Å—Ç–≤–∞ –∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –≤ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ò–ò', 'desc': '–°—Ç–∞—Ç—å—è —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏—é —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö —Å –ø–æ–º–æ—â—å—é –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –æ—Ü–µ–Ω–∏–≤–∞—Ç—å –∞–ª–≥–æ—Ä–∏—Ç–º—ã –ø–æ –∫–∞—á–µ—Å—Ç–≤—É, —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—é –∏ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –≤–∞–∂–Ω–æ—Å—Ç—å –±–∞–ª–∞–Ω—Å–∞ —ç—Ç–∏—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –∏ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ —Å–∞–º–æ—É–ª—É—á—à–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –æ—Ç–º–µ—á–∞—é—Ç, —á—Ç–æ –º–Ω–æ–≥–∏–µ –º–æ–¥–µ–ª–∏ –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É—é—Ç—Å—è —Ç–æ–ª—å–∫–æ –ø–æ –∫–∞—á–µ—Å—Ç–≤—É –≤—ã—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞—è –∏—Ö —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –∏ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª —Å–∞–º–æ—É–ª—É—á—à–µ–Ω–∏—è.'}, 'en': {'title': 'Balancing Quality, Diversity, and Complexity in Synthetic Data Generation', 'desc': 'This paper discusses the generation of synthetic data using Large Language Models (LLMs) and its importance in enhancing natural data for various tasks. It evaluates synthetic data generation algorithms based on three key characteristics: quality, diversity, and complexity, which are crucial for the performance of downstream models. The authors highlight the trade-offs between these characteristics, noting that while quality is vital for in-distribution generalization, diversity is necessary for out-of-distribution generalization. The paper emphasizes the need for a balanced approach to these trade-offs to improve the effectiveness of reinforcement learning and self-improvement algorithms.'}, 'zh': {'title': 'ÂêàÊàêÊï∞ÊçÆÁîüÊàêÔºöÂπ≥Ë°°Ë¥®Èáè‰∏éÂ§öÊ†∑ÊÄß', 'desc': 'Êú¨ËÆ∫ÊñáÊé¢ËÆ®‰∫Ü‰ΩøÁî®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁîüÊàêÂêàÊàêÊï∞ÊçÆÁöÑÊΩúÂäõÔºåÂº∫Ë∞É‰∫ÜÂêàÊàêÊï∞ÊçÆÂú®Ëá™ÁÑ∂Êï∞ÊçÆÂ¢ûÂº∫‰∏≠ÁöÑÈáçË¶ÅÊÄß„ÄÇÊàë‰ª¨ÊèêÂá∫ÈÄöËøáÊï∞ÊçÆË¥®Èáè„ÄÅÂ§öÊ†∑ÊÄßÂíåÂ§çÊùÇÊÄßÊù•ËØÑ‰º∞ÂêàÊàêÊï∞ÊçÆÁîüÊàêÁÆóÊ≥ïÔºåËøô‰∏âËÄÖÂØπ‰∏ãÊ∏∏Ê®°ÂûãÁöÑËÉΩÂäõÊúâÊòæËëóÂΩ±Âìç„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÊï∞ÊçÆË¥®ÈáèÂØπÊ®°ÂûãÁöÑÂàÜÂ∏ÉÂÜÖÊ≥õÂåñËá≥ÂÖ≥ÈáçË¶ÅÔºåËÄåÂ§öÊ†∑ÊÄßÂàôÂØπÂàÜÂ∏ÉÂ§ñÊ≥õÂåñËá≥ÂÖ≥ÈáçË¶ÅÔºåÂ§çÊùÇÊÄßÂØπ‰∏§ËÄÖÈÉΩÊúâÁõä„ÄÇÊàë‰ª¨ËøòÂº∫Ë∞É‰∫ÜËÆ≠ÁªÉÊï∞ÊçÆ‰∏≠ÁöÑË¥®Èáè-Â§öÊ†∑ÊÄßÊùÉË°°ÂèäÂÖ∂ÂØπÊ®°ÂûãÊÄßËÉΩÁöÑÂΩ±ÂìçÔºåËÆ§‰∏∫Âú®Êú™Êù•ÁöÑËá™ÊàëÊîπËøõÁÆóÊ≥ï‰∏≠Âπ≥Ë°°Ëøô‰∫õÊùÉË°°ÊòØËá≥ÂÖ≥ÈáçË¶ÅÁöÑ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2412.03187', 'title': 'Weighted-Reward Preference Optimization for Implicit Model Fusion', 'url': 'https://huggingface.co/papers/2412.03187', 'abstract': 'While fusing heterogeneous open-source LLMs with varying architectures and sizes can potentially integrate the strengths of different models, existing fusion methods face significant challenges, such as vocabulary alignment and merging distribution matrices. These procedures are not only complex but also prone to introducing noise and errors. In this paper, we propose an implicit fusion method, Weighted-Reward Preference Optimization (WRPO), which leverages preference optimization between the source LLMs and the target LLM to transfer their capabilities effectively. WRPO eliminates the need for vocabulary alignment and matrix fusion and can be efficiently scaled to accommodate various LLMs. To address distributional deviations between the source and target LLMs, WRPO introduces a progressive adaptation strategy that gradually shifts reliance on preferred examples from the target LLM to the source LLMs. Extensive experiments on the MT-Bench, AlpacaEval-2, and Arena-Hard benchmarks demonstrate that WRPO consistently outperforms existing knowledge fusion methods and various fine-tuning baselines. When applied to LLaMA3-8B-Instruct as the target model, WRPO achieves a length-controlled win rate of 55.9% against GPT-4-Preview-1106 on AlpacaEval-2 and a win rate of 46.2% against GPT-4-0314 on Arena-Hard. Our code is available at https://github.com/SLIT-AI/WRPO.', 'score': 4, 'issue_id': 961, 'pub_date': '2024-12-04', 'pub_date_card': {'ru': '4 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 4', 'zh': '12Êúà4Êó•'}, 'hash': '6da11fbf4e1ea7d9', 'authors': ['Ziyi Yang', 'Fanqi Wan', 'Longguang Zhong', 'Tianyuan Shi', 'Xiaojun Quan'], 'affiliations': ['School of Computer Science and Engineering, Sun Yat-sen University, China'], 'pdf_title_img': 'assets/pdf/title_img/2412.03187.jpg', 'data': {'categories': ['#optimization', '#transfer_learning', '#open_source', '#architecture', '#training', '#benchmark'], 'emoji': 'üß†', 'ru': {'title': 'WRPO: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Å–ª–∏—è–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –±–µ–∑ –ø—Ä—è–º–æ–≥–æ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤', 'desc': '–í —ç—Ç–æ–π —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —Å–ª–∏—è–Ω–∏—è —Ä–∞–∑–Ω–æ—Ä–æ–¥–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –æ—Ç–∫—Ä—ã—Ç—ã–º –∏—Å—Ö–æ–¥–Ω—ã–º –∫–æ–¥–æ–º - Weighted-Reward Preference Optimization (WRPO). WRPO –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –º–µ–∂–¥—É –∏—Å—Ö–æ–¥–Ω—ã–º–∏ –∏ —Ü–µ–ª–µ–≤–æ–π –º–æ–¥–µ–ª—è–º–∏ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –ø–µ—Ä–µ–Ω–æ—Å–∞ –∏—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π, —É—Å—Ç—Ä–∞–Ω—è—è –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –≤ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–∏ —Å–ª–æ–≤–∞—Ä–µ–π –∏ —Å–ª–∏—è–Ω–∏–∏ –º–∞—Ç—Ä–∏—Ü —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è. –ú–µ—Ç–æ–¥ –≤–≤–æ–¥–∏—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–π –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã —Ä–∞–∑–ª–∏—á–∏–π –≤ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è—Ö –º–µ–∂–¥—É –º–æ–¥–µ–ª—è–º–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ WRPO –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã —Å–ª–∏—è–Ω–∏—è –∑–Ω–∞–Ω–∏–π –∏ –±–∞–∑–æ–≤—ã–µ –ø–æ–¥—Ö–æ–¥—ã –∫ –¥–æ–æ–±—É—á–µ–Ω–∏—é –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö.'}, 'en': {'title': 'Effortless Fusion of LLMs with WRPO!', 'desc': 'This paper introduces a new method called Weighted-Reward Preference Optimization (WRPO) for fusing different open-source large language models (LLMs). WRPO simplifies the fusion process by avoiding complex tasks like vocabulary alignment and distribution matrix merging, which often introduce errors. Instead, it uses a preference optimization approach to effectively transfer capabilities from source LLMs to a target LLM. The method also includes a progressive adaptation strategy to manage differences in distributions between models, leading to improved performance on various benchmarks compared to existing methods.'}, 'zh': {'title': 'Âä†ÊùÉÂ•ñÂä±ÂÅèÂ•Ω‰ºòÂåñÔºöÈ´òÊïàËûçÂêàÂ§öÁßçÂ§ßËØ≠Ë®ÄÊ®°Âûã', 'desc': 'Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÈöêÂºèËûçÂêàÊñπÊ≥ïÔºåÁß∞‰∏∫Âä†ÊùÉÂ•ñÂä±ÂÅèÂ•Ω‰ºòÂåñÔºàWRPOÔºâÔºåÊó®Âú®ÊúâÊïàÊï¥Âêà‰∏çÂêåÊû∂ÊûÑÂíåËßÑÊ®°ÁöÑÂºÄÊ∫êÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâ„ÄÇWRPOÈÄöËøá‰ºòÂåñÊ∫êÊ®°Âûã‰∏éÁõÆÊ†áÊ®°Âûã‰πãÈó¥ÁöÑÂÅèÂ•ΩÔºåÈÅøÂÖç‰∫ÜËØçÊ±áÂØπÈΩêÂíåÁü©ÈòµËûçÂêàÁöÑÂ§çÊùÇÊÄß„ÄÇËØ•ÊñπÊ≥ïÂºïÂÖ•‰∫ÜÊ∏êËøõÈÄÇÂ∫îÁ≠ñÁï•ÔºåÈÄêÊ≠•Ë∞ÉÊï¥ÂØπÁõÆÊ†áÊ®°ÂûãÂíåÊ∫êÊ®°ÂûãÁöÑ‰æùËµñÔºå‰ªéËÄåËß£ÂÜ≥‰∫ÜÂàÜÂ∏ÉÂÅèÂ∑ÆÈóÆÈ¢ò„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåWRPOÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰ºò‰∫éÁé∞ÊúâÁöÑÁü•ËØÜËûçÂêàÊñπÊ≥ïÂíåÂæÆË∞ÉÂü∫Á∫ø„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2412.00177', 'title': 'LumiNet: Latent Intrinsics Meets Diffusion Models for Indoor Scene Relighting', 'url': 'https://huggingface.co/papers/2412.00177', 'abstract': "We introduce LumiNet, a novel architecture that leverages generative models and latent intrinsic representations for effective lighting transfer. Given a source image and a target lighting image, LumiNet synthesizes a relit version of the source scene that captures the target's lighting. Our approach makes two key contributions: a data curation strategy from the StyleGAN-based relighting model for our training, and a modified diffusion-based ControlNet that processes both latent intrinsic properties from the source image and latent extrinsic properties from the target image. We further improve lighting transfer through a learned adaptor (MLP) that injects the target's latent extrinsic properties via cross-attention and fine-tuning.   Unlike traditional ControlNet, which generates images with conditional maps from a single scene, LumiNet processes latent representations from two different images - preserving geometry and albedo from the source while transferring lighting characteristics from the target. Experiments demonstrate that our method successfully transfers complex lighting phenomena including specular highlights and indirect illumination across scenes with varying spatial layouts and materials, outperforming existing approaches on challenging indoor scenes using only images as input.", 'score': 2, 'issue_id': 969, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 –Ω–æ—è–±—Ä—è', 'en': 'November 29', 'zh': '11Êúà29Êó•'}, 'hash': '210b042d1a430116', 'authors': ['Xiaoyan Xing', 'Konrad Groh', 'Sezer Karaoglu', 'Theo Gevers', 'Anand Bhattad'], 'affiliations': ['BCAI-Bosch', 'Toyota Technological Institute at Chicago', 'UvA-Bosch Delta Lab'], 'pdf_title_img': 'assets/pdf/title_img/2412.00177.jpg', 'data': {'categories': ['#data', '#optimization', '#cv', '#diffusion', '#architecture'], 'emoji': 'üí°', 'ru': {'title': 'LumiNet: –†–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–π –ø–µ—Ä–µ–Ω–æ—Å –æ—Å–≤–µ—â–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é –ª–∞—Ç–µ–Ω—Ç–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π', 'desc': 'LumiNet - —ç—Ç–æ –Ω–æ–≤–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–ª—è –ø–µ—Ä–µ–Ω–æ—Å–∞ –æ—Å–≤–µ—â–µ–Ω–∏—è –º–µ–∂–¥—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∏ –ª–∞—Ç–µ–Ω—Ç–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è. –û–Ω–∞ –≤–∫–ª—é—á–∞–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –æ—Å–Ω–æ–≤–µ StyleGAN –∏ –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é ControlNet, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—â—É—é –ª–∞—Ç–µ–Ω—Ç–Ω—ã–µ —Å–≤–æ–π—Å—Ç–≤–∞ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –∏ —Ü–µ–ª–µ–≤–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. LumiNet –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç –ø–µ—Ä–µ–Ω–æ—Å –æ—Å–≤–µ—â–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–µ–Ω–Ω–æ–≥–æ –∞–¥–∞–ø—Ç–µ—Ä–∞, –≤–Ω–µ–¥—Ä—è—é—â–µ–≥–æ –ª–∞—Ç–µ–Ω—Ç–Ω—ã–µ –≤–Ω–µ—à–Ω–∏–µ —Å–≤–æ–π—Å—Ç–≤–∞ —Ü–µ–ª–µ–≤–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –º–µ—Ç–æ–¥ —É—Å–ø–µ—à–Ω–æ –ø–µ—Ä–µ–Ω–æ—Å–∏—Ç —Å–ª–æ–∂–Ω—ã–µ —Å–≤–µ—Ç–æ–≤—ã–µ —ç—Ñ—Ñ–µ–∫—Ç—ã –º–µ–∂–¥—É —Å—Ü–µ–Ω–∞–º–∏ —Å —Ä–∞–∑–ª–∏—á–Ω–æ–π –≥–µ–æ–º–µ—Ç—Ä–∏–µ–π –∏ –º–∞—Ç–µ—Ä–∏–∞–ª–∞–º–∏.'}, 'en': {'title': 'LumiNet: Mastering Lighting Transfer with Generative Models', 'desc': 'LumiNet is a new machine learning architecture designed for transferring lighting effects from one image to another. It uses generative models to create a relit version of a source image by applying the lighting characteristics of a target image. The model incorporates a unique data curation strategy and a modified diffusion-based ControlNet that processes both intrinsic and extrinsic properties of the images. By utilizing cross-attention and fine-tuning, LumiNet effectively captures complex lighting phenomena, outperforming traditional methods in challenging indoor environments.'}, 'zh': {'title': 'LumiNetÔºöÈ´òÊïàÂÖâÁÖßËΩ¨ÁßªÁöÑÊñ∞ÊñπÊ≥ï', 'desc': 'LumiNetÊòØ‰∏ÄÁßçÊñ∞È¢ñÁöÑÊû∂ÊûÑÔºåÂà©Áî®ÁîüÊàêÊ®°ÂûãÂíåÊΩúÂú®ÂÜÖÂú®Ë°®Á§∫Êù•ÂÆûÁé∞ÊúâÊïàÁöÑÂÖâÁÖßËΩ¨Áßª„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáËæìÂÖ•Ê∫êÂõæÂÉèÂíåÁõÆÊ†áÂÖâÁÖßÂõæÂÉèÔºåÂêàÊàêÂá∫‰∏Ä‰∏™ÊçïÊçâÁõÆÊ†áÂÖâÁÖßÁöÑÈáçÊñ∞ÁÖßÊòéÁâàÊú¨„ÄÇLumiNetÁöÑ‰∏§‰∏™ÂÖ≥ÈîÆË¥°ÁåÆÂåÖÊã¨Âü∫‰∫éStyleGANÁöÑÈáçÊñ∞ÁÖßÊòéÊ®°ÂûãÁöÑÊï∞ÊçÆÊï¥ÁêÜÁ≠ñÁï•Ôºå‰ª•ÂèäÂ§ÑÁêÜÊ∫êÂõæÂÉèÁöÑÊΩúÂú®ÂÜÖÂú®Â±ûÊÄßÂíåÁõÆÊ†áÂõæÂÉèÁöÑÊΩúÂú®Â§ñÂú®Â±ûÊÄßÁöÑÊîπËøõÊâ©Êï£ÊéßÂà∂ÁΩëÁªú„ÄÇÈÄöËøá‰∫§ÂèâÊ≥®ÊÑèÂäõÂíåÂæÆË∞ÉÔºåLumiNetËøõ‰∏ÄÊ≠•ÈÄöËøáÂ≠¶‰π†ÈÄÇÈÖçÂô®ÔºàMLPÔºâÊ≥®ÂÖ•ÁõÆÊ†áÁöÑÊΩúÂú®Â§ñÂú®Â±ûÊÄßÔºå‰ªéËÄåÊîπÂñÑÂÖâÁÖßËΩ¨ÁßªÊïàÊûú„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2412.01824', 'title': 'X-Prompt: Towards Universal In-Context Image Generation in Auto-Regressive Vision Language Foundation Models', 'url': 'https://huggingface.co/papers/2412.01824', 'abstract': "In-context generation is a key component of large language models' (LLMs) open-task generalization capability. By leveraging a few examples as context, LLMs can perform both in-domain and out-of-domain tasks. Recent advancements in auto-regressive vision-language models (VLMs) built upon LLMs have showcased impressive performance in text-to-image generation. However, the potential of in-context learning for general image generation tasks remains largely unexplored. To address this, we introduce X-Prompt, a purely auto-regressive large-vision language model designed to deliver competitive performance across a wide range of both seen and unseen image generation tasks, all within a unified in-context learning framework. X-Prompt incorporates a specialized design that efficiently compresses valuable features from in-context examples, supporting longer in-context token sequences and improving its ability to generalize to unseen tasks. A unified training task for both text and image prediction enables X-Prompt to handle general image generation with enhanced task awareness from in-context examples. Extensive experiments validate the model's performance across diverse seen image generation tasks and its capacity to generalize to previously unseen tasks.", 'score': 37, 'issue_id': 911, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 2', 'zh': '12Êúà2Êó•'}, 'hash': '6b7c2d6555d8df8d', 'authors': ['Zeyi Sun', 'Ziyang Chu', 'Pan Zhang', 'Tong Wu', 'Xiaoyi Dong', 'Yuhang Zang', 'Yuanjun Xiong', 'Dahua Lin', 'Jiaqi Wang'], 'affiliations': ['CPII under InnoHK', 'MThreads AI', 'Shanghai AI Laboratory', 'Shanghai Jiao Tong University', 'The Chinese University of Hong Kong', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2412.01824.jpg', 'data': {'categories': ['#agi', '#cv', '#games', '#optimization', '#training', '#multimodal'], 'emoji': 'üé®', 'ru': {'title': 'X-Prompt: —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–º –æ–±—É—á–µ–Ω–∏–µ–º', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç X-Prompt - –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—É—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –ú–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –µ–π —Ä–µ—à–∞—Ç—å –∫–∞–∫ –∑–Ω–∞–∫–æ–º—ã–µ, —Ç–∞–∫ –∏ –Ω–æ–≤—ã–µ –∑–∞–¥–∞—á–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. X-Prompt —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ —Å–∂–∏–º–∞–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ –∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –¥–ª–∏–Ω–Ω—ã–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ç–æ–∫–µ–Ω–æ–≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞—é—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ —Ä–µ—à–∞—Ç—å —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –∑–∞–¥–∞—á–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –≤–∫–ª—é—á–∞—è —Ä–∞–Ω–µ–µ –Ω–µ –≤—Å—Ç—Ä–µ—á–∞–≤—à–∏–µ—Å—è.'}, 'en': {'title': 'X-Prompt: Unlocking Image Generation with In-Context Learning', 'desc': 'This paper presents X-Prompt, an advanced auto-regressive vision-language model that enhances image generation through in-context learning. By utilizing a few examples as context, X-Prompt can effectively generate images for both familiar and novel tasks. The model is designed to compress important features from these examples, allowing it to manage longer sequences of context and improve generalization. Extensive testing shows that X-Prompt performs well on a variety of image generation tasks, demonstrating its ability to adapt to new challenges.'}, 'zh': {'title': 'X-PromptÔºöÊèêÂçáÂõæÂÉèÁîüÊàêÁöÑ‰∏ä‰∏ãÊñáÂ≠¶‰π†ËÉΩÂäõ', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂêç‰∏∫X-PromptÁöÑËá™ÂõûÂΩíÂ§ßËßÑÊ®°ËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºåÊó®Âú®ÊèêÂçáÂõæÂÉèÁîüÊàê‰ªªÂä°ÁöÑË°®Áé∞„ÄÇX-PromptÈÄöËøáÂà©Áî®‰∏ä‰∏ãÊñá‰∏≠ÁöÑÁ§∫‰æãÔºåËÉΩÂ§üÂú®Â∑≤Áü•ÂíåÊú™Áü•ÁöÑÂõæÂÉèÁîüÊàê‰ªªÂä°‰∏≠ÂÆûÁé∞Á´û‰∫âÂäõÁöÑÊÄßËÉΩ„ÄÇËØ•Ê®°ÂûãÈááÁî®‰∫Ü‰∏ìÈó®ÁöÑËÆæËÆ°ÔºåËÉΩÂ§üÊúâÊïàÂéãÁº©‰∏ä‰∏ãÊñáÁ§∫‰æã‰∏≠ÁöÑÈáçË¶ÅÁâπÂæÅÔºå‰ªéËÄåÊîØÊåÅÊõ¥ÈïøÁöÑ‰∏ä‰∏ãÊñáÂ∫èÂàóÂπ∂ÊèêÈ´òÂØπÊú™Áü•‰ªªÂä°ÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇÈÄöËøáÁªü‰∏ÄÁöÑËÆ≠ÁªÉ‰ªªÂä°ÔºåX-PromptÂú®ÊñáÊú¨ÂíåÂõæÂÉèÈ¢ÑÊµãÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤ÔºåÈ™åËØÅ‰∫ÜÂÖ∂Âú®Â§öÊ†∑ÂåñÂõæÂÉèÁîüÊàê‰ªªÂä°‰∏≠ÁöÑÊúâÊïàÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2412.00131', 'title': 'Open-Sora Plan: Open-Source Large Video Generation Model', 'url': 'https://huggingface.co/papers/2412.00131', 'abstract': 'We introduce Open-Sora Plan, an open-source project that aims to contribute a large generation model for generating desired high-resolution videos with long durations based on various user inputs. Our project comprises multiple components for the entire video generation process, including a Wavelet-Flow Variational Autoencoder, a Joint Image-Video Skiparse Denoiser, and various condition controllers. Moreover, many assistant strategies for efficient training and inference are designed, and a multi-dimensional data curation pipeline is proposed for obtaining desired high-quality data. Benefiting from efficient thoughts, our Open-Sora Plan achieves impressive video generation results in both qualitative and quantitative evaluations. We hope our careful design and practical experience can inspire the video generation research community. All our codes and model weights are publicly available at https://github.com/PKU-YuanGroup/Open-Sora-Plan.', 'score': 19, 'issue_id': 911, 'pub_date': '2024-11-28', 'pub_date_card': {'ru': '28 –Ω–æ—è–±—Ä—è', 'en': 'November 28', 'zh': '11Êúà28Êó•'}, 'hash': 'fa9b0009de797ca2', 'authors': ['Bin Lin', 'Yunyang Ge', 'Xinhua Cheng', 'Zongjian Li', 'Bin Zhu', 'Shaodong Wang', 'Xianyi He', 'Yang Ye', 'Shenghai Yuan', 'Liuhan Chen', 'Tanghui Jia', 'Junwu Zhang', 'Zhenyu Tang', 'Yatian Pang', 'Bin She', 'Cen Yan', 'Zhiheng Hu', 'Xiaoyi Dong', 'Lin Chen', 'Zhang Pan', 'Xing Zhou', 'Shaoling Dong', 'Yonghong Tian', 'Li Yuan'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2412.00131.jpg', 'data': {'categories': ['#open_source', '#data', '#inference', '#training', '#video'], 'emoji': 'üé¨', 'ru': {'title': '–û—Ç–∫—Ä—ã—Ç–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –≤–∏–¥–µ–æ', 'desc': '–ü—Ä–æ–µ–∫—Ç Open-Sora Plan –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –æ—Ç–∫—Ä—ã—Ç—É—é –º–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –≤—ã—Å–æ–∫–æ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –ú–æ–¥–µ–ª—å –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è –≤–µ–π–≤–ª–µ—Ç-–ø–æ—Ç–æ—á–Ω—ã–π –≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω—ã–π –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä, —Å–æ–≤–º–µ—Å—Ç–Ω—ã–π –¥–µ–Ω–æ–π–∑–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –≤–∏–¥–µ–æ, –∞ —Ç–∞–∫–∂–µ —Ä–∞–∑–ª–∏—á–Ω—ã–µ –∫–æ–Ω—Ç—Ä–æ–ª–ª–µ—Ä—ã —É—Å–ª–æ–≤–∏–π. –†–∞–∑—Ä–∞–±–æ—Ç–∞–Ω—ã —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏ –≤—ã–≤–æ–¥–∞, –∞ —Ç–∞–∫–∂–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–Ω–æ–≥–æ–º–µ—Ä–Ω—ã–π –∫–æ–Ω–≤–µ–π–µ—Ä –∫—É—Ä–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö. –ü—Ä–æ–µ–∫—Ç –¥–æ—Å—Ç–∏–≥ –≤–ø–µ—á–∞—Ç–ª—è—é—â–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –∫–∞–∫ –≤ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö, —Ç–∞–∫ –∏ –≤ –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ—Ü–µ–Ω–∫–∞—Ö.'}, 'en': {'title': 'Empowering High-Resolution Video Generation with Open-Sora Plan', 'desc': 'The Open-Sora Plan is an open-source initiative focused on creating a large-scale generative model for producing high-resolution videos that can last for extended periods, tailored to user specifications. It integrates several advanced components, including a Wavelet-Flow Variational Autoencoder and a Joint Image-Video Skiparse Denoiser, to enhance the video generation process. The project also introduces various condition controllers and efficient training strategies, along with a multi-dimensional data curation pipeline to ensure high-quality output. Overall, the Open-Sora Plan demonstrates significant advancements in video generation, aiming to inspire further research in the field.'}, 'zh': {'title': 'ÂºÄÊîæÊ∫ê‰ª£Á†ÅÔºåÁîüÊàêÈ´òË¥®ÈáèËßÜÈ¢ëÁöÑÊú™Êù•', 'desc': 'Open-SoraËÆ°ÂàíÊòØ‰∏Ä‰∏™ÂºÄÊ∫êÈ°πÁõÆÔºåÊó®Âú®Âü∫‰∫éÁî®Êà∑ËæìÂÖ•ÁîüÊàêÈ´òÂàÜËæ®ÁéáÁöÑÈïøÊó∂ËßÜÈ¢ë„ÄÇËØ•È°πÁõÆÂåÖÂê´Â§ö‰∏™ÁªÑ‰ª∂ÔºåÂ¶ÇÂ∞èÊ≥¢ÊµÅÂèòÂàÜËá™ÁºñÁ†ÅÂô®ÂíåËÅîÂêàÂõæÂÉè-ËßÜÈ¢ëÂéªÂô™Âô®ÔºåÊîØÊåÅÊï¥‰∏™ËßÜÈ¢ëÁîüÊàêËøáÁ®ã„ÄÇÊàë‰ª¨ËøòËÆæËÆ°‰∫ÜÂ§öÁßçÈ´òÊïàÁöÑËÆ≠ÁªÉÂíåÊé®ÁêÜÁ≠ñÁï•ÔºåÂπ∂ÊèêÂá∫‰∫ÜÂ§öÁª¥Êï∞ÊçÆÁ≠ñÂàíÁÆ°ÈÅìÔºå‰ª•Ëé∑ÂèñÈ´òË¥®ÈáèÊï∞ÊçÆ„ÄÇÈÄöËøáËøô‰∫õÈ´òÊïàÁöÑËÆæËÆ°ÔºåOpen-SoraËÆ°ÂàíÂú®ËßÜÈ¢ëÁîüÊàêÁöÑÂÆöÊÄßÂíåÂÆöÈáèËØÑ‰º∞‰∏≠ÂèñÂæó‰∫Ü‰ª§‰∫∫Âç∞Ë±°Ê∑±ÂàªÁöÑÁªìÊûú„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2412.00927', 'title': 'VISTA: Enhancing Long-Duration and High-Resolution Video Understanding by Video Spatiotemporal Augmentation', 'url': 'https://huggingface.co/papers/2412.00927', 'abstract': 'Current large multimodal models (LMMs) face significant challenges in processing and comprehending long-duration or high-resolution videos, which is mainly due to the lack of high-quality datasets. To address this issue from a data-centric perspective, we propose VISTA, a simple yet effective Video Spatiotemporal Augmentation framework that synthesizes long-duration and high-resolution video instruction-following pairs from existing video-caption datasets. VISTA spatially and temporally combines videos to create new synthetic videos with extended durations and enhanced resolutions, and subsequently produces question-answer pairs pertaining to these newly synthesized videos. Based on this paradigm, we develop seven video augmentation methods and curate VISTA-400K, a video instruction-following dataset aimed at enhancing long-duration and high-resolution video understanding. Finetuning various video LMMs on our data resulted in an average improvement of 3.3% across four challenging benchmarks for long-video understanding. Furthermore, we introduce the first comprehensive high-resolution video understanding benchmark HRVideoBench, on which our finetuned models achieve a 6.5% performance gain. These results highlight the effectiveness of our framework.', 'score': 17, 'issue_id': 912, 'pub_date': '2024-12-01', 'pub_date_card': {'ru': '1 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 1', 'zh': '12Êúà1Êó•'}, 'hash': 'b4065e6e554dea31', 'authors': ['Weiming Ren', 'Huan Yang', 'Jie Min', 'Cong Wei', 'Wenhu Chen'], 'affiliations': ['University of Waterloo', 'Vector Institute'], 'pdf_title_img': 'assets/pdf/title_img/2412.00927.jpg', 'data': {'categories': ['#multimodal', '#video', '#dataset', '#data', '#long_context', '#benchmark', '#synthetic'], 'emoji': 'üé•', 'ru': {'title': '–°–∏–Ω—Ç–µ–∑ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –ò–ò –ø–æ–Ω–∏–º–∞–Ω–∏—é —Å–ª–æ–∂–Ω—ã—Ö –≤–∏–¥–µ–æ', 'desc': 'VISTA - —ç—Ç–æ —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –¥–ª–∏–Ω–Ω—ã—Ö –∏ –≤—ã—Å–æ–∫–æ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –≤–∏–¥–µ–æ –±–æ–ª—å—à–∏–º–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏. –û–Ω–∞ —Å–∏–Ω—Ç–µ–∑–∏—Ä—É–µ—Ç –Ω–æ–≤—ã–µ –≤–∏–¥–µ–æ, –∫–æ–º–±–∏–Ω–∏—Ä—É—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ –∏ –≤—Ä–µ–º–µ–Ω–Ω–æ, –∏ —Å–æ–∑–¥–∞–µ—Ç –≤–æ–ø—Ä–æ—Å–Ω–æ-–æ—Ç–≤–µ—Ç–Ω—ã–µ –ø–∞—Ä—ã –∫ –Ω–∏–º. –ù–∞ –æ—Å–Ω–æ–≤–µ VISTA —Å–æ–∑–¥–∞–Ω –¥–∞—Ç–∞—Å–µ—Ç VISTA-400K, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª–∏–ª —É–ª—É—á—à–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –º–æ–¥–µ–ª–µ–π –Ω–∞ 3.3% –≤ –∑–∞–¥–∞—á–∞—Ö –ø–æ–Ω–∏–º–∞–Ω–∏—è –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ. –¢–∞–∫–∂–µ –∞–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ –ø–µ—Ä–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ HRVideoBench –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è –≤–∏–¥–µ–æ –≤—ã—Å–æ–∫–æ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è.'}, 'en': {'title': 'Enhancing Video Understanding with VISTA: A Data-Centric Approach', 'desc': "This paper introduces VISTA, a framework designed to improve the understanding of long-duration and high-resolution videos by creating synthetic video instruction-following pairs. VISTA utilizes existing video-caption datasets to spatially and temporally augment videos, generating new high-quality video data. The authors developed seven augmentation methods and created the VISTA-400K dataset, which significantly enhances the training of large multimodal models (LMMs). The results show that finetuning LMMs on this dataset leads to notable performance improvements on various benchmarks, demonstrating the framework's effectiveness in video comprehension tasks."}, 'zh': {'title': 'VISTAÔºöÊèêÂçáËßÜÈ¢ëÁêÜËß£ÁöÑÊñ∞ÊñπÊ≥ï', 'desc': 'ÂΩìÂâçÁöÑÂ§ßÂûãÂ§öÊ®°ÊÄÅÊ®°ÂûãÂú®Â§ÑÁêÜÈïøÊó∂ÈïøÊàñÈ´òÂàÜËæ®ÁéáËßÜÈ¢ëÊó∂Èù¢‰∏¥ÈáçÂ§ßÊåëÊàòÔºå‰∏ªË¶ÅÊòØÁî±‰∫éÁº∫‰πèÈ´òË¥®ÈáèÁöÑÊï∞ÊçÆÈõÜ„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜVISTAÔºå‰∏Ä‰∏™ÁÆÄÂçïËÄåÊúâÊïàÁöÑËßÜÈ¢ëÊó∂Á©∫Â¢ûÂº∫Ê°ÜÊû∂ÔºåËÉΩÂ§ü‰ªéÁé∞ÊúâÁöÑËßÜÈ¢ë-Â≠óÂπïÊï∞ÊçÆÈõÜ‰∏≠ÂêàÊàêÈïøÊó∂ÈïøÂíåÈ´òÂàÜËæ®ÁéáÁöÑËßÜÈ¢ëÊåá‰ª§ÂØπ„ÄÇVISTAÈÄöËøáÁ©∫Èó¥ÂíåÊó∂Èó¥ÁöÑÁªÑÂêàÔºåÂàõÂª∫Êñ∞ÁöÑÂêàÊàêËßÜÈ¢ëÔºåÂπ∂ÁîüÊàê‰∏éËøô‰∫õÊñ∞ÂêàÊàêËßÜÈ¢ëÁõ∏ÂÖ≥ÁöÑÈóÆÈ¢ò-Á≠îÊ°àÂØπ„ÄÇÈÄöËøáÂú®Êàë‰ª¨ÁöÑÊï∞ÊçÆ‰∏äÂæÆË∞ÉÂêÑÁßçËßÜÈ¢ëÂ§öÊ®°ÊÄÅÊ®°ÂûãÔºåÂπ≥ÂùáÊèêÈ´ò‰∫Ü3.3%ÁöÑÊÄßËÉΩÔºåËøõ‰∏ÄÊ≠•È™åËØÅ‰∫ÜÊàë‰ª¨Ê°ÜÊû∂ÁöÑÊúâÊïàÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2411.18499', 'title': 'GATE OpenING: A Comprehensive Benchmark for Judging Open-ended Interleaved Image-Text Generation', 'url': 'https://huggingface.co/papers/2411.18499', 'abstract': 'Multimodal Large Language Models (MLLMs) have made significant strides in visual understanding and generation tasks. However, generating interleaved image-text content remains a challenge, which requires integrated multimodal understanding and generation abilities. While the progress in unified models offers new solutions, existing benchmarks are insufficient for evaluating these methods due to data size and diversity limitations. To bridge this gap, we introduce GATE OpenING (OpenING), a comprehensive benchmark comprising 5,400 high-quality human-annotated instances across 56 real-world tasks. OpenING covers diverse daily scenarios such as travel guide, design, and brainstorming, offering a robust platform for challenging interleaved generation methods. In addition, we present IntJudge, a judge model for evaluating open-ended multimodal generation methods. Trained with a novel data pipeline, our IntJudge achieves an agreement rate of 82. 42% with human judgments, outperforming GPT-based evaluators by 11.34%. Extensive experiments on OpenING reveal that current interleaved generation methods still have substantial room for improvement. Key findings on interleaved image-text generation are further presented to guide the development of next-generation models. The OpenING is open-sourced at https://opening.github.io.', 'score': 16, 'issue_id': 914, 'pub_date': '2024-11-27', 'pub_date_card': {'ru': '27 –Ω–æ—è–±—Ä—è', 'en': 'November 27', 'zh': '11Êúà27Êó•'}, 'hash': 'bec1bf0079934886', 'authors': ['Pengfei Zhou', 'Xiaopeng Peng', 'Jiajun Song', 'Chuanhao Li', 'Zhaopan Xu', 'Yue Yang', 'Ziyao Guo', 'Hao Zhang', 'Yuqi Lin', 'Yefei He', 'Lirui Zhao', 'Shuo Liu', 'Tianhua Li', 'Yuxuan Xie', 'Xiaojun Chang', 'Yu Qiao', 'Wenqi Shao', 'Kaipeng Zhang'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2411.18499.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#open_source', '#dataset', '#games'], 'emoji': 'üîÄ', 'ru': {'title': '–ù–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö GATE OpenING –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –∑–∞–¥–∞—á–∞—Ö –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —á–µ—Ä–µ–¥—É—é—â–µ–≥–æ—Å—è —Ç–µ–∫—Å—Ç–æ–≤–æ-–≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞. OpenING –≤–∫–ª—é—á–∞–µ—Ç 5400 –∞–Ω–Ω–æ—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ –∏–∑ 56 —Ä–µ–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–æ–¥–µ–ª—å IntJudge –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–∫—Ä—ã—Ç–æ–≥–æ —Ç–∏–ø–∞, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç GPT-based –æ—Ü–µ–Ω—â–∏–∫–æ–≤ –Ω–∞ 11.34%. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —á–µ—Ä–µ–¥—É—é—â–µ–≥–æ—Å—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∏–º–µ—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è.'}, 'en': {'title': 'Bridging the Gap in Multimodal Generation with OpenING!', 'desc': 'This paper discusses the advancements in Multimodal Large Language Models (MLLMs) for tasks that involve both images and text. It highlights the challenges of generating content that combines these two modalities effectively, which requires a deep understanding of both. To address the limitations of existing benchmarks, the authors introduce GATE OpenING, a new dataset with 5,400 annotated examples across 56 tasks that reflect real-world scenarios. Additionally, they present IntJudge, a model designed to evaluate multimodal generation methods, which shows improved agreement with human judgments compared to previous evaluators.'}, 'zh': {'title': 'Êé®Âä®Â§öÊ®°ÊÄÅÁîüÊàêÁöÑÂü∫ÂáÜ‰∏éËØÑ‰º∞', 'desc': 'Â§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÂú®ËßÜËßâÁêÜËß£ÂíåÁîüÊàê‰ªªÂä°‰∏äÂèñÂæó‰∫ÜÊòæËëóËøõÂ±ï„ÄÇÁÑ∂ËÄåÔºåÁîüÊàê‰∫§ÈîôÁöÑÂõæÂÉè-ÊñáÊú¨ÂÜÖÂÆπ‰ªçÁÑ∂ÊòØ‰∏Ä‰∏™ÊåëÊàòÔºåËøôÈúÄË¶ÅÁªºÂêàÁöÑÂ§öÊ®°ÊÄÅÁêÜËß£ÂíåÁîüÊàêËÉΩÂäõ„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏ÄÈóÆÈ¢òÔºåÊàë‰ª¨ÂºïÂÖ•‰∫ÜGATE OpenINGÔºàOpenINGÔºâÔºåËøôÊòØ‰∏Ä‰∏™ÂåÖÂê´5400‰∏™È´òË¥®Èáè‰∫∫Á±ªÊ†áÊ≥®ÂÆû‰æãÁöÑÁªºÂêàÂü∫ÂáÜÔºåÊ∂µÁõñ56‰∏™ÁúüÂÆû‰∏ñÁïå‰ªªÂä°„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂ËøòÊèêÂá∫‰∫ÜIntJudgeÔºå‰∏Ä‰∏™Áî®‰∫éËØÑ‰º∞ÂºÄÊîæÂºèÂ§öÊ®°ÊÄÅÁîüÊàêÊñπÊ≥ïÁöÑËØÑ‰º∞Ê®°ÂûãÔºåÊòæÁ§∫Âá∫‰∏é‰∫∫Á±ªÂà§Êñ≠ÁöÑÈ´ò‰∏ÄËá¥ÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2412.01819', 'title': 'Switti: Designing Scale-Wise Transformers for Text-to-Image Synthesis', 'url': 'https://huggingface.co/papers/2412.01819', 'abstract': 'This work presents Switti, a scale-wise transformer for text-to-image generation. Starting from existing next-scale prediction AR models, we first explore them for T2I generation and propose architectural modifications to improve their convergence and overall performance. We then observe that self-attention maps of our pretrained scale-wise AR model exhibit weak dependence on preceding scales. Based on this insight, we propose a non-AR counterpart facilitating {sim}11% faster sampling and lower memory usage while also achieving slightly better generation quality.Furthermore, we reveal that classifier-free guidance at high-resolution scales is often unnecessary and can even degrade performance. %may be not only unnecessary but potentially detrimental. By disabling guidance at these scales, we achieve an additional sampling acceleration of {sim}20% and improve the generation of fine-grained details. Extensive human preference studies and automated evaluations show that Switti outperforms existing T2I AR models and competes with state-of-the-art T2I diffusion models while being up to 7{times} faster.', 'score': 15, 'issue_id': 914, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 2', 'zh': '12Êúà2Êó•'}, 'hash': '25cf4512f592aea4', 'authors': ['Anton Voronov', 'Denis Kuznedelev', 'Mikhail Khoroshikh', 'Valentin Khrulkov', 'Dmitry Baranchuk'], 'affiliations': ['HSE University', 'MIPT', 'Skoltech', 'Yandex Research'], 'pdf_title_img': 'assets/pdf/title_img/2412.01819.jpg', 'data': {'categories': ['#diffusion', '#architecture', '#video', '#cv', '#optimization'], 'emoji': 'üñºÔ∏è', 'ru': {'title': 'Switti: –±—ã—Å—Ç—Ä—ã–π –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç—É', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Switti - —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É –æ–ø–∏—Å–∞–Ω–∏—é. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –ù–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è –æ —Å–ª–∞–±–æ–π –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –∫–∞—Ä—Ç –≤–Ω–∏–º–∞–Ω–∏—è –æ—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –º–∞—Å—à—Ç–∞–±–æ–≤, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–∞ –Ω–µ–∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—é—â–∞—è —É—Å–∫–æ—Ä–µ–Ω–∏–µ –∏ —Å–Ω–∏–∂–µ–Ω–∏–µ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏—è –ø–∞–º—è—Ç–∏. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ç–∞–∫–∂–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –æ—Ç–∫–ª—é—á–µ–Ω–∏–µ guidance –Ω–∞ –≤—ã—Å–æ–∫–∏—Ö —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è—Ö –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ —É—Å–∫–æ—Ä—è–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –∏ —É–ª—É—á—à–∞–µ—Ç –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—é.'}, 'en': {'title': 'Switti: Accelerating Text-to-Image Generation with Scale-Wise Transformers', 'desc': 'This paper introduces Switti, a new transformer model designed for generating images from text. It builds on existing autoregressive (AR) models by making architectural changes that enhance performance and speed. The authors find that the self-attention mechanisms in their model do not rely heavily on previous scales, allowing for a non-autoregressive approach that speeds up sampling and reduces memory usage. Additionally, they demonstrate that removing classifier-free guidance at higher resolutions can improve detail generation and further accelerate the process, leading to a model that is significantly faster than current state-of-the-art methods.'}, 'zh': {'title': 'SwittiÔºöÂä†ÈÄüÊñáÊú¨Âà∞ÂõæÂÉèÁîüÊàêÁöÑÂèòÊç¢Âô®', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫ÜSwittiÔºå‰∏ÄÁßçÁî®‰∫éÊñáÊú¨Âà∞ÂõæÂÉèÁîüÊàêÁöÑÂ∞∫Â∫¶ÂèòÊç¢Âô®„ÄÇÊàë‰ª¨‰ªéÁé∞ÊúâÁöÑ‰∏ã‰∏ÄÂ∞∫Â∫¶È¢ÑÊµãËá™ÂõûÂΩíÊ®°ÂûãÂá∫ÂèëÔºåÊé¢Á¥¢ÂÖ∂Âú®T2IÁîüÊàê‰∏≠ÁöÑÂ∫îÁî®ÔºåÂπ∂ÊèêÂá∫Êû∂ÊûÑ‰øÆÊîπ‰ª•ÊèêÈ´òÊî∂ÊïõÊÄßÂíåÊï¥‰ΩìÊÄßËÉΩ„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÊàë‰ª¨ÁöÑÈ¢ÑËÆ≠ÁªÉÂ∞∫Â∫¶Ëá™ÂõûÂΩíÊ®°ÂûãÁöÑËá™Ê≥®ÊÑèÂäõÂõæÂØπÂâç‰∏ÄÂ∞∫Â∫¶ÁöÑ‰æùËµñÊÄßËæÉÂº±ÔºåÂõ†Ê≠§Êàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÈùûËá™ÂõûÂΩíÁöÑÊõø‰ª£ÊñπÊ°àÔºåËÉΩÂ§üÂÆûÁé∞Á∫¶11%ÁöÑÈááÊ†∑Âä†ÈÄüÂíåÊõ¥‰ΩéÁöÑÂÜÖÂ≠ò‰ΩøÁî®ÔºåÂêåÊó∂ÁîüÊàêË¥®ÈáèÁï•ÊúâÊèêÂçá„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ÂèëÁé∞È´òÂàÜËæ®ÁéáÂ∞∫Â∫¶‰∏ãÁöÑÊó†ÂàÜÁ±ªÂô®ÂºïÂØºÈÄöÂ∏∏ÊòØ‰∏çÂøÖË¶ÅÁöÑÔºåÁîöËá≥ÂèØËÉΩ‰ºöÈôç‰ΩéÊÄßËÉΩ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2412.00154', 'title': 'o1-Coder: an o1 Replication for Coding', 'url': 'https://huggingface.co/papers/2412.00154', 'abstract': "The technical report introduces O1-CODER, an attempt to replicate OpenAI's o1 model with a focus on coding tasks. It integrates reinforcement learning (RL) and Monte Carlo Tree Search (MCTS) to enhance the model's System-2 thinking capabilities. The framework includes training a Test Case Generator (TCG) for standardized code testing, using MCTS to generate code data with reasoning processes, and iteratively fine-tuning the policy model to initially produce pseudocode, followed by the generation of the full code. The report also addresses the opportunities and challenges in deploying o1-like models in real-world applications, suggesting transitioning to the System-2 paradigm and highlighting the imperative for environment state updates. Updated model progress and experimental results will be reported in subsequent versions. All source code, curated datasets, as well as the derived models will be disclosed at https://github.com/ADaM-BJTU/O1-CODER .", 'score': 14, 'issue_id': 912, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 –Ω–æ—è–±—Ä—è', 'en': 'November 29', 'zh': '11Êúà29Êó•'}, 'hash': '4a9b72e0b9a6ba64', 'authors': ['Yuxiang Zhang', 'Shangxi Wu', 'Yuqi Yang', 'Jiangming Shu', 'Jinlin Xiao', 'Chao Kong', 'Jitao Sang'], 'affiliations': ['School of Computer Science and Technology, Beijing Jiaotong University, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2412.00154.jpg', 'data': {'categories': ['#dataset', '#training', '#open_source', '#optimization', '#reasoning', '#rl'], 'emoji': 'üß†', 'ru': {'title': 'O1-CODER: –£—Å–∏–ª–µ–Ω–∏–µ –ò–ò –¥–ª—è –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è —á–µ—Ä–µ–∑ –°–∏—Å—Ç–µ–º—É-2 –º—ã—à–ª–µ–Ω–∏—è', 'desc': '–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –æ—Ç—á–µ—Ç –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç O1-CODER - –ø–æ–ø—ã—Ç–∫—É –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ—Å—Ç–∏ –º–æ–¥–µ–ª—å o1 –æ—Ç OpenAI –¥–ª—è –∑–∞–¥–∞—á –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è. –ú–æ–¥–µ–ª—å –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –∏ –º–µ—Ç–æ–¥ –ú–æ–Ω—Ç–µ-–ö–∞—Ä–ª–æ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –º—ã—à–ª–µ–Ω–∏—è –°–∏—Å—Ç–µ–º—ã-2. –§—Ä–µ–π–º–≤–æ—Ä–∫ –≤–∫–ª—é—á–∞–µ—Ç –æ–±—É—á–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö —Å–ª—É—á–∞–µ–≤, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ MCTS –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞ —Å –ø—Ä–æ—Ü–µ—Å—Å–∞–º–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, –∏ –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω—É—é –¥–æ–≤–æ–¥–∫—É –º–æ–¥–µ–ª–∏ –ø–æ–ª–∏—Ç–∏–∫–∏. –û—Ç—á–µ—Ç —Ç–∞–∫–∂–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –∏ –ø—Ä–æ–±–ª–µ–º—ã —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è –ø–æ–¥–æ–±–Ω—ã—Ö o1 –º–æ–¥–µ–ª–µ–π –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è—Ö.'}, 'en': {'title': 'Enhancing Coding with O1-CODER: A New Approach to System-2 Thinking', 'desc': "The paper presents O1-CODER, a model designed to replicate OpenAI's o1 specifically for coding tasks. It combines reinforcement learning (RL) with Monte Carlo Tree Search (MCTS) to improve the model's ability to think critically and reason through problems. The framework includes a Test Case Generator (TCG) for consistent code testing and uses MCTS to create code data that incorporates reasoning steps. The report discusses the potential and challenges of applying o1-like models in practical scenarios, emphasizing the need for updates in the environment state during the learning process."}, 'zh': {'title': 'O1-CODERÔºöÊèêÂçáÁºñÁ†Å‰ªªÂä°ÁöÑÊô∫ËÉΩÊ®°Âûã', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫ÜO1-CODERÔºåËøôÊòØ‰∏Ä‰∏™Êó®Âú®Â§çÂà∂OpenAIÁöÑo1Ê®°ÂûãÔºå‰∏ìÊ≥®‰∫éÁºñÁ†Å‰ªªÂä°ÁöÑÊäÄÊúØÊä•Âëä„ÄÇËØ•Ê®°ÂûãÁªìÂêà‰∫ÜÂº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâÂíåËíôÁâπÂç°Ê¥õÊ†ëÊêúÁ¥¢ÔºàMCTSÔºâÔºå‰ª•Â¢ûÂº∫ÂÖ∂Á≥ªÁªü2ÊÄùÁª¥ËÉΩÂäõ„ÄÇÊ°ÜÊû∂‰∏≠ÂåÖÊã¨ËÆ≠ÁªÉÊµãËØïÁî®‰æãÁîüÊàêÂô®ÔºàTCGÔºâ‰ª•ËøõË°åÊ†áÂáÜÂåñ‰ª£Á†ÅÊµãËØïÔºåÂà©Áî®MCTSÁîüÊàêÂ∏¶ÊúâÊé®ÁêÜËøáÁ®ãÁöÑ‰ª£Á†ÅÊï∞ÊçÆÔºåÂπ∂Ëø≠‰ª£ÂæÆË∞ÉÁ≠ñÁï•Ê®°ÂûãÔºåÂàùÊ≠•ÁîüÊàê‰º™‰ª£Á†ÅÔºåÈöèÂêéÁîüÊàêÂÆåÊï¥‰ª£Á†Å„ÄÇÊä•ÂëäËøòËÆ®ËÆ∫‰∫ÜÂú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÈÉ®ÁΩ≤Á±ª‰ººo1Ê®°ÂûãÁöÑÊú∫ÈÅáÂíåÊåëÊàòÔºåÂª∫ËÆÆËΩ¨ÂêëÁ≥ªÁªü2ËåÉÂºèÔºåÂπ∂Âº∫Ë∞ÉÁéØÂ¢ÉÁä∂ÊÄÅÊõ¥Êñ∞ÁöÑÈáçË¶ÅÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2411.18671', 'title': 'TAPTRv3: Spatial and Temporal Context Foster Robust Tracking of Any Point in Long Video', 'url': 'https://huggingface.co/papers/2411.18671', 'abstract': 'In this paper, we present TAPTRv3, which is built upon TAPTRv2 to improve its point tracking robustness in long videos. TAPTRv2 is a simple DETR-like framework that can accurately track any point in real-world videos without requiring cost-volume. TAPTRv3 improves TAPTRv2 by addressing its shortage in querying high quality features from long videos, where the target tracking points normally undergo increasing variation over time. In TAPTRv3, we propose to utilize both spatial and temporal context to bring better feature querying along the spatial and temporal dimensions for more robust tracking in long videos. For better spatial feature querying, we present Context-aware Cross-Attention (CCA), which leverages surrounding spatial context to enhance the quality of attention scores when querying image features. For better temporal feature querying, we introduce Visibility-aware Long-Temporal Attention (VLTA) to conduct temporal attention to all past frames while considering their corresponding visibilities, which effectively addresses the feature drifting problem in TAPTRv2 brought by its RNN-like long-temporal modeling. TAPTRv3 surpasses TAPTRv2 by a large margin on most of the challenging datasets and obtains state-of-the-art performance. Even when compared with methods trained with large-scale extra internal data, TAPTRv3 is still competitive.', 'score': 13, 'issue_id': 909, 'pub_date': '2024-11-27', 'pub_date_card': {'ru': '27 –Ω–æ—è–±—Ä—è', 'en': 'November 27', 'zh': '11Êúà27Êó•'}, 'hash': 'f99e1015e9222dc6', 'authors': ['Jinyuan Qu', 'Hongyang Li', 'Shilong Liu', 'Tianhe Ren', 'Zhaoyang Zeng', 'Lei Zhang'], 'affiliations': ['International Digital Economy Academy (IDEA)', 'South China University of Technology', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2411.18671.jpg', 'data': {'categories': ['#long_context', '#benchmark', '#video', '#cv', '#optimization'], 'emoji': 'üëÅÔ∏è', 'ru': {'title': '–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ–≥–æ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è —Ç–æ—á–µ–∫ –≤ –≤–∏–¥–µ–æ', 'desc': 'TAPTRv3 - —ç—Ç–æ —É–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è TAPTRv2 –¥–ª—è –±–æ–ª–µ–µ –Ω–∞–¥–µ–∂–Ω–æ–≥–æ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è —Ç–æ—á–µ–∫ –≤ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ. –°–∏—Å—Ç–µ–º–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π –∏ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –∑–∞–ø—Ä–æ—Å–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤. –í–≤–µ–¥–µ–Ω—ã –¥–≤–∞ –Ω–æ–≤—ã—Ö –º–µ—Ö–∞–Ω–∏–∑–º–∞: Context-aware Cross-Attention (CCA) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ –∏ Visibility-aware Long-Temporal Attention (VLTA) –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞. TAPTRv3 –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –ø—Ä–µ–¥—ã–¥—É—â—É—é –≤–µ—Ä—Å–∏—é –∏ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –Ω–∞–∏–ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ —Å–ª–æ–∂–Ω—ã—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö.'}, 'en': {'title': 'Enhancing Video Point Tracking with TAPTRv3', 'desc': 'TAPTRv3 is an advanced framework designed to enhance point tracking in lengthy videos, building on the previous version, TAPTRv2. It improves the ability to query high-quality features by incorporating both spatial and temporal contexts, which helps in maintaining robust tracking despite variations over time. The paper introduces Context-aware Cross-Attention (CCA) for better spatial feature querying and Visibility-aware Long-Temporal Attention (VLTA) for improved temporal feature querying, effectively addressing issues like feature drifting. TAPTRv3 demonstrates significant performance improvements over TAPTRv2 and competes well against other state-of-the-art methods, even those trained on larger datasets.'}, 'zh': {'title': 'TAPTRv3ÔºöÈïøËßÜÈ¢ëÁÇπË∑üË∏™ÁöÑÊñ∞Á™ÅÁ†¥', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫ÜTAPTRv3ÔºåËøôÊòØÂú®TAPTRv2Âü∫Á°Ä‰∏äÂºÄÂèëÁöÑÔºåÊó®Âú®ÊèêÈ´òÈïøËßÜÈ¢ë‰∏≠ÁöÑÁÇπË∑üË∏™È≤ÅÊ£íÊÄß„ÄÇTAPTRv2ÊòØ‰∏Ä‰∏™ÁÆÄÂçïÁöÑÁ±ª‰ººDETRÁöÑÊ°ÜÊû∂ÔºåÂèØ‰ª•ÂáÜÁ°ÆË∑üË∏™Áé∞ÂÆûËßÜÈ¢ë‰∏≠ÁöÑ‰ªªÊÑèÁÇπÔºåËÄåÊó†ÈúÄÊàêÊú¨‰ΩìÁßØ„ÄÇTAPTRv3ÈÄöËøáÂà©Áî®Á©∫Èó¥ÂíåÊó∂Èó¥‰∏ä‰∏ãÊñáÊù•ÊîπÂñÑÁâπÂæÅÊü•ËØ¢Ôºå‰ªéËÄåÂú®ÈïøËßÜÈ¢ë‰∏≠ÂÆûÁé∞Êõ¥Á®≥ÂÅ•ÁöÑË∑üË∏™„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ä‰∏ãÊñáÊÑüÁü•‰∫§ÂèâÊ≥®ÊÑèÂäõÔºàCCAÔºâÂíåÂèØËßÅÊÄßÊÑüÁü•ÈïøÊó∂Èó¥Ê≥®ÊÑèÂäõÔºàVLTAÔºâÔºåÊòæËëóÊèêÂçá‰∫ÜÁâπÂæÅÊü•ËØ¢ÁöÑË¥®ÈáèÔºåË∂ÖË∂ä‰∫ÜTAPTRv2ÔºåËææÂà∞‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2412.00100', 'title': 'Steering Rectified Flow Models in the Vector Field for Controlled Image Generation', 'url': 'https://huggingface.co/papers/2412.00100', 'abstract': 'Diffusion models (DMs) excel in photorealism, image editing, and solving inverse problems, aided by classifier-free guidance and image inversion techniques. However, rectified flow models (RFMs) remain underexplored for these tasks. Existing DM-based methods often require additional training, lack generalization to pretrained latent models, underperform, and demand significant computational resources due to extensive backpropagation through ODE solvers and inversion processes. In this work, we first develop a theoretical and empirical understanding of the vector field dynamics of RFMs in efficiently guiding the denoising trajectory. Our findings reveal that we can navigate the vector field in a deterministic and gradient-free manner. Utilizing this property, we propose FlowChef, which leverages the vector field to steer the denoising trajectory for controlled image generation tasks, facilitated by gradient skipping. FlowChef is a unified framework for controlled image generation that, for the first time, simultaneously addresses classifier guidance, linear inverse problems, and image editing without the need for extra training, inversion, or intensive backpropagation. Finally, we perform extensive evaluations and show that FlowChef significantly outperforms baselines in terms of performance, memory, and time requirements, achieving new state-of-the-art results. Project Page: https://flowchef.github.io.', 'score': 12, 'issue_id': 911, 'pub_date': '2024-11-27', 'pub_date_card': {'ru': '27 –Ω–æ—è–±—Ä—è', 'en': 'November 27', 'zh': '11Êúà27Êó•'}, 'hash': '5fceae277a0e3d85', 'authors': ['Maitreya Patel', 'Song Wen', 'Dimitris N. Metaxas', 'Yezhou Yang'], 'affiliations': ['Arizona State University', 'Rutgers University'], 'pdf_title_img': 'assets/pdf/title_img/2412.00100.jpg', 'data': {'categories': ['#dataset', '#cv', '#optimization', '#diffusion', '#benchmark'], 'emoji': 'üåä', 'ru': {'title': 'FlowChef: –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω—ã–º –ø–æ–ª–µ–º –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ø—Ä–∞–≤–ª—è–µ–º–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º FlowChef. –û–Ω –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –¥–∏–Ω–∞–º–∏–∫–∏ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–ª—è –≤ —Ä–µ–∫—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (RFM) –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–µ–π —à—É–º–æ–ø–æ–¥–∞–≤–ª–µ–Ω–∏—è. FlowChef –ø–æ–∑–≤–æ–ª—è–µ—Ç —Ä–µ—à–∞—Ç—å –∑–∞–¥–∞—á–∏ —É–ø—Ä–∞–≤–ª—è–µ–º–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –ª–∏–Ω–µ–π–Ω—ã—Ö –æ–±—Ä–∞—Ç–Ω—ã—Ö –∑–∞–¥–∞—á –∏ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏–ª–∏ –∏–Ω–≤–µ—Ä—Å–∏–∏. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ FlowChef –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã –ø–æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º –∫ –ø–∞–º—è—Ç–∏ –∏ –≤—Ä–µ–º–µ–Ω–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π.'}, 'en': {'title': 'FlowChef: Revolutionizing Controlled Image Generation with Efficiency', 'desc': 'This paper introduces FlowChef, a new framework that enhances controlled image generation using rectified flow models (RFMs). Unlike traditional diffusion models, FlowChef efficiently guides the denoising process without requiring additional training or extensive computational resources. It utilizes a unique property of RFMs to navigate the vector field in a deterministic way, allowing for effective classifier guidance and image editing. The results demonstrate that FlowChef outperforms existing methods in performance, memory usage, and processing time, setting new benchmarks in the field.'}, 'zh': {'title': 'FlowChefÔºöÈ´òÊïàÁöÑÂèóÊéßÂõæÂÉèÁîüÊàêÊñ∞ÊñπÊ≥ï', 'desc': 'Êâ©Êï£Ê®°ÂûãÂú®ÁîüÊàêÁúüÂÆûÊÑüÂõæÂÉè„ÄÅÂõæÂÉèÁºñËæëÂíåËß£ÂÜ≥ÈÄÜÈóÆÈ¢òÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜ‰øÆÊ≠£ÊµÅÊ®°ÂûãÂú®Ëøô‰∫õ‰ªªÂä°‰∏≠‰ªçÊú™ÂæóÂà∞ÂÖÖÂàÜÊé¢Á¥¢„ÄÇÁé∞ÊúâÁöÑÂü∫‰∫éÊâ©Êï£Ê®°ÂûãÁöÑÊñπÊ≥ïÈÄöÂ∏∏ÈúÄË¶ÅÈ¢ùÂ§ñÁöÑËÆ≠ÁªÉÔºåÁº∫‰πèÂØπÈ¢ÑËÆ≠ÁªÉÊΩúÂú®Ê®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõÔºåÂπ∂‰∏îÂú®ÊÄßËÉΩ‰∏äË°®Áé∞‰∏ç‰Ω≥ÔºåËÆ°ÁÆóËµÑÊ∫êÊ∂àËÄóÂ§ß„ÄÇÊú¨ÊñáÊèêÂá∫FlowChefÔºåÈÄöËøáÊúâÊïàÂºïÂØºÂéªÂô™ËΩ®ËøπÔºåÂà©Áî®ÂêëÈáèÂú∫ÁöÑÁâπÊÄßÔºåÂÆûÁé∞‰∫ÜÂèóÊéßÂõæÂÉèÁîüÊàê‰ªªÂä°Ôºå‰∏îÊó†ÈúÄÈ¢ùÂ§ñËÆ≠ÁªÉÊàñÂ§çÊùÇÁöÑÂèçÂêë‰º†Êí≠„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåFlowChefÂú®ÊÄßËÉΩ„ÄÅÂÜÖÂ≠òÂíåÊó∂Èó¥ÈúÄÊ±Ç‰∏äÊòæËëó‰ºò‰∫éÂü∫Á∫øÊñπÊ≥ïÔºåËææÂà∞‰∫ÜÊñ∞ÁöÑÊúÄÂÖàËøõÁªìÊûú„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2412.01199', 'title': 'TinyFusion: Diffusion Transformers Learned Shallow', 'url': 'https://huggingface.co/papers/2412.01199', 'abstract': 'Diffusion Transformers have demonstrated remarkable capabilities in image generation but often come with excessive parameterization, resulting in considerable inference overhead in real-world applications. In this work, we present TinyFusion, a depth pruning method designed to remove redundant layers from diffusion transformers via end-to-end learning. The core principle of our approach is to create a pruned model with high recoverability, allowing it to regain strong performance after fine-tuning. To accomplish this, we introduce a differentiable sampling technique to make pruning learnable, paired with a co-optimized parameter to simulate future fine-tuning. While prior works focus on minimizing loss or error after pruning, our method explicitly models and optimizes the post-fine-tuning performance of pruned models. Experimental results indicate that this learnable paradigm offers substantial benefits for layer pruning of diffusion transformers, surpassing existing importance-based and error-based methods. Additionally, TinyFusion exhibits strong generalization across diverse architectures, such as DiTs, MARs, and SiTs. Experiments with DiT-XL show that TinyFusion can craft a shallow diffusion transformer at less than 7% of the pre-training cost, achieving a 2times speedup with an FID score of 2.86, outperforming competitors with comparable efficiency. Code is available at https://github.com/VainF/TinyFusion.', 'score': 11, 'issue_id': 912, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 2', 'zh': '12Êúà2Êó•'}, 'hash': '7d1de7010c3fabd7', 'authors': ['Gongfan Fang', 'Kunjun Li', 'Xinyin Ma', 'Xinchao Wang'], 'affiliations': ['National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2412.01199.jpg', 'data': {'categories': ['#inference', '#diffusion', '#training', '#optimization', '#architecture'], 'emoji': '‚úÇÔ∏è', 'ru': {'title': 'TinyFusion: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –æ–±—Ä–µ–∑–∫–∞ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞', 'desc': 'TinyFusion - —ç—Ç–æ –º–µ—Ç–æ–¥ –æ–±—Ä–µ–∑–∫–∏ –≥–ª—É–±–∏–Ω—ã –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞—Ö. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä—É–µ–º—É—é —Ç–µ—Ö–Ω–∏–∫—É —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –æ–±—É—á–∞–µ–º–æ–π –æ–±—Ä–µ–∑–∫–∏ –∏ –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –ø–æ—Å–ª–µ –¥–æ–æ–±—É—á–µ–Ω–∏—è. TinyFusion –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã –æ–±—Ä–µ–∑–∫–∏ –∏ —Ö–æ—Ä–æ—à–æ –æ–±–æ–±—â–∞–µ—Ç—Å—è –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –º–µ—Ç–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–æ–∑–¥–∞—Ç—å –∫–æ–º–ø–∞–∫—Ç–Ω—ã–π –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä —Å –¥–≤—É–∫—Ä–∞—Ç–Ω—ã–º —É—Å–∫–æ—Ä–µ–Ω–∏–µ–º –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.'}, 'en': {'title': 'TinyFusion: Efficient Layer Pruning for Fast and Effective Diffusion Transformers', 'desc': 'This paper introduces TinyFusion, a method for optimizing diffusion transformers by removing unnecessary layers through a process called depth pruning. The approach focuses on maintaining high performance after the model is pruned, using a learnable technique that allows the model to adapt during fine-tuning. Unlike previous methods that only aim to reduce loss or error, TinyFusion specifically targets the performance of the pruned model post-fine-tuning. Experimental results show that TinyFusion significantly improves layer pruning efficiency and generalization across various architectures, achieving faster inference times and better performance metrics.'}, 'zh': {'title': 'TinyFusionÔºöÈ´òÊïàÂâ™ÊûùÔºåÊèêÂçáÊâ©Êï£ÂèòÊç¢Âô®ÊÄßËÉΩ', 'desc': 'Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫TinyFusionÁöÑÊ∑±Â∫¶Ââ™ÊûùÊñπÊ≥ïÔºåÊó®Âú®ÂáèÂ∞ëÊâ©Êï£ÂèòÊç¢Âô®‰∏≠ÁöÑÂÜó‰ΩôÂ±ÇÔºå‰ªéËÄåÈôç‰ΩéÊé®ÁêÜÂºÄÈîÄ„ÄÇÊàë‰ª¨ÁöÑÊñπÊ≥ïÈÄöËøáÁ´ØÂà∞Á´ØÂ≠¶‰π†ÂÆûÁé∞Ââ™ÊûùÔºåÂπ∂Á°Æ‰øùÂâ™ÊûùÂêéÁöÑÊ®°ÂûãÂú®ÂæÆË∞ÉÂêéËÉΩÂ§üÊÅ¢Â§çÂº∫Â§ßÁöÑÊÄßËÉΩ„ÄÇTinyFusionÂºïÂÖ•‰∫Ü‰∏ÄÁßçÂèØÂæÆÂàÜÈááÊ†∑ÊäÄÊúØÔºå‰ΩøÂæóÂâ™ÊûùËøáÁ®ãÂèØÂ≠¶‰π†ÔºåÂπ∂‰∏éÂÖ±Âêå‰ºòÂåñÁöÑÂèÇÊï∞ÁªìÂêàÔºå‰ª•Ê®°ÊãüÊú™Êù•ÁöÑÂæÆË∞ÉÊïàÊûú„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåTinyFusionÂú®Êâ©Êï£ÂèòÊç¢Âô®ÁöÑÂ±ÇÂâ™ÊûùÊñπÈù¢‰ºò‰∫éÁé∞ÊúâÁöÑÊñπÊ≥ïÔºåÂ±ïÁé∞Âá∫Âº∫Â§ßÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2412.00568', 'title': 'The Well: a Large-Scale Collection of Diverse Physics Simulations for Machine Learning', 'url': 'https://huggingface.co/papers/2412.00568', 'abstract': 'Machine learning based surrogate models offer researchers powerful tools for accelerating simulation-based workflows. However, as standard datasets in this space often cover small classes of physical behavior, it can be difficult to evaluate the efficacy of new approaches. To address this gap, we introduce the Well: a large-scale collection of datasets containing numerical simulations of a wide variety of spatiotemporal physical systems. The Well draws from domain experts and numerical software developers to provide 15TB of data across 16 datasets covering diverse domains such as biological systems, fluid dynamics, acoustic scattering, as well as magneto-hydrodynamic simulations of extra-galactic fluids or supernova explosions. These datasets can be used individually or as part of a broader benchmark suite. To facilitate usage of the Well, we provide a unified PyTorch interface for training and evaluating models. We demonstrate the function of this library by introducing example baselines that highlight the new challenges posed by the complex dynamics of the Well. The code and data is available at https://github.com/PolymathicAI/the_well.', 'score': 10, 'issue_id': 923, 'pub_date': '2024-11-30', 'pub_date_card': {'ru': '30 –Ω–æ—è–±—Ä—è', 'en': 'November 30', 'zh': '11Êúà30Êó•'}, 'hash': '1352e219e54ac56e', 'authors': ['Ruben Ohana', 'Michael McCabe', 'Lucas Meyer', 'Rudy Morel', 'Fruzsina J. Agocs', 'Miguel Beneitez', 'Marsha Berger', 'Blakesley Burkhart', 'Stuart B. Dalziel', 'Drummond B. Fielding', 'Daniel Fortunato', 'Jared A. Goldberg', 'Keiya Hirashima', 'Yan-Fei Jiang', 'Rich R. Kerswell', 'Suryanarayana Maddu', 'Jonah Miller', 'Payel Mukhopadhyay', 'Stefan S. Nixon', 'Jeff Shen', 'Romain Watteaux', 'Bruno R√©galdo-Saint Blancard', 'Fran√ßois Rozet', 'Liam H. Parker', 'Miles Cranmer', 'Shirley Ho'], 'affiliations': ['CEA DAM', 'Cornell University', 'Flatiron Institute', 'Los Alamos National Laboratory', 'New York University', 'Polymathic AI', 'Princeton University', 'Rutgers University', 'University of California, Berkeley', 'University of Cambridge', 'University of Colorado, Boulder', 'University of Li√®ge', 'University of Tokyo'], 'pdf_title_img': 'assets/pdf/title_img/2412.00568.jpg', 'data': {'categories': ['#data', '#dataset', '#benchmark'], 'emoji': 'üåä', 'ru': {'title': 'Well: –º–∞—Å—à—Ç–∞–±–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è —Å—É—Ä—Ä–æ–≥–∞—Ç–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ —Ñ–∏–∑–∏—á–µ—Å–∫–æ–º –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–∏', 'desc': "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º 'Well' –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å—É—Ä—Ä–æ–≥–∞—Ç–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –≤ –æ–±–ª–∞—Å—Ç–∏ —á–∏—Å–ª–µ–Ω–Ω–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è. 'Well' —Å–æ–¥–µ—Ä–∂–∏—Ç 15 –¢–ë –¥–∞–Ω–Ω—ã—Ö –∏–∑ 16 –Ω–∞–±–æ—Ä–æ–≤, –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏—Ö —Ä–∞–∑–ª–∏—á–Ω—ã–µ –æ–±–ª–∞—Å—Ç–∏ —Ñ–∏–∑–∏–∫–∏, –≤–∫–ª—é—á–∞—è –±–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ —Å–∏—Å—Ç–µ–º—ã, –≥–∏–¥—Ä–æ–¥–∏–Ω–∞–º–∏–∫—É –∏ –∞—Å—Ç—Ä–æ—Ñ–∏–∑–∏–∫—É. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—é—Ç —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å PyTorch –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π –Ω–∞ —ç—Ç–∏—Ö –¥–∞–Ω–Ω—ã—Ö. –ù–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–≤–∞–Ω –ø–æ–º–æ—á—å –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—è–º –≤ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è —Ä–∞–±–æ—á–∏—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã—Ö –Ω–∞ —Å–∏–º—É–ª—è—Ü–∏—è—Ö."}, 'en': {'title': 'Unlocking Complex Simulations with the Well: A New Dataset for Machine Learning', 'desc': 'This paper presents a large-scale collection of datasets called the Well, designed to enhance machine learning surrogate models for simulation-based workflows. The Well contains 15TB of data across 16 diverse datasets, covering various physical systems such as fluid dynamics and biological systems. By providing a unified PyTorch interface, the Well facilitates the training and evaluation of machine learning models on these complex datasets. The authors also introduce example baselines to demonstrate the unique challenges posed by the intricate dynamics represented in the Well.'}, 'zh': {'title': 'Êú∫Âô®Â≠¶‰π†Âä†ÈÄü‰ªøÁúüÔºöÊé¢Á¥¢"Well"Êï∞ÊçÆÈõÜ', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂü∫‰∫éÊú∫Âô®Â≠¶‰π†ÁöÑÊõø‰ª£Ê®°ÂûãÔºåÊó®Âú®Âä†ÈÄüÂü∫‰∫é‰ªøÁúüÁöÑÂ∑•‰ΩúÊµÅÁ®ã„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏Ä‰∏™Âêç‰∏∫"Well"ÁöÑÂ§ßËßÑÊ®°Êï∞ÊçÆÈõÜÔºåÂåÖÂê´Â§öÁßçÊó∂Á©∫Áâ©ÁêÜÁ≥ªÁªüÁöÑÊï∞ÂÄº‰ªøÁúüÊï∞ÊçÆÔºåÊÄªËÆ°15TBÔºåÊ∂µÁõñÁîüÁâ©Á≥ªÁªü„ÄÅÊµÅ‰ΩìÂä®ÂäõÂ≠¶„ÄÅÂ£∞Êï£Â∞ÑÁ≠âÂ§ö‰∏™È¢ÜÂüü„ÄÇËØ•Êï∞ÊçÆÈõÜ‰∏∫Á†îÁ©∂‰∫∫ÂëòÊèê‰æõ‰∫Ü‰∏∞ÂØåÁöÑËµÑÊ∫êÔºå‰ª•ËØÑ‰º∞Êñ∞ÊñπÊ≥ïÁöÑÊúâÊïàÊÄßÔºåÂπ∂ÂèØÂçïÁã¨‰ΩøÁî®Êàñ‰Ωú‰∏∫Êõ¥ÂπøÊ≥õÂü∫ÂáÜÂ•ó‰ª∂ÁöÑ‰∏ÄÈÉ®ÂàÜ„ÄÇ‰∏∫‰∫ÜÊñπ‰æø‰ΩøÁî®ÔºåÊàë‰ª¨Êèê‰æõ‰∫ÜÁªü‰∏ÄÁöÑPyTorchÊé•Âè£ÔºåÂ∏ÆÂä©ËÆ≠ÁªÉÂíåËØÑ‰º∞Ê®°ÂûãÔºåÂπ∂Â±ïÁ§∫‰∫ÜÊñ∞ÁöÑÊåëÊàò„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2412.00174', 'title': 'SOLAMI: Social Vision-Language-Action Modeling for Immersive Interaction with 3D Autonomous Characters', 'url': 'https://huggingface.co/papers/2412.00174', 'abstract': "Human beings are social animals. How to equip 3D autonomous characters with similar social intelligence that can perceive, understand and interact with humans remains an open yet foundamental problem. In this paper, we introduce SOLAMI, the first end-to-end Social vision-Language-Action (VLA) Modeling framework for Immersive interaction with 3D autonomous characters. Specifically, SOLAMI builds 3D autonomous characters from three aspects: (1) Social VLA Architecture: We propose a unified social VLA framework to generate multimodal response (speech and motion) based on the user's multimodal input to drive the character for social interaction. (2) Interactive Multimodal Data: We present SynMSI, a synthetic multimodal social interaction dataset generated by an automatic pipeline using only existing motion datasets to address the issue of data scarcity. (3) Immersive VR Interface: We develop a VR interface that enables users to immersively interact with these characters driven by various architectures. Extensive quantitative experiments and user studies demonstrate that our framework leads to more precise and natural character responses (in both speech and motion) that align with user expectations with lower latency.", 'score': 10, 'issue_id': 913, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 –Ω–æ—è–±—Ä—è', 'en': 'November 29', 'zh': '11Êúà29Êó•'}, 'hash': '5a30d9c535229ab0', 'authors': ['Jianping Jiang', 'Weiye Xiao', 'Zhengyu Lin', 'Huaizhong Zhang', 'Tianxiang Ren', 'Yang Gao', 'Zhiqian Lin', 'Zhongang Cai', 'Lei Yang', 'Ziwei Liu'], 'affiliations': ['S-Lab, Nanyang Technological University', 'SenseTime Research'], 'pdf_title_img': 'assets/pdf/title_img/2412.00174.jpg', 'data': {'categories': ['#synthetic', '#games', '#dataset', '#agents', '#3d', '#multimodal'], 'emoji': 'ü§ñ', 'ru': {'title': '–°–æ–∑–¥–∞–Ω–∏–µ —Å–æ—Ü–∏–∞–ª—å–Ω–æ —É–º–Ω—ã—Ö 3D –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π –¥–ª—è –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–π —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω SOLAMI - –ø–µ—Ä–≤—ã–π —Å–∫–≤–æ–∑–Ω–æ–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Å–æ—Ü–∏–∞–ª—å–Ω–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –∑—Ä–µ–Ω–∏—è-—è–∑—ã–∫–∞-–¥–µ–π—Å—Ç–≤–∏—è (VLA) –¥–ª—è –∏–º–º–µ—Ä—Å–∏–≤–Ω–æ–≥–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å 3D –∞–≤—Ç–æ–Ω–æ–º–Ω—ã–º–∏ –ø–µ—Ä—Å–æ–Ω–∞–∂–∞–º–∏. –§—Ä–µ–π–º–≤–æ—Ä–∫ –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É —Å–æ—Ü–∏–∞–ª—å–Ω–æ–≥–æ VLA –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –≤–≤–æ–¥–∞. –î–ª—è —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã –Ω–µ—Ö–≤–∞—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö –∞–≤—Ç–æ—Ä—ã —Å–æ–∑–¥–∞–ª–∏ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö SynMSI, –∏—Å–ø–æ–ª—å–∑—É—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö –æ –¥–≤–∏–∂–µ–Ω–∏–∏. –¢–∞–∫–∂–µ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω VR-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è –∏–º–º–µ—Ä—Å–∏–≤–Ω–æ–≥–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π —Å –ø–µ—Ä—Å–æ–Ω–∞–∂–∞–º–∏.'}, 'en': {'title': 'Empowering 3D Characters with Social Intelligence through SOLAMI', 'desc': 'This paper presents SOLAMI, a novel framework designed to enhance the social intelligence of 3D autonomous characters. It integrates a Social Vision-Language-Action (VLA) architecture that allows characters to respond to user inputs with both speech and motion, facilitating more natural interactions. To support this, the authors introduce SynMSI, a synthetic dataset that helps overcome the challenge of limited training data for social interactions. Additionally, a virtual reality interface is developed to provide users with an immersive experience when interacting with these characters, resulting in improved response accuracy and reduced latency.'}, 'zh': {'title': 'Ëµã‰∫à3DËßíËâ≤Á§æ‰∫§Êô∫ËÉΩÁöÑÂàõÊñ∞Ê°ÜÊû∂', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫ÜSOLAMIÔºåËøôÊòØÁ¨¨‰∏Ä‰∏™Á´ØÂà∞Á´ØÁöÑÁ§æ‰ºöËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÔºàVLAÔºâÂª∫Ê®°Ê°ÜÊû∂ÔºåÊó®Âú®‰∏é3DËá™‰∏ªËßíËâ≤ËøõË°åÊ≤âÊµ∏Âºè‰∫íÂä®„ÄÇSOLAMI‰ªé‰∏â‰∏™ÊñπÈù¢ÊûÑÂª∫3DËá™‰∏ªËßíËâ≤ÔºöÈ¶ñÂÖàÔºåÊèêÂá∫‰∫ÜÁªü‰∏ÄÁöÑÁ§æ‰ºöVLAÊû∂ÊûÑÔºåÊ†πÊçÆÁî®Êà∑ÁöÑÂ§öÊ®°ÊÄÅËæìÂÖ•ÁîüÊàêÂ§öÊ®°ÊÄÅÂìçÂ∫îÔºàËØ≠Èü≥ÂíåÂä®‰ΩúÔºâÔºå‰ª•È©±Âä®ËßíËâ≤ËøõË°åÁ§æ‰∫§‰∫íÂä®„ÄÇÂÖ∂Ê¨°Ôºå‰ªãÁªç‰∫ÜSynMSIÔºåËøôÊòØ‰∏Ä‰∏™ÂêàÊàêÁöÑÂ§öÊ®°ÊÄÅÁ§æ‰∫§‰∫íÂä®Êï∞ÊçÆÈõÜÔºåÈÄöËøáËá™Âä®ÂåñÊµÅÁ®ãÁîüÊàêÔºåËß£ÂÜ≥‰∫ÜÊï∞ÊçÆÁ®ÄÁº∫ÁöÑÈóÆÈ¢ò„ÄÇÊúÄÂêéÔºåÂºÄÂèë‰∫Ü‰∏Ä‰∏™ËôöÊãüÁé∞ÂÆûÊé•Âè£Ôºå‰ΩøÁî®Êà∑ËÉΩÂ§ü‰∏éËøô‰∫õËßíËâ≤ËøõË°åÊ≤âÊµ∏Âºè‰∫íÂä®ÔºåÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•Ê°ÜÊû∂ËÉΩÂ§üÊèê‰æõÊõ¥Á≤æÁ°ÆÂíåËá™ÁÑ∂ÁöÑËßíËâ≤ÂìçÂ∫î„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2412.01822', 'title': 'VLsI: Verbalized Layers-to-Interactions from Large to Small Vision Language Models', 'url': 'https://huggingface.co/papers/2412.01822', 'abstract': 'The recent surge in high-quality visual instruction tuning samples from closed-source vision-language models (VLMs) such as GPT-4V has accelerated the release of open-source VLMs across various model sizes. However, scaling VLMs to improve performance using larger models brings significant computational challenges, especially for deployment on resource-constrained devices like mobile platforms and robots. To address this, we propose VLsI: Verbalized Layers-to-Interactions, a new VLM family in 2B and 7B model sizes, which prioritizes efficiency without compromising accuracy. VLsI leverages a unique, layer-wise distillation process, introducing intermediate "verbalizers" that map features from each layer to natural language space, allowing smaller VLMs to flexibly align with the reasoning processes of larger VLMs. This approach mitigates the training instability often encountered in output imitation and goes beyond typical final-layer tuning by aligning the small VLMs\' layer-wise progression with that of the large ones. We validate VLsI across ten challenging vision-language benchmarks, achieving notable performance gains (11.0% for 2B and 17.4% for 7B) over GPT-4V without the need for model scaling, merging, or architectural changes.', 'score': 9, 'issue_id': 916, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 2', 'zh': '12Êúà2Êó•'}, 'hash': '0c3ac9e6ce3f4cd2', 'authors': ['Byung-Kwan Lee', 'Ryo Hachiuma', 'Yu-Chiang Frank Wang', 'Yong Man Ro', 'Yueh-Hua Wu'], 'affiliations': ['KAIST', 'NVIDIA'], 'pdf_title_img': 'assets/pdf/title_img/2412.01822.jpg', 'data': {'categories': ['#dataset', '#small_models', '#architecture', '#transfer_learning', '#optimization', '#training', '#benchmark', '#open_source'], 'emoji': 'üî¨', 'ru': {'title': '–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤–æ–µ –æ–±—É—á–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –ø–æ—Å–ª–æ–π–Ω—É—é –≤–µ—Ä–±–∞–ª–∏–∑–∞—Ü–∏—é', 'desc': "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ –Ω–æ–≤–æ–µ —Å–µ–º–µ–π—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º VLsI, –∫–æ—Ç–æ—Ä–æ–µ —Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è –Ω–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –±–µ–∑ —É—â–µ—Ä–±–∞ –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏. VLsI –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å –ø–æ—Å–ª–æ–π–Ω–æ–π –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏, –≤–≤–æ–¥—è –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ '–≤–µ—Ä–±–∞–ª–∏–∑–∞—Ç–æ—Ä—ã', –∫–æ—Ç–æ—Ä—ã–µ –æ—Ç–æ–±—Ä–∞–∂–∞—é—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ—è –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–µ–Ω—å—à–∏–º –º–æ–¥–µ–ª—è–º –≥–∏–±–∫–æ —Å–æ–≥–ª–∞—Å–æ–≤—ã–≤–∞—Ç—å—Å—è —Å –ø—Ä–æ—Ü–µ—Å—Å–∞–º–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –±–æ–ª–µ–µ –∫—Ä—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞–ª–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ VLsI –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å GPT-4V –Ω–∞ –¥–µ—Å—è—Ç–∏ —Å–ª–æ–∂–Ω—ã—Ö –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö —Ç–µ—Å—Ç–∞—Ö –±–µ–∑ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ —É–≤–µ–ª–∏—á–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–∞ –º–æ–¥–µ–ª–∏ –∏–ª–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã."}, 'en': {'title': 'Efficient Vision-Language Models with Layer-wise Distillation', 'desc': "This paper introduces VLsI, a new family of vision-language models designed to enhance efficiency while maintaining accuracy. It employs a layer-wise distillation technique that uses intermediate 'verbalizers' to translate features from each layer into natural language, enabling smaller models to better mimic the reasoning of larger models. This method addresses the common issue of training instability seen in traditional output imitation approaches. The results show significant performance improvements on various benchmarks, demonstrating that smaller models can achieve competitive results without needing to increase their size or alter their architecture."}, 'zh': {'title': 'È´òÊïàËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÁöÑÂàõÊñ∞‰πãË∑Ø', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMÔºâÂÆ∂ÊóèÔºåÁß∞‰∏∫VLsIÔºåÊó®Âú®ÊèêÈ´òÊ®°ÂûãÁöÑÊïàÁéáËÄå‰∏çÁâ∫Áâ≤ÂáÜÁ°ÆÊÄß„ÄÇVLsIÈááÁî®‰∫Ü‰∏ÄÁßçÁã¨ÁâπÁöÑÂ±ÇÁ∫ßËí∏È¶èËøáÁ®ãÔºåÈÄöËøáÂºïÂÖ•‰∏≠Èó¥ÁöÑ‚ÄúËØ≠Ë®ÄÂåñÂô®‚ÄùÔºåÂ∞ÜÊØè‰∏ÄÂ±ÇÁöÑÁâπÂæÅÊò†Â∞ÑÂà∞Ëá™ÁÑ∂ËØ≠Ë®ÄÁ©∫Èó¥Ôºå‰ΩøÂæóËæÉÂ∞èÁöÑVLMËÉΩÂ§üÁÅµÊ¥ªÂú∞‰∏éËæÉÂ§ßVLMÁöÑÊé®ÁêÜËøáÁ®ãÂØπÈΩê„ÄÇËØ•ÊñπÊ≥ïÊúâÊïàÁºìËß£‰∫ÜËæìÂá∫Ê®°‰ªø‰∏≠Â∏∏ËßÅÁöÑËÆ≠ÁªÉ‰∏çÁ®≥ÂÆöÊÄßÔºåÂπ∂ÈÄöËøáÂØπÈΩêÂ∞èÂûãVLMÁöÑÂ±ÇÁ∫ßËøõÂ±ï‰∏éÂ§ßÂûãVLMÁöÑÂ±ÇÁ∫ßËøõÂ±ïÔºåË∂ÖË∂ä‰∫ÜÂÖ∏ÂûãÁöÑÊúÄÁªàÂ±ÇË∞É‰ºò„ÄÇÊàë‰ª¨Âú®ÂçÅ‰∏™ÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑËßÜËßâËØ≠Ë®ÄÂü∫ÂáÜ‰∏äÈ™åËØÅ‰∫ÜVLsIÔºåÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2412.01064', 'title': 'FLOAT: Generative Motion Latent Flow Matching for Audio-driven Talking Portrait', 'url': 'https://huggingface.co/papers/2412.01064', 'abstract': 'With the rapid advancement of diffusion-based generative models, portrait image animation has achieved remarkable results. However, it still faces challenges in temporally consistent video generation and fast sampling due to its iterative sampling nature. This paper presents FLOAT, an audio-driven talking portrait video generation method based on flow matching generative model. We shift the generative modeling from the pixel-based latent space to a learned motion latent space, enabling efficient design of temporally consistent motion. To achieve this, we introduce a transformer-based vector field predictor with a simple yet effective frame-wise conditioning mechanism. Additionally, our method supports speech-driven emotion enhancement, enabling a natural incorporation of expressive motions. Extensive experiments demonstrate that our method outperforms state-of-the-art audio-driven talking portrait methods in terms of visual quality, motion fidelity, and efficiency.', 'score': 8, 'issue_id': 912, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 2', 'zh': '12Êúà2Êó•'}, 'hash': '1978e3ecf42cac0c', 'authors': ['Taekyung Ki', 'Dongchan Min', 'Gyoungsu Chae'], 'affiliations': ['DeepBrain AI Inc.', 'Graduate School of AI, KAIST'], 'pdf_title_img': 'assets/pdf/title_img/2412.01064.jpg', 'data': {'categories': ['#audio', '#diffusion', '#multimodal', '#video', '#architecture'], 'emoji': 'üó£Ô∏è', 'ru': {'title': '–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö –≤–∏–¥–µ–æ —Å –≥–æ–≤–æ—Ä—è—â–∏–º –ø–æ—Ä—Ç—Ä–µ—Ç–æ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞—É–¥–∏–æ –∏ –ø–æ—Ç–æ–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç FLOAT - –º–µ—Ç–æ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ —Å –≥–æ–≤–æ—Ä—è—â–∏–º –ø–æ—Ä—Ç—Ä–µ—Ç–æ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞—É–¥–∏–æ, –∏—Å–ø–æ–ª—å–∑—É—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—É—é –º–æ–¥–µ–ª—å —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è –ø–æ—Ç–æ–∫–æ–≤. –ê–≤—Ç–æ—Ä—ã –ø–µ—Ä–µ–Ω–æ—Å—è—Ç –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑ –ø–∏–∫—Å–µ–ª—å–Ω–æ–≥–æ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –¥–≤–∏–∂–µ–Ω–∏—è, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ —Å–æ–∑–¥–∞–≤–∞—Ç—å —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω—ã–µ –≤–æ –≤—Ä–µ–º–µ–Ω–∏ –¥–≤–∏–∂–µ–Ω–∏—è. –ú–µ—Ç–æ–¥ –≤–∫–ª—é—á–∞–µ—Ç –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–ª—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ —Å –ø–æ–∫–∞–¥—Ä–æ–≤—ã–º –º–µ—Ö–∞–Ω–∏–∑–º–æ–º –æ–±—É—Å–ª–æ–≤–ª–∏–≤–∞–Ω–∏—è. FLOAT —Ç–∞–∫–∂–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —É—Å–∏–ª–µ–Ω–∏–µ —ç–º–æ—Ü–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ—á–∏, –ø–æ–∑–≤–æ–ª—è—è –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ –¥–æ–±–∞–≤–ª—è—Ç—å –≤—ã—Ä–∞–∑–∏—Ç–µ–ª—å–Ω—ã–µ –¥–≤–∏–∂–µ–Ω–∏—è.'}, 'en': {'title': 'FLOAT: Revolutionizing Audio-Driven Talking Portraits with Motion Consistency', 'desc': 'This paper introduces FLOAT, a novel method for generating talking portrait videos driven by audio. It addresses the challenges of creating temporally consistent animations and speeding up the sampling process by utilizing a flow matching generative model. By transitioning from pixel-based latent spaces to learned motion latent spaces, FLOAT enhances the design of motion consistency. The method also incorporates a transformer-based vector field predictor that allows for effective frame-wise conditioning and supports emotion enhancement based on speech, leading to improved visual quality and motion fidelity.'}, 'zh': {'title': 'FLOATÔºöÈ´òÊïàÈü≥È¢ëÈ©±Âä®ÁöÑ‰∫∫ÂÉèËßÜÈ¢ëÁîüÊàê', 'desc': 'Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫FLOATÁöÑÈü≥È¢ëÈ©±Âä®‰∫∫ÂÉèËßÜÈ¢ëÁîüÊàêÊñπÊ≥ïÔºåÂü∫‰∫éÊµÅÂåπÈÖçÁîüÊàêÊ®°Âûã„ÄÇÊàë‰ª¨Â∞ÜÁîüÊàêÂª∫Ê®°‰ªéÂü∫‰∫éÂÉèÁ¥†ÁöÑÊΩúÂú®Á©∫Èó¥ËΩ¨ÁßªÂà∞Â≠¶‰π†ÁöÑËøêÂä®ÊΩúÂú®Á©∫Èó¥Ôºå‰ªéËÄåÂÆûÁé∞Êó∂Èó¥‰∏ÄËá¥ÁöÑËøêÂä®ËÆæËÆ°„ÄÇËØ•ÊñπÊ≥ïÂºïÂÖ•‰∫ÜÂü∫‰∫éÂèòÊç¢Âô®ÁöÑÂêëÈáèÂú∫È¢ÑÊµãÂô®ÔºåÂπ∂ÈááÁî®ÁÆÄÂçïÊúâÊïàÁöÑÈÄêÂ∏ßÊù°‰ª∂Êú∫Âà∂„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂú®ËßÜËßâË¥®Èáè„ÄÅËøêÂä®‰øùÁúüÂ∫¶ÂíåÊïàÁéáÊñπÈù¢‰ºò‰∫éÁé∞ÊúâÁöÑÈü≥È¢ëÈ©±Âä®‰∫∫ÂÉèÁîüÊàêÊñπÊ≥ï„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2411.18933', 'title': 'Efficient Track Anything', 'url': 'https://huggingface.co/papers/2411.18933', 'abstract': 'Segment Anything Model 2 (SAM 2) has emerged as a powerful tool for video object segmentation and tracking anything. Key components of SAM 2 that drive the impressive video object segmentation performance include a large multistage image encoder for frame feature extraction and a memory mechanism that stores memory contexts from past frames to help current frame segmentation. The high computation complexity of multistage image encoder and memory module has limited its applications in real-world tasks, e.g., video object segmentation on mobile devices. To address this limitation, we propose EfficientTAMs, lightweight track anything models that produce high-quality results with low latency and model size. Our idea is based on revisiting the plain, nonhierarchical Vision Transformer (ViT) as an image encoder for video object segmentation, and introducing an efficient memory module, which reduces the complexity for both frame feature extraction and memory computation for current frame segmentation. We take vanilla lightweight ViTs and efficient memory module to build EfficientTAMs, and train the models on SA-1B and SA-V datasets for video object segmentation and track anything tasks. We evaluate on multiple video segmentation benchmarks including semi-supervised VOS and promptable video segmentation, and find that our proposed EfficientTAM with vanilla ViT perform comparably to SAM 2 model (HieraB+SAM 2) with ~2x speedup on A100 and ~2.4x parameter reduction. On segment anything image tasks, our EfficientTAMs also perform favorably over original SAM with ~20x speedup on A100 and ~20x parameter reduction. On mobile devices such as iPhone 15 Pro Max, our EfficientTAMs can run at ~10 FPS for performing video object segmentation with reasonable quality, highlighting the capability of small models for on-device video object segmentation applications.', 'score': 8, 'issue_id': 912, 'pub_date': '2024-11-28', 'pub_date_card': {'ru': '28 –Ω–æ—è–±—Ä—è', 'en': 'November 28', 'zh': '11Êúà28Êó•'}, 'hash': '86ff7f63f69fa104', 'authors': ['Yunyang Xiong', 'Chong Zhou', 'Xiaoyu Xiang', 'Lemeng Wu', 'Chenchen Zhu', 'Zechun Liu', 'Saksham Suri', 'Balakrishnan Varadarajan', 'Ramya Akula', 'Forrest Iandola', 'Raghuraman Krishnamoorthi', 'Bilge Soran', 'Vikas Chandra'], 'affiliations': ['Meta AI', 'Nanyang Technological University'], 'pdf_title_img': 'assets/pdf/title_img/2411.18933.jpg', 'data': {'categories': ['#video', '#small_models', '#benchmark', '#optimization', '#architecture'], 'emoji': 'üé•', 'ru': {'title': '–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –≤–∏–¥–µ–æ –Ω–∞ –º–æ–±–∏–ª—å–Ω—ã—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞—Ö', 'desc': 'EfficientTAMs - —ç—Ç–æ –æ–±–ª–µ–≥—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –∏ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –≤–∏–¥–µ–æ. –û–Ω–∏ –æ—Å–Ω–æ–≤–∞–Ω—ã –Ω–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –ø—Ä–æ—Å—Ç–æ–≥–æ Vision Transformer (ViT) –≤ –∫–∞—á–µ—Å—Ç–≤–µ —ç–Ω–∫–æ–¥–µ—Ä–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –º–æ–¥—É–ª—è –ø–∞–º—è—Ç–∏. EfficientTAMs –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, —Å—Ä–∞–≤–Ω–∏–º—ã–µ —Å SAM 2, –Ω–æ —Ä–∞–±–æ—Ç–∞—é—Ç –≤ 2 —Ä–∞–∑–∞ –±—ã—Å—Ç—Ä–µ–µ –∏ –∏–º–µ—é—Ç –≤ 2,4 —Ä–∞–∑–∞ –º–µ–Ω—å—à–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤. –ù–∞ –º–æ–±–∏–ª—å–Ω—ã—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞—Ö, —Ç–∞–∫–∏—Ö –∫–∞–∫ iPhone 15 Pro Max, EfficientTAMs –º–æ–≥—É—Ç –≤—ã–ø–æ–ª–Ω—è—Ç—å —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—é –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –≤–∏–¥–µ–æ —Å–æ —Å–∫–æ—Ä–æ—Å—Ç—å—é –æ–∫–æ–ª–æ 10 –∫–∞–¥—Ä–æ–≤ –≤ —Å–µ–∫—É–Ω–¥—É.'}, 'en': {'title': 'EfficientTAMs: Lightweight Video Segmentation for Mobile Devices', 'desc': 'The Segment Anything Model 2 (SAM 2) is a sophisticated tool for video object segmentation, but its complexity limits its use on mobile devices. To overcome this, the authors introduce EfficientTAMs, which are lightweight models designed to maintain high-quality segmentation while reducing latency and model size. They utilize a simple Vision Transformer (ViT) for feature extraction and an efficient memory module to streamline the segmentation process. The results show that EfficientTAMs can achieve comparable performance to SAM 2 with significant improvements in speed and resource efficiency, making them suitable for real-time applications on mobile platforms.'}, 'zh': {'title': 'È´òÊïàËßÜÈ¢ëÁâ©‰ΩìÂàÜÂâ≤ÔºåËΩªÈáèÂåñÊ®°ÂûãÊñ∞ÈÄâÊã©', 'desc': 'Segment Anything Model 2ÔºàSAM 2ÔºâÊòØ‰∏ÄÁßçÂº∫Â§ßÁöÑËßÜÈ¢ëÁâ©‰ΩìÂàÜÂâ≤ÂíåË∑üË∏™Â∑•ÂÖ∑„ÄÇ‰∏∫‰∫ÜÊèêÈ´òÊÄßËÉΩÔºåSAM 2‰ΩøÁî®‰∫ÜÂ§öÈò∂ÊÆµÂõæÂÉèÁºñÁ†ÅÂô®ÂíåËÆ∞ÂøÜÊú∫Âà∂Ôºå‰ΩÜÂÖ∂ËÆ°ÁÆóÂ§çÊùÇÊÄßÈôêÂà∂‰∫ÜÂú®ÁßªÂä®ËÆæÂ§á‰∏äÁöÑÂ∫îÁî®„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜÈ´òÊïàÁöÑË∑üË∏™Ê®°ÂûãEfficientTAMsÔºåÂÆÉ‰ΩøÁî®ËΩªÈáèÁ∫ßÁöÑËßÜËßâÂèòÊç¢Âô®ÔºàViTÔºâ‰Ωú‰∏∫ÂõæÂÉèÁºñÁ†ÅÂô®ÔºåÂπ∂ÂºïÂÖ•È´òÊïàÁöÑËÆ∞ÂøÜÊ®°ÂùóÔºå‰ªéËÄåÈôç‰Ωé‰∫ÜËÆ°ÁÆóÂ§çÊùÇÊÄß„ÄÇÊàë‰ª¨ÁöÑEfficientTAMsÂú®Â§ö‰∏™ËßÜÈ¢ëÂàÜÂâ≤Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåËÉΩÂ§üÂú®ÁßªÂä®ËÆæÂ§á‰∏ä‰ª•ÂêàÁêÜÁöÑË¥®ÈáèËøõË°åËßÜÈ¢ëÁâ©‰ΩìÂàÜÂâ≤„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2411.17459', 'title': 'WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent Video Diffusion Model', 'url': 'https://huggingface.co/papers/2411.17459', 'abstract': 'Video Variational Autoencoder (VAE) encodes videos into a low-dimensional latent space, becoming a key component of most Latent Video Diffusion Models (LVDMs) to reduce model training costs. However, as the resolution and duration of generated videos increase, the encoding cost of Video VAEs becomes a limiting bottleneck in training LVDMs. Moreover, the block-wise inference method adopted by most LVDMs can lead to discontinuities of latent space when processing long-duration videos. The key to addressing the computational bottleneck lies in decomposing videos into distinct components and efficiently encoding the critical information. Wavelet transform can decompose videos into multiple frequency-domain components and improve the efficiency significantly, we thus propose Wavelet Flow VAE (WF-VAE), an autoencoder that leverages multi-level wavelet transform to facilitate low-frequency energy flow into latent representation. Furthermore, we introduce a method called Causal Cache, which maintains the integrity of latent space during block-wise inference. Compared to state-of-the-art video VAEs, WF-VAE demonstrates superior performance in both PSNR and LPIPS metrics, achieving 2x higher throughput and 4x lower memory consumption while maintaining competitive reconstruction quality. Our code and models are available at https://github.com/PKU-YuanGroup/WF-VAE.', 'score': 7, 'issue_id': 909, 'pub_date': '2024-11-26', 'pub_date_card': {'ru': '26 –Ω–æ—è–±—Ä—è', 'en': 'November 26', 'zh': '11Êúà26Êó•'}, 'hash': '9b68162718865b81', 'authors': ['Zongjian Li', 'Bin Lin', 'Yang Ye', 'Liuhan Chen', 'Xinhua Cheng', 'Shenghai Yuan', 'Li Yuan'], 'affiliations': ['Peking University', 'Peng Cheng Laboratory', 'Rabbitpre Intelligence'], 'pdf_title_img': 'assets/pdf/title_img/2411.17459.jpg', 'data': {'categories': ['#diffusion', '#data', '#video', '#architecture', '#open_source', '#optimization', '#training'], 'emoji': 'üåä', 'ru': {'title': '–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é –≤–µ–π–≤–ª–µ—Ç–æ–≤', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –≤–∏–¥–µ–æ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Wavelet Flow VAE (WF-VAE). –≠—Ç–æ—Ç –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤–æ–µ –≤–µ–π–≤–ª–µ—Ç-–ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∏–∑–∫–æ—á–∞—Å—Ç–æ—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –≤–∏–¥–µ–æ. WF-VAE —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–∏–¥–µ–æ –≤—ã—Å–æ–∫–æ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –∏ –±–æ–ª—å—à–æ–π –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ –ª–∞—Ç–µ–Ω—Ç–Ω—ã—Ö –≤–∏–¥–µ–æ-–¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö (LVDM). –ú–µ—Ç–æ–¥ —Ç–∞–∫–∂–µ –≤–∫–ª—é—á–∞–µ—Ç —Ç–µ—Ö–Ω–∏–∫—É Causal Cache –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –ø—Ä–∏ –ø–æ–±–ª–æ—á–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–µ.'}, 'en': {'title': 'Efficient Video Encoding with Wavelet Flow VAE', 'desc': 'The paper introduces Wavelet Flow VAE (WF-VAE), a novel approach to encoding videos into a low-dimensional latent space using wavelet transforms. This method addresses the computational bottleneck of traditional Video VAEs, especially when generating high-resolution and long-duration videos. By decomposing videos into frequency-domain components, WF-VAE enhances the efficiency of encoding critical information. Additionally, the proposed Causal Cache method ensures continuity in the latent space during block-wise inference, resulting in improved performance metrics compared to existing video VAEs.'}, 'zh': {'title': 'Â∞èÊ≥¢ÊµÅVAEÔºöÈ´òÊïàËßÜÈ¢ëÁºñÁ†ÅÁöÑÊñ∞ÊñπÊ≥ï', 'desc': 'ËßÜÈ¢ëÂèòÂàÜËá™ÁºñÁ†ÅÂô®ÔºàVAEÔºâÂ∞ÜËßÜÈ¢ëÁºñÁ†Å‰∏∫‰ΩéÁª¥ÊΩúÂú®Á©∫Èó¥ÔºåÊòØÂ§ßÂ§öÊï∞ÊΩúÂú®ËßÜÈ¢ëÊâ©Êï£Ê®°ÂûãÔºàLVDMsÔºâÁöÑÂÖ≥ÈîÆÁªÑÊàêÈÉ®ÂàÜÔºåËÉΩÂ§üÈôç‰ΩéÊ®°ÂûãËÆ≠ÁªÉÊàêÊú¨„ÄÇÁÑ∂ËÄåÔºåÈöèÁùÄÁîüÊàêËßÜÈ¢ëÁöÑÂàÜËæ®ÁéáÂíåÊó∂ÈïøÂ¢ûÂä†ÔºåËßÜÈ¢ëVAEÁöÑÁºñÁ†ÅÊàêÊú¨Êàê‰∏∫ËÆ≠ÁªÉLVDMsÁöÑÁì∂È¢à„ÄÇÊ≠§Â§ñÔºåÂ§ßÂ§öÊï∞LVDMsÈááÁî®ÁöÑÂùóÁä∂Êé®ÁêÜÊñπÊ≥ïÂú®Â§ÑÁêÜÈïøÊó∂ÈïøËßÜÈ¢ëÊó∂ÂèØËÉΩÂØºËá¥ÊΩúÂú®Á©∫Èó¥ÁöÑ‰∏çËøûÁª≠ÊÄß„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥ËÆ°ÁÆóÁì∂È¢àÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜÂ∞èÊ≥¢ÊµÅVAEÔºàWF-VAEÔºâÔºåÈÄöËøáÂ§öÁ∫ßÂ∞èÊ≥¢ÂèòÊç¢ÊúâÊïàÁºñÁ†ÅËßÜÈ¢ëÁöÑÂÖ≥ÈîÆ‰ø°ÊÅØÔºåÂπ∂ÂºïÂÖ•Âõ†ÊûúÁºìÂ≠òÊñπÊ≥ï‰ª•‰øùÊåÅÊΩúÂú®Á©∫Èó¥ÁöÑÂÆåÊï¥ÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2412.01316', 'title': 'Long Video Diffusion Generation with Segmented Cross-Attention and Content-Rich Video Data Curation', 'url': 'https://huggingface.co/papers/2412.01316', 'abstract': 'We introduce Presto, a novel video diffusion model designed to generate 15-second videos with long-range coherence and rich content. Extending video generation methods to maintain scenario diversity over long durations presents significant challenges. To address this, we propose a Segmented Cross-Attention (SCA) strategy, which splits hidden states into segments along the temporal dimension, allowing each segment to cross-attend to a corresponding sub-caption. SCA requires no additional parameters, enabling seamless incorporation into current DiT-based architectures. To facilitate high-quality long video generation, we build the LongTake-HD dataset, consisting of 261k content-rich videos with scenario coherence, annotated with an overall video caption and five progressive sub-captions. Experiments show that our Presto achieves 78.5% on the VBench Semantic Score and 100% on the Dynamic Degree, outperforming existing state-of-the-art video generation methods. This demonstrates that our proposed Presto significantly enhances content richness, maintains long-range coherence, and captures intricate textual details. More details are displayed on our project page: https://presto-video.github.io/.', 'score': 6, 'issue_id': 910, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 2', 'zh': '12Êúà2Êó•'}, 'hash': '5bca597a08ec0a14', 'authors': ['Xin Yan', 'Yuxuan Cai', 'Qiuyue Wang', 'Yuan Zhou', 'Wenhao Huang', 'Huan Yang'], 'affiliations': ['01.AI'], 'pdf_title_img': 'assets/pdf/title_img/2412.01316.jpg', 'data': {'categories': ['#video', '#dataset', '#architecture', '#diffusion', '#long_context'], 'emoji': 'üé¨', 'ru': {'title': 'Presto: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é –ò–ò', 'desc': 'Presto - —ç—Ç–æ –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å –¥–∏—Ñ—Ñ—É–∑–∏–∏ –≤–∏–¥–µ–æ, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ 15-—Å–µ–∫—É–Ω–¥–Ω—ã—Ö –≤–∏–¥–µ–æ—Ä–æ–ª–∏–∫–æ–≤ —Å –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–π —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å—é –∏ –±–æ–≥–∞—Ç—ã–º —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ–º. –ú–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—é Segmented Cross-Attention (SCA), –∫–æ—Ç–æ—Ä–∞—è —Ä–∞–∑–¥–µ–ª—è–µ—Ç —Å–∫—Ä—ã—Ç—ã–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –Ω–∞ —Å–µ–≥–º–µ–Ω—Ç—ã –≤–¥–æ–ª—å –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –∏–∑–º–µ—Ä–µ–Ω–∏—è, –ø–æ–∑–≤–æ–ª—è—è –∫–∞–∂–¥–æ–º—É —Å–µ–≥–º–µ–Ω—Ç—É –ø–µ—Ä–µ–∫—Ä–µ—Å—Ç–Ω–æ –æ–±—Ä–∞—â–∞—Ç—å—Å—è –∫ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–π –ø–æ–¥–ø–∏—Å–∏. –î–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –±—ã–ª —Å–æ–∑–¥–∞–Ω –¥–∞—Ç–∞—Å–µ—Ç LongTake-HD, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π 261 —Ç—ã—Å—è—á—É –≤–∏–¥–µ–æ —Å –±–æ–≥–∞—Ç—ã–º —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ–º –∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å—é —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ Presto –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –ø–æ –ø–æ–∫–∞–∑–∞—Ç–µ–ª—è–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –æ—Ü–µ–Ω–∫–∏ –∏ —Å—Ç–µ–ø–µ–Ω–∏ –¥–∏–Ω–∞–º–∏—á–Ω–æ—Å—Ç–∏.'}, 'en': {'title': 'Presto: Revolutionizing Video Generation with Long-Range Coherence', 'desc': 'Presto is a new video diffusion model that generates 15-second videos while ensuring long-range coherence and rich content. It introduces a Segmented Cross-Attention (SCA) strategy that divides hidden states into segments, allowing each segment to focus on specific sub-captions without needing extra parameters. To support this model, the LongTake-HD dataset was created, containing 261,000 videos with coherent scenarios and detailed captions. Experiments show that Presto outperforms existing methods, achieving high scores in semantic understanding and dynamic content generation.'}, 'zh': {'title': 'PrestoÔºöÁîüÊàêÈïøÊó∂Èó¥‰∏ÄËá¥ÊÄßËßÜÈ¢ëÁöÑÊñ∞ÊñπÊ≥ï', 'desc': 'Êàë‰ª¨‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑËßÜÈ¢ëÊâ©Êï£Ê®°ÂûãPrestoÔºåÊó®Âú®ÁîüÊàêÂÖ∑ÊúâÈïøÊó∂Èó¥‰∏ÄËá¥ÊÄßÂíå‰∏∞ÂØåÂÜÖÂÆπÁöÑ15ÁßíËßÜÈ¢ë„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Âú®ÈïøÊó∂Èó¥ÂÜÖ‰øùÊåÅÂú∫ÊôØÂ§öÊ†∑ÊÄßÁöÑÊåëÊàòÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂàÜÊÆµ‰∫§ÂèâÊ≥®ÊÑèÂäõ(SCA)Á≠ñÁï•ÔºåËØ•Á≠ñÁï•Â∞ÜÈöêËóèÁä∂ÊÄÅÊ≤øÊó∂Èó¥Áª¥Â∫¶ÂàÜÊÆµÔºå‰ΩøÊØè‰∏™ÊÆµËÉΩÂ§ü‰∏éÁõ∏Â∫îÁöÑÂ≠êÊ†áÈ¢òËøõË°å‰∫§ÂèâÂÖ≥Ê≥®„ÄÇSCA‰∏çÈúÄË¶ÅÈ¢ùÂ§ñÁöÑÂèÇÊï∞ÔºåÂèØ‰ª•Êó†ÁºùÈõÜÊàêÂà∞Áé∞ÊúâÁöÑÂü∫‰∫éDiTÁöÑÊû∂ÊûÑ‰∏≠„ÄÇÊàë‰ª¨ÁöÑÂÆûÈ™åË°®ÊòéÔºåPrestoÂú®ËßÜÈ¢ëÁîüÊàêÊñπÈù¢ÊòæËëó‰ºò‰∫éÁé∞ÊúâÁöÑÊúÄÂÖàËøõÊñπÊ≥ïÔºåÊèêÂçá‰∫ÜÂÜÖÂÆπ‰∏∞ÂØåÊÄßÂíåÈïøË∑ùÁ¶ª‰∏ÄËá¥ÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2412.01800', 'title': 'PhysGame: Uncovering Physical Commonsense Violations in Gameplay Videos', 'url': 'https://huggingface.co/papers/2412.01800', 'abstract': 'Recent advancements in video-based large language models (Video LLMs) have witnessed the emergence of diverse capabilities to reason and interpret dynamic visual content. Among them, gameplay videos stand out as a distinctive data source, often containing glitches that defy physics commonsense. This characteristic renders them an effective benchmark for assessing the under-explored capability of physical commonsense understanding in video LLMs. In this paper, we propose PhysGame as a pioneering benchmark to evaluate physical commonsense violations in gameplay videos. PhysGame comprises 880 videos associated with glitches spanning four fundamental domains (i.e., mechanics, kinematics, optics, and material properties) and across 12 distinct physical commonsense. Through extensively evaluating various state-ofthe-art video LLMs, our findings reveal that the performance of current open-source video LLMs significantly lags behind that of proprietary counterparts. To bridge this gap, we curate an instruction tuning dataset PhysInstruct with 140,057 question-answering pairs to facilitate physical commonsense learning. In addition, we also propose a preference optimization dataset PhysDPO with 34,358 training pairs, where the dis-preferred responses are generated conditioned on misleading titles (i.e., meta information hacking), fewer frames (i.e., temporal hacking) and lower spatial resolutions (i.e., spatial hacking). Based on the suite of datasets, we propose PhysVLM as a physical knowledge-enhanced video LLM. Extensive experiments on both physical-oriented benchmark PhysGame and general video understanding benchmarks demonstrate the state-ofthe-art performance of PhysVLM.', 'score': 4, 'issue_id': 917, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 2', 'zh': '12Êúà2Êó•'}, 'hash': '43cc035146493599', 'authors': ['Meng Cao', 'Haoran Tang', 'Haoze Zhao', 'Hangyu Guo', 'Jiaheng Liu', 'Ge Zhang', 'Ruyang Liu', 'Qiang Sun', 'Ian Reid', 'Xiaodan Liang'], 'affiliations': ['Alibaba Group', 'Mohamed bin Zayed University of Artificial Intelligence', 'Peking University', 'Sun Yat-sen University', 'University of Toronto', 'University of Waterloo'], 'pdf_title_img': 'assets/pdf/title_img/2412.01800.jpg', 'data': {'categories': ['#dataset', '#video', '#open_source', '#optimization', '#benchmark', '#reasoning', '#multimodal', '#interpretability'], 'emoji': 'üéÆ', 'ru': {'title': 'PhysVLM: –û–±—É—á–µ–Ω–∏–µ –≤–∏–¥–µ–æ—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Ñ–∏–∑–∏—á–µ—Å–∫–æ–º—É –∑–¥—Ä–∞–≤–æ–º—É —Å–º—ã—Å–ª—É —á–µ—Ä–µ–∑ –∏–≥—Ä–æ–≤—ã–µ –≥–ª–∏—Ç—á–∏', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ PhysGame –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è —Ñ–∏–∑–∏—á–µ—Å–∫–∏—Ö –∑–∞–∫–æ–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç–µ–π –≤ –≤–∏–¥–µ–æ—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (Video LLM) –Ω–∞ –æ—Å–Ω–æ–≤–µ –≥–µ–π–º–ø–ª–µ–π–Ω—ã—Ö –≤–∏–¥–µ–æ —Å –≥–ª–∏—Ç—á–∞–º–∏. –û–Ω–∏ —Å–æ–∑–¥–∞–ª–∏ –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö PhysInstruct –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π —Ñ–∏–∑–∏—á–µ—Å–∫–æ–º—É –∑–¥—Ä–∞–≤–æ–º—É —Å–º—ã—Å–ª—É –∏ –Ω–∞–±–æ—Ä PhysDPO –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π. –ù–∞ –±–∞–∑–µ —ç—Ç–∏—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö –±—ã–ª–∞ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–∞ –º–æ–¥–µ–ª—å PhysVLM, –ø–æ–∫–∞–∑–∞–≤—à–∞—è –ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–∞–∫ –Ω–∞ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–º –±–µ–Ω—á–º–∞—Ä–∫–µ PhysGame, —Ç–∞–∫ –∏ –Ω–∞ –æ–±—â–∏—Ö —Ç–µ—Å—Ç–∞—Ö –ø–æ–Ω–∏–º–∞–Ω–∏—è –≤–∏–¥–µ–æ.'}, 'en': {'title': 'Enhancing Video LLMs with Physical Commonsense Understanding', 'desc': 'This paper introduces PhysGame, a new benchmark designed to evaluate how well video-based large language models (Video LLMs) understand physical commonsense by analyzing glitches in gameplay videos. The benchmark includes 880 videos that highlight violations of physical laws across four domains: mechanics, kinematics, optics, and material properties. To improve the performance of Video LLMs, the authors created two additional datasets: PhysInstruct for instruction tuning and PhysDPO for preference optimization, which help models learn from common misconceptions and misleading information. The proposed PhysVLM model, enhanced with physical knowledge, shows superior performance on both the PhysGame benchmark and general video understanding tasks compared to existing models.'}, 'zh': {'title': 'ÊèêÂçáËßÜÈ¢ëÊ®°ÂûãÁöÑÁâ©ÁêÜÂ∏∏ËØÜÁêÜËß£ËÉΩÂäõ', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÂü∫ÂáÜÊµãËØïPhysGameÔºåÁî®‰∫éËØÑ‰º∞ËßÜÈ¢ëÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂú®Ê∏∏ÊàèËßÜÈ¢ë‰∏≠ÂØπÁâ©ÁêÜÂ∏∏ËØÜÁöÑÁêÜËß£ËÉΩÂäõ„ÄÇÊ∏∏ÊàèËßÜÈ¢ë‰∏≠Â∏∏Â∏∏Âá∫Áé∞ËøùÂèçÁâ©ÁêÜÂ∏∏ËØÜÁöÑÊïÖÈöúÔºåËøô‰ΩøÂæóÂÆÉ‰ª¨Êàê‰∏∫ËØÑ‰º∞Ê®°ÂûãËÉΩÂäõÁöÑÊúâÊïàÊï∞ÊçÆÊ∫ê„ÄÇÊàë‰ª¨ËøòÂàõÂª∫‰∫ÜPhysInstructÂíåPhysDPO‰∏§‰∏™Êï∞ÊçÆÈõÜÔºå‰ª•Â∏ÆÂä©Ê®°ÂûãÂ≠¶‰π†Áâ©ÁêÜÂ∏∏ËØÜÂπ∂‰ºòÂåñÂÖ∂ÂÅèÂ•Ω„ÄÇÈÄöËøáËøô‰∫õÊï∞ÊçÆÈõÜÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜPhysVLMÔºå‰∏Ä‰∏™Â¢ûÂº∫Áâ©ÁêÜÁü•ËØÜÁöÑËßÜÈ¢ëÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºåÂπ∂Âú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Â±ïÁ§∫‰∫ÜÂÖ∂‰ºòË∂äÁöÑÊÄßËÉΩ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2411.19939', 'title': 'VLSBench: Unveiling Visual Leakage in Multimodal Safety', 'url': 'https://huggingface.co/papers/2411.19939', 'abstract': 'Safety concerns of Multimodal large language models (MLLMs) have gradually become an important problem in various applications. Surprisingly, previous works indicate a counter-intuitive phenomenon that using textual unlearning to align MLLMs achieves comparable safety performances with MLLMs trained with image-text pairs. To explain such a counter-intuitive phenomenon, we discover a visual safety information leakage (VSIL) problem in existing multimodal safety benchmarks, i.e., the potentially risky and sensitive content in the image has been revealed in the textual query. In this way, MLLMs can easily refuse these sensitive text-image queries according to textual queries. However, image-text pairs without VSIL are common in real-world scenarios and are overlooked by existing multimodal safety benchmarks. To this end, we construct multimodal visual leakless safety benchmark (VLSBench) preventing visual safety leakage from image to textual query with 2.4k image-text pairs. Experimental results indicate that VLSBench poses a significant challenge to both open-source and close-source MLLMs, including LLaVA, Qwen2-VL, Llama3.2-Vision, and GPT-4o. This study demonstrates that textual alignment is enough for multimodal safety scenarios with VSIL, while multimodal alignment is a more promising solution for multimodal safety scenarios without VSIL. Please see our code and data at: http://hxhcreate.github.io/VLSBench', 'score': 4, 'issue_id': 914, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 –Ω–æ—è–±—Ä—è', 'en': 'November 29', 'zh': '11Êúà29Êó•'}, 'hash': '03d8c636cf8c9486', 'authors': ['Xuhao Hu', 'Dongrui Liu', 'Hao Li', 'Xuanjing Huang', 'Jing Shao'], 'affiliations': ['Beihang University', 'Fudan University', 'Shanghai Artificial Intelligence Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2411.19939.jpg', 'data': {'categories': ['#ethics', '#multimodal', '#benchmark', '#leakage', '#open_source', '#dataset'], 'emoji': 'üîç', 'ru': {'title': '–ù–æ–≤—ã–π –≤–∑–≥–ª—è–¥ –Ω–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –ò–ò-–º–æ–¥–µ–ª–µ–π', 'desc': '–°—Ç–∞—Ç—å—è —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (MLLM). –ê–≤—Ç–æ—Ä—ã –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏ —Ñ–µ–Ω–æ–º–µ–Ω —É—Ç–µ—á–∫–∏ –≤–∏–∑—É–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ (VSIL) –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —Ç–µ—Å—Ç–∞—Ö –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏. –î–ª—è —Ä–µ—à–µ–Ω–∏—è —ç—Ç–æ–π –ø—Ä–æ–±–ª–µ–º—ã –æ–Ω–∏ —Å–æ–∑–¥–∞–ª–∏ –Ω–æ–≤—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö VLSBench, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π 2400 –ø–∞—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ-—Ç–µ–∫—Å—Ç –±–µ–∑ VSIL. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ VLSBench –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—É—é —Å–ª–æ–∂–Ω–æ—Å—Ç—å –¥–ª—è —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö MLLM –∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤ –±–µ–∑ VSIL.'}, 'en': {'title': 'Unveiling Safety in Multimodal Models: The VSIL Challenge', 'desc': 'This paper addresses safety issues in Multimodal Large Language Models (MLLMs) by highlighting a problem called Visual Safety Information Leakage (VSIL). It shows that using textual unlearning can achieve safety performance similar to training with image-text pairs, which is surprising. The authors create a new benchmark, VLSBench, to test MLLMs without the influence of VSIL, using 2.4k image-text pairs. The findings suggest that while textual alignment can ensure safety in scenarios with VSIL, multimodal alignment is necessary for scenarios without it.'}, 'zh': {'title': 'Â§öÊ®°ÊÄÅÂÆâÂÖ®ÊÄßÁöÑÊñ∞ÊåëÊàò‰∏éËß£ÂÜ≥ÊñπÊ°à', 'desc': 'ËøôÁØáËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÂ§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÂú®ÂÆâÂÖ®ÊÄßÊñπÈù¢ÁöÑÊåëÊàò„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÊñáÊú¨ÂéªÂ≠¶‰π†ÂèØ‰ª•‰∏é‰ΩøÁî®ÂõæÂÉè-ÊñáÊú¨ÂØπËÆ≠ÁªÉÁöÑÊ®°ÂûãÂú®ÂÆâÂÖ®ÊÄßË°®Áé∞‰∏äÁõ∏ÂΩì„ÄÇ‰ΩúËÄÖÊåáÂá∫ÔºåÁé∞ÊúâÁöÑÂ§öÊ®°ÊÄÅÂÆâÂÖ®Âü∫ÂáÜÂ≠òÂú®ËßÜËßâÂÆâÂÖ®‰ø°ÊÅØÊ≥ÑÊºèÔºàVSILÔºâÈóÆÈ¢òÔºåËøô‰ΩøÂæóÊ®°ÂûãËÉΩÂ§üËΩªÊòìÊãíÁªùÊïèÊÑüÁöÑÊñáÊú¨-ÂõæÂÉèÊü•ËØ¢„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÁ†îÁ©∂ËÄÖÊûÑÂª∫‰∫Ü‰∏Ä‰∏™Êñ∞ÁöÑÂ§öÊ®°ÊÄÅËßÜËßâÊó†Ê≥ÑÊºèÂÆâÂÖ®Âü∫ÂáÜÔºàVLSBenchÔºâÔºå‰ª•Êõ¥Â•ΩÂú∞ËØÑ‰º∞Ê®°ÂûãÂú®Ê≤°ÊúâVSILÊÉÖÂÜµ‰∏ãÁöÑÂÆâÂÖ®ÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2412.00947', 'title': 'VisOnlyQA: Large Vision Language Models Still Struggle with Visual Perception of Geometric Information', 'url': 'https://huggingface.co/papers/2412.00947', 'abstract': 'Errors in understanding visual information in images (i.e., visual perception errors) remain a major source of mistakes in Large Vision Language Models (LVLMs). While further analysis is essential, there is a deficiency in datasets for evaluating the visual perception of LVLMs. In this work, we introduce VisOnlyQA, a new dataset designed to directly evaluate the visual perception capabilities of LVLMs on questions about geometric and numerical information in scientific figures. Our dataset enables us to analyze the visual perception of LVLMs for fine-grained visual information, independent of other capabilities such as reasoning. The evaluation set of VisOnlyQA includes 1,200 multiple-choice questions in 12 tasks on four categories of figures. We also provide synthetic training data consisting of 70k instances. Our experiments on VisOnlyQA highlight the following findings: (i) 20 LVLMs we evaluate, including GPT-4o and Gemini 1.5 Pro, work poorly on the visual perception tasks in VisOnlyQA, while human performance is nearly perfect. (ii) Fine-tuning on synthetic training data demonstrates the potential for enhancing the visual perception of LVLMs, but observed improvements are limited to certain tasks and specific models. (iii) Stronger language models improve the visual perception of LVLMs. In summary, our experiments suggest that both training data and model architectures should be improved to enhance the visual perception capabilities of LVLMs. The datasets, code, and model responses are provided at https://github.com/psunlpgroup/VisOnlyQA.', 'score': 4, 'issue_id': 911, 'pub_date': '2024-12-01', 'pub_date_card': {'ru': '1 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 1', 'zh': '12Êúà1Êó•'}, 'hash': '7135f8936b0d3ee8', 'authors': ['Ryo Kamoi', 'Yusen Zhang', 'Sarkar Snigdha Sarathi Das', 'Ranran Haoran Zhang', 'Rui Zhang'], 'affiliations': ['Penn State University'], 'pdf_title_img': 'assets/pdf/title_img/2412.00947.jpg', 'data': {'categories': ['#dataset', '#interpretability', '#cv', '#synthetic', '#training'], 'emoji': 'üëÅÔ∏è', 'ru': {'title': 'VisOnlyQA: –Ω–æ–≤—ã–π –ø—É—Ç—å –∫ —É–ª—É—á—à–µ–Ω–∏—é –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è AI', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ –Ω–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç VisOnlyQA –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è –±–æ–ª—å—à–∏—Ö –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LVLM). –î–∞—Ç–∞—Å–µ—Ç —Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è –Ω–∞ –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–æ–π –∏ —á–∏—Å–ª–æ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –Ω–∞—É—á–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö, –ø–æ–∑–≤–æ–ª—è—è –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –∫ —Ç–æ–Ω–∫–æ–º—É –≤–∏–∑—É–∞–ª—å–Ω–æ–º—É –≤–æ—Å–ø—Ä–∏—è—Ç–∏—é. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –¥–∞–∂–µ –ø–µ—Ä–µ–¥–æ–≤—ã–µ LVLM, —Ç–∞–∫–∏–µ –∫–∞–∫ GPT-4o –∏ Gemini 1.5 Pro, –ø–ª–æ—Ö–æ —Å–ø—Ä–∞–≤–ª—è—é—Ç—Å—è —Å –∑–∞–¥–∞—á–∞–º–∏ VisOnlyQA, –≤ —Ç–æ –≤—Ä–µ–º—è –∫–∞–∫ –ª—é–¥–∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –ø–æ—á—Ç–∏ –∏–¥–µ–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å —É–ª—É—á—à–µ–Ω–∏—è –∫–∞–∫ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö, —Ç–∞–∫ –∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä –º–æ–¥–µ–ª–µ–π –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è LVLM.'}, 'en': {'title': 'Enhancing Visual Perception in LVLMs with VisOnlyQA', 'desc': 'This paper addresses the issue of visual perception errors in Large Vision Language Models (LVLMs), which can lead to mistakes when interpreting images. The authors introduce a new dataset called VisOnlyQA, specifically designed to evaluate LVLMs on geometric and numerical questions related to scientific figures. The dataset includes 1,200 multiple-choice questions across various tasks and provides synthetic training data to improve model performance. The findings reveal that while LVLMs struggle with visual perception tasks, fine-tuning with synthetic data can enhance their capabilities, particularly when using stronger language models.'}, 'zh': {'title': 'ÊèêÂçáËßÜËßâÊÑüÁü•ÔºåÂä©ÂäõÂ§ßÂûãËßÜËßâËØ≠Ë®ÄÊ®°Âûã', 'desc': 'Êú¨Á†îÁ©∂‰ªãÁªç‰∫Ü‰∏Ä‰∏™Êñ∞ÁöÑÊï∞ÊçÆÈõÜVisOnlyQAÔºåÊó®Âú®Áõ¥Êé•ËØÑ‰º∞Â§ßÂûãËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàLVLMsÔºâÂú®ÁßëÂ≠¶ÂõæÂΩ¢‰∏≠Âá†‰ΩïÂíåÊï∞ÂÄº‰ø°ÊÅØÈóÆÈ¢ò‰∏äÁöÑËßÜËßâÊÑüÁü•ËÉΩÂäõ„ÄÇËØ•Êï∞ÊçÆÈõÜÂåÖÂê´1200‰∏™Â§öÈ°πÈÄâÊã©È¢òÔºåÊ∂µÁõñÂõõÁ±ªÂõæÂΩ¢ÁöÑ12‰∏™‰ªªÂä°ÔºåÂ∏ÆÂä©ÂàÜÊûêLVLMsÂØπÁªÜÁ≤íÂ∫¶ËßÜËßâ‰ø°ÊÅØÁöÑÁêÜËß£„ÄÇÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåËØÑ‰º∞ÁöÑ20‰∏™LVLMsÂú®ËßÜËßâÊÑüÁü•‰ªªÂä°‰∏äÁöÑË°®Áé∞ËæÉÂ∑ÆÔºåËÄå‰∫∫Á±ªÁöÑË°®Áé∞Âá†‰πéÂÆåÁæé„ÄÇÈÄöËøáÂêàÊàêËÆ≠ÁªÉÊï∞ÊçÆËøõË°åÂæÆË∞ÉÂèØ‰ª•ÊèêÂçáLVLMsÁöÑËßÜËßâÊÑüÁü•ËÉΩÂäõÔºå‰ΩÜÊîπËøõÊïàÊûúÊúâÈôêÔºå‰∏îÊõ¥Âº∫ÁöÑËØ≠Ë®ÄÊ®°ÂûãËÉΩÂ§üÊèêÈ´òËßÜËßâÊÑüÁü•ËÉΩÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2412.00176', 'title': 'Art-Free Generative Models: Art Creation Without Graphic Art Knowledge', 'url': 'https://huggingface.co/papers/2412.00176', 'abstract': 'We explore the question: "How much prior art knowledge is needed to create art?" To investigate this, we propose a text-to-image generation model trained without access to art-related content. We then introduce a simple yet effective method to learn an art adapter using only a few examples of selected artistic styles. Our experiments show that art generated using our method is perceived by users as comparable to art produced by models trained on large, art-rich datasets. Finally, through data attribution techniques, we illustrate how examples from both artistic and non-artistic datasets contributed to the creation of new artistic styles.', 'score': 3, 'issue_id': 921, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 –Ω–æ—è–±—Ä—è', 'en': 'November 29', 'zh': '11Êúà29Êó•'}, 'hash': '41f372dd1c030fbd', 'authors': ['Hui Ren', 'Joanna Materzynska', 'Rohit Gandikota', 'David Bau', 'Antonio Torralba'], 'affiliations': ['MIT', 'Northeastern University', 'ShanghaiTech University'], 'pdf_title_img': 'assets/pdf/title_img/2412.00176.jpg', 'data': {'categories': ['#cv', '#dataset', '#synthetic', '#games', '#multimodal'], 'emoji': 'üé®', 'ru': {'title': '–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç —Ç–≤–æ—Ä–∏—Ç —à–µ–¥–µ–≤—Ä—ã —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º –æ–±—É—á–µ–Ω–∏–µ–º', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –∏–∑—É—á–∞—é—Ç –≤–æ–ø—Ä–æ—Å –æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ–º –æ–±—ä–µ–º–µ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π –æ–± –∏—Å–∫—É—Å—Å—Ç–≤–µ –¥–ª—è –µ–≥–æ —Å–æ–∑–¥–∞–Ω–∏—è. –û–Ω–∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç—É, –æ–±—É—á–µ–Ω–Ω—É—é –±–µ–∑ –¥–æ—Å—Ç—É–ø–∞ –∫ –∫–æ–Ω—Ç–µ–Ω—Ç—É, —Å–≤—è–∑–∞–Ω–Ω–æ–º—É —Å –∏—Å–∫—É—Å—Å—Ç–≤–æ–º. –ó–∞—Ç–µ–º –∞–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç –ø—Ä–æ—Å—Ç–æ–π, –Ω–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è –∞–¥–∞–ø—Ç–µ—Ä–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∏—Å–∫—É—Å—Å—Ç–≤–∞, –∏—Å–ø–æ–ª—å–∑—É—è –ª–∏—à—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö —Ö—É–¥–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Å—Ç–∏–ª–µ–π. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –∏—Å–∫—É—Å—Å—Ç–≤–æ, —Å–æ–∑–¥–∞–Ω–Ω–æ–µ —Å –ø–æ–º–æ—â—å—é —ç—Ç–æ–≥–æ –º–µ—Ç–æ–¥–∞, –≤–æ—Å–ø—Ä–∏–Ω–∏–º–∞–µ—Ç—Å—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º–∏ –Ω–∞ —É—Ä–æ–≤–Ω–µ, —Å—Ä–∞–≤–Ω–∏–º–æ–º —Å –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è–º–∏ –º–æ–¥–µ–ª–µ–π, –æ–±—É—á–µ–Ω–Ω—ã—Ö –Ω–∞ –±–æ–ª—å—à–∏—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö, –±–æ–≥–∞—Ç—ã—Ö –∏—Å–∫—É—Å—Å—Ç–≤–æ–º.'}, 'en': {'title': 'Creating Art with Minimal Prior Knowledge', 'desc': "This paper investigates the necessity of prior art knowledge in generating art through a text-to-image model. The authors propose a novel approach that trains the model without any art-related content, relying instead on a small number of examples from specific artistic styles to create an 'art adapter'. Their experiments reveal that the art produced by this method is perceived similarly to that generated by models trained on extensive art datasets. Additionally, they employ data attribution techniques to demonstrate how both artistic and non-artistic examples influence the development of new artistic styles."}, 'zh': {'title': 'ÂàõÈÄ†Ëâ∫ÊúØÊó†ÈúÄ‰∏∞ÂØåÁöÑËâ∫ÊúØÁü•ËØÜ', 'desc': 'Êàë‰ª¨Êé¢ËÆ®‰∫ÜÂàõÈÄ†Ëâ∫ÊúØÈúÄË¶ÅÂ§öÂ∞ëÂÖàÂâçÁöÑËâ∫ÊúØÁü•ËØÜ„ÄÇ‰∏∫Ê≠§ÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñáÊú¨Âà∞ÂõæÂÉèÁîüÊàêÊ®°ÂûãÔºåËØ•Ê®°ÂûãÂú®Ê≤°ÊúâËâ∫ÊúØÁõ∏ÂÖ≥ÂÜÖÂÆπÁöÑÊÉÖÂÜµ‰∏ãËøõË°åËÆ≠ÁªÉ„ÄÇÊàë‰ª¨ËøòÂºïÂÖ•‰∫Ü‰∏ÄÁßçÁÆÄÂçïÊúâÊïàÁöÑÊñπÊ≥ïÔºå‰ªÖ‰ΩøÁî®Â∞ëÈáèÈÄâÂÆöËâ∫ÊúØÈ£éÊ†ºÁöÑÁ§∫‰æãÊù•Â≠¶‰π†Ëâ∫ÊúØÈÄÇÈÖçÂô®„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºå‰ΩøÁî®Êàë‰ª¨ÁöÑÊñπÊ≥ïÁîüÊàêÁöÑËâ∫ÊúØ‰ΩúÂìÅÂú®Áî®Êà∑Áúº‰∏≠‰∏éÂú®Â§ßÂûãËâ∫ÊúØ‰∏∞ÂØåÊï∞ÊçÆÈõÜ‰∏äËÆ≠ÁªÉÁöÑÊ®°ÂûãÁîüÊàêÁöÑËâ∫ÊúØ‰ΩúÂìÅÁõ∏ÂΩì„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2412.01250', 'title': 'Collaborative Instance Navigation: Leveraging Agent Self-Dialogue to Minimize User Input', 'url': 'https://huggingface.co/papers/2412.01250', 'abstract': 'Existing embodied instance goal navigation tasks, driven by natural language, assume human users to provide complete and nuanced instance descriptions prior to the navigation, which can be impractical in the real world as human instructions might be brief and ambiguous. To bridge this gap, we propose a new task, Collaborative Instance Navigation (CoIN), with dynamic agent-human interaction during navigation to actively resolve uncertainties about the target instance in natural, template-free, open-ended dialogues. To address CoIN, we propose a novel method, Agent-user Interaction with UncerTainty Awareness (AIUTA), leveraging the perception capability of Vision Language Models (VLMs) and the capability of Large Language Models (LLMs). First, upon object detection, a Self-Questioner model initiates a self-dialogue to obtain a complete and accurate observation description, while a novel uncertainty estimation technique mitigates inaccurate VLM perception. Then, an Interaction Trigger module determines whether to ask a question to the user, continue or halt navigation, minimizing user input. For evaluation, we introduce CoIN-Bench, a benchmark supporting both real and simulated humans. AIUTA achieves competitive performance in instance navigation against state-of-the-art methods, demonstrating great flexibility in handling user inputs.', 'score': 3, 'issue_id': 915, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 2', 'zh': '12Êúà2Êó•'}, 'hash': '8b768de35e4c5114', 'authors': ['Francesco Taioli', 'Edoardo Zorzi', 'Gianni Franchi', 'Alberto Castellini', 'Alessandro Farinelli', 'Marco Cristani', 'Yiming Wang'], 'affiliations': ['Fondazione Bruno Kessler', 'Polytechnic of Turin', 'U2IS, ENSTA Paris, Institut Polytechnique de Paris', 'University of Verona'], 'pdf_title_img': 'assets/pdf/title_img/2412.01250.jpg', 'data': {'categories': ['#benchmark', '#interpretability', '#multimodal', '#reasoning', '#agents'], 'emoji': 'üó∫Ô∏è', 'ru': {'title': '–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–∞—è –Ω–∞–≤–∏–≥–∞—Ü–∏—è —Ä–æ–±–æ—Ç–∞ —Å —É—Ç–æ—á–Ω–µ–Ω–∏–µ–º —Ü–µ–ª–∏ —É —á–µ–ª–æ–≤–µ–∫–∞', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –∑–∞–¥–∞—á—É –Ω–∞–≤–∏–≥–∞—Ü–∏–∏ –ø–æ –∏–Ω—Å—Ç–∞–Ω—Å–∞–º –æ–±—ä–µ–∫—Ç–æ–≤ - Collaborative Instance Navigation (CoIN), –≥–¥–µ –∞–≥–µ–Ω—Ç –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤—É–µ—Ç —Å —á–µ–ª–æ–≤–µ–∫–æ–º –≤–æ –≤—Ä–µ–º—è –Ω–∞–≤–∏–≥–∞—Ü–∏–∏ –¥–ª—è —É—Ç–æ—á–Ω–µ–Ω–∏—è —Ü–µ–ª–µ–≤–æ–≥–æ –æ–±—ä–µ–∫—Ç–∞. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ AIUTA, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π Vision Language Models –∏ Large Language Models –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤–∏–∑—É–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–æ–ø—Ä–æ—Å–æ–≤. AIUTA –≤–∫–ª—é—á–∞–µ—Ç –º–æ–¥—É–ª–∏ Self-Questioner –¥–ª—è —Å–∞–º–æ–∞–Ω–∞–ª–∏–∑–∞ –Ω–∞–±–ª—é–¥–µ–Ω–∏–π –∏ Interaction Trigger –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –∑–∞–¥–∞—Ç—å –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ –±–µ–Ω—á–º–∞—Ä–∫ CoIN-Bench –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø–æ–¥–æ–±–Ω—ã—Ö —Å–∏—Å—Ç–µ–º.'}, 'en': {'title': 'Navigating Together: Enhancing Instance Navigation with Human-AI Collaboration', 'desc': 'This paper introduces a new task called Collaborative Instance Navigation (CoIN), which allows agents to interact with humans during navigation to clarify ambiguous instructions. The proposed method, Agent-user Interaction with UncerTainty Awareness (AIUTA), uses Vision Language Models (VLMs) and Large Language Models (LLMs) to enhance the navigation process. It includes a Self-Questioner model that helps the agent gather detailed information about the target instance and an Interaction Trigger module that decides when to engage the user for input. The authors also present CoIN-Bench, a benchmark for evaluating the performance of navigation systems in both real and simulated environments, showing that AIUTA performs well compared to existing methods.'}, 'zh': {'title': 'Âçè‰ΩúÂÆû‰æãÂØºËà™ÔºöËÆ©Êú∫Âô®Êõ¥ÊáÇ‰∫∫Á±ªÊåá‰ª§', 'desc': 'Áé∞ÊúâÁöÑÂÆû‰æãÁõÆÊ†áÂØºËà™‰ªªÂä°ÈÄöÂ∏∏ÈúÄË¶ÅÁî®Êà∑Êèê‰æõËØ¶ÁªÜÁöÑÊèèËø∞Ôºå‰ΩÜÂú®Áé∞ÂÆû‰∏≠ÔºåËøôÁßçË¶ÅÊ±ÇÂæÄÂæÄ‰∏çÂàáÂÆûÈôÖ„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑ‰ªªÂä°ÔºåÁß∞‰∏∫Âçè‰ΩúÂÆû‰æãÂØºËà™ÔºàCoINÔºâÔºåÈÄöËøáÂä®ÊÄÅÁöÑ‰ª£ÁêÜ-Áî®Êà∑‰∫íÂä®Êù•Ëß£ÂÜ≥ÂØºËà™‰∏≠ÁöÑ‰∏çÁ°ÆÂÆöÊÄß„ÄÇÊàë‰ª¨ÁöÑÊñπÊ≥ïÔºå‰ª£ÁêÜ-Áî®Êà∑‰∫íÂä®‰∏é‰∏çÁ°ÆÂÆöÊÄßÊÑèËØÜÔºàAIUTAÔºâÔºåÁªìÂêà‰∫ÜËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÂíåÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑËÉΩÂäõÔºåËÉΩÂ§üÂú®ÂØºËà™ËøáÁ®ã‰∏≠‰∏ªÂä®‰∏éÁî®Êà∑ÂØπËØù„ÄÇÈÄöËøáÂºïÂÖ•CoIN-BenchÂü∫ÂáÜÔºåÊàë‰ª¨ÁöÑAIUTAÊñπÊ≥ïÂú®ÂÆû‰æãÂØºËà™‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÂ±ïÁ§∫‰∫ÜÂ§ÑÁêÜÁî®Êà∑ËæìÂÖ•ÁöÑÁÅµÊ¥ªÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2411.19799', 'title': 'INCLUDE: Evaluating Multilingual Language Understanding with Regional Knowledge', 'url': 'https://huggingface.co/papers/2411.19799', 'abstract': 'The performance differential of large language models (LLM) between languages hinders their effective deployment in many regions, inhibiting the potential economic and societal value of generative AI tools in many communities. However, the development of functional LLMs in many languages (\\ie, multilingual LLMs) is bottlenecked by the lack of high-quality evaluation resources in languages other than English. Moreover, current practices in multilingual benchmark construction often translate English resources, ignoring the regional and cultural knowledge of the environments in which multilingual systems would be used. In this work, we construct an evaluation suite of 197,243 QA pairs from local exam sources to measure the capabilities of multilingual LLMs in a variety of regional contexts. Our novel resource, INCLUDE, is a comprehensive knowledge- and reasoning-centric benchmark across 44 written languages that evaluates multilingual LLMs for performance in the actual language environments where they would be deployed.', 'score': 3, 'issue_id': 914, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 –Ω–æ—è–±—Ä—è', 'en': 'November 29', 'zh': '11Êúà29Êó•'}, 'hash': '7b0cbdd28adecf2c', 'authors': ['Angelika Romanou', 'Negar Foroutan', 'Anna Sotnikova', 'Zeming Chen', 'Sree Harsha Nelaturu', 'Shivalika Singh', 'Rishabh Maheshwary', 'Micol Altomare', 'Mohamed A. Haggag', 'Snegha A', 'Alfonso Amayuelas', 'Azril Hafizi Amirudin', 'Viraat Aryabumi', 'Danylo Boiko', 'Michael Chang', 'Jenny Chim', 'Gal Cohen', 'Aditya Kumar Dalmia', 'Abraham Diress', 'Sharad Duwal', 'Daniil Dzenhaliou', 'Daniel Fernando Erazo Florez', 'Fabian Farestam', 'Joseph Marvin Imperial', 'Shayekh Bin Islam', 'Perttu Isotalo', 'Maral Jabbarishiviari', 'B√∂rje F. Karlsson', 'Eldar Khalilov', 'Christopher Klamm', 'Fajri Koto', 'Dominik Krzemi≈Ñski', 'Gabriel Adriano de Melo', 'Syrielle Montariol', 'Yiyang Nan', 'Joel Niklaus', 'Jekaterina Novikova', 'Johan Samir Obando Ceron', 'Debjit Paul', 'Esther Ploeger', 'Jebish Purbey', 'Swati Rajwal', 'Selvan Sunitha Ravi', 'Sara Rydell', 'Roshan Santhosh', 'Drishti Sharma', 'Marjana Prifti Skenduli', 'Arshia Soltani Moakhar', 'Bardia Soltani Moakhar', 'Ran Tamir', 'Ayush Kumar Tarun', 'Azmine Toushik Wasi', 'Thenuka Ovin Weerasinghe', 'Serhan Yilmaz', 'Mike Zhang', 'Imanol Schlag', 'Marzieh Fadaee', 'Sara Hooker', 'Antoine Bosselut'], 'affiliations': ['Cohere For AI', 'Cohere For AI Community', 'EPFL', 'ETH Zurich', 'Swiss AI Initiative'], 'pdf_title_img': 'assets/pdf/title_img/2411.19799.jpg', 'data': {'categories': ['#multilingual', '#low_resource', '#benchmark', '#reasoning'], 'emoji': 'üåç', 'ru': {'title': '–ì–ª–æ–±–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö –ò–ò-–º–æ–¥–µ–ª–µ–π –≤ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞—Ö', 'desc': '–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –ø—Ä–æ–±–ª–µ–º–µ –Ω–µ—Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —è–∑—ã–∫–æ–≤. –ê–≤—Ç–æ—Ä—ã —Å–æ–∑–¥–∞–ª–∏ –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö INCLUDE –∏–∑ 197,243 –ø–∞—Ä –≤–æ–ø—Ä–æ—Å–æ–≤ –∏ –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ 44 —è–∑—ã–∫–∞—Ö –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö LLM. –≠—Ç–æ—Ç —Ä–µ—Å—É—Ä—Å –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ –º–µ—Å—Ç–Ω—ã—Ö —ç–∫–∑–∞–º–µ–Ω–∞—Ü–∏–æ–Ω–Ω—ã—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–∞—Ö –∏ –æ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ä–µ–≥–∏–æ–Ω–∞–ª—å–Ω—ã–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç—ã. INCLUDE –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ—Ü–µ–Ω–∏–≤–∞—Ç—å –∑–Ω–∞–Ω–∏—è –∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö LLM –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö —Å—Ä–µ–¥–∞—Ö –∏—Ö –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è.'}, 'en': {'title': 'Bridging Language Gaps with INCLUDE: A Multilingual Benchmark for LLMs', 'desc': 'This paper addresses the performance gap of large language models (LLMs) across different languages, which limits their use in various regions. It highlights the challenge of developing multilingual LLMs due to the scarcity of high-quality evaluation resources in non-English languages. The authors propose a new evaluation suite called INCLUDE, consisting of 197,243 question-and-answer pairs sourced from local exams. This benchmark is designed to assess the capabilities of multilingual LLMs in real-world contexts, taking into account regional and cultural knowledge.'}, 'zh': {'title': 'ÊèêÂçáÂ§öËØ≠Ë®ÄÊ®°ÂûãÁöÑÂÆûÈôÖÂ∫îÁî®ËÉΩÂäõ', 'desc': 'ËøôÁØáËÆ∫ÊñáËÆ®ËÆ∫‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÂú®‰∏çÂêåËØ≠Ë®Ä‰πãÈó¥ÁöÑÊÄßËÉΩÂ∑ÆÂºÇÔºåËøôÂΩ±Âìç‰∫ÜÂÆÉ‰ª¨Âú®ËÆ∏Â§öÂú∞Âå∫ÁöÑÊúâÊïàÂ∫îÁî®„ÄÇ‰∏∫‰∫ÜÂÖãÊúçÂ§öËØ≠Ë®ÄLLMÂºÄÂèë‰∏≠ÁöÑÁì∂È¢àÔºå‰ΩúËÄÖÊûÑÂª∫‰∫Ü‰∏Ä‰∏™ÂåÖÂê´197,243‰∏™ÈóÆÁ≠îÂØπÁöÑËØÑ‰º∞Â•ó‰ª∂ÔºåÊù•Ê∫ê‰∫éÂΩìÂú∞ËÄÉËØïÊùêÊñô„ÄÇËøô‰∏™Êñ∞ËµÑÊ∫êINCLUDEÊòØ‰∏Ä‰∏™ÂÖ®Èù¢ÁöÑÁü•ËØÜÂíåÊé®ÁêÜ‰∏≠ÂøÉÂü∫ÂáÜÔºåÊ∂µÁõñ44Áßç‰π¶Èù¢ËØ≠Ë®ÄÔºåÊó®Âú®ËØÑ‰º∞Â§öËØ≠Ë®ÄLLMÂú®ÂÆûÈôÖËØ≠Ë®ÄÁéØÂ¢É‰∏≠ÁöÑË°®Áé∞„ÄÇÈÄöËøáËøôÁßçÊñπÂºèÔºåÁ†îÁ©∂Â∏åÊúõÊèêÂçáÁîüÊàêÊÄß‰∫∫Â∑•Êô∫ËÉΩÂ∑•ÂÖ∑Âú®‰∏çÂêåÁ§æÂå∫ÁöÑÁªèÊµéÂíåÁ§æ‰ºö‰ª∑ÂÄº„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2412.01821', 'title': 'World-consistent Video Diffusion with Explicit 3D Modeling', 'url': 'https://huggingface.co/papers/2412.01821', 'abstract': 'Recent advancements in diffusion models have set new benchmarks in image and video generation, enabling realistic visual synthesis across single- and multi-frame contexts. However, these models still struggle with efficiently and explicitly generating 3D-consistent content. To address this, we propose World-consistent Video Diffusion (WVD), a novel framework that incorporates explicit 3D supervision using XYZ images, which encode global 3D coordinates for each image pixel. More specifically, we train a diffusion transformer to learn the joint distribution of RGB and XYZ frames. This approach supports multi-task adaptability via a flexible inpainting strategy. For example, WVD can estimate XYZ frames from ground-truth RGB or generate novel RGB frames using XYZ projections along a specified camera trajectory. In doing so, WVD unifies tasks like single-image-to-3D generation, multi-view stereo, and camera-controlled video generation. Our approach demonstrates competitive performance across multiple benchmarks, providing a scalable solution for 3D-consistent video and image generation with a single pretrained model.', 'score': 2, 'issue_id': 922, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 2', 'zh': '12Êúà2Êó•'}, 'hash': '843891601e85de06', 'authors': ['Qihang Zhang', 'Shuangfei Zhai', 'Miguel Angel Bautista', 'Kevin Miao', 'Alexander Toshev', 'Joshua Susskind', 'Jiatao Gu'], 'affiliations': ['Apple', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2412.01821.jpg', 'data': {'categories': ['#3d', '#benchmark', '#diffusion', '#video'], 'emoji': 'üé•', 'ru': {'title': '3D-—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å World-consistent Video Diffusion (WVD) –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ 3D-—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞. WVD –∏—Å–ø–æ–ª—å–∑—É–µ—Ç XYZ-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è —è–≤–Ω–æ–≥–æ 3D-–∫–æ–Ω—Ç—Ä–æ–ª—è –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –¥–∏—Ñ—Ñ—É–∑–∏–∏. –ú–æ–¥–µ–ª—å –æ–±—É—á–∞–µ—Ç—Å—è —Å–æ–≤–º–µ—Å—Ç–Ω–æ–º—É —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—é RGB –∏ XYZ –∫–∞–¥—Ä–æ–≤, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Ä–µ—à–∞—Ç—å —Ä–∞–∑–ª–∏—á–Ω—ã–µ –∑–∞–¥–∞—á–∏, –≤–∫–ª—é—á–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—é 3D –∏–∑ –æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ —Å–æ–∑–¥–∞–Ω–∏–µ –≤–∏–¥–µ–æ —Å –∫–æ–Ω—Ç—Ä–æ–ª–µ–º –∫–∞–º–µ—Ä—ã. WVD –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ—Å–ø–æ—Å–æ–±–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —ç—Ç–∞–ª–æ–Ω–Ω—ã—Ö —Ç–µ—Å—Ç–∞—Ö.'}, 'en': {'title': 'Unifying 3D Consistency in Video Generation with WVD', 'desc': 'This paper introduces World-consistent Video Diffusion (WVD), a new framework that enhances video and image generation by incorporating 3D supervision. WVD uses XYZ images to provide global 3D coordinates for each pixel, allowing for more accurate and consistent 3D content generation. The framework employs a diffusion transformer to learn the relationship between RGB and XYZ frames, enabling tasks like generating new RGB frames from 3D data. Overall, WVD offers a versatile solution for various tasks, achieving strong performance across benchmarks with a single pretrained model.'}, 'zh': {'title': 'Áªü‰∏Ä3D‰∏ÄËá¥ÊÄßÁöÑËßÜÈ¢ëÁîüÊàêÊñ∞Ê°ÜÊû∂', 'desc': 'ÊúÄËøëÔºåÊâ©Êï£Ê®°ÂûãÂú®ÂõæÂÉèÂíåËßÜÈ¢ëÁîüÊàêÊñπÈù¢ÂèñÂæó‰∫ÜÊòæËëóËøõÂ±ïÔºåËÉΩÂ§üÂú®ÂçïÂ∏ßÂíåÂ§öÂ∏ß‰∏ä‰∏ãÊñá‰∏≠ÂÆûÁé∞ÈÄºÁúüÁöÑËßÜËßâÂêàÊàê„ÄÇÁÑ∂ËÄåÔºåËøô‰∫õÊ®°ÂûãÂú®È´òÊïà‰∏îÊòéÁ°ÆÂú∞ÁîüÊàê3D‰∏ÄËá¥ÂÜÖÂÆπÊñπÈù¢‰ªçÁÑ∂Â≠òÂú®ÊåëÊàò„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ñÁïå‰∏ÄËá¥ËßÜÈ¢ëÊâ©Êï£ÔºàWVDÔºâÔºåËøôÊòØ‰∏Ä‰∏™Êñ∞È¢ñÁöÑÊ°ÜÊû∂ÔºåÂà©Áî®XYZÂõæÂÉèËøõË°åÊòéÁ°ÆÁöÑ3DÁõëÁù£ÔºåÁºñÁ†ÅÊØè‰∏™ÂõæÂÉèÂÉèÁ¥†ÁöÑÂÖ®Â±Ä3DÂùêÊ†á„ÄÇWVDÈÄöËøáÁÅµÊ¥ªÁöÑ‰øÆË°•Á≠ñÁï•ÊîØÊåÅÂ§ö‰ªªÂä°ÈÄÇÂ∫îÊÄßÔºåËÉΩÂ§ü‰ªéÁúüÂÆûÁöÑRGB‰º∞ËÆ°XYZÂ∏ßÔºåÊàñÊ≤øÊåáÂÆöÁöÑÁõ∏Êú∫ËΩ®ËøπÁîüÊàêÊñ∞ÁöÑRGBÂ∏ß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2412.00319', 'title': 'Improving speaker verification robustness with synthetic emotional utterances', 'url': 'https://huggingface.co/papers/2412.00319', 'abstract': 'A speaker verification (SV) system offers an authentication service designed to confirm whether a given speech sample originates from a specific speaker. This technology has paved the way for various personalized applications that cater to individual preferences. A noteworthy challenge faced by SV systems is their ability to perform consistently across a range of emotional spectra. Most existing models exhibit high error rates when dealing with emotional utterances compared to neutral ones. Consequently, this phenomenon often leads to missing out on speech of interest. This issue primarily stems from the limited availability of labeled emotional speech data, impeding the development of robust speaker representations that encompass diverse emotional states.   To address this concern, we propose a novel approach employing the CycleGAN framework to serve as a data augmentation method. This technique synthesizes emotional speech segments for each specific speaker while preserving the unique vocal identity. Our experimental findings underscore the effectiveness of incorporating synthetic emotional data into the training process. The models trained using this augmented dataset consistently outperform the baseline models on the task of verifying speakers in emotional speech scenarios, reducing equal error rate by as much as 3.64% relative.', 'score': 1, 'issue_id': 926, 'pub_date': '2024-11-30', 'pub_date_card': {'ru': '30 –Ω–æ—è–±—Ä—è', 'en': 'November 30', 'zh': '11Êúà30Êó•'}, 'hash': 'e9e71338ed876bc0', 'authors': ['Nikhil Kumar Koditala', 'Chelsea Jui-Ting Ju', 'Ruirui Li', 'Minho Jin', 'Aman Chadha', 'Andreas Stolcke'], 'affiliations': ['Amazon Alexa AI, USA', 'Amazon Business, USA', 'Amazon GenAI, USA', 'Amazon Search Experience Science, USA', 'Amazon Web Services, USA'], 'pdf_title_img': 'assets/pdf/title_img/2412.00319.jpg', 'data': {'categories': ['#training', '#dataset', '#synthetic', '#audio'], 'emoji': 'üé≠', 'ru': {'title': 'CycleGAN –¥–ª—è –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–π —Ä–µ—á–∏ —É–ª—É—á—à–∞–µ—Ç –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—é –≥–æ–≤–æ—Ä—è—â–µ–≥–æ', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏ –≥–æ–≤–æ—Ä—è—â–µ–≥–æ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º CycleGAN –¥–ª—è –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–π —Ä–µ—á–∏. –ê–≤—Ç–æ—Ä—ã —Å–∏–Ω—Ç–µ–∑–∏—Ä—É—é—Ç —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ —Ä–µ—á–µ–≤—ã–µ —Å–µ–≥–º–µ–Ω—Ç—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –¥–∏–∫—Ç–æ—Ä–∞, —Å–æ—Ö—Ä–∞–Ω—è—è –ø—Ä–∏ —ç—Ç–æ–º —É–Ω–∏–∫–∞–ª—å–Ω—É—é –≥–æ–ª–æ—Å–æ–≤—É—é –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç—å. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –º–æ–¥–µ–ª–∏, –æ–±—É—á–µ–Ω–Ω—ã–µ –Ω–∞ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç –±–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –≤ –∑–∞–¥–∞—á–µ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏ –≥–æ–≤–æ—Ä—è—â–∏—Ö –≤ —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö —Å —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–π —Ä–µ—á—å—é. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ —Å–Ω–∏–∂–∞–µ—Ç —Ä–∞–≤–Ω—É—é –æ—à–∏–±–∫—É (EER) –¥–æ 3.64% –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –±–∞–∑–æ–≤–æ–π –ª–∏–Ω–∏–∏.'}, 'en': {'title': 'Enhancing Speaker Verification with Synthetic Emotional Data', 'desc': "This paper discusses a speaker verification (SV) system that authenticates speakers based on their voice. A major challenge for these systems is accurately verifying speakers when they express different emotions, as most existing models struggle with emotional speech. The authors propose using a CycleGAN framework to create synthetic emotional speech data, which helps improve the training of SV models. Their results show that incorporating this augmented data significantly enhances the models' performance, reducing error rates in emotional speech verification."}, 'zh': {'title': 'ÊÉÖÊÑüËØ≠Èü≥È™åËØÅÁöÑÊñ∞Á™ÅÁ†¥', 'desc': 'Êú¨Á†îÁ©∂ÊèêÂá∫‰∫Ü‰∏ÄÁßçËØ¥ËØù‰∫∫È™åËØÅÁ≥ªÁªüÔºåÊó®Âú®Á°ÆËÆ§ÁâπÂÆöËØ≠Èü≥Ê†∑Êú¨ÊòØÂê¶Êù•Ëá™ÁâπÂÆöËØ¥ËØùËÄÖ„ÄÇËØ•Á≥ªÁªüÈù¢‰∏¥ÁöÑ‰∏ªË¶ÅÊåëÊàòÊòØÂ¶Ç‰ΩïÂú®‰∏çÂêåÊÉÖÊÑüÁä∂ÊÄÅ‰∏ã‰øùÊåÅ‰∏ÄËá¥ÁöÑÊÄßËÉΩ„ÄÇ‰∏∫‰∫ÜÂÖãÊúçËøô‰∏ÄÈóÆÈ¢òÔºåÊàë‰ª¨ÈááÁî®CycleGANÊ°ÜÊû∂ËøõË°åÊï∞ÊçÆÂ¢ûÂº∫ÔºåÂêàÊàêÊØè‰∏™ËØ¥ËØùËÄÖÁöÑÊÉÖÊÑüËØ≠Èü≥ÁâáÊÆµÔºåÂêåÊó∂‰øùÁïôÂÖ∂Áã¨ÁâπÁöÑÂ£∞Èü≥ÁâπÂæÅ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºå‰ΩøÁî®ÂêàÊàêÊÉÖÊÑüÊï∞ÊçÆËÆ≠ÁªÉÁöÑÊ®°ÂûãÂú®ÊÉÖÊÑüËØ≠Èü≥È™åËØÅ‰ªªÂä°‰∏≠Ë°®Áé∞‰ºò‰∫éÂü∫Á∫øÊ®°ÂûãÔºåÈîôËØØÁéáÈôç‰Ωé‰∫Ü3.64%„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2412.00869', 'title': 'Exploring the Abilities of Large Language Models to Solve Proportional Analogies via Knowledge-Enhanced Prompting', 'url': 'https://huggingface.co/papers/2412.00869', 'abstract': 'Making analogies is fundamental to cognition. Proportional analogies, which consist of four terms, are often used to assess linguistic and cognitive abilities. For instance, completing analogies like "Oxygen is to Gas as <blank> is to <blank>" requires identifying the semantic relationship (e.g., "type of") between the first pair of terms ("Oxygen" and "Gas") and finding a second pair that shares the same relationship (e.g., "Aluminum" and "Metal"). In this work, we introduce a 15K Multiple-Choice Question Answering (MCQA) dataset for proportional analogy completion and evaluate the performance of contemporary Large Language Models (LLMs) in various knowledge-enhanced prompt settings. Specifically, we augment prompts with three types of knowledge: exemplar, structured, and targeted. Our results show that despite extensive training data, solving proportional analogies remains challenging for current LLMs, with the best model achieving an accuracy of 55%. Notably, we find that providing targeted knowledge can better assist models in completing proportional analogies compared to providing exemplars or collections of structured knowledge.', 'score': 1, 'issue_id': 926, 'pub_date': '2024-12-01', 'pub_date_card': {'ru': '1 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 1', 'zh': '12Êúà1Êó•'}, 'hash': '6816796631155a8c', 'authors': ['Thilini Wijesiriwardene', 'Ruwan Wickramarachchi', 'Sreeram Vennam', 'Vinija Jain', 'Aman Chadha', 'Amitava Das', 'Ponnurangam Kumaraguru', 'Amit Sheth'], 'affiliations': ['AI Institute, University of South Carolina, USA', 'Amazon GenAI, USA', 'IIIT Hyderabad, India', 'Meta, USA', 'Stanford University, USA'], 'pdf_title_img': 'assets/pdf/title_img/2412.00869.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#reasoning', '#data'], 'emoji': 'üß†', 'ru': {'title': '–ë–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –≤—Å–µ –µ—â–µ –Ω–µ –º–∞—Å—Ç–µ—Ä–∞ –∞–Ω–∞–ª–æ–≥–∏–π', 'desc': '–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) —Ä–µ—à–∞—Ç—å –ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –∞–Ω–∞–ª–æ–≥–∏–∏. –ê–≤—Ç–æ—Ä—ã —Å–æ–∑–¥–∞–ª–∏ –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∏–∑ 15 000 –≤–æ–ø—Ä–æ—Å–æ–≤ —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º –≤—ã–±–æ—Ä–æ–º –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –∞–Ω–∞–ª–æ–≥–∏–π. –û–Ω–∏ –æ—Ü–µ–Ω–∏–ª–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö LLM –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö –ø—Ä–æ–º–ø—Ç–æ–≤ —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –∑–Ω–∞–Ω–∏—è–º–∏. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –¥–∞–∂–µ –ª—É—á—à–∏–µ –º–æ–¥–µ–ª–∏ –¥–æ—Å—Ç–∏–≥–∞—é—Ç —Ç–æ—á–Ω–æ—Å—Ç–∏ –ª–∏—à—å 55%, –∏ —Ü–µ–ª–µ–≤—ã–µ –∑–Ω–∞–Ω–∏—è –ø–æ–º–æ–≥–∞—é—Ç –±–æ–ª—å—à–µ, —á–µ–º –ø—Ä–∏–º–µ—Ä—ã –∏–ª–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è.'}, 'en': {'title': 'Enhancing Analogy Completion in LLMs with Targeted Knowledge', 'desc': 'This paper focuses on the challenge of solving proportional analogies, which are important for assessing cognitive abilities. The authors present a new dataset containing 15,000 multiple-choice questions designed for this purpose. They evaluate how well current Large Language Models (LLMs) perform on these analogies, especially when given different types of knowledge prompts. The findings reveal that while LLMs struggle with these tasks, targeted knowledge prompts significantly improve their performance compared to other types of knowledge.'}, 'zh': {'title': 'ÊèêÂçáÁ±ªÊØîËÉΩÂäõÔºåÁü•ËØÜÊòØÂÖ≥ÈîÆÔºÅ', 'desc': 'Á±ªÊØîÊòØËÆ§Áü•ÁöÑÈáçË¶ÅÈÉ®ÂàÜÔºåÊØî‰æãÁ±ªÊØîÁî±Âõõ‰∏™ÊúØËØ≠ÁªÑÊàêÔºåÂ∏∏Áî®‰∫éËØÑ‰º∞ËØ≠Ë®ÄÂíåËÆ§Áü•ËÉΩÂäõ„ÄÇÊú¨Êñá‰ªãÁªç‰∫Ü‰∏Ä‰∏™ÂåÖÂê´15,000‰∏™Â§öÈ°πÈÄâÊã©È¢òÁöÑÊØî‰æãÁ±ªÊØîÂÆåÊàêÊï∞ÊçÆÈõÜÔºåÂπ∂ËØÑ‰º∞‰∫ÜÂΩìÂâçÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®‰∏çÂêåÁü•ËØÜÂ¢ûÂº∫ÊèêÁ§∫ËÆæÁΩÆ‰∏ãÁöÑË°®Áé∞„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÂ∞ΩÁÆ°Ê®°ÂûãÁªèËøáÂ§ßÈáèËÆ≠ÁªÉÔºåËß£ÂÜ≥ÊØî‰æãÁ±ªÊØî‰ªçÁÑ∂ÂÖ∑ÊúâÊåëÊàòÊÄßÔºåÊúÄ‰Ω≥Ê®°ÂûãÁöÑÂáÜÁ°ÆÁéá‰ªÖ‰∏∫55%„ÄÇÁâπÂà´ÊòØÔºåÊèê‰æõÈíàÂØπÊÄßÁöÑÁü•ËØÜÊØîÊèê‰æõÁ§∫‰æãÊàñÁªìÊûÑÂåñÁü•ËØÜÊõ¥ËÉΩÂ∏ÆÂä©Ê®°ÂûãÂÆåÊàêÊØî‰æãÁ±ªÊØî„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2412.01408', 'title': 'Towards Cross-Lingual Audio Abuse Detection in Low-Resource Settings with Few-Shot Learning', 'url': 'https://huggingface.co/papers/2412.01408', 'abstract': 'Online abusive content detection, particularly in low-resource settings and within the audio modality, remains underexplored. We investigate the potential of pre-trained audio representations for detecting abusive language in low-resource languages, in this case, in Indian languages using Few Shot Learning (FSL). Leveraging powerful representations from models such as Wav2Vec and Whisper, we explore cross-lingual abuse detection using the ADIMA dataset with FSL. Our approach integrates these representations within the Model-Agnostic Meta-Learning (MAML) framework to classify abusive language in 10 languages. We experiment with various shot sizes (50-200) evaluating the impact of limited data on performance. Additionally, a feature visualization study was conducted to better understand model behaviour. This study highlights the generalization ability of pre-trained models in low-resource scenarios and offers valuable insights into detecting abusive language in multilingual contexts.', 'score': 1, 'issue_id': 918, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 2', 'zh': '12Êúà2Êó•'}, 'hash': 'f4c5e5e0485d3969', 'authors': ['Aditya Narayan Sankaran', 'Reza Farahbaksh', 'Noel Crespi'], 'affiliations': ['SAMOVAR, T√©l√©com SudParis Institut Polytechnique de Paris 91120 Palaiseau, France'], 'pdf_title_img': 'assets/pdf/title_img/2412.01408.jpg', 'data': {'categories': ['#low_resource', '#interpretability', '#dataset', '#multilingual', '#audio', '#training'], 'emoji': 'üéôÔ∏è', 'ru': {'title': '–ë–æ—Ä—å–±–∞ —Å –æ—Å–∫–æ—Ä–±–ª–µ–Ω–∏—è–º–∏ –≤ –∞—É–¥–∏–æ: –º–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö, –º–Ω–æ–≥–æ —è–∑—ã–∫–æ–≤', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—é –æ—Å–∫–æ—Ä–±–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –≤ –∞—É–¥–∏–æ –Ω–∞ —è–∑—ã–∫–∞—Ö —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏, –≤ —á–∞—Å—Ç–Ω–æ—Å—Ç–∏ –Ω–∞ –∏–Ω–¥–∏–π—Å–∫–∏—Ö —è–∑—ã–∫–∞—Ö, —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Few Shot Learning (FSL). –ê–≤—Ç–æ—Ä—ã –ø—Ä–∏–º–µ–Ω—è—é—Ç –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –∞—É–¥–∏–æ-–ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –∏–∑ –º–æ–¥–µ–ª–µ–π Wav2Vec –∏ Whisper –≤ —Ä–∞–º–∫–∞—Ö Model-Agnostic Meta-Learning (MAML) –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –æ—Å–∫–æ—Ä–±–∏—Ç–µ–ª—å–Ω–æ–≥–æ —è–∑—ã–∫–∞ –≤ 10 —è–∑—ã–∫–∞—Ö. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø—Ä–æ–≤–æ–¥–∏–ª–∏—Å—å —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ —Ä–∞–∑–º–µ—Ä–∞–º–∏ –≤—ã–±–æ—Ä–æ–∫ (50-200) –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤–ª–∏—è–Ω–∏—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∫ –æ–±–æ–±—â–µ–Ω–∏—é –≤ —É—Å–ª–æ–≤–∏—è—Ö –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤ –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç —Ü–µ–Ω–Ω—ã–µ insights –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –æ—Å–∫–æ—Ä–±–∏—Ç–µ–ª—å–Ω–æ–≥–æ —è–∑—ã–∫–∞ –≤ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ.'}, 'en': {'title': 'Empowering Low-Resource Languages: Detecting Abuse in Audio with Few Shot Learning', 'desc': 'This paper focuses on detecting abusive language in audio, especially in languages with limited resources, like many Indian languages. It uses Few Shot Learning (FSL) techniques with pre-trained audio models such as Wav2Vec and Whisper to improve detection accuracy. The authors apply the Model-Agnostic Meta-Learning (MAML) framework to classify abusive content across 10 different languages, even with minimal training data. Their findings demonstrate how well pre-trained models can generalize in low-resource settings, providing insights for better abuse detection in multilingual environments.'}, 'zh': {'title': 'Âà©Áî®È¢ÑËÆ≠ÁªÉÊ®°ÂûãÊèêÂçá‰ΩéËµÑÊ∫êÁéØÂ¢É‰∏ãÁöÑËæ±È™ÇÂÜÖÂÆπÊ£ÄÊµã', 'desc': 'Êú¨Á†îÁ©∂Êé¢ËÆ®‰∫ÜÂú®‰ΩéËµÑÊ∫êÁéØÂ¢É‰∏≠ÔºåÁâπÂà´ÊòØÈü≥È¢ëÊ®°Âºè‰∏ãÔºåÊ£ÄÊµãÂú®Á∫øËæ±È™ÇÂÜÖÂÆπÁöÑÊΩúÂäõ„ÄÇÊàë‰ª¨‰ΩøÁî®È¢ÑËÆ≠ÁªÉÁöÑÈü≥È¢ëË°®Á§∫ÔºåÁªìÂêàÂ∞ëÈáèÂ≠¶‰π†ÔºàFew Shot LearningÔºâÔºåÂú®Âç∞Â∫¶ËØ≠Ë®Ä‰∏≠ËøõË°åËæ±È™ÇËØ≠Ë®ÄÁöÑÊ£ÄÊµã„ÄÇÈÄöËøáÂà©Áî®Wav2VecÂíåWhisperÁ≠âÊ®°ÂûãÁöÑÂº∫Â§ßË°®Á§∫ÔºåÊàë‰ª¨Âú®ADIMAÊï∞ÊçÆÈõÜ‰∏äËøõË°åË∑®ËØ≠Ë®ÄÁöÑËæ±È™ÇÊ£ÄÊµã„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãÂú®‰ΩéËµÑÊ∫êÂú∫ÊôØ‰∏≠ÁöÑÊ≥õÂåñËÉΩÂäõÔºå‰∏∫Â§öËØ≠Ë®ÄÁéØÂ¢É‰∏≠ÁöÑËæ±È™ÇËØ≠Ë®ÄÊ£ÄÊµãÊèê‰æõ‰∫ÜÊúâ‰ª∑ÂÄºÁöÑËßÅËß£„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2411.19477', 'title': 'A Simple and Provable Scaling Law for the Test-Time Compute of Large Language Models', 'url': 'https://huggingface.co/papers/2411.19477', 'abstract': 'We propose a general two-stage algorithm that enjoys a provable scaling law for the test-time compute of large language models (LLMs). Given an input problem, the proposed algorithm first generates N candidate solutions, and then chooses the best one via a multiple-round knockout tournament where each pair of candidates are compared for K times and only the winners move on to the next round. In a minimalistic implementation, both stages can be executed with a black-box LLM alone and nothing else (e.g., no external verifier or reward model), and a total of N times (K + 1) highly parallelizable LLM calls are needed for solving an input problem. Assuming that a generated candidate solution is correct with probability p_{gen} > 0 and a comparison between a pair of correct and incorrect solutions identifies the right winner with probability p_{comp} > 0.5 (i.e., better than a random guess), we prove theoretically that the failure probability of the proposed algorithm decays to zero exponentially with respect to N and K: $P(final output is incorrect) le (1 - p_{gen})^N + lceil log_2 N rceil e^{-2 K (p_{comp} - 0.5)^2}.$ Our empirical results with the challenging MMLU-Pro benchmark validate the technical assumptions, as well as the efficacy of the proposed algorithm and the gains from scaling up its test-time compute.', 'score': 1, 'issue_id': 912, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 –Ω–æ—è–±—Ä—è', 'en': 'November 29', 'zh': '11Êúà29Êó•'}, 'hash': '6057902d5fcbe3f4', 'authors': ['Yanxi Chen', 'Xuchen Pan', 'Yaliang Li', 'Bolin Ding', 'Jingren Zhou'], 'affiliations': ['Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2411.19477.jpg', 'data': {'categories': ['#training', '#benchmark', '#architecture', '#optimization'], 'emoji': 'üèÜ', 'ru': {'title': '–ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º —É–ª—É—á—à–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º, –∫–æ—Ç–æ—Ä—ã–π –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—ã–π –∑–∞–∫–æ–Ω –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≤–æ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è. –ê–ª–≥–æ—Ä–∏—Ç–º —Å–Ω–∞—á–∞–ª–∞ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç N –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ —Ä–µ—à–µ–Ω–∏–π, –∞ –∑–∞—Ç–µ–º –≤—ã–±–∏—Ä–∞–µ—Ç –ª—É—á—à–µ–µ —Å –ø–æ–º–æ—â—å—é –º–Ω–æ–≥–æ—Ä–∞—É–Ω–¥–æ–≤–æ–≥–æ —Ç—É—Ä–Ω–∏—Ä–∞ –Ω–∞ –≤—ã–±—ã–≤–∞–Ω–∏–µ. –¢–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏ –¥–æ–∫–∞–∑–∞–Ω–æ, —á—Ç–æ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –æ—à–∏–±–∫–∏ –∞–ª–≥–æ—Ä–∏—Ç–º–∞ —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–æ —É–º–µ–Ω—å—à–∞–µ—Ç—Å—è —Å —É–≤–µ–ª–∏—á–µ–Ω–∏–µ–º N –∏ K. –≠–º–ø–∏—Ä–∏—á–µ—Å–∫–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ —Å–ª–æ–∂–Ω–æ–º –±–µ–Ω—á–º–∞—Ä–∫–µ MMLU-Pro –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–æ–≥–æ –º–µ—Ç–æ–¥–∞.'}, 'en': {'title': 'Efficient Solution Selection for Large Language Models', 'desc': 'This paper introduces a two-stage algorithm designed to improve the efficiency of large language models (LLMs) during test time. The first stage generates multiple candidate solutions for a given problem, while the second stage employs a knockout tournament to select the best solution through repeated comparisons. The algorithm can operate solely with a black-box LLM, requiring a manageable number of parallel calls to generate and evaluate candidates. The authors provide theoretical proof that the likelihood of incorrect outputs decreases exponentially as the number of candidates and comparisons increases, supported by empirical results from the MMLU-Pro benchmark.'}, 'zh': {'title': 'È´òÊïàÈÄâÊã©Ôºö‰∏§Èò∂ÊÆµÁÆóÊ≥ï‰ºòÂåñÂ§ßËØ≠Ë®ÄÊ®°ÂûãËÆ°ÁÆó', 'desc': 'Êàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÈÄöÁî®ÁöÑ‰∏§Èò∂ÊÆµÁÆóÊ≥ïÔºåËÉΩÂ§üÂú®Â§ßËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÁöÑÊµãËØïÊó∂Èó¥ËÆ°ÁÆó‰∏≠ÂÆûÁé∞ÂèØËØÅÊòéÁöÑÊâ©Â±ïËßÑÂæã„ÄÇËØ•ÁÆóÊ≥ïÈ¶ñÂÖàÁîüÊàêN‰∏™ÂÄôÈÄâËß£ÔºåÁÑ∂ÂêéÈÄöËøáÂ§öËΩÆÊ∑òÊ±∞ËµõÈÄâÊã©ÊúÄ‰Ω≥Ëß£ÔºåÊØèÂØπÂÄôÈÄâËß£ÊØîËæÉKÊ¨°ÔºåÂè™ÊúâËÉúËÄÖËøõÂÖ•‰∏ã‰∏ÄËΩÆ„ÄÇËØ•ÁÆóÊ≥ïÁöÑÊúÄÁÆÄÂÆûÁé∞‰ªÖÈúÄ‰ΩøÁî®ÈªëÁÆ±LLMÔºåÊó†ÈúÄÂ§ñÈÉ®È™åËØÅÂô®ÊàñÂ•ñÂä±Ê®°ÂûãÔºåÊÄªÂÖ±ÈúÄË¶ÅNÊ¨°(K + 1)È´òÂ∫¶ÂèØÂπ∂Ë°åÁöÑLLMË∞ÉÁî®Êù•Ëß£ÂÜ≥ËæìÂÖ•ÈóÆÈ¢ò„ÄÇÁêÜËÆ∫ËØÅÊòéË°®ÊòéÔºåÂÅáËÆæÁîüÊàêÁöÑÂÄôÈÄâËß£Ê≠£Á°ÆÁöÑÊ¶ÇÁéá‰∏∫p_{gen} > 0Ôºå‰∏îÊ≠£Á°Æ‰∏éÈîôËØØËß£ÁöÑÊØîËæÉËÉΩ‰ª•Ê¶ÇÁéáp_{comp} > 0.5ËØÜÂà´Âá∫Ê≠£Á°ÆÁöÑËÉúËÄÖÔºåÂàôËØ•ÁÆóÊ≥ïÁöÑÂ§±Ë¥•Ê¶ÇÁéáÈöèÁùÄNÂíåKÁöÑÂ¢ûÂä†ÂëàÊåáÊï∞Á∫ß‰∏ãÈôç„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2412.02259', 'title': 'VideoGen-of-Thought: A Collaborative Framework for Multi-Shot Video Generation', 'url': 'https://huggingface.co/papers/2412.02259', 'abstract': 'Current video generation models excel at generating short clips but still struggle with creating multi-shot, movie-like videos. Existing models trained on large-scale data on the back of rich computational resources are unsurprisingly inadequate for maintaining a logical storyline and visual consistency across multiple shots of a cohesive script since they are often trained with a single-shot objective. To this end, we propose VideoGen-of-Thought (VGoT), a collaborative and training-free architecture designed specifically for multi-shot video generation. VGoT is designed with three goals in mind as follows. Multi-Shot Video Generation: We divide the video generation process into a structured, modular sequence, including (1) Script Generation, which translates a curt story into detailed prompts for each shot; (2) Keyframe Generation, responsible for creating visually consistent keyframes faithful to character portrayals; and (3) Shot-Level Video Generation, which transforms information from scripts and keyframes into shots; (4) Smoothing Mechanism that ensures a consistent multi-shot output. Reasonable Narrative Design: Inspired by cinematic scriptwriting, our prompt generation approach spans five key domains, ensuring logical consistency, character development, and narrative flow across the entire video. Cross-Shot Consistency: We ensure temporal and identity consistency by leveraging identity-preserving (IP) embeddings across shots, which are automatically created from the narrative. Additionally, we incorporate a cross-shot smoothing mechanism, which integrates a reset boundary that effectively combines latent features from adjacent shots, resulting in smooth transitions and maintaining visual coherence throughout the video. Our experiments demonstrate that VGoT surpasses existing video generation methods in producing high-quality, coherent, multi-shot videos.', 'score': 43, 'issue_id': 933, 'pub_date': '2024-12-03', 'pub_date_card': {'ru': '3 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 3', 'zh': '12Êúà3Êó•'}, 'hash': '5a007f38be3e3ba7', 'authors': ['Mingzhe Zheng', 'Yongqi Xu', 'Haojian Huang', 'Xuran Ma', 'Yexin Liu', 'Wenjie Shu', 'Yatian Pang', 'Feilong Tang', 'Qifeng Chen', 'Harry Yang', 'Ser-Nam Lim'], 'affiliations': ['Everlyn AI', 'Hong Kong University of Science and Technology', 'National University of Singapore', 'Peking University', 'University of Central Florida', 'University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2412.02259.jpg', 'data': {'categories': ['#video', '#story_generation', '#games'], 'emoji': 'üé¨', 'ru': {'title': 'VGoT: –ù–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –º–Ω–æ–≥–æ–∫–∞–¥—Ä–æ–≤—ã—Ö –≤–∏–¥–µ–æ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –ª–æ–≥–∏–∫–∏ –ø–æ–≤–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –º–Ω–æ–≥–æ–∫–∞–¥—Ä–æ–≤—ã—Ö –≤–∏–¥–µ–æ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º VideoGen-of-Thought (VGoT). –≠—Ç–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Ä–∞–∑–¥–µ–ª—è–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ —ç—Ç–∞–ø–æ–≤: –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Å—Ü–µ–Ω–∞—Ä–∏—è, —Å–æ–∑–¥–∞–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö –∫–∞–¥—Ä–æ–≤, –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Å—Ü–µ–Ω –∏ –º–µ—Ö–∞–Ω–∏–∑–º —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏—è. VGoT –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≤—Å—Ç—Ä–∞–∏–≤–∞–Ω–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç–∏ –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π –º–µ–∂–¥—É —Å—Ü–µ–Ω–∞–º–∏ –∏ –ø—Ä–∏–º–µ–Ω—è–µ—Ç –º–µ–∂–∫–∞–¥—Ä–æ–≤–æ–µ —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –≤–∏–∑—É–∞–ª—å–Ω–æ–π —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ VGoT –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã –≤ —Å–æ–∑–¥–∞–Ω–∏–∏ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∏ —Å–≤—è–∑–Ω—ã—Ö –º–Ω–æ–≥–æ–∫–∞–¥—Ä–æ–≤—ã—Ö –≤–∏–¥–µ–æ.'}, 'en': {'title': 'Revolutionizing Multi-Shot Video Generation with VGoT', 'desc': 'The paper introduces VideoGen-of-Thought (VGoT), a novel architecture aimed at improving multi-shot video generation. Unlike traditional models that focus on single-shot outputs, VGoT employs a structured approach that includes script generation, keyframe creation, and shot-level video generation, ensuring a cohesive narrative. It emphasizes reasonable narrative design by incorporating principles from cinematic scriptwriting, which enhances character development and logical flow. Additionally, VGoT utilizes identity-preserving embeddings and a cross-shot smoothing mechanism to maintain visual consistency and smooth transitions across multiple shots.'}, 'zh': {'title': 'Â§öÈïúÂ§¥ËßÜÈ¢ëÁîüÊàêÁöÑÊñ∞Á™ÅÁ†¥', 'desc': 'ÂΩìÂâçÁöÑËßÜÈ¢ëÁîüÊàêÊ®°ÂûãÂú®ÁîüÊàêÁü≠ÁâáÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÂú®ÂàõÂª∫Â§öÈïúÂ§¥„ÄÅÁîµÂΩ±Ëà¨ÁöÑËßÜÈ¢ëÊó∂‰ªçÁÑ∂Â≠òÂú®Âõ∞Èöæ„ÄÇÁé∞ÊúâÊ®°ÂûãÈÄöÂ∏∏Âè™ÈíàÂØπÂçïÈïúÂ§¥ÁõÆÊ†áËøõË°åËÆ≠ÁªÉÔºåÂõ†Ê≠§Âú®‰øùÊåÅÈÄªËæëÊïÖ‰∫ãÁ∫øÂíåËßÜËßâ‰∏ÄËá¥ÊÄßÊñπÈù¢ÊòæÂæó‰∏çË∂≥„ÄÇ‰∏∫Ê≠§ÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜVideoGen-of-ThoughtÔºàVGoTÔºâÔºåËøôÊòØ‰∏ÄÁßç‰∏ìÈó®‰∏∫Â§öÈïúÂ§¥ËßÜÈ¢ëÁîüÊàêËÆæËÆ°ÁöÑÂçè‰ΩúÂíåÊó†ËÆ≠ÁªÉÊû∂ÊûÑ„ÄÇVGoTÈÄöËøáËÑöÊú¨ÁîüÊàê„ÄÅÂÖ≥ÈîÆÂ∏ßÁîüÊàêÂíåÈïúÂ§¥Á∫ßËßÜÈ¢ëÁîüÊàêÁ≠âÊ®°ÂùóÂåñÊ≠•È™§ÔºåÁ°Æ‰øù‰∫ÜÂêàÁêÜÁöÑÂèô‰∫ãËÆæËÆ°ÂíåË∑®ÈïúÂ§¥ÁöÑ‰∏ÄËá¥ÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2411.19943', 'title': "Critical Tokens Matter: Token-Level Contrastive Estimation Enhence LLM's Reasoning Capability", 'url': 'https://huggingface.co/papers/2411.19943', 'abstract': "Large Language Models (LLMs) have exhibited remarkable performance on reasoning tasks. They utilize autoregressive token generation to construct reasoning trajectories, enabling the development of a coherent chain of thought. In this work, we explore the impact of individual tokens on the final outcomes of reasoning tasks. We identify the existence of ``critical tokens'' that lead to incorrect reasoning trajectories in LLMs. Specifically, we find that LLMs tend to produce positive outcomes when forced to decode other tokens instead of critical tokens. Motivated by this observation, we propose a novel approach - cDPO - designed to automatically recognize and conduct token-level rewards for the critical tokens during the alignment process. Specifically, we develop a contrastive estimation approach to automatically identify critical tokens. It is achieved by comparing the generation likelihood of positive and negative models. To achieve this, we separately fine-tune the positive and negative models on various reasoning trajectories, consequently, they are capable of identifying identify critical tokens within incorrect trajectories that contribute to erroneous outcomes. Moreover, to further align the model with the critical token information during the alignment process, we extend the conventional DPO algorithms to token-level DPO and utilize the differential likelihood from the aforementioned positive and negative model as important weight for token-level DPO learning.Experimental results on GSM8K and MATH500 benchmarks with two-widely used models Llama-3 (8B and 70B) and deepseek-math (7B) demonstrate the effectiveness of the propsoed approach cDPO.", 'score': 33, 'issue_id': 933, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 –Ω–æ—è–±—Ä—è', 'en': 'November 29', 'zh': '11Êúà29Êó•'}, 'hash': 'aaf523f6bd9412e3', 'authors': ['Zicheng Lin', 'Tian Liang', 'Jiahao Xu', 'Xing Wang', 'Ruilin Luo', 'Chufan Shi', 'Siheng Li', 'Yujiu Yang', 'Zhaopeng Tu'], 'affiliations': ['Tsinghua University', 'Tencent AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2411.19943.jpg', 'data': {'categories': ['#math', '#training', '#rlhf', '#reasoning', '#benchmark', '#alignment'], 'emoji': 'üîç', 'ru': {'title': '–ü–æ–≤—ã—à–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π LLM –ø—É—Ç–µ–º –≤—ã—è–≤–ª–µ–Ω–∏—è –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤', 'desc': "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ –≤–ª–∏—è–Ω–∏—é –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –Ω–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM). –ê–≤—Ç–æ—Ä—ã –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ '–∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤', –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–∏–≤–æ–¥—è—Ç –∫ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–º —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—è–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –û–Ω–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ cDPO –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –∏ –æ–±—É—á–µ–Ω–∏—è —Å —É—á–µ—Ç–æ–º –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö GSM8K –∏ MATH500 —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–æ–¥–µ–ª–µ–π Llama-3 –∏ deepseek-math –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞–ª–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞."}, 'en': {'title': 'Enhancing Reasoning in LLMs by Identifying Critical Tokens', 'desc': "This paper investigates how individual tokens in Large Language Models (LLMs) affect reasoning outcomes. It identifies 'critical tokens' that can lead to incorrect reasoning paths, suggesting that LLMs perform better when they focus on non-critical tokens. The authors introduce a new method called cDPO, which uses contrastive estimation to automatically detect these critical tokens during the model's alignment process. Experimental results show that cDPO improves reasoning performance on benchmark datasets by effectively managing token-level rewards."}, 'zh': {'title': 'ËØÜÂà´ÂÖ≥ÈîÆtokenÔºåÊèêÂçáÊé®ÁêÜÂáÜÁ°ÆÊÄß', 'desc': 'Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Êé®ÁêÜ‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåËÉΩÂ§üÈÄöËøáËá™ÂõûÂΩíÁöÑÊñπÂºèÁîüÊàêÊé®ÁêÜËøáÁ®ã„ÄÇÊú¨ÊñáÊé¢ËÆ®‰∫ÜÂçï‰∏™tokenÂØπÊé®ÁêÜ‰ªªÂä°ÊúÄÁªàÁªìÊûúÁöÑÂΩ±ÂìçÔºåÂèëÁé∞Â≠òÂú®‚ÄúÂÖ≥ÈîÆtoken‚ÄùÔºåËøô‰∫õtoken‰ºöÂØºËá¥ÈîôËØØÁöÑÊé®ÁêÜËΩ®Ëøπ„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÊñπÊ≥ïcDPOÔºåÊó®Âú®Ëá™Âä®ËØÜÂà´ÂÖ≥ÈîÆtokenÂπ∂Âú®ÂØπÈΩêËøáÁ®ã‰∏≠ËøõË°åtokenÁ∫ßÂ•ñÂä±„ÄÇÈÄöËøáÂØπÊØîÊ≠£Ë¥üÊ®°ÂûãÁöÑÁîüÊàêÂèØËÉΩÊÄßÔºåÊàë‰ª¨ËÉΩÂ§üËØÜÂà´Âá∫Âú®ÈîôËØØËΩ®Ëøπ‰∏≠ÂØºËá¥ÈîôËØØÁªìÊûúÁöÑÂÖ≥ÈîÆtokenÔºå‰ªéËÄåÊèêÈ´òÊ®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2412.01928', 'title': 'MALT: Improving Reasoning with Multi-Agent LLM Training', 'url': 'https://huggingface.co/papers/2412.01928', 'abstract': 'Enabling effective collaboration among LLMs is a crucial step toward developing autonomous systems capable of solving complex problems. While LLMs are typically used as single-model generators, where humans critique and refine their outputs, the potential for jointly-trained collaborative models remains largely unexplored. Despite promising results in multi-agent communication and debate settings, little progress has been made in training models to work together on tasks. In this paper, we present a first step toward "Multi-agent LLM training" (MALT) on reasoning problems. Our approach employs a sequential multi-agent setup with heterogeneous LLMs assigned specialized roles: a generator, verifier, and refinement model iteratively solving problems. We propose a trajectory-expansion-based synthetic data generation process and a credit assignment strategy driven by joint outcome based rewards. This enables our post-training setup to utilize both positive and negative trajectories to autonomously improve each model\'s specialized capabilities as part of a joint sequential system. We evaluate our approach across MATH, GSM8k, and CQA, where MALT on Llama 3.1 8B models achieves relative improvements of 14.14%, 7.12%, and 9.40% respectively over the same baseline model. This demonstrates an early advance in multi-agent cooperative capabilities for performance on mathematical and common sense reasoning questions. More generally, our work provides a concrete direction for research around multi-agent LLM training approaches.', 'score': 19, 'issue_id': 943, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 2', 'zh': '12Êúà2Êó•'}, 'hash': '980ef49924f6a484', 'authors': ['Sumeet Ramesh Motwani', 'Chandler Smith', 'Rocktim Jyoti Das', 'Markian Rybchuk', 'Philip H. S. Torr', 'Ivan Laptev', 'Fabio Pizzati', 'Ronald Clark', 'Christian Schroeder de Witt'], 'affiliations': ['University of Oxford', 'Cooperative AI Foundation', 'MBZUAI', 'UC Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2412.01928.jpg', 'data': {'categories': ['#math', '#training', '#synthetic', '#agents', '#reasoning'], 'emoji': 'ü§ñ', 'ru': {'title': '–°–æ–≤–º–µ—Å—Ç–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ LLM –¥–ª—è —Ä–µ—à–µ–Ω–∏—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è', 'desc': "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–Ω–æ–π —Ä–∞–±–æ—Ç—ã –Ω–∞–¥ —Å–ª–æ–∂–Ω—ã–º–∏ –∑–∞–¥–∞—á–∞–º–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥ '–ú—É–ª—å—Ç–∏-–∞–≥–µ–Ω—Ç–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è LLM' (MALT), –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—É—é setup —Å –≥–µ—Ç–µ—Ä–æ–≥–µ–Ω–Ω—ã–º–∏ LLM –≤ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ä–æ–ª—è—Ö: –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä, –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ç–æ—Ä –∏ –º–æ–¥–µ–ª—å —É—Ç–æ—á–Ω–µ–Ω–∏—è. –ü—Ä–æ—Ü–µ—Å—Å –≤–∫–ª—é—á–∞–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏—é —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π –∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—é —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫—Ä–µ–¥–∏—Ç–∞, —É–ø—Ä–∞–≤–ª—è–µ–º—É—é —Å–æ–≤–º–µ—Å—Ç–Ω—ã–º–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ –∑–∞–¥–∞—á–∞—Ö –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –∏ –∑–¥—Ä–∞–≤–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –±–∞–∑–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏."}, 'en': {'title': 'Unlocking Collaborative Intelligence in LLMs', 'desc': 'This paper introduces a novel approach called Multi-agent LLM training (MALT) aimed at enhancing collaboration among large language models (LLMs) for solving complex reasoning tasks. The authors propose a structured setup where different LLMs take on specialized roles‚Äîsuch as generator, verifier, and refinement model‚Äîto iteratively tackle problems. They implement a synthetic data generation process and a credit assignment strategy that rewards models based on their joint performance, allowing them to learn from both successful and unsuccessful attempts. The results show significant performance improvements on various reasoning benchmarks, highlighting the potential of cooperative multi-agent systems in advancing LLM capabilities.'}, 'zh': {'title': 'Â§öÊô∫ËÉΩ‰ΩìÂçè‰ΩúËÆ≠ÁªÉÔºåÊèêÂçáÊé®ÁêÜËÉΩÂäõÔºÅ', 'desc': 'Êú¨ÊñáÊé¢ËÆ®‰∫ÜÂ§öÊô∫ËÉΩ‰ΩìÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâËÆ≠ÁªÉÁöÑÊΩúÂäõÔºåÊó®Âú®ÊèêÈ´òÊ®°ÂûãÂú®Â§çÊùÇÈóÆÈ¢ò‰∏äÁöÑÂçè‰ΩúËÉΩÂäõ„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂ§öÊô∫ËÉΩ‰ΩìËÆæÁΩÆÔºåÊ®°ÂûãÂàÜ‰∏∫ÁîüÊàêÂô®„ÄÅÈ™åËØÅÂô®ÂíåÁ≤æÁÇºÊ®°ÂûãÔºåÂçèÂêåËß£ÂÜ≥Êé®ÁêÜÈóÆÈ¢ò„ÄÇÈÄöËøáËΩ®ËøπÊâ©Â±ïÁöÑÂêàÊàêÊï∞ÊçÆÁîüÊàêÂíåÂü∫‰∫éËÅîÂêàÁªìÊûúÁöÑÂ•ñÂä±Á≠ñÁï•ÔºåÊàë‰ª¨ÁöÑÊ®°ÂûãËÉΩÂ§üËá™‰∏ªÊèêÂçáÂêÑËá™ÁöÑ‰∏ì‰∏öËÉΩÂäõ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºå‰ΩøÁî®MALTÊñπÊ≥ïÁöÑLlama 3.1 8BÊ®°ÂûãÂú®Êï∞Â≠¶ÂíåÂ∏∏ËØÜÊé®ÁêÜ‰ªªÂä°‰∏äÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2412.01981', 'title': 'Free Process Rewards without Process Labels', 'url': 'https://huggingface.co/papers/2412.01981', 'abstract': "Different from its counterpart outcome reward models (ORMs), which evaluate the entire responses, a process reward model (PRM) scores a reasoning trajectory step by step, providing denser and more fine grained rewards. However, training a PRM requires labels annotated at every intermediate step, presenting significant challenges for both manual and automatic data collection. This paper aims to address this challenge. Both theoretically and empirically, we show that an implicit PRM can be obtained at no additional cost, by simply training an ORM on the cheaper response-level labels. The only assumption is to parameterize the outcome reward as the log-likelihood ratios of the policy and reference models, which can be optimized regardless of the specific choice of loss objectives. In experiments, we instantiate our implicit PRMs with various objectives and evaluate their performance on MATH. We show that our implicit PRM outperforms a strong MCTS-based baseline \\'a la Math-Shepherd using less than 1/38 of the training data. Its performance can be further improved with majority voting. We further find that scaling up instructions and responses benefits our implicit PRM, and the latter brings a larger gain. Particularly, we find that our implicit PRM, when instantiated with the cross-entropy (CE) loss, is more data-efficient and can keep improving generation models even when trained with only one response per instruction, the setup that suffers from extreme data scarcity and imbalance. Further, instructions should be relevant to downstream tasks while the diversity of responses does not bring gains. Surprisingly, training on extra Math-Shepherd step labels brings no further improvements to our implicit PRM trained on only outcome data. We hope that our work will encourage a rethinking of PRM training approaches and contribute to making training PRMs more accessible.", 'score': 18, 'issue_id': 933, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 2', 'zh': '12Êúà2Êó•'}, 'hash': '13434e4f301a0d88', 'authors': ['Lifan Yuan', 'Wendi Li', 'Huayu Chen', 'Ganqu Cui', 'Ning Ding', 'Kaiyan Zhang', 'Bowen Zhou', 'Zhiyuan Liu', 'Hao Peng'], 'affiliations': ['Huazhong University of Science and Technology', 'Tsinghua University', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2412.01981.jpg', 'data': {'categories': ['#optimization', '#math', '#training', '#reasoning', '#data', '#low_resource'], 'emoji': 'üß†', 'ru': {'title': '–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ PRM –±–µ–∑ –ø–æ—à–∞–≥–æ–≤–æ–π —Ä–∞–∑–º–µ—Ç–∫–∏', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –ø—Ä–æ—Ü–µ—Å—Å—É–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è (PRM) –≤ –º–∞—à–∏–Ω–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏. –ê–≤—Ç–æ—Ä—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –Ω–µ—è–≤–Ω—É—é PRM –º–æ–∂–Ω–æ –ø–æ–ª—É—á–∏—Ç—å –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–∞—Ç—Ä–∞—Ç, –ø—Ä–æ—Å—Ç–æ –æ–±—É—á–∞—è –º–æ–¥–µ–ª—å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –∑–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç (ORM) –Ω–∞ –±–æ–ª–µ–µ –¥–µ—à–µ–≤—ã—Ö –º–µ—Ç–∫–∞—Ö —É—Ä–æ–≤–Ω—è –æ—Ç–≤–µ—Ç–æ–≤. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ MATH –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç, —á—Ç–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å–∏–ª—å–Ω—ã–π –±–µ–π–∑–ª–∞–π–Ω –Ω–∞ –æ—Å–Ω–æ–≤–µ MCTS, –∏—Å–ø–æ–ª—å–∑—É—è –º–µ–Ω–µ–µ 1/38 –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ç–∞–∫–∂–µ –≤—ã—è–≤–ª—è–µ—Ç, —á—Ç–æ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –∏ –æ—Ç–≤–µ—Ç–æ–≤ —É–ª—É—á—à–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–µ—è–≤–Ω–æ–π PRM.'}, 'en': {'title': 'Unlocking Efficient Training for Process Reward Models', 'desc': 'This paper introduces a novel approach to training process reward models (PRMs) that score reasoning steps individually, as opposed to outcome reward models (ORMs) which evaluate entire responses. The authors propose that an implicit PRM can be derived from an ORM trained on simpler response-level labels, thus avoiding the need for detailed step-by-step annotations. Through theoretical and empirical analysis, they demonstrate that this implicit PRM can achieve superior performance on tasks like MATH, using significantly less training data than traditional methods. The findings suggest that optimizing the outcome reward as log-likelihood ratios enhances data efficiency and model performance, even in scenarios with limited training examples.'}, 'zh': {'title': 'ÈöêÂºèËøáÁ®ãÂ•ñÂä±Ê®°ÂûãÔºöÈ´òÊïàËÆ≠ÁªÉÁöÑÊñ∞ÊÄùË∑Ø', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑËøáÁ®ãÂ•ñÂä±Ê®°ÂûãÔºàPRMÔºâÔºå‰∏é‰º†ÁªüÁöÑÁªìÊûúÂ•ñÂä±Ê®°ÂûãÔºàORMÔºâ‰∏çÂêåÔºåPRMËÉΩÂ§üÈÄêÊ≠•ËØÑ‰º∞Êé®ÁêÜËøáÁ®ãÔºåÊèê‰æõÊõ¥ÁªÜËá¥ÁöÑÂ•ñÂä±„ÄÇÁÑ∂ËÄåÔºåËÆ≠ÁªÉPRMÈúÄË¶ÅÂú®ÊØè‰∏™‰∏≠Èó¥Ê≠•È™§ÈÉΩÊúâÊ†áÊ≥®ÔºåËøôÂú®Êï∞ÊçÆÊî∂ÈõÜ‰∏äÈù¢‰∏¥ÊåëÊàò„ÄÇÊàë‰ª¨Â±ïÁ§∫‰∫ÜÈÄöËøáËÆ≠ÁªÉORMÂπ∂‰ΩøÁî®ÂìçÂ∫îÁ∫ßÂà´ÁöÑÊ†áÁ≠æÔºåÂèØ‰ª•Âú®Ê≤°ÊúâÈ¢ùÂ§ñÊàêÊú¨ÁöÑÊÉÖÂÜµ‰∏ãËé∑ÂæóÈöêÂºèPRM„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÈöêÂºèPRMÂú®Êï∞ÊçÆÊïàÁéáÂíåÊÄßËÉΩ‰∏ä‰ºò‰∫é‰º†ÁªüÊñπÊ≥ïÔºåÂ∞§ÂÖ∂ÊòØÂú®Êï∞ÊçÆÁ®ÄÁº∫ÁöÑÊÉÖÂÜµ‰∏ã„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2412.02611', 'title': 'AV-Odyssey Bench: Can Your Multimodal LLMs Really Understand Audio-Visual Information?', 'url': 'https://huggingface.co/papers/2412.02611', 'abstract': 'Recently, multimodal large language models (MLLMs), such as GPT-4o, Gemini 1.5 Pro, and Reka Core, have expanded their capabilities to include vision and audio modalities. While these models demonstrate impressive performance across a wide range of audio-visual applications, our proposed DeafTest reveals that MLLMs often struggle with simple tasks humans find trivial: 1) determining which of two sounds is louder, and 2) determining which of two sounds has a higher pitch. Motivated by these observations, we introduce AV-Odyssey Bench, a comprehensive audio-visual benchmark designed to assess whether those MLLMs can truly understand the audio-visual information. This benchmark encompasses 4,555 carefully crafted problems, each incorporating text, visual, and audio components. To successfully infer answers, models must effectively leverage clues from both visual and audio inputs. To ensure precise and objective evaluation of MLLM responses, we have structured the questions as multiple-choice, eliminating the need for human evaluation or LLM-assisted assessment. We benchmark a series of closed-source and open-source models and summarize the observations. By revealing the limitations of current models, we aim to provide useful insight for future dataset collection and model development.', 'score': 17, 'issue_id': 933, 'pub_date': '2024-12-03', 'pub_date_card': {'ru': '3 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 3', 'zh': '12Êúà3Êó•'}, 'hash': 'f63565048b4948b4', 'authors': ['Kaixiong Gong', 'Kaituo Feng', 'Bohao Li', 'Yibing Wang', 'Mofan Cheng', 'Shijia Yang', 'Jiaming Han', 'Benyou Wang', 'Yutong Bai', 'Zhuoran Yang', 'Xiangyu Yue'], 'affiliations': ['CUHK (SZ)', 'CUHK MMLab', 'Stanford University', 'UC Berkeley', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2412.02611.jpg', 'data': {'categories': ['#open_source', '#benchmark', '#interpretability', '#multimodal', '#games'], 'emoji': 'üéß', 'ru': {'title': '–°–ª—ã—à–∞—Ç –ª–∏ –ò–ò-–º–æ–¥–µ–ª–∏ —Ç–æ, —á—Ç–æ –≤–∏–¥—è—Ç?', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π —Ç–µ—Å—Ç DeafTest –∏ –±–µ–Ω—á–º–∞—Ä–∫ AV-Odyssey Bench –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (MLLM) –≤ –∑–∞–¥–∞—á–∞—Ö –∞—É–¥–∏–æ-–≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è. –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ MLLM —á–∞—Å—Ç–æ –Ω–µ —Å–ø—Ä–∞–≤–ª—è—é—Ç—Å—è —Å –ø—Ä–æ—Å—Ç—ã–º–∏ –∑–∞–¥–∞—á–∞–º–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≥—Ä–æ–º–∫–æ—Å—Ç–∏ –∏ –≤—ã—Å–æ—Ç—ã –∑–≤—É–∫–æ–≤. –ë–µ–Ω—á–º–∞—Ä–∫ –≤–∫–ª—é—á–∞–µ—Ç 4,555 –∑–∞–¥–∞—á —Å —Ç–µ–∫—Å—Ç–æ–º, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ –∏ –∞—É–¥–∏–æ, —Ç—Ä–µ–±—É—é—â–∏—Ö —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –æ–±–æ–∏—Ö –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π. –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ–≤–µ–ª–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä—è–¥–∞ –∑–∞–∫—Ä—ã—Ç—ã—Ö –∏ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –º–æ–¥–µ–ª–µ–π, –≤—ã—è–≤–∏–≤ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —Ç–µ–∫—É—â–∏—Ö —Å–∏—Å—Ç–µ–º –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ —É–ª—É—á—à–µ–Ω–∏—è —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –º–æ–¥–µ–ª–µ–π.'}, 'en': {'title': 'Unveiling the Limits of Multimodal Models with AV-Odyssey Bench', 'desc': 'This paper introduces DeafTest, a benchmark that highlights the limitations of multimodal large language models (MLLMs) in understanding basic audio tasks that humans find easy. The authors present AV-Odyssey Bench, which consists of 4,555 problems that require models to analyze and integrate audio, visual, and text information. The benchmark is designed to objectively evaluate MLLM performance through multiple-choice questions, avoiding reliance on human judgment. By assessing various models, the study aims to shed light on the shortcomings of current MLLMs and guide future improvements in model training and dataset creation.'}, 'zh': {'title': 'Êè≠Á§∫Â§öÊ®°ÊÄÅÊ®°ÂûãÁöÑÂ±ÄÈôêÊÄß', 'desc': 'ÊúÄËøëÔºåÂ§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÂ¶ÇGPT-4o„ÄÅGemini 1.5 ProÂíåReka CoreÔºåÊâ©Â±ï‰∫ÜÂÖ∂Âú®ËßÜËßâÂíåÈü≥È¢ëÊñπÈù¢ÁöÑËÉΩÂäõ„ÄÇÂ∞ΩÁÆ°Ëøô‰∫õÊ®°ÂûãÂú®Â§öÁßçÈü≥È¢ë-ËßÜËßâÂ∫îÁî®‰∏≠Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÊàë‰ª¨ÁöÑDeafTestÊòæÁ§∫ÔºåMLLMsÂú®‰∏Ä‰∫õ‰∫∫Á±ªËÆ§‰∏∫ÁÆÄÂçïÁöÑ‰ªªÂä°‰∏äÂ∏∏Â∏∏Ë°®Áé∞‰∏ç‰Ω≥Ôºå‰æãÂ¶ÇÂà§Êñ≠‰∏§‰∏™Â£∞Èü≥Âì™‰∏™Êõ¥ÂìçÂíåÂì™‰∏™Èü≥Ë∞ÉÊõ¥È´ò„ÄÇ‰∏∫Ê≠§ÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜAV-Odyssey BenchÔºåËøôÊòØ‰∏Ä‰∏™ÂÖ®Èù¢ÁöÑÈü≥È¢ë-ËßÜËßâÂü∫ÂáÜÔºåÊó®Âú®ËØÑ‰º∞Ëøô‰∫õMLLMsÊòØÂê¶ÁúüÊ≠£ÁêÜËß£Èü≥È¢ë-ËßÜËßâ‰ø°ÊÅØ„ÄÇËØ•Âü∫ÂáÜÂåÖÂê´4555‰∏™Á≤æÂøÉËÆæËÆ°ÁöÑÈóÆÈ¢òÔºåË¶ÅÊ±ÇÊ®°ÂûãÊúâÊïàÂà©Áî®ËßÜËßâÂíåÈü≥È¢ëËæìÂÖ•‰∏≠ÁöÑÁ∫øÁ¥¢Ôºå‰ª•ÂáÜÁ°ÆÊé®Êñ≠Á≠îÊ°à„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2412.02114', 'title': 'OmniCreator: Self-Supervised Unified Generation with Universal Editing', 'url': 'https://huggingface.co/papers/2412.02114', 'abstract': 'We introduce OmniCreator, a novel framework that can conduct text-prompted unified (image+video) generation as well as editing all in one place. OmniCreator acquires generative and universal editing capabilities in a self-supervised manner, taking original text-video pairs as conditions while utilizing the same video as a denoising target to learn the semantic correspondence between video and text. During inference, when presented with a text prompt and a video, OmniCreator is capable of generating a target that is faithful to both, achieving a universal editing effect that is unconstrained as opposed to existing editing work that primarily focuses on certain editing types or relies on additional controls (e.g., structural conditions, attention features, or DDIM inversion). On the other hand, when presented with a text prompt only, OmniCreator becomes generative, producing high-quality video as a result of the semantic correspondence learned. Importantly, we found that the same capabilities extend to images as is, making OmniCreator a truly unified framework. Further, due to the lack of existing generative video editing benchmarks, we introduce the OmniBench-99 dataset, designed to evaluate the performance of generative video editing models comprehensively. Extensive experiments demonstrate that OmniCreator exhibits substantial superiority over all other models.', 'score': 12, 'issue_id': 939, 'pub_date': '2024-12-03', 'pub_date_card': {'ru': '3 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 3', 'zh': '12Êúà3Êó•'}, 'hash': '62bf26709baf7f97', 'authors': ['Haodong Chen', 'Lan Wang', 'Harry Yang', 'Ser-Nam Lim'], 'affiliations': ['Everlyn AI', 'HKUST', 'MSU', 'UCF'], 'pdf_title_img': 'assets/pdf/title_img/2412.02114.jpg', 'data': {'categories': ['#diffusion', '#multimodal', '#benchmark', '#dataset', '#video', '#optimization'], 'emoji': 'üé¨', 'ru': {'title': '–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –∏ —Ä–µ–¥–∞–∫—Ç–æ—Ä –º–µ–¥–∏–∞ –ø–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É –∑–∞–ø—Ä–æ—Å—É', 'desc': 'OmniCreator - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –≤–∏–¥–µ–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤. –û–Ω–∞ –æ–±—É—á–∞–µ—Ç—Å—è —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ, –∏—Å–ø–æ–ª—å–∑—É—è –ø–∞—Ä—ã —Ç–µ–∫—Å—Ç-–≤–∏–¥–µ–æ, –∏ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —Å–≤—è–∑–∏ –º–µ–∂–¥—É –≤–∏–¥–µ–æ –∏ —Ç–µ–∫—Å—Ç–æ–º. OmniCreator –º–æ–∂–µ—Ç –∫–∞–∫ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –Ω–æ–≤—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –ø–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É –æ–ø–∏—Å–∞–Ω–∏—é, —Ç–∞–∫ –∏ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –≤–∏–¥–µ–æ –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ –Ω–æ–≤—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö OmniBench-99 –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –≤–∏–¥–µ–æ.'}, 'en': {'title': 'OmniCreator: Unified Text-Prompted Image and Video Generation and Editing', 'desc': 'OmniCreator is a new framework that allows for the generation and editing of both images and videos using text prompts. It learns to understand the relationship between text and video through self-supervised learning, using original text-video pairs as input. During use, it can either generate new videos based on text prompts or edit existing videos while maintaining fidelity to the provided text. Additionally, the framework is evaluated using a new dataset called OmniBench-99, which helps assess the performance of generative video editing models.'}, 'zh': {'title': 'OmniCreatorÔºöÁªü‰∏ÄÁöÑÂõæÂÉè‰∏éËßÜÈ¢ëÁîüÊàê‰∏éÁºñËæëÊ°ÜÊû∂', 'desc': 'OmniCreatorÊòØ‰∏Ä‰∏™Êñ∞È¢ñÁöÑÊ°ÜÊû∂ÔºåËÉΩÂ§üÂú®‰∏Ä‰∏™Âπ≥Âè∞‰∏äËøõË°åÊñáÊú¨ÊèêÁ§∫ÁöÑÁªü‰∏ÄÁîüÊàêÔºàÂõæÂÉè+ËßÜÈ¢ëÔºâÂíåÁºñËæë„ÄÇÂÆÉÈÄöËøáËá™ÁõëÁù£Â≠¶‰π†ÔºåÂà©Áî®ÂéüÂßãÁöÑÊñáÊú¨-ËßÜÈ¢ëÂØπ‰Ωú‰∏∫Êù°‰ª∂ÔºåÂêåÊó∂‰ΩøÁî®Áõ∏ÂêåÁöÑËßÜÈ¢ë‰Ωú‰∏∫ÂéªÂô™ÁõÆÊ†áÔºåÂ≠¶‰π†ËßÜÈ¢ë‰∏éÊñáÊú¨‰πãÈó¥ÁöÑËØ≠‰πâÂØπÂ∫îÂÖ≥Á≥ª„ÄÇÂú®Êé®ÁêÜÈò∂ÊÆµÔºåOmniCreatorËÉΩÂ§üÊ†πÊçÆÊñáÊú¨ÊèêÁ§∫ÂíåËßÜÈ¢ëÁîüÊàêÂø†ÂÆû‰∫é‰∏§ËÄÖÁöÑÁõÆÊ†áÔºåÂÆûÁé∞Êó†Á∫¶ÊùüÁöÑÈÄöÁî®ÁºñËæëÊïàÊûú„ÄÇÊàë‰ª¨ËøòÂºïÂÖ•‰∫ÜOmniBench-99Êï∞ÊçÆÈõÜÔºå‰ª•ÂÖ®Èù¢ËØÑ‰º∞ÁîüÊàêËßÜÈ¢ëÁºñËæëÊ®°ÂûãÁöÑÊÄßËÉΩÔºåÂÆûÈ™åÁªìÊûúË°®ÊòéOmniCreatorÂú®ÊâÄÊúâÊ®°Âûã‰∏≠Ë°®Áé∞Âá∫ÊòæËëóÁöÑ‰ºòÂäø„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2411.19655', 'title': 'Truth or Mirage? Towards End-to-End Factuality Evaluation with LLM-OASIS', 'url': 'https://huggingface.co/papers/2411.19655', 'abstract': 'After the introduction of Large Language Models (LLMs), there have been substantial improvements in the performance of Natural Language Generation (NLG) tasks, including Text Summarization and Machine Translation. However, LLMs still produce outputs containing hallucinations, that is, content not grounded in factual information. Therefore, developing methods to assess the factuality of LLMs has become urgent.   Indeed, resources for factuality evaluation have recently emerged. Although challenging, these resources face one or more of the following limitations: (i) they are tailored to a specific task or domain; (ii) they are limited in size, thereby preventing the training of new factuality evaluators; (iii) they are designed for simpler verification tasks, such as claim verification.   To address these issues, we introduce LLM-Oasis, to the best of our knowledge the largest resource for training end-to-end factuality evaluators. LLM-Oasis is constructed by extracting claims from Wikipedia, falsifying a subset of these claims, and generating pairs of factual and unfactual texts. We then rely on human annotators to both validate the quality of our dataset and to create a gold standard test set for benchmarking factuality evaluation systems.   Our experiments demonstrate that LLM-Oasis presents a significant challenge for state-of-the-art LLMs, with GPT-4o achieving up to 60% accuracy in our proposed end-to-end factuality evaluation task, highlighting its potential to drive future research in the field.', 'score': 11, 'issue_id': 947, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 –Ω–æ—è–±—Ä—è', 'en': 'November 29', 'zh': '11Êúà29Êó•'}, 'hash': 'd1255811f49c640e', 'authors': ['Alessandro Scir√®', 'Andrei Stefan Bejgu', 'Simone Tedeschi', 'Karim Ghonim', 'Federico Martelli', 'Roberto Navigli'], 'affiliations': ['Babelscape, Italy', 'Sapienza University of Rome'], 'pdf_title_img': 'assets/pdf/title_img/2411.19655.jpg', 'data': {'categories': ['#hallucinations', '#benchmark', '#multimodal', '#machine_translation', '#dataset'], 'emoji': 'üîç', 'ru': {'title': 'LLM-Oasis: –ù–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ñ–∞–∫—Ç–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç LLM-Oasis - –∫—Ä—É–ø–Ω–µ–π—à–∏–π —Ä–µ—Å—É—Ä—Å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –æ—Ü–µ–Ω—â–∏–∫–æ–≤ —Ñ–∞–∫—Ç–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). LLM-Oasis —Å–æ–∑–¥–∞–Ω –ø—É—Ç–µ–º –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–π –∏–∑ –í–∏–∫–∏–ø–µ–¥–∏–∏, —Ñ–∞–ª—å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —á–∞—Å—Ç–∏ —ç—Ç–∏—Ö —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–π –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–∞—Ä —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö –∏ –Ω–µ—Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ LLM-Oasis –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—É—é —Å–ª–æ–∂–Ω–æ—Å—Ç—å –¥–ª—è —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö LLM, –ø—Ä–∏ —ç—Ç–æ–º GPT-4 –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç–∏ –¥–æ 60% –≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–æ–π –∑–∞–¥–∞—á–µ –æ—Ü–µ–Ω–∫–∏ —Ñ–∞–∫—Ç–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏. –†–µ—Å—É—Ä—Å –∏–º–µ–µ—Ç –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –¥–ª—è —Å—Ç–∏–º—É–ª–∏—Ä–æ–≤–∞–Ω–∏—è –±—É–¥—É—â–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –≤ —ç—Ç–æ–π –æ–±–ª–∞—Å—Ç–∏.'}, 'en': {'title': 'LLM-Oasis: A New Frontier for Factuality Evaluation in Language Models', 'desc': 'This paper discusses the advancements in Natural Language Generation (NLG) tasks due to Large Language Models (LLMs), while also addressing the issue of hallucinations, which are inaccuracies in generated content. It introduces LLM-Oasis, a comprehensive resource designed to train evaluators for assessing the factuality of LLM outputs. LLM-Oasis is created by extracting claims from Wikipedia, falsifying some, and generating pairs of factual and unfactual texts, validated by human annotators. The experiments show that LLM-Oasis poses a significant challenge to current LLMs, indicating its potential to enhance future research in factuality evaluation.'}, 'zh': {'title': 'Êé®Âä®‰∫ãÂÆûÊÄßËØÑ‰º∞ÁöÑÊú™Êù•Á†îÁ©∂', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Ëá™ÁÑ∂ËØ≠Ë®ÄÁîüÊàê‰ªªÂä°‰∏≠ÁöÑÂ∫îÁî®ÔºåÂ∞§ÂÖ∂ÊòØÂú®ÊñáÊú¨ÊëòË¶ÅÂíåÊú∫Âô®ÁøªËØëÊñπÈù¢ÁöÑËøõÂ±ï„ÄÇÁÑ∂ËÄåÔºåLLMs ‰ªçÁÑ∂‰ºö‰∫ßÁîüËôöÂÅá‰ø°ÊÅØÔºåÂç≥‰∏é‰∫ãÂÆû‰∏çÁ¨¶ÁöÑÂÜÖÂÆπÔºåÂõ†Ê≠§ËØÑ‰º∞ LLMs ÁöÑ‰∫ãÂÆûÊÄßÂèòÂæóÈùûÂ∏∏ÈáçË¶Å„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏ÄÈóÆÈ¢òÔºåÊú¨ÊñáÊèêÂá∫‰∫Ü LLM-OasisÔºåËøôÊòØ‰∏Ä‰∏™Áî®‰∫éËÆ≠ÁªÉÁ´ØÂà∞Á´Ø‰∫ãÂÆûÊÄßËØÑ‰º∞Âô®ÁöÑÊúÄÂ§ßËµÑÊ∫ê„ÄÇÈÄöËøá‰ªéÁª¥Âü∫ÁôæÁßëÊèêÂèñÂ£∞ÊòéÂπ∂ÁîüÊàêÁúüÂÆû‰∏éËôöÂÅáÁöÑÊñáÊú¨ÂØπÔºåLLM-Oasis ‰∏∫‰∫ãÂÆûÊÄßËØÑ‰º∞Á≥ªÁªüÁöÑÂü∫ÂáÜÊµãËØïÊèê‰æõ‰∫ÜÈáçË¶ÅÁöÑÊï∞ÊçÆÈõÜ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2412.02632', 'title': 'Scaling Image Tokenizers with Grouped Spherical Quantization', 'url': 'https://huggingface.co/papers/2412.02632', 'abstract': 'Vision tokenizers have gained a lot of attraction due to their scalability and compactness; previous works depend on old-school GAN-based hyperparameters, biased comparisons, and a lack of comprehensive analysis of the scaling behaviours. To tackle those issues, we introduce Grouped Spherical Quantization (GSQ), featuring spherical codebook initialization and lookup regularization to constrain codebook latent to a spherical surface. Our empirical analysis of image tokenizer training strategies demonstrates that GSQ-GAN achieves superior reconstruction quality over state-of-the-art methods with fewer training iterations, providing a solid foundation for scaling studies. Building on this, we systematically examine the scaling behaviours of GSQ, specifically in latent dimensionality, codebook size, and compression ratios, and their impact on model performance. Our findings reveal distinct behaviours at high and low spatial compression levels, underscoring challenges in representing high-dimensional latent spaces. We show that GSQ can restructure high-dimensional latent into compact, low-dimensional spaces, thus enabling efficient scaling with improved quality. As a result, GSQ-GAN achieves a 16x down-sampling with a reconstruction FID (rFID) of 0.50.', 'score': 9, 'issue_id': 933, 'pub_date': '2024-12-03', 'pub_date_card': {'ru': '3 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 3', 'zh': '12Êúà3Êó•'}, 'hash': '60eda94a31cded90', 'authors': ['Jiangtao Wang', 'Zhen Qin', 'Yifan Zhang', 'Vincent Tao Hu', 'Bj√∂rn Ommer', 'Rania Briq', 'Stefan Kesselheim'], 'affiliations': ['CompVis @ LMU Munich', 'J√ºlich Supercomputing Centre', 'TapTap', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2412.02632.jpg', 'data': {'categories': ['#training', '#inference', '#cv', '#data'], 'emoji': 'üîç', 'ru': {'title': 'GSQ: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ —Å—Ñ–µ—Ä–∏—á–µ—Å–∫–æ–π –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–∏', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π - –ì—Ä—É–ø–ø–æ–≤–∞—è –°—Ñ–µ—Ä–∏—á–µ—Å–∫–∞—è –ö–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è (GSQ). GSQ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å—Ñ–µ—Ä–∏—á–µ—Å–∫—É—é –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é –∫–æ–¥–æ–≤–æ–π –∫–Ω–∏–≥–∏ –∏ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é –ø–æ–∏—Å–∫–∞ –¥–ª—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –ª–∞—Ç–µ–Ω—Ç–Ω–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –∫–æ–¥–æ–≤–æ–π –∫–Ω–∏–≥–∏ —Å—Ñ–µ—Ä–∏—á–µ—Å–∫–æ–π –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç—å—é. –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ–≤–µ–ª–∏ —ç–º–ø–∏—Ä–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –æ–±—É—á–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ GSQ-GAN –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã –ø–æ –∫–∞—á–µ—Å—Ç–≤—É —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –ø—Ä–∏ –º–µ–Ω—å—à–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ –∏—Ç–µ—Ä–∞—Ü–∏–π –æ–±—É—á–µ–Ω–∏—è. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç–∏ GSQ –≤—ã—è–≤–∏–ª–æ —Ä–∞–∑–ª–∏—á–Ω–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ –ø—Ä–∏ –≤—ã—Å–æ–∫–∏—Ö –∏ –Ω–∏–∑–∫–∏—Ö —É—Ä–æ–≤–Ω—è—Ö –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —Å–∂–∞—Ç–∏—è, –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –≤—ã—Å–æ–∫–æ—Ä–∞–∑–º–µ—Ä–Ω—ã—Ö –ª–∞—Ç–µ–Ω—Ç–Ω—ã—Ö –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤.'}, 'en': {'title': 'Efficient Image Processing with Grouped Spherical Quantization', 'desc': "This paper introduces a new method called Grouped Spherical Quantization (GSQ) for improving vision tokenizers, which are tools used to process images efficiently. GSQ uses a special technique to initialize and regularize a spherical codebook, helping to keep the data organized on a spherical surface. The authors demonstrate that GSQ-GAN, a model based on this method, can reconstruct images with high quality while requiring fewer training iterations compared to existing methods. Their analysis also reveals how different factors like latent dimensionality and codebook size affect the model's performance, particularly in handling high-dimensional data efficiently."}, 'zh': {'title': 'ÂàÜÁªÑÁêÉÈù¢ÈáèÂåñÔºöÈ´òÊïàÁöÑËßÜËßâÊ†áËÆ∞Âô®Êñ∞ÊñπÊ≥ï', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑËßÜËßâÊ†áËÆ∞Âô®ÊñπÊ≥ïÔºåÁß∞‰∏∫ÂàÜÁªÑÁêÉÈù¢ÈáèÂåñÔºàGSQÔºâÔºåÊó®Âú®Ëß£ÂÜ≥‰º†ÁªüÊñπÊ≥ï‰∏≠ÁöÑ‰∏Ä‰∫õÈóÆÈ¢ò„ÄÇGSQÈÄöËøáÁêÉÈù¢‰ª£Á†ÅÊú¨ÂàùÂßãÂåñÂíåÊü•ÊâæÊ≠£ÂàôÂåñÔºåÈôêÂà∂‰∫Ü‰ª£Á†ÅÊú¨ÊΩúÂú®Á©∫Èó¥Âú®ÁêÉÈù¢‰∏äÁöÑÂàÜÂ∏É„ÄÇÊàë‰ª¨ÁöÑÂÆûËØÅÂàÜÊûêË°®ÊòéÔºåGSQ-GANÂú®ÈáçÂª∫Ë¥®Èáè‰∏ä‰ºò‰∫éÁé∞ÊúâÁöÑÊúÄÂÖàËøõÊñπÊ≥ïÔºåÂπ∂‰∏îËÆ≠ÁªÉËø≠‰ª£Ê¨°Êï∞Êõ¥Â∞ë„ÄÇÁ†îÁ©∂ËøòÁ≥ªÁªüÂú∞ËÄÉÂØü‰∫ÜGSQÂú®ÊΩúÂú®Áª¥Â∫¶„ÄÅ‰ª£Á†ÅÊú¨Â§ßÂ∞èÂíåÂéãÁº©ÊØîÁ≠âÊñπÈù¢ÁöÑÊâ©Â±ïË°å‰∏∫ÔºåÊè≠Á§∫‰∫ÜÈ´òÁª¥ÊΩúÂú®Á©∫Èó¥Ë°®Á§∫ÁöÑÊåëÊàò„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2412.02592', 'title': 'OCR Hinders RAG: Evaluating the Cascading Impact of OCR on Retrieval-Augmented Generation', 'url': 'https://huggingface.co/papers/2412.02592', 'abstract': "Retrieval-augmented Generation (RAG) enhances Large Language Models (LLMs) by integrating external knowledge to reduce hallucinations and incorporate up-to-date information without retraining. As an essential part of RAG, external knowledge bases are commonly built by extracting structured data from unstructured PDF documents using Optical Character Recognition (OCR). However, given the imperfect prediction of OCR and the inherent non-uniform representation of structured data, knowledge bases inevitably contain various OCR noises. In this paper, we introduce OHRBench, the first benchmark for understanding the cascading impact of OCR on RAG systems. OHRBench includes 350 carefully selected unstructured PDF documents from six real-world RAG application domains, along with Q&As derived from multimodal elements in documents, challenging existing OCR solutions used for RAG To better understand OCR's impact on RAG systems, we identify two primary types of OCR noise: Semantic Noise and Formatting Noise and apply perturbation to generate a set of structured data with varying degrees of each OCR noise. Using OHRBench, we first conduct a comprehensive evaluation of current OCR solutions and reveal that none is competent for constructing high-quality knowledge bases for RAG systems. We then systematically evaluate the impact of these two noise types and demonstrate the vulnerability of RAG systems. Furthermore, we discuss the potential of employing Vision-Language Models (VLMs) without OCR in RAG systems. Code: https://github.com/opendatalab/OHR-Bench", 'score': 8, 'issue_id': 937, 'pub_date': '2024-12-03', 'pub_date_card': {'ru': '3 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 3', 'zh': '12Êúà3Êó•'}, 'hash': '91dbac114744b1e9', 'authors': ['Junyuan Zhang', 'Qintong Zhang', 'Bin Wang', 'Linke Ouyang', 'Zichen Wen', 'Ying Li', 'Ka-Ho Chow', 'Conghui He', 'Wentao Zhang'], 'affiliations': ['Beihang University', 'Peking University', 'Shanghai AI Laboratory', 'Shanghai Jiaotong University', 'The University of HongKong'], 'pdf_title_img': 'assets/pdf/title_img/2412.02592.jpg', 'data': {'categories': ['#hallucinations', '#dataset', '#multimodal', '#benchmark', '#rag', '#optimization', '#survey'], 'emoji': 'üîç', 'ru': {'title': 'OHRBench: —Ä–∞—Å–∫—Ä—ã–≤–∞—è –≤–ª–∏—è–Ω–∏–µ OCR –Ω–∞ —Å–∏—Å—Ç–µ–º—ã RAG', 'desc': 'OHRBench - —ç—Ç–æ –ø–µ—Ä–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤–ª–∏—è–Ω–∏—è –æ–ø—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Å–∏–º–≤–æ–ª–æ–≤ (OCR) –Ω–∞ —Å–∏—Å—Ç–µ–º—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ (RAG). –û–Ω –≤–∫–ª—é—á–∞–µ—Ç 350 –Ω–µ—Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö PDF-–¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ —à–µ—Å—Ç–∏ —Ä–µ–∞–ª—å–Ω—ã—Ö –æ–±–ª–∞—Å—Ç–µ–π –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è RAG, –∞ —Ç–∞–∫–∂–µ –≤–æ–ø—Ä–æ—Å—ã –∏ –æ—Ç–≤–µ—Ç—ã, —Å–æ–∑–¥–∞–Ω–Ω—ã–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –≤—ã–¥–µ–ª—è—é—Ç –¥–≤–∞ –æ—Å–Ω–æ–≤–Ω—ã—Ö —Ç–∏–ø–∞ —à—É–º–∞ OCR: —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∏ —Ñ–æ—Ä–º–∞—Ç–Ω—ã–π, –∏ –ø—Ä–∏–º–µ–Ω—è—é—Ç –≤–æ–∑–º—É—â–µ–Ω–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å —Ä–∞–∑–ª–∏—á–Ω–æ–π —Å—Ç–µ–ø–µ–Ω—å—é –∫–∞–∂–¥–æ–≥–æ —Ç–∏–ø–∞ —à—É–º–∞. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç —É—è–∑–≤–∏–º–æ—Å—Ç—å —Å–∏—Å—Ç–µ–º RAG –∫ –æ—à–∏–±–∫–∞–º OCR –∏ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (VLM) –±–µ–∑ OCR –≤ —Å–∏—Å—Ç–µ–º–∞—Ö RAG.'}, 'en': {'title': "Enhancing RAG: Understanding OCR's Impact with OHRBench", 'desc': 'This paper presents OHRBench, a benchmark designed to evaluate the effects of Optical Character Recognition (OCR) errors on Retrieval-Augmented Generation (RAG) systems. It identifies two main types of OCR noise: Semantic Noise, which affects the meaning of the extracted data, and Formatting Noise, which impacts the structure and presentation. The study reveals that current OCR solutions are inadequate for creating high-quality knowledge bases necessary for effective RAG applications. Additionally, it explores the potential of using Vision-Language Models (VLMs) as an alternative to traditional OCR methods in RAG systems.'}, 'zh': {'title': 'Êè≠Á§∫OCRÂØπRAGÁ≥ªÁªüÁöÑÂΩ±Âìç', 'desc': 'Êú¨ËÆ∫Êñá‰ªãÁªç‰∫ÜOHRBenchÔºåËøôÊòØÁ¨¨‰∏Ä‰∏™Áî®‰∫éÁêÜËß£ÂÖâÂ≠¶Â≠óÁ¨¶ËØÜÂà´ÔºàOCRÔºâÂØπÊ£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàêÔºàRAGÔºâÁ≥ªÁªüÂΩ±ÂìçÁöÑÂü∫ÂáÜ„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåOCRÂú®Â§ÑÁêÜÈùûÁªìÊûÑÂåñPDFÊñáÊ°£Êó∂‰ºöÂºïÂÖ•ËØ≠‰πâÂô™Â£∞ÂíåÊ†ºÂºèÂô™Â£∞ÔºåÂØºËá¥Áü•ËØÜÂ∫ìË¥®Èáè‰∏ãÈôç„ÄÇÈÄöËøáÂØπ350‰∏™ÁúüÂÆû‰∏ñÁïåÂ∫îÁî®È¢ÜÂüüÁöÑÊñáÊ°£ËøõË°åËØÑ‰º∞ÔºåÁªìÊûúÊòæÁ§∫Áé∞ÊúâÁöÑOCRËß£ÂÜ≥ÊñπÊ°àÊó†Ê≥ïÊúâÊïàÊûÑÂª∫È´òË¥®ÈáèÁöÑÁü•ËØÜÂ∫ì„ÄÇÊúÄÂêéÔºåËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÂú®RAGÁ≥ªÁªü‰∏≠‰ΩøÁî®ËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMsÔºâËÄå‰∏ç‰æùËµñOCRÁöÑÊΩúÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2412.01292', 'title': 'LSceneLLM: Enhancing Large 3D Scene Understanding Using Adaptive Visual Preferences', 'url': 'https://huggingface.co/papers/2412.01292', 'abstract': "Research on 3D Vision-Language Models (3D-VLMs) is gaining increasing attention, which is crucial for developing embodied AI within 3D scenes, such as visual navigation and embodied question answering. Due to the high density of visual features, especially in large 3D scenes, accurately locating task-relevant visual information is challenging. Existing works attempt to segment all objects and consider their features as scene representations. However, these task-agnostic object features include much redundant information and missing details for the task-relevant area. To tackle these problems, we propose LSceneLLM, an adaptive framework that automatically identifies task-relevant areas by leveraging LLM's visual preference for different tasks, followed by a plug-and-play scene magnifier module to capture fine-grained details in focused areas. Specifically, a dense token selector examines the attention map of LLM to identify visual preferences for the instruction input. It then magnifies fine-grained details of the focusing area. An adaptive self-attention module is leveraged to fuse the coarse-grained and selected fine-grained visual information. To comprehensively evaluate the large scene understanding ability of 3D-VLMs, we further introduce a cross-room understanding benchmark, XR-Scene, which contains a series of large scene understanding tasks including XR-QA, XR-EmbodiedPlanning, and XR-SceneCaption. Experiments show that our method surpasses existing methods on both large scene understanding and existing scene understanding benchmarks. Plunging our scene magnifier module into the existing 3D-VLMs also brings significant improvement.", 'score': 7, 'issue_id': 933, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 2', 'zh': '12Êúà2Êó•'}, 'hash': 'e8f8ddd05e13e9ef', 'authors': ['Hongyan Zhi', 'Peihao Chen', 'Junyan Li', 'Shuailei Ma', 'Xinyu Sun', 'Tianhang Xiang', 'Yinjie Lei', 'Mingkui Tan', 'Chuang Gan'], 'affiliations': ['MIT-IBM Watson AI Lab', 'Northeastern University', 'Pazhou Laboratory', 'Sichuan University', 'South China University of Technology', 'Tencent Robotics X', 'UMass Amherst'], 'pdf_title_img': 'assets/pdf/title_img/2412.01292.jpg', 'data': {'categories': ['#architecture', '#benchmark', '#3d', '#multimodal'], 'emoji': 'üñºÔ∏è', 'ru': {'title': '–£–º–Ω–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ 3D-—Å—Ü–µ–Ω —Å LSceneLLM', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ 3D Vision-Language Models (3D-VLMs) –≤–∞–∂–Ω–æ –¥–ª—è —Ä–∞–∑–≤–∏—Ç–∏—è –≤–æ–ø–ª–æ—â–µ–Ω–Ω–æ–≥–æ AI –≤ 3D-—Å—Ü–µ–Ω–∞—Ö, —Ç–∞–∫–∏—Ö –∫–∞–∫ –≤–∏–∑—É–∞–ª—å–Ω–∞—è –Ω–∞–≤–∏–≥–∞—Ü–∏—è –∏ –æ—Ç–≤–µ—Ç—ã –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã. –ò–∑-–∑–∞ –≤—ã—Å–æ–∫–æ–π –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤ –±–æ–ª—å—à–∏—Ö 3D-—Å—Ü–µ–Ω–∞—Ö —Å–ª–æ–∂–Ω–æ —Ç–æ—á–Ω–æ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –≤–∞–∂–Ω—É—é –¥–ª—è –∑–∞–¥–∞—á–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é. –ü—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –∞–¥–∞–ø—Ç–∏–≤–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ LSceneLLM, –∫–æ—Ç–æ—Ä–∞—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –≤–∞–∂–Ω—ã–µ –æ–±–ª–∞—Å—Ç–∏, –∏—Å–ø–æ–ª—å–∑—É—è –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è LLM –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∑–∞–¥–∞—á. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –Ω–∞—à –º–µ—Ç–æ–¥ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ –±–æ–ª—å—à–∏—Ö —Å—Ü–µ–Ω –∏ —É–ª—É—á—à–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–∏ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ 3D-VLMs.'}, 'en': {'title': 'Enhancing 3D Scene Understanding with LSceneLLM', 'desc': 'This paper introduces LSceneLLM, a novel framework designed to enhance 3D Vision-Language Models (3D-VLMs) for better understanding of large 3D scenes. It addresses the challenge of identifying task-relevant visual information amidst the dense features present in these scenes. By utilizing a dense token selector and an adaptive self-attention module, the framework effectively magnifies important details while reducing redundant information. The authors also present a new benchmark, XR-Scene, to evaluate the performance of 3D-VLMs on various large scene understanding tasks, demonstrating that their approach significantly outperforms existing methods.'}, 'zh': {'title': 'Ëá™ÈÄÇÂ∫î3DËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºåÊèêÂçáÂú∫ÊôØÁêÜËß£ËÉΩÂäõ', 'desc': '3DËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºà3D-VLMsÔºâÁöÑÁ†îÁ©∂Ë∂äÊù•Ë∂äÂèóÂà∞ÂÖ≥Ê≥®ÔºåËøôÂØπÂú®3DÂú∫ÊôØ‰∏≠ÂèëÂ±ïÂÖ∑Ë∫´‰∫∫Â∑•Êô∫ËÉΩËá≥ÂÖ≥ÈáçË¶ÅÔºåÂ¶ÇËßÜËßâÂØºËà™ÂíåÂÖ∑Ë∫´ÈóÆÁ≠î„ÄÇÁî±‰∫é3DÂú∫ÊôØ‰∏≠ËßÜËßâÁâπÂæÅÁöÑÈ´òÂØÜÂ∫¶ÔºåÂáÜÁ°ÆÂÆö‰Ωç‰∏é‰ªªÂä°Áõ∏ÂÖ≥ÁöÑËßÜËßâ‰ø°ÊÅØÂèòÂæóÂÖ∑ÊúâÊåëÊàòÊÄß„ÄÇÁé∞ÊúâÊñπÊ≥ïÂ∞ùËØïÂØπÊâÄÊúâÂØπË±°ËøõË°åÂàÜÂâ≤ÔºåÂπ∂Â∞ÜÂÖ∂ÁâπÂæÅËßÜ‰∏∫Âú∫ÊôØË°®Á§∫Ôºå‰ΩÜËøô‰∫õ‰∏é‰ªªÂä°Êó†ÂÖ≥ÁöÑÂØπË±°ÁâπÂæÅÂåÖÂê´Â§ßÈáèÂÜó‰Ωô‰ø°ÊÅØÂíåÁº∫Â§±ÁöÑÁªÜËäÇ„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜLSceneLLMÔºå‰∏Ä‰∏™Ëá™ÈÄÇÂ∫îÊ°ÜÊû∂ÔºåÈÄöËøáÂà©Áî®Â§ßËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÂØπ‰∏çÂêå‰ªªÂä°ÁöÑËßÜËßâÂÅèÂ•ΩÔºåËá™Âä®ËØÜÂà´‰∏é‰ªªÂä°Áõ∏ÂÖ≥ÁöÑÂå∫ÂüüÔºåÂπ∂ÈÄöËøáÂèØÊèíÊãîÁöÑÂú∫ÊôØÊîæÂ§ßÊ®°ÂùóÊçïÊçâËÅöÁÑ¶Âå∫ÂüüÁöÑÁªÜÁ≤íÂ∫¶ÁªÜËäÇ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2412.02700', 'title': 'Motion Prompting: Controlling Video Generation with Motion Trajectories', 'url': 'https://huggingface.co/papers/2412.02700', 'abstract': 'Motion control is crucial for generating expressive and compelling video content; however, most existing video generation models rely mainly on text prompts for control, which struggle to capture the nuances of dynamic actions and temporal compositions. To this end, we train a video generation model conditioned on spatio-temporally sparse or dense motion trajectories. In contrast to prior motion conditioning work, this flexible representation can encode any number of trajectories, object-specific or global scene motion, and temporally sparse motion; due to its flexibility we refer to this conditioning as motion prompts. While users may directly specify sparse trajectories, we also show how to translate high-level user requests into detailed, semi-dense motion prompts, a process we term motion prompt expansion. We demonstrate the versatility of our approach through various applications, including camera and object motion control, "interacting" with an image, motion transfer, and image editing. Our results showcase emergent behaviors, such as realistic physics, suggesting the potential of motion prompts for probing video models and interacting with future generative world models. Finally, we evaluate quantitatively, conduct a human study, and demonstrate strong performance. Video results are available on our webpage: https://motion-prompting.github.io/', 'score': 5, 'issue_id': 949, 'pub_date': '2024-12-03', 'pub_date_card': {'ru': '3 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 3', 'zh': '12Êúà3Êó•'}, 'hash': '6adffbc375f9f4f5', 'authors': ['Daniel Geng', 'Charles Herrmann', 'Junhwa Hur', 'Forrester Cole', 'Serena Zhang', 'Tobias Pfaff', 'Tatiana Lopez-Guevara', 'Carl Doersch', 'Yusuf Aytar', 'Michael Rubinstein', 'Chen Sun', 'Oliver Wang', 'Andrew Owens', 'Deqing Sun'], 'affiliations': ['Brown University', 'Google DeepMind', 'University of Michigan'], 'pdf_title_img': 'assets/pdf/title_img/2412.02700.jpg', 'data': {'categories': ['#video', '#optimization', '#multimodal', '#games'], 'emoji': 'üé•', 'ru': {'title': '–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–≤–∏–∂–µ–Ω–∏–µ–º –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç –Ω–æ–≤—ã–µ –≥–æ—Ä–∏–∑–æ–Ω—Ç—ã –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ', 'desc': "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–≤–∏–∂–µ–Ω–∏–µ–º. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è –º–æ–∂–µ—Ç –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –≤–∏–¥–µ–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã—Ö –∏–ª–∏ –ø–ª–æ—Ç–Ω—ã—Ö —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π –¥–≤–∏–∂–µ–Ω–∏—è, –Ω–∞–∑—ã–≤–∞–µ–º—ã—Ö 'motion prompts'. –≠—Ç–∞ –≥–∏–±–∫–∞—è —Å–∏—Å—Ç–µ–º–∞ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä–æ–≤–∞—Ç—å –¥–≤–∏–∂–µ–Ω–∏–µ –∫–∞–º–µ—Ä—ã, –æ–±—ä–µ–∫—Ç–æ–≤ –∏ –æ–±—â—É—é –¥–∏–Ω–∞–º–∏–∫—É —Å—Ü–µ–Ω—ã. –ú–æ–¥–µ–ª—å –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –≤–ø–µ—á–∞—Ç–ª—è—é—â–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è—Ö, –≤–∫–ª—é—á–∞—è –ø–µ—Ä–µ–Ω–æ—Å –¥–≤–∏–∂–µ–Ω–∏—è –∏ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∞ —Ç–∞–∫–∂–µ –ø—Ä–æ—è–≤–ª—è–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∫ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ–π —Ñ–∏–∑–∏–∫–µ."}, 'en': {'title': 'Empowering Video Generation with Flexible Motion Prompts', 'desc': 'This paper presents a novel approach to video generation by using motion prompts, which are flexible representations of motion trajectories. Unlike traditional models that rely solely on text prompts, this method allows for the encoding of various types of motion, including object-specific and global scene movements. The authors introduce a technique called motion prompt expansion, enabling users to convert high-level requests into detailed motion trajectories. The results indicate that this approach not only enhances video generation but also allows for realistic interactions and behaviors within the generated content.'}, 'zh': {'title': 'ËøêÂä®ÊèêÁ§∫ÔºöËßÜÈ¢ëÁîüÊàêÁöÑÊñ∞ÊñπÂºè', 'desc': 'Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑËßÜÈ¢ëÁîüÊàêÊ®°ÂûãÔºåÂà©Áî®ËøêÂä®ËΩ®Ëøπ‰Ωú‰∏∫ÊéßÂà∂ÊâãÊÆµÔºåÂÖãÊúç‰∫Ü‰º†ÁªüÊñáÊú¨ÊèêÁ§∫Âú®Âä®ÊÄÅÂä®‰ΩúÂíåÊó∂Èó¥ÁªÑÂêà‰∏äÁöÑÂ±ÄÈôêÊÄß„ÄÇÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏ÄÁßçÁÅµÊ¥ªÁöÑËøêÂä®ÊèêÁ§∫Ë°®Á§∫ÔºåÂèØ‰ª•ÁºñÁ†Å‰ªªÊÑèÊï∞ÈáèÁöÑËΩ®ËøπÔºåÂåÖÊã¨ÁâπÂÆöÁâ©‰ΩìÊàñÂÖ®Â±ÄÂú∫ÊôØÁöÑËøêÂä®„ÄÇÁî®Êà∑ÂèØ‰ª•Áõ¥Êé•ÊåáÂÆöÁ®ÄÁñèËΩ®ËøπÔºåÊàë‰ª¨ËøòÂ±ïÁ§∫‰∫ÜÂ¶Ç‰ΩïÂ∞ÜÈ´òÂ±ÇÊ¨°ÁöÑÁî®Êà∑ËØ∑Ê±ÇËΩ¨Âåñ‰∏∫ËØ¶ÁªÜÁöÑÂçäÁ®ÄÁñèËøêÂä®ÊèêÁ§∫„ÄÇÈÄöËøáÂ§öÁßçÂ∫îÁî®Â±ïÁ§∫‰∫ÜÊàë‰ª¨ÊñπÊ≥ïÁöÑÂ§öÊ†∑ÊÄßÔºåÂåÖÊã¨Áõ∏Êú∫ÂíåÁâ©‰ΩìËøêÂä®ÊéßÂà∂„ÄÅ‰∏éÂõæÂÉèÁöÑ‰∫§‰∫í„ÄÅËøêÂä®ËΩ¨ÁßªÂíåÂõæÂÉèÁºñËæë„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2411.19542', 'title': 'A dynamic parallel method for performance optimization on hybrid CPUs', 'url': 'https://huggingface.co/papers/2411.19542', 'abstract': 'The AIPC concept is gaining popularity, and more and more hybrid CPUs will be running AI models on client devices. However, the current AI inference framework overlooks the imbalanced hardware capability of hybrid CPUs, leading to low inference performance. To address this issue, we have introduced a dynamic parallel method for hybrid CPUs, which significantly increases LLM inference performance by balancing the workload for each core of a hybrid CPU before the parallel work starts. This method has enabled Neural Speed to achieve more than 90% (on average) of memory bandwidth on two hybrid Intel CPUs.', 'score': 5, 'issue_id': 936, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 –Ω–æ—è–±—Ä—è', 'en': 'November 29', 'zh': '11Êúà29Êó•'}, 'hash': '27226211eddf71d4', 'authors': ['Luo Yu', 'Liu Yucheng', 'Shen Haihao'], 'affiliations': ['Intel Corporation'], 'pdf_title_img': 'assets/pdf/title_img/2411.19542.jpg', 'data': {'categories': ['#inference', '#architecture', '#optimization'], 'emoji': 'üöÄ', 'ru': {'title': '–£—Å–∫–æ—Ä–µ–Ω–∏–µ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –ò–ò –Ω–∞ –≥–∏–±—Ä–∏–¥–Ω—ã—Ö CPU: –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –º–µ—Ç–æ–¥ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –¥–ª—è –≥–∏–±—Ä–∏–¥–Ω—ã—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–æ–≤, –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É—é—â–∏–π –∏–Ω—Ñ–µ—Ä–µ–Ω—Å –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –ê–≤—Ç–æ—Ä—ã –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∏ –¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –ò–ò –Ω–µ —É—á–∏—Ç—ã–≤–∞—é—Ç –Ω–µ—Ä–∞–≤–Ω–æ–º–µ—Ä–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —è–¥–µ—Ä –≤ –≥–∏–±—Ä–∏–¥–Ω—ã—Ö CPU, —á—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –Ω–∏–∑–∫–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –±–∞–ª–∞–Ω—Å–∏—Ä—É–µ—Ç –Ω–∞–≥—Ä—É–∑–∫—É –º–µ–∂–¥—É —è–¥—Ä–∞–º–∏ –ø–µ—Ä–µ–¥ –Ω–∞—á–∞–ª–æ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π —Ä–∞–±–æ—Ç—ã, —á—Ç–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø–æ–≤—ã—à–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ LLM. –í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ, –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç Neural Speed –¥–æ—Å—Ç–∏–≥ –±–æ–ª–µ–µ 90% –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø—Ä–æ–ø—É—Å–∫–Ω–æ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –ø–∞–º—è—Ç–∏ –Ω–∞ –¥–≤—É—Ö –≥–∏–±—Ä–∏–¥–Ω—ã—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞—Ö Intel.'}, 'en': {'title': 'Boosting AI Inference with Dynamic Workload Balancing on Hybrid CPUs', 'desc': 'The paper discusses the growing trend of using hybrid CPUs for running AI models on client devices. It highlights a problem where existing AI inference frameworks do not effectively utilize the varying hardware capabilities of these hybrid CPUs, resulting in suboptimal performance. To solve this, the authors propose a dynamic parallel method that balances the workload across CPU cores before starting parallel processing. This approach has led to significant improvements in inference performance, achieving over 90% memory bandwidth utilization on hybrid Intel CPUs.'}, 'zh': {'title': 'Âä®ÊÄÅÂπ≥Ë°°ÔºåÊèêÂçáAIÊé®ÁêÜÊÄßËÉΩÔºÅ', 'desc': 'AIPCÊ¶ÇÂøµË∂äÊù•Ë∂äÂèóÊ¨¢ËøéÔºåË∂äÊù•Ë∂äÂ§öÁöÑÊ∑∑ÂêàCPUÂ∞ÜÂú®ÂÆ¢Êà∑Á´ØËÆæÂ§á‰∏äËøêË°åAIÊ®°Âûã„ÄÇÁÑ∂ËÄåÔºåÁõÆÂâçÁöÑAIÊé®ÁêÜÊ°ÜÊû∂ÂøΩËßÜ‰∫ÜÊ∑∑ÂêàCPUÁöÑ‰∏çÂπ≥Ë°°Á°¨‰ª∂ËÉΩÂäõÔºåÂØºËá¥Êé®ÁêÜÊÄßËÉΩ‰Ωé‰∏ã„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂä®ÊÄÅÂπ∂Ë°åÊñπÊ≥ïÔºåÊòæËëóÊèêÈ´ò‰∫ÜÊ∑∑ÂêàCPUÁöÑLLMÊé®ÁêÜÊÄßËÉΩÔºåÈÄöËøáÂú®Âπ∂Ë°åÂ∑•‰ΩúÂºÄÂßã‰πãÂâçÂπ≥Ë°°ÊØè‰∏™Ê†∏ÂøÉÁöÑÂ∑•‰ΩúË¥üËΩΩ„ÄÇËØ•ÊñπÊ≥ï‰ΩøNeural SpeedÂú®‰∏§Ê¨æÊ∑∑ÂêàIntel CPU‰∏äÂÆûÁé∞‰∫ÜË∂ÖËøá90%ÁöÑÂÜÖÂ≠òÂ∏¶ÂÆΩ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2411.19067', 'title': 'MaskRIS: Semantic Distortion-aware Data Augmentation for Referring Image Segmentation', 'url': 'https://huggingface.co/papers/2411.19067', 'abstract': "Referring Image Segmentation (RIS) is an advanced vision-language task that involves identifying and segmenting objects within an image as described by free-form text descriptions. While previous studies focused on aligning visual and language features, exploring training techniques, such as data augmentation, remains underexplored. In this work, we explore effective data augmentation for RIS and propose a novel training framework called Masked Referring Image Segmentation (MaskRIS). We observe that the conventional image augmentations fall short of RIS, leading to performance degradation, while simple random masking significantly enhances the performance of RIS. MaskRIS uses both image and text masking, followed by Distortion-aware Contextual Learning (DCL) to fully exploit the benefits of the masking strategy. This approach can improve the model's robustness to occlusions, incomplete information, and various linguistic complexities, resulting in a significant performance improvement. Experiments demonstrate that MaskRIS can easily be applied to various RIS models, outperforming existing methods in both fully supervised and weakly supervised settings. Finally, MaskRIS achieves new state-of-the-art performance on RefCOCO, RefCOCO+, and RefCOCOg datasets. Code is available at https://github.com/naver-ai/maskris.", 'score': 5, 'issue_id': 934, 'pub_date': '2024-11-28', 'pub_date_card': {'ru': '28 –Ω–æ—è–±—Ä—è', 'en': 'November 28', 'zh': '11Êúà28Êó•'}, 'hash': '74d4a17af3574a5d', 'authors': ['Minhyun Lee', 'Seungho Lee', 'Song Park', 'Dongyoon Han', 'Byeongho Heo', 'Hyunjung Shim'], 'affiliations': ['KAIST AI', 'NAVER AI Lab', 'Yonsei University'], 'pdf_title_img': 'assets/pdf/title_img/2411.19067.jpg', 'data': {'categories': ['#optimization', '#cv', '#survey', '#dataset', '#training'], 'emoji': 'üé≠', 'ru': {'title': '–ú–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ –æ–ø–∏—Å–∞–Ω–∏—é', 'desc': '–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –∑–∞–¥–∞—á–µ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É –æ–ø–∏—Å–∞–Ω–∏—é (RIS). –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º MaskRIS, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ —Ç–µ–∫—Å—Ç–∞. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ —É–ª—É—á—à–∞–µ—Ç —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –∫ –æ–∫–∫–ª—é–∑–∏—è–º, –Ω–µ–ø–æ–ª–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏ —è–∑—ã–∫–æ–≤—ã–º —Å–ª–æ–∂–Ω–æ—Å—Ç—è–º. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ MaskRIS –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã –∫–∞–∫ –ø—Ä–∏ –ø–æ–ª–Ω–æ—Å—Ç—å—é –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–º, —Ç–∞–∫ –∏ –ø—Ä–∏ —Å–ª–∞–±–æ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–º –æ–±—É—á–µ–Ω–∏–∏.'}, 'en': {'title': 'Enhancing Referring Image Segmentation with Masking Techniques', 'desc': 'Referring Image Segmentation (RIS) is a task that combines visual and language understanding to identify and segment objects in images based on text descriptions. This paper introduces a new training framework called Masked Referring Image Segmentation (MaskRIS), which focuses on effective data augmentation techniques for RIS. The authors found that traditional image augmentations were inadequate, while their method of random masking significantly improved performance. MaskRIS enhances model robustness against occlusions and linguistic variations, achieving state-of-the-art results on several benchmark datasets.'}, 'zh': {'title': 'Masked Referring Image SegmentationÔºöÊèêÂçáÂõæÂÉèÂàÜÂâ≤ÊÄßËÉΩÁöÑÊñ∞ÊñπÊ≥ï', 'desc': 'ÂºïÁî®ÂõæÂÉèÂàÜÂâ≤ÔºàRISÔºâÊòØ‰∏ÄÁßçÂÖàËøõÁöÑËßÜËßâ-ËØ≠Ë®Ä‰ªªÂä°ÔºåÊó®Âú®Ê†πÊçÆËá™Áî±ÂΩ¢ÂºèÁöÑÊñáÊú¨ÊèèËø∞ËØÜÂà´ÂíåÂàÜÂâ≤ÂõæÂÉè‰∏≠ÁöÑÂØπË±°„ÄÇÊú¨ÊñáÊé¢ËÆ®‰∫ÜÊúâÊïàÁöÑÊï∞ÊçÆÂ¢ûÂº∫ÊäÄÊúØÔºåÂπ∂ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑËÆ≠ÁªÉÊ°ÜÊû∂ÔºåÁß∞‰∏∫Masked Referring Image SegmentationÔºàMaskRISÔºâ„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºå‰º†ÁªüÁöÑÂõæÂÉèÂ¢ûÂº∫ÊñπÊ≥ïÂú®RIS‰∏≠ÊïàÊûú‰∏ç‰Ω≥ÔºåËÄåÁÆÄÂçïÁöÑÈöèÊú∫ÈÅÆÁΩ©ÊòæËëóÊèêÂçá‰∫ÜRISÁöÑÊÄßËÉΩ„ÄÇMaskRISÁªìÂêà‰∫ÜÂõæÂÉèÂíåÊñáÊú¨ÈÅÆÁΩ©ÔºåÂπ∂ÈááÁî®‰∫ÜÂ§±ÁúüÊÑüÁü•‰∏ä‰∏ãÊñáÂ≠¶‰π†ÔºàDCLÔºâÔºå‰ªéËÄåÊèêÈ´ò‰∫ÜÊ®°ÂûãÂØπÈÅÆÊå°„ÄÅ‰∏çÂÆåÊï¥‰ø°ÊÅØÂíåËØ≠Ë®ÄÂ§çÊùÇÊÄßÁöÑÈ≤ÅÊ£íÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2412.00239', 'title': 'Generating a Low-code Complete Workflow via Task Decomposition and RAG', 'url': 'https://huggingface.co/papers/2412.00239', 'abstract': 'AI technologies are moving rapidly from research to production. With the popularity of Foundation Models (FMs) that generate text, images, and video, AI-based systems are increasing their complexity. Compared to traditional AI-based software, systems employing FMs, or GenAI-based systems, are more difficult to design due to their scale and versatility. This makes it necessary to document best practices, known as design patterns in software engineering, that can be used across GenAI applications. Our first contribution is to formalize two techniques, Task Decomposition and Retrieval-Augmented Generation (RAG), as design patterns for GenAI-based systems. We discuss their trade-offs in terms of software quality attributes and comment on alternative approaches. We recommend to AI practitioners to consider these techniques not only from a scientific perspective but also from the standpoint of desired engineering properties such as flexibility, maintainability, safety, and security. As a second contribution, we describe our industry experience applying Task Decomposition and RAG to build a complex real-world GenAI application for enterprise users: Workflow Generation. The task of generating workflows entails generating a specific plan using data from the system environment, taking as input a user requirement. As these two patterns affect the entire AI development cycle, we explain how they impacted the dataset creation, model training, model evaluation, and deployment phases.', 'score': 2, 'issue_id': 948, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 –Ω–æ—è–±—Ä—è', 'en': 'November 29', 'zh': '11Êúà29Êó•'}, 'hash': '44bf29d0fbeafbc3', 'authors': ['Orlando Marquez Ayala', 'Patrice B√©chard'], 'affiliations': ['ServiceNow'], 'pdf_title_img': 'assets/pdf/title_img/2412.00239.jpg', 'data': {'categories': ['#optimization', '#training', '#rag', '#dataset', '#security', '#benchmark'], 'emoji': 'üß©', 'ru': {'title': '–ü–∞—Ç—Ç–µ—Ä–Ω—ã –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è —Å–∏—Å—Ç–µ–º –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –ò–ò: –æ—Ç —Ç–µ–æ—Ä–∏–∏ –∫ –ø—Ä–∞–∫—Ç–∏–∫–µ', 'desc': '–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ —Ñ–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–≤—É—Ö —Ç–µ—Ö–Ω–∏–∫ - –¥–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏–∏ –∑–∞–¥–∞—á –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å –¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ–º –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ (RAG) - –∫–∞–∫ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è —Å–∏—Å—Ç–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –ò–ò. –ê–≤—Ç–æ—Ä—ã –æ–±—Å—É–∂–¥–∞—é—Ç –∫–æ–º–ø—Ä–æ–º–∏—Å—Å—ã —ç—Ç–∏—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ —Å —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è –∞—Ç—Ä–∏–±—É—Ç–æ–≤ –∫–∞—á–µ—Å—Ç–≤–∞ –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–≥–æ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –∏ —Ä–µ–∫–æ–º–µ–Ω–¥—É—é—Ç —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—Ç—å –∏—Ö –Ω–µ —Ç–æ–ª—å–∫–æ —Å –Ω–∞—É—á–Ω–æ–π —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è, –Ω–æ –∏ —Å –ø–æ–∑–∏—Ü–∏–∏ –∂–µ–ª–∞–µ–º—ã—Ö –∏–Ω–∂–µ–Ω–µ—Ä–Ω—ã—Ö —Å–≤–æ–π—Å—Ç–≤. –í —Å—Ç–∞—Ç—å–µ —Ç–∞–∫–∂–µ –æ–ø–∏—Å—ã–≤–∞–µ—Ç—Å—è –æ–ø—ã—Ç –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è —ç—Ç–∏—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Å–ª–æ–∂–Ω–æ–≥–æ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –ò–ò –¥–ª—è –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π - –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–∞–±–æ—á–∏—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤. –ê–≤—Ç–æ—Ä—ã –æ–±—ä—è—Å–Ω—è—é—Ç, –∫–∞–∫ —ç—Ç–∏ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –ø–æ–≤–ª–∏—è–ª–∏ –Ω–∞ –≤–µ—Å—å —Ü–∏–∫–ª —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –ò–ò, –≤–∫–ª—é—á–∞—è —Å–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤, –æ–±—É—á–µ–Ω–∏–µ –∏ –æ—Ü–µ–Ω–∫—É –º–æ–¥–µ–ª–µ–π, –∞ —Ç–∞–∫–∂–µ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ.'}, 'en': {'title': 'Streamlining GenAI Development with Design Patterns', 'desc': 'This paper discusses the challenges of designing AI systems that use Foundation Models (FMs) due to their complexity and versatility. It introduces two design patterns, Task Decomposition and Retrieval-Augmented Generation (RAG), which can help streamline the development of GenAI applications. The authors analyze the trade-offs of these techniques in relation to software quality attributes like flexibility and security. Additionally, they share their practical experience in applying these patterns to create a Workflow Generation application for enterprise users, highlighting their influence on various stages of the AI development cycle.'}, 'zh': {'title': 'ËÆæËÆ°Ê®°ÂºèÂä©ÂäõÁîüÊàêAIÁ≥ªÁªüÁöÑÂºÄÂèë', 'desc': 'ÈöèÁùÄÂü∫Á°ÄÊ®°ÂûãÔºàFMsÔºâÂú®ÊñáÊú¨„ÄÅÂõæÂÉèÂíåËßÜÈ¢ëÁîüÊàê‰∏≠ÁöÑÊôÆÂèäÔºåAIÁ≥ªÁªüÁöÑÂ§çÊùÇÊÄß‰∏çÊñ≠Â¢ûÂä†„ÄÇ‰∏é‰º†ÁªüÁöÑAIËΩØ‰ª∂Áõ∏ÊØîÔºåÂü∫‰∫éÁîüÊàêAIÔºàGenAIÔºâÁöÑÁ≥ªÁªüÂú®ËÆæËÆ°‰∏äÊõ¥ÂÖ∑ÊåëÊàòÊÄßÔºåÂõ†Ê≠§ÈúÄË¶ÅËÆ∞ÂΩïÊúÄ‰Ω≥ÂÆûË∑µÔºåÁß∞‰∏∫ËÆæËÆ°Ê®°Âºè„ÄÇÊú¨ÊñáÈ¶ñÊ¨°Â∞Ü‰ªªÂä°ÂàÜËß£ÂíåÊ£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàêÔºàRAGÔºâÂΩ¢ÂºèÂåñ‰∏∫GenAIÁ≥ªÁªüÁöÑËÆæËÆ°Ê®°ÂºèÔºåÂπ∂ËÆ®ËÆ∫ÂÆÉ‰ª¨Âú®ËΩØ‰ª∂Ë¥®ÈáèÂ±ûÊÄßÊñπÈù¢ÁöÑÊùÉË°°„ÄÇÊàë‰ª¨ËøòÂàÜ‰∫´‰∫ÜÂú®‰ºÅ‰∏öÁî®Êà∑ÁöÑÂ§çÊùÇGenAIÂ∫îÁî®‚Äî‚ÄîÂ∑•‰ΩúÊµÅÁîüÊàê‰∏≠ÁöÑÂÆûÈôÖÁªèÈ™åÔºåËØ¥ÊòéËøô‰∫õÊ®°ÂºèÂ¶Ç‰ΩïÂΩ±ÂìçÊï¥‰∏™AIÂºÄÂèëÂë®Êúü„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2412.01558', 'title': 'VideoLights: Feature Refinement and Cross-Task Alignment Transformer for Joint Video Highlight Detection and Moment Retrieval', 'url': 'https://huggingface.co/papers/2412.01558', 'abstract': 'Video Highlight Detection and Moment Retrieval (HD/MR) are essential in video analysis. Recent joint prediction transformer models often overlook their cross-task dynamics and video-text alignment and refinement. Moreover, most models typically use limited, uni-directional attention mechanisms, resulting in weakly integrated representations and suboptimal performance in capturing the interdependence between video and text modalities. Although large-language and vision-language models (LLM/LVLMs) have gained prominence across various domains, their application in this field remains relatively underexplored. Here we propose VideoLights, a novel HD/MR framework addressing these limitations through (i) Convolutional Projection and Feature Refinement modules with an alignment loss for better video-text feature alignment, (ii) Bi-Directional Cross-Modal Fusion network for strongly coupled query-aware clip representations, and (iii) Uni-directional joint-task feedback mechanism enhancing both tasks through correlation. In addition, (iv) we introduce hard positive/negative losses for adaptive error penalization and improved learning, and (v) leverage LVLMs like BLIP-2 for enhanced multimodal feature integration and intelligent pretraining using synthetic data generated from LVLMs. Comprehensive experiments on QVHighlights, TVSum, and Charades-STA benchmarks demonstrate state-of-the-art performance. Codes and models are available at https://github.com/dpaul06/VideoLights .', 'score': 2, 'issue_id': 935, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 2', 'zh': '12Êúà2Êó•'}, 'hash': '12235c4ebf26fe4a', 'authors': ['Dhiman Paul', 'Md Rizwan Parvez', 'Nabeel Mohammed', 'Shafin Rahman'], 'affiliations': ['Department of Electrical and Computer Engineering, North South University, Dhaka, Bangladesh', 'Qatar Computing Research Institute (QCRI), Qatar'], 'pdf_title_img': 'assets/pdf/title_img/2412.01558.jpg', 'data': {'categories': ['#video', '#games', '#synthetic', '#architecture', '#benchmark', '#multimodal'], 'emoji': 'üé•', 'ru': {'title': 'VideoLights: –ù–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∞–Ω–∞–ª–∏–∑—É –∫–ª—é—á–µ–≤—ã—Ö –º–æ–º–µ–Ω—Ç–æ–≤ –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤—ã—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç VideoLights - –Ω–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∫–ª—é—á–µ–≤—ã—Ö –º–æ–º–µ–Ω—Ç–æ–≤ –≤–∏–¥–µ–æ –∏ –ø–æ–∏—Å–∫–∞ –ø–æ –Ω–∏–º. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —É–ª—É—á—à–µ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –≤–∏–¥–µ–æ –∏ —Ç–µ–∫—Å—Ç–∞, –¥–≤—É–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–µ —Å–ª–∏—è–Ω–∏–µ –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π –∏ –º–µ—Ö–∞–Ω–∏–∑–º –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ –º–µ–∂–¥—É –∑–∞–¥–∞—á–∞–º–∏. –û–Ω–∏ —Ç–∞–∫–∂–µ –≤–≤–æ–¥—è—Ç –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç –±–æ–ª—å—à–∏–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ VideoLights –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö.'}, 'en': {'title': 'Enhancing Video-Text Integration with VideoLights', 'desc': 'This paper presents VideoLights, a new framework for Video Highlight Detection and Moment Retrieval that improves the integration of video and text data. It introduces several innovative components, including Convolutional Projection and Feature Refinement modules to enhance video-text alignment, and a Bi-Directional Cross-Modal Fusion network for better representation of clips. The framework also employs a Uni-directional joint-task feedback mechanism to strengthen the relationship between the two tasks and introduces hard positive/negative losses for more effective learning. The results show that VideoLights achieves state-of-the-art performance on multiple benchmarks, demonstrating its effectiveness in video analysis.'}, 'zh': {'title': 'VideoLightsÔºöÊèêÂçáËßÜÈ¢ë‰∏éÊñáÊú¨ÂàÜÊûêÁöÑÂÖ®Êñ∞Ê°ÜÊû∂', 'desc': 'Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫VideoLightsÁöÑËßÜÈ¢ëÈ´ò‰∫ÆÊ£ÄÊµãÂíåÊó∂ÂàªÊ£ÄÁ¥¢Ê°ÜÊû∂ÔºåÊó®Âú®Ëß£ÂÜ≥Áé∞ÊúâÊ®°ÂûãÂú®ËßÜÈ¢ë‰∏éÊñáÊú¨ÂØπÈΩêÂíåË∑®‰ªªÂä°Âä®ÊÄÅÊñπÈù¢ÁöÑ‰∏çË∂≥„ÄÇÊàë‰ª¨ÂºïÂÖ•‰∫ÜÂç∑ÁßØÊäïÂΩ±ÂíåÁâπÂæÅÁ≤æÁÇºÊ®°ÂùóÔºå‰ª•ÊèêÈ´òËßÜÈ¢ëÂíåÊñáÊú¨ÁâπÂæÅÁöÑÂØπÈΩêÊïàÊûúÔºåÂπ∂ÈááÁî®ÂèåÂêëË∑®Ê®°ÊÄÅËûçÂêàÁΩëÁªúÊù•Â¢ûÂº∫Êü•ËØ¢ÊÑüÁü•ÁöÑÁâáÊÆµË°®Á§∫„ÄÇÈÄöËøáÂçïÂêëËÅîÂêà‰ªªÂä°ÂèçÈ¶àÊú∫Âà∂ÔºåÊàë‰ª¨ËÉΩÂ§üÊèêÂçá‰∏§‰∏™‰ªªÂä°‰πãÈó¥ÁöÑÁõ∏ÂÖ≥ÊÄßÔºåÂêåÊó∂ÂºïÂÖ•Á°¨Ê≠£Ë¥üÊçüÂ§±‰ª•ÊîπÂñÑÂ≠¶‰π†ÊïàÊûú„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåVideoLightsÂú®Â§ö‰∏™Âü∫ÂáÜÊï∞ÊçÆÈõÜ‰∏äË°®Áé∞Âá∫Ëâ≤ÔºåËææÂà∞‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2411.18478', 'title': 'Beyond Examples: High-level Automated Reasoning Paradigm in In-Context Learning via MCTS', 'url': 'https://huggingface.co/papers/2411.18478', 'abstract': "In-context Learning (ICL) enables large language models (LLMs) to tackle downstream tasks through sophisticated prompting and high-quality demonstrations. However, this traditional ICL paradigm shows limitations when facing complex mathematical reasoning tasks, primarily due to its heavy dependence on example quality and the necessity for human intervention in challenging scenarios. To address these limitations, this paper presents HiAR-ICL, a High-level Automated Reasoning paradigm in ICL that shifts focus from specific examples to abstract thinking patterns, extending the conventional concept of context in ICL. HiAR-ICL introduces five atomic reasoning actions as fundamental components for constructing chain-structured patterns. Using Monte Carlo Tree Search, we explore reasoning paths and construct thought cards to guide subsequent inference. We then develop a cognitive complexity framework that dynamically matches problems with appropriate thought cards. Experimental results demonstrate HiAR-ICL's effectiveness, achieving state-of-the-art accuracy (79.6%) on the MATH benchmark with Qwen2.5-7B-Instruct, surpassing GPT-4o (76.6%) and Claude 3.5 (71.1%).", 'score': 21, 'issue_id': 890, 'pub_date': '2024-11-27', 'pub_date_card': {'ru': '27 –Ω–æ—è–±—Ä—è', 'en': 'November 27', 'zh': '11Êúà27Êó•'}, 'hash': '05890d0739faa85c', 'authors': ['Jinyang Wu', 'Mingkuan Feng', 'Shuai Zhang', 'Feihu Che', 'Zengqi Wen', 'Jianhua Tao'], 'affiliations': ['Department of Automation, Tsinghua University', 'Beijing National Research Center for Information Science and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2411.18478.jpg', 'data': {'categories': ['#training', '#inference', '#reasoning', '#math'], 'emoji': 'üß†', 'ru': {'title': 'HiAR-ICL: –ê–±—Å—Ç—Ä–∞–∫—Ç–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ –¥–ª—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': "HiAR-ICL - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–π —Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è –Ω–∞ –∞–±—Å—Ç—Ä–∞–∫—Ç–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–∞—Ö –º—ã—à–ª–µ–Ω–∏—è –≤–º–µ—Å—Ç–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—è—Ç—å –±–∞–∑–æ–≤—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è —Ü–µ–ø–æ—á–µ–∫ –º—ã—à–ª–µ–Ω–∏—è. –ü—Ä–∏–º–µ–Ω—è—è –ø–æ–∏—Å–∫ –ú–æ–Ω—Ç–µ-–ö–∞—Ä–ª–æ, HiAR-ICL —Å–æ–∑–¥–∞–µ—Ç '–∫–∞—Ä—Ç–æ—á–∫–∏ –º—ã—Å–ª–µ–π' –¥–ª—è —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–∞ –≤—ã–≤–æ–¥–∞–º–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ HiAR-ICL –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–µ MATH, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è GPT-4 –∏ Claude 3.5."}, 'en': {'title': 'Revolutionizing Mathematical Reasoning with HiAR-ICL', 'desc': "This paper introduces HiAR-ICL, a new approach to In-context Learning (ICL) that enhances large language models' ability to perform complex mathematical reasoning tasks. Traditional ICL relies heavily on the quality of examples and often requires human input, which can be limiting. HiAR-ICL shifts the focus from specific examples to abstract reasoning patterns, utilizing five atomic reasoning actions to create structured reasoning chains. The method employs Monte Carlo Tree Search to explore reasoning paths and develop a cognitive complexity framework that matches problems with suitable thought cards, achieving superior performance on the MATH benchmark."}, 'zh': {'title': 'È´òÂ±ÇÊ¨°Ëá™Âä®Êé®ÁêÜÔºöË∂ÖË∂ä‰º†Áªü‰∏ä‰∏ãÊñáÂ≠¶‰π†ÁöÑÂ±ÄÈôêÊÄß', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÈ´òÂ±ÇÊ¨°Ëá™Âä®Êé®ÁêÜËåÉÂºèHiAR-ICLÔºåÊó®Âú®Ëß£ÂÜ≥‰º†Áªü‰∏ä‰∏ãÊñáÂ≠¶‰π†Âú®Â§çÊùÇÊï∞Â≠¶Êé®ÁêÜ‰ªªÂä°‰∏≠ÁöÑÂ±ÄÈôêÊÄß„ÄÇHiAR-ICLÈÄöËøáÂºïÂÖ•‰∫îÁßçÂü∫Êú¨Êé®ÁêÜÂä®‰ΩúÔºåËΩ¨Âèò‰∫ÜÂØπÂÖ∑‰ΩìÁ§∫‰æãÁöÑ‰æùËµñÔºåÂº∫Ë∞ÉÊäΩË±°ÊÄùÁª¥Ê®°ÂºèÁöÑÈáçË¶ÅÊÄß„ÄÇËØ•ÊñπÊ≥ïÂà©Áî®ËíôÁâπÂç°Ê¥õÊ†ëÊêúÁ¥¢Êé¢Á¥¢Êé®ÁêÜË∑ØÂæÑÔºåÂπ∂ÊûÑÂª∫ÊÄùÁª¥Âç°Áâá‰ª•ÊåáÂØºÂêéÁª≠Êé®ÁêÜ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåHiAR-ICLÂú®MATHÂü∫ÂáÜÊµãËØï‰∏≠ÂèñÂæó‰∫Ü79.6%ÁöÑÂáÜÁ°ÆÁéáÔºåË∂ÖË∂ä‰∫ÜÂÖ∂‰ªñÂÖàËøõÊ®°Âûã„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2411.19930', 'title': 'On Domain-Specific Post-Training for Multimodal Large Language Models', 'url': 'https://huggingface.co/papers/2411.19930', 'abstract': 'Recent years have witnessed the rapid development of general multimodal large language models (MLLMs). However, adapting general MLLMs to specific domains, such as scientific fields and industrial applications, remains less explored. This paper systematically investigates domain adaptation of MLLMs through post-training, focusing on data synthesis, training pipelines, and task evaluation. (1) Data Synthesis: Using open-source models, we develop a visual instruction synthesizer that effectively generates diverse visual instruction tasks from domain-specific image-caption pairs. Our synthetic tasks surpass those generated by manual rules, GPT-4, and GPT-4V in enhancing the domain-specific performance of MLLMs. (2) Training Pipeline: While the two-stage training--initially on image-caption pairs followed by visual instruction tasks--is commonly adopted for developing general MLLMs, we apply a single-stage training pipeline to enhance task diversity for domain-specific post-training. (3) Task Evaluation: We conduct experiments in two domains, biomedicine and food, by post-training MLLMs of different sources and scales (e.g., Qwen2-VL-2B, LLaVA-v1.6-8B, Llama-3.2-11B), and then evaluating MLLM performance on various domain-specific tasks. To support further research in MLLM domain adaptation, we will open-source our implementations.', 'score': 18, 'issue_id': 885, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 –Ω–æ—è–±—Ä—è', 'en': 'November 29', 'zh': '11Êúà29Êó•'}, 'hash': '5d14749b38f15e60', 'authors': ['Daixuan Cheng', 'Shaohan Huang', 'Ziyu Zhu', 'Xintong Zhang', 'Wayne Xin Zhao', 'Zhongzhi Luan', 'Bo Dai', 'Zhenliang Zhang'], 'affiliations': ['Beihang University', 'Beijing Institute of Technology', 'Renmin University of China', 'State Key Laboratory of General Artificial Intelligence, BIGAI', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2411.19930.jpg', 'data': {'categories': ['#transfer_learning', '#dataset', '#training', '#open_source', '#multimodal', '#synthetic'], 'emoji': 'üî¨', 'ru': {'title': '–ê–¥–∞–ø—Ç–∞—Ü–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –ò–ò –∫ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –æ–±–ª–∞—Å—Ç—è–º', 'desc': '–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –∞–¥–∞–ø—Ç–∞—Ü–∏—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (MLLM) –∫ —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏–º –¥–æ–º–µ–Ω–∞–º. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ —Å–∏–Ω—Ç–µ–∑–∞—Ç–æ—Ä –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –¥–æ–º–µ–Ω–Ω—ã–µ –ø–∞—Ä—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ-–ø–æ–¥–ø–∏—Å—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –∑–∞–¥–∞—á. –û–Ω–∏ –ø—Ä–∏–º–µ–Ω–∏–ª–∏ –æ–¥–Ω–æ—ç—Ç–∞–ø–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –∑–∞–¥–∞—á –ø—Ä–∏ –¥–æ–º–µ–Ω–Ω–æ–π –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–µ. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø—Ä–æ–≤–æ–¥–∏–ª–∏—Å—å –≤ –±–∏–æ–º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–π –∏ –ø–∏—â–µ–≤–æ–π –æ–±–ª–∞—Å—Ç—è—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã—Ö MLLM, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞.'}, 'en': {'title': 'Enhancing MLLMs for Specific Domains through Innovative Adaptation Techniques', 'desc': 'This paper explores how to adapt general multimodal large language models (MLLMs) for specific fields like biomedicine and food. It introduces a method for data synthesis that creates diverse visual instruction tasks from image-caption pairs, outperforming previous methods. The authors propose a single-stage training pipeline to improve task diversity during post-training, rather than the usual two-stage approach. Finally, they evaluate the performance of various MLLMs on domain-specific tasks and plan to share their findings to aid future research in this area.'}, 'zh': {'title': 'ÊèêÂçáÂ§öÊ®°ÊÄÅÊ®°ÂûãÁöÑÈ¢ÜÂüüÈÄÇÂ∫îÊÄß', 'desc': 'ËøëÂπ¥Êù•ÔºåÈÄöÁî®Â§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâËøÖÈÄüÂèëÂ±ï„ÄÇÁÑ∂ËÄåÔºåÂ∞ÜÈÄöÁî®MLLMsÈÄÇÂ∫î‰∫éÁâπÂÆöÈ¢ÜÂüüÔºåÂ¶ÇÁßëÂ≠¶ÂíåÂ∑•‰∏öÂ∫îÁî®Ôºå‰ªçÁÑ∂ËæÉÂ∞ëË¢´Êé¢Á¥¢„ÄÇÊú¨ÊñáÁ≥ªÁªüÁ†îÁ©∂‰∫ÜMLLMsÁöÑÈ¢ÜÂüüÈÄÇÂ∫îÊÄßÔºåÈáçÁÇπÂú®‰∫éÊï∞ÊçÆÂêàÊàê„ÄÅËÆ≠ÁªÉÊµÅÁ®ãÂíå‰ªªÂä°ËØÑ‰º∞„ÄÇÊàë‰ª¨ÂºÄÂèë‰∫Ü‰∏ÄÁßçËßÜËßâÊåá‰ª§ÂêàÊàêÂô®ÔºåËÉΩÂ§üÊúâÊïàÁîüÊàêÂ§öÊ†∑ÂåñÁöÑËßÜËßâÊåá‰ª§‰ªªÂä°Ôºå‰ªéËÄåÊèêÂçáMLLMsÂú®ÁâπÂÆöÈ¢ÜÂüüÁöÑË°®Áé∞„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2411.19189', 'title': 'Video Depth without Video Models', 'url': 'https://huggingface.co/papers/2411.19189', 'abstract': 'Video depth estimation lifts monocular video clips to 3D by inferring dense depth at every frame. Recent advances in single-image depth estimation, brought about by the rise of large foundation models and the use of synthetic training data, have fueled a renewed interest in video depth. However, naively applying a single-image depth estimator to every frame of a video disregards temporal continuity, which not only leads to flickering but may also break when camera motion causes sudden changes in depth range. An obvious and principled solution would be to build on top of video foundation models, but these come with their own limitations; including expensive training and inference, imperfect 3D consistency, and stitching routines for the fixed-length (short) outputs. We take a step back and demonstrate how to turn a single-image latent diffusion model (LDM) into a state-of-the-art video depth estimator. Our model, which we call RollingDepth, has two main ingredients: (i) a multi-frame depth estimator that is derived from a single-image LDM and maps very short video snippets (typically frame triplets) to depth snippets. (ii) a robust, optimization-based registration algorithm that optimally assembles depth snippets sampled at various different frame rates back into a consistent video. RollingDepth is able to efficiently handle long videos with hundreds of frames and delivers more accurate depth videos than both dedicated video depth estimators and high-performing single-frame models. Project page: rollingdepth.github.io.', 'score': 16, 'issue_id': 889, 'pub_date': '2024-11-28', 'pub_date_card': {'ru': '28 –Ω–æ—è–±—Ä—è', 'en': 'November 28', 'zh': '11Êúà28Êó•'}, 'hash': '1fc611a9a44595a1', 'authors': ['Bingxin Ke', 'Dominik Narnhofer', 'Shengyu Huang', 'Lei Ke', 'Torben Peters', 'Katerina Fragkiadaki', 'Anton Obukhov', 'Konrad Schindler'], 'affiliations': ['Carnegie Mellon University', 'ETH Zurich'], 'pdf_title_img': 'assets/pdf/title_img/2411.19189.jpg', 'data': {'categories': ['#3d', '#diffusion', '#video', '#optimization'], 'emoji': 'üé•', 'ru': {'title': 'RollingDepth: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ—Ü–µ–Ω–∫–µ –≥–ª—É–±–∏–Ω—ã –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é LDM', 'desc': '–î–∞–Ω–Ω–∞—è —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ—Ü–µ–Ω–∫–µ –≥–ª—É–±–∏–Ω—ã –≤–∏–¥–µ–æ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º RollingDepth. –ú–æ–¥–µ–ª—å –æ—Å–Ω–æ–≤–∞–Ω–∞ –Ω–∞ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–π –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ (LDM) –¥–ª—è –æ–¥–∏–Ω–æ—á–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –≤–∫–ª—é—á–∞–µ—Ç –¥–≤–∞ –∫–ª—é—á–µ–≤—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞: –æ—Ü–µ–Ω—â–∏–∫ –≥–ª—É–±–∏–Ω—ã –¥–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö –≤–∏–¥–µ–æ—Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –∏ –∞–ª–≥–æ—Ä–∏—Ç–º —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏ –¥–ª—è —Å–±–æ—Ä–∫–∏ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –≤ —Ü–µ–ª–æ—Å—Ç–Ω–æ–µ –≤–∏–¥–µ–æ. RollingDepth —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –¥–ª–∏–Ω–Ω—ã–µ –≤–∏–¥–µ–æ –∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –ø–æ —Ç–æ—á–Ω–æ—Å—Ç–∏ –∫–∞–∫ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –æ—Ü–µ–Ω—â–∏–∫–∏ –≥–ª—É–±–∏–Ω—ã –≤–∏–¥–µ–æ, —Ç–∞–∫ –∏ –≤—ã—Å–æ–∫–æ–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –∫–∞–¥—Ä–æ–≤. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã –≤—Ä–µ–º–µ–Ω–Ω–æ–π –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ—Å—Ç–∏ –∏ –∏–∑–º–µ–Ω–µ–Ω–∏–π –¥–∏–∞–ø–∞–∑–æ–Ω–∞ –≥–ª—É–±–∏–Ω—ã –ø—Ä–∏ –¥–≤–∏–∂–µ–Ω–∏–∏ –∫–∞–º–µ—Ä—ã.'}, 'en': {'title': 'Transforming Monocular Videos into Accurate 3D Depth with RollingDepth', 'desc': 'This paper presents RollingDepth, a novel approach for estimating depth in videos by leveraging a single-image latent diffusion model (LDM). The method addresses the challenge of temporal continuity in video depth estimation, which often leads to flickering and inconsistencies when using traditional single-image models. RollingDepth utilizes a multi-frame depth estimator to analyze short video snippets and a robust registration algorithm to combine these depth estimates into a coherent video output. The results show that RollingDepth outperforms existing video depth estimators and single-frame models, providing accurate depth information for long video sequences.'}, 'zh': {'title': 'Â∞ÜÂçïÂõæÂÉèÊ∑±Â∫¶‰º∞ËÆ°ÊèêÂçá‰∏∫ËßÜÈ¢ëÊ∑±Â∫¶‰º∞ËÆ°ÁöÑÂàõÊñ∞‰πãË∑Ø', 'desc': 'ËßÜÈ¢ëÊ∑±Â∫¶‰º∞ËÆ°ÈÄöËøáÊé®Êñ≠ÊØèÂ∏ßÁöÑÂØÜÈõÜÊ∑±Â∫¶ÔºåÂ∞ÜÂçïÁõÆËßÜÈ¢ëÁâáÊÆµÊèêÂçá‰∏∫3D„ÄÇÊúÄËøëÔºåÂçïÂõæÂÉèÊ∑±Â∫¶‰º∞ËÆ°ÁöÑËøõÂ±ïÊøÄÂèë‰∫ÜÂØπËßÜÈ¢ëÊ∑±Â∫¶ÁöÑÂÖ≥Ê≥®Ôºå‰ΩÜÁÆÄÂçïÂú∞Â∞ÜÂçïÂõæÂÉèÊ∑±Â∫¶‰º∞ËÆ°Âô®Â∫îÁî®‰∫éÊØèÂ∏ß‰ºöÂøΩÁï•Êó∂Èó¥ËøûÁª≠ÊÄßÔºåÂØºËá¥Èó™ÁÉÅÂíåÊ∑±Â∫¶ËåÉÂõ¥ÁöÑÁ™ÅÁÑ∂ÂèòÂåñ„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫RollingDepthÁöÑÊ®°ÂûãÔºåÂÆÉÁªìÂêà‰∫ÜÂ§öÂ∏ßÊ∑±Â∫¶‰º∞ËÆ°Âíå‰ºòÂåñÁöÑÊ≥®ÂÜåÁÆóÊ≥ïÔºåËÉΩÂ§üÊúâÊïàÂ§ÑÁêÜÈïøËßÜÈ¢ëÂπ∂Êèê‰æõÊõ¥ÂáÜÁ°ÆÁöÑÊ∑±Â∫¶ËßÜÈ¢ë„ÄÇËØ•Ê®°ÂûãÂú®ÊÄßËÉΩ‰∏äË∂ÖË∂ä‰∫Ü‰∏ìÁî®ËßÜÈ¢ëÊ∑±Â∫¶‰º∞ËÆ°Âô®ÂíåÈ´òÊÄßËÉΩÂçïÂ∏ßÊ®°Âûã„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2411.19146', 'title': 'Puzzle: Distillation-Based NAS for Inference-Optimized LLMs', 'url': 'https://huggingface.co/papers/2411.19146', 'abstract': "Large language models (LLMs) have demonstrated remarkable capabilities, but their adoption is limited by high computational costs during inference. While increasing parameter counts enhances accuracy, it also widens the gap between state-of-the-art capabilities and practical deployability. We present Puzzle, a framework to accelerate LLM inference on specific hardware while preserving their capabilities. Through an innovative application of neural architecture search (NAS) at an unprecedented scale, Puzzle systematically optimizes models with tens of billions of parameters under hardware constraints. Our approach utilizes blockwise local knowledge distillation (BLD) for parallel architecture exploration and employs mixed-integer programming for precise constraint optimization.   We demonstrate the real-world impact of our framework through Llama-3.1-Nemotron-51B-Instruct (Nemotron-51B), a publicly available model derived from Llama-3.1-70B-Instruct. Nemotron-51B achieves a 2.17x inference throughput speedup, fitting on a single NVIDIA H100 GPU while preserving 98.4% of the original model's capabilities. Nemotron-51B currently stands as the most accurate language model capable of inference on a single GPU with large batch sizes. Remarkably, this transformation required just 45B training tokens, compared to over 15T tokens used for the 70B model it was derived from. This establishes a new paradigm where powerful models can be optimized for efficient deployment with only negligible compromise of their capabilities, demonstrating that inference performance, not parameter count alone, should guide model selection. With the release of Nemotron-51B and the presentation of the Puzzle framework, we provide practitioners immediate access to state-of-the-art language modeling capabilities at significantly reduced computational costs.", 'score': 9, 'issue_id': 886, 'pub_date': '2024-11-28', 'pub_date_card': {'ru': '28 –Ω–æ—è–±—Ä—è', 'en': 'November 28', 'zh': '11Êúà28Êó•'}, 'hash': 'b33bb17742a81e99', 'authors': ['Akhiad Bercovich', 'Tomer Ronen', 'Talor Abramovich', 'Nir Ailon', 'Nave Assaf', 'Mohammad Dabbah', 'Ido Galil', 'Amnon Geifman', 'Yonatan Geifman', 'Izhak Golan', 'Netanel Haber', 'Ehud Karpas', 'Itay Levy', 'Shahar Mor', 'Zach Moshe', 'Najeeb Nabwani', 'Omri Puny', 'Ran Rubin', 'Itamar Schen', 'Ido Shahaf', 'Oren Tropp', 'Omer Ullman Argov', 'Ran Zilberstein', 'Ran El-Yaniv'], 'affiliations': ['NVIDIA'], 'pdf_title_img': 'assets/pdf/title_img/2411.19146.jpg', 'data': {'categories': ['#architecture', '#optimization', '#training', '#inference'], 'emoji': 'üß©', 'ru': {'title': '–£—Å–∫–æ—Ä–µ–Ω–∏–µ LLM –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –ò–ò', 'desc': '–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ Puzzle –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–∏ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –∏—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π. –ò—Å–ø–æ–ª—å–∑—É—è –Ω–µ–π—Ä–æ–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–π –ø–æ–∏—Å–∫ (NAS) –∏ –±–ª–æ—á–Ω—É—é –ª–æ–∫–∞–ª—å–Ω—É—é –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—é –∑–Ω–∞–Ω–∏–π (BLD), Puzzle –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç –º–æ–¥–µ–ª–∏ —Å –¥–µ—Å—è—Ç–∫–∞–º–∏ –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø–æ–¥ –∞–ø–ø–∞—Ä–∞—Ç–Ω—ã–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è. –ù–∞ –ø—Ä–∏–º–µ—Ä–µ –º–æ–¥–µ–ª–∏ Nemotron-51B, –ø–æ–ª—É—á–µ–Ω–Ω–æ–π –∏–∑ Llama-3.1-70B-Instruct, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç—Å—è 2.17-–∫—Ä–∞—Ç–Ω–æ–µ —É—Å–∫–æ—Ä–µ–Ω–∏–µ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ 98.4% –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –∏—Å—Ö–æ–¥–Ω–æ–π –º–æ–¥–µ–ª–∏. –≠—Ç–æ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞, –∞ –Ω–µ —Ç–æ–ª—å–∫–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –¥–æ–ª–∂–Ω–∞ –æ–ø—Ä–µ–¥–µ–ª—è—Ç—å –≤—ã–±–æ—Ä –º–æ–¥–µ–ª–∏.'}, 'en': {'title': 'Optimizing Large Language Models for Efficient Inference', 'desc': "This paper introduces Puzzle, a framework designed to enhance the inference speed of large language models (LLMs) while maintaining their performance. It employs neural architecture search (NAS) to optimize models with billions of parameters specifically for certain hardware, addressing the challenge of high computational costs. The framework utilizes blockwise local knowledge distillation (BLD) and mixed-integer programming to efficiently explore and optimize model architectures. The resulting model, Nemotron-51B, achieves a significant speedup in inference on a single GPU while retaining most of the original model's capabilities, showcasing a new approach to deploying powerful LLMs more efficiently."}, 'zh': {'title': 'È´òÊïàÊé®ÁêÜÔºåÂº∫Â§ßÊ®°ÂûãÁöÑÊñ∞ËåÉÂºè', 'desc': 'Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Êé®ÁêÜÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÈ´òËÆ°ÁÆóÊàêÊú¨ÈôêÂà∂‰∫ÜÂÆÉ‰ª¨ÁöÑÂ∫îÁî®„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜPuzzleÊ°ÜÊû∂ÔºåÈÄöËøáÁ•ûÁªèÊû∂ÊûÑÊêúÁ¥¢ÔºàNASÔºâÂú®ÁâπÂÆöÁ°¨‰ª∂‰∏äÂä†ÈÄüLLMÊé®ÁêÜÔºåÂêåÊó∂‰øùÊåÅÂÖ∂ËÉΩÂäõ„ÄÇËØ•Ê°ÜÊû∂Âà©Áî®ÂùóÁä∂Â±ÄÈÉ®Áü•ËØÜËí∏È¶èÔºàBLDÔºâËøõË°åÂπ∂Ë°åÊû∂ÊûÑÊé¢Á¥¢ÔºåÂπ∂ÈááÁî®Ê∑∑ÂêàÊï¥Êï∞ËßÑÂàíËøõË°åÁ≤æÁ°ÆÁ∫¶Êùü‰ºòÂåñ„ÄÇÈÄöËøáNemotron-51BÊ®°ÂûãÔºåÊàë‰ª¨Â±ïÁ§∫‰∫ÜÂú®Âçï‰∏™NVIDIA H100 GPU‰∏äÂÆûÁé∞2.17ÂÄçÊé®ÁêÜÂêûÂêêÈáèÊèêÂçáÁöÑÂÆûÈôÖÊïàÊûúÔºåÂêåÊó∂‰øùÁïô‰∫Ü98.4%ÁöÑÂéüÂßãÊ®°ÂûãËÉΩÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2411.19108', 'title': "Timestep Embedding Tells: It's Time to Cache for Video Diffusion Model", 'url': 'https://huggingface.co/papers/2411.19108', 'abstract': 'As a fundamental backbone for video generation, diffusion models are challenged by low inference speed due to the sequential nature of denoising. Previous methods speed up the models by caching and reusing model outputs at uniformly selected timesteps. However, such a strategy neglects the fact that differences among model outputs are not uniform across timesteps, which hinders selecting the appropriate model outputs to cache, leading to a poor balance between inference efficiency and visual quality. In this study, we introduce Timestep Embedding Aware Cache (TeaCache), a training-free caching approach that estimates and leverages the fluctuating differences among model outputs across timesteps. Rather than directly using the time-consuming model outputs, TeaCache focuses on model inputs, which have a strong correlation with the modeloutputs while incurring negligible computational cost. TeaCache first modulates the noisy inputs using the timestep embeddings to ensure their differences better approximating those of model outputs. TeaCache then introduces a rescaling strategy to refine the estimated differences and utilizes them to indicate output caching. Experiments show that TeaCache achieves up to 4.41x acceleration over Open-Sora-Plan with negligible (-0.07% Vbench score) degradation of visual quality.', 'score': 9, 'issue_id': 886, 'pub_date': '2024-11-28', 'pub_date_card': {'ru': '28 –Ω–æ—è–±—Ä—è', 'en': 'November 28', 'zh': '11Êúà28Êó•'}, 'hash': '02a6c2edf156e9d3', 'authors': ['Feng Liu', 'Shiwei Zhang', 'Xiaofeng Wang', 'Yujie Wei', 'Haonan Qiu', 'Yuzhong Zhao', 'Yingya Zhang', 'Qixiang Ye', 'Fang Wan'], 'affiliations': ['Alibaba Group', 'Fudan University', 'Institute of Automation, Chinese Academy of Sciences', 'Nanyang Technological University', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2411.19108.jpg', 'data': {'categories': ['#diffusion', '#optimization', '#video', '#inference'], 'emoji': '‚è±Ô∏è', 'ru': {'title': '–£–º–Ω–æ–µ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É—Å–∫–æ—Ä–µ–Ω–∏—é –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º TeaCache. –ú–µ—Ç–æ–¥ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç —Ä–∞–∑–ª–∏—á–∏—è –º–µ–∂–¥—É –≤—ã—Ö–æ–¥–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ä–∞–∑–Ω—ã—Ö –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —à–∞–≥–∞—Ö, –∏—Å–ø–æ–ª—å–∑—É—è –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏ –≤—Å—Ç—Ä–∞–∏–≤–∞–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —à–∞–≥–æ–≤. TeaCache –ø—Ä–∏–º–µ–Ω—è–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è —É—Ç–æ—á–Ω–µ–Ω–∏—è –æ—Ü–µ–Ω–æ–∫ —Ä–∞–∑–ª–∏—á–∏–π –∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∏—Ö –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è –≤—ã—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ TeaCache –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —É—Å–∫–æ—Ä–µ–Ω–∏—è –¥–æ 4,41 —Ä–∞–∑–∞ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å Open-Sora-Plan –ø—Ä–∏ –Ω–µ–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–º —Å–Ω–∏–∂–µ–Ω–∏–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏.'}, 'en': {'title': 'Accelerating Video Generation with Smart Caching', 'desc': 'This paper presents TeaCache, a novel caching method designed to enhance the efficiency of diffusion models in video generation. Traditional approaches cache model outputs at fixed timesteps, which can lead to suboptimal performance due to the uneven differences in outputs across timesteps. TeaCache addresses this by focusing on modulating noisy inputs with timestep embeddings, allowing for a more accurate estimation of output differences without the heavy computational cost. The results demonstrate that TeaCache significantly accelerates inference speed while maintaining high visual quality, achieving a notable performance improvement over existing methods.'}, 'zh': {'title': 'ÊèêÂçáËßÜÈ¢ëÁîüÊàêÈÄüÂ∫¶ÁöÑÊñ∞ÊñπÊ≥ïÔºöTeaCache', 'desc': 'Êú¨Á†îÁ©∂ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÁºìÂ≠òÊñπÊ≥ïÔºåÁß∞‰∏∫Êó∂Èó¥Ê≠•ÂµåÂÖ•ÊÑüÁü•ÁºìÂ≠òÔºàTeaCacheÔºâÔºåÊó®Âú®ÊèêÈ´òËßÜÈ¢ëÁîüÊàê‰∏≠ÁöÑÊâ©Êï£Ê®°ÂûãÁöÑÊé®ÁêÜÈÄüÂ∫¶„ÄÇ‰º†ÁªüÊñπÊ≥ïÈÄöËøáÂú®ÂùáÂåÄÈÄâÊã©ÁöÑÊó∂Èó¥Ê≠•ÁºìÂ≠òÊ®°ÂûãËæìÂá∫Ôºå‰ΩÜÂøΩÁï•‰∫Ü‰∏çÂêåÊó∂Èó¥Ê≠•‰πãÈó¥ËæìÂá∫Â∑ÆÂºÇÁöÑ‰∏çÂùáÂåÄÊÄß„ÄÇTeaCacheÈÄöËøáË∞ÉËäÇÂô™Â£∞ËæìÂÖ•ÔºåÂà©Áî®Êó∂Èó¥Ê≠•ÂµåÂÖ•Êù•Êõ¥Â•ΩÂú∞Ëøë‰ººÊ®°ÂûãËæìÂá∫ÁöÑÂ∑ÆÂºÇÔºå‰ªéËÄå‰ºòÂåñÁºìÂ≠òÈÄâÊã©„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåTeaCacheÂú®‰øùÊåÅËßÜËßâË¥®ÈáèÁöÑÂêåÊó∂ÔºåÊé®ÁêÜÈÄüÂ∫¶ÊèêÈ´ò‰∫Ü4.41ÂÄç„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2411.19324', 'title': 'Trajectory Attention for Fine-grained Video Motion Control', 'url': 'https://huggingface.co/papers/2411.19324', 'abstract': 'Recent advancements in video generation have been greatly driven by video diffusion models, with camera motion control emerging as a crucial challenge in creating view-customized visual content. This paper introduces trajectory attention, a novel approach that performs attention along available pixel trajectories for fine-grained camera motion control. Unlike existing methods that often yield imprecise outputs or neglect temporal correlations, our approach possesses a stronger inductive bias that seamlessly injects trajectory information into the video generation process. Importantly, our approach models trajectory attention as an auxiliary branch alongside traditional temporal attention. This design enables the original temporal attention and the trajectory attention to work in synergy, ensuring both precise motion control and new content generation capability, which is critical when the trajectory is only partially available. Experiments on camera motion control for images and videos demonstrate significant improvements in precision and long-range consistency while maintaining high-quality generation. Furthermore, we show that our approach can be extended to other video motion control tasks, such as first-frame-guided video editing, where it excels in maintaining content consistency over large spatial and temporal ranges.', 'score': 9, 'issue_id': 885, 'pub_date': '2024-11-28', 'pub_date_card': {'ru': '28 –Ω–æ—è–±—Ä—è', 'en': 'November 28', 'zh': '11Êúà28Êó•'}, 'hash': '02a266f597ae69e7', 'authors': ['Zeqi Xiao', 'Wenqi Ouyang', 'Yifan Zhou', 'Shuai Yang', 'Lei Yang', 'Jianlou Si', 'Xingang Pan'], 'affiliations': ['S-Lab, Nanyang Technological University', 'Sensetime Research', 'Wangxuan Institute of Computer Technology, Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2411.19324.jpg', 'data': {'categories': ['#diffusion', '#video'], 'emoji': 'üé•', 'ru': {'title': '–¢–æ—á–Ω—ã–π –∫–æ–Ω—Ç—Ä–æ–ª—å –¥–≤–∏–∂–µ–Ω–∏—è –∫–∞–º–µ—Ä—ã –≤ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –≤–∏–¥–µ–æ–º–æ–¥–µ–ª—è—Ö —Å –ø–æ–º–æ—â—å—é trajectory attention', 'desc': "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º 'trajectory attention' –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª—è –¥–≤–∏–∂–µ–Ω–∏—è –∫–∞–º–µ—Ä—ã –≤ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –≤–∏–¥–µ–æ–º–æ–¥–µ–ª—è—Ö. –ú–µ—Ç–æ–¥ –≤—ã–ø–æ–ª–Ω—è–µ—Ç –≤–Ω–∏–º–∞–Ω–∏–µ –≤–¥–æ–ª—å –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π –ø–∏–∫—Å–µ–ª–µ–π, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ –≤–Ω–µ–¥—Ä—è—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –≤ –ø—Ä–æ—Ü–µ—Å—Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ. Trajectory attention —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–∞–∫ –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è –≤–µ—Ç–≤—å –Ω–∞—Ä—è–¥—É —Å —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–º –≤—Ä–µ–º–µ–Ω–Ω—ã–º –≤–Ω–∏–º–∞–Ω–∏–µ–º, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è –∫–∞–∫ —Ç–æ—á–Ω—ã–π –∫–æ–Ω—Ç—Ä–æ–ª—å –¥–≤–∏–∂–µ–Ω–∏—è, —Ç–∞–∫ –∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –Ω–æ–≤–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è –≤ —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –Ω–∞ –±–æ–ª—å—à–∏—Ö —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è—Ö –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏."}, 'en': {'title': 'Enhancing Video Generation with Trajectory Attention', 'desc': 'This paper presents a new method called trajectory attention for improving video generation, particularly in controlling camera motion. It enhances the traditional temporal attention mechanism by incorporating pixel trajectories, allowing for more precise and consistent video outputs. The method effectively combines trajectory information with temporal attention, addressing challenges in generating view-customized content. Experiments show that this approach not only improves motion control but also maintains high-quality video generation across various tasks.'}, 'zh': {'title': 'ËΩ®ËøπÊ≥®ÊÑèÂäõÔºöÁ≤æÁ°ÆÊéßÂà∂ËßÜÈ¢ëÁîüÊàê‰∏≠ÁöÑÁõ∏Êú∫ËøêÂä®', 'desc': 'Êú¨ËÆ∫Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑËΩ®ËøπÊ≥®ÊÑèÂäõÊú∫Âà∂ÔºåÁî®‰∫éËßÜÈ¢ëÁîüÊàê‰∏≠ÁöÑÁõ∏Êú∫ËøêÂä®ÊéßÂà∂„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïËÉΩÂ§üÊõ¥Á≤æÁ°ÆÂú∞Â§ÑÁêÜËøêÂä®ÊéßÂà∂ÔºåÂπ∂ÊúâÊïàÂú∞ÁªìÂêà‰∫ÜËΩ®Ëøπ‰ø°ÊÅØ„ÄÇÈÄöËøáÂ∞ÜËΩ®ËøπÊ≥®ÊÑèÂäõ‰Ωú‰∏∫ËæÖÂä©ÂàÜÊîØ‰∏é‰º†ÁªüÊó∂Èó¥Ê≥®ÊÑèÂäõÁªìÂêàÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂú®ÁîüÊàêÊñ∞ÂÜÖÂÆπÁöÑÂêåÊó∂ÔºåÁ°Æ‰øù‰∫ÜËøêÂä®ÊéßÂà∂ÁöÑÁ≤æÁ°ÆÊÄß„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®ÂõæÂÉèÂíåËßÜÈ¢ëÁöÑÁõ∏Êú∫ËøêÂä®ÊéßÂà∂‰∏≠ÊòæËëóÊèêÈ´ò‰∫ÜÁ≤æÂ∫¶ÂíåÈïøË∑ùÁ¶ª‰∏ÄËá¥ÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2411.18552', 'title': 'FAM Diffusion: Frequency and Attention Modulation for High-Resolution Image Generation with Stable Diffusion', 'url': 'https://huggingface.co/papers/2411.18552', 'abstract': 'Diffusion models are proficient at generating high-quality images. They are however effective only when operating at the resolution used during training. Inference at a scaled resolution leads to repetitive patterns and structural distortions. Retraining at higher resolutions quickly becomes prohibitive. Thus, methods enabling pre-existing diffusion models to operate at flexible test-time resolutions are highly desirable. Previous works suffer from frequent artifacts and often introduce large latency overheads. We propose two simple modules that combine to solve these issues. We introduce a Frequency Modulation (FM) module that leverages the Fourier domain to improve the global structure consistency, and an Attention Modulation (AM) module which improves the consistency of local texture patterns, a problem largely ignored in prior works. Our method, coined Fam diffusion, can seamlessly integrate into any latent diffusion model and requires no additional training. Extensive qualitative results highlight the effectiveness of our method in addressing structural and local artifacts, while quantitative results show state-of-the-art performance. Also, our method avoids redundant inference tricks for improved consistency such as patch-based or progressive generation, leading to negligible latency overheads.', 'score': 8, 'issue_id': 892, 'pub_date': '2024-11-27', 'pub_date_card': {'ru': '27 –Ω–æ—è–±—Ä—è', 'en': 'November 27', 'zh': '11Êúà27Êó•'}, 'hash': 'dd1bf99b66f1b34d', 'authors': ['Haosen Yang', 'Adrian Bulat', 'Isma Hadji', 'Hai X. Pham', 'Xiatian Zhu', 'Georgios Tzimiropoulos', 'Brais Martinez'], 'affiliations': ['Queen Mary University, UK', 'Samsung AI Center, Cambridge, UK', 'University of Surrey, UK'], 'pdf_title_img': 'assets/pdf/title_img/2411.18552.jpg', 'data': {'categories': ['#cv', '#optimization', '#diffusion', '#inference'], 'emoji': 'üñºÔ∏è', 'ru': {'title': '–ì–∏–±–∫–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è', 'desc': '–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Fam diffusion –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞–±–æ—Ç—ã –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –ø—Ä–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–∏ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –¥–≤–∞ –º–æ–¥—É–ª—è: Frequency Modulation (FM) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≥–ª–æ–±–∞–ª—å–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–æ–π —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –∏ Attention Modulation (AM) –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –ª–æ–∫–∞–ª—å–Ω—ã—Ö —Ç–µ–∫—Å—Ç—É—Ä–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤. –ú–µ—Ç–æ–¥ –ª–µ–≥–∫–æ –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç—Å—è –≤ –ª—é–±—É—é –º–æ–¥–µ–ª—å –ª–∞—Ç–µ–Ω—Ç–Ω–æ–π –¥–∏—Ñ—Ñ—É–∑–∏–∏ –∏ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –º–µ—Ç–æ–¥–∞ –≤ —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã—Ö –∏ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—è –ø—Ä–∏ —ç—Ç–æ–º –ª—É—á—à—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –ø–æ–¥—Ö–æ–¥–∞–º–∏.'}, 'en': {'title': 'Flexible Image Generation with Fam Diffusion', 'desc': 'This paper presents a novel approach to enhance diffusion models for image generation, allowing them to work effectively at various resolutions without retraining. The proposed Fam diffusion method introduces two key modules: Frequency Modulation (FM) for improving global structure consistency and Attention Modulation (AM) for refining local texture patterns. These modules address common issues like repetitive patterns and structural distortions that occur when using scaled resolutions. The method integrates seamlessly into existing latent diffusion models, demonstrating state-of-the-art performance with minimal latency overheads and improved image quality.'}, 'zh': {'title': 'ÁÅµÊ¥ªÂàÜËæ®Áéá‰∏ãÁöÑÈ´òË¥®ÈáèÂõæÂÉèÁîüÊàê', 'desc': 'Êâ©Êï£Ê®°ÂûãÂú®ÁîüÊàêÈ´òË¥®ÈáèÂõæÂÉèÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜ‰ªÖÂú®ËÆ≠ÁªÉÊó∂‰ΩøÁî®ÁöÑÂàÜËæ®Áéá‰∏ãÊúâÊïà„ÄÇÂú®‰∏çÂêåÁöÑÂàÜËæ®Áéá‰∏ãÊé®ÁêÜ‰ºöÂØºËá¥ÈáçÂ§çÊ®°ÂºèÂíåÁªìÊûÑÂ§±Áúü„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏§‰∏™ÁÆÄÂçïÁöÑÊ®°ÂùóÔºåÈ¢ëÁéáË∞ÉÂà∂ÔºàFMÔºâÊ®°ÂùóÂíåÊ≥®ÊÑèÂäõË∞ÉÂà∂ÔºàAMÔºâÊ®°ÂùóÔºåÊù•Ëß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢ò„ÄÇÊàë‰ª¨ÁöÑFamÊâ©Êï£ÊñπÊ≥ïÂèØ‰ª•Êó†ÁºùÈõÜÊàêÂà∞‰ªª‰ΩïÊΩúÂú®Êâ©Êï£Ê®°Âûã‰∏≠ÔºåÊó†ÈúÄÈ¢ùÂ§ñËÆ≠ÁªÉÔºåÂπ∂‰∏îÂú®Â§ÑÁêÜÁªìÊûÑÂíåÂ±ÄÈÉ®‰º™ÂΩ±ÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2411.19527', 'title': 'DisCoRD: Discrete Tokens to Continuous Motion via Rectified Flow Decoding', 'url': 'https://huggingface.co/papers/2411.19527', 'abstract': 'Human motion, inherently continuous and dynamic, presents significant challenges for generative models. Despite their dominance, discrete quantization methods, such as VQ-VAEs, suffer from inherent limitations, including restricted expressiveness and frame-wise noise artifacts. Continuous approaches, while producing smoother and more natural motions, often falter due to high-dimensional complexity and limited training data. To resolve this "discord" between discrete and continuous representations, we introduce DisCoRD: Discrete Tokens to Continuous Motion via Rectified Flow Decoding, a novel method that decodes discrete motion tokens into continuous motion through rectified flow. By employing an iterative refinement process in the continuous space, DisCoRD captures fine-grained dynamics and ensures smoother and more natural motions. Compatible with any discrete-based framework, our method enhances naturalness without compromising faithfulness to the conditioning signals. Extensive evaluations demonstrate that DisCoRD achieves state-of-the-art performance, with FID of 0.032 on HumanML3D and 0.169 on KIT-ML. These results solidify DisCoRD as a robust solution for bridging the divide between discrete efficiency and continuous realism. Our project page is available at: https://whwjdqls.github.io/discord.github.io/.', 'score': 8, 'issue_id': 891, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 –Ω–æ—è–±—Ä—è', 'en': 'November 29', 'zh': '11Êúà29Êó•'}, 'hash': 'b1fc0d8f7ba13620', 'authors': ['Jungbin Cho', 'Junwan Kim', 'Jisoo Kim', 'Minseo Kim', 'Mingu Kang', 'Sungeun Hong', 'Tae-Hyun Oh', 'Youngjae Yu'], 'affiliations': ['POSTECH', 'Sungkyunkwan University', 'Yonsei University'], 'pdf_title_img': 'assets/pdf/title_img/2411.19527.jpg', 'data': {'categories': ['#video', '#dataset', '#training'], 'emoji': 'ü§ñ', 'ru': {'title': 'DisCoRD: –ú–æ—Å—Ç –º–µ–∂–¥—É –¥–∏—Å–∫—Ä–µ—Ç–Ω–æ–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å—é –∏ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–º —Ä–µ–∞–ª–∏–∑–º–æ–º –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–≤–∏–∂–µ–Ω–∏–π', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ DisCoRD –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–≤–∏–∂–µ–Ω–∏–π —á–µ–ª–æ–≤–µ–∫–∞. –ú–µ—Ç–æ–¥ –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã—Ö –∏ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã—Ö –ø–æ–¥—Ö–æ–¥–æ–≤, –∏—Å–ø–æ–ª—å–∑—É—è –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –∏ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–µ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —á–µ—Ä–µ–∑ –≤—ã–ø—Ä—è–º–ª–µ–Ω–Ω—ã–π –ø–æ—Ç–æ–∫. DisCoRD –ø—Ä–∏–º–µ–Ω—è–µ—Ç –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–µ —É—Ç–æ—á–Ω–µ–Ω–∏–µ –≤ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –¥–ª—è –∑–∞—Ö–≤–∞—Ç–∞ —Ç–æ–Ω–∫–∏—Ö –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ –¥–≤–∏–∂–µ–Ω–∏—è. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –º–µ—Ç–æ–¥ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –Ω–∞–∏–ª—É—á—à–∏—Ö –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π –ø–æ –º–µ—Ç—Ä–∏–∫–µ FID –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö HumanML3D –∏ KIT-ML.'}, 'en': {'title': 'Bridging the Gap: DisCoRD for Smooth Human Motion Generation', 'desc': 'This paper addresses the challenges of generating human motion using machine learning models, particularly the limitations of discrete quantization methods like VQ-VAEs. It introduces a new method called DisCoRD, which stands for Discrete Tokens to Continuous Motion via Rectified Flow Decoding. DisCoRD effectively converts discrete motion tokens into smooth continuous motion by using an iterative refinement process in the continuous space. The results show that DisCoRD outperforms existing methods, achieving state-of-the-art performance metrics on benchmark datasets, thus providing a solution that balances discrete efficiency with continuous realism.'}, 'zh': {'title': 'ÊâìÁ†¥Á¶ªÊï£‰∏éËøûÁª≠ÁöÑÁïåÈôêÔºåÊèêÂçáËøêÂä®ÁîüÊàêËá™ÁÑ∂ÊÄß', 'desc': '‰∫∫Á±ªËøêÂä®ÊòØËøûÁª≠ÂíåÂä®ÊÄÅÁöÑÔºåËøôÁªôÁîüÊàêÊ®°ÂûãÂ∏¶Êù•‰∫ÜÂæàÂ§ßÊåëÊàò„ÄÇÂ∞ΩÁÆ°Á¶ªÊï£ÈáèÂåñÊñπÊ≥ïÔºàÂ¶ÇVQ-VAEsÔºâÂç†‰∏ªÂØºÂú∞‰ΩçÔºå‰ΩÜÂÆÉ‰ª¨Âú®Ë°®ËææËÉΩÂäõÂíåÂ∏ßÂô™Â£∞ÊñπÈù¢Â≠òÂú®Â±ÄÈôê„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÊñπÊ≥ïDisCoRDÔºåÈÄöËøá‰øÆÊ≠£ÊµÅËß£Á†ÅÂ∞ÜÁ¶ªÊï£ËøêÂä®Ê†áËÆ∞Ëß£Á†Å‰∏∫ËøûÁª≠ËøêÂä®ÔºåËß£ÂÜ≥‰∫ÜÁ¶ªÊï£ÂíåËøûÁª≠Ë°®Á§∫‰πãÈó¥ÁöÑÁüõÁõæ„ÄÇDisCoRDÂú®ËøûÁª≠Á©∫Èó¥‰∏≠ËøõË°åËø≠‰ª£‰ºòÂåñÔºåÊçïÊçâÁªÜÂæÆÂä®ÊÄÅÔºåÁ°Æ‰øùËøêÂä®Êõ¥Âä†Âπ≥ÊªëËá™ÁÑ∂Ôºå‰∏î‰∏é‰ªª‰ΩïÂü∫‰∫éÁ¶ªÊï£ÁöÑÊ°ÜÊû∂ÂÖºÂÆπ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2411.19950', 'title': 'AlphaTablets: A Generic Plane Representation for 3D Planar Reconstruction from Monocular Videos', 'url': 'https://huggingface.co/papers/2411.19950', 'abstract': 'We introduce AlphaTablets, a novel and generic representation of 3D planes that features continuous 3D surface and precise boundary delineation. By representing 3D planes as rectangles with alpha channels, AlphaTablets combine the advantages of current 2D and 3D plane representations, enabling accurate, consistent and flexible modeling of 3D planes. We derive differentiable rasterization on top of AlphaTablets to efficiently render 3D planes into images, and propose a novel bottom-up pipeline for 3D planar reconstruction from monocular videos. Starting with 2D superpixels and geometric cues from pre-trained models, we initialize 3D planes as AlphaTablets and optimize them via differentiable rendering. An effective merging scheme is introduced to facilitate the growth and refinement of AlphaTablets. Through iterative optimization and merging, we reconstruct complete and accurate 3D planes with solid surfaces and clear boundaries. Extensive experiments on the ScanNet dataset demonstrate state-of-the-art performance in 3D planar reconstruction, underscoring the great potential of AlphaTablets as a generic 3D plane representation for various applications. Project page is available at: https://hyzcluster.github.io/alphatablets', 'score': 5, 'issue_id': 888, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 –Ω–æ—è–±—Ä—è', 'en': 'November 29', 'zh': '11Êúà29Êó•'}, 'hash': '9f7d2daec9cb311d', 'authors': ['Yuze He', 'Wang Zhao', 'Shaohui Liu', 'Yubin Hu', 'Yushi Bai', 'Yu-Hui Wen', 'Yong-Jin Liu'], 'affiliations': ['Beijing Jiaotong University', 'ETH Zurich', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2411.19950.jpg', 'data': {'categories': ['#3d'], 'emoji': 'üìê', 'ru': {'title': 'AlphaTablets: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–∏ 3D –ø–ª–æ—Å–∫–æ—Å—Ç–µ–π', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω AlphaTablets - –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—é —Ç—Ä–µ—Ö–º–µ—Ä–Ω—ã—Ö –ø–ª–æ—Å–∫–æ—Å—Ç–µ–π –≤ –≤–∏–¥–µ –ø—Ä—è–º–æ—É–≥–æ–ª—å–Ω–∏–∫–æ–≤ —Å –∞–ª—å—Ñ–∞-–∫–∞–Ω–∞–ª–∞–º–∏. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–æ—á–µ—Ç–∞—Ç—å –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö 2D –∏ 3D –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è —Ç–æ—á–Ω–æ–µ –∏ –≥–∏–±–∫–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ 3D –ø–ª–æ—Å–∫–æ—Å—Ç–µ–π. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä—É–µ–º—É—é —Ä–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ 3D –ø–ª–æ—Å–∫–æ—Å—Ç–µ–π –∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –Ω–æ–≤—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º 3D —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –ø–ª–æ—Å–∫–æ—Å—Ç–µ–π –∏–∑ –º–æ–Ω–æ–∫—É–ª—è—Ä–Ω—ã—Ö –≤–∏–¥–µ–æ. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö ScanNet –ø–æ–∫–∞–∑–∞–ª–∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –∑–∞–¥–∞—á–µ 3D —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –ø–ª–æ—Å–∫–æ—Å—Ç–µ–π.'}, 'en': {'title': 'AlphaTablets: Revolutionizing 3D Plane Representation', 'desc': 'AlphaTablets is a new way to represent 3D planes that combines the benefits of both 2D and 3D models. It uses rectangles with alpha channels to create smooth surfaces and clear edges for accurate modeling. The paper introduces a method for rendering these planes efficiently and a pipeline for reconstructing 3D planes from single videos. By optimizing the representation through merging and iterative processes, AlphaTablets achieves high-quality 3D reconstructions, as shown in tests on the ScanNet dataset.'}, 'zh': {'title': 'AlphaTabletsÔºö3DÂπ≥Èù¢ÈáçÂª∫ÁöÑÊñ∞ÊñπÊ≥ï', 'desc': 'Êàë‰ª¨‰ªãÁªç‰∫ÜAlphaTabletsÔºåËøôÊòØ‰∏ÄÁßçÊñ∞È¢ñ‰∏îÈÄöÁî®ÁöÑ3DÂπ≥Èù¢Ë°®Á§∫ÊñπÊ≥ïÔºåÂÖ∑ÊúâËøûÁª≠ÁöÑ3DË°®Èù¢ÂíåÁ≤æÁ°ÆÁöÑËæπÁïåÂàíÂàÜ„ÄÇÈÄöËøáÂ∞Ü3DÂπ≥Èù¢Ë°®Á§∫‰∏∫Â∏¶ÊúâalphaÈÄöÈÅìÁöÑÁü©ÂΩ¢ÔºåAlphaTabletsÁªìÂêà‰∫ÜÂΩìÂâç2DÂíå3DÂπ≥Èù¢Ë°®Á§∫ÁöÑ‰ºòÁÇπÔºåÂÆûÁé∞‰∫Ü3DÂπ≥Èù¢ÁöÑÂáÜÁ°Æ„ÄÅ‰∏ÄËá¥ÂíåÁÅµÊ¥ªÂª∫Ê®°„ÄÇÊàë‰ª¨Âú®AlphaTabletsÁöÑÂü∫Á°Ä‰∏äÊé®ÂØºÂá∫ÂèØÂæÆÂàÜÂÖâÊ†ÖÂåñÊäÄÊúØÔºå‰ª•È´òÊïàÂú∞Â∞Ü3DÂπ≥Èù¢Ê∏≤Êüì‰∏∫ÂõæÂÉèÔºåÂπ∂ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑËá™‰∏ãËÄå‰∏äÁöÑÂçïÁõÆËßÜÈ¢ë3DÂπ≥Èù¢ÈáçÂª∫ÁÆ°ÈÅì„ÄÇÈÄöËøáËø≠‰ª£‰ºòÂåñÂíåÂêàÂπ∂ÔºåÊàë‰ª¨ËÉΩÂ§üÈáçÂª∫Âá∫ÂÆåÊï¥‰∏îÂáÜÁ°ÆÁöÑ3DÂπ≥Èù¢ÔºåÂÖ∑ÊúâÂùöÂÆûÁöÑË°®Èù¢ÂíåÊ∏ÖÊô∞ÁöÑËæπÁïå„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2411.19460', 'title': 'Look Every Frame All at Once: Video-Ma$^2$mba for Efficient Long-form Video Understanding with Multi-Axis Gradient Checkpointing', 'url': 'https://huggingface.co/papers/2411.19460', 'abstract': 'With the growing scale and complexity of video data, efficiently processing long video sequences poses significant challenges due to the quadratic increase in memory and computational demands associated with existing transformer-based Large Multi-modal Models (LMMs). To address these issues, we introduce Video-Ma^2mba, a novel architecture that incorporates State Space Models (SSMs) within the Mamba-2 framework, replacing the attention mechanisms. This allows the LMMs to scale linearly in terms of time and memory requirements, making it feasible to handle long-duration video content. Furthermore, we enhance the memory efficiency introducing the Multi-Axis Gradient Checkpointing (MA-GC) method, which strategically manages memory by retaining only essential activations across multiple computational axes. Our approach significantly reduces the memory footprint compared to standard gradient checkpointing. Empirical analyses show that Video-Ma^2mba can process extensive video sequences-equivalent to millions of tokens or over two hours of continuous sequences at 1 FPS-on a single GPU. By maintaining a detailed capture of temporal dynamics, our model improves the accuracy and relevance of responses in long video understanding tasks, demonstrating substantial advantages over existing frameworks.', 'score': 5, 'issue_id': 886, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 –Ω–æ—è–±—Ä—è', 'en': 'November 29', 'zh': '11Êúà29Êó•'}, 'hash': 'b96751a3db484750', 'authors': ['Hosu Lee', 'Junho Kim', 'Hyunjun Kim', 'Yong Man Ro'], 'affiliations': ['Integrated Vision and Language Lab, KAIST, South Korea'], 'pdf_title_img': 'assets/pdf/title_img/2411.19460.jpg', 'data': {'categories': ['#architecture', '#long_context', '#video', '#optimization'], 'emoji': 'üé•', 'ru': {'title': '–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é Video-Ma^2mba', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Video-Ma^2mba - –Ω–æ–≤—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π, –∏—Å–ø–æ–ª—å–∑—É—é—â—É—é –º–æ–¥–µ–ª–∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ —Å–æ—Å—Ç–æ—è–Ω–∏–π –≤–º–µ—Å—Ç–æ –º–µ—Ö–∞–Ω–∏–∑–º–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ª–∏–Ω–µ–π–Ω–æ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞—Ç—å –±–æ–ª—å—à–∏–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ (LMM) –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –∏ –ø–∞–º—è—Ç–∏. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ –≤–≤–æ–¥—è—Ç –º–µ—Ç–æ–¥ –º—É–ª—å—Ç–∏–æ—Å–µ–≤–æ–≥–æ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ —á–µ–∫–ø–æ–π–Ω—Ç–∏–Ω–≥–∞ (MA-GC) –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏. –≠–º–ø–∏—Ä–∏—á–µ—Å–∫–∏–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ Video-Ma^2mba –º–æ–∂–µ—Ç –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –¥–ª–∏–Ω–Ω—ã–µ –≤–∏–¥–µ–æ–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ –æ–¥–Ω–æ–º GPU, —É–ª—É—á—à–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –æ—Ç–≤–µ—Ç–æ–≤ –≤ –∑–∞–¥–∞—á–∞—Ö –ø–æ–Ω–∏–º–∞–Ω–∏—è –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ.'}, 'en': {'title': 'Revolutionizing Long Video Processing with Linear Scalability', 'desc': 'The paper presents Video-Ma^2mba, a new architecture designed to efficiently process long video sequences by integrating State Space Models (SSMs) into the Mamba-2 framework. This innovative approach replaces traditional attention mechanisms, allowing the model to scale linearly in memory and computational requirements, which is crucial for handling extensive video data. Additionally, the introduction of Multi-Axis Gradient Checkpointing (MA-GC) optimizes memory usage by retaining only necessary activations, significantly reducing the memory footprint. Empirical results indicate that Video-Ma^2mba can effectively manage video sequences equivalent to millions of tokens, enhancing the accuracy of long video understanding tasks compared to existing models.'}, 'zh': {'title': 'È´òÊïàÂ§ÑÁêÜÈïøËßÜÈ¢ëÂ∫èÂàóÁöÑÊñ∞ÊñπÊ≥ï', 'desc': 'ÈöèÁùÄËßÜÈ¢ëÊï∞ÊçÆËßÑÊ®°ÂíåÂ§çÊùÇÊÄßÁöÑÂ¢ûÂä†ÔºåÂ§ÑÁêÜÈïøËßÜÈ¢ëÂ∫èÂàóÈù¢‰∏¥ÁùÄÊòæËëóÁöÑÊåëÊàò„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞Êû∂ÊûÑVideo-Ma^2mbaÔºåÂÆÉÂú®Mamba-2Ê°ÜÊû∂‰∏≠ÂºïÂÖ•‰∫ÜÁä∂ÊÄÅÁ©∫Èó¥Ê®°ÂûãÔºàSSMsÔºâÔºåÊõø‰ª£‰∫Ü‰º†ÁªüÁöÑÊ≥®ÊÑèÂäõÊú∫Âà∂Ôºå‰ªéËÄå‰ΩøÂæóÂ§ßËßÑÊ®°Â§öÊ®°ÊÄÅÊ®°ÂûãÔºàLMMsÔºâÂú®Êó∂Èó¥ÂíåÂÜÖÂ≠òÈúÄÊ±Ç‰∏äÂÆûÁé∞Á∫øÊÄßÊâ©Â±ï„ÄÇÊàë‰ª¨ËøòÂºïÂÖ•‰∫ÜÂ§öËΩ¥Ê¢ØÂ∫¶Ê£ÄÊü•ÁÇπÔºàMA-GCÔºâÊñπÊ≥ïÔºå‰ºòÂåñÂÜÖÂ≠òÁÆ°ÁêÜÔºå‰ªÖ‰øùÁïôÂøÖË¶ÅÁöÑÊøÄÊ¥ª‰ø°ÊÅØÔºåÊòæËëóÈôç‰Ωé‰∫ÜÂÜÖÂ≠òÂç†Áî®„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåVideo-Ma^2mbaËÉΩÂ§üÂú®Âçï‰∏™GPU‰∏äÂ§ÑÁêÜÁõ∏ÂΩì‰∫éÊï∞Áôæ‰∏á‰∏™Ê†áËÆ∞ÊàñË∂ÖËøá‰∏§Â∞èÊó∂ÁöÑËøûÁª≠ËßÜÈ¢ëÂ∫èÂàóÔºåÊèêÂçá‰∫ÜÈïøËßÜÈ¢ëÁêÜËß£‰ªªÂä°ÁöÑÂáÜÁ°ÆÊÄßÂíåÁõ∏ÂÖ≥ÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2411.19865', 'title': 'Reverse Thinking Makes LLMs Stronger Reasoners', 'url': 'https://huggingface.co/papers/2411.19865', 'abstract': "Reverse thinking plays a crucial role in human reasoning. Humans can reason not only from a problem to a solution but also in reverse, i.e., start from the solution and reason towards the problem. This often enhances overall reasoning performance as it enables consistency checks between their forward and backward thinking. To enable Large Language Models (LLMs) to perform reverse thinking, we introduce Reverse-Enhanced Thinking (RevThink), a framework composed of data augmentation and learning objectives. In RevThink, we augment the dataset by collecting structured forward-backward reasoning from a teacher model, consisting of: (1) the original question, (2) forward reasoning, (3) backward question, and (4) backward reasoning. We then employ three objectives to train a smaller student model in a multi-task learning fashion: (a) generate forward reasoning from a question, (b) generate a backward question from a question, and (c) generate backward reasoning from the backward question. Experiments across 12 datasets covering commonsense, math, and logical reasoning show an average 13.53% improvement over the student model's zero-shot performance and a 6.84% improvement over the strongest knowledge distillation baselines. Moreover, our method demonstrates sample efficiency -- using only 10% of the correct forward reasoning from the training data, it outperforms a standard fine-tuning method trained on 10x more forward reasoning. RevThink also exhibits strong generalization to out-of-distribution held-out datasets.", 'score': 4, 'issue_id': 899, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 –Ω–æ—è–±—Ä—è', 'en': 'November 29', 'zh': '11Êúà29Êó•'}, 'hash': '8f066f57ddff0ae8', 'authors': ['Justin Chih-Yao Chen', 'Zifeng Wang', 'Hamid Palangi', 'Rujun Han', 'Sayna Ebrahimi', 'Long Le', 'Vincent Perot', 'Swaroop Mishra', 'Mohit Bansal', 'Chen-Yu Lee', 'Tomas Pfister'], 'affiliations': ['Google Cloud AI Research', 'Google DeepMind', 'UNC Chapel Hill'], 'pdf_title_img': 'assets/pdf/title_img/2411.19865.jpg', 'data': {'categories': ['#data', '#training', '#transfer_learning', '#small_models', '#dataset', '#reasoning'], 'emoji': 'üîÑ', 'ru': {'title': 'RevThink: –£—Å–∏–ª–µ–Ω–∏–µ LLM –æ–±—Ä–∞—Ç–Ω—ã–º –º—ã—à–ª–µ–Ω–∏–µ–º', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Reverse-Enhanced Thinking (RevThink) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). RevThink –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–±—Ä–∞—Ç–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ, –¥–æ–ø–æ–ª–Ω—è—è –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø—Ä—è–º—ã–º–∏ –∏ –æ–±—Ä–∞—Ç–Ω—ã–º–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º–∏ –æ—Ç –º–æ–¥–µ–ª–∏-—É—á–∏—Ç–µ–ª—è. –ú–µ—Ç–æ–¥ –≤–∫–ª—é—á–∞–µ—Ç —Ç—Ä–∏ –∑–∞–¥–∞—á–∏ –æ–±—É—á–µ–Ω–∏—è: –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä—è–º—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, –æ–±—Ä–∞—Ç–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ –∏ –æ–±—Ä–∞—Ç–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –±–∞–∑–æ–≤—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏.'}, 'en': {'title': 'Empowering LLMs with Reverse Reasoning for Enhanced Performance', 'desc': 'This paper introduces Reverse-Enhanced Thinking (RevThink), a framework designed to improve Large Language Models (LLMs) by enabling them to perform reverse reasoning. RevThink enhances reasoning performance by augmenting datasets with structured forward and backward reasoning examples, allowing models to learn from both directions. The framework employs multi-task learning objectives to train a smaller student model, focusing on generating forward reasoning, backward questions, and backward reasoning. Experimental results show significant improvements in reasoning tasks, demonstrating the effectiveness and efficiency of RevThink in enhancing model performance with limited data.'}, 'zh': {'title': 'ÂèçÂêëÊÄùÁª¥ÔºöÊèêÂçáÊé®ÁêÜËÉΩÂäõÁöÑÊñ∞ÊñπÊ≥ï', 'desc': 'ÂèçÂêëÊÄùÁª¥Âú®‰∫∫ÁöÑÊé®ÁêÜ‰∏≠Ëµ∑ÁùÄÈáçË¶Å‰ΩúÁî®„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫ÂèçÂêëÂ¢ûÂº∫ÊÄùÁª¥ÔºàRevThinkÔºâÁöÑÊ°ÜÊû∂ÔºåÊó®Âú®‰ΩøÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâËÉΩÂ§üËøõË°åÂèçÂêëÊé®ÁêÜ„ÄÇËØ•Ê°ÜÊû∂ÈÄöËøáÊï∞ÊçÆÂ¢ûÂº∫ÂíåÂ≠¶‰π†ÁõÆÊ†áÊù•ÂÆûÁé∞ÔºåÊî∂ÈõÜÁªìÊûÑÂåñÁöÑÂâçÂêëÂíåÂêéÂêëÊé®ÁêÜÊï∞ÊçÆ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåRevThinkÂú®Â§ö‰∏™Êï∞ÊçÆÈõÜ‰∏äÊòæËëóÊèêÈ´ò‰∫ÜÊ®°ÂûãÁöÑÊé®ÁêÜÊÄßËÉΩÔºåÂπ∂Â±ïÁ§∫‰∫ÜËâØÂ•ΩÁöÑÊ†∑Êú¨ÊïàÁéáÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2411.19638', 'title': 'LLM Teacher-Student Framework for Text Classification With No Manually Annotated Data: A Case Study in IPTC News Topic Classification', 'url': 'https://huggingface.co/papers/2411.19638', 'abstract': "With the ever-increasing number of news stories available online, classifying them by topic, regardless of the language they are written in, has become crucial for enhancing readers' access to relevant content. To address this challenge, we propose a teacher-student framework based on large language models (LLMs) for developing multilingual news classification models of reasonable size with no need for manual data annotation. The framework employs a Generative Pretrained Transformer (GPT) model as the teacher model to develop an IPTC Media Topic training dataset through automatic annotation of news articles in Slovenian, Croatian, Greek, and Catalan. The teacher model exhibits a high zero-shot performance on all four languages. Its agreement with human annotators is comparable to that between the human annotators themselves. To mitigate the computational limitations associated with the requirement of processing millions of texts daily, smaller BERT-like student models are fine-tuned on the GPT-annotated dataset. These student models achieve high performance comparable to the teacher model. Furthermore, we explore the impact of the training data size on the performance of the student models and investigate their monolingual, multilingual and zero-shot cross-lingual capabilities. The findings indicate that student models can achieve high performance with a relatively small number of training instances, and demonstrate strong zero-shot cross-lingual abilities. Finally, we publish the best-performing news topic classifier, enabling multilingual classification with the top-level categories of the IPTC Media Topic schema.", 'score': 4, 'issue_id': 889, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 –Ω–æ—è–±—Ä—è', 'en': 'November 29', 'zh': '11Êúà29Êó•'}, 'hash': '98bf5f113194343b', 'authors': ['Taja Kuzman', 'Nikola Ljube≈°iƒá'], 'affiliations': ['Department of Knowledge Technologies, Jo≈æef Stefan Institute, 1000 Ljubljana, Slovenia', 'Jo≈æef Stefan International Postgraduate School, 1000 Ljubljana, Slovenia', 'University of Ljubljana, 1000 Ljubljana, Slovenia'], 'pdf_title_img': 'assets/pdf/title_img/2411.19638.jpg', 'data': {'categories': ['#machine_translation', '#training', '#low_resource', '#multilingual', '#dataset'], 'emoji': 'üì∞', 'ru': {'title': '–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π –±–µ–∑ —Ä—É—á–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–∏', 'desc': "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –Ω–æ–≤–æ—Å—Ç–µ–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–æ–¥–µ–ª–∏ '—É—á–∏—Ç–µ–ª—å-—É—á–µ–Ω–∏–∫'. –ë–æ–ª—å—à–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å GPT –≤—ã—Å—Ç—É–ø–∞–µ—Ç –≤ —Ä–æ–ª–∏ —É—á–∏—Ç–µ–ª—è, –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∞–Ω–Ω–æ—Ç–∏—Ä—É—è –Ω–æ–≤–æ—Å—Ç–Ω—ã–µ —Å—Ç–∞—Ç—å–∏ –Ω–∞ —á–µ—Ç—ã—Ä–µ—Ö —è–∑—ã–∫–∞—Ö. –ú–æ–¥–µ–ª–∏-—É—á–µ–Ω–∏–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ BERT –æ–±—É—á–∞—é—Ç—Å—è –Ω–∞ —ç—Ç–∏—Ö –¥–∞–Ω–Ω—ã—Ö, –¥–æ—Å—Ç–∏–≥–∞—è –≤—ã—Å–æ–∫–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–∏ –º–µ–Ω—å—à–∏—Ö –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–∞—Ç—Ä–∞—Ç–∞—Ö. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –º–µ—Ç–æ–¥–∞ –¥–ª—è –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–π –∏ –∫—Ä–æ—Å—Å-—è–∑—ã–∫–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –Ω–æ–≤–æ—Å—Ç–µ–π."}, 'en': {'title': 'Empowering Multilingual News Classification with Teacher-Student LLMs', 'desc': 'This paper presents a teacher-student framework utilizing large language models (LLMs) for multilingual news classification without manual data annotation. The teacher model, a Generative Pretrained Transformer (GPT), automatically annotates news articles in multiple languages, achieving high zero-shot performance and agreement with human annotators. Smaller BERT-like student models are then fine-tuned on this annotated dataset, demonstrating comparable performance to the teacher model while being computationally efficient. The study also highlights the effectiveness of the student models in multilingual and zero-shot cross-lingual tasks, ultimately providing a robust news topic classifier for diverse languages.'}, 'zh': {'title': 'Â§öËØ≠Ë®ÄÊñ∞ÈóªÂàÜÁ±ªÁöÑÊñ∞ÊñπÊ≥ï', 'desc': 'ÈöèÁùÄÂú®Á∫øÊñ∞ÈóªÊï∞ÈáèÁöÑ‰∏çÊñ≠Â¢ûÂä†ÔºåÊåâ‰∏ªÈ¢òÂØπÊñ∞ÈóªËøõË°åÂàÜÁ±ªÂèòÂæóËá≥ÂÖ≥ÈáçË¶Å„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÊïôÂ∏à-Â≠¶ÁîüÊ°ÜÊû∂ÔºåÁî®‰∫éÂºÄÂèëÂ§öËØ≠Ë®ÄÊñ∞ÈóªÂàÜÁ±ªÊ®°ÂûãÔºå‰∏îÊó†ÈúÄÊâãÂä®Êï∞ÊçÆÊ†áÊ≥®„ÄÇÊïôÂ∏àÊ®°Âûã‰ΩøÁî®ÁîüÊàêÈ¢ÑËÆ≠ÁªÉÂèòÊç¢Âô®ÔºàGPTÔºâËá™Âä®Ê†áÊ≥®Êñ∞ÈóªÊñáÁ´†ÔºåÂ±ïÁ§∫Âá∫Âú®Â§öÁßçËØ≠Ë®Ä‰∏äÁöÑÈ´òÈõ∂Ê†∑Êú¨ÊÄßËÉΩ„ÄÇÈÄöËøáÂæÆË∞ÉËæÉÂ∞èÁöÑBERTÁ±ªÂ≠¶ÁîüÊ®°ÂûãÔºåËøô‰∫õÊ®°ÂûãÂú®Áõ∏ÂØπËæÉÂ∞ëÁöÑËÆ≠ÁªÉÂÆû‰æã‰∏ã‰πüËÉΩËææÂà∞‰∏éÊïôÂ∏àÊ®°ÂûãÁõ∏ÂΩìÁöÑÈ´òÊÄßËÉΩ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2411.18673', 'title': 'AC3D: Analyzing and Improving 3D Camera Control in Video Diffusion Transformers', 'url': 'https://huggingface.co/papers/2411.18673', 'abstract': 'Numerous works have recently integrated 3D camera control into foundational text-to-video models, but the resulting camera control is often imprecise, and video generation quality suffers. In this work, we analyze camera motion from a first principles perspective, uncovering insights that enable precise 3D camera manipulation without compromising synthesis quality. First, we determine that motion induced by camera movements in videos is low-frequency in nature. This motivates us to adjust train and test pose conditioning schedules, accelerating training convergence while improving visual and motion quality. Then, by probing the representations of an unconditional video diffusion transformer, we observe that they implicitly perform camera pose estimation under the hood, and only a sub-portion of their layers contain the camera information. This suggested us to limit the injection of camera conditioning to a subset of the architecture to prevent interference with other video features, leading to 4x reduction of training parameters, improved training speed and 10% higher visual quality. Finally, we complement the typical dataset for camera control learning with a curated dataset of 20K diverse dynamic videos with stationary cameras. This helps the model disambiguate the difference between camera and scene motion, and improves the dynamics of generated pose-conditioned videos. We compound these findings to design the Advanced 3D Camera Control (AC3D) architecture, the new state-of-the-art model for generative video modeling with camera control.', 'score': 4, 'issue_id': 886, 'pub_date': '2024-11-27', 'pub_date_card': {'ru': '27 –Ω–æ—è–±—Ä—è', 'en': 'November 27', 'zh': '11Êúà27Êó•'}, 'hash': '1ea35d3552a278a3', 'authors': ['Sherwin Bahmani', 'Ivan Skorokhodov', 'Guocheng Qian', 'Aliaksandr Siarohin', 'Willi Menapace', 'Andrea Tagliasacchi', 'David B. Lindell', 'Sergey Tulyakov'], 'affiliations': ['SFU', 'Snap Inc.', 'University of Toronto', 'Vector Institute'], 'pdf_title_img': 'assets/pdf/title_img/2411.18673.jpg', 'data': {'categories': ['#dataset', '#architecture', '#diffusion', '#games', '#3d', '#training', '#optimization', '#video'], 'emoji': 'üé•', 'ru': {'title': '–ü—Ä–µ—Ü–∏–∑–∏–æ–Ω–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ 3D-–∫–∞–º–µ—Ä–æ–π –≤ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –≤–∏–¥–µ–æ–º–æ–¥–µ–ª—è—Ö', 'desc': '–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—é 3D-–∫–∞–º–µ—Ä–æ–π –≤ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –≤–∏–¥–µ–æ–º–æ–¥–µ–ª—è—Ö. –ê–≤—Ç–æ—Ä—ã –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç –¥–≤–∏–∂–µ–Ω–∏–µ –∫–∞–º–µ—Ä—ã —Å —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–æ–π —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è –∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ —É–ª—É—á—à–µ–Ω–∏–π, –≤–∫–ª—é—á–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –≥—Ä–∞—Ñ–∏–∫–æ–≤ –æ–±—É—á–µ–Ω–∏—è –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è, –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –≤–Ω–µ–¥—Ä–µ–Ω–∏—è —É—Å–ª–æ–≤–∏–π –∫–∞–º–µ—Ä—ã –≤ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–µ —Å–ª–æ–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –ø–æ–¥–æ–±—Ä–∞–Ω–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è. –†–µ–∑—É–ª—å—Ç–∞—Ç–æ–º —è–≤–ª—è–µ—Ç—Å—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ Advanced 3D Camera Control (AC3D), –∫–æ—Ç–æ—Ä–∞—è –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∫–∞–º–µ—Ä–æ–π –±–µ–∑ —É—â–µ—Ä–±–∞ –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–∞ —Å–∏–Ω—Ç–µ–∑–∞ –≤–∏–¥–µ–æ. –ú–æ–¥–µ–ª—å AC3D –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –Ω–æ–≤–æ–≥–æ —É—Ä–æ–≤–Ω—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –≤–∏–¥–µ–æ–º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è —Å —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º –∫–∞–º–µ—Ä–æ–π.'}, 'en': {'title': 'Precision in 3D Camera Control for Enhanced Video Generation', 'desc': 'This paper presents a novel approach to improve 3D camera control in text-to-video models, addressing issues of imprecision and video quality. By analyzing camera motion, the authors discover that it is primarily low-frequency, which leads to adjustments in training and testing schedules that enhance convergence and visual fidelity. They also find that only certain layers of a video diffusion transformer contain relevant camera information, allowing for a more efficient architecture that reduces training parameters while boosting quality. Finally, the introduction of a curated dataset of dynamic videos aids the model in distinguishing between camera and scene motion, culminating in the development of the Advanced 3D Camera Control (AC3D) model, which sets a new standard in generative video modeling.'}, 'zh': {'title': 'ÂÖàËøõÁöÑ3DÁõ∏Êú∫ÊéßÂà∂ÔºåÊèêÂçáËßÜÈ¢ëÁîüÊàêË¥®Èáè', 'desc': 'Êú¨Á†îÁ©∂ÂàÜÊûê‰∫Ü3DÁõ∏Êú∫ÊéßÂà∂Âú®ÊñáÊú¨Âà∞ËßÜÈ¢ëÊ®°Âûã‰∏≠ÁöÑÂ∫îÁî®ÔºåÂèëÁé∞Áõ∏Êú∫ËøêÂä®ÂØπËßÜÈ¢ëÁîüÊàêË¥®ÈáèÊúâÊòæËëóÂΩ±Âìç„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑËÆ≠ÁªÉÂíåÊµãËØïÂßøÊÄÅË∞ÉËäÇÁ≠ñÁï•Ôºå‰ª•ÊèêÈ´òËÆ≠ÁªÉÊî∂ÊïõÈÄüÂ∫¶ÂíåËßÜÈ¢ëÁöÑËßÜËßâË¥®Èáè„ÄÇÈÄöËøáÂØπÊó†Êù°‰ª∂ËßÜÈ¢ëÊâ©Êï£ÂèòÊç¢Âô®ÁöÑË°®Á§∫ËøõË°åÊé¢ÊµãÔºåÊàë‰ª¨ÂèëÁé∞Áõ∏Êú∫ÂßøÊÄÅ‰º∞ËÆ°Âú®Ê®°ÂûãÂÜÖÈÉ®ÈöêÂºèÊâßË°åÔºåÂõ†Ê≠§Êàë‰ª¨ÈôêÂà∂‰∫ÜÁõ∏Êú∫Êù°‰ª∂ÁöÑÊ≥®ÂÖ•Ôºå‰ª•ÂáèÂ∞ëÂØπÂÖ∂‰ªñËßÜÈ¢ëÁâπÂæÅÁöÑÂπ≤Êâ∞„ÄÇÊúÄÁªàÔºåÊàë‰ª¨ËÆæËÆ°‰∫ÜÂÖàËøõÁöÑ3DÁõ∏Êú∫ÊéßÂà∂Êû∂ÊûÑÔºàAC3DÔºâÔºåÊàê‰∏∫ÂÖ∑ÊúâÁõ∏Êú∫ÊéßÂà∂ÁöÑÁîüÊàêËßÜÈ¢ëÂª∫Ê®°ÁöÑÊñ∞‰∏Ä‰ª£Ê®°Âûã„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2411.19842', 'title': 'Scaling Transformers for Low-Bitrate High-Quality Speech Coding', 'url': 'https://huggingface.co/papers/2411.19842', 'abstract': 'The tokenization of speech with neural audio codec models is a vital part of modern AI pipelines for the generation or understanding of speech, alone or in a multimodal context. Traditionally such tokenization models have concentrated on low parameter-count architectures using only components with strong inductive biases. In this work we show that by scaling a transformer architecture with large parameter count to this problem, and applying a flexible Finite Scalar Quantization (FSQ) based bottleneck, it is possible to reach state-of-the-art speech quality at extremely low bit-rates of 400 or 700 bits-per-second. The trained models strongly out-perform existing baselines in both objective and subjective tests.', 'score': 3, 'issue_id': 900, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 –Ω–æ—è–±—Ä—è', 'en': 'November 29', 'zh': '11Êúà29Êó•'}, 'hash': '23e49aedef71b878', 'authors': ['Julian D Parker', 'Anton Smirnov', 'Jordi Pons', 'CJ Carr', 'Zack Zukowski', 'Zach Evans', 'Xubo Liu'], 'affiliations': ['Stability AI'], 'pdf_title_img': 'assets/pdf/title_img/2411.19842.jpg', 'data': {'categories': ['#multimodal', '#architecture', '#audio', '#training'], 'emoji': 'üó£Ô∏è', 'ru': {'title': '–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã –ø–æ–∫–æ—Ä—è—é—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é —Ä–µ—á–∏', 'desc': '–î–∞–Ω–Ω–∞—è —Å—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ —Ä–µ—á–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö –∞—É–¥–∏–æ–∫–æ–¥–µ–∫–æ–≤. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç –º–æ–¥–µ–ª—å –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ —Å –±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ–º –≥–∏–±–∫–æ–≥–æ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è FSQ. –≠—Ç–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –ø–æ–∑–≤–æ–ª—è–µ—Ç –¥–æ—Å—Ç–∏—á—å –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–µ—á–∏ –ø—Ä–∏ –∫—Ä–∞–π–Ω–µ –Ω–∏–∑–∫–∏—Ö –±–∏—Ç—Ä–µ–π—Ç–∞—Ö –≤ 400-700 –±–∏—Ç/—Å. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –±–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –∫–∞–∫ –≤ –æ–±—ä–µ–∫—Ç–∏–≤–Ω—ã—Ö, —Ç–∞–∫ –∏ –≤ —Å—É–±—ä–µ–∫—Ç–∏–≤–Ω—ã—Ö —Ç–µ—Å—Ç–∞—Ö.'}, 'en': {'title': 'Transforming Speech Quality with Scalable Neural Models', 'desc': 'This paper discusses the importance of tokenizing speech using neural audio codec models in AI systems. It highlights a new approach that utilizes a large-scale transformer architecture combined with Finite Scalar Quantization (FSQ) to improve speech quality. The proposed method achieves impressive results, delivering high-quality speech at very low bit-rates of 400 or 700 bits-per-second. The models developed in this study significantly outperform existing methods in both objective measures and subjective evaluations.'}, 'zh': {'title': 'ÈÄöËøáÊâ©Â±ïÂèòÊç¢Âô®ÂÆûÁé∞È´òË¥®Èáè‰ΩéÊØîÁâπÁéáËØ≠Èü≥Ê†áËÆ∞Âåñ', 'desc': 'Êú¨ËÆ∫ÊñáÊé¢ËÆ®‰∫Ü‰ΩøÁî®Á•ûÁªèÈü≥È¢ëÁºñËß£Á†ÅÊ®°ÂûãÂØπËØ≠Èü≥ËøõË°åÊ†áËÆ∞ÂåñÁöÑÈáçË¶ÅÊÄß„ÄÇ‰º†ÁªüÁöÑÊ†áËÆ∞ÂåñÊ®°ÂûãÈÄöÂ∏∏ÈááÁî®‰ΩéÂèÇÊï∞ÈáèÁöÑÊû∂ÊûÑÔºå‰æùËµñ‰∫éÂº∫ÁöÑÂΩíÁ∫≥ÂÅèÁΩÆ„ÄÇÊàë‰ª¨Â±ïÁ§∫‰∫ÜÈÄöËøáÊâ©Â±ïÂèòÊç¢Âô®Êû∂ÊûÑÂπ∂Â∫îÁî®ÁÅµÊ¥ªÁöÑÊúâÈôêÊ†áÈáèÈáèÂåñÔºàFSQÔºâÁì∂È¢àÔºåÂèØ‰ª•Âú®ÊûÅ‰ΩéÁöÑÊØîÁâπÁéá‰∏ãÂÆûÁé∞ÊúÄÂÖàËøõÁöÑËØ≠Èü≥Ë¥®Èáè„ÄÇËÆ≠ÁªÉÂêéÁöÑÊ®°ÂûãÂú®ÂÆ¢ËßÇÂíå‰∏ªËßÇÊµãËØï‰∏≠ÂùáÊòæËëóË∂ÖË∂ä‰∫ÜÁé∞ÊúâÂü∫ÂáÜ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2411.18664', 'title': 'Spatiotemporal Skip Guidance for Enhanced Video Diffusion Sampling', 'url': 'https://huggingface.co/papers/2411.18664', 'abstract': 'Diffusion models have emerged as a powerful tool for generating high-quality images, videos, and 3D content. While sampling guidance techniques like CFG improve quality, they reduce diversity and motion. Autoguidance mitigates these issues but demands extra weak model training, limiting its practicality for large-scale models. In this work, we introduce Spatiotemporal Skip Guidance (STG), a simple training-free sampling guidance method for enhancing transformer-based video diffusion models. STG employs an implicit weak model via self-perturbation, avoiding the need for external models or additional training. By selectively skipping spatiotemporal layers, STG produces an aligned, degraded version of the original model to boost sample quality without compromising diversity or dynamic degree. Our contributions include: (1) introducing STG as an efficient, high-performing guidance technique for video diffusion models, (2) eliminating the need for auxiliary models by simulating a weak model through layer skipping, and (3) ensuring quality-enhanced guidance without compromising sample diversity or dynamics unlike CFG. For additional results, visit https://junhahyung.github.io/STGuidance.', 'score': 2, 'issue_id': 900, 'pub_date': '2024-11-27', 'pub_date_card': {'ru': '27 –Ω–æ—è–±—Ä—è', 'en': 'November 27', 'zh': '11Êúà27Êó•'}, 'hash': '3576f88bbf3e4567', 'authors': ['Junha Hyung', 'Kinam Kim', 'Susung Hong', 'Min-Jung Kim', 'Jaegul Choo'], 'affiliations': ['KAIST AI', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2411.18664.jpg', 'data': {'categories': ['#diffusion', '#architecture', '#3d', '#training', '#video'], 'emoji': 'üé¨', 'ru': {'title': 'STG: –£–ª—É—á—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –≤–∏–¥–µ–æ –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π - Spatiotemporal Skip Guidance (STG). STG –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–∞–º–æ–≤–æ–∑–º—É—â–µ–Ω–∏–µ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Å–ª–∞–±–æ–π –º–æ–¥–µ–ª–∏. –ú–µ—Ç–æ–¥ –∏–∑–±–∏—Ä–∞—Ç–µ–ª—å–Ω–æ –ø—Ä–æ–ø—É—Å–∫–∞–µ—Ç –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ-–≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Å–ª–æ–∏, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —É–ª—É—á—à–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ —Å—ç–º–ø–ª–æ–≤ –±–µ–∑ —É—â–µ—Ä–±–∞ –¥–ª—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –∏ –¥–∏–Ω–∞–º–∏–∫–∏. STG –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã, —Ç–∞–∫–∏–µ –∫–∞–∫ CFG –∏ –∞–≤—Ç–æ–≥–∞–π–¥–µ–Ω—Å, –Ω–µ —Ç—Ä–µ–±—É—è –ø—Ä–∏ —ç—Ç–æ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∏–ª–∏ –æ–±—É—á–µ–Ω–∏—è.'}, 'en': {'title': 'Enhancing Video Diffusion with Spatiotemporal Skip Guidance', 'desc': 'This paper presents Spatiotemporal Skip Guidance (STG), a novel method for improving video diffusion models without requiring additional training or external models. STG enhances the quality of generated videos by using a self-perturbation technique that simulates a weak model through selective skipping of spatiotemporal layers. This approach allows for better sample quality while maintaining diversity and dynamic motion, addressing the limitations of existing methods like CFG. The authors demonstrate that STG is an efficient and effective guidance technique that enhances the performance of transformer-based video diffusion models.'}, 'zh': {'title': 'Êó∂Á©∫Ë∑≥Ë∑ÉÂºïÂØºÔºöÊèêÂçáËßÜÈ¢ëÊâ©Êï£Ê®°ÂûãÁöÑÈ´òÊïàÊñπÊ≥ï', 'desc': 'Êâ©Êï£Ê®°ÂûãÂ∑≤Êàê‰∏∫ÁîüÊàêÈ´òË¥®ÈáèÂõæÂÉè„ÄÅËßÜÈ¢ëÂíå3DÂÜÖÂÆπÁöÑÂº∫Â§ßÂ∑•ÂÖ∑„ÄÇËôΩÁÑ∂ÈááÊ†∑ÂºïÂØºÊäÄÊúØÂ¶ÇCFGÂèØ‰ª•ÊèêÈ´òË¥®ÈáèÔºå‰ΩÜ‰ºöÈôç‰ΩéÂ§öÊ†∑ÊÄßÂíåÂä®ÊÄÅÊÄß„ÄÇËá™ÂºïÂØºÊñπÊ≥ïËôΩÁÑ∂ÂèØ‰ª•ÁºìËß£Ëøô‰∫õÈóÆÈ¢òÔºå‰ΩÜÈúÄË¶ÅÈ¢ùÂ§ñÁöÑÂº±Ê®°ÂûãËÆ≠ÁªÉÔºåÈôêÂà∂‰∫ÜÂÖ∂Âú®Â§ßËßÑÊ®°Ê®°Âûã‰∏≠ÁöÑÂÆûÁî®ÊÄß„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜÊó∂Á©∫Ë∑≥Ë∑ÉÂºïÂØºÔºàSTGÔºâÔºåËøôÊòØ‰∏ÄÁßçÁÆÄÂçïÁöÑÊó†ËÆ≠ÁªÉÈááÊ†∑ÂºïÂØºÊñπÊ≥ïÔºåÊó®Âú®Â¢ûÂº∫Âü∫‰∫éÂèòÊç¢Âô®ÁöÑËßÜÈ¢ëÊâ©Êï£Ê®°Âûã„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2411.18092', 'title': 'Training Noise Token Pruning', 'url': 'https://huggingface.co/papers/2411.18092', 'abstract': "In the present work we present Training Noise Token (TNT) Pruning for vision transformers. Our method relaxes the discrete token dropping condition to continuous additive noise, providing smooth optimization in training, while retaining discrete dropping computational gains in deployment settings. We provide theoretical connections to Rate-Distortion literature, and empirical evaluations on the ImageNet dataset using ViT and DeiT architectures demonstrating TNT's advantages over previous pruning methods.", 'score': 1, 'issue_id': 905, 'pub_date': '2024-11-27', 'pub_date_card': {'ru': '27 –Ω–æ—è–±—Ä—è', 'en': 'November 27', 'zh': '11Êúà27Êó•'}, 'hash': '570d01745d1c7f3d', 'authors': ['Mingxing Rao', 'Bohan Jiang', 'Daniel Moyer'], 'affiliations': ['Vanderbilt University, Nashville, TN 37235, USA'], 'pdf_title_img': 'assets/pdf/title_img/2411.18092.jpg', 'data': {'categories': ['#optimization', '#training', '#inference', '#cv', '#architecture'], 'emoji': '‚úÇÔ∏è', 'ru': {'title': '–ü–ª–∞–≤–Ω–∞—è –æ–±—Ä–µ–∑–∫–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ —Å –ø–æ–º–æ—â—å—é —à—É–º–∞', 'desc': '–í —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –º–µ—Ç–æ–¥ –æ–±—Ä–µ–∑–∫–∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –¥–ª—è –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Training Noise Token (TNT). –ú–µ—Ç–æ–¥ –∑–∞–º–µ–Ω—è–µ—Ç –¥–∏—Å–∫—Ä–µ—Ç–Ω–æ–µ –æ—Ç–±—Ä–∞—Å—ã–≤–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤ –Ω–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–≥–æ —à—É–º–∞, —á—Ç–æ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –ø–ª–∞–≤–Ω—É—é –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏, —Å–æ—Ö—Ä–∞–Ω—è—è –ø—Ä–∏ —ç—Ç–æ–º –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –¥–∏—Å–∫—Ä–µ—Ç–Ω–æ–≥–æ –æ—Ç–±—Ä–∞—Å—ã–≤–∞–Ω–∏—è –ø—Ä–∏ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–∏. –ê–≤—Ç–æ—Ä—ã –ø—Ä–∏–≤–æ–¥—è—Ç —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–µ —Å–≤—è–∑–∏ —Å –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–æ–π –ø–æ —Å–∫–æ—Ä–æ—Å—Ç–∏-–∏—Å–∫–∞–∂–µ–Ω–∏—é –∏ —ç–º–ø–∏—Ä–∏—á–µ—Å–∫–∏–µ –æ—Ü–µ–Ω–∫–∏ –Ω–∞ –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö ImageNet —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä ViT –∏ DeiT. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ TNT –ø–µ—Ä–µ–¥ –ø—Ä–µ–¥—ã–¥—É—â–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ –æ–±—Ä–µ–∑–∫–∏.'}, 'en': {'title': 'Smooth Optimization with TNT Pruning for Vision Transformers', 'desc': 'This paper introduces a novel approach called Training Noise Token (TNT) Pruning for vision transformers, which enhances the training process by allowing continuous additive noise instead of strictly dropping tokens. This method enables smoother optimization during training while still benefiting from the computational efficiency of discrete token dropping during deployment. The authors establish theoretical links to Rate-Distortion theory, which helps to understand the trade-offs involved in token pruning. Empirical results on the ImageNet dataset show that TNT Pruning outperforms existing pruning techniques when applied to ViT and DeiT architectures.'}, 'zh': {'title': 'ËÆ≠ÁªÉÂô™Â£∞Ê†áËÆ∞Ââ™ÊûùÔºö‰ºòÂåñ‰∏éÊïàÁéáÁöÑÁªìÂêà', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÁî®‰∫éËßÜËßâÂèòÊç¢Âô®ÁöÑËÆ≠ÁªÉÂô™Â£∞Ê†áËÆ∞ÔºàTNTÔºâÂâ™ÊûùÊñπÊ≥ï„ÄÇÊàë‰ª¨ÁöÑÊñπÊ≥ïÂ∞ÜÁ¶ªÊï£ÁöÑÊ†áËÆ∞‰∏¢ÂºÉÊù°‰ª∂ÊîæÂÆΩ‰∏∫ËøûÁª≠ÁöÑÂä†ÊÄßÂô™Â£∞Ôºå‰ªéËÄåÂú®ËÆ≠ÁªÉ‰∏≠ÂÆûÁé∞Âπ≥Êªë‰ºòÂåñÔºåÂêåÊó∂Âú®ÈÉ®ÁΩ≤ÁéØÂ¢É‰∏≠‰øùÁïôÁ¶ªÊï£‰∏¢ÂºÉÁöÑËÆ°ÁÆó‰ºòÂäø„ÄÇÊàë‰ª¨ËøòÊèê‰æõ‰∫Ü‰∏éÁéáÂ§±ÁúüÊñáÁåÆÁöÑÁêÜËÆ∫ËÅîÁ≥ªÔºåÂπ∂Âú®ImageNetÊï∞ÊçÆÈõÜ‰∏ä‰ΩøÁî®ViTÂíåDeiTÊû∂ÊûÑËøõË°å‰∫ÜÂÆûËØÅËØÑ‰º∞ÔºåÂ±ïÁ§∫‰∫ÜTNTÁõ∏ËæÉ‰∫é‰πãÂâçÂâ™ÊûùÊñπÊ≥ïÁöÑ‰ºòÂäø„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2411.18665', 'title': 'SpotLight: Shadow-Guided Object Relighting via Diffusion', 'url': 'https://huggingface.co/papers/2411.18665', 'abstract': 'Recent work has shown that diffusion models can be used as powerful neural rendering engines that can be leveraged for inserting virtual objects into images. Unlike typical physics-based renderers, however, neural rendering engines are limited by the lack of manual control over the lighting setup, which is often essential for improving or personalizing the desired image outcome. In this paper, we show that precise lighting control can be achieved for object relighting simply by specifying the desired shadows of the object. Rather surprisingly, we show that injecting only the shadow of the object into a pre-trained diffusion-based neural renderer enables it to accurately shade the object according to the desired light position, while properly harmonizing the object (and its shadow) within the target background image. Our method, SpotLight, leverages existing neural rendering approaches and achieves controllable relighting results with no additional training. Specifically, we demonstrate its use with two neural renderers from the recent literature. We show that SpotLight achieves superior object compositing results, both quantitatively and perceptually, as confirmed by a user study, outperforming existing diffusion-based models specifically designed for relighting.', 'score': 1, 'issue_id': 903, 'pub_date': '2024-11-27', 'pub_date_card': {'ru': '27 –Ω–æ—è–±—Ä—è', 'en': 'November 27', 'zh': '11Êúà27Êó•'}, 'hash': '1b31caf705bc142d', 'authors': ['Fr√©d√©ric Fortier-Chouinard', 'Zitian Zhang', 'Louis-Etienne Messier', 'Mathieu Garon', 'Anand Bhattad', 'Jean-Fran√ßois Lalonde'], 'affiliations': ['Depix Technologies', 'Toyota Technological Institute at Chicago', 'Universite Laval'], 'pdf_title_img': 'assets/pdf/title_img/2411.18665.jpg', 'data': {'categories': ['#3d', '#cv', '#diffusion'], 'emoji': 'üí°', 'ru': {'title': '–ö–æ–Ω—Ç—Ä–æ–ª—å –æ—Å–≤–µ—â–µ–Ω–∏—è –≤ –Ω–µ–π—Ä–æ–Ω–Ω–æ–º —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–µ —á–µ—Ä–µ–∑ —Ç–µ–Ω–∏ –æ–±—ä–µ–∫—Ç–æ–≤', 'desc': '–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç –º–µ—Ç–æ–¥ SpotLight –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–≥–æ –æ—Å–≤–µ—â–µ–Ω–∏—è –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –Ω–µ–π—Ä–æ–Ω–Ω–æ–º —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–µ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ —Ç–µ–Ω–∏ –æ–±—ä–µ–∫—Ç–∞ –≤ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–π –Ω–µ–π—Ä–æ–Ω–Ω—ã–π —Ä–µ–Ω–¥–µ—Ä–µ—Ä –ø–æ–∑–≤–æ–ª—è–µ—Ç —Ç–æ—á–Ω–æ –∑–∞—Ç–µ–Ω—è—Ç—å –æ–±—ä–µ–∫—Ç –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å –∂–µ–ª–∞–µ–º—ã–º –ø–æ–ª–æ–∂–µ–Ω–∏–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞ —Å–≤–µ—Ç–∞. –ú–µ—Ç–æ–¥ SpotLight –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏ –º–æ–∂–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –Ω–µ–π—Ä–æ–Ω–Ω—ã–º–∏ —Ä–µ–Ω–¥–µ—Ä–µ—Ä–∞–º–∏. –°–æ–≥–ª–∞—Å–Ω–æ –ø—Ä–æ–≤–µ–¥–µ–Ω–Ω–æ–º—É –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—é, SpotLight –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏, —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–ª—è –ø–µ—Ä–µ—Å—Ç–∞–Ω–æ–≤–∫–∏ –æ—Å–≤–µ—â–µ–Ω–∏—è.'}, 'en': {'title': 'SpotLight: Control Lighting with Shadows in Neural Rendering', 'desc': "This paper introduces SpotLight, a method that enhances neural rendering by allowing precise control over lighting for virtual objects in images. It achieves this by enabling users to specify the desired shadows of the object, which the diffusion model uses to accurately shade the object based on the light's position. SpotLight integrates seamlessly with existing pre-trained diffusion-based neural renderers, requiring no additional training. The results demonstrate significant improvements in object compositing, both in quantitative metrics and user perception, compared to traditional diffusion models designed for relighting."}, 'zh': {'title': 'SpotLightÔºöÁ≤æÂáÜÊéßÂà∂ËôöÊãüÁâ©‰ΩìÂÖâÁÖßÁöÑÂàõÊñ∞ÊñπÊ≥ï', 'desc': 'Êú¨ËÆ∫Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂêç‰∏∫SpotLightÁöÑÊñπÊ≥ïÔºåÂà©Áî®Êâ©Êï£Ê®°ÂûãËøõË°åËôöÊãüÁâ©‰ΩìÁöÑÈáçÂÖâÁÖß„ÄÇ‰∏é‰º†ÁªüÁöÑÁâ©ÁêÜÊ∏≤ÊüìÂô®‰∏çÂêåÔºåSpotLightÈÄöËøá‰ªÖÊåáÂÆöÁâ©‰ΩìÁöÑÈò¥ÂΩ±Êù•ÂÆûÁé∞Á≤æÁ°ÆÁöÑÂÖâÁÖßÊéßÂà∂„ÄÇÊàë‰ª¨ÁöÑÊñπÊ≥ïËÉΩÂ§üÂú®‰∏çÈúÄË¶ÅÈ¢ùÂ§ñËÆ≠ÁªÉÁöÑÊÉÖÂÜµ‰∏ãÔºåÂáÜÁ°ÆÂú∞Ê†πÊçÆÊâÄÈúÄÁöÑÂÖâÊ∫ê‰ΩçÁΩÆ‰∏∫Áâ©‰Ωì‰∏äËâ≤ÔºåÂπ∂‰∏éËÉåÊôØÂõæÂÉèÂíåË∞êËûçÂêà„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåSpotLightÂú®Áâ©‰ΩìÂêàÊàêÊïàÊûú‰∏ä‰ºò‰∫éÁé∞ÊúâÁöÑÊâ©Êï£Ê®°ÂûãÔºåÂæóÂà∞‰∫ÜÁî®Êà∑Á†îÁ©∂ÁöÑÊîØÊåÅ„ÄÇ'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (13)', '#agents (3)', '#agi (1)', '#alignment (2)', '#architecture (21)', '#audio (4)', '#benchmark (30)', '#cv (20)', '#data (11)', '#dataset (30)', '#diffusion (24)', '#ethics (1)', '#games (9)', '#graphs', '#hallucinations (2)', '#healthcare', '#inference (10)', '#interpretability (5)', '#leakage (1)', '#long_context (5)', '#low_resource (5)', '#machine_translation (2)', '#math (5)', '#multilingual (3)', '#multimodal (28)', '#open_source (12)', '#optimization (39)', '#plp', '#rag (2)', '#reasoning (11)', '#rl (2)', '#rlhf (1)', '#robotics', '#science (1)', '#security (1)', '#small_models (3)', '#story_generation (1)', '#survey (2)', '#synthetic (10)', '#training (39)', '#transfer_learning (5)', '#video (24)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `üè∑Ô∏è ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `üè∑Ô∏è ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            üî∫ ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'üîÑ ' + getTimeDiff('2024-12-06 03:33',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "—Ä–µ–π—Ç–∏–Ω–≥—É",
                    pub_date: "–¥–∞—Ç–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏",
                    issue_id: "–¥–æ–±–∞–≤–ª–µ–Ω–∏—é –Ω–∞ HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "ËØÑÂàÜ",
                    pub_date: "ÂèëÂ∏ÉÊó•Êúü",
                    issue_id: "HF‰∏ä‰º†Êó•Êúü"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2024-12-06 03:33')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2024-12-06 03:33')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('monthly'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    