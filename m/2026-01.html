
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 39 papers. January 2026.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #7a30efcf;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: #7a30efcf;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #7a30ef17;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ", "Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹", "Ğ¼Ğ¸Ğ½ÑƒÑ‚"],
                hour: ["Ñ‡Ğ°Ñ", "Ñ‡Ğ°ÑĞ°", "Ñ‡Ğ°ÑĞ¾Ğ²"],
                day: ["Ğ´ĞµĞ½ÑŒ", "Ğ´Ğ½Ñ", "Ğ´Ğ½ĞµĞ¹"],
                justNow: "Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ñ‚Ğ¾",
                ago: "Ğ½Ğ°Ğ·Ğ°Ğ´"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["åˆ†é’Ÿ", "åˆ†é’Ÿ", "åˆ†é’Ÿ"],
                hour: ["å°æ—¶", "å°æ—¶", "å°æ—¶"],
                day: ["å¤©", "å¤©", "å¤©"],
                justNow: "åˆšåˆš",
                ago: "å‰"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            } else if (lastDigit === 1) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒÑ";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒĞ¸";
            } else {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ç¯‡è®ºæ–‡"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">ğŸ”º</h1><h1 class="title-text" id="doomgrad">hf monthly</h1></a>
            <p><span id="title-date">Ğ¯Ğ½Ğ²Ğ°Ñ€ÑŒ 2026</span> | <span id="title-articles-count">39 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/m/2025-12.html">â¬…ï¸ <span id="prev-date">12.2025</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/m/2026-02.html">â¡ï¸ <span id="next-date">02.2026</span></a></span>
            <span class="nav-item" id="nav-daily"><a href="https://hfday.ru">ğŸ“ˆ <span id='top-day-label'>Ğ”ĞµĞ½ÑŒ</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">ğŸ”€ <span id="sort-label-text">Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ</option>
                    <option value="pub_date">Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸</option>
                    <option value="issue_id">Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">ğŸ·ï¸ Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> AâˆªB</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> Aâˆ©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">ğŸ§¹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> âœ–ï¸ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': 'Ğ¯Ğ½Ğ²Ğ°Ñ€ÑŒ 2026', 'en': 'January 2026', 'zh': '1æœˆ2026å¹´'};
        let feedDateNext = {'ru': '02.2026', 'en': '02/2026', 'zh': '2æœˆ2026å¹´'};
        let feedDatePrev = {'ru': '12.2025', 'en': '12/2025', 'zh': '12æœˆ2025å¹´'};
        let filterLabel = {'ru': 'Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€', 'en': 'Topics', 'zh': 'ä¸»é¢˜ç­›é€‰'}
        let publishedLabel = {'ru': 'ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ñ‚ ', 'en': 'published on ', 'zh': 'å‘è¡¨äº'}
        let sortLabel = {'ru': 'Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾', 'en': 'Sort by', 'zh': 'æ’åºæ–¹å¼'}
        let paperLabel = {'ru': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ', 'en': 'Paper', 'zh': 'è®ºæ–‡'}
        let topMonthLabel = {'ru': 'ĞœĞµÑÑÑ†', 'en': 'Month', 'zh': 'æœˆåº¦è®ºæ–‡'}
        let topDayLabel = {'ru': 'Ğ”ĞµĞ½ÑŒ', 'en': 'Day', 'zh': 'æ—¥åº¦è®ºæ–‡'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf monthly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2512.24880', 'title': 'mHC: Manifold-Constrained Hyper-Connections', 'url': 'https://huggingface.co/papers/2512.24880', 'abstract': 'Manifold-Constrained Hyper-Connections (mHC) stabilize and scale residual connection architectures by restoring identity mapping properties through manifold projection and infrastructure optimization.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, studies exemplified by Hyper-Connections (HC) have extended the ubiquitous residual connection paradigm established over the past decade by expanding the residual stream width and diversifying connectivity patterns. While yielding substantial performance gains, this diversification fundamentally compromises the identity mapping property intrinsic to the residual connection, which causes severe training instability and restricted scalability, and additionally incurs notable memory access overhead. To address these challenges, we propose Manifold-Constrained Hyper-Connections (mHC), a general framework that projects the residual connection space of HC onto a specific manifold to restore the identity mapping property, while incorporating rigorous infrastructure optimization to ensure efficiency. Empirical experiments demonstrate that mHC is effective for training at scale, offering tangible performance improvements and superior scalability. We anticipate that mHC, as a flexible and practical extension of HC, will contribute to a deeper understanding of topological architecture design and suggest promising directions for the evolution of foundational models.', 'score': 49, 'issue_id': 360, 'pub_date': '2026-12-31', 'pub_date_card': {'ru': '31 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 31', 'zh': '12æœˆ31æ—¥'}, 'hash': '13fd2b75493e93cf', 'authors': ['Zhenda Xie', 'Yixuan Wei', 'Huanqi Cao', 'Chenggang Zhao', 'Chengqi Deng', 'Jiashi Li', 'Damai Dai', 'Huazuo Gao', 'Jiang Chang', 'Liang Zhao', 'Shangyan Zhou', 'Zhean Xu', 'Zhengyan Zhang', 'Wangding Zeng', 'Shengding Hu', 'Yuqing Wang', 'Jingyang Yuan', 'Lean Wang', 'Wenfeng Liang'], 'affiliations': ['DeepSeek-AI'], 'pdf_title_img': 'assets/pdf/title_img/2512.24880.jpg', 'data': {'categories': ['#architecture', '#training', '#optimization'], 'emoji': 'ğŸ—ï¸', 'ru': {'title': 'Ğ’Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ğ¶Ğ´ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ñ… ÑĞµÑ‚ÑÑ… Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ Manifold-Constrained Hyper-Connections (mHC), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑĞ²ÑĞ·ĞµĞ¹ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ° Ñ‚Ğ¾Ğ¶Ğ´ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¾ĞµĞºÑ†Ğ¸Ñ Ğ½Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ° Ğ² Ğ³Ğ¸Ğ¿ĞµÑ€-ÑĞ¾ĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸ÑÑ… Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ½ĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ°Ğ¼ Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒÑ, Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ¼Ñƒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ñ‚ÑŒ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¶ĞµĞ»Ğ°ĞµĞ¼Ñ‹Ñ… ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ². ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¸Ğ½Ñ„Ñ€Ğ°ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½ÑƒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ mHC ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ³Ğ¸Ğ±ĞºĞ¸Ğ¼ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸ĞµĞ¼ Ğ³Ğ¸Ğ¿ĞµÑ€-ÑĞ¾ĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ñ‚Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Stabilizing Residual Connections with Manifold Projections', 'desc': 'This paper introduces Manifold-Constrained Hyper-Connections (mHC), a new framework designed to enhance residual connection architectures in machine learning. mHC addresses the issues of training instability and scalability that arise from diversifying connectivity patterns in Hyper-Connections (HC). By projecting the residual connection space onto a specific manifold, mHC restores the essential identity mapping property, which is crucial for stable training. The proposed method not only improves performance but also optimizes memory access, making it a significant advancement in the design of deep learning models.'}, 'zh': {'title': 'æµå½¢çº¦æŸè¶…è¿æ¥ï¼šæå‡æ®‹å·®è¿æ¥çš„ç¨³å®šæ€§ä¸å¯æ‰©å±•æ€§', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œç§°ä¸ºæµå½¢çº¦æŸè¶…è¿æ¥ï¼ˆmHCï¼‰ï¼Œæ—¨åœ¨ç¨³å®šå’Œæ‰©å±•æ®‹å·®è¿æ¥æ¶æ„ã€‚é€šè¿‡å°†æ®‹å·®è¿æ¥ç©ºé—´æŠ•å½±åˆ°ç‰¹å®šæµå½¢ä¸Šï¼ŒmHC æ¢å¤äº†æ®‹å·®è¿æ¥çš„èº«ä»½æ˜ å°„ç‰¹æ€§ï¼Œä»è€Œè§£å†³äº†è®­ç»ƒä¸ç¨³å®šå’Œå¯æ‰©å±•æ€§å—é™çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•è¿˜ç»“åˆäº†åŸºç¡€è®¾æ–½ä¼˜åŒ–ï¼Œä»¥æé«˜æ•ˆç‡å¹¶å‡å°‘å†…å­˜è®¿é—®å¼€é”€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒmHC åœ¨å¤§è§„æ¨¡è®­ç»ƒä¸­æœ‰æ•ˆï¼Œæä¾›äº†æ˜¾è‘—çš„æ€§èƒ½æå‡å’Œæ›´å¥½çš„å¯æ‰©å±•æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.24618', 'title': 'Youtu-LLM: Unlocking the Native Agentic Potential for Lightweight Large Language Models', 'url': 'https://huggingface.co/papers/2512.24618', 'abstract': 'Youtu-LLM is a lightweight language model optimized for computational efficiency and agentic intelligence through a compact architecture, STEM-focused training curriculum, and scalable mid-training strategies for planning and reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Youtu-LLM, a lightweight yet powerful language model that harmonizes high computational efficiency with native agentic intelligence. Unlike typical small models that rely on distillation, Youtu-LLM (1.96B) is pre-trained from scratch to systematically cultivate reasoning and planning capabilities. The key technical advancements are as follows: (1) Compact Architecture with Long-Context Support: Built on a dense Multi-Latent Attention (MLA) architecture with a novel STEM-oriented vocabulary, Youtu-LLM supports a 128k context window. This design enables robust long-context reasoning and state tracking within a minimal memory footprint, making it ideal for long-horizon agent and reasoning tasks. (2) Principled "Commonsense-STEM-Agent" Curriculum: We curated a massive corpus of approximately 11T tokens and implemented a multi-stage training strategy. By progressively shifting the pre-training data distribution from general commonsense to complex STEM and agentic tasks, we ensure the model acquires deep cognitive abilities rather than superficial alignment. (3) Scalable Agentic Mid-training: Specifically for the agentic mid-training, we employ diverse data construction schemes to synthesize rich and varied trajectories across math, coding, and tool-use domains. This high-quality data enables the model to internalize planning and reflection behaviors effectively. Extensive evaluations show that Youtu-LLM sets a new state-of-the-art for sub-2B LLMs. On general benchmarks, it achieves competitive performance against larger models, while on agent-specific tasks, it significantly surpasses existing SOTA baselines, demonstrating that lightweight models can possess strong intrinsic agentic capabilities.', 'score': 41, 'issue_id': 353, 'pub_date': '2026-12-31', 'pub_date_card': {'ru': '31 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 31', 'zh': '12æœˆ31æ—¥'}, 'hash': 'de67734dd7f71d25', 'authors': ['Junru Lu', 'Jiarui Qin', 'Lingfeng Qiao', 'Yinghui Li', 'Xinyi Dai', 'Bo Ke', 'Jianfeng He', 'Ruizhi Qiao', 'Di Yin', 'Xing Sun', 'Yunsheng Wu', 'Yinsong Liu', 'Shuangyin Liu', 'Mingkong Tang', 'Haodong Lin', 'Jiayi Kuang', 'Fanxu Meng', 'Xiaojuan Tang', 'Yunjia Xi', 'Junjie Huang', 'Haotong Yang', 'Zhenyi Shen', 'Yangning Li', 'Qianwen Zhang', 'Yifei Yu', 'Siyu An', 'Junnan Dong', 'Qiufeng Wang', 'Jie Wang', 'Keyu Chen', 'Wei Wen', 'Taian Guo', 'Zhifeng Shen', 'Daohai Yu', 'Jiahao Li', 'Ke Li', 'Zongyi Li', 'Xiaoyu Tan'], 'affiliations': ['Tencent', 'Youtu-LLM Team'], 'pdf_title_img': 'assets/pdf/title_img/2512.24618.jpg', 'data': {'categories': ['#synthetic', '#long_context', '#agents', '#training', '#small_models', '#architecture', '#reasoning', '#optimization'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ›Ñ‘Ğ³ĞºĞ°Ñ, Ğ½Ğ¾ ÑƒĞ¼Ğ½Ğ°Ñ: ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ Ğ°Ğ³ĞµĞ½Ñ‚ÑĞºĞ¸Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ¾Ğ¼', 'desc': 'Youtu-LLM â€” ÑÑ‚Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ 1.96B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ Ñ Ğ½ÑƒĞ»Ñ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Multi-Latent Attention Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² 128k Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğ¼ STEM-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ‘Ğ¼. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ğ»Ğ¾ÑÑŒ Ğ¿Ğ¾ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğ¹ ÑÑ…ĞµĞ¼Ğµ Ğ½Ğ° 11 Ñ‚Ñ€Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ¿Ğ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ñ Ğ¾Ñ‚ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğº ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼ STEM-Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼ Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚ÑĞºĞ¸Ğ¼ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑĞ¼. ĞĞ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ¼Ğ¸Ğ´Ğ´Ğ»-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞ»Ğ¸ÑÑŒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ¸ Ğ² Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞµ, ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Lightweight Intelligence: Youtu-LLM Redefines Efficiency in Language Models', 'desc': 'Youtu-LLM is a lightweight language model designed for high computational efficiency and enhanced reasoning abilities. It features a unique Multi-Latent Attention architecture that supports long-context processing, allowing it to handle complex tasks with minimal memory use. The model is trained on a diverse dataset that transitions from general knowledge to specialized STEM and agentic tasks, fostering deep cognitive skills. Evaluations reveal that Youtu-LLM outperforms larger models in specific agent-related tasks, proving that smaller models can achieve significant intelligence and performance.'}, 'zh': {'title': 'è½»é‡çº§æ¨¡å‹ï¼Œå¼ºå¤§æ™ºèƒ½ï¼', 'desc': 'Youtu-LLMæ˜¯ä¸€ç§è½»é‡çº§è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨æé«˜è®¡ç®—æ•ˆç‡å’Œæ™ºèƒ½ä»£ç†èƒ½åŠ›ã€‚å®ƒé‡‡ç”¨ç´§å‡‘çš„æ¶æ„å’Œä»¥STEMä¸ºé‡ç‚¹çš„è®­ç»ƒè¯¾ç¨‹ï¼Œä»é›¶å¼€å§‹é¢„è®­ç»ƒï¼Œä»¥åŸ¹å…»æ¨ç†å’Œè§„åˆ’èƒ½åŠ›ã€‚è¯¥æ¨¡å‹æ”¯æŒ128kçš„é•¿ä¸Šä¸‹æ–‡çª—å£ï¼Œé€‚åˆé•¿æ—¶é—´çš„ä»£ç†å’Œæ¨ç†ä»»åŠ¡ã€‚é€šè¿‡å¤šé˜¶æ®µçš„è®­ç»ƒç­–ç•¥ï¼ŒYoutu-LLMåœ¨å¤æ‚çš„STEMå’Œä»£ç†ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œè¯æ˜äº†è½»é‡çº§æ¨¡å‹ä¹Ÿèƒ½å…·å¤‡å¼ºå¤§çš„å†…åœ¨æ™ºèƒ½èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.24873', 'title': 'Let It Flow: Agentic Crafting on Rock and Roll, Building the ROME Model within an Open Agentic Learning Ecosystem', 'url': 'https://huggingface.co/papers/2512.24873', 'abstract': 'The Agentic Learning Ecosystem (ALE) introduces a principled infrastructure for agent development, combining post-training optimization, sandbox environments, and policy alignment to enhance long-horizon training stability and performance in real-world tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Agentic crafting requires LLMs to operate in real-world environments over multiple turns by taking actions, observing outcomes, and iteratively refining artifacts. Despite its importance, the open-source community lacks a principled, end-to-end ecosystem to streamline agent development. We introduce the Agentic Learning Ecosystem (ALE), a foundational infrastructure that optimizes the production pipeline for agent LLMs. ALE consists of three components: ROLL, a post-training framework for weight optimization; ROCK, a sandbox environment manager for trajectory generation; and iFlow CLI, an agent framework for efficient context engineering. We release ROME (ROME is Obviously an Agentic Model), an open-source agent grounded by ALE and trained on over one million trajectories. Our approach includes data composition protocols for synthesizing complex behaviors and a novel policy optimization algorithm, Interaction-based Policy Alignment (IPA), which assigns credit over semantic interaction chunks rather than individual tokens to improve long-horizon training stability. Empirically, we evaluate ROME within a structured setting and introduce Terminal Bench Pro, a benchmark with improved scale and contamination control. ROME demonstrates strong performance across benchmarks like SWE-bench Verified and Terminal Bench, proving the effectiveness of the ALE infrastructure.', 'score': 32, 'issue_id': 353, 'pub_date': '2026-12-31', 'pub_date_card': {'ru': '31 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 31', 'zh': '12æœˆ31æ—¥'}, 'hash': '2b323e652f7b8183', 'authors': ['Weixun Wang', 'XiaoXiao Xu', 'Wanhe An', 'Fangwen Dai', 'Wei Gao', 'Yancheng He', 'Ju Huang', 'Qiang Ji', 'Hanqi Jin', 'Xiaoyang Li', 'Yang Li', 'Zhongwen Li', 'Shirong Lin', 'Jiashun Liu', 'Zenan Liu', 'Tao Luo', 'Dilxat Muhtar', 'Yuanbin Qu', 'Jiaqiang Shi', 'Qinghui Sun', 'Yingshui Tan', 'Hao Tang', 'Runze Wang', 'Yi Wang', 'Zhaoguo Wang', 'Yanan Wu', 'Shaopan Xiong', 'Binchen Xu', 'Xander Xu', 'Yuchi Xu', 'Qipeng Zhang', 'Xixia Zhang', 'Haizhou Zhao', 'Jie Zhao', 'Shuaibing Zhao', 'Baihui Zheng', 'Jianhui Zheng', 'Suhang Zheng', 'Yanni Zhu', 'Mengze Cai', 'Kerui Cao', 'Xitong Chen', 'Yue Dai', 'Lifan Du', 'Tao Feng', 'Tao He', 'Jin Hu', 'Yijie Hu', 'Ziyu Jiang', 'Cheng Li', 'Xiang Li', 'Jing Liang', 'Chonghuan Liu', 'ZhenDong Liu', 'Haodong Mi', 'Yanhu Mo', 'Junjia Ni', 'Shixin Pei', 'Jingyu Shen', 'XiaoShuai Song', 'Cecilia Wang', 'Chaofan Wang', 'Kangyu Wang', 'Pei Wang', 'Tao Wang', 'Wei Wang', 'Ke Xiao', 'Mingyu Xu', 'Tiange Xu', 'Nan Ya', 'Siran Yang', 'Jianan Ye', 'Yaxing Zang', 'Duo Zhang', 'Junbo Zhang', 'Boren Zheng', 'Wanxi Deng', 'Ling Pan', 'Lin Qu', 'Wenbo Su', 'Jiamang Wang', 'Wei Wang', 'Hu Wei', 'Minggang Wu', 'Cheng Yu', 'Bing Zhao', 'Zhicheng Zheng', 'Bo Zheng'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2512.24873.jpg', 'data': {'categories': ['#alignment', '#open_source', '#synthetic', '#dataset', '#agents', '#training', '#rlhf', '#optimization', '#benchmark'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ­ĞºĞ¾ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Agentic Learning Ecosystem (ALE) â€” ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½ÑƒÑ Ğ¸Ğ½Ñ„Ñ€Ğ°ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLM, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ROLL Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²ĞµÑĞ¾Ğ², Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ ROCK Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ¸ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº iFlow Ğ´Ğ»Ñ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Interaction-based Policy Alignment (IPA), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡Ğ°ĞµÑ‚ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ±Ğ»Ğ¾ĞºĞ°Ğ¼ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ğ°Ñ… Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ’ Ñ€Ğ°Ğ¼ĞºĞ°Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ ROME â€” Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹, Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Terminal Bench Pro Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ¸ ĞºĞ¾Ğ½Ñ‚Ğ°Ğ¼Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… SWE-bench Verified Ğ¸ Terminal Bench Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ñ€Ğ°ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ°.'}, 'en': {'title': 'Streamlining Agent Development with ALE', 'desc': "The Agentic Learning Ecosystem (ALE) provides a structured framework for developing AI agents, particularly large language models (LLMs), to perform tasks in real-world settings. It integrates three key components: ROLL for optimizing model weights post-training, ROCK for managing sandbox environments to generate action trajectories, and iFlow CLI for enhancing context management. The system introduces a novel policy optimization method called Interaction-based Policy Alignment (IPA), which improves training stability by focusing on meaningful interactions rather than individual data points. The open-source agent ROME, built on ALE, showcases significant performance improvements across various benchmarks, demonstrating the framework's effectiveness in agent development."}, 'zh': {'title': 'æ™ºèƒ½ä½“å¼€å‘çš„æ–°åŸºç¡€è®¾æ–½ï¼šAgentic Learning Ecosystem', 'desc': 'Agentic Learning Ecosystem (ALE) æ˜¯ä¸€ä¸ªä¸ºæ™ºèƒ½ä½“å¼€å‘æä¾›çš„åŸºç¡€è®¾æ–½ï¼Œæ—¨åœ¨é€šè¿‡åè®­ç»ƒä¼˜åŒ–ã€æ²™ç›’ç¯å¢ƒå’Œç­–ç•¥å¯¹é½æ¥æé«˜é•¿æ—¶é—´è®­ç»ƒçš„ç¨³å®šæ€§å’Œæ€§èƒ½ã€‚è¯¥ç³»ç»ŸåŒ…æ‹¬ä¸‰ä¸ªä¸»è¦ç»„ä»¶ï¼šROLLç”¨äºæƒé‡ä¼˜åŒ–çš„åè®­ç»ƒæ¡†æ¶ï¼ŒROCKç”¨äºè½¨è¿¹ç”Ÿæˆçš„æ²™ç›’ç¯å¢ƒç®¡ç†å™¨ï¼Œä»¥åŠiFlow CLIç”¨äºé«˜æ•ˆä¸Šä¸‹æ–‡å·¥ç¨‹çš„æ™ºèƒ½ä½“æ¡†æ¶ã€‚æˆ‘ä»¬è¿˜å‘å¸ƒäº†ROMEï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºALEçš„å¼€æºæ™ºèƒ½ä½“ï¼Œç»è¿‡è¶…è¿‡ä¸€ç™¾ä¸‡æ¡è½¨è¿¹çš„è®­ç»ƒã€‚é€šè¿‡å¼•å…¥åŸºäºäº¤äº’çš„ç­–ç•¥å¯¹é½ç®—æ³•ï¼ˆIPAï¼‰ï¼Œæˆ‘ä»¬èƒ½å¤Ÿåœ¨å¤æ‚è¡Œä¸ºåˆæˆå’Œé•¿æ—¶é—´è®­ç»ƒç¨³å®šæ€§æ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.25073', 'title': 'GaMO: Geometry-aware Multi-view Diffusion Outpainting for Sparse-View 3D Reconstruction', 'url': 'https://huggingface.co/papers/2512.25073', 'abstract': 'GaMO enhances sparse-view 3D reconstruction by using geometry-aware multi-view outpainting to improve scene coverage and consistency, achieving state-of-the-art performance with reduced computational cost.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in 3D reconstruction have achieved remarkable progress in high-quality scene capture from dense multi-view imagery, yet struggle when input views are limited. Various approaches, including regularization techniques, semantic priors, and geometric constraints, have been implemented to address this challenge. Latest diffusion-based methods have demonstrated substantial improvements by generating novel views from new camera poses to augment training data, surpassing earlier regularization and prior-based techniques. Despite this progress, we identify three critical limitations in these state-of-the-art approaches: inadequate coverage beyond known view peripheries, geometric inconsistencies across generated views, and computationally expensive pipelines. We introduce GaMO (Geometry-aware Multi-view Outpainter), a framework that reformulates sparse-view reconstruction through multi-view outpainting. Instead of generating new viewpoints, GaMO expands the field of view from existing camera poses, which inherently preserves geometric consistency while providing broader scene coverage. Our approach employs multi-view conditioning and geometry-aware denoising strategies in a zero-shot manner without training. Extensive experiments on Replica and ScanNet++ demonstrate state-of-the-art reconstruction quality across 3, 6, and 9 input views, outperforming prior methods in PSNR and LPIPS, while achieving a 25times speedup over SOTA diffusion-based methods with processing time under 10 minutes. Project page: https://yichuanh.github.io/GaMO/', 'score': 21, 'issue_id': 358, 'pub_date': '2026-12-31', 'pub_date_card': {'ru': '31 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 31', 'zh': '12æœˆ31æ—¥'}, 'hash': 'a90af97d4a64b013', 'authors': ['Yi-Chuan Huang', 'Hao-Jen Chien', 'Chin-Yang Lin', 'Ying-Huan Chen', 'Yu-Lun Liu'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2512.25073.jpg', 'data': {'categories': ['#3d', '#diffusion', '#optimization', '#multimodal'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ»Ñ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ²Ğ¸Ğ´Ğ¾Ğ² Ğ´Ğ»Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½', 'desc': 'GaMO â€” ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½ Ğ¸Ğ· Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ğ¸Ğ´Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ²Ğ¸Ğ´Ğ¾Ğ²ÑƒÑ Ğ´Ğ¾Ğ¿Ğ°Ğ¸Ğ²ĞºÑƒ (outpainting) Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ‚Ğ¾Ñ‡ĞµĞº Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ°. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»Ñ Ğ·Ñ€ĞµĞ½Ğ¸Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… ĞºĞ°Ğ¼ĞµÑ€, Ñ‡Ñ‚Ğ¾ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ¼ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¸Ğ´Ğ°Ğ¼Ğ¸ Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğµ Ğ¿Ğ¾ĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ ÑÑ†ĞµĞ½Ñ‹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸-Ğ¾ÑĞ²ĞµĞ´Ğ¾Ğ¼Ğ»Ñ‘Ğ½Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ´ĞµĞ½Ğ¾Ğ¹Ğ·Ğ¸Ğ½Ğ³Ğ° Ğ² Ğ½ÑƒĞ»ĞµĞ²Ğ¾Ğ¼ Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ (Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ) Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ² 25 Ñ€Ğ°Ğ· Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ². ĞĞ° ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Replica Ğ¸ ScanNet++ GaMO Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼ PSNR Ğ¸ LPIPS Ğ¿Ñ€Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¼ĞµĞ½ĞµĞµ 10 Ğ¼Ğ¸Ğ½ÑƒÑ‚.'}, 'en': {'title': 'Enhancing 3D Reconstruction with Geometry-Aware Outpainting', 'desc': 'GaMO (Geometry-aware Multi-view Outpainter) is a novel framework designed to enhance sparse-view 3D reconstruction by improving scene coverage and consistency. It achieves this by expanding the field of view from existing camera positions rather than generating new viewpoints, which helps maintain geometric accuracy. The method utilizes multi-view conditioning and geometry-aware denoising techniques in a zero-shot manner, eliminating the need for extensive training. Experimental results show that GaMO outperforms previous methods in reconstruction quality while significantly reducing computational costs, achieving a 25 times speedup over state-of-the-art diffusion-based techniques.'}, 'zh': {'title': 'GaMOï¼šæå‡ç¨€ç–è§†å›¾3Dé‡å»ºçš„å‡ ä½•æ„ŸçŸ¥æ–¹æ³•', 'desc': 'GaMOï¼ˆå‡ ä½•æ„ŸçŸ¥å¤šè§†å›¾å¤–æ¨å™¨ï¼‰é€šè¿‡å¤šè§†å›¾å¤–æ¨æŠ€æœ¯å¢å¼ºç¨€ç–è§†å›¾çš„3Dé‡å»ºï¼Œæ”¹å–„äº†åœºæ™¯è¦†ç›–å’Œä¸€è‡´æ€§ã€‚è¯¥æ–¹æ³•ä¸å†ç”Ÿæˆæ–°è§†ç‚¹ï¼Œè€Œæ˜¯æ‰©å±•ç°æœ‰ç›¸æœºä½ç½®çš„è§†é‡ï¼Œä»è€Œä¿æŒå‡ ä½•ä¸€è‡´æ€§å¹¶æä¾›æ›´å¹¿æ³›çš„åœºæ™¯è¦†ç›–ã€‚GaMOé‡‡ç”¨å¤šè§†å›¾æ¡ä»¶å’Œå‡ ä½•æ„ŸçŸ¥å»å™ªç­–ç•¥ï¼Œä»¥é›¶æ ·æœ¬æ–¹å¼è¿›è¡Œï¼Œæ— éœ€è®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGaMOåœ¨é‡å»ºè´¨é‡ä¸Šè¶…è¶Šäº†ä¹‹å‰çš„æ–¹æ³•ï¼Œå¹¶åœ¨å¤„ç†é€Ÿåº¦ä¸Šå®ç°äº†æ˜¾è‘—æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.23380', 'title': 'A unified framework for detecting point and collective anomalies in operating system logs via collaborative transformers', 'url': 'https://huggingface.co/papers/2512.23380', 'abstract': "CoLog, a log anomaly detection framework, employs collaborative transformers and multi-head impressed attention with a modality adaptation layer to achieve high-precision detection of both point and collective anomalies across diverse log modalities.  \t\t\t\t\tAI-generated summary \t\t\t\t Log anomaly detection is crucial for preserving the security of operating systems. Depending on the source of log data collection, various information is recorded in logs that can be considered log modalities. In light of this intuition, unimodal methods often struggle by ignoring the different modalities of log data. Meanwhile, multimodal methods fail to handle the interactions between these modalities. Applying multimodal sentiment analysis to log anomaly detection, we propose CoLog, a framework that collaboratively encodes logs utilizing various modalities. CoLog utilizes collaborative transformers and multi-head impressed attention to learn interactions among several modalities, ensuring comprehensive anomaly detection. To handle the heterogeneity caused by these interactions, CoLog incorporates a modality adaptation layer, which adapts the representations from different log modalities. This methodology enables CoLog to learn nuanced patterns and dependencies within the data, enhancing its anomaly detection capabilities. Extensive experiments demonstrate CoLog's superiority over existing state-of-the-art methods. Furthermore, in detecting both point and collective anomalies, CoLog achieves a mean precision of 99.63%, a mean recall of 99.59%, and a mean F1 score of 99.61% across seven benchmark datasets for log-based anomaly detection. The comprehensive detection capabilities of CoLog make it highly suitable for cybersecurity, system monitoring, and operational efficiency. CoLog represents a significant advancement in log anomaly detection, providing a sophisticated and effective solution to point and collective anomaly detection through a unified framework and a solution to the complex challenges automatic log data analysis poses. We also provide the implementation of CoLog at https://github.com/NasirzadehMoh/CoLog.", 'score': 17, 'issue_id': 366, 'pub_date': '2026-12-29', 'pub_date_card': {'ru': '29 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 29', 'zh': '12æœˆ29æ—¥'}, 'hash': '60dd192b220eb6aa', 'authors': ['Mohammad Nasirzadeh', 'Jafar Tahmoresnezhad', 'Parviz Rashidi-Khazaee'], 'affiliations': ['Urmia University of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2512.23380.jpg', 'data': {'categories': ['#open_source', '#architecture', '#benchmark', '#security', '#multimodal'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ¡Ğ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ»Ğ¾Ğ³Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹', 'desc': 'CoLog â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹ Ğ² Ğ»Ğ¾Ğ³Ğ°Ñ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸Ñ‡Ğ°ÑÑ‰Ğ¸Ğ¼Ğ¸ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°Ğ¼Ğ¸ Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ³Ğ¾Ğ»Ğ¾Ğ²Ñ‹Ğ¼ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ»Ğ¾Ğ³Ğ¾Ğ² (Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ñ‚Ğ¸Ğ¿Ñ‹ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…) Ğ¸ Ğ²Ñ‹ÑĞ²Ğ»ÑÑ‚ÑŒ ĞºĞ°Ğº Ñ‚Ğ¾Ñ‡ĞµÑ‡Ğ½Ñ‹Ğµ, Ñ‚Ğ°Ğº Ğ¸ ĞºĞ¾Ğ»Ğ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¸. Ğ”Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğº Ğ³ĞµÑ‚ĞµÑ€Ğ¾Ğ³ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞ»Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. ĞĞ° ÑĞµĞ¼Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº-Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ… CoLog Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ğ¿ĞµÑ‡Ğ°Ñ‚Ğ»ÑÑÑ‰Ğ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ²: Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ 99.63%, Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚Ñƒ 99.59% Ğ¸ F1-Ğ¼ĞµÑ€Ñƒ 99.61%.'}, 'en': {'title': 'CoLog: Revolutionizing Log Anomaly Detection with Collaborative Transformers', 'desc': 'CoLog is a log anomaly detection framework that utilizes collaborative transformers and multi-head impressed attention to effectively identify both point and collective anomalies in diverse log modalities. It addresses the limitations of unimodal and multimodal methods by incorporating a modality adaptation layer, which helps in managing the interactions between different log data types. This innovative approach allows CoLog to learn complex patterns and dependencies, significantly improving its detection accuracy. With impressive performance metrics, including a mean precision of 99.63%, CoLog stands out as a powerful tool for enhancing cybersecurity and operational efficiency.'}, 'zh': {'title': 'CoLogï¼šé«˜æ•ˆçš„æ—¥å¿—å¼‚å¸¸æ£€æµ‹æ¡†æ¶', 'desc': 'CoLogæ˜¯ä¸€ä¸ªæ—¥å¿—å¼‚å¸¸æ£€æµ‹æ¡†æ¶ï¼Œé‡‡ç”¨åä½œå˜æ¢å™¨å’Œå¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼Œç»“åˆæ¨¡æ€é€‚åº”å±‚ï¼Œå®ç°å¯¹å¤šç§æ—¥å¿—æ¨¡æ€ä¸­ç‚¹å¼‚å¸¸å’Œé›†ä½“å¼‚å¸¸çš„é«˜ç²¾åº¦æ£€æµ‹ã€‚è¯¥æ¡†æ¶é€šè¿‡ååŒç¼–ç ä¸åŒæ¨¡æ€çš„æ—¥å¿—ï¼Œå…‹æœäº†å•æ¨¡æ€æ–¹æ³•å¿½è§†æ—¥å¿—æ•°æ®å¤šæ ·æ€§çš„é—®é¢˜ã€‚CoLogèƒ½å¤Ÿå­¦ä¹ æ¨¡æ€ä¹‹é—´çš„äº¤äº’ï¼Œé€‚åº”ä¸åŒæ—¥å¿—æ¨¡æ€çš„è¡¨ç¤ºï¼Œä»è€Œæé«˜å¼‚å¸¸æ£€æµ‹çš„èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCoLogåœ¨ä¸ƒä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå¹³å‡ç²¾åº¦è¾¾åˆ°99.63%ï¼Œä¸ºç½‘ç»œå®‰å…¨å’Œç³»ç»Ÿç›‘æ§æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.25070', 'title': 'Scaling Open-Ended Reasoning to Predict the Future', 'url': 'https://huggingface.co/papers/2512.25070', 'abstract': 'High-stakes decision making involves reasoning under uncertainty about the future. In this work, we train language models to make predictions on open-ended forecasting questions. To scale up training data, we synthesize novel forecasting questions from global events reported in daily news, using a fully automated, careful curation recipe. We train the Qwen3 thinking models on our dataset, OpenForesight. To prevent leakage of future information during training and evaluation, we use an offline news corpus, both for data generation and retrieval in our forecasting system. Guided by a small validation set, we show the benefits of retrieval, and an improved reward function for reinforcement learning (RL). Once we obtain our final forecasting system, we perform held-out testing between May to August 2025. Our specialized model, OpenForecaster 8B, matches much larger proprietary models, with our training improving the accuracy, calibration, and consistency of predictions. We find calibration improvements from forecasting training generalize across popular benchmarks. We open-source all our models, code, and data to make research on language model forecasting broadly accessible.', 'score': 12, 'issue_id': 353, 'pub_date': '2026-12-31', 'pub_date_card': {'ru': '31 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 31', 'zh': '12æœˆ31æ—¥'}, 'hash': '3b992c58072db0d3', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#optimization', '#open_source', '#dataset', '#training', '#leakage', '#small_models', '#reasoning', '#rl', '#synthetic', '#data', '#benchmark'], 'emoji': 'ğŸ”®', 'ru': {'title': 'Ğ¯Ğ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ°Ğº ÑĞºÑĞ¿ĞµÑ€Ñ‚Ñ‹ Ğ¿Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ±ÑƒĞ´ÑƒÑ‰ĞµĞ³Ğ¾', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´ĞµĞ»Ğ°Ñ‚ÑŒ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ OpenForesight Ñ Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ¸Ğ· Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹. Ğ”Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ ÑƒÑ‚ĞµÑ‡ĞºĞ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ Ğ±ÑƒĞ´ÑƒÑ‰ĞµĞ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ğ¹ ĞºĞ¾Ñ€Ğ¿ÑƒÑ Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚ĞµĞ¹ ĞºĞ°Ğº Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ğµ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ OpenForecaster 8B Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ reinforcement learning Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ¹ Ñ„ÑƒĞ½ĞºÑ†Ğ¸ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ, ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºÑƒ Ğ¸ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ»Ğ¸ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ´, Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ñ‹, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ»Ñ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾Ğ³Ğ¾ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ğ°.'}, 'en': {'title': 'Empowering Predictions: OpenForesight for Uncertain Futures', 'desc': 'This paper focuses on enhancing decision-making in uncertain situations by training language models to predict outcomes for open-ended forecasting questions. The authors create a large dataset called OpenForesight by synthesizing forecasting questions from global news events, ensuring that future information does not leak during training. They utilize reinforcement learning with an improved reward function and a validation set to refine their forecasting system. The resulting model, OpenForecaster 8B, demonstrates competitive performance against larger models, showing better accuracy and consistency in predictions, and the authors make their resources available for further research.'}, 'zh': {'title': 'å¼€æ”¾å¼é¢„æµ‹ï¼šæå‡è¯­è¨€æ¨¡å‹çš„å†³ç­–èƒ½åŠ›', 'desc': 'æœ¬ç ”ç©¶æ—¨åœ¨è®­ç»ƒè¯­è¨€æ¨¡å‹ä»¥åº”å¯¹é«˜é£é™©å†³ç­–ä¸­çš„ä¸ç¡®å®šæ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å¼€æ”¾å¼é¢„æµ‹é—®é¢˜ä¸Šã€‚æˆ‘ä»¬é€šè¿‡ä»æ¯æ—¥æ–°é—»ä¸­æå–å…¨çƒäº‹ä»¶ï¼Œè‡ªåŠ¨åˆæˆæ–°çš„é¢„æµ‹é—®é¢˜ï¼Œä»¥æ‰©å¤§è®­ç»ƒæ•°æ®é›†ã€‚æˆ‘ä»¬ä½¿ç”¨ç¦»çº¿æ–°é—»è¯­æ–™åº“æ¥é˜²æ­¢åœ¨è®­ç»ƒå’Œè¯„ä¼°è¿‡ç¨‹ä¸­æ³„éœ²æœªæ¥ä¿¡æ¯ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šè®­ç»ƒæˆ‘ä»¬çš„é¢„æµ‹ç³»ç»Ÿã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬çš„ä¸“ç”¨æ¨¡å‹OpenForecaster 8Båœ¨å‡†ç¡®æ€§ã€æ ¡å‡†å’Œä¸€è‡´æ€§æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œèƒ½å¤Ÿä¸æ›´å¤§è§„æ¨¡çš„ä¸“æœ‰æ¨¡å‹ç›¸åª²ç¾ã€‚'}}, 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2512.24551', 'title': 'PhyGDPO: Physics-Aware Groupwise Direct Preference Optimization for Physically Consistent Text-to-Video Generation', 'url': 'https://huggingface.co/papers/2512.24551', 'abstract': 'Recent advances in text-to-video (T2V) generation have achieved good visual quality, yet synthesizing videos that faithfully follow physical laws remains an open challenge. Existing methods mainly based on graphics or prompt extension struggle to generalize beyond simple simulated environments or learn implicit physical reasoning. The scarcity of training data with rich physics interactions and phenomena is also a problem. In this paper, we first introduce a Physics-Augmented video data construction Pipeline, PhyAugPipe, that leverages a vision-language model (VLM) with chain-of-thought reasoning to collect a large-scale training dataset, PhyVidGen-135K. Then we formulate a principled Physics-aware Groupwise Direct Preference Optimization, PhyGDPO, framework that builds upon the groupwise Plackett-Luce probabilistic model to capture holistic preferences beyond pairwise comparisons. In PhyGDPO, we design a Physics-Guided Rewarding (PGR) scheme that embeds VLM-based physics rewards to steer optimization toward physical consistency. We also propose a LoRA-Switch Reference (LoRA-SR) scheme that eliminates memory-heavy reference duplication for efficient training. Experiments show that our method significantly outperforms state-of-the-art open-source methods on PhyGenBench and VideoPhy2. Please check our project page at https://caiyuanhao1998.github.io/project/PhyGDPO for more video results. Our code, models, and data will be released at https://github.com/caiyuanhao1998/Open-PhyGDPO', 'score': 12, 'issue_id': 353, 'pub_date': '2026-12-31', 'pub_date_card': {'ru': '31 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 31', 'zh': '12æœˆ31æ—¥'}, 'hash': '581846a1ed6df832', 'authors': ['Yuanhao Cai', 'Kunpeng Li', 'Menglin Jia', 'Jialiang Wang', 'Junzhe Sun', 'Feng Liang', 'Weifeng Chen', 'Felix Juefei-Xu', 'Chu Wang', 'Ali Thabet', 'Xiaoliang Dai', 'Xuan Ju', 'Alan Yuille', 'Ji Hou'], 'affiliations': ['CUHK', 'Johns Hopkins University', 'Meta BizAI', 'Meta Superintelligence Labs'], 'pdf_title_img': 'assets/pdf/title_img/2512.24551.jpg', 'data': {'categories': ['#multimodal', '#dataset', '#training', '#rlhf', '#video', '#data'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ¤Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾ÑĞ¾Ğ·Ğ½Ğ°Ğ½Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ° Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹', 'desc': 'Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ñ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ PhyVidGen-135K, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ vision-language model Ğ¸ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºÑƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑĞ±Ğ¾Ñ€Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑĞ¼Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ PhyGDPO, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Plackett-Luce Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑ‡Ñ‘Ñ‚Ğ° Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´Ğ°ÑÑ‰ÑƒÑ ÑÑ…ĞµĞ¼Ñƒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ vision-language model Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ LoRA-Switch Ğ´Ğ»Ñ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğº Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸.'}, 'en': {'title': 'Generating Physically Consistent Videos with Advanced Optimization', 'desc': 'This paper addresses the challenge of generating videos that adhere to physical laws in text-to-video (T2V) generation. The authors introduce a new data construction pipeline called PhyAugPipe, which uses a vision-language model to create a large dataset with rich physics interactions. They also propose a novel optimization framework, PhyGDPO, that incorporates a physics-guided rewarding system to enhance the physical consistency of generated videos. Experimental results demonstrate that their approach significantly improves performance compared to existing methods.'}, 'zh': {'title': 'ç‰©ç†ä¸€è‡´æ€§çš„è§†é¢‘ç”Ÿæˆæ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§ç‰©ç†å¢å¼ºçš„è§†é¢‘æ•°æ®æ„å»ºç®¡é“PhyAugPipeï¼Œåˆ©ç”¨è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å’Œæ€ç»´é“¾æ¨ç†æ¥æ”¶é›†å¤§è§„æ¨¡è®­ç»ƒæ•°æ®é›†PhyVidGen-135Kã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§åŸºäºç‰©ç†çš„ç¾¤ä½“ç›´æ¥åå¥½ä¼˜åŒ–æ¡†æ¶PhyGDPOï¼Œæ—¨åœ¨æ•æ‰æ•´ä½“åå¥½å¹¶è¶…è¶Šç®€å•çš„æˆå¯¹æ¯”è¾ƒã€‚PhyGDPOä¸­è®¾è®¡çš„ç‰©ç†å¼•å¯¼å¥–åŠ±æœºåˆ¶ï¼ˆPGRï¼‰é€šè¿‡åµŒå…¥VLMåŸºç¡€çš„ç‰©ç†å¥–åŠ±æ¥å¼•å¯¼ä¼˜åŒ–ï¼Œç¡®ä¿ç”Ÿæˆè§†é¢‘çš„ç‰©ç†ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºçš„LoRA-Switch Referenceæ–¹æ¡ˆæœ‰æ•ˆå‡å°‘äº†å†…å­˜å ç”¨ï¼Œæé«˜äº†è®­ç»ƒæ•ˆç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.24210', 'title': 'GR-Dexter Technical Report', 'url': 'https://huggingface.co/papers/2512.24210', 'abstract': 'GR-Dexter introduces a hardware-model-data framework for bimanual dexterous-hand robot manipulation using vision-language-action models, combining teleoperation data and multimodal datasets to achieve robust generalization.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-language-action (VLA) models have enabled language-conditioned, long-horizon robot manipulation, but most existing systems are limited to grippers. Scaling VLA policies to bimanual robots with high degree-of-freedom (DoF) dexterous hands remains challenging due to the expanded action space, frequent hand-object occlusions, and the cost of collecting real-robot data. We present GR-Dexter, a holistic hardware-model-data framework for VLA-based generalist manipulation on a bimanual dexterous-hand robot. Our approach combines the design of a compact 21-DoF robotic hand, an intuitive bimanual teleoperation system for real-robot data collection, and a training recipe that leverages teleoperated robot trajectories together with large-scale vision-language and carefully curated cross-embodiment datasets. Across real-world evaluations spanning long-horizon everyday manipulation and generalizable pick-and-place, GR-Dexter achieves strong in-domain performance and improved robustness to unseen objects and unseen instructions. We hope GR-Dexter serves as a practical step toward generalist dexterous-hand robotic manipulation.', 'score': 11, 'issue_id': 353, 'pub_date': '2026-12-30', 'pub_date_card': {'ru': '30 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 30', 'zh': '12æœˆ30æ—¥'}, 'hash': '999fb5bf63af5286', 'authors': ['Ruoshi Wen', 'Guangzeng Chen', 'Zhongren Cui', 'Min Du', 'Yang Gou', 'Zhigang Han', 'Liqun Huang', 'Mingyu Lei', 'Yunfei Li', 'Zhuohang Li', 'Wenlei Liu', 'Yuxiao Liu', 'Xiao Ma', 'Hao Niu', 'Yutao Ouyang', 'Zeyu Ren', 'Haixin Shi', 'Wei Xu', 'Haoxiang Zhang', 'Jiajun Zhang', 'Xiao Zhang', 'Liwei Zheng', 'Weiheng Zhong', 'Yifei Zhou', 'Zhengming Zhu', 'Hang Li'], 'affiliations': ['ByteDance'], 'pdf_title_img': 'assets/pdf/title_img/2512.24210.jpg', 'data': {'categories': ['#robotics', '#dataset', '#multimodal', '#training'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞĞ±ÑƒÑ‡Ğ°ĞµĞ¼Ğ°Ñ Ğ»Ğ¾Ğ²ĞºĞ¾ÑÑ‚ÑŒ: ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ´Ğ²ÑƒÑ€ÑƒÑ‡Ğ½Ğ°Ñ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸', 'desc': 'GR-Dexter Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ°Ğ¿Ğ¿Ğ°Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ, Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ²ÑƒÑ€ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°-Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ° Ñ Ğ»Ğ¾Ğ²ĞºĞ¸Ğ¼Ğ¸ ĞºĞ¸ÑÑ‚ÑĞ¼Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ñ‚ĞµĞ»ĞµĞ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ğ¼Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ĞºÑ€Ğ¾ÑÑ-Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰Ñ‘Ğ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½ÑƒÑ 21-ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½ÑƒÑ ĞºĞ¸ÑÑ‚ÑŒ, Ğ¸Ğ½Ñ‚ÑƒĞ¸Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ñ‚ĞµĞ»ĞµĞ¾Ğ¿Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑĞ±Ğ¾Ñ€Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ€ĞµÑ†ĞµĞ¿Ñ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹ Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑĞ¸Ğ»ÑŒĞ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¾ Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğº Ğ½ĞµĞ²Ğ¸Ğ´Ğ¸Ğ¼Ñ‹Ğ¼ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼ Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ´Ğ¾Ğ»Ğ³Ğ¾Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ğ½Ñ‹Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹.'}, 'en': {'title': 'Empowering Bimanual Robots with Vision-Language Action!', 'desc': 'GR-Dexter is a framework designed to enhance the manipulation capabilities of bimanual dexterous-hand robots using vision-language-action (VLA) models. It addresses challenges such as the complex action space and occlusions that arise with high degree-of-freedom robotic hands. By integrating teleoperation data with multimodal datasets, GR-Dexter enables robust generalization in robot manipulation tasks. The framework demonstrates strong performance in real-world scenarios, making it a significant advancement in the field of robotic manipulation.'}, 'zh': {'title': 'GR-Dexterï¼šåŒæ‰‹çµå·§æœºå™¨äººæ“ä½œçš„æ–°çªç ´', 'desc': 'GR-Dexteræå‡ºäº†ä¸€ç§ç¡¬ä»¶-æ¨¡å‹-æ•°æ®æ¡†æ¶ï¼Œç”¨äºåŒæ‰‹çµå·§æœºå™¨äººæ“ä½œï¼Œç»“åˆäº†è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ã€‚è¯¥æ¡†æ¶é€šè¿‡æ•´åˆé¥æ“ä½œæ•°æ®å’Œå¤šæ¨¡æ€æ•°æ®é›†ï¼Œå®ç°äº†å¯¹å¤æ‚æ“ä½œçš„å¼ºå¤§æ³›åŒ–èƒ½åŠ›ã€‚GR-Dexterçš„è®¾è®¡åŒ…æ‹¬ä¸€ä¸ªç´§å‡‘çš„21è‡ªç”±åº¦æœºå™¨äººæ‰‹å’Œç›´è§‚çš„åŒæ‰‹é¥æ“ä½œç³»ç»Ÿï¼Œä»¥ä¾¿æ”¶é›†çœŸå®æœºå™¨äººæ•°æ®ã€‚é€šè¿‡åœ¨çœŸå®ç¯å¢ƒä¸­çš„è¯„ä¼°ï¼ŒGR-Dexteråœ¨æ—¥å¸¸æ“ä½œå’Œå¯æ³›åŒ–çš„æŠ“å–-æ”¾ç½®ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå¢å¼ºäº†å¯¹æœªçŸ¥ç‰©ä½“å’ŒæŒ‡ä»¤çš„é²æ£’æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.23343', 'title': 'AI Meets Brain: Memory Systems from Cognitive Neuroscience to Autonomous Agents', 'url': 'https://huggingface.co/papers/2512.23343', 'abstract': 'Memory serves as the pivotal nexus bridging past and future, providing both humans and AI systems with invaluable concepts and experience to navigate complex tasks. Recent research on autonomous agents has increasingly focused on designing efficient memory workflows by drawing on cognitive neuroscience. However, constrained by interdisciplinary barriers, existing works struggle to assimilate the essence of human memory mechanisms. To bridge this gap, we systematically synthesizes interdisciplinary knowledge of memory, connecting insights from cognitive neuroscience with LLM-driven agents. Specifically, we first elucidate the definition and function of memory along a progressive trajectory from cognitive neuroscience through LLMs to agents. We then provide a comparative analysis of memory taxonomy, storage mechanisms, and the complete management lifecycle from both biological and artificial perspectives. Subsequently, we review the mainstream benchmarks for evaluating agent memory. Additionally, we explore memory security from dual perspectives of attack and defense. Finally, we envision future research directions, with a focus on multimodal memory systems and skill acquisition.', 'score': 10, 'issue_id': 358, 'pub_date': '2026-12-29', 'pub_date_card': {'ru': '29 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 29', 'zh': '12æœˆ29æ—¥'}, 'hash': 'c907d7b6498bd21c', 'authors': ['Jiafeng Liang', 'Hao Li', 'Chang Li', 'Jiaqi Zhou', 'Shixin Jiang', 'Zekun Wang', 'Changkai Ji', 'Zhihao Zhu', 'Runxuan Liu', 'Tao Ren', 'Jinlan Fu', 'See-Kiong Ng', 'Xia Liang', 'Ming Liu', 'Bing Qin'], 'affiliations': ['Fudan University, China', 'Harbin Institute of Technology, China', 'National University of Singapore, Singapore', 'Peking University, China'], 'pdf_title_img': 'assets/pdf/title_img/2512.23343.jpg', 'data': {'categories': ['#agents', '#benchmark', '#multimodal'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞŸĞ°Ğ¼ÑÑ‚ÑŒ ĞºĞ°Ğº Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°: Ğ¾Ñ‚ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ°ÑƒĞºĞ¸ Ğº Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLM', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´ÑÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ² Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ñ…, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ·Ğ½Ğ°Ğ½Ğ¸ÑÑ… ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ°ÑƒĞºĞ¸ Ğ¸ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ°Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½Ğ¸ Ğ¸Ğ·ÑƒÑ‡Ğ°ÑÑ‚ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ, Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¸ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ñ Ğ±Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¼ Ğ¶Ğ¸Ğ·Ğ½ĞµĞ½Ğ½Ğ¾Ğ¼ Ñ†Ğ¸ĞºĞ»Ğµ. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ñ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ°Ñ‚Ğ°Ğº Ğ¸ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹, ÑĞ¾ÑÑ€ĞµĞ´Ğ¾Ñ‚Ğ¾Ñ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¾Ğ±Ñ€ĞµÑ‚ĞµĞ½Ğ¸Ğ¸ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ².'}, 'en': {'title': 'Bridging Human Memory and AI: A New Frontier in Autonomous Agents', 'desc': 'This paper explores the integration of human memory mechanisms into the design of autonomous agents, particularly those driven by large language models (LLMs). It synthesizes knowledge from cognitive neuroscience to enhance the understanding of memory functions and workflows in AI systems. The authors provide a comparative analysis of memory types, storage methods, and management processes from both biological and artificial viewpoints. They also discuss the evaluation of agent memory and propose future research directions, emphasizing the importance of multimodal memory systems and skill acquisition.'}, 'zh': {'title': 'è®°å¿†ï¼šè¿æ¥è¿‡å»ä¸æœªæ¥çš„æ¡¥æ¢', 'desc': 'æœ¬æ–‡æ¢è®¨äº†è®°å¿†åœ¨äººå·¥æ™ºèƒ½ç³»ç»Ÿä¸­çš„é‡è¦æ€§ï¼Œå¼ºè°ƒäº†è®°å¿†å¦‚ä½•å¸®åŠ©è‡ªä¸»æ™ºèƒ½ä½“å®Œæˆå¤æ‚ä»»åŠ¡ã€‚ç ”ç©¶ç»“åˆäº†è®¤çŸ¥ç¥ç»ç§‘å­¦ä¸å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œç³»ç»Ÿæ€§åœ°åˆ†æäº†è®°å¿†çš„å®šä¹‰ã€åŠŸèƒ½åŠå…¶åœ¨ç”Ÿç‰©å’Œäººå·¥æ™ºèƒ½ä¸­çš„ç®¡ç†ç”Ÿå‘½å‘¨æœŸã€‚æ–‡ç« è¿˜æ¯”è¾ƒäº†è®°å¿†çš„åˆ†ç±»ã€å­˜å‚¨æœºåˆ¶ï¼Œå¹¶å›é¡¾äº†è¯„ä¼°æ™ºèƒ½ä½“è®°å¿†çš„ä¸»æµåŸºå‡†ã€‚æœ€åï¼Œä½œè€…å±•æœ›äº†æœªæ¥çš„ç ”ç©¶æ–¹å‘ï¼Œç‰¹åˆ«æ˜¯å¤šæ¨¡æ€è®°å¿†ç³»ç»Ÿå’ŒæŠ€èƒ½è·å–ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.23988', 'title': 'Fantastic Reasoning Behaviors and Where to Find Them: Unsupervised Discovery of the Reasoning Process', 'url': 'https://huggingface.co/papers/2512.23988', 'abstract': "An unsupervised framework using sparse auto-encoders identifies and controls interpretable reasoning behaviors in large language models through disentangled latent vectors.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite the growing reasoning capabilities of recent large language models (LLMs), their internal mechanisms during the reasoning process remain underexplored. Prior approaches often rely on human-defined concepts (e.g., overthinking, reflection) at the word level to analyze reasoning in a supervised manner. However, such methods are limited, as it is infeasible to capture the full spectrum of potential reasoning behaviors, many of which are difficult to define in token space. In this work, we propose an unsupervised framework (namely, RISE: Reasoning behavior Interpretability via Sparse auto-Encoder) for discovering reasoning vectors, which we define as directions in the activation space that encode distinct reasoning behaviors. By segmenting chain-of-thought traces into sentence-level 'steps' and training sparse auto-encoders (SAEs) on step-level activations, we uncover disentangled features corresponding to interpretable behaviors such as reflection and backtracking. Visualization and clustering analyses show that these behaviors occupy separable regions in the decoder column space. Moreover, targeted interventions on SAE-derived vectors can controllably amplify or suppress specific reasoning behaviors, altering inference trajectories without retraining. Beyond behavior-specific disentanglement, SAEs capture structural properties such as response length, revealing clusters of long versus short reasoning traces. More interestingly, SAEs enable the discovery of novel behaviors beyond human supervision. We demonstrate the ability to control response confidence by identifying confidence-related vectors in the SAE decoder space. These findings underscore the potential of unsupervised latent discovery for both interpreting and controllably steering reasoning in LLMs.", 'score': 6, 'issue_id': 353, 'pub_date': '2026-12-30', 'pub_date_card': {'ru': '30 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 30', 'zh': '12æœˆ30æ—¥'}, 'hash': 'eee1ab8177524c42', 'authors': ['Zhenyu Zhang', 'Shujian Zhang', 'John Lambert', 'Wenxuan Zhou', 'Zhangyang Wang', 'Mingqing Chen', 'Andrew Hard', 'Rajiv Mathews', 'Lun Wang'], 'affiliations': ['Google DeepMind', 'The University of Texas at Austin'], 'pdf_title_img': 'assets/pdf/title_img/2512.23988.jpg', 'data': {'categories': ['#reasoning', '#interpretability'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ Ğ°ÑĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² LLM Ñ‡ĞµÑ€ĞµĞ· Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ½ĞµĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° RISE Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ÑÑ‚ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ğ¼Ñ‹ÑĞ»ĞµĞ¹ Ğ½Ğ° Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑˆĞ°Ğ³Ğ¸ Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ SAE Ğ½Ğ° Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸ÑÑ… ÑƒÑ€Ğ¾Ğ²Ğ½Ñ ÑˆĞ°Ğ³Ğ¾Ğ², Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ñ‚ÑŒ Ğ´Ğ¸sentangled Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸, ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¼ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸ÑĞ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‚ Ñ†ĞµĞ»ĞµĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾ ÑƒÑĞ¸Ğ»Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ»Ğ¸ Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ÑÑ‚ÑŒ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ, Ğ²Ñ‹Ñ…Ğ¾Ğ´ÑÑ‰Ğ¸Ğµ Ğ·Ğ° Ñ€Ğ°Ğ¼ĞºĞ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ½Ğ°Ğ´Ğ·Ğ¾Ñ€Ğ°, Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ñ….'}, 'en': {'title': 'Unleashing Interpretability: Controlling Reasoning in LLMs with Sparse Auto-Encoders', 'desc': "This paper introduces an unsupervised framework called RISE, which uses sparse auto-encoders to identify and control reasoning behaviors in large language models (LLMs). Unlike previous methods that rely on predefined concepts, RISE discovers reasoning vectors that represent distinct behaviors in the model's activation space. By analyzing sentence-level steps in reasoning, the framework reveals interpretable features such as reflection and backtracking, which can be visualized and clustered. Additionally, RISE allows for targeted interventions to manipulate these reasoning behaviors, enhancing our understanding and control over LLMs without the need for retraining."}, 'zh': {'title': 'æ— ç›‘ç£æ¨ç†è¡Œä¸ºæ§åˆ¶çš„åˆ›æ–°æ¡†æ¶', 'desc': 'æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ— ç›‘ç£æ¡†æ¶ï¼Œåˆ©ç”¨ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEï¼‰æ¥è¯†åˆ«å’Œæ§åˆ¶å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„å¯è§£é‡Šæ¨ç†è¡Œä¸ºã€‚é€šè¿‡å°†æ¨ç†è¿‡ç¨‹åˆ†è§£ä¸ºå¥å­çº§çš„â€œæ­¥éª¤â€ï¼Œå¹¶åœ¨è¿™äº›æ­¥éª¤çš„æ¿€æ´»ä¸Šè®­ç»ƒç¨€ç–è‡ªç¼–ç å™¨ï¼Œæˆ‘ä»¬å‘ç°äº†ä¸åæ€å’Œå›æº¯ç­‰å¯è§£é‡Šè¡Œä¸ºç›¸å¯¹åº”çš„è§£è€¦ç‰¹å¾ã€‚å¯è§†åŒ–å’Œèšç±»åˆ†æè¡¨æ˜ï¼Œè¿™äº›è¡Œä¸ºåœ¨è§£ç å™¨åˆ—ç©ºé—´ä¸­å æ®å¯åˆ†ç¦»çš„åŒºåŸŸã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥é€šè¿‡å¯¹SAEå¯¼å‡ºçš„å‘é‡è¿›è¡Œæœ‰é’ˆå¯¹æ€§çš„å¹²é¢„ï¼Œæ¥å¯æ§åœ°æ”¾å¤§æˆ–æŠ‘åˆ¶ç‰¹å®šçš„æ¨ç†è¡Œä¸ºï¼Œè€Œæ— éœ€é‡æ–°è®­ç»ƒæ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.25075', 'title': 'SpaceTimePilot: Generative Rendering of Dynamic Scenes Across Space and Time', 'url': 'https://huggingface.co/papers/2512.25075', 'abstract': "SpaceTimePilot is a video diffusion model enabling independent control of spatial viewpoint and temporal motion through a time-embedding mechanism, temporal-warping training, and a synthetic dataset for precise space-time disentanglement.  \t\t\t\t\tAI-generated summary \t\t\t\t We present SpaceTimePilot, a video diffusion model that disentangles space and time for controllable generative rendering. Given a monocular video, SpaceTimePilot can independently alter the camera viewpoint and the motion sequence within the generative process, re-rendering the scene for continuous and arbitrary exploration across space and time. To achieve this, we introduce an effective animation time-embedding mechanism in the diffusion process, allowing explicit control of the output video's motion sequence with respect to that of the source video. As no datasets provide paired videos of the same dynamic scene with continuous temporal variations, we propose a simple yet effective temporal-warping training scheme that repurposes existing multi-view datasets to mimic temporal differences. This strategy effectively supervises the model to learn temporal control and achieve robust space-time disentanglement. To further enhance the precision of dual control, we introduce two additional components: an improved camera-conditioning mechanism that allows altering the camera from the first frame, and CamxTime, the first synthetic space-and-time full-coverage rendering dataset that provides fully free space-time video trajectories within a scene. Joint training on the temporal-warping scheme and the CamxTime dataset yields more precise temporal control. We evaluate SpaceTimePilot on both real-world and synthetic data, demonstrating clear space-time disentanglement and strong results compared to prior work. Project page: https://zheninghuang.github.io/Space-Time-Pilot/ Code: https://github.com/ZheningHuang/spacetimepilot", 'score': 5, 'issue_id': 353, 'pub_date': '2026-12-31', 'pub_date_card': {'ru': '31 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 31', 'zh': '12æœˆ31æ—¥'}, 'hash': 'a5a1be5c32218359', 'authors': ['Zhening Huang', 'Hyeonho Jeong', 'Xuelin Chen', 'Yulia Gryaditskaya', 'Tuanfeng Y. Wang', 'Joan Lasenby', 'Chun-Hao Huang'], 'affiliations': ['Adobe Research', 'University of Cambridge'], 'pdf_title_img': 'assets/pdf/title_img/2512.25075.jpg', 'data': {'categories': ['#diffusion', '#dataset', '#architecture', '#video', '#synthetic'], 'emoji': 'ğŸ¬', 'ru': {'title': 'ĞĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ĞºĞ°Ğ¼ĞµÑ€Ğ¾Ğ¹ Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ĞµĞ¼ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµÑ€ĞµĞ· Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸', 'desc': 'SpaceTimePilot â€” ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ Ğ¸Ğ·Ğ¼ĞµĞ½ÑÑ‚ÑŒ Ñ‚Ğ¾Ñ‡ĞºÑƒ Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ° ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² ÑÑ†ĞµĞ½Ğµ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñƒ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ ÑÑ…ĞµĞ¼Ñƒ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ´ĞµÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿ĞµÑ€ĞµĞ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ²Ğ¸Ğ´Ğ¾Ğ²Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ Ğ¸Ğ¼Ğ¸Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¹. Ğ”Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ½Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ CamxTime Ñ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾ĞºÑ€Ñ‹Ñ‚Ğ¸ĞµĞ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Mastering Space and Time in Video Generation', 'desc': 'SpaceTimePilot is a novel video diffusion model that allows users to independently manipulate both the viewpoint and motion of a scene in generated videos. It utilizes a time-embedding mechanism and a unique temporal-warping training approach to achieve effective space-time disentanglement. By leveraging a synthetic dataset called CamxTime, the model can generate videos with continuous and arbitrary changes in both space and time. The results show that SpaceTimePilot outperforms previous methods in terms of precision and control over video generation.'}, 'zh': {'title': 'ç‹¬ç«‹æ§åˆ¶æ—¶ç©ºçš„åˆ›æ–°è§†é¢‘ç”Ÿæˆæ¨¡å‹', 'desc': 'SpaceTimePilotæ˜¯ä¸€ç§è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œèƒ½å¤Ÿç‹¬ç«‹æ§åˆ¶ç©ºé—´è§†è§’å’Œæ—¶é—´è¿åŠ¨ã€‚é€šè¿‡æ—¶é—´åµŒå…¥æœºåˆ¶å’Œæ—¶é—´æ‰­æ›²è®­ç»ƒï¼Œè¯¥æ¨¡å‹å®ç°äº†ç²¾ç¡®çš„æ—¶ç©ºè§£è€¦ã€‚å®ƒå¯ä»¥æ ¹æ®å•ç›®è§†é¢‘ï¼Œé‡æ–°æ¸²æŸ“åœºæ™¯ï¼Œå…è®¸ç”¨æˆ·åœ¨ç©ºé—´å’Œæ—¶é—´ä¸Šè¿›è¡Œè¿ç»­çš„æ¢ç´¢ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†CamxTimeæ•°æ®é›†ï¼Œä»¥å¢å¼ºæ¨¡å‹çš„åŒé‡æ§åˆ¶ç²¾åº¦ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.23851', 'title': 'Pretraining Frame Preservation in Autoregressive Video Memory Compression', 'url': 'https://huggingface.co/papers/2512.23851', 'abstract': 'We present PFP, a neural network structure to compress long videos into short contexts, with an explicit pretraining objective to preserve the high-frequency details of single frames at arbitrary temporal positions. The baseline model can compress a 20-second video into a context at about 5k length, where random frames can be retrieved with perceptually preserved appearances. Such pretrained models can be directly fine-tuned as memory encoders for autoregressive video models, enabling long history memory with low context cost and relatively low fidelity loss. We evaluate the framework with ablative settings and discuss the trade-offs of possible neural architecture designs.', 'score': 5, 'issue_id': 353, 'pub_date': '2026-12-29', 'pub_date_card': {'ru': '29 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 29', 'zh': '12æœˆ29æ—¥'}, 'hash': '0e65f9e6890d2909', 'authors': ['Lvmin Zhang', 'Shengqu Cai', 'Muyang Li', 'Chong Zeng', 'Beijia Lu', 'Anyi Rao', 'Song Han', 'Gordon Wetzstein', 'Maneesh Agrawala'], 'affiliations': ['Carnegie Mellon University', 'HKUST', 'MIT', 'Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2512.23851.jpg', 'data': {'categories': ['#video', '#architecture', '#training'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹ ĞºĞ°Ğ´Ñ€Ğ¾Ğ²', 'desc': 'ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ PFP â€” Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ÑĞµÑ‚Ğ¸ Ğ´Ğ»Ñ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ² ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ñ‹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ñ ÑĞ²Ğ½Ğ¾Ğ¹ Ñ†ĞµĞ»ÑŒÑ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸ÑÑ…. Ğ‘Ğ°Ğ·Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° ÑĞ¶Ğ°Ñ‚ÑŒ 20-ÑĞµĞºÑƒĞ½Ğ´Ğ½Ğ¾Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ´Ğ»Ğ¸Ğ½Ğ¾Ğ¹ Ğ¾ĞºĞ¾Ğ»Ğ¾ 5000 Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ¸Ğ· ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ñ‹Ğµ ĞºĞ°Ğ´Ñ€Ñ‹ Ñ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼. ĞŸÑ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ»ĞµĞ³ĞºĞ¾ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ÑÑ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ² Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ Ğº Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ¹ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ñ Ğ½Ğ¸Ğ·ĞºĞ¸Ğ¼Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Efficient Video Compression with High-Fidelity Frame Retrieval', 'desc': 'The paper introduces PFP, a novel neural network architecture designed to compress lengthy videos into concise contexts while maintaining high-frequency details of individual frames. It achieves this by using a pretraining objective that allows for the retrieval of random frames with visually preserved quality. The model can effectively reduce a 20-second video to a context of approximately 5k, making it efficient for memory encoding in autoregressive video models. The authors also explore various neural architecture designs and their trade-offs through extensive evaluations.'}, 'zh': {'title': 'é«˜æ•ˆè§†é¢‘å‹ç¼©ä¸è®°å¿†ç¼–ç ', 'desc': 'æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºPFPçš„ç¥ç»ç½‘ç»œç»“æ„ï¼Œç”¨äºå°†é•¿è§†é¢‘å‹ç¼©æˆçŸ­çš„ä¸Šä¸‹æ–‡ã€‚è¯¥æ¨¡å‹çš„é¢„è®­ç»ƒç›®æ ‡æ˜¯ä¿ç•™å•å¸§çš„é«˜é¢‘ç»†èŠ‚ï¼Œèƒ½å¤Ÿåœ¨ä»»æ„æ—¶é—´ä½ç½®è¿›è¡Œå‹ç¼©ã€‚åŸºçº¿æ¨¡å‹å¯ä»¥å°†20ç§’çš„è§†é¢‘å‹ç¼©ä¸ºçº¦5ké•¿åº¦çš„ä¸Šä¸‹æ–‡ï¼Œå¹¶ä¸”å¯ä»¥ä»¥æ„ŸçŸ¥ä¸Šä¿ç•™çš„å¤–è§‚éšæœºæ£€ç´¢å¸§ã€‚è¿™ç§é¢„è®­ç»ƒæ¨¡å‹å¯ä»¥ç›´æ¥å¾®è°ƒä¸ºè‡ªå›å½’è§†é¢‘æ¨¡å‹çš„è®°å¿†ç¼–ç å™¨ï¼Œä»è€Œä»¥è¾ƒä½çš„ä¸Šä¸‹æ–‡æˆæœ¬å®ç°é•¿å†å²è®°å¿†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.22564', 'title': 'Geometry-Aware Optimization for Respiratory Sound Classification: Enhancing Sensitivity with SAM-Optimized Audio Spectrogram Transformers', 'url': 'https://huggingface.co/papers/2512.22564', 'abstract': 'Respiratory sound classification is hindered by the limited size, high noise levels, and severe class imbalance of benchmark datasets like ICBHI 2017. While Transformer-based models offer powerful feature extraction capabilities, they are prone to overfitting and often converge to sharp minima in the loss landscape when trained on such constrained medical data. To address this, we introduce a framework that enhances the Audio Spectrogram Transformer (AST) using Sharpness-Aware Minimization (SAM). Instead of merely minimizing the training loss, our approach optimizes the geometry of the loss surface, guiding the model toward flatter minima that generalize better to unseen patients. We also implement a weighted sampling strategy to handle class imbalance effectively. Our method achieves a state-of-the-art score of 68.10% on the ICBHI 2017 dataset, outperforming existing CNN and hybrid baselines. More importantly, it reaches a sensitivity of 68.31%, a crucial improvement for reliable clinical screening. Further analysis using t-SNE and attention maps confirms that the model learns robust, discriminative features rather than memorizing background noise.', 'score': 5, 'issue_id': 357, 'pub_date': '2026-12-27', 'pub_date_card': {'ru': '27 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 27', 'zh': '12æœˆ27æ—¥'}, 'hash': '2e3862f4d88666a8', 'authors': ['Atakan IÅŸÄ±k', 'Selin Vulga IÅŸÄ±k', 'Ahmet Feridun IÅŸÄ±k', 'MahÅŸuk Taylan'], 'affiliations': ['nRespiratory'], 'pdf_title_img': 'assets/pdf/title_img/2512.22564.jpg', 'data': {'categories': ['#architecture', '#audio', '#training', '#healthcare'], 'emoji': 'ğŸ«', 'ru': {'title': 'ĞŸĞ»Ğ¾ÑĞºĞ¸Ğµ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼ÑƒĞ¼Ñ‹ Ğ´Ğ»Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸ Ğ´Ñ‹Ñ…Ğ°Ğ½Ğ¸Ñ', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ·Ğ²ÑƒĞºĞ¾Ğ² Ğ´Ñ‹Ñ…Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Audio Spectrogram Transformer, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ… Ñ Ğ´Ğ¸ÑĞ±Ğ°Ğ»Ğ°Ğ½ÑĞ¾Ğ¼ ĞºĞ»Ğ°ÑÑĞ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Sharpness-Aware Minimization Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ, Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ÑÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğº Ğ¿Ğ»Ğ¾ÑĞºĞ¸Ğ¼ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼ÑƒĞ¼Ğ°Ğ¼ Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ĞµĞ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒÑ. Ğ”Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ²Ğ·Ğ²ĞµÑˆĞµĞ½Ğ½Ğ°Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ ÑĞµĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ´Ğ¸ÑĞ±Ğ°Ğ»Ğ°Ğ½ÑĞ° ĞºĞ»Ğ°ÑÑĞ¾Ğ². Ğ”Ğ¾ÑÑ‚Ğ¸Ğ³Ğ½ÑƒÑ‚Ñ‹ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ state-of-the-art Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ ICBHI 2017 Ñ Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ 68.31%, Ñ‡Ñ‚Ğ¾ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ğ°Ğ¶Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞºÑ€Ğ¸Ğ½Ğ¸Ğ½Ğ³Ğ°.'}, 'en': {'title': 'Enhancing Respiratory Sound Classification with SAM and AST', 'desc': 'This paper addresses the challenges of respiratory sound classification, particularly the issues of limited dataset size, high noise levels, and class imbalance. The authors propose an enhanced Audio Spectrogram Transformer (AST) framework that incorporates Sharpness-Aware Minimization (SAM) to improve model generalization by optimizing the loss surface geometry. By focusing on flatter minima, the model is less likely to overfit and performs better on unseen data. Additionally, a weighted sampling strategy is introduced to effectively manage class imbalance, resulting in state-of-the-art performance on the ICBHI 2017 dataset.'}, 'zh': {'title': 'ä¼˜åŒ–å‘¼å¸å£°éŸ³åˆ†ç±»çš„æ·±åº¦å­¦ä¹ æ–¹æ³•', 'desc': 'æœ¬ç ”ç©¶é’ˆå¯¹å‘¼å¸å£°éŸ³åˆ†ç±»ä¸­çš„æ•°æ®é›†é™åˆ¶é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ”¹è¿›çš„éŸ³é¢‘è°±å›¾å˜æ¢å™¨ï¼ˆASTï¼‰æ¡†æ¶ã€‚æˆ‘ä»¬é‡‡ç”¨äº†æ•æ„Ÿåº¦æ„è¯†æœ€å°åŒ–ï¼ˆSAMï¼‰æ–¹æ³•ï¼Œä¼˜åŒ–æŸå¤±è¡¨é¢çš„å‡ ä½•å½¢çŠ¶ï¼Œä»¥å¼•å¯¼æ¨¡å‹æœå‘æ›´å¹³å¦çš„æœ€å°å€¼ï¼Œä»è€Œæé«˜å¯¹æœªè§æ‚£è€…çš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å®æ–½äº†åŠ æƒé‡‡æ ·ç­–ç•¥ï¼Œæœ‰æ•ˆå¤„ç†ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ICBHI 2017æ•°æ®é›†ä¸Šè¾¾åˆ°äº†68.10%çš„æœ€æ–°æˆç»©ï¼Œæ˜¾è‘—æé«˜äº†ä¸´åºŠç­›æŸ¥çš„å¯é æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.24885', 'title': 'BEDA: Belief Estimation as Probabilistic Constraints for Performing Strategic Dialogue Acts', 'url': 'https://huggingface.co/papers/2512.24885', 'abstract': 'A framework called BEDA uses probabilistic constraints on belief estimation to improve strategic dialogue through formalized adversarial and alignment acts, outperforming baselines across multiple task settings.  \t\t\t\t\tAI-generated summary \t\t\t\t Strategic dialogue requires agents to execute distinct dialogue acts, for which belief estimation is essential. While prior work often estimates beliefs accurately, it lacks a principled mechanism to use those beliefs during generation. We bridge this gap by first formalizing two core acts Adversarial and Alignment, and by operationalizing them via probabilistic constraints on what an agent may generate. We instantiate this idea in BEDA, a framework that consists of the world set, the belief estimator for belief estimation, and the conditional generator that selects acts and realizes utterances consistent with the inferred beliefs. Across three settings, Conditional Keeper Burglar (CKBG, adversarial), Mutual Friends (MF, cooperative), and CaSiNo (negotiation), BEDA consistently outperforms strong baselines: on CKBG it improves success rate by at least 5.0 points across backbones and by 20.6 points with GPT-4.1-nano; on Mutual Friends it achieves an average improvement of 9.3 points; and on CaSiNo it achieves the optimal deal relative to all baselines. These results indicate that casting belief estimation as constraints provides a simple, general mechanism for reliable strategic dialogue.', 'score': 4, 'issue_id': 357, 'pub_date': '2026-12-31', 'pub_date_card': {'ru': '31 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 31', 'zh': '12æœˆ31æ—¥'}, 'hash': 'da71873530f907a7', 'authors': ['Hengli Li', 'Zhaoxin Yu', 'Qi Shen', 'Chenxi Li', 'Mengmeng Wang', 'Tinglang Wu', 'Yipeng Kang', 'Yuxuan Wang', 'Song-Chun Zhu', 'Zixia Jia', 'Zilong Zheng'], 'affiliations': ['Department of Automation, THU', 'Institute for Artificial Intelligence, PKU', 'Institute of Automation, CAS', 'NLCo, BIGAI', 'School of Artificial Intelligence, BUPT', 'Yuanpei College, PKU'], 'pdf_title_img': 'assets/pdf/title_img/2512.24885.jpg', 'data': {'categories': ['#reasoning', '#alignment'], 'emoji': 'ğŸ­', 'ru': {'title': 'Ğ£Ğ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ñ ĞºĞ°Ğº Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° BEDA, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾Ñ†ĞµĞ½ĞºÑƒ ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·ÑƒÑÑ‚ Ğ´Ğ²Ğ° ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ° Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ° â€” Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾Ğ±Ğ¾Ñ€ÑÑ‚Ğ²Ğ¾ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ğµ â€” Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·ÑƒÑÑ‚ Ğ¸Ñ… Ñ‡ĞµÑ€ĞµĞ· Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ğ²Ñ‹ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¸Ñ€Ğ°, Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ĞµĞ»Ñ ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ Ñ€ĞµĞ¿Ğ»Ğ¸ĞºĞ¸, ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ĞµĞ¼Ñ‹Ğ¼Ğ¸ ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸ ÑĞ¾Ğ±ĞµÑĞµĞ´Ğ½Ğ¸ĞºĞ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ñ‚Ñ€Ñ‘Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸: Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ñ€Ğ¾ÑĞ»Ğ° Ğ½Ğ° 20.6 Ğ¿ÑƒĞ½ĞºÑ‚Ğ° Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ GPT-4.1-nano.'}, 'en': {'title': 'Enhancing Strategic Dialogue with BEDA: Belief Estimation Meets Probabilistic Constraints', 'desc': 'The paper introduces BEDA, a framework that enhances strategic dialogue by using probabilistic constraints on belief estimation. It formalizes two key dialogue acts: Adversarial and Alignment, which help agents make better decisions during conversations. By integrating a belief estimator and a conditional generator, BEDA ensures that the generated responses align with the inferred beliefs of the agents. The framework shows significant improvements in performance across various task settings, demonstrating the effectiveness of using belief estimation as a guiding principle in dialogue generation.'}, 'zh': {'title': 'ä¿¡å¿µçº¦æŸæå‡æˆ˜ç•¥å¯¹è¯çš„æœ‰æ•ˆæ€§', 'desc': 'BEDAæ¡†æ¶é€šè¿‡å¯¹ä¿¡å¿µä¼°è®¡æ–½åŠ æ¦‚ç‡çº¦æŸï¼Œæå‡äº†æˆ˜ç•¥å¯¹è¯çš„æ•ˆæœã€‚è¯¥æ¡†æ¶æ­£å¼åŒ–äº†å¯¹æŠ—æ€§å’Œå¯¹é½æ€§ä¸¤ä¸ªæ ¸å¿ƒå¯¹è¯è¡Œä¸ºï¼Œå¹¶é€šè¿‡æ¦‚ç‡çº¦æŸæ¥æ“ä½œè¿™äº›è¡Œä¸ºã€‚BEDAåŒ…æ‹¬ä¸–ç•Œé›†ã€ä¿¡å¿µä¼°è®¡å™¨å’Œæ¡ä»¶ç”Ÿæˆå™¨ï¼Œèƒ½å¤Ÿæ ¹æ®æ¨æ–­çš„ä¿¡å¿µé€‰æ‹©å¯¹è¯è¡Œä¸ºå¹¶ç”Ÿæˆä¸€è‡´çš„å‘è¨€ã€‚åœ¨å¤šä¸ªä»»åŠ¡è®¾ç½®ä¸­ï¼ŒBEDAçš„è¡¨ç°å‡ä¼˜äºå¼ºåŸºçº¿ï¼Œæ˜¾ç¤ºå‡ºå°†ä¿¡å¿µä¼°è®¡ä½œä¸ºçº¦æŸçš„æ–¹å¼ä¸ºå¯é çš„æˆ˜ç•¥å¯¹è¯æä¾›äº†ç®€å•è€Œé€šç”¨çš„æœºåˆ¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.24385', 'title': 'Forging Spatial Intelligence: A Roadmap of Multi-Modal Data Pre-Training for Autonomous Systems', 'url': 'https://huggingface.co/papers/2512.24385', 'abstract': 'The rapid advancement of autonomous systems, including self-driving vehicles and drones, has intensified the need to forge true Spatial Intelligence from multi-modal onboard sensor data. While foundation models excel in single-modal contexts, integrating their capabilities across diverse sensors like cameras and LiDAR to create a unified understanding remains a formidable challenge. This paper presents a comprehensive framework for multi-modal pre-training, identifying the core set of techniques driving progress toward this goal. We dissect the interplay between foundational sensor characteristics and learning strategies, evaluating the role of platform-specific datasets in enabling these advancements. Our central contribution is the formulation of a unified taxonomy for pre-training paradigms: ranging from single-modality baselines to sophisticated unified frameworks that learn holistic representations for advanced tasks like 3D object detection and semantic occupancy prediction. Furthermore, we investigate the integration of textual inputs and occupancy representations to facilitate open-world perception and planning. Finally, we identify critical bottlenecks, such as computational efficiency and model scalability, and propose a roadmap toward general-purpose multi-modal foundation models capable of achieving robust Spatial Intelligence for real-world deployment.', 'score': 4, 'issue_id': 354, 'pub_date': '2026-12-30', 'pub_date_card': {'ru': '30 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 30', 'zh': '12æœˆ30æ—¥'}, 'hash': 'bf38f19718f26a0d', 'authors': ['Song Wang', 'Lingdong Kong', 'Xiaolu Liu', 'Hao Shi', 'Wentong Li', 'Jianke Zhu', 'Steven C. H. Hoi'], 'affiliations': ['Alibaba Group', 'Nanjing University of Aeronautics', 'National University of Singapore', 'Singapore Management University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2512.24385.jpg', 'data': {'categories': ['#benchmark', '#3d', '#multimodal', '#dataset', '#robotics', '#survey'], 'emoji': 'ğŸš—', 'ru': {'title': 'Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ñ‡Ğ¸ĞºĞ¾Ğ² (ĞºĞ°Ğ¼ĞµÑ€ Ğ¸ LiDAR) Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ² Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ĞµĞ´Ğ¸Ğ½ÑƒÑ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¾Ñ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ… Ğ¾Ğ´Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ 3D-Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ·Ğ°Ğ½ÑÑ‚Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ²Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ·Ğ°Ğ½ÑÑ‚Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¼Ğ¸Ñ€Ğ° Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ»ÑÑÑ‚ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑƒĞ·ĞºĞ¸Ğµ Ğ¼ĞµÑÑ‚Ğ° Ğ² Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°Ñ Ğ´Ğ¾Ñ€Ğ¾Ğ¶Ğ½ÑƒÑ ĞºĞ°Ñ€Ñ‚Ñƒ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Building Spatial Intelligence through Multi-Modal Integration', 'desc': 'This paper addresses the challenge of creating Spatial Intelligence in autonomous systems by integrating data from various sensors like cameras and LiDAR. It introduces a framework for multi-modal pre-training that enhances the ability of models to understand and process diverse sensor inputs. The authors propose a unified taxonomy for pre-training methods, which helps in developing advanced tasks such as 3D object detection. Additionally, the paper highlights key challenges like computational efficiency and scalability, providing a roadmap for building versatile multi-modal foundation models for real-world applications.'}, 'zh': {'title': 'æ„å»ºå¤šæ¨¡æ€ç©ºé—´æ™ºèƒ½çš„æœªæ¥ä¹‹è·¯', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†å¦‚ä½•ä»å¤šæ¨¡æ€ä¼ æ„Ÿå™¨æ•°æ®ä¸­æ„å»ºçœŸæ­£çš„ç©ºé—´æ™ºèƒ½ï¼Œå°¤å…¶æ˜¯åœ¨è‡ªåŠ¨é©¾é©¶å’Œæ— äººæœºç­‰è‡ªä¸»ç³»ç»Ÿä¸­ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§å…¨é¢çš„å¤šæ¨¡æ€é¢„è®­ç»ƒæ¡†æ¶ï¼Œåˆ†æäº†åŸºç¡€ä¼ æ„Ÿå™¨ç‰¹æ€§ä¸å­¦ä¹ ç­–ç•¥ä¹‹é—´çš„ç›¸äº’ä½œç”¨ã€‚é€šè¿‡å»ºç«‹ç»Ÿä¸€çš„é¢„è®­ç»ƒèŒƒç•´ï¼Œæˆ‘ä»¬èƒ½å¤Ÿä»å•æ¨¡æ€åŸºçº¿åˆ°å¤æ‚çš„ç»Ÿä¸€æ¡†æ¶ï¼Œå­¦ä¹ ç”¨äºé«˜çº§ä»»åŠ¡çš„æ•´ä½“è¡¨ç¤ºã€‚æœ€åï¼Œæˆ‘ä»¬è¯†åˆ«äº†è®¡ç®—æ•ˆç‡å’Œæ¨¡å‹å¯æ‰©å±•æ€§ç­‰å…³é”®ç“¶é¢ˆï¼Œå¹¶æå‡ºäº†å®ç°é€šç”¨å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹çš„è·¯çº¿å›¾ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.24297', 'title': 'Figure It Out: Improving the Frontier of Reasoning with Active Visual Thinking', 'url': 'https://huggingface.co/papers/2512.24297', 'abstract': 'Complex reasoning problems often involve implicit spatial, geometric, and structural relationships that are not explicitly encoded in text. While recent reasoning models have achieved strong performance across many domains, purely text-based reasoning struggles to represent global structural constraints in complex settings. In this paper, we introduce FIGR, which integrates active visual thinking into multi-turn reasoning via end-to-end reinforcement learning. FIGR externalizes intermediate structural hypotheses by constructing visual representations during problem solving. By adaptively regulating when and how visual reasoning should be invoked, FIGR enables more stable and coherent reasoning over global structural properties that are difficult to capture from text alone. Experiments on challenging mathematical reasoning benchmarks demonstrate that FIGR outperforms strong text-only chain-of-thought baselines. In particular, FIGR improves the base model by 13.12% on AIME 2025 and 11.00% on BeyondAIME, highlighting the effectiveness of figure-guided multimodal reasoning in enhancing the stability and reliability of complex reasoning.', 'score': 4, 'issue_id': 353, 'pub_date': '2026-12-30', 'pub_date_card': {'ru': '30 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 30', 'zh': '12æœˆ30æ—¥'}, 'hash': '0f388b52a58f75b9', 'authors': ['Meiqi Chen', 'Fandong Meng', 'Jie Zhou'], 'affiliations': ['WeChat AI, Tencent Inc'], 'pdf_title_img': 'assets/pdf/title_img/2512.24297.jpg', 'data': {'categories': ['#rl', '#math', '#multimodal', '#benchmark'], 'emoji': 'ğŸ“Š', 'ru': {'title': 'Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ÑÑ FIGR â€” ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑĞºÑÑ‚ĞµÑ€Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ Ğ³Ğ¸Ğ¿Ğ¾Ñ‚ĞµĞ·Ñ‹, ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ. ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ñ€ĞµĞ³ÑƒĞ»ÑÑ†Ğ¸Ñ Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚Ğ° Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ: 13.12% Ğ½Ğ° AIME 2025 Ğ¸ 11.00% Ğ½Ğ° BeyondAIME Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Enhancing Reasoning with Visual Thinking: Introducing FIGR', 'desc': 'This paper presents FIGR, a novel approach that enhances multi-turn reasoning by incorporating visual thinking through reinforcement learning. Unlike traditional text-based models, FIGR constructs visual representations to capture complex spatial and structural relationships during problem solving. By dynamically deciding when to use visual reasoning, FIGR improves the coherence and stability of reasoning processes. Experimental results show that FIGR significantly outperforms existing text-only models on challenging mathematical reasoning tasks, demonstrating the benefits of integrating visual information.'}, 'zh': {'title': 'è§†è§‰å¼•å¯¼çš„å¤šæ¨¡æ€æ¨ç†æå‡å¤æ‚æ¨ç†èƒ½åŠ›', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºFIGRçš„æ¨¡å‹ï¼Œå®ƒé€šè¿‡ç«¯åˆ°ç«¯çš„å¼ºåŒ–å­¦ä¹ å°†ä¸»åŠ¨è§†è§‰æ€ç»´æ•´åˆåˆ°å¤šè½®æ¨ç†ä¸­ã€‚FIGRåœ¨è§£å†³é—®é¢˜çš„è¿‡ç¨‹ä¸­ï¼Œé€šè¿‡æ„å»ºè§†è§‰è¡¨ç¤ºæ¥å¤–åŒ–ä¸­é—´ç»“æ„å‡è®¾ï¼Œä»è€Œæ›´å¥½åœ°å¤„ç†å¤æ‚çš„æ¨ç†é—®é¢˜ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿè‡ªé€‚åº”åœ°è°ƒèŠ‚ä½•æ—¶ä»¥åŠå¦‚ä½•è°ƒç”¨è§†è§‰æ¨ç†ï¼Œä½¿å¾—å¯¹å…¨çƒç»“æ„å±æ€§çš„æ¨ç†æ›´åŠ ç¨³å®šå’Œè¿è´¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFIGRåœ¨æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºä»…åŸºäºæ–‡æœ¬çš„æ¨ç†æ¨¡å‹ï¼Œæå‡äº†æ¨ç†çš„ç¨³å®šæ€§å’Œå¯é æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.24097', 'title': 'Factorized Learning for Temporally Grounded Video-Language Models', 'url': 'https://huggingface.co/papers/2512.24097', 'abstract': 'Recent video-language models have shown great potential for video understanding, but still struggle with accurate temporal grounding for event-level perception. We observe that two main factors in video understanding (i.e., temporal grounding and textual response) form a logical hierarchy: accurate temporal evidence grounding lays the foundation for reliable textual response. However, existing works typically handle these two tasks in a coupled manner without a clear logical structure, leading to sub-optimal objectives. We address this from a factorized learning perspective. We first propose D^2VLM, a framework that decouples the learning of these two tasks while also emphasizing their inherent dependency. We adopt a "grounding then answering with evidence referencing" paradigm and introduce evidence tokens for evidence grounding, which emphasize event-level visual semantic capture beyond the focus on timestamp representation in existing works. To further facilitate the learning of these two tasks, we introduce a novel factorized preference optimization (FPO) algorithm. Unlike standard preference optimization, FPO explicitly incorporates probabilistic temporal grounding modeling into the optimization objective, enabling preference learning for both temporal grounding and textual response. We also construct a synthetic dataset to address the lack of suitable datasets for factorized preference learning with explicit temporal grounding. Experiments on various tasks demonstrate the clear advantage of our approach. Our source code is available at https://github.com/nusnlp/d2vlm.', 'score': 4, 'issue_id': 354, 'pub_date': '2026-12-30', 'pub_date_card': {'ru': '30 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 30', 'zh': '12æœˆ30æ—¥'}, 'hash': '9ba4701895c566d2', 'authors': ['Wenzheng Zeng', 'Difei Gao', 'Mike Zheng Shou', 'Hwee Tou Ng'], 'affiliations': ['National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2512.24097.jpg', 'data': {'categories': ['#training', '#multimodal', '#video', '#rlhf', '#dataset'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ DÂ²VLM â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ´Ğ²Ğµ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ² Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ¸ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾, Ñ‡Ñ‚Ğ¾ Ğ»ÑƒÑ‡ÑˆĞµ, Ñ‡ĞµĞ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğº. Ğ”Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ĞµĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ (FPO), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ²Ğ½Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ½ÑƒÑ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ² Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸. ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, ÑĞ¾Ğ·Ğ´Ğ°Ğ½ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ ÑĞ²Ğ½Ğ¾Ğ¹ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¾Ğ¹ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹.'}, 'en': {'title': 'Decoupling Temporal Grounding and Textual Response for Enhanced Video Understanding', 'desc': "This paper introduces D^2VLM, a new framework designed to improve video understanding by separating the tasks of temporal grounding and textual response. The authors argue that accurate temporal grounding is essential for generating reliable textual responses, and they propose a method that emphasizes this relationship. By using a 'grounding then answering with evidence referencing' approach, the framework incorporates evidence tokens to enhance event-level visual semantic understanding. Additionally, the novel factorized preference optimization (FPO) algorithm is introduced to better model the dependencies between these tasks, leading to improved performance in video-language tasks."}, 'zh': {'title': 'è§£è€¦è§†é¢‘ç†è§£ä¸­çš„æ—¶é—´å®šä½ä¸æ–‡æœ¬å“åº”', 'desc': 'æœ€è¿‘çš„è§†é¢‘è¯­è¨€æ¨¡å‹åœ¨è§†é¢‘ç†è§£æ–¹é¢å±•ç°äº†å¾ˆå¤§çš„æ½œåŠ›ï¼Œä½†åœ¨äº‹ä»¶çº§æ„ŸçŸ¥çš„æ—¶é—´å®šä½ä¸Šä»ç„¶å­˜åœ¨å›°éš¾ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œè§†é¢‘ç†è§£ä¸­çš„ä¸¤ä¸ªä¸»è¦å› ç´ ï¼ˆæ—¶é—´å®šä½å’Œæ–‡æœ¬å“åº”ï¼‰å½¢æˆäº†ä¸€ä¸ªé€»è¾‘å±‚æ¬¡ï¼šå‡†ç¡®çš„æ—¶é—´è¯æ®å®šä½ä¸ºå¯é çš„æ–‡æœ¬å“åº”å¥ å®šäº†åŸºç¡€ã€‚æˆ‘ä»¬æå‡ºäº†D^2VLMæ¡†æ¶ï¼Œè§£è€¦è¿™ä¸¤ä¸ªä»»åŠ¡çš„å­¦ä¹ ï¼ŒåŒæ—¶å¼ºè°ƒå®ƒä»¬ä¹‹é—´çš„å†…åœ¨ä¾èµ–å…³ç³»ã€‚é€šè¿‡å¼•å…¥æ–°çš„å› å­åŒ–åå¥½ä¼˜åŒ–ç®—æ³•ï¼ˆFPOï¼‰ï¼Œæˆ‘ä»¬èƒ½å¤Ÿæ›´å¥½åœ°ä¼˜åŒ–æ—¶é—´å®šä½å’Œæ–‡æœ¬å“åº”çš„å­¦ä¹ ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.24176', 'title': 'Guiding a Diffusion Transformer with the Internal Dynamics of Itself', 'url': 'https://huggingface.co/papers/2512.24176', 'abstract': "The diffusion model presents a powerful ability to capture the entire (conditional) data distribution. However, due to the lack of sufficient training and data to learn to cover low-probability areas, the model will be penalized for failing to generate high-quality images corresponding to these areas. To achieve better generation quality, guidance strategies such as classifier free guidance (CFG) can guide the samples to the high-probability areas during the sampling stage. However, the standard CFG often leads to over-simplified or distorted samples. On the other hand, the alternative line of guiding diffusion model with its bad version is limited by carefully designed degradation strategies, extra training and additional sampling steps. In this paper, we proposed a simple yet effective strategy Internal Guidance (IG), which introduces an auxiliary supervision on the intermediate layer during training process and extrapolates the intermediate and deep layer's outputs to obtain generative results during sampling process. This simple strategy yields significant improvements in both training efficiency and generation quality on various baselines. On ImageNet 256x256, SiT-XL/2+IG achieves FID=5.31 and FID=1.75 at 80 and 800 epochs. More impressively, LightningDiT-XL/1+IG achieves FID=1.34 which achieves a large margin between all of these methods. Combined with CFG, LightningDiT-XL/1+IG achieves the current state-of-the-art FID of 1.19.", 'score': 3, 'issue_id': 361, 'pub_date': '2026-12-30', 'pub_date_card': {'ru': '30 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 30', 'zh': '12æœˆ30æ—¥'}, 'hash': 'fd34611418fdc611', 'authors': ['Xingyu Zhou', 'Qifan Li', 'Xiaobin Hu', 'Hai Chen', 'Shuhang Gu'], 'affiliations': ['National University of Singapore', 'North China Institute of Computer Systems Engineering', 'Sun Yat-sen University', 'University of Electronic Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2512.24176.jpg', 'data': {'categories': ['#cv', '#diffusion', '#training', '#architecture'], 'emoji': 'âœ¨', 'ru': {'title': 'Ğ’Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ĞµĞµ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ´Ğ»Ñ ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Internal Guidance (IG) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ supervĞ¸Ğ·Ğ¸Ñ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ ÑĞ»Ğ¾Ğ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞºÑÑ‚Ñ€Ğ°Ğ¿Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ñ… Ğ²Ñ‹Ñ…Ğ¾Ğ´Ñ‹ Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ classifier free guidance, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼. ĞĞ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ ImageNet 256x256 Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ğ¿ĞµÑ‡Ğ°Ñ‚Ğ»ÑÑÑ‰Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ñ FID=1.19, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹.'}, 'en': {'title': 'Enhancing Diffusion Models with Internal Guidance for Superior Image Generation', 'desc': 'This paper introduces a new strategy called Internal Guidance (IG) to improve the performance of diffusion models in generating high-quality images. The authors highlight that traditional methods like classifier free guidance (CFG) can lead to oversimplified outputs, while existing alternatives require complex degradation strategies and additional training. IG enhances the training process by providing auxiliary supervision on intermediate layers, which helps in generating better samples during the sampling phase. The results show significant improvements in image quality, achieving state-of-the-art performance on benchmarks like ImageNet.'}, 'zh': {'title': 'å†…éƒ¨å¼•å¯¼ï¼šæå‡æ‰©æ•£æ¨¡å‹ç”Ÿæˆè´¨é‡çš„æœ‰æ•ˆç­–ç•¥', 'desc': 'æ‰©æ•£æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆæ•æ‰æ•´ä¸ªæ¡ä»¶æ•°æ®åˆ†å¸ƒï¼Œä½†ç”±äºè®­ç»ƒæ•°æ®ä¸è¶³ï¼Œæ¨¡å‹åœ¨ç”Ÿæˆä½æ¦‚ç‡åŒºåŸŸçš„é«˜è´¨é‡å›¾åƒæ—¶ä¼šå—åˆ°æƒ©ç½šã€‚ä¸ºæé«˜ç”Ÿæˆè´¨é‡ï¼Œç ”ç©¶è€…ä»¬æå‡ºäº†æ— åˆ†ç±»å™¨å¼•å¯¼ï¼ˆCFGï¼‰ç­–ç•¥ï¼Œä»¥åœ¨é‡‡æ ·é˜¶æ®µå¼•å¯¼æ ·æœ¬è¿›å…¥é«˜æ¦‚ç‡åŒºåŸŸã€‚ç„¶è€Œï¼Œæ ‡å‡†çš„CFGå¸¸å¸¸å¯¼è‡´æ ·æœ¬è¿‡äºç®€å•æˆ–å¤±çœŸã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„å†…éƒ¨å¼•å¯¼ï¼ˆIGï¼‰ç­–ç•¥ï¼Œé€šè¿‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¯¹ä¸­é—´å±‚è¿›è¡Œè¾…åŠ©ç›‘ç£ï¼Œå¹¶åœ¨é‡‡æ ·è¿‡ç¨‹ä¸­å¤–æ¨ä¸­é—´å±‚å’Œæ·±å±‚çš„è¾“å‡ºï¼Œä»è€Œæ˜¾è‘—æé«˜äº†è®­ç»ƒæ•ˆç‡å’Œç”Ÿæˆè´¨é‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.22905', 'title': 'JavisGPT: A Unified Multi-modal LLM for Sounding-Video Comprehension and Generation', 'url': 'https://huggingface.co/papers/2512.22905', 'abstract': 'This paper presents JavisGPT, the first unified multimodal large language model (MLLM) for Joint Audio-Video (JAV) comprehension and generation. JavisGPT adopts a concise encoder-LLM-decoder architecture, featuring a SyncFusion module for spatio-temporal audio-video fusion and synchrony-aware learnable queries to bridge a pretrained JAV-DiT generator. This design enables temporally coherent video-audio understanding and generation from multimodal instructions. We design an effective three-stage training pipeline consisting of multimodal pretraining, audio-video fine-tuning, and large-scale instruction-tuning, to progressively build multimodal comprehension and generation from existing vision-language models. To support this, we further construct JavisInst-Omni, a high-quality instruction dataset with over 200K GPT-4o-curated audio-video-text dialogues that span diverse and multi-level comprehension and generation scenarios. Extensive experiments on JAV comprehension and generation benchmarks show that JavisGPT outperforms existing MLLMs, particularly in complex and temporally synchronized settings.', 'score': 3, 'issue_id': 354, 'pub_date': '2026-12-28', 'pub_date_card': {'ru': '28 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 28', 'zh': '12æœˆ28æ—¥'}, 'hash': '0d7aa22be1582d4f', 'authors': ['Kai Liu', 'Jungang Li', 'Yuchong Sun', 'Shengqiong Wu', 'Jianzhang Gao', 'Daoan Zhang', 'Wei Zhang', 'Sheng Jin', 'Sicheng Yu', 'Geng Zhan', 'Jiayi Ji', 'Fan Zhou', 'Liang Zheng', 'Shuicheng Yan', 'Hao Fei', 'Tat-Seng Chua'], 'affiliations': ['ANU', 'HKUST(GZ)', 'HZCU', 'NTU', 'NUS', 'RUC', 'SMU', 'UR', 'USYD', 'ZJU'], 'pdf_title_img': 'assets/pdf/title_img/2512.22905.jpg', 'data': {'categories': ['#training', '#open_source', '#multimodal', '#video', '#audio', '#architecture', '#dataset'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ¡Ğ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾-Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµÑ€ĞµĞ· Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ LLM', 'desc': 'JavisGPT â€” ÑÑ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ğ°Ñ ĞµĞ´Ğ¸Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾-Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ encoder-LLM-decoder Ñ Ğ¼Ğ¾Ğ´ÑƒĞ»ĞµĞ¼ SyncFusion Ğ´Ğ»Ñ ÑĞ¿atio-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ñ‚Ñ€Ñ‘Ñ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ pipeline Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ: Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ´Ñ‚Ñ€ĞµĞ½Ğ¸Ğ½Ğ³, Ñ‚Ğ¾Ğ½ĞºĞ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ½Ğ° Ğ°ÑƒĞ´Ğ¸Ğ¾-Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑÑ…. Ğ”Ğ»Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¸ ÑÑ‚Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… JavisInst-Omni Ñ 200K Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ², Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… GPT-4o, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°.'}, 'en': {'title': 'JavisGPT: Unifying Audio and Video Understanding with Multimodal Intelligence', 'desc': 'JavisGPT is a groundbreaking multimodal large language model designed for understanding and generating audio and video together. It uses a unique architecture that combines an encoder, a large language model (LLM), and a decoder, along with a SyncFusion module to effectively merge audio and video data. The model is trained through a three-stage process that includes pretraining, fine-tuning, and instruction-tuning, enhancing its ability to handle complex multimodal tasks. With the support of a comprehensive dataset of audio-video-text dialogues, JavisGPT demonstrates superior performance in tasks requiring synchronized audio and video comprehension and generation.'}, 'zh': {'title': 'JavisGPTï¼šéŸ³è§†é¢‘ç†è§£ä¸ç”Ÿæˆçš„ç»Ÿä¸€æ¨¡å‹', 'desc': 'æœ¬æ–‡ä»‹ç»äº†JavisGPTï¼Œè¿™æ˜¯é¦–ä¸ªç»Ÿä¸€çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œç”¨äºè”åˆéŸ³é¢‘-è§†é¢‘ï¼ˆJAVï¼‰ç†è§£å’Œç”Ÿæˆã€‚JavisGPTé‡‡ç”¨ç®€æ´çš„ç¼–ç å™¨-LLM-è§£ç å™¨æ¶æ„ï¼Œé…å¤‡SyncFusionæ¨¡å—ï¼Œå®ç°éŸ³é¢‘å’Œè§†é¢‘çš„æ—¶ç©ºèåˆï¼Œå¹¶ä½¿ç”¨åŒæ­¥æ„ŸçŸ¥çš„å¯å­¦ä¹ æŸ¥è¯¢æ¥è¿æ¥é¢„è®­ç»ƒçš„JAV-DiTç”Ÿæˆå™¨ã€‚è¯¥è®¾è®¡ä½¿å¾—ä»å¤šæ¨¡æ€æŒ‡ä»¤ä¸­è¿›è¡Œæ—¶é—´ä¸€è‡´çš„éŸ³é¢‘è§†é¢‘ç†è§£å’Œç”Ÿæˆæˆä¸ºå¯èƒ½ã€‚æˆ‘ä»¬è¿˜è®¾è®¡äº†ä¸€ä¸ªæœ‰æ•ˆçš„ä¸‰é˜¶æ®µè®­ç»ƒæµç¨‹ï¼ŒåŒ…æ‹¬å¤šæ¨¡æ€é¢„è®­ç»ƒã€éŸ³é¢‘è§†é¢‘å¾®è°ƒå’Œå¤§è§„æ¨¡æŒ‡ä»¤å¾®è°ƒï¼Œä»¥é€æ­¥æ„å»ºå¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.22280', 'title': 'Valori: A Deterministic Memory Substrate for AI Systems', 'url': 'https://huggingface.co/papers/2512.22280', 'abstract': 'Valori introduces a deterministic AI memory system using fixed-point arithmetic to ensure bit-identical results across platforms, addressing non-determinism in vector embeddings and similarity search for trustworthy AI.  \t\t\t\t\tAI-generated summary \t\t\t\t Modern AI systems rely on vector embeddings stored and searched using floating-point arithmetic. While effective for approximate similarity search, this design introduces fundamental non-determinism: identical models, inputs, and code can produce different memory states and retrieval results across hardware architectures (e.g., x86 vs. ARM). This prevents replayability and safe deployment, leading to silent data divergence that prevents post-hoc verification and compromises audit trails in regulated sectors. We present Valori, a deterministic AI memory substrate that replaces floating-point memory operations with fixed-point arithmetic (Q16.16) and models memory as a replayable state machine. Valori guarantees bit-identical memory states, snapshots, and search results across platforms. We demonstrate that non-determinism arises before indexing or retrieval and show how Valori enforces determinism at the memory boundary. Our results suggest that deterministic memory is a necessary primitive for trustworthy AI systems. The reference implementation is open-source and available at https://github.com/varshith-Git/Valori-Kernel (archived at https://zenodo.org/records/18022660).', 'score': 3, 'issue_id': 355, 'pub_date': '2026-12-25', 'pub_date_card': {'ru': '25 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 25', 'zh': '12æœˆ25æ—¥'}, 'hash': 'd3f900163e2812ae', 'authors': ['Varshith Gudur'], 'affiliations': ['Independent Researcher', 'Valori Kernel Project'], 'pdf_title_img': 'assets/pdf/title_img/2512.22280.jpg', 'data': {'categories': [], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ”ĞµÑ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ñ‹Ñ… AI-ÑĞ¸ÑÑ‚ĞµĞ¼ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ñ€Ğ¸Ñ„Ğ¼ĞµÑ‚Ğ¸ĞºÑƒ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡ĞºĞ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Valori â€” ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´ĞµÑ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ´Ğ»Ñ AI, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ñ€Ğ¸Ñ„Ğ¼ĞµÑ‚Ğ¸ĞºÑƒ Ñ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡ĞºĞ¾Ğ¹ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ‡Ğ¸ÑĞµĞ» Ñ Ğ¿Ğ»Ğ°Ğ²Ğ°ÑÑ‰ĞµĞ¹ Ñ‚Ğ¾Ñ‡ĞºĞ¾Ğ¹ Ğ´Ğ»Ñ Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ° ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ°. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ… Ğ¾Ğ±Ğ¾Ñ€ÑƒĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ·Ğ°Ñ‚Ñ€ÑƒĞ´Ğ½ÑĞµÑ‚ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ°ÑƒĞ´Ğ¸Ñ‚ AI-ÑĞ¸ÑÑ‚ĞµĞ¼. Ğ’Ğ°Ğ»Ğ¾Ñ€Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ ĞºĞ°Ğº Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ğ¹ ĞºĞ¾Ğ½ĞµÑ‡Ğ½Ñ‹Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚, Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ñ€ÑƒÑ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ñ€Ğ¾Ğ². Ğ­Ñ‚Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ğ°Ğ¶Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ñ… AI-ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ² Ñ€ĞµĞ³ÑƒĞ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¾Ñ‚Ñ€Ğ°ÑĞ»ÑÑ….'}, 'en': {'title': 'Ensuring Consistency in AI with Deterministic Memory', 'desc': 'Valori is a new AI memory system that uses fixed-point arithmetic to ensure consistent results across different hardware platforms. Traditional AI systems often use floating-point arithmetic, which can lead to non-deterministic outcomes, meaning the same inputs can yield different results on different machines. This non-determinism can cause issues in verifying AI outputs and maintaining reliable audit trails, especially in regulated industries. By modeling memory as a replayable state machine, Valori guarantees that memory states and search results are identical, enhancing the trustworthiness of AI applications.'}, 'zh': {'title': 'æ„å»ºå¯ä¿¡èµ–AIçš„ç¡®å®šæ€§å†…å­˜ç³»ç»Ÿ', 'desc': 'Valori æ˜¯ä¸€ç§ç¡®å®šæ€§çš„äººå·¥æ™ºèƒ½å†…å­˜ç³»ç»Ÿï¼Œä½¿ç”¨å®šç‚¹ç®—æœ¯æ¥ç¡®ä¿è·¨å¹³å°çš„ä¸€è‡´æ€§ã€‚ä¼ ç»Ÿçš„æµ®ç‚¹ç®—æœ¯åœ¨å‘é‡åµŒå…¥å’Œç›¸ä¼¼æ€§æœç´¢ä¸­å¼•å…¥äº†éç¡®å®šæ€§ï¼Œå¯¼è‡´ç›¸åŒçš„æ¨¡å‹å’Œè¾“å…¥åœ¨ä¸åŒç¡¬ä»¶ä¸Šäº§ç”Ÿä¸åŒçš„ç»“æœã€‚Valori é€šè¿‡å°†æµ®ç‚¹å†…å­˜æ“ä½œæ›¿æ¢ä¸ºå®šç‚¹ç®—æœ¯ï¼Œç¡®ä¿å†…å­˜çŠ¶æ€å’Œæœç´¢ç»“æœåœ¨å„ä¸ªå¹³å°ä¸Šå®Œå…¨ç›¸åŒã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œç¡®å®šæ€§å†…å­˜æ˜¯æ„å»ºå¯ä¿¡èµ–äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„å¿…è¦åŸºç¡€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.23959', 'title': 'Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling', 'url': 'https://huggingface.co/papers/2512.23959', 'abstract': 'Multi-step retrieval-augmented generation (RAG) has become a widely adopted strategy for enhancing large language models (LLMs) on tasks that demand global comprehension and intensive reasoning. Many RAG systems incorporate a working memory module to consolidate retrieved information. However, existing memory designs function primarily as passive storage that accumulates isolated facts for the purpose of condensing the lengthy inputs and generating new sub-queries through deduction. This static nature overlooks the crucial high-order correlations among primitive facts, the compositions of which can often provide stronger guidance for subsequent steps. Therefore, their representational strength and impact on multi-step reasoning and knowledge evolution are limited, resulting in fragmented reasoning and weak global sense-making capacity in extended contexts. We introduce HGMem, a hypergraph-based memory mechanism that extends the concept of memory beyond simple storage into a dynamic, expressive structure for complex reasoning and global understanding. In our approach, memory is represented as a hypergraph whose hyperedges correspond to distinct memory units, enabling the progressive formation of higher-order interactions within memory. This mechanism connects facts and thoughts around the focal problem, evolving into an integrated and situated knowledge structure that provides strong propositions for deeper reasoning in subsequent steps. We evaluate HGMem on several challenging datasets designed for global sense-making. Extensive experiments and in-depth analyses show that our method consistently improves multi-step RAG and substantially outperforms strong baseline systems across diverse tasks.', 'score': 79, 'issue_id': 386, 'pub_date': '2026-12-30', 'pub_date_card': {'ru': '30 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 30', 'zh': '12æœˆ30æ—¥'}, 'hash': '7840b0b37077c947', 'authors': ['Chulun Zhou', 'Chunkang Zhang', 'Guoxin Yu', 'Fandong Meng', 'Jie Zhou', 'Wai Lam', 'Mo Yu'], 'affiliations': ['The Chinese University of Hong Kong', 'WeChat AI'], 'pdf_title_img': 'assets/pdf/title_img/2512.23959.jpg', 'data': {'categories': ['#graphs', '#training', '#rag', '#reasoning', '#long_context'], 'emoji': 'ğŸ•¸ï¸', 'ru': {'title': 'ĞÑ‚ Ñ…Ñ€Ğ°Ğ½Ğ¸Ğ»Ğ¸Ñ‰Ğ° Ğº ÑĞµÑ‚ĞµĞ²Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸: Ğ³Ğ¸Ğ¿ĞµÑ€Ğ³Ñ€Ğ°Ñ„Ñ‹ Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ HGMem Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ retrieval-augmented generation, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°ĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², Ğ³Ğ´Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ°Ğº Ğ¿Ğ°ÑÑĞ¸Ğ²Ğ½Ğ¾Ğµ Ñ…Ñ€Ğ°Ğ½Ğ¸Ğ»Ğ¸Ñ‰Ğµ Ñ„Ğ°ĞºÑ‚Ğ¾Ğ². ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ³Ğ¸Ğ¿ĞµÑ€Ğ³Ñ€Ğ°Ñ„Ğ°Ñ…, Ğ³Ğ´Ğµ Ğ³Ğ¸Ğ¿ĞµÑ€Ñ€ĞµĞ±Ñ€Ğ° Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ ĞµĞ´Ğ¸Ğ½Ğ¸Ñ†Ñ‹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¿Ğ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾ Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ„Ğ°ĞºÑ‚Ğ°Ğ¼Ğ¸ Ğ¸ Ğ¼Ñ‹ÑĞ»ÑĞ¼Ğ¸. Ğ­Ñ‚Ğ¾ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğµ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ğµ ÑˆĞ°Ğ³Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ HGMem Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°.'}, 'en': {'title': 'Revolutionizing Memory for Enhanced Reasoning in Language Models', 'desc': "This paper presents HGMem, a new memory mechanism designed to enhance multi-step retrieval-augmented generation (RAG) in large language models (LLMs). Unlike traditional memory systems that simply store isolated facts, HGMem uses a hypergraph structure to create dynamic connections between information, allowing for richer reasoning and better understanding of complex tasks. By representing memory as a hypergraph, it captures higher-order relationships among facts, which improves the model's ability to make sense of information over multiple steps. The authors demonstrate that HGMem significantly outperforms existing methods on various datasets, showcasing its effectiveness in enhancing global comprehension and reasoning capabilities."}, 'zh': {'title': 'è¶…å›¾è®°å¿†ï¼šæå‡å¤šæ­¥éª¤æ¨ç†çš„å…³é”®', 'desc': 'å¤šæ­¥éª¤æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ˜¯ä¸€ç§æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å…¨çƒç†è§£å’Œå¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„ç­–ç•¥ã€‚ç°æœ‰çš„è®°å¿†è®¾è®¡ä¸»è¦ä½œä¸ºè¢«åŠ¨å­˜å‚¨ï¼Œæ— æ³•æœ‰æ•ˆåˆ©ç”¨é«˜é˜¶å…³è”æ¥å¢å¼ºæ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†HGMemï¼Œä¸€ç§åŸºäºè¶…å›¾çš„è®°å¿†æœºåˆ¶ï¼Œå°†è®°å¿†æ‰©å±•ä¸ºåŠ¨æ€ã€è¡¨è¾¾ä¸°å¯Œçš„ç»“æ„ï¼Œä»¥æ”¯æŒå¤æ‚æ¨ç†å’Œå…¨çƒç†è§£ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒHGMemåœ¨å¤šæ­¥éª¤RAGä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—è¶…è¶Šäº†å¼ºåŸºçº¿ç³»ç»Ÿã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.24617', 'title': 'Dynamic Large Concept Models: Latent Reasoning in an Adaptive Semantic Space', 'url': 'https://huggingface.co/papers/2512.24617', 'abstract': 'Large Language Models (LLMs) apply uniform computation to all tokens, despite language exhibiting highly non-uniform information density. This token-uniform regime wastes capacity on locally predictable spans while under-allocating computation to semantically critical transitions. We propose Dynamic Large Concept Models (DLCM), a hierarchical language modeling framework that learns semantic boundaries from latent representations and shifts computation from tokens to a compressed concept space where reasoning is more efficient. DLCM discovers variable-length concepts end-to-end without relying on predefined linguistic units. Hierarchical compression fundamentally changes scaling behavior. We introduce the first compression-aware scaling law, which disentangles token-level capacity, concept-level reasoning capacity, and compression ratio, enabling principled compute allocation under fixed FLOPs. To stably train this heterogeneous architecture, we further develop a decoupled Î¼P parametrization that supports zero-shot hyperparameter transfer across widths and compression regimes. At a practical setting (R=4, corresponding to an average of four tokens per concept), DLCM reallocates roughly one-third of inference compute into a higher-capacity reasoning backbone, achieving a +2.69\\% average improvement across 12 zero-shot benchmarks under matched inference FLOPs.', 'score': 40, 'issue_id': 375, 'pub_date': '2026-12-31', 'pub_date_card': {'ru': '31 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 31', 'zh': '12æœˆ31æ—¥'}, 'hash': '14b3c28c67b8e772', 'authors': ['Xingwei Qu', 'Shaowen Wang', 'Zihao Huang', 'Kai Hua', 'Fan Yin', 'Rui-Jie Zhu', 'Jundong Zhou', 'Qiyang Min', 'Zihao Wang', 'Yizhi Li', 'Tianyu Zhang', 'He Xing', 'Zheng Zhang', 'Yuxuan Song', 'Tianyu Zheng', 'Zhiyuan Zeng', 'Chenghua Lin', 'Ge Zhang', 'Wenhao Huang'], 'affiliations': ['ByteDance', 'M-A-P', 'Mila - Quebec AI Institute', 'Tsinghua University', 'University of Manchester'], 'pdf_title_img': 'assets/pdf/title_img/2512.24617.jpg', 'data': {'categories': ['#training', '#architecture'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ¡Ğ¼Ğ°Ñ€Ñ‚-ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ: ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚Ñ‹ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚Ğ¾Ğ² (DLCM), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ²ÑĞµÑ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ Ğ¾Ğ´Ğ¸Ğ½Ğ°ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½ĞµÑ€Ğ°Ğ²Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² ÑĞ·Ñ‹ĞºĞµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑƒÑ‡Ğ¸Ñ‚ÑÑ Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ğ¸ ÑĞ¶Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ² ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾, Ğ³Ğ´Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑÑ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹Ğ²ĞµĞ»Ğ¸ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ·Ğ°ĞºĞ¾Ğ½ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ ÑƒÑ‡Ñ‘Ñ‚Ğ¾Ğ¼ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ÑÑ‚ÑŒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¾Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¾ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚Ğ°Ñ…. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ°Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ, Ğ¸ Ğ½Ğ° Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸ĞºĞµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ° 2.69% Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ñ‚Ğ¾Ğ³Ğ¾ Ğ¶Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Revolutionizing Language Models with Dynamic Concept Learning', 'desc': 'This paper introduces Dynamic Large Concept Models (DLCM), which improve how language models process information by focusing on important semantic transitions instead of treating all tokens equally. By learning semantic boundaries and shifting computation to a compressed concept space, DLCM enhances reasoning efficiency. The framework allows for the discovery of variable-length concepts without relying on fixed linguistic units, fundamentally changing how models scale. Additionally, it presents a new scaling law that optimizes compute allocation, leading to significant performance improvements in zero-shot tasks.'}, 'zh': {'title': 'åŠ¨æ€å¤§å‹æ¦‚å¿µæ¨¡å‹ï¼šæå‡è¯­è¨€æ¨¡å‹æ¨ç†æ•ˆç‡çš„åˆ›æ–°', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯¹æ‰€æœ‰æ ‡è®°åº”ç”¨ç»Ÿä¸€è®¡ç®—ï¼Œä½†è¯­è¨€çš„ä¿¡æ¯å¯†åº¦å¹¶ä¸å‡åŒ€ã€‚è¿™ç§ç»Ÿä¸€çš„è®¡ç®—æ–¹å¼åœ¨å¯é¢„æµ‹çš„åŒºåŸŸæµªè´¹äº†è®¡ç®—èƒ½åŠ›ï¼Œè€Œåœ¨è¯­ä¹‰å…³é”®çš„è½¬å˜ä¸Šåˆ™åˆ†é…ä¸è¶³ã€‚æˆ‘ä»¬æå‡ºäº†åŠ¨æ€å¤§å‹æ¦‚å¿µæ¨¡å‹ï¼ˆDLCMï¼‰ï¼Œå®ƒé€šè¿‡å­¦ä¹ æ½œåœ¨è¡¨ç¤ºçš„è¯­ä¹‰è¾¹ç•Œï¼Œå°†è®¡ç®—ä»æ ‡è®°è½¬ç§»åˆ°å‹ç¼©çš„æ¦‚å¿µç©ºé—´ï¼Œä»è€Œæé«˜æ¨ç†æ•ˆç‡ã€‚DLCMèƒ½å¤Ÿç«¯åˆ°ç«¯åœ°å‘ç°å¯å˜é•¿åº¦çš„æ¦‚å¿µï¼Œå¹¶å¼•å…¥äº†é¦–ä¸ªè€ƒè™‘å‹ç¼©çš„æ‰©å±•æ³•åˆ™ï¼Œä¼˜åŒ–äº†è®¡ç®—èµ„æºçš„åˆ†é…ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.24165', 'title': 'DiffThinker: Towards Generative Multimodal Reasoning with Diffusion Models', 'url': 'https://huggingface.co/papers/2512.24165', 'abstract': 'While recent Multimodal Large Language Models (MLLMs) have attained significant strides in multimodal reasoning, their reasoning processes remain predominantly text-centric, leading to suboptimal performance in complex long-horizon, vision-centric tasks. In this paper, we establish a novel Generative Multimodal Reasoning paradigm and introduce DiffThinker, a diffusion-based reasoning framework. Conceptually, DiffThinker reformulates multimodal reasoning as a native generative image-to-image task, achieving superior logical consistency and spatial precision in vision-centric tasks. We perform a systematic comparison between DiffThinker and MLLMs, providing the first in-depth investigation into the intrinsic characteristics of this paradigm, revealing four core properties: efficiency, controllability, native parallelism, and collaboration. Extensive experiments across four domains (sequential planning, combinatorial optimization, constraint satisfaction, and spatial configuration) demonstrate that DiffThinker significantly outperforms leading closed source models including GPT-5 (+314.2\\%) and Gemini-3-Flash (+111.6\\%), as well as the fine-tuned Qwen3-VL-32B baseline (+39.0\\%), highlighting generative multimodal reasoning as a promising approach for vision-centric reasoning.', 'score': 37, 'issue_id': 378, 'pub_date': '2026-12-30', 'pub_date_card': {'ru': '30 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 30', 'zh': '12æœˆ30æ—¥'}, 'hash': '7e8814af6b3a30c5', 'authors': ['Zefeng He', 'Xiaoye Qu', 'Yafu Li', 'Tong Zhu', 'Siyuan Huang', 'Yu Cheng'], 'affiliations': ['Nanjing University', 'Shanghai AI Laboratory', 'Shanghai Jiao Tong University', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2512.24165.jpg', 'data': {'categories': ['#cv', '#reasoning', '#diffusion', '#architecture', '#multimodal'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ”Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ: Ğ¾Ñ‚ Ñ‚ĞµĞºÑÑ‚Ğ° Ğº Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½Ğ°Ñ DiffThinker. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ ĞºĞ°Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆÑƒÑ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾-Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ° Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ, ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾ÑÑ‚ÑŒ, Ğ²ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»Ğ¸Ğ·Ğ¼ Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ… Ñ‚Ğ¸Ğ¿Ğ°Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ DiffThinker Ğ½Ğ°Ğ´ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ GPT-5 Ğ¸ Gemini-3-Flash.'}, 'en': {'title': 'Revolutionizing Vision-Centric Reasoning with DiffThinker', 'desc': 'This paper introduces DiffThinker, a new framework for Generative Multimodal Reasoning that enhances the reasoning capabilities of Multimodal Large Language Models (MLLMs) in vision-centric tasks. Unlike traditional MLLMs that focus primarily on text, DiffThinker treats multimodal reasoning as a generative image-to-image task, improving logical consistency and spatial accuracy. The authors highlight four key advantages of this approach: efficiency, controllability, native parallelism, and collaboration. Through extensive experiments, DiffThinker demonstrates significant performance improvements over existing models, establishing it as a leading method for complex reasoning in visual contexts.'}, 'zh': {'title': 'ç”Ÿæˆå¤šæ¨¡æ€æ¨ç†çš„æ–°çªç ´', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ç”Ÿæˆå¤šæ¨¡æ€æ¨ç†èŒƒå¼ï¼Œå¹¶å¼•å…¥äº†DiffThinkerï¼Œä¸€ä¸ªåŸºäºæ‰©æ•£çš„æ¨ç†æ¡†æ¶ã€‚DiffThinkerå°†å¤šæ¨¡æ€æ¨ç†é‡æ–°å®šä¹‰ä¸ºä¸€ç§ç”Ÿæˆçš„å›¾åƒåˆ°å›¾åƒä»»åŠ¡ï¼Œä»è€Œåœ¨è§†è§‰ä¸­å¿ƒä»»åŠ¡ä¸­å®ç°äº†æ›´é«˜çš„é€»è¾‘ä¸€è‡´æ€§å’Œç©ºé—´ç²¾åº¦ã€‚é€šè¿‡ä¸ç°æœ‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰è¿›è¡Œç³»ç»Ÿæ¯”è¾ƒï¼Œæ­ç¤ºäº†DiffThinkerçš„å››ä¸ªæ ¸å¿ƒç‰¹æ€§ï¼šæ•ˆç‡ã€å¯æ§æ€§ã€åŸç”Ÿå¹¶è¡Œæ€§å’Œåä½œæ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒDiffThinkeråœ¨å¤šä¸ªé¢†åŸŸçš„è¡¨ç°æ˜¾è‘—ä¼˜äºé¢†å…ˆçš„é—­æºæ¨¡å‹ï¼Œå±•ç¤ºäº†ç”Ÿæˆå¤šæ¨¡æ€æ¨ç†åœ¨è§†è§‰æ¨ç†ä¸­çš„æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.22630', 'title': 'On the Role of Discreteness in Diffusion LLMs', 'url': 'https://huggingface.co/papers/2512.22630', 'abstract': 'Diffusion models offer appealing properties for language generation, such as parallel decoding and iterative refinement, but the discrete and highly structured nature of text challenges the direct application of diffusion principles. In this paper, we revisit diffusion language modeling from the view of diffusion process and language modeling, and outline five properties that separate diffusion mechanics from language-specific requirements. We first categorize existing approaches into continuous diffusion in embedding space and discrete diffusion over tokens. We then show that each satisfies only part of the five essential properties and therefore reflects a structural trade-off. Through analyses of recent large diffusion language models, we identify two central issues: (i) uniform corruption does not respect how information is distributed across positions, and (ii) token-wise marginal training cannot capture multi-token dependencies during parallel decoding. These observations motivate diffusion processes that align more closely with the structure of text, and encourage future work toward more coherent diffusion language models.', 'score': 14, 'issue_id': 375, 'pub_date': '2026-12-27', 'pub_date_card': {'ru': '27 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 27', 'zh': '12æœˆ27æ—¥'}, 'hash': '8ee34765599976ba', 'authors': ['Ziqi Jin', 'Bin Wang', 'Xiang Lin', 'Lidong Bing', 'Aixin Sun'], 'affiliations': ['MiroMind AI', 'Nanyang Technological University'], 'pdf_title_img': 'assets/pdf/title_img/2512.22630.jpg', 'data': {'categories': ['#training', '#architecture', '#diffusion'], 'emoji': 'ğŸ”„', 'ru': {'title': 'Ğ”Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ñ Ğ´Ğ»Ñ Ñ‚ĞµĞºÑÑ‚Ğ°: Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ñ Ğ¿Ñ€Ğ¸Ñ€Ğ¾Ğ´Ğ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ°', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ÑÑ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾Ğ±Ğ»Ğ°Ğ´Ğ°ÑÑ‚ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ğ¼Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğ¼Ğ¸ ĞºĞ°Ğº Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿ĞµÑ€ĞµĞ¾ÑĞ¼Ñ‹ÑĞ»Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğº ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ²Ñ‹Ğ´ĞµĞ»ÑÑÑ‚ Ğ¿ÑÑ‚ÑŒ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ², Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸ĞºÑƒ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ¾Ñ‚ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğº Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ñ‚ĞµĞºÑÑ‚Ğ°. ĞĞ½Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹, Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑÑ Ğ¸Ñ… Ğ½Ğ° Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½ÑƒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ğ¸ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½ÑƒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ñ Ğ½Ğ°Ğ´ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼Ğ¸, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑÑ‹ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹: Ğ¾Ğ´Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ²Ñ€ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ½Ğµ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸ÑĞ¼, Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¼Ğ°Ñ€Ğ¶Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑÑ… Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ½Ğµ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ¸Ñ‚ÑŒ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ‚Ğ¾ĞºĞµĞ½Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸.'}, 'en': {'title': 'Enhancing Language Generation with Structured Diffusion Models', 'desc': 'This paper explores the challenges of applying diffusion models to language generation due to the structured nature of text. It categorizes existing methods into two types: continuous diffusion in embedding space and discrete diffusion over tokens, highlighting their limitations. The authors identify two main issues: the uniform corruption of information and the inability to capture dependencies between multiple tokens during decoding. They propose that future diffusion models should better align with the inherent structure of language to improve coherence in generated text.'}, 'zh': {'title': 'ä¼˜åŒ–æ‰©æ•£æ¨¡å‹ä»¥é€‚åº”è¯­è¨€ç»“æ„', 'desc': 'æ‰©æ•£æ¨¡å‹åœ¨è¯­è¨€ç”Ÿæˆä¸­å…·æœ‰å¸å¼•äººçš„ç‰¹æ€§ï¼Œå¦‚å¹¶è¡Œè§£ç å’Œè¿­ä»£ä¼˜åŒ–ï¼Œä½†æ–‡æœ¬çš„ç¦»æ•£æ€§å’Œé«˜åº¦ç»“æ„åŒ–ç‰¹å¾ä½¿å¾—ç›´æ¥åº”ç”¨æ‰©æ•£åŸç†é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬æ–‡ä»æ‰©æ•£è¿‡ç¨‹å’Œè¯­è¨€å»ºæ¨¡çš„è§’åº¦é‡æ–°å®¡è§†æ‰©æ•£è¯­è¨€å»ºæ¨¡ï¼Œå¹¶æ¦‚è¿°äº†äº”ä¸ªå°†æ‰©æ•£æœºåˆ¶ä¸è¯­è¨€ç‰¹å®šè¦æ±‚åŒºåˆ†å¼€æ¥çš„ç‰¹æ€§ã€‚æˆ‘ä»¬å°†ç°æœ‰æ–¹æ³•åˆ†ä¸ºåµŒå…¥ç©ºé—´ä¸­çš„è¿ç»­æ‰©æ•£å’Œæ ‡è®°ä¸Šçš„ç¦»æ•£æ‰©æ•£ï¼Œå¹¶æŒ‡å‡ºæ¯ç§æ–¹æ³•ä»…æ»¡è¶³äº”ä¸ªåŸºæœ¬ç‰¹æ€§ä¸­çš„ä¸€éƒ¨åˆ†ï¼Œå› æ­¤åæ˜ å‡ºç»“æ„ä¸Šçš„æƒè¡¡ã€‚é€šè¿‡å¯¹æœ€è¿‘å¤§å‹æ‰©æ•£è¯­è¨€æ¨¡å‹çš„åˆ†æï¼Œæˆ‘ä»¬è¯†åˆ«å‡ºä¸¤ä¸ªæ ¸å¿ƒé—®é¢˜ï¼šå‡åŒ€è…èš€ä¸å°Šé‡ä¿¡æ¯åœ¨ä½ç½®ä¸Šçš„åˆ†å¸ƒï¼Œä»¥åŠé€æ ‡è®°è¾¹é™…è®­ç»ƒæ— æ³•åœ¨å¹¶è¡Œè§£ç ä¸­æ•æ‰å¤šæ ‡è®°ä¾èµ–å…³ç³»ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.24766', 'title': 'Dream2Flow: Bridging Video Generation and Open-World Manipulation with 3D Object Flow', 'url': 'https://huggingface.co/papers/2512.24766', 'abstract': 'Generative video modeling has emerged as a compelling tool to zero-shot reason about plausible physical interactions for open-world manipulation. Yet, it remains a challenge to translate such human-led motions into the low-level actions demanded by robotic systems. We observe that given an initial image and task instruction, these models excel at synthesizing sensible object motions. Thus, we introduce Dream2Flow, a framework that bridges video generation and robotic control through 3D object flow as an intermediate representation. Our method reconstructs 3D object motions from generated videos and formulates manipulation as object trajectory tracking. By separating the state changes from the actuators that realize those changes, Dream2Flow overcomes the embodiment gap and enables zero-shot guidance from pre-trained video models to manipulate objects of diverse categories-including rigid, articulated, deformable, and granular. Through trajectory optimization or reinforcement learning, Dream2Flow converts reconstructed 3D object flow into executable low-level commands without task-specific demonstrations. Simulation and real-world experiments highlight 3D object flow as a general and scalable interface for adapting video generation models to open-world robotic manipulation. Videos and visualizations are available at https://dream2flow.github.io/.', 'score': 6, 'issue_id': 387, 'pub_date': '2026-12-31', 'pub_date_card': {'ru': '31 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 31', 'zh': '12æœˆ31æ—¥'}, 'hash': '7ee4e8c52d41ebf1', 'authors': ['Karthik Dharmarajan', 'Wenlong Huang', 'Jiajun Wu', 'Li Fei-Fei', 'Ruohan Zhang'], 'affiliations': ['Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2512.24766.jpg', 'data': {'categories': ['#robotics', '#rl', '#video', '#3d', '#open_source', '#reasoning'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞÑ‚ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğº Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞµ Ñ‡ĞµÑ€ĞµĞ· 3D-Ğ¿Ğ¾Ñ‚Ğ¾Ğº Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ²', 'desc': 'Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Dream2Flow â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ¼-Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ¾Ğ¼ Ğ±ĞµĞ· Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ° Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² ĞºĞ°Ğº Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ ÑĞ²ÑĞ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ½Ğ¸Ğ·ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ°Ğ¼Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ¼. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ 3D-Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸Ğ· ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ñ ĞºĞ°Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ ÑÑ‚Ğ¸Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹. Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ñ€Ñ„Ğ¾Ğ»Ğ¾Ğ³Ğ¸ĞµĞ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² â€” Ğ¶Ñ‘ÑÑ‚ĞºĞ¸Ğ¼Ğ¸, ÑĞ¾Ñ‡Ğ»ĞµĞ½Ñ‘Ğ½Ğ½Ñ‹Ğ¼Ğ¸, Ğ´ĞµÑ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ¸ ÑÑ‹Ğ¿ÑƒÑ‡Ğ¸Ğ¼Ğ¸ â€” Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ¸Ğ»Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼.'}, 'en': {'title': 'Bridging Video Generation and Robotic Control with Dream2Flow', 'desc': 'This paper presents Dream2Flow, a novel framework that connects generative video modeling with robotic control. It focuses on translating high-level human motions into low-level robotic actions by using 3D object flow as a key representation. The framework allows robots to manipulate various object types by tracking their trajectories, effectively bridging the gap between video generation and physical manipulation. Dream2Flow enables zero-shot learning, meaning it can adapt to new tasks without needing specific training for each one, showcasing its versatility in real-world applications.'}, 'zh': {'title': 'Dream2Flowï¼šè¿æ¥è§†é¢‘ç”Ÿæˆä¸æœºå™¨äººæ§åˆ¶çš„æ¡¥æ¢', 'desc': 'ç”Ÿæˆè§†é¢‘å»ºæ¨¡å·²æˆä¸ºä¸€ç§æœ‰æ•ˆå·¥å…·ï¼Œå¯ä»¥åœ¨å¼€æ”¾ä¸–ç•Œä¸­è¿›è¡Œåˆç†çš„ç‰©ç†äº¤äº’æ¨ç†ã€‚ç„¶è€Œï¼Œå°†äººç±»çš„åŠ¨ä½œè½¬åŒ–ä¸ºæœºå™¨äººç³»ç»Ÿæ‰€éœ€çš„ä½çº§åŠ¨ä½œä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†Dream2Flowæ¡†æ¶ï¼Œé€šè¿‡3Dç‰©ä½“æµä½œä¸ºä¸­é—´è¡¨ç¤ºï¼Œè¿æ¥è§†é¢‘ç”Ÿæˆå’Œæœºå™¨äººæ§åˆ¶ã€‚è¯¥æ–¹æ³•ä»ç”Ÿæˆçš„è§†é¢‘ä¸­é‡å»º3Dç‰©ä½“è¿åŠ¨ï¼Œå¹¶å°†æ“ä½œå½¢å¼åŒ–ä¸ºç‰©ä½“è½¨è¿¹è·Ÿè¸ªï¼Œä»è€Œå®ç°äº†é›¶æ ·æœ¬æŒ‡å¯¼ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.24724', 'title': 'FlowBlending: Stage-Aware Multi-Model Sampling for Fast and High-Fidelity Video Generation', 'url': 'https://huggingface.co/papers/2512.24724', 'abstract': 'In this work, we show that the impact of model capacity varies across timesteps: it is crucial for the early and late stages but largely negligible during the intermediate stage. Accordingly, we propose FlowBlending, a stage-aware multi-model sampling strategy that employs a large model and a small model at capacity-sensitive stages and intermediate stages, respectively. We further introduce simple criteria to choose stage boundaries and provide a velocity-divergence analysis as an effective proxy for identifying capacity-sensitive regions. Across LTX-Video (2B/13B) and WAN 2.1 (1.3B/14B), FlowBlending achieves up to 1.65x faster inference with 57.35% fewer FLOPs, while maintaining the visual fidelity, temporal coherence, and semantic alignment of the large models. FlowBlending is also compatible with existing sampling-acceleration techniques, enabling up to 2x additional speedup. Project page is available at: https://jibin86.github.io/flowblending_project_page.', 'score': 4, 'issue_id': 387, 'pub_date': '2026-12-31', 'pub_date_card': {'ru': '31 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 31', 'zh': '12æœˆ31æ—¥'}, 'hash': '9fe956fc654521e6', 'authors': ['Jibin Song', 'Mingi Kwon', 'Jaeseok Jeong', 'Youngjung Uh'], 'affiliations': ['Yonsei University'], 'pdf_title_img': 'assets/pdf/title_img/2512.24724.jpg', 'data': {'categories': ['#video', '#small_models', '#inference'], 'emoji': 'âš¡', 'ru': {'title': 'ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ¼ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ¼Ğ¾Ñ‰Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑÑ‚Ğ°Ğ¿Ñ‹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ğ° Ğ½Ğ° Ñ€Ğ°Ğ½Ğ½Ğ¸Ñ… Ğ¸ Ğ¿Ğ¾Ğ·Ğ´Ğ½Ğ¸Ñ… ÑÑ‚Ğ°Ğ´Ğ¸ÑÑ…, Ğ° Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ğ¿Ğ°Ñ… Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. ĞĞ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ FlowBlending â€” ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞµĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ¸ Ğ¼Ğ°Ğ»ĞµĞ½ÑŒĞºĞ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑÑ‚Ğ°Ğ¿Ğ° Ğº ĞµĞ¼ĞºĞ¾ÑÑ‚Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ´Ğ¾ 1.65x Ğ¸ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ½Ğ° 57% Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'Optimizing Model Capacity for Efficient Inference with FlowBlending', 'desc': 'This paper investigates how the capacity of machine learning models affects their performance at different stages of processing. It finds that model capacity is important at the beginning and end of the process, but not as much in the middle. To address this, the authors introduce FlowBlending, a method that uses a large model for critical stages and a smaller model for less critical ones. This approach significantly speeds up inference and reduces computational load while preserving the quality of the output.'}, 'zh': {'title': 'é˜¶æ®µæ„ŸçŸ¥çš„æ¨¡å‹é‡‡æ ·ç­–ç•¥ï¼šFlowBlending', 'desc': 'æœ¬ç ”ç©¶è¡¨æ˜ï¼Œæ¨¡å‹å®¹é‡åœ¨ä¸åŒæ—¶é—´æ­¥çš„å½±å“æ˜¯ä¸åŒçš„ï¼šåœ¨æ—©æœŸå’Œæ™šæœŸé˜¶æ®µè‡³å…³é‡è¦ï¼Œè€Œåœ¨ä¸­é—´é˜¶æ®µåˆ™å‡ ä¹å¯ä»¥å¿½ç•¥ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºFlowBlendingçš„é˜¶æ®µæ„ŸçŸ¥å¤šæ¨¡å‹é‡‡æ ·ç­–ç•¥ï¼Œåœ¨å¯¹å®¹é‡æ•æ„Ÿçš„é˜¶æ®µä½¿ç”¨å¤§æ¨¡å‹ï¼Œè€Œåœ¨ä¸­é—´é˜¶æ®µä½¿ç”¨å°æ¨¡å‹ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ç®€å•çš„æ ‡å‡†æ¥é€‰æ‹©é˜¶æ®µè¾¹ç•Œï¼Œå¹¶æä¾›äº†é€Ÿåº¦-å‘æ•£åˆ†æä½œä¸ºè¯†åˆ«å®¹é‡æ•æ„ŸåŒºåŸŸçš„æœ‰æ•ˆä»£ç†ã€‚é€šè¿‡åœ¨LTX-Videoå’ŒWAN 2.1ä¸Šçš„å®éªŒï¼ŒFlowBlendingå®ç°äº†é«˜è¾¾1.65å€çš„æ¨ç†é€Ÿåº¦æå‡ï¼ŒåŒæ—¶å‡å°‘äº†57.35%çš„FLOPsï¼Œä¿æŒäº†å¤§æ¨¡å‹çš„è§†è§‰ä¿çœŸåº¦ã€æ—¶é—´ä¸€è‡´æ€§å’Œè¯­ä¹‰å¯¹é½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.24007', 'title': 'TESO Tabu Enhanced Simulation Optimization for Noisy Black Box Problems', 'url': 'https://huggingface.co/papers/2512.24007', 'abstract': "Simulation optimization (SO) is frequently challenged by noisy evaluations, high computational costs, and complex, multimodal search landscapes. This paper introduces Tabu-Enhanced Simulation Optimization (TESO), a novel metaheuristic framework integrating adaptive search with memory-based strategies. TESO leverages a short-term Tabu List to prevent cycling and encourage diversification, and a long-term Elite Memory to guide intensification by perturbing high-performing solutions. An aspiration criterion allows overriding tabu restrictions for exceptional candidates. This combination facilitates a dynamic balance between exploration and exploitation in stochastic environments. We demonstrate TESO's effectiveness and reliability using an queue optimization problem, showing improved performance compared to benchmarks and validating the contribution of its memory components. Source code and data are available at: https://github.com/bulentsoykan/TESO.", 'score': 2, 'issue_id': 388, 'pub_date': '2026-12-30', 'pub_date_card': {'ru': '30 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 30', 'zh': '12æœˆ30æ—¥'}, 'hash': 'cd7eb2b7c3a9a044', 'authors': ['Bulent Soykan', 'Sean Mondesire', 'Ghaith Rabadi'], 'affiliations': ['University of Central Florida'], 'pdf_title_img': 'assets/pdf/title_img/2512.24007.jpg', 'data': {'categories': ['#optimization', '#open_source'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞŸĞ°Ğ¼ÑÑ‚ÑŒ Ğ¸ Ñ‚Ğ°Ğ±Ñƒ Ğ´Ğ»Ñ ÑƒĞ¼Ğ½Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ² ÑˆÑƒĞ¼Ğ½Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ…', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ TESO â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ°ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ÑÑ Ñ ÑˆÑƒĞ¼Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼Ğ¸ Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ»Ğ°Ğ½Ğ´ÑˆĞ°Ñ„Ñ‚Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ°. ĞĞ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ĞºÑ€Ğ°Ñ‚ĞºĞ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Tabu List Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ Ñ†Ğ¸ĞºĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ´Ğ¸Ğ²ĞµÑ€ÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½ÑƒÑ Elite Memory Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµĞ½ÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ±Ğ°Ğ·Ğµ Ğ²Ğ¾Ğ·Ğ¼ÑƒÑ‰ĞµĞ½Ğ¸Ğ¹ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. ĞšÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸Ğ¹ Ğ°ÑĞ¿Ğ¸Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑ‚ÑŒ tabu-Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¸ÑĞºĞ»ÑÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ ÑĞºÑĞ¿Ğ»ÑƒĞ°Ñ‚Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ² ÑÑ‚Ğ¾Ñ…Ğ°ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸ÑÑ…. Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ TESO Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ‡ĞµÑ€ĞµĞ´ĞµĞ¹, Ğ³Ğ´Ğµ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ» ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Balancing Exploration and Exploitation in Simulation Optimization with TESO', 'desc': 'This paper presents a new method called Tabu-Enhanced Simulation Optimization (TESO) to tackle challenges in simulation optimization, such as noise and high computational costs. TESO combines adaptive search techniques with memory strategies, using a short-term Tabu List to avoid revisiting previous solutions and a long-term Elite Memory to focus on improving the best solutions found. An aspiration criterion is included to allow for flexibility in exploring exceptional solutions despite tabu restrictions. The results show that TESO outperforms existing methods in a queue optimization problem, highlighting the effectiveness of its memory components.'}, 'zh': {'title': 'ç¦å¿Œå¢å¼ºæ¨¡æ‹Ÿä¼˜åŒ–ï¼šæ¢ç´¢ä¸å¼€å‘çš„åŠ¨æ€å¹³è¡¡', 'desc': 'æ¨¡æ‹Ÿä¼˜åŒ–ï¼ˆSOï¼‰å¸¸å¸¸é¢ä¸´å™ªå£°è¯„ä¼°ã€é«˜è®¡ç®—æˆæœ¬å’Œå¤æ‚çš„å¤šæ¨¡æ€æœç´¢ç¯å¢ƒçš„æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å…ƒå¯å‘å¼æ¡†æ¶â€”â€”ç¦å¿Œå¢å¼ºæ¨¡æ‹Ÿä¼˜åŒ–ï¼ˆTESOï¼‰ï¼Œå®ƒå°†è‡ªé€‚åº”æœç´¢ä¸åŸºäºè®°å¿†çš„ç­–ç•¥ç›¸ç»“åˆã€‚TESOåˆ©ç”¨çŸ­æœŸç¦å¿Œåˆ—è¡¨æ¥é˜²æ­¢å¾ªç¯å¹¶é¼“åŠ±å¤šæ ·åŒ–ï¼ŒåŒæ—¶ä½¿ç”¨é•¿æœŸç²¾è‹±è®°å¿†æ¥é€šè¿‡æ‰°åŠ¨é«˜æ€§èƒ½è§£æ¥æŒ‡å¯¼å¼ºåŒ–ã€‚é€šè¿‡åœ¨éšæœºç¯å¢ƒä¸­åŠ¨æ€å¹³è¡¡æ¢ç´¢ä¸å¼€å‘ï¼ŒTESOåœ¨é˜Ÿåˆ—ä¼˜åŒ–é—®é¢˜ä¸Šå±•ç¤ºäº†å…¶æœ‰æ•ˆæ€§å’Œå¯é æ€§ï¼Œä¸”æ€§èƒ½ä¼˜äºåŸºå‡†ï¼ŒéªŒè¯äº†å…¶è®°å¿†ç»„ä»¶çš„è´¡çŒ®ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.00393', 'title': 'NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos', 'url': 'https://huggingface.co/papers/2601.00393', 'abstract': 'In this paper, we propose NeoVerse, a versatile 4D world model that is capable of 4D reconstruction, novel-trajectory video generation, and rich downstream applications. We first identify a common limitation of scalability in current 4D world modeling methods, caused either by expensive and specialized multi-view 4D data or by cumbersome training pre-processing. In contrast, our NeoVerse is built upon a core philosophy that makes the full pipeline scalable to diverse in-the-wild monocular videos. Specifically, NeoVerse features pose-free feed-forward 4D reconstruction, online monocular degradation pattern simulation, and other well-aligned techniques. These designs empower NeoVerse with versatility and generalization to various domains. Meanwhile, NeoVerse achieves state-of-the-art performance in standard reconstruction and generation benchmarks. Our project page is available at https://neoverse-4d.github.io', 'score': 74, 'issue_id': 407, 'pub_date': '2026-01-01', 'pub_date_card': {'ru': '1 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 1', 'zh': '1æœˆ1æ—¥'}, 'hash': '4b85bcdecc1e3e88', 'authors': ['Yuxue Yang', 'Lue Fan', 'Ziqi Shi', 'Junran Peng', 'Feng Wang', 'Zhaoxiang Zhang'], 'affiliations': ['CreateAI', 'NLPR & MAIS, CASIA'], 'pdf_title_img': 'assets/pdf/title_img/2601.00393.jpg', 'data': {'categories': ['#benchmark', '#video', '#3d'], 'emoji': 'ğŸ¬', 'ru': {'title': 'ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ°Ñ 4D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾Ğ¹ ĞºĞ°Ğ¼ĞµÑ€Ñ‹', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° NeoVerse â€” ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ 4D-Ğ¼Ğ¸Ñ€Ğ°, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ°Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ 4D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ½Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² 4D-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‚ Ğ´Ğ¾Ñ€Ğ¾Ğ³Ğ¾ÑÑ‚Ğ¾ÑÑ‰Ğ¸Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ²Ğ¸Ğ´Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ»Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¹ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸. NeoVerse Ñ€ĞµÑˆĞ°ĞµÑ‚ ÑÑ‚Ñƒ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‰ĞµĞ¹ ÑĞ¾ ÑĞºÑ€Ğ¾Ğ¼Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ½Ğ¾ĞºÑƒĞ»ÑÑ€Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ°, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ pose-free feed-forward Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ğ¸ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ñ Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ğ¾Ğ² Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ³Ğ¾ÑÑƒĞ´Ğ°Ñ€ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ° Ğ² Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'NeoVerse: Scalable 4D World Modeling for All!', 'desc': 'This paper introduces NeoVerse, a 4D world model designed for efficient 4D reconstruction and video generation. It addresses the scalability issues found in existing methods that rely on complex multi-view data and extensive pre-processing. NeoVerse utilizes a pose-free feed-forward approach and simulates monocular degradation patterns, allowing it to work effectively with standard monocular videos. As a result, NeoVerse demonstrates superior performance in reconstruction and generation tasks across various applications.'}, 'zh': {'title': 'NeoVerseï¼šå¤šåŠŸèƒ½çš„4Dä¸–ç•Œæ¨¡å‹', 'desc': 'æœ¬æ–‡æå‡ºäº†NeoVerseï¼Œä¸€ä¸ªå¤šåŠŸèƒ½çš„4Dä¸–ç•Œæ¨¡å‹ï¼Œèƒ½å¤Ÿè¿›è¡Œ4Dé‡å»ºã€ç”Ÿæˆæ–°è½¨è¿¹è§†é¢‘ä»¥åŠä¸°å¯Œçš„ä¸‹æ¸¸åº”ç”¨ã€‚æˆ‘ä»¬é¦–å…ˆè¯†åˆ«äº†å½“å‰4Dä¸–ç•Œå»ºæ¨¡æ–¹æ³•åœ¨å¯æ‰©å±•æ€§æ–¹é¢çš„å…±åŒé™åˆ¶ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºæ˜‚è´µä¸”ä¸“ä¸šçš„å¤šè§†è§’4Dæ•°æ®æˆ–ç¹ççš„è®­ç»ƒé¢„å¤„ç†ã€‚ä¸æ­¤ä¸åŒï¼ŒNeoVerseåŸºäºä¸€ç§æ ¸å¿ƒç†å¿µï¼Œä½¿æ•´ä¸ªæµç¨‹èƒ½å¤Ÿæ‰©å±•åˆ°å„ç§é‡å¤–å•ç›®è§†é¢‘ã€‚NeoVerseå…·æœ‰æ— å§¿æ€å‰é¦ˆ4Dé‡å»ºã€åœ¨çº¿å•ç›®é™è§£æ¨¡å¼æ¨¡æ‹Ÿç­‰æŠ€æœ¯ï¼Œèµ‹äºˆå…¶åœ¨å¤šä¸ªé¢†åŸŸçš„å¤šæ ·æ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼ŒåŒæ—¶åœ¨æ ‡å‡†é‡å»ºå’Œç”ŸæˆåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.24615', 'title': 'Youtu-Agent: Scaling Agent Productivity with Automated Generation and Hybrid Policy Optimization', 'url': 'https://huggingface.co/papers/2512.24615', 'abstract': 'Existing Large Language Model (LLM) agent frameworks face two significant challenges: high configuration costs and static capabilities. Building a high-quality agent often requires extensive manual effort in tool integration and prompt engineering, while deployed agents struggle to adapt to dynamic environments without expensive fine-tuning. To address these issues, we propose Youtu-Agent, a modular framework designed for the automated generation and continuous evolution of LLM agents. Youtu-Agent features a structured configuration system that decouples execution environments, toolkits, and context management, enabling flexible reuse and automated synthesis. We introduce two generation paradigms: a Workflow mode for standard tasks and a Meta-Agent mode for complex, non-standard requirements, capable of automatically generating tool code, prompts, and configurations. Furthermore, Youtu-Agent establishes a hybrid policy optimization system: (1) an Agent Practice module that enables agents to accumulate experience and improve performance through in-context optimization without parameter updates; and (2) an Agent RL module that integrates with distributed training frameworks to enable scalable and stable reinforcement learning of any Youtu-Agents in an end-to-end, large-scale manner. Experiments demonstrate that Youtu-Agent achieves state-of-the-art performance on WebWalkerQA (71.47\\%) and GAIA (72.8\\%) using open-weight models. Our automated generation pipeline achieves over 81\\% tool synthesis success rate, while the Practice module improves performance on AIME 2024/2025 by +2.7\\% and +5.4\\% respectively. Moreover, our Agent RL training achieves 40\\% speedup with steady performance improvement on 7B LLMs, enhancing coding/reasoning and searching capabilities respectively up to 35\\% and 21\\% on Maths and general/multi-hop QA benchmarks.', 'score': 73, 'issue_id': 405, 'pub_date': '2026-12-31', 'pub_date_card': {'ru': '31 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 31', 'zh': '12æœˆ31æ—¥'}, 'hash': '40053cb88310e90a', 'authors': ['Yuchen Shi', 'Yuzheng Cai', 'Siqi Cai', 'Zihan Xu', 'Lichao Chen', 'Yulei Qin', 'Zhijian Zhou', 'Xiang Fei', 'Chaofan Qiu', 'Xiaoyu Tan', 'Gang Li', 'Zongyi Li', 'Haojia Lin', 'Guocan Cai', 'Yong Mao', 'Yunsheng Wu', 'Ke Li', 'Xing Sun'], 'affiliations': ['Tencent'], 'pdf_title_img': 'assets/pdf/title_img/2512.24615.jpg', 'data': {'categories': ['#rl', '#reasoning', '#benchmark', '#optimization', '#training', '#agents'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ±ĞµĞ· Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Youtu-Agent â€” Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ñ€ĞµÑˆĞ°ÑÑ‰Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ½Ğ° ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ, Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ³Ğ¸Ğ±ĞºĞ¾ Ğ¿ĞµÑ€ĞµĞ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹ Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´Ğ²Ğ° Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸: Ñ€ĞµĞ¶Ğ¸Ğ¼ Workflow Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ñ€ĞµĞ¶Ğ¸Ğ¼ Meta-Agent Ğ´Ğ»Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ ĞºĞ¾Ğ´Ğ° Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ¸ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¹. ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ°Ñ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Agent Practice Ğ´Ğ»Ñ Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ¿Ñ‹Ñ‚Ğ° Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Agent RL Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ² Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾Ğ¹ ÑÑ€ĞµĞ´Ğµ.'}, 'en': {'title': 'Youtu-Agent: Revolutionizing LLM Agent Flexibility and Performance', 'desc': 'The paper introduces Youtu-Agent, a modular framework that addresses the challenges of high configuration costs and static capabilities in existing Large Language Model (LLM) agent frameworks. It allows for automated generation and continuous evolution of LLM agents through a structured configuration system that separates execution environments, toolkits, and context management. Youtu-Agent employs two generation paradigms: Workflow mode for standard tasks and Meta-Agent mode for complex tasks, enabling the automatic creation of tool code and prompts. Additionally, it features a hybrid policy optimization system that enhances agent performance through in-context optimization and scalable reinforcement learning, achieving state-of-the-art results in various benchmarks.'}, 'zh': {'title': 'Youtu-Agentï¼šè‡ªåŠ¨åŒ–ç”Ÿæˆä¸æŒç»­æ¼”åŒ–çš„LLMä»£ç†æ¡†æ¶', 'desc': 'ç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†æ¡†æ¶é¢ä¸´ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šé«˜é…ç½®æˆæœ¬å’Œé™æ€èƒ½åŠ›ã€‚æ„å»ºé«˜è´¨é‡ä»£ç†é€šå¸¸éœ€è¦å¤§é‡çš„æ‰‹åŠ¨å·¥å…·é›†æˆå’Œæç¤ºå·¥ç¨‹ï¼Œè€Œå·²éƒ¨ç½²çš„ä»£ç†åœ¨åŠ¨æ€ç¯å¢ƒä¸­éš¾ä»¥é€‚åº”ï¼Œä¸”ä¸æ˜“è¿›è¡Œæ˜‚è´µçš„å¾®è°ƒã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Youtu-Agentï¼Œä¸€ä¸ªæ—¨åœ¨è‡ªåŠ¨ç”Ÿæˆå’ŒæŒç»­æ¼”åŒ–LLMä»£ç†çš„æ¨¡å—åŒ–æ¡†æ¶ã€‚Youtu-Agenté€šè¿‡ç»“æ„åŒ–é…ç½®ç³»ç»Ÿå®ç°æ‰§è¡Œç¯å¢ƒã€å·¥å…·åŒ…å’Œä¸Šä¸‹æ–‡ç®¡ç†çš„è§£è€¦ï¼Œæ”¯æŒçµæ´»é‡ç”¨å’Œè‡ªåŠ¨åˆæˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.00664', 'title': 'Avatar Forcing: Real-Time Interactive Head Avatar Generation for Natural Conversation', 'url': 'https://huggingface.co/papers/2601.00664', 'abstract': "A framework called Avatar Forcing uses diffusion forcing and direct preference optimization with synthetic losing samples to enable real-time, expressive multimodal interactions in talking head avatars without labeled data.  \t\t\t\t\tAI-generated summary \t\t\t\t Talking head generation creates lifelike avatars from static portraits for virtual communication and content creation. However, current models do not yet convey the feeling of truly interactive communication, often generating one-way responses that lack emotional engagement. We identify two key challenges toward truly interactive avatars: generating motion in real-time under causal constraints and learning expressive, vibrant reactions without additional labeled data. To address these challenges, we propose Avatar Forcing, a new framework for interactive head avatar generation that models real-time user-avatar interactions through diffusion forcing. This design allows the avatar to process real-time multimodal inputs, including the user's audio and motion, with low latency for instant reactions to both verbal and non-verbal cues such as speech, nods, and laughter. Furthermore, we introduce a direct preference optimization method that leverages synthetic losing samples constructed by dropping user conditions, enabling label-free learning of expressive interaction. Experimental results demonstrate that our framework enables real-time interaction with low latency (approximately 500ms), achieving 6.8X speedup compared to the baseline, and produces reactive and expressive avatar motion, which is preferred over 80% against the baseline.", 'score': 40, 'issue_id': 405, 'pub_date': '2026-01-02', 'pub_date_card': {'ru': '2 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 2', 'zh': '1æœˆ2æ—¥'}, 'hash': 'f2dbe0c7e036ea97', 'authors': ['Taekyung Ki', 'Sangwon Jang', 'Jaehyeong Jo', 'Jaehong Yoon', 'Sung Ju Hwang'], 'affiliations': ['DeepAuto.ai', 'KAIST', 'NTU Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2601.00664.jpg', 'data': {'categories': ['#video', '#optimization', '#training', '#multimodal', '#synthetic', '#diffusion', '#rlhf'], 'emoji': 'ğŸ­', 'ru': {'title': 'Ğ˜Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ°Ğ²Ğ°Ñ‚Ğ°Ñ€Ñ‹ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¸Ğ½ÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ', 'desc': 'Avatar Forcing â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ°Ğ²Ğ°Ñ‚Ğ°Ñ€Ğ¾Ğ² Ñ Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰ĞµĞ¹ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¸Ğ½ÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸ÑÑ… Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ°Ğ²Ğ°Ñ‚Ğ°Ñ€Ñƒ Ñ€ĞµĞ°Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ° Ñ€ĞµÑ‡ÑŒ, ĞºĞ¸Ğ²ĞºĞ¸ Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ½ĞµĞ²ĞµÑ€Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ñ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¾Ñ‚Ñ€Ğ¸Ñ†Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ€Ğ°Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ñ€ĞµĞ°ĞºÑ†Ğ¸ÑĞ¼ Ğ±ĞµĞ· Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ² 6.8 Ñ€Ğ°Ğ·Ğ° Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ 80% Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğµ Ğ² ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¸ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Real-Time, Expressive Interactions with Avatar Forcing', 'desc': 'The paper introduces Avatar Forcing, a novel framework designed to enhance real-time interactions with talking head avatars. It utilizes diffusion forcing and direct preference optimization to generate expressive responses without the need for labeled data. The framework addresses challenges in creating lifelike avatars that can react to both verbal and non-verbal cues in real-time. Experimental results show significant improvements in interaction speed and expressiveness, making avatars more engaging for users.'}, 'zh': {'title': 'å®æ—¶äº¤äº’ï¼Œç”ŸåŠ¨å¤´åƒçš„æœªæ¥', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºAvatar Forcingçš„æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°å®æ—¶ã€å¯Œæœ‰è¡¨ç°åŠ›çš„å¤šæ¨¡æ€äº¤äº’ï¼Œç‰¹åˆ«æ˜¯åœ¨å¯¹è¯å¤´åƒç”Ÿæˆä¸­ã€‚è¯¥æ¡†æ¶ç»“åˆäº†æ‰©æ•£å¼ºåˆ¶å’Œç›´æ¥åå¥½ä¼˜åŒ–ï¼Œèƒ½å¤Ÿåœ¨æ²¡æœ‰æ ‡è®°æ•°æ®çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡åˆæˆçš„å¤±è´¥æ ·æœ¬è¿›è¡Œå­¦ä¹ ã€‚Avatar Forcingèƒ½å¤Ÿå¤„ç†ç”¨æˆ·çš„éŸ³é¢‘å’ŒåŠ¨ä½œè¾“å…¥ï¼Œå®ç°ä½å»¶è¿Ÿçš„å³æ—¶ååº”ï¼Œæå‡äº†äº¤äº’çš„çœŸå®æ„Ÿå’Œæƒ…æ„Ÿå‚ä¸åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å®æ—¶äº¤äº’ä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„é€Ÿåº¦æå‡å’Œæ›´å…·è¡¨ç°åŠ›çš„å¤´åƒåŠ¨ä½œã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.24330', 'title': 'SenseNova-MARS: Empowering Multimodal Agentic Reasoning and Search via Reinforcement Learning', 'url': 'https://huggingface.co/papers/2512.24330', 'abstract': "While Vision-Language Models (VLMs) can solve complex tasks through agentic reasoning, their capabilities remain largely constrained to text-oriented chain-of-thought or isolated tool invocation. They fail to exhibit the human-like proficiency required to seamlessly interleave dynamic tool manipulation with continuous reasoning, particularly in knowledge-intensive and visually complex scenarios that demand coordinated external tools such as search and image cropping. In this work, we introduce SenseNova-MARS, a novel Multimodal Agentic Reasoning and Search framework that empowers VLMs with interleaved visual reasoning and tool-use capabilities via reinforcement learning (RL). Specifically, SenseNova-MARS dynamically integrates the image search, text search, and image crop tools to tackle fine-grained and knowledge-intensive visual understanding challenges. In the RL stage, we propose the Batch-Normalized Group Sequence Policy Optimization (BN-GSPO) algorithm to improve the training stability and advance the model's ability to invoke tools and reason effectively. To comprehensively evaluate the agentic VLMs on complex visual tasks, we introduce the HR-MMSearch benchmark, the first search-oriented benchmark composed of high-resolution images with knowledge-intensive and search-driven questions. Experiments demonstrate that SenseNova-MARS achieves state-of-the-art performance on open-source search and fine-grained image understanding benchmarks. Specifically, on search-oriented benchmarks, SenseNova-MARS-8B scores 67.84 on MMSearch and 41.64 on HR-MMSearch, surpassing proprietary models such as Gemini-3-Flash and GPT-5. SenseNova-MARS represents a promising step toward agentic VLMs by providing effective and robust tool-use capabilities. To facilitate further research in this field, we will release all code, models, and datasets.", 'score': 29, 'issue_id': 405, 'pub_date': '2026-12-30', 'pub_date_card': {'ru': '30 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 30', 'zh': '12æœˆ30æ—¥'}, 'hash': 'ff90ddac66fbb641', 'authors': ['Yong Xien Chng', 'Tao Hu', 'Wenwen Tong', 'Xueheng Li', 'Jiandong Chen', 'Haojia Yu', 'Jiefan Lu', 'Hewei Guo', 'Hanming Deng', 'Chengjun Xie', 'Gao Huang', 'Dahua Lin', 'Lewei Lu'], 'affiliations': ['SenseTime Research', 'Tsinghua University', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2512.24330.jpg', 'data': {'categories': ['#rl', '#reasoning', '#benchmark', '#dataset', '#cv', '#open_source', '#optimization', '#training', '#multimodal', '#agents'], 'emoji': 'ğŸ”', 'ru': {'title': 'ĞĞ³ĞµĞ½Ñ‚ÑĞºĞ¾Ğµ Ğ²Ğ¸Ğ´ĞµĞ½Ğ¸Ğµ: Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° SenseNova-MARS â€” Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚ÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² (Ğ¿Ğ¾Ğ¸ÑĞº Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¿Ğ¾Ğ¸ÑĞº Ñ‚ĞµĞºÑÑ‚Ğ°, ĞºĞ°Ğ´Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ) Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ BN-GSPO Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑĞ¼. Ğ”Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ°Ğ³ĞµĞ½Ñ‚ÑĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ²ĞµĞ´ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº HR-MMSearch, ÑĞ¾ÑÑ‚Ğ¾ÑÑ‰Ğ¸Ğ¹ Ğ¸Ğ· Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ ÑĞ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼Ğ¸, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ SenseNova-MARS Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ñ… Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ´Ğ°Ğ¶Ğµ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Gemini Ğ¸ GPT-5.'}, 'en': {'title': 'Empowering VLMs with Dynamic Tool Manipulation and Reasoning', 'desc': "This paper presents SenseNova-MARS, a new framework designed to enhance Vision-Language Models (VLMs) by enabling them to perform dynamic tool manipulation alongside continuous reasoning. Unlike traditional VLMs that struggle with complex tasks requiring coordinated tool use, SenseNova-MARS integrates image search, text search, and image cropping tools through reinforcement learning. The authors introduce a novel training algorithm called Batch-Normalized Group Sequence Policy Optimization (BN-GSPO) to improve the model's stability and effectiveness in invoking tools. The framework is evaluated using the HR-MMSearch benchmark, demonstrating superior performance in knowledge-intensive visual tasks compared to existing models."}, 'zh': {'title': 'æå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„æ™ºèƒ½æ¨ç†èƒ½åŠ›', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„å¤šæ¨¡æ€æ™ºèƒ½æ¨ç†å’Œæœç´¢æ¡†æ¶SenseNova-MARSï¼Œæ—¨åœ¨æå‡è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨åŠ¨æ€å·¥å…·æ“ä½œä¸è¿ç»­æ¨ç†ä¸­çš„èƒ½åŠ›ã€‚é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼ŒSenseNova-MARSèƒ½å¤Ÿå°†å›¾åƒæœç´¢ã€æ–‡æœ¬æœç´¢å’Œå›¾åƒè£å‰ªå·¥å…·åŠ¨æ€æ•´åˆï¼Œä»¥åº”å¯¹å¤æ‚çš„è§†è§‰ç†è§£ä»»åŠ¡ã€‚æˆ‘ä»¬æå‡ºçš„Batch-Normalized Group Sequence Policy Optimizationï¼ˆBN-GSPOï¼‰ç®—æ³•æé«˜äº†è®­ç»ƒçš„ç¨³å®šæ€§ï¼Œå¢å¼ºäº†æ¨¡å‹è°ƒç”¨å·¥å…·å’Œæœ‰æ•ˆæ¨ç†çš„èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSenseNova-MARSåœ¨å¼€æ”¾æºä»£ç çš„æœç´¢å’Œç»†ç²’åº¦å›¾åƒç†è§£åŸºå‡†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå±•ç¤ºäº†å…¶åœ¨æ™ºèƒ½VLMsé¢†åŸŸçš„æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.24271', 'title': "Taming Hallucinations: Boosting MLLMs' Video Understanding via Counterfactual Video Generation", 'url': 'https://huggingface.co/papers/2512.24271', 'abstract': 'Multimodal Large Language Models (MLLMs) have made remarkable progress in video understanding. However, they suffer from a critical vulnerability: an over-reliance on language priors, which can lead to visual ungrounded hallucinations, especially when processing counterfactual videos that defy common sense. This limitation, stemming from the intrinsic data imbalance between text and video, is challenging to address due to the substantial cost of collecting and annotating counterfactual data. To address this, we introduce DualityForge, a novel counterfactual data synthesis framework that employs controllable, diffusion-based video editing to transform real-world videos into counterfactual scenarios. By embedding structured contextual information into the video editing and QA generation processes, the framework automatically produces high-quality QA pairs together with original-edited video pairs for contrastive training. Based on this, we build DualityVidQA, a large-scale video dataset designed to reduce MLLM hallucinations. In addition, to fully exploit the contrastive nature of our paired data, we propose Duality-Normalized Advantage Training (DNA-Train), a two-stage SFT-RL training regime where the RL phase applies pair-wise ell_1 advantage normalization, thereby enabling a more stable and efficient policy optimization. Experiments on DualityVidQA-Test demonstrate that our method substantially reduces model hallucinations on counterfactual videos, yielding a relative improvement of 24.0% over the Qwen2.5-VL-7B baseline. Moreover, our approach achieves significant gains across both hallucination and general-purpose benchmarks, indicating strong generalization capability. We will open-source our dataset and code.', 'score': 23, 'issue_id': 405, 'pub_date': '2026-12-30', 'pub_date_card': {'ru': '30 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 30', 'zh': '12æœˆ30æ—¥'}, 'hash': '01479f4c4234799d', 'authors': ['Zhe Huang', 'Hao Wen', 'Aiming Hao', 'Bingze Song', 'Meiqi Wu', 'Jiahong Wu', 'Xiangxiang Chu', 'Sheng Lu', 'Haoqian Wang'], 'affiliations': ['AMAP, Alibaba Group', 'Beihang University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2512.24271.jpg', 'data': {'categories': ['#rl', '#dataset', '#video', '#open_source', '#training', '#multimodal', '#synthetic', '#diffusion', '#hallucinations'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ‘Ğ¾Ñ€ÑŒĞ±Ğ° Ñ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ‡ĞµÑ€ĞµĞ· ÑĞ¸Ğ½Ñ‚ĞµĞ· ĞºĞ¾Ğ½Ñ‚Ñ€Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…', 'desc': 'Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ‡Ñ€ĞµĞ·Ğ¼ĞµÑ€Ğ½Ğ¾ Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ÑÑ‚ÑÑ Ğ½Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ DualityForge â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° ĞºĞ¾Ğ½Ñ‚Ñ€Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ½ĞµĞ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ² Ğ¸Ğ· Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ DualityVidQA Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ DNA-Train, ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ supervised fine-tuning Ğ¸ reinforcement learning Ñ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¿Ğ°Ñ€Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ½Ğ° 24% Ğ¸ Ñ…Ğ¾Ñ€Ğ¾ÑˆÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ….'}, 'en': {'title': 'Transforming Video Understanding with Counterfactual Data Synthesis', 'desc': 'This paper addresses the limitations of Multimodal Large Language Models (MLLMs) in video understanding, particularly their tendency to generate visual hallucinations when faced with counterfactual scenarios. The authors introduce DualityForge, a framework that synthesizes counterfactual data through advanced video editing techniques, allowing for the creation of high-quality question-answer pairs. They also present DualityVidQA, a large-scale dataset aimed at training MLLMs to reduce hallucinations by providing contrastive examples. Additionally, the proposed Duality-Normalized Advantage Training (DNA-Train) enhances the training process, leading to significant improvements in model performance on both hallucination and general benchmarks.'}, 'zh': {'title': 'å‡å°‘å¹»è§‰ï¼Œæå‡è§†é¢‘ç†è§£çš„åŒé‡åŠ›é‡', 'desc': 'å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†é¢‘ç†è§£æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å®ƒä»¬å­˜åœ¨ä¸€ä¸ªå…³é”®çš„å¼±ç‚¹ï¼šè¿‡åº¦ä¾èµ–è¯­è¨€å…ˆéªŒï¼Œå¯¼è‡´åœ¨å¤„ç†è¿åå¸¸è¯†çš„åäº‹å®è§†é¢‘æ—¶å‡ºç°è§†è§‰æ— æ ¹çš„å¹»è§‰ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†DualityForgeï¼Œä¸€ä¸ªæ–°é¢–çš„åäº‹å®æ•°æ®åˆæˆæ¡†æ¶ï¼Œé€šè¿‡å¯æ§çš„æ‰©æ•£è§†é¢‘ç¼–è¾‘å°†çœŸå®è§†é¢‘è½¬åŒ–ä¸ºåäº‹å®åœºæ™¯ã€‚è¯¥æ¡†æ¶åœ¨è§†é¢‘ç¼–è¾‘å’Œé—®ç­”ç”Ÿæˆè¿‡ç¨‹ä¸­åµŒå…¥ç»“æ„åŒ–ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œè‡ªåŠ¨ç”Ÿæˆé«˜è´¨é‡çš„é—®ç­”å¯¹åŠåŸå§‹-ç¼–è¾‘è§†é¢‘å¯¹ï¼Œä»¥ä¾¿è¿›è¡Œå¯¹æ¯”è®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—å‡å°‘äº†æ¨¡å‹åœ¨åäº‹å®è§†é¢‘ä¸Šçš„å¹»è§‰ï¼Œæ˜¾ç¤ºå‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.00796', 'title': 'AdaGaR: Adaptive Gabor Representation for Dynamic Scene Reconstruction', 'url': 'https://huggingface.co/papers/2601.00796', 'abstract': 'Reconstructing dynamic 3D scenes from monocular videos requires simultaneously capturing high-frequency appearance details and temporally continuous motion. Existing methods using single Gaussian primitives are limited by their low-pass filtering nature, while standard Gabor functions introduce energy instability. Moreover, lack of temporal continuity constraints often leads to motion artifacts during interpolation. We propose AdaGaR, a unified framework addressing both frequency adaptivity and temporal continuity in explicit dynamic scene modeling. We introduce Adaptive Gabor Representation, extending Gaussians through learnable frequency weights and adaptive energy compensation to balance detail capture and stability. For temporal continuity, we employ Cubic Hermite Splines with Temporal Curvature Regularization to ensure smooth motion evolution. An Adaptive Initialization mechanism combining depth estimation, point tracking, and foreground masks establishes stable point cloud distributions in early training. Experiments on Tap-Vid DAVIS demonstrate state-of-the-art performance (PSNR 35.49, SSIM 0.9433, LPIPS 0.0723) and strong generalization across frame interpolation, depth consistency, video editing, and stereo view synthesis. Project page: https://jiewenchan.github.io/AdaGaR/', 'score': 17, 'issue_id': 408, 'pub_date': '2026-01-02', 'pub_date_card': {'ru': '2 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 2', 'zh': '1æœˆ2æ—¥'}, 'hash': 'eb50cf753bd4888a', 'authors': ['Jiewen Chan', 'Zhenjun Zhao', 'Yu-Lun Liu'], 'affiliations': ['National Yang Ming Chiao Tung University', 'University of Zaragoza'], 'pdf_title_img': 'assets/pdf/title_img/2601.00796.jpg', 'data': {'categories': ['#video', '#3d', '#architecture'], 'emoji': 'ğŸ¬', 'ru': {'title': 'ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ“Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ»Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ†ĞµĞ½ Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ AdaGaR Ğ´Ğ»Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… 3D ÑÑ†ĞµĞ½ Ğ¸Ğ· Ğ¼Ğ¾Ğ½Ğ¾ĞºÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°ĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ³Ğ°ÑƒÑÑĞ¸Ğ°Ğ½Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ“Ğ°Ğ±Ğ¾Ñ€Ğ° Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ²ĞµÑĞ°Ğ¼Ğ¸ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚ Ğ¸ ĞºĞ¾Ğ¼Ğ¿ĞµĞ½ÑĞ°Ñ†Ğ¸ĞµĞ¹ ÑĞ½ĞµÑ€Ğ³Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ñ‹ Ğ±ĞµĞ· Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ğ¾Ğ² Ğ½ĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. Ğ”Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ÑÑ ĞºÑƒĞ±Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ¿Ğ»Ğ°Ğ¹Ğ½Ñ‹ Ğ­Ñ€Ğ¼Ğ¸Ñ‚Ğ° Ñ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ ĞºÑ€Ğ¸Ğ²Ğ¸Ğ·Ğ½Ñ‹. ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹, Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡ĞµĞº Ğ¸ Ğ¼Ğ°ÑĞºĞ¸ Ğ¿ĞµÑ€ĞµĞ´Ğ½ĞµĞ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ°, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±Ğ»Ğ°ĞºĞ° Ñ‚Ğ¾Ñ‡ĞµĞº Ğ½Ğ° Ñ€Ğ°Ğ½Ğ½Ğ¸Ñ… ÑÑ‚Ğ°Ğ¿Ğ°Ñ… Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'AdaGaR: Enhancing 3D Scene Reconstruction with Adaptive Gabor Representation', 'desc': 'This paper presents AdaGaR, a new framework for reconstructing dynamic 3D scenes from single videos. It improves upon existing methods by using Adaptive Gabor Representation, which allows for better detail capture and stability through learnable frequency weights. To ensure smooth motion over time, the framework incorporates Cubic Hermite Splines with Temporal Curvature Regularization. The results show that AdaGaR achieves state-of-the-art performance in various tasks, demonstrating its effectiveness in dynamic scene modeling.'}, 'zh': {'title': 'è‡ªé€‚åº”Gaboré‡å»ºåŠ¨æ€3Dåœºæ™¯', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºAdaGaRçš„ç»Ÿä¸€æ¡†æ¶ï¼Œç”¨äºä»å•ç›®è§†é¢‘é‡å»ºåŠ¨æ€3Dåœºæ™¯ã€‚è¯¥æ–¹æ³•é€šè¿‡è‡ªé€‚åº”Gaborè¡¨ç¤ºï¼Œåˆ©ç”¨å¯å­¦ä¹ çš„é¢‘ç‡æƒé‡å’Œèƒ½é‡è¡¥å¿æ¥å¹³è¡¡ç»†èŠ‚æ•æ‰å’Œç¨³å®šæ€§ã€‚ä¸ºäº†ç¡®ä¿æ—¶é—´è¿ç»­æ€§ï¼Œé‡‡ç”¨äº†å¸¦æœ‰æ—¶é—´æ›²ç‡æ­£åˆ™åŒ–çš„ä¸‰æ¬¡Hermiteæ ·æ¡ï¼Œä»¥å®ç°å¹³æ»‘çš„è¿åŠ¨æ¼”å˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAdaGaRåœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.00417', 'title': 'Deep Delta Learning', 'url': 'https://huggingface.co/papers/2601.00417', 'abstract': "The efficacy of deep residual networks is fundamentally predicated on the identity shortcut connection. While this mechanism effectively mitigates the vanishing gradient problem, it imposes a strictly additive inductive bias on feature transformations, thereby limiting the network's capacity to model complex state transitions. In this paper, we introduce Deep Delta Learning (DDL), a novel architecture that generalizes the standard residual connection by modulating the identity shortcut with a learnable, data-dependent geometric transformation. This transformation, termed the Delta Operator, constitutes a rank-1 perturbation of the identity matrix, parameterized by a reflection direction vector k(X) and a gating scalar Î²(X). We provide a spectral analysis of this operator, demonstrating that the gate Î²(X) enables dynamic interpolation between identity mapping, orthogonal projection, and geometric reflection. Furthermore, we restructure the residual update as a synchronous rank-1 injection, where the gate acts as a dynamic step size governing both the erasure of old information and the writing of new features. This unification empowers the network to explicitly control the spectrum of its layer-wise transition operator, enabling the modeling of complex, non-monotonic dynamics while preserving the stable training characteristics of gated residual architectures.", 'score': 14, 'issue_id': 405, 'pub_date': '2026-01-01', 'pub_date_card': {'ru': '1 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 1', 'zh': '1æœˆ1æ—¥'}, 'hash': 'b59181a06be243db', 'authors': ['Yifan Zhang', 'Yifeng Liu', 'Mengdi Wang', 'Quanquan Gu'], 'affiliations': ['Princeton University', 'University of California, Los Angeles'], 'pdf_title_img': 'assets/pdf/title_img/2601.00417.jpg', 'data': {'categories': ['#architecture'], 'emoji': 'ğŸ”„', 'ru': {'title': 'Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ ÑĞ²ÑĞ·Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Deep Delta Learning (DDL) â€” Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑĞ²ÑĞ·ĞµĞ¹ Ğ² Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ÑÑ… Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ¾Ğ´ÑƒĞ»ÑÑ†Ğ¸Ñ Ñ‚Ğ¾Ğ¶Ğ´ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ğ¼ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼. Ğ”ĞµĞ»ÑŒÑ‚Ğ°-Ğ¾Ğ¿ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ñ€Ğ°Ğ½Ğ³-1 Ğ²Ğ¾Ğ·Ğ¼ÑƒÑ‰ĞµĞ½Ğ¸Ğµ ĞµĞ´Ğ¸Ğ½Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ñ‹, Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ¼ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞºĞ°Ğ»ÑÑ€Ğ½Ğ¾Ğ¹ Ğ²ĞµĞ½Ñ‚Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ„ÑƒĞ½ĞºÑ†Ğ¸ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ·Ğ°Ğ²Ğ¸ÑÑÑ‚ Ğ¾Ñ‚ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ¡Ğ¿ĞµĞºÑ‚Ñ€Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ²ĞµĞ½Ñ‚Ğ¸Ğ»ÑŒ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚Ğ¾Ğ¶Ğ´ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ĞµĞ¼, Ğ¾Ñ€Ñ‚Ğ¾Ğ³Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾ĞµĞºÑ†Ğ¸ĞµĞ¹ Ğ¸ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¾Ñ‚Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ĞµĞ¼. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞµÑ‚Ğ¸ ÑĞ²Ğ½Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ¿ĞµĞºÑ‚Ñ€ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ¾Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ»Ğ¾ÑĞ¼Ğ¸, Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒÑ ÑĞ»Ğ¾Ğ¶Ğ½ÑƒÑ Ğ½ĞµĞ»Ğ¸Ğ½ĞµĞ¹Ğ½ÑƒÑ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºÑƒ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Empowering Residual Networks with Dynamic Transformations', 'desc': "This paper presents Deep Delta Learning (DDL), a new architecture that enhances deep residual networks by introducing a learnable transformation to the identity shortcut connection. The Delta Operator, a key component of DDL, allows for dynamic adjustments to feature transformations, overcoming limitations of traditional residual connections. By using a gating scalar and a reflection direction vector, DDL can interpolate between various transformations, improving the network's ability to model complex dynamics. This approach maintains the stability of training while enabling richer feature representation and more effective learning of non-linear relationships."}, 'zh': {'title': 'æ·±åº¦å¢é‡å­¦ä¹ ï¼šè¶…è¶Šä¼ ç»Ÿæ®‹å·®è¿æ¥çš„åˆ›æ–°', 'desc': 'æ·±åº¦æ®‹å·®ç½‘ç»œçš„æœ‰æ•ˆæ€§ä¾èµ–äºèº«ä»½å¿«æ·è¿æ¥ã€‚è™½ç„¶è¿™ç§æœºåˆ¶æœ‰æ•ˆåœ°ç¼“è§£äº†æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼Œä½†å®ƒå¯¹ç‰¹å¾å˜æ¢æ–½åŠ äº†ä¸¥æ ¼çš„åŠ æ³•å½’çº³åç½®ï¼Œä»è€Œé™åˆ¶äº†ç½‘ç»œå»ºæ¨¡å¤æ‚çŠ¶æ€è½¬å˜çš„èƒ½åŠ›ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ¶æ„â€”â€”æ·±åº¦å¢é‡å­¦ä¹ ï¼ˆDDLï¼‰ï¼Œé€šè¿‡å¯å­¦ä¹ çš„æ•°æ®ä¾èµ–å‡ ä½•å˜æ¢æ¥æ¨å¹¿æ ‡å‡†æ®‹å·®è¿æ¥ã€‚æˆ‘ä»¬å¼•å…¥çš„å¢é‡ç®—å­ä½¿ç½‘ç»œèƒ½å¤ŸåŠ¨æ€æ§åˆ¶å±‚é—´è¿‡æ¸¡æ“ä½œçš„è°±ï¼Œä»è€Œå»ºæ¨¡å¤æ‚çš„éå•è°ƒåŠ¨æ€ï¼ŒåŒæ—¶ä¿æŒé—¨æ§æ®‹å·®æ¶æ„çš„ç¨³å®šè®­ç»ƒç‰¹æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.24695', 'title': 'Nested Learning: The Illusion of Deep Learning Architectures', 'url': 'https://huggingface.co/papers/2512.24695', 'abstract': "Despite the recent progresses, particularly in developing Language Models, there are fundamental challenges and unanswered questions about how such models can continually learn/memorize, self-improve, and find effective solutions. In this paper, we present a new learning paradigm, called Nested Learning (NL), that coherently represents a machine learning model with a set of nested, multi-level, and/or parallel optimization problems, each of which with its own context flow. Through the lenses of NL, existing deep learning methods learns from data through compressing their own context flow, and in-context learning naturally emerges in large models. NL suggests a philosophy to design more expressive learning algorithms with more levels, resulting in higher-order in-context learning and potentially unlocking effective continual learning capabilities. We advocate for NL by presenting three core contributions: (1) Expressive Optimizers: We show that known gradient-based optimizers, such as Adam, SGD with Momentum, etc., are in fact associative memory modules that aim to compress the gradients' information (by gradient descent). Building on this insight, we present other more expressive optimizers with deep memory and/or more powerful learning rules; (2) Self-Modifying Learning Module: Taking advantage of NL's insights on learning algorithms, we present a sequence model that learns how to modify itself by learning its own update algorithm; and (3) Continuum Memory System: We present a new formulation for memory system that generalizes the traditional viewpoint of long/short-term memory. Combining our self-modifying sequence model with the continuum memory system, we present a continual learning module, called Hope, showing promising results in language modeling, knowledge incorporation, and few-shot generalization tasks, continual learning, and long-context reasoning tasks.", 'score': 10, 'issue_id': 407, 'pub_date': '2026-12-31', 'pub_date_card': {'ru': '31 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 31', 'zh': '12æœˆ31æ—¥'}, 'hash': '52823ab1600cf692', 'authors': ['Ali Behrouz', 'Meisam Razaviyayn', 'Peilin Zhong', 'Vahab Mirrokni'], 'affiliations': ['Columbia University', 'Google'], 'pdf_title_img': 'assets/pdf/title_img/2512.24695.jpg', 'data': {'categories': ['#reasoning', '#training', '#architecture', '#long_context', '#optimization'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Ğ’Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ: Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° ÑĞ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸ĞµÑÑ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Nested Learning (Ğ²Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ĞºĞ°Ğº Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ²Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ°Ğ¼Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ñ‹Ğµ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ñ‹ (Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Adam Ğ¸ SGD Ñ Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ¼) ÑĞ²Ğ»ÑÑÑ‚ÑÑ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑĞ¼Ğ¸ Ğ°ÑÑĞ¾Ñ†Ğ¸Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸, ÑĞ¶Ğ¸Ğ¼Ğ°ÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ°Ñ…. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ insight Ğ¾Ğ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹Ñ€Ğ°Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ñ‹ Ñ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ Ğ¸ ÑĞ°Ğ¼Ğ¾Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒÑÑ‰Ğ¸ĞµÑÑ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ÑƒĞ»Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¸Ğ·Ğ¼ĞµĞ½ÑÑ‚ÑŒ ÑĞ²Ğ¾Ğ¹ ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ. ĞšĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒÑ ÑÑ‚Ğ¸ Ğ¸Ğ´ĞµĞ¸, Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Hope Ğ´Ğ»Ñ continual learning, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸, Ğ´Ğ¾Ğ»Ğ³Ğ¾ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¸ Ğ¸ few-shot Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¸.'}, 'en': {'title': 'Unlocking Continual Learning with Nested Learning Paradigm', 'desc': 'This paper introduces a new learning approach called Nested Learning (NL), which organizes machine learning models into a structure of nested optimization problems. NL allows models to learn from their own context flows, enhancing their ability to perform in-context learning and continual learning. The authors propose three main contributions: expressive optimizers that act as memory modules, a self-modifying learning module that adapts its own learning rules, and a continuum memory system that improves memory management. Together, these innovations aim to create more powerful and flexible learning algorithms that can better handle complex tasks in language modeling and reasoning.'}, 'zh': {'title': 'åµŒå¥—å­¦ä¹ ï¼šè§£é”æŒç»­å­¦ä¹ çš„æ–°èŒƒå¼', 'desc': 'å°½ç®¡æœ€è¿‘åœ¨è¯­è¨€æ¨¡å‹çš„å‘å±•ä¸Šå–å¾—äº†è¿›å±•ï¼Œä½†ä»ç„¶å­˜åœ¨ä¸€äº›åŸºæœ¬æŒ‘æˆ˜å’Œæœªè§£çš„é—®é¢˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å­¦ä¹ èŒƒå¼ï¼Œç§°ä¸ºåµŒå¥—å­¦ä¹ ï¼ˆNested Learningï¼ŒNLï¼‰ï¼Œå®ƒé€šè¿‡ä¸€ç»„åµŒå¥—çš„å¤šå±‚æ¬¡å’Œ/æˆ–å¹¶è¡Œä¼˜åŒ–é—®é¢˜æ¥ä¸€è‡´åœ°è¡¨ç¤ºæœºå™¨å­¦ä¹ æ¨¡å‹ã€‚NLæä¾›äº†ä¸€ç§è®¾è®¡æ›´å…·è¡¨ç°åŠ›çš„å­¦ä¹ ç®—æ³•çš„å“²å­¦ï¼Œèƒ½å¤Ÿå®ç°æ›´é«˜é˜¶çš„ä¸Šä¸‹æ–‡å­¦ä¹ ï¼Œå¹¶å¯èƒ½è§£é”æœ‰æ•ˆçš„æŒç»­å­¦ä¹ èƒ½åŠ›ã€‚æˆ‘ä»¬é€šè¿‡ä¸‰ä¸ªæ ¸å¿ƒè´¡çŒ®æ¥æ”¯æŒNLçš„è§‚ç‚¹ï¼ŒåŒ…æ‹¬è¡¨ç°åŠ›ä¼˜åŒ–å™¨ã€è‡ªæˆ‘ä¿®æ”¹å­¦ä¹ æ¨¡å—å’Œè¿ç»­è®°å¿†ç³»ç»Ÿã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.22955', 'title': 'Diversity or Precision? A Deep Dive into Next Token Prediction', 'url': 'https://huggingface.co/papers/2512.22955', 'abstract': "Recent advancements have shown that reinforcement learning (RL) can substantially improve the reasoning abilities of large language models (LLMs). The effectiveness of such RL training, however, depends critically on the exploration space defined by the pre-trained model's token-output distribution. In this paper, we revisit the standard cross-entropy loss, interpreting it as a specific instance of policy gradient optimization applied within a single-step episode. To systematically study how the pre-trained distribution shapes the exploration potential for subsequent RL, we propose a generalized pre-training objective that adapts on-policy RL principles to supervised learning. By framing next-token prediction as a stochastic decision process, we introduce a reward-shaping strategy that explicitly balances diversity and precision. Our method employs a positive reward scaling factor to control probability concentration on ground-truth tokens and a rank-aware mechanism that treats high-ranking and low-ranking negative tokens asymmetrically. This allows us to reshape the pre-trained token-output distribution and investigate how to provide a more favorable exploration space for RL, ultimately enhancing end-to-end reasoning performance. Contrary to the intuition that higher distribution entropy facilitates effective exploration, we find that imposing a precision-oriented prior yields a superior exploration space for RL.", 'score': 3, 'issue_id': 410, 'pub_date': '2026-12-28', 'pub_date_card': {'ru': '28 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 28', 'zh': '12æœˆ28æ—¥'}, 'hash': '77fedbb4aebb0ea1', 'authors': ['Haoyuan Wu', 'Hai Wang', 'Jiajia Wu', 'Jinxiang Ou', 'Keyao Wang', 'Weile Chen', 'Zihao Zheng', 'Bei Yu'], 'affiliations': ['Tencent', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2512.22955.jpg', 'data': {'categories': ['#reasoning', '#training', '#optimization', '#rl'], 'emoji': 'ğŸ”', 'ru': {'title': 'ĞŸĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ, ĞºĞ°Ğº Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ»Ğ¸ÑĞµÑ‚ Ğ½Ğ° ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿ĞµÑ€ĞµĞ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒÑÑ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ ĞºÑ€Ğ¾ÑÑ-ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸ ĞºĞ°Ğº Ñ‡Ğ°ÑÑ‚Ğ½Ñ‹Ğ¹ ÑĞ»ÑƒÑ‡Ğ°Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ñ‘Ğ½Ğ½ÑƒÑ Ñ†ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒÑÑ‰ÑƒÑ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ñ‹ on-policy RL Ğº ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ° ĞºĞ°Ğº ÑÑ‚Ğ¾Ñ…Ğ°ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸ĞµĞ¼ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ shaping Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ°ÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡Ğ½Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ½Ğ³Ğ¾Ğ². ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾Ñ€ĞµÑ‡Ğ¸Ñ‚ Ğ¸Ğ½Ñ‚ÑƒĞ¸Ñ†Ğ¸Ğ¸: Ğ±Ğ¾Ğ»ĞµĞµ ÑĞºĞ¾Ğ½Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ (Ğ½Ğ¸Ğ·ĞºĞ°Ñ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ñ) Ñ Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ½Ğ° Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ Ğ»ÑƒÑ‡ÑˆĞµĞµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ RL Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¸Ñ‚Ğ¾Ğ³Ğ¾Ğ²ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°.'}, 'en': {'title': 'Enhancing Language Models: Precision Over Diversity in Exploration', 'desc': 'This paper explores how reinforcement learning (RL) can enhance the reasoning capabilities of large language models (LLMs) by improving their exploration strategies. It critiques the traditional cross-entropy loss, viewing it as a form of policy gradient optimization in a single-step context. The authors propose a new pre-training objective that integrates on-policy RL concepts into supervised learning, allowing for better exploration of the token-output distribution. Their findings suggest that a focus on precision, rather than just diversity, in the token distribution leads to more effective exploration and improved reasoning performance in LLMs.'}, 'zh': {'title': 'ä¼˜åŒ–æ¢ç´¢ç©ºé—´ï¼Œæå‡æ¨ç†èƒ½åŠ›', 'desc': 'æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯ä»¥æ˜¾è‘—æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼ŒRLè®­ç»ƒçš„æœ‰æ•ˆæ€§åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–äºé¢„è®­ç»ƒæ¨¡å‹çš„è¾“å‡ºåˆ†å¸ƒæ‰€å®šä¹‰çš„æ¢ç´¢ç©ºé—´ã€‚æœ¬æ–‡é‡æ–°å®¡è§†äº†æ ‡å‡†çš„äº¤å‰ç†µæŸå¤±ï¼Œå°†å…¶è§£é‡Šä¸ºåœ¨å•æ­¥æƒ…å¢ƒä¸­åº”ç”¨çš„ç­–ç•¥æ¢¯åº¦ä¼˜åŒ–çš„ç‰¹å®šå®ä¾‹ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§é€šç”¨çš„é¢„è®­ç»ƒç›®æ ‡ï¼Œç»“åˆäº†åœ¨çº¿RLåŸåˆ™ä¸ç›‘ç£å­¦ä¹ ï¼Œä»¥æ”¹å–„RLçš„æ¢ç´¢æ½œåŠ›ï¼Œä»è€Œæå‡æ¨ç†æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.00671', 'title': 'Fast-weight Product Key Memory', 'url': 'https://huggingface.co/papers/2601.00671', 'abstract': 'FwPKM introduces a dynamic, fast-weight episodic memory mechanism for sequence modeling that balances storage capacity and efficiency, achieving strong performance on long-context tasks like Needle in a Haystack evaluations.  \t\t\t\t\tAI-generated summary \t\t\t\t Sequence modeling layers in modern language models typically face a trade-off between storage capacity and computational efficiency. While Softmax attention offers unbounded storage at prohibitive quadratic costs, linear variants provide efficiency but suffer from limited, fixed-size storage. We propose Fast-weight Product Key Memory (FwPKM), a novel architecture that resolves this tension by transforming the sparse Product Key Memory (PKM) from a static module into a dynamic, "fast-weight" episodic memory. Unlike PKM, FwPKM updates its parameters dynamically at both training and inference time via local chunk-level gradient descent, allowing the model to rapidly memorize and retrieve new key-value pairs from input sequences. Experiments reveal that FwPKM functions as an effective episodic memory that complements the semantic memory of standard modules, yielding significant perplexity reductions on long-context datasets. Notably, in Needle in a Haystack evaluations, FwPKM generalizes to 128K-token contexts despite being trained on only 4K-token sequences.', 'score': 1, 'issue_id': 405, 'pub_date': '2026-01-02', 'pub_date_card': {'ru': '2 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 2', 'zh': '1æœˆ2æ—¥'}, 'hash': '072aff4bbbc7f7a5', 'authors': ['Tianyu Zhao', 'Llion Jones'], 'affiliations': ['Sakana AI'], 'pdf_title_img': 'assets/pdf/title_img/2601.00671.jpg', 'data': {'categories': ['#optimization', '#training', '#architecture', '#long_context'], 'emoji': 'ğŸ“š', 'ru': {'title': 'Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ°Ñ ÑĞ¿Ğ¸Ğ·Ğ¾Ğ´Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° FwPKM, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Product Key Memory Ğ² Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ¿Ğ¸Ğ·Ğ¾Ğ´Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ñ Ğ±Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¼Ğ¸ Ğ²ĞµÑĞ°Ğ¼Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑĞµÑ‚ ÑĞ²Ğ¾Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ñ‡ĞµÑ€ĞµĞ· Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ ÑĞ¿ÑƒÑĞº Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ±Ğ»Ğ¾ĞºĞ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾ Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ñ‚ÑŒ Ğ¸ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°Ñ‚ÑŒ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ğ°Ñ€Ñ‹ ĞºĞ»ÑÑ‡-Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ Ğ¸Ğ· Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞ°ĞµÑ‚ ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‘Ğ¼ĞºĞ¾ÑÑ‚ÑŒÑ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ² ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¸ Ñ ĞºĞ²Ğ°Ğ´Ñ€Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Softmax-Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ´Ğ¾ 128K Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° 4K-Ñ‚Ğ¾ĞºĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑÑ….'}, 'en': {'title': 'Dynamic Memory for Efficient Sequence Modeling', 'desc': 'FwPKM introduces a new memory mechanism for sequence modeling that enhances both storage and efficiency. It transforms the traditional static Product Key Memory into a dynamic, fast-weight episodic memory that can adapt during training and inference. This allows the model to quickly memorize and retrieve information from long input sequences, addressing the limitations of existing attention mechanisms. Experiments show that FwPKM significantly improves performance on long-context tasks, demonstrating its ability to handle large amounts of data effectively.'}, 'zh': {'title': 'åŠ¨æ€å¿«é€Ÿæƒé‡è®°å¿†ï¼Œæå‡åºåˆ—å»ºæ¨¡æ•ˆç‡', 'desc': 'FwPKMæå‡ºäº†ä¸€ç§åŠ¨æ€çš„å¿«é€Ÿæƒé‡æƒ…èŠ‚è®°å¿†æœºåˆ¶ï¼Œç”¨äºåºåˆ—å»ºæ¨¡ï¼Œå¹³è¡¡äº†å­˜å‚¨å®¹é‡å’Œæ•ˆç‡ã€‚åœ¨ç°ä»£è¯­è¨€æ¨¡å‹ä¸­ï¼Œåºåˆ—å»ºæ¨¡å±‚é€šå¸¸é¢ä¸´å­˜å‚¨å®¹é‡ä¸è®¡ç®—æ•ˆç‡ä¹‹é—´çš„æƒè¡¡ã€‚FwPKMé€šè¿‡å°†ç¨€ç–çš„äº§å“é”®è®°å¿†ï¼ˆPKMï¼‰ä»é™æ€æ¨¡å—è½¬å˜ä¸ºåŠ¨æ€çš„â€œå¿«é€Ÿæƒé‡â€æƒ…èŠ‚è®°å¿†ï¼Œè§£å†³äº†è¿™ä¸€çŸ›ç›¾ã€‚å®éªŒè¡¨æ˜ï¼ŒFwPKMä½œä¸ºä¸€ç§æœ‰æ•ˆçš„æƒ…èŠ‚è®°å¿†ï¼Œæ˜¾è‘—é™ä½äº†é•¿ä¸Šä¸‹æ–‡æ•°æ®é›†çš„å›°æƒ‘åº¦ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.00575', 'title': 'InfoSynth: Information-Guided Benchmark Synthesis for LLMs', 'url': 'https://huggingface.co/papers/2601.00575', 'abstract': 'InfoSynth generates novel, diverse coding benchmarks for large language models using information-theoretic metrics and genetic algorithms, enabling scalable and self-verifying evaluation.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have demonstrated significant advancements in reasoning and code generation. However, efficiently creating new benchmarks to evaluate these capabilities remains a challenge. Traditional benchmark creation relies on manual human effort, a process that is both expensive and time-consuming. Furthermore, existing benchmarks often contaminate LLM training data, necessitating novel and diverse benchmarks to accurately assess their genuine capabilities. This work introduces InfoSynth, a novel framework for automatically generating and evaluating reasoning benchmarks guided by information-theoretic principles. We propose metrics based on KL-divergence and entropy to quantify benchmark novelty and diversity without relying on costly model evaluations. Building on this framework, we develop an end-to-end pipeline that synthesizes robust Python coding problems from seed datasets using genetic algorithms and iterative code feedback. Our method generates accurate test cases and solutions to new problems 97% of the time, and the synthesized benchmarks consistently exhibit higher novelty and diversity compared to their seed datasets. Moreover, our algorithm provides a method for controlling the novelty/diversity and difficulty of generated problems. InfoSynth offers a scalable, self-verifying pipeline for constructing high-quality, novel and diverse benchmarks for LLMs. Project Page: https://ishirgarg.github.io/infosynth_web/', 'score': 1, 'issue_id': 408, 'pub_date': '2026-01-02', 'pub_date_card': {'ru': '2 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 2', 'zh': '1æœˆ2æ—¥'}, 'hash': '2606b67e886459d6', 'authors': ['Ishir Garg', 'Neel Kolhe', 'Xuandong Zhao', 'Dawn Song'], 'affiliations': ['University of California, Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2601.00575.jpg', 'data': {'categories': ['#dataset', '#reasoning', '#open_source', '#synthetic', '#benchmark', '#plp'], 'emoji': 'ğŸ§¬', 'ru': {'title': 'ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ² Ğ´Ğ»Ñ LLM Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ñ‚ĞµĞ¾Ñ€Ğ¸Ñ', 'desc': 'InfoSynth â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾-Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ° KL-Ğ´Ğ¸Ğ²ĞµÑ€Ğ³ĞµĞ½Ñ†Ğ¸Ğ¸ Ğ¸ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸, Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ±ĞµĞ· Ğ´Ğ¾Ñ€Ğ¾Ğ³Ğ¾ÑÑ‚Ğ¾ÑÑ‰Ğ¸Ñ… Ğ¾Ñ†ĞµĞ½Ğ¾Ğº Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ñ‹ Ğ¸ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ Ğ¿Ğ¾ ĞºĞ¾Ğ´Ñƒ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ½Ğ° Python Ğ¸Ğ· Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ², ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ñ‹Ğµ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ 97%. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ Ğ¸ ÑĞ°Ğ¼Ğ¾Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ğ¾Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ², Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡.'}, 'en': {'title': 'Automating Diverse Benchmark Creation for LLMs with InfoSynth', 'desc': 'InfoSynth is a framework designed to automatically create diverse and novel coding benchmarks for large language models (LLMs) using information-theoretic metrics and genetic algorithms. It addresses the challenges of traditional benchmark creation, which is often labor-intensive and can lead to contamination of training data. By employing metrics like KL-divergence and entropy, InfoSynth quantifies the novelty and diversity of generated benchmarks without the need for expensive model evaluations. The system can generate accurate coding problems with a high success rate and allows for control over the difficulty and diversity of the benchmarks, making it a scalable solution for evaluating LLM capabilities.'}, 'zh': {'title': 'InfoSynthï¼šè‡ªåŠ¨ç”Ÿæˆå¤šæ ·åŒ–ç¼–ç¨‹åŸºå‡†çš„åˆ›æ–°æ¡†æ¶', 'desc': 'InfoSynth æ˜¯ä¸€ä¸ªæ–°æ¡†æ¶ï¼Œåˆ©ç”¨ä¿¡æ¯è®ºæŒ‡æ ‡å’Œé—ä¼ ç®—æ³•è‡ªåŠ¨ç”Ÿæˆå’Œè¯„ä¼°æ¨ç†åŸºå‡†ã€‚è¯¥æ–¹æ³•é€šè¿‡ KL æ•£åº¦å’Œç†µæ¥é‡åŒ–åŸºå‡†çš„æ–°é¢–æ€§å’Œå¤šæ ·æ€§ï¼Œé¿å…äº†æ˜‚è´µçš„æ¨¡å‹è¯„ä¼°ã€‚é€šè¿‡é—ä¼ ç®—æ³•å’Œè¿­ä»£ä»£ç åé¦ˆï¼ŒInfoSynth èƒ½å¤Ÿä»ç§å­æ•°æ®é›†ä¸­åˆæˆå‡ºå¼ºå¥çš„ Python ç¼–ç¨‹é—®é¢˜ï¼Œç”Ÿæˆçš„æµ‹è¯•ç”¨ä¾‹å’Œè§£å†³æ–¹æ¡ˆå‡†ç¡®ç‡é«˜è¾¾ 97%ã€‚æ­¤å¤–ï¼Œè¯¥ç®—æ³•è¿˜æä¾›äº†æ§åˆ¶ç”Ÿæˆé—®é¢˜çš„æ–°é¢–æ€§ã€å¤šæ ·æ€§å’Œéš¾åº¦çš„æ–¹æ³•ï¼Œç¡®ä¿åŸºå‡†çš„é«˜è´¨é‡å’Œå¤šæ ·æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.00204', 'title': 'MorphAny3D: Unleashing the Power of Structured Latent in 3D Morphing', 'url': 'https://huggingface.co/papers/2601.00204', 'abstract': '3D morphing remains challenging due to the difficulty of generating semantically consistent and temporally smooth deformations, especially across categories. We present MorphAny3D, a training-free framework that leverages Structured Latent (SLAT) representations for high-quality 3D morphing. Our key insight is that intelligently blending source and target SLAT features within the attention mechanisms of 3D generators naturally produces plausible morphing sequences. To this end, we introduce Morphing Cross-Attention (MCA), which fuses source and target information for structural coherence, and Temporal-Fused Self-Attention (TFSA), which enhances temporal consistency by incorporating features from preceding frames. An orientation correction strategy further mitigates the pose ambiguity within the morphing steps. Extensive experiments show that our method generates state-of-the-art morphing sequences, even for challenging cross-category cases. MorphAny3D further supports advanced applications such as decoupled morphing and 3D style transfer, and can be generalized to other SLAT-based generative models. Project page: https://xiaokunsun.github.io/MorphAny3D.github.io/.', 'score': 1, 'issue_id': 407, 'pub_date': '2026-01-01', 'pub_date_card': {'ru': '1 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 1', 'zh': '1æœˆ1æ—¥'}, 'hash': 'a7f85192fb47366b', 'authors': ['Xiaokun Sun', 'Zeyu Cai', 'Hao Tang', 'Ying Tai', 'Jian Yang', 'Zhenyu Zhang'], 'affiliations': ['Nanjing University', 'Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2601.00204.jpg', 'data': {'categories': ['#3d', '#architecture'], 'emoji': 'ğŸ”„', 'ru': {'title': 'ĞŸĞ»Ğ°Ğ²Ğ½Ñ‹Ğµ 3D Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· ÑƒĞ¼Ğ½Ğ¾Ğµ ÑĞ¼ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ² ÑĞµÑ‚ÑÑ… Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ', 'desc': 'ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ MorphAny3D, Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ»Ğ°Ğ²Ğ½Ñ‹Ñ… 3D Ğ¼Ğ¾Ñ€Ñ„Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¹. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ ÑĞ¼ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ° Ğ¸ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ° Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ (SLAT). Ğ”Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¼Ğ¾Ñ€Ñ„Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²ĞµĞ´Ğ¸Ğ»Ğ¸ Ğ´Ğ²Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°: Morphing Cross-Attention Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Temporal-Fused Self-Attention Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ğ´Ñ€Ğ°Ğ¼Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´Ğ°Ğ¶Ğµ Ğ´Ğ»Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑĞ»ÑƒÑ‡Ğ°ĞµĞ² Ğ¼Ğ¾Ñ€Ñ„Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ñ€Ğ°Ğ·Ğ²ÑĞ·Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¼Ğ¾Ñ€Ñ„Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ 3D Ñ‚Ñ€Ğ°Ğ½ÑÑ„ĞµÑ€ ÑÑ‚Ğ¸Ğ»Ñ.'}, 'en': {'title': 'Seamless 3D Morphing with MorphAny3D', 'desc': 'MorphAny3D is a novel framework designed to improve 3D morphing by ensuring that the deformations are both semantically consistent and temporally smooth. It utilizes Structured Latent (SLAT) representations to blend features from source and target objects effectively within attention mechanisms of 3D generators. The introduction of Morphing Cross-Attention (MCA) helps maintain structural coherence, while Temporal-Fused Self-Attention (TFSA) ensures that the morphing sequences are temporally consistent by referencing previous frames. This approach not only achieves state-of-the-art results in morphing across different categories but also opens up possibilities for advanced applications like decoupled morphing and 3D style transfer.'}, 'zh': {'title': 'MorphAny3Dï¼šé«˜è´¨é‡3Då˜å½¢çš„æ–°æ–¹æ³•', 'desc': '3Då˜å½¢ä¸€ç›´æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå› ä¸ºç”Ÿæˆè¯­ä¹‰ä¸€è‡´å’Œæ—¶é—´å¹³æ»‘çš„å˜å½¢éå¸¸å›°éš¾ï¼Œå°¤å…¶æ˜¯åœ¨ä¸åŒç±»åˆ«ä¹‹é—´ã€‚æˆ‘ä»¬æå‡ºäº†MorphAny3Dï¼Œè¿™æ˜¯ä¸€ä¸ªæ— è®­ç»ƒçš„æ¡†æ¶ï¼Œåˆ©ç”¨ç»“æ„æ½œåœ¨ï¼ˆSLATï¼‰è¡¨ç¤ºæ¥å®ç°é«˜è´¨é‡çš„3Då˜å½¢ã€‚æˆ‘ä»¬çš„å…³é”®è§è§£æ˜¯ï¼Œåœ¨3Dç”Ÿæˆå™¨çš„æ³¨æ„åŠ›æœºåˆ¶ä¸­æ™ºèƒ½åœ°èåˆæºå’Œç›®æ ‡çš„SLATç‰¹å¾ï¼Œè‡ªç„¶äº§ç”Ÿå¯ä¿¡çš„å˜å½¢åºåˆ—ã€‚é€šè¿‡å¼•å…¥å˜å½¢äº¤å‰æ³¨æ„åŠ›ï¼ˆMCAï¼‰å’Œæ—¶é—´èåˆè‡ªæ³¨æ„åŠ›ï¼ˆTFSAï¼‰ï¼Œæˆ‘ä»¬å¢å¼ºäº†ç»“æ„ä¸€è‡´æ€§å’Œæ—¶é—´ä¸€è‡´æ€§ï¼Œå®éªŒç»“æœè¡¨æ˜æˆ‘ä»¬çš„æ–¹æ³•åœ¨å˜å½¢åºåˆ—ç”Ÿæˆä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ã€‚'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (6)', '#agents (5)', '#agi', '#alignment (2)', '#architecture (16)', '#audio (2)', '#benchmark (10)', '#cv (3)', '#data (2)', '#dataset (11)', '#diffusion (7)', '#ethics', '#games', '#graphs (1)', '#hallucinations (1)', '#healthcare (1)', '#inference (1)', '#interpretability (1)', '#leakage (1)', '#long_context (4)', '#low_resource', '#machine_translation', '#math (1)', '#multilingual', '#multimodal (13)', '#open_source (9)', '#optimization (12)', '#plp (1)', '#rag (1)', '#reasoning (12)', '#rl (7)', '#rlhf (4)', '#robotics (3)', '#science', '#security (1)', '#small_models (3)', '#story_generation', '#survey (1)', '#synthetic (7)', '#training (21)', '#transfer_learning', '#video (11)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            ğŸ”º ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'ğŸ”„ ' + getTimeDiff('2026-01-05 15:27',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ",
                    pub_date: "Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸",
                    issue_id: "Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "è¯„åˆ†",
                    pub_date: "å‘å¸ƒæ—¥æœŸ",
                    issue_id: "HFä¸Šä¼ æ—¥æœŸ"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2026-01-05 15:27')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2026-01-05 15:27')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('monthly'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    