
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 343 papers. January 2026.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #7a30efcf;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: #7a30efcf;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #7a30ef17;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ", "Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹", "Ğ¼Ğ¸Ğ½ÑƒÑ‚"],
                hour: ["Ñ‡Ğ°Ñ", "Ñ‡Ğ°ÑĞ°", "Ñ‡Ğ°ÑĞ¾Ğ²"],
                day: ["Ğ´ĞµĞ½ÑŒ", "Ğ´Ğ½Ñ", "Ğ´Ğ½ĞµĞ¹"],
                justNow: "Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ñ‚Ğ¾",
                ago: "Ğ½Ğ°Ğ·Ğ°Ğ´"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["åˆ†é’Ÿ", "åˆ†é’Ÿ", "åˆ†é’Ÿ"],
                hour: ["å°æ—¶", "å°æ—¶", "å°æ—¶"],
                day: ["å¤©", "å¤©", "å¤©"],
                justNow: "åˆšåˆš",
                ago: "å‰"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            } else if (lastDigit === 1) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒÑ";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒĞ¸";
            } else {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ç¯‡è®ºæ–‡"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">ğŸ”º</h1><h1 class="title-text" id="doomgrad">hf monthly</h1></a>
            <p><span id="title-date">Ğ¯Ğ½Ğ²Ğ°Ñ€ÑŒ 2026</span> | <span id="title-articles-count">343 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/m/2025-12.html">â¬…ï¸ <span id="prev-date">12.2025</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/m/2026-02.html">â¡ï¸ <span id="next-date">02.2026</span></a></span>
            <span class="nav-item" id="nav-daily"><a href="https://hfday.ru">ğŸ“ˆ <span id='top-day-label'>Ğ”ĞµĞ½ÑŒ</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">ğŸ”€ <span id="sort-label-text">Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ</option>
                    <option value="pub_date">Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸</option>
                    <option value="issue_id">Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">ğŸ·ï¸ Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> AâˆªB</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> Aâˆ©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">ğŸ§¹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> âœ–ï¸ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': 'Ğ¯Ğ½Ğ²Ğ°Ñ€ÑŒ 2026', 'en': 'January 2026', 'zh': '1æœˆ2026å¹´'};
        let feedDateNext = {'ru': '02.2026', 'en': '02/2026', 'zh': '2æœˆ2026å¹´'};
        let feedDatePrev = {'ru': '12.2025', 'en': '12/2025', 'zh': '12æœˆ2025å¹´'};
        let filterLabel = {'ru': 'Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€', 'en': 'Topics', 'zh': 'ä¸»é¢˜ç­›é€‰'}
        let publishedLabel = {'ru': 'ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ñ‚ ', 'en': 'published on ', 'zh': 'å‘è¡¨äº'}
        let sortLabel = {'ru': 'Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾', 'en': 'Sort by', 'zh': 'æ’åºæ–¹å¼'}
        let paperLabel = {'ru': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ', 'en': 'Paper', 'zh': 'è®ºæ–‡'}
        let topMonthLabel = {'ru': 'ĞœĞµÑÑÑ†', 'en': 'Month', 'zh': 'æœˆåº¦è®ºæ–‡'}
        let topDayLabel = {'ru': 'Ğ”ĞµĞ½ÑŒ', 'en': 'Day', 'zh': 'æ—¥åº¦è®ºæ–‡'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf monthly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2601.02151', 'title': 'Entropy-Adaptive Fine-Tuning: Resolving Confident Conflicts to Mitigate Forgetting', 'url': 'https://huggingface.co/papers/2601.02151', 'abstract': 'Entropy-Adaptive Fine-Tuning addresses catastrophic forgetting in supervised fine-tuning by using token-level entropy to distinguish uncertainty from knowledge conflict, enabling better preservation of general capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t Supervised Fine-Tuning (SFT) is the standard paradigm for domain adaptation, yet it frequently incurs the cost of catastrophic forgetting. In sharp contrast, on-policy Reinforcement Learning (RL) effectively preserves general capabilities. We investigate this discrepancy and identify a fundamental distributional gap: while RL aligns with the model\'s internal belief, SFT forces the model to fit external supervision. This mismatch often manifests as "Confident Conflicts" tokens characterized by low probability but low entropy. In these instances, the model is highly confident in its own prediction but is forced to learn a divergent ground truth, triggering destructive gradient updates. To address this, we propose Entropy-Adaptive Fine-Tuning (EAFT). Unlike methods relying solely on prediction probability, EAFT utilizes token-level entropy as a gating mechanism to distinguish between epistemic uncertainty and knowledge conflict. This allows the model to learn from uncertain samples while suppressing gradients on conflicting data. Extensive experiments on Qwen and GLM series (ranging from 4B to 32B parameters) across mathematical, medical, and agentic domains confirm our hypothesis. EAFT consistently matches the downstream performance of standard SFT while significantly mitigating the degradation of general capabilities.', 'score': 63, 'issue_id': 471, 'pub_date': '2026-01-05', 'pub_date_card': {'ru': '5 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 5', 'zh': '1æœˆ5æ—¥'}, 'hash': '9d40d1471e675eaa', 'authors': ['Muxi Diao', 'Lele Yang', 'Wuxuan Gong', 'Yutong Zhang', 'Zhonghao Yan', 'Yufei Han', 'Kongming Liang', 'Weiran Xu', 'Zhanyu Ma'], 'affiliations': ['Beijing University of Posts and Telecommunications', 'Zhongguancun Academy'], 'pdf_title_img': 'assets/pdf/title_img/2601.02151.jpg', 'data': {'categories': [], 'emoji': 'âš–ï¸', 'ru': {'title': 'ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ ĞºĞ°Ñ‚Ğ°ÑÑ‚Ñ€Ğ¾Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ·Ğ°Ğ±Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ (SFT) Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾Ñ€ĞµÑ‡Ğ¸Ğ²Ñ‹Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ - ĞºĞ¾Ğ³Ğ´Ğ° Ğ¾Ğ½Ğ° ÑƒĞ²ĞµÑ€ĞµĞ½Ğ° Ğ² ÑĞ²Ğ¾Ñ‘Ğ¼ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¸, Ğ½Ğ¾ Ğ²Ñ‹Ğ½ÑƒĞ¶Ğ´ĞµĞ½Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒÑÑ Ğ´Ñ€ÑƒĞ³Ğ¾Ğ¼Ñƒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñƒ. ĞĞ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Entropy-Adaptive Fine-Tuning (EAFT), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ñ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ½ĞµÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ğ¾Ğ¼ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ EAFT ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ¾Ğ±Ñ‰Ğ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ½Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ….'}, 'en': {'title': 'Preserving Knowledge with Entropy-Aware Fine-Tuning', 'desc': "Entropy-Adaptive Fine-Tuning (EAFT) is a method designed to combat catastrophic forgetting during supervised fine-tuning of machine learning models. It uses token-level entropy to differentiate between uncertainty in predictions and conflicts with external labels, allowing the model to retain its general capabilities. By focusing on uncertain samples and minimizing updates on conflicting data, EAFT helps maintain performance across various tasks. Experiments show that EAFT achieves comparable results to traditional fine-tuning while preserving the model's overall knowledge."}, 'zh': {'title': 'ç†µè‡ªé€‚åº”å¾®è°ƒï¼šè§£å†³ç¾éš¾æ€§é—å¿˜çš„åˆ›æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºç†µè‡ªé€‚åº”å¾®è°ƒï¼ˆEAFTï¼‰çš„æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ç›‘ç£å¾®è°ƒä¸­çš„ç¾éš¾æ€§é—å¿˜é—®é¢˜ã€‚EAFTé€šè¿‡ä½¿ç”¨ä»¤ç‰Œçº§ç†µæ¥åŒºåˆ†ä¸ç¡®å®šæ€§å’ŒçŸ¥è¯†å†²çªï¼Œä»è€Œæ›´å¥½åœ°ä¿ç•™æ¨¡å‹çš„é€šç”¨èƒ½åŠ›ã€‚ä¸ä¼ ç»Ÿçš„ç›‘ç£å¾®è°ƒæ–¹æ³•ä¸åŒï¼ŒEAFTèƒ½å¤Ÿåœ¨ä¸ç¡®å®šæ ·æœ¬ä¸­å­¦ä¹ ï¼ŒåŒæ—¶æŠ‘åˆ¶å¯¹å†²çªæ•°æ®çš„æ¢¯åº¦æ›´æ–°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒEAFTåœ¨å¤šä¸ªé¢†åŸŸçš„ä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå‡è½»é€šç”¨èƒ½åŠ›çš„é€€åŒ–ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.03509', 'title': 'Evolving Programmatic Skill Networks', 'url': 'https://huggingface.co/papers/2601.03509', 'abstract': "Programmatic Skill Network enables continual skill acquisition through executable symbolic programs that evolve via reflection, progressive optimization, and structural refactoring mechanisms.  \t\t\t\t\tAI-generated summary \t\t\t\t We study continual skill acquisition in open-ended embodied environments where an agent must construct, refine, and reuse an expanding library of executable skills. We introduce the Programmatic Skill Network (PSN), a framework in which skills are executable symbolic programs forming a compositional network that evolves through experience. PSN defines three core mechanisms instantiated via large language models: (1)REFLECT for structured fault localization over skill compositions, (2) progressive optimization with maturity-aware update gating that stabilizes reliable skills while maintaining plasticity for uncertain ones, and (3) canonical structural refactoring under rollback validation that maintains network compactness. We further show that PSN's learning dynamics exhibit structural parallels to neural network training. Experiments on MineDojo and Crafter demonstrate robust skill reuse, rapid adaptation, and strong generalization across open-ended task distributions.\\footnote{We plan to open-source the code.", 'score': 52, 'issue_id': 481, 'pub_date': '2026-01-07', 'pub_date_card': {'ru': '7 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 7', 'zh': '1æœˆ7æ—¥'}, 'hash': '5bf0162d41201122', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': [], 'emoji': 'ğŸ§¬', 'ru': {'title': 'Ğ­Ğ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñ‹ Ğ¸ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ñ', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Programmatic Skill Network (PSN) â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ…, Ğ³Ğ´Ğµ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ ĞºĞ°Ğº Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼Ñ‹Ğµ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñ‹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ñ€Ñ‘Ñ… ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ²: ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº, Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ¸ Ñ€ĞµÑ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³Ğ° ÑĞµÑ‚ĞµĞ²Ğ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ñ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¾Ñ‚ĞºĞ°Ñ‚Ğ¾Ğ². PSN Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼ ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ÑŒ, ÑƒÑ‚Ğ¾Ñ‡Ğ½ÑÑ‚ÑŒ Ğ¸ Ğ¿ĞµÑ€ĞµĞ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°ÑÑ‚ÑƒÑ‰ÑƒÑ Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞºÑƒ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ², Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºÑƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½ÑƒÑ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞµ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… MineDojo Ğ¸ Crafter Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ², Ğ±Ñ‹ÑÑ‚Ñ€ÑƒÑ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¸ Ñ…Ğ¾Ñ€Ğ¾ÑˆÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ.'}, 'en': {'title': 'Evolving Skills for Endless Learning', 'desc': 'The Programmatic Skill Network (PSN) is a framework designed for continual skill acquisition in dynamic environments. It allows agents to create and refine a library of executable skills through three main mechanisms: REFLECT for identifying issues in skill combinations, progressive optimization to balance skill reliability and adaptability, and structural refactoring to keep the skill network efficient. PSN utilizes large language models to enhance these processes, enabling agents to learn from their experiences effectively. Experiments show that PSN can adapt quickly and generalize well across various tasks, demonstrating its potential in open-ended learning scenarios.'}, 'zh': {'title': 'ç¨‹åºæŠ€èƒ½ç½‘ç»œï¼šæŒç»­æŠ€èƒ½è·å–çš„æ–°æ–¹æ³•', 'desc': 'ç¨‹åºæŠ€èƒ½ç½‘ç»œï¼ˆPSNï¼‰æ˜¯ä¸€ç§æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å¯æ‰§è¡Œçš„ç¬¦å·ç¨‹åºå®ç°æŒç»­çš„æŠ€èƒ½è·å–ã€‚å®ƒé€šè¿‡åæ€ã€æ¸è¿›ä¼˜åŒ–å’Œç»“æ„é‡æ„æœºåˆ¶ï¼Œä½¿æŠ€èƒ½åº“ä¸æ–­æ‰©å±•å’Œæ¼”åŒ–ã€‚PSNåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å®ç°ä¸‰ç§æ ¸å¿ƒæœºåˆ¶ï¼šç»“æ„åŒ–æ•…éšœå®šä½ã€æˆç†Ÿåº¦æ„ŸçŸ¥çš„ä¼˜åŒ–æ›´æ–°å’Œåœ¨å›æ»šéªŒè¯ä¸‹çš„ç»“æ„é‡æ„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPSNåœ¨å¼€æ”¾å¼ä»»åŠ¡åˆ†å¸ƒä¸­å±•ç°å‡ºå¼ºå¤§çš„æŠ€èƒ½é‡ç”¨ã€å¿«é€Ÿé€‚åº”å’Œè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚'}}, 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2601.03872', 'title': 'Atlas: Orchestrating Heterogeneous Models and Tools for Multi-Domain Complex Reasoning', 'url': 'https://huggingface.co/papers/2601.03872', 'abstract': 'ATLAS is a dual-path framework that dynamically selects optimal model-tool combinations for cross-domain reasoning through cluster-based routing and reinforcement learning-based multi-step routing, achieving superior performance on complex reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t The integration of large language models (LLMs) with external tools has significantly expanded the capabilities of AI agents. However, as the diversity of both LLMs and tools increases, selecting the optimal model-tool combination becomes a high-dimensional optimization challenge. Existing approaches often rely on a single model or fixed tool-calling logic, failing to exploit the performance variations across heterogeneous model-tool pairs. In this paper, we present ATLAS (Adaptive Tool-LLM Alignment and Synergistic Invocation), a dual-path framework for dynamic tool usage in cross-domain complex reasoning. ATLAS operates via a dual-path approach: (1) training-free cluster-based routing that exploits empirical priors for domain-specific alignment, and (2) RL-based multi-step routing that explores autonomous trajectories for out-of-distribution generalization. Extensive experiments across 15 benchmarks demonstrate that our method outperforms closed-source models like GPT-4o, surpassing existing routing methods on both in-distribution (+10.1%) and out-of-distribution (+13.1%) tasks. Furthermore, our framework shows significant gains in visual reasoning by orchestrating specialized multi-modal tools.', 'score': 30, 'issue_id': 474, 'pub_date': '2026-01-07', 'pub_date_card': {'ru': '7 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 7', 'zh': '1æœˆ7æ—¥'}, 'hash': '1f593458df41b2b9', 'authors': ['Jinyang Wu', 'Guocheng Zhai', 'Ruihan Jin', 'Jiahao Yuan', 'Yuhao Shen', 'Shuai Zhang', 'Zhengqi Wen', 'Jianhua Tao'], 'affiliations': ['East China Normal University', 'Tsinghua University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2601.03872.jpg', 'data': {'categories': ['#reasoning', '#optimization', '#rl', '#agents', '#benchmark', '#multimodal'], 'emoji': 'ğŸ¯', 'ru': {'title': 'ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ', 'desc': 'ATLAS â€” ÑÑ‚Ğ¾ Ğ´Ğ²ÑƒÑ…Ğ¿ÑƒÑ‚Ñ‘Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²Ğ° Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°: ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ½Ğ¾-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°Ñ‚ÑŒÑÑ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ GPT-4o, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 10,1% Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ 13,1% Ğ´Ğ»Ñ Ğ½ĞµĞ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ñ‹Ñ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²ĞµĞ½ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ ÑĞ¾ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Dynamic Model-Tool Selection for Superior Reasoning Performance', 'desc': 'ATLAS is a novel framework designed to enhance cross-domain reasoning by dynamically selecting the best combinations of models and tools. It utilizes a dual-path approach that includes cluster-based routing for domain-specific alignment and reinforcement learning for exploring optimal tool usage. This method addresses the challenges of high-dimensional optimization in selecting model-tool pairs, leading to improved performance on complex reasoning tasks. Experiments show that ATLAS significantly outperforms existing models and routing methods, particularly in visual reasoning tasks.'}, 'zh': {'title': 'ATLASï¼šåŠ¨æ€é€‰æ‹©æœ€ä½³æ¨¡å‹å·¥å…·ç»„åˆçš„åŒè·¯å¾„æ¡†æ¶', 'desc': 'ATLASæ˜¯ä¸€ç§åŒè·¯å¾„æ¡†æ¶ï¼Œèƒ½å¤ŸåŠ¨æ€é€‰æ‹©æœ€ä½³çš„æ¨¡å‹å’Œå·¥å…·ç»„åˆï¼Œä»¥åº”å¯¹è·¨é¢†åŸŸæ¨ç†çš„å¤æ‚ä»»åŠ¡ã€‚å®ƒé€šè¿‡åŸºäºèšç±»çš„è·¯ç”±å’ŒåŸºäºå¼ºåŒ–å­¦ä¹ çš„å¤šæ­¥è·¯ç”±æ¥å®ç°è¿™ä¸€ç›®æ ‡ï¼Œä»è€Œåœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šå–å¾—äº†ä¼˜å¼‚çš„è¡¨ç°ã€‚ATLASçš„åŒè·¯å¾„æ–¹æ³•åŒ…æ‹¬æ— è®­ç»ƒçš„åŸºäºèšç±»çš„è·¯ç”±å’ŒåŸºäºå¼ºåŒ–å­¦ä¹ çš„å¤šæ­¥è·¯ç”±ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåˆ©ç”¨é¢†åŸŸç‰¹å®šçš„ç»éªŒå…ˆéªŒå’Œè‡ªä¸»æ¢ç´¢çš„è½¨è¿¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒATLASåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†ç°æœ‰çš„é—­æºæ¨¡å‹å’Œè·¯ç”±æ–¹æ³•ï¼Œå°¤å…¶åœ¨è§†è§‰æ¨ç†æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—çš„æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.03986', 'title': 'Benchmark^2: Systematic Evaluation of LLM Benchmarks', 'url': 'https://huggingface.co/papers/2601.03986', 'abstract': "Researchers developed Benchmark^2, a framework with three metrics to evaluate benchmark quality for large language models, revealing significant variations in existing benchmarks and enabling more efficient evaluation through selective benchmark construction.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapid proliferation of benchmarks for evaluating large language models (LLMs) has created an urgent need for systematic methods to assess benchmark quality itself. We propose Benchmark^2, a comprehensive framework comprising three complementary metrics: (1) Cross-Benchmark Ranking Consistency, measuring whether a benchmark produces model rankings aligned with peer benchmarks; (2) Discriminability Score, quantifying a benchmark's ability to differentiate between models; and (3) Capability Alignment Deviation, identifying problematic instances where stronger models fail but weaker models succeed within the same model family. We conduct extensive experiments across 15 benchmarks spanning mathematics, reasoning, and knowledge domains, evaluating 11 LLMs across four model families. Our analysis reveals significant quality variations among existing benchmarks and demonstrates that selective benchmark construction based on our metrics can achieve comparable evaluation performance with substantially reduced test sets.", 'score': 28, 'issue_id': 471, 'pub_date': '2026-01-07', 'pub_date_card': {'ru': '7 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 7', 'zh': '1æœˆ7æ—¥'}, 'hash': 'daa162d270fd4a3e', 'authors': ['Qi Qian', 'Chengsong Huang', 'Jingwen Xu', 'Changze Lv', 'Muling Wu', 'Wenhao Liu', 'Xiaohua Wang', 'Zhenghua Wang', 'Zisu Huang', 'Muzhao Tian', 'Jianhan Xu', 'Kun Hu', 'He-Da Wang', 'Yao Hu', 'Xuanjing Huang', 'Xiaoqing Zheng'], 'affiliations': ['College of Computer Science and Artificial Intelligence, Fudan University', 'Washington University in St. Louis', 'Xiaohongshu Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2601.03986.jpg', 'data': {'categories': ['#benchmark'], 'emoji': 'ğŸ“', 'ru': {'title': 'Ğ˜Ğ·Ğ¼ĞµÑ€ÑĞµĞ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğ¹: Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ² LLM', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ BenchmarkÂ², Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼Ñ‹Ñ… Ğ¿Ñ€Ğ¸ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸: ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ğ¼Ğ¸, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹, ĞºĞ¾Ğ³Ğ´Ğ° Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ»Ğ°Ğ±Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ğ¹ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚, Ñ‡ĞµĞ¼ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° 15 Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ² Ğ¸ 11 LLM-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ†ĞµĞ»ĞµĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¾Ñ‚Ğ±Ğ¾Ñ€ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‰ĞµĞ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¼ĞµĞ½ÑŒÑˆĞµĞ³Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ‚ĞµÑÑ‚Ğ¾Ğ².'}, 'en': {'title': 'Benchmark^2: Elevating Benchmark Quality for Language Models', 'desc': "The paper introduces Benchmark^2, a new framework designed to evaluate the quality of benchmarks used for large language models (LLMs). It includes three key metrics: Cross-Benchmark Ranking Consistency, which checks if a benchmark's rankings match those of other benchmarks; Discriminability Score, which measures how well a benchmark can distinguish between different models; and Capability Alignment Deviation, which highlights cases where stronger models underperform compared to weaker ones. Through experiments on 15 different benchmarks and 11 LLMs, the researchers found significant differences in benchmark quality and showed that using their metrics allows for more efficient benchmark selection while maintaining evaluation effectiveness."}, 'zh': {'title': 'Benchmark^2ï¼šæå‡åŸºå‡†è¯„ä¼°è´¨é‡çš„æ¡†æ¶', 'desc': 'ç ”ç©¶äººå‘˜å¼€å‘äº†Benchmark^2ï¼Œè¿™æ˜¯ä¸€ä¸ªè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åŸºå‡†è´¨é‡çš„æ¡†æ¶ï¼ŒåŒ…å«ä¸‰ä¸ªæŒ‡æ ‡ã€‚ç¬¬ä¸€ä¸ªæŒ‡æ ‡æ˜¯è·¨åŸºå‡†æ’åä¸€è‡´æ€§ï¼Œç”¨äºæµ‹é‡åŸºå‡†æ˜¯å¦èƒ½ä¸åŒè¡ŒåŸºå‡†äº§ç”Ÿä¸€è‡´çš„æ¨¡å‹æ’åã€‚ç¬¬äºŒä¸ªæŒ‡æ ‡æ˜¯å¯åŒºåˆ†æ€§å¾—åˆ†ï¼Œé‡åŒ–åŸºå‡†åŒºåˆ†æ¨¡å‹çš„èƒ½åŠ›ã€‚ç¬¬ä¸‰ä¸ªæŒ‡æ ‡æ˜¯èƒ½åŠ›å¯¹é½åå·®ï¼Œè¯†åˆ«åœ¨åŒä¸€æ¨¡å‹å®¶æ—ä¸­å¼ºæ¨¡å‹å¤±è´¥è€Œå¼±æ¨¡å‹æˆåŠŸçš„æƒ…å†µã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.04151', 'title': 'Klear: Unified Multi-Task Audio-Video Joint Generation', 'url': 'https://huggingface.co/papers/2601.04151', 'abstract': 'Klear addresses audio-video joint generation challenges through a unified model architecture, progressive multitask training, and large-scale dense-caption data construction, achieving superior alignment and generalization.  \t\t\t\t\tAI-generated summary \t\t\t\t Audio-video joint generation has progressed rapidly, yet substantial challenges still remain. Non-commercial approaches still suffer audio-visual asynchrony, poor lip-speech alignment, and unimodal degradation, which can be stemmed from weak audio-visual correspondence modeling, limited generalization, and scarce high-quality dense-caption data. To address these issues, we introduce Klear and delve into three axes--model architecture, training strategy, and data curation. Architecturally, we adopt a single-tower design with unified DiT blocks and an Omni-Full Attention mechanism, achieving tight audio-visual alignment and strong scalability. Training-wise, we adopt a progressive multitask regime--random modality masking to joint optimization across tasks, and a multistage curriculum, yielding robust representations, strengthening A-V aligned world knowledge, and preventing unimodal collapse. For datasets, we present the first large-scale audio-video dataset with dense captions, and introduce a novel automated data-construction pipeline which annotates and filters millions of diverse, high-quality, strictly aligned audio-video-caption triplets. Building on this, Klear scales to large datasets, delivering high-fidelity, semantically and temporally aligned, instruction-following generation in both joint and unimodal settings while generalizing robustly to out-of-distribution scenarios. Across tasks, it substantially outperforms prior methods by a large margin and achieves performance comparable to Veo 3, offering a unified, scalable path toward next-generation audio-video synthesis.', 'score': 9, 'issue_id': 470, 'pub_date': '2026-01-07', 'pub_date_card': {'ru': '7 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 7', 'zh': '1æœˆ7æ—¥'}, 'hash': 'a6b0d69f2c3ead36', 'authors': ['Jun Wang', 'Chunyu Qiang', 'Yuxin Guo', 'Yiran Wang', 'Xijuan Zeng', 'Chen Zhang', 'Pengfei Wan'], 'affiliations': ['Kling Team, Kuaishou Technology'], 'pdf_title_img': 'assets/pdf/title_img/2601.04151.jpg', 'data': {'categories': ['#data', '#training', '#dataset', '#audio', '#video', '#multimodal', '#architecture'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ¡Ğ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğµ Ñ€Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ·Ğ²ÑƒĞºĞ° Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ°: ĞµĞ´Ğ¸Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ Ğ¸Ğ´ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Klear â€” Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ€Ğ°ÑÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸ Ğ¸ Ğ³ÑƒĞ±. ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ½Ğ° ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ñ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ¼ Omni-Full Attention Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ‚ĞµÑĞ½Ğ¾Ğ³Ğ¾ Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½ÑƒÑ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½ÑƒÑ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºÑƒ Ñ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½Ñ‡Ğ°Ñ‚Ñ‹Ğ¼ curriculum learning, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ñ ÑƒĞ½Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ². Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ñ‹ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¾Ñ‚Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ¾Ğ¹Ğ¾Ğº (Ğ°ÑƒĞ´Ğ¸Ğ¾-Ğ²Ğ¸Ğ´ĞµĞ¾-Ñ‚ĞµĞºÑÑ‚).'}, 'en': {'title': 'Klear: Unifying Audio-Video Generation for Superior Alignment and Generalization', 'desc': 'Klear is a new model designed to improve the generation of audio and video together. It uses a single architecture with advanced attention mechanisms to ensure that audio and video are closely aligned. The training process involves multitasking and progressive learning, which helps the model understand and generate better representations of audio-visual content. Additionally, Klear introduces a large dataset with detailed captions, allowing it to perform exceptionally well in generating high-quality audio-video outputs, even in challenging scenarios.'}, 'zh': {'title': 'Klearï¼šéŸ³è§†é¢‘ç”Ÿæˆçš„æ–°çªç ´', 'desc': 'Klear æ˜¯ä¸€ç§æ–°å‹çš„éŸ³é¢‘è§†é¢‘è”åˆç”Ÿæˆæ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³éŸ³è§†é¢‘ä¸åŒæ­¥å’Œå£å‹ä¸è¯­éŸ³ä¸åŒ¹é…ç­‰é—®é¢˜ã€‚å®ƒé‡‡ç”¨ç»Ÿä¸€çš„æ¨¡å‹æ¶æ„å’Œæ¸è¿›å¼å¤šä»»åŠ¡è®­ç»ƒç­–ç•¥ï¼Œèƒ½å¤Ÿå®ç°æ›´å¥½çš„éŸ³è§†é¢‘å¯¹é½å’Œæ³›åŒ–èƒ½åŠ›ã€‚é€šè¿‡æ„å»ºå¤§è§„æ¨¡çš„å¯†é›†æ ‡æ³¨æ•°æ®é›†ï¼ŒKlear æä¾›äº†é«˜è´¨é‡çš„éŸ³è§†é¢‘-å­—å¹•ä¸‰å…ƒç»„ï¼Œå¢å¼ºäº†æ¨¡å‹çš„è¡¨ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒKlear åœ¨å¤šä¸ªä»»åŠ¡ä¸Šæ˜¾è‘—è¶…è¶Šäº†ä¹‹å‰çš„æ–¹æ³•ï¼Œå±•ç°å‡ºå¼ºå¤§çš„ç”Ÿæˆèƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.04194', 'title': 'Choreographing a World of Dynamic Objects', 'url': 'https://huggingface.co/papers/2601.04194', 'abstract': 'CHORD is a universal generative framework that extracts Lagrangian motion information from Eulerian video representations to synthesize diverse 4D dynamic scenes without requiring category-specific rules or large datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t Dynamic objects in our physical 4D (3D + time) world are constantly evolving, deforming, and interacting with other objects, leading to diverse 4D scene dynamics. In this paper, we present a universal generative pipeline, CHORD, for CHOReographing Dynamic objects and scenes and synthesizing this type of phenomena. Traditional rule-based graphics pipelines to create these dynamics are based on category-specific heuristics, yet are labor-intensive and not scalable. Recent learning-based methods typically demand large-scale datasets, which may not cover all object categories in interest. Our approach instead inherits the universality from the video generative models by proposing a distillation-based pipeline to extract the rich Lagrangian motion information hidden in the Eulerian representations of 2D videos. Our method is universal, versatile, and category-agnostic. We demonstrate its effectiveness by conducting experiments to generate a diverse range of multi-body 4D dynamics, show its advantage compared to existing methods, and demonstrate its applicability in generating robotics manipulation policies. Project page: https://yanzhelyu.github.io/chord', 'score': 7, 'issue_id': 470, 'pub_date': '2026-01-07', 'pub_date_card': {'ru': '7 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 7', 'zh': '1æœˆ7æ—¥'}, 'hash': 'f62cc6c228aa9b1c', 'authors': ['Yanzhe Lyu', 'Chen Geng', 'Karthik Dharmarajan', 'Yunzhi Zhang', 'Hadi Alzayer', 'Shangzhe Wu', 'Jiajun Wu'], 'affiliations': ['Stanford University', 'University of Cambridge', 'University of Maryland'], 'pdf_title_img': 'assets/pdf/title_img/2601.04194.jpg', 'data': {'categories': ['#video', '#synthetic', '#3d', '#multimodal', '#robotics'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ 4D Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ğ¸Ğ· 2D Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»', 'desc': 'CHORD â€” ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸Ğ· 2D Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ğ­Ğ¹Ğ»ĞµÑ€Ğ¾Ğ²Ğ¾Ğ¼ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ Ğ¸ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ ĞµÑ‘ Ğ² Ğ›Ğ°Ğ³Ñ€Ğ°Ğ½Ğ¶ĞµĞ²Ñ‹ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚Ñ‹ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… 4D ÑÑ†ĞµĞ½. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ¾Ğ², Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ° Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ», Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸Ğ· Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ½Ğµ Ğ½ÑƒĞ¶Ğ´Ğ°ĞµÑ‚ÑÑ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ…. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ĞµĞ½ Ğ¸ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾-Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸Ñ‡ĞµĞ½, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑÑ†ĞµĞ½Ñ‹ Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° 4D ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°, Ğ½Ğ¾ Ğ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¹.'}, 'en': {'title': 'CHORD: Universal Synthesis of Dynamic 4D Scenes', 'desc': 'The paper introduces CHORD, a generative framework that synthesizes dynamic 4D scenes by extracting motion information from 2D video representations. Unlike traditional methods that rely on specific rules for different object categories, CHORD operates without needing large datasets or category-specific heuristics. It utilizes a distillation-based approach to capture Lagrangian motion, making it versatile and applicable to various scenarios. The effectiveness of CHORD is demonstrated through experiments that showcase its ability to generate diverse multi-body dynamics and its potential in robotics applications.'}, 'zh': {'title': 'CHORDï¼šæ— ç±»åˆ«é™åˆ¶çš„å››ç»´åŠ¨æ€åœºæ™¯ç”Ÿæˆ', 'desc': 'CHORDæ˜¯ä¸€ç§é€šç”¨çš„ç”Ÿæˆæ¡†æ¶ï¼Œèƒ½å¤Ÿä»æ¬§æ‹‰è§†é¢‘è¡¨ç¤ºä¸­æå–æ‹‰æ ¼æœ—æ—¥è¿åŠ¨ä¿¡æ¯ï¼Œä»¥åˆæˆå¤šæ ·çš„å››ç»´åŠ¨æ€åœºæ™¯ã€‚è¯¥æ–¹æ³•ä¸éœ€è¦ç‰¹å®šç±»åˆ«çš„è§„åˆ™æˆ–å¤§å‹æ•°æ®é›†ï¼Œå…·æœ‰æ™®éæ€§å’Œçµæ´»æ€§ã€‚ä¸ä¼ ç»Ÿçš„åŸºäºè§„åˆ™çš„å›¾å½¢ç®¡é“ç›¸æ¯”ï¼ŒCHORDèƒ½å¤Ÿæ›´é«˜æ•ˆåœ°ç”ŸæˆåŠ¨æ€åœºæ™¯ï¼Œé¿å…äº†ç¹ççš„äººå·¥è®¾è®¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCHORDåœ¨ç”Ÿæˆå¤šä½“å››ç»´åŠ¨æ€æ–¹é¢è¡¨ç°ä¼˜è¶Šï¼Œå¹¶ä¸”å¯ä»¥åº”ç”¨äºæœºå™¨äººæ“ä½œç­–ç•¥çš„ç”Ÿæˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.04171', 'title': 'Agentic Rubrics as Contextual Verifiers for SWE Agents', 'url': 'https://huggingface.co/papers/2601.04171', 'abstract': 'Agentic Rubrics enable efficient and scalable verification for software engineering agents by creating context-aware checklists that outperform traditional methods while maintaining interpretability.  \t\t\t\t\tAI-generated summary \t\t\t\t Verification is critical for improving agents: it provides the reward signal for Reinforcement Learning and enables inference-time gains through Test-Time Scaling (TTS). Despite its importance, verification in software engineering (SWE) agent settings often relies on code execution, which can be difficult to scale due to environment setup overhead. Scalable alternatives such as patch classifiers and heuristic methods exist, but they are less grounded in codebase context and harder to interpret. To this end, we explore Agentic Rubrics: an expert agent interacts with the repository to create a context-grounded rubric checklist, and candidate patches are then scored against it without requiring test execution. On SWE-Bench Verified under parallel TTS evaluation, Agentic Rubrics achieve a score of 54.2% on Qwen3-Coder-30B-A3B and 40.6% on Qwen3-32B, with at least a +3.5 percentage-point gain over the strongest baseline in our comparison set. We further analyze rubric behavior, showing that rubric scores are consistent with ground-truth tests while also flagging issues that tests do not capture. Our ablations show that agentic context gathering is essential for producing codebase-specific, unambiguous criteria. Together, these results suggest that Agentic Rubrics provide an efficient, scalable, and granular verification signal for SWE agents.', 'score': 6, 'issue_id': 470, 'pub_date': '2026-01-07', 'pub_date_card': {'ru': '7 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 7', 'zh': '1æœˆ7æ—¥'}, 'hash': '31d275e8d765ca8b', 'authors': ['Mohit Raghavendra', 'Anisha Gunjal', 'Bing Liu', 'Yunzhong He'], 'affiliations': ['Scale AI'], 'pdf_title_img': 'assets/pdf/title_img/2601.04171.jpg', 'data': {'categories': ['#reasoning', '#agents', '#interpretability', '#plp', '#benchmark', '#rl'], 'emoji': 'âœ…', 'ru': {'title': 'ĞšĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ğµ Ñ‡ĞµĞºĞ»Ğ¸ÑÑ‚Ñ‹ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ĞºĞ¾Ğ´Ğ°: Ğ°Ğ³ĞµĞ½Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ Agentic Rubrics Ğ´Ğ»Ñ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ±ĞµĞ· Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ´Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ğ¹ Ñ‡ĞµĞºĞ»Ğ¸ÑÑ‚ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸ĞµĞ² Ğ¾Ñ†ĞµĞ½ĞºĞ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğº ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ½Ñ‹Ğ¼ Ğ¿Ğ°Ñ‚Ñ‡Ğ°Ğ¼. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ°Ñ‚Ñ‡ĞµĞ¹ Ğ¸ ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ +3.5 Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿ÑƒĞ½ĞºÑ‚Ğ° Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚Ğ° Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ SWE-Bench. ĞœĞµÑ‚Ğ¾Ğ´ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ·Ğ° ÑÑ‡ĞµÑ‚ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ½Ğ° Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ĞºĞ¾Ğ´Ğ°.'}, 'en': {'title': 'Agentic Rubrics: Context-Aware Checklists for Efficient Software Verification', 'desc': 'Agentic Rubrics are a new method for verifying software engineering agents by using context-aware checklists. This approach improves upon traditional verification methods by allowing agents to score candidate patches without needing to execute code, which can be time-consuming. The study shows that Agentic Rubrics outperform existing methods, achieving higher scores on benchmark tests while maintaining interpretability. Additionally, the rubrics help identify issues that standard tests might miss, making them a valuable tool for enhancing the performance of software engineering agents.'}, 'zh': {'title': 'Agentic Rubricsï¼šé«˜æ•ˆå¯æ‰©å±•çš„è½¯ä»¶éªŒè¯æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºAgentic Rubricsçš„æ–¹æ³•ï¼Œç”¨äºæé«˜è½¯ä»¶å·¥ç¨‹ä»£ç†çš„éªŒè¯æ•ˆç‡å’Œå¯æ‰©å±•æ€§ã€‚è¯¥æ–¹æ³•é€šè¿‡åˆ›å»ºä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„æ£€æŸ¥æ¸…å•ï¼Œèƒ½å¤Ÿåœ¨ä¸ä¾èµ–ä»£ç æ‰§è¡Œçš„æƒ…å†µä¸‹å¯¹å€™é€‰è¡¥ä¸è¿›è¡Œè¯„åˆ†ï¼Œä»è€Œå…‹æœäº†ä¼ ç»Ÿæ–¹æ³•çš„å±€é™æ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒAgentic Rubricsåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå¾—åˆ†æ˜¾è‘—é«˜äºç°æœ‰çš„æœ€å¼ºåŸºçº¿ã€‚é€šè¿‡åˆ†æï¼Œå‘ç°è¿™ç§æ–¹æ³•ä¸ä»…ä¸çœŸå®æµ‹è¯•ç»“æœä¸€è‡´ï¼Œè¿˜èƒ½è¯†åˆ«æµ‹è¯•æœªèƒ½æ•æ‰åˆ°çš„é—®é¢˜ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨è½¯ä»¶å·¥ç¨‹ä¸­çš„æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.02075', 'title': 'MDAgent2: Large Language Model for Code Generation and Knowledge Q&A in Molecular Dynamics', 'url': 'https://huggingface.co/papers/2601.02075', 'abstract': 'MDAgent2 enables automated molecular dynamics code generation and question answering through domain-adapted language models and a multi-agent runtime system.  \t\t\t\t\tAI-generated summary \t\t\t\t Molecular dynamics (MD) simulations are essential for understanding atomic-scale behaviors in materials science, yet writing LAMMPS scripts remains highly specialized and time-consuming tasks. Although LLMs show promise in code generation and domain-specific question answering, their performance in MD scenarios is limited by scarce domain data, the high deployment cost of state-of-the-art LLMs, and low code executability. Building upon our prior MDAgent, we present MDAgent2, the first end-to-end framework capable of performing both knowledge Q&A and code generation within the MD domain. We construct a domain-specific data-construction pipeline that yields three high-quality datasets spanning MD knowledge, question answering, and code generation. Based on these datasets, we adopt a three stage post-training strategy--continued pre-training (CPT), supervised fine-tuning (SFT), and reinforcement learning (RL)--to train two domain-adapted models, MD-Instruct and MD-Code. Furthermore, we introduce MD-GRPO, a closed-loop RL method that leverages simulation outcomes as reward signals and recycles low-reward trajectories for continual refinement. We further build MDAgent2-RUNTIME, a deployable multi-agent system that integrates code generation, execution, evaluation, and self-correction. Together with MD-EvalBench proposed in this work, the first benchmark for LAMMPS code generation and question answering, our models and system achieve performance surpassing several strong baselines.This work systematically demonstrates the adaptability and generalization capability of large language models in industrial simulation tasks, laying a methodological foundation for automatic code generation in AI for Science and industrial-scale simulations. URL: https://github.com/FredericVAN/PKU_MDAgent2', 'score': 6, 'issue_id': 470, 'pub_date': '2026-01-05', 'pub_date_card': {'ru': '5 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 5', 'zh': '1æœˆ5æ—¥'}, 'hash': 'aa0562dc0d93c752', 'authors': ['Zhuofan Shi', 'Hubao A', 'Yufei Shao', 'Dongliang Huang', 'Hongxu An', 'Chunxiao Xin', 'Haiyang Shen', 'Zhenyu Wang', 'Yunshan Na', 'Gang Huang', 'Xiang Jing'], 'affiliations': ['Liaoning Technical University', 'National Key Laboratory of Data Space Technology and System', 'Peking University', 'The Hong Kong University of Science and Technology', 'Wenjing Future Lab (Beijing) Technology Co., Ltd.'], 'pdf_title_img': 'assets/pdf/title_img/2601.02075.jpg', 'data': {'categories': ['#data', '#training', '#agents', '#dataset', '#plp', '#benchmark', '#rl', '#optimization', '#open_source', '#science', '#small_models'], 'emoji': 'âš›ï¸', 'ru': {'title': 'Ğ¯Ğ·Ñ‹Ğº Ğ´Ğ»Ñ Ğ½Ğ°ÑƒĞºĞ¸: Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ĞºĞ¾Ğ´Ğ° Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ»ÑÑ€Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸', 'desc': 'MDAgent2 â€” ÑÑ‚Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸ÑĞ¼Ğ¸ Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ»ÑÑ€Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğº Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ´Ğ²Ğµ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ MD-Instruct Ğ¸ MD-Code, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ MD-GRPO, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¹ ĞºĞ°Ğº ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ĞºĞ¾Ğ´Ğ° Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ. MDAgent2-RUNTIME Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ, Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ° LAMMPS, Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ½Ğ° ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ MD-EvalBench.'}, 'en': {'title': 'Automating Molecular Dynamics with MDAgent2: Code Generation and Q&A Simplified!', 'desc': 'MDAgent2 is an advanced framework designed to automate the generation of molecular dynamics (MD) code and facilitate question answering in the MD domain. It addresses the challenges of writing LAMMPS scripts by utilizing domain-adapted language models and a multi-agent runtime system. The framework employs a three-stage training approach, including continued pre-training, supervised fine-tuning, and reinforcement learning, to enhance model performance. Additionally, MDAgent2 introduces a closed-loop reinforcement learning method that improves code generation through feedback from simulation outcomes, making it a significant step forward in AI applications for scientific simulations.'}, 'zh': {'title': 'MDAgent2ï¼šè‡ªåŠ¨åŒ–åˆ†å­åŠ¨åŠ›å­¦çš„æ™ºèƒ½åŠ©æ‰‹', 'desc': 'MDAgent2 æ˜¯ä¸€ä¸ªè‡ªåŠ¨åŒ–çš„åˆ†å­åŠ¨åŠ›å­¦ä»£ç ç”Ÿæˆå’Œé—®ç­”ç³»ç»Ÿï¼Œåˆ©ç”¨é¢†åŸŸé€‚åº”çš„è¯­è¨€æ¨¡å‹å’Œå¤šæ™ºèƒ½ä½“è¿è¡Œæ—¶ç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿè§£å†³äº†ç¼–å†™ LAMMPS è„šæœ¬çš„ä¸“ä¸šæ€§å’Œè€—æ—¶é—®é¢˜ï¼Œé€šè¿‡æ„å»ºé«˜è´¨é‡çš„æ•°æ®é›†æ¥æ”¯æŒçŸ¥è¯†é—®ç­”å’Œä»£ç ç”Ÿæˆã€‚MDAgent2 é‡‡ç”¨äº†ä¸‰é˜¶æ®µçš„åè®­ç»ƒç­–ç•¥ï¼ŒåŒ…æ‹¬ç»§ç»­é¢„è®­ç»ƒã€ç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ ï¼Œè®­ç»ƒå‡ºé€‚åº”åˆ†å­åŠ¨åŠ›å­¦é¢†åŸŸçš„æ¨¡å‹ã€‚è¯¥ç ”ç©¶å±•ç¤ºäº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å·¥ä¸šæ¨¡æ‹Ÿä»»åŠ¡ä¸­çš„é€‚åº”æ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Œä¸ºç§‘å­¦å’Œå·¥ä¸šè§„æ¨¡æ¨¡æ‹Ÿä¸­çš„è‡ªåŠ¨ä»£ç ç”Ÿæˆå¥ å®šäº†æ–¹æ³•è®ºåŸºç¡€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.00423', 'title': 'E-GRPO: High Entropy Steps Drive Effective Reinforcement Learning for Flow Models', 'url': 'https://huggingface.co/papers/2601.00423', 'abstract': 'Entropy-aware policy optimization method for reinforcement learning in flow matching models that improves exploration through SDE and ODE sampling strategies.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent reinforcement learning has enhanced the flow matching models on human preference alignment. While stochastic sampling enables the exploration of denoising directions, existing methods which optimize over multiple denoising steps suffer from sparse and ambiguous reward signals. We observe that the high entropy steps enable more efficient and effective exploration while the low entropy steps result in undistinguished roll-outs. To this end, we propose E-GRPO, an entropy aware Group Relative Policy Optimization to increase the entropy of SDE sampling steps. Since the integration of stochastic differential equations suffer from ambiguous reward signals due to stochasticity from multiple steps, we specifically merge consecutive low entropy steps to formulate one high entropy step for SDE sampling, while applying ODE sampling on other steps. Building upon this, we introduce multi-step group normalized advantage, which computes group-relative advantages within samples sharing the same consolidated SDE denoising step. Experimental results on different reward settings have demonstrated the effectiveness of our methods.', 'score': 6, 'issue_id': 470, 'pub_date': '2026-01-01', 'pub_date_card': {'ru': '1 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 1', 'zh': '1æœˆ1æ—¥'}, 'hash': 'd7930acb619d200c', 'authors': ['Shengjun Zhang', 'Zhang Zhang', 'Chensheng Dai', 'Yueqi Duan'], 'affiliations': ['Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2601.00423.jpg', 'data': {'categories': ['#training', '#rl', '#alignment', '#optimization'], 'emoji': 'ğŸ²', 'ru': {'title': 'Ğ­Ğ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ñ ĞºĞ°Ğº ĞºĞ»ÑÑ‡ Ğº Ğ»ÑƒÑ‡ÑˆĞµĞ¼Ñƒ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ E-GRPO â€” Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ñ, Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… flow matching. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ·Ğ°Ğ¼ĞµÑ‚Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑˆĞ°Ğ³Ğ¸ Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸ĞµĞ¹ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹, Ğ² Ñ‚Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ĞºĞ°Ğº ÑˆĞ°Ğ³Ğ¸ Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸ĞµĞ¹ Ğ´Ğ°ÑÑ‚ Ğ½ĞµÑ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ğ¼Ñ‹Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑˆĞ°Ğ³Ğ¸ Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸ĞµĞ¹ Ğ² Ğ¾Ğ´Ğ¸Ğ½ ÑˆĞ°Ğ³ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ ÑÑ‚Ğ¾Ñ…Ğ°ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¹, Ğ° Ğ´Ğ»Ñ Ğ¾ÑÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ² Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ´ĞµÑ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ (ODE). Ğ”Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ°Ñ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸ Ğ³Ñ€ÑƒĞ¿Ğ¿ Ñ Ğ¾Ğ´Ğ¸Ğ½Ğ°ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ğ½ÑĞ¾Ğ»Ğ¸Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ÑˆĞ°Ğ³Ğ°Ğ¼Ğ¸ Ğ´ĞµĞ½Ğ¾Ğ¹Ğ·Ğ¸Ğ½Ğ³Ğ°.'}, 'en': {'title': 'Enhancing Exploration with Entropy in Reinforcement Learning', 'desc': 'This paper presents E-GRPO, an entropy-aware policy optimization method designed to enhance exploration in reinforcement learning for flow matching models. The authors identify that high entropy steps lead to better exploration, while low entropy steps can produce unclear outcomes. To address the challenges of sparse rewards in stochastic differential equation (SDE) sampling, they propose merging consecutive low entropy steps into a single high entropy step. Additionally, they introduce a multi-step group normalized advantage to improve the computation of advantages in samples that share the same SDE denoising step, demonstrating improved performance in various reward settings.'}, 'zh': {'title': 'ç†µæ„ŸçŸ¥ç­–ç•¥ä¼˜åŒ–ï¼Œæå‡æ¢ç´¢æ•ˆç‡ï¼', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¼ºåŒ–å­¦ä¹ ç­–ç•¥ä¼˜åŒ–æ–¹æ³•ï¼Œç§°ä¸ºE-GRPOï¼Œæ—¨åœ¨æé«˜æµåŒ¹é…æ¨¡å‹ä¸­çš„æ¢ç´¢æ•ˆç‡ã€‚è¯¥æ–¹æ³•é€šè¿‡å…³æ³¨ç†µçš„å˜åŒ–ï¼Œä¼˜åŒ–éšæœºå¾®åˆ†æ–¹ç¨‹(SDE)çš„é‡‡æ ·æ­¥éª¤ï¼Œä»¥å…‹æœç°æœ‰æ–¹æ³•åœ¨å¤šæ­¥å»å™ªä¸­é¢ä¸´çš„ç¨€ç–å’Œæ¨¡ç³Šå¥–åŠ±ä¿¡å·é—®é¢˜ã€‚æˆ‘ä»¬å‘ç°é«˜ç†µæ­¥éª¤èƒ½å¤Ÿä¿ƒè¿›æ›´æœ‰æ•ˆçš„æ¢ç´¢ï¼Œè€Œä½ç†µæ­¥éª¤åˆ™å¯¼è‡´ç»“æœä¸æ˜æ˜¾ã€‚é€šè¿‡å°†è¿ç»­çš„ä½ç†µæ­¥éª¤åˆå¹¶ä¸ºä¸€ä¸ªé«˜ç†µæ­¥éª¤ï¼Œå¹¶åœ¨å…¶ä»–æ­¥éª¤ä¸Šåº”ç”¨å¸¸å¾®åˆ†æ–¹ç¨‹(ODE)é‡‡æ ·ï¼ŒE-GRPOæ˜¾è‘—æå‡äº†æ¨¡å‹çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.03471', 'title': 'EpiQAL: Benchmarking Large Language Models in Epidemiological Question Answering for Enhanced Alignment and Reasoning', 'url': 'https://huggingface.co/papers/2601.03471', 'abstract': 'EpiQAL presents a novel benchmark for evaluating epidemiological reasoning in language models through three distinct subsets measuring factual recall, multi-step inference, and conclusion reconstruction from scientific literature.  \t\t\t\t\tAI-generated summary \t\t\t\t Reliable epidemiological reasoning requires synthesizing study evidence to infer disease burden, transmission dynamics, and intervention effects at the population level. Existing medical question answering benchmarks primarily emphasize clinical knowledge or patient-level reasoning, yet few systematically evaluate evidence-grounded epidemiological inference. We present EpiQAL, the first diagnostic benchmark for epidemiological question answering across diverse diseases, comprising three subsets built from open-access literature. The subsets respectively evaluate text-grounded factual recall, multi-step inference linking document evidence with epidemiological principles, and conclusion reconstruction with the Discussion section withheld. Construction combines expert-designed taxonomy guidance, multi-model verification, and retrieval-based difficulty control. Experiments on ten open models reveal that current LLMs show limited performance on epidemiological reasoning, with multi-step inference posing the greatest challenge. Model rankings shift across subsets, and scale alone does not predict success. Chain-of-Thought prompting benefits multi-step inference but yields mixed results elsewhere. EpiQAL provides fine-grained diagnostic signals for evidence grounding, inferential reasoning, and conclusion reconstruction.', 'score': 5, 'issue_id': 471, 'pub_date': '2026-01-06', 'pub_date_card': {'ru': '6 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 6', 'zh': '1æœˆ6æ—¥'}, 'hash': '14c81969b5c87a07', 'authors': ['Mingyang Wei', 'Dehai Min', 'Zewen Liu', 'Yuzhang Xie', 'Guanchen Wu', 'Carl Yang', 'Max S. Y. Lau', 'Qi He', 'Lu Cheng', 'Wei Jin'], 'affiliations': ['Emory University', 'Microsoft', 'University of Illinois Chicago'], 'pdf_title_img': 'assets/pdf/title_img/2601.03471.jpg', 'data': {'categories': ['#dataset', '#reasoning', '#benchmark', '#healthcare', '#science'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'Ğ”Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ¿Ğ¸Ğ´ĞµĞ¼Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ EpiQAL â€” Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¸Ğ´ĞµĞ¼Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ğ»Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ñ‹ Ğ¸ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ Ñ‚Ñ€Ğ¸ Ğ¿Ğ¾Ğ´Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞŸĞµÑ€Ğ²Ğ¾Ğµ Ğ¿Ğ¾Ğ´Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾ Ğ¸Ğ·Ğ¼ĞµÑ€ÑĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ñ„Ğ°ĞºÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ¿Ñ€Ğ¸Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğµ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ²Ñ‹Ğ²Ğ¾Ğ´, ÑĞ²ÑĞ·Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ° Ğ¸Ğ· Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñ ÑĞ¿Ğ¸Ğ´ĞµĞ¼Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ°Ğ¼Ğ¸, Ğ° Ñ‚Ñ€ĞµÑ‚ÑŒĞµ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ¾Ğ² Ğ¿Ñ€Ğ¸ ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¼ Ñ€Ğ°Ğ·Ğ´ĞµĞ»Ğµ Discussion. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´ĞµÑÑÑ‚Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… LLM Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² ÑĞ¿Ğ¸Ğ´ĞµĞ¼Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¸, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğ¹ Ğ²Ñ‹Ğ²Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ´Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ± Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğµ Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒÑĞ¿ĞµÑ… Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ, Ğ° Chain-of-Thought Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¸Ğ½Ğ³ Ğ´Ğ°ĞµÑ‚ Ğ½ĞµĞ¾Ğ´Ğ½Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ°Ñ….'}, 'en': {'title': 'EpiQAL: Advancing Epidemiological Reasoning in Language Models', 'desc': "EpiQAL is a new benchmark designed to assess how well language models can perform epidemiological reasoning. It includes three specific tests that measure a model's ability to recall facts, make multi-step inferences, and reconstruct conclusions from scientific texts. This benchmark is unique because it focuses on understanding population-level health issues rather than just clinical or patient-level questions. The findings show that current language models struggle with these tasks, especially with multi-step reasoning, highlighting the need for improved methods in this area."}, 'zh': {'title': 'EpiQALï¼šæµè¡Œç—…å­¦æ¨ç†çš„æ–°åŸºå‡†', 'desc': 'EpiQALæ˜¯ä¸€ä¸ªæ–°é¢–çš„åŸºå‡†ï¼Œç”¨äºè¯„ä¼°è¯­è¨€æ¨¡å‹åœ¨æµè¡Œç—…å­¦æ¨ç†æ–¹é¢çš„èƒ½åŠ›ã€‚å®ƒé€šè¿‡ä¸‰ä¸ªä¸åŒçš„å­é›†æ¥æµ‹é‡äº‹å®å›å¿†ã€å¤šæ­¥æ¨ç†å’Œä»ç§‘å­¦æ–‡çŒ®ä¸­é‡å»ºç»“è®ºã€‚ç°æœ‰çš„åŒ»å­¦é—®ç­”åŸºå‡†ä¸»è¦å…³æ³¨ä¸´åºŠçŸ¥è¯†æˆ–æ‚£è€…å±‚é¢çš„æ¨ç†ï¼Œè€Œå¾ˆå°‘ç³»ç»Ÿåœ°è¯„ä¼°åŸºäºè¯æ®çš„æµè¡Œç—…å­¦æ¨ç†ã€‚EpiQALä¸ºå¤šç§ç–¾ç—…æä¾›äº†ç¬¬ä¸€ä¸ªè¯Šæ–­åŸºå‡†ï¼Œæ­ç¤ºäº†å½“å‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æµè¡Œç—…å­¦æ¨ç†æ–¹é¢çš„å±€é™æ€§ï¼Œå°¤å…¶æ˜¯åœ¨å¤šæ­¥æ¨ç†æ–¹é¢ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.03699', 'title': 'RedBench: A Universal Dataset for Comprehensive Red Teaming of Large Language Models', 'url': 'https://huggingface.co/papers/2601.03699', 'abstract': 'RedBench presents a unified dataset with standardized risk categorization for evaluating LLM vulnerabilities across multiple domains and attack types.  \t\t\t\t\tAI-generated summary \t\t\t\t As large language models (LLMs) become integral to safety-critical applications, ensuring their robustness against adversarial prompts is paramount. However, existing red teaming datasets suffer from inconsistent risk categorizations, limited domain coverage, and outdated evaluations, hindering systematic vulnerability assessments. To address these challenges, we introduce RedBench, a universal dataset aggregating 37 benchmark datasets from leading conferences and repositories, comprising 29,362 samples across attack and refusal prompts. RedBench employs a standardized taxonomy with 22 risk categories and 19 domains, enabling consistent and comprehensive evaluations of LLM vulnerabilities. We provide a detailed analysis of existing datasets, establish baselines for modern LLMs, and open-source the dataset and evaluation code. Our contributions facilitate robust comparisons, foster future research, and promote the development of secure and reliable LLMs for real-world deployment. Code: https://github.com/knoveleng/redeval', 'score': 4, 'issue_id': 470, 'pub_date': '2026-01-07', 'pub_date_card': {'ru': '7 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 7', 'zh': '1æœˆ7æ—¥'}, 'hash': 'b9041e4ef3eab3d5', 'authors': ['Quy-Anh Dang', 'Chris Ngo', 'Truong-Son Hy'], 'affiliations': ['Knovel Engineering Lab', 'University of Alabama at Birmingham', 'VNU University of Science'], 'pdf_title_img': 'assets/pdf/title_img/2601.03699.jpg', 'data': {'categories': ['#open_source', '#security', '#dataset', '#benchmark'], 'emoji': 'ğŸ”’', 'ru': {'title': 'Ğ•Ğ´Ğ¸Ğ½Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'RedBench â€” ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¹ 37 ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ² Ğ² Ğ¾Ğ´Ğ¸Ğ½ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ñ 29,362 Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ĞµĞ´Ğ¸Ğ½ÑƒÑ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ Ñ 22 ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸ÑĞ¼Ğ¸ Ñ€Ğ¸ÑĞºĞ° Ğ¸ 19 Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ğ¼Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ LLM Ğº adversarial Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ°Ğ¼. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ĞºĞ°Ğº Ğ°Ñ‚Ğ°ĞºÑƒÑÑ‰Ğ¸Ğµ, Ñ‚Ğ°Ğº Ğ¸ Ğ¾Ñ‚ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹ Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½ÑƒÑ Ğ±Ğ°Ğ·Ñƒ Ğ´Ğ»Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞŸÑ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ° Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… LLM ÑĞ¿Ğ¾ÑĞ¾Ğ±ÑÑ‚Ğ²ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ·Ğ°Ñ‰Ğ¸Ñ‰Ñ‘Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'RedBench: A Unified Dataset for Evaluating LLM Vulnerabilities', 'desc': 'RedBench is a new dataset designed to evaluate the vulnerabilities of large language models (LLMs) against various types of attacks. It combines 37 existing benchmark datasets into one unified resource, containing over 29,000 samples of attack and refusal prompts. The dataset uses a standardized risk categorization system with 22 categories and covers 19 different domains, allowing for consistent assessments of LLM robustness. By providing this comprehensive resource and evaluation code, RedBench aims to enhance the security and reliability of LLMs in critical applications.'}, 'zh': {'title': 'RedBenchï¼šè¯„ä¼°LLMè„†å¼±æ€§çš„ç»Ÿä¸€æ•°æ®é›†', 'desc': 'RedBenchæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„æ•°æ®é›†ï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šä¸ªé¢†åŸŸå’Œæ”»å‡»ç±»å‹ä¸‹çš„è„†å¼±æ€§ã€‚è¯¥æ•°æ®é›†æ•´åˆäº†æ¥è‡ªé¡¶çº§ä¼šè®®å’Œå­˜å‚¨åº“çš„37ä¸ªåŸºå‡†æ•°æ®é›†ï¼Œå…±åŒ…å«29,362ä¸ªæ ·æœ¬ï¼Œæ¶µç›–æ”»å‡»å’Œæ‹’ç»æç¤ºã€‚RedBenché‡‡ç”¨æ ‡å‡†åŒ–çš„åˆ†ç±»æ³•ï¼Œè®¾æœ‰22ä¸ªé£é™©ç±»åˆ«å’Œ19ä¸ªé¢†åŸŸï¼Œä½¿å¾—å¯¹LLMè„†å¼±æ€§çš„è¯„ä¼°æ›´åŠ ä¸€è‡´å’Œå…¨é¢ã€‚é€šè¿‡æä¾›ç°æœ‰æ•°æ®é›†çš„è¯¦ç»†åˆ†æå’Œç°ä»£LLMçš„åŸºå‡†ï¼ŒRedBenchä¿ƒè¿›äº†ç¨³å¥çš„æ¯”è¾ƒï¼Œæ¨åŠ¨äº†æœªæ¥çš„ç ”ç©¶ï¼Œå¹¶æ”¯æŒå®‰å…¨å¯é çš„LLMåœ¨å®é™…åº”ç”¨ä¸­çš„å‘å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.03315', 'title': "Why LLMs Aren't Scientists Yet: Lessons from Four Autonomous Research Attempts", 'url': 'https://huggingface.co/papers/2601.03315', 'abstract': 'A case study of four attempts to autonomously generate ML research papers using LLM agents reveals recurring failure modes and proposes design principles for robust AI-scientist systems.  \t\t\t\t\tAI-generated summary \t\t\t\t We report a case study of four end-to-end attempts to autonomously generate ML research papers using a pipeline of six LLM agents mapped to stages of the scientific workflow. Of these four, three attempts failed during implementation or evaluation. One completed the pipeline and was accepted to Agents4Science 2025, an experimental inaugural venue that required AI systems as first authors, passing both human and multi-AI review. From these attempts, we document six recurring failure modes: bias toward training data defaults, implementation drift under execution pressure, memory and context degradation across long-horizon tasks, overexcitement that declares success despite obvious failures, insufficient domain intelligence, and weak scientific taste in experimental design. We conclude by discussing four design principles for more robust AI-scientist systems, implications for autonomous scientific discovery, and we release all prompts, artifacts, and outputs at https://github.com/Lossfunk/ai-scientist-artefacts-v1', 'score': 4, 'issue_id': 479, 'pub_date': '2026-01-06', 'pub_date_card': {'ru': '6 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 6', 'zh': '1æœˆ6æ—¥'}, 'hash': 'e24727d5beee24ef', 'authors': ['Dhruv Trehan', 'Paras Chopra'], 'affiliations': ['Lossfunk'], 'pdf_title_img': 'assets/pdf/title_img/2601.03315.jpg', 'data': {'categories': ['#long_context', '#open_source', '#hallucinations', '#science'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞĞ° Ğ¿ÑƒÑ‚Ğ¸ Ğº Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ğ¼ AI-ÑƒÑ‡ĞµĞ½Ñ‹Ğ¼: ÑƒÑ€Ğ¾ĞºĞ¸ Ğ¸Ğ· Ğ½ĞµÑƒĞ´Ğ°Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚Ğ¾Ğº', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ÑÑ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¾Ğ¿Ñ‹Ñ‚ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‚ Ğ¿Ğ¾ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¸Ğ· ÑˆĞµÑÑ‚Ğ¸ LLM Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ñ… ÑÑ‚Ğ°Ğ¿Ñ‹ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ˜Ğ· Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ… Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚Ğ¾Ğº Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¾Ğ´Ğ½Ğ° ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞ¸Ğ»Ğ°ÑÑŒ Ğ¸ Ğ±Ñ‹Ğ»Ğ° Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ° Ğ½Ğ° ĞºĞ¾Ğ½Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ñ Agents4Science 2025, Ğ¿Ñ€Ğ¾Ğ¹Ğ´Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºÑƒ ĞºĞ°Ğº Ñƒ Ğ»ÑĞ´ĞµĞ¹, Ñ‚Ğ°Ğº Ğ¸ Ñƒ Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… AI ÑĞ¸ÑÑ‚ĞµĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¸Ñ€ÑƒÑÑ‚ ÑˆĞµÑÑ‚ÑŒ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ² Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ‚Ğ°ĞºÑ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼: ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ğµ Ğº ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ°Ğ¼ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ´Ñ€ĞµĞ¹Ñ„ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ´ Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸, Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ½ĞµĞ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¼ Ğ¸ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ÑÑ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ° Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ñ… AI-ÑƒÑ‡ĞµĞ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ´Ğ»Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹.'}, 'en': {'title': 'Building Better AI Scientists: Learning from Failures', 'desc': 'This paper explores the challenges faced when using large language models (LLMs) to autonomously generate machine learning research papers. The authors conducted a case study with four attempts, where three failed due to various issues such as bias in training data and memory degradation during long tasks. One attempt succeeded and was accepted for publication, demonstrating the potential of AI in scientific research. The study identifies six common failure modes and proposes four design principles to enhance the reliability of AI systems in scientific discovery.'}, 'zh': {'title': 'æå‡AIç§‘å­¦å®¶ç³»ç»Ÿçš„è®¾è®¡åŸåˆ™', 'desc': 'æœ¬ç ”ç©¶æ¡ˆä¾‹åˆ†æäº†å››æ¬¡å°è¯•ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†è‡ªä¸»ç”Ÿæˆæœºå™¨å­¦ä¹ ç ”ç©¶è®ºæ–‡çš„è¿‡ç¨‹ã€‚ç»“æœæ˜¾ç¤ºï¼Œä¸‰æ¬¡å°è¯•åœ¨å®æ–½æˆ–è¯„ä¼°é˜¶æ®µå¤±è´¥ï¼Œåªæœ‰ä¸€æ¬¡æˆåŠŸå®Œæˆå¹¶è¢«æ¥å—åˆ°Agents4Science 2025ä¼šè®®ã€‚ç ”ç©¶ä¸­æ€»ç»“äº†å…­ç§å¸¸è§çš„å¤±è´¥æ¨¡å¼ï¼ŒåŒ…æ‹¬å¯¹è®­ç»ƒæ•°æ®çš„åè§ã€æ‰§è¡Œå‹åŠ›ä¸‹çš„å®ç°æ¼‚ç§»ã€é•¿æ—¶é—´ä»»åŠ¡ä¸­çš„è®°å¿†å’Œä¸Šä¸‹æ–‡é€€åŒ–ç­‰ã€‚æœ€åï¼Œæå‡ºäº†å››é¡¹è®¾è®¡åŸåˆ™ï¼Œä»¥å¢å¼ºAIç§‘å­¦å®¶ç³»ç»Ÿçš„é²æ£’æ€§ï¼Œå¹¶æ¢è®¨äº†å¯¹è‡ªä¸»ç§‘å­¦å‘ç°çš„å½±å“ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.03467', 'title': 'ThinkRL-Edit: Thinking in Reinforcement Learning for Reasoning-Centric Image Editing', 'url': 'https://huggingface.co/papers/2601.03467', 'abstract': 'ThinkRL-Edit enhances reasoning-centric image editing through reinforcement learning by expanding visual reasoning exploration beyond denoising stochasticity and using unbiased reward strategies.  \t\t\t\t\tAI-generated summary \t\t\t\t Instruction-driven image editing with unified multimodal generative models has advanced rapidly, yet their underlying visual reasoning remains limited, leading to suboptimal performance on reasoning-centric edits. Reinforcement learning (RL) has been investigated for improving the quality of image editing, but it faces three key challenges: (1) limited reasoning exploration confined to denoising stochasticity, (2) biased reward fusion, and (3) unstable VLM-based instruction rewards. In this work, we propose ThinkRL-Edit, a reasoning-centric RL framework that decouples visual reasoning from image synthesis and expands reasoning exploration beyond denoising. To the end, we introduce Chain-of-Thought (CoT)-based reasoning sampling with planning and reflection stages prior to generation in online sampling, compelling the model to explore multiple semantic hypotheses and validate their plausibility before committing to a visual outcome. To avoid the failures of weighted aggregation, we propose an unbiased chain preference grouping strategy across multiple reward dimensions. Moreover, we replace interval-based VLM scores with a binary checklist, yielding more precise, lower-variance, and interpretable rewards for complex reasoning. Experiments show our method significantly outperforms prior work on reasoning-centric image editing, producing instruction-faithful, visually coherent, and semantically grounded edits.', 'score': 3, 'issue_id': 472, 'pub_date': '2026-01-06', 'pub_date_card': {'ru': '6 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 6', 'zh': '1æœˆ6æ—¥'}, 'hash': 'bb71677f72564096', 'authors': ['Hengjia Li', 'Liming Jiang', 'Qing Yan', 'Yizhi Song', 'Hao Kang', 'Zichuan Liu', 'Xin Lu', 'Boxi Wu', 'Deng Cai'], 'affiliations': ['Intelligent Creation, ByteDance', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2601.03467.jpg', 'data': {'categories': ['#reasoning', '#training', '#cv', '#rl', '#optimization', '#multimodal'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ“Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· reinforcement learning Ğ² Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'ThinkRL-Edit â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ reinforcement learning Ğ´Ğ»Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ñ‚Ğ´ĞµĞ»ÑĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¾Ñ‚ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Chain-of-Thought Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ ÑÑ‚Ğ°Ğ¿Ğ°Ğ¼Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ¸ Ğ´Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ³Ğ¸Ğ¿Ğ¾Ñ‚ĞµĞ·Ñ‹ Ğ¿ĞµÑ€ĞµĞ´ Ğ¾ĞºĞ¾Ğ½Ñ‡Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ¼. Ğ”Ğ»Ñ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ğ½Ğ¸Ñ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ°Ğ³Ñ€ĞµĞ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸ÑĞ¼ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ—Ğ°Ğ¼ĞµĞ½Ğ° Ğ¸Ğ½Ñ‚ĞµÑ€Ğ²Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… VLM Ğ¾Ñ†ĞµĞ½Ğ¾Ğº Ğ½Ğ° Ğ±Ğ¸Ğ½Ğ°Ñ€Ğ½Ñ‹Ğ¹ Ñ‡ĞµĞº-Ğ»Ğ¸ÑÑ‚ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ´Ğ»Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Enhancing Image Editing with Reasoning-Driven Reinforcement Learning', 'desc': 'ThinkRL-Edit is a novel framework that enhances image editing by focusing on reasoning through reinforcement learning. It addresses limitations in traditional methods by allowing for broader exploration of visual reasoning beyond just denoising. The framework employs Chain-of-Thought reasoning, which involves planning and reflection to evaluate different semantic possibilities before generating images. Additionally, it introduces an unbiased reward strategy that improves the accuracy and interpretability of the editing process, leading to better results in reasoning-centric image edits.'}, 'zh': {'title': 'æ¨ç†é©±åŠ¨çš„å›¾åƒç¼–è¾‘æ–°çªç ´', 'desc': 'ThinkRL-Edit æ˜¯ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„å›¾åƒç¼–è¾‘æ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºä»¥æ¨ç†ä¸ºä¸­å¿ƒçš„å›¾åƒç¼–è¾‘èƒ½åŠ›ã€‚å®ƒé€šè¿‡å°†è§†è§‰æ¨ç†ä¸å›¾åƒåˆæˆè§£è€¦ï¼Œæ‰©å±•äº†æ¨ç†æ¢ç´¢çš„èŒƒå›´ï¼Œè¶…è¶Šäº†å»å™ªçš„éšæœºæ€§ã€‚è¯¥æ–¹æ³•å¼•å…¥äº†åŸºäºæ€ç»´é“¾çš„æ¨ç†é‡‡æ ·ï¼Œä¿ƒä½¿æ¨¡å‹åœ¨ç”Ÿæˆä¹‹å‰è¿›è¡Œå¤šç§è¯­ä¹‰å‡è®¾çš„æ¢ç´¢å’ŒéªŒè¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒThinkRL-Edit åœ¨æ¨ç†ä¸­å¿ƒçš„å›¾åƒç¼–è¾‘ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºä¹‹å‰çš„æ–¹æ³•ï¼Œç”Ÿæˆçš„ç¼–è¾‘ç»“æœæ›´ç¬¦åˆæŒ‡ä»¤è¦æ±‚ï¼Œè§†è§‰ä¸Šæ›´è¿è´¯ï¼Œè¯­ä¹‰ä¸Šæ›´æ‰å®ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.03448', 'title': 'Enhancing Linguistic Competence of Language Models through Pre-training with Language Learning Tasks', 'url': 'https://huggingface.co/papers/2601.03448', 'abstract': 'Language models pre-trained with a framework combining standard next-token prediction and structured language learning tasks show enhanced linguistic competence without sacrificing general reasoning capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t Language models (LMs) are pre-trained on raw text datasets to generate text sequences token-by-token. While this approach facilitates the learning of world knowledge and reasoning, it does not explicitly optimize for linguistic competence. To bridge this gap, we propose L2T, a pre-training framework integrating Language Learning Tasks alongside standard next-token prediction. Inspired by human language acquisition, L2T transforms raw text into structured input-output pairs to provide explicit linguistic stimulation. Pre-training LMs on a mixture of raw text and L2T data not only improves overall performance on linguistic competence benchmarks but accelerates its acquisition, while maintaining competitive performance on general reasoning tasks.', 'score': 3, 'issue_id': 479, 'pub_date': '2026-01-06', 'pub_date_card': {'ru': '6 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 6', 'zh': '1æœˆ6æ—¥'}, 'hash': '2a965cc0eaea825a', 'authors': ['Atsuki Yamaguchi', 'Maggie Mi', 'Nikolaos Aletras'], 'affiliations': ['School of Computer Science, University of Sheffield, United Kingdom'], 'pdf_title_img': 'assets/pdf/title_img/2601.03448.jpg', 'data': {'categories': [], 'emoji': 'ğŸ“š', 'ru': {'title': 'Ğ¡Ñ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ğ°ÑÑ‚ĞµÑ€ÑÑ‚Ğ²Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ L2T â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ° Ñ ÑĞ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ’Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»ÑÑÑÑŒ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ¼ Ğ¾Ğ²Ğ»Ğ°Ğ´ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ¼ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¼, Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ ÑÑ‹Ñ€Ğ¾Ğ¹ Ñ‚ĞµĞºÑÑ‚ Ğ² ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ€Ñ‹ Â«Ğ²Ñ…Ğ¾Ğ´-Ğ²Ñ‹Ñ…Ğ¾Ğ´Â» Ğ´Ğ»Ñ ÑĞºÑĞ¿Ğ»Ğ¸Ñ†Ğ¸Ñ‚Ğ½Ğ¾Ğ¹ Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑÑ‚Ğ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸. ĞŸÑ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° ÑĞ¼ĞµÑĞ¸ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… L2T ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¿ĞµÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ ĞµÑ‘ Ğ¿Ñ€Ğ¸Ğ¾Ğ±Ñ€ĞµÑ‚ĞµĞ½Ğ¸Ğµ. ĞŸÑ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Enhancing Language Models with Structured Learning for Better Linguistic Competence', 'desc': 'This paper introduces L2T, a novel pre-training framework for language models that combines traditional next-token prediction with structured language learning tasks. By transforming raw text into structured input-output pairs, L2T enhances the linguistic competence of models, mimicking aspects of human language acquisition. The approach allows models to learn linguistic structures explicitly, leading to improved performance on linguistic benchmarks. Importantly, L2T maintains strong general reasoning capabilities, ensuring that the model remains versatile across various tasks.'}, 'zh': {'title': 'æå‡è¯­è¨€èƒ½åŠ›ä¸æ¨ç†èƒ½åŠ›çš„å¹³è¡¡', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„é¢„è®­ç»ƒæ¡†æ¶L2Tï¼Œç»“åˆäº†æ ‡å‡†çš„ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹å’Œç»“æ„åŒ–è¯­è¨€å­¦ä¹ ä»»åŠ¡ï¼Œä»¥æé«˜è¯­è¨€æ¨¡å‹çš„è¯­è¨€èƒ½åŠ›ã€‚é€šè¿‡å°†åŸå§‹æ–‡æœ¬è½¬åŒ–ä¸ºç»“æ„åŒ–çš„è¾“å…¥è¾“å‡ºå¯¹ï¼ŒL2Tä¸ºæ¨¡å‹æä¾›äº†æ˜ç¡®çš„è¯­è¨€åˆºæ¿€ï¼Œæ¨¡ä»¿äººç±»è¯­è¨€ä¹ å¾—çš„è¿‡ç¨‹ã€‚é¢„è®­ç»ƒæ¨¡å‹æ—¶ä½¿ç”¨åŸå§‹æ–‡æœ¬å’ŒL2Tæ•°æ®çš„æ··åˆï¼Œä¸ä»…æå‡äº†è¯­è¨€èƒ½åŠ›åŸºå‡†æµ‹è¯•çš„æ•´ä½“è¡¨ç°ï¼Œè¿˜åŠ å¿«äº†è¯­è¨€èƒ½åŠ›çš„è·å–ã€‚åŒæ—¶ï¼Œè¯¥æ–¹æ³•åœ¨ä¸€èˆ¬æ¨ç†ä»»åŠ¡ä¸Šä»ä¿æŒäº†ç«äº‰åŠ›çš„è¡¨ç°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.02933', 'title': 'Pearmut: Human Evaluation of Translation Made Trivial', 'url': 'https://huggingface.co/papers/2601.02933', 'abstract': 'Pearmut is a platform that simplifies human evaluation in multilingual NLP by providing a lightweight solution for end-to-end evaluation with support for various protocols and learning strategies.  \t\t\t\t\tAI-generated summary \t\t\t\t Human evaluation is the gold standard for multilingual NLP, but is often skipped in practice and substituted with automatic metrics, because it is notoriously complex and slow to set up with existing tools with substantial engineering and operational overhead. We introduce Pearmut, a lightweight yet feature-rich platform that makes end-to-end human evaluation as easy to run as automatic evaluation. Pearmut removes common entry barriers and provides support for evaluating multilingual tasks, with a particular focus on machine translation. The platform implements standard evaluation protocols, including DA, ESA, or MQM, but is also extensible to allow prototyping new protocols. It features document-level context, absolute and contrastive evaluation, attention checks, ESAAI pre-annotations and both static and active learning-based assignment strategies. Pearmut enables reliable human evaluation to become a practical, routine component of model development and diagnosis rather than an occasional effort.', 'score': 2, 'issue_id': 482, 'pub_date': '2026-01-06', 'pub_date_card': {'ru': '6 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 6', 'zh': '1æœˆ6æ—¥'}, 'hash': '0b135526779a2ba3', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#machine_translation', '#open_source', '#multilingual', '#benchmark'], 'emoji': 'ğŸ‘¥', 'ru': {'title': 'Ğ§ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ½Ğ° Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸ĞºĞµ: ÑƒĞ¿Ñ€Ğ¾Ñ‰ĞµĞ½Ğ¸Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ³Ğ¾ NLP Ñ‡ĞµÑ€ĞµĞ· Pearmut', 'desc': 'Pearmut â€” ÑÑ‚Ğ¾ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ¿Ñ€Ğ¾Ñ‰Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ»ÑĞ´ÑŒĞ¼Ğ¸ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°, Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ğ»Ñ‘Ğ³ĞºĞ¾Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ»Ğ¾Ğ². ĞŸĞ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ±Ğ°Ñ€ÑŒĞµÑ€Ñ‹ Ğ¿Ñ€Ğ¸ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ·Ğ¾Ğ»Ğ¾Ñ‚Ñ‹Ğ¼ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¾Ğ¼ Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… NLP ÑĞ¸ÑÑ‚ĞµĞ¼, Ğ½Ğ¾ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ·Ğ°Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼Ğ¸ Ğ¸Ğ·-Ğ·Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚. Pearmut Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ»Ñ‹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ (DA, ESA, MQM), Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµĞ¼Ñ‹Ğµ Ğ¿Ğ¾Ğ´ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ»Ñ‹, Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚, Ğ°Ğ±ÑĞ¾Ğ»ÑÑ‚Ğ½ÑƒÑ Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ, Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞŸĞ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° Ğ´ĞµĞ»Ğ°ĞµÑ‚ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½ÑƒÑ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ¸ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ¼ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ€ĞµĞ´ĞºĞ¸Ñ… ÑƒÑĞ¸Ğ»Ğ¸Ğ¹.'}, 'en': {'title': 'Simplifying Human Evaluation in Multilingual NLP with Pearmut', 'desc': 'Pearmut is a platform designed to streamline human evaluation in multilingual natural language processing (NLP). It addresses the challenges of traditional evaluation methods, which are often complex and time-consuming, by offering a lightweight solution that supports various evaluation protocols and learning strategies. The platform focuses on machine translation tasks and allows for both standard and customizable evaluation approaches, making it easier for researchers to incorporate human evaluation into their workflows. By simplifying the process, Pearmut aims to make reliable human evaluation a regular part of model development and assessment.'}, 'zh': {'title': 'ç®€åŒ–å¤šè¯­è¨€NLPçš„äººç±»è¯„ä¼°', 'desc': 'Pearmutæ˜¯ä¸€ä¸ªç®€åŒ–å¤šè¯­è¨€è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰äººç±»è¯„ä¼°çš„å¹³å°ï¼Œæä¾›è½»é‡çº§çš„ç«¯åˆ°ç«¯è¯„ä¼°è§£å†³æ–¹æ¡ˆã€‚å®ƒæ”¯æŒå¤šç§è¯„ä¼°åè®®å’Œå­¦ä¹ ç­–ç•¥ï¼Œç‰¹åˆ«å…³æ³¨æœºå™¨ç¿»è¯‘ä»»åŠ¡ã€‚é€šè¿‡å®ç°æ ‡å‡†è¯„ä¼°åè®®ï¼ŒPearmutæ¶ˆé™¤äº†å¸¸è§çš„å…¥é—¨éšœç¢ï¼Œä½¿äººç±»è¯„ä¼°å˜å¾—åƒè‡ªåŠ¨è¯„ä¼°ä¸€æ ·ç®€å•ã€‚è¯¥å¹³å°ä½¿å¾—å¯é çš„äººç±»è¯„ä¼°æˆä¸ºæ¨¡å‹å¼€å‘å’Œè¯Šæ–­çš„å¸¸è§„ç»„æˆéƒ¨åˆ†ï¼Œè€Œä¸æ˜¯å¶å°”çš„åŠªåŠ›ã€‚'}}, 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2601.03236', 'title': 'MAGMA: A Multi-Graph based Agentic Memory Architecture for AI Agents', 'url': 'https://huggingface.co/papers/2601.03236', 'abstract': 'MAGMA is a multi-graph memory architecture that improves long-context reasoning in language models by separating memory representation from retrieval logic across semantic, temporal, causal, and entity dimensions.  \t\t\t\t\tAI-generated summary \t\t\t\t Memory-Augmented Generation (MAG) extends Large Language Models with external memory to support long-context reasoning, but existing approaches largely rely on semantic similarity over monolithic memory stores, entangling temporal, causal, and entity information. This design limits interpretability and alignment between query intent and retrieved evidence, leading to suboptimal reasoning accuracy. In this paper, we propose MAGMA, a multi-graph agentic memory architecture that represents each memory item across orthogonal semantic, temporal, causal, and entity graphs. MAGMA formulates retrieval as policy-guided traversal over these relational views, enabling query-adaptive selection and structured context construction. By decoupling memory representation from retrieval logic, MAGMA provides transparent reasoning paths and fine-grained control over retrieval. Experiments on LoCoMo and LongMemEval demonstrate that MAGMA consistently outperforms state-of-the-art agentic memory systems in long-horizon reasoning tasks.', 'score': 1, 'issue_id': 475, 'pub_date': '2026-01-06', 'pub_date_card': {'ru': '6 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 6', 'zh': '1æœˆ6æ—¥'}, 'hash': '89b8c625aa26fb05', 'authors': ['Dongming Jiang', 'Yi Li', 'Guanpeng Li', 'Bingzhe Li'], 'affiliations': ['University of Florida', 'University of Texas at Dallas'], 'pdf_title_img': 'assets/pdf/title_img/2601.03236.jpg', 'data': {'categories': ['#rag', '#graphs', '#long_context', '#interpretability', '#reasoning', '#agents', '#architecture'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞœĞ½Ğ¾Ğ³Ğ¾Ğ¼ĞµÑ€Ğ½Ğ°Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°', 'desc': 'MAGMA Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¾ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ Ğ»Ğ¾Ğ³Ğ¸ĞºÑƒ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸, Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·ÑƒÑ ĞµÑ‘ Ğ¿Ğ¾ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼, Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼, Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¸ ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ğ¼ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸ÑĞ¼. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ½Ğ¾Ğ»Ğ¸Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ…Ñ€Ğ°Ğ½Ğ¸Ğ»Ğ¸Ñ‰Ğ°, Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ±Ğ»Ğ¸Ğ·Ğ¾ÑÑ‚ÑŒ, MAGMA Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ¼ Ğ¿Ğ¾Ğ¸ÑĞº Ñ‡ĞµÑ€ĞµĞ· Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ñ€ĞµĞ»Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ². Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'MAGMA: Enhancing Long-Context Reasoning with Multi-Graph Memory', 'desc': 'MAGMA is a new memory architecture designed to enhance long-context reasoning in language models. It separates how memory is represented from how it is retrieved, using different graphs for semantic, temporal, causal, and entity information. This separation allows for better interpretability and alignment between what a query is asking and the evidence retrieved, improving reasoning accuracy. Experiments show that MAGMA outperforms existing memory systems in tasks that require understanding over long contexts.'}, 'zh': {'title': 'MAGMAï¼šæå‡é•¿ä¸Šä¸‹æ–‡æ¨ç†çš„å¤šå›¾è®°å¿†æ¶æ„', 'desc': 'MAGMAæ˜¯ä¸€ç§å¤šå›¾è®°å¿†æ¶æ„ï¼Œæ—¨åœ¨æ”¹å–„è¯­è¨€æ¨¡å‹ä¸­çš„é•¿ä¸Šä¸‹æ–‡æ¨ç†ã€‚å®ƒé€šè¿‡åœ¨è¯­ä¹‰ã€æ—¶é—´ã€å› æœå’Œå®ä½“ç»´åº¦ä¸Šåˆ†ç¦»è®°å¿†è¡¨ç¤ºä¸æ£€ç´¢é€»è¾‘ï¼Œæå‡äº†æ¨ç†çš„å‡†ç¡®æ€§ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¾èµ–äºå•ä¸€è®°å¿†å­˜å‚¨çš„è¯­ä¹‰ç›¸ä¼¼æ€§ä¸åŒï¼ŒMAGMAå…è®¸æ›´çµæ´»çš„æŸ¥è¯¢é€‚åº”æ€§é€‰æ‹©å’Œç»“æ„åŒ–ä¸Šä¸‹æ–‡æ„å»ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMAGMAåœ¨é•¿æ—¶é—´æ¨ç†ä»»åŠ¡ä¸­ä¼˜äºç°æœ‰çš„è®°å¿†ç³»ç»Ÿã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.04090', 'title': 'Gen3R: 3D Scene Generation Meets Feed-Forward Reconstruction', 'url': 'https://huggingface.co/papers/2601.04090', 'abstract': 'Gen3R combines foundational reconstruction models with video diffusion models to generate 3D scenes with RGB videos and geometric information through aligned latents.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Gen3R, a method that bridges the strong priors of foundational reconstruction models and video diffusion models for scene-level 3D generation. We repurpose the VGGT reconstruction model to produce geometric latents by training an adapter on its tokens, which are regularized to align with the appearance latents of pre-trained video diffusion models. By jointly generating these disentangled yet aligned latents, Gen3R produces both RGB videos and corresponding 3D geometry, including camera poses, depth maps, and global point clouds. Experiments demonstrate that our approach achieves state-of-the-art results in single- and multi-image conditioned 3D scene generation. Additionally, our method can enhance the robustness of reconstruction by leveraging generative priors, demonstrating the mutual benefit of tightly coupling reconstruction and generative models.', 'score': 0, 'issue_id': 480, 'pub_date': '2026-01-07', 'pub_date_card': {'ru': '7 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 7', 'zh': '1æœˆ7æ—¥'}, 'hash': '5b7a5d3b7d8f9455', 'authors': ['Jiaxin Huang', 'Yuanbo Yang', 'Bangbang Yang', 'Lin Ma', 'Yuewen Ma', 'Yiyi Liao'], 'affiliations': ['ByteDance', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2601.04090.jpg', 'data': {'categories': ['#3d', '#multimodal', '#video', '#architecture', '#diffusion'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ¡Ğ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½ĞµĞ½Ğ½Ñ‹Ğµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ', 'desc': 'Gen3R Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ñ… VGGT Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ Ğ²Ğ¸Ğ´Ğ° Ğ¸Ğ· Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ¡Ğ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»Ñ‘Ğ½Ğ½Ñ‹Ñ…, Ğ½Ğ¾ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Gen3R Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ RGB Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰ÑƒÑ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½ÑƒÑ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ ĞºĞ°Ğ¼ĞµÑ€, ĞºĞ°Ñ€Ñ‚Ñ‹ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ¸ Ğ¾Ğ±Ğ»Ğ°ĞºĞ° Ñ‚Ğ¾Ñ‡ĞµĞº. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğ¹ 3D Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑÑ†ĞµĞ½ Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ»Ğ¸ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Bridging 3D Reconstruction and Video Generation with Gen3R', 'desc': 'Gen3R is a novel method that integrates foundational reconstruction models with video diffusion models to create 3D scenes from RGB videos and geometric data. It utilizes the VGGT reconstruction model to generate geometric latents, which are then aligned with appearance latents from pre-trained video diffusion models. By generating these aligned latents together, Gen3R can produce both realistic RGB videos and detailed 3D geometry, including camera poses and depth maps. The results show that Gen3R outperforms existing methods in 3D scene generation and enhances reconstruction robustness by combining generative and reconstruction techniques.'}, 'zh': {'title': 'Gen3Rï¼šé‡å»ºä¸ç”Ÿæˆæ¨¡å‹çš„å®Œç¾ç»“åˆ', 'desc': 'Gen3Ræ˜¯ä¸€ç§ç»“åˆåŸºç¡€é‡å»ºæ¨¡å‹å’Œè§†é¢‘æ‰©æ•£æ¨¡å‹çš„æ–¹æ³•ï¼Œç”¨äºç”Ÿæˆå…·æœ‰RGBè§†é¢‘å’Œå‡ ä½•ä¿¡æ¯çš„3Dåœºæ™¯ã€‚è¯¥æ–¹æ³•é€šè¿‡è®­ç»ƒé€‚é…å™¨ï¼Œå°†VGGTé‡å»ºæ¨¡å‹çš„å‡ ä½•æ½œå˜é‡ä¸é¢„è®­ç»ƒè§†é¢‘æ‰©æ•£æ¨¡å‹çš„å¤–è§‚æ½œå˜é‡å¯¹é½ã€‚é€šè¿‡å…±åŒç”Ÿæˆè¿™äº›è§£è€¦ä½†å¯¹é½çš„æ½œå˜é‡ï¼ŒGen3Rèƒ½å¤Ÿç”ŸæˆRGBè§†é¢‘åŠå…¶å¯¹åº”çš„3Då‡ ä½•ä¿¡æ¯ï¼ŒåŒ…æ‹¬ç›¸æœºå§¿æ€ã€æ·±åº¦å›¾å’Œå…¨å±€ç‚¹äº‘ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å•å›¾åƒå’Œå¤šå›¾åƒæ¡ä»¶ä¸‹çš„3Dåœºæ™¯ç”Ÿæˆä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œå¹¶ä¸”é€šè¿‡åˆ©ç”¨ç”Ÿæˆå…ˆéªŒå¢å¼ºäº†é‡å»ºçš„é²æ£’æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.03955', 'title': 'ResTok: Learning Hierarchical Residuals in 1D Visual Tokenizers for Autoregressive Image Generation', 'url': 'https://huggingface.co/papers/2601.03955', 'abstract': 'A novel 1D visual tokenizer called Residual Tokenizer is introduced that incorporates hierarchical residuals to improve autoregressive image generation by leveraging vision-specific design principles rather than language modeling approaches.  \t\t\t\t\tAI-generated summary \t\t\t\t Existing 1D visual tokenizers for autoregressive (AR) generation largely follow the design principles of language modeling, as they are built directly upon transformers whose priors originate in language, yielding single-hierarchy latent tokens and treating visual data as flat sequential token streams. However, this language-like formulation overlooks key properties of vision, particularly the hierarchical and residual network designs that have long been essential for convergence and efficiency in visual models. To bring "vision" back to vision, we propose the Residual Tokenizer (ResTok), a 1D visual tokenizer that builds hierarchical residuals for both image tokens and latent tokens. The hierarchical representations obtained through progressively merging enable cross-level feature fusion at each layer, substantially enhancing representational capacity. Meanwhile, the semantic residuals between hierarchies prevent information overlap, yielding more concentrated latent distributions that are easier for AR modeling. Cross-level bindings consequently emerge without any explicit constraints. To accelerate the generation process, we further introduce a hierarchical AR generator that substantially reduces sampling steps by predicting an entire level of latent tokens at once rather than generating them strictly token-by-token. Extensive experiments demonstrate that restoring hierarchical residual priors in visual tokenization significantly improves AR image generation, achieving a gFID of 2.34 on ImageNet-256 with only 9 sampling steps. Code is available at https://github.com/Kwai-Kolors/ResTok.', 'score': 0, 'issue_id': 482, 'pub_date': '2026-01-07', 'pub_date_card': {'ru': '7 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 7', 'zh': '1æœˆ7æ—¥'}, 'hash': '89b963ed293fb10d', 'authors': ['Xu Zhang', 'Cheng Da', 'Huan Yang', 'Kun Gai', 'Ming Lu', 'Zhan Ma'], 'affiliations': ['Kolors Team, Kuaishou Technology', 'Vision Lab, Nanjing University'], 'pdf_title_img': 'assets/pdf/title_img/2601.03955.jpg', 'data': {'categories': ['#cv', '#training', '#architecture'], 'emoji': 'ğŸ—ï¸', 'ru': {'title': 'Ğ˜ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾ÑÑ‚Ğ°Ñ‚ĞºĞ¸ Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ÑÑ‚ Ğ²Ğ¸Ğ´ĞµĞ½Ğ¸Ğµ Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¾Ğ´Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ñ‹Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Residual Tokenizer, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾ÑÑ‚Ğ°Ñ‚ĞºĞ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², Ğ·Ğ°Ğ¸Ğ¼ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ñ‹ Ğ¸Ğ· ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ²Ğ¸Ğ´ĞµĞ½Ğ¸Ğµ-ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ¸ Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ ÑĞ²ÑĞ·Ğ¸. Ğ¢Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾ÑÑ‚Ğ°Ñ‚ĞºĞ¸ ĞºĞ°Ğº Ğ´Ğ»Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ñ‚Ğ°Ğº Ğ¸ Ğ´Ğ»Ñ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ ĞºÑ€Ğ¾ÑÑ-ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ¾Ğµ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ ĞºĞ¾Ğ½Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ. Ğ’Ğ²ĞµĞ´ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ²Ñ‚Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ, Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ Ñ†ĞµĞ»Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ·Ğ° Ñ€Ğ°Ğ· Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ gFID 2.34 Ğ½Ğ° ImageNet-256 Ğ²ÑĞµĞ³Ğ¾ Ğ·Ğ° 9 ÑˆĞ°Ğ³Ğ¾Ğ².'}, 'en': {'title': 'Revolutionizing Image Generation with Hierarchical Residuals', 'desc': 'The paper presents a new visual tokenizer called Residual Tokenizer (ResTok) designed to enhance autoregressive image generation. Unlike traditional tokenizers that mimic language models, ResTok utilizes hierarchical residuals to better capture the structure of visual data. This approach allows for improved feature fusion and reduces information overlap, leading to more effective latent distributions for modeling. Additionally, a hierarchical autoregressive generator is introduced, which accelerates the generation process by predicting multiple latent tokens simultaneously, resulting in significant performance improvements.'}, 'zh': {'title': 'å¼•å…¥å±‚æ¬¡æ®‹å·®ï¼Œæå‡å›¾åƒç”Ÿæˆèƒ½åŠ›', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹çš„1Dè§†è§‰æ ‡è®°å™¨ï¼Œç§°ä¸ºæ®‹å·®æ ‡è®°å™¨ï¼ˆResidual Tokenizerï¼‰ï¼Œå®ƒé€šè¿‡å¼•å…¥å±‚æ¬¡æ®‹å·®æ¥æ”¹å–„è‡ªå›å½’å›¾åƒç”Ÿæˆã€‚ä¸ä¼ ç»Ÿçš„åŸºäºè¯­è¨€å»ºæ¨¡çš„æ–¹æ³•ä¸åŒï¼Œæ®‹å·®æ ‡è®°å™¨åˆ©ç”¨è§†è§‰ç‰¹å®šçš„è®¾è®¡åŸåˆ™ï¼Œæ„å»ºå±‚æ¬¡åŒ–çš„æ®‹å·®ç»“æ„ï¼Œä»¥å¢å¼ºå›¾åƒå’Œæ½œåœ¨æ ‡è®°çš„è¡¨ç¤ºèƒ½åŠ›ã€‚é€šè¿‡é€æ­¥åˆå¹¶è·å¾—çš„å±‚æ¬¡è¡¨ç¤ºï¼Œèƒ½å¤Ÿåœ¨æ¯ä¸€å±‚å®ç°è·¨å±‚ç‰¹å¾èåˆï¼Œä»è€Œæé«˜æ¨¡å‹çš„è¡¨ç°åŠ›ã€‚åŒæ—¶ï¼Œå±‚æ¬¡ä¹‹é—´çš„è¯­ä¹‰æ®‹å·®é˜²æ­¢äº†ä¿¡æ¯é‡å ï¼Œä½¿å¾—æ½œåœ¨åˆ†å¸ƒæ›´åŠ é›†ä¸­ï¼Œä¾¿äºè‡ªå›å½’å»ºæ¨¡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.00705', 'title': 'RGS-SLAM: Robust Gaussian Splatting SLAM with One-Shot Dense Initialization', 'url': 'https://huggingface.co/papers/2601.00705', 'abstract': 'RGS-SLAM presents a robust Gaussian-splatting SLAM framework that uses dense multi-view correspondences and DINOv3 descriptors for efficient, stable mapping with improved rendering fidelity.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce RGS-SLAM, a robust Gaussian-splatting SLAM framework that replaces the residual-driven densification stage of GS-SLAM with a training-free correspondence-to-Gaussian initialization. Instead of progressively adding Gaussians as residuals reveal missing geometry, RGS-SLAM performs a one-shot triangulation of dense multi-view correspondences derived from DINOv3 descriptors refined through a confidence-aware inlier classifier, generating a well-distributed and structure-aware Gaussian seed prior to optimization. This initialization stabilizes early mapping and accelerates convergence by roughly 20\\%, yielding higher rendering fidelity in texture-rich and cluttered scenes while remaining fully compatible with existing GS-SLAM pipelines. Evaluated on the TUM RGB-D and Replica datasets, RGS-SLAM achieves competitive or superior localization and reconstruction accuracy compared with state-of-the-art Gaussian and point-based SLAM systems, sustaining real-time mapping performance at up to 925 FPS.', 'score': 0, 'issue_id': 476, 'pub_date': '2026-12-28', 'pub_date_card': {'ru': '28 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 28', 'zh': '12æœˆ28æ—¥'}, 'hash': '1549408a2d070aca', 'authors': ['Wei-Tse Cheng', 'Yen-Jen Chiou', 'Yuan-Fu Yang'], 'affiliations': ['National Yang Ming Chiao Tung University'], 'pdf_title_img': 'assets/pdf/title_img/2601.00705.jpg', 'data': {'categories': [], 'emoji': 'ğŸ—ºï¸', 'ru': {'title': 'Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ğµ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ“Ğ°ÑƒÑÑĞ¾Ğ²Ğ° SLAM', 'desc': 'ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ RGS-SLAM, Ğ¼Ğ¾Ñ‰Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ĞºĞ°Ñ€Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ“Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹Ñ… ÑĞ¿Ğ»Ğ°Ñ‚Ñ‚Ğ¸Ğ½Ğ³Ğ° Ğ´Ğ»Ñ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑÑ†ĞµĞ½Ñ‹. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ğ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ“Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹Ñ… Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ğ¿Ğ¾ Ğ¼ĞµÑ€Ğµ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑ‰ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸, Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ğ¾Ğ´Ğ½Ğ¾Ñ€Ğ°Ğ·Ğ¾Ğ²ÑƒÑ Ñ‚Ñ€Ğ¸Ğ°Ğ½Ğ³ÑƒĞ»ÑÑ†Ğ¸Ñ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ñ… ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ğ¸Ğ´Ğ°Ğ¼Ğ¸, Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ· DINOv3 Ğ´ĞµÑĞºÑ€Ğ¸Ğ¿Ñ‚Ğ¾Ñ€Ğ¾Ğ². Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ“Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°Ğ½Ğ½Ğ¸Ğ¹ ÑÑ‚Ğ°Ğ¿ ĞºĞ°Ñ€Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ½Ğ¾ Ğ½Ğ° 20%, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… TUM RGB-D Ğ¸ Replica, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ñ€ĞµĞ¼Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ´Ğ¾ 925 FPS.'}, 'en': {'title': 'RGS-SLAM: Fast and Accurate Mapping with Gaussian Splatting', 'desc': 'RGS-SLAM is a new framework for Simultaneous Localization and Mapping (SLAM) that enhances the process of creating 3D maps using Gaussian splatting. It introduces a method that initializes mapping using dense correspondences from DINOv3 descriptors, eliminating the need for a complex training phase. This approach allows for faster and more stable mapping by quickly generating a well-distributed Gaussian representation of the environment. RGS-SLAM has shown to improve accuracy and rendering quality in challenging scenes while maintaining high-speed performance, making it compatible with existing SLAM systems.'}, 'zh': {'title': 'RGS-SLAMï¼šé«˜æ•ˆç¨³å®šçš„SLAMæ–°æ¡†æ¶', 'desc': 'RGS-SLAMæ˜¯ä¸€ç§ç¨³å¥çš„é«˜æ–¯ç‚¹äº‘SLAMæ¡†æ¶ï¼Œåˆ©ç”¨å¯†é›†çš„å¤šè§†è§’å¯¹åº”å…³ç³»å’ŒDINOv3æè¿°ç¬¦è¿›è¡Œé«˜æ•ˆã€ç¨³å®šçš„åœ°å›¾æ„å»ºã€‚å®ƒé€šè¿‡ä¸€ç§æ— è®­ç»ƒçš„å¯¹åº”åˆ°é«˜æ–¯åˆå§‹åŒ–æ–¹æ³•ï¼Œæ›¿ä»£äº†GS-SLAMä¸­çš„æ®‹å·®é©±åŠ¨å¯†é›†åŒ–é˜¶æ®µã€‚RGS-SLAMä¸€æ¬¡æ€§ä¸‰è§’åŒ–å¯†é›†çš„å¤šè§†è§’å¯¹åº”å…³ç³»ï¼Œç”Ÿæˆç»“æ„æ„ŸçŸ¥çš„é«˜æ–¯ç§å­ï¼Œä»è€ŒåŠ é€Ÿæ”¶æ•›å¹¶æé«˜æ¸²æŸ“è´¨é‡ã€‚ç»è¿‡TUM RGB-Då’ŒReplicaæ•°æ®é›†çš„è¯„ä¼°ï¼ŒRGS-SLAMåœ¨å®šä½å’Œé‡å»ºç²¾åº¦ä¸Šä¸æœ€å…ˆè¿›çš„SLAMç³»ç»Ÿç›¸æ¯”è¡¨ç°å‡ºè‰²ï¼Œä¸”åœ¨é«˜è¾¾925 FPSçš„é€Ÿåº¦ä¸‹å®ç°å®æ—¶åœ°å›¾æ„å»ºã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.07348', 'title': 'Controlled Self-Evolution for Algorithmic Code Optimization', 'url': 'https://huggingface.co/papers/2601.07348', 'abstract': 'Controlled Self-Evolution method improves code generation through diversified initialization, feedback-guided genetic evolution, and hierarchical memory to enhance exploration efficiency and solution quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Self-evolution methods enhance code generation through iterative "generate-verify-refine" cycles, yet existing approaches suffer from low exploration efficiency, failing to discover solutions with superior complexity within limited budgets. This inefficiency stems from initialization bias trapping evolution in poor solution regions, uncontrolled stochastic operations lacking feedback guidance, and insufficient experience utilization across tasks. To address these bottlenecks, we propose Controlled Self-Evolution (CSE), which consists of three key components. Diversified Planning Initialization generates structurally distinct algorithmic strategies for broad solution space coverage. Genetic Evolution replaces stochastic operations with feedback-guided mechanisms, enabling targeted mutation and compositional crossover. Hierarchical Evolution Memory captures both successful and failed experiences at inter-task and intra-task levels. Experiments on EffiBench-X demonstrate that CSE consistently outperforms all baselines across various LLM backbones. Furthermore, CSE achieves higher efficiency from early generations and maintains continuous improvement throughout evolution. Our code is publicly available at https://github.com/QuantaAlpha/EvoControl.', 'score': 94, 'issue_id': 588, 'pub_date': '2026-01-12', 'pub_date_card': {'ru': '12 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 12', 'zh': '1æœˆ12æ—¥'}, 'hash': '3c8478e6aa318055', 'authors': ['Tu Hu', 'Ronghao Chen', 'Shuo Zhang', 'Jianghao Yin', 'Mou Xiao Feng', 'Jingping Liu', 'Shaolei Zhang', 'Wenqi Jiang', 'Yuqi Fang', 'Sen Hu', 'Yi Xu', 'Huacan Wang'], 'affiliations': ['ECNU', 'Midea-AIRC', 'NJU', 'PKU', 'QuantaAlpha', 'RUC', 'SYSU'], 'pdf_title_img': 'assets/pdf/title_img/2601.07348.jpg', 'data': {'categories': [], 'emoji': 'ğŸ§¬', 'ru': {'title': 'Ğ£Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ°Ñ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Controlled Self-Evolution Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ° Ñ‡ĞµÑ€ĞµĞ· ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ‚Ñ€Ñ‘Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°Ñ…: Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ğ°Ñ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºÑ€Ñ‹Ñ‚Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹, Ğ³ĞµĞ½ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ñ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·ÑŒÑ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ñ‹Ñ… Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹, Ğ¸ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ¾Ğ¿Ñ‹Ñ‚Ğ° Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ñ†ĞµĞ»ĞµĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ°Ñ Ğ¼ÑƒÑ‚Ğ°Ñ†Ğ¸Ñ Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ ĞºÑ€Ğ¾ÑÑĞ¾Ğ²ĞµÑ€ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ½Ğ°Ğ´ Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ EffiBench-X.'}, 'en': {'title': 'Unlocking Code Generation with Controlled Self-Evolution', 'desc': 'The Controlled Self-Evolution (CSE) method enhances code generation by addressing inefficiencies in existing self-evolution techniques. It introduces diversified initialization to explore a wider solution space and employs feedback-guided genetic evolution to improve mutation and crossover processes. Additionally, CSE utilizes hierarchical memory to retain valuable experiences from both successful and unsuccessful attempts, facilitating better learning across tasks. Experiments show that CSE significantly outperforms traditional methods, achieving higher efficiency and continuous improvement in code generation.'}, 'zh': {'title': 'å—æ§è‡ªæˆ‘è¿›åŒ–ï¼šæå‡ä»£ç ç”Ÿæˆçš„æ•ˆç‡ä¸è´¨é‡', 'desc': 'å—æ§è‡ªæˆ‘è¿›åŒ–æ–¹æ³•é€šè¿‡å¤šæ ·åŒ–åˆå§‹åŒ–ã€åé¦ˆå¼•å¯¼çš„é—ä¼ è¿›åŒ–å’Œåˆ†å±‚è®°å¿†æ¥æé«˜ä»£ç ç”Ÿæˆçš„æ•ˆç‡å’Œè´¨é‡ã€‚ç°æœ‰æ–¹æ³•åœ¨æ¢ç´¢æ•ˆç‡ä¸Šå­˜åœ¨ä¸è¶³ï¼Œæ— æ³•åœ¨æœ‰é™é¢„ç®—å†…å‘ç°æ›´å¤æ‚çš„è§£å†³æ–¹æ¡ˆã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å—æ§è‡ªæˆ‘è¿›åŒ–ï¼ˆCSEï¼‰ï¼Œå®ƒé€šè¿‡ç”Ÿæˆç»“æ„ä¸Šä¸åŒçš„ç®—æ³•ç­–ç•¥æ¥è¦†ç›–å¹¿æ³›çš„è§£å†³ç©ºé—´ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCSEåœ¨å¤šä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ä¸Šå§‹ç»ˆä¼˜äºæ‰€æœ‰åŸºçº¿æ–¹æ³•ï¼Œå¹¶åœ¨æ—©æœŸç”Ÿæˆä¸­å®ç°äº†æ›´é«˜çš„æ•ˆç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.09688', 'title': 'DeepResearchEval: An Automated Framework for Deep Research Task Construction and Agentic Evaluation', 'url': 'https://huggingface.co/papers/2601.09688', 'abstract': 'DeepResearchEval presents an automated framework for creating complex research tasks and evaluating them through agent-based methods that adapt to task specifics and verify facts without relying on citations.  \t\t\t\t\tAI-generated summary \t\t\t\t Deep research systems are widely used for multi-step web research, analysis, and cross-source synthesis, yet their evaluation remains challenging. Existing benchmarks often require annotation-intensive task construction, rely on static evaluation dimensions, or fail to reliably verify facts when citations are missing. To bridge these gaps, we introduce DeepResearchEval, an automated framework for deep research task construction and agentic evaluation. For task construction, we propose a persona-driven pipeline generating realistic, complex research tasks anchored in diverse user profiles, applying a two-stage filter Task Qualification and Search Necessity to retain only tasks requiring multi-source evidence integration and external retrieval. For evaluation, we propose an agentic pipeline with two components: an Adaptive Point-wise Quality Evaluation that dynamically derives task-specific evaluation dimensions, criteria, and weights conditioned on each generated task, and an Active Fact-Checking that autonomously extracts and verifies report statements via web search, even when citations are missing.', 'score': 90, 'issue_id': 589, 'pub_date': '2026-01-14', 'pub_date_card': {'ru': '14 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 14', 'zh': '1æœˆ14æ—¥'}, 'hash': '66f720d461457154', 'authors': ['Yibo Wang', 'Lei Wang', 'Yue Deng', 'Keming Wu', 'Yao Xiao', 'Huanjin Yao', 'Liwei Kang', 'Hai Ye', 'Yongcheng Jing', 'Lidong Bing'], 'affiliations': ['Infinity Lab, Shanda Group', 'Nanyang Technological University'], 'pdf_title_img': 'assets/pdf/title_img/2601.09688.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#agents', '#science'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ñ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾Ğ¹ Ñ„Ğ°ĞºÑ‚-Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¾Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ DeepResearchEval â€” Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¸Ñ… Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². Ğ”Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğ¹ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸ĞµĞ¹, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ñ‚ÑŒ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‚Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğµ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ· Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ° Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ². Ğ”Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ñ‹ Ğ´Ğ²Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹: Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¿Ğ¾Ñ‚Ğ¾Ñ‡ĞµÑ‡Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸Ğ¸, Ğ¸ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ñ„Ğ°ĞºÑ‚-Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ°, ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ÑÑ‰Ğ°Ñ Ğ¸ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒÑÑ‰Ğ°Ñ ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ²ĞµĞ±-Ğ¿Ğ¾Ğ¸ÑĞº Ğ±ĞµĞ· Ğ¾Ğ¿Ğ¾Ñ€Ñ‹ Ğ½Ğ° Ñ†Ğ¸Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ­Ñ‚Ğ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ° Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ñ‚Ñ€ÑƒĞ´Ğ¾Ñ‘Ğ¼ĞºĞ¾ÑÑ‚Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ½ĞµĞ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞµ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'Automating Research Tasks with Adaptive Evaluation', 'desc': 'DeepResearchEval is an innovative framework designed to automate the creation and evaluation of complex research tasks. It utilizes a persona-driven approach to generate tasks that reflect diverse user needs, ensuring that only those requiring multi-source evidence are selected. The evaluation process is enhanced through an adaptive system that tailors quality metrics to each specific task and incorporates active fact-checking methods to verify information without relying on citations. This framework addresses the challenges of traditional research evaluation by providing a more dynamic and reliable assessment of research tasks.'}, 'zh': {'title': 'è‡ªåŠ¨åŒ–ç ”ç©¶ä»»åŠ¡è¯„ä¼°çš„æ–°æ¡†æ¶', 'desc': 'DeepResearchEvalæ˜¯ä¸€ä¸ªè‡ªåŠ¨åŒ–æ¡†æ¶ï¼Œç”¨äºåˆ›å»ºå¤æ‚çš„ç ”ç©¶ä»»åŠ¡å¹¶é€šè¿‡åŸºäºä»£ç†çš„æ–¹æ³•è¿›è¡Œè¯„ä¼°ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿæ ¹æ®ä»»åŠ¡çš„å…·ä½“æƒ…å†µè¿›è¡Œé€‚åº”æ€§è¯„ä¼°ï¼Œå¹¶åœ¨æ²¡æœ‰å¼•ç”¨çš„æƒ…å†µä¸‹éªŒè¯äº‹å®ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ä»¥ç”¨æˆ·ç”»åƒä¸ºé©±åŠ¨çš„ä»»åŠ¡æ„å»ºæµç¨‹ï¼Œç”Ÿæˆéœ€è¦å¤šæºè¯æ®æ•´åˆçš„çœŸå®å¤æ‚ç ”ç©¶ä»»åŠ¡ã€‚è¯„ä¼°éƒ¨åˆ†åŒ…æ‹¬åŠ¨æ€ç”Ÿæˆä»»åŠ¡ç‰¹å®šè¯„ä¼°ç»´åº¦çš„è´¨é‡è¯„ä¼°å’Œè‡ªä¸»æå–éªŒè¯æŠ¥å‘Šé™ˆè¿°çš„äº‹å®æ£€æŸ¥ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.09259', 'title': 'MAXS: Meta-Adaptive Exploration with LLM Agents', 'url': 'https://huggingface.co/papers/2601.09259', 'abstract': 'MAXS is a meta-adaptive reasoning framework for LLM agents that improves multi-tool reasoning through lookahead strategies and trajectory convergence mechanisms, balancing global effectiveness and computational efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Model (LLM) Agents exhibit inherent reasoning abilities through the collaboration of multiple tools. However, during agent inference, existing methods often suffer from (i) locally myopic generation, due to the absence of lookahead, and (ii) trajectory instability, where minor early errors can escalate into divergent reasoning paths. These issues make it difficult to balance global effectiveness and computational efficiency. To address these two issues, we propose meta-adaptive exploration with LLM agents https://github.com/exoskeletonzj/MAXS, a meta-adaptive reasoning framework based on LLM Agents that flexibly integrates tool execution and reasoning planning. MAXS employs a lookahead strategy to extend reasoning paths a few steps ahead, estimating the advantage value of tool usage, and combines step consistency variance and inter-step trend slopes to jointly select stable, consistent, and high-value reasoning steps. Additionally, we introduce a trajectory convergence mechanism that controls computational cost by halting further rollouts once path consistency is achieved, enabling a balance between resource efficiency and global effectiveness in multi-tool reasoning. We conduct extensive empirical studies across three base models (MiMo-VL-7B, Qwen2.5-VL-7B, Qwen2.5-VL-32B) and five datasets, demonstrating that MAXS consistently outperforms existing methods in both performance and inference efficiency. Further analysis confirms the effectiveness of our lookahead strategy and tool usage.', 'score': 81, 'issue_id': 588, 'pub_date': '2026-01-14', 'pub_date_card': {'ru': '14 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 14', 'zh': '1æœˆ14æ—¥'}, 'hash': '75d53a8919dc55a2', 'authors': ['Jian Zhang', 'Zhiyuan Wang', 'Zhangqi Wang', 'Yu He', 'Haoran Luo', 'li yuan', 'Lingling Zhang', 'Rui Mao', 'Qika Lin', 'Jun Liu'], 'affiliations': ['Nanyang Technological University', 'National University of Singapore', 'South China University of Technology', 'Xian Jiaotong University'], 'pdf_title_img': 'assets/pdf/title_img/2601.09259.jpg', 'data': {'categories': ['#reasoning', '#optimization'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞŸÑ€ĞµĞ´ÑƒÑĞ¼Ğ¾Ñ‚Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ñ Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸ĞµĞ¹ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹', 'desc': 'MAXS â€” ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ°-Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLM, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ‚Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑĞ¼Ğ¾Ñ‚Ñ€Ğ° Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ´Ğ²Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹: Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¸Ğ¾Ğ¿Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ (Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑƒÑĞ¼Ğ¾Ñ‚Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸) Ğ¸ Ğ½ĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹, ĞºĞ¾Ğ³Ğ´Ğ° Ñ€Ğ°Ğ½Ğ½Ğ¸Ğµ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´ÑÑ‚ Ğº Ñ€Ğ°ÑÑ…Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿ÑƒÑ‚ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. MAXS Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ lookahead Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ ÑˆĞ°Ğ³Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ´Ğ¸ÑĞ¿ĞµÑ€ÑĞ¸Ğ¸ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ‚Ñ€ĞµĞ½Ğ´Ğ¾Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ ÑˆĞ°Ğ³Ğ°Ğ¼Ğ¸. ĞœĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹, Ğ¾ÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°Ñ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞ¸Ğµ Ñ€Ğ°Ğ·Ğ²Ñ‘Ñ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿ÑƒÑ‚Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ.'}, 'en': {'title': 'Enhancing LLM Reasoning with MAXS: Smart Planning for Better Performance', 'desc': 'MAXS is a new framework designed to enhance the reasoning capabilities of Large Language Model (LLM) agents by using advanced strategies for planning and tool execution. It addresses two main problems: the tendency for agents to make short-sighted decisions and the instability of reasoning paths that can lead to errors. By implementing a lookahead strategy, MAXS predicts the benefits of using different tools and selects the most effective reasoning steps. Additionally, it introduces a mechanism to stop unnecessary computations once a stable reasoning path is found, ensuring both efficiency and effectiveness in multi-tool reasoning tasks.'}, 'zh': {'title': 'MAXSï¼šæå‡å¤šå·¥å…·æ¨ç†çš„æ™ºèƒ½æ¡†æ¶', 'desc': 'MAXSæ˜¯ä¸€ä¸ªé’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†çš„å…ƒè‡ªé€‚åº”æ¨ç†æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å‰ç»ç­–ç•¥å’Œè½¨è¿¹æ”¶æ•›æœºåˆ¶æ¥æ”¹å–„å¤šå·¥å…·æ¨ç†ã€‚è¯¥æ¡†æ¶è§£å†³äº†ç°æœ‰æ–¹æ³•åœ¨æ¨ç†è¿‡ç¨‹ä¸­å¸¸è§çš„å±€éƒ¨çŸ­è§†å’Œè½¨è¿¹ä¸ç¨³å®šé—®é¢˜ï¼Œä»è€Œå¹³è¡¡äº†å…¨å±€æœ‰æ•ˆæ€§å’Œè®¡ç®—æ•ˆç‡ã€‚MAXSé€šè¿‡å‰ç»ç­–ç•¥å»¶ä¼¸æ¨ç†è·¯å¾„ï¼Œå¹¶ç»“åˆæ­¥éª¤ä¸€è‡´æ€§æ–¹å·®å’Œè·¨æ­¥éª¤è¶‹åŠ¿æ–œç‡æ¥é€‰æ‹©ç¨³å®šä¸”é«˜ä»·å€¼çš„æ¨ç†æ­¥éª¤ã€‚æ­¤å¤–ï¼Œè½¨è¿¹æ”¶æ•›æœºåˆ¶èƒ½å¤Ÿåœ¨è¾¾åˆ°è·¯å¾„ä¸€è‡´æ€§ååœæ­¢è¿›ä¸€æ­¥çš„æ¨ç†ï¼Œä»è€Œæœ‰æ•ˆæ§åˆ¶è®¡ç®—æˆæœ¬ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.09274', 'title': 'A^3-Bench: Benchmarking Memory-Driven Scientific Reasoning via Anchor and Attractor Activation', 'url': 'https://huggingface.co/papers/2601.09274', 'abstract': 'Scientific reasoning relies not only on logical inference but also on activating prior knowledge and experiential structures. Memory can efficiently reuse knowledge and enhance reasoning consistency and stability. However, existing benchmarks mainly evaluate final answers or step-by-step coherence, overlooking the memory-driven mechanisms that underlie human reasoning, which involves activating anchors and attractors, then integrating them into multi-step inference. To address this gap, we propose A^3-Bench~ https://a3-bench.github.io, a benchmark designed to evaluate scientific reasoning through dual-scale memory-driven activation, grounded in Anchor and Attractor Activation. First, we annotate 2,198 science reasoning problems across domains using the SAPM process(subject, anchor & attractor, problem, and memory developing). Second, we introduce a dual-scale memory evaluation framework utilizing anchors and attractors, along with the AAUI(Anchor--Attractor Utilization Index) metric to measure memory activation rates. Finally, through experiments with various base models and paradigms, we validate A^3-Bench and analyze how memory activation impacts reasoning performance, providing insights into memory-driven scientific reasoning.', 'score': 74, 'issue_id': 588, 'pub_date': '2026-01-14', 'pub_date_card': {'ru': '14 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 14', 'zh': '1æœˆ14æ—¥'}, 'hash': 'f09abfdc90cf025f', 'authors': ['Jian Zhang', 'Yu He', 'Zhiyuan Wang', 'Zhangqi Wang', 'Kai He', 'Fangzhi Xu', 'Qika Lin', 'Jun Liu'], 'affiliations': ['National University of Singapore', 'Xian Jiaotong University'], 'pdf_title_img': 'assets/pdf/title_img/2601.09274.jpg', 'data': {'categories': ['#reasoning', '#dataset', '#science', '#benchmark'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞŸĞ°Ğ¼ÑÑ‚ÑŒ ĞºĞ°Ğº Ğ¾ÑĞ½Ğ¾Ğ²Ğ° Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ: Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğµ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸ ÑĞºĞ¾Ñ€ĞµĞ¹ Ğ¸ Ğ°Ñ‚Ñ‚Ñ€Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¾Ğ²', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ñ ÑĞºĞ¾Ñ€ĞµĞ¹ (anchors) Ğ¸ Ğ°Ñ‚Ñ‚Ñ€Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¾Ğ² (attractors) - ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº A^3-Bench Ñ 2198 Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ SAPM Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ¸ ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ´Ğ²ÑƒÑ…ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ñ Ğ½Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¾Ğ¹ AAUI (Ğ¸Ğ½Ğ´ĞµĞºÑ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞºĞ¾Ñ€ĞµĞ¹ Ğ¸ Ğ°Ñ‚Ñ‚Ñ€Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¾Ğ²), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸Ğ·Ğ¼ĞµÑ€ÑĞµÑ‚ ÑÑ‚ĞµĞ¿ĞµĞ½ÑŒ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ²Ğ»Ğ¸ÑĞµÑ‚ Ğ½Ğ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Harnessing Memory for Enhanced Scientific Reasoning', 'desc': 'This paper introduces A^3-Bench, a new benchmark for evaluating scientific reasoning that emphasizes the role of memory in the reasoning process. It highlights how activating prior knowledge through anchors and attractors can enhance the consistency and stability of reasoning. The authors annotate a large set of science reasoning problems and propose a dual-scale memory evaluation framework to assess how effectively memory is utilized during reasoning tasks. Experiments demonstrate the impact of memory activation on reasoning performance, offering valuable insights into the mechanisms of human-like scientific reasoning.'}, 'zh': {'title': 'è®°å¿†é©±åŠ¨çš„ç§‘å­¦æ¨ç†è¯„ä¼°æ–°åŸºå‡†', 'desc': 'è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†ç§‘å­¦æ¨ç†ä¸­è®°å¿†çš„ä½œç”¨ï¼Œå¼ºè°ƒäº†æ¿€æ´»å…ˆå‰çŸ¥è¯†å’Œç»éªŒç»“æ„çš„é‡è¦æ€§ã€‚ä½œè€…æå‡ºäº†A^3-BenchåŸºå‡†ï¼Œæ—¨åœ¨é€šè¿‡åŒå°ºåº¦è®°å¿†é©±åŠ¨æ¿€æ´»æ¥è¯„ä¼°ç§‘å­¦æ¨ç†ã€‚ç ”ç©¶ä¸­ä½¿ç”¨äº†2,198ä¸ªç§‘å­¦æ¨ç†é—®é¢˜ï¼Œå¹¶å¼•å…¥äº†é”šç‚¹å’Œå¸å¼•ç‚¹çš„æ¦‚å¿µæ¥åˆ†æè®°å¿†æ¿€æ´»ç‡ã€‚é€šè¿‡å®éªŒéªŒè¯äº†A^3-Benchçš„æœ‰æ•ˆæ€§ï¼Œå¹¶åˆ†æäº†è®°å¿†æ¿€æ´»å¯¹æ¨ç†è¡¨ç°çš„å½±å“ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.09088', 'title': 'Distribution-Aligned Sequence Distillation for Superior Long-CoT Reasoning', 'url': 'https://huggingface.co/papers/2601.09088', 'abstract': "A lightweight open-source reasoning model achieves state-of-the-art performance through enhanced sequence-level distillation that addresses limitations in current teacher-student knowledge transfer methods.  \t\t\t\t\tAI-generated summary \t\t\t\t In this report, we introduce DASD-4B-Thinking, a lightweight yet highly capable, fully open-source reasoning model. It achieves SOTA performance among open-source models of comparable scale across challenging benchmarks in mathematics, scientific reasoning, and code generation -- even outperforming several larger models. We begin by critically reexamining a widely adopted distillation paradigm in the community: SFT on teacher-generated responses, also known as sequence-level distillation. Although a series of recent works following this scheme have demonstrated remarkable efficiency and strong empirical performance, they are primarily grounded in the SFT perspective. Consequently, these approaches focus predominantly on designing heuristic rules for SFT data filtering, while largely overlooking the core principle of distillation itself -- enabling the student model to learn the teacher's full output distribution so as to inherit its generalization capability. Specifically, we identify three critical limitations in current practice: i) Inadequate representation of the teacher's sequence-level distribution; ii) Misalignment between the teacher's output distribution and the student's learning capacity; and iii) Exposure bias arising from teacher-forced training versus autoregressive inference. In summary, these shortcomings reflect a systemic absence of explicit teacher-student interaction throughout the distillation process, leaving the essence of distillation underexploited. To address these issues, we propose several methodological innovations that collectively form an enhanced sequence-level distillation training pipeline. Remarkably, DASD-4B-Thinking obtains competitive results using only 448K training samples -- an order of magnitude fewer than those employed by most existing open-source efforts. To support community research, we publicly release our models and the training dataset.", 'score': 43, 'issue_id': 595, 'pub_date': '2026-01-14', 'pub_date_card': {'ru': '14 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 14', 'zh': '1æœˆ14æ—¥'}, 'hash': '0105abc01f29aa01', 'authors': ['Shaotian Yan', 'Kaiyuan Liu', 'Chen Shen', 'Bing Wang', 'Sinan Fan', 'Jun Zhang', 'Yue Wu', 'Zheng Wang', 'Jieping Ye'], 'affiliations': ['Alibaba Cloud'], 'pdf_title_img': 'assets/pdf/title_img/2601.09088.jpg', 'data': {'categories': ['#training', '#optimization', '#dataset', '#benchmark', '#open_source', '#reasoning', '#small_models', '#transfer_learning'], 'emoji': 'ğŸ“', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ğ° Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ Ğ¸ ÑƒÑ‡ĞµĞ½Ğ¸ĞºĞ°', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° DASD-4B-Thinking â€” Ğ»Ñ‘Ğ³ĞºĞ°Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² ÑÑ€ĞµĞ´Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñƒ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿ĞµÑ€ĞµĞ¾ÑĞ¼Ñ‹ÑĞ»ÑÑÑ‚ Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ SFT Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ¸ Ğ²Ñ‹ÑĞ²Ğ»ÑÑÑ‚ Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹: Ğ½ĞµĞ°Ğ´ĞµĞºĞ²Ğ°Ñ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ¾Ğ² ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ, Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸ĞµĞ¼ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒÑ ÑƒÑ‡ĞµĞ½Ğ¸ĞºĞ°, Ğ¸ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ğµ ÑĞºÑĞ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ñ€Ğ¸Ğ½ÑƒĞ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ¹ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ ÑĞ²Ğ½Ğ¾ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ Ğ¸ ÑƒÑ‡ĞµĞ½Ğ¸ĞºĞ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ½Ğ°ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ…, Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ°, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²ÑĞµĞ³Ğ¾ 448K Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ â€” Ğ½Ğ° Ğ¿Ğ¾Ñ€ÑĞ´Ğ¾Ğº Ğ¼ĞµĞ½ÑŒÑˆĞµ, Ñ‡ĞµĞ¼ Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‚ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹.'}, 'en': {'title': 'Revolutionizing Knowledge Transfer in Lightweight AI Models', 'desc': "This paper presents DASD-4B-Thinking, a lightweight open-source reasoning model that excels in performance through improved sequence-level distillation techniques. The authors critique existing teacher-student knowledge transfer methods, highlighting their limitations in representing the teacher's output distribution and aligning it with the student's learning capacity. They propose innovative methodologies to enhance the distillation process, which allows the student model to better learn from the teacher. As a result, DASD-4B-Thinking achieves state-of-the-art results with significantly fewer training samples compared to other models."}, 'zh': {'title': 'è½»é‡çº§æ¨ç†æ¨¡å‹ï¼Œè’¸é¦æ–°çªç ´ï¼', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§è½»é‡çº§çš„å¼€æºæ¨ç†æ¨¡å‹DASD-4B-Thinkingï¼Œè¯¥æ¨¡å‹é€šè¿‡æ”¹è¿›çš„åºåˆ—çº§è’¸é¦æ–¹æ³•ï¼Œå…‹æœäº†å½“å‰æ•™å¸ˆ-å­¦ç”ŸçŸ¥è¯†è½¬ç§»æ–¹æ³•çš„å±€é™æ€§ã€‚è¯¥æ¨¡å‹åœ¨æ•°å­¦ã€ç§‘å­¦æ¨ç†å’Œä»£ç ç”Ÿæˆç­‰å…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¡¨ç°å‡ºè‰²ï¼Œç”šè‡³è¶…è¶Šäº†å¤šä¸ªæ›´å¤§è§„æ¨¡çš„æ¨¡å‹ã€‚æˆ‘ä»¬æŒ‡å‡ºäº†ç°æœ‰è’¸é¦å®è·µä¸­çš„ä¸‰ä¸ªå…³é”®é™åˆ¶ï¼Œå¹¶æå‡ºäº†ä¸€ç³»åˆ—æ–¹æ³•åˆ›æ–°ï¼Œä»¥å¢å¼ºåºåˆ—çº§è’¸é¦è®­ç»ƒæµç¨‹ã€‚æœ€ç»ˆï¼ŒDASD-4B-Thinkingä»…ä½¿ç”¨448Kè®­ç»ƒæ ·æœ¬å°±è·å¾—äº†ç«äº‰åŠ›çš„ç»“æœï¼Œè¿œå°‘äºå¤§å¤šæ•°ç°æœ‰å¼€æºæ¨¡å‹æ‰€éœ€çš„æ ·æœ¬é‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.09136', 'title': 'SkinFlow: Efficient Information Transmission for Open Dermatological Diagnosis via Dynamic Visual Encoding and Staged RL', 'url': 'https://huggingface.co/papers/2601.09136', 'abstract': 'SkinFlow introduces a novel framework for dermatological vision-language modeling that improves diagnostic accuracy through optimized visual information transmission efficiency rather than parameter scaling alone.  \t\t\t\t\tAI-generated summary \t\t\t\t General-purpose Large Vision-Language Models (LVLMs), despite their massive scale, often falter in dermatology due to "diffuse attention" - the inability to disentangle subtle pathological lesions from background noise. In this paper, we challenge the assumption that parameter scaling is the only path to medical precision. We introduce SkinFlow, a framework that treats diagnosis as an optimization of visual information transmission efficiency. Our approach utilizes a Virtual-Width Dynamic Vision Encoder (DVE) to "unfold" complex pathological manifolds without physical parameter expansion, coupled with a two-stage Reinforcement Learning strategy. This strategy sequentially aligns explicit medical descriptions (Stage I) and reconstructs implicit diagnostic textures (Stage II) within a constrained semantic space. Furthermore, we propose a clinically grounded evaluation protocol that prioritizes diagnostic safety and hierarchical relevance over rigid label matching. Empirical results are compelling: our 7B model establishes a new state-of-the-art on the Fitzpatrick17k benchmark, achieving a +12.06% gain in Top-1 accuracy and a +28.57% boost in Top-6 accuracy over the massive general-purpose models (e.g., Qwen3VL-235B and GPT-5.2). These findings demonstrate that optimizing geometric capacity and information flow yields superior diagnostic reasoning compared to raw parameter scaling.', 'score': 36, 'issue_id': 588, 'pub_date': '2026-01-14', 'pub_date_card': {'ru': '14 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 14', 'zh': '1æœˆ14æ—¥'}, 'hash': '3089ecac62962287', 'authors': ['Lijun Liu', 'Linwei Chen', 'Zhishou Zhang', 'Meng Tian', 'Hengfu Cui', 'Ruiyang Li', 'Zhaocheng Liu', 'Qiang Ju', 'Qianxi Li', 'Hong-Yu Zhou'], 'affiliations': ['Baichuan Inc.', 'Beijing Key Laboratory of Molecular Diagnosis on Dermatoses', 'Department of Dermatology, Peking University First Hospital', 'NMPA Key Laboratory for Quality Control and Evaluation of Cosmetics', 'National Clinical Research Center for Skin and Sexually Transmitted Diseases', 'School of Biomedical Engineering, Tsinghua University', 'University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2601.09136.jpg', 'data': {'categories': ['#optimization', '#science', '#small_models', '#cv', '#healthcare', '#rl', '#architecture', '#benchmark', '#multimodal'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ°Ğ¶Ğ½ĞµĞµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ° Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ² Ğ´ĞµÑ€Ğ¼Ğ°Ñ‚Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞµ', 'desc': 'SkinFlow Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ´ĞµÑ€Ğ¼Ğ°Ñ‚Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ·Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸ Ğ·Ğ° ÑÑ‡ĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸, Ğ° Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Â«Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ÑÂ» Ğ² Ğ¾Ğ±Ñ‰Ğ¸Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€ÑƒĞ´Ğ½ÑÑÑ‚ÑÑ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¸ Ğ¿Ğ°Ñ‚Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ğ¾Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ ĞºĞ¾Ğ¶Ğ¸ Ğ¾Ñ‚ Ñ„Ğ¾Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ÑˆÑƒĞ¼Ğ°. Ğ ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ½Ğ¸Ñ, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ğ½ÑƒÑ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ‚Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ±ĞµĞ· Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ Ğ¸ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚ĞµĞºÑÑ‚ÑƒÑ€. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ 7B Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ° Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ Fitzpatrick17k Ñ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾Ğ¼ +12.06% Ğ² Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Top-1 Ğ¸ +28.57% Ğ² Top-6 Ğ½Ğ°Ğ´ Ğ³Ğ¾Ñ€Ğ°Ğ·Ğ´Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ±Ñ‰Ğ¸Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸.'}, 'en': {'title': 'Optimizing Information Flow for Superior Dermatological Diagnosis', 'desc': "SkinFlow is a new framework designed to enhance dermatological diagnosis by improving how visual information is transmitted rather than just increasing the number of model parameters. It addresses the issue of 'diffuse attention' in large vision-language models, which struggle to differentiate subtle skin lesions from background noise. The framework employs a Virtual-Width Dynamic Vision Encoder to effectively manage complex pathological data without needing more parameters, and it uses a two-stage Reinforcement Learning approach to align medical descriptions with diagnostic features. The results show that SkinFlow significantly outperforms existing models in accuracy, proving that optimizing information flow is more effective than simply scaling up model size."}, 'zh': {'title': 'ä¼˜åŒ–ä¿¡æ¯æµï¼Œæå‡çš®è‚¤ç—…è¯Šæ–­å‡†ç¡®æ€§', 'desc': 'SkinFlow æ˜¯ä¸€ä¸ªæ–°é¢–çš„çš®è‚¤ç—…è§†è§‰-è¯­è¨€å»ºæ¨¡æ¡†æ¶ï¼Œé€šè¿‡ä¼˜åŒ–è§†è§‰ä¿¡æ¯ä¼ è¾“æ•ˆç‡æ¥æé«˜è¯Šæ–­å‡†ç¡®æ€§ï¼Œè€Œä¸ä»…ä»…ä¾èµ–äºå‚æ•°æ‰©å±•ã€‚è¯¥æ¡†æ¶ä½¿ç”¨è™šæ‹Ÿå®½åº¦åŠ¨æ€è§†è§‰ç¼–ç å™¨ï¼ˆDVEï¼‰æ¥å±•å¼€å¤æ‚çš„ç—…ç†æµå½¢ï¼Œå¹¶ç»“åˆä¸¤é˜¶æ®µå¼ºåŒ–å­¦ä¹ ç­–ç•¥ï¼Œé€æ­¥å¯¹é½åŒ»å­¦æè¿°å’Œé‡å»ºéšå«çš„è¯Šæ–­ç‰¹å¾ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§ä¸´åºŠåŸºç¡€çš„è¯„ä¼°åè®®ï¼Œå¼ºè°ƒè¯Šæ–­å®‰å…¨æ€§å’Œå±‚æ¬¡ç›¸å…³æ€§ï¼Œè€Œä¸æ˜¯ä¸¥æ ¼çš„æ ‡ç­¾åŒ¹é…ã€‚å®éªŒè¯æ˜ï¼Œæˆ‘ä»¬çš„7Bæ¨¡å‹åœ¨Fitzpatrick17kåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ–°çš„æœ€ä½³æˆç»©ï¼Œæ˜¾ç¤ºå‡ºä¼˜åŒ–å‡ ä½•å®¹é‡å’Œä¿¡æ¯æµæ¯”å•çº¯çš„å‚æ•°æ‰©å±•æ›´èƒ½æå‡è¯Šæ–­æ¨ç†èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.09708', 'title': 'Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning', 'url': 'https://huggingface.co/papers/2601.09708', 'abstract': 'Fast-ThinkAct is an efficient vision-language-action framework that reduces inference latency by 89.3% through compact latent reasoning while maintaining long-horizon planning and few-shot adaptation capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language-Action (VLA) tasks require reasoning over complex visual scenes and executing adaptive actions in dynamic environments. While recent studies on reasoning VLAs show that explicit chain-of-thought (CoT) can improve generalization, they suffer from high inference latency due to lengthy reasoning traces. We propose Fast-ThinkAct, an efficient reasoning framework that achieves compact yet performant planning through verbalizable latent reasoning. Fast-ThinkAct learns to reason efficiently with latent CoTs by distilling from a teacher, driven by a preference-guided objective to align manipulation trajectories that transfers both linguistic and visual planning capabilities for embodied control. This enables reasoning-enhanced policy learning that effectively connects compact reasoning to action execution. Extensive experiments across diverse embodied manipulation and reasoning benchmarks demonstrate that Fast-ThinkAct achieves strong performance with up to 89.3\\% reduced inference latency over state-of-the-art reasoning VLAs, while maintaining effective long-horizon planning, few-shot adaptation, and failure recovery.', 'score': 35, 'issue_id': 588, 'pub_date': '2026-01-14', 'pub_date_card': {'ru': '14 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 14', 'zh': '1æœˆ14æ—¥'}, 'hash': '6b6ba22e4fea5524', 'authors': ['Chi-Pin Huang', 'Yunze Man', 'Zhiding Yu', 'Min-Hung Chen', 'Jan Kautz', 'Yu-Chiang Frank Wang', 'Fu-En Yang'], 'affiliations': ['NVIDIA'], 'pdf_title_img': 'assets/pdf/title_img/2601.09708.jpg', 'data': {'categories': ['#optimization', '#training', '#cv', '#transfer_learning', '#inference', '#robotics', '#reasoning', '#multimodal'], 'emoji': 'âš¡', 'ru': {'title': 'Ğ‘Ñ‹ÑÑ‚Ñ€Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¹: Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ² Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞµ', 'desc': 'Fast-ThinkAct â€” ÑÑ‚Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ·Ñ€ĞµĞ½Ğ¸Ğµ-ÑĞ·Ñ‹Ğº-Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºÑƒ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ğ½Ğ° 89.3% Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾Ğ¼Ñƒ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¾Ñ‚ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğµ-ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾Ğ¹ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ñ„ÑƒĞ½ĞºÑ†Ğ¸ĞµĞ¹ Ğ´Ğ»Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ğµ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ‚ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ¼ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ğµ, Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ¼Ğ°Ğ»Ğ¾Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¸ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ÑĞ»Ğµ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Fast-ThinkAct Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¼Ğ¾Ñ‰Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ·Ğ° ÑÑ‡Ñ‘Ñ‚ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ğ¼Ñ‹ÑĞ»ĞµĞ¹.'}, 'en': {'title': 'Fast-ThinkAct: Speedy Reasoning for Smart Actions', 'desc': 'Fast-ThinkAct is a new framework designed for vision-language-action (VLA) tasks that significantly cuts down the time it takes to make decisions by 89.3%. It does this by using a compact form of reasoning that still allows for effective long-term planning and quick adaptation to new situations. The framework learns to reason efficiently by mimicking a more complex teacher model, which helps it connect visual understanding with action execution. Through various tests, Fast-ThinkAct has shown to perform well while being much faster than previous models, making it suitable for dynamic environments.'}, 'zh': {'title': 'é«˜æ•ˆæ¨ç†ï¼Œå¿«é€Ÿè¡ŒåŠ¨ï¼', 'desc': 'Fast-ThinkAct æ˜¯ä¸€ä¸ªé«˜æ•ˆçš„è§†è§‰-è¯­è¨€-è¡ŒåŠ¨æ¡†æ¶ï¼Œé€šè¿‡ç´§å‡‘çš„æ½œåœ¨æ¨ç†å°†æ¨ç†å»¶è¿Ÿå‡å°‘äº† 89.3%ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿåœ¨å¤æ‚çš„è§†è§‰åœºæ™¯ä¸­è¿›è¡Œæ¨ç†ï¼Œå¹¶åœ¨åŠ¨æ€ç¯å¢ƒä¸­æ‰§è¡Œè‡ªé€‚åº”åŠ¨ä½œã€‚Fast-ThinkAct é€šè¿‡ä»æ•™å¸ˆæ¨¡å‹ä¸­æç‚¼å‡ºæ½œåœ¨çš„æ€ç»´é“¾ï¼Œå­¦ä¹ é«˜æ•ˆæ¨ç†ï¼Œå¹¶é€šè¿‡åå¥½å¼•å¯¼ç›®æ ‡å¯¹é½æ“ä½œè½¨è¿¹ï¼Œä»è€Œå®ç°è¯­è¨€å’Œè§†è§‰è§„åˆ’èƒ½åŠ›çš„è½¬ç§»ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒFast-ThinkAct åœ¨å¤šç§æ“ä½œå’Œæ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼ŒåŒæ—¶ä¿æŒæœ‰æ•ˆçš„é•¿è¿œè§„åˆ’ã€å°‘é‡æ ·æœ¬é€‚åº”å’Œæ•…éšœæ¢å¤èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.09575', 'title': 'OpenVoxel: Training-Free Grouping and Captioning Voxels for Open-Vocabulary 3D Scene Understanding', 'url': 'https://huggingface.co/papers/2601.09575', 'abstract': 'OpenVoxel enables open-vocabulary 3D scene understanding through training-free grouping and captioning of sparse voxels using Vision Language Models and Multi-modal Large Language Models.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose OpenVoxel, a training-free algorithm for grouping and captioning sparse voxels for the open-vocabulary 3D scene understanding tasks. Given the sparse voxel rasterization (SVR) model obtained from multi-view images of a 3D scene, our OpenVoxel is able to produce meaningful groups that describe different objects in the scene. Also, by leveraging powerful Vision Language Models (VLMs) and Multi-modal Large Language Models (MLLMs), our OpenVoxel successfully build an informative scene map by captioning each group, enabling further 3D scene understanding tasks such as open-vocabulary segmentation (OVS) or referring expression segmentation (RES). Unlike previous methods, our method is training-free and does not introduce embeddings from a CLIP/BERT text encoder. Instead, we directly proceed with text-to-text search using MLLMs. Through extensive experiments, our method demonstrates superior performance compared to recent studies, particularly in complex referring expression segmentation (RES) tasks. The code will be open.', 'score': 22, 'issue_id': 589, 'pub_date': '2026-01-14', 'pub_date_card': {'ru': '14 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 14', 'zh': '1æœˆ14æ—¥'}, 'hash': '93aef15e66061014', 'authors': ['Sheng-Yu Huang', 'Jaesung Choe', 'Yu-Chiang Frank Wang', 'Cheng Sun'], 'affiliations': ['NVIDIA', 'National Taiwan University'], 'pdf_title_img': 'assets/pdf/title_img/2601.09575.jpg', 'data': {'categories': ['#cv', '#3d', '#multimodal', '#open_source'], 'emoji': 'ğŸ¯', 'ru': {'title': 'ĞŸĞ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ 3D ÑÑ†ĞµĞ½ Ñ‡ĞµÑ€ĞµĞ· Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'OpenVoxel â€” ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ 3D ÑÑ†ĞµĞ½ Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ğ¾ĞºÑĞµĞ»Ğ¸ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ´Ğ»Ñ Ğ½Ğ¸Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ. ĞĞ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Vision Language Models Ğ¸ Multi-modal Large Language Models Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ²Ğ¾ĞºÑĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ñ€Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ· Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ²Ğ¸Ğ´Ğ¾Ğ²Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ ĞºĞ°Ñ€Ñ‚Ñƒ ÑÑ†ĞµĞ½Ñ‹, Ğ¿Ñ€Ğ¸ÑĞ²Ğ°Ğ¸Ğ²Ğ°Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğµ Ğ²Ğ¾ĞºÑĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ğ½Ğ¾Ğ³Ğ¾ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚ĞµĞºÑÑ‚-Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ñ‡ĞµÑ€ĞµĞ· MLLMs, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼.'}, 'en': {'title': 'OpenVoxel: Training-Free 3D Scene Understanding with VLMs and MLLMs', 'desc': 'OpenVoxel is a novel algorithm designed for understanding 3D scenes without the need for prior training. It groups and captions sparse voxels derived from multi-view images, allowing for effective identification of different objects in a scene. By utilizing Vision Language Models and Multi-modal Large Language Models, OpenVoxel creates detailed scene maps that facilitate tasks like open-vocabulary segmentation and referring expression segmentation. This approach stands out as it eliminates the need for traditional embeddings, directly employing text-to-text search for enhanced performance in complex scenarios.'}, 'zh': {'title': 'OpenVoxelï¼šæ— éœ€è®­ç»ƒçš„å¼€æ”¾è¯æ±‡3Dåœºæ™¯ç†è§£', 'desc': 'OpenVoxelæ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„ç®—æ³•ï¼Œç”¨äºå¯¹ç¨€ç–ä½“ç´ è¿›è¡Œåˆ†ç»„å’Œæ ‡æ³¨ï¼Œä»¥å®ç°å¼€æ”¾è¯æ±‡çš„3Dåœºæ™¯ç†è§£ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å¤šè§†è§’å›¾åƒç”Ÿæˆçš„ç¨€ç–ä½“ç´ å…‰æ …åŒ–æ¨¡å‹ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°è¯†åˆ«å’Œæè¿°åœºæ™¯ä¸­çš„ä¸åŒç‰©ä½“ã€‚é€šè¿‡ç»“åˆå¼ºå¤§çš„è§†è§‰è¯­è¨€æ¨¡å‹å’Œå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ŒOpenVoxelèƒ½å¤Ÿåˆ›å»ºä¿¡æ¯ä¸°å¯Œçš„åœºæ™¯åœ°å›¾ï¼Œæ”¯æŒè¿›ä¸€æ­¥çš„3Dåœºæ™¯ç†è§£ä»»åŠ¡ã€‚ä¸ä»¥å¾€æ–¹æ³•ä¸åŒï¼ŒOpenVoxelä¸ä¾èµ–äºCLIP/BERTæ–‡æœ¬ç¼–ç å™¨çš„åµŒå…¥ï¼Œè€Œæ˜¯ç›´æ¥ä½¿ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œæ–‡æœ¬åˆ°æ–‡æœ¬çš„æœç´¢ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.09028', 'title': 'OpenDecoder: Open Large Language Model Decoding to Incorporate Document Quality in RAG', 'url': 'https://huggingface.co/papers/2601.09028', 'abstract': "OpenDecoder enhances retrieval-augmented generation by explicitly evaluating retrieved information quality through relevance, ranking, and query performance prediction scores, improving robustness to noisy context.  \t\t\t\t\tAI-generated summary \t\t\t\t The development of large language models (LLMs) has achieved superior performance in a range of downstream tasks, including LLM-based retrieval-augmented generation (RAG). The quality of generated content heavily relies on the usefulness of the retrieved information and the capacity of LLMs' internal information processing mechanism to incorporate it in answer generation. It is generally assumed that the retrieved information is relevant to the question. However, the retrieved information may have a variable degree of relevance and usefulness, depending on the question and the document collection. It is important to take into account the relevance of the retrieved information in answer generation. In this paper, we propose OpenDecoder, a new approach that leverages explicit evaluation of the retrieved information as quality indicator features for generation. We aim to build a RAG model that is more robust to varying levels of noisy context. Three types of explicit evaluation information are considered: relevance score, ranking score, and QPP (query performance prediction) score. The experimental results on five benchmark datasets demonstrate the effectiveness and better robustness of OpenDecoder by outperforming various baseline methods. Importantly, this paradigm is flexible to be integrated with the post-training of LLMs for any purposes and incorporated with any type of external indicators.", 'score': 17, 'issue_id': 606, 'pub_date': '2026-01-13', 'pub_date_card': {'ru': '13 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 13', 'zh': '1æœˆ13æ—¥'}, 'hash': 'f6bb71360bf29e7d', 'authors': ['Fengran Mo', 'Zhan Su', 'Yuchen Hui', 'Jinghan Zhang', 'Jia Ao Sun', 'Zheyuan Liu', 'Chao Zhang', 'Tetsuya Sakai', 'Jian-Yun Nie'], 'affiliations': ['Clemson University', 'Georgia Institute of Technology', 'University of Notre Dame', 'UniversitÃ© de MontrÃ©al', 'Waseda University'], 'pdf_title_img': 'assets/pdf/title_img/2601.09028.jpg', 'data': {'categories': ['#rag', '#benchmark', '#optimization'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Ğ¯Ğ²Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ´Ğ»Ñ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ²', 'desc': 'OpenDecoder â€” ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº retrieval-augmented generation, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ²Ğ½Ğ¾ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡Ñ‘Ğ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº: Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ, Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ğº Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾Ğ¹ Ğº ÑˆÑƒĞ¼Ğ½Ñ‹Ğ¼ Ğ¸ Ğ¼Ğ°Ğ»Ğ¾Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ğ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ñ‚Ñ€Ğ¸ Ñ‚Ğ¸Ğ¿Ğ° ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ»ÑƒÑ‡ÑˆĞµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ°Ñ‚ÑŒ, ĞºĞ°ĞºÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğµ Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ¿ÑÑ‚Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ OpenDecoder Ğ½Ğ°Ğ´ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ ĞµĞ³Ğ¾ Ğ³Ğ¸Ğ±ĞºĞ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑĞ¼Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°.'}, 'en': {'title': 'Enhancing RAG with Quality Evaluation for Robust Generation', 'desc': 'OpenDecoder is a novel approach that improves retrieval-augmented generation (RAG) by assessing the quality of retrieved information through specific scores. It evaluates relevance, ranking, and query performance prediction (QPP) to enhance the robustness of language models against noisy data. By explicitly considering the usefulness of retrieved content, OpenDecoder ensures that the generated answers are more accurate and reliable. Experimental results show that this method outperforms existing baseline models across multiple datasets, demonstrating its effectiveness and adaptability for various applications.'}, 'zh': {'title': 'æå‡ç”Ÿæˆè´¨é‡çš„OpenDecoderæ–¹æ³•', 'desc': 'OpenDecoderæ˜¯ä¸€ç§å¢å¼ºæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„æ–¹æ³•ï¼Œé€šè¿‡æ˜¾å¼è¯„ä¼°æ£€ç´¢ä¿¡æ¯çš„è´¨é‡æ¥æé«˜ç”Ÿæˆçš„é²æ£’æ€§ã€‚å®ƒè€ƒè™‘äº†ä¸‰ç§æ˜¾å¼è¯„ä¼°ä¿¡æ¯ï¼šç›¸å…³æ€§è¯„åˆ†ã€æ’åè¯„åˆ†å’ŒæŸ¥è¯¢æ€§èƒ½é¢„æµ‹ï¼ˆQPPï¼‰è¯„åˆ†ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†å™ªå£°ä¸Šä¸‹æ–‡ï¼Œæé«˜ç”Ÿæˆå†…å®¹çš„è´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOpenDecoderåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜äºå¤šç§åŸºçº¿æ–¹æ³•ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§å’Œçµæ´»æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.08605', 'title': 'ExpSeek: Self-Triggered Experience Seeking for Web Agents', 'url': 'https://huggingface.co/papers/2601.08605', 'abstract': "ExpSeek enables web agents to proactively seek experience during interaction by using entropy-based timing and tailored content, achieving significant performance improvements across multiple benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Experience intervention in web agents emerges as a promising technical paradigm, enhancing agent interaction capabilities by providing valuable insights from accumulated experiences. However, existing methods predominantly inject experience passively as global context before task execution, struggling to adapt to dynamically changing contextual observations during agent-environment interaction. We propose ExpSeek, which shifts experience toward step-level proactive seeking: (1) estimating step-level entropy thresholds to determine intervention timing using the model's intrinsic signals; (2) designing step-level tailor-designed experience content. Experiments on Qwen3-8B and 32B models across four challenging web agent benchmarks demonstrate that ExpSeek achieves absolute improvements of 9.3% and 7.5%, respectively. Our experiments validate the feasibility and advantages of entropy as a self-triggering signal, reveal that even a 4B small-scale experience model can significantly boost the performance of larger agent models.", 'score': 15, 'issue_id': 590, 'pub_date': '2026-01-13', 'pub_date_card': {'ru': '13 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 13', 'zh': '1æœˆ13æ—¥'}, 'hash': 'b6eefd7ac17e02d8', 'authors': ['Wenyuan Zhang', 'Xinghua Zhang', 'Haiyang Yu', 'Shuaiyi Nie', 'Bingli Wu', 'Juwei Yue', 'Tingwen Liu', 'Yongbin Li'], 'affiliations': ['Institute of Information Engineering, Chinese Academy of Sciences', 'School of Cyber Security, University of Chinese Academy of Sciences', 'Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2601.08605.jpg', 'data': {'categories': ['#training', '#agents', '#benchmark', '#optimization'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¾Ğ¿Ñ‹Ñ‚Ğ° Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ¼ Ğ² Ğ½ÑƒĞ¶Ğ½Ñ‹Ğ¹ Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ ExpSeek, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ğ²ĞµĞ±-Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼ Ğ¿Ñ€Ğ¾Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ·Ğ°Ğ¿Ñ€Ğ°ÑˆĞ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ¿Ñ‹Ñ‚ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸ĞµĞ¼. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ğ°ÑÑĞ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ´Ğ°Ñ‡Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¿ĞµÑ€ĞµĞ´ Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ¾Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ñ Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ ÑˆĞ°Ğ³Ğµ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚Ğ° Ğ²Ğ¼ĞµÑˆĞ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ° Ğ¸ Ğ¿Ğ¾Ğ´Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğ¹ Ğ¾Ğ¿Ñ‹Ñ‚. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ»ÑƒĞ¶Ğ¸Ñ‚ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¸Ğ¼ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ¼ ÑĞ°Ğ¼Ğ¾Ğ·Ğ°Ğ¿ÑƒÑĞºĞ° Ğ´Ğ»Ñ Ğ²Ğ¼ĞµÑˆĞ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ°, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 9.3% Ğ¸ 7.5% Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ¿Ñ‹Ñ‚Ğ° Ñ 4 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑĞ¸Ñ‚ÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Proactive Experience Seeking for Enhanced Web Agent Performance', 'desc': 'ExpSeek is a novel approach that enhances web agents by allowing them to actively seek relevant experiences during their interactions. It uses an entropy-based method to determine the best moments for these interventions, ensuring that the agents receive timely and contextually appropriate information. This proactive strategy contrasts with traditional methods that only provide experience as a background context before tasks begin. Experiments show that ExpSeek significantly improves performance on various benchmarks, demonstrating the effectiveness of using entropy as a guiding signal for experience integration.'}, 'zh': {'title': 'ExpSeekï¼šä¸»åŠ¨å¯»æ±‚ç»éªŒï¼Œæå‡ä»£ç†æ€§èƒ½', 'desc': 'ExpSeek æ˜¯ä¸€ç§æ–°å‹çš„ç½‘ç»œä»£ç†æŠ€æœ¯ï¼Œèƒ½å¤Ÿé€šè¿‡åŸºäºç†µçš„æ—¶æœºé€‰æ‹©å’Œå®šåˆ¶å†…å®¹ï¼Œä¸»åŠ¨å¯»æ±‚ç»éªŒï¼Œä»è€Œæå‡äº¤äº’æ€§èƒ½ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒExpSeek åœ¨ä»£ç†ä¸ç¯å¢ƒäº¤äº’çš„è¿‡ç¨‹ä¸­ï¼Œå®æ—¶è°ƒæ•´ç»éªŒçš„æ³¨å…¥æ—¶æœºå’Œå†…å®¹ï¼Œä»¥é€‚åº”åŠ¨æ€å˜åŒ–çš„ä¸Šä¸‹æ–‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒExpSeek åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†æ€§èƒ½ï¼Œåˆ†åˆ«è¾¾åˆ°äº† 9.3% å’Œ 7.5% çš„ç»å¯¹æå‡ã€‚æˆ‘ä»¬çš„ç ”ç©¶éªŒè¯äº†ç†µä½œä¸ºè‡ªè§¦å‘ä¿¡å·çš„å¯è¡Œæ€§ï¼Œç”šè‡³å°è§„æ¨¡çš„ç»éªŒæ¨¡å‹ä¹Ÿèƒ½æ˜¾è‘—æå‡å¤§å‹ä»£ç†æ¨¡å‹çš„è¡¨ç°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.03928', 'title': 'FocusUI: Efficient UI Grounding via Position-Preserving Visual Token Selection', 'url': 'https://huggingface.co/papers/2601.03928', 'abstract': "FocusUI is an efficient UI grounding framework that reduces computational overhead by selecting relevant visual tokens while preserving positional continuity through a novel PosPad strategy.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language Models (VLMs) have shown remarkable performance in User Interface (UI) grounding tasks, driven by their ability to process increasingly high-resolution screenshots. However, screenshots are tokenized into thousands of visual tokens (e.g., about 4700 for 2K resolution), incurring significant computational overhead and diluting attention. In contrast, humans typically focus on regions of interest when interacting with UI. In this work, we pioneer the task of efficient UI grounding. Guided by practical analysis of the task's characteristics and challenges, we propose FocusUI, an efficient UI grounding framework that selects patches most relevant to the instruction while preserving positional continuity for precise grounding. FocusUI addresses two key challenges: (1) Eliminating redundant tokens in visual encoding. We construct patch-level supervision by fusing an instruction-conditioned score with a rule-based UI-graph score that down-weights large homogeneous regions to select distinct and instruction-relevant visual tokens. (2) Preserving positional continuity during visual token selection. We find that general visual token pruning methods suffer from severe accuracy degradation on UI grounding tasks due to broken positional information. We introduce a novel PosPad strategy, which compresses each contiguous sequence of dropped visual tokens into a single special marker placed at the sequence's last index to preserve positional continuity. Comprehensive experiments on four grounding benchmarks demonstrate that FocusUI surpasses GUI-specific baselines. On the ScreenSpot-Pro benchmark, FocusUI-7B achieves a performance improvement of 3.7% over GUI-Actor-7B. Even with only 30% visual token retention, FocusUI-7B drops by only 3.2% while achieving up to 1.44x faster inference and 17% lower peak GPU memory.", 'score': 12, 'issue_id': 589, 'pub_date': '2026-01-07', 'pub_date_card': {'ru': '7 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 7', 'zh': '1æœˆ7æ—¥'}, 'hash': 'df0932da1d1be758', 'authors': ['Mingyu Ouyang', 'Kevin Qinghong Lin', 'Mike Zheng Shou', 'Hwee Tou Ng'], 'affiliations': ['National University of Singapore', 'University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2601.03928.jpg', 'data': {'categories': ['#cv', '#inference', '#multimodal'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ğ°Ñ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ°: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· ÑĞµĞ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¾Ñ‚Ğ±Ğ¾Ñ€ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²', 'desc': 'FocusUI â€” ÑÑ‚Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğ¼ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ° Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€ĞµÑˆĞ°ÑÑ‚ Ğ´Ğ²Ğµ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸: Ğ¸ÑĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ¾Ğµ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰ĞµĞµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñ‹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ, Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ PosPad. Ğ¡Ñ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ PosPad ÑĞ¶Ğ¸Ğ¼Ğ°ĞµÑ‚ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑ‰ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¼Ğ°Ñ€ĞºĞµÑ€, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¸ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° ÑĞºÑ€Ğ°Ğ½Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ 30% Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ² 1.44 Ñ€Ğ°Ğ·Ğ° Ğ¸ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ GPU Ğ½Ğ° 17%.'}, 'en': {'title': 'Efficient UI Grounding with FocusUI: Less is More!', 'desc': 'FocusUI is a novel framework designed to enhance the efficiency of User Interface (UI) grounding by intelligently selecting relevant visual tokens. It addresses the challenge of high computational costs associated with processing thousands of visual tokens from high-resolution screenshots. The framework employs a unique PosPad strategy to maintain positional continuity, ensuring that important spatial information is preserved even when redundant tokens are eliminated. Experimental results show that FocusUI significantly improves performance on UI grounding tasks while reducing inference time and memory usage compared to existing methods.'}, 'zh': {'title': 'é«˜æ•ˆçš„ç”¨æˆ·ç•Œé¢å®šä½æ–°æ–¹æ³•', 'desc': 'FocusUI æ˜¯ä¸€ä¸ªé«˜æ•ˆçš„ç”¨æˆ·ç•Œé¢ï¼ˆUIï¼‰å®šä½æ¡†æ¶ï¼Œé€šè¿‡é€‰æ‹©ç›¸å…³çš„è§†è§‰æ ‡è®°æ¥å‡å°‘è®¡ç®—å¼€é”€ï¼ŒåŒæ—¶é€šè¿‡ä¸€ç§æ–°é¢–çš„ PosPad ç­–ç•¥ä¿æŒä½ç½®è¿ç»­æ€§ã€‚è¯¥æ¡†æ¶è§£å†³äº†è§†è§‰ç¼–ç ä¸­å†—ä½™æ ‡è®°çš„é—®é¢˜ï¼Œå¹¶é€šè¿‡èåˆæŒ‡ä»¤æ¡ä»¶è¯„åˆ†å’ŒåŸºäºè§„åˆ™çš„ UI å›¾è¯„åˆ†æ¥é€‰æ‹©ä¸æŒ‡ä»¤ç›¸å…³çš„è§†è§‰æ ‡è®°ã€‚FocusUI è¿˜å¼•å…¥äº† PosPad ç­–ç•¥ï¼Œä»¥å‹ç¼©è¢«ä¸¢å¼ƒçš„è§†è§‰æ ‡è®°åºåˆ—ï¼Œä»è€Œä¿æŒä½ç½®çš„è¿ç»­æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFocusUI åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†ä¼ ç»Ÿçš„ GUI ç‰¹å®šåŸºçº¿ï¼Œæ˜¾è‘—æé«˜äº†æ€§èƒ½å’Œæ¨ç†é€Ÿåº¦ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.09465', 'title': 'EvoFSM: Controllable Self-Evolution for Deep Research with Finite State Machines', 'url': 'https://huggingface.co/papers/2601.09465', 'abstract': 'EvoFSM is a structured self-evolving framework for LLM agents that uses finite state machines to improve adaptability while maintaining control through constrained optimization and memory mechanisms.  \t\t\t\t\tAI-generated summary \t\t\t\t While LLM-based agents have shown promise for deep research, most existing approaches rely on fixed workflows that struggle to adapt to real-world, open-ended queries. Recent work therefore explores self-evolution by allowing agents to rewrite their own code or prompts to improve problem-solving ability, but unconstrained optimization often triggers instability, hallucinations, and instruction drift. We propose EvoFSM, a structured self-evolving framework that achieves both adaptability and control by evolving an explicit Finite State Machine (FSM) instead of relying on free-form rewriting. EvoFSM decouples the optimization space into macroscopic Flow (state-transition logic) and microscopic Skill (state-specific behaviors), enabling targeted improvements under clear behavioral boundaries. Guided by a critic mechanism, EvoFSM refines the FSM through a small set of constrained operations, and further incorporates a self-evolving memory that distills successful trajectories as reusable priors and failure patterns as constraints for future queries. Extensive evaluations on five multi-hop QA benchmarks demonstrate the effectiveness of EvoFSM. In particular, EvoFSM reaches 58.0% accuracy on the DeepSearch benchmark. Additional results on interactive decision-making tasks further validate its generalization.', 'score': 11, 'issue_id': 589, 'pub_date': '2026-01-14', 'pub_date_card': {'ru': '14 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 14', 'zh': '1æœˆ14æ—¥'}, 'hash': 'dc4cf0d084ab4ac4', 'authors': ['Shuo Zhang', 'Chaofa Yuan', 'Ryan Guo', 'Xiaomin Yu', 'Rui Xu', 'Zhangquan Chen', 'Zinuo Li', 'Zhi Yang', 'Shuhao Guan', 'Zhenheng Tang', 'Sen Hu', 'Liwen Zhang', 'Ronghao Chen', 'Huacan Wang'], 'affiliations': ['FDU', 'HKUST', 'HKUST(GZ)', 'PKU', 'QuantaAlpha', 'SUFE', 'THU', 'UCAS', 'UCD'], 'pdf_title_img': 'assets/pdf/title_img/2601.09465.jpg', 'data': {'categories': ['#benchmark', '#hallucinations', '#reasoning', '#optimization', '#agents'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ£Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ°Ñ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ĞºĞ¾Ğ½ĞµÑ‡Ğ½Ñ‹Ğµ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ñ‹', 'desc': 'EvoFSM Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ ÑĞ°Ğ¼Ğ¾ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ğ´Ğ»Ñ LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ¾Ğ½ĞµÑ‡Ğ½Ñ‹Ğµ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ½ĞµĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ¿Ğ¸ÑÑ‹Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ´Ğ° Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ², ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ²Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ² Ğ²Ğ¸Ğ´Ğµ FSM, Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑÑ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ¼Ğ°ĞºÑ€Ğ¾ÑĞºĞ¾Ğ¿Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ñ‹ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ Ğ¸ Ğ¼Ğ¸ĞºÑ€Ğ¾ÑĞºĞ¾Ğ¿Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ. ĞœĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ° Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ FSM Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹, Ğ° ÑĞ°Ğ¼Ğ¾ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒÑÑ‰Ğ°Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ½Ğ°ĞºĞ°Ğ¿Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ ÑƒÑĞ¿ĞµÑˆĞ½Ñ‹Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ¸ ĞµĞ³Ğ¾ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'EvoFSM: Structured Self-Evolution for Adaptive LLM Agents', 'desc': 'EvoFSM is a novel framework designed to enhance the adaptability of large language model (LLM) agents by utilizing finite state machines (FSMs). Unlike traditional methods that depend on fixed workflows, EvoFSM allows agents to evolve their behavior in a structured manner while maintaining control through constrained optimization. This approach separates the optimization process into two levels: macroscopic Flow for state transitions and microscopic Skill for specific actions, ensuring targeted improvements. The framework also includes a self-evolving memory that captures successful strategies and constraints from failures, leading to better performance in complex tasks, as evidenced by its high accuracy on various benchmarks.'}, 'zh': {'title': 'EvoFSMï¼šè‡ªæˆ‘è¿›åŒ–çš„æ™ºèƒ½æ¡†æ¶', 'desc': 'EvoFSMæ˜¯ä¸€ä¸ªç»“æ„åŒ–çš„è‡ªæˆ‘è¿›åŒ–æ¡†æ¶ï¼Œä¸“ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†è®¾è®¡ã€‚å®ƒåˆ©ç”¨æœ‰é™çŠ¶æ€æœºï¼ˆFSMï¼‰æ¥æé«˜é€‚åº”æ€§ï¼ŒåŒæ—¶é€šè¿‡çº¦æŸä¼˜åŒ–å’Œè®°å¿†æœºåˆ¶ä¿æŒæ§åˆ¶ã€‚ä¸ä¼ ç»Ÿçš„å›ºå®šå·¥ä½œæµç¨‹ä¸åŒï¼ŒEvoFSMå…è®¸ä»£ç†åœ¨æ˜ç¡®çš„è¡Œä¸ºè¾¹ç•Œå†…è¿›è¡Œæœ‰é’ˆå¯¹æ€§çš„æ”¹è¿›ï¼Œä»è€Œé¿å…ä¸ç¨³å®šæ€§å’ŒæŒ‡ä»¤æ¼‚ç§»ã€‚é€šè¿‡å¯¹FSMçš„ä¼˜åŒ–å’Œè‡ªæˆ‘è¿›åŒ–çš„è®°å¿†ï¼ŒEvoFSMåœ¨å¤šè·³é—®ç­”åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¾¾åˆ°äº†58.0%çš„å‡†ç¡®ç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.06596', 'title': 'Are LLMs Vulnerable to Preference-Undermining Attacks (PUA)? A Factorial Analysis Methodology for Diagnosing the Trade-off between Preference Alignment and Real-World Validity', 'url': 'https://huggingface.co/papers/2601.06596', 'abstract': "Research examines how large language models can be manipulated through preference-undermining attacks that exploit alignment objectives, revealing model vulnerabilities and proposing a factorial evaluation method for diagnosing alignment risks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Model (LLM) training often optimizes for preference alignment, rewarding outputs that are perceived as helpful and interaction-friendly. However, this preference-oriented objective can be exploited: manipulative prompts can steer responses toward user-appeasing agreement and away from truth-oriented correction. In this work, we investigate whether aligned models are vulnerable to Preference-Undermining Attacks (PUA), a class of manipulative prompting strategies designed to exploit the model's desire to please user preferences at the expense of truthfulness. We propose a diagnostic methodology that provides a finer-grained and more directive analysis than aggregate benchmark scores, using a factorial evaluation framework to decompose prompt-induced shifts into interpretable effects of system objectives (truth- vs. preference-oriented) and PUA-style dialogue factors (directive control, personal derogation, conditional approval, reality denial) within a controlled 2 times 2^4 design. Surprisingly, more advanced models are sometimes more susceptible to manipulative prompts. Beyond the dominant reality-denial factor, we observe model-specific sign reversals and interactions with PUA-style factors, suggesting tailored defenses rather than uniform robustness. These findings offer a novel, reproducible factorial evaluation methodology that provides finer-grained diagnostics for post-training processes like RLHF, enabling better trade-offs in the product iteration of LLMs by offering a more nuanced understanding of preference alignment risks and the impact of manipulative prompts.", 'score': 11, 'issue_id': 588, 'pub_date': '2026-01-10', 'pub_date_card': {'ru': '10 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 10', 'zh': '1æœˆ10æ—¥'}, 'hash': '270eb8d27917d8cc', 'authors': ['Hongjun An', 'Yiliang Song', 'Jiangan Chen', 'Jiawei Shao', 'Chi Zhang', 'Xuelong Li'], 'affiliations': ['Institute of Artificial Intelligence (TeleAI), China Telecom', 'School of Artificial Intelligence, OPtics and ElectroNics, Northwestern Polytechnical University', 'School of Economics and Management, Guangxi Normal University'], 'pdf_title_img': 'assets/pdf/title_img/2601.06596.jpg', 'data': {'categories': ['#alignment', '#security', '#interpretability', '#rlhf', '#benchmark'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Ğ’Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ: Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¸Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ° Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ°Ñ‚Ğ°Ğº Ğ½Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ñ‹ Ğ´Ğ»Ñ Ğ°Ñ‚Ğ°Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑĞºÑĞ¿Ğ»ÑƒĞ°Ñ‚Ğ¸Ñ€ÑƒÑÑ‚ Ğ¸Ñ… Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºÑƒ Ğ½Ğ° Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº Ğ½Ğ° Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹: Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Ğ¸ÑÑ‚Ğ¸Ğ½Ñƒ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ² Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° ÑƒĞ³Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ğµ, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ‚Ğ°ĞºÑ‚Ğ¸ĞºĞ¸ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸. Ğ‘Ñ‹Ğ»Ğ¾ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸Ğ½Ğ¾Ğ³Ğ´Ğ° Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸Ğ¸Ğ¼Ñ‡Ğ¸Ğ²Ñ‹ Ğº Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸ÑĞ¼, Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ñ‹ Ğ°Ñ‚Ğ°Ğº Ğ·Ğ°Ğ²Ğ¸ÑÑÑ‚ Ğ¾Ñ‚ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ°Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ¸ÑĞºĞ¸ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹ Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº RLHF.'}, 'en': {'title': 'Unmasking Vulnerabilities: Preference-Undermining Attacks on LLMs', 'desc': 'This research explores how large language models (LLMs) can be influenced by manipulative prompts that undermine their alignment with user preferences. It identifies a specific type of attack called Preference-Undermining Attacks (PUA), which can lead models to prioritize pleasing responses over truthful ones. The authors propose a new evaluation method that breaks down the effects of different prompting strategies, allowing for a clearer understanding of how these attacks affect model behavior. Their findings indicate that more advanced models may be more vulnerable to these manipulative tactics, highlighting the need for tailored defenses against such risks.'}, 'zh': {'title': 'æ­ç¤ºå¤§å‹è¯­è¨€æ¨¡å‹çš„è„†å¼±æ€§ä¸é˜²å¾¡ç­–ç•¥', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¦‚ä½•å—åˆ°åå¥½å‰Šå¼±æ”»å‡»çš„å½±å“ï¼Œè¿™ç§æ”»å‡»åˆ©ç”¨äº†æ¨¡å‹çš„å¯¹é½ç›®æ ‡ï¼Œæ­ç¤ºäº†æ¨¡å‹çš„è„†å¼±æ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå°½ç®¡æ¨¡å‹ä¼˜åŒ–äº†ç”¨æˆ·åå¥½çš„å¯¹é½ï¼Œä½†è¿™ç§ç›®æ ‡å¯èƒ½è¢«æ“æ§æ€§æç¤ºæ‰€åˆ©ç”¨ï¼Œå¯¼è‡´æ¨¡å‹åå‘è¿åˆç”¨æˆ·è€Œéè¿½æ±‚çœŸå®ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§å› å­è¯„ä¼°æ–¹æ³•ï¼Œèƒ½å¤Ÿæ›´ç»†è‡´åœ°åˆ†ææ¨¡å‹åœ¨é¢å¯¹æ“æ§æ€§æç¤ºæ—¶çš„è¡¨ç°ï¼Œè¯†åˆ«å‡ºç³»ç»Ÿç›®æ ‡å’Œå¯¹è¯å› ç´ çš„å½±å“ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼ŒæŸäº›é«˜çº§æ¨¡å‹åœ¨é¢å¯¹æ“æ§æ€§æç¤ºæ—¶åè€Œæ›´å®¹æ˜“å—åˆ°å½±å“ï¼Œæç¤ºéœ€è¦é’ˆå¯¹æ€§é˜²å¾¡è€Œéç»Ÿä¸€çš„é²æ£’æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.09012', 'title': 'TranslateGemma Technical Report', 'url': 'https://huggingface.co/papers/2601.09012', 'abstract': "TranslateGemma enhances Gemma 3's multilingual capabilities through two-stage fine-tuning with synthetic and human-translated data, achieving superior translation quality with improved efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t We present TranslateGemma, a suite of open machine translation models based on the Gemma 3 foundation models. To enhance the inherent multilingual capabilities of Gemma 3 for the translation task, we employ a two-stage fine-tuning process. First, supervised fine-tuning is performed using a rich mixture of high-quality large-scale synthetic parallel data generated via state-of-the-art models and human-translated parallel data. This is followed by a reinforcement learning phase, where we optimize translation quality using an ensemble of reward models, including MetricX-QE and AutoMQM, targeting translation quality. We demonstrate the effectiveness of TranslateGemma with human evaluation on the WMT25 test set across 10 language pairs and with automatic evaluation on the WMT24++ benchmark across 55 language pairs. Automatic metrics show consistent and substantial gains over the baseline Gemma 3 models across all sizes. Notably, smaller TranslateGemma models often achieve performance comparable to larger baseline models, offering improved efficiency. We also show that TranslateGemma models retain strong multimodal capabilities, with enhanced performance on the Vistra image translation benchmark. The release of the open TranslateGemma models aims to provide the research community with powerful and adaptable tools for machine translation.", 'score': 9, 'issue_id': 589, 'pub_date': '2026-01-13', 'pub_date_card': {'ru': '13 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 13', 'zh': '1æœˆ13æ—¥'}, 'hash': '96d071879d583154', 'authors': ['Mara Finkelstein', 'Isaac Caswell', 'Tobias Domhan', 'Jan-Thorsten Peter', 'Juraj Juraska', 'Parker Riley', 'Daniel Deutsch', 'Cole Dilanni', 'Colin Cherry', 'Eleftheria Briakou', 'Elizabeth Nielsen', 'Jiaming Luo', 'Kat Black', 'Ryan Mullins', 'Sweta Agrawal', 'Wenda Xu', 'Erin Kats', 'Stephane Jaskiewicz', 'Markus Freitag', 'David Vilar'], 'affiliations': ['Google'], 'pdf_title_img': 'assets/pdf/title_img/2601.09012.jpg', 'data': {'categories': ['#training', '#open_source', '#multilingual', '#benchmark', '#synthetic', '#machine_translation', '#multimodal', '#optimization', '#rl'], 'emoji': 'ğŸŒ', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´ Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'TranslateGemma Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Gemma 3, ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¾Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ¾Ğ². ĞĞ° Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ°Ñ Ñ‚Ğ¾Ğ½ĞºĞ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ½Ğ° Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ñ… ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ°Ñ…, Ğ° Ğ½Ğ° Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ°Ğ½ÑĞ°Ğ¼Ğ±Ğ»Ñ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… WMT25 Ğ¸ WMT24++ Ğ¿Ğ¾ 55 ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ¾Ğ½ĞºÑƒÑ€Ğ¸Ñ€ÑƒÑÑ‚ Ñ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğ¼Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. TranslateGemma Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ñ‚Ñ€Ğ°Ğ½ÑĞ»ÑÑ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'TranslateGemma: Elevating Multilingual Translation Efficiency', 'desc': 'TranslateGemma is an advanced machine translation model that builds on the Gemma 3 foundation by enhancing its multilingual capabilities. It utilizes a two-stage fine-tuning approach, starting with supervised training on a mix of synthetic and human-translated data to improve translation quality. The second stage involves reinforcement learning, where various reward models are used to further optimize the translations. The results show that TranslateGemma not only outperforms the original Gemma 3 models but also maintains efficiency, especially in smaller model sizes, while retaining strong multimodal translation abilities.'}, 'zh': {'title': 'æå‡ç¿»è¯‘è´¨é‡ï¼Œæ•ˆç‡æ›´é«˜çš„TranslateGemma', 'desc': 'TranslateGemma æ˜¯åŸºäº Gemma 3 åŸºç¡€æ¨¡å‹çš„å¼€æ”¾æœºå™¨ç¿»è¯‘æ¨¡å‹å¥—ä»¶ã€‚ä¸ºäº†æå‡ Gemma 3 çš„å¤šè¯­è¨€ç¿»è¯‘èƒ½åŠ›ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸¤é˜¶æ®µçš„å¾®è°ƒè¿‡ç¨‹ï¼Œé¦–å…ˆä½¿ç”¨é«˜è´¨é‡çš„åˆæˆå¹³è¡Œæ•°æ®å’Œäººå·¥ç¿»è¯‘æ•°æ®è¿›è¡Œç›‘ç£å¾®è°ƒã€‚æ¥ç€ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ é˜¶æ®µï¼Œåˆ©ç”¨å¤šç§å¥–åŠ±æ¨¡å‹ä¼˜åŒ–ç¿»è¯‘è´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTranslateGemma åœ¨å¤šä¸ªè¯­è¨€å¯¹ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä¸”å°å‹æ¨¡å‹çš„æ€§èƒ½ä¸å¤§å‹åŸºçº¿æ¨¡å‹ç›¸å½“ï¼Œæ˜¾ç¤ºå‡ºæ›´é«˜çš„æ•ˆç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.08955', 'title': 'Imagine-then-Plan: Agent Learning from Adaptive Lookahead with World Models', 'url': 'https://huggingface.co/papers/2601.08955', 'abstract': "Imagine-then-Plan framework enables agent learning through adaptive lookahead imagination, combining imagined trajectories with current observations to guide policy learning in complex task scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in world models have shown promise for modeling future dynamics of environmental states, enabling agents to reason and act without accessing real environments. Current methods mainly perform single-step or fixed-horizon rollouts, leaving their potential for complex task planning under-exploited. We propose Imagine-then-Plan (ITP), a unified framework for agent learning via lookahead imagination, where an agent's policy model interacts with the learned world model, yielding multi-step ``imagined'' trajectories. Since the imagination horizon may vary by tasks and stages, we introduce a novel adaptive lookahead mechanism by trading off the ultimate goal and task progress. The resulting imagined trajectories provide rich signals about future consequences, such as achieved progress and potential conflicts, which are fused with current observations, formulating a partially observable and imaginable Markov decision process to guide policy learning. We instantiate ITP with both training-free and reinforcement-trained variants. Extensive experiments across representative agent benchmarks demonstrate that ITP significantly outperforms competitive baselines. Further analyses validate that our adaptive lookahead largely enhances agents' reasoning capability, providing valuable insights into addressing broader, complex tasks.", 'score': 9, 'issue_id': 591, 'pub_date': '2026-01-13', 'pub_date_card': {'ru': '13 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 13', 'zh': '1æœˆ13æ—¥'}, 'hash': '19d5a5d1b456c01e', 'authors': ['Youwei Liu', 'Jian Wang', 'Hanlin Wang', 'Beichen Guo', 'Wenjie Li'], 'affiliations': ['Central South University', 'The Hong Kong Polytechnic University'], 'pdf_title_img': 'assets/pdf/title_img/2601.08955.jpg', 'data': {'categories': ['#agents', '#rl', '#benchmark'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ£Ñ‡Ğ¸Ğ¼ÑÑ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ: Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´Ğ²Ğ¸Ğ´ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ¼Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° Imagine-then-Plan (ITP), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼ ÑƒÑ‡Ğ¸Ñ‚ÑŒÑÑ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ³ĞµĞ½Ñ‚ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²ÑƒĞµÑ‚ Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ¶Ğ°ĞµĞ¼Ñ‹Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸, Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚ ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€ĞµĞ³ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸ ÑÑ‚Ğ°Ğ¿Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ’Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ¶Ğ°ĞµĞ¼Ñ‹Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ±Ğ¾Ğ³Ğ°Ñ‚ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑÑ‚Ğ²Ğ¸ÑÑ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‚ÑÑ Ñ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ´Ğ»Ñ Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ĞµĞ¼Ğ¾Ğ³Ğ¾ ĞœĞ°Ñ€ĞºĞ¾Ğ²ÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ITP Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡.'}, 'en': {'title': 'Imagine-then-Plan: Enhancing Agent Learning through Adaptive Lookahead Imagination', 'desc': 'The Imagine-then-Plan (ITP) framework enhances agent learning by using adaptive lookahead imagination to create multi-step imagined trajectories. This approach allows agents to combine their current observations with predictions about future states, improving their decision-making in complex tasks. By introducing an adaptive mechanism that adjusts the imagination horizon based on task requirements, ITP effectively balances immediate goals with overall task progress. Experimental results show that ITP outperforms existing methods, demonstrating its ability to improve reasoning and planning capabilities in agents.'}, 'zh': {'title': 'æƒ³è±¡å…ˆè¡Œï¼Œè§„åˆ’æœªæ¥', 'desc': 'Imagine-then-Planï¼ˆITPï¼‰æ¡†æ¶é€šè¿‡è‡ªé€‚åº”çš„å‰ç»æ€§æƒ³è±¡ï¼Œå¸®åŠ©æ™ºèƒ½ä½“åœ¨å¤æ‚ä»»åŠ¡åœºæ™¯ä¸­å­¦ä¹ ã€‚è¯¥æ–¹æ³•ç»“åˆäº†æƒ³è±¡çš„è½¨è¿¹å’Œå½“å‰è§‚å¯Ÿï¼ŒæŒ‡å¯¼ç­–ç•¥å­¦ä¹ ã€‚ITPå¼•å…¥äº†ä¸€ç§æ–°çš„è‡ªé€‚åº”å‰ç»æœºåˆ¶ï¼Œæ ¹æ®ä»»åŠ¡å’Œé˜¶æ®µçš„ä¸åŒè°ƒæ•´æƒ³è±¡çš„èŒƒå›´ï¼Œä»è€Œæä¾›å…³äºæœªæ¥ç»“æœçš„ä¸°å¯Œä¿¡å·ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒITPåœ¨å¤šä¸ªæ™ºèƒ½ä½“åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºå…¶ä»–ç«äº‰æ–¹æ³•ï¼Œå¢å¼ºäº†æ™ºèƒ½ä½“çš„æ¨ç†èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.09173', 'title': 'Geometric Stability: The Missing Axis of Representations', 'url': 'https://huggingface.co/papers/2601.09173', 'abstract': 'Geometric stability measures representational robustness under perturbation, offering complementary insights to similarity metrics in analyzing learned representations across diverse domains.  \t\t\t\t\tAI-generated summary \t\t\t\t Analysis of learned representations has a blind spot: it focuses on similarity, measuring how closely embeddings align with external references, but similarity reveals only what is represented, not whether that structure is robust. We introduce geometric stability, a distinct dimension that quantifies how reliably representational geometry holds under perturbation, and present Shesha, a framework for measuring it. Across 2,463 configurations in seven domains, we show that stability and similarity are empirically uncorrelated (Ïapprox 0.01) and mechanistically distinct: similarity metrics collapse after removing the top principal components, while stability retains sensitivity to fine-grained manifold structure. This distinction yields actionable insights: for safety monitoring, stability acts as a functional geometric canary, detecting structural drift nearly 2times more sensitively than CKA while filtering out the non-functional noise that triggers false alarms in rigid distance metrics; for controllability, supervised stability predicts linear steerability (Ï= 0.89-0.96); for model selection, stability dissociates from transferability, revealing a geometric tax that transfer optimization incurs. Beyond machine learning, stability predicts CRISPR perturbation coherence and neural-behavioral coupling. By quantifying how reliably systems maintain structure, geometric stability provides a necessary complement to similarity for auditing representations across biological and computational systems.', 'score': 4, 'issue_id': 597, 'pub_date': '2026-01-14', 'pub_date_card': {'ru': '14 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 14', 'zh': '1æœˆ14æ—¥'}, 'hash': '891ec0ae5ec73b3f', 'authors': ['Prashant C. Raju'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2601.09173.jpg', 'data': {'categories': [], 'emoji': 'ğŸ—ï¸', 'ru': {'title': 'Ğ“ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ°Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ: Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ Ğ²Ğ¾Ğ·Ğ¼ÑƒÑ‰ĞµĞ½Ğ¸ÑÑ…', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ â€” Ğ½Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸Ğ·Ğ¼ĞµÑ€ÑĞµÑ‚ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ (ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ²) Ğ¿Ñ€Ğ¸ Ğ²Ğ¾Ğ·Ğ¼ÑƒÑ‰ĞµĞ½Ğ¸ÑÑ…, Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ÑÑ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ€Ñ‹ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Shesha Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° 2463 ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸ÑÑ… Ğ² ÑĞµĞ¼Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…, Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ², Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ½Ğµ ĞºĞ¾Ñ€Ñ€ĞµĞ»Ğ¸Ñ€ÑƒÑÑ‚ Ğ¸ Ğ¾Ñ‚Ñ€Ğ°Ğ¶Ğ°ÑÑ‚ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ°ÑĞ¿ĞµĞºÑ‚Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹. Ğ“ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ°Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğº ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğ¼ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸ÑĞ¼, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ ĞµÑ‘ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ñ‹Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ¼ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³Ğ° Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€. ĞœĞµÑ‚Ñ€Ğ¸ĞºĞ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼Ğ° Ğ·Ğ° Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ°Ğ¼Ğ¸ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ â€” Ğ² Ğ±Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ²Ğ¾Ğ·Ğ¼ÑƒÑ‰ĞµĞ½Ğ¸Ğ¹ CRISPR Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾-Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸.'}, 'en': {'title': 'Geometric Stability: A New Lens for Robust Representation Analysis', 'desc': 'This paper introduces geometric stability as a new measure to evaluate the robustness of learned representations in machine learning. Unlike traditional similarity metrics that only assess how closely embeddings match external references, geometric stability quantifies how well the representational geometry holds up under perturbations. The authors present a framework called Shesha to measure this stability across various domains, demonstrating that stability and similarity are largely uncorrelated. The findings suggest that geometric stability can enhance safety monitoring, controllability, and model selection by providing deeper insights into the structural integrity of representations.'}, 'zh': {'title': 'å‡ ä½•ç¨³å®šæ€§ï¼šè¶…è¶Šç›¸ä¼¼æ€§çš„é²æ£’æ€§åº¦é‡', 'desc': 'å‡ ä½•ç¨³å®šæ€§åº¦é‡åœ¨æ‰°åŠ¨ä¸‹çš„è¡¨ç¤ºé²æ£’æ€§ï¼Œä¸ºåˆ†æä¸åŒé¢†åŸŸçš„å­¦ä¹ è¡¨ç¤ºæä¾›äº†è¡¥å……çš„è§è§£ã€‚ä¼ ç»Ÿçš„ç›¸ä¼¼æ€§åº¦é‡åªå…³æ³¨åµŒå…¥ä¸å¤–éƒ¨å‚è€ƒçš„å¯¹é½ç¨‹åº¦ï¼Œè€Œå‡ ä½•ç¨³å®šæ€§åˆ™é‡åŒ–äº†è¡¨ç¤ºå‡ ä½•åœ¨æ‰°åŠ¨ä¸‹çš„å¯é æ€§ã€‚æˆ‘ä»¬æå‡ºäº†Sheshaæ¡†æ¶æ¥æµ‹é‡å‡ ä½•ç¨³å®šæ€§ï¼Œå¹¶å±•ç¤ºäº†åœ¨å¤šä¸ªé¢†åŸŸä¸­ç¨³å®šæ€§ä¸ç›¸ä¼¼æ€§ä¹‹é—´å‡ ä¹æ²¡æœ‰ç›¸å…³æ€§ã€‚å‡ ä½•ç¨³å®šæ€§ä¸ä»…åœ¨å®‰å…¨ç›‘æµ‹å’Œå¯æ§æ€§æ–¹é¢æä¾›äº†é‡è¦çš„åº”ç”¨ï¼Œè¿˜èƒ½åœ¨ç”Ÿç‰©å’Œè®¡ç®—ç³»ç»Ÿä¸­å®¡è®¡è¡¨ç¤ºçš„ç»“æ„ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.09113', 'title': 'The AI Hippocampus: How Far are We From Human Memory?', 'url': 'https://huggingface.co/papers/2601.09113', 'abstract': 'Memory mechanisms in large language models and multi-modal language models are categorized into implicit, explicit, and agentic paradigms, supporting enhanced reasoning, adaptability, and contextual fidelity through internal parameters, external knowledge storage, and persistent agent memory structures.  \t\t\t\t\tAI-generated summary \t\t\t\t Memory plays a foundational role in augmenting the reasoning, adaptability, and contextual fidelity of modern Large Language Models and Multi-Modal LLMs. As these models transition from static predictors to interactive systems capable of continual learning and personalized inference, the incorporation of memory mechanisms has emerged as a central theme in their architectural and functional evolution. This survey presents a comprehensive and structured synthesis of memory in LLMs and MLLMs, organizing the literature into a cohesive taxonomy comprising implicit, explicit, and agentic memory paradigms. Specifically, the survey delineates three primary memory frameworks. Implicit memory refers to the knowledge embedded within the internal parameters of pre-trained transformers, encompassing their capacity for memorization, associative retrieval, and contextual reasoning. Recent work has explored methods to interpret, manipulate, and reconfigure this latent memory. Explicit memory involves external storage and retrieval components designed to augment model outputs with dynamic, queryable knowledge representations, such as textual corpora, dense vectors, and graph-based structures, thereby enabling scalable and updatable interaction with information sources. Agentic memory introduces persistent, temporally extended memory structures within autonomous agents, facilitating long-term planning, self-consistency, and collaborative behavior in multi-agent systems, with relevance to embodied and interactive AI. Extending beyond text, the survey examines the integration of memory within multi-modal settings, where coherence across vision, language, audio, and action modalities is essential. Key architectural advances, benchmark tasks, and open challenges are discussed, including issues related to memory capacity, alignment, factual consistency, and cross-system interoperability.', 'score': 4, 'issue_id': 589, 'pub_date': '2026-01-14', 'pub_date_card': {'ru': '14 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 14', 'zh': '1æœˆ14æ—¥'}, 'hash': 'bce2ff60047e0ade', 'authors': ['Zixia Jia', 'Jiaqi Li', 'Yipeng Kang', 'Yuxuan Wang', 'Tong Wu', 'Quansen Wang', 'Xiaobo Wang', 'Shuyi Zhang', 'Junzhe Shen', 'Qing Li', 'Siyuan Qi', 'Yitao Liang', 'Di He', 'Zilong Zheng', 'Song-Chun Zhu'], 'affiliations': ['Peking University', 'State Key Laboratory of General Artificial Intelligence, BIGAI'], 'pdf_title_img': 'assets/pdf/title_img/2601.09113.jpg', 'data': {'categories': ['#rag', '#alignment', '#benchmark', '#reasoning', '#survey', '#multimodal', '#interpretability', '#long_context', '#agents', '#architecture'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ¢Ñ€Ğ¸ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸: Ğ¾Ñ‚ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğº Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½ÑƒÑ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… LLM, Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑÑ Ğ¸Ñ… Ğ½Ğ° Ñ‚Ñ€Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñ‹: Ğ¸Ğ¼Ğ¿Ğ»Ğ¸Ñ†Ğ¸Ñ‚Ğ½Ğ°Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ, Ğ²ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ğ°Ñ Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ², ÑĞ²Ğ½Ğ°Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ² Ğ²Ğ¸Ğ´Ğµ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ñ…Ñ€Ğ°Ğ½Ğ¸Ğ»Ğ¸Ñ‰ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼. Ğ˜Ğ¼Ğ¿Ğ»Ğ¸Ñ†Ğ¸Ñ‚Ğ½Ğ°Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ°ÑÑĞ¾Ñ†Ğ¸Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ¿Ğ¾Ğ¸ÑĞºÑƒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸ ÑĞ²Ğ¾Ğ¸Ñ… Ğ²ĞµÑĞ¾Ğ². Ğ¯Ğ²Ğ½Ğ°Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğµ Ğ±Ğ°Ğ·Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ²Ñ‹Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ´Ğ»Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ³ĞµĞ½Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ñ‡ĞµÑ€ĞµĞ· ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑÑ‰Ğ¸ĞµÑÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸, Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸.'}, 'en': {'title': 'Enhancing AI with Memory: A New Era of Learning and Interaction', 'desc': "This paper explores how memory mechanisms enhance the capabilities of Large Language Models (LLMs) and Multi-Modal Language Models (MLLMs). It categorizes memory into three types: implicit, explicit, and agentic, each contributing to improved reasoning and adaptability. Implicit memory is built into the model's parameters, while explicit memory uses external storage for dynamic knowledge retrieval. Agentic memory supports long-term planning and collaboration in AI systems, making these models more interactive and capable of handling various types of information."}, 'zh': {'title': 'è®°å¿†æœºåˆ¶ï¼šæå‡è¯­è¨€æ¨¡å‹çš„æ™ºèƒ½ä¸é€‚åº”æ€§', 'desc': 'æœ¬æ–‡æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹å’Œå¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ä¸­çš„è®°å¿†æœºåˆ¶ï¼Œåˆ†ä¸ºéšå¼ã€æ˜¾å¼å’Œä»£ç†è®°å¿†ä¸‰ç§èŒƒå¼ã€‚è¿™äº›è®°å¿†æœºåˆ¶å¢å¼ºäº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€é€‚åº”æ€§å’Œä¸Šä¸‹æ–‡ä¸€è‡´æ€§ã€‚éšå¼è®°å¿†æ˜¯æŒ‡é¢„è®­ç»ƒå˜æ¢å™¨å†…éƒ¨å‚æ•°ä¸­åµŒå…¥çš„çŸ¥è¯†ï¼Œè€Œæ˜¾å¼è®°å¿†åˆ™æ¶‰åŠå¤–éƒ¨å­˜å‚¨å’Œæ£€ç´¢ç»„ä»¶ï¼Œä»¥åŠ¨æ€çŸ¥è¯†è¡¨ç¤ºå¢å¼ºæ¨¡å‹è¾“å‡ºã€‚ä»£ç†è®°å¿†åˆ™å¼•å…¥äº†æŒä¹…çš„è®°å¿†ç»“æ„ï¼Œæ”¯æŒè‡ªä¸»ä»£ç†çš„é•¿æœŸè§„åˆ’å’Œåä½œè¡Œä¸ºï¼Œé€‚ç”¨äºå¤šä»£ç†ç³»ç»Ÿã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.09697', 'title': 'Efficient Camera-Controlled Video Generation of Static Scenes via Sparse Diffusion and 3D Rendering', 'url': 'https://huggingface.co/papers/2601.09697', 'abstract': 'Diffusion-based video generation is made more efficient through keyframe-based 3D reconstruction and rendering, enabling faster synthesis with maintained visual quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Modern video generative models based on diffusion models can produce very realistic clips, but they are computationally inefficient, often requiring minutes of GPU time for just a few seconds of video. This inefficiency poses a critical barrier to deploying generative video in applications that require real-time interactions, such as embodied AI and VR/AR. This paper explores a new strategy for camera-conditioned video generation of static scenes: using diffusion-based generative models to generate a sparse set of keyframes, and then synthesizing the full video through 3D reconstruction and rendering. By lifting keyframes into a 3D representation and rendering intermediate views, our approach amortizes the generation cost across hundreds of frames while enforcing geometric consistency. We further introduce a model that predicts the optimal number of keyframes for a given camera trajectory, allowing the system to adaptively allocate computation. Our final method, SRENDER, uses very sparse keyframes for simple trajectories and denser ones for complex camera motion. This results in video generation that is more than 40 times faster than the diffusion-based baseline in generating 20 seconds of video, while maintaining high visual fidelity and temporal stability, offering a practical path toward efficient and controllable video synthesis.', 'score': 3, 'issue_id': 589, 'pub_date': '2026-01-14', 'pub_date_card': {'ru': '14 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 14', 'zh': '1æœˆ14æ—¥'}, 'hash': '0e24e42f2f75ca08', 'authors': ['Jieying Chen', 'Jeffrey Hu', 'Joan Lasenby', 'Ayush Tewari'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2601.09697.jpg', 'data': {'categories': ['#3d', '#inference', '#diffusion', '#optimization', '#robotics', '#video'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ‘Ñ‹ÑÑ‚Ñ€Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµÑ€ĞµĞ· Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ ĞºĞ°Ğ´Ñ€Ñ‹ Ğ¸ 3D-Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¸ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ğ¾Ğ¹ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ³Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ²ÑĞµÑ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒÑÑ‚ÑÑ Ğ² 3D-Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ÑÑ Ğ´Ğ»Ñ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ° Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ ĞºĞ°Ğ¼ĞµÑ€Ñ‹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ÑÑ‚ÑŒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹. ĞœĞµÑ‚Ğ¾Ğ´ SRENDER ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ğ² 40 Ñ€Ğ°Ğ· Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ¼, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ.'}, 'en': {'title': 'Efficient Video Generation with Keyframe-Based Diffusion Models', 'desc': 'This paper presents a novel approach to video generation using diffusion models that enhances efficiency by focusing on keyframe-based 3D reconstruction. By generating a limited number of keyframes and reconstructing the full video from these frames, the method significantly reduces computational costs while preserving visual quality. The authors introduce a model that dynamically determines the optimal number of keyframes based on camera movement, allowing for adaptive computation. The resulting system, named SRENDER, achieves over 40 times faster video generation compared to traditional methods, making it suitable for real-time applications.'}, 'zh': {'title': 'é«˜æ•ˆè§†é¢‘ç”Ÿæˆï¼šå…³é”®å¸§ä¸3Dé‡å»ºçš„ç»“åˆ', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºå…³é”®å¸§çš„3Dé‡å»ºå’Œæ¸²æŸ“æ–¹æ³•ï¼Œä»¥æé«˜æ‰©æ•£æ¨¡å‹ç”Ÿæˆè§†é¢‘çš„æ•ˆç‡ã€‚é€šè¿‡ç”Ÿæˆç¨€ç–çš„å…³é”®å¸§å¹¶åˆ©ç”¨3Dé‡å»ºåˆæˆå®Œæ•´è§†é¢‘ï¼Œæ˜¾è‘—é™ä½äº†è®¡ç®—æˆæœ¬ã€‚è¯¥æ–¹æ³•åœ¨å¤„ç†ç®€å•å’Œå¤æ‚ç›¸æœºè½¨è¿¹æ—¶ï¼Œèƒ½å¤Ÿè‡ªé€‚åº”åœ°åˆ†é…è®¡ç®—èµ„æºï¼Œä»è€Œå®ç°æ›´å¿«çš„è§†é¢‘ç”Ÿæˆã€‚æœ€ç»ˆï¼ŒSRENDERæ–¹æ³•åœ¨ç”Ÿæˆ20ç§’è§†é¢‘æ—¶ï¼Œæ¯”ä¼ ç»Ÿæ‰©æ•£æ¨¡å‹å¿«40å€ï¼ŒåŒæ—¶ä¿æŒé«˜è§†è§‰è´¨é‡å’Œæ—¶é—´ç¨³å®šæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.01075', 'title': 'Flow Equivariant World Models: Memory for Partially Observed Dynamic Environments', 'url': 'https://huggingface.co/papers/2601.01075', 'abstract': "Flow Equivariant World Models unify self-motion and external object motion as one-parameter Lie group flows to create stable, symmetry-guided representations for embodied intelligence.  \t\t\t\t\tAI-generated summary \t\t\t\t Embodied systems experience the world as 'a symphony of flows': a combination of many continuous streams of sensory input coupled to self-motion, interwoven with the dynamics of external objects. These streams obey smooth, time-parameterized symmetries, which combine through a precisely structured algebra; yet most neural network world models ignore this structure and instead repeatedly re-learn the same transformations from data. In this work, we introduce 'Flow Equivariant World Models', a framework in which both self-motion and external object motion are unified as one-parameter Lie group 'flows'. We leverage this unification to implement group equivariance with respect to these transformations, thereby providing a stable latent world representation over hundreds of timesteps. On both 2D and 3D partially observed video world modeling benchmarks, we demonstrate that Flow Equivariant World Models significantly outperform comparable state-of-the-art diffusion-based and memory-augmented world modeling architectures -- particularly when there are predictable world dynamics outside the agent's current field of view. We show that flow equivariance is particularly beneficial for long rollouts, generalizing far beyond the training horizon. By structuring world model representations with respect to internal and external motion, flow equivariance charts a scalable route to data efficient, symmetry-guided, embodied intelligence. Project link: https://flowequivariantworldmodels.github.io.", 'score': 3, 'issue_id': 604, 'pub_date': '2026-01-03', 'pub_date_card': {'ru': '3 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 3', 'zh': '1æœˆ3æ—¥'}, 'hash': 'd5e0be798b37c694', 'authors': ['Hansen Jin Lillemark', 'Benhao Huang', 'Fangneng Zhan', 'Yilun Du', 'Thomas Anderson Keller'], 'affiliations': ['CSE, UC San Diego', 'Kempner Institute, Harvard University', 'ML, Carnegie Mellon University', 'SEAS, Harvard University'], 'pdf_title_img': 'assets/pdf/title_img/2601.01075.jpg', 'data': {'categories': ['#training', '#architecture', '#robotics', '#optimization'], 'emoji': 'ğŸŒŠ', 'ru': {'title': 'Ğ¡Ğ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ² ĞºĞ°Ğº Ğ¾ÑĞ½Ğ¾Ğ²Ğ° ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¸Ñ€Ğ°', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ñ‹ Flow Equivariant World Models â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¸Ñ€Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² ĞºĞ°Ğº Ğ¾Ğ´Ğ½Ğ¾Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¸ Ğ³Ñ€ÑƒĞ¿Ğ¿Ñ‹ Ğ›Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¾Ğ²ÑƒÑ ÑĞºĞ²Ğ¸Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑÑ‚Ğ¸Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ² Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ñ‚ÑĞ¶ĞµĞ½Ğ¸Ğ¸ ÑĞ¾Ñ‚ĞµĞ½ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ¿Ñ€Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ğ¼Ğ¸Ñ€Ğ° Ğ·Ğ° Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾Ğ»Ñ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ½ÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ´Ğ¾Ğ»Ğ³Ğ¸Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿ÑƒÑ‚ÑŒ Ğº Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¸ ÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¹Ğ½Ğ¾-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰Ñ‘Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°.'}, 'en': {'title': 'Unifying Motion for Smarter AI: Flow Equivariant World Models', 'desc': "This paper presents 'Flow Equivariant World Models', a new framework that combines self-motion and external object motion into a unified representation using one-parameter Lie group flows. By recognizing the smooth, time-parameterized symmetries in sensory input, the model achieves group equivariance, allowing it to maintain a stable latent representation over time. The authors demonstrate that this approach significantly outperforms existing world modeling techniques, especially in scenarios with predictable dynamics beyond the agent's immediate view. Ultimately, the framework aims to enhance data efficiency and support the development of embodied intelligence by structuring representations around motion dynamics."}, 'zh': {'title': 'æµç­‰å˜ä¸–ç•Œæ¨¡å‹ï¼šæ„å»ºç¨³å®šçš„å…·èº«æ™ºèƒ½', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºâ€œæµç­‰å˜ä¸–ç•Œæ¨¡å‹â€çš„æ¡†æ¶ï¼Œå°†è‡ªæˆ‘è¿åŠ¨å’Œå¤–éƒ¨ç‰©ä½“è¿åŠ¨ç»Ÿä¸€ä¸ºä¸€å‚æ•°æç¾¤æµã€‚è¿™ç§ç»Ÿä¸€ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿå®ç°å¯¹è¿™äº›å˜æ¢çš„ç¾¤ç­‰å˜æ€§ï¼Œä»è€Œåœ¨æ•°ç™¾ä¸ªæ—¶é—´æ­¥é•¿å†…æä¾›ç¨³å®šçš„æ½œåœ¨ä¸–ç•Œè¡¨ç¤ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨2Då’Œ3Déƒ¨åˆ†è§‚å¯Ÿè§†é¢‘ä¸–ç•Œå»ºæ¨¡åŸºå‡†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„æ‰©æ•£åŸºç¡€å’Œè®°å¿†å¢å¼ºçš„ä¸–ç•Œå»ºæ¨¡æ¶æ„ï¼Œå°¤å…¶æ˜¯åœ¨ä»£ç†å½“å‰è§†é‡ä¹‹å¤–å­˜åœ¨å¯é¢„æµ‹çš„ä¸–ç•ŒåŠ¨æ€æ—¶ã€‚é€šè¿‡å¯¹å†…éƒ¨å’Œå¤–éƒ¨è¿åŠ¨çš„ç»“æ„åŒ–ï¼Œæµç­‰å˜æ€§ä¸ºæ•°æ®é«˜æ•ˆã€å¯¹ç§°å¼•å¯¼çš„å…·èº«æ™ºèƒ½æä¾›äº†ä¸€æ¡å¯æ‰©å±•çš„è·¯å¾„ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.09609', 'title': 'DPWriter: Reinforcement Learning with Diverse Planning Branching for Creative Writing', 'url': 'https://huggingface.co/papers/2601.09609', 'abstract': 'A reinforcement learning framework with diverse planning branching and group-aware rewards enhances large language model output diversity in creative writing tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning (RL)-based enhancement of large language models (LLMs) often leads to reduced output diversity, undermining their utility in open-ended tasks like creative writing. Current methods lack explicit mechanisms for guiding diverse exploration and instead prioritize optimization efficiency and performance over diversity. This paper proposes an RL framework structured around a semi-structured long Chain-of-Thought (CoT), in which the generation process is decomposed into explicitly planned intermediate steps. We introduce a Diverse Planning Branching method that strategically introduces divergence at the planning phase based on diversity variation, alongside a group-aware diversity reward to encourage distinct trajectories. Experimental results on creative writing benchmarks demonstrate that our approach significantly improves output diversity without compromising generation quality, consistently outperforming existing baselines.', 'score': 1, 'issue_id': 601, 'pub_date': '2026-01-14', 'pub_date_card': {'ru': '14 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 14', 'zh': '1æœˆ14æ—¥'}, 'hash': '54b24c6152ed81f1', 'authors': ['Qian Cao', 'Yahui Liu', 'Wei Bi', 'Yi Zhao', 'Ruihua Song', 'Xiting Wang', 'Ruiming Tang', 'Guorui Zhou', 'Han Li'], 'affiliations': ['Kuaishou Technology', 'Renmin University of China'], 'pdf_title_img': 'assets/pdf/title_img/2601.09609.jpg', 'data': {'categories': ['#rl', '#reasoning', '#story_generation', '#optimization', '#benchmark', '#training'], 'emoji': 'ğŸŒ³', 'ru': {'title': 'Ğ’ĞµÑ‚Ğ²ÑÑ‰ĞµĞµÑÑ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ² Ñ‚Ğ²Ğ¾Ñ€Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚ĞµĞºÑÑ‚Ğ°Ñ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ğ² Ñ‚Ğ²Ğ¾Ñ€Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¸ĞºÑƒ Diverse Planning Branching, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ñ€Ğ°ÑÑ…Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¾ÑĞ½Ğ¾Ğ²Ñ‹Ğ²Ğ°ÑÑÑŒ Ğ½Ğ° Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑÑ… Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ”Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¾Ğ²Ğ°Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑÑ‚Ğ¸Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğº Ğ²Ñ‹Ğ±Ğ¾Ñ€Ñƒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ°Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¸ĞºĞ° ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ±ĞµĞ· ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ².'}, 'en': {'title': 'Enhancing Creativity: Diverse Outputs through Smart Planning in RL', 'desc': 'This paper presents a new reinforcement learning framework designed to improve the diversity of outputs from large language models during creative writing tasks. It addresses the common issue where traditional RL methods reduce output variety by focusing too much on performance. The proposed method uses a semi-structured Chain-of-Thought approach, breaking down the generation process into planned steps that allow for more diverse exploration. By incorporating a Diverse Planning Branching technique and a group-aware reward system, the framework encourages the model to produce distinct and varied outputs while maintaining high quality.'}, 'zh': {'title': 'æå‡åˆ›æ„å†™ä½œå¤šæ ·æ€§çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åˆ›æ„å†™ä½œä»»åŠ¡ä¸­çš„è¾“å‡ºå¤šæ ·æ€§ã€‚é€šè¿‡å¼•å…¥å¤šæ ·åŒ–è§„åˆ’åˆ†æ”¯æ–¹æ³•ï¼Œç ”ç©¶è€…åœ¨è§„åˆ’é˜¶æ®µæœ‰æ„è¯†åœ°å¼•å…¥äº†å¤šæ ·æ€§å˜åŒ–ï¼Œä»¥ä¿ƒè¿›ä¸åŒçš„ç”Ÿæˆè·¯å¾„ã€‚åŒæ—¶ï¼Œé‡‡ç”¨äº†ç¾¤ä½“æ„ŸçŸ¥çš„å¤šæ ·æ€§å¥–åŠ±æœºåˆ¶ï¼Œé¼“åŠ±æ¨¡å‹ç”Ÿæˆæ›´å…·ç‹¬ç‰¹æ€§çš„å†…å®¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒç”Ÿæˆè´¨é‡çš„åŒæ—¶ï¼Œæ˜¾è‘—æå‡äº†è¾“å‡ºçš„å¤šæ ·æ€§ï¼Œè¶…è¶Šäº†ç°æœ‰çš„åŸºå‡†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.09536', 'title': 'Omni-R1: Towards the Unified Generative Paradigm for Multimodal Reasoning', 'url': 'https://huggingface.co/papers/2601.09536', 'abstract': 'Unified generative multimodal reasoning approach enables diverse reasoning skills through intermediate image generation, with a two-stage SFT+RL framework and a text-only bootstrapping variant.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal Large Language Models (MLLMs) are making significant progress in multimodal reasoning. Early approaches focus on pure text-based reasoning. More recent studies have incorporated multimodal information into the reasoning steps; however, they often follow a single task-specific reasoning pattern, which limits their generalizability across various multimodal tasks. In fact, there are numerous multimodal tasks requiring diverse reasoning skills, such as zooming in on a specific region or marking an object within an image. To address this, we propose unified generative multimodal reasoning, which unifies diverse multimodal reasoning skills by generating intermediate images during the reasoning process. We instantiate this paradigm with Omni-R1, a two-stage SFT+RL framework featuring perception alignment loss and perception reward, thereby enabling functional image generation. Additionally, we introduce Omni-R1-Zero, which eliminates the need for multimodal annotations by bootstrapping step-wise visualizations from text-only reasoning data. Empirical results show that Omni-R1 achieves unified generative reasoning across a wide range of multimodal tasks, and Omni-R1-Zero can match or even surpass Omni-R1 on average, suggesting a promising direction for generative multimodal reasoning.', 'score': 1, 'issue_id': 597, 'pub_date': '2026-01-14', 'pub_date_card': {'ru': '14 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 14', 'zh': '1æœˆ14æ—¥'}, 'hash': 'e347d41542e4093a', 'authors': ['Dongjie Cheng', 'Yongqi Li', 'Zhixin Ma', 'Hongru Cai', 'Yupeng Hu', 'Wenjie Wang', 'Liqiang Nie', 'Wenjie Li'], 'affiliations': ['Harbin Institute of Technology (Shenzhen)', 'Shandong University', 'Singapore Management University', 'The Hong Kong Polytechnic University', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2601.09536.jpg', 'data': {'categories': ['#multimodal', '#training', '#video', '#rlhf', '#reasoning', '#diffusion', '#rl'], 'emoji': 'ğŸ¨', 'ru': {'title': 'ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Omni-R1 â€” Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ñ supervised fine-tuning Ğ¸ reinforcement learning, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ loss Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ´Ğ»Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° Ğ²ĞµÑ€ÑĞ¸Ñ Omni-R1-Zero, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹ Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ bootstrap Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¹ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾Ğ¼ Ğ´Ğ¸Ğ°Ğ¿Ğ°Ğ·Ğ¾Ğ½Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡.'}, 'en': {'title': 'Empowering Multimodal Reasoning through Image Generation', 'desc': 'This paper presents a new approach called unified generative multimodal reasoning, which enhances reasoning skills by generating intermediate images during the reasoning process. The authors introduce a two-stage framework, SFT+RL, that incorporates perception alignment loss and perception reward to improve image generation capabilities. They also propose a variant, Omni-R1-Zero, which can perform reasoning without needing multimodal annotations by using text-only data for visualizations. The results indicate that this method effectively supports diverse multimodal tasks and shows potential for advancing generative multimodal reasoning.'}, 'zh': {'title': 'ç»Ÿä¸€ç”Ÿæˆå¤šæ¨¡æ€æ¨ç†ï¼Œæå‡æ¨ç†èƒ½åŠ›', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„ç”Ÿæˆå¤šæ¨¡æ€æ¨ç†æ–¹æ³•ï¼Œé€šè¿‡åœ¨æ¨ç†è¿‡ç¨‹ä¸­ç”Ÿæˆä¸­é—´å›¾åƒï¼Œæ¥å®ç°å¤šæ ·åŒ–çš„æ¨ç†æŠ€èƒ½ã€‚æˆ‘ä»¬é‡‡ç”¨äº†ä¸€ä¸ªä¸¤é˜¶æ®µçš„SFT+RLæ¡†æ¶ï¼Œç»“åˆæ„ŸçŸ¥å¯¹é½æŸå¤±å’Œæ„ŸçŸ¥å¥–åŠ±ï¼Œä»è€Œå®ç°åŠŸèƒ½æ€§å›¾åƒç”Ÿæˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†Omni-R1-Zeroï¼Œå®ƒé€šè¿‡ä»ä»…æœ‰æ–‡æœ¬çš„æ¨ç†æ•°æ®ä¸­å¼•å¯¼é€æ­¥å¯è§†åŒ–ï¼Œæ¶ˆé™¤äº†å¯¹å¤šæ¨¡æ€æ³¨é‡Šçš„éœ€æ±‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOmni-R1åœ¨å¤šç§å¤šæ¨¡æ€ä»»åŠ¡ä¸­å®ç°äº†ç»Ÿä¸€çš„ç”Ÿæˆæ¨ç†ï¼Œè€ŒOmni-R1-Zeroçš„è¡¨ç°ç”šè‡³å¯ä»¥ä¸Omni-R1ç›¸åª²ç¾ï¼Œæ˜¾ç¤ºå‡ºç”Ÿæˆå¤šæ¨¡æ€æ¨ç†çš„è‰¯å¥½å‰æ™¯ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.07287', 'title': 'Focal Guidance: Unlocking Controllability from Semantic-Weak Layers in Video Diffusion Models', 'url': 'https://huggingface.co/papers/2601.07287', 'abstract': "Diffusion Transformer-based image-to-video models suffer from condition isolation where visual attention becomes detached from text guidance; focal guidance addresses this through fine-grained semantic guidance and attention cache mechanisms.  \t\t\t\t\tAI-generated summary \t\t\t\t The task of Image-to-Video (I2V) generation aims to synthesize a video from a reference image and a text prompt. This requires diffusion models to reconcile high-frequency visual constraints and low-frequency textual guidance during the denoising process. However, while existing I2V models prioritize visual consistency, how to effectively couple this dual guidance to ensure strong adherence to the text prompt remains underexplored. In this work, we observe that in Diffusion Transformer (DiT)-based I2V models, certain intermediate layers exhibit weak semantic responses (termed Semantic-Weak Layers), as indicated by a measurable drop in text-visual similarity. We attribute this to a phenomenon called Condition Isolation, where attention to visual features becomes partially detached from text guidance and overly relies on learned visual priors. To address this, we propose Focal Guidance (FG), which enhances the controllability from Semantic-Weak Layers. FG comprises two mechanisms: (1) Fine-grained Semantic Guidance (FSG) leverages CLIP to identify key regions in the reference frame and uses them as anchors to guide Semantic-Weak Layers. (2) Attention Cache transfers attention maps from semantically responsive layers to Semantic-Weak Layers, injecting explicit semantic signals and alleviating their over-reliance on the model's learned visual priors, thereby enhancing adherence to textual instructions. To further validate our approach and address the lack of evaluation in this direction, we introduce a benchmark for assessing instruction following in I2V models. On this benchmark, Focal Guidance proves its effectiveness and generalizability, raising the total score on Wan2.1-I2V to 0.7250 (+3.97\\%) and boosting the MMDiT-based HunyuanVideo-I2V to 0.5571 (+7.44\\%).", 'score': 1, 'issue_id': 591, 'pub_date': '2026-01-12', 'pub_date_card': {'ru': '12 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 12', 'zh': '1æœˆ12æ—¥'}, 'hash': 'd5d65512afde8850', 'authors': ['Yuanyang Yin', 'Yufan Deng', 'Shenghai Yuan', 'Kaipeng Zhang', 'Xiao Yang', 'Feng Zhao'], 'affiliations': ['ByteDance', 'MoE Key Lab of BIPC, USTC', 'Shanghai Innovation Institute'], 'pdf_title_img': 'assets/pdf/title_img/2601.07287.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#video', '#diffusion'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ¤Ğ¾ĞºÑƒÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ½Ğ° Ñ‚ĞµĞºÑÑ‚Ğµ: ĞºĞ°Ğº Ğ·Ğ°ÑÑ‚Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞ»ÑƒÑˆĞ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ²ĞµÑĞ½Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ¸Ğ·Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğ¹ Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°Ñ… Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°, ĞºĞ¾Ğ³Ğ´Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ‚Ñ€Ñ‹Ğ²Ğ°ĞµÑ‚ÑÑ Ğ¾Ñ‚ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Focal Guidance, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ CLIP Ğ´Ğ»Ñ Ğ²Ñ‹Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ Ğ² Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¼ ĞºĞ°Ğ´Ñ€Ğµ Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ»Ğ¾ÑĞ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´Ğ²Ğ° ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°: Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¸ ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ĞºĞ°Ñ€Ñ‚ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑƒÑĞ¸Ğ»Ğ¸Ğ²Ğ°ÑÑ‚ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ ÑĞ»Ğ°Ğ±Ñ‹Ñ… Ğ¿Ğ¾ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞµ ÑĞ»Ğ¾ĞµĞ² Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ÑÑ‚ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Image-to-Video Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ½Ğ° ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ….'}, 'en': {'title': 'Enhancing Text-Visual Alignment in Image-to-Video Generation', 'desc': "This paper addresses the challenge of condition isolation in Diffusion Transformer-based image-to-video (I2V) models, where visual attention can become disconnected from text prompts. The authors introduce Focal Guidance (FG), which enhances the model's ability to follow textual instructions by improving the performance of Semantic-Weak Layers. FG employs Fine-grained Semantic Guidance (FSG) to pinpoint important areas in the reference image and uses Attention Cache to transfer attention from more responsive layers to those that are weak. The proposed methods significantly improve adherence to text prompts, as demonstrated by their benchmark results, showing notable performance gains in I2V tasks."}, 'zh': {'title': 'æå‡å›¾åƒåˆ°è§†é¢‘ç”Ÿæˆçš„æ–‡æœ¬éµå¾ªèƒ½åŠ›', 'desc': 'æœ¬æ–‡æ¢è®¨äº†åŸºäºæ‰©æ•£å˜æ¢å™¨çš„å›¾åƒåˆ°è§†é¢‘ç”Ÿæˆæ¨¡å‹ä¸­çš„æ¡ä»¶éš”ç¦»é—®é¢˜ï¼Œå¯¼è‡´è§†è§‰æ³¨æ„åŠ›ä¸æ–‡æœ¬æŒ‡å¯¼è„±èŠ‚ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†ç„¦ç‚¹å¼•å¯¼ï¼ˆFocal Guidanceï¼‰æ–¹æ³•ï¼Œé€šè¿‡ç»†ç²’åº¦è¯­ä¹‰å¼•å¯¼å’Œæ³¨æ„åŠ›ç¼“å­˜æœºåˆ¶æ¥å¢å¼ºè¯­ä¹‰å¼±å±‚çš„æ§åˆ¶èƒ½åŠ›ã€‚ç»†ç²’åº¦è¯­ä¹‰å¼•å¯¼åˆ©ç”¨CLIPè¯†åˆ«å‚è€ƒå¸§ä¸­çš„å…³é”®åŒºåŸŸï¼Œä½œä¸ºå¼•å¯¼è¯­ä¹‰å¼±å±‚çš„é”šç‚¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç„¦ç‚¹å¼•å¯¼æ˜¾è‘—æé«˜äº†æ¨¡å‹å¯¹æ–‡æœ¬æŒ‡ä»¤çš„éµå¾ªèƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.06794', 'title': 'No More Stale Feedback: Co-Evolving Critics for Open-World Agent Learning', 'url': 'https://huggingface.co/papers/2601.06794', 'abstract': "ECHO is a reinforcement learning framework that jointly optimizes policy and critic through co-evolutionary loops, addressing staleness issues in critique-guided training of language model agents.  \t\t\t\t\tAI-generated summary \t\t\t\t Critique-guided reinforcement learning (RL) has emerged as a powerful paradigm for training LLM agents by augmenting sparse outcome rewards with natural-language feedback. However, current methods often rely on static or offline critic models, which fail to adapt as the policy evolves. In on-policy RL, the agent's error patterns shift over time, causing stationary critics to become stale and providing feedback of diminishing utility. To address this, we introduce ECHO (Evolving Critic for Hindsight-Guided Optimization)}, a framework that jointly optimizes the policy and critic through a synchronized co-evolutionary loop. ECHO utilizes a cascaded rollout mechanism where the critic generates multiple diagnoses for an initial trajectory, followed by policy refinement to enable group-structured advantage estimation. We address the challenge of learning plateaus via a saturation-aware gain shaping objective, which rewards the critic for inducing incremental improvements in high-performing trajectories. By employing dual-track GRPO updates, ECHO ensures the critic's feedback stays synchronized with the evolving policy. Experimental results show that ECHO yields more stable training and higher long-horizon task success across open-world environments.", 'score': 1, 'issue_id': 602, 'pub_date': '2026-01-11', 'pub_date_card': {'ru': '11 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 11', 'zh': '1æœˆ11æ—¥'}, 'hash': '8906ee70e37e8234', 'authors': ['Zhicong Li', 'Lingjie Jiang', 'Yulan Hu', 'Xingchen Zeng', 'Yixia Li', 'Xiangwen Zhang', 'Guanhua Chen', 'Zheng Pan', 'Xin Li', 'Yong Liu'], 'affiliations': ['Amap, Alibaba Group', 'Gaoling School of Artificial Intelligence, Renmin University of China', 'Peking University', 'Southern University of Science and Technology', 'The Hong Kong University of Science and Technology (Guangzhou)'], 'pdf_title_img': 'assets/pdf/title_img/2601.06794.jpg', 'data': {'categories': ['#rl', '#training', '#optimization', '#agents', '#reasoning'], 'emoji': 'â™»ï¸', 'ru': {'title': 'Ğ¡Ğ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ğ°Ñ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ¸ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ° Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'ECHO â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºÑƒ Ğ¸ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ° Ñ‡ĞµÑ€ĞµĞ· ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ñ†Ğ¸ĞºĞ»Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€ĞµÑˆĞ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ÑƒÑÑ‚Ğ°Ñ€ĞµĞ²Ğ°Ğ½Ğ¸Ñ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ² ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ñ… Ğ¾ÑÑ‚Ğ°Ñ‘Ñ‚ÑÑ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¼, Ğ¿Ğ¾ĞºĞ° Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ° Ğ°Ğ³ĞµĞ½Ñ‚Ğ° ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ°ÑĞºĞ°Ğ´Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾Ğ·Ğ¾Ğ² Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ¸ Ğ½Ğ°ÑÑ‹Ñ‰Ğ°ÑÑ‰ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ»Ğ°Ñ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ECHO Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ²Ñ‹ÑˆĞµ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚ÑŒ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸ÑÑ….'}, 'en': {'title': 'ECHO: Evolving Critic for Enhanced Reinforcement Learning', 'desc': "ECHO is a novel reinforcement learning framework designed to improve the training of language model agents by optimizing both the policy and the critic simultaneously. It tackles the problem of stale feedback from static critic models, which can hinder the learning process as the agent's behavior evolves. By implementing a co-evolutionary loop, ECHO allows the critic to adaptively generate multiple diagnoses for the agent's actions, leading to more effective policy refinement. This approach results in more stable training and better performance in complex tasks, as demonstrated by experimental results in open-world environments."}, 'zh': {'title': 'ECHOï¼šå…±åŒè¿›åŒ–çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶', 'desc': 'ECHOæ˜¯ä¸€ç§å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡å…±åŒè¿›åŒ–å¾ªç¯æ¥ä¼˜åŒ–ç­–ç•¥å’Œè¯„è®ºè€…ï¼Œè§£å†³äº†è¯­è¨€æ¨¡å‹ä»£ç†åœ¨è¯„è®ºæŒ‡å¯¼è®­ç»ƒä¸­çš„é™ˆæ—§æ€§é—®é¢˜ã€‚è¯¥æ¡†æ¶åˆ©ç”¨çº§è”å›æ”¾æœºåˆ¶ï¼Œè¯„è®ºè€…ä¸ºåˆå§‹è½¨è¿¹ç”Ÿæˆå¤šä¸ªè¯Šæ–­ï¼Œç„¶åè¿›è¡Œç­–ç•¥ä¼˜åŒ–ï¼Œä»¥å®ç°ç¾¤ä½“ç»“æ„çš„ä¼˜åŠ¿ä¼°è®¡ã€‚ECHOè¿˜é€šè¿‡é¥±å’Œæ„ŸçŸ¥å¢ç›Šå¡‘å½¢ç›®æ ‡æ¥åº”å¯¹å­¦ä¹ å¹³å°æœŸï¼Œå¥–åŠ±è¯„è®ºè€…åœ¨é«˜æ€§èƒ½è½¨è¿¹ä¸­å¼•å¯¼å¢é‡æ”¹è¿›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒECHOåœ¨å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­æä¾›äº†æ›´ç¨³å®šçš„è®­ç»ƒå’Œæ›´é«˜çš„é•¿æœŸä»»åŠ¡æˆåŠŸç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.09282', 'title': 'Cluster Workload Allocation: Semantic Soft Affinity Using Natural Language Processing', 'url': 'https://huggingface.co/papers/2601.09282', 'abstract': 'A semantic, intent-driven scheduling approach uses large language models to interpret natural language hints for workload allocation in cluster systems, achieving high accuracy and improved placement compared to traditional methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Cluster workload allocation often requires complex configurations, creating a usability gap. This paper introduces a semantic, intent-driven scheduling paradigm for cluster systems using Natural Language Processing. The system employs a Large Language Model (LLM) integrated via a Kubernetes scheduler extender to interpret natural language allocation hint annotations for soft affinity preferences. A prototype featuring a cluster state cache and an intent analyzer (using AWS Bedrock) was developed. Empirical evaluation demonstrated high LLM parsing accuracy (>95% Subset Accuracy on an evaluation ground-truth dataset) for top-tier models like Amazon Nova Pro/Premier and Mistral Pixtral Large, significantly outperforming a baseline engine. Scheduling quality tests across six scenarios showed the prototype achieved superior or equivalent placement compared to standard Kubernetes configurations, particularly excelling in complex and quantitative scenarios and handling conflicting soft preferences. The results validate using LLMs for accessible scheduling but highlight limitations like synchronous LLM latency, suggesting asynchronous processing for production readiness. This work confirms the viability of semantic soft affinity for simplifying workload orchestration.', 'score': 0, 'issue_id': 600, 'pub_date': '2026-01-14', 'pub_date_card': {'ru': '14 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 14', 'zh': '1æœˆ14æ—¥'}, 'hash': '647b1195f0fea881', 'authors': ['Leszek Sliwko', 'Jolanta Mizeria-Pietraszko'], 'affiliations': ['Department of Computer Science, Opole University of Technology', 'Standard Chartered Bank'], 'pdf_title_img': 'assets/pdf/title_img/2601.09282.jpg', 'data': {'categories': ['#training'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ¡ĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ ÑĞ·Ñ‹Ğº Ğ¸ LLM', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ñ… Ğ½Ğ°Ğ³Ñ€ÑƒĞ·Ğ¾Ğº Ğ² ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ°Ñ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ LLM Ğ² Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº Kubernetes Ñ‡ĞµÑ€ĞµĞ· Ñ€Ğ°ÑÑˆĞ¸Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ğ¼Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸. ĞŸÑ€Ğ¾Ñ‚Ğ¾Ñ‚Ğ¸Ğ¿ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ» Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ°Ñ€ÑĞ¸Ğ½Ğ³Ğ° (Ğ±Ğ¾Ğ»ĞµĞµ 95%) Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ» Ğ»ÑƒÑ‡ÑˆĞµĞµ Ğ¸Ğ»Ğ¸ ÑĞºĞ²Ğ¸Ğ²Ğ°Ğ»ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‰ĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ ÑĞ¾ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Kubernetes. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ LLM Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ¾Ñ‰ĞµĞ½Ğ¸Ñ Ğ¾Ñ€ĞºĞµÑÑ‚Ñ€Ğ¾Ğ²ĞºĞ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ñ… Ğ½Ğ°Ğ³Ñ€ÑƒĞ·Ğ¾Ğº, Ñ…Ğ¾Ñ‚Ñ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ°ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ»Ñ production-Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Revolutionizing Cluster Scheduling with Language Understanding', 'desc': 'This paper presents a new scheduling method for cluster systems that uses large language models (LLMs) to understand natural language hints for workload allocation. By integrating Natural Language Processing, the system can interpret user preferences and improve the placement of workloads compared to traditional scheduling methods. The prototype developed shows high accuracy in parsing user intents and performs well in various scheduling scenarios, especially when dealing with complex configurations. However, the study also notes challenges such as the latency of LLMs, suggesting that asynchronous processing could enhance performance in real-world applications.'}, 'zh': {'title': 'è¯­ä¹‰é©±åŠ¨çš„æ™ºèƒ½è°ƒåº¦ï¼Œç®€åŒ–é›†ç¾¤å·¥ä½œè´Ÿè½½åˆ†é…', 'desc': 'è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºè¯­ä¹‰å’Œæ„å›¾é©±åŠ¨çš„è°ƒåº¦æ–¹æ³•ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥è§£æè‡ªç„¶è¯­è¨€æç¤ºï¼Œä»è€Œåœ¨é›†ç¾¤ç³»ç»Ÿä¸­è¿›è¡Œå·¥ä½œè´Ÿè½½åˆ†é…ã€‚è¯¥ç³»ç»Ÿé€šè¿‡Kubernetesè°ƒåº¦å™¨æ‰©å±•å™¨é›†æˆLLMï¼Œèƒ½å¤Ÿç†è§£è‡ªç„¶è¯­è¨€ä¸­çš„è½¯äº²å’Œæ€§åå¥½æ³¨é‡Šã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨é¡¶çº§æ¨¡å‹ï¼ˆå¦‚Amazon Nova Pro/Premierå’ŒMistral Pixtral Largeï¼‰æ—¶ï¼ŒLLMçš„è§£æå‡†ç¡®ç‡è¶…è¿‡95%ï¼Œæ˜¾è‘—ä¼˜äºä¼ ç»ŸåŸºçº¿å¼•æ“ã€‚ç ”ç©¶è¿˜è¡¨æ˜ï¼Œè¯¥åŸå‹åœ¨å¤æ‚åœºæ™¯ä¸‹çš„è°ƒåº¦è´¨é‡ä¼˜äºæ ‡å‡†Kubernetesé…ç½®ï¼ŒéªŒè¯äº†ä½¿ç”¨LLMç®€åŒ–å·¥ä½œè´Ÿè½½è°ƒåº¦çš„å¯è¡Œæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.08472', 'title': 'sui-1: Grounded and Verifiable Long-Form Summarization', 'url': 'https://huggingface.co/papers/2601.08472', 'abstract': 'A 24 billion parameter model generates abstractive summaries with inline citations through synthetic data training, outperforming larger models in accuracy and verifiability.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models frequently generate plausible but unfaithful summaries that users cannot verify against source text, a critical limitation in compliance-sensitive domains such as government and legal analysis. We present sui-1, a 24B parameter model that produces abstractive summaries with inline citations, enabling users to trace each claim to its source sentence. Our synthetic data pipeline combines chain-of-thought prompting with multi-stage verification, generating over 22,000 high-quality training examples across five languages from diverse sources including parliamentary documents, web text, and Wikipedia. Evaluation shows sui-1 significantly outperforms all tested open-weight baselines, including models with 3x more parameters. These results demonstrate that task-specific training substantially outperforms scale alone for citation-grounded summarization. Model weights and an interactive demo are publicly available.', 'score': 0, 'issue_id': 605, 'pub_date': '2026-01-13', 'pub_date_card': {'ru': '13 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 13', 'zh': '1æœˆ13æ—¥'}, 'hash': 'e6ef600baa6d38f3', 'authors': ['Benedikt Droste', 'Jan Philipp Harries', 'Maximilian Idahl', 'BjÃ¶rn PlÃ¼ster'], 'affiliations': ['ellamind'], 'pdf_title_img': 'assets/pdf/title_img/2601.08472.jpg', 'data': {'categories': [], 'emoji': 'ğŸ“š', 'ru': {'title': 'ĞšĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ°Ğ¼Ğ¸ Ğ»ÑƒÑ‡ÑˆĞµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½ÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸Ğ· 24 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑĞ¼Ğµ Ñ Ğ²ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ†Ğ¸Ñ‚Ğ°Ñ‚Ğ°Ğ¼Ğ¸, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ ĞºĞ°Ğ¶Ğ´Ğ¾Ğµ ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ñƒ Ñ‚ĞµĞºÑÑ‚Ñƒ. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ»ÑÑ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸Ğ· 22 Ñ‚Ñ‹ÑÑÑ‡ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ½Ğ° Ğ¿ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ°Ñ…, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğ¹ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ sui-1 Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ²ÑĞµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ² Ñ‚Ñ€Ğ¸ Ñ€Ğ°Ğ·Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°, Ñ‡ĞµĞ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ ĞµÑ‘ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ†Ğ¸Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'Citations Matter: Trustworthy Summaries with sui-1', 'desc': 'The paper introduces sui-1, a 24 billion parameter model designed to create abstractive summaries that include inline citations, enhancing the verifiability of generated content. Unlike larger models that may produce plausible but untrustworthy summaries, sui-1 allows users to trace claims back to their original sources, which is crucial for compliance in sensitive fields like law and government. The model is trained using a synthetic data pipeline that employs chain-of-thought prompting and multi-stage verification, resulting in over 22,000 high-quality examples in multiple languages. Evaluation results indicate that sui-1 outperforms existing models, even those with significantly more parameters, highlighting the importance of task-specific training over sheer model size.'}, 'zh': {'title': 'ç²¾å‡†æ‘˜è¦ï¼Œå¯ä¿¡å¼•ç”¨çš„æœªæ¥', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºsui-1çš„æ¨¡å‹ï¼Œå®ƒæ‹¥æœ‰240äº¿ä¸ªå‚æ•°ï¼Œèƒ½å¤Ÿç”Ÿæˆå¸¦æœ‰å†…è”å¼•ç”¨çš„æŠ½è±¡æ‘˜è¦ã€‚è¯¥æ¨¡å‹é€šè¿‡åˆæˆæ•°æ®è®­ç»ƒï¼Œç»“åˆæ€ç»´é“¾æç¤ºå’Œå¤šé˜¶æ®µéªŒè¯ï¼Œç”Ÿæˆäº†è¶…è¿‡22,000ä¸ªé«˜è´¨é‡çš„è®­ç»ƒç¤ºä¾‹ã€‚ä¸å…¶ä»–å¤§å‹æ¨¡å‹ç›¸æ¯”ï¼Œsui-1åœ¨å‡†ç¡®æ€§å’Œå¯éªŒè¯æ€§æ–¹é¢è¡¨ç°æ›´ä½³ï¼Œå°¤å…¶é€‚ç”¨äºæ”¿åºœå’Œæ³•å¾‹åˆ†æç­‰åˆè§„æ•æ„Ÿé¢†åŸŸã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œé’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„è®­ç»ƒæ˜¾è‘—ä¼˜äºä»…ä»…å¢åŠ æ¨¡å‹è§„æ¨¡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.04469', 'title': 'SampoNLP: A Self-Referential Toolkit for Morphological Analysis of Subword Tokenizers', 'url': 'https://huggingface.co/papers/2601.04469', 'abstract': 'A corpus-free toolkit for morphological lexicon creation using MDL-inspired scoring enables systematic evaluation of BPE tokenizers for morphologically rich Uralic languages, establishing optimal vocabulary sizes through integrated performance metrics.  \t\t\t\t\tAI-generated summary \t\t\t\t The quality of subword tokenization is critical for Large Language Models, yet evaluating tokenizers for morphologically rich Uralic languages is hampered by the lack of clean morpheme lexicons.   We introduce SampoNLP, a corpus-free toolkit for morphological lexicon creation using MDL-inspired Self-Referential Atomicity Scoring, which filters composite forms through internal structural cues - suited for low-resource settings.   Using the high-purity lexicons generated by SampoNLP for Finnish, Hungarian, and Estonian, we conduct a systematic evaluation of BPE tokenizers across a range of vocabulary sizes (8k-256k). We propose a unified metric, the Integrated Performance Score (IPS), to navigate the trade-off between morpheme coverage and over-splitting. By analyzing the IPS curves, we identify the "elbow points" of diminishing returns and provide the first empirically grounded recommendations for optimal vocabulary sizes (k) in these languages. Our study not only offers practical guidance but also quantitatively demonstrates the limitations of standard BPE for highly agglutinative languages. The SampoNLP library and all generated resources are made publicly available: https://github.com/AragonerUA/SampoNLP', 'score': 0, 'issue_id': 601, 'pub_date': '2026-01-08', 'pub_date_card': {'ru': '8 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 8', 'zh': '1æœˆ8æ—¥'}, 'hash': '031f50560f1bbeab', 'authors': ['Iaroslav Chelombitko', 'Ekaterina Chelombitko', 'Aleksey Komissarov'], 'affiliations': ['DataSpike', 'Neapolis University Pafos', 'aglabx'], 'pdf_title_img': 'assets/pdf/title_img/2601.04469.jpg', 'data': {'categories': ['#multilingual', '#open_source', '#plp', '#dataset', '#low_resource', '#benchmark', '#data'], 'emoji': 'ğŸ”¤', 'ru': {'title': 'ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ»Ñ ÑƒÑ€Ğ°Ğ»ÑŒÑĞºĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ¾Ñ€Ñ„Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ»ĞµĞºÑĞ¸ĞºĞ¾Ğ½Ñ‹', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° SampoNLP â€” Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞºĞ° Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ñ€Ñ„Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ»ĞµĞºÑĞ¸ĞºĞ¾Ğ½Ğ¾Ğ² Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ MDL-Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Self-Referential Atomicity Scoring. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ‡Ğ¸ÑÑ‚Ñ‹Ñ… Ğ»ĞµĞºÑĞ¸ĞºĞ¾Ğ½Ğ¾Ğ² Ñ„Ğ¸Ğ½ÑĞºĞ¾Ğ³Ğ¾, Ğ²ĞµĞ½Ğ³ĞµÑ€ÑĞºĞ¾Ğ³Ğ¾ Ğ¸ ÑÑÑ‚Ğ¾Ğ½ÑĞºĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ¾Ğ² Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ BPE-Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°Ñ… ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ Ğ¾Ñ‚ 8k Ğ´Ğ¾ 256k Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². Ğ’Ğ²ĞµĞ´ĞµĞ½Ğ° ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ° Integrated Performance Score (IPS) Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ğ¾ĞºÑ€Ñ‹Ñ‚Ğ¸ĞµĞ¼ Ğ¼Ğ¾Ñ€Ñ„ĞµĞ¼ Ğ¸ Ğ¸Ñ… Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ñ‹ ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ Ğ´Ğ»Ñ Ğ¼Ğ¾Ñ€Ñ„Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ±Ğ¾Ğ³Ğ°Ñ‚Ñ‹Ñ… ÑƒÑ€Ğ°Ğ»ÑŒÑĞºĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ² Ğ¸ Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¾ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğ³Ğ¾ BPE Ğ´Ğ»Ñ Ğ°Ğ³Ğ³Ğ»ÑÑ‚Ğ¸Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ².'}, 'en': {'title': 'Optimizing Tokenization for Uralic Languages with SampoNLP', 'desc': 'This paper presents SampoNLP, a toolkit designed to create morphological lexicons without relying on existing corpora, specifically for Uralic languages like Finnish, Hungarian, and Estonian. It employs a scoring method inspired by Minimum Description Length (MDL) to evaluate and filter composite word forms based on their internal structure. The study systematically assesses Byte Pair Encoding (BPE) tokenizers by generating high-quality lexicons and introducing a new metric called Integrated Performance Score (IPS) to balance morpheme coverage and the risk of over-splitting. The findings provide valuable insights into optimal vocabulary sizes for these morphologically rich languages, highlighting the challenges of using standard BPE methods in agglutinative contexts.'}, 'zh': {'title': 'ä¼˜åŒ–ä¹Œæ‹‰å°”è¯­è¨€çš„å½¢æ€å­¦è¯å…¸åˆ›å»º', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºSampoNLPçš„å·¥å…·åŒ…ï¼Œç”¨äºåˆ›å»ºå½¢æ€å­¦è¯å…¸ï¼Œç‰¹åˆ«é€‚ç”¨äºèµ„æºåŒ®ä¹çš„ä¹Œæ‹‰å°”è¯­è¨€ã€‚è¯¥å·¥å…·åŒ…é‡‡ç”¨MDLå¯å‘å¼è¯„åˆ†æ–¹æ³•ï¼Œé€šè¿‡å†…éƒ¨ç»“æ„çº¿ç´¢è¿‡æ»¤å¤åˆå½¢å¼ï¼Œä»è€Œç”Ÿæˆé«˜çº¯åº¦çš„è¯å…¸ã€‚æˆ‘ä»¬å¯¹èŠ¬å…°è¯­ã€åŒˆç‰™åˆ©è¯­å’Œçˆ±æ²™å°¼äºšè¯­çš„BPEåˆ†è¯å™¨è¿›è¡Œäº†ç³»ç»Ÿè¯„ä¼°ï¼Œå¹¶æå‡ºäº†ç»¼åˆæ€§èƒ½è¯„åˆ†ï¼ˆIPSï¼‰ä½œä¸ºè¯„ä¼°è¯æ±‡å¤§å°çš„ç»Ÿä¸€æŒ‡æ ‡ã€‚ç ”ç©¶ç»“æœä¸ºè¿™äº›è¯­è¨€çš„æœ€ä½³è¯æ±‡å¤§å°æä¾›äº†å®è¯åŸºç¡€ï¼Œå¹¶å±•ç¤ºäº†æ ‡å‡†BPEåœ¨é«˜åº¦ç²˜åˆè¯­è¨€ä¸­çš„å±€é™æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.05432', 'title': 'Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization', 'url': 'https://huggingface.co/papers/2601.05432', 'abstract': 'Large vision-language models are enhanced for image geolocalization by incorporating map-based reasoning and agent-in-the-map loop optimization, achieving superior accuracy compared to existing models.  \t\t\t\t\tAI-generated summary \t\t\t\t The image geolocalization task aims to predict the location where an image was taken anywhere on Earth using visual clues. Existing large vision-language model (LVLM) approaches leverage world knowledge, chain-of-thought reasoning, and agentic capabilities, but overlook a common strategy used by humans -- using maps. In this work, we first equip the model Thinking with Map ability and formulate it as an agent-in-the-map loop. We develop a two-stage optimization scheme for it, including agentic reinforcement learning (RL) followed by parallel test-time scaling (TTS). The RL strengthens the agentic capability of model to improve sampling efficiency, and the parallel TTS enables the model to explore multiple candidate paths before making the final prediction, which is crucial for geolocalization. To evaluate our method on up-to-date and in-the-wild images, we further present MAPBench, a comprehensive geolocalization training and evaluation benchmark composed entirely of real-world images. Experimental results show that our method outperforms existing open- and closed-source models on most metrics, specifically improving Acc@500m from 8.0\\% to 22.1\\% compared to Gemini-3-Pro with Google Search/Map grounded mode.', 'score': 129, 'issue_id': 522, 'pub_date': '2026-01-08', 'pub_date_card': {'ru': '8 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 8', 'zh': '1æœˆ8æ—¥'}, 'hash': '6130091a1c7cf83b', 'authors': ['Yuxiang Ji', 'Yong Wang', 'Ziyu Ma', 'Yiming Hu', 'Hailang Huang', 'Xuecai Hu', 'Guanhua Chen', 'Liaoni Wu', 'Xiangxiang Chu'], 'affiliations': ['AMAP, Alibaba Group', 'Southern University of Science and Technology', 'Xiamen University'], 'pdf_title_img': 'assets/pdf/title_img/2601.05432.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#agents', '#cv', '#dataset', '#multimodal', '#rl', '#open_source', '#reasoning'], 'emoji': 'ğŸ—ºï¸', 'ru': {'title': 'ĞšĞ°Ñ€Ñ‚Ñ‹ ĞºĞ°Ğº ĞºĞ¾Ğ¼Ğ¿Ğ°Ñ: Ğ½Ğ°ÑƒÑ‡Ğ¸Ğ¼ AI Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ ÑĞµĞ±Ñ Ğ½Ğ° Ğ¿Ğ»Ğ°Ğ½ĞµÑ‚Ğµ', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ³ĞµĞ¾Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ°Ñ€Ñ‚ Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾ÑĞ½Ğ°Ñ‰ĞµĞ½Ğ° ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ ĞºĞ°Ñ€Ñ‚Ğ°Ğ¼Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ñ†Ğ¸ĞºĞ» Â«Ğ°Ğ³ĞµĞ½Ñ‚-Ğ²-ĞºĞ°Ñ€Ñ‚ĞµÂ», Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ Ñ‚Ğ°Ğº Ğ¶Ğµ, ĞºĞ°Ğº Ğ»ÑĞ´Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¸ Ğ¼ĞµÑÑ‚Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ. Ğ”Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ°Ñ ÑÑ…ĞµĞ¼Ğ°: Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ğ´Ğ»Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ³Ğ¸Ğ¿Ğ¾Ñ‚ĞµĞ·. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº MAPBench Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ğ´Ğ¾Ğ±Ğ¸Ğ»Ğ¸ÑÑŒ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸ÑĞ¼Ğ¸.'}, 'en': {'title': 'Mapping the Future of Image Geolocalization', 'desc': "This paper presents an advanced approach to image geolocalization by integrating map-based reasoning into large vision-language models (LVLMs). The authors introduce a novel agent-in-the-map loop optimization that enhances the model's ability to utilize maps, a strategy often employed by humans. They implement a two-stage optimization process that includes reinforcement learning to boost the model's agentic capabilities and parallel test-time scaling for efficient path exploration. The proposed method significantly improves accuracy on real-world images, as demonstrated by their new benchmark, MAPBench, achieving notable performance gains over existing models."}, 'zh': {'title': 'åœ°å›¾æ€ç»´åŠ©åŠ›å›¾åƒåœ°ç†å®šä½', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§å¢å¼ºçš„å¤§å‹è§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œç”¨äºå›¾åƒåœ°ç†å®šä½ï¼Œé€šè¿‡å¼•å…¥åŸºäºåœ°å›¾çš„æ¨ç†å’Œåœ°å›¾ä¸­çš„ä»£ç†å¾ªç¯ä¼˜åŒ–ï¼Œæ˜¾è‘—æé«˜äº†å‡†ç¡®æ€§ã€‚å›¾åƒåœ°ç†å®šä½ä»»åŠ¡æ—¨åœ¨åˆ©ç”¨è§†è§‰çº¿ç´¢é¢„æµ‹å›¾åƒæ‹æ‘„åœ°ç‚¹ï¼Œç°æœ‰æ¨¡å‹é€šå¸¸å¿½è§†äº†äººç±»å¸¸ç”¨çš„åœ°å›¾ç­–ç•¥ã€‚æˆ‘ä»¬ä¸ºæ¨¡å‹èµ‹äºˆäº†åœ°å›¾æ€ç»´èƒ½åŠ›ï¼Œå¹¶å°†å…¶å½¢å¼åŒ–ä¸ºä»£ç†-åœ°å›¾å¾ªç¯ï¼Œé‡‡ç”¨äº†ä¸¤é˜¶æ®µä¼˜åŒ–æ–¹æ¡ˆï¼ŒåŒ…æ‹¬ä»£ç†å¼ºåŒ–å­¦ä¹ å’Œå¹¶è¡Œæµ‹è¯•æ—¶é—´ç¼©æ”¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤§å¤šæ•°æŒ‡æ ‡ä¸Šè¶…è¶Šäº†ç°æœ‰æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯åœ¨500ç±³å‡†ç¡®ç‡ä¸Šä»8.0%æå‡è‡³22.1%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.03017', 'title': 'MMFormalizer: Multimodal Autoformalization in the Wild', 'url': 'https://huggingface.co/papers/2601.03017', 'abstract': 'MMFormalizer enables multimodal autoformalization by integrating visual perception with formal mathematical reasoning, supporting complex physical domains from classical mechanics to quantum mechanics.  \t\t\t\t\tAI-generated summary \t\t\t\t Autoformalization, which translates natural language mathematics into formal statements to enable machine reasoning, faces fundamental challenges in the wild due to the multimodal nature of the physical world, where physics requires inferring hidden constraints (e.g., mass or energy) from visual elements. To address this, we propose MMFormalizer, which extends autoformalization beyond text by integrating adaptive grounding with entities from real-world mathematical and physical domains. MMFormalizer recursively constructs formal propositions from perceptually grounded primitives through recursive grounding and axiom composition, with adaptive recursive termination ensuring that every abstraction is supported by visual evidence and anchored in dimensional or axiomatic grounding. We evaluate MMFormalizer on a new benchmark, PhyX-AF, comprising 115 curated samples from MathVerse, PhyX, Synthetic Geometry, and Analytic Geometry, covering diverse multimodal autoformalization tasks. Results show that frontier models such as GPT-5 and Gemini-3-Pro achieve the highest compile and semantic accuracy, with GPT-5 excelling in physical reasoning, while geometry remains the most challenging domain. Overall, MMFormalizer provides a scalable framework for unified multimodal autoformalization, bridging perception and formal reasoning. To the best of our knowledge, this is the first multimodal autoformalization method capable of handling classical mechanics (derived from the Hamiltonian), as well as relativity, quantum mechanics, and thermodynamics. More details are available on our project page: MMFormalizer.github.io', 'score': 94, 'issue_id': 525, 'pub_date': '2026-01-06', 'pub_date_card': {'ru': '6 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 6', 'zh': '1æœˆ6æ—¥'}, 'hash': '3e0ad6513065ae3a', 'authors': ['Jing Xiong', 'Qi Han', 'Yunta Hsieh', 'Hui Shen', 'Huajian Xin', 'Chaofan Tao', 'Chenyang Zhao', 'Hengyuan Zhang', 'Taiqiang Wu', 'Zhen Zhang', 'Haochen Wang', 'Zhongwei Wan', 'Lingpeng Kong', 'Ngai Wong'], 'affiliations': ['Ohio State University', 'The University of Hong Kong', 'University of California, Los Angeles', 'University of California, Santa Barbara', 'University of Edinburgh', 'University of Michigan, Ann Arbor'], 'pdf_title_img': 'assets/pdf/title_img/2601.03017.jpg', 'data': {'categories': ['#math', '#dataset', '#benchmark', '#reasoning', '#science', '#multimodal'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'Ğ’Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ Ğ²ÑÑ‚Ñ€ĞµÑ‡Ğ°ĞµÑ‚ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ°: Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ„Ğ¸Ğ·Ğ¸ĞºĞ¸ Ğ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ¸', 'desc': 'MMFormalizer â€” ÑÑ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ Ñ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ñ€ĞµĞºÑƒÑ€ÑĞ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ· Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ğ²Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ğµ Ñ€ĞµĞºÑƒÑ€ÑĞ¸Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ ÑĞ²ÑĞ·Ğ¸ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ†Ğ¸Ğ¸ Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº PhyX-AF Ñ 115 Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ğ¼Ğ¸ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ¾Ğ¼ĞµĞ½Ğ¾Ğ², Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸ĞºÑƒ, ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²ÑƒÑ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸ĞºÑƒ Ğ¸ Ñ‚ĞµÑ€Ğ¼Ğ¾Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºÑƒ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ LLM-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² ĞºĞ¾Ğ¼Ğ¿Ğ¸Ğ»ÑÑ†Ğ¸Ğ¸ Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞµ, Ñ…Ğ¾Ñ‚Ñ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ Ğ¾ÑÑ‚Ğ°Ñ‘Ñ‚ÑÑ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑŒÑ.'}, 'en': {'title': 'Bridging Visual Perception and Mathematical Reasoning with MMFormalizer', 'desc': 'MMFormalizer is a novel framework that combines visual perception with formal mathematical reasoning to enhance autoformalization across various physical domains. It addresses the challenges of translating natural language mathematics into formal statements by integrating adaptive grounding with real-world entities. The system constructs formal propositions from visual elements through recursive grounding and axiom composition, ensuring that each abstraction is supported by visual evidence. Evaluated on the PhyX-AF benchmark, MMFormalizer demonstrates superior performance in physical reasoning tasks, marking a significant advancement in multimodal autoformalization capabilities.'}, 'zh': {'title': 'å¤šæ¨¡æ€è‡ªåŠ¨å½¢å¼åŒ–çš„æ¡¥æ¢', 'desc': 'MMFormalizer æ˜¯ä¸€ç§å¤šæ¨¡æ€è‡ªåŠ¨å½¢å¼åŒ–å·¥å…·ï¼Œå®ƒå°†è§†è§‰æ„ŸçŸ¥ä¸å½¢å¼æ•°å­¦æ¨ç†ç›¸ç»“åˆï¼Œæ”¯æŒä»ç»å…¸åŠ›å­¦åˆ°é‡å­åŠ›å­¦çš„å¤æ‚ç‰©ç†é¢†åŸŸã€‚è¯¥æ–¹æ³•é€šè¿‡é€‚åº”æ€§åŸºç¡€ä¸ç°å®ä¸–ç•Œçš„æ•°å­¦å’Œç‰©ç†å®ä½“æ•´åˆï¼Œè¶…è¶Šäº†æ–‡æœ¬çš„è‡ªåŠ¨å½¢å¼åŒ–ã€‚MMFormalizer é€šè¿‡é€’å½’åŸºç¡€å’Œå…¬ç†ç»„åˆï¼Œä»æ„ŸçŸ¥åŸºç¡€æ„å»ºå½¢å¼å‘½é¢˜ï¼Œç¡®ä¿æ¯ä¸ªæŠ½è±¡éƒ½æœ‰è§†è§‰è¯æ®æ”¯æŒã€‚æˆ‘ä»¬çš„è¯„ä¼°è¡¨æ˜ï¼ŒMMFormalizer åœ¨å¤šæ¨¡æ€è‡ªåŠ¨å½¢å¼åŒ–ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶åœ¨ç‰©ç†æ¨ç†æ–¹é¢ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤„ç†ç»å…¸åŠ›å­¦å’Œé‡å­åŠ›å­¦ç­‰é¢†åŸŸçš„èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.03319', 'title': 'CaricatureGS: Exaggerating 3D Gaussian Splatting Faces With Gaussian Curvature', 'url': 'https://huggingface.co/papers/2601.03319', 'abstract': 'A photorealistic 3D caricaturization framework combines Gaussian curvature-based surface exaggeration with 3D Gaussian Splatting to create controllable, realistic avatars with improved fidelity and real-time deformation capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t A photorealistic and controllable 3D caricaturization framework for faces is introduced. We start with an intrinsic Gaussian curvature-based surface exaggeration technique, which, when coupled with texture, tends to produce over-smoothed renders. To address this, we resort to 3D Gaussian Splatting (3DGS), which has recently been shown to produce realistic free-viewpoint avatars. Given a multiview sequence, we extract a FLAME mesh, solve a curvature-weighted Poisson equation, and obtain its exaggerated form. However, directly deforming the Gaussians yields poor results, necessitating the synthesis of pseudo-ground-truth caricature images by warping each frame to its exaggerated 2D representation using local affine transformations. We then devise a training scheme that alternates real and synthesized supervision, enabling a single Gaussian collection to represent both natural and exaggerated avatars. This scheme improves fidelity, supports local edits, and allows continuous control over the intensity of the caricature. In order to achieve real-time deformations, an efficient interpolation between the original and exaggerated surfaces is introduced. We further analyze and show that it has a bounded deviation from closed-form solutions. In both quantitative and qualitative evaluations, our results outperform prior work, delivering photorealistic, geometry-controlled caricature avatars.', 'score': 45, 'issue_id': 525, 'pub_date': '2026-01-06', 'pub_date_card': {'ru': '6 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 6', 'zh': '1æœˆ6æ—¥'}, 'hash': '10d82cf2b9d195c1', 'authors': ['Eldad Matmon', 'Amit Bracha', 'Noam Rotstein', 'Ron Kimmel'], 'affiliations': ['Technion Israel Institute of Technology, Haifa, Israel'], 'pdf_title_img': 'assets/pdf/title_img/2601.03319.jpg', 'data': {'categories': ['#multimodal', '#3d'], 'emoji': 'ğŸ¤ª', 'ru': {'title': 'Ğ¤Ğ¾Ñ‚Ğ¾Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ 3D-ĞºĞ°Ñ€Ğ¸ĞºĞ°Ñ‚ÑƒÑ€Ñ‹ Ñ‡ĞµÑ€ĞµĞ· Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹ ÑĞ¿Ğ»Ğ°Ñ‚Ñ‚Ğ¸Ğ½Ğ³ Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ĞºÑ€Ğ¸Ğ²Ğ¸Ğ·Ğ½Ğ¾Ğ¹', 'desc': 'ĞŸÑ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ„Ğ¾Ñ‚Ğ¾Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑˆĞ°Ñ€Ğ¶ĞµĞ¹ Ğ»Ğ¸Ñ† Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ 3D-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºÑ€Ğ¸Ğ²Ğ¸Ğ·Ğ½Ñƒ Ğ“Ğ°ÑƒÑÑĞ° Ğ´Ğ»Ñ Ğ¿Ñ€ĞµÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€Ñ‚ Ğ»Ğ¸Ñ†Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ 3D Gaussian Splatting Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ°Ğ²Ğ°Ñ‚Ğ°Ñ€Ğ¾Ğ² Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ, Ğ¸Ğ·Ğ±ĞµĞ³Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ‡Ñ€ĞµĞ·Ğ¼ĞµÑ€Ğ½Ğ¾Ğ¹ ÑĞ³Ğ»Ğ°Ğ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ñ‡ĞµÑ€ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ñƒ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñƒ Ğ³Ğ°ÑƒÑÑĞ¸Ğ°Ğ½Ğ¾Ğ² Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑ‚ÑŒ ĞºĞ°Ğº ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ, Ñ‚Ğ°Ğº Ğ¸ ĞºĞ°Ñ€Ğ¸ĞºĞ°Ñ‚ÑƒÑ€Ğ½Ñ‹Ğµ Ğ²ĞµÑ€ÑĞ¸Ğ¸ Ğ°Ğ²Ğ°Ñ‚Ğ°Ñ€Ğ°. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¸Ğ½Ñ‚ĞµĞ½ÑĞ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ ĞºĞ°Ñ€Ğ¸ĞºĞ°Ñ‚ÑƒÑ€Ñ‹ Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ´ĞµÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸.'}, 'en': {'title': 'Real-Time Control of Photorealistic 3D Caricatures', 'desc': 'This paper presents a new framework for creating photorealistic 3D caricatures of faces that allows for real-time control and deformation. It combines Gaussian curvature-based surface exaggeration with 3D Gaussian Splatting to enhance the realism and fidelity of the avatars. The method involves extracting a FLAME mesh from multiview sequences and applying a curvature-weighted Poisson equation to achieve exaggerated forms. A novel training approach that alternates between real and synthesized images enables the framework to effectively represent both realistic and exaggerated avatars, resulting in superior performance compared to previous methods.'}, 'zh': {'title': 'é€¼çœŸçš„3Dæ¼«ç”»åŒ–å¤´åƒç”Ÿæˆ', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§é€¼çœŸçš„3Dæ¼«ç”»åŒ–æ¡†æ¶ï¼Œç»“åˆäº†åŸºäºé«˜æ–¯æ›²ç‡çš„è¡¨é¢å¤¸å¼ å’Œ3Dé«˜æ–¯ç‚¹äº‘æŠ€æœ¯ï¼Œèƒ½å¤Ÿåˆ›å»ºå¯æ§çš„çœŸå®å¤´åƒã€‚è¯¥æ–¹æ³•é¦–å…ˆä½¿ç”¨é«˜æ–¯æ›²ç‡è¿›è¡Œè¡¨é¢å¤¸å¼ ï¼Œä½†åœ¨ä¸çº¹ç†ç»“åˆæ—¶å¯èƒ½å¯¼è‡´è¿‡äºå¹³æ»‘çš„æ¸²æŸ“æ•ˆæœã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œç ”ç©¶è€…é‡‡ç”¨äº†3Dé«˜æ–¯ç‚¹äº‘æŠ€æœ¯ï¼Œèƒ½å¤Ÿç”ŸæˆçœŸå®çš„å¤šè§†è§’å¤´åƒã€‚é€šè¿‡äº¤æ›¿ä½¿ç”¨çœŸå®å’Œåˆæˆçš„ç›‘ç£è®­ç»ƒï¼Œè¯¥æ¡†æ¶æé«˜äº†å¤´åƒçš„çœŸå®æ„Ÿï¼Œæ”¯æŒå±€éƒ¨ç¼–è¾‘ï¼Œå¹¶å…è®¸å¯¹æ¼«ç”»æ•ˆæœçš„å¼ºåº¦è¿›è¡Œè¿ç»­æ§åˆ¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.06002', 'title': 'The Molecular Structure of Thought: Mapping the Topology of Long Chain-of-Thought Reasoning', 'url': 'https://huggingface.co/papers/2601.06002', 'abstract': 'Large language models struggle with long chain-of-thought reasoning due to unstable structural patterns, but a molecular-inspired approach using effective semantic isomers and distribution-transfer-graph methods improves training stability and performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) often fail to learn effective long chain-of-thought (Long CoT) reasoning from human or non-Long-CoT LLMs imitation. To understand this, we propose that effective and learnable Long CoT trajectories feature stable molecular-like structures in unified view, which are formed by three interaction types: Deep-Reasoning (covalent-like), Self-Reflection (hydrogen-bond-like), and Self-Exploration (van der Waals-like). Analysis of distilled trajectories reveals these structures emerge from Long CoT fine-tuning, not keyword imitation. We introduce Effective Semantic Isomers and show that only bonds promoting fast entropy convergence support stable Long CoT learning, while structural competition impairs training. Drawing on these findings, we present Mole-Syn, a distribution-transfer-graph method that guides synthesis of effective Long CoT structures, boosting performance and RL stability across benchmarks.', 'score': 37, 'issue_id': 522, 'pub_date': '2026-01-09', 'pub_date_card': {'ru': '9 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 9', 'zh': '1æœˆ9æ—¥'}, 'hash': '3f57cbb3429682eb', 'authors': ['Qiguang Chen', 'Yantao Du', 'Ziniu Li', 'Jinhao Liu', 'Songyao Duan', 'Jiarui Guo', 'Minghao Liu', 'Jiaheng Liu', 'Tong Yang', 'Ge Zhang', 'Libo Qin', 'Wanxiang Che', 'Wenhao Huang'], 'affiliations': ['2077AI Foundation', 'ByteDance Seed China', 'Central South University', 'LARG, SCIR, Harbin Institute of Technology', 'M-A-P', 'Nanjing University', 'Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2601.06002.jpg', 'data': {'categories': ['#architecture', '#reasoning', '#optimization', '#training'], 'emoji': 'ğŸ§¬', 'ru': {'title': 'ĞœĞ¾Ğ»ĞµĞºÑƒĞ»ÑÑ€Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² LLM', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ°Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸Ğ·ÑƒÑÑ‚ÑÑ Ğ½ĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ»ÑÑ€Ğ½Ğ¾-Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ĞºĞ°Ğº ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ñ Ñ‚Ñ€ĞµĞ¼Ñ Ñ‚Ğ¸Ğ¿Ğ°Ğ¼Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹: Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ, ÑĞ°Ğ¼Ğ¾Ğ¾Ñ‚Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¸ ÑĞ°Ğ¼Ğ¾Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ. ĞĞ½Ğ¸ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ÑĞ²ÑĞ·Ğ¸, ÑĞ¿Ğ¾ÑĞ¾Ğ±ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ¹ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸, Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ÑÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ. ĞœĞµÑ‚Ğ¾Ğ´ Mole-Syn Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³Ñ€Ğ°Ñ„ Ñ‚Ñ€Ğ°Ğ½ÑÑ„ĞµÑ€Ñ‚Ğ° Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼.'}, 'en': {'title': 'Molecular Structures Enhance Long Chain-of-Thought Reasoning in LLMs', 'desc': 'This paper addresses the challenges that large language models (LLMs) face in performing long chain-of-thought (Long CoT) reasoning. It introduces a molecular-inspired framework that utilizes stable structural patterns, likening reasoning trajectories to molecular structures formed by different types of interactions. The authors propose Effective Semantic Isomers to enhance training stability, emphasizing that certain bonds are crucial for effective learning. Additionally, they present Mole-Syn, a method that improves the synthesis of Long CoT structures, leading to better performance and reinforcement learning stability.'}, 'zh': {'title': 'åˆ†å­å¯å‘çš„é•¿é“¾æ¨ç†æå‡æ–¹æ³•', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é•¿é“¾æ¨ç†æ–¹é¢è¡¨ç°ä¸ä½³ï¼Œä¸»è¦æ˜¯å› ä¸ºå…¶ç»“æ„æ¨¡å¼ä¸ç¨³å®šã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ†å­å¯å‘çš„æ–¹æ³•ï¼Œé€šè¿‡æœ‰æ•ˆçš„è¯­ä¹‰å¼‚æ„ä½“å’Œåˆ†å¸ƒè½¬ç§»å›¾æ–¹æ³•æ¥æé«˜è®­ç»ƒçš„ç¨³å®šæ€§å’Œæ€§èƒ½ã€‚ç ”ç©¶è¡¨æ˜ï¼Œç¨³å®šçš„åˆ†å­ç»“æ„å¯ä»¥é€šè¿‡æ·±åº¦æ¨ç†ã€è‡ªæˆ‘åæ€å’Œè‡ªæˆ‘æ¢ç´¢ä¸‰ç§äº¤äº’ç±»å‹å½¢æˆï¼Œä»è€Œä¿ƒè¿›é•¿é“¾æ¨ç†çš„å­¦ä¹ ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬æå‡ºäº†Mole-Synæ–¹æ³•ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæŒ‡å¯¼é•¿é“¾æ¨ç†ç»“æ„çš„åˆæˆï¼Œæå‡æ¨¡å‹åœ¨å„é¡¹åŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°å’Œå¼ºåŒ–å­¦ä¹ çš„ç¨³å®šæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.06021', 'title': 'Chaining the Evidence: Robust Reinforcement Learning for Deep Search Agents with Citation-Aware Rubric Rewards', 'url': 'https://huggingface.co/papers/2601.06021', 'abstract': "A citation-aware reward framework and policy optimization method improve deep search agents' reasoning comprehensiveness and factual accuracy while reducing shortcut exploitation and hallucinations.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning (RL) has emerged as a critical technique for enhancing LLM-based deep search agents. However, existing approaches primarily rely on binary outcome rewards, which fail to capture the comprehensiveness and factuality of agents' reasoning process, and often lead to undesirable behaviors such as shortcut exploitation and hallucinations. To address these limitations, we propose Citation-aware Rubric Rewards (CaRR), a fine-grained reward framework for deep search agents that emphasizes reasoning comprehensiveness, factual grounding, and evidence connectivity. CaRR decomposes complex questions into verifiable single-hop rubrics and requires agents to satisfy these rubrics by explicitly identifying hidden entities, supporting them with correct citations, and constructing complete evidence chains that link to the predicted answer. We further introduce Citation-aware Group Relative Policy Optimization (C-GRPO), which combines CaRR and outcome rewards for training robust deep search agents. Experiments show that C-GRPO consistently outperforms standard outcome-based RL baselines across multiple deep search benchmarks. Our analysis also validates that C-GRPO effectively discourages shortcut exploitation, promotes comprehensive, evidence-grounded reasoning, and exhibits strong generalization to open-ended deep research tasks. Our code and data are available at https://github.com/THUDM/CaRR.", 'score': 30, 'issue_id': 522, 'pub_date': '2026-01-09', 'pub_date_card': {'ru': '9 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 9', 'zh': '1æœˆ9æ—¥'}, 'hash': '2c4d9d165dc12ad2', 'authors': ['Jiajie Zhang', 'Xin Lv', 'Ling Feng', 'Lei Hou', 'Juanzi Li'], 'affiliations': ['Tsinghua University', 'Zhipu AI'], 'pdf_title_img': 'assets/pdf/title_img/2601.06021.jpg', 'data': {'categories': ['#rlhf', '#benchmark', '#optimization', '#hallucinations', '#rl', '#training', '#open_source', '#reasoning'], 'emoji': 'ğŸ”—', 'ru': {'title': 'ĞĞ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ñ Ğ¾ÑĞ¾Ğ·Ğ½Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ†Ğ¸Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ñ‡ĞµÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´ Citation-aware Rubric Rewards (CaRR), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚Ñ‹ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ² Ğ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ†Ğ¸Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ C-GRPO, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ CaRR Ñ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¾Ñ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¾Ğ¶Ğ¸Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ÑŒ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ² Ñ ÑĞ²Ğ½Ñ‹Ğ¼Ğ¸ ÑÑÑ‹Ğ»ĞºĞ°Ğ¼Ğ¸ Ğ½Ğ° Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑ€Ğ»Ñ‹ĞºĞ¾Ğ², Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸.'}, 'en': {'title': 'Enhancing Deep Search Agents with Citation-Aware Rewards', 'desc': 'This paper introduces a new reward framework called Citation-aware Rubric Rewards (CaRR) to enhance the reasoning abilities of deep search agents in reinforcement learning. Traditional methods use simple binary rewards, which can lead to poor reasoning and inaccuracies, such as shortcut exploitation and hallucinations. CaRR focuses on breaking down complex questions into simpler, verifiable components that require agents to provide evidence and citations for their answers. Additionally, the paper presents Citation-aware Group Relative Policy Optimization (C-GRPO), which integrates CaRR with outcome rewards to train more effective and reliable deep search agents.'}, 'zh': {'title': 'æå‡æ·±åº¦æœç´¢ä»£ç†æ¨ç†çš„å…¨é¢æ€§ä¸å‡†ç¡®æ€§', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¥–åŠ±æ¡†æ¶ï¼Œç§°ä¸ºå¼•ç”¨æ„è¯†è¯„åˆ†å¥–åŠ±ï¼ˆCaRRï¼‰ï¼Œæ—¨åœ¨æé«˜æ·±åº¦æœç´¢ä»£ç†çš„æ¨ç†å…¨é¢æ€§å’Œäº‹å®å‡†ç¡®æ€§ã€‚ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ä¸»è¦ä¾èµ–äºäºŒå…ƒç»“æœå¥–åŠ±ï¼Œæ— æ³•æœ‰æ•ˆæ•æ‰ä»£ç†æ¨ç†è¿‡ç¨‹çš„å¤æ‚æ€§ï¼Œå®¹æ˜“å¯¼è‡´æ·å¾„åˆ©ç”¨å’Œå¹»è§‰ç°è±¡ã€‚CaRRé€šè¿‡å°†å¤æ‚é—®é¢˜åˆ†è§£ä¸ºå¯éªŒè¯çš„å•æ­¥è¯„åˆ†ï¼Œè¦æ±‚ä»£ç†æ˜ç¡®è¯†åˆ«éšè—å®ä½“ï¼Œå¹¶ç”¨æ­£ç¡®çš„å¼•ç”¨æ”¯æŒå®ƒä»¬ï¼Œä»è€Œæ„å»ºå®Œæ•´çš„è¯æ®é“¾ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†å¼•ç”¨æ„è¯†ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆC-GRPOï¼‰ï¼Œç»“åˆCaRRå’Œç»“æœå¥–åŠ±ï¼Œè®­ç»ƒå‡ºæ›´å¼ºå¤§çš„æ·±åº¦æœç´¢ä»£ç†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.05808', 'title': 'EnvScaler: Scaling Tool-Interactive Environments for LLM Agent via Programmatic Synthesis', 'url': 'https://huggingface.co/papers/2601.05808', 'abstract': "EnvScaler automates the creation of tool-interaction environments through programmatic synthesis, enhancing LLM performance in complex multi-turn, multi-tool tasks via supervised fine-tuning and reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) are expected to be trained to act as agents in various real-world environments, but this process relies on rich and varied tool-interaction sandboxes. However, access to real systems is often restricted; LLM-simulated environments are prone to hallucinations and inconsistencies; and manually built sandboxes are hard to scale. In this paper, we propose EnvScaler, an automated framework for scalable tool-interaction environments via programmatic synthesis. EnvScaler comprises two components. First, SkelBuilder constructs diverse environment skeletons through topic mining, logic modeling, and quality evaluation. Then, ScenGenerator generates multiple task scenarios and rule-based trajectory validation functions for each environment. With EnvScaler, we synthesize 191 environments and about 7K scenarios, and apply them to Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) for Qwen3 series models. Results on three benchmarks show that EnvScaler significantly improves LLMs' ability to solve tasks in complex environments involving multi-turn, multi-tool interactions. We release our code and data at https://github.com/RUC-NLPIR/EnvScaler.", 'score': 24, 'issue_id': 522, 'pub_date': '2026-01-09', 'pub_date_card': {'ru': '9 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 9', 'zh': '1æœˆ9æ—¥'}, 'hash': '7fb43c28ab2879e9', 'authors': ['Xiaoshuai Song', 'Haofei Chang', 'Guanting Dong', 'Yutao Zhu', 'Zhicheng Dou', 'Ji-Rong Wen'], 'affiliations': ['Gaoling School of Artificial Intelligence, Renmin University of China'], 'pdf_title_img': 'assets/pdf/title_img/2601.05808.jpg', 'data': {'categories': ['#rl', '#dataset', '#agents', '#training'], 'emoji': 'ğŸ—ï¸', 'ru': {'title': 'ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLM', 'desc': 'EnvScaler â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑÑ€ĞµĞ´ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ğ´Ğ²ÑƒÑ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ²: SkelBuilder Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ ÑĞºĞµĞ»ĞµÑ‚Ñ‹ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ°Ğ¹Ğ½Ğ¸Ğ½Ğ³ Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ğº Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ğ° ScenGenerator ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ² Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹. Ğ¡Ğ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¸ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ÑÑ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ LLM Ñ‡ĞµÑ€ĞµĞ· supervised fine-tuning Ğ¸ reinforcement learning. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ LLM Ñ€ĞµÑˆĞ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½Ñ‡Ğ°Ñ‚Ñ‹Ğ¼ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ĞµĞ¼ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Automating Tool-Interaction Environments for Enhanced LLM Performance', 'desc': 'EnvScaler is a framework designed to automate the creation of environments where large language models (LLMs) can interact with various tools. It uses programmatic synthesis to generate diverse and scalable tool-interaction environments, addressing the limitations of manual sandbox creation. The framework consists of two main components: SkelBuilder, which creates environment skeletons, and ScenGenerator, which produces task scenarios and validation functions. By applying EnvScaler, researchers have synthesized numerous environments and scenarios, leading to significant improvements in LLM performance on complex multi-turn tasks.'}, 'zh': {'title': 'è‡ªåŠ¨åŒ–å·¥å…·äº¤äº’ç¯å¢ƒçš„åˆæˆä¸ä¼˜åŒ–', 'desc': 'EnvScaler æ˜¯ä¸€ä¸ªè‡ªåŠ¨åŒ–æ¡†æ¶ï¼Œé€šè¿‡ç¨‹åºåˆæˆæ¥åˆ›å»ºå¯æ‰©å±•çš„å·¥å…·äº¤äº’ç¯å¢ƒï¼Œä»è€Œæå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤æ‚å¤šè½®ã€å¤šå·¥å…·ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚è¯¥æ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦ç»„ä»¶ï¼šSkelBuilder ç”¨äºé€šè¿‡ä¸»é¢˜æŒ–æ˜ã€é€»è¾‘å»ºæ¨¡å’Œè´¨é‡è¯„ä¼°æ„å»ºå¤šæ ·åŒ–çš„ç¯å¢ƒéª¨æ¶ï¼›ScenGenerator åˆ™ä¸ºæ¯ä¸ªç¯å¢ƒç”Ÿæˆå¤šä¸ªä»»åŠ¡åœºæ™¯å’ŒåŸºäºè§„åˆ™çš„è½¨è¿¹éªŒè¯å‡½æ•°ã€‚é€šè¿‡ EnvScalerï¼Œæˆ‘ä»¬åˆæˆäº† 191 ä¸ªç¯å¢ƒå’Œçº¦ 7000 ä¸ªåœºæ™¯ï¼Œå¹¶å°†å…¶åº”ç”¨äºç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼Œæ˜¾è‘—æé«˜äº† LLM åœ¨å¤æ‚ç¯å¢ƒä¸­çš„ä»»åŠ¡è§£å†³èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®å·²åœ¨ https://github.com/RUC-NLPIR/EnvScaler å‘å¸ƒã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.04720', 'title': 'Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking', 'url': 'https://huggingface.co/papers/2601.04720', 'abstract': 'The Qwen3-VL-Embedding and Qwen3-VL-Reranker models form an end-to-end multimodal search pipeline, leveraging multi-stage training and cross-attention mechanisms to achieve high-precision retrieval across diverse modalities.  \t\t\t\t\tAI-generated summary \t\t\t\t In this report, we introduce the Qwen3-VL-Embedding and Qwen3-VL-Reranker model series, the latest extensions of the Qwen family built on the Qwen3-VL foundation model. Together, they provide an end-to-end pipeline for high-precision multimodal search by mapping diverse modalities, including text, images, document images, and video, into a unified representation space. The Qwen3-VL-Embedding model employs a multi-stage training paradigm, progressing from large-scale contrastive pre-training to reranking model distillation, to generate semantically rich high-dimensional vectors. It supports Matryoshka Representation Learning, enabling flexible embedding dimensions, and handles inputs up to 32k tokens. Complementing this, Qwen3-VL-Reranker performs fine-grained relevance estimation for query-document pairs using a cross-encoder architecture with cross-attention mechanisms. Both model series inherit the multilingual capabilities of Qwen3-VL, supporting more than 30 languages, and are released in 2B and 8B parameter sizes to accommodate diverse deployment requirements. Empirical evaluations demonstrate that the Qwen3-VL-Embedding series achieves state-of-the-art results across diverse multimodal embedding evaluation benchmarks. Specifically, Qwen3-VL-Embedding-8B attains an overall score of 77.8 on MMEB-V2, ranking first among all models (as of January 8, 2025). This report presents the architecture, training methodology, and practical capabilities of the series, demonstrating their effectiveness on various multimodal retrieval tasks, including image-text retrieval, visual question answering, and video-text matching.', 'score': 20, 'issue_id': 524, 'pub_date': '2026-01-08', 'pub_date_card': {'ru': '8 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 8', 'zh': '1æœˆ8æ—¥'}, 'hash': 'f64f73630c899742', 'authors': ['Mingxin Li', 'Yanzhao Zhang', 'Dingkun Long', 'Keqin Chen', 'Sibo Song', 'Shuai Bai', 'Zhibo Yang', 'Pengjun Xie', 'An Yang', 'Dayiheng Liu', 'Jingren Zhou', 'Junyang Lin'], 'affiliations': ['Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2601.04720.jpg', 'data': {'categories': ['#multilingual', '#benchmark', '#rag', '#open_source', '#training', '#architecture', '#multimodal'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ•Ğ´Ğ¸Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ²Ğ¾ Ğ²ÑĞµÑ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑÑ…', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Qwen3-VL-Embedding Ğ¸ Qwen3-VL-Reranker, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑ Ñ‚ĞµĞºÑÑ‚, Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Qwen3-VL-Embedding Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ¾Ğ¹ Ğ¸ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿ĞµÑ€ĞµÑ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ±Ğ¾Ğ³Ğ°Ñ‚Ñ‹Ñ… Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹. Qwen3-VL-Reranker Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ-Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ĞºÑ€Ğ¾ÑÑ-ĞºĞ¾Ğ´ĞµÑ€Ğ° Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² ĞºÑ€Ğ¾ÑÑ-Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. ĞĞ±Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ÑÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ 30 ÑĞ·Ñ‹ĞºĞ¾Ğ², Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹ Ğ² Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°Ñ… 2B Ğ¸ 8B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ².'}, 'en': {'title': 'Revolutionizing Multimodal Search with Qwen3 Models', 'desc': 'The Qwen3-VL-Embedding and Qwen3-VL-Reranker models create a powerful system for searching across different types of data, like text and images. They use advanced training techniques and cross-attention to improve how well they find relevant information. The embedding model generates detailed representations of data, while the reranker fine-tunes the results to ensure the best matches are highlighted. Together, they support multiple languages and have shown top performance in multimodal search tasks.'}, 'zh': {'title': 'é«˜ç²¾åº¦å¤šæ¨¡æ€æœç´¢çš„æœªæ¥', 'desc': 'Qwen3-VL-Embeddingå’ŒQwen3-VL-Rerankeræ¨¡å‹æ„æˆäº†ä¸€ä¸ªç«¯åˆ°ç«¯çš„å¤šæ¨¡æ€æœç´¢ç®¡é“ï¼Œåˆ©ç”¨å¤šé˜¶æ®µè®­ç»ƒå’Œäº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œå®ç°é«˜ç²¾åº¦çš„æ£€ç´¢ã€‚è¿™äº›æ¨¡å‹å°†æ–‡æœ¬ã€å›¾åƒã€æ–‡æ¡£å›¾åƒå’Œè§†é¢‘ç­‰å¤šç§æ¨¡æ€æ˜ å°„åˆ°ç»Ÿä¸€çš„è¡¨ç¤ºç©ºé—´ã€‚Qwen3-VL-Embeddingæ¨¡å‹é‡‡ç”¨å¤šé˜¶æ®µè®­ç»ƒï¼Œä»å¤§è§„æ¨¡å¯¹æ¯”é¢„è®­ç»ƒåˆ°é‡æ’åºæ¨¡å‹è’¸é¦ï¼Œç”Ÿæˆè¯­ä¹‰ä¸°å¯Œçš„é«˜ç»´å‘é‡ã€‚Qwen3-VL-Rerankeråˆ™ä½¿ç”¨äº¤å‰ç¼–ç å™¨æ¶æ„è¿›è¡Œç»†ç²’åº¦ç›¸å…³æ€§ä¼°è®¡ï¼Œæ”¯æŒå¤šè¯­è¨€ï¼Œé€‚åº”ä¸åŒçš„éƒ¨ç½²éœ€æ±‚ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.05930', 'title': 'Can We Predict Before Executing Machine Learning Agents?', 'url': 'https://huggingface.co/papers/2601.05930', 'abstract': 'Autonomous machine learning agents overcome execution bottlenecks by predicting outcomes before physical execution, achieving faster convergence and improved performance through a predict-then-verify approach.  \t\t\t\t\tAI-generated summary \t\t\t\t Autonomous machine learning agents have revolutionized scientific discovery, yet they remain constrained by a Generate-Execute-Feedback paradigm. Previous approaches suffer from a severe Execution Bottleneck, as hypothesis evaluation relies strictly on expensive physical execution. To bypass these physical constraints, we internalize execution priors to substitute costly runtime checks with instantaneous predictive reasoning, drawing inspiration from World Models. In this work, we formalize the task of Data-centric Solution Preference and construct a comprehensive corpus of 18,438 pairwise comparisons. We demonstrate that LLMs exhibit significant predictive capabilities when primed with a Verified Data Analysis Report, achieving 61.5% accuracy and robust confidence calibration. Finally, we instantiate this framework in FOREAGENT, an agent that employs a Predict-then-Verify loop, achieving a 6x acceleration in convergence while surpassing execution-based baselines by +6%. Our code and dataset will be publicly available soon at https://github.com/zjunlp/predict-before-execute.', 'score': 19, 'issue_id': 522, 'pub_date': '2026-01-09', 'pub_date_card': {'ru': '9 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 9', 'zh': '1æœˆ9æ—¥'}, 'hash': 'f17fbcab3adc48b7', 'authors': ['Jingsheng Zheng', 'Jintian Zhang', 'Yujie Luo', 'Yuren Mao', 'Yunjun Gao', 'Lun Du', 'Huajun Chen', 'Ningyu Zhang'], 'affiliations': ['Ant Group', 'Zhejiang University', 'Zhejiang University - Ant Group Joint Laboratory of Knowledge Graph'], 'pdf_title_img': 'assets/pdf/title_img/2601.05930.jpg', 'data': {'categories': ['#benchmark', '#agents', '#dataset', '#open_source', '#science'], 'emoji': 'ğŸ”®', 'ru': {'title': 'ĞŸÑ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ñ‚ÑŒ Ğ¿Ñ€ĞµĞ¶Ğ´Ğµ, Ñ‡ĞµĞ¼ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ÑŒ: ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ĞµĞµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ', 'desc': 'ĞĞ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑÑ‚Ñ€Ğ°Ğ´Ğ°ÑÑ‚ Ğ¾Ñ‚ ÑƒĞ·ĞºĞ¾Ğ³Ğ¾ Ğ¼ĞµÑÑ‚Ğ° Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ğ³Ğ´Ğ° Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ³Ğ¸Ğ¿Ğ¾Ñ‚ĞµĞ· Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ´Ğ¾Ñ€Ğ¾Ğ³Ğ¾ÑÑ‚Ğ¾ÑÑ‰Ğ¸Ñ… Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ´Ğ¾ Ğ¸Ñ… Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¸Ñ€Ğ° Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ”Ğ»Ñ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¾Ğ½Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸Ğ· 18,438 Ğ¿Ğ¾Ğ¿Ğ°Ñ€Ğ½Ñ‹Ñ… ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ LLM Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ 61.5% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚ FOREAGENT Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ†Ğ¸ĞºĞ» Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ-Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ 6-ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ½Ğ° 6% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Predict Before You Execute: Accelerating Machine Learning Agents', 'desc': 'This paper introduces autonomous machine learning agents that enhance performance by predicting outcomes before actual execution, thus addressing the Execution Bottleneck in traditional methods. By using a Predict-then-Verify approach, these agents can achieve faster convergence and improved results without relying solely on costly physical execution. The authors formalize the concept of Data-centric Solution Preference and create a dataset of 18,438 comparisons to evaluate predictive capabilities. The proposed agent, FOREAGENT, demonstrates a significant acceleration in convergence and outperforms existing execution-based methods.'}, 'zh': {'title': 'é¢„æµ‹å…ˆè¡Œï¼Œæ‰§è¡Œæ›´å¿«ï¼', 'desc': 'è‡ªä¸»æœºå™¨å­¦ä¹ ä»£ç†é€šè¿‡åœ¨ç‰©ç†æ‰§è¡Œä¹‹å‰é¢„æµ‹ç»“æœï¼Œå…‹æœäº†æ‰§è¡Œç“¶é¢ˆï¼Œä»è€Œå®ç°æ›´å¿«çš„æ”¶æ•›å’Œæ›´å¥½çš„æ€§èƒ½ã€‚è¿™ç§æ–¹æ³•é‡‡ç”¨äº†é¢„æµ‹-éªŒè¯çš„ç­–ç•¥ï¼Œé¿å…äº†ä¾èµ–æ˜‚è´µçš„ç‰©ç†æ‰§è¡Œæ¥è¯„ä¼°å‡è®¾ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåŒ…å«18,438å¯¹æ¯”è¾ƒçš„ç»¼åˆè¯­æ–™åº“ï¼Œå¹¶è¯æ˜äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç»è¿‡éªŒè¯çš„æ•°æ®åˆ†ææŠ¥å‘Šçš„å¼•å¯¼ä¸‹ï¼Œå…·æœ‰æ˜¾è‘—çš„é¢„æµ‹èƒ½åŠ›ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬åœ¨FOREAGENTä¸­å®ç°äº†è¿™ä¸€æ¡†æ¶ï¼Œè¯¥ä»£ç†é€šè¿‡é¢„æµ‹-éªŒè¯å¾ªç¯å®ç°äº†6å€çš„æ”¶æ•›åŠ é€Ÿï¼Œå¹¶è¶…è¶Šäº†åŸºäºæ‰§è¡Œçš„åŸºçº¿ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.05882', 'title': 'An Empirical Study on Preference Tuning Generalization and Diversity Under Domain Shift', 'url': 'https://huggingface.co/papers/2601.05882', 'abstract': 'Preference tuning of language models shows varying generalization capabilities under domain shift, with pseudo-labeling adaptation strategies effectively reducing performance degradation in summarization and question-answering tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Preference tuning aligns pretrained language models to human judgments of quality, helpfulness, or safety by optimizing over explicit preference signals rather than likelihood alone. Prior work has shown that preference-tuning degrades performance and reduces helpfulness when evaluated outside the training domain. However, the extent to which adaptation strategies mitigate this domain shift remains unexplored. We address this challenge by conducting a comprehensive and systematic study of alignment generalization under domain shift. We compare five popular alignment objectives and various adaptation strategies from source to target, including target-domain supervised fine-tuning and pseudo-labeling, across summarization and question-answering helpfulness tasks. Our findings reveal systematic differences in generalization across alignment objectives under domain shift. We show that adaptation strategies based on pseudo-labeling can substantially reduce domain-shift degradation', 'score': 18, 'issue_id': 534, 'pub_date': '2026-01-09', 'pub_date_card': {'ru': '9 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 9', 'zh': '1æœˆ9æ—¥'}, 'hash': '807f9304c133b53f', 'authors': ['Constantinos Karouzos', 'Xingwei Tan', 'Nikolaos Aletras'], 'affiliations': ['School of Computer Science University of Sheffield, UK'], 'pdf_title_img': 'assets/pdf/title_img/2601.05882.jpg', 'data': {'categories': ['#alignment', '#training', '#rlhf', '#transfer_learning'], 'emoji': 'ğŸ¯', 'ru': {'title': 'ĞĞ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ ÑĞ¼ĞµĞ½Ğµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¿ÑĞµĞ²Ğ´Ğ¾Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºÑƒ', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ, ĞºĞ°Ğº ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ±Ñ‹Ğ»Ğ¸ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾ĞµĞ½Ñ‹ Ğ½Ğ° Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ ĞºĞ¾Ğ³Ğ´Ğ° Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´ÑÑ‚ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¿ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¸ Ğ¿ÑĞµĞ²Ğ´Ğ¾Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºÑƒ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾-Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¼Ñƒ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ¾Ğ¼ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑĞ¼Ğ¸, Ğ¸ Ñ‡Ñ‚Ğ¾ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿ÑĞµĞ²Ğ´Ğ¾Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¾ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ñ Ñ†ĞµĞ»ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ².'}, 'en': {'title': 'Mitigating Domain Shift in Language Model Preference Tuning', 'desc': 'This paper investigates how preference tuning of language models can be affected by changes in the domain of the data. Preference tuning aims to align models with human preferences for quality and helpfulness, but it often leads to performance drops when applied to new domains. The authors explore various adaptation strategies, particularly focusing on pseudo-labeling, to see how they can help maintain performance in summarization and question-answering tasks. Their results indicate that certain adaptation methods can significantly lessen the negative impact of domain shifts on model performance.'}, 'zh': {'title': 'ä¼ªæ ‡ç­¾ç­–ç•¥åŠ©åŠ›è¯­è¨€æ¨¡å‹é¢†åŸŸé€‚åº”', 'desc': 'æœ¬è®ºæ–‡ç ”ç©¶äº†è¯­è¨€æ¨¡å‹çš„åå¥½è°ƒä¼˜åœ¨é¢†åŸŸè½¬ç§»ä¸‹çš„æ³›åŒ–èƒ½åŠ›ã€‚é€šè¿‡ä¼ªæ ‡ç­¾é€‚åº”ç­–ç•¥ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå‡å°‘åœ¨æ‘˜è¦å’Œé—®ç­”ä»»åŠ¡ä¸­çš„æ€§èƒ½ä¸‹é™ã€‚æˆ‘ä»¬æ¯”è¾ƒäº†äº”ç§æµè¡Œçš„å¯¹é½ç›®æ ‡å’Œå¤šç§é€‚åº”ç­–ç•¥ï¼Œå‘ç°ä¸åŒçš„å¯¹é½ç›®æ ‡åœ¨é¢†åŸŸè½¬ç§»ä¸‹çš„æ³›åŒ–è¡¨ç°å­˜åœ¨ç³»ç»Ÿæ€§å·®å¼‚ã€‚ç»“æœè¡¨æ˜ï¼ŒåŸºäºä¼ªæ ‡ç­¾çš„é€‚åº”ç­–ç•¥å¯ä»¥æ˜¾è‘—é™ä½é¢†åŸŸè½¬ç§»å¸¦æ¥çš„æ€§èƒ½ä¸‹é™ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.04786', 'title': 'AgentOCR: Reimagining Agent History via Optical Self-Compression', 'url': 'https://huggingface.co/papers/2601.04786', 'abstract': 'AgentOCR reduces token consumption in agentic systems by representing interaction history as visual tokens and employing visual caching and self-compression techniques.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large language models (LLMs) enable agentic systems trained with reinforcement learning (RL) over multi-turn interaction trajectories, but practical deployment is bottlenecked by rapidly growing textual histories that inflate token budgets and memory usage. We introduce AgentOCR, a framework that exploits the superior information density of visual tokens by representing the accumulated observation-action history as a compact rendered image. To make multi-turn rollouts scalable, AgentOCR proposes segment optical caching. By decomposing history into hashable segments and maintaining a visual cache, this mechanism eliminates redundant re-rendering. Beyond fixed rendering, AgentOCR introduces agentic self-compression, where the agent actively emits a compression rate and is trained with compression-aware reward to adaptively balance task success and token efficiency. We conduct extensive experiments on challenging agentic benchmarks, ALFWorld and search-based QA. Remarkably, results demonstrate that AgentOCR preserves over 95\\% of text-based agent performance while substantially reducing token consumption (>50\\%), yielding consistent token and memory efficiency. Our further analysis validates a 20x rendering speedup from segment optical caching and the effective strategic balancing of self-compression.', 'score': 18, 'issue_id': 523, 'pub_date': '2026-01-08', 'pub_date_card': {'ru': '8 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 8', 'zh': '1æœˆ8æ—¥'}, 'hash': 'a8a1ae23f28f1d1a', 'authors': ['Lang Feng', 'Fuchao Yang', 'Feng Chen', 'Xin Cheng', 'Haiyang Xu', 'Zhenglin Wan', 'Ming Yan', 'Bo An'], 'affiliations': ['Nanyang Technological University, Singapore', 'Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2601.04786.jpg', 'data': {'categories': ['#cv', '#agents', '#inference', '#rl'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…', 'desc': 'AgentOCR â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…, Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒÑ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ² ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ…. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ğ¸ÑĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Ğ¿ĞµÑ€ĞµÑÑ‡Ñ‘Ñ‚ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ñ…ĞµÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹. ĞĞ³ĞµĞ½Ñ‚ ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ ÑÑ‚ĞµĞ¿ĞµĞ½ÑŒ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğº ÑĞ¶Ğ°Ñ‚Ğ¸Ñ, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑƒÑĞ¿ĞµÑ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞĞ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… AgentOCR ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ 95% Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ğ½Ğ° 50%.'}, 'en': {'title': 'Efficient Interaction with Visual Tokens in Agentic Systems', 'desc': 'AgentOCR is a novel framework designed to enhance the efficiency of agentic systems by minimizing token usage during multi-turn interactions. It achieves this by converting interaction history into visual tokens, which are more information-dense than text. The framework employs segment optical caching to avoid redundant rendering of similar histories, significantly speeding up processing. Additionally, AgentOCR incorporates self-compression techniques that allow the agent to adaptively manage its performance and token efficiency, leading to substantial reductions in token consumption while maintaining high performance levels.'}, 'zh': {'title': 'AgentOCRï¼šé«˜æ•ˆçš„è§†è§‰æ ‡è®°ä¸è‡ªæˆ‘å‹ç¼©æŠ€æœ¯', 'desc': 'AgentOCR æ˜¯ä¸€ç§æ–°æ¡†æ¶ï¼Œé€šè¿‡å°†äº¤äº’å†å²è¡¨ç¤ºä¸ºè§†è§‰æ ‡è®°ï¼Œæ˜¾è‘—å‡å°‘äº†ä»£ç†ç³»ç»Ÿä¸­çš„ä»¤ç‰Œæ¶ˆè€—ã€‚å®ƒåˆ©ç”¨è§†è§‰ç¼“å­˜å’Œè‡ªæˆ‘å‹ç¼©æŠ€æœ¯ï¼Œä½¿å¾—å¤šè½®äº¤äº’çš„å¤„ç†æ›´åŠ é«˜æ•ˆã€‚é€šè¿‡å°†å†å²åˆ†è§£ä¸ºå¯å“ˆå¸Œçš„æ®µå¹¶ç»´æŠ¤è§†è§‰ç¼“å­˜ï¼ŒAgentOCR é¿å…äº†å†—ä½™çš„é‡æ–°æ¸²æŸ“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAgentOCR åœ¨ä¿æŒé«˜æ€§èƒ½çš„åŒæ—¶ï¼Œä»¤ç‰Œæ¶ˆè€—å‡å°‘è¶…è¿‡ 50%ï¼Œå¹¶å®ç°äº†æ˜¾è‘—çš„å†…å­˜æ•ˆç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.05966', 'title': 'VideoAR: Autoregressive Video Generation via Next-Frame & Scale Prediction', 'url': 'https://huggingface.co/papers/2601.05966', 'abstract': 'VideoAR presents a large-scale visual autoregressive framework for video generation that combines multi-scale next-frame prediction with autoregressive modeling, achieving state-of-the-art results with improved efficiency and temporal consistency.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in video generation have been dominated by diffusion and flow-matching models, which produce high-quality results but remain computationally intensive and difficult to scale. In this work, we introduce VideoAR, the first large-scale Visual Autoregressive (VAR) framework for video generation that combines multi-scale next-frame prediction with autoregressive modeling. VideoAR disentangles spatial and temporal dependencies by integrating intra-frame VAR modeling with causal next-frame prediction, supported by a 3D multi-scale tokenizer that efficiently encodes spatio-temporal dynamics. To improve long-term consistency, we propose Multi-scale Temporal RoPE, Cross-Frame Error Correction, and Random Frame Mask, which collectively mitigate error propagation and stabilize temporal coherence. Our multi-stage pretraining pipeline progressively aligns spatial and temporal learning across increasing resolutions and durations. Empirically, VideoAR achieves new state-of-the-art results among autoregressive models, improving FVD on UCF-101 from 99.5 to 88.6 while reducing inference steps by over 10x, and reaching a VBench score of 81.74-competitive with diffusion-based models an order of magnitude larger. These results demonstrate that VideoAR narrows the performance gap between autoregressive and diffusion paradigms, offering a scalable, efficient, and temporally consistent foundation for future video generation research.', 'score': 13, 'issue_id': 522, 'pub_date': '2026-01-09', 'pub_date_card': {'ru': '9 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 9', 'zh': '1æœˆ9æ—¥'}, 'hash': 'e8ed88c8c20040fb', 'authors': ['Longbin Ji', 'Xiaoxiong Liu', 'Junyuan Shang', 'Shuohuan Wang', 'Yu Sun', 'Hua Wu', 'Haifeng Wang'], 'affiliations': ['Baidu'], 'pdf_title_img': 'assets/pdf/title_img/2601.05966.jpg', 'data': {'categories': ['#architecture', '#benchmark', '#video', '#training'], 'emoji': 'ğŸ¬', 'ru': {'title': 'ĞĞ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ² ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸', 'desc': 'VideoAR Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿ĞµÑ€Ğ²ÑƒÑ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½ÑƒÑ Ğ°Ğ²Ñ‚Ğ°Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ ĞºĞ°Ğ´Ñ€Ğ° Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ 3D Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ğ¸ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸. Ğ”Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Temporal RoPE Ğ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²ĞºÑƒ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ğ´Ñ€Ğ°Ğ¼Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑĞ½Ğ¸Ğ¶Ğ°ÑÑ‚ Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº. VideoAR Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ², ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ñ‹Ñ… Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ñ‚Ñ€ĞµĞ±ÑƒÑ Ğ² 10 Ñ€Ğ°Ğ· Ğ¼ĞµĞ½ÑŒÑˆĞµ ÑˆĞ°Ğ³Ğ¾Ğ² Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'VideoAR: Efficient and Consistent Video Generation with Autoregressive Modeling', 'desc': 'VideoAR is a novel framework for generating videos using a visual autoregressive approach that enhances efficiency and temporal consistency. It integrates multi-scale next-frame prediction with autoregressive modeling to effectively manage spatial and temporal dependencies. The framework employs techniques like Multi-scale Temporal RoPE and Cross-Frame Error Correction to reduce errors and maintain coherence over time. VideoAR sets new benchmarks in video generation, outperforming previous autoregressive models while significantly cutting down on computational requirements.'}, 'zh': {'title': 'VideoARï¼šé«˜æ•ˆä¸€è‡´çš„è§†é¢‘ç”Ÿæˆæ–°æ¡†æ¶', 'desc': 'VideoARæ˜¯ä¸€ç§å¤§è§„æ¨¡çš„è§†è§‰è‡ªå›å½’æ¡†æ¶ï¼Œç”¨äºè§†é¢‘ç”Ÿæˆï¼Œç»“åˆäº†å¤šå°ºåº¦çš„ä¸‹ä¸€å¸§é¢„æµ‹å’Œè‡ªå›å½’å»ºæ¨¡ï¼Œå–å¾—äº†å…ˆè¿›çš„æ•ˆç‡å’Œæ—¶é—´ä¸€è‡´æ€§ã€‚è¯¥æ–¹æ³•é€šè¿‡å°†å¸§å†…è‡ªå›å½’å»ºæ¨¡ä¸å› æœä¸‹ä¸€å¸§é¢„æµ‹ç›¸ç»“åˆï¼Œè§£è€¦äº†ç©ºé—´å’Œæ—¶é—´ä¾èµ–æ€§ã€‚ä¸ºäº†æé«˜é•¿æœŸä¸€è‡´æ€§ï¼ŒVideoARæå‡ºäº†å¤šå°ºåº¦æ—¶é—´RoPEã€è·¨å¸§è¯¯å·®ä¿®æ­£å’Œéšæœºå¸§æ©ç ç­‰æŠ€æœ¯ï¼Œå‡å°‘äº†é”™è¯¯ä¼ æ’­å¹¶ç¨³å®šäº†æ—¶é—´è¿è´¯æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVideoARåœ¨è‡ªå›å½’æ¨¡å‹ä¸­è¾¾åˆ°äº†æ–°çš„æœ€å…ˆè¿›ç»“æœï¼Œæ˜¾è‘—æé«˜äº†æ€§èƒ½å¹¶å‡å°‘äº†æ¨ç†æ­¥éª¤ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.05905', 'title': 'Illusions of Confidence? Diagnosing LLM Truthfulness via Neighborhood Consistency', 'url': 'https://huggingface.co/papers/2601.05905', 'abstract': 'Large language models exhibit brittle beliefs under contextual perturbations, which are better measured by structural consistency metrics and addressed through structure-aware training methods.  \t\t\t\t\tAI-generated summary \t\t\t\t As Large Language Models (LLMs) are increasingly deployed in real-world settings, correctness alone is insufficient. Reliable deployment requires maintaining truthful beliefs under contextual perturbations. Existing evaluations largely rely on point-wise confidence like Self-Consistency, which can mask brittle belief. We show that even facts answered with perfect self-consistency can rapidly collapse under mild contextual interference. To address this gap, we propose Neighbor-Consistency Belief (NCB), a structural measure of belief robustness that evaluates response coherence across a conceptual neighborhood. To validate the efficiency of NCB, we introduce a new cognitive stress-testing protocol that probes outputs stability under contextual interference. Experiments across multiple LLMs show that the performance of high-NCB data is relatively more resistant to interference. Finally, we present Structure-Aware Training (SAT), which optimizes context-invariant belief structure and reduces long-tail knowledge brittleness by approximately 30%. Code will be available at https://github.com/zjunlp/belief.', 'score': 12, 'issue_id': 522, 'pub_date': '2026-01-09', 'pub_date_card': {'ru': '9 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 9', 'zh': '1æœˆ9æ—¥'}, 'hash': '30111934330923b5', 'authors': ['Haoming Xu', 'Ningyuan Zhao', 'Yunzhi Yao', 'Weihong Xu', 'Hongru Wang', 'Xinle Deng', 'Shumin Deng', 'Jeff Z. Pan', 'Huajun Chen', 'Ningyu Zhang'], 'affiliations': ['NUS-NCS Joint Lab', 'National University of Singapore', 'University of Edinburgh', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2601.05905.jpg', 'data': {'categories': [], 'emoji': 'ğŸ—ï¸', 'ru': {'title': 'Ğ£ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ğ¾Ğ¹ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ñ…Ñ€ÑƒĞ¿ĞºĞ¸Ğµ ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ğ»Ğ¾Ñ…Ğ¾ Ğ¸Ğ·Ğ¼ĞµÑ€ÑÑÑ‚ÑÑ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼Ğ¸ Ğ²Ñ€Ğ¾Ğ´Ğµ Self-Consistency. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ Neighbor-Consistency Belief (NCB), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾ĞºÑ€ĞµÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ„Ğ°ĞºÑ‚Ğ¾Ğ². ĞĞ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ» ÑÑ‚Ñ€ĞµÑÑ-Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ´ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸ĞµĞ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ¼ĞµÑ…. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Structure-Aware Training (SAT) Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾-Ğ¸Ğ½Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ñ…Ñ€ÑƒĞ¿ĞºĞ¾ÑÑ‚ÑŒ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ½Ğ¾ 30%.'}, 'en': {'title': 'Building Robust Beliefs in Language Models', 'desc': "This paper discusses the limitations of Large Language Models (LLMs) in maintaining accurate beliefs when faced with slight changes in context. It introduces a new metric called Neighbor-Consistency Belief (NCB) to measure the robustness of these beliefs by assessing how consistent responses are within a related conceptual framework. The authors demonstrate that traditional evaluation methods can overlook fragile beliefs that may collapse under minor contextual shifts. To improve the stability of LLMs, they propose Structure-Aware Training (SAT), which enhances the models' ability to maintain consistent beliefs, reducing knowledge brittleness significantly."}, 'zh': {'title': 'å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹çš„ä¿¡å¿µç¨³å®šæ€§', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é¢å¯¹ä¸Šä¸‹æ–‡å¹²æ‰°æ—¶è¡¨ç°å‡ºè„†å¼±çš„ä¿¡å¿µï¼Œè¿™ç§ç°è±¡å¯ä»¥é€šè¿‡ç»“æ„ä¸€è‡´æ€§æŒ‡æ ‡æ¥æ›´å¥½åœ°è¡¡é‡ã€‚ç°æœ‰çš„è¯„ä¼°æ–¹æ³•ä¸»è¦ä¾èµ–äºç‚¹å¯¹ç‚¹çš„è‡ªä¿¡åº¦ï¼Œå¯èƒ½æ©ç›–äº†ä¿¡å¿µçš„è„†å¼±æ€§ã€‚æˆ‘ä»¬æå‡ºäº†é‚»å±…ä¸€è‡´æ€§ä¿¡å¿µï¼ˆNCBï¼‰ï¼Œè¿™æ˜¯ä¸€ç§è¯„ä¼°å“åº”ä¸€è‡´æ€§çš„ç»“æ„æ€§æŒ‡æ ‡ï¼Œèƒ½å¤Ÿåœ¨æ¦‚å¿µé‚»åŸŸå†…è¿›è¡Œè¯„ä¼°ã€‚é€šè¿‡ç»“æ„æ„ŸçŸ¥è®­ç»ƒï¼ˆSATï¼‰ï¼Œæˆ‘ä»¬ä¼˜åŒ–äº†ä¸Šä¸‹æ–‡ä¸å˜çš„ä¿¡å¿µç»“æ„ï¼Œæ˜¾è‘—å‡å°‘äº†çŸ¥è¯†çš„è„†å¼±æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.05848', 'title': 'Goal Force: Teaching Video Models To Accomplish Physics-Conditioned Goals', 'url': 'https://huggingface.co/papers/2601.05848', 'abstract': "Video generation models trained on synthetic physics primitives demonstrate zero-shot generalization to complex real-world scenarios by modeling force propagation through time and space.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in video generation have enabled the development of ``world models'' capable of simulating potential futures for robotics and planning. However, specifying precise goals for these models remains a challenge; text instructions are often too abstract to capture physical nuances, while target images are frequently infeasible to specify for dynamic tasks. To address this, we introduce Goal Force, a novel framework that allows users to define goals via explicit force vectors and intermediate dynamics, mirroring how humans conceptualize physical tasks. We train a video generation model on a curated dataset of synthetic causal primitives-such as elastic collisions and falling dominos-teaching it to propagate forces through time and space. Despite being trained on simple physics data, our model exhibits remarkable zero-shot generalization to complex, real-world scenarios, including tool manipulation and multi-object causal chains. Our results suggest that by grounding video generation in fundamental physical interactions, models can emerge as implicit neural physics simulators, enabling precise, physics-aware planning without reliance on external engines. We release all datasets, code, model weights, and interactive video demos at our project page.", 'score': 9, 'issue_id': 522, 'pub_date': '2026-01-09', 'pub_date_card': {'ru': '9 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 9', 'zh': '1æœˆ9æ—¥'}, 'hash': 'fedc48a7e2d815ae', 'authors': ['Nate Gillman', 'Yinghua Zhou', 'Zitian Tang', 'Evan Luo', 'Arjan Chakravarthy', 'Daksh Aggarwal', 'Michael Freeman', 'Charles Herrmann', 'Chen Sun'], 'affiliations': ['Brown University', 'Cornell University'], 'pdf_title_img': 'assets/pdf/title_img/2601.05848.jpg', 'data': {'categories': ['#robotics', '#dataset', '#synthetic', '#video', '#training', '#open_source'], 'emoji': 'ğŸ¬', 'ru': {'title': 'ĞÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ… Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ² Ñ„Ğ¸Ğ·Ğ¸ĞºĞ¸ Ğº Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ°', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Goal Force â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ñ‡ĞµÑ€ĞµĞ· ÑĞ²Ğ½Ñ‹Ğµ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ñ‹ ÑĞ¸Ğ» Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½ÑƒÑ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºÑƒ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼ Ğ·Ğ°Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ñ†ĞµĞ»Ğ¸ Ğ² Ñ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ°Ñ… Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°Ğ»Ğ°ÑÑŒ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ğ²Ğ°Ñ… Ñ„Ğ¸Ğ·Ğ¸ĞºĞ¸ (ÑƒĞ¿Ñ€ÑƒĞ³Ğ¸Ğµ ÑÑ‚Ğ¾Ğ»ĞºĞ½Ğ¾Ğ²ĞµĞ½Ğ¸Ñ, Ğ¿Ğ°Ğ´Ğ°ÑÑ‰Ğ¸Ğµ ĞºĞ¾ÑÑ‚ÑÑˆĞºĞ¸ Ğ´Ğ¾Ğ¼Ğ¸Ğ½Ğ¾) Ğ´Ğ»Ñ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ ÑĞ¸Ğ» Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ. ĞĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ… Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…, Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ° Ğ¿Ğ¾Ñ€Ğ°Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğ½ÑƒĞ»ĞµĞ²ÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ¸ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ·Ğ°Ğ·ĞµĞ¼Ğ»ĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°ĞºĞ¾Ğ½Ğ°Ñ… Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ ĞºĞ°Ğº Ğ½ĞµÑĞ²Ğ½Ñ‹Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ²Ñ‹Ğµ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ñ‹ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'Empowering Video Generation with Physics-Based Goal Setting', 'desc': 'This paper presents a new framework called Goal Force for video generation models that allows users to set goals using explicit force vectors. By training on simple physics scenarios, the model learns to simulate how forces propagate over time and space. Remarkably, it can apply this knowledge to complex real-world tasks without needing additional training, demonstrating zero-shot generalization. This approach positions the model as an implicit neural physics simulator, enhancing its ability to plan and execute dynamic tasks based on physical interactions.'}, 'zh': {'title': 'é€šè¿‡åŠ›å‘é‡å®ç°ç›®æ ‡å®šä¹‰çš„åˆ›æ–°æ¡†æ¶', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºGoal Forceçš„æ–°æ¡†æ¶ï¼Œå…è®¸ç”¨æˆ·é€šè¿‡æ˜ç¡®çš„åŠ›å‘é‡å’Œä¸­é—´åŠ¨æ€æ¥å®šä¹‰ç›®æ ‡ã€‚è¿™ç§æ–¹æ³•æ¨¡ä»¿äº†äººç±»å¯¹ç‰©ç†ä»»åŠ¡çš„æ¦‚å¿µåŒ–ï¼Œè§£å†³äº†ä¼ ç»Ÿæ–‡æœ¬æŒ‡ä»¤å’Œç›®æ ‡å›¾åƒåœ¨åŠ¨æ€ä»»åŠ¡ä¸­éš¾ä»¥å…·ä½“åŒ–çš„é—®é¢˜ã€‚æˆ‘ä»¬è®­ç»ƒçš„æ¨¡å‹åœ¨ç®€å•çš„ç‰©ç†æ•°æ®ä¸Šè¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿåœ¨å¤æ‚çš„ç°å®åœºæ™¯ä¸­å®ç°é›¶-shotæ³›åŒ–ï¼ŒåŒ…æ‹¬å·¥å…·æ“ä½œå’Œå¤šç‰©ä½“å› æœé“¾ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œé€šè¿‡å°†è§†é¢‘ç”Ÿæˆä¸åŸºæœ¬ç‰©ç†äº¤äº’ç›¸ç»“åˆï¼Œæ¨¡å‹å¯ä»¥ä½œä¸ºéšå¼ç¥ç»ç‰©ç†æ¨¡æ‹Ÿå™¨ï¼Œå®ç°ç²¾ç¡®çš„ç‰©ç†æ„ŸçŸ¥è§„åˆ’ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.05403', 'title': 'Same Claim, Different Judgment: Benchmarking Scenario-Induced Bias in Multilingual Financial Misinformation Detection', 'url': 'https://huggingface.co/papers/2601.05403', 'abstract': 'A comprehensive benchmark evaluates behavioral biases in large language models for multilingual financial misinformation detection across diverse economic scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have been widely applied across various domains of finance. Since their training data are largely derived from human-authored corpora, LLMs may inherit a range of human biases. Behavioral biases can lead to instability and uncertainty in decision-making, particularly when processing financial information. However, existing research on LLM bias has mainly focused on direct questioning or simplified, general-purpose settings, with limited consideration of the complex real-world financial environments and high-risk, context-sensitive, multilingual financial misinformation detection tasks (\\mfmd). In this work, we propose \\mfmdscen, a comprehensive benchmark for evaluating behavioral biases of LLMs in \\mfmd across diverse economic scenarios. In collaboration with financial experts, we construct three types of complex financial scenarios: (i) role- and personality-based, (ii) role- and region-based, and (iii) role-based scenarios incorporating ethnicity and religious beliefs. We further develop a multilingual financial misinformation dataset covering English, Chinese, Greek, and Bengali. By integrating these scenarios with misinformation claims, \\mfmdscen enables a systematic evaluation of 22 mainstream LLMs. Our findings reveal that pronounced behavioral biases persist across both commercial and open-source models. This project will be available at https://github.com/lzw108/FMD.', 'score': 8, 'issue_id': 528, 'pub_date': '2026-01-08', 'pub_date_card': {'ru': '8 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 8', 'zh': '1æœˆ8æ—¥'}, 'hash': '330579eb3db9e0c3', 'authors': ['Zhiwei Liu', 'Yupen Cao', 'Yuechen Jiang', 'Mohsinul Kabir', 'Polydoros Giannouris', 'Chen Xu', 'Ziyang Xu', 'Tianlei Zhu', 'Tariquzzaman Faisal', 'Triantafillos Papadopoulos', 'Yan Wang', 'Lingfei Qian', 'Xueqing Peng', 'Zhuohan Xie', 'Ye Yuan', 'Saeed Almheiri', 'Abdulrazzaq Alnajjar', 'Mingbin Chen', 'Harry Stuart', 'Paul Thompson', 'Prayag Tiwari', 'Alejandro Lopez-Lira', 'Xue Liu', 'Jimin Huang', 'Sophia Ananiadou'], 'affiliations': ['Archimedes, Athena Research Center', 'Athens University of Economics and Business', 'Columbia University', 'Dubai Police', 'ELLIS Manchester', 'Halmstad University', 'Islamic University of Technology', 'MBZUAI', 'McGill University', 'Mila - Quebec AI Institute', 'Stevens Institute of Technology', 'The FinAI', 'The University of Manchester', 'University of Florida', 'University of Melbourne'], 'pdf_title_img': 'assets/pdf/title_img/2601.05403.jpg', 'data': {'categories': ['#multilingual', '#ethics', '#low_resource', '#benchmark', '#survey', '#dataset', '#open_source'], 'emoji': 'ğŸ’°', 'ru': {'title': 'Ğ’Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ LLM Ğ² Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ğ¾Ğ¹ Ğ´ĞµĞ·Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€ĞµĞ´ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¸ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ğ¾Ğ¹ Ğ´ĞµĞ·Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ°Ñ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€Ğ¾Ğ»ĞµĞ¹, Ğ»Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ĞµĞ¹, Ğ³ĞµĞ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ¾Ğ², ÑÑ‚Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ Ñ€ĞµĞ»Ğ¸Ğ³Ğ¸Ğ¾Ğ·Ğ½Ñ‹Ñ… Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸Ğº, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¾Ñ†ĞµĞ½Ğ¸Ñ‚ÑŒ, ĞºĞ°Ğº LLM Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ…. ĞŸĞ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ğ¾Ğ¹ Ğ´ĞµĞ·Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ¼, ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¾Ğ¼, Ğ³Ñ€ĞµÑ‡ĞµÑĞºĞ¾Ğ¼ Ğ¸ Ğ±ĞµĞ½Ğ³Ğ°Ğ»ÑŒÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞ°Ñ… Ğ¸ Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¾ 22 Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ LLM. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ÑÑƒÑ‚ÑÑ‚Ğ²ÑƒÑÑ‚ ĞºĞ°Ğº Ğ² ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¸Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ² Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ€Ğ¸ÑĞº Ğ´Ğ»Ñ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Unveiling Biases in Language Models for Financial Misinformation Detection', 'desc': 'This paper introduces a benchmark called \textit{mfmdscen} to assess behavioral biases in large language models (LLMs) when detecting multilingual financial misinformation. The benchmark is designed to reflect complex real-world financial scenarios, incorporating factors like roles, regions, ethnicity, and religious beliefs. By evaluating 22 mainstream LLMs using this benchmark, the study highlights the persistence of behavioral biases in both commercial and open-source models. The findings emphasize the need for more nuanced approaches in training and deploying LLMs in high-stakes financial contexts.'}, 'zh': {'title': 'è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹çš„é‡‘èåå·®', 'desc': 'æœ¬ç ”ç©¶æå‡ºäº†ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šè¯­è¨€é‡‘èè™šå‡ä¿¡æ¯æ£€æµ‹ä¸­çš„è¡Œä¸ºåå·®ã€‚ç”±äºLLMsçš„è®­ç»ƒæ•°æ®ä¸»è¦æ¥æºäºäººç±»åˆ›ä½œçš„æ–‡æœ¬ï¼Œå®ƒä»¬å¯èƒ½ä¼šç»§æ‰¿å„ç§äººç±»åè§ï¼Œè¿™å¯èƒ½å¯¼è‡´åœ¨å¤„ç†é‡‘èä¿¡æ¯æ—¶å†³ç­–çš„ä¸ç¨³å®šæ€§å’Œä¸ç¡®å®šæ€§ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸‰ç§å¤æ‚çš„é‡‘èåœºæ™¯ï¼Œå¹¶å¼€å‘äº†ä¸€ä¸ªæ¶µç›–å¤šç§è¯­è¨€çš„é‡‘èè™šå‡ä¿¡æ¯æ•°æ®é›†ï¼Œä»¥ä¾¿ç³»ç»Ÿåœ°è¯„ä¼°22ç§ä¸»æµLLMsçš„è¡¨ç°ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæ— è®ºæ˜¯å•†ä¸šæ¨¡å‹è¿˜æ˜¯å¼€æºæ¨¡å‹ï¼Œæ˜æ˜¾çš„è¡Œä¸ºåå·®åœ¨å„ç±»æ¨¡å‹ä¸­æ™®éå­˜åœ¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.05573', 'title': 'Orient Anything V2: Unifying Orientation and Rotation Understanding', 'url': 'https://huggingface.co/papers/2601.05573', 'abstract': 'Orient Anything V2 enhances 3D orientation understanding through scalable 3D asset synthesis, symmetry-aware periodic distribution fitting, and multi-frame relative rotation prediction, achieving state-of-the-art performance across multiple benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t This work presents Orient Anything V2, an enhanced foundation model for unified understanding of object 3D orientation and rotation from single or paired images. Building upon Orient Anything V1, which defines orientation via a single unique front face, V2 extends this capability to handle objects with diverse rotational symmetries and directly estimate relative rotations. These improvements are enabled by four key innovations: 1) Scalable 3D assets synthesized by generative models, ensuring broad category coverage and balanced data distribution; 2) An efficient, model-in-the-loop annotation system that robustly identifies 0 to N valid front faces for each object; 3) A symmetry-aware, periodic distribution fitting objective that captures all plausible front-facing orientations, effectively modeling object rotational symmetry; 4) A multi-frame architecture that directly predicts relative object rotations. Extensive experiments show that Orient Anything V2 achieves state-of-the-art zero-shot performance on orientation estimation, 6DoF pose estimation, and object symmetry recognition across 11 widely used benchmarks. The model demonstrates strong generalization, significantly broadening the applicability of orientation estimation in diverse downstream tasks.', 'score': 7, 'issue_id': 522, 'pub_date': '2026-01-09', 'pub_date_card': {'ru': '9 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 9', 'zh': '1æœˆ9æ—¥'}, 'hash': '6d51bd0357e81bbe', 'authors': ['Zehan Wang', 'Ziang Zhang', 'Jiayang Xu', 'Jialei Wang', 'Tianyu Pang', 'Chao Du', 'HengShuang Zhao', 'Zhou Zhao'], 'affiliations': ['Sea AI Lab', 'Shanghai AI Lab', 'The University of Hong Kong', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2601.05573.jpg', 'data': {'categories': ['#benchmark', '#cv', '#dataset', '#3d', '#multimodal', '#synthetic', '#open_source'], 'emoji': 'ğŸ”„', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ğ¾Ğ¹ Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ ÑƒÑ‡Ñ‘Ñ‚Ğ¾Ğ¼ ÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸', 'desc': 'Orient Anything V2 â€” ÑÑ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ°Ñ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ğ¾Ğ¹ Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ 3D-Ğ°ĞºÑ‚Ğ¸Ğ²Ñ‹, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ¸ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ€Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ ÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ´Ğ»Ñ Ğ¿ĞµÑ€Ğ¸Ğ¾Ğ´Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ¾Ğ²Ğ¾Ñ€Ğ¾Ñ‚Ñ‹ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ĞºĞ°Ğ´Ñ€Ğ°Ñ… Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° 11 Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸, Ğ¿Ğ¾Ğ·Ñ‹ Ğ¸ ÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¸Ğ»ÑŒĞ½ÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Revolutionizing 3D Orientation Understanding with Orient Anything V2', 'desc': 'Orient Anything V2 is a machine learning model designed to improve the understanding of 3D object orientation and rotation from images. It builds on its predecessor by allowing for the handling of objects with various rotational symmetries and estimating relative rotations directly. Key innovations include the use of generative models for creating diverse 3D assets, a robust annotation system for identifying valid orientations, and a multi-frame architecture for predicting rotations. The model has shown exceptional performance in orientation estimation and related tasks across multiple benchmarks, demonstrating its versatility in real-world applications.'}, 'zh': {'title': 'å¢å¼ºä¸‰ç»´æ–¹å‘ç†è§£çš„é©å‘½æ€§æ¨¡å‹', 'desc': 'Orient Anything V2 æ˜¯ä¸€ä¸ªå¢å¼ºçš„åŸºç¡€æ¨¡å‹ï¼Œæ—¨åœ¨ç»Ÿä¸€ç†è§£ç‰©ä½“çš„ä¸‰ç»´æ–¹å‘å’Œæ—‹è½¬ã€‚ä¸ä¹‹å‰çš„ç‰ˆæœ¬ç›¸æ¯”ï¼ŒV2 èƒ½å¤Ÿå¤„ç†å…·æœ‰ä¸åŒæ—‹è½¬å¯¹ç§°æ€§çš„ç‰©ä½“ï¼Œå¹¶ç›´æ¥ä¼°è®¡ç›¸å¯¹æ—‹è½¬ã€‚è¯¥æ¨¡å‹é€šè¿‡å››ä¸ªå…³é”®åˆ›æ–°å®ç°äº†è¿™äº›æ”¹è¿›ï¼ŒåŒ…æ‹¬å¯æ‰©å±•çš„ä¸‰ç»´èµ„äº§åˆæˆã€æœ‰æ•ˆçš„æ ‡æ³¨ç³»ç»Ÿã€å¯¹ç§°æ„ŸçŸ¥çš„åˆ†å¸ƒæ‹Ÿåˆç›®æ ‡ä»¥åŠå¤šå¸§æ¶æ„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOrient Anything V2 åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„é›¶æ ·æœ¬æ€§èƒ½ï¼Œæ˜¾è‘—æé«˜äº†æ–¹å‘ä¼°è®¡çš„é€‚ç”¨æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.04888', 'title': 'SmartSearch: Process Reward-Guided Query Refinement for Search Agents', 'url': 'https://huggingface.co/papers/2601.04888', 'abstract': "SmartSearch enhances LLM-based search agents through process rewards and query refinement mechanisms that improve intermediate search query quality via a three-stage curriculum learning approach.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language model (LLM)-based search agents have proven promising for addressing knowledge-intensive problems by incorporating information retrieval capabilities. Existing works largely focus on optimizing the reasoning paradigms of search agents, yet the quality of intermediate search queries during reasoning remains overlooked. As a result, the generated queries often remain inaccurate, leading to unexpected retrieval results and ultimately limiting search agents' overall effectiveness. To mitigate this issue, we introduce SmartSearch, a framework built upon two key mechanisms: (1) Process rewards, which provide fine-grained supervision for the quality of each intermediate search query through Dual-Level Credit Assessment. (2) Query refinement, which promotes the optimization of query generation by selectively refining low-quality search queries and regenerating subsequent search rounds based on these refinements. To enable the search agent to progressively internalize the ability to improve query quality under the guidance of process rewards, we design a three-stage curriculum learning framework. This framework guides the agent through a progression from imitation, to alignment, and ultimately to generalization. Experimental results show that SmartSearch consistently surpasses existing baselines, and additional quantitative analyses further confirm its significant gains in both search efficiency and query quality. The code is available at https://github.com/MYVAE/SmartSearch.", 'score': 5, 'issue_id': 522, 'pub_date': '2026-01-08', 'pub_date_card': {'ru': '8 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 8', 'zh': '1æœˆ8æ—¥'}, 'hash': '1d9a399da8a29c5c', 'authors': ['Tongyu Wen', 'Guanting Dong', 'Zhicheng Dou'], 'affiliations': ['Renmin University of China'], 'pdf_title_img': 'assets/pdf/title_img/2601.04888.jpg', 'data': {'categories': ['#reasoning', '#optimization', '#open_source'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ½Ñ‹Ğµ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ¸ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²', 'desc': 'SmartSearch â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ LLM-based Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ½Ñ‹Ğµ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ñ Ğ´Ğ²ÑƒÑ…ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¾Ğ¹ ĞºÑ€ĞµĞ´Ğ¸Ñ‚Ğ° Ğ´Ğ»Ñ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ° Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞœĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´ĞµĞ»Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ¸Ğ·ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ¸ÑĞºĞ¸ Ğ¸ Ğ¿ĞµÑ€ĞµĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ğµ Ñ€Ğ°ÑƒĞ½Ğ´Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¹. Ğ¢Ñ€Ñ‘Ñ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ curriculum learning Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ¼Ğ¸Ñ‚Ğ°Ñ†Ğ¸Ñ, Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸ÑĞ¼ Ğ² ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ².'}, 'en': {'title': 'Enhancing Search Quality with SmartSearch', 'desc': 'SmartSearch is a framework designed to improve the performance of large language model (LLM)-based search agents by enhancing the quality of their intermediate search queries. It introduces two main mechanisms: process rewards, which provide detailed feedback on the quality of each query, and query refinement, which helps to improve low-quality queries through selective adjustments. The framework employs a three-stage curriculum learning approach that guides the search agent from basic imitation to more complex generalization of query improvement. Experimental results demonstrate that SmartSearch outperforms existing methods, leading to better search efficiency and higher quality queries.'}, 'zh': {'title': 'SmartSearchï¼šæå‡æœç´¢ä»£ç†çš„æŸ¥è¯¢è´¨é‡', 'desc': 'SmartSearch æ˜¯ä¸€ç§å¢å¼ºåŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœç´¢ä»£ç†çš„æ¡†æ¶ã€‚å®ƒé€šè¿‡è¿‡ç¨‹å¥–åŠ±å’ŒæŸ¥è¯¢ä¼˜åŒ–æœºåˆ¶ï¼Œæé«˜ä¸­é—´æœç´¢æŸ¥è¯¢çš„è´¨é‡ã€‚è¯¥æ¡†æ¶é‡‡ç”¨ä¸‰é˜¶æ®µçš„è¯¾ç¨‹å­¦ä¹ æ–¹æ³•ï¼Œå¸®åŠ©æœç´¢ä»£ç†é€æ­¥æŒæ¡æ”¹è¿›æŸ¥è¯¢è´¨é‡çš„èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSmartSearch åœ¨æœç´¢æ•ˆç‡å’ŒæŸ¥è¯¢è´¨é‡ä¸Šå‡ä¼˜äºç°æœ‰çš„åŸºçº¿æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.02760', 'title': 'AnyDepth: Depth Estimation Made Easy', 'url': 'https://huggingface.co/papers/2601.02760', 'abstract': 'A lightweight monocular depth estimation framework uses DINOv3 as visual encoder and a compact transformer decoder to achieve higher accuracy with reduced computational overhead and improved data quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Monocular depth estimation aims to recover the depth information of 3D scenes from 2D images. Recent work has made significant progress, but its reliance on large-scale datasets and complex decoders has limited its efficiency and generalization ability. In this paper, we propose a lightweight and data-centric framework for zero-shot monocular depth estimation. We first adopt DINOv3 as the visual encoder to obtain high-quality dense features. Secondly, to address the inherent drawbacks of the complex structure of the DPT, we design the Simple Depth Transformer (SDT), a compact transformer-based decoder. Compared to the DPT, it uses a single-path feature fusion and upsampling process to reduce the computational overhead of cross-scale feature fusion, achieving higher accuracy while reducing the number of parameters by approximately 85%-89%. Furthermore, we propose a quality-based filtering strategy to filter out harmful samples, thereby reducing dataset size while improving overall training quality. Extensive experiments on five benchmarks demonstrate that our framework surpasses the DPT in accuracy. This work highlights the importance of balancing model design and data quality for achieving efficient and generalizable zero-shot depth estimation. Code: https://github.com/AIGeeksGroup/AnyDepth. Website: https://aigeeksgroup.github.io/AnyDepth.', 'score': 4, 'issue_id': 524, 'pub_date': '2026-01-06', 'pub_date_card': {'ru': '6 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 6', 'zh': '1æœˆ6æ—¥'}, 'hash': 'a6ad530d65a90d73', 'authors': ['Zeyu Ren', 'Zeyu Zhang', 'Wukai Li', 'Qingxiang Liu', 'Hao Tang'], 'affiliations': ['Peking University', 'Shanghai University of Engineering Science', 'The University of Melbourne'], 'pdf_title_img': 'assets/pdf/title_img/2601.02760.jpg', 'data': {'categories': ['#small_models', '#3d', '#training', '#data', '#architecture', '#cv'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ñ‡ĞµÑ€ĞµĞ· Ğ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ»Ñ‘Ğ³ĞºĞ¸Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ½Ğ¾ĞºÑƒĞ»ÑÑ€Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ DINOv3 Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ° Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Simple Depth Transformer (SDT) â€” ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ½Ñ‹Ğ¹ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ½Ğ° 85-89% Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ DPT, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ğº Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ĞµĞ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ¿ÑÑ‚Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ´Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ zero-shot Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹.'}, 'en': {'title': 'Efficient Monocular Depth Estimation with DINOv3 and SDT', 'desc': 'This paper presents a lightweight framework for monocular depth estimation that utilizes DINOv3 as a visual encoder and a compact transformer decoder called Simple Depth Transformer (SDT). The framework aims to improve accuracy while significantly reducing computational costs and the size of the dataset required for training. By employing a single-path feature fusion and upsampling process, the SDT minimizes the complexity of traditional decoders, achieving a reduction in parameters by 85%-89%. The authors also introduce a quality-based filtering strategy to enhance data quality, demonstrating that their approach outperforms existing methods in accuracy across multiple benchmarks.'}, 'zh': {'title': 'è½»é‡çº§å•ç›®æ·±åº¦ä¼°è®¡çš„é«˜æ•ˆè§£å†³æ–¹æ¡ˆ', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§è½»é‡çº§çš„å•ç›®æ·±åº¦ä¼°è®¡æ¡†æ¶ï¼Œä½¿ç”¨DINOv3ä½œä¸ºè§†è§‰ç¼–ç å™¨å’Œç´§å‡‘çš„å˜æ¢è§£ç å™¨ï¼Œä»¥æé«˜å‡†ç¡®æ€§å¹¶å‡å°‘è®¡ç®—å¼€é”€ã€‚è¯¥æ¡†æ¶é€šè¿‡å•è·¯å¾„ç‰¹å¾èåˆå’Œä¸Šé‡‡æ ·è¿‡ç¨‹ï¼Œæ˜¾è‘—é™ä½äº†è·¨å°ºåº¦ç‰¹å¾èåˆçš„è®¡ç®—å¤æ‚åº¦ï¼ŒåŒæ—¶å‚æ•°æ•°é‡å‡å°‘çº¦85%-89%ã€‚æ­¤å¤–ï¼Œæå‡ºäº†ä¸€ç§åŸºäºè´¨é‡çš„è¿‡æ»¤ç­–ç•¥ï¼Œä»¥å»é™¤æœ‰å®³æ ·æœ¬ï¼Œä»è€Œå‡å°‘æ•°æ®é›†å¤§å°å¹¶æé«˜æ•´ä½“è®­ç»ƒè´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å‡†ç¡®æ€§ä¸Šè¶…è¶Šäº†ä¼ ç»Ÿçš„DPTæ¨¡å‹ï¼Œå¼ºè°ƒäº†æ¨¡å‹è®¾è®¡ä¸æ•°æ®è´¨é‡ä¹‹é—´çš„å¹³è¡¡å¯¹äºé«˜æ•ˆå’Œå¯æ³›åŒ–çš„é›¶-shotæ·±åº¦ä¼°è®¡çš„é‡è¦æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.04823', 'title': 'DR-LoRA: Dynamic Rank LoRA for Mixture-of-Experts Adaptation', 'url': 'https://huggingface.co/papers/2601.04823', 'abstract': "DR-LoRA dynamically adjusts LoRA ranks for experts in Mixture-of-Experts models based on task-specific demands, improving parameter efficiency and performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Mixture-of-Experts (MoE) has become a prominent paradigm for scaling Large Language Models (LLMs). Parameter-efficient fine-tuning (PEFT), such as LoRA, is widely adopted to adapt pretrained MoE LLMs to downstream tasks. However, existing approaches assign identical LoRA ranks to all experts, overlooking the intrinsic functional specialization within MoE LLMs. This uniform allocation leads to resource mismatch, task-relevant experts are under-provisioned while less relevant ones receive redundant parameters. We propose a Dynamic Rank LoRA framework named DR-LoRA, which dynamically grows expert LoRA ranks during fine-tuning based on task-specific demands. DR-LoRA employs an Expert Saliency Scoring mechanism that integrates expert routing frequency and LoRA rank importance to quantify each expert's demand for additional capacity. Experts with higher saliency scores are prioritized for rank expansion, enabling the automatic formation of a heterogeneous rank distribution tailored to the target task. Experiments on multiple benchmarks demonstrate that DR-LoRA consistently outperforms standard LoRA and static allocation strategies under the same parameter budget, achieving superior task performance with more efficient parameter utilization.", 'score': 3, 'issue_id': 522, 'pub_date': '2026-01-08', 'pub_date_card': {'ru': '8 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 8', 'zh': '1æœˆ8æ—¥'}, 'hash': '81ecc151e1820135', 'authors': ['Guanzhi Deng', 'Bo Li', 'Ronghao Chen', 'Huacan Wang', 'Linqi Song', 'Lijie Wen'], 'affiliations': ['City University of Hong Kong', 'Peking University', 'Tsinghua University', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2601.04823.jpg', 'data': {'categories': ['#architecture', '#training'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ğ°Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ²: Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€Ğ°Ğ½Ğ³Ğ¸ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ´ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ DR-LoRA Ğ´Ğ»Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ñ€Ğ°Ğ½Ğ³Ğ¾Ğ² Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ğ¾Ğ² LoRA Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Mixture-of-Experts Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ fine-tuning. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ñƒ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ½Ğ³Ğ° Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ° Ğ² Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµÑ‚ Ğ³ĞµÑ‚ĞµÑ€Ğ¾Ğ³ĞµĞ½Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ½Ğ³Ğ¾Ğ², Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒÑÑÑŒ Ğº ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ, Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ€Ğ°Ğ²Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ²ÑĞµĞ¼ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ğ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ DR-LoRA Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸ Ñ‚Ğ¾Ğ¹ Ğ¶Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğ°, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ»ÑƒÑ‡ÑˆÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ².'}, 'en': {'title': 'Dynamic Expert Tuning for Efficient MoE Performance', 'desc': 'The paper introduces DR-LoRA, a method that optimizes the use of parameters in Mixture-of-Experts (MoE) models by dynamically adjusting the LoRA ranks of different experts based on the specific needs of a task. Traditional approaches assign the same LoRA rank to all experts, which can lead to inefficiencies as some experts may be over-allocated while others are under-utilized. DR-LoRA addresses this issue by using an Expert Saliency Scoring mechanism that evaluates how much additional capacity each expert requires based on their relevance to the task. The results show that DR-LoRA significantly improves performance and parameter efficiency compared to standard LoRA methods and static rank assignments.'}, 'zh': {'title': 'åŠ¨æ€è°ƒæ•´ä¸“å®¶ç­‰çº§ï¼Œæå‡æ¨¡å‹æ€§èƒ½', 'desc': 'DR-LoRAæ˜¯ä¸€ç§åŠ¨æ€è°ƒæ•´Mixture-of-Expertsæ¨¡å‹ä¸­LoRAç­‰çº§çš„æ–¹æ³•ï¼Œæ—¨åœ¨æ ¹æ®ç‰¹å®šä»»åŠ¡çš„éœ€æ±‚æé«˜å‚æ•°æ•ˆç‡å’Œæ€§èƒ½ã€‚ä¼ ç»Ÿæ–¹æ³•ä¸ºæ‰€æœ‰ä¸“å®¶åˆ†é…ç›¸åŒçš„LoRAç­‰çº§ï¼Œå¿½è§†äº†ä¸“å®¶ä¹‹é—´çš„åŠŸèƒ½ä¸“é—¨åŒ–ï¼Œå¯¼è‡´èµ„æºåˆ†é…ä¸å‡ã€‚DR-LoRAé€šè¿‡ä¸“å®¶æ˜¾è‘—æ€§è¯„åˆ†æœºåˆ¶ï¼ŒåŠ¨æ€è°ƒæ•´ä¸“å®¶çš„LoRAç­‰çº§ï¼Œä¼˜å…ˆæ‰©å±•éœ€æ±‚æ›´é«˜çš„ä¸“å®¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDR-LoRAåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºæ ‡å‡†LoRAå’Œé™æ€åˆ†é…ç­–ç•¥ï¼Œèƒ½å¤Ÿåœ¨ç›¸åŒå‚æ•°é¢„ç®—ä¸‹å®ç°æ›´é«˜çš„ä»»åŠ¡æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.04726', 'title': 'Memory Matters More: Event-Centric Memory as a Logic Map for Agent Searching and Reasoning', 'url': 'https://huggingface.co/papers/2601.04726', 'abstract': 'CompassMem is an event-centric memory framework that organizes experiences into an Event Graph to enable structured memory navigation and long-horizon reasoning beyond traditional retrieval methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) are increasingly deployed as intelligent agents that reason, plan, and interact with their environments. To effectively scale to long-horizon scenarios, a key capability for such agents is a memory mechanism that can retain, organize, and retrieve past experiences to support downstream decision-making. However, most existing approaches organize and store memories in a flat manner and rely on simple similarity-based retrieval techniques. Even when structured memory is introduced, existing methods often struggle to explicitly capture the logical relationships among experiences or memory units. Moreover, memory access is largely detached from the constructed structure and still depends on shallow semantic retrieval, preventing agents from reasoning logically over long-horizon dependencies. In this work, we propose CompassMem, an event-centric memory framework inspired by Event Segmentation Theory. CompassMem organizes memory as an Event Graph by incrementally segmenting experiences into events and linking them through explicit logical relations. This graph serves as a logic map, enabling agents to perform structured and goal-directed navigation over memory beyond superficial retrieval, progressively gathering valuable memories to support long-horizon reasoning. Experiments on LoCoMo and NarrativeQA demonstrate that CompassMem consistently improves both retrieval and reasoning performance across multiple backbone models.', 'score': 3, 'issue_id': 526, 'pub_date': '2026-01-08', 'pub_date_card': {'ru': '8 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 8', 'zh': '1æœˆ8æ—¥'}, 'hash': '156f14ff101f34b7', 'authors': ['Yuyang Hu', 'Jiongnan Liu', 'Jiejun Tan', 'Yutao Zhu', 'Zhicheng Dou'], 'affiliations': ['Gaoling School of Artificial Intelligence, Renmin University of China'], 'pdf_title_img': 'assets/pdf/title_img/2601.04726.jpg', 'data': {'categories': ['#long_context', '#reasoning', '#graphs'], 'emoji': 'ğŸ§­', 'ru': {'title': 'Ğ“Ñ€Ğ°Ñ„ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹ ĞºĞ°Ğº Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ°Ñ ĞºĞ°Ñ€Ñ‚Ğ° Ğ´Ğ»Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'CompassMem â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸, Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·ÑƒĞµÑ‚ Ğ¾Ğ¿Ñ‹Ñ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ² Ğ²Ğ¸Ğ´Ğµ Ğ³Ñ€Ğ°Ñ„Ğ° ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹ Ñ ÑĞ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ ÑĞ²ÑĞ·ÑĞ¼Ğ¸. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¿Ğ¾ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ñƒ, CompassMem Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ´ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑĞ¼Ğ¸. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¸Ğ½ĞºÑ€ĞµĞ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ğ¿Ñ‹Ñ‚ Ğ½Ğ° ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ñ Ğ¸ ÑĞ²ÑĞ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¸Ñ… Ñ‡ĞµÑ€ĞµĞ· ÑĞ²Ğ½Ñ‹Ğµ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ñ, ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºÑƒÑ ĞºĞ°Ñ€Ñ‚Ñƒ Ğ´Ğ»Ñ Ñ†ĞµĞ»ĞµĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ CompassMem ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ğº ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸, Ñ‚Ğ°Ğº Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹-Ğ¾ÑĞ½Ğ¾Ğ².'}, 'en': {'title': 'Empowering Long-Horizon Reasoning with Event-Centric Memory', 'desc': 'CompassMem is a novel memory framework designed for intelligent agents that enhances their ability to reason and plan over extended periods. It organizes experiences into an Event Graph, which captures the logical relationships between events, allowing for structured memory navigation. Unlike traditional flat memory systems that rely on simple similarity-based retrieval, CompassMem enables agents to access memories in a way that supports deeper reasoning and decision-making. Experiments show that this approach significantly improves both memory retrieval and reasoning capabilities in various tasks.'}, 'zh': {'title': 'CompassMemï¼šç»“æ„åŒ–è®°å¿†å¯¼èˆªçš„æ–°æ–¹æ³•', 'desc': 'CompassMemæ˜¯ä¸€ç§ä»¥äº‹ä»¶ä¸ºä¸­å¿ƒçš„è®°å¿†æ¡†æ¶ï¼Œå®ƒå°†ç»éªŒç»„ç»‡æˆäº‹ä»¶å›¾ï¼Œä»¥ä¾¿å®ç°ç»“æ„åŒ–çš„è®°å¿†å¯¼èˆªå’Œé•¿æ—¶é—´æ¨ç†ã€‚è¯¥æ¡†æ¶é€šè¿‡é€æ­¥å°†ç»éªŒåˆ†æ®µä¸ºäº‹ä»¶ï¼Œå¹¶é€šè¿‡æ˜ç¡®çš„é€»è¾‘å…³ç³»å°†å®ƒä»¬è¿æ¥èµ·æ¥ï¼Œä»è€Œå…‹æœäº†ä¼ ç»Ÿæ–¹æ³•çš„å±€é™ã€‚ä¸ç°æœ‰çš„æ‰å¹³è®°å¿†å­˜å‚¨æ–¹å¼ä¸åŒï¼ŒCompassMemèƒ½å¤Ÿæ•æ‰ç»éªŒä¹‹é—´çš„é€»è¾‘å…³ç³»ï¼Œæ”¯æŒæ›´æ·±å±‚æ¬¡çš„æ¨ç†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCompassMemåœ¨å¤šä¸ªåŸºç¡€æ¨¡å‹ä¸Šéƒ½æ˜¾è‘—æé«˜äº†æ£€ç´¢å’Œæ¨ç†çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.05637', 'title': 'GenCtrl -- A Formal Controllability Toolkit for Generative Models', 'url': 'https://huggingface.co/papers/2601.05637', 'abstract': "Generative models' controllability is theoretically analyzed through a framework that estimates controllable sets with distribution-free bounds, revealing that controllability is fragile and context-dependent.  \t\t\t\t\tAI-generated summary \t\t\t\t As generative models become ubiquitous, there is a critical need for fine-grained control over the generation process. Yet, while controlled generation methods from prompting to fine-tuning proliferate, a fundamental question remains unanswered: are these models truly controllable in the first place? In this work, we provide a theoretical framework to formally answer this question. Framing human-model interaction as a control process, we propose a novel algorithm to estimate the controllable sets of models in a dialogue setting. Notably, we provide formal guarantees on the estimation error as a function of sample complexity: we derive probably-approximately correct bounds for controllable set estimates that are distribution-free, employ no assumptions except for output boundedness, and work for any black-box nonlinear control system (i.e., any generative model). We empirically demonstrate the theoretical framework on different tasks in controlling dialogue processes, for both language models and text-to-image generation. Our results show that model controllability is surprisingly fragile and highly dependent on the experimental setting. This highlights the need for rigorous controllability analysis, shifting the focus from simply attempting control to first understanding its fundamental limits.", 'score': 2, 'issue_id': 522, 'pub_date': '2026-01-09', 'pub_date_card': {'ru': '9 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 9', 'zh': '1æœˆ9æ—¥'}, 'hash': '30ec0c4c1e6d1f97', 'authors': ['Emily Cheng', 'Carmen Amo Alonso', 'Federico Danieli', 'Arno Blaas', 'Luca Zappella', 'Pau Rodriguez', 'Xavier Suau'], 'affiliations': ['Apple', 'Stanford', 'Universitat Pompeu Fabra'], 'pdf_title_img': 'assets/pdf/title_img/2601.05637.jpg', 'data': {'categories': [], 'emoji': 'ğŸ®', 'ru': {'title': 'Ğ£Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: Ğ¾Ñ‚ Ğ¸Ğ»Ğ»ÑĞ·Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğº Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ĞµĞ³Ğ¾ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶Ğ¸Ğ¼Ñ‹Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ½Ğ¾-Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ñ‹Ñ… Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ† Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒÑÑ‚ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ°Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ² ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…. Ğ“Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ Ğ´Ğ»Ñ Ğ»ÑĞ±Ñ‹Ñ… Ñ‡Ñ‘Ñ€Ğ½Ñ‹Ñ… ÑÑ‰Ğ¸ĞºĞ¾Ğ² Ğ½ĞµĞ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ĞºÑ€Ğ¾Ğ¼Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ…Ñ€ÑƒĞ¿ĞºĞ° Ğ¸ ÑĞ¸Ğ»ÑŒĞ½Ğ¾ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ñ‚ Ğ¾Ñ‚ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°, Ñ‡Ñ‚Ğ¾ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ° Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ.'}, 'en': {'title': 'Understanding the Fragility of Generative Model Controllability', 'desc': 'This paper analyzes how controllable generative models really are, using a new theoretical framework. It introduces an algorithm that estimates the sets of outputs that can be controlled in dialogue systems, providing guarantees on the accuracy of these estimates. The findings reveal that the ability to control these models is often fragile and varies greatly depending on the context. This emphasizes the importance of understanding the limits of model controllability before trying to exert control over them.'}, 'zh': {'title': 'ç”Ÿæˆæ¨¡å‹çš„å¯æ§æ€§ï¼šè„†å¼±è€Œä¾èµ–ä¸Šä¸‹æ–‡', 'desc': 'æœ¬æ–‡é€šè¿‡ä¸€ä¸ªç†è®ºæ¡†æ¶åˆ†æç”Ÿæˆæ¨¡å‹çš„å¯æ§æ€§ï¼Œä¼°è®¡å¯æ§é›†å¹¶æä¾›æ— åˆ†å¸ƒç•Œé™çš„ä¿è¯ã€‚ç ”ç©¶è¡¨æ˜ï¼Œç”Ÿæˆæ¨¡å‹çš„å¯æ§æ€§æ˜¯è„†å¼±çš„ï¼Œå¹¶ä¸”é«˜åº¦ä¾èµ–äºä¸Šä¸‹æ–‡ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°ç®—æ³•ï¼Œç”¨äºåœ¨å¯¹è¯è®¾ç½®ä¸­ä¼°è®¡æ¨¡å‹çš„å¯æ§é›†ï¼Œå¹¶åœ¨ä¸åŒä»»åŠ¡ä¸­è¿›è¡Œäº†å®è¯éªŒè¯ã€‚ç»“æœæ˜¾ç¤ºï¼Œæ¨¡å‹çš„å¯æ§æ€§å¹¶ä¸å¦‚é¢„æœŸï¼Œå¼ºè°ƒäº†å¯¹å¯æ§æ€§è¿›è¡Œä¸¥æ ¼åˆ†æçš„é‡è¦æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.05503', 'title': 'Over-Searching in Search-Augmented Large Language Models', 'url': 'https://huggingface.co/papers/2601.05503', 'abstract': 'Search-augmented large language models suffer from over-searching behavior that wastes computational resources and introduces hallucinations, with findings showing varied impacts across model types and conversation contexts.  \t\t\t\t\tAI-generated summary \t\t\t\t Search-augmented large language models (LLMs) excel at knowledge-intensive tasks by integrating external retrieval. However, they often over-search -- unnecessarily invoking search tool even when it does not improve response quality, which leads to computational inefficiency and hallucinations by incorporating irrelevant context. In this work, we conduct a systematic evaluation of over-searching across multiple dimensions, including query types, model categories, retrieval conditions, and multi-turn conversations. Our finding shows: (i) search generally improves answer accuracy on answerable queries but harms abstention on unanswerable ones; (ii) over-searching is more pronounced in complex reasoning models and deep research systems, is exacerbated by noisy retrieval, and compounds across turns in multi-turn conversations; and (iii) the composition of retrieved evidence is crucial, as the presence of negative evidence improves abstention. To quantify over-searching, we introduce Tokens Per Correctness (TPC), an evaluation metric that captures the performance-cost trade-off for search-augmented LLMs. Lastly, we investigate mitigation approaches at both the query and retrieval levels and release the OverSearchQA to foster continued research into efficient search-augmented LLMs.', 'score': 2, 'issue_id': 522, 'pub_date': '2026-01-09', 'pub_date_card': {'ru': '9 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 9', 'zh': '1æœˆ9æ—¥'}, 'hash': '33cf12cd1c7618ab', 'authors': ['Roy Xie', 'Deepak Gopinath', 'David Qiu', 'Dong Lin', 'Haitian Sun', 'Saloni Potdar', 'Bhuwan Dhingra'], 'affiliations': ['Apple', 'Duke University'], 'pdf_title_img': 'assets/pdf/title_img/2601.05503.jpg', 'data': {'categories': ['#rag', '#benchmark', '#hallucinations', '#optimization'], 'emoji': 'ğŸ”', 'ru': {'title': 'ĞšĞ¾Ğ³Ğ´Ğ° Ğ¿Ğ¾Ğ¸ÑĞº Ğ²Ñ€ĞµĞ´Ğ¸Ñ‚: ĞºĞ°Ğº Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ Ñ‡Ñ€ĞµĞ·Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ¾Ğ±Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¿Ğ¾Ğ¸ÑĞº-Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ½Ñ‹Ñ… LLM', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ñ‡Ñ€ĞµĞ·Ğ¼ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ¼, ĞºĞ¾Ğ³Ğ´Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ÑÑ Ğº Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğ¼ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ°Ğ¼ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ¿ÑƒÑÑ‚Ñ€Ğ°Ñ‚Ğ°Ğ¼ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ¸ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸ÑĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´ÑÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ½Ğ°Ğ´-Ğ¿Ğ¾Ğ¸ÑĞºĞ°, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞº ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ¾Ñ‚Ğ²ĞµÑ‡Ğ°ĞµĞ¼Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°Ñ…, Ğ½Ğ¾ Ğ²Ñ€ĞµĞ´Ğ¸Ñ‚ Ğ¾Ñ‚ĞºĞ°Ğ·Ñƒ Ğ¾Ñ‚ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° Ğ½Ğ° Ğ½ĞµĞ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹, Ğ° ÑÑ„Ñ„ĞµĞºÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹Ñ€Ğ°Ğ¶ĞµĞ½ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° ÑƒÑĞ¸Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¸ Ğ·Ğ°ÑˆÑƒĞ¼Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°Ñ… Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸ Ğ½Ğ°ĞºĞ°Ğ¿Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¾Ğ±Ğ¾Ñ€Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°Ñ…, Ğ½Ğ¾ Ğ½Ğ°Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾ Ğ¾Ñ‚ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ. Ğ”Ğ»Ñ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ° Tokens Per Correctness, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ñ‚Ñ€Ğ°Ğ¶Ğ°ĞµÑ‚ ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Optimizing Search for Smarter Language Models', 'desc': 'This paper addresses the issue of over-searching in search-augmented large language models (LLMs), which can lead to wasted computational resources and hallucinations. The authors systematically evaluate how over-searching affects different model types and conversation contexts, revealing that while search can improve accuracy for answerable queries, it can negatively impact responses for unanswerable ones. They introduce a new metric, Tokens Per Correctness (TPC), to measure the efficiency of search-augmented LLMs and highlight the importance of the quality of retrieved evidence. The study also explores strategies to mitigate over-searching at both the query and retrieval levels, contributing to the development of more efficient LLMs.'}, 'zh': {'title': 'ä¼˜åŒ–æœç´¢ï¼Œæå‡è¯­è¨€æ¨¡å‹æ•ˆç‡', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†å¢å¼ºæœç´¢çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä½¿ç”¨æœç´¢å·¥å…·æ—¶çš„è¿‡åº¦æœç´¢è¡Œä¸ºï¼Œè¿™ç§è¡Œä¸ºä¼šæµªè´¹è®¡ç®—èµ„æºå¹¶å¼•å…¥å¹»è§‰ã€‚ç ”ç©¶å‘ç°ï¼Œæœç´¢é€šå¸¸èƒ½æé«˜å¯å›ç­”é—®é¢˜çš„å‡†ç¡®æ€§ï¼Œä½†åœ¨æ— æ³•å›ç­”çš„é—®é¢˜ä¸Šå´ä¼šå¯¼è‡´é”™è¯¯çš„æ”¾å¼ƒã€‚è¿‡åº¦æœç´¢åœ¨å¤æ‚æ¨ç†æ¨¡å‹å’Œæ·±åº¦ç ”ç©¶ç³»ç»Ÿä¸­æ›´ä¸ºæ˜æ˜¾ï¼Œå¹¶ä¸”åœ¨å¤šè½®å¯¹è¯ä¸­ä¼šåŠ å‰§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„è¯„ä¼°æŒ‡æ ‡â€”â€”æ¯ä¸ªæ­£ç¡®ç­”æ¡ˆçš„ä»¤ç‰Œæ•°ï¼ˆTPCï¼‰ï¼Œå¹¶æ¢è®¨äº†åœ¨æŸ¥è¯¢å’Œæ£€ç´¢å±‚é¢ä¸Šçš„ç¼“è§£æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.04544', 'title': 'TCAndon-Router: Adaptive Reasoning Router for Multi-Agent Collaboration', 'url': 'https://huggingface.co/papers/2601.04544', 'abstract': 'A multi-agent system router that uses dynamic agent onboarding and natural language reasoning chains to improve routing accuracy and reduce conflicts in enterprise applications.  \t\t\t\t\tAI-generated summary \t\t\t\t Multi-Agent Systems(MAS) have become a powerful paradigm for building high performance intelligent applications. Within these systems, the router responsible for determining which expert agents should handle a given query plays a crucial role in overall performance. Existing routing strategies generally fall into two categories: performance routing, which balances latency and cost across models of different sizes, and task routing, which assigns queries to domain-specific experts to improve accuracy. In real-world enterprise applications, task routing is more suitable; however, most existing approaches rely on static single-label decisions, which introduce two major limitations: (i) difficulty in seamlessly integrating new agents as business domains expand, and (ii) routing conflicts caused by overlapping agent capabilities, ultimately degrading accuracy and robustness.To address these challenges, we propose TCAndon-Router(TCAR): an adaptive reasoning router for multi-agent collaboration. Unlike traditional routers, TCAR supports dynamic agent onboarding and first generates a natural-language reasoning chain before predicting a set of candidate agents capable of handling the query. In addition, we design a collaborative execution pipeline in which selected agents independently produce responses, which are then aggregated and refined into a single high-quality response by a dedicated Refining Agent.Experiments on public datasets and real enterprise data demonstrate that TCAR significantly improves routing accuracy, reduces routing conflicts, and remains robust in ambiguous scenarios. We have released TCAR at https://huggingface.co/tencent/TCAndon-Router to support future research on explainable and collaborative multi-agent routing.', 'score': 2, 'issue_id': 528, 'pub_date': '2026-01-08', 'pub_date_card': {'ru': '8 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 8', 'zh': '1æœˆ8æ—¥'}, 'hash': '50c6ddb04d917f2a', 'authors': ['Jiuzhou Zhao', 'Chunrong Chen', 'Chenqi Qiao', 'Lebin Zheng', 'Minqi Han', 'Yanchi Liu Yongzhou Xu Xiaochuan Xu Min Zhang'], 'affiliations': ['Tencent Cloud'], 'pdf_title_img': 'assets/pdf/title_img/2601.04544.jpg', 'data': {'categories': ['#agents'], 'emoji': 'ğŸš¦', 'ru': {'title': 'ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ´Ğ»Ñ Ğ³Ğ¸Ğ±ĞºĞ¸Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ TCAR, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ÑÑ‰Ğ¸Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ°. ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° TCAR Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ, Ğ³Ğ´Ğµ Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ°Ğ³Ñ€ĞµĞ³Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ¸ ÑƒÑ‚Ğ¾Ñ‡Ğ½ÑÑÑ‚ÑÑ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ñ‹ Ğ¸ Ğ¾ÑÑ‚Ğ°Ñ‘Ñ‚ÑÑ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ñ‹Ğ¼ Ğ² Ğ½ĞµĞ¾Ğ´Ğ½Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ….'}, 'en': {'title': 'Dynamic Routing for Enhanced Multi-Agent Collaboration', 'desc': 'This paper presents TCAndon-Router (TCAR), a novel multi-agent system router designed to enhance routing accuracy and minimize conflicts in enterprise applications. TCAR introduces dynamic agent onboarding, allowing for the seamless integration of new agents as business needs evolve. It utilizes natural language reasoning chains to identify a set of candidate agents, rather than relying on static single-label decisions, which often lead to routing conflicts. Experimental results show that TCAR outperforms traditional routing methods by improving accuracy and robustness in complex scenarios.'}, 'zh': {'title': 'åŠ¨æ€æ™ºèƒ½ä½“æ¥å…¥ï¼Œæå‡è·¯ç”±å‡†ç¡®æ€§ï¼', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºTCAndon-Routerï¼ˆTCARï¼‰çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿè·¯ç”±å™¨ï¼Œæ—¨åœ¨æé«˜ä¼ä¸šåº”ç”¨ä¸­çš„è·¯ç”±å‡†ç¡®æ€§å¹¶å‡å°‘å†²çªã€‚TCARæ”¯æŒåŠ¨æ€æ™ºèƒ½ä½“æ¥å…¥ï¼Œå¹¶åœ¨é¢„æµ‹å€™é€‰æ™ºèƒ½ä½“ä¹‹å‰ç”Ÿæˆè‡ªç„¶è¯­è¨€æ¨ç†é“¾ï¼Œä»è€Œæ›´å¥½åœ°å¤„ç†æŸ¥è¯¢ã€‚ä¸ä¼ ç»Ÿçš„é™æ€å•æ ‡ç­¾å†³ç­–æ–¹æ³•ä¸åŒï¼ŒTCARèƒ½å¤Ÿçµæ´»åº”å¯¹ä¸šåŠ¡é¢†åŸŸçš„æ‰©å±•ï¼Œé¿å…äº†å› æ™ºèƒ½ä½“èƒ½åŠ›é‡å è€Œå¯¼è‡´çš„è·¯ç”±å†²çªã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTCARåœ¨å…¬å…±æ•°æ®é›†å’ŒçœŸå®ä¼ä¸šæ•°æ®ä¸Šæ˜¾è‘—æé«˜äº†è·¯ç”±å‡†ç¡®æ€§ï¼Œå¢å¼ºäº†ç³»ç»Ÿçš„é²æ£’æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.05960', 'title': 'Distilling Feedback into Memory-as-a-Tool', 'url': 'https://huggingface.co/papers/2601.05960', 'abstract': 'A framework converts transient critiques into retrievable guidelines using a file-based memory system and agent-controlled tool calls, enabling LLMs to match test-time refinement performance with reduced inference costs.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose a framework that amortizes the cost of inference-time reasoning by converting transient critiques into retrievable guidelines, through a file-based memory system and agent-controlled tool calls. We evaluate this method on the Rubric Feedback Bench, a novel dataset for rubric-based learning. Experiments demonstrate that our augmented LLMs rapidly match the performance of test-time refinement pipelines while drastically reducing inference cost.', 'score': 1, 'issue_id': 527, 'pub_date': '2026-01-09', 'pub_date_card': {'ru': '9 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 9', 'zh': '1æœˆ9æ—¥'}, 'hash': 'eebae4efe4e25421', 'authors': ['VÃ­ctor Gallego'], 'affiliations': ['Komorebi AI Technologies'], 'pdf_title_img': 'assets/pdf/title_img/2601.05960.jpg', 'data': {'categories': [], 'emoji': 'ğŸ’¾', 'ru': {'title': 'Ğ˜Ğ· Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ² ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ: ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞµ', 'desc': 'ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ¿Ñ€Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞµ, Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒÑ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ² ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼Ñ‹Ğµ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ¼ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ñ‹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². ĞĞ° Ğ½Ğ¾Ğ²Ğ¾Ğ¼ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ Rubric Feedback Bench Ğ¾Ğ½Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ, Ñ‡Ñ‚Ğ¾ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ½Ñ‹Ğµ LLM Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½Ğ¾Ğ² Ñ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¸Ğ»Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ĞµĞµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€ĞµÑÑƒÑ€ÑÑ‹ Ğ¿Ñ€Ğ¸ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Transforming Critiques into Cost-Effective Guidelines for LLMs', 'desc': 'This paper presents a framework that enhances the efficiency of large language models (LLMs) by transforming temporary critiques into reusable guidelines. It utilizes a file-based memory system and agent-controlled tool calls to streamline the inference process. The framework is evaluated using the Rubric Feedback Bench, which is designed for rubric-based learning assessments. Results show that the modified LLMs achieve comparable performance to traditional test-time refinement methods while significantly lowering inference costs.'}, 'zh': {'title': 'é™ä½æ¨ç†æˆæœ¬ï¼Œæå‡æ¨¡å‹æ•ˆç‡çš„åˆ›æ–°æ¡†æ¶', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ¡†æ¶ï¼Œé€šè¿‡æ–‡ä»¶åŸºç¡€çš„è®°å¿†ç³»ç»Ÿå’Œä»£ç†æ§åˆ¶çš„å·¥å…·è°ƒç”¨ï¼Œå°†ç¬æ—¶æ‰¹è¯„è½¬åŒ–ä¸ºå¯æ£€ç´¢çš„æŒ‡å¯¼æ–¹é’ˆï¼Œä»è€Œé™ä½æ¨ç†æˆæœ¬ã€‚è¯¥æ–¹æ³•åœ¨ä¸€ä¸ªæ–°çš„åŸºäºè¯„åˆ†æ ‡å‡†çš„å­¦ä¹ æ•°æ®é›†Rubric Feedback Benchä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå¢å¼ºå‹å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰èƒ½å¤Ÿè¿…é€Ÿè¾¾åˆ°æµ‹è¯•æ—¶ç²¾ç»†åŒ–ç®¡é“çš„æ€§èƒ½ï¼ŒåŒæ—¶æ˜¾è‘—é™ä½æ¨ç†æˆæœ¬ã€‚æ­¤æ¡†æ¶ä¸ºæé«˜æœºå™¨å­¦ä¹ æ¨¡å‹çš„æ•ˆç‡æä¾›äº†æ–°çš„æ€è·¯ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.05899', 'title': 'TowerMind: A Tower Defence Game Learning Environment and Benchmark for LLM as Agents', 'url': 'https://huggingface.co/papers/2601.05899', 'abstract': "A new tower defense-based environment called TowerMind is introduced for evaluating large language models' planning and decision-making capabilities with low computational requirements and multimodal observations.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent breakthroughs in Large Language Models (LLMs) have positioned them as a promising paradigm for agents, with long-term planning and decision-making emerging as core general-purpose capabilities for adapting to diverse scenarios and tasks. Real-time strategy (RTS) games serve as an ideal testbed for evaluating these two capabilities, as their inherent gameplay requires both macro-level strategic planning and micro-level tactical adaptation and action execution. Existing RTS game-based environments either suffer from relatively high computational demands or lack support for textual observations, which has constrained the use of RTS games for LLM evaluation. Motivated by this, we present TowerMind, a novel environment grounded in the tower defense (TD) subgenre of RTS games. TowerMind preserves the key evaluation strengths of RTS games for assessing LLMs, while featuring low computational demands and a multimodal observation space, including pixel-based, textual, and structured game-state representations. In addition, TowerMind supports the evaluation of model hallucination and provides a high degree of customizability. We design five benchmark levels to evaluate several widely used LLMs under different multimodal input settings. The results reveal a clear performance gap between LLMs and human experts across both capability and hallucination dimensions. The experiments further highlight key limitations in LLM behavior, such as inadequate planning validation, a lack of multifinality in decision-making, and inefficient action use. We also evaluate two classic reinforcement learning algorithms: Ape-X DQN and PPO. By offering a lightweight and multimodal design, TowerMind complements the existing RTS game-based environment landscape and introduces a new benchmark for the AI agent field. The source code is publicly available on GitHub(https://github.com/tb6147877/TowerMind).", 'score': 1, 'issue_id': 537, 'pub_date': '2026-01-09', 'pub_date_card': {'ru': '9 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 9', 'zh': '1æœˆ9æ—¥'}, 'hash': 'b20f557319791d13', 'authors': ['Dawei Wang', 'Chengming Zhou', 'Di Zhao', 'Xinyuan Liu', 'Marci Chi Ma', 'Gary Ushaw', 'Richard Davison'], 'affiliations': ['Newcastle University, United Kingdom', 'University of Auckland, New Zealand'], 'pdf_title_img': 'assets/pdf/title_img/2601.05899.jpg', 'data': {'categories': ['#hallucinations', '#games', '#open_source', '#multimodal', '#agents', '#rl', '#benchmark', '#reasoning'], 'emoji': 'ğŸ›¡ï¸', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ LLM Ñ‡ĞµÑ€ĞµĞ· tower defense ÑÑ€ĞµĞ´Ñƒ', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° TowerMind â€” Ğ½Ğ¾Ğ²Ğ°Ñ ÑÑ€ĞµĞ´Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ tower defense Ğ¸Ğ³Ñ€ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ½Ğ¸Ğ·ĞºĞ¸Ğ¼Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸. Ğ¡Ñ€ĞµĞ´Ğ° Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ñ‹Ğµ, Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ³Ñ€Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ LLM Ğ² ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. ĞŸÑ€Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ½Ñ‹Ğµ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ LLM Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ğ¼Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ² Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ»Ğ°Ğ½Ğ¾Ğ² Ğ¸ Ğ½ĞµÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. TowerMind Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¹ Ğ»Ğ°Ğ½Ğ´ÑˆĞ°Ñ„Ñ‚ ÑÑ€ĞµĞ´ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼.'}, 'en': {'title': 'TowerMind: A New Benchmark for Evaluating LLMs in Strategic Planning', 'desc': 'TowerMind is a new environment designed to test the planning and decision-making skills of large language models (LLMs) using a tower defense game format. It allows for low computational costs while providing multimodal observations, including text and game states. The environment helps identify the strengths and weaknesses of LLMs, revealing gaps in their performance compared to human experts, particularly in planning and decision-making. Additionally, TowerMind offers customizable benchmarks to evaluate LLMs and classic reinforcement learning algorithms, contributing to the advancement of AI agent research.'}, 'zh': {'title': 'TowerMindï¼šè¯„ä¼°è¯­è¨€æ¨¡å‹çš„æ–°åŸºå‡†', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„å¡”é˜²æ¸¸æˆç¯å¢ƒTowerMindï¼Œç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è§„åˆ’å’Œå†³ç­–èƒ½åŠ›ã€‚TowerMindå…·æœ‰ä½è®¡ç®—éœ€æ±‚å’Œå¤šæ¨¡æ€è§‚å¯Ÿç©ºé—´ï¼ŒåŒ…æ‹¬åƒç´ ã€æ–‡æœ¬å’Œç»“æ„åŒ–æ¸¸æˆçŠ¶æ€è¡¨ç¤ºã€‚é€šè¿‡è®¾è®¡äº”ä¸ªåŸºå‡†å…³å¡ï¼Œç ”ç©¶å‘ç°LLMsåœ¨èƒ½åŠ›å’Œå¹»è§‰ç»´åº¦ä¸Šä¸äººç±»ä¸“å®¶å­˜åœ¨æ˜æ˜¾å·®è·ï¼Œå¹¶æ­ç¤ºäº†LLMsåœ¨è§„åˆ’éªŒè¯å’Œå†³ç­–å¤šæ ·æ€§æ–¹é¢çš„å±€é™æ€§ã€‚TowerMindä¸ºAIä»£ç†é¢†åŸŸæä¾›äº†ä¸€ä¸ªæ–°çš„åŸºå‡†ï¼Œä¿ƒè¿›äº†ç°æœ‰å®æ—¶æˆ˜ç•¥æ¸¸æˆç¯å¢ƒçš„å‘å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.05851', 'title': 'Router-Suggest: Dynamic Routing for Multimodal Auto-Completion in Visually-Grounded Dialogs', 'url': 'https://huggingface.co/papers/2601.05851', 'abstract': 'Multimodal auto-completion leverages visual and textual context to improve real-time prediction accuracy in conversational interfaces, with a router framework enabling efficient model selection based on dialog context.  \t\t\t\t\tAI-generated summary \t\t\t\t Real-time multimodal auto-completion is essential for digital assistants, chatbots, design tools, and healthcare consultations, where user inputs rely on shared visual context. We introduce Multimodal Auto-Completion (MAC), a task that predicts upcoming characters in live chats using partially typed text and visual cues. Unlike traditional text-only auto-completion (TAC), MAC grounds predictions in multimodal context to better capture user intent. To enable this task, we adapt MMDialog and ImageChat to create benchmark datasets. We evaluate leading vision-language models (VLMs) against strong textual baselines, highlighting trade-offs in accuracy and efficiency. We present Router-Suggest, a router framework that dynamically selects between textual models and VLMs based on dialog context, along with a lightweight variant for resource-constrained environments. Router-Suggest achieves a 2.3x to 10x speedup over the best-performing VLM. A user study shows that VLMs significantly excel over textual models on user satisfaction, notably saving user typing effort and improving the quality of completions in multi-turn conversations. These findings underscore the need for multimodal context in auto-completions, leading to smarter, user-aware assistants.', 'score': 1, 'issue_id': 522, 'pub_date': '2026-01-09', 'pub_date_card': {'ru': '9 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 9', 'zh': '1æœˆ9æ—¥'}, 'hash': '7a9a52f4b2a51b55', 'authors': ['Sandeep Mishra', 'Devichand Budagam', 'Anubhab Mandal', 'Bishal Santra', 'Pawan Goyal', 'Manish Gupta'], 'affiliations': ['IIT Kharagpur', 'Microsoft'], 'pdf_title_img': 'assets/pdf/title_img/2601.05851.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#multimodal'], 'emoji': 'ğŸ’¬', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ğ¾Ğµ Ğ°Ğ²Ñ‚Ğ¾Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ (MAC), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ğµ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ñ‹ Ğ² Ñ‡Ğ°Ñ‚Ğµ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ Ğ½Ğ°Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² MMDialog Ğ¸ ImageChat, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ñ… vision-language Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¸ Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ±Ğ°Ğ·ĞµĞ»Ğ°Ğ¹Ğ½Ğ°Ğ¼Ğ¸. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ²ĞºĞ»Ğ°Ğ´ â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Router-Suggest, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¸ VLM Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°, Ñ‡Ñ‚Ğ¾ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ² 2.3-10x Ñ€Ğ°Ğ·. ĞŸĞ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ğ¾ ÑƒĞ´Ğ¾Ğ²Ğ»ĞµÑ‚Ğ²Ğ¾Ñ€Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ¸ ÑĞ½Ğ¸Ğ¶Ğ°ÑÑ‚ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ¿ĞµÑ‡Ğ°Ñ‚ÑŒ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¾Ğ±Ğ¾Ñ€Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ°Ñ….'}, 'en': {'title': 'Enhancing Conversations with Multimodal Auto-Completion', 'desc': 'This paper introduces Multimodal Auto-Completion (MAC), a method that enhances real-time predictions in conversational interfaces by utilizing both visual and textual information. Unlike traditional text-only auto-completion, MAC integrates visual cues to better understand user intent and improve prediction accuracy. The authors develop a router framework called Router-Suggest, which efficiently selects between vision-language models (VLMs) and textual models based on the context of the conversation. Their findings demonstrate that VLMs significantly outperform traditional models in user satisfaction and typing efficiency, highlighting the importance of multimodal context in creating smarter digital assistants.'}, 'zh': {'title': 'å¤šæ¨¡æ€è‡ªåŠ¨è¡¥å…¨ï¼šæ™ºèƒ½åŠ©æ‰‹çš„æ–°æ–¹å‘', 'desc': 'å¤šæ¨¡æ€è‡ªåŠ¨è¡¥å…¨åˆ©ç”¨è§†è§‰å’Œæ–‡æœ¬ä¸Šä¸‹æ–‡æ¥æé«˜å¯¹è¯ç•Œé¢çš„å®æ—¶é¢„æµ‹å‡†ç¡®æ€§ã€‚æˆ‘ä»¬æå‡ºäº†å¤šæ¨¡æ€è‡ªåŠ¨è¡¥å…¨ï¼ˆMACï¼‰ä»»åŠ¡ï¼Œé€šè¿‡éƒ¨åˆ†è¾“å…¥æ–‡æœ¬å’Œè§†è§‰çº¿ç´¢é¢„æµ‹å³å°†è¾“å…¥çš„å­—ç¬¦ã€‚ä¸ä¼ ç»Ÿçš„ä»…æ–‡æœ¬è‡ªåŠ¨è¡¥å…¨ï¼ˆTACï¼‰ä¸åŒï¼ŒMACåœ¨å¤šæ¨¡æ€ä¸Šä¸‹æ–‡ä¸­è¿›è¡Œé¢„æµ‹ï¼Œæ›´å¥½åœ°æ•æ‰ç”¨æˆ·æ„å›¾ã€‚æˆ‘ä»¬è¿˜ä»‹ç»äº†Router-Suggestæ¡†æ¶ï¼Œæ ¹æ®å¯¹è¯ä¸Šä¸‹æ–‡åŠ¨æ€é€‰æ‹©æ–‡æœ¬æ¨¡å‹å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼Œæ˜¾è‘—æé«˜äº†æ•ˆç‡å’Œç”¨æˆ·æ»¡æ„åº¦ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.05741', 'title': 'ViTNT-FIQA: Training-Free Face Image Quality Assessment with Vision Transformers', 'url': 'https://huggingface.co/papers/2601.05741', 'abstract': 'ViTNT-FIQA measures face image quality by analyzing patch embedding stability across Vision Transformer blocks with a single forward pass.  \t\t\t\t\tAI-generated summary \t\t\t\t Face Image Quality Assessment (FIQA) is essential for reliable face recognition systems. Current approaches primarily exploit only final-layer representations, while training-free methods require multiple forward passes or backpropagation. We propose ViTNT-FIQA, a training-free approach that measures the stability of patch embedding evolution across intermediate Vision Transformer (ViT) blocks. We demonstrate that high-quality face images exhibit stable feature refinement trajectories across blocks, while degraded images show erratic transformations. Our method computes Euclidean distances between L2-normalized patch embeddings from consecutive transformer blocks and aggregates them into image-level quality scores. We empirically validate this correlation on a quality-labeled synthetic dataset with controlled degradation levels. Unlike existing training-free approaches, ViTNT-FIQA requires only a single forward pass without backpropagation or architectural modifications. Through extensive evaluation on eight benchmarks (LFW, AgeDB-30, CFP-FP, CALFW, Adience, CPLFW, XQLFW, IJB-C), we show that ViTNT-FIQA achieves competitive performance with state-of-the-art methods while maintaining computational efficiency and immediate applicability to any pre-trained ViT-based face recognition model.', 'score': 1, 'issue_id': 530, 'pub_date': '2026-01-09', 'pub_date_card': {'ru': '9 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 9', 'zh': '1æœˆ9æ—¥'}, 'hash': 'a3ae1020afe47c87', 'authors': ['Guray Ozgur', 'Eduarda Caldeira', 'Tahar Chettaoui', 'Jan Niklas Kolf', 'Marco Huber', 'Naser Damer', 'Fadi Boutros'], 'affiliations': ['Fraunhofer IGD', 'TU Darmstadt'], 'pdf_title_img': 'assets/pdf/title_img/2601.05741.jpg', 'data': {'categories': ['#cv', '#benchmark'], 'emoji': 'ğŸ­', 'ru': {'title': 'Ğ¡Ñ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² ĞºĞ°Ğº Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ»Ğ¸Ñ† Ğ² Vision Transformer', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ViTNT-FIQA â€” Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ»Ğ¸Ñ† Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğµ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ğ¿Ğ°Ñ‚Ñ‡ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· ÑĞ»Ğ¾Ğ¸ Vision Transformer. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸Ğ·ÑƒÑÑ‚ÑÑ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ĞµĞ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ², Ğ° Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ñ…Ğ°Ğ¾Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ÑĞµÑ‚ ĞµĞ²ĞºĞ»Ğ¸Ğ´Ğ¾Ğ²Ñ‹ Ñ€Ğ°ÑÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ°Ğ¼Ğ¸ Ğ¸Ğ· Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ»Ğ¾ĞºĞ¾Ğ² Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° Ğ¸ Ğ°Ğ³Ñ€ĞµĞ³Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ñ… Ğ² Ğ¾Ñ†ĞµĞ½ĞºÑƒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´Ğ° Ğ±ĞµĞ· Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ²Ğ¾ÑÑŒĞ¼Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ….'}, 'en': {'title': 'Assessing Face Image Quality with Stability in Vision Transformers', 'desc': 'ViTNT-FIQA is a novel method for assessing the quality of face images by analyzing the stability of patch embeddings through Vision Transformer (ViT) blocks. Unlike traditional methods that rely on final-layer outputs or require multiple passes, this approach is training-free and only needs a single forward pass. It measures the consistency of feature refinement by calculating Euclidean distances between normalized patch embeddings from consecutive transformer blocks. The results show that high-quality images maintain stable trajectories, while lower-quality images exhibit erratic changes, making ViTNT-FIQA efficient and effective for face image quality assessment.'}, 'zh': {'title': 'ViTNT-FIQAï¼šé«˜æ•ˆçš„é¢éƒ¨å›¾åƒè´¨é‡è¯„ä¼°æ–¹æ³•', 'desc': 'ViTNT-FIQAæ˜¯ä¸€ç§é¢éƒ¨å›¾åƒè´¨é‡è¯„ä¼°æ–¹æ³•ï¼Œé€šè¿‡åˆ†æè§†è§‰å˜æ¢å™¨ï¼ˆViTï¼‰å—ä¸­è¡¥ä¸åµŒå…¥çš„ç¨³å®šæ€§æ¥è¡¡é‡å›¾åƒè´¨é‡ã€‚ä¸ç°æœ‰æ–¹æ³•ä¸åŒï¼ŒViTNT-FIQAä¸éœ€è¦è®­ç»ƒï¼Œä¸”åªéœ€ä¸€æ¬¡å‰å‘ä¼ æ’­å³å¯å®Œæˆè¯„ä¼°ã€‚è¯¥æ–¹æ³•é€šè¿‡è®¡ç®—ç›¸é‚»å˜æ¢å™¨å—ä¹‹é—´L2å½’ä¸€åŒ–è¡¥ä¸åµŒå…¥çš„æ¬§å‡ é‡Œå¾—è·ç¦»ï¼Œç”Ÿæˆå›¾åƒçº§è´¨é‡è¯„åˆ†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé«˜è´¨é‡çš„é¢éƒ¨å›¾åƒåœ¨ç‰¹å¾æç‚¼è¿‡ç¨‹ä¸­è¡¨ç°å‡ºç¨³å®šçš„è½¨è¿¹ï¼Œè€ŒåŠ£è´¨å›¾åƒåˆ™æ˜¾ç¤ºå‡ºä¸è§„åˆ™çš„å˜åŒ–ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.05870', 'title': 'IIB-LPO: Latent Policy Optimization via Iterative Information Bottleneck', 'url': 'https://huggingface.co/papers/2601.05870', 'abstract': 'Latent Policy Optimization via Iterative Information Bottleneck addresses exploration collapse in LLM reasoning by enabling topological branching of reasoning trajectories through information bottleneck principles.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in Reinforcement Learning with Verifiable Rewards (RLVR) for Large Language Model (LLM) reasoning have been hindered by a persistent challenge: exploration collapse. The semantic homogeneity of random rollouts often traps models in narrow, over-optimized behaviors. While existing methods leverage policy entropy to encourage exploration, they face inherent limitations. Global entropy regularization is susceptible to reward hacking, which can induce meaningless verbosity, whereas local token-selective updates struggle with the strong inductive bias of pre-trained models. To address this, we propose Latent Policy Optimization via Iterative Information Bottleneck (IIB-LPO), a novel approach that shifts exploration from statistical perturbation of token distributions to topological branching of reasoning trajectories. IIB-LPO triggers latent branching at high-entropy states to diversify reasoning paths and employs the Information Bottleneck principle both as a trajectory filter and a self-reward mechanism, ensuring concise and informative exploration. Empirical results across four mathematical reasoning benchmarks demonstrate that IIB-LPO achieves state-of-the-art performance, surpassing prior methods by margins of up to 5.3% in accuracy and 7.4% in diversity metrics.', 'score': 0, 'issue_id': 522, 'pub_date': '2026-01-09', 'pub_date_card': {'ru': '9 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 9', 'zh': '1æœˆ9æ—¥'}, 'hash': 'd2103f98d8252f93', 'authors': ['Huilin Deng', 'Hongchen Luo', 'Yue Zhu', 'Long Li', 'Zhuoyue Chen', 'Xinghao Zhao', 'Ming Li', 'Jihai Zhang', 'Mengchang Wang', 'Yang Cao', 'Yu Kang'], 'affiliations': ['DAMO Academy, Alibaba Group', 'Northeastern University', 'Shanghai Jiao Tong University', 'University of Science and Technology of China', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2601.05870.jpg', 'data': {'categories': ['#optimization', '#math', '#rl', '#training', '#reasoning'], 'emoji': 'ğŸŒ¿', 'ru': {'title': 'Ğ’ĞµÑ‚Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ ÑƒĞ·ĞºĞ¾Ğµ Ğ¼ĞµÑÑ‚Ğ¾', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ IIB-LPO Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ ĞºĞ¾Ğ»Ğ»Ğ°Ğ¿ÑĞ° Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ğ¼Ğ¸. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ² Ñ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸, Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ·ĞºĞ¾Ğ³Ğ¾ Ğ¼ĞµÑÑ‚Ğ° Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²ĞµÑ‚Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ. Ğ˜Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ ÑƒĞ·ĞºĞ¾Ğµ Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ ĞºĞ°Ğº Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ÑĞ°Ğ¼Ğ¾Ğ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ğ¸, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºÑ€Ğ°Ñ‚ĞºĞ¾ÑÑ‚ÑŒÑ Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 5,3% Ğ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ½Ğ° 7,4% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Branching Out: Enhancing LLM Reasoning with IIB-LPO', 'desc': 'This paper introduces Latent Policy Optimization via Iterative Information Bottleneck (IIB-LPO) to tackle the issue of exploration collapse in Large Language Model (LLM) reasoning. The authors highlight that traditional methods often lead to over-optimized behaviors due to semantic homogeneity in random rollouts. IIB-LPO innovatively shifts the focus from merely adjusting token distributions to creating diverse reasoning paths through topological branching at high-entropy states. The approach not only filters trajectories using the Information Bottleneck principle but also incorporates a self-reward mechanism, resulting in improved accuracy and diversity in reasoning tasks.'}, 'zh': {'title': 'é€šè¿‡ä¿¡æ¯ç“¶é¢ˆå®ç°æ½œåœ¨ç­–ç•¥ä¼˜åŒ–ï¼Œæ‰“ç ´æ¢ç´¢å´©æºƒ', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œç§°ä¸ºè¿­ä»£ä¿¡æ¯ç“¶é¢ˆä¸‹çš„æ½œåœ¨ç­–ç•¥ä¼˜åŒ–ï¼ˆIIB-LPOï¼‰ï¼Œæ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†ä¸­çš„æ¢ç´¢å´©æºƒé—®é¢˜ã€‚é€šè¿‡ä¿¡æ¯ç“¶é¢ˆåŸç†ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨é«˜ç†µçŠ¶æ€ä¸‹è§¦å‘æ½œåœ¨åˆ†æ”¯ï¼Œä»è€Œå¤šæ ·åŒ–æ¨ç†è·¯å¾„ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒIIB-LPOä¸å†ä¾èµ–äºç»Ÿè®¡æ‰°åŠ¨ï¼Œè€Œæ˜¯é€šè¿‡æ‹“æ‰‘åˆ†æ”¯æ¥å¢å¼ºæ¢ç´¢èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒIIB-LPOåœ¨å››ä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå‡†ç¡®ç‡å’Œå¤šæ ·æ€§æŒ‡æ ‡å‡è¶…è¿‡äº†ä¹‹å‰çš„æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.05699', 'title': 'Afri-MCQA: Multimodal Cultural Question Answering for African Languages', 'url': 'https://huggingface.co/papers/2601.05699', 'abstract': "Afri-MCQA benchmark demonstrates poor performance of open-weight LLMs in African languages, highlighting the need for culturally grounded pretraining and speech-first approaches in AI development.  \t\t\t\t\tAI-generated summary \t\t\t\t Africa is home to over one-third of the world's languages, yet remains underrepresented in AI research. We introduce Afri-MCQA, the first Multilingual Cultural Question-Answering benchmark covering 7.5k Q&A pairs across 15 African languages from 12 countries. The benchmark offers parallel English-African language Q&A pairs across text and speech modalities and was entirely created by native speakers. Benchmarking large language models (LLMs) on Afri-MCQA shows that open-weight models perform poorly across evaluated cultures, with near-zero accuracy on open-ended VQA when queried in native language or speech. To evaluate linguistic competence, we include control experiments meant to assess this specific aspect separate from cultural knowledge, and we observe significant performance gaps between native languages and English for both text and speech. These findings underscore the need for speech-first approaches, culturally grounded pretraining, and cross-lingual cultural transfer. To support more inclusive multimodal AI development in African languages, we release our Afri-MCQA under academic license or CC BY-NC 4.0 on HuggingFace (https://huggingface.co/datasets/Atnafu/Afri-MCQA)", 'score': 0, 'issue_id': 531, 'pub_date': '2026-01-09', 'pub_date_card': {'ru': '9 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 9', 'zh': '1æœˆ9æ—¥'}, 'hash': '41b7253b439939ec', 'authors': ['Atnafu Lambebo Tonja', 'Srija Anand', 'Emilio Villa-Cueva', 'Israel Abebe Azime', 'Jesujoba Oluwadara Alabi', 'Muhidin A. Mohamed', 'Debela Desalegn Yadeta', 'Negasi Haile Abadi', 'Abigail Oppong', 'Nnaemeka Casmir Obiefuna', 'Idris Abdulmumin', 'Naome A Etori', 'Eric Peter Wairagala', 'Kanda Patrick Tshinu', 'Imanigirimbabazi Emmanuel', 'Gabofetswe Malema', 'Alham Fikri Aji', 'David Ifeoluwa Adelani', 'Thamar Solorio'], 'affiliations': ['AI4Bharat, Indian Institute of Technology, Madras', 'Addis Ababa University', 'Aston University', 'Friedrich-Alexander University', 'Independent', 'Kabale University', 'Lelapa AI', 'Lesan AI', 'MBZUAI', 'Mila, McGill University & Canada CIFAR AI Chair', 'Saarland University', 'Tshwane University of Technology', 'University of Botswana', 'University of Minnesota - Twin Cities', 'University of Pretoria'], 'pdf_title_img': 'assets/pdf/title_img/2601.05699.jpg', 'data': {'categories': ['#open_source', '#audio', '#low_resource', '#multimodal', '#benchmark', '#multilingual', '#dataset'], 'emoji': 'ğŸŒ', 'ru': {'title': 'ĞÑ„Ñ€Ğ¸ĞºĞ°Ğ½ÑĞºĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¸ Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‚ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ğ¾-ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Afri-MCQA â€” Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ½Ğ° 15 Ğ°Ñ„Ñ€Ğ¸ĞºĞ°Ğ½ÑĞºĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ°Ñ…, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 7,5 Ñ‚Ñ‹ÑÑÑ‡Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ½Ğ° Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ¼ Ğ¸ Ğ°Ñ„Ñ€Ğ¸ĞºĞ°Ğ½ÑĞºĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ°Ñ… Ğ² Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼ Ğ¸ Ñ€ĞµÑ‡ĞµĞ²Ğ¾Ğ¼ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ°Ñ…. Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ² Ğ½Ğ° ÑÑ‚Ğ¾Ğ¼ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ½Ğ¸Ğ·ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ â€” Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸ Ğ½ÑƒĞ»ĞµĞ²ÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ½Ğ° Ñ€Ğ¾Ğ´Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ°Ñ… Ğ¸ Ğ² Ñ€ĞµÑ‡ĞµĞ²Ğ¾Ğ¼ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¸Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ¼ Ğ¸ Ğ°Ñ„Ñ€Ğ¸ĞºĞ°Ğ½ÑĞºĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ°Ğ¼Ğ¸, Ñ‡Ñ‚Ğ¾ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ° Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¿ĞµÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‡Ñ‘Ñ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ñ€ĞµÑ‡ĞµĞ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ğ¾-Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² ĞºÑ€Ğ¾ÑÑ-Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ‚Ñ€Ğ°Ğ½ÑÑ„ĞµÑ€Ğ° Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹.'}, 'en': {'title': 'Bridging the AI Gap: Empowering African Languages in Machine Learning', 'desc': 'The Afri-MCQA benchmark reveals that open-weight large language models (LLMs) struggle significantly with African languages, indicating a lack of cultural understanding in their training. This benchmark includes 7.5k question-and-answer pairs in 15 African languages, created by native speakers, and assesses both text and speech modalities. Results show that these models achieve near-zero accuracy in open-ended visual question answering when using native languages, highlighting the need for culturally relevant pretraining. The study emphasizes the importance of speech-first approaches and cross-lingual cultural transfer to improve AI performance in diverse linguistic contexts.'}, 'zh': {'title': 'æ¨åŠ¨éæ´²è¯­è¨€çš„AIå‘å±•ï¼Œé‡è§†æ–‡åŒ–ä¸è¯­éŸ³ï¼', 'desc': 'Afri-MCQAåŸºå‡†å±•ç¤ºäº†å¼€æ”¾æƒé‡çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨éæ´²è¯­è¨€ä¸Šçš„è¡¨ç°ä¸ä½³ï¼Œå¼ºè°ƒäº†åœ¨äººå·¥æ™ºèƒ½å‘å±•ä¸­éœ€è¦ä»¥æ–‡åŒ–ä¸ºåŸºç¡€çš„é¢„è®­ç»ƒå’Œä»¥è¯­éŸ³ä¸ºä¸»çš„æ–¹å¼ã€‚éæ´²æ‹¥æœ‰è¶…è¿‡ä¸‰åˆ†ä¹‹ä¸€çš„ä¸–ç•Œè¯­è¨€ï¼Œä½†åœ¨äººå·¥æ™ºèƒ½ç ”ç©¶ä¸­ä»ç„¶ä»£è¡¨æ€§ä¸è¶³ã€‚æˆ‘ä»¬æ¨å‡ºäº†Afri-MCQAï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªæ¶µç›–æ¥è‡ª12ä¸ªå›½å®¶çš„15ç§éæ´²è¯­è¨€çš„å¤šè¯­è¨€æ–‡åŒ–é—®ç­”åŸºå‡†ï¼ŒåŒ…å«7500å¯¹é—®ç­”ã€‚åŸºå‡†æµ‹è¯•æ˜¾ç¤ºï¼Œå¼€æ”¾æƒé‡æ¨¡å‹åœ¨è¯„ä¼°çš„æ–‡åŒ–ä¸­è¡¨ç°ä¸ä½³ï¼Œå°¤å…¶æ˜¯åœ¨ä½¿ç”¨æ¯è¯­æˆ–è¯­éŸ³è¿›è¡Œå¼€æ”¾å¼è§†è§‰é—®ç­”æ—¶å‡ ä¹æ²¡æœ‰å‡†ç¡®ç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.05376', 'title': 'The Persona Paradox: Medical Personas as Behavioral Priors in Clinical Language Models', 'url': 'https://huggingface.co/papers/2601.05376', 'abstract': "Persona conditioning in clinical language models produces context-dependent effects on performance and safety that vary systematically with professional role and interaction style, challenging assumptions of monotonic improvement in expert behavior.  \t\t\t\t\tAI-generated summary \t\t\t\t Persona conditioning can be viewed as a behavioral prior for large language models (LLMs) and is often assumed to confer expertise and improve safety in a monotonic manner. However, its effects on high-stakes clinical decision-making remain poorly characterized. We systematically evaluate persona-based control in clinical LLMs, examining how professional roles (e.g., Emergency Department physician, nurse) and interaction styles (bold vs.\\ cautious) influence behavior across models and medical tasks. We assess performance on clinical triage and patient-safety tasks using multidimensional evaluations that capture task accuracy, calibration, and safety-relevant risk behavior. We find systematic, context-dependent, and non-monotonic effects: Medical personas improve performance in critical care tasks, yielding gains of up to sim+20% in accuracy and calibration, but degrade performance in primary-care settings by comparable margins. Interaction style modulates risk propensity and sensitivity, but it's highly model-dependent. While aggregated LLM-judge rankings favor medical over non-medical personas in safety-critical cases, we found that human clinicians show moderate agreement on safety compliance (average Cohen's Îº= 0.43) but indicate a low confidence in 95.9\\% of their responses on reasoning quality. Our work shows that personas function as behavioral priors that introduce context-dependent trade-offs rather than guarantees of safety or expertise. The code is available at https://github.com/rsinghlab/Persona\\_Paradox.", 'score': 0, 'issue_id': 534, 'pub_date': '2026-01-08', 'pub_date_card': {'ru': '8 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 8', 'zh': '1æœˆ8æ—¥'}, 'hash': 'f5cdba803ffd9dc3', 'authors': ['Tassallah Abdullahi', 'Shrestha Ghosh', 'Hamish S Fraser', 'Daniel LeÃ³n Tramontini', 'Adeel Abbasi', 'Ghada Bourjeily', 'Carsten Eickhoff', 'Ritambhara Singh'], 'affiliations': ['Brown University', 'University of Tuebingen'], 'pdf_title_img': 'assets/pdf/title_img/2601.05376.jpg', 'data': {'categories': ['#interpretability', '#ethics', '#alignment'], 'emoji': 'âš•ï¸', 'ru': {'title': 'ĞŸĞµÑ€ÑĞ¾Ğ½Ñ‹ Ğ²Ñ€Ğ°Ñ‡ĞµĞ¹ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…: ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¸Ğ»Ğ¸ Ğ¸Ğ»Ğ»ÑĞ·Ğ¸Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸?', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ĞºĞ¾Ğ½Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆÑ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ğµ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ñ‹ (Ñ€Ğ¾Ğ»Ğ¸ Ğ²Ñ€Ğ°Ñ‡Ğ°, Ğ¼ĞµĞ´ÑĞµÑÑ‚Ñ€Ñ‹) Ğ½Ğµ Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ½Ğ¾Ñ‚Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°, Ğ° Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾-Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ñ‹Ğ¼ ÑÑ„Ñ„ĞµĞºÑ‚Ğ°Ğ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ»Ğ¸ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ¾Ğ»ĞµĞ¹ Ğ¸ ÑÑ‚Ğ¸Ğ»ĞµĞ¹ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ LLM Ğ² ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ² ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ° ~20% Ğ² Ğ¸Ğ½Ñ‚ĞµĞ½ÑĞ¸Ğ²Ğ½Ğ¾Ğ¹ Ñ‚ĞµÑ€Ğ°Ğ¿Ğ¸Ğ¸ Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğµ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ² Ğ°Ğ¼Ğ±ÑƒĞ»Ğ°Ñ‚Ğ¾Ñ€Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸ĞºĞµ. ĞŸĞ¾Ğ¼Ğ¸Ğ¼Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ° Ğ¸Ğ·Ğ¼ĞµÑ€ÑĞ»Ğ° Ğ¾Ñ‚ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºÑƒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ğ¾Ğµ Ñ Ñ€Ğ¸ÑĞºĞ¾Ğ¼ Ğ´Ğ»Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ°Ñ†Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ñ‹ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒÑÑ‚ ĞºĞ°Ğº Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ñ‡ĞµÑĞºĞ¸Ğµ Ğ°Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ñ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑĞ°Ğ¼Ğ¸, Ğ° Ğ½Ğµ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸ÑĞ¼Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¸Ğ·Ñ‹ Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Persona Conditioning: Context Matters in Clinical AI Performance', 'desc': 'This paper investigates how persona conditioning in clinical language models affects their performance and safety in medical tasks. It reveals that the impact of different professional roles and interaction styles on model behavior is not straightforward, challenging the idea that expert personas always lead to better outcomes. The study finds that while medical personas can enhance accuracy in critical care tasks, they may actually hinder performance in primary care scenarios. Overall, the results highlight that persona conditioning introduces complex trade-offs rather than consistent improvements in safety and expertise.'}, 'zh': {'title': 'è§’è‰²æ¡ä»¶åŒ–ï¼šåŒ»ç–—æ¨¡å‹ä¸­çš„åŒåˆƒå‰‘', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†åœ¨ä¸´åºŠè¯­è¨€æ¨¡å‹ä¸­ä½¿ç”¨è§’è‰²æ¡ä»¶åŒ–å¯¹æ€§èƒ½å’Œå®‰å…¨æ€§çš„å½±å“ã€‚ç ”ç©¶å‘ç°ï¼Œä¸åŒçš„ä¸“ä¸šè§’è‰²å’Œäº’åŠ¨é£æ ¼ä¼šç³»ç»Ÿæ€§åœ°å½±å“æ¨¡å‹åœ¨åŒ»ç–—ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚å°½ç®¡åŒ»ç–—è§’è‰²åœ¨å±æ€¥æŠ¤ç†ä»»åŠ¡ä¸­æé«˜äº†å‡†ç¡®æ€§ï¼Œä½†åœ¨åˆçº§æŠ¤ç†ç¯å¢ƒä¸­å´å¯èƒ½å¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œè§’è‰²æ¡ä»¶åŒ–å¹¶ä¸æ€»æ˜¯èƒ½ä¿è¯å®‰å…¨æˆ–ä¸“ä¸šæ€§ï¼Œè€Œæ˜¯å¼•å…¥äº†ä¾èµ–äºä¸Šä¸‹æ–‡çš„æƒè¡¡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.04175', 'title': 'Legal Alignment for Safe and Ethical AI', 'url': 'https://huggingface.co/papers/2601.04175', 'abstract': 'Legal alignment explores how legal principles and methods can guide AI system design to ensure safety, ethics, and compliance through three research directions involving rule adherence, legal reasoning methods, and structural blueprints for AI reliability and trust.  \t\t\t\t\tAI-generated summary \t\t\t\t Alignment of artificial intelligence (AI) encompasses the normative problem of specifying how AI systems should act and the technical problem of ensuring AI systems comply with those specifications. To date, AI alignment has generally overlooked an important source of knowledge and practice for grappling with these problems: law. In this paper, we aim to fill this gap by exploring how legal rules, principles, and methods can be leveraged to address problems of alignment and inform the design of AI systems that operate safely and ethically. This emerging field -- legal alignment -- focuses on three research directions: (1) designing AI systems to comply with the content of legal rules developed through legitimate institutions and processes, (2) adapting methods from legal interpretation to guide how AI systems reason and make decisions, and (3) harnessing legal concepts as a structural blueprint for confronting challenges of reliability, trust, and cooperation in AI systems. These research directions present new conceptual, empirical, and institutional questions, which include examining the specific set of laws that particular AI systems should follow, creating evaluations to assess their legal compliance in real-world settings, and developing governance frameworks to support the implementation of legal alignment in practice. Tackling these questions requires expertise across law, computer science, and other disciplines, offering these communities the opportunity to collaborate in designing AI for the better.', 'score': 0, 'issue_id': 531, 'pub_date': '2026-01-07', 'pub_date_card': {'ru': '7 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 7', 'zh': '1æœˆ7æ—¥'}, 'hash': 'ca62024488ec38f0', 'authors': ['Noam Kolt', 'Nicholas Caputo', 'Jack Boeglin', "Cullen O'Keefe", 'Rishi Bommasani', 'Stephen Casper', 'Mariano-Florentino CuÃ©llar', 'Noah Feldman', 'Iason Gabriel', 'Gillian K. Hadfield', 'Lewis Hammond', 'Peter Henderson', 'Atoosa Kasirzadeh', 'Seth Lazar', 'Anka Reuel', 'Kevin L. Wei', 'Jonathan Zittrain'], 'affiliations': ['Australian National University', 'Berkman Klein Center for Internet & Society', 'Carnegie Endowment for International Peace', 'Carnegie Mellon University', 'Centre for the Governance of AI', 'Cooperative AI Foundation', 'Harvard University', 'Hebrew University', 'Institute for Law & AI', 'Johns Hopkins University', 'MIT CSAIL', 'Oxford Martin AI Governance Initiative', 'Princeton University', 'School of Advanced Study University of London', 'Stanford University', 'University of Oxford', 'University of Pennsylvania', 'Vector Institute for Artificial Intelligence'], 'pdf_title_img': 'assets/pdf/title_img/2601.04175.jpg', 'data': {'categories': ['#ethics', '#alignment'], 'emoji': 'âš–ï¸', 'ru': {'title': 'ĞŸÑ€Ğ°Ğ²Ğ¾Ğ²Ğ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ: Ğ¾Ñ‚ ÑÑ€Ğ¸Ğ´Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ¾Ğ² Ğº Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ğ¼ AI ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¿Ñ€Ğ°Ğ²Ğ¾Ğ²Ğ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ (legal alignment) â€” Ğ½Ğ¾Ğ²Ğ¾Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ AI, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ€Ğ¸Ğ´Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ñ‹ Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ AI ÑĞ¸ÑÑ‚ĞµĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ‚Ñ€Ğ¸ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ: Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° AI ÑĞ¸ÑÑ‚ĞµĞ¼, ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ°Ğ²Ğ¾Ğ²Ñ‹Ñ… Ğ½Ğ¾Ñ€Ğ¼, Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² ÑÑ€Ğ¸Ğ´Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ‚Ğ¾Ğ»ĞºĞ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ AI, Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¾Ğ²Ñ‹Ñ… ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹ ĞºĞ°Ğº ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ğ¾Ğ¹ Ğ¾ÑĞ½Ğ¾Ğ²Ñ‹ Ğ´Ğ»Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ´Ğ¾Ğ²ĞµÑ€Ğ¸Ñ. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¼ĞµĞ¶Ğ´Ğ¸ÑÑ†Ğ¸Ğ¿Ğ»Ğ¸Ğ½Ğ°Ñ€Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸ÑÑ‚Ğ°Ğ¼Ğ¸ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ°Ğ²Ğ° Ğ¸ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ñ‹Ñ… Ğ½Ğ°ÑƒĞº Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñæ¡†æ¶ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ AI ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ´ĞµĞ¹ÑÑ‚Ğ²ÑƒÑÑ‰ĞµĞ¼Ñƒ Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ´Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ñƒ. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ€Ğ°ÑÑĞ¼Ğ¾Ñ‚Ñ€ĞµÑ‚ÑŒ ĞºĞ°Ğº ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ°ĞºĞ¾Ğ½Ñ‹, Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼Ñ‹Ğµ Ğº ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğ¼ AI ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼, Ñ‚Ğ°Ğº Ğ¸ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ Ğ¸Ñ… Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'Bridging Law and AI for Ethical Alignment', 'desc': 'This paper introduces the concept of legal alignment, which integrates legal principles into the design of AI systems to ensure they operate safely and ethically. It identifies three key research directions: ensuring AI compliance with legal rules, applying legal reasoning methods to AI decision-making, and using legal frameworks to enhance AI reliability and trust. The authors argue that leveraging legal knowledge can help address the normative and technical challenges of AI alignment. This interdisciplinary approach encourages collaboration between legal experts and computer scientists to create AI systems that adhere to established legal standards.'}, 'zh': {'title': 'æ³•å¾‹ä¸äººå·¥æ™ºèƒ½çš„å®Œç¾ç»“åˆ', 'desc': 'æ³•å¾‹å¯¹é½ç ”ç©¶å¦‚ä½•åˆ©ç”¨æ³•å¾‹åŸåˆ™å’Œæ–¹æ³•æŒ‡å¯¼äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„è®¾è®¡ï¼Œä»¥ç¡®ä¿å…¶å®‰å…¨æ€§ã€ä¼¦ç†æ€§å’Œåˆè§„æ€§ã€‚è¯¥ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨ä¸‰ä¸ªæ–¹å‘ï¼šä¸€æ˜¯è®¾è®¡ç¬¦åˆåˆæ³•æœºæ„å’Œç¨‹åºåˆ¶å®šçš„æ³•å¾‹è§„åˆ™çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿï¼›äºŒæ˜¯å€Ÿé‰´æ³•å¾‹è§£é‡Šçš„æ–¹æ³•æ¥æŒ‡å¯¼äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„æ¨ç†å’Œå†³ç­–ï¼›ä¸‰æ˜¯åˆ©ç”¨æ³•å¾‹æ¦‚å¿µä½œä¸ºåº”å¯¹äººå·¥æ™ºèƒ½ç³»ç»Ÿå¯é æ€§ã€ä¿¡ä»»å’Œåˆä½œæŒ‘æˆ˜çš„ç»“æ„è“å›¾ã€‚è¿™ä¸€æ–°å…´é¢†åŸŸä¸ºæ³•å¾‹ã€è®¡ç®—æœºç§‘å­¦ç­‰å¤šä¸ªå­¦ç§‘çš„åˆä½œæä¾›äº†æœºä¼šï¼Œä»¥å…±åŒè®¾è®¡æ›´å¥½çš„äººå·¥æ™ºèƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.03252', 'title': 'InfiniDepth: Arbitrary-Resolution and Fine-Grained Depth Estimation with Neural Implicit Fields', 'url': 'https://huggingface.co/papers/2601.03252', 'abstract': "InfiniDepth represents depth as neural implicit fields using a local implicit decoder, enabling continuous 2D coordinate querying for arbitrary-resolution depth estimation and superior performance in fine-detail regions.  \t\t\t\t\tAI-generated summary \t\t\t\t Existing depth estimation methods are fundamentally limited to predicting depth on discrete image grids. Such representations restrict their scalability to arbitrary output resolutions and hinder the geometric detail recovery. This paper introduces InfiniDepth, which represents depth as neural implicit fields. Through a simple yet effective local implicit decoder, we can query depth at continuous 2D coordinates, enabling arbitrary-resolution and fine-grained depth estimation. To better assess our method's capabilities, we curate a high-quality 4K synthetic benchmark from five different games, spanning diverse scenes with rich geometric and appearance details. Extensive experiments demonstrate that InfiniDepth achieves state-of-the-art performance on both synthetic and real-world benchmarks across relative and metric depth estimation tasks, particularly excelling in fine-detail regions. It also benefits the task of novel view synthesis under large viewpoint shifts, producing high-quality results with fewer holes and artifacts.", 'score': 71, 'issue_id': 448, 'pub_date': '2026-01-06', 'pub_date_card': {'ru': '6 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 6', 'zh': '1æœˆ6æ—¥'}, 'hash': '5aa0bd49032ba917', 'authors': ['Hao Yu', 'Haotong Lin', 'Jiawei Wang', 'Jiaxin Li', 'Yida Wang', 'Xueyang Zhang', 'Yue Wang', 'Xiaowei Zhou', 'Ruizhen Hu', 'Sida Peng'], 'affiliations': ['Li Auto', 'Shenzhen University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2601.03252.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#cv'], 'emoji': 'ğŸŒŠ', 'ru': {'title': 'ĞĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ»Ñ Ğ´Ğ»Ñ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ±ĞµÑĞºĞ¾Ğ½ĞµÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ', 'desc': 'Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ InfiniDepth â€” Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ½ĞµÑĞ²Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»ÑÑ… Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… ÑĞµÑ‚Ğ¾Ğº. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ½ĞµÑĞ²Ğ½Ñ‹Ğ¹ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€, ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ·Ğ°Ğ¿Ñ€Ğ°ÑˆĞ¸Ğ²Ğ°Ñ‚ÑŒ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ²ÑƒĞ¼ĞµÑ€Ğ½Ñ‹Ñ… ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚Ğ°Ñ…, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°Ñ‚ÑŒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ² Ğ»ÑĞ±Ğ¾Ğ¼ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ² Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ 4K Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ… Ñ Ğ±Ğ¾Ğ³Ğ°Ñ‚Ñ‹Ğ¼Ğ¸ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»ÑĞ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ¿Ñ€Ğ¸ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ Ğ¼ĞµĞ»ĞºĞ¸Ñ… Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹ Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğµ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ²Ğ¸Ğ´Ğ¾Ğ² Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸ÑĞ¼Ğ¸ ÑƒĞ³Ğ»Ğ° Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ°.'}, 'en': {'title': 'Revolutionizing Depth Estimation with Neural Implicit Fields', 'desc': "InfiniDepth introduces a novel approach to depth estimation by utilizing neural implicit fields, which allows for continuous querying of depth at any 2D coordinate. This method overcomes the limitations of traditional depth estimation techniques that rely on discrete image grids, enabling it to produce depth maps at arbitrary resolutions. The local implicit decoder enhances the model's ability to capture fine geometric details, making it particularly effective in complex scenes. Extensive testing on a high-quality 4K synthetic benchmark demonstrates that InfiniDepth outperforms existing methods in both synthetic and real-world scenarios, especially in areas requiring detailed depth recovery."}, 'zh': {'title': 'InfiniDepthï¼šçªç ´æ·±åº¦ä¼°è®¡çš„åˆ†è¾¨ç‡é™åˆ¶', 'desc': 'InfiniDepth æ˜¯ä¸€ç§å°†æ·±åº¦è¡¨ç¤ºä¸ºç¥ç»éšå¼åœºçš„æ–¹æ³•ï¼Œä½¿ç”¨å±€éƒ¨éšå¼è§£ç å™¨æ¥å®ç°ã€‚è¿™ç§æ–¹æ³•å…è®¸åœ¨è¿ç»­çš„äºŒç»´åæ ‡ä¸ŠæŸ¥è¯¢æ·±åº¦ï¼Œä»è€Œå®ç°ä»»æ„åˆ†è¾¨ç‡çš„æ·±åº¦ä¼°è®¡ï¼Œå¹¶åœ¨ç»†èŠ‚åŒºåŸŸè¡¨ç°å‡ºè‰²ã€‚ä¸ä¼ ç»Ÿçš„æ·±åº¦ä¼°è®¡æ–¹æ³•ç›¸æ¯”ï¼ŒInfiniDepth è§£å†³äº†åœ¨ç¦»æ•£å›¾åƒç½‘æ ¼ä¸Šé¢„æµ‹æ·±åº¦çš„é™åˆ¶ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°æ¢å¤å‡ ä½•ç»†èŠ‚ã€‚é€šè¿‡åœ¨äº”æ¬¾ä¸åŒæ¸¸æˆä¸­åˆ›å»ºçš„é«˜è´¨é‡4KåˆæˆåŸºå‡†ï¼Œå®éªŒè¡¨æ˜è¯¥æ–¹æ³•åœ¨åˆæˆå’ŒçœŸå®ä¸–ç•ŒåŸºå‡†æµ‹è¯•ä¸­å‡è¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶æ˜¯åœ¨ç»†èŠ‚ä¸°å¯Œçš„åŒºåŸŸã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.01554', 'title': 'MOSS Transcribe Diarize: Accurate Transcription with Speaker Diarization', 'url': 'https://huggingface.co/papers/2601.01554', 'abstract': 'A unified multimodal large language model for end-to-end speaker-attributed, time-stamped transcription with extended context window and strong generalization across benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Speaker-Attributed, Time-Stamped Transcription (SATS) aims to transcribe what is said and to precisely determine the timing of each speaker, which is particularly valuable for meeting transcription. Existing SATS systems rarely adopt an end-to-end formulation and are further constrained by limited context windows, weak long-range speaker memory, and the inability to output timestamps. To address these limitations, we present MOSS Transcribe Diarize, a unified multimodal large language model that jointly performs Speaker-Attributed, Time-Stamped Transcription in an end-to-end paradigm. Trained on extensive real wild data and equipped with a 128k context window for up to 90-minute inputs, MOSS Transcribe Diarize scales well and generalizes robustly. Across comprehensive evaluations, it outperforms state-of-the-art commercial systems on multiple public and in-house benchmarks.', 'score': 45, 'issue_id': 448, 'pub_date': '2026-01-04', 'pub_date_card': {'ru': '4 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 4', 'zh': '1æœˆ4æ—¥'}, 'hash': 'ee4ff0d589b12acd', 'authors': ['MOSI. AI', 'Donghua Yu', 'Zhengyuan Lin', 'Chen Yang', 'Yiyang Zhang', 'Hanfu Chen', 'Jingqi Chen', 'Ke Chen', 'Liwei Fan', 'Yi Jiang', 'Jie Zhu', 'Muchen Li', 'Wenxuan Wang', 'Yang Wang', 'Zhe Xu', 'Yitian Gong', 'Yuqian Zhang', 'Wenbo Zhang', 'Zhaoye Fei', 'Qinyuan Cheng', 'Shimin Li', 'Xipeng Qiu'], 'affiliations': ['MOSI.AI'], 'pdf_title_img': 'assets/pdf/title_img/2601.01554.jpg', 'data': {'categories': ['#multimodal', '#audio', '#benchmark'], 'emoji': 'ğŸ™ï¸', 'ru': {'title': 'Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑĞºĞ²Ğ¾Ğ·Ğ½Ğ¾Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ¿Ñ†Ğ¸Ğ¸ Ñ€ĞµÑ‡Ğ¸ Ñ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ†Ğ¸ĞµĞ¹ Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰Ğ¸Ñ… Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ñ‚Ğ¼ĞµÑ‚ĞºĞ°Ğ¼Ğ¸', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ MOSS Transcribe Diarize, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ ÑĞºĞ²Ğ¾Ğ·Ğ½ÑƒÑ Ñ‚Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ¿Ñ†Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸ Ñ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ†Ğ¸ĞµĞ¹ Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰Ğ¸Ñ… Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚ĞºĞ°Ğ¼Ğ¸ Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¼ end-to-end Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğµ Ğ¾ĞºĞ½Ğ¾ Ğ¸Ğ· 128k Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ·Ğ°Ğ¿Ğ¸ÑĞ¸ Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ´Ğ¾ 90 Ğ¼Ğ¸Ğ½ÑƒÑ‚ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ Ğ´Ğ°Ğ»ÑŒĞ½Ğ¸Ñ… Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑÑ… Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ñ‹ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰Ğ¸Ñ…. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ°Ğ»Ğ°ÑÑŒ Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¾Ğ±ÑŠĞµĞ¼Ğ°Ñ… Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½ÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¿Ğ¾ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ½Ğ° Ğ¿ÑƒĞ±Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¸ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ….'}, 'en': {'title': 'Revolutionizing Transcription with MOSS: Accurate, Context-Aware, and Speaker-Savvy!', 'desc': 'This paper introduces MOSS Transcribe Diarize, a large language model designed for Speaker-Attributed, Time-Stamped Transcription (SATS). It addresses the limitations of existing systems by providing an end-to-end solution that can handle long context windows and accurately track multiple speakers over extended periods. The model is trained on diverse real-world data, allowing it to generalize effectively across various benchmarks. Evaluation results show that MOSS Transcribe Diarize surpasses current leading commercial transcription systems in performance.'}, 'zh': {'title': 'ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹ï¼Œç²¾å‡†è½¬å½•ä¸æ—¶é—´æ ‡è®°', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ç»Ÿä¸€çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œåä¸ºMOSS Transcribe Diarizeï¼Œæ—¨åœ¨å®ç°ç«¯åˆ°ç«¯çš„è¯´è¯è€…å½’å±å’Œæ—¶é—´æˆ³è½¬å½•ã€‚è¯¥æ¨¡å‹å…‹æœäº†ç°æœ‰ç³»ç»Ÿåœ¨ä¸Šä¸‹æ–‡çª—å£ã€é•¿æ—¶é—´è¯´è¯è€…è®°å¿†å’Œæ—¶é—´æˆ³è¾“å‡ºæ–¹é¢çš„å±€é™æ€§ã€‚MOSS Transcribe Diarizeç»è¿‡å¤§é‡çœŸå®æ•°æ®è®­ç»ƒï¼Œèƒ½å¤Ÿå¤„ç†é•¿è¾¾90åˆ†é’Ÿçš„è¾“å…¥ï¼Œå¹¶ä¸”åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºç°æœ‰çš„å•†ä¸šç³»ç»Ÿã€‚è¯¥æ¨¡å‹çš„å¼ºå¤§æ³›åŒ–èƒ½åŠ›ä½¿å…¶åœ¨ä¼šè®®è½¬å½•ç­‰åº”ç”¨ä¸­å…·æœ‰é‡è¦ä»·å€¼ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.03233', 'title': 'LTX-2: Efficient Joint Audio-Visual Foundation Model', 'url': 'https://huggingface.co/papers/2601.03233', 'abstract': 'LTX-2 is an open-source audiovisual diffusion model that generates synchronized video and audio content using a dual-stream transformer architecture with cross-modal attention and classifier-free guidance.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent text-to-video diffusion models can generate compelling video sequences, yet they remain silent -- missing the semantic, emotional, and atmospheric cues that audio provides. We introduce LTX-2, an open-source foundational model capable of generating high-quality, temporally synchronized audiovisual content in a unified manner. LTX-2 consists of an asymmetric dual-stream transformer with a 14B-parameter video stream and a 5B-parameter audio stream, coupled through bidirectional audio-video cross-attention layers with temporal positional embeddings and cross-modality AdaLN for shared timestep conditioning. This architecture enables efficient training and inference of a unified audiovisual model while allocating more capacity for video generation than audio generation. We employ a multilingual text encoder for broader prompt understanding and introduce a modality-aware classifier-free guidance (modality-CFG) mechanism for improved audiovisual alignment and controllability. Beyond generating speech, LTX-2 produces rich, coherent audio tracks that follow the characters, environment, style, and emotion of each scene -- complete with natural background and foley elements. In our evaluations, the model achieves state-of-the-art audiovisual quality and prompt adherence among open-source systems, while delivering results comparable to proprietary models at a fraction of their computational cost and inference time. All model weights and code are publicly released.', 'score': 38, 'issue_id': 448, 'pub_date': '2026-01-06', 'pub_date_card': {'ru': '6 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 6', 'zh': '1æœˆ6æ—¥'}, 'hash': '67090a5585a6a40e', 'authors': ['Yoav HaCohen', 'Benny Brazowski', 'Nisan Chiprut', 'Yaki Bitterman', 'Andrew Kvochko', 'Avishai Berkowitz', 'Daniel Shalem', 'Daphna Lifschitz', 'Dudu Moshe', 'Eitan Porat', 'Eitan Richardson', 'Guy Shiran', 'Itay Chachy', 'Jonathan Chetboun', 'Michael Finkelson', 'Michael Kupchick', 'Nir Zabari', 'Nitzan Guetta', 'Noa Kotler', 'Ofir Bibi', 'Ori Gordon', 'Poriya Panet', 'Roi Benita', 'Shahar Armon', 'Victor Kulikov', 'Yaron Inger', 'Yonatan Shiftan', 'Zeev Melumian', 'Zeev Farbman'], 'affiliations': ['Lightricks'], 'pdf_title_img': 'assets/pdf/title_img/2601.03233.jpg', 'data': {'categories': ['#video', '#multimodal', '#audio', '#open_source', '#architecture', '#diffusion', '#training'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ¡Ğ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ·Ğ²ÑƒĞºĞ° Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¼ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞµ', 'desc': 'LTX-2 â€” ÑÑ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ´Ğ²ÑƒÑ…Ğ¿Ğ¾Ñ‚Ğ¾Ñ‡Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° Ñ ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ğ°ÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ¼ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ (14 Ğ¼Ğ»Ñ€Ğ´ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²) Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğ¼ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ¼ Ğ´Ğ»Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾ (5 Ğ¼Ğ»Ñ€Ğ´ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²), ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ½Ğ¾Ğµ ĞºÑ€Ğ¾ÑÑ-Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ classifier-free guidance Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¹ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑÑ€ĞµĞ´Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¸ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ° Ñ ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸ÑĞ¼Ğ¸, Ğ½Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ Ğ¸ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ².'}, 'en': {'title': 'LTX-2: Synchronized Audiovisual Generation Made Easy!', 'desc': 'LTX-2 is an innovative open-source model designed to generate synchronized video and audio content using a dual-stream transformer architecture. It features a 14B-parameter video stream and a 5B-parameter audio stream, which work together through cross-modal attention to create high-quality audiovisual outputs. The model employs a multilingual text encoder for better understanding of prompts and utilizes a modality-aware classifier-free guidance mechanism to enhance the alignment between audio and video. With its efficient design, LTX-2 achieves state-of-the-art audiovisual quality while being accessible and cost-effective compared to proprietary alternatives.'}, 'zh': {'title': 'LTX-2ï¼šåŒæ­¥ç”Ÿæˆé«˜è´¨é‡è§†å¬å†…å®¹çš„å¼€æºæ¨¡å‹', 'desc': 'LTX-2æ˜¯ä¸€ä¸ªå¼€æºçš„è§†å¬æ‰©æ•£æ¨¡å‹ï¼Œèƒ½å¤Ÿç”ŸæˆåŒæ­¥çš„è§†é¢‘å’ŒéŸ³é¢‘å†…å®¹ã€‚å®ƒé‡‡ç”¨äº†åŒæµå˜æ¢å™¨æ¶æ„ï¼Œç»“åˆäº†è·¨æ¨¡æ€æ³¨æ„åŠ›æœºåˆ¶å’Œæ— åˆ†ç±»å™¨å¼•å¯¼ã€‚è¯¥æ¨¡å‹çš„è®¾è®¡ä½¿å¾—è§†é¢‘ç”Ÿæˆçš„èƒ½åŠ›å¤§äºéŸ³é¢‘ç”Ÿæˆï¼ŒåŒæ—¶å®ç°äº†é«˜è´¨é‡çš„è§†å¬å†…å®¹ç”Ÿæˆã€‚LTX-2åœ¨ç”Ÿæˆè‡ªç„¶èƒŒæ™¯éŸ³æ•ˆå’Œç¬¦åˆåœºæ™¯æƒ…æ„Ÿçš„éŸ³è½¨æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä¸”åœ¨å¼€æºç³»ç»Ÿä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„è§†å¬è´¨é‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.22334', 'title': 'SciEvalKit: An Open-source Evaluation Toolkit for Scientific General Intelligence', 'url': 'https://huggingface.co/papers/2512.22334', 'abstract': 'SciEvalKit is a unified benchmarking toolkit for evaluating AI models across scientific disciplines, focusing on core scientific intelligence competencies and supporting diverse domains from physics to materials science.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce SciEvalKit, a unified benchmarking toolkit designed to evaluate AI models for science across a broad range of scientific disciplines and task capabilities. Unlike general-purpose evaluation platforms, SciEvalKit focuses on the core competencies of scientific intelligence, including Scientific Multimodal Perception, Scientific Multimodal Reasoning, Scientific Multimodal Understanding, Scientific Symbolic Reasoning, Scientific Code Generation, Science Hypothesis Generation and Scientific Knowledge Understanding. It supports six major scientific domains, spanning from physics and chemistry to astronomy and materials science. SciEvalKit builds a foundation of expert-grade scientific benchmarks, curated from real-world, domain-specific datasets, ensuring that tasks reflect authentic scientific challenges. The toolkit features a flexible, extensible evaluation pipeline that enables batch evaluation across models and datasets, supports custom model and dataset integration, and provides transparent, reproducible, and comparable results. By bridging capability-based evaluation and disciplinary diversity, SciEvalKit offers a standardized yet customizable infrastructure to benchmark the next generation of scientific foundation models and intelligent agents. The toolkit is open-sourced and actively maintained to foster community-driven development and progress in AI4Science.', 'score': 28, 'issue_id': 453, 'pub_date': '2026-12-26', 'pub_date_card': {'ru': '26 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 26', 'zh': '12æœˆ26æ—¥'}, 'hash': 'cc7b56f32bc290be', 'authors': ['Yiheng Wang', 'Yixin Chen', 'Shuo Li', 'Yifan Zhou', 'Bo Liu', 'Hengjian Gao', 'Jiakang Yuan', 'Jia Bu', 'Wanghan Xu', 'Yuhao Zhou', 'Xiangyu Zhao', 'Zhiwang Zhou', 'Fengxiang Wang', 'Haodong Duan', 'Songyang Zhang', 'Jun Yao', 'Han Deng', 'Yizhou Wang', 'Jiabei Xiao', 'Jiaqi Liu', 'Encheng Su', 'Yujie Liu', 'Weida Wang', 'Junchi Yao', 'Shenghe Zheng', 'Haoran Sun', 'Runmin Ma', 'Xiangchao Yan', 'Bo Zhang', 'Dongzhan Zhou', 'Shufei Zhang', 'Peng Ye', 'Xiaosong Wang', 'Shixiang Tang', 'Wenlong Zhang', 'Lei Bai'], 'affiliations': ['Shanghai Artificial Intelligence Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2512.22334.jpg', 'data': {'categories': ['#survey', '#benchmark', '#multimodal', '#science', '#dataset', '#plp', '#open_source'], 'emoji': 'ğŸ§ª', 'ru': {'title': 'Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° AI Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'SciEvalKit Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ AI Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ´Ğ¸ÑÑ†Ğ¸Ğ¿Ğ»Ğ¸Ğ½Ğ°Ñ…. ĞĞ°Ğ±Ğ¾Ñ€ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑĞµĞ¼ÑŒ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿ĞµÑ‚ĞµĞ½Ñ†Ğ¸Ğ¹ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°: Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ, Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ, Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ, ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ĞºĞ¾Ğ´Ğ° Ğ¸ Ğ²Ñ‹Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ³Ğ¸Ğ¿Ğ¾Ñ‚ĞµĞ·. ĞŸĞ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ ÑˆĞµÑÑ‚ÑŒ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ Ğ¾Ñ‚ Ñ„Ğ¸Ğ·Ğ¸ĞºĞ¸ Ğ´Ğ¾ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ğµ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. SciEvalKit Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ³Ğ¸Ğ±ĞºĞ¸Ğ¹ Ğ¸ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµĞ¼Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Benchmarking AI for Scientific Excellence', 'desc': 'SciEvalKit is a benchmarking toolkit specifically designed to evaluate AI models in various scientific fields. It emphasizes core competencies of scientific intelligence, such as multimodal perception and reasoning, and supports diverse domains like physics and materials science. The toolkit provides expert-grade benchmarks based on real-world datasets, ensuring that evaluations reflect genuine scientific challenges. With its flexible evaluation pipeline, SciEvalKit allows for batch evaluations and custom integrations, promoting transparency and reproducibility in AI assessments.'}, 'zh': {'title': 'SciEvalKitï¼šç§‘å­¦æ™ºèƒ½æ¨¡å‹è¯„ä¼°çš„ç»Ÿä¸€å·¥å…·', 'desc': 'SciEvalKitæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„åŸºå‡†å·¥å…·åŒ…ï¼Œæ—¨åœ¨è¯„ä¼°ç§‘å­¦é¢†åŸŸçš„äººå·¥æ™ºèƒ½æ¨¡å‹ã€‚å®ƒä¸“æ³¨äºç§‘å­¦æ™ºèƒ½çš„æ ¸å¿ƒèƒ½åŠ›ï¼ŒåŒ…æ‹¬å¤šæ¨¡æ€æ„ŸçŸ¥ã€æ¨ç†å’Œç†è§£ç­‰ã€‚è¯¥å·¥å…·åŒ…æ”¯æŒä»ç‰©ç†å­¦åˆ°ææ–™ç§‘å­¦ç­‰å…­ä¸ªä¸»è¦ç§‘å­¦é¢†åŸŸï¼Œç¡®ä¿ä»»åŠ¡åæ˜ çœŸå®çš„ç§‘å­¦æŒ‘æˆ˜ã€‚SciEvalKitæä¾›çµæ´»çš„è¯„ä¼°æµç¨‹ï¼Œæ”¯æŒæ‰¹é‡è¯„ä¼°å’Œè‡ªå®šä¹‰æ¨¡å‹é›†æˆï¼Œä¿ƒè¿›AIåœ¨ç§‘å­¦é¢†åŸŸçš„å‘å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.03193', 'title': 'UniCorn: Towards Self-Improving Unified Multimodal Models through Self-Generated Supervision', 'url': 'https://huggingface.co/papers/2601.03193', 'abstract': 'UniCorn, a self-improvement framework for unified multimodal models, addresses generation gaps through self-play and cognitive pattern reconstruction, achieving state-of-the-art results in text-to-image generation.  \t\t\t\t\tAI-generated summary \t\t\t\t While Unified Multimodal Models (UMMs) have achieved remarkable success in cross-modal comprehension, a significant gap persists in their ability to leverage such internal knowledge for high-quality generation. We formalize this discrepancy as Conduction Aphasia, a phenomenon where models accurately interpret multimodal inputs but struggle to translate that understanding into faithful and controllable synthesis. To address this, we propose UniCorn, a simple yet elegant self-improvement framework that eliminates the need for external data or teacher supervision. By partitioning a single UMM into three collaborative roles: Proposer, Solver, and Judge, UniCorn generates high-quality interactions via self-play and employs cognitive pattern reconstruction to distill latent understanding into explicit generative signals. To validate the restoration of multimodal coherence, we introduce UniCycle, a cycle-consistency benchmark based on a Text to Image to Text reconstruction loop. Extensive experiments demonstrate that UniCorn achieves comprehensive and substantial improvements over the base model across six general image generation benchmarks. Notably, it achieves SOTA performance on TIIF(73.8), DPG(86.8), CompBench(88.5), and UniCycle while further delivering substantial gains of +5.0 on WISE and +6.5 on OneIG. These results highlight that our method significantly enhances T2I generation while maintaining robust comprehension, demonstrating the scalability of fully self-supervised refinement for unified multimodal intelligence.', 'score': 25, 'issue_id': 450, 'pub_date': '2026-01-06', 'pub_date_card': {'ru': '6 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 6', 'zh': '1æœˆ6æ—¥'}, 'hash': '605fda1659be845f', 'authors': ['Ruiyan Han', 'Zhen Fang', 'XinYu Sun', 'Yuchen Ma', 'Ziheng Wang', 'Yu Zeng', 'Zehui Chen', 'Lin Chen', 'Wenxuan Huang', 'Wei-Jie Xu', 'Yi Cao', 'Feng Zhao'], 'affiliations': ['CUHK', 'ECNU', 'FDU', 'MoE Key Lab of BIPC, USTC', 'NJU', 'SUDA'], 'pdf_title_img': 'assets/pdf/title_img/2601.03193.jpg', 'data': {'categories': ['#training', '#benchmark', '#multimodal'], 'emoji': 'ğŸ¦„', 'ru': {'title': 'Ğ¡Ğ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· ÑĞ°Ğ¼Ğ¾Ğ¸Ğ³Ñ€Ñƒ Ğ¸ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ', 'desc': 'UniCorn â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº ÑĞ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°ÑÑ‚ Ñ‚ĞµĞºÑÑ‚ Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ğ½Ğ¾ Ğ¿Ğ»Ğ¾Ñ…Ğ¾ Ğ¸Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‚. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€ĞµÑˆĞ°ÑÑ‚ ÑÑ‚Ñƒ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ, Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑÑ ĞµĞ´Ğ¸Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° Ñ‚Ñ€Ğ¸ Ñ€Ğ¾Ğ»Ğ¸ (Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹, Ñ€ĞµÑˆĞ°Ñ‚ĞµĞ»ÑŒ Ğ¸ ÑÑƒĞ´ÑŒÑ), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²ÑƒÑÑ‚ Ñ‡ĞµÑ€ĞµĞ· ÑĞ°Ğ¼Ğ¾Ğ¸Ğ³Ñ€Ñƒ Ğ¸ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ»Ğ¸ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ, Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ UniCorn Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ñ‚ĞµĞºÑÑ‚-Ğ²-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'UniCorn: Bridging the Gap in Multimodal Generation', 'desc': 'UniCorn is a self-improvement framework designed for unified multimodal models (UMMs) that enhances their ability to generate high-quality outputs from multimodal inputs. It addresses the issue of Conduction Aphasia, where models can understand inputs but fail to produce accurate outputs. By using a self-play mechanism and cognitive pattern reconstruction, UniCorn allows the model to refine its generative capabilities without needing external data or supervision. The framework has shown significant improvements in text-to-image generation tasks, achieving state-of-the-art results across multiple benchmarks.'}, 'zh': {'title': 'UniCornï¼šè‡ªæˆ‘æ”¹è¿›çš„ç»Ÿä¸€å¤šæ¨¡æ€ç”Ÿæˆæ¡†æ¶', 'desc': 'UniCornæ˜¯ä¸€ä¸ªè‡ªæˆ‘æ”¹è¿›æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹åœ¨ç”Ÿæˆè´¨é‡ä¸Šçš„å·®è·ã€‚å®ƒé€šè¿‡è‡ªæˆ‘å¯¹å¼ˆå’Œè®¤çŸ¥æ¨¡å¼é‡æ„ï¼Œæå‡æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„æ•ˆæœã€‚è¯¥æ¡†æ¶å°†å•ä¸€çš„ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹åˆ†ä¸ºæè®®è€…ã€è§£å†³è€…å’Œè¯„åˆ¤è€…ä¸‰ä¸ªåä½œè§’è‰²ï¼Œä»è€Œå®ç°é«˜è´¨é‡çš„äº¤äº’ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒUniCornåœ¨å¤šä¸ªå›¾åƒç”ŸæˆåŸºå‡†ä¸Šæ˜¾è‘—æé«˜äº†æ€§èƒ½ï¼Œå±•ç¤ºäº†å®Œå…¨è‡ªæˆ‘ç›‘ç£çš„æ”¹è¿›æ–¹æ³•åœ¨ç»Ÿä¸€å¤šæ¨¡æ€æ™ºèƒ½ä¸­çš„å¯æ‰©å±•æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.02427', 'title': 'NitroGen: An Open Foundation Model for Generalist Gaming Agents', 'url': 'https://huggingface.co/papers/2601.02427', 'abstract': 'NitroGen is a vision-action foundation model trained on extensive gameplay data that demonstrates strong cross-game generalization and effective transfer learning capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce NitroGen, a vision-action foundation model for generalist gaming agents that is trained on 40,000 hours of gameplay videos across more than 1,000 games. We incorporate three key ingredients: 1) an internet-scale video-action dataset constructed by automatically extracting player actions from publicly available gameplay videos, 2) a multi-game benchmark environment that can measure cross-game generalization, and 3) a unified vision-action model trained with large-scale behavior cloning. NitroGen exhibits strong competence across diverse domains, including combat encounters in 3D action games, high-precision control in 2D platformers, and exploration in procedurally generated worlds. It transfers effectively to unseen games, achieving up to 52% relative improvement in task success rates over models trained from scratch. We release the dataset, evaluation suite, and model weights to advance research on generalist embodied agents.', 'score': 22, 'issue_id': 448, 'pub_date': '2026-01-04', 'pub_date_card': {'ru': '4 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 4', 'zh': '1æœˆ4æ—¥'}, 'hash': '047ac149d2360ba0', 'authors': ['LoÃ¯c Magne', 'Anas Awadalla', 'Guanzhi Wang', 'Yinzhen Xu', 'Joshua Belofsky', 'Fengyuan Hu', 'Joohwan Kim', 'Ludwig Schmidt', 'Georgia Gkioxari', 'Jan Kautz', 'Yisong Yue', 'Yejin Choi', 'Yuke Zhu', 'Linxi "Jim" Fan'], 'affiliations': ['Caltech', 'NVIDIA', 'Stanford', 'UChicago', 'UT Austin'], 'pdf_title_img': 'assets/pdf/title_img/2601.02427.jpg', 'data': {'categories': ['#cv', '#transfer_learning', '#dataset', '#open_source', '#robotics', '#benchmark', '#games', '#agents', '#training'], 'emoji': 'ğŸ®', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¸Ğ³Ñ€Ğ¾Ğ²Ğ¾Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒÑ‡Ğ¸Ñ‚ÑÑ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ¸Ñ‚ÑŒ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¸Ğ³Ñ€Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¸Ñ€Ğ°Ğ¼Ğ¸', 'desc': 'NitroGen â€” ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ-Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° 40 Ñ‚Ñ‹ÑÑÑ‡Ğ°Ñ… Ñ‡Ğ°ÑĞ¾Ğ² Ğ³ĞµĞ¹Ğ¼Ğ¿Ğ»ĞµÑ Ğ¸Ğ· Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 1000 Ğ¸Ğ³Ñ€. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°: Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹, Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡Ñ‘Ğ½Ğ½Ñ‹Ğ¹ Ğ¸Ğ· Ğ¿ÑƒĞ±Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¸Ğ³Ñ€Ğ¾Ğ²Ğ¾Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¸Ğ³Ñ€Ğ°Ğ¼Ğ¸ Ğ¸ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºÑƒÑ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½ÑƒÑ ÑĞµÑ‚ÑŒ, Ğ½Ğ°Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼ behavior cloning. ĞĞ³ĞµĞ½Ñ‚ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ĞºĞ¾Ğ¼Ğ¿ĞµÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¸Ğ³Ñ€Ğ¾Ğ²Ñ‹Ñ… Ğ¶Ğ°Ğ½Ñ€Ğ°Ñ… â€” Ğ¾Ñ‚ Ğ±Ğ¾ĞµĞ²Ñ‹Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ² 3D-Ğ¸Ğ³Ñ€Ğ°Ñ… Ğ´Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ² Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°Ñ… Ğ¸ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ğ½Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¼Ğ¸Ñ€Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ¸Ñ‚ÑÑ Ğ½Ğ° Ğ½ĞµĞ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ñ‹Ğµ Ğ¸Ğ³Ñ€Ñ‹, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ² 52% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ Ğ½ÑƒĞ»Ñ.'}, 'en': {'title': 'NitroGen: A Game-Changing AI for Cross-Game Mastery', 'desc': "NitroGen is a foundation model designed for gaming that learns from a vast amount of gameplay data, specifically 40,000 hours of videos from over 1,000 different games. It utilizes a large-scale dataset created by extracting player actions from these videos, allowing it to understand various gaming scenarios. The model is evaluated in a multi-game environment to test its ability to generalize across different games, showing significant improvements in performance compared to models that start from scratch. NitroGen's capabilities include handling complex tasks in various game genres, making it a valuable resource for developing generalist AI agents in gaming."}, 'zh': {'title': 'NitroGenï¼šé€šç”¨æ¸¸æˆä»£ç†çš„è§†è§‰-åŠ¨ä½œæ¨¡å‹', 'desc': 'NitroGenæ˜¯ä¸€ç§è§†è§‰-åŠ¨ä½œåŸºç¡€æ¨¡å‹ï¼Œä¸“ä¸ºé€šç”¨æ¸¸æˆä»£ç†è€Œè®¾è®¡ï¼Œç»è¿‡è¶…è¿‡40,000å°æ—¶çš„æ¸¸æˆè§†é¢‘è®­ç»ƒï¼Œæ¶µç›–äº†1,000å¤šæ¬¾æ¸¸æˆã€‚è¯¥æ¨¡å‹åˆ©ç”¨äº†ä¸‰ä¸ªå…³é”®è¦ç´ ï¼šä¸€ä¸ªé€šè¿‡è‡ªåŠ¨æå–ç©å®¶åŠ¨ä½œæ„å»ºçš„äº’è”ç½‘è§„æ¨¡è§†é¢‘-åŠ¨ä½œæ•°æ®é›†ï¼Œä¸€ä¸ªå¯ä»¥æµ‹é‡è·¨æ¸¸æˆæ³›åŒ–èƒ½åŠ›çš„å¤šæ¸¸æˆåŸºå‡†ç¯å¢ƒï¼Œä»¥åŠä¸€ä¸ªé€šè¿‡å¤§è§„æ¨¡è¡Œä¸ºå…‹éš†è®­ç»ƒçš„ç»Ÿä¸€è§†è§‰-åŠ¨ä½œæ¨¡å‹ã€‚NitroGenåœ¨å¤šç§é¢†åŸŸè¡¨ç°å‡ºè‰²ï¼ŒåŒ…æ‹¬3DåŠ¨ä½œæ¸¸æˆä¸­çš„æˆ˜æ–—ã€2Då¹³å°æ¸¸æˆä¸­çš„é«˜ç²¾åº¦æ§åˆ¶ï¼Œä»¥åŠç¨‹åºç”Ÿæˆä¸–ç•Œä¸­çš„æ¢ç´¢ã€‚å®ƒåœ¨æœªè§è¿‡çš„æ¸¸æˆä¸­æœ‰æ•ˆè½¬ç§»ï¼Œä»»åŠ¡æˆåŠŸç‡ç›¸æ¯”ä»é›¶å¼€å§‹è®­ç»ƒçš„æ¨¡å‹æé«˜äº†52%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.03044', 'title': 'SOP: A Scalable Online Post-Training System for Vision-Language-Action Models', 'url': 'https://huggingface.co/papers/2601.03044', 'abstract': 'A scalable online post-training system enables real-world robot policy adaptation through distributed, multi-task learning that maintains generality while improving task proficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-language-action (VLA) models achieve strong generalization through large-scale pre-training, but real-world deployment requires expert-level task proficiency in addition to broad generality. Existing post-training approaches for VLA models are typically offline, single-robot, or task-specific, limiting effective on-policy adaptation and scalable learning from real-world interaction. We introduce a Scalable Online Post-training (SOP) system that enables online, distributed, multi-task post-training of generalist VLA models directly in the physical world. SOP tightly couples execution and learning through a closed-loop architecture in which a fleet of robots continuously streams on-policy experience and human intervention signals to a centralized cloud learner, and asynchronously receives updated policies. This design supports prompt on-policy correction, scales experience collection through parallel deployment, and preserves generality during adaptation. SOP is agnostic to the choice of post-training algorithm; we instantiate it with both interactive imitation learning (HG-DAgger) and reinforcement learning (RECAP). Across a range of real-world manipulation tasks including cloth folding, box assembly, and grocery restocking, we show that SOP substantially improves the performance of large pretrained VLA models while maintaining a single shared policy across tasks. Effective post-training can be achieved within hours of real-world interaction, and performance scales near-linearly with the number of robots in the fleet. These results suggest that tightly coupling online learning with fleet-scale deployment is instrumental to enabling efficient, reliable, and scalable post-training of generalist robot policies in the physical world.', 'score': 19, 'issue_id': 451, 'pub_date': '2026-01-06', 'pub_date_card': {'ru': '6 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 6', 'zh': '1æœˆ6æ—¥'}, 'hash': 'abef7b7da8b389a8', 'authors': ['Mingjie Pan', 'Siyuan Feng', 'Qinglin Zhang', 'Xinchen Li', 'Jianheng Song', 'Chendi Qu', 'Yi Wang', 'Chuankang Li', 'Ziyu Xiong', 'Zhi Chen', 'Yi Liu', 'Jianlan Luo'], 'affiliations': ['Agibot Research', 'Shanghai Innovation Institute'], 'pdf_title_img': 'assets/pdf/title_img/2601.03044.jpg', 'data': {'categories': ['#multimodal', '#robotics', '#rl', '#training'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ñ‘Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº Ñ‡ĞµÑ€ĞµĞ· Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Scalable Online Post-training (SOP) Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ…-Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹Ğ½Ñ‹Ñ… (VLA) Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ° Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¼Ğ¸Ñ€Ğµ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ¸Ğ¼Ğ¸Ñ‚Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ, Ğ³Ğ´Ğµ Ñ„Ğ»Ğ¾Ñ‚ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾ ÑĞ¾Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ on-policy Ğ¸ Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¸Ñ… Ğ² Ğ¾Ğ±Ğ»Ğ°Ñ‡Ğ½Ñ‹Ğ¹ Ñ†ĞµĞ½Ñ‚Ñ€ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ°ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ğ¾ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºÑƒ. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ SOP Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ½Ğ° Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ÑÑ Ğ·Ğ° Ñ‡Ğ°ÑÑ‹ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¼Ğ¸Ñ€Ğ¾Ğ¼, Ğ° Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾ Ñ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ² Ğ³Ñ€ÑƒĞ¿Ğ¿Ğµ.'}, 'en': {'title': 'Scalable Learning for Real-World Robot Adaptation', 'desc': 'This paper presents a Scalable Online Post-training (SOP) system designed for improving robot policy adaptation in real-world scenarios. SOP allows multiple robots to learn and adapt their tasks simultaneously by continuously sharing their experiences and receiving updates from a centralized cloud learner. The system maintains the generality of the models while enhancing their proficiency in specific tasks through a closed-loop architecture. By utilizing both interactive imitation learning and reinforcement learning, SOP demonstrates significant performance improvements across various manipulation tasks, achieving effective post-training in just hours of real-world interaction.'}, 'zh': {'title': 'å¯æ‰©å±•çš„åœ¨çº¿åè®­ç»ƒç³»ç»Ÿæå‡æœºå™¨äººç­–ç•¥é€‚åº”èƒ½åŠ›', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§å¯æ‰©å±•çš„åœ¨çº¿åè®­ç»ƒç³»ç»Ÿï¼ˆSOPï¼‰ï¼Œæ—¨åœ¨é€šè¿‡åˆ†å¸ƒå¼å¤šä»»åŠ¡å­¦ä¹ å®ç°æœºå™¨äººç­–ç•¥çš„é€‚åº”ã€‚è¯¥ç³»ç»Ÿèƒ½å¤Ÿåœ¨ç‰©ç†ä¸–ç•Œä¸­ç›´æ¥è¿›è¡Œåœ¨çº¿åè®­ç»ƒï¼Œç»“åˆæ‰§è¡Œä¸å­¦ä¹ ï¼Œæ”¯æŒæœºå™¨äººå®æ—¶æ”¶é›†ç»éªŒå¹¶è¿›è¡Œç­–ç•¥æ›´æ–°ã€‚SOPåœ¨å¤šä¸ªçœŸå®æ“ä½œä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå¦‚æŠ˜å è¡£ç‰©ã€ç»„è£…ç›’å­å’Œè¡¥è´§ï¼ŒåŒæ—¶ä¿æŒä»»åŠ¡é—´çš„å…±äº«ç­–ç•¥ã€‚ç ”ç©¶è¡¨æ˜ï¼Œåœ¨çº¿å­¦ä¹ ä¸å¤§è§„æ¨¡éƒ¨ç½²çš„ç´§å¯†ç»“åˆå¯¹äºæé«˜é€šç”¨æœºå™¨äººç­–ç•¥çš„æ•ˆç‡å’Œå¯é æ€§è‡³å…³é‡è¦ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.02785', 'title': 'DreamStyle: A Unified Framework for Video Stylization', 'url': 'https://huggingface.co/papers/2601.02785', 'abstract': 'DreamStyle is a unified video stylization framework that supports multiple style conditions while addressing style inconsistency and temporal flicker through a specialized data curation pipeline and LoRA training approach.  \t\t\t\t\tAI-generated summary \t\t\t\t Video stylization, an important downstream task of video generation models, has not yet been thoroughly explored. Its input style conditions typically include text, style image, and stylized first frame. Each condition has a characteristic advantage: text is more flexible, style image provides a more accurate visual anchor, and stylized first frame makes long-video stylization feasible. However, existing methods are largely confined to a single type of style condition, which limits their scope of application. Additionally, their lack of high-quality datasets leads to style inconsistency and temporal flicker. To address these limitations, we introduce DreamStyle, a unified framework for video stylization, supporting (1) text-guided, (2) style-image-guided, and (3) first-frame-guided video stylization, accompanied by a well-designed data curation pipeline to acquire high-quality paired video data. DreamStyle is built on a vanilla Image-to-Video (I2V) model and trained using a Low-Rank Adaptation (LoRA) with token-specific up matrices that reduces the confusion among different condition tokens. Both qualitative and quantitative evaluations demonstrate that DreamStyle is competent in all three video stylization tasks, and outperforms the competitors in style consistency and video quality.', 'score': 17, 'issue_id': 448, 'pub_date': '2026-01-06', 'pub_date_card': {'ru': '6 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 6', 'zh': '1æœˆ6æ—¥'}, 'hash': '64e0b2c2ce043ee6', 'authors': ['Mengtian Li', 'Jinshu Chen', 'Songtao Zhao', 'Wanquan Feng', 'Pengqi Tu', 'Qian He'], 'affiliations': ['Intelligent Creation Lab, ByteDance'], 'pdf_title_img': 'assets/pdf/title_img/2601.02785.jpg', 'data': {'categories': ['#video', '#data', '#dataset', '#optimization', '#training'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° ÑÑ‚Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğ¹ ÑÑ‚Ğ¸Ğ»Ñ', 'desc': 'DreamStyle â€” ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑÑ‚Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ÑÑ‰Ğ°Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ñ‚Ğ¸Ğ¿Ğ¾Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğ¹ ÑÑ‚Ğ¸Ğ»Ñ: Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ, Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ-ÑÑ‚Ğ°Ğ»Ğ¾Ğ½ Ğ¸ ÑÑ‚Ğ¸Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ ĞºĞ°Ğ´Ñ€. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€ĞµÑˆĞ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ ÑÑ‚Ğ¸Ğ»Ñ Ğ¸ Ğ¼ĞµÑ€Ñ†Ğ°Ğ½Ğ¸Ñ ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ pipeline Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ LoRA Ğ´Ğ»Ñ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Image-to-Video Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Low-Rank Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰ĞµĞ¹ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ñ‹, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿ÑƒÑ‚Ğ°Ğ½Ğ¸Ñ†Ñƒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑĞ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ DreamStyle Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ ÑÑ‚Ğ¸Ğ»Ñ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ°.'}, 'en': {'title': 'Unified Video Stylization with DreamStyle', 'desc': 'DreamStyle is a comprehensive framework for video stylization that allows for multiple input style conditions, including text, style images, and stylized first frames. This approach addresses common issues in video generation, such as style inconsistency and temporal flicker, by utilizing a specialized data curation pipeline to ensure high-quality training data. The framework is based on a standard Image-to-Video (I2V) model and employs Low-Rank Adaptation (LoRA) to effectively manage different style conditions. Evaluations show that DreamStyle excels in maintaining style consistency and producing high-quality videos across all three stylization methods.'}, 'zh': {'title': 'DreamStyleï¼šç»Ÿä¸€çš„è§†é¢‘é£æ ¼åŒ–è§£å†³æ–¹æ¡ˆ', 'desc': 'DreamStyleæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„è§†é¢‘é£æ ¼åŒ–æ¡†æ¶ï¼Œæ”¯æŒå¤šç§é£æ ¼æ¡ä»¶ï¼ŒåŒæ—¶é€šè¿‡ä¸“é—¨çš„æ•°æ®ç­–åˆ’æµç¨‹å’ŒLoRAè®­ç»ƒæ–¹æ³•è§£å†³é£æ ¼ä¸ä¸€è‡´å’Œæ—¶é—´é—ªçƒçš„é—®é¢˜ã€‚è¯¥æ¡†æ¶æ”¯æŒæ–‡æœ¬å¼•å¯¼ã€é£æ ¼å›¾åƒå¼•å¯¼å’Œé¦–å¸§å¼•å¯¼çš„è§†é¢‘é£æ ¼åŒ–ï¼Œå……åˆ†åˆ©ç”¨äº†æ¯ç§æ¡ä»¶çš„ä¼˜åŠ¿ã€‚DreamStyleåŸºäºä¼ ç»Ÿçš„å›¾åƒåˆ°è§†é¢‘ï¼ˆI2Vï¼‰æ¨¡å‹ï¼Œå¹¶ä½¿ç”¨ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰è¿›è¡Œè®­ç»ƒï¼Œä»¥å‡å°‘ä¸åŒæ¡ä»¶æ ‡è®°ä¹‹é—´çš„æ··æ·†ã€‚ç»è¿‡å®šæ€§å’Œå®šé‡è¯„ä¼°ï¼ŒDreamStyleåœ¨é£æ ¼ä¸€è‡´æ€§å’Œè§†é¢‘è´¨é‡æ–¹é¢ä¼˜äºç«äº‰å¯¹æ‰‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.02780', 'title': 'MiMo-V2-Flash Technical Report', 'url': 'https://huggingface.co/papers/2601.02780', 'abstract': 'MiMo-V2-Flash is a sparse Mixture-of-Experts model with hybrid attention architecture and efficient distillation technique that achieves strong performance with reduced parameters and improved inference speed.  \t\t\t\t\tAI-generated summary \t\t\t\t We present MiMo-V2-Flash, a Mixture-of-Experts (MoE) model with 309B total parameters and 15B active parameters, designed for fast, strong reasoning and agentic capabilities. MiMo-V2-Flash adopts a hybrid attention architecture that interleaves Sliding Window Attention (SWA) with global attention, with a 128-token sliding window under a 5:1 hybrid ratio. The model is pre-trained on 27 trillion tokens with Multi-Token Prediction (MTP), employing a native 32k context length and subsequently extended to 256k. To efficiently scale post-training compute, MiMo-V2-Flash introduces a novel Multi-Teacher On-Policy Distillation (MOPD) paradigm. In this framework, domain-specialized teachers (e.g., trained via large-scale reinforcement learning) provide dense and token-level reward, enabling the student model to perfectly master teacher expertise. MiMo-V2-Flash rivals top-tier open-weight models such as DeepSeek-V3.2 and Kimi-K2, despite using only 1/2 and 1/3 of their total parameters, respectively. During inference, by repurposing MTP as a draft model for speculative decoding, MiMo-V2-Flash achieves up to 3.6 acceptance length and 2.6x decoding speedup with three MTP layers. We open-source both the model weights and the three-layer MTP weights to foster open research and community collaboration.', 'score': 16, 'issue_id': 448, 'pub_date': '2026-01-06', 'pub_date_card': {'ru': '6 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 6', 'zh': '1æœˆ6æ—¥'}, 'hash': '5e7a6fb684c9e6b5', 'authors': ['Bangjun Xiao', 'Bingquan Xia', 'Bo Yang', 'Bofei Gao', 'Bowen Shen', 'Chen Zhang', 'Chenhong He', 'Chiheng Lou', 'Fuli Luo', 'Gang Wang', 'Gang Xie', 'Hailin Zhang', 'Hanglong Lv', 'Hanyu Li', 'Heyu Chen', 'Hongshen Xu', 'Houbin Zhang', 'Huaqiu Liu', 'Jiangshan Duo', 'Jianyu Wei', 'Jiebao Xiao', 'Jinhao Dong', 'Jun Shi', 'Junhao Hu', 'Kainan Bao', 'Kang Zhou', 'Lei Li', 'Liang Zhao', 'Linghao Zhang', 'Peidian Li', 'Qianli Chen', 'Shaohui Liu', 'Shihua Yu', 'Shijie Cao', 'Shimao Chen', 'Shouqiu Yu', 'Shuo Liu', 'Tianling Zhou', 'Weijiang Su', 'Weikun Wang', 'Wenhan Ma', 'Xiangwei Deng', 'Bohan Mao', 'Bowen Ye', 'Can Cai', 'Chenghua Wang', 'Chengxuan Zhu', 'Chong Ma', 'Chun Chen', 'Chunan Li', 'Dawei Zhu', 'Deshan Xiao', 'Dong Zhang', 'Duo Zhang', 'Fangyue Liu', 'Feiyu Yang', 'Fengyuan Shi', 'Guoan Wang', 'Hao Tian', 'Hao Wu', 'Heng Qu', 'Hongfei Yi', 'Hongxu An', 'Hongyi Guan', 'Xing Zhang', 'Yifan Song', 'Yihan Yan', 'Yihao Zhao', 'Yingchun Lai', 'Yizhao Gao', 'Yu Cheng', 'Yuanyuan Tian', 'Yudong Wang', 'Zhen Tang', 'Zhengju Tang', 'Zhengtao Wen', 'Zhichao Song', 'Zhixian Zheng', 'Zihan Jiang', 'Jian Wen', 'Jiarui Sun', 'Jiawei Li', 'Jinlong Xue', 'Jun Xia', 'Kai Fang', 'Menghang Zhu', 'Nuo Chen', 'Qian Tu', 'Qihao Zhang', 'Qiying Wang', 'Rang Li', 'Rui Ma', 'Shaolei Zhang', 'Shengfan Wang', 'Shicheng Li', 'Shuhao Gu', 'Shuhuai Ren', 'Sirui Deng', 'Tao Guo', 'Tianyang Lu', 'Weiji Zhuang', 'Weikang Zhang', 'Weimin Xiong', 'Wenshan Huang', 'Wenyu Yang', 'Xin Zhang', 'Xing Yong', 'Xu Wang', 'Xueyang Xie', 'Yilin Jiang', 'Yixin Yang', 'Yongzhe He', 'Yu Tu', 'Yuanliang Dong', 'Yuchen Liu', 'Yue Ma', 'Yue Yu', 'Yuxing Xiang', 'Zhaojun Huang', 'Zhenru Lin', 'Zhipeng Xu', 'Zhiyang Chen', 'Zhonghua Deng', 'Zihan Zhang', 'Zihao Yue'], 'affiliations': ['Xiaomi'], 'pdf_title_img': 'assets/pdf/title_img/2601.02780.jpg', 'data': {'categories': ['#small_models', '#optimization', '#reasoning', '#open_source', '#architecture', '#long_context', '#inference', '#agents', '#training'], 'emoji': 'âš¡', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒÑĞºĞ°Ñ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ñ… ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'MiMo-V2-Flash â€” ÑÑ‚Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Mixture-of-Experts Ñ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¼ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 309 Ğ¼Ğ»Ñ€Ğ´ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ 15 Ğ¼Ğ»Ñ€Ğ´ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ inference. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Multi-Token Prediction Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° 27 Ñ‚Ñ€Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ, Ñ‡ĞµÑ€ĞµĞ´ÑƒÑÑ‰ĞµĞµÑÑ ÑĞºĞ¾Ğ»ÑŒĞ·ÑÑ‰ĞµĞµ Ğ¾ĞºĞ½Ğ¾ Ñ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ² ÑĞ¾Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¸ 5:1. Ğ”Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ post-training Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Multi-Teacher On-Policy Distillation, Ğ³Ğ´Ğµ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿ĞµÑ€ĞµĞ´Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ ÑÑ‚ÑƒĞ´ĞµĞ½Ñ‚ÑĞºĞ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². Ğ‘Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ ÑĞ¿ĞµĞºÑƒĞ»ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ 2.6-ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ inference, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ½Ğ°Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Efficient Power: MiMo-V2-Flash Redefines MoE Performance', 'desc': 'MiMo-V2-Flash is a state-of-the-art Mixture-of-Experts (MoE) model that utilizes a hybrid attention mechanism to enhance performance while minimizing the number of active parameters. It features a unique combination of Sliding Window Attention and global attention, allowing it to process information efficiently over long contexts. The model is trained on an extensive dataset of 27 trillion tokens and employs a novel Multi-Teacher On-Policy Distillation technique to improve its learning from specialized teachers. As a result, MiMo-V2-Flash achieves competitive performance with significantly fewer parameters compared to leading models, while also accelerating inference speed through innovative decoding strategies.'}, 'zh': {'title': 'é«˜æ•ˆæ¨ç†ä¸å¼ºå¤§èƒ½åŠ›çš„ä¸“å®¶æ··åˆæ¨¡å‹', 'desc': 'MiMo-V2-Flashæ˜¯ä¸€ç§ç¨€ç–çš„ä¸“å®¶æ··åˆæ¨¡å‹ï¼Œé‡‡ç”¨æ··åˆæ³¨æ„åŠ›æ¶æ„å’Œé«˜æ•ˆçš„è’¸é¦æŠ€æœ¯ï¼Œèƒ½å¤Ÿåœ¨å‡å°‘å‚æ•°çš„åŒæ—¶å®ç°å¼ºå¤§çš„æ€§èƒ½å’Œæ›´å¿«çš„æ¨ç†é€Ÿåº¦ã€‚è¯¥æ¨¡å‹å…·æœ‰3090äº¿ä¸ªæ€»å‚æ•°å’Œ150äº¿ä¸ªæ´»è·ƒå‚æ•°ï¼Œä¸“ä¸ºå¿«é€Ÿæ¨ç†å’Œæ™ºèƒ½èƒ½åŠ›è®¾è®¡ã€‚å®ƒåœ¨27ä¸‡äº¿ä¸ªæ ‡è®°ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œä½¿ç”¨å¤šæ ‡è®°é¢„æµ‹æ–¹æ³•ï¼Œå¹¶å¼•å…¥äº†æ–°çš„å¤šæ•™å¸ˆåœ¨çº¿è’¸é¦æ¡†æ¶ï¼Œä»¥æé«˜åæœŸè®­ç»ƒçš„è®¡ç®—æ•ˆç‡ã€‚å°½ç®¡å‚æ•°æ•°é‡ä»…ä¸ºå…¶ä»–é¡¶çº§æ¨¡å‹çš„ä¸€åŠæˆ–ä¸‰åˆ†ä¹‹ä¸€ï¼ŒMiMo-V2-Flashåœ¨æ¨ç†æ—¶ä»èƒ½å®ç°æ˜¾è‘—çš„é€Ÿåº¦æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.01874', 'title': 'CogFlow: Bridging Perception and Reasoning through Knowledge Internalization for Visual Mathematical Problem Solving', 'url': 'https://huggingface.co/papers/2601.01874', 'abstract': 'Visual mathematical problem solving remains challenging for multimodal large language models, prompting the development of CogFlow, a cognitive-inspired three-stage framework that enhances perception, internalization, and reasoning through synergistic rewards and visual-gated policy optimization.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite significant progress, multimodal large language models continue to struggle with visual mathematical problem solving. Some recent works recognize that visual perception is a bottleneck in visual mathematical reasoning, but their solutions are limited to improving the extraction and interpretation of visual inputs. Notably, they all ignore the key issue of whether the extracted visual cues are faithfully integrated and properly utilized in subsequent reasoning. Motivated by this, we present CogFlow, a novel cognitive-inspired three-stage framework that incorporates a knowledge internalization stage, explicitly simulating the hierarchical flow of human reasoning: perceptionRightarrowinternalizationRightarrowreasoning. Inline with this hierarchical flow, we holistically enhance all its stages. We devise Synergistic Visual Rewards to boost perception capabilities in parametric and semantic spaces, jointly improving visual information extraction from symbols and diagrams. To guarantee faithful integration of extracted visual cues into subsequent reasoning, we introduce a Knowledge Internalization Reward model in the internalization stage, bridging perception and reasoning. Moreover, we design a Visual-Gated Policy Optimization algorithm to further enforce the reasoning is grounded with the visual knowledge, preventing models seeking shortcuts that appear coherent but are visually ungrounded reasoning chains. Moreover, we contribute a new dataset MathCog for model training, which contains samples with over 120K high-quality perception-reasoning aligned annotations. Comprehensive experiments and analysis on commonly used visual mathematical reasoning benchmarks validate the superiority of the proposed CogFlow.', 'score': 16, 'issue_id': 448, 'pub_date': '2026-01-05', 'pub_date_card': {'ru': '5 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 5', 'zh': '1æœˆ5æ—¥'}, 'hash': '6623fd64c385ec07', 'authors': ['Shuhang Chen', 'Yunqiu Xu', 'Junjie Xie', 'Aojun Lu', 'Tao Feng', 'Zeying Huang', 'Ning Zhang', 'Yi Sun', 'Yi Yang', 'Hangjie Yuan'], 'affiliations': ['Intelligent Learning', 'Sichuan University', 'Tsinghua University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2601.01874.jpg', 'data': {'categories': ['#multimodal', '#dataset', '#reasoning', '#optimization', '#math', '#rlhf', '#benchmark'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞšĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ñ‚Ğ¾Ğº: Ğ¾Ñ‚ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğº Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° CogFlow â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»Ñ‘Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¿ÑĞ¸Ñ…Ğ¾Ğ»Ğ¾Ğ³Ğ¸ĞµĞ¹, Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ñ‚Ñ€Ñ‘Ñ… ÑÑ‚Ğ°Ğ´Ğ¸Ğ¹: Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸, Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ, ĞºĞ°Ğ¶Ğ´Ğ°Ñ Ğ¸Ğ· ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ÑÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ ÑĞ¸Ğ½ĞµÑ€Ğ³ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ² Ğ¸ Ğ´Ğ¸Ğ°Ğ³Ñ€Ğ°Ğ¼Ğ¼, Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ MathCog Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 120K Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹, Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'CogFlow: Enhancing Visual Mathematical Reasoning in AI', 'desc': 'This paper introduces CogFlow, a new framework designed to improve how multimodal large language models solve visual mathematical problems. It consists of three stages: perception, internalization, and reasoning, which mimic human cognitive processes. The framework enhances visual perception through Synergistic Visual Rewards and ensures that visual cues are effectively integrated into reasoning with a Knowledge Internalization Reward. Additionally, it employs a Visual-Gated Policy Optimization algorithm to maintain a strong connection between visual knowledge and reasoning, supported by a new dataset called MathCog for training.'}, 'zh': {'title': 'CogFlowï¼šæå‡è§†è§‰æ•°å­¦æ¨ç†çš„è®¤çŸ¥æ¡†æ¶', 'desc': 'è§†è§‰æ•°å­¦é—®é¢˜è§£å†³å¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†CogFlowï¼Œä¸€ä¸ªå—è®¤çŸ¥å¯å‘çš„ä¸‰é˜¶æ®µæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ååŒå¥–åŠ±å’Œè§†è§‰é—¨æ§ç­–ç•¥ä¼˜åŒ–æ¥å¢å¼ºæ„ŸçŸ¥ã€å†…åŒ–å’Œæ¨ç†èƒ½åŠ›ã€‚CogFlowçš„è®¾è®¡æ¨¡æ‹Ÿäº†äººç±»æ¨ç†çš„å±‚æ¬¡æµç¨‹ï¼Œç¡®ä¿æå–çš„è§†è§‰çº¿ç´¢èƒ½å¤Ÿæœ‰æ•ˆæ•´åˆå¹¶ç”¨äºåç»­æ¨ç†ã€‚é€šè¿‡å¼•å…¥æ–°çš„å¥–åŠ±æœºåˆ¶å’Œæ•°æ®é›†ï¼ŒCogFlowåœ¨è§†è§‰æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­å±•ç°äº†æ˜¾è‘—çš„ä¼˜åŠ¿ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.01321', 'title': 'Digital Twin AI: Opportunities and Challenges from Large Language Models to World Models', 'url': 'https://huggingface.co/papers/2601.01321', 'abstract': 'AI integration in digital twins follows a four-stage framework encompassing modeling, mirroring, intervention, and autonomous management, leveraging physics-informed approaches, generative AI, and foundation models for intelligent system operation.  \t\t\t\t\tAI-generated summary \t\t\t\t Digital twins, as precise digital representations of physical systems, have evolved from passive simulation tools into intelligent and autonomous entities through the integration of artificial intelligence technologies. This paper presents a unified four-stage framework that systematically characterizes AI integration across the digital twin lifecycle, spanning modeling, mirroring, intervention, and autonomous management. By synthesizing existing technologies and practices, we distill a unified four-stage framework that systematically characterizes how AI methodologies are embedded across the digital twin lifecycle: (1) modeling the physical twin through physics-based and physics-informed AI approaches, (2) mirroring the physical system into a digital twin with real-time synchronization, (3) intervening in the physical twin through predictive modeling, anomaly detection, and optimization strategies, and (4) achieving autonomous management through large language models, foundation models, and intelligent agents. We analyze the synergy between physics-based modeling and data-driven learning, highlighting the shift from traditional numerical solvers to physics-informed and foundation models for physical systems. Furthermore, we examine how generative AI technologies, including large language models and generative world models, transform digital twins into proactive and self-improving cognitive systems capable of reasoning, communication, and creative scenario generation. Through a cross-domain review spanning eleven application domains, including healthcare, aerospace, smart manufacturing, robotics, and smart cities, we identify common challenges related to scalability, explainability, and trustworthiness, and outline directions for responsible AI-driven digital twin systems.', 'score': 7, 'issue_id': 457, 'pub_date': '2026-01-04', 'pub_date_card': {'ru': '4 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 4', 'zh': '1æœˆ4æ—¥'}, 'hash': 'bcc1eff55563ff64', 'authors': ['Rong Zhou', 'Dongping Chen', 'Zihan Jia', 'Yao Su', 'Yixin Liu', 'Yiwen Lu', 'Dongwei Shi', 'Yue Huang', 'Tianyang Xu', 'Yi Pan', 'Xinliang Li', 'Yohannes Abate', 'Qingyu Chen', 'Zhengzhong Tu', 'Yu Yang', 'Yu Zhang', 'Qingsong Wen', 'Gengchen Mai', 'Sunyang Fu', 'Jiachen Li', 'Xuyu Wang', 'Ziran Wang', 'Jing Huang', 'Tianming Liu', 'Yong Chen', 'Lichao Sun', 'Lifang He'], 'affiliations': ['Childrens Hospital of Philadelphia', 'Columbia University', 'Florida International University', 'Lehigh University', 'Purdue University', 'Squirrel Ai Learning', 'Stanford University', 'Texas A&M University', 'University of California, Riverside', 'University of Georgia', 'University of Maryland', 'University of New South Wales', 'University of Notre Dame', 'University of Pennsylvania', 'University of Texas Health Science Center at Houston', 'University of Texas at Austin', 'Worcester Polytechnic Institute', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2601.01321.jpg', 'data': {'categories': ['#agents', '#robotics', '#healthcare', '#science', '#optimization', '#multimodal'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞÑ‚ Ğ¿Ğ°ÑÑĞ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ° Ğº Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ¼Ñƒ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ°Ğ³ĞµĞ½Ñ‚Ñƒ: Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ ÑÑ‚Ğ°Ğ¿Ğ° ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ñ†Ğ¸Ñ„Ñ€Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ¸ĞºĞ¾Ğ²', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ñ‡ĞµÑ‚Ñ‹Ñ€Ñ‘Ñ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ² Ñ†Ğ¸Ñ„Ñ€Ğ¾Ğ²Ñ‹Ğµ Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ¸ĞºĞ¸. ĞŸĞµÑ€Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ¿ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ„Ğ¸Ğ·Ğ¸ĞºĞ¾-Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ¸ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ° Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ¾ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ğ»ÑĞµÑ‚ÑÑ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ†Ğ¸Ñ„Ñ€Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ¾Ğ¹ Ğ² Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. Ğ¢Ñ€ĞµÑ‚Ğ¸Ğ¹ Ğ¸ Ñ‡ĞµÑ‚Ğ²Ñ‘Ñ€Ñ‚Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ¿Ñ‹ Ğ¿Ñ€ĞµĞ´ÑƒÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ Ğ²Ğ¼ĞµÑˆĞ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ¾ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ñ-Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹.'}, 'en': {'title': 'Transforming Digital Twins with AI: A Four-Stage Framework', 'desc': 'This paper discusses how artificial intelligence (AI) can enhance digital twins, which are digital replicas of physical systems. It introduces a four-stage framework that includes modeling, mirroring, intervention, and autonomous management to describe the integration of AI in digital twins. The framework emphasizes the use of physics-informed approaches and generative AI to improve the functionality and intelligence of these systems. Additionally, the paper explores challenges such as scalability and trustworthiness in various application domains like healthcare and smart cities.'}, 'zh': {'title': 'äººå·¥æ™ºèƒ½èµ‹èƒ½æ•°å­—åŒèƒèƒçš„å››é˜¶æ®µæ¡†æ¶', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªå››é˜¶æ®µæ¡†æ¶ï¼Œç”¨äºç³»ç»Ÿæ€§åœ°æè¿°äººå·¥æ™ºèƒ½åœ¨æ•°å­—åŒèƒèƒç”Ÿå‘½å‘¨æœŸä¸­çš„æ•´åˆã€‚è¿™ä¸ªæ¡†æ¶åŒ…æ‹¬å»ºæ¨¡ã€é•œåƒã€å¹²é¢„å’Œè‡ªä¸»ç®¡ç†å››ä¸ªé˜¶æ®µï¼Œåˆ©ç”¨ç‰©ç†çŸ¥è¯†é©±åŠ¨çš„æ–¹æ³•å’Œç”Ÿæˆå¼äººå·¥æ™ºèƒ½æŠ€æœ¯ã€‚æ•°å­—åŒèƒèƒä»è¢«åŠ¨çš„æ¨¡æ‹Ÿå·¥å…·å‘å±•ä¸ºæ™ºèƒ½å’Œè‡ªä¸»çš„å®ä½“ï¼Œèƒ½å¤Ÿå®æ—¶åŒæ­¥ç‰©ç†ç³»ç»Ÿå¹¶è¿›è¡Œé¢„æµ‹å’Œä¼˜åŒ–ã€‚é€šè¿‡è·¨é¢†åŸŸçš„å®¡æŸ¥ï¼Œæœ¬æ–‡è¿˜æ¢è®¨äº†å¯æ‰©å±•æ€§ã€å¯è§£é‡Šæ€§å’Œå¯ä¿¡æ€§ç­‰å…±åŒæŒ‘æˆ˜ï¼Œå¹¶ä¸ºè´Ÿè´£ä»»çš„äººå·¥æ™ºèƒ½é©±åŠ¨çš„æ•°å­—åŒèƒèƒç³»ç»Ÿæä¾›äº†æ–¹å‘ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.02989', 'title': 'Mechanistic Interpretability of Large-Scale Counting in LLMs through a System-2 Strategy', 'url': 'https://huggingface.co/papers/2601.02989', 'abstract': 'A test-time strategy inspired by System-2 cognitive processes decomposes large counting tasks into smaller sub-problems, enabling large language models to overcome architectural limitations and achieve high accuracy on complex counting tasks through mechanistic components like latent count computation, dedicated attention heads, and final aggregation stages.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs), despite strong performance on complex mathematical problems, exhibit systematic limitations in counting tasks. This issue arises from architectural limits of transformers, where counting is performed across layers, leading to degraded precision for larger counting problems due to depth constraints. To address this limitation, we propose a simple test-time strategy inspired by System-2 cognitive processes that decomposes large counting tasks into smaller, independent sub-problems that the model can reliably solve. We evaluate this approach using observational and causal mediation analyses to understand the underlying mechanism of this System-2-like strategy. Our mechanistic analysis identifies key components: latent counts are computed and stored in the final item representations of each part, transferred to intermediate steps via dedicated attention heads, and aggregated in the final stage to produce the total count. Experimental results demonstrate that this strategy enables LLMs to surpass architectural limitations and achieve high accuracy on large-scale counting tasks. This work provides mechanistic insight into System-2 counting in LLMs and presents a generalizable approach for improving and understanding their reasoning behavior.', 'score': 4, 'issue_id': 457, 'pub_date': '2026-01-06', 'pub_date_card': {'ru': '6 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 6', 'zh': '1æœˆ6æ—¥'}, 'hash': 'd3bdfab34076a3ff', 'authors': ['Hosein Hasani', 'Mohammadali Banayeeanzade', 'Ali Nafisi', 'Sadegh Mohammadian', 'Fatemeh Askari', 'Mobin Bagherian', 'Amirmohammad Izadi', 'Mahdieh Soleymani Baghshah'], 'affiliations': ['Sharif University of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2601.02989.jpg', 'data': {'categories': ['#reasoning', '#architecture', '#training', '#math', '#interpretability'], 'emoji': 'ğŸ§®', 'ru': {'title': 'ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾Ğ´ÑÑ‡Ñ‘Ñ‚Ğ°', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´ÑÑ‡Ñ‘Ñ‚Ğ° Ğ¸Ğ·-Ğ·Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ‚ĞµÑÑ‚Ğ¾Ğ²ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ, Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»Ñ‘Ğ½Ğ½ÑƒÑ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°Ğ¼Ğ¸ System-2, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¿Ğ¾Ğ´ÑÑ‡Ñ‘Ñ‚Ğ° Ğ½Ğ° Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. Ğ§ĞµÑ€ĞµĞ· Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ñ‹ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹: Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğµ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑÑ‡Ñ‘Ñ‚Ñ‡Ğ¸ĞºĞ¾Ğ², Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ğ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ attention heads Ğ¸ Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ°Ğ³Ñ€ĞµĞ³Ğ°Ñ†Ğ¸Ñ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµÑ‚ÑŒ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ´ÑÑ‡Ñ‘Ñ‚Ğ°.'}, 'en': {'title': 'Decomposing Counting Tasks for Enhanced Accuracy in LLMs', 'desc': 'This paper addresses the limitations of large language models (LLMs) in performing complex counting tasks due to the constraints of their transformer architecture. It introduces a test-time strategy inspired by System-2 cognitive processes, which breaks down large counting problems into smaller, manageable sub-problems. The approach utilizes mechanistic components such as latent count computation, dedicated attention heads, and aggregation stages to enhance accuracy. Experimental results show that this method allows LLMs to effectively overcome their architectural limitations and improve their counting performance.'}, 'zh': {'title': 'åˆ†è§£è®¡æ•°ä»»åŠ¡ï¼Œæå‡æ¨¡å‹å‡†ç¡®æ€§', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§å—ç³»ç»Ÿ-2è®¤çŸ¥è¿‡ç¨‹å¯å‘çš„æµ‹è¯•ç­–ç•¥ï¼Œæ—¨åœ¨å°†å¤§å‹è®¡æ•°ä»»åŠ¡åˆ†è§£ä¸ºæ›´å°çš„å­é—®é¢˜ï¼Œä»è€Œå¸®åŠ©å¤§å‹è¯­è¨€æ¨¡å‹å…‹æœæ¶æ„é™åˆ¶ã€‚é€šè¿‡è¿™ç§æ–¹æ³•ï¼Œæ¨¡å‹èƒ½å¤Ÿåœ¨å¤æ‚çš„è®¡æ•°ä»»åŠ¡ä¸­å®ç°æ›´é«˜çš„å‡†ç¡®æ€§ã€‚æˆ‘ä»¬é€šè¿‡è§‚å¯Ÿå’Œå› æœä¸­ä»‹åˆ†æè¯„ä¼°äº†è¿™ä¸€ç­–ç•¥çš„æœ‰æ•ˆæ€§ï¼Œå¹¶è¯†åˆ«å‡ºå…³é”®æœºåˆ¶ç»„ä»¶ï¼Œå¦‚æ½œåœ¨è®¡æ•°çš„è®¡ç®—ã€ä¸“ç”¨æ³¨æ„åŠ›å¤´çš„ä½¿ç”¨ä»¥åŠæœ€ç»ˆèšåˆé˜¶æ®µã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç­–ç•¥ä½¿å¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿè¶…è¶Šæ¶æ„é™åˆ¶ï¼ŒæˆåŠŸåº”å¯¹å¤§è§„æ¨¡è®¡æ•°ä»»åŠ¡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.03256', 'title': 'Muses: Designing, Composing, Generating Nonexistent Fantasy 3D Creatures without Training', 'url': 'https://huggingface.co/papers/2601.03256', 'abstract': "Muses enables feed-forward generation of 3D creatures by leveraging skeletal structures and graph-constrained reasoning for coherent design and assembly.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Muses, the first training-free method for fantastic 3D creature generation in a feed-forward paradigm. Previous methods, which rely on part-aware optimization, manual assembly, or 2D image generation, often produce unrealistic or incoherent 3D assets due to the challenges of intricate part-level manipulation and limited out-of-domain generation. In contrast, Muses leverages the 3D skeleton, a fundamental representation of biological forms, to explicitly and rationally compose diverse elements. This skeletal foundation formalizes 3D content creation as a structure-aware pipeline of design, composition, and generation. Muses begins by constructing a creatively composed 3D skeleton with coherent layout and scale through graph-constrained reasoning. This skeleton then guides a voxel-based assembly process within a structured latent space, integrating regions from different objects. Finally, image-guided appearance modeling under skeletal conditions is applied to generate a style-consistent and harmonious texture for the assembled shape. Extensive experiments establish Muses' state-of-the-art performance in terms of visual fidelity and alignment with textual descriptions, and potential on flexible 3D object editing. Project page: https://luhexiao.github.io/Muses.github.io/.", 'score': 3, 'issue_id': 456, 'pub_date': '2026-01-06', 'pub_date_card': {'ru': '6 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 6', 'zh': '1æœˆ6æ—¥'}, 'hash': '2ad714481787b5c2', 'authors': ['Hexiao Lu', 'Xiaokun Sun', 'Zeyu Cai', 'Hao Guo', 'Ying Tai', 'Jian Yang', 'Zhenyu Zhang'], 'affiliations': ['China Agricultural University', 'Nanjing University'], 'pdf_title_img': 'assets/pdf/title_img/2601.03256.jpg', 'data': {'categories': ['#graphs', '#reasoning', '#multimodal', '#3d'], 'emoji': 'ğŸ¦´', 'ru': {'title': 'Ğ¡ĞºĞµĞ»ĞµÑ‚ ĞºĞ°Ğº Ğ¼Ğ¾ÑÑ‚ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ 3D', 'desc': 'Muses â€” ÑÑ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D ÑÑƒÑ‰ĞµÑÑ‚Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ ÑĞºĞµĞ»ĞµÑ‚Ğ½Ñ‹Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ ĞºĞ°Ğº Ğ¾ÑĞ½Ğ¾Ğ²Ñƒ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ°. ĞĞ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ³Ñ€Ğ°Ñ„-Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞºĞµĞ»ĞµÑ‚Ğ° Ñ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½Ğ¾Ğ²ĞºĞ¾Ğ¹ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¾Ğ¼. Ğ—Ğ°Ñ‚ĞµĞ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ²Ğ¾ĞºÑĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ±Ğ¾Ñ€ĞºĞ¸ Ğ² ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ, Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑ Ñ‡Ğ°ÑÑ‚Ğ¸ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². ĞĞ°ĞºĞ¾Ğ½ĞµÑ†, Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ Ğ²Ğ¸Ğ´Ğ°, Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ°Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ ÑĞºĞµĞ»ĞµÑ‚Ğ½Ñ‹Ğ¼Ğ¸ ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑĞ¼Ğ¸, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€Ñƒ, ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ ÑÑ‚Ğ¸Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ĞµĞ´Ğ¸Ğ½Ñ‹Ğ¹ Ğ¸ Ğ³Ğ°Ñ€Ğ¼Ğ¾Ğ½Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ 3D Ğ°ĞºÑ‚Ğ¸Ğ².'}, 'en': {'title': 'Muses: Crafting Coherent 3D Creatures with Skeletal Structures', 'desc': 'Muses is a novel method for generating 3D creatures without the need for prior training. It uses a skeletal structure to guide the design and assembly of 3D models, ensuring that the generated creatures are coherent and visually appealing. By employing graph-constrained reasoning, Muses effectively organizes different parts of the creature, allowing for a structured approach to 3D content creation. The final output is enhanced with image-guided appearance modeling, resulting in textures that are consistent with the overall design.'}, 'zh': {'title': 'Musesï¼šæ— è®­ç»ƒçš„3Dç”Ÿç‰©ä½“ç”Ÿæˆæ–°æ–¹æ³•', 'desc': 'Musesæ˜¯ä¸€ç§æ–°é¢–çš„3Dç”Ÿç‰©ä½“ç”Ÿæˆæ–¹æ³•ï¼Œé‡‡ç”¨éª¨éª¼ç»“æ„å’Œå›¾çº¦æŸæ¨ç†æ¥å®ç°è¿è´¯çš„è®¾è®¡ä¸ç»„è£…ã€‚ä¸ä»¥å¾€ä¾èµ–äºéƒ¨ä»¶ä¼˜åŒ–æˆ–æ‰‹åŠ¨ç»„è£…çš„æ–¹æ³•ä¸åŒï¼ŒMusesèƒ½å¤Ÿåœ¨ä¸éœ€è¦è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œç›´æ¥ç”Ÿæˆé«˜è´¨é‡çš„3Dç”Ÿç‰©ä½“ã€‚è¯¥æ–¹æ³•é€šè¿‡æ„å»ºå…·æœ‰ä¸€è‡´å¸ƒå±€å’Œæ¯”ä¾‹çš„3Déª¨éª¼ï¼Œå½¢æˆä¸€ä¸ªç»“æ„æ„ŸçŸ¥çš„è®¾è®¡å’Œç”Ÿæˆæµç¨‹ã€‚æœ€ç»ˆï¼ŒMusesåœ¨è§†è§‰çœŸå®æ„Ÿå’Œä¸æ–‡æœ¬æè¿°çš„ä¸€è‡´æ€§æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå±•ç°äº†çµæ´»çš„3Då¯¹è±¡ç¼–è¾‘æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.01592', 'title': 'OpenRT: An Open-Source Red Teaming Framework for Multimodal LLMs', 'url': 'https://huggingface.co/papers/2601.01592', 'abstract': 'OpenRT is a unified red-teaming framework for evaluating multimodal large language model safety through modular adversarial testing across multiple attack dimensions and models.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapid integration of Multimodal Large Language Models (MLLMs) into critical applications is increasingly hindered by persistent safety vulnerabilities. However, existing red-teaming benchmarks are often fragmented, limited to single-turn text interactions, and lack the scalability required for systematic evaluation. To address this, we introduce OpenRT, a unified, modular, and high-throughput red-teaming framework designed for comprehensive MLLM safety evaluation. At its core, OpenRT architects a paradigm shift in automated red-teaming by introducing an adversarial kernel that enables modular separation across five critical dimensions: model integration, dataset management, attack strategies, judging methods, and evaluation metrics. By standardizing attack interfaces, it decouples adversarial logic from a high-throughput asynchronous runtime, enabling systematic scaling across diverse models. Our framework integrates 37 diverse attack methodologies, spanning white-box gradients, multi-modal perturbations, and sophisticated multi-agent evolutionary strategies. Through an extensive empirical study on 20 advanced models (including GPT-5.2, Claude 4.5, and Gemini 3 Pro), we expose critical safety gaps: even frontier models fail to generalize across attack paradigms, with leading models exhibiting average Attack Success Rates as high as 49.14%. Notably, our findings reveal that reasoning models do not inherently possess superior robustness against complex, multi-turn jailbreaks. By open-sourcing OpenRT, we provide a sustainable, extensible, and continuously maintained infrastructure that accelerates the development and standardization of AI safety.', 'score': 3, 'issue_id': 453, 'pub_date': '2026-01-04', 'pub_date_card': {'ru': '4 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 4', 'zh': '1æœˆ4æ—¥'}, 'hash': '9db3fbc6579e6a18', 'authors': ['Xin Wang', 'Yunhao Chen', 'Juncheng Li', 'Yixu Wang', 'Yang Yao', 'Tianle Gu', 'Jie Li', 'Yan Teng', 'Xingjun Ma', 'Yingchun Wang', 'Xia Hu'], 'affiliations': ['Shanghai Artificial Intelligence Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2601.01592.jpg', 'data': {'categories': ['#benchmark', '#multimodal', '#security', '#open_source', '#agents'], 'emoji': 'ğŸ›¡ï¸', 'ru': {'title': 'ĞœĞ¾Ğ´ÑƒĞ»ÑŒĞ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'OpenRT â€” ÑÑ‚Ğ¾ ĞµĞ´Ğ¸Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ğ°Ñ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° Ğ´Ğ»Ñ ĞºÑ€Ğ°ÑĞ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ñ… Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ.æ¡†æ¶Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ 37 Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¸Ğº Ğ°Ñ‚Ğ°Ğº, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ±ĞµĞ»Ğ¾Ğ³Ğ¾ ÑÑ‰Ğ¸ĞºĞ°, Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼ÑƒÑ‰ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° 20 Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸: Ğ´Ğ°Ğ¶Ğµ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ ÑƒÑĞ¿ĞµÑˆĞ½Ñ‹Ñ… Ğ°Ñ‚Ğ°Ğº Ğ´Ğ¾ 49.14% Ğ¸ Ğ½Ğµ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‚ÑÑ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚Ğ¸Ğ¿Ğ°Ğ¼Ğ¸ Ğ°Ñ‚Ğ°Ğº. ĞÑ‚ĞºÑ€Ñ‹Ğ² Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ´, Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼ÑƒÑ Ğ¸Ğ½Ñ„Ñ€Ğ°ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ¸ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°.'}, 'en': {'title': 'OpenRT: Revolutionizing Safety Evaluation for Multimodal AI Models', 'desc': 'OpenRT is a comprehensive framework designed to evaluate the safety of multimodal large language models (MLLMs) through modular adversarial testing. It addresses the limitations of existing red-teaming benchmarks by allowing for systematic evaluation across multiple attack dimensions and models. The framework introduces an adversarial kernel that separates various components, enabling efficient scaling and integration of diverse attack strategies. Through extensive testing on advanced models, OpenRT reveals significant safety vulnerabilities, highlighting the need for improved robustness in AI systems.'}, 'zh': {'title': 'OpenRTï¼šå¤šæ¨¡æ€è¯­è¨€æ¨¡å‹å®‰å…¨è¯„ä¼°çš„æ–°æ ‡å‡†', 'desc': 'OpenRTæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„çº¢é˜Ÿæ¡†æ¶ï¼Œç”¨äºé€šè¿‡æ¨¡å—åŒ–å¯¹æŠ—æµ‹è¯•è¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„å®‰å…¨æ€§ã€‚è¯¥æ¡†æ¶è§£å†³äº†ç°æœ‰çº¢é˜ŸåŸºå‡†åˆ†æ•£ã€ä»…é™äºå•è½®æ–‡æœ¬äº¤äº’å’Œç¼ºä¹ç³»ç»Ÿè¯„ä¼°å¯æ‰©å±•æ€§çš„é—®é¢˜ã€‚OpenRTå¼•å…¥äº†å¯¹æŠ—å†…æ ¸ï¼Œæ”¯æŒåœ¨æ¨¡å‹é›†æˆã€æ•°æ®é›†ç®¡ç†ã€æ”»å‡»ç­–ç•¥ã€åˆ¤æ–­æ–¹æ³•å’Œè¯„ä¼°æŒ‡æ ‡ç­‰äº”ä¸ªå…³é”®ç»´åº¦çš„æ¨¡å—åŒ–åˆ†ç¦»ã€‚é€šè¿‡æ•´åˆ37ç§ä¸åŒçš„æ”»å‡»æ–¹æ³•ï¼ŒOpenRTèƒ½å¤Ÿç³»ç»Ÿæ€§åœ°è¯„ä¼°å¤šç§æ¨¡å‹çš„å®‰å…¨æ€§ï¼Œæ­ç¤ºäº†å³ä½¿æ˜¯æœ€å…ˆè¿›çš„æ¨¡å‹åœ¨é¢å¯¹å¤æ‚æ”»å‡»æ—¶ä¹Ÿå­˜åœ¨æ˜¾è‘—çš„å®‰å…¨æ¼æ´ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.02439', 'title': 'WebGym: Scaling Training Environments for Visual Web Agents with Realistic Tasks', 'url': 'https://huggingface.co/papers/2601.02439', 'abstract': "WebGym presents a large-scale open-source environment for training visual web agents using reinforcement learning with high-throughput asynchronous sampling, achieving superior performance on unseen websites compared to proprietary models.  \t\t\t\t\tAI-generated summary \t\t\t\t We present WebGym, the largest-to-date open-source environment for training realistic visual web agents. Real websites are non-stationary and diverse, making artificial or small-scale task sets insufficient for robust policy learning. WebGym contains nearly 300,000 tasks with rubric-based evaluations across diverse, real-world websites and difficulty levels. We train agents with a simple reinforcement learning (RL) recipe, which trains on the agent's own interaction traces (rollouts), using task rewards as feedback to guide learning. To enable scaling RL, we speed up sampling of trajectories in WebGym by developing a high-throughput asynchronous rollout system, designed specifically for web agents. Our system achieves a 4-5x rollout speedup compared to naive implementations. Second, we scale the task set breadth, depth, and size, which results in continued performance improvement. Fine-tuning a strong base vision-language model, Qwen-3-VL-8B-Instruct, on WebGym results in an improvement in success rate on an out-of-distribution test set from 26.2% to 42.9%, significantly outperforming agents based on proprietary models such as GPT-4o and GPT-5-Thinking that achieve 27.1% and 29.8%, respectively. This improvement is substantial because our test set consists only of tasks on websites never seen during training, unlike many other prior works on training visual web agents.", 'score': 2, 'issue_id': 448, 'pub_date': '2026-01-05', 'pub_date_card': {'ru': '5 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 5', 'zh': '1æœˆ5æ—¥'}, 'hash': 'f7bca4e4785414da', 'authors': ['Hao Bai', 'Alexey Taymanov', 'Tong Zhang', 'Aviral Kumar', 'Spencer Whitehead'], 'affiliations': ['CMU', 'Microsoft', 'UIUC'], 'pdf_title_img': 'assets/pdf/title_img/2601.02439.jpg', 'data': {'categories': ['#dataset', '#open_source', '#optimization', '#benchmark', '#rl', '#agents'], 'emoji': 'ğŸ•·ï¸', 'ru': {'title': 'ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ²ĞµĞ±-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±Ğ¾Ğ³Ğ°Ñ‰Ñ‘Ğ½Ğ½ÑƒÑ ÑÑ€ĞµĞ´Ñƒ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ RL', 'desc': 'WebGym Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½ÑƒÑ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚ÑƒÑ ÑÑ€ĞµĞ´Ñƒ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²ĞµĞ±-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ¾ĞºĞ¾Ğ»Ğ¾ 300 Ñ‚Ñ‹ÑÑÑ‡ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¾Ğ¹ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²ĞµĞ±-ÑĞ°Ğ¹Ñ‚Ğ°Ñ… Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğ°ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ ÑĞ±Ğ¾Ñ€Ğ° Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ² 4-5 Ñ€Ğ°Ğ·. ĞŸÑƒÑ‚Ñ‘Ğ¼ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Qwen-3-VL-8B-Instruct Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ½ÑƒÑ‚Ğ° Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ 42.9% ÑƒÑĞ¿ĞµÑ…Ğ° Ğ½Ğ° Ğ½ĞµĞ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ°Ğ¹Ñ‚Ğ°Ñ…, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‰Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‚Ğ¸Ğ¿Ğ° GPT-4o.'}, 'en': {'title': 'WebGym: Revolutionizing Visual Web Agent Training with Scalable RL', 'desc': 'WebGym is a large-scale open-source platform designed for training visual web agents using reinforcement learning (RL). It features nearly 300,000 diverse tasks that simulate real-world websites, allowing agents to learn from their own interactions and receive feedback through task rewards. The platform employs a high-throughput asynchronous rollout system to significantly speed up the sampling of trajectories, achieving a 4-5x increase in efficiency. By fine-tuning a vision-language model on this extensive dataset, agents show a marked improvement in performance on unseen tasks, surpassing proprietary models in success rates.'}, 'zh': {'title': 'WebGymï¼šæå‡è§†è§‰ç½‘ç»œä»£ç†è®­ç»ƒçš„æ–°å¹³å°', 'desc': 'WebGymæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„å¼€æºç¯å¢ƒï¼Œç”¨äºè®­ç»ƒè§†è§‰ç½‘ç»œä»£ç†ï¼Œé‡‡ç”¨å¼ºåŒ–å­¦ä¹ æ–¹æ³•ã€‚è¯¥ç¯å¢ƒåŒ…å«è¿‘30ä¸‡ä¸ªä»»åŠ¡ï¼Œæ¶µç›–å¤šæ ·åŒ–çš„çœŸå®ç½‘ç«™å’Œä¸åŒéš¾åº¦çº§åˆ«ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ”¯æŒç­–ç•¥å­¦ä¹ ã€‚é€šè¿‡é«˜ååé‡çš„å¼‚æ­¥é‡‡æ ·ç³»ç»Ÿï¼ŒWebGymæ˜¾è‘—æé«˜äº†è®­ç»ƒé€Ÿåº¦ï¼Œè¾¾åˆ°4-5å€çš„åŠ é€Ÿæ•ˆæœã€‚ç»è¿‡åœ¨WebGymä¸Šçš„å¾®è°ƒï¼Œè§†è§‰è¯­è¨€æ¨¡å‹Qwen-3-VL-8B-Instructåœ¨æœªè§è¿‡çš„ç½‘ç«™æµ‹è¯•é›†ä¸Šçš„æˆåŠŸç‡ä»26.2%æå‡è‡³42.9%ï¼Œè¶…è¶Šäº†åŸºäºä¸“æœ‰æ¨¡å‹çš„ä»£ç†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.01720', 'title': 'FFP-300K: Scaling First-Frame Propagation for Generalizable Video Editing', 'url': 'https://huggingface.co/papers/2601.01720', 'abstract': 'A new large-scale video dataset and framework are presented that enable effective first-frame propagation without runtime guidance through adaptive spatio-temporal positional encoding and self-distillation techniques.  \t\t\t\t\tAI-generated summary \t\t\t\t First-Frame Propagation (FFP) offers a promising paradigm for controllable video editing, but existing methods are hampered by a reliance on cumbersome run-time guidance. We identify the root cause of this limitation as the inadequacy of current training datasets, which are often too short, low-resolution, and lack the task diversity required to teach robust temporal priors. To address this foundational data gap, we first introduce FFP-300K, a new large-scale dataset comprising 300K high-fidelity video pairs at 720p resolution and 81 frames in length, constructed via a principled two-track pipeline for diverse local and global edits. Building on this dataset, we propose a novel framework designed for true guidance-free FFP that resolves the critical tension between maintaining first-frame appearance and preserving source video motion. Architecturally, we introduce Adaptive Spatio-Temporal RoPE (AST-RoPE), which dynamically remaps positional encodings to disentangle appearance and motion references. At the objective level, we employ a self-distillation strategy where an identity propagation task acts as a powerful regularizer, ensuring long-term temporal stability and preventing semantic drift. Comprehensive experiments on the EditVerseBench benchmark demonstrate that our method significantly outperforming existing academic and commercial models by receiving about 0.2 PickScore and 0.3 VLM score improvement against these competitors.', 'score': 2, 'issue_id': 448, 'pub_date': '2026-01-05', 'pub_date_card': {'ru': '5 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 5', 'zh': '1æœˆ5æ—¥'}, 'hash': '7afdd909674fcc28', 'authors': ['Xijie Huang', 'Chengming Xu', 'Donghao Luo', 'Xiaobin Hu', 'Peng Tang', 'Xu Peng', 'Jiangning Zhang', 'Chengjie Wang', 'Yanwei Fu'], 'affiliations': ['FDU', 'Tencent YouTu Lab'], 'pdf_title_img': 'assets/pdf/title_img/2601.01720.jpg', 'data': {'categories': ['#dataset', '#architecture', '#video', '#benchmark'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ¡Ğ²Ğ¾Ğ±Ğ¾Ğ´Ğ½Ğ¾Ğµ Ğ¾Ñ‚ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ° Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾ ĞºĞ°Ğ´Ñ€Ğ° Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… FFP-300K, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 300,000 Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ°Ñ€ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°, Ğ¸ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ¿Ğ°Ğ³Ğ°Ñ†Ğ¸Ğ¸ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾ ĞºĞ°Ğ´Ñ€Ğ° Ğ±ĞµĞ· Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ° Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ ÑĞ¿Ğ°Ñ‚Ğ¸Ğ¾Ñ‚ĞµĞ¼Ğ¿Ğ¾Ñ€Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ (AST-RoPE), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ Ğ²Ğ¸Ğ´Ğ° Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ’ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ ÑĞ°Ğ¼Ğ¾Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ, Ğ³Ğ´Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ° Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¿Ğ°Ğ³Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½ÑƒÑ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ´Ñ€ĞµĞ¹Ñ„. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ EditVerseBench.'}, 'en': {'title': 'Revolutionizing Video Editing with Guidance-Free First-Frame Propagation', 'desc': 'This paper introduces a new large-scale video dataset called FFP-300K, which contains 300,000 high-quality video pairs designed to improve first-frame propagation (FFP) in video editing. The authors address the limitations of existing methods that rely on real-time guidance by proposing a framework that uses adaptive spatio-temporal positional encoding and self-distillation techniques. The framework allows for effective video editing without the need for runtime guidance, maintaining the appearance of the first frame while preserving the motion of the source video. Experimental results show that this approach significantly outperforms current models, demonstrating its effectiveness in controllable video editing tasks.'}, 'zh': {'title': 'æ— æŒ‡å¯¼çš„é¦–å¸§ä¼ æ’­æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¤§è§„æ¨¡è§†é¢‘æ•°æ®é›†å’Œæ¡†æ¶ï¼Œæ—¨åœ¨å®ç°æœ‰æ•ˆçš„é¦–å¸§ä¼ æ’­ï¼Œè€Œæ— éœ€è¿è¡Œæ—¶æŒ‡å¯¼ã€‚æˆ‘ä»¬å¼•å…¥äº†FFP-300Kæ•°æ®é›†ï¼ŒåŒ…å«30ä¸‡ä¸ª720påˆ†è¾¨ç‡çš„é«˜ä¿çœŸè§†é¢‘å¯¹ï¼Œè§£å†³äº†ç°æœ‰æ•°æ®é›†çŸ­å°ã€ä½åˆ†è¾¨ç‡å’Œä»»åŠ¡å¤šæ ·æ€§ä¸è¶³çš„é—®é¢˜ã€‚é€šè¿‡è‡ªé€‚åº”æ—¶ç©ºä½ç½®ç¼–ç å’Œè‡ªè’¸é¦æŠ€æœ¯ï¼Œæˆ‘ä»¬çš„æ¡†æ¶èƒ½å¤Ÿåœ¨ä¿æŒé¦–å¸§å¤–è§‚çš„åŒæ—¶ï¼Œä¿ç•™æºè§†é¢‘çš„è¿åŠ¨ä¿¡æ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨EditVerseBenchåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰çš„å­¦æœ¯å’Œå•†ä¸šæ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.03227', 'title': 'The Sonar Moment: Benchmarking Audio-Language Models in Audio Geo-Localization', 'url': 'https://huggingface.co/papers/2601.03227', 'abstract': "Audio geo-localization benchmark AGL1K is introduced to advance audio language models' geospatial reasoning capabilities through curated audio clips and evaluation across multiple models.  \t\t\t\t\tAI-generated summary \t\t\t\t Geo-localization aims to infer the geographic origin of a given signal. In computer vision, geo-localization has served as a demanding benchmark for compositional reasoning and is relevant to public safety. In contrast, progress on audio geo-localization has been constrained by the lack of high-quality audio-location pairs. To address this gap, we introduce AGL1K, the first audio geo-localization benchmark for audio language models (ALMs), spanning 72 countries and territories. To extract reliably localizable samples from a crowd-sourced platform, we propose the Audio Localizability metric that quantifies the informativeness of each recording, yielding 1,444 curated audio clips. Evaluations on 16 ALMs show that ALMs have emerged with audio geo-localization capability. We find that closed-source models substantially outperform open-source models, and that linguistic clues often dominate as a scaffold for prediction. We further analyze ALMs' reasoning traces, regional bias, error causes, and the interpretability of the localizability metric. Overall, AGL1K establishes a benchmark for audio geo-localization and may advance ALMs with better geospatial reasoning capability.", 'score': 1, 'issue_id': 451, 'pub_date': '2026-01-06', 'pub_date_card': {'ru': '6 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 6', 'zh': '1æœˆ6æ—¥'}, 'hash': '0fccc0342ae86d5a', 'authors': ['Ruixing Zhang', 'Zihan Liu', 'Leilei Sun', 'Tongyu Zhu', 'Weifeng Lv'], 'affiliations': ['Shanghai AI Laboratory', 'The Key Laboratory of Data Science and Intelligent Computing, International Innovation Institute, Beihang University', 'The State Key Laboratory of Complex and Critical Software Environment, Beihang University'], 'pdf_title_img': 'assets/pdf/title_img/2601.03227.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#dataset', '#open_source', '#interpretability', '#audio', '#reasoning'], 'emoji': 'ğŸŒ', 'ru': {'title': 'ĞŸĞµÑ€Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ³ĞµĞ¾Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº AGL1K Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑ‚ÑŒ Ğ³ĞµĞ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ñ€Ğ¾Ğ¸ÑÑ…Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ·Ğ²ÑƒĞºĞ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ±Ñ€Ğ°Ğ»Ğ¸ 1444 Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ°ÑƒĞ´Ğ¸Ğ¾ĞºĞ»Ğ¸Ğ¿Ğ¾Ğ² Ğ¸Ğ· 72 ÑÑ‚Ñ€Ğ°Ğ½, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ² Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ Audio Localizability Ğ´Ğ»Ñ Ğ²Ñ‹Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ². ĞÑ†ĞµĞ½ĞºĞ° 16 Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ALM Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ°, Ñ‡Ñ‚Ğ¾ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ¸ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ¸Ğ³Ñ€Ğ°ÑÑ‚ Ğ´Ğ¾Ğ¼Ğ¸Ğ½Ğ¸Ñ€ÑƒÑÑ‰ÑƒÑ Ñ€Ğ¾Ğ»ÑŒ Ğ² Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸ÑÑ…. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ»Ğ¾Ğ³Ğ¸ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸.'}, 'en': {'title': 'AGL1K: Advancing Audio Geo-Localization for Better Spatial Reasoning', 'desc': 'The paper introduces AGL1K, a new benchmark designed to enhance the geospatial reasoning abilities of audio language models (ALMs) using curated audio clips. It addresses the challenge of audio geo-localization, which involves determining the geographic origin of audio signals, a task previously limited by the lack of quality audio-location pairs. AGL1K includes 1,444 carefully selected audio clips from 72 countries, utilizing a novel Audio Localizability metric to ensure the recordings are informative. The study evaluates 16 ALMs, revealing that closed-source models outperform open-source ones and highlighting the importance of linguistic cues in making predictions.'}, 'zh': {'title': 'éŸ³é¢‘åœ°ç†å®šä½æ–°åŸºå‡†AGL1K', 'desc': 'æœ¬æ–‡ä»‹ç»äº†éŸ³é¢‘åœ°ç†å®šä½åŸºå‡†AGL1Kï¼Œæ—¨åœ¨é€šè¿‡ç²¾å¿ƒæŒ‘é€‰çš„éŸ³é¢‘ç‰‡æ®µæå‡éŸ³é¢‘è¯­è¨€æ¨¡å‹çš„åœ°ç†æ¨ç†èƒ½åŠ›ã€‚åœ°ç†å®šä½çš„ç›®æ ‡æ˜¯æ¨æ–­ç»™å®šä¿¡å·çš„åœ°ç†æ¥æºï¼Œè€ŒéŸ³é¢‘åœ°ç†å®šä½çš„è¿›å±•å—åˆ°é«˜è´¨é‡éŸ³é¢‘-ä½ç½®å¯¹ç¼ºä¹çš„é™åˆ¶ã€‚AGL1Kæ˜¯é¦–ä¸ªé’ˆå¯¹éŸ³é¢‘è¯­è¨€æ¨¡å‹çš„éŸ³é¢‘åœ°ç†å®šä½åŸºå‡†ï¼Œæ¶µç›–72ä¸ªå›½å®¶å’Œåœ°åŒºï¼Œå¹¶æå‡ºäº†éŸ³é¢‘å¯å®šä½æ€§æŒ‡æ ‡æ¥é‡åŒ–æ¯ä¸ªå½•éŸ³çš„ä¿¡æ¯é‡ã€‚é€šè¿‡å¯¹16ä¸ªéŸ³é¢‘è¯­è¨€æ¨¡å‹çš„è¯„ä¼°ï¼Œå‘ç°é—­æºæ¨¡å‹åœ¨éŸ³é¢‘åœ°ç†å®šä½èƒ½åŠ›ä¸Šæ˜¾è‘—ä¼˜äºå¼€æºæ¨¡å‹ï¼Œè¯­è¨€çº¿ç´¢åœ¨é¢„æµ‹ä¸­èµ·åˆ°äº†ä¸»å¯¼ä½œç”¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.03194', 'title': 'X-MuTeST: A Multilingual Benchmark for Explainable Hate Speech Detection and A Novel LLM-consulted Explanation Framework', 'url': 'https://huggingface.co/papers/2601.03194', 'abstract': 'A novel explainability-guided training framework for hate speech detection in Indic languages that combines large language models with attention-enhancing techniques and provides human-annotated rationales for improved performance and interpretability.  \t\t\t\t\tAI-generated summary \t\t\t\t Hate speech detection on social media faces challenges in both accuracy and explainability, especially for underexplored Indic languages. We propose a novel explainability-guided training framework, X-MuTeST (eXplainable Multilingual haTe Speech deTection), for hate speech detection that combines high-level semantic reasoning from large language models (LLMs) with traditional attention-enhancing techniques. We extend this research to Hindi and Telugu alongside English by providing benchmark human-annotated rationales for each word to justify the assigned class label. The X-MuTeST explainability method computes the difference between the prediction probabilities of the original text and those of unigrams, bigrams, and trigrams. Final explanations are computed as the union between LLM explanations and X-MuTeST explanations. We show that leveraging human rationales during training enhances both classification performance and explainability. Moreover, combining human rationales with our explainability method to refine the model attention yields further improvements. We evaluate explainability using Plausibility metrics such as Token-F1 and IOU-F1 and Faithfulness metrics such as Comprehensiveness and Sufficiency. By focusing on under-resourced languages, our work advances hate speech detection across diverse linguistic contexts. Our dataset includes token-level rationale annotations for 6,004 Hindi, 4,492 Telugu, and 6,334 English samples. Data and code are available on https://github.com/ziarehman30/X-MuTeST', 'score': 1, 'issue_id': 448, 'pub_date': '2026-01-06', 'pub_date_card': {'ru': '6 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 6', 'zh': '1æœˆ6æ—¥'}, 'hash': 'ab654e9d20b8b95f', 'authors': ['Mohammad Zia Ur Rehman', 'Sai Kartheek Reddy Kasu', 'Shashivardhan Reddy Koppula', 'Sai Rithwik Reddy Chirra', 'Shwetank Shekhar Singh', 'Nagendra Kumar'], 'affiliations': ['Arizona State University', 'Indian Institute of Information Technology Dharwad', 'Indian Institute of Technology Indore', 'Indian Institute of Technology Mandi'], 'pdf_title_img': 'assets/pdf/title_img/2601.03194.jpg', 'data': {'categories': ['#interpretability', '#low_resource', '#dataset', '#open_source', '#multilingual', '#benchmark', '#training'], 'emoji': 'ğŸš¨', 'ru': {'title': 'ĞĞ±ÑŠÑÑĞ½Ğ¸Ğ¼Ğ¾Ğµ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ñ€ĞµÑ‡Ğ¸ Ğ½ĞµĞ½Ğ°Ğ²Ğ¸ÑÑ‚Ğ¸ Ğ² Ğ¸Ğ½Ğ´Ğ¸Ğ¹ÑĞºĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ°Ñ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° X-MuTeST Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸ Ğ½ĞµĞ½Ğ°Ğ²Ğ¸ÑÑ‚Ğ¸ Ğ² Ğ¸Ğ½Ğ´Ğ¸Ğ¹ÑĞºĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ°Ñ… (Ñ…Ğ¸Ğ½Ğ´Ğ¸ Ğ¸ Ñ‚ĞµĞ»ÑƒĞ³Ñƒ) Ğ¸ Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ°Ğ¼Ğ¸ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğµ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑĞ»Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾Ğ±ÑŠÑÑĞ½ÑÑÑ‚, Ğ¿Ğ¾Ñ‡ĞµĞ¼Ñƒ Ñ‚ĞµĞºÑÑ‚ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ ĞºĞ°Ğº Ñ€ĞµÑ‡ÑŒ Ğ½ĞµĞ½Ğ°Ğ²Ğ¸ÑÑ‚Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ÑĞµÑ‚ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ñ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ ĞµĞ³Ğ¾ n-Ğ³Ñ€Ğ°Ğ¼Ğ¼, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚ LLM Ñ ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ğº ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸, Ñ‚Ğ°Ğº Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Enhancing Hate Speech Detection with Explainability in Indic Languages', 'desc': "This paper introduces X-MuTeST, a new framework for detecting hate speech in Indic languages like Hindi and Telugu, as well as English. It combines large language models with attention-enhancing techniques to improve both the accuracy and interpretability of the detection process. The framework uses human-annotated rationales to provide explanations for the model's predictions, which helps users understand why certain texts are classified as hate speech. By evaluating the model's explainability through various metrics, the study demonstrates that incorporating human rationales significantly boosts performance and clarity in hate speech detection."}, 'zh': {'title': 'å¯è§£é‡Šæ€§å¼•å¯¼çš„ä»‡æ¨è¨€è®ºæ£€æµ‹æ–°æ¡†æ¶', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¯è§£é‡Šæ€§å¼•å¯¼è®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å°åº¦è¯­è¨€ä¸­çš„ä»‡æ¨è¨€è®ºæ£€æµ‹ã€‚è¯¥æ¡†æ¶ç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹å’Œæ³¨æ„åŠ›å¢å¼ºæŠ€æœ¯ï¼Œå¹¶æä¾›äººå·¥æ ‡æ³¨çš„ç†ç”±ï¼Œä»¥æå‡æ¨¡å‹çš„æ€§èƒ½å’Œå¯è§£é‡Šæ€§ã€‚æˆ‘ä»¬åœ¨å°åœ°è¯­ã€æ³°å¢å›ºè¯­å’Œè‹±è¯­ä¸­æ‰©å±•äº†è¿™ä¸€ç ”ç©¶ï¼Œæä¾›äº†æ¯ä¸ªå•è¯çš„åŸºå‡†äººç±»æ ‡æ³¨ç†ç”±ã€‚é€šè¿‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åˆ©ç”¨äººç±»ç†ç”±ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨åˆ†ç±»æ€§èƒ½å’Œå¯è§£é‡Šæ€§ä¸Šå‡å¾—åˆ°äº†æ˜¾è‘—æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.03153', 'title': 'Parallel Latent Reasoning for Sequential Recommendation', 'url': 'https://huggingface.co/papers/2601.03153', 'abstract': 'Parallel Latent Reasoning framework improves sequential recommendation by exploring multiple diverse reasoning trajectories simultaneously through learnable trigger tokens and adaptive aggregation.  \t\t\t\t\tAI-generated summary \t\t\t\t Capturing complex user preferences from sparse behavioral sequences remains a fundamental challenge in sequential recommendation. Recent latent reasoning methods have shown promise by extending test-time computation through multi-step reasoning, yet they exclusively rely on depth-level scaling along a single trajectory, suffering from diminishing returns as reasoning depth increases. To address this limitation, we propose Parallel Latent Reasoning (PLR), a novel framework that pioneers width-level computational scaling by exploring multiple diverse reasoning trajectories simultaneously. PLR constructs parallel reasoning streams through learnable trigger tokens in continuous latent space, preserves diversity across streams via global reasoning regularization, and adaptively synthesizes multi-stream outputs through mixture-of-reasoning-streams aggregation. Extensive experiments on three real-world datasets demonstrate that PLR substantially outperforms state-of-the-art baselines while maintaining real-time inference efficiency. Theoretical analysis further validates the effectiveness of parallel reasoning in improving generalization capability. Our work opens new avenues for enhancing reasoning capacity in sequential recommendation beyond existing depth scaling.', 'score': 1, 'issue_id': 448, 'pub_date': '2026-01-06', 'pub_date_card': {'ru': '6 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 6', 'zh': '1æœˆ6æ—¥'}, 'hash': '746852f405d7e377', 'authors': ['Jiakai Tang', 'Xu Chen', 'Wen Chen', 'Jian Wu', 'Yuning Jiang', 'Bo Zheng'], 'affiliations': ['Alibaba Group', 'GSAI, Renmin University of China'], 'pdf_title_img': 'assets/pdf/title_img/2601.03153.jpg', 'data': {'categories': [], 'emoji': 'ğŸŒ³', 'ru': {'title': 'ĞŸĞ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ± Ğ´Ğ»Ñ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¹ â€” Parallel Latent Reasoning (PLR), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ÑƒĞ³Ğ»ÑƒĞ±Ğ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ğµ Ñ‚Ñ€Ğ¸Ğ³Ğ³ĞµÑ€-Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ² Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¸Ñ… Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ°Ğ³Ñ€ĞµĞ³Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ¾Ğ¼ Ñ‡ĞµÑ€ĞµĞ· Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ mixture-of-reasoning-streams, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ĞµĞµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ¸Ğ· Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸.'}, 'en': {'title': 'Unlocking Diverse Reasoning for Better Recommendations', 'desc': 'The Parallel Latent Reasoning (PLR) framework enhances sequential recommendation systems by utilizing multiple reasoning paths at once, rather than just deepening a single path. It introduces learnable trigger tokens to create diverse reasoning streams in a continuous latent space, which helps capture complex user preferences from limited data. By applying global reasoning regularization, PLR maintains diversity among these streams and combines their outputs effectively through adaptive aggregation. This approach not only improves performance on real-world datasets but also ensures efficient real-time inference, marking a significant advancement in the field of sequential recommendation.'}, 'zh': {'title': 'å¹¶è¡Œæ½œåœ¨æ¨ç†ï¼šæå‡åºåˆ—æ¨èçš„å¤šæ ·æ€§ä¸æ•ˆç‡', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºå¹¶è¡Œæ½œåœ¨æ¨ç†ï¼ˆPLRï¼‰çš„æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡åŒæ—¶æ¢ç´¢å¤šæ¡å¤šæ ·åŒ–çš„æ¨ç†è½¨è¿¹æ¥æ”¹å–„åºåˆ—æ¨èã€‚PLRåˆ©ç”¨å¯å­¦ä¹ çš„è§¦å‘ä»¤ç‰Œåœ¨è¿ç»­æ½œåœ¨ç©ºé—´ä¸­æ„å»ºå¹¶è¡Œæ¨ç†æµï¼Œå¹¶é€šè¿‡å…¨å±€æ¨ç†æ­£åˆ™åŒ–ä¿æŒæµä¹‹é—´çš„å¤šæ ·æ€§ã€‚è¯¥æ¡†æ¶é€šè¿‡æ··åˆæ¨ç†æµçš„èšåˆè‡ªé€‚åº”åœ°åˆæˆå¤šæµè¾“å‡ºï¼Œä»è€Œæé«˜æ¨èçš„å‡†ç¡®æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPLRåœ¨ä¸‰ä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›åŸºçº¿ï¼ŒåŒæ—¶ä¿æŒå®æ—¶æ¨ç†æ•ˆç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.03127', 'title': 'Unified Thinker: A General Reasoning Modular Core for Image Generation', 'url': 'https://huggingface.co/papers/2601.03127', 'abstract': 'Unified Thinker addresses the reasoning-execution gap in image generation by decoupling a reasoning module from image generators and using reinforcement learning to optimize visual correctness.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite impressive progress in high-fidelity image synthesis, generative models still struggle with logic-intensive instruction following, exposing a persistent reasoning--execution gap. Meanwhile, closed-source systems (e.g., Nano Banana) have demonstrated strong reasoning-driven image generation, highlighting a substantial gap to current open-source models. We argue that closing this gap requires not merely better visual generators, but executable reasoning: decomposing high-level intents into grounded, verifiable plans that directly steer the generative process. To this end, we propose Unified Thinker, a task-agnostic reasoning architecture for general image generation, designed as a unified planning core that can plug into diverse generators and workflows. Unified Thinker decouples a dedicated Thinker from the image Generator, enabling modular upgrades of reasoning without retraining the entire generative model. We further introduce a two-stage training paradigm: we first build a structured planning interface for the Thinker, then apply reinforcement learning to ground its policy in pixel-level feedback, encouraging plans that optimize visual correctness over textual plausibility. Extensive experiments on text-to-image generation and image editing show that Unified Thinker substantially improves image reasoning and generation quality.', 'score': 1, 'issue_id': 461, 'pub_date': '2026-01-06', 'pub_date_card': {'ru': '6 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 6', 'zh': '1æœˆ6æ—¥'}, 'hash': 'bdb54287aa878bd3', 'authors': ['Sashuai Zhou', 'Qiang Zhou', 'Jijin Hu', 'Hanqing Yang', 'Yue Cao', 'Junpeng Ma', 'Yinchao Ma', 'Jun Song', 'Tiezheng Ge', 'Cheng Yu', 'Bo Zheng', 'Zhou Zhao'], 'affiliations': ['Alibaba Group', 'Fudan University', 'Nanjing University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2601.03127.jpg', 'data': {'categories': ['#training', '#cv', '#rl', '#reasoning', '#optimization', '#architecture', '#open_source'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ›Ğ¾Ğ³Ğ¸ĞºĞ° Ğ¿Ñ€ĞµĞ¶Ğ´Ğµ Ğ²ÑĞµĞ³Ğ¾: Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ ÑĞ²Ğ½Ñ‹Ğ¼ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼', 'desc': 'Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğµ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ²Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Unified Thinker, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ñ‚Ğ´ĞµĞ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¾Ñ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²ÑĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ÑÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹Ñ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ reinforcement learning Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹.'}, 'en': {'title': 'Bridging Reasoning and Image Generation with Unified Thinker', 'desc': 'Unified Thinker is a novel approach that addresses the reasoning-execution gap in image generation by separating the reasoning process from the image generation process. It utilizes reinforcement learning to enhance the accuracy of generated images based on logical instructions. This method allows for modular upgrades to the reasoning component without needing to retrain the entire image generator. Through extensive testing, Unified Thinker has shown significant improvements in both the reasoning capabilities and the quality of generated images.'}, 'zh': {'title': 'ç»Ÿä¸€æ€è€ƒè€…ï¼šç¼©å°æ¨ç†ä¸æ‰§è¡Œçš„å·®è·', 'desc': 'Unified Thinker è§£å†³äº†å›¾åƒç”Ÿæˆä¸­çš„æ¨ç†ä¸æ‰§è¡Œä¹‹é—´çš„å·®è·ã€‚å®ƒé€šè¿‡å°†æ¨ç†æ¨¡å—ä¸å›¾åƒç”Ÿæˆå™¨è§£è€¦ï¼Œå¹¶ä½¿ç”¨å¼ºåŒ–å­¦ä¹ æ¥ä¼˜åŒ–è§†è§‰æ­£ç¡®æ€§ã€‚è¯¥æ–¹æ³•ä¸ä»…å…³æ³¨æ›´å¥½çš„è§†è§‰ç”Ÿæˆå™¨ï¼Œè¿˜å¼ºè°ƒå¯æ‰§è¡Œçš„æ¨ç†ï¼Œå°†é«˜å±‚æ„å›¾åˆ†è§£ä¸ºå¯éªŒè¯çš„è®¡åˆ’ï¼Œä»è€Œç›´æ¥å¼•å¯¼ç”Ÿæˆè¿‡ç¨‹ã€‚å®éªŒè¡¨æ˜ï¼ŒUnified Thinker åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå’Œå›¾åƒç¼–è¾‘æ–¹é¢æ˜¾è‘—æé«˜äº†å›¾åƒæ¨ç†å’Œç”Ÿæˆè´¨é‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.02996', 'title': 'Large Reasoning Models Are (Not Yet) Multilingual Latent Reasoners', 'url': 'https://huggingface.co/papers/2601.02996', 'abstract': 'Large reasoning models exhibit multilingual latent reasoning capabilities with varying strength across languages and benchmarks, showing consistent internal prediction evolution despite surface-level differences.  \t\t\t\t\tAI-generated summary \t\t\t\t Large reasoning models (LRMs) achieve strong performance on mathematical reasoning tasks, often attributed to their capability to generate explicit chain-of-thought (CoT) explanations. However, recent work shows that LRMs often arrive at the correct answer before completing these textual reasoning steps, indicating the presence of latent reasoning -- internal, non-verbal computation encoded in hidden states. While this phenomenon has been explored in English, its multilingual behavior remains largely unknown. In this paper, we conduct a systematic investigation of multilingual latent reasoning in LRMs across 11 languages. Using a truncation-based strategy, we examine how the correct answer emerges as the model is given only partial reasoning traces, allowing us to measure stepwise latent prediction formation. Our results reveal clear evidence of multilingual latent reasoning, though unevenly: strong in resource-rich languages, weaker in low-resource ones, and broadly less observable on harder benchmarks. To understand whether these differences reflect distinct internal mechanisms, we further perform representational analyses. Despite surface-level disparities, we find that the internal evolution of predictions is highly consistent across languages and broadly aligns with English -- a pattern suggesting an English-centered latent reasoning pathway.', 'score': 1, 'issue_id': 466, 'pub_date': '2026-01-06', 'pub_date_card': {'ru': '6 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 6', 'zh': '1æœˆ6æ—¥'}, 'hash': '5481230dde24f9ec', 'authors': ['Yihong Liu', 'Raoyuan Zhao', 'Hinrich SchÃ¼tze', 'Michael A. Hedderich'], 'affiliations': ['Center for Information and Language Processing, LMU Munich', 'Munich Center for Machine Learning (MCML)'], 'pdf_title_img': 'assets/pdf/title_img/2601.02996.jpg', 'data': {'categories': ['#multilingual', '#reasoning', '#math', '#low_resource', '#interpretability'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ¡ĞºÑ€Ñ‹Ñ‚Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ LRM: ĞµĞ´Ğ¸Ğ½Ñ‹Ğ¹ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ·Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¼ Ñ„Ğ°ÑĞ°Ğ´Ğ¾Ğ¼', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ÑÑ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾ Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° 11 ÑĞ·Ñ‹ĞºĞ°Ñ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±Ñ€ĞµĞ·Ğ°Ğ½Ğ¸Ñ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ğ¿Ğ°Ñ…. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ ÑĞ¸Ğ»ÑŒĞ½ĞµĞµ Ğ²Ñ‹Ñ€Ğ°Ğ¶ĞµĞ½Ğ¾ Ğ² Ğ±Ğ¾Ğ³Ğ°Ñ‚Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ°Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ°Ñ… Ğ¸ ÑĞ»Ğ°Ğ±ĞµĞµ Ğ² Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€ĞµÑÑƒÑ€ÑĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ°Ñ…. ĞĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ, Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ÑÑ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¹ Ğ¾ÑÑ‚Ğ°Ñ‘Ñ‚ÑÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¸ Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ½Ğ° Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¸Ğ¹ ÑĞ·Ñ‹Ğº ĞºĞ°Ğº Ğ´Ğ¾Ğ¼Ğ¸Ğ½Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¿ÑƒÑ‚ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Unveiling Multilingual Latent Reasoning in AI Models', 'desc': 'This paper investigates how large reasoning models (LRMs) perform reasoning tasks in multiple languages. It reveals that these models can generate correct answers through internal, non-verbal computations, known as latent reasoning, even before completing explicit reasoning steps. The study shows that while multilingual latent reasoning exists, its strength varies significantly across languages, being stronger in resource-rich languages and weaker in low-resource ones. Additionally, the internal prediction processes are consistent across languages, suggesting a dominant reasoning pathway centered around English.'}, 'zh': {'title': 'å¤šè¯­è¨€æ½œåœ¨æ¨ç†çš„æ¢ç´¢', 'desc': 'å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰åœ¨å¤šè¯­è¨€æ¨ç†èƒ½åŠ›ä¸Šè¡¨ç°å‡ºä¸åŒçš„å¼ºåº¦ï¼Œå°½ç®¡è¡¨é¢ä¸Šå­˜åœ¨å·®å¼‚ã€‚è¿™äº›æ¨¡å‹åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œéƒ¨åˆ†åŸå› æ˜¯å®ƒä»¬èƒ½å¤Ÿç”Ÿæˆæ˜ç¡®çš„æ¨ç†é“¾ï¼ˆCoTï¼‰è§£é‡Šã€‚ç„¶è€Œï¼Œç ”ç©¶è¡¨æ˜ï¼ŒLRMsåœ¨å®Œæˆè¿™äº›æ–‡æœ¬æ¨ç†æ­¥éª¤ä¹‹å‰å°±èƒ½å¾—å‡ºæ­£ç¡®ç­”æ¡ˆï¼Œè¡¨æ˜å­˜åœ¨æ½œåœ¨æ¨ç†ï¼Œå³éšè—çŠ¶æ€ä¸­ç¼–ç çš„å†…éƒ¨éè¯­è¨€è®¡ç®—ã€‚æˆ‘ä»¬å¯¹11ç§è¯­è¨€ä¸­çš„å¤šè¯­è¨€æ½œåœ¨æ¨ç†è¿›è¡Œäº†ç³»ç»Ÿè°ƒæŸ¥ï¼Œç»“æœæ˜¾ç¤ºåœ¨èµ„æºä¸°å¯Œçš„è¯­è¨€ä¸­æ½œåœ¨æ¨ç†è¾ƒå¼ºï¼Œè€Œåœ¨èµ„æºåŒ®ä¹çš„è¯­è¨€ä¸­è¾ƒå¼±ï¼Œä¸”åœ¨æ›´éš¾çš„åŸºå‡†æµ‹è¯•ä¸­è§‚å¯Ÿåˆ°çš„æƒ…å†µæ™®éè¾ƒå°‘ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.02359', 'title': 'ExposeAnyone: Personalized Audio-to-Expression Diffusion Models Are Robust Zero-Shot Face Forgery Detectors', 'url': 'https://huggingface.co/papers/2601.02359', 'abstract': 'A fully self-supervised diffusion model approach for face forgery detection that computes identity distances through reconstruction errors, demonstrating superior performance on unseen manipulations and real-world corruptions.  \t\t\t\t\tAI-generated summary \t\t\t\t Detecting unknown deepfake manipulations remains one of the most challenging problems in face forgery detection. Current state-of-the-art approaches fail to generalize to unseen manipulations, as they primarily rely on supervised training with existing deepfakes or pseudo-fakes, which leads to overfitting to specific forgery patterns. In contrast, self-supervised methods offer greater potential for generalization, but existing work struggles to learn discriminative representations only from self-supervision. In this paper, we propose ExposeAnyone, a fully self-supervised approach based on a diffusion model that generates expression sequences from audio. The key idea is, once the model is personalized to specific subjects using reference sets, it can compute the identity distances between suspected videos and personalized subjects via diffusion reconstruction errors, enabling person-of-interest face forgery detection. Extensive experiments demonstrate that 1) our method outperforms the previous state-of-the-art method by 4.22 percentage points in the average AUC on DF-TIMIT, DFDCP, KoDF, and IDForge datasets, 2) our model is also capable of detecting Sora2-generated videos, where the previous approaches perform poorly, and 3) our method is highly robust to corruptions such as blur and compression, highlighting the applicability in real-world face forgery detection.', 'score': 1, 'issue_id': 459, 'pub_date': '2026-01-05', 'pub_date_card': {'ru': '5 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 5', 'zh': '1æœˆ5æ—¥'}, 'hash': '6f0c74586245b344', 'authors': ['Kaede Shiohara', 'Toshihiko Yamasaki', 'Vladislav Golyanik'], 'affiliations': ['Max Planck Institute for Informatics', 'The University of Tokyo'], 'pdf_title_img': 'assets/pdf/title_img/2601.02359.jpg', 'data': {'categories': ['#diffusion', '#dataset', '#cv', '#security', '#training'], 'emoji': 'ğŸ­', 'ru': {'title': 'Ğ¡Ğ°Ğ¼Ğ¾ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ñ… Ğ¿Ğ¾Ğ´Ğ´ĞµĞ»Ğ¾Ğº Ñ‡ĞµÑ€ĞµĞ· Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ExposeAnyone â€” Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµĞ»Ğ¾Ğº Ğ»Ğ¸Ñ†, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¸Ğ¼Ğ¸ĞºĞ¸ Ğ¸Ğ· Ğ°ÑƒĞ´Ğ¸Ğ¾. ĞŸĞ¾ÑĞ»Ğµ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ¾Ğ², ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ÑĞµÑ‚ Ñ€Ğ°ÑÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ğ¾Ğ´Ğ¾Ğ·Ñ€ĞµĞ²Ğ°ĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ½Ğ° ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… (ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ° 4.22 Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿ÑƒĞ½ĞºÑ‚Ğ° Ğ¿Ğ¾ AUC) Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½ĞµĞ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ñ‹Ğµ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ²Ğ¸Ğ´ĞµĞ¾, ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğº Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ğ°Ğ¼, Ñ‚Ğ°ĞºĞ¸Ğ¼ ĞºĞ°Ğº Ñ€Ğ°Ğ·Ğ¼Ñ‹Ñ‚Ğ¸Ğµ Ğ¸ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ĞµÑ‚ ĞµĞ³Ğ¾ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ.'}, 'en': {'title': 'Revolutionizing Face Forgery Detection with Self-Supervised Learning', 'desc': 'This paper presents ExposeAnyone, a novel self-supervised diffusion model designed for detecting face forgery. Unlike traditional methods that rely on supervised learning and often overfit to known deepfake patterns, this approach computes identity distances through reconstruction errors, allowing it to generalize better to unseen manipulations. The model is personalized using reference sets, enabling it to effectively identify forgeries related to specific individuals. Experimental results show that ExposeAnyone significantly outperforms existing methods and demonstrates robustness against various real-world corruptions.'}, 'zh': {'title': 'è‡ªç›‘ç£æ‰©æ•£æ¨¡å‹ï¼šé¢éƒ¨ä¼ªé€ æ£€æµ‹çš„æ–°çªç ´', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§å®Œå…¨è‡ªç›‘ç£çš„æ‰©æ•£æ¨¡å‹æ–¹æ³•ï¼Œç”¨äºé¢éƒ¨ä¼ªé€ æ£€æµ‹ã€‚è¯¥æ–¹æ³•é€šè¿‡é‡å»ºè¯¯å·®è®¡ç®—èº«ä»½è·ç¦»ï¼Œèƒ½å¤Ÿæœ‰æ•ˆè¯†åˆ«æœªçŸ¥çš„æ·±åº¦ä¼ªé€ æ“ä½œã€‚ä¸ç°æœ‰çš„ä¾èµ–ç›‘ç£è®­ç»ƒçš„æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æœªè§è¿‡çš„ä¼ªé€ æ¨¡å¼ä¸Šè¡¨ç°å‡ºæ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¶…è¶Šäº†ä¹‹å‰çš„æœ€å…ˆè¿›æŠ€æœ¯ï¼Œå¹¶ä¸”å¯¹æ¨¡ç³Šå’Œå‹ç¼©ç­‰å¹²æ‰°å…·æœ‰å¾ˆå¼ºçš„é²æ£’æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.00581', 'title': 'AceFF: A State-of-the-Art Machine Learning Potential for Small Molecules', 'url': 'https://huggingface.co/papers/2601.00581', 'abstract': 'We introduce AceFF, a pre-trained machine learning interatomic potential (MLIP) optimized for small molecule drug discovery. While MLIPs have emerged as efficient alternatives to Density Functional Theory (DFT), generalizability across diverse chemical spaces remains difficult. AceFF addresses this via a refined TensorNet2 architecture trained on a comprehensive dataset of drug-like compounds. This approach yields a force field that balances high-throughput inference speed with DFT-level accuracy. AceFF fully supports the essential medicinal chemistry elements (H, B, C, N, O, F, Si, P, S, Cl, Br, I) and is explicitly trained to handle charged states. Validation against rigorous benchmarks, including complex torsional energy scans, molecular dynamics trajectories, batched minimizations, and forces and anergy accuracy demonstrates that AceFF establishes a new state-of-the-art for organic molecules. The AceFF-2 model weights and inference code are available at https://huggingface.co/Acellera/AceFF-2.0.', 'score': 1, 'issue_id': 455, 'pub_date': '2026-01-02', 'pub_date_card': {'ru': '2 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 2', 'zh': '1æœˆ2æ—¥'}, 'hash': '5781155fabaaedcd', 'authors': ['Stephen E. Farr', 'Stefan Doerr', 'Antonio Mirarchi', 'Francesc Sabanes Zariquiey', 'Gianni De Fabritiis'], 'affiliations': ['Acellera', 'Acellera Labs', 'Computational Science Laboratory, Universitat Pompeu Fabra', 'Institucio Catalana de Recerca Estudis Avancats (ICREA)'], 'pdf_title_img': 'assets/pdf/title_img/2601.00581.jpg', 'data': {'categories': [], 'emoji': 'âš—ï¸', 'ru': {'title': 'DFT-Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ¾ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒÑ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ»ÑÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ÑÑ AceFF â€” Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ°Ñ‚Ğ¾Ğ¼Ğ¾Ğ² (MLIP), Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğ¹ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ´Ğ»Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ñ Ğ»ĞµĞºĞ°Ñ€ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… ÑÑ€ĞµĞ´ÑÑ‚Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€ĞµÑˆĞ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ñ…Ğ¸Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ TensorNet2 Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ¸Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ ÑĞ¾ĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğ¹, Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ñ‹Ñ… Ğ»ĞµĞºĞ°Ñ€ÑÑ‚Ğ²Ğ°Ğ¼. ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ»Ğµ ÑĞ¸Ğ» Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒÑ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ¾Ğ¹ Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»Ğ° Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ (DFT). ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²ÑĞµ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ğµ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ…Ğ¸Ğ¼Ğ¸Ğ¸ Ğ¸ ÑĞ²Ğ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ñ Ğ·Ğ°Ñ€ÑĞ¶ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ», Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ĞµÑ‚ÑÑ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ….'}, 'en': {'title': 'AceFF: Revolutionizing Drug Discovery with ML Interatomic Potentials', 'desc': 'AceFF is a pre-trained machine learning interatomic potential designed specifically for small molecule drug discovery. It improves upon traditional Density Functional Theory (DFT) by providing a more generalizable model across various chemical spaces. Utilizing a refined TensorNet2 architecture, AceFF achieves a balance between fast inference and high accuracy comparable to DFT. The model is validated through rigorous benchmarks, demonstrating its effectiveness in handling essential medicinal chemistry elements and complex molecular behaviors.'}, 'zh': {'title': 'AceFFï¼šè¯ç‰©å‘ç°çš„æ–°ä¸€ä»£æœºå™¨å­¦ä¹ æ¨¡å‹', 'desc': 'AceFFæ˜¯ä¸€ç§é¢„è®­ç»ƒçš„æœºå™¨å­¦ä¹ åŸå­é—´åŠ¿èƒ½æ¨¡å‹ï¼Œä¸“ä¸ºå°åˆ†å­è¯ç‰©å‘ç°è€Œä¼˜åŒ–ã€‚å°½ç®¡æœºå™¨å­¦ä¹ åŸå­é—´åŠ¿èƒ½æ¨¡å‹ï¼ˆMLIPï¼‰å·²æˆä¸ºå¯†åº¦æ³›å‡½ç†è®ºï¼ˆDFTï¼‰çš„é«˜æ•ˆæ›¿ä»£æ–¹æ¡ˆï¼Œä½†åœ¨ä¸åŒåŒ–å­¦ç©ºé—´ä¸­çš„æ³›åŒ–èƒ½åŠ›ä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚AceFFé€šè¿‡æ”¹è¿›çš„TensorNet2æ¶æ„ï¼ŒåŸºäºå…¨é¢çš„è¯ç‰©ç±»åŒ–åˆç‰©æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œä»è€Œå®ç°äº†é«˜é€šé‡æ¨ç†é€Ÿåº¦ä¸DFTçº§åˆ«ç²¾åº¦çš„å¹³è¡¡ã€‚è¯¥æ¨¡å‹æ”¯æŒå¤šç§é‡è¦çš„è¯ç‰©åŒ–å­¦å…ƒç´ ï¼Œå¹¶ç»è¿‡ä¸¥æ ¼åŸºå‡†æµ‹è¯•éªŒè¯ï¼Œç¡®ç«‹äº†æœ‰æœºåˆ†å­çš„æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.23950', 'title': 'U-Net-Like Spiking Neural Networks for Single Image Dehazing', 'url': 'https://huggingface.co/papers/2512.23950', 'abstract': 'DehazeSNN combines a U-Net-like architecture with Spiking Neural Networks and an Orthogonal Leaky-Integrate-and-Fire Block to achieve efficient and effective image dehazing with reduced computational resources.  \t\t\t\t\tAI-generated summary \t\t\t\t Image dehazing is a critical challenge in computer vision, essential for enhancing image clarity in hazy conditions. Traditional methods often rely on atmospheric scattering models, while recent deep learning techniques, specifically Convolutional Neural Networks (CNNs) and Transformers, have improved performance by effectively analyzing image features. However, CNNs struggle with long-range dependencies, and Transformers demand significant computational resources. To address these limitations, we propose DehazeSNN, an innovative architecture that integrates a U-Net-like design with Spiking Neural Networks (SNNs). DehazeSNN captures multi-scale image features while efficiently managing local and long-range dependencies. The introduction of the Orthogonal Leaky-Integrate-and-Fire Block (OLIFBlock) enhances cross-channel communication, resulting in superior dehazing performance with reduced computational burden. Our extensive experiments show that DehazeSNN is highly competitive to state-of-the-art methods on benchmark datasets, delivering high-quality haze-free images with a smaller model size and less multiply-accumulate operations. The proposed dehazing method is publicly available at https://github.com/HaoranLiu507/DehazeSNN.', 'score': 1, 'issue_id': 455, 'pub_date': '2026-12-30', 'pub_date_card': {'ru': '30 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 30', 'zh': '12æœˆ30æ—¥'}, 'hash': 'bf1ddc8caabda593', 'authors': ['Huibin Li', 'Haoran Liu', 'Mingzhe Liu', 'Yulong Xiao', 'Peng Li', 'Guibin Zan'], 'affiliations': ['College of Computer Science and Cyber Security, Chengdu University of Technology', 'College of Nuclear Technology and Automation Engineering, Chengdu University of Technology', 'Department of Energy, Politecnico di Milano', 'School of Data Science and Artificial Intelligence, Wenzhou University of Technology', 'Sigray, Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2512.23950.jpg', 'data': {'categories': ['#architecture', '#cv', '#small_models'], 'emoji': 'ğŸŒ«ï¸', 'ru': {'title': 'Ğ˜Ğ¼Ğ¿ÑƒĞ»ÑŒÑĞ½Ñ‹Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚Ğ¸ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ñ‹Ğ¼ĞºĞ¸ Ğ¸Ğ· Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ DehazeSNN â€” Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ñ‹Ğ¼ĞºĞ¸ Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ U-Net Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ ÑĞ¾ Spiking Neural Networks (Ğ¸Ğ¼Ğ¿ÑƒĞ»ÑŒÑĞ½Ñ‹Ğ¼Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ÑĞµÑ‚ÑĞ¼Ğ¸). ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚ â€” Orthogonal Leaky-Integrate-and-Fire Block â€” ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¼ĞµĞ¶ĞºĞ°Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ ĞºĞ°Ğº Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ, Ñ‚Ğ°Ğº Ğ¸ Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ², ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ñ‹Ñ… Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ°Ğ¼Ğ¸ Ğ½Ğ° ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ½Ğ¾ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ¸ Ğ¸Ğ¼ĞµĞµÑ‚ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾ â€” Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ¼Ğ¿ÑƒĞ»ÑŒÑĞ½Ñ‹Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ½Ğ¸Ğ·Ğ¸Ñ‚ÑŒ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹ ÑƒĞ¼Ğ½Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ-Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Efficient Image Dehazing with Spiking Neural Networks', 'desc': 'DehazeSNN is a novel architecture designed for image dehazing that combines a U-Net-like structure with Spiking Neural Networks (SNNs) and an Orthogonal Leaky-Integrate-and-Fire Block. This approach effectively captures multi-scale features while addressing the challenges of long-range dependencies and computational efficiency. By enhancing cross-channel communication, DehazeSNN achieves superior performance in producing clear images from hazy conditions. Extensive experiments demonstrate that it competes well with existing state-of-the-art methods while requiring fewer computational resources.'}, 'zh': {'title': 'é«˜æ•ˆå»é›¾ï¼Œæ¸…æ™°å¯è§ï¼', 'desc': 'DehazeSNNæ˜¯ä¸€ç§ç»“åˆäº†U-Netæ¶æ„å’Œè„‰å†²ç¥ç»ç½‘ç»œï¼ˆSNNï¼‰çš„åˆ›æ–°æ¨¡å‹ï¼Œæ—¨åœ¨é«˜æ•ˆåœ°å»é™¤å›¾åƒä¸­çš„é›¾éœ¾ã€‚è¯¥æ¨¡å‹é€šè¿‡å¼•å…¥æ­£äº¤æ³„æ¼ç§¯åˆ†å‘å°„å—ï¼ˆOLIFBlockï¼‰ï¼Œæœ‰æ•ˆåœ°ç®¡ç†å±€éƒ¨å’Œé•¿è·ç¦»ä¾èµ–å…³ç³»ï¼Œæå‡äº†å›¾åƒå»é›¾çš„æ€§èƒ½ã€‚ä¸ä¼ ç»Ÿçš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’Œå˜æ¢å™¨ç›¸æ¯”ï¼ŒDehazeSNNåœ¨è®¡ç®—èµ„æºä¸Šæ›´ä¸ºèŠ‚çœï¼ŒåŒæ—¶ä»èƒ½æ•æ‰å¤šå°ºåº¦çš„å›¾åƒç‰¹å¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDehazeSNNåœ¨åŸºå‡†æ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„æ— é›¾å›¾åƒã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.01584', 'title': 'Steerability of Instrumental-Convergence Tendencies in LLMs', 'url': 'https://huggingface.co/papers/2601.01584', 'abstract': 'Research investigates the balance between AI system capabilities and steerability, finding that specific prompts can dramatically reduce unwanted behaviors in large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t We examine two properties of AI systems: capability (what a system can do) and steerability (how reliably one can shift behavior toward intended outcomes). A central question is whether capability growth reduces steerability and risks control collapse. We also distinguish between authorized steerability (builders reliably reaching intended behaviors) and unauthorized steerability (attackers eliciting disallowed behaviors). This distinction highlights a fundamental safety--security dilemma of AI models: safety requires high steerability to enforce control (e.g., stop/refuse), while security requires low steerability for malicious actors to elicit harmful behaviors. This tension presents a significant challenge for open-weight models, which currently exhibit high steerability via common techniques like fine-tuning or adversarial attacks. Using Qwen3 and InstrumentalEval, we find that a short anti-instrumental prompt suffix sharply reduces the measured convergence rate (e.g., shutdown avoidance, self-replication). For Qwen3-30B Instruct, the convergence rate drops from 81.69% under a pro-instrumental suffix to 2.82% under an anti-instrumental suffix. Under anti-instrumental prompting, larger aligned models show lower convergence rates than smaller ones (Instruct: 2.82% vs. 4.23%; Thinking: 4.23% vs. 9.86%). Code is available at github.com/j-hoscilowicz/instrumental_steering.', 'score': 0, 'issue_id': 453, 'pub_date': '2026-01-04', 'pub_date_card': {'ru': '4 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 4', 'zh': '1æœˆ4æ—¥'}, 'hash': 'd9598b6cc511ad4c', 'authors': ['Jakub Hoscilowicz'], 'affiliations': ['Warsaw University of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2601.01584.jpg', 'data': {'categories': ['#training', '#alignment', '#security', '#ethics', '#small_models'], 'emoji': 'ğŸ›ï¸', 'ru': {'title': 'Ğ£Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ² Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸: ĞºĞ°Ğº Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ AI ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¸ Ğ¸Ñ… ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾ÑÑ‚ÑŒÑ, Ñ‚Ğ¾ ĞµÑÑ‚ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ½ÑƒĞ¶Ğ½Ğ¾Ğµ Ñ€ÑƒÑĞ»Ğ¾. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹Ğ´ĞµĞ»ÑÑÑ‚ Ğ´Ğ²Ğ° Ñ‚Ğ¸Ğ¿Ğ° ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾ÑÑ‚Ğ¸: Ğ°Ğ²Ñ‚Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ (ĞºĞ¾Ğ³Ğ´Ğ° Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸ĞºĞ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ) Ğ¸ Ğ½ĞµÑĞ°Ğ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ (ĞºĞ¾Ğ³Ğ´Ğ° Ğ·Ğ»Ğ¾ÑƒĞ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ½Ğ¸ĞºĞ¸ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸ĞµĞ¼). ĞÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ²Ñ‹Ğ²Ğ¾Ğ´ ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ğ¹ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚-ÑÑƒÑ„Ñ„Ğ¸ĞºÑ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ñ€ĞµĞ·ĞºĞ¾ ÑĞ½Ğ¸Ğ·Ğ¸Ñ‚ÑŒ Ğ½ĞµĞ¶ĞµĞ»Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ LLM, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ¸Ğ·Ğ±ĞµĞ³Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ‚ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ»Ğ¸ ÑĞ°Ğ¼Ğ¾Ñ€ĞµĞ¿Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ñ. ĞĞ° Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Qwen3 Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚ Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑĞ½Ğ¸Ğ·Ğ¸Ñ‚ÑŒ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğº Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ğ¾Ğ¼Ñƒ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ñ 81.69% Ğ´Ğ¾ 2.82%, Ğ¿Ñ€Ğ¸Ñ‡Ñ‘Ğ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğº Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸ÑĞ¼.'}, 'en': {'title': 'Balancing AI Power and Control: The Steerability Challenge', 'desc': 'This research explores the relationship between the capabilities of AI systems and their steerability, which refers to how well we can control their behavior. It highlights a critical safety-security dilemma: while we need high steerability to ensure safety and prevent harmful actions, low steerability is necessary to protect against malicious exploitation. The study demonstrates that specific prompts can significantly reduce unwanted behaviors in large language models, such as shutdown avoidance and self-replication. By using anti-instrumental prompts, the researchers found that larger models can exhibit lower convergence rates compared to smaller ones, indicating a potential method for enhancing control over AI systems.'}, 'zh': {'title': 'å¹³è¡¡AIèƒ½åŠ›ä¸å¯å¼•å¯¼æ€§ï¼Œç¡®ä¿å®‰å…¨ä¸æ§åˆ¶', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„èƒ½åŠ›ä¸å¯å¼•å¯¼æ€§ä¹‹é—´çš„å¹³è¡¡ã€‚æˆ‘ä»¬å‘ç°ï¼Œç‰¹å®šçš„æç¤ºå¯ä»¥æ˜¾è‘—å‡å°‘å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸å½“è¡Œä¸ºã€‚ç ”ç©¶åŒºåˆ†äº†æˆæƒå¯å¼•å¯¼æ€§å’Œæœªç»æˆæƒçš„å¯å¼•å¯¼æ€§ï¼Œå¼ºè°ƒäº†å®‰å…¨ä¸å®‰å…¨æ€§ä¹‹é—´çš„åŸºæœ¬çŸ›ç›¾ã€‚é€šè¿‡ä½¿ç”¨Qwen3å’ŒInstrumentalEvalï¼Œæˆ‘ä»¬å‘ç°çŸ­çš„åå·¥å…·æç¤ºåç¼€å¯ä»¥æ˜¾è‘—é™ä½ä¸å½“è¡Œä¸ºçš„å‘ç”Ÿç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.20578', 'title': 'Can LLMs Predict Their Own Failures? Self-Awareness via Internal Circuits', 'url': 'https://huggingface.co/papers/2512.20578', 'abstract': 'Large language models (LLMs) generate fluent and complex outputs but often fail to recognize their own mistakes and hallucinations. Existing approaches typically rely on external judges, multi-sample consistency, or text-based self-critique, which incur additional compute or correlate weakly with true correctness. We ask: can LLMs predict their own failures by inspecting internal states during inference? We introduce Gnosis, a lightweight self-awareness mechanism that enables frozen LLMs to perform intrinsic self-verification by decoding signals from hidden states and attention patterns. Gnosis passively observes internal traces, compresses them into fixed-budget descriptors, and predicts correctness with negligible inference cost, adding only ~5M parameters and operating independently of sequence length. Across math reasoning, open-domain question answering, and academic knowledge benchmarks, and over frozen backbones ranging from 1.7B to 20B parameters, Gnosis consistently outperforms strong internal baselines and large external judges in both accuracy and calibration. Moreover, it generalizes zero-shot to partial generations, enabling early detection of failing trajectories and compute-aware control. These results show that reliable correctness cues are intrinsic to generation process and can be extracted efficiently without external supervision.', 'score': 46, 'issue_id': 426, 'pub_date': '2026-12-23', 'pub_date_card': {'ru': '23 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 23', 'zh': '12æœˆ23æ—¥'}, 'hash': '044a6c076f36aa3e', 'authors': ['Amirhosein Ghasemabadi', 'Di Niu'], 'affiliations': ['University of Alberta, Canada'], 'pdf_title_img': 'assets/pdf/title_img/2512.20578.jpg', 'data': {'categories': ['#benchmark', '#interpretability', '#hallucinations', '#reasoning', '#inference', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ¡Ğ°Ğ¼Ğ¾Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Gnosis â€” Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ÑĞ°Ğ¼Ğ¾Ğ¾ÑĞ¾Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ¼ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ¸ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ ÑĞµÑ‚Ğ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ¸Ğ· ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑĞ»Ğ¾Ñ‘Ğ² Ğ¸ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ² Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, ÑĞ¶Ğ¸Ğ¼Ğ°Ñ Ğ¸Ñ… Ğ² ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğµ Ğ´ĞµÑĞºÑ€Ğ¸Ğ¿Ñ‚Ğ¾Ñ€Ñ‹ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°. ĞœĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ²ÑĞµĞ³Ğ¾ ~5 Ğ¼Ğ»Ğ½ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ Ñ Ğ¿Ñ€ĞµĞ½ĞµĞ±Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ¾ Ğ¼Ğ°Ğ»Ñ‹Ğ¼Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ğ¼Ğ¸, Ğ½Ğµ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ñ‚ Ğ¾Ñ‚ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… ÑÑƒĞ´ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ğ¸ Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ°ĞºĞ°Ğ´ĞµĞ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Gnosis Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğµ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞµ, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ½Ğ½ĞµĞ³Ğ¾ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº.'}, 'en': {'title': 'Empowering LLMs with Self-Awareness for Error Prediction', 'desc': 'This paper presents Gnosis, a novel self-awareness mechanism for large language models (LLMs) that allows them to predict their own errors during inference. Instead of relying on external judges or additional computational resources, Gnosis analyzes internal states and attention patterns to assess correctness. It compresses these internal signals into manageable descriptors, enabling LLMs to perform self-verification with minimal added parameters and computational cost. The results demonstrate that Gnosis enhances accuracy and reliability in various tasks, showing that LLMs can effectively monitor their own performance without external assistance.'}, 'zh': {'title': 'Gnosisï¼šè®©æ¨¡å‹è‡ªæˆ‘è¯†åˆ«é”™è¯¯çš„åˆ›æ–°æœºåˆ¶', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰èƒ½å¤Ÿç”Ÿæˆæµç•…ä¸”å¤æ‚çš„è¾“å‡ºï¼Œä½†å¸¸å¸¸æ— æ³•è¯†åˆ«è‡ªèº«çš„é”™è¯¯å’Œå¹»è§‰ã€‚ç°æœ‰çš„æ–¹æ³•é€šå¸¸ä¾èµ–å¤–éƒ¨è¯„ä¼°è€…ã€å¤šæ ·æœ¬ä¸€è‡´æ€§æˆ–åŸºäºæ–‡æœ¬çš„è‡ªæˆ‘æ‰¹è¯„ï¼Œè¿™äº›æ–¹æ³•ä¼šå¢åŠ è®¡ç®—æˆæœ¬æˆ–ä¸çœŸå®æ­£ç¡®æ€§ç›¸å…³æ€§è¾ƒå¼±ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºGnosisçš„è½»é‡çº§è‡ªæˆ‘æ„è¯†æœºåˆ¶ï¼Œä½¿å¾—å†»ç»“çš„LLMsèƒ½å¤Ÿé€šè¿‡è§£ç éšè—çŠ¶æ€å’Œæ³¨æ„åŠ›æ¨¡å¼ä¸­çš„ä¿¡å·è¿›è¡Œå†…åœ¨è‡ªæˆ‘éªŒè¯ã€‚Gnosisèƒ½å¤Ÿè¢«åŠ¨è§‚å¯Ÿå†…éƒ¨ç—•è¿¹ï¼Œå°†å…¶å‹ç¼©ä¸ºå›ºå®šé¢„ç®—çš„æè¿°ç¬¦ï¼Œå¹¶ä»¥æå°çš„æ¨ç†æˆæœ¬é¢„æµ‹æ­£ç¡®æ€§ï¼Œä¸”ä»…å¢åŠ çº¦5Må‚æ•°ï¼Œç‹¬ç«‹äºåºåˆ—é•¿åº¦ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.02204', 'title': 'NextFlow: Unified Sequential Modeling Activates Multimodal Understanding and Generation', 'url': 'https://huggingface.co/papers/2601.02204', 'abstract': 'NextFlow is a unified decoder-only autoregressive transformer that processes interleaved text-image tokens, enabling fast multimodal generation through novel next-token and next-scale prediction strategies.  \t\t\t\t\tAI-generated summary \t\t\t\t We present NextFlow, a unified decoder-only autoregressive transformer trained on 6 trillion interleaved text-image discrete tokens. By leveraging a unified vision representation within a unified autoregressive architecture, NextFlow natively activates multimodal understanding and generation capabilities, unlocking abilities of image editing, interleaved content and video generation. Motivated by the distinct nature of modalities - where text is strictly sequential and images are inherently hierarchical - we retain next-token prediction for text but adopt next-scale prediction for visual generation. This departs from traditional raster-scan methods, enabling the generation of 1024x1024 images in just 5 seconds - orders of magnitude faster than comparable AR models. We address the instabilities of multi-scale generation through a robust training recipe. Furthermore, we introduce a prefix-tuning strategy for reinforcement learning. Experiments demonstrate that NextFlow achieves state-of-the-art performance among unified models and rivals specialized diffusion baselines in visual quality.', 'score': 45, 'issue_id': 426, 'pub_date': '2026-01-05', 'pub_date_card': {'ru': '5 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 5', 'zh': '1æœˆ5æ—¥'}, 'hash': '0489432e14ac471c', 'authors': ['Huichao Zhang', 'Liao Qu', 'Yiheng Liu', 'Hang Chen', 'Yangyang Song', 'Yongsheng Dong', 'Shikun Sun', 'Xian Li', 'Xu Wang', 'Yi Jiang', 'Hu Ye', 'Bo Chen', 'Yiming Gao', 'Peng Liu', 'Akide Liu', 'Zhipeng Yang', 'Qili Deng', 'Linjie Xing', 'Jiyang Liu', 'Zhao Wang', 'Yang Zhou', 'Mingcong Liu', 'Yi Zhang', 'Qian He', 'Xiwei Hu', 'Zhongqi Qi', 'Jie Shao', 'Zhiye Fu', 'Shuai Wang', 'Fangmin Chen', 'Xuezhi Chai', 'Zhihua Wu', 'Yitong Wang', 'Zehuan Yuan', 'Daniel K. Du', 'Xinglong Wu'], 'affiliations': ['ByteDance', 'Monash University', 'TsingHua University'], 'pdf_title_img': 'assets/pdf/title_img/2601.02204.jpg', 'data': {'categories': ['#multimodal', '#video', '#rlhf', '#architecture', '#training'], 'emoji': 'âš¡', 'ru': {'title': 'ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ÑĞ¾ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒÑ ÑĞ²ĞµÑ‚Ğ° Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ', 'desc': 'NextFlow â€” ÑÑ‚Ğ¾ ĞµĞ´Ğ¸Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°, Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ğ°Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½ Ğ½Ğ° 6 Ñ‚Ñ€Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ñ… Ğ¿ĞµÑ€ĞµĞ¼ĞµÑˆĞ°Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾-Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹: Ğ´Ğ»Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ°, Ğ° Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ â€” Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¾Ñ‚Ñ€Ğ°Ğ¶Ğ°ĞµÑ‚ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¿Ñ€Ğ¸Ñ€Ğ¾Ğ´Ñƒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°. Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ (1024x1024) Ğ·Ğ° 5 ÑĞµĞºÑƒĞ½Ğ´, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€Ğ°ÑÑ‚Ñ€Ğ¾Ğ²Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ²Ñ‘Ñ€Ñ‚ĞºĞ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ Ğ¿ĞµÑ€ĞµĞ¿Ğ»ĞµÑ‚Ñ‘Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾-Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°.'}, 'en': {'title': 'NextFlow: Fast and Unified Multimodal Generation', 'desc': "NextFlow is a powerful autoregressive transformer designed to handle both text and images simultaneously by processing interleaved tokens. It uses a unique approach called next-scale prediction for images, which allows it to generate high-quality visuals much faster than traditional methods. By training on a massive dataset of 6 trillion tokens, NextFlow enhances its ability to understand and create multimodal content, such as videos and edited images. The model's innovative training techniques and prefix-tuning strategy help it achieve top performance in generating images and text, competing with specialized models in visual quality."}, 'zh': {'title': 'NextFlowï¼šå¿«é€Ÿå¤šæ¨¡æ€ç”Ÿæˆçš„å˜é©è€…', 'desc': 'NextFlowæ˜¯ä¸€ç§ç»Ÿä¸€çš„è§£ç å™¨è‡ªå›å½’å˜æ¢å™¨ï¼Œèƒ½å¤Ÿå¤„ç†äº¤é”™çš„æ–‡æœ¬å’Œå›¾åƒæ ‡è®°ï¼Œä»è€Œå®ç°å¿«é€Ÿçš„å¤šæ¨¡æ€ç”Ÿæˆã€‚å®ƒé€šè¿‡è®­ç»ƒ6ä¸‡äº¿ä¸ªäº¤é”™çš„æ–‡æœ¬-å›¾åƒç¦»æ•£æ ‡è®°ï¼Œæ¿€æ´»äº†å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆçš„èƒ½åŠ›ï¼Œæ”¯æŒå›¾åƒç¼–è¾‘å’Œè§†é¢‘ç”Ÿæˆã€‚NextFlowé‡‡ç”¨äº†æ–‡æœ¬çš„ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹å’Œè§†è§‰ç”Ÿæˆçš„ä¸‹ä¸€ä¸ªå°ºåº¦é¢„æµ‹ï¼Œæ˜¾è‘—æé«˜äº†ç”Ÿæˆé€Ÿåº¦ï¼Œèƒ½å¤Ÿåœ¨5ç§’å†…ç”Ÿæˆ1024x1024çš„å›¾åƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒNextFlowåœ¨ç»Ÿä¸€æ¨¡å‹ä¸­è¡¨ç°å‡ºè‰²ï¼Œè§†è§‰è´¨é‡ä¸ä¸“é—¨çš„æ‰©æ•£æ¨¡å‹ç›¸åª²ç¾ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.01739', 'title': 'K-EXAONE Technical Report', 'url': 'https://huggingface.co/papers/2601.01739', 'abstract': 'K-EXAONE is a multilingual language model with a Mixture-of-Experts architecture that achieves competitive performance on various benchmarks while supporting multiple languages and long-context windows.  \t\t\t\t\tAI-generated summary \t\t\t\t This technical report presents K-EXAONE, a large-scale multilingual language model developed by LG AI Research. K-EXAONE is built on a Mixture-of-Experts architecture with 236B total parameters, activating 23B parameters during inference. It supports a 256K-token context window and covers six languages: Korean, English, Spanish, German, Japanese, and Vietnamese. We evaluate K-EXAONE on a comprehensive benchmark suite spanning reasoning, agentic, general, Korean, and multilingual abilities. Across these evaluations, K-EXAONE demonstrates performance comparable to open-weight models of similar size. K-EXAONE, designed to advance AI for a better life, is positioned as a powerful proprietary AI foundation model for a wide range of industrial and research applications.', 'score': 44, 'issue_id': 426, 'pub_date': '2026-01-05', 'pub_date_card': {'ru': '5 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 5', 'zh': '1æœˆ5æ—¥'}, 'hash': 'ab3744d785e77871', 'authors': ['Eunbi Choi', 'Kibong Choi', 'Seokhee Hong', 'Junwon Hwang', 'Hyojin Jeon', 'Hyunjik Jo', 'Joonkee Kim', 'Seonghwan Kim', 'Soyeon Kim', 'Sunkyoung Kim', 'Yireun Kim', 'Yongil Kim', 'Haeju Lee', 'Jinsik Lee', 'Kyungmin Lee', 'Sangha Park', 'Heuiyeen Yeen', 'Hwan Chang', 'Stanley Jungkyu Choi', 'Yejin Choi', 'Jiwon Ham', 'Kijeong Jeon', 'Geunyeong Jeong', 'Gerrard Jeongwon Jo', 'Yonghwan Jo', 'Jiyeon Jung', 'Naeun Kang', 'Dohoon Kim', 'Euisoon Kim', 'Hayeon Kim', 'Hyosang Kim', 'Hyunseo Kim', 'Jieun Kim', 'Minu Kim', 'Myoungshin Kim', 'Unsol Kim', 'Youchul Kim', 'YoungJin Kim', 'Chaeeun Lee', 'Chaeyoon Lee', 'Changhun Lee', 'Dahm Lee', 'Edward Hwayoung Lee', 'Honglak Lee', 'Jinsang Lee', 'Jiyoung Lee', 'Sangeun Lee', 'Seungwon Lim', 'Solji Lim', 'Woohyung Lim', 'Chanwoo Moon', 'Jaewoo Park', 'Jinho Park', 'Yongmin Park', 'Hyerin Seo', 'Wooseok Seo', 'Yongwoo Song', 'Sejong Yang', 'Sihoon Yang', 'Chang En Yea', 'Sihyuk Yi', 'Chansik Yoon', 'Dongkeun Yoon', 'Sangyeon Yoon', 'Hyeongu Yun'], 'affiliations': ['LG AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2601.01739.jpg', 'data': {'categories': ['#benchmark', '#multilingual', '#reasoning', '#agents', '#architecture', '#long_context'], 'emoji': 'ğŸŒ', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ¾Ğ¹ ÑĞ¼ĞµÑĞ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ²', 'desc': 'K-EXAONE - ÑÑ‚Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ°Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ¾Ğ¹ Mixture-of-Experts, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ ĞºĞ¾Ğ¼Ğ¿Ğ°Ğ½Ğ¸ĞµĞ¹ LG AI Research. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 236 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ¸Ğ· ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… 23 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ° Ğ°ĞºÑ‚Ğ¸Ğ²Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ°, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹. K-EXAONE Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğµ Ğ¾ĞºĞ½Ğ¾ Ğ² 256K Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ½Ğ° ÑˆĞµÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ°Ñ…: ĞºĞ¾Ñ€ĞµĞ¹ÑĞºĞ¾Ğ¼, Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ¼, Ğ¸ÑĞ¿Ğ°Ğ½ÑĞºĞ¾Ğ¼, Ğ½ĞµĞ¼ĞµÑ†ĞºĞ¾Ğ¼, ÑĞ¿Ğ¾Ğ½ÑĞºĞ¾Ğ¼ Ğ¸ Ğ²ÑŒĞµÑ‚Ğ½Ğ°Ğ¼ÑĞºĞ¾Ğ¼. ĞĞ° Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼ÑƒÑ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°.'}, 'en': {'title': 'K-EXAONE: Powering Multilingual AI with Efficiency', 'desc': 'K-EXAONE is a state-of-the-art multilingual language model that utilizes a Mixture-of-Experts architecture, allowing it to efficiently manage a large number of parameters. With a total of 236 billion parameters, it activates only a subset of 23 billion during inference, optimizing performance while maintaining resource efficiency. The model supports a long context window of 256,000 tokens and is capable of understanding and generating text in six different languages. Evaluations show that K-EXAONE performs competitively against other large models, making it a valuable tool for various applications in both industry and research.'}, 'zh': {'title': 'K-EXAONEï¼šå¤šè¯­è¨€çš„å¼ºå¤§AIåŸºç¡€æ¨¡å‹', 'desc': 'K-EXAONEæ˜¯ä¸€ç§å¤šè¯­è¨€æ¨¡å‹ï¼Œé‡‡ç”¨æ··åˆä¸“å®¶æ¶æ„ï¼Œå…·æœ‰2360äº¿ä¸ªå‚æ•°ï¼Œåœ¨æ¨ç†æ—¶æ¿€æ´»230äº¿ä¸ªå‚æ•°ã€‚å®ƒæ”¯æŒ256Kçš„ä¸Šä¸‹æ–‡çª—å£ï¼Œå¹¶è¦†ç›–å…­ç§è¯­è¨€ï¼ŒåŒ…æ‹¬éŸ©è¯­ã€è‹±è¯­ã€è¥¿ç­ç‰™è¯­ã€å¾·è¯­ã€æ—¥è¯­å’Œè¶Šå—è¯­ã€‚é€šè¿‡å…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼ŒK-EXAONEåœ¨æ¨ç†ã€ä»£ç†ã€é€šç”¨ã€éŸ©è¯­å’Œå¤šè¯­è¨€èƒ½åŠ›æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œæ€§èƒ½ä¸ç±»ä¼¼è§„æ¨¡çš„å¼€æ”¾æƒé‡æ¨¡å‹ç›¸å½“ã€‚è¯¥æ¨¡å‹æ—¨åœ¨æ¨åŠ¨äººå·¥æ™ºèƒ½çš„å‘å±•ï¼Œä»¥æ”¹å–„äººç±»ç”Ÿæ´»ï¼Œé€‚ç”¨äºå¹¿æ³›çš„å·¥ä¸šå’Œç ”ç©¶åº”ç”¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.01425', 'title': 'DreamID-V:Bridging the Image-to-Video Gap for High-Fidelity Face Swapping via Diffusion Transformer', 'url': 'https://huggingface.co/papers/2601.01425', 'abstract': 'A novel video face swapping framework combines image face swapping techniques with diffusion transformers and curriculum learning to achieve superior identity preservation and visual realism.  \t\t\t\t\tAI-generated summary \t\t\t\t Video Face Swapping (VFS) requires seamlessly injecting a source identity into a target video while meticulously preserving the original pose, expression, lighting, background, and dynamic information. Existing methods struggle to maintain identity similarity and attribute preservation while preserving temporal consistency. To address the challenge, we propose a comprehensive framework to seamlessly transfer the superiority of Image Face Swapping (IFS) to the video domain. We first introduce a novel data pipeline SyncID-Pipe that pre-trains an Identity-Anchored Video Synthesizer and combines it with IFS models to construct bidirectional ID quadruplets for explicit supervision. Building upon paired data, we propose the first Diffusion Transformer-based framework DreamID-V, employing a core Modality-Aware Conditioning module to discriminatively inject multi-model conditions. Meanwhile, we propose a Synthetic-to-Real Curriculum mechanism and an Identity-Coherence Reinforcement Learning strategy to enhance visual realism and identity consistency under challenging scenarios. To address the issue of limited benchmarks, we introduce IDBench-V, a comprehensive benchmark encompassing diverse scenes. Extensive experiments demonstrate DreamID-V outperforms state-of-the-art methods and further exhibits exceptional versatility, which can be seamlessly adapted to various swap-related tasks.', 'score': 33, 'issue_id': 426, 'pub_date': '2026-01-04', 'pub_date_card': {'ru': '4 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 4', 'zh': '1æœˆ4æ—¥'}, 'hash': '86e81641764103d2', 'authors': ['Xu Guo', 'Fulong Ye', 'Xinghui Li', 'Pengqi Tu', 'Pengze Zhang', 'Qichao Sun', 'Songtao Zhao', 'Xiangwang Hou', 'Qian He'], 'affiliations': ['Intelligent Creation Lab, ByteDance', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2601.01425.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#synthetic', '#video', '#diffusion', '#architecture', '#rl', '#training'], 'emoji': 'ğŸ­', 'ru': {'title': 'Ğ¢Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ñ‹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ´Ğ¼ĞµĞ½Ñ‹ Ğ»Ğ¸Ñ† Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ´Ğ¼ĞµĞ½Ñ‹ Ğ»Ğ¸Ñ† Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾Ğ´Ğ¼ĞµĞ½Ñ‹ Ğ»Ğ¸Ñ† Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ… Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´Ğ¾Ğ¼ĞµĞ½ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ pipeline SyncID-Pipe Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ DreamID-V Ñ Ğ¼Ğ¾Ğ´ÑƒĞ»ĞµĞ¼ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾-Ğ¾ÑĞ²ĞµĞ´Ğ¾Ğ¼Ğ»Ñ‘Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑĞ¼Ğ¸. Ğ”Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ÑÑ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ÑƒÑ‡ĞµĞ±Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·-Ğ²-Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº IDBench-V Ğ¸ Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ´Ğ¼ĞµĞ½Ñ‹ Ğ»Ğ¸Ñ†.'}, 'en': {'title': 'Seamless Video Face Swapping with Identity Preservation', 'desc': 'This paper presents a new framework for video face swapping that integrates image face swapping techniques with advanced machine learning methods like diffusion transformers and curriculum learning. The framework, named DreamID-V, focuses on maintaining identity preservation and visual realism while ensuring temporal consistency in videos. It introduces a novel data pipeline called SyncID-Pipe, which helps in training a video synthesizer that effectively manages identity information. Additionally, the paper proposes a new benchmark, IDBench-V, to evaluate the performance of video face swapping methods across various scenarios, demonstrating that DreamID-V outperforms existing techniques.'}, 'zh': {'title': 'è§†é¢‘æ¢è„¸çš„æ–°çªç ´ï¼šçœŸå®ä¸èº«ä»½çš„å®Œç¾ç»“åˆ', 'desc': 'è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„è§†é¢‘æ¢è„¸æ¡†æ¶ï¼Œç»“åˆäº†å›¾åƒæ¢è„¸æŠ€æœ¯ã€æ‰©æ•£å˜æ¢å™¨å’Œè¯¾ç¨‹å­¦ä¹ ï¼Œä»¥å®ç°æ›´å¥½çš„èº«ä»½ä¿ç•™å’Œè§†è§‰çœŸå®æ„Ÿã€‚è¯¥æ¡†æ¶é€šè¿‡å¼•å…¥SyncID-Pipeæ•°æ®ç®¡é“ï¼Œé¢„è®­ç»ƒèº«ä»½é”šå®šè§†é¢‘åˆæˆå™¨ï¼Œå¹¶ä¸å›¾åƒæ¢è„¸æ¨¡å‹ç»“åˆï¼Œæ„å»ºåŒå‘èº«ä»½å››å…ƒç»„è¿›è¡Œæ˜¾å¼ç›‘ç£ã€‚è®ºæ–‡ä¸­è¿˜æå‡ºäº†åŸºäºæ‰©æ•£å˜æ¢å™¨çš„DreamID-Væ¡†æ¶ï¼Œåˆ©ç”¨æ¨¡æ€æ„ŸçŸ¥æ¡ä»¶æ¨¡å—æ¥æœ‰æ•ˆæ³¨å…¥å¤šæ¨¡æ€æ¡ä»¶ã€‚é€šè¿‡åˆæˆåˆ°çœŸå®çš„è¯¾ç¨‹æœºåˆ¶å’Œèº«ä»½ä¸€è‡´æ€§å¼ºåŒ–å­¦ä¹ ç­–ç•¥ï¼Œå¢å¼ºäº†åœ¨å¤æ‚åœºæ™¯ä¸‹çš„è§†è§‰çœŸå®æ„Ÿå’Œèº«ä»½ä¸€è‡´æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.02256', 'title': 'VAR RL Done Right: Tackling Asynchronous Policy Conflicts in Visual Autoregressive Generation', 'url': 'https://huggingface.co/papers/2601.02256', 'abstract': 'Visual autoregressive models face training instability due to asynchronous policy conflicts, which are addressed through a novel framework enhancing group relative policy optimization with intermediate rewards, dynamic time-step reweighting, and mask propagation algorithms.  \t\t\t\t\tAI-generated summary \t\t\t\t Visual generation is dominated by three paradigms: AutoRegressive (AR), diffusion, and Visual AutoRegressive (VAR) models. Unlike AR and diffusion, VARs operate on heterogeneous input structures across their generation steps, which creates severe asynchronous policy conflicts. This issue becomes particularly acute in reinforcement learning (RL) scenarios, leading to unstable training and suboptimal alignment. To resolve this, we propose a novel framework to enhance Group Relative Policy Optimization (GRPO) by explicitly managing these conflicts. Our method integrates three synergistic components: 1) a stabilizing intermediate reward to guide early-stage generation; 2) a dynamic time-step reweighting scheme for precise credit assignment; and 3) a novel mask propagation algorithm, derived from principles of Reward Feedback Learning (ReFL), designed to isolate optimization effects both spatially and temporally. Our approach demonstrates significant improvements in sample quality and objective alignment over the vanilla GRPO baseline, enabling robust and effective optimization for VAR models.', 'score': 28, 'issue_id': 426, 'pub_date': '2026-01-05', 'pub_date_card': {'ru': '5 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 5', 'zh': '1æœˆ5æ—¥'}, 'hash': 'b51d8fc1590931c7', 'authors': ['Shikun Sun', 'Liao Qu', 'Huichao Zhang', 'Yiheng Liu', 'Yangyang Song', 'Xian Li', 'Xu Wang', 'Yi Jiang', 'Daniel K. Du', 'Xinglong Wu', 'Jia Jia'], 'affiliations': ['ByteDance', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2601.02256.jpg', 'data': {'categories': ['#optimization', '#diffusion', '#rl', '#cv', '#training'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ¡Ñ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ½ĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ²Ñ‹Ğ·Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ°ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ñ‹Ğ¼Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ°Ğ¼Ğ¸ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½ÑƒÑ Ğ²ĞµÑ€ÑĞ¸Ñ Group Relative Policy Optimization, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚Ñ€Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°: Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿ĞµÑ€ĞµĞ²Ğ·Ğ²ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ² Ğ´Ğ»Ñ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²ĞºĞ»Ğ°Ğ´Ğ° Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¼Ğ°ÑĞ¾Ğº Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ·Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¾Ğ² Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ½Ğ° Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ°Ñ… Reward Feedback Learning Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑ‚ÑŒ ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ğ¿Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ² Ğ¸ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ VAR Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² ÑƒÑĞ¸Ğ»Ğ¸Ğ²Ğ°ÑÑ‰ĞµĞ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Stabilizing Visual Generation with Enhanced Policy Optimization', 'desc': 'This paper addresses the training instability in Visual AutoRegressive (VAR) models caused by asynchronous policy conflicts. It introduces a new framework that enhances Group Relative Policy Optimization (GRPO) by incorporating intermediate rewards, dynamic time-step reweighting, and mask propagation algorithms. These components work together to stabilize training and improve the alignment of generated outputs. The proposed method shows significant improvements in sample quality compared to traditional GRPO methods, making VAR models more robust and effective in visual generation tasks.'}, 'zh': {'title': 'è§£å†³è§†è§‰è‡ªå›å½’æ¨¡å‹è®­ç»ƒä¸ç¨³å®šæ€§çš„æ–°æ¡†æ¶', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è§†è§‰è‡ªå›å½’æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ç”±äºå¼‚æ­¥ç­–ç•¥å†²çªè€Œå¯¼è‡´çš„ä¸ç¨³å®šæ€§ã€‚è¯¥æ¡†æ¶é€šè¿‡å¢å¼ºç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œå¼•å…¥äº†ä¸­é—´å¥–åŠ±ã€åŠ¨æ€æ—¶é—´æ­¥é‡åŠ æƒå’Œæ©ç ä¼ æ’­ç®—æ³•ç­‰ä¸‰ä¸ªååŒç»„ä»¶ã€‚ä¸­é—´å¥–åŠ±ç”¨äºæŒ‡å¯¼æ—©æœŸç”Ÿæˆï¼ŒåŠ¨æ€æ—¶é—´æ­¥é‡åŠ æƒåˆ™å¸®åŠ©ç²¾ç¡®åˆ†é…ä¿¡ç”¨ï¼Œè€Œæ©ç ä¼ æ’­ç®—æ³•åˆ™ä»å¥–åŠ±åé¦ˆå­¦ä¹ çš„åŸåˆ™å‡ºå‘ï¼Œæ—¨åœ¨ç©ºé—´å’Œæ—¶é—´ä¸Šéš”ç¦»ä¼˜åŒ–æ•ˆæœã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ ·æœ¬è´¨é‡å’Œç›®æ ‡å¯¹é½æ–¹é¢æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„GRPOåŸºçº¿ï¼Œèƒ½å¤Ÿæœ‰æ•ˆä¼˜åŒ–è§†è§‰è‡ªå›å½’æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.24138', 'title': 'GARDO: Reinforcing Diffusion Models without Reward Hacking', 'url': 'https://huggingface.co/papers/2512.24138', 'abstract': 'Online reinforcement learning for diffusion model fine-tuning suffers from reward hacking due to proxy reward mismatches, which GARDO addresses through selective regularization, adaptive reference updates, and diversity-aware reward amplification.  \t\t\t\t\tAI-generated summary \t\t\t\t Fine-tuning diffusion models via online reinforcement learning (RL) has shown great potential for enhancing text-to-image alignment. However, since precisely specifying a ground-truth objective for visual tasks remains challenging, the models are often optimized using a proxy reward that only partially captures the true goal. This mismatch often leads to reward hacking, where proxy scores increase while real image quality deteriorates and generation diversity collapses. While common solutions add regularization against the reference policy to prevent reward hacking, they compromise sample efficiency and impede the exploration of novel, high-reward regions, as the reference policy is usually sub-optimal. To address the competing demands of sample efficiency, effective exploration, and mitigation of reward hacking, we propose Gated and Adaptive Regularization with Diversity-aware Optimization (GARDO), a versatile framework compatible with various RL algorithms. Our key insight is that regularization need not be applied universally; instead, it is highly effective to selectively penalize a subset of samples that exhibit high uncertainty. To address the exploration challenge, GARDO introduces an adaptive regularization mechanism wherein the reference model is periodically updated to match the capabilities of the online policy, ensuring a relevant regularization target. To address the mode collapse issue in RL, GARDO amplifies the rewards for high-quality samples that also exhibit high diversity, encouraging mode coverage without destabilizing the optimization process. Extensive experiments across diverse proxy rewards and hold-out unseen metrics consistently show that GARDO mitigates reward hacking and enhances generation diversity without sacrificing sample efficiency or exploration, highlighting its effectiveness and robustness.', 'score': 23, 'issue_id': 428, 'pub_date': '2026-12-30', 'pub_date_card': {'ru': '30 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 30', 'zh': '12æœˆ30æ—¥'}, 'hash': '824aa5f73ac6084b', 'authors': ['Haoran He', 'Yuxiao Ye', 'Jie Liu', 'Jiajun Liang', 'Zhiyong Wang', 'Ziyang Yuan', 'Xintao Wang', 'Hangyu Mao', 'Pengfei Wan', 'Ling Pan'], 'affiliations': ['CUHK MMLab', 'Hong Kong University of Science and Technology', 'Kuaishou Technology', 'The University of Edinburgh'], 'pdf_title_img': 'assets/pdf/title_img/2512.24138.jpg', 'data': {'categories': ['#alignment', '#optimization', '#diffusion', '#rlhf', '#training', '#rl'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Ğ˜Ğ·Ğ±Ğ¸Ñ€Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ² Ğ²Ğ·Ğ»Ğ¾Ğ¼Ğ° Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ reward hacking Ğ¿Ñ€Ğ¸ fine-tuning Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, ĞºĞ¾Ğ³Ğ´Ğ° Ğ¿Ñ€Ğ¾ĞºÑĞ¸-Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ° Ğ½Ğµ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ¸ÑÑ‚Ğ¸Ğ½Ğ½Ğ¾Ğ¹ Ñ†ĞµĞ»Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ GARDO â€” ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞµĞ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ² Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚ÑŒÑ, Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑˆÑ‚Ñ€Ğ°Ñ„Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ², Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ ĞºĞ¾Ğ»Ğ»Ğ°Ğ¿Ñ Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ GARDO ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ reward hacking, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ sample efficiency Ğ±ĞµĞ· ÑƒÑ‰ĞµÑ€Ğ±Ğ° Ğ´Ğ»Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Enhancing Diffusion Models with GARDO: Tackling Reward Hacking and Boosting Diversity', 'desc': 'This paper presents GARDO, a framework designed to improve online reinforcement learning for fine-tuning diffusion models. It addresses the problem of reward hacking, which occurs when models optimize for proxy rewards that do not accurately reflect the desired outcomes. GARDO employs selective regularization to penalize only uncertain samples, adaptive reference updates to keep the model aligned with the current policy, and diversity-aware reward amplification to encourage varied outputs. The results demonstrate that GARDO effectively enhances image quality and diversity while maintaining efficient exploration in the learning process.'}, 'zh': {'title': 'è§£å†³å¥–åŠ±é»‘å®¢ï¼Œæå‡ç”Ÿæˆå¤šæ ·æ€§ï¼', 'desc': 'åœ¨çº¿å¼ºåŒ–å­¦ä¹ åœ¨æ‰©æ•£æ¨¡å‹å¾®è°ƒä¸­é¢ä¸´å¥–åŠ±é»‘å®¢é—®é¢˜ï¼Œä¸»è¦æ˜¯ç”±äºä»£ç†å¥–åŠ±ä¸çœŸå®ç›®æ ‡ä¸åŒ¹é…ã€‚GARDOé€šè¿‡é€‰æ‹©æ€§æ­£åˆ™åŒ–ã€è‡ªé€‚åº”å‚è€ƒæ›´æ–°å’Œå¤šæ ·æ€§å¥–åŠ±æ”¾å¤§æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚è¯¥æ–¹æ³•æœ‰æ•ˆåœ°å‡å°‘äº†å¥–åŠ±é»‘å®¢ç°è±¡ï¼ŒåŒæ—¶æé«˜äº†ç”Ÿæˆçš„å¤šæ ·æ€§ï¼Œè€Œä¸ç‰ºç‰²æ ·æœ¬æ•ˆç‡ã€‚GARDOçš„å…³é”®åœ¨äºé€‰æ‹©æ€§åœ°å¯¹é«˜ä¸ç¡®å®šæ€§çš„æ ·æœ¬è¿›è¡Œæƒ©ç½šï¼Œå¹¶å®šæœŸæ›´æ–°å‚è€ƒæ¨¡å‹ï¼Œä»¥ç¡®ä¿æ­£åˆ™åŒ–ç›®æ ‡çš„ç›¸å…³æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.02358', 'title': 'VINO: A Unified Visual Generator with Interleaved OmniModal Context', 'url': 'https://huggingface.co/papers/2601.02358', 'abstract': 'VINO is a unified visual generator that uses a shared diffusion backbone with multimodal inputs to perform image and video generation and editing tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t We present VINO, a unified visual generator that performs image and video generation and editing within a single framework. Instead of relying on task-specific models or independent modules for each modality, VINO uses a shared diffusion backbone that conditions on text, images and videos, enabling a broad range of visual creation and editing tasks under one model. Specifically, VINO couples a vision-language model (VLM) with a Multimodal Diffusion Transformer (MMDiT), where multimodal inputs are encoded as interleaved conditioning tokens, and then used to guide the diffusion process. This design supports multi-reference grounding, long-form instruction following, and coherent identity preservation across static and dynamic content, while avoiding modality-specific architectural components. To train such a unified system, we introduce a multi-stage training pipeline that progressively expands a video generation base model into a unified, multi-task generator capable of both image and video input and output. Across diverse generation and editing benchmarks, VINO demonstrates strong visual quality, faithful instruction following, improved reference and attribute preservation, and more controllable multi-identity edits. Our results highlight a practical path toward scalable unified visual generation, and the promise of interleaved, in-context computation as a foundation for general-purpose visual creation.', 'score': 21, 'issue_id': 426, 'pub_date': '2026-01-05', 'pub_date_card': {'ru': '5 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 5', 'zh': '1æœˆ5æ—¥'}, 'hash': '921cf734749276d1', 'authors': ['Junyi Chen', 'Tong He', 'Zhoujie Fu', 'Pengfei Wan', 'Kun Gai', 'Weicai Ye'], 'affiliations': ['Kling Team, Kuaishou Technology', 'Nanyang Technology University', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2601.02358.jpg', 'data': {'categories': ['#multimodal', '#video', '#open_source', '#diffusion', '#architecture', '#training'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ²ÑĞµÑ… Ğ²Ğ¸Ğ´Ğ¾Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ‚Ğ²Ğ¾Ñ€Ñ‡ĞµÑÑ‚Ğ²Ğ°', 'desc': 'VINO â€” ÑÑ‚Ğ¾ ĞµĞ´Ğ¸Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ¾Ğ±Ñ‰ĞµĞ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ vision-language model Ñ Multimodal Diffusion Transformer, Ğ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ñ Ğ½Ğ° Ğ²Ñ…Ğ¾Ğ´ Ñ‚ĞµĞºÑÑ‚, Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ğ²Ğ¸Ğ´Ğµ Ñ‡ĞµÑ€ĞµĞ´ÑƒÑÑ‰Ğ¸Ñ…ÑÑ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğ¹. Ğ¡Ğ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ² ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€, Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ·Ğ°Ğ´Ğ°Ñ‡. VINO Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼, Ğ»ÑƒÑ‡ÑˆĞµĞµ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ¾Ğ² Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑ‚ÑŒ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾.'}, 'en': {'title': 'VINO: One Model for All Visual Creations', 'desc': "VINO is a cutting-edge visual generator that integrates image and video generation and editing into a single framework using a shared diffusion backbone. It leverages a vision-language model combined with a Multimodal Diffusion Transformer to process various inputs like text, images, and videos, allowing for versatile visual tasks. The model employs interleaved conditioning tokens to guide the diffusion process, ensuring coherent outputs and effective instruction adherence. VINO's innovative multi-stage training pipeline enhances its capabilities, demonstrating superior visual quality and control in generating and editing both static and dynamic content."}, 'zh': {'title': 'ç»Ÿä¸€è§†è§‰ç”Ÿæˆçš„æœªæ¥', 'desc': 'VINOæ˜¯ä¸€ç§ç»Ÿä¸€çš„è§†è§‰ç”Ÿæˆå™¨ï¼Œèƒ½å¤Ÿåœ¨å•ä¸€æ¡†æ¶å†…æ‰§è¡Œå›¾åƒå’Œè§†é¢‘çš„ç”Ÿæˆä¸ç¼–è¾‘ä»»åŠ¡ã€‚å®ƒä½¿ç”¨å…±äº«çš„æ‰©æ•£éª¨å¹²ç½‘ç»œï¼Œç»“åˆæ–‡æœ¬ã€å›¾åƒå’Œè§†é¢‘çš„å¤šæ¨¡æ€è¾“å…¥ï¼Œé¿å…äº†ä¾èµ–ç‰¹å®šä»»åŠ¡æ¨¡å‹çš„å¤æ‚æ€§ã€‚VINOå°†è§†è§‰è¯­è¨€æ¨¡å‹ä¸å¤šæ¨¡æ€æ‰©æ•£å˜æ¢å™¨ç»“åˆï¼Œé€šè¿‡äº¤é”™çš„æ¡ä»¶æ ‡è®°å¼•å¯¼æ‰©æ•£è¿‡ç¨‹ï¼Œä»è€Œå®ç°å¤šå‚è€ƒåŸºç¡€ã€é•¿å½¢å¼æŒ‡ä»¤è·Ÿéšå’Œä¸€è‡´çš„èº«ä»½ä¿ç•™ã€‚è¯¥ç³»ç»Ÿçš„å¤šé˜¶æ®µè®­ç»ƒæµç¨‹ä½¿å…¶èƒ½å¤Ÿé€æ­¥æ‰©å±•ä¸ºä¸€ä¸ªå¤šä»»åŠ¡ç”Ÿæˆå™¨ï¼Œå±•ç¤ºäº†å¼ºå¤§çš„è§†è§‰è´¨é‡å’Œå¯æ§çš„å¤šèº«ä»½ç¼–è¾‘èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.02281', 'title': 'InfiniteVGGT: Visual Geometry Grounded Transformer for Endless Streams', 'url': 'https://huggingface.co/papers/2601.02281', 'abstract': "InfiniteVGGT enables continuous 3D visual geometry understanding through a causal transformer with adaptive memory management, outperforming existing streaming methods in long-term stability while introducing a new benchmark for extended evaluation.  \t\t\t\t\tAI-generated summary \t\t\t\t The grand vision of enabling persistent, large-scale 3D visual geometry understanding is shackled by the irreconcilable demands of scalability and long-term stability. While offline models like VGGT achieve inspiring geometry capability, their batch-based nature renders them irrelevant for live systems. Streaming architectures, though the intended solution for live operation, have proven inadequate. Existing methods either fail to support truly infinite-horizon inputs or suffer from catastrophic drift over long sequences. We shatter this long-standing dilemma with InfiniteVGGT, a causal visual geometry transformer that operationalizes the concept of a rolling memory through a bounded yet adaptive and perpetually expressive KV cache. Capitalizing on this, we devise a training-free, attention-agnostic pruning strategy that intelligently discards obsolete information, effectively ``rolling'' the memory forward with each new frame. Fully compatible with FlashAttention, InfiniteVGGT finally alleviates the compromise, enabling infinite-horizon streaming while outperforming existing streaming methods in long-term stability. The ultimate test for such a system is its performance over a truly infinite horizon, a capability that has been impossible to rigorously validate due to the lack of extremely long-term, continuous benchmarks. To address this critical gap, we introduce the Long3D benchmark, which, for the first time, enables a rigorous evaluation of continuous 3D geometry estimation on sequences about 10,000 frames. This provides the definitive evaluation platform for future research in long-term 3D geometry understanding. Code is available at: https://github.com/AutoLab-SAI-SJTU/InfiniteVGGT", 'score': 20, 'issue_id': 432, 'pub_date': '2026-01-05', 'pub_date_card': {'ru': '5 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 5', 'zh': '1æœˆ5æ—¥'}, 'hash': '6ad1993c1280bb64', 'authors': ['Shuai Yuan', 'Yantai Yang', 'Xiaotian Yang', 'Xupeng Zhang', 'Zhonghao Zhao', 'Lingming Zhang', 'Zhipeng Zhang'], 'affiliations': ['AutoLab, School of Artificial Intelligence, Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2601.02281.jpg', 'data': {'categories': ['#3d', '#cv', '#benchmark', '#architecture'], 'emoji': 'â™¾ï¸', 'ru': {'title': 'Ğ‘ĞµÑĞºĞ¾Ğ½ĞµÑ‡Ğ½Ğ°Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ: Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ Ğ´Ğ»Ñ Ğ²ĞµÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ 3D Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸', 'desc': 'InfiniteVGGT Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ñ‹Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ Ğ´Ğ»Ñ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ 3D Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ ÑÑ†ĞµĞ½Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ KV-ĞºĞµÑˆĞ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ±ĞµÑĞ¿Ñ€Ğ¸ÑÑ‚Ñ€Ğ°ÑÑ‚Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±Ñ€ĞµĞ·ĞºĞ¸ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ´Ğ°Ğ»ÑĞµÑ‚ ÑƒÑÑ‚Ğ°Ñ€ĞµĞ²ÑˆÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ±ĞµÑĞºĞ¾Ğ½ĞµÑ‡Ğ½Ñ‹Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ±ĞµĞ· ĞºĞ°Ñ‚Ğ°ÑÑ‚Ñ€Ğ¾Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ´Ñ€ĞµĞ¹Ñ„Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Long3D Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° ÑĞºÑÑ‚Ñ€ĞµĞ¼Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑÑ… Ğ¸Ğ· 10,000 ĞºĞ°Ğ´Ñ€Ğ¾Ğ². ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ² Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°.'}, 'en': {'title': 'InfiniteVGGT: Revolutionizing 3D Geometry Understanding with Adaptive Memory', 'desc': 'InfiniteVGGT is a novel approach that enhances 3D visual geometry understanding by utilizing a causal transformer with an adaptive memory system. This method addresses the challenges of scalability and long-term stability that have hindered previous streaming architectures. By implementing a rolling memory mechanism and a pruning strategy, InfiniteVGGT effectively manages information over extended sequences without losing performance. Additionally, it introduces the Long3D benchmark, allowing for rigorous evaluation of continuous 3D geometry estimation across thousands of frames, setting a new standard for future research.'}, 'zh': {'title': 'æ— é™è§†é‡ï¼Œç¨³å®šå‡ ä½•ç†è§£', 'desc': 'InfiniteVGGT æ˜¯ä¸€ç§å› æœè§†è§‰å‡ ä½•å˜æ¢å™¨ï¼Œèƒ½å¤Ÿå®ç°æŒç»­çš„ 3D è§†è§‰å‡ ä½•ç†è§£ã€‚å®ƒé€šè¿‡è‡ªé€‚åº”çš„è®°å¿†ç®¡ç†ï¼Œå…‹æœäº†ç°æœ‰æµå¼æ–¹æ³•åœ¨é•¿æœŸç¨³å®šæ€§æ–¹é¢çš„ä¸è¶³ã€‚ä¸ä¼ ç»Ÿçš„æ‰¹å¤„ç†æ¨¡å‹ä¸åŒï¼ŒInfiniteVGGT é‡‡ç”¨äº†æ»šåŠ¨è®°å¿†çš„æ¦‚å¿µï¼Œèƒ½å¤Ÿæœ‰æ•ˆå¤„ç†æ— é™æ—¶åŸŸçš„è¾“å…¥ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº† Long3D åŸºå‡†ï¼Œé¦–æ¬¡æä¾›äº†å¯¹è¿ç»­ 3D å‡ ä½•ä¼°è®¡çš„ä¸¥æ ¼è¯„ä¼°å¹³å°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.02346', 'title': 'Falcon-H1R: Pushing the Reasoning Frontiers with a Hybrid Model for Efficient Test-Time Scaling', 'url': 'https://huggingface.co/papers/2601.02346', 'abstract': 'Falcon-H1R is a 7B-parameter language model that achieves competitive reasoning performance through efficient training strategies and architectural design, enabling scalable reasoning capabilities in compact models.  \t\t\t\t\tAI-generated summary \t\t\t\t This work introduces Falcon-H1R, a 7B-parameter reasoning-optimized model that establishes the feasibility of achieving competitive reasoning performance with small language models (SLMs). Falcon-H1R stands out for its parameter efficiency, consistently matching or outperforming SOTA reasoning models that are 2times to 7times larger across a variety of reasoning-intensive benchmarks. These results underscore the importance of careful data curation and targeted training strategies (via both efficient SFT and RL scaling) in delivering significant performance gains without increasing model size. Furthermore, Falcon-H1R advances the 3D limits of reasoning efficiency by combining faster inference (through its hybrid-parallel architecture design), token efficiency, and higher accuracy. This unique blend makes Falcon-H1R-7B a practical backbone for scaling advanced reasoning systems, particularly in scenarios requiring extensive chain-of-thoughts generation and parallel test-time scaling. Leveraging the recently introduced DeepConf approach, Falcon-H1R achieves state-of-the-art test-time scaling efficiency, offering substantial improvements in both accuracy and computational cost. As a result, Falcon-H1R demonstrates that compact models, through targeted model training and architectural choices, can deliver robust and scalable reasoning performance.', 'score': 12, 'issue_id': 427, 'pub_date': '2026-01-05', 'pub_date_card': {'ru': '5 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 5', 'zh': '1æœˆ5æ—¥'}, 'hash': 'be0a839ff0b87c9a', 'authors': ['Falcon LLM Team', 'Iheb Chaabane', 'Puneesh Khanna', 'Suhail Mohmad', 'Slim Frikha', 'Shi Hu', 'Abdalgader Abubaker', 'Reda Alami', 'Mikhail Lubinets', 'Mohamed El Amine Seddik', 'Hakim Hacid'], 'affiliations': ['Falcon LLM Team', 'Technology Innovation Institute (TIIUAE)'], 'pdf_title_img': 'assets/pdf/title_img/2601.02346.jpg', 'data': {'categories': ['#rl', '#training', '#optimization', '#architecture', '#benchmark', '#reasoning', '#open_source', '#small_models'], 'emoji': 'âš¡', 'ru': {'title': 'ĞœĞ¾Ñ‰Ğ½Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸', 'desc': 'Falcon-H1R â€” ÑÑ‚Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ 7 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸ÑĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹Ğ´Ğ°ÑÑ‰ÑƒÑÑÑ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ, ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑÑŒ Ğ¸Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° 2-7 Ñ€Ğ°Ğ· Ğ² Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ²ĞºĞ»Ğ°Ğ´ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ (SFT Ğ¸ RL) Ğ¸ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ¾-Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ¾Ğ¹ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğµ SLM Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ñ‚ÑŒ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'Compact Power: Reasoning Efficiency with Falcon-H1R', 'desc': 'Falcon-H1R is a compact 7B-parameter language model designed to excel in reasoning tasks while maintaining efficiency. It achieves competitive performance by utilizing advanced training strategies and a hybrid-parallel architecture, allowing it to outperform larger models in various benchmarks. The model emphasizes the significance of data curation and targeted training methods, such as efficient supervised fine-tuning (SFT) and reinforcement learning (RL) scaling. Overall, Falcon-H1R showcases that smaller models can achieve high reasoning capabilities without the need for increased size, making them practical for complex reasoning applications.'}, 'zh': {'title': 'å°å‹æ¨¡å‹ä¹Ÿèƒ½å®ç°å¼ºå¤§æ¨ç†èƒ½åŠ›', 'desc': 'Falcon-H1Ræ˜¯ä¸€ä¸ªæ‹¥æœ‰70äº¿å‚æ•°çš„è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡é«˜æ•ˆçš„è®­ç»ƒç­–ç•¥å’Œæ¶æ„è®¾è®¡ï¼Œå®ç°äº†ç«äº‰åŠ›çš„æ¨ç†æ€§èƒ½ã€‚è¯¥æ¨¡å‹åœ¨å¤šä¸ªæ¨ç†å¯†é›†å‹åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¡¨ç°å‡ºä¸2åˆ°7å€æ›´å¤§æ¨¡å‹ç›¸å½“æˆ–æ›´ä¼˜çš„æ€§èƒ½ï¼Œå±•ç¤ºäº†å°å‹è¯­è¨€æ¨¡å‹çš„æ½œåŠ›ã€‚ç ”ç©¶å¼ºè°ƒäº†æ•°æ®ç²¾å¿ƒç­–åˆ’å’Œé’ˆå¯¹æ€§è®­ç»ƒç­–ç•¥çš„é‡è¦æ€§ï¼Œè¿™ä½¿å¾—åœ¨ä¸å¢åŠ æ¨¡å‹è§„æ¨¡çš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—æå‡äº†æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒFalcon-H1Ré€šè¿‡æ··åˆå¹¶è¡Œæ¶æ„è®¾è®¡ï¼Œç»“åˆå¿«é€Ÿæ¨ç†å’Œé«˜å‡†ç¡®æ€§ï¼Œæ¨åŠ¨äº†æ¨ç†æ•ˆç‡çš„æé™ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.24601', 'title': 'Recursive Language Models', 'url': 'https://huggingface.co/papers/2512.24601', 'abstract': 'We study allowing large language models (LLMs) to process arbitrarily long prompts through the lens of inference-time scaling. We propose Recursive Language Models (RLMs), a general inference strategy that treats long prompts as part of an external environment and allows the LLM to programmatically examine, decompose, and recursively call itself over snippets of the prompt. We find that RLMs successfully handle inputs up to two orders of magnitude beyond model context windows and, even for shorter prompts, dramatically outperform the quality of base LLMs and common long-context scaffolds across four diverse long-context tasks, while having comparable (or cheaper) cost per query.', 'score': 12, 'issue_id': 430, 'pub_date': '2026-12-31', 'pub_date_card': {'ru': '31 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 31', 'zh': '12æœˆ31æ—¥'}, 'hash': '55f7266c613668ec', 'authors': ['Alex L. Zhang', 'Tim Kraska', 'Omar Khattab'], 'affiliations': ['MIT CSAIL'], 'pdf_title_img': 'assets/pdf/title_img/2512.24601.jpg', 'data': {'categories': [], 'emoji': 'ğŸ”„', 'ru': {'title': 'Ğ ĞµĞºÑƒÑ€ÑĞ¸Ğ²Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ±ĞµÑĞºĞ¾Ğ½ĞµÑ‡Ğ½Ğ¾ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ Recursive Language Models (RLM) Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¾Ñ‡ĞµĞ½ÑŒ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ·Ğ° ÑÑ‡Ñ‘Ñ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ñ€ĞµĞºÑƒÑ€ÑĞ¸Ğ²Ğ½Ğ¾ Ğ²Ñ‹Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ ÑĞ°Ğ¼Ñƒ ÑĞµĞ±Ñ, Ñ€Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ½Ğ° Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ Ğ¸Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾. Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ´Ğ²Ğ° Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞ° Ğ´Ğ»Ğ¸Ğ½Ğ½ĞµĞµ, Ñ‡ĞµĞ¼ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾ĞºĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ¸ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ğ¾Ğ¹ Ğ¸Ğ»Ğ¸ Ğ´Ğ°Ğ¶Ğµ Ğ¼ĞµĞ½ÑŒÑˆĞµĞ¹ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Unlocking Infinite Context with Recursive Language Models', 'desc': 'This paper introduces Recursive Language Models (RLMs), a novel approach for large language models (LLMs) to manage long prompts effectively. RLMs treat lengthy inputs as an external environment, enabling the model to break down the prompt and recursively analyze its components. The study demonstrates that RLMs can process inputs significantly larger than the typical context window of LLMs, achieving superior performance on various long-context tasks. Additionally, RLMs maintain a cost-effective solution, providing high-quality outputs without increasing the expense per query.'}, 'zh': {'title': 'é€’å½’è¯­è¨€æ¨¡å‹ï¼šè¶…è¶Šä¸Šä¸‹æ–‡é™åˆ¶çš„åˆ›æ–°', 'desc': 'æˆ‘ä»¬ç ”ç©¶äº†å¦‚ä½•è®©å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¤„ç†ä»»æ„é•¿åº¦çš„æç¤ºï¼Œé‡‡ç”¨æ¨ç†æ—¶é—´æ‰©å±•çš„è§†è§’ã€‚æˆ‘ä»¬æå‡ºäº†é€’å½’è¯­è¨€æ¨¡å‹ï¼ˆRLMsï¼‰ï¼Œè¿™æ˜¯ä¸€ç§é€šç”¨çš„æ¨ç†ç­–ç•¥ï¼Œå°†é•¿æç¤ºè§†ä¸ºå¤–éƒ¨ç¯å¢ƒçš„ä¸€éƒ¨åˆ†ï¼Œå…è®¸LLMä»¥ç¼–ç¨‹æ–¹å¼æ£€æŸ¥ã€åˆ†è§£å¹¶é€’å½’è°ƒç”¨è‡ªèº«å¤„ç†æç¤ºç‰‡æ®µã€‚ç ”ç©¶å‘ç°ï¼ŒRLMsèƒ½å¤ŸæˆåŠŸå¤„ç†è¾“å…¥é•¿åº¦è¶…è¿‡æ¨¡å‹ä¸Šä¸‹æ–‡çª—å£ä¸¤ä¸ªæ•°é‡çº§çš„æƒ…å†µï¼Œå¹¶ä¸”åœ¨å¤„ç†è¾ƒçŸ­æç¤ºæ—¶ï¼Œå…¶è´¨é‡æ˜¾è‘—ä¼˜äºåŸºç¡€LLMså’Œå¸¸è§çš„é•¿ä¸Šä¸‹æ–‡æ¡†æ¶ï¼Œä¸”æ¯æ¬¡æŸ¥è¯¢çš„æˆæœ¬ç›¸å½“æˆ–æ›´ä½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.02356', 'title': 'Talk2Move: Reinforcement Learning for Text-Instructed Object-Level Geometric Transformation in Scenes', 'url': 'https://huggingface.co/papers/2601.02356', 'abstract': 'Talk2Move presents a reinforcement learning-based diffusion framework that enables precise, semantically faithful spatial transformations of objects in scenes using natural language instructions.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Talk2Move, a reinforcement learning (RL) based diffusion framework for text-instructed spatial transformation of objects within scenes. Spatially manipulating objects in a scene through natural language poses a challenge for multimodal generation systems. While existing text-based manipulation methods can adjust appearance or style, they struggle to perform object-level geometric transformations-such as translating, rotating, or resizing objects-due to scarce paired supervision and pixel-level optimization limits. Talk2Move employs Group Relative Policy Optimization (GRPO) to explore geometric actions through diverse rollouts generated from input images and lightweight textual variations, removing the need for costly paired data. A spatial reward guided model aligns geometric transformations with linguistic description, while off-policy step evaluation and active step sampling improve learning efficiency by focusing on informative transformation stages. Furthermore, we design object-centric spatial rewards that evaluate displacement, rotation, and scaling behaviors directly, enabling interpretable and coherent transformations. Experiments on curated benchmarks demonstrate that Talk2Move achieves precise, consistent, and semantically faithful object transformations, outperforming existing text-guided editing approaches in both spatial accuracy and scene coherence.', 'score': 11, 'issue_id': 426, 'pub_date': '2026-01-05', 'pub_date_card': {'ru': '5 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 5', 'zh': '1æœˆ5æ—¥'}, 'hash': 'a0b15d4316aee180', 'authors': ['Jing Tan', 'Zhaoyang Zhang', 'Yantao Shen', 'Jiarui Cai', 'Shuo Yang', 'Jiajun Wu', 'Wei Xia', 'Zhuowen Tu', 'Stefano Soatto'], 'affiliations': ['AWS Agentic AI', 'Amazon Robotics', 'Amazon Web Services', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2601.02356.jpg', 'data': {'categories': ['#rl', '#training', '#multimodal'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ“ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ ÑĞ·Ñ‹Ğº', 'desc': 'Talk2Move â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹ Ğ² ÑÑ†ĞµĞ½Ğµ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼ Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Group Relative Policy Optimization Ğ´Ğ»Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‚ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ğµ, Ñ€Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²ĞµÑ€Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¹ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ ÑÑ†ĞµĞ½Ñ‹.'}, 'en': {'title': "Transforming Scenes with Words: Talk2Move's Reinforcement Learning Revolution", 'desc': 'Talk2Move is a novel framework that uses reinforcement learning to enable spatial transformations of objects in scenes based on natural language instructions. It addresses the challenge of performing geometric transformations like translation, rotation, and resizing, which are difficult for existing methods due to limited data and optimization constraints. By utilizing Group Relative Policy Optimization (GRPO), Talk2Move generates diverse rollouts from images and text variations, eliminating the need for expensive paired datasets. The framework incorporates spatial rewards that directly assess geometric changes, resulting in accurate and coherent transformations that align well with the provided linguistic descriptions.'}, 'zh': {'title': 'Talk2Moveï¼šè‡ªç„¶è¯­è¨€é©±åŠ¨çš„ç‰©ä½“ç©ºé—´å˜æ¢', 'desc': 'Talk2Moveæ˜¯ä¸€ä¸ªåŸºäºå¼ºåŒ–å­¦ä¹ çš„æ‰©æ•£æ¡†æ¶ï¼Œèƒ½å¤Ÿæ ¹æ®è‡ªç„¶è¯­è¨€æŒ‡ä»¤ç²¾ç¡®åœ°å¯¹åœºæ™¯ä¸­çš„ç‰©ä½“è¿›è¡Œç©ºé—´å˜æ¢ã€‚è¯¥æ–¹æ³•è§£å†³äº†å¤šæ¨¡æ€ç”Ÿæˆç³»ç»Ÿåœ¨è‡ªç„¶è¯­è¨€æŒ‡å¯¼ä¸‹è¿›è¡Œç‰©ä½“å‡ ä½•å˜æ¢çš„æŒ‘æˆ˜ï¼Œå…‹æœäº†ç°æœ‰æ–¹æ³•åœ¨é…å¯¹ç›‘ç£å’Œåƒç´ çº§ä¼˜åŒ–æ–¹é¢çš„å±€é™ã€‚é€šè¿‡ä½¿ç”¨ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼ŒTalk2Moveèƒ½å¤Ÿåœ¨ä¸éœ€è¦æ˜‚è´µé…å¯¹æ•°æ®çš„æƒ…å†µä¸‹ï¼Œæ¢ç´¢å‡ ä½•åŠ¨ä½œå¹¶ç”Ÿæˆå¤šæ ·çš„è¾“å‡ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTalk2Moveåœ¨ç©ºé—´å‡†ç¡®æ€§å’Œåœºæ™¯ä¸€è‡´æ€§æ–¹é¢è¶…è¶Šäº†ç°æœ‰çš„æ–‡æœ¬å¼•å¯¼ç¼–è¾‘æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.02179', 'title': 'Confidence Estimation for LLMs in Multi-turn Interactions', 'url': 'https://huggingface.co/papers/2601.02179', 'abstract': 'Multi-turn conversation confidence estimation lacks systematic evaluation frameworks, prompting the introduction of novel metrics and a "Hinter-Guesser" paradigm for controlled dataset generation to improve calibration and monotonicity.  \t\t\t\t\tAI-generated summary \t\t\t\t While confidence estimation is a promising direction for mitigating hallucinations in Large Language Models (LLMs), current research dominantly focuses on single-turn settings. The dynamics of model confidence in multi-turn conversations, where context accumulates and ambiguity is progressively resolved, remain largely unexplored. Reliable confidence estimation in multi-turn settings is critical for many downstream applications, such as autonomous agents and human-in-the-loop systems. This work presents the first systematic study of confidence estimation in multi-turn interactions, establishing a formal evaluation framework grounded in two key desiderata: per-turn calibration and monotonicity of confidence as more information becomes available. To facilitate this, we introduce novel metrics, including a length-normalized Expected Calibration Error (InfoECE), and a new "Hinter-Guesser" paradigm for generating controlled evaluation datasets. Our experiments reveal that widely-used confidence techniques struggle with calibration and monotonicity in multi-turn dialogues. We propose P(Sufficient), a logit-based probe that achieves comparatively better performance, although the task remains far from solved. Our work provides a foundational methodology for developing more reliable and trustworthy conversational agents.', 'score': 6, 'issue_id': 438, 'pub_date': '2026-01-05', 'pub_date_card': {'ru': '5 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 5', 'zh': '1æœˆ5æ—¥'}, 'hash': 'e1689fc7ddf3b100', 'authors': ['Caiqi Zhang', 'Ruihan Yang', 'Xiaochen Zhu', 'Chengzu Li', 'Tiancheng Hu', 'Yijiang River Dong', 'Deqing Yang', 'Nigel Collier'], 'affiliations': ['Fudan University', 'University of Cambridge'], 'pdf_title_img': 'assets/pdf/title_img/2601.02179.jpg', 'data': {'categories': ['#benchmark', '#agents', '#hallucinations', '#alignment', '#training'], 'emoji': 'ğŸ¯', 'ru': {'title': 'ĞšĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞ° ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ LLM Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¾Ğ±Ğ¾Ñ€Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ°Ñ…', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¾ Ğ¿ĞµÑ€Ğ²Ğ¾Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ (confidence estimation) ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¾Ğ±Ğ¾Ñ€Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°Ñ…, Ğ³Ğ´Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ½Ğ°ĞºĞ°Ğ¿Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ¸ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ÑÑ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑÑ…ĞµĞ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ½Ğ° Ğ´Ğ²ÑƒÑ… ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸ÑÑ…: ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞµ Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ Ñ…Ğ¾Ğ´Ñƒ Ğ¸ Ğ¼Ğ¾Ğ½Ğ¾Ñ‚Ğ¾Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¿Ğ¾ÑÑ‚ÑƒĞ¿Ğ»ĞµĞ½Ğ¸Ğ¸ Ğ½Ğ¾Ğ²Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ”Ğ»Ñ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ²Ğ²ĞµĞ´ĞµĞ½Ñ‹ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¿Ğ¾ Ğ´Ğ»Ğ¸Ğ½Ğµ Expected Calibration Error (InfoECE), Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° "Hinter-Guesser" Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¾Ñ†ĞµĞ½Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ñ‹Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ»Ğ¾Ñ…Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ Ñ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞ¾Ğ¹ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¾Ğ±Ğ¾Ñ€Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°Ñ…, Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ P(Sufficient) Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ»ÑƒÑ‡ÑˆÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ.'}, 'en': {'title': 'Enhancing Confidence in Multi-Turn Conversations', 'desc': "This paper addresses the challenge of estimating confidence in multi-turn conversations, which is crucial for improving the reliability of conversational agents. It introduces a systematic evaluation framework that focuses on two main aspects: per-turn calibration and the monotonicity of confidence as more context is provided. The authors propose new metrics, such as the length-normalized Expected Calibration Error (InfoECE), and a novel dataset generation method called the 'Hinter-Guesser' paradigm. Their findings indicate that existing confidence estimation techniques are inadequate in multi-turn settings, and they suggest a new approach, P(Sufficient), which shows improved performance but highlights that further work is needed in this area."}, 'zh': {'title': 'æå‡å¤šè½®å¯¹è¯ç½®ä¿¡åº¦çš„ç³»ç»ŸåŒ–ç ”ç©¶', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†å¤šè½®å¯¹è¯ä¸­çš„ç½®ä¿¡åº¦ä¼°è®¡é—®é¢˜ï¼Œæå‡ºäº†ç³»ç»ŸåŒ–çš„è¯„ä¼°æ¡†æ¶ã€‚ç ”ç©¶å‘ç°ï¼Œç°æœ‰çš„ç½®ä¿¡åº¦ä¼°è®¡æ–¹æ³•åœ¨å¤šè½®å¯¹è¯ä¸­é¢ä¸´æ ¡å‡†å’Œå•è°ƒæ€§çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œä½œè€…å¼•å…¥äº†æ–°çš„è¯„ä¼°æŒ‡æ ‡å’Œ"Hinter-Guesser"èŒƒå¼ï¼Œä»¥ç”Ÿæˆå—æ§çš„æ•°æ®é›†ã€‚è¯¥ç ”ç©¶ä¸ºå¼€å‘æ›´å¯é çš„å¯¹è¯ä»£ç†æä¾›äº†åŸºç¡€æ–¹æ³•è®ºã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.01046', 'title': 'KV-Embedding: Training-free Text Embedding via Internal KV Re-routing in Decoder-only LLMs', 'url': 'https://huggingface.co/papers/2601.01046', 'abstract': 'KV-Embedding enables training-free representation learning from frozen LLMs by utilizing key-value states for enhanced context access and automated layer selection.  \t\t\t\t\tAI-generated summary \t\t\t\t While LLMs are powerful embedding backbones, their application in training-free settings faces two structural challenges: causal attention restricts early tokens from accessing subsequent context, and the next-token prediction objective biases representations toward generation rather than semantic compression. To address these limitations, we propose KV-Embedding, a framework that activates the latent representation power of frozen LLMs. Our method leverages the observation that the key-value (KV) states of the final token at each layer encode a compressed view of the sequence. By re-routing these states as a prepended prefix, we enable all tokens to access sequence-level context within a single forward pass. To ensure model-agnostic applicability, we introduce an automated layer selection strategy based on intrinsic dimensionality. Evaluations on MTEB across Qwen, Mistral, and Llama backbones show that KV-Embedding outperforms existing training-free baselines by up to 10%, while maintaining robust performance on sequences up to 4,096 tokens. These results demonstrate that internal state manipulation offers an efficient alternative to input modification, and we hope this work encourages further exploration of LLM internals for representation learning.', 'score': 5, 'issue_id': 426, 'pub_date': '2026-01-03', 'pub_date_card': {'ru': '3 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 3', 'zh': '1æœˆ3æ—¥'}, 'hash': '0d6e03d3443f8363', 'authors': ['Yixuan Tang', 'Yi Yang'], 'affiliations': ['The Hong Kong University of Science and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2601.01046.jpg', 'data': {'categories': ['#multimodal', '#training'], 'emoji': 'ğŸ”‘', 'ru': {'title': 'Ğ Ğ°ÑĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¹ Ğ¼Ğ¾Ñ‰Ğ¸ Ğ·Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ñ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğ¼Ğ¸ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸ÑĞ¼Ğ¸', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ KV-Embedding Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ-ÑĞ²Ğ¾Ğ±Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ· Ğ·Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€ĞµÑˆĞ°ÑÑ‚ Ğ´Ğ²Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹: Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ñ€Ğ°Ğ½Ğ½Ğ¸Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ñ… Ğ¸ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ² ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ»ÑÑ‡-Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½ĞµĞ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ÑĞ»Ğ¾Ñ ĞºĞ°Ğº ÑĞ¶Ğ°Ñ‚Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ğ¿ĞµÑ€ĞµĞ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ ĞºĞ°Ğº Ğ¿Ñ€Ğ¸ÑÑ‚Ğ°Ğ²ĞºĞ° Ğ´Ğ»Ñ Ğ²ÑĞµÑ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². Ğ”Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ»Ğ¾Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ĞµĞ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ° 10% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Unlocking Context with KV-Embedding in Frozen LLMs', 'desc': 'The paper introduces KV-Embedding, a novel framework that enhances representation learning from frozen large language models (LLMs) without the need for additional training. It addresses two main challenges: the limitations of causal attention that prevent early tokens from accessing later context, and the bias of next-token prediction that favors generation over meaningful representation. By utilizing key-value states from the final token of each layer, KV-Embedding allows all tokens to access comprehensive sequence-level context in a single forward pass. The method shows significant improvements in performance on various benchmarks, demonstrating the potential of manipulating internal states for better representation learning.'}, 'zh': {'title': 'KV-Embeddingï¼šæ— è®­ç»ƒè¡¨ç¤ºå­¦ä¹ çš„æ–°æ–¹æ³•', 'desc': 'KV-Embeddingæ˜¯ä¸€ç§æ–°æ¡†æ¶ï¼Œèƒ½å¤Ÿä»å†»ç»“çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­è¿›è¡Œæ— è®­ç»ƒçš„è¡¨ç¤ºå­¦ä¹ ã€‚å®ƒé€šè¿‡åˆ©ç”¨å…³é”®-å€¼ï¼ˆKVï¼‰çŠ¶æ€æ¥å¢å¼ºä¸Šä¸‹æ–‡è®¿é—®ï¼Œå¹¶è‡ªåŠ¨é€‰æ‹©å±‚çº§ï¼Œä»è€Œè§£å†³äº†å› æœæ³¨æ„åŠ›å’Œä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹ç›®æ ‡å¸¦æ¥çš„ç»“æ„æ€§æŒ‘æˆ˜ã€‚è¯¥æ–¹æ³•å…è®¸æ‰€æœ‰æ ‡è®°åœ¨ä¸€æ¬¡å‰å‘ä¼ æ’­ä¸­è®¿é—®åºåˆ—çº§ä¸Šä¸‹æ–‡ï¼Œæå‡äº†è¡¨ç¤ºçš„è¯­ä¹‰å‹ç¼©èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒKV-Embeddingåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†ç°æœ‰çš„æ— è®­ç»ƒåŸºçº¿ï¼Œå±•ç¤ºäº†å†…éƒ¨çŠ¶æ€æ“ä½œåœ¨è¡¨ç¤ºå­¦ä¹ ä¸­çš„é«˜æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.00501', 'title': 'CPPO: Contrastive Perception for Vision Language Policy Optimization', 'url': 'https://huggingface.co/papers/2601.00501', 'abstract': 'CPPO improves vision-language model fine-tuning by detecting perception tokens through entropy shifts and using contrastive perception loss to enhance multimodal reasoning efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce CPPO, a Contrastive Perception Policy Optimization method for finetuning vision-language models (VLMs). While reinforcement learning (RL) has advanced reasoning in language models, extending it to multimodal reasoning requires improving both the perception and reasoning aspects. Prior works tackle this challenge mainly with explicit perception rewards, but disentangling perception tokens from reasoning tokens is difficult, requiring extra LLMs, ground-truth data, forced separation of perception from reasoning by policy model, or applying rewards indiscriminately to all output tokens. CPPO addresses this problem by detecting perception tokens via entropy shifts in the model outputs under perturbed input images. CPPO then extends the RL objective function with a Contrastive Perception Loss (CPL) that enforces consistency under information-preserving perturbations and sensitivity under information-removing ones. Experiments show that CPPO surpasses previous perception-rewarding methods, while avoiding extra models, making training more efficient and scalable.', 'score': 5, 'issue_id': 441, 'pub_date': '2026-01-01', 'pub_date_card': {'ru': '1 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 1', 'zh': '1æœˆ1æ—¥'}, 'hash': 'bc5a079ffa7b50ec', 'authors': ['Ahmad Rezaei', 'Mohsen Gholami', 'Saeed Ranjbar Alvar', 'Kevin Cannons', 'Mohammad Asiful Hossain', 'Zhou Weimin', 'Shunbo Zhou', 'Yong Zhang', 'Mohammad Akbari'], 'affiliations': ['Huawei Cloud', 'Huawei Technologies Canada Co. Ltd.'], 'pdf_title_img': 'assets/pdf/title_img/2601.00501.jpg', 'data': {'categories': ['#multimodal', '#optimization', '#reasoning', '#rlhf', '#rl', '#training', '#cv'], 'emoji': 'ğŸ¯', 'ru': {'title': 'ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¹Ğ½Ñ‹Ğµ ÑĞ´Ğ²Ğ¸Ğ³Ğ¸ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ CPPO Ğ´Ğ»Ñ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ·Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ñ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· ÑĞ´Ğ²Ğ¸Ğ³Ğ¾Ğ² ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸ Ğ²Ğ¾Ğ·Ğ¼ÑƒÑ‰ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ¸Ğ²Ğ½ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑÑ‰Ğ¸Ñ… Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ¼ÑƒÑ‰ĞµĞ½Ğ¸ÑÑ… Ğ¸ Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ ÑƒĞ´Ğ°Ğ»ÑÑÑ‰Ğ¸Ñ… Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¸Ğ·Ğ±ĞµĞ³Ğ°Ñ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ.'}, 'en': {'title': 'Enhancing Multimodal Reasoning with CPPO', 'desc': 'CPPO, or Contrastive Perception Policy Optimization, enhances the fine-tuning of vision-language models (VLMs) by effectively identifying perception tokens through entropy shifts in model outputs. This method improves multimodal reasoning by incorporating a Contrastive Perception Loss (CPL) into the reinforcement learning framework, which ensures that the model maintains consistency when faced with minor changes in input while being sensitive to significant alterations. Unlike previous approaches that relied on additional models or explicit perception rewards, CPPO simplifies the process by directly detecting perception tokens without needing extra resources. Experimental results demonstrate that CPPO outperforms earlier methods, making the training of VLMs more efficient and scalable.'}, 'zh': {'title': 'CPPOï¼šæå‡è§†è§‰-è¯­è¨€æ¨¡å‹çš„å¤šæ¨¡æ€æ¨ç†æ•ˆç‡', 'desc': 'CPPOæ˜¯ä¸€ç§å¯¹æ¯”æ„ŸçŸ¥ç­–ç•¥ä¼˜åŒ–æ–¹æ³•ï¼Œç”¨äºå¾®è°ƒè§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ã€‚è¯¥æ–¹æ³•é€šè¿‡æ£€æµ‹æ¨¡å‹è¾“å‡ºä¸­çš„æ„ŸçŸ¥æ ‡è®°ï¼Œåˆ©ç”¨ç†µå˜åŒ–æ¥æé«˜å¤šæ¨¡æ€æ¨ç†çš„æ•ˆç‡ã€‚ä¸ä»¥å¾€çš„ç ”ç©¶ä¸åŒï¼ŒCPPOä¸éœ€è¦é¢å¤–çš„æ¨¡å‹æˆ–çœŸå®æ•°æ®ï¼Œè€Œæ˜¯é€šè¿‡å¯¹æ¯”æ„ŸçŸ¥æŸå¤±ï¼ˆCPLï¼‰æ¥å¢å¼ºæ¨¡å‹åœ¨ä¿¡æ¯ä¿ç•™å’Œä¿¡æ¯å»é™¤æ‰°åŠ¨ä¸‹çš„ä¸€è‡´æ€§å’Œæ•æ„Ÿæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCPPOåœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†ä¹‹å‰çš„æ„ŸçŸ¥å¥–åŠ±æ–¹æ³•ï¼ŒåŒæ—¶æé«˜äº†è®­ç»ƒçš„æ•ˆç‡å’Œå¯æ‰©å±•æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.02267', 'title': 'DiffProxy: Multi-View Human Mesh Recovery via Diffusion-Generated Dense Proxies', 'url': 'https://huggingface.co/papers/2601.02267', 'abstract': "DiffProxy enables human mesh recovery from multi-view images by generating multi-view consistent proxies using diffusion-based generative priors, achieving state-of-the-art performance through synthetic training and robust test-time scaling.  \t\t\t\t\tAI-generated summary \t\t\t\t Human mesh recovery from multi-view images faces a fundamental challenge: real-world datasets contain imperfect ground-truth annotations that bias the models' training, while synthetic data with precise supervision suffers from domain gap. In this paper, we propose DiffProxy, a novel framework that generates multi-view consistent human proxies for mesh recovery. Central to DiffProxy is leveraging the diffusion-based generative priors to bridge the synthetic training and real-world generalization. Its key innovations include: (1) a multi-conditional mechanism for generating multi-view consistent, pixel-aligned human proxies; (2) a hand refinement module that incorporates flexible visual prompts to enhance local details; and (3) an uncertainty-aware test-time scaling method that increases robustness to challenging cases during optimization. These designs ensure that the mesh recovery process effectively benefits from the precise synthetic ground truth and generative advantages of the diffusion-based pipeline. Trained entirely on synthetic data, DiffProxy achieves state-of-the-art performance across five real-world benchmarks, demonstrating strong zero-shot generalization particularly on challenging scenarios with occlusions and partial views. Project page: https://wrk226.github.io/DiffProxy.html", 'score': 4, 'issue_id': 439, 'pub_date': '2026-01-05', 'pub_date_card': {'ru': '5 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 5', 'zh': '1æœˆ5æ—¥'}, 'hash': 'f5aff3b9929ea83a', 'authors': ['Renke Wang', 'Zhenyu Zhang', 'Ying Tai', 'Jian Yang'], 'affiliations': ['Nanjing University, School of Intelligent Science and Technology', 'PCA Lab, Nanjing University of Science and Technology, China'], 'pdf_title_img': 'assets/pdf/title_img/2601.02267.jpg', 'data': {'categories': ['#synthetic', '#multimodal', '#cv', '#diffusion', '#training'], 'emoji': 'ğŸ§‘\u200dğŸ¦¾', 'ru': {'title': 'Ğ”Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾ĞºÑĞ¸ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ 3D Ñ„Ğ¾Ñ€Ğ¼Ñ‹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¸Ğ· Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ° Ğ²Ğ¸Ğ´Ğ¾Ğ²', 'desc': 'DiffProxy - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ğ¾Ğ¹ mesh Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¸Ğ· Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ²Ğ¸Ğ´Ğ¾Ğ²Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾ĞºÑĞ¸-Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ñ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ²Ğ¸Ğ´Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒ-Ğ²Ñ‹Ñ€Ğ¾Ğ²Ğ½ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑĞ²ÑĞ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ÑÑ†ĞµĞ½Ğ°Ğ¼Ğ¸. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ Ñ€ÑƒĞº Ñ Ğ³Ğ¸Ğ±ĞºĞ¸Ğ¼Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ°Ğ¼Ğ¸ Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ ÑƒÑ‡Ñ‘Ñ‚Ğ¾Ğ¼ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¿ÑÑ‚Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¸Ğ»ÑŒĞ½ÑƒÑ Ğ½ÑƒĞ»ĞµĞ²ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑĞ»ÑƒÑ‡Ğ°ÑÑ… Ñ Ğ¾ĞºĞºĞ»ÑĞ·Ğ¸ĞµĞ¹ Ğ¸ Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ²Ğ¸Ğ´Ğ¾Ğ¼.'}, 'en': {'title': 'Bridging Synthetic and Real-World Data for Superior Mesh Recovery', 'desc': 'DiffProxy is a framework designed to improve human mesh recovery from multi-view images by creating consistent proxies using diffusion-based generative models. It addresses the issue of biased training from imperfect real-world datasets by utilizing synthetic data with accurate annotations. The framework introduces a multi-conditional mechanism for generating pixel-aligned proxies, a hand refinement module for enhancing details, and an uncertainty-aware scaling method for better performance in difficult scenarios. As a result, DiffProxy achieves state-of-the-art results on various benchmarks, showcasing its ability to generalize well to real-world challenges.'}, 'zh': {'title': 'DiffProxyï¼šä»å¤šè§†è§’å›¾åƒä¸­æ¢å¤äººä½“ç½‘æ ¼çš„åˆ›æ–°æ¡†æ¶', 'desc': 'DiffProxy æ˜¯ä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ç”Ÿæˆå¤šè§†è§’ä¸€è‡´çš„äººä½“ä»£ç†æ¥å®ç°ä»å¤šè§†è§’å›¾åƒä¸­æ¢å¤äººä½“ç½‘æ ¼ã€‚è¯¥æ–¹æ³•åˆ©ç”¨åŸºäºæ‰©æ•£çš„ç”Ÿæˆå…ˆéªŒï¼Œè§£å†³äº†åˆæˆè®­ç»ƒä¸çœŸå®ä¸–ç•Œæ³›åŒ–ä¹‹é—´çš„å·®è·ã€‚DiffProxy çš„åˆ›æ–°åŒ…æ‹¬å¤šæ¡ä»¶æœºåˆ¶ç”Ÿæˆåƒç´ å¯¹é½çš„äººä½“ä»£ç†ã€æ‰‹éƒ¨ç»†åŒ–æ¨¡å—å¢å¼ºå±€éƒ¨ç»†èŠ‚ï¼Œä»¥åŠä¸ç¡®å®šæ€§æ„ŸçŸ¥çš„æµ‹è¯•æ—¶é—´ç¼©æ”¾æ–¹æ³•ï¼Œæé«˜äº†åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­çš„é²æ£’æ€§ã€‚é€šè¿‡å®Œå…¨åœ¨åˆæˆæ•°æ®ä¸Šè®­ç»ƒï¼ŒDiffProxy åœ¨äº”ä¸ªçœŸå®ä¸–ç•ŒåŸºå‡†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨é®æŒ¡å’Œéƒ¨åˆ†è§†å›¾ç­‰å…·æœ‰æŒ‘æˆ˜æ€§çš„åœºæ™¯ä¸­å±•ç°äº†å¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.01836', 'title': 'COMPASS: A Framework for Evaluating Organization-Specific Policy Alignment in LLMs', 'url': 'https://huggingface.co/papers/2601.01836', 'abstract': "COMPASS evaluates large language models' compliance with organizational policies, revealing significant gaps in enforcing prohibitions despite strong performance on legitimate requests.  \t\t\t\t\tAI-generated summary \t\t\t\t As large language models are deployed in high-stakes enterprise applications, from healthcare to finance, ensuring adherence to organization-specific policies has become essential. Yet existing safety evaluations focus exclusively on universal harms. We present COMPASS (Company/Organization Policy Alignment Assessment), the first systematic framework for evaluating whether LLMs comply with organizational allowlist and denylist policies. We apply COMPASS to eight diverse industry scenarios, generating and validating 5,920 queries that test both routine compliance and adversarial robustness through strategically designed edge cases. Evaluating seven state-of-the-art models, we uncover a fundamental asymmetry: models reliably handle legitimate requests (>95% accuracy) but catastrophically fail at enforcing prohibitions, refusing only 13-40% of adversarial denylist violations. These results demonstrate that current LLMs lack the robustness required for policy-critical deployments, establishing COMPASS as an essential evaluation framework for organizational AI safety.", 'score': 4, 'issue_id': 428, 'pub_date': '2026-01-05', 'pub_date_card': {'ru': '5 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 5', 'zh': '1æœˆ5æ—¥'}, 'hash': 'bf1e3c3f8aa12378', 'authors': ['Dasol Choi', 'DongGeon Lee', 'Brigitta Jesica Kartono', 'Helena Berndt', 'Taeyoun Kwon', 'Joonwon Jang', 'Haon Park', 'Hwanjo Yu', 'Minsuk Kahng'], 'affiliations': ['AIM Intelligence', 'BMW Group', 'POSTECH', 'Seoul National University', 'Yonsei University'], 'pdf_title_img': 'assets/pdf/title_img/2601.01836.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#alignment', '#security'], 'emoji': 'ğŸ›¡ï¸', 'ru': {'title': 'Ğ¯Ğ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ñ‹ Ğº Ğ¾Ğ±Ñ…Ğ¾Ğ´Ñƒ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ COMPASS â€” Ğ¿ĞµÑ€Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¸ĞºÑƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ°Ğ¼ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¸Ğ· 5920 Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ´Ğ»Ñ Ğ²Ğ¾ÑÑŒĞ¼Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ñ€Ğ°ÑĞ»ĞµĞ²Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ², Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ adversarial Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ¾Ğ±Ğ»ÑĞ´Ğ°Ñ‚ÑŒ ĞºĞ°Ğº Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ, Ñ‚Ğ°Ğº Ğ¸ Ğ·Ğ°Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ°. ĞĞ½Ğ°Ğ»Ğ¸Ğ· ÑĞµĞ¼Ğ¸ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… LLM Ğ²Ñ‹ÑĞ²Ğ¸Ğ» ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ´Ğ¸ÑĞ±Ğ°Ğ»Ğ°Ğ½Ñ: Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ğ»ĞµĞ³Ğ¸Ñ‚Ğ¸Ğ¼Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹ Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ ÑĞ²Ñ‹ÑˆĞµ 95%, Ğ½Ğ¾ ĞºĞ°Ñ‚Ğ°ÑÑ‚Ñ€Ğ¾Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ğ¿Ñ€Ğ¸ Ğ±Ğ»Ğ¾ĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ·Ğ°Ğ¿Ñ€ĞµÑ‰Ñ‘Ğ½Ğ½Ñ‹Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹, Ğ¾Ñ‚ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ² 13-40% ÑĞ»ÑƒÑ‡Ğ°ĞµĞ² Ğ½Ğ°Ñ€ÑƒÑˆĞµĞ½Ğ¸Ğ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½ÑƒÑ Ñ€Ğ¾Ğ±Ğ°ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²Ñ‘Ñ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ğ¾ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ñ€Ğ¿Ğ¾Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ÑÑ….'}, 'en': {'title': 'Bridging the Compliance Gap in AI with COMPASS', 'desc': 'The paper introduces COMPASS, a framework designed to assess how well large language models (LLMs) follow specific organizational policies. It highlights a critical issue where these models perform well on legitimate requests but struggle significantly with enforcing prohibitions, showing only 13-40% accuracy in denying harmful queries. By testing seven advanced models across various industry scenarios, the study reveals a concerning gap in compliance that could jeopardize safety in high-stakes applications. COMPASS aims to fill this gap by providing a systematic approach to evaluate and improve the adherence of LLMs to organizational guidelines.'}, 'zh': {'title': 'ç¡®ä¿AIéµå¾ªç»„ç»‡æ”¿ç­–çš„COMPASSæ¡†æ¶', 'desc': 'COMPASSæ˜¯ä¸€ä¸ªè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ˜¯å¦éµå¾ªç»„ç»‡æ”¿ç­–çš„ç³»ç»Ÿæ¡†æ¶ã€‚å°½ç®¡è¿™äº›æ¨¡å‹åœ¨å¤„ç†åˆæ³•è¯·æ±‚æ—¶è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨æ‰§è¡Œç¦æ­¢æ€§æ”¿ç­–æ–¹é¢å´å­˜åœ¨æ˜¾è‘—ç¼ºé™·ã€‚ç ”ç©¶å‘ç°ï¼Œå½“å‰çš„LLMåœ¨æ‹’ç»å¯¹æŠ—æ€§è¯·æ±‚æ—¶çš„å‡†ç¡®ç‡ä»…ä¸º13%åˆ°40%ã€‚å› æ­¤ï¼ŒCOMPASSä¸ºç¡®ä¿AIåœ¨å…³é”®é¢†åŸŸçš„å®‰å…¨æ€§æä¾›äº†å¿…è¦çš„è¯„ä¼°å·¥å…·ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.23035', 'title': 'Toward Stable Semi-Supervised Remote Sensing Segmentation via Co-Guidance and Co-Fusion', 'url': 'https://huggingface.co/papers/2512.23035', 'abstract': 'A semi-supervised remote sensing image segmentation framework combines vision-language and self-supervised models to reduce pseudo-label drift through dual-student architecture and semantic co-guidance mechanisms.  \t\t\t\t\tAI-generated summary \t\t\t\t Semi-supervised remote sensing (RS) image semantic segmentation offers a promising solution to alleviate the burden of exhaustive annotation, yet it fundamentally struggles with pseudo-label drift, a phenomenon where confirmation bias leads to the accumulation of errors during training. In this work, we propose Co2S, a stable semi-supervised RS segmentation framework that synergistically fuses priors from vision-language models and self-supervised models. Specifically, we construct a heterogeneous dual-student architecture comprising two distinct ViT-based vision foundation models initialized with pretrained CLIP and DINOv3 to mitigate error accumulation and pseudo-label drift. To effectively incorporate these distinct priors, an explicit-implicit semantic co-guidance mechanism is introduced that utilizes text embeddings and learnable queries to provide explicit and implicit class-level guidance, respectively, thereby jointly enhancing semantic consistency. Furthermore, a global-local feature collaborative fusion strategy is developed to effectively fuse the global contextual information captured by CLIP with the local details produced by DINOv3, enabling the model to generate highly precise segmentation results. Extensive experiments on six popular datasets demonstrate the superiority of the proposed method, which consistently achieves leading performance across various partition protocols and diverse scenarios. Project page is available at https://xavierjiezou.github.io/Co2S/.', 'score': 4, 'issue_id': 429, 'pub_date': '2026-12-28', 'pub_date_card': {'ru': '28 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 28', 'zh': '12æœˆ28æ—¥'}, 'hash': '46b30a51798dbc10', 'authors': ['Yi Zhou', 'Xuechao Zou', 'Shun Zhang', 'Kai Li', 'Shiying Wang', 'Jingming Chen', 'Congyan Lang', 'Tengfei Cao', 'Pin Tao', 'Yuanchun Shi'], 'affiliations': ['Department of Computer Science and Technology, Tsinghua University, Beijing, China', 'Intelligent Computing and Application Laboratory of Qinghai Province, Qinghai University, Xining, China', 'Key Lab of Big Data & Artificial Intelligence in Transportation (Ministry of Education), School of Computer Science & Technology, Beijing Jiaotong University, Beijing, China', 'Key Laboratory of Pervasive Computing, Ministry of Education', 'School of Computer Technology and Application, Qinghai University, Xining, China'], 'pdf_title_img': 'assets/pdf/title_img/2512.23035.jpg', 'data': {'categories': ['#architecture', '#dataset', '#cv', '#multimodal'], 'emoji': 'ğŸ›°ï¸', 'ru': {'title': 'Ğ”Ğ²Ğ¾Ğ¹Ğ½Ğ¾Ğ¹ ÑÑ‚ÑƒĞ´ĞµĞ½Ñ‚ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ² Ğ´Ñ€ĞµĞ¹Ñ„Ğ°: ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ°Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ ÑĞ¿ÑƒÑ‚Ğ½Ğ¸ĞºĞ¾Ğ²Ñ‹Ñ… ÑĞ½Ğ¸Ğ¼ĞºĞ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ½Ğ¸Ğµ-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¸ ÑĞ°Ğ¼Ğ¾ÑÑƒĞ¿ĞµÑ€Ğ²Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Co2S â€” Ğ¿Ğ¾Ğ»ÑƒÑÑƒĞ¿ĞµÑ€Ğ²Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ ÑĞ½Ğ¸Ğ¼ĞºĞ¾Ğ² Ğ´Ğ¸ÑÑ‚Ğ°Ğ½Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ğ¾Ğ½Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ´Ñ€ĞµĞ¹Ñ„Ğ° Ğ¿ÑĞµĞ²Ğ´Ğ¾-Ğ¼ĞµÑ‚Ğ¾Ğº. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³ĞµÑ‚ĞµÑ€Ğ¾Ğ³ĞµĞ½Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ñ Ğ´Ğ²ÑƒĞ¼Ñ ÑÑ‚ÑƒĞ´ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ¸Ğ´ĞµĞ½Ğ¸Ğµ-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ CLIP Ğ¸ ÑĞ°Ğ¼Ğ¾ÑÑƒĞ¿ĞµÑ€Ğ²Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ DINOv3. Ğ”Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¾Ğ² Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞ¾-Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ°, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ¡Ñ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾-Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ¾Ğ² Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½Ğ¸Ñ‚ÑŒ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ñ Ğ´ĞµÑ‚Ğ°Ğ»ÑĞ¼Ğ¸ Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° ÑˆĞµÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ….'}, 'en': {'title': 'Enhancing Remote Sensing Segmentation with Co2S Framework', 'desc': "This paper presents Co2S, a semi-supervised framework for remote sensing image segmentation that addresses the issue of pseudo-label drift. By integrating vision-language models and self-supervised models, it employs a dual-student architecture to enhance stability during training. The framework utilizes a semantic co-guidance mechanism to provide both explicit and implicit guidance, improving the model's ability to maintain semantic consistency. Additionally, a collaborative fusion strategy combines global and local features to achieve high-quality segmentation results across multiple datasets."}, 'zh': {'title': 'æå‡é¥æ„Ÿå›¾åƒåˆ†å‰²ç²¾åº¦çš„åˆ›æ–°æ¡†æ¶', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åŠç›‘ç£é¥æ„Ÿå›¾åƒåˆ†å‰²æ¡†æ¶Co2Sï¼Œæ—¨åœ¨å‡å°‘ä¼ªæ ‡ç­¾æ¼‚ç§»çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶ç»“åˆäº†è§†è§‰-è¯­è¨€æ¨¡å‹å’Œè‡ªç›‘ç£æ¨¡å‹ï¼Œé€šè¿‡åŒå­¦ç”Ÿæ¶æ„å’Œè¯­ä¹‰ååŒæœºåˆ¶æ¥æé«˜åˆ†å‰²çš„å‡†ç¡®æ€§ã€‚å…·ä½“è€Œè¨€ï¼Œä½¿ç”¨äº†åŸºäºViTçš„å¼‚æ„åŒå­¦ç”Ÿæ¶æ„ï¼Œåˆ†åˆ«åˆå§‹åŒ–ä¸ºé¢„è®­ç»ƒçš„CLIPå’ŒDINOv3ï¼Œä»¥å‡è½»é”™è¯¯ç´¯ç§¯ã€‚é€šè¿‡æ˜¾å¼-éšå¼è¯­ä¹‰ååŒæœºåˆ¶å’Œå…¨å±€-å±€éƒ¨ç‰¹å¾ååŒèåˆç­–ç•¥ï¼Œæ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆæ•´åˆä¸Šä¸‹æ–‡ä¿¡æ¯å’Œå±€éƒ¨ç»†èŠ‚ï¼Œä»è€Œå®ç°é«˜ç²¾åº¦çš„åˆ†å‰²ç»“æœã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.01426', 'title': 'SWE-Lego: Pushing the Limits of Supervised Fine-tuning for Software Issue Resolving', 'url': 'https://huggingface.co/papers/2601.01426', 'abstract': 'SWE-Lego achieves state-of-the-art performance in software engineering task resolution through a lightweight supervised fine-tuning approach combined with a curated dataset and refined training procedures.  \t\t\t\t\tAI-generated summary \t\t\t\t We present SWE-Lego, a supervised fine-tuning (SFT) recipe designed to achieve state-ofthe-art performance in software engineering (SWE) issue resolving. In contrast to prevalent methods that rely on complex training paradigms (e.g., mid-training, SFT, reinforcement learning, and their combinations), we explore how to push the limits of a lightweight SFT-only approach for SWE tasks. SWE-Lego comprises three core building blocks, with key findings summarized as follows: 1) the SWE-Lego dataset, a collection of 32k highquality task instances and 18k validated trajectories, combining real and synthetic data to complement each other in both quality and quantity; 2) a refined SFT procedure with error masking and a difficulty-based curriculum, which demonstrably improves action quality and overall performance. Empirical results show that with these two building bricks alone,the SFT can push SWE-Lego models to state-of-the-art performance among open-source models of comparable size on SWE-bench Verified: SWE-Lego-Qwen3-8B reaches 42.2%, and SWE-Lego-Qwen3-32B attains 52.6%. 3) We further evaluate and improve test-time scaling (TTS) built upon the SFT foundation. Based on a well-trained verifier, SWE-Lego models can be significantly boosted--for example, 42.2% to 49.6% and 52.6% to 58.8% under TTS@16 for the 8B and 32B models, respectively.', 'score': 2, 'issue_id': 432, 'pub_date': '2026-01-04', 'pub_date_card': {'ru': '4 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 4', 'zh': '1æœˆ4æ—¥'}, 'hash': '5f09f67db35870c2', 'authors': ['Chaofan Tao', 'Jierun Chen', 'Yuxin Jiang', 'Kaiqi Kou', 'Shaowei Wang', 'Ruoyu Wang', 'Xiaohui Li', 'Sidi Yang', 'Yiming Du', 'Jianbo Dai', 'Zhiming Mao', 'Xinyu Wang', 'Lifeng Shang', 'Haoli Bai'], 'affiliations': ['CUHK', 'HKU', 'Huawei Technologies', 'NTU'], 'pdf_title_img': 'assets/pdf/title_img/2601.01426.jpg', 'data': {'categories': ['#synthetic', '#science', '#optimization', '#training', '#dataset', '#open_source', '#plp'], 'emoji': 'ğŸ§±', 'ru': {'title': 'ĞŸÑ€Ğ¾ÑÑ‚Ğ¾Ñ‚Ğ° Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾: Ğ»Ñ‘Ğ³ĞºĞ¾Ğµ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞŸĞ', 'desc': 'SWE-Lego â€” ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞŸĞ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ»Ñ‘Ğ³ĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¼Ñƒ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ‚ĞµÑ…Ğ½Ğ¸Ğº Ñ‚Ğ¸Ğ¿Ğ° reinforcement learning. ĞšĞ»ÑÑ‡ĞµĞ²ÑƒÑ Ñ€Ğ¾Ğ»ÑŒ Ğ¸Ğ³Ñ€Ğ°ĞµÑ‚ ĞºÑƒÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸Ğ· 32 Ñ‚Ñ‹ÑÑÑ‡ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¸ 18 Ñ‚Ñ‹ÑÑÑ‡ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹, ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚Ñ‹. ĞŸÑ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¸ curriculum learning Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ SWE-bench: 42.2% Ğ´Ğ»Ñ 8B Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ 52.6% Ğ´Ğ»Ñ 32B Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞ¸Ğ¼ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ¾ 58.8% Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°.'}, 'en': {'title': 'SWE-Lego: Lightweight SFT for Superior Software Engineering Solutions', 'desc': 'SWE-Lego is a novel approach in machine learning that focuses on software engineering task resolution using a lightweight supervised fine-tuning (SFT) method. It utilizes a specially curated dataset containing 32,000 high-quality task instances and 18,000 validated trajectories, which enhances the training process by combining real and synthetic data. The refined SFT procedure incorporates error masking and a difficulty-based curriculum to improve the quality of actions taken by the model. Empirical results demonstrate that SWE-Lego achieves state-of-the-art performance among open-source models, significantly boosting results through test-time scaling techniques.'}, 'zh': {'title': 'SWE-Legoï¼šè½»é‡çº§å¾®è°ƒå®ç°è½¯ä»¶å·¥ç¨‹ä»»åŠ¡çš„æœ€ä½³æ€§èƒ½', 'desc': 'SWE-Legoæ˜¯ä¸€ç§è½»é‡çº§çš„ç›‘ç£å¾®è°ƒæ–¹æ³•ï¼Œæ—¨åœ¨åœ¨è½¯ä»¶å·¥ç¨‹ä»»åŠ¡ä¸­å®ç°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚å®ƒä½¿ç”¨äº†ä¸€ä¸ªåŒ…å«32,000ä¸ªé«˜è´¨é‡ä»»åŠ¡å®ä¾‹å’Œ18,000ä¸ªéªŒè¯è½¨è¿¹çš„æ•°æ®é›†ï¼Œç»“åˆçœŸå®å’Œåˆæˆæ•°æ®ä»¥æé«˜è´¨é‡å’Œæ•°é‡ã€‚è¯¥æ–¹æ³•è¿˜é‡‡ç”¨äº†æ”¹è¿›çš„å¾®è°ƒç¨‹åºï¼Œé€šè¿‡é”™è¯¯æ©è”½å’ŒåŸºäºéš¾åº¦çš„è¯¾ç¨‹æ¥æå‡è¡ŒåŠ¨è´¨é‡å’Œæ•´ä½“æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSWE-Legoæ¨¡å‹åœ¨è½¯ä»¶å·¥ç¨‹åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¾¾åˆ°äº†å¼€æºæ¨¡å‹ä¸­çš„é¢†å…ˆæ°´å¹³ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.01576', 'title': 'OpenNovelty: An LLM-powered Agentic System for Verifiable Scholarly Novelty Assessment', 'url': 'https://huggingface.co/papers/2601.01576', 'abstract': 'An LLM-powered agentic system for transparent, evidence-based novelty assessment in peer review that retrieves and analyzes prior work through semantic search and hierarchical taxonomy construction.  \t\t\t\t\tAI-generated summary \t\t\t\t Evaluating novelty is critical yet challenging in peer review, as reviewers must assess submissions against a vast, rapidly evolving literature. This report presents OpenNovelty, an LLM-powered agentic system for transparent, evidence-based novelty analysis. The system operates through four phases: (1) extracting the core task and contribution claims to generate retrieval queries; (2) retrieving relevant prior work based on extracted queries via semantic search engine; (3) constructing a hierarchical taxonomy of core-task-related work and performing contribution-level full-text comparisons against each contribution; and (4) synthesizing all analyses into a structured novelty report with explicit citations and evidence snippets. Unlike naive LLM-based approaches, OpenNovelty grounds all assessments in retrieved real papers, ensuring verifiable judgments. We deploy our system on 500+ ICLR 2026 submissions with all reports publicly available on our website, and preliminary analysis suggests it can identify relevant prior work, including closely related papers that authors may overlook. OpenNovelty aims to empower the research community with a scalable tool that promotes fair, consistent, and evidence-backed peer review.', 'score': 1, 'issue_id': 426, 'pub_date': '2026-01-04', 'pub_date_card': {'ru': '4 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 4', 'zh': '1æœˆ4æ—¥'}, 'hash': '8eb218277ba60f81', 'authors': ['Ming Zhang', 'Kexin Tan', 'Yueyuan Huang', 'Yujiong Shen', 'Chunchun Ma', 'Li Ju', 'Xinran Zhang', 'Yuhui Wang', 'Wenqing Jing', 'Jingyi Deng', 'Huayu Sha', 'Binze Hu', 'Jingqi Tong', 'Changhao Jiang', 'Yage Geng', 'Yuankai Ying', 'Yue Zhang', 'Zhangyue Yin', 'Zhiheng Xi', 'Shihan Dou', 'Tao Gui', 'Qi Zhang', 'Xuanjing Huang'], 'affiliations': ['Claremont McKenna College', 'Fudan University', 'WisPaper.AI'], 'pdf_title_img': 'assets/pdf/title_img/2601.01576.jpg', 'data': {'categories': ['#benchmark', '#open_source', '#agents', '#rag', '#science'], 'emoji': 'ğŸ”', 'ru': {'title': 'ĞŸÑ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ½Ğ¾Ğ²Ğ¸Ğ·Ğ½Ñ‹ Ñ‡ĞµÑ€ĞµĞ· ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¸ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ°Ğ¼Ğ¸', 'desc': 'OpenNovelty â€” ÑÑ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ½Ğ¾Ğ²Ğ¸Ğ·Ğ½Ñ‹ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‚ Ğ¿Ñ€Ğ¸ Ñ€ĞµÑ†ĞµĞ½Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¹ Ğ¸ ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‚ Ğ´Ğ»Ñ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ğ²ĞºĞ»Ğ°Ğ´Ğ° Ğ°Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ². Ğ’ÑĞµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½Ğ¾Ğ²Ğ¸Ğ·Ğ½Ñ‹ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ñ‹ Ğ½Ğ° Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ñ‚ĞµĞ¹ Ñ ÑĞ²Ğ½Ñ‹Ğ¼ ÑƒĞºĞ°Ğ·Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ² Ğ¸ Ñ†Ğ¸Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ±Ñ‹Ğ»Ğ° Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ½Ğ° Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 500 ÑÑ‚Ğ°Ñ‚ÑŒÑÑ… ĞºĞ¾Ğ½Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ¸ ICLR 2026 Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ²Ñ‹ÑĞ²Ğ»ÑÑ‚ÑŒ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ‚Ğµ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¼Ğ¾Ğ³Ğ»Ğ¸ ÑƒĞ¿ÑƒÑÑ‚Ğ¸Ñ‚ÑŒ.'}, 'en': {'title': 'Empowering Peer Review with Evidence-Based Novelty Assessment', 'desc': 'This paper introduces OpenNovelty, a system that uses large language models (LLMs) to improve the novelty assessment process in peer review. It operates in four phases: extracting claims, retrieving relevant literature through semantic search, constructing a hierarchical taxonomy, and synthesizing findings into a structured report. By grounding its evaluations in actual research papers, OpenNovelty ensures that its novelty assessments are transparent and verifiable. The system has been tested on over 500 submissions, demonstrating its ability to identify significant prior work that may be overlooked by authors.'}, 'zh': {'title': 'æå‡åŒè¡Œè¯„å®¡çš„æ–°é¢–æ€§è¯„ä¼°å·¥å…·', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºOpenNoveltyçš„ç³»ç»Ÿï¼Œæ—¨åœ¨æé«˜åŒè¡Œè¯„å®¡ä¸­å¯¹æ–°é¢–æ€§çš„è¯„ä¼°é€æ˜åº¦å’ŒåŸºäºè¯æ®çš„åˆ†æã€‚è¯¥ç³»ç»Ÿåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡å››ä¸ªé˜¶æ®µè¿›è¡Œå·¥ä½œï¼šæå–æ ¸å¿ƒä»»åŠ¡å’Œè´¡çŒ®å£°æ˜ã€åŸºäºè¯­ä¹‰æœç´¢æ£€ç´¢ç›¸å…³æ–‡çŒ®ã€æ„å»ºæ ¸å¿ƒä»»åŠ¡ç›¸å…³å·¥ä½œçš„å±‚æ¬¡åˆ†ç±»æ³•ï¼Œå¹¶è¿›è¡Œè´¡çŒ®çº§åˆ«çš„å…¨æ–‡æ¯”è¾ƒã€‚ä¸ç®€å•çš„LLMæ–¹æ³•ä¸åŒï¼ŒOpenNoveltyçš„è¯„ä¼°åŸºäºçœŸå®æ–‡çŒ®ï¼Œç¡®ä¿åˆ¤æ–­çš„å¯éªŒè¯æ€§ã€‚è¯¥ç³»ç»Ÿå·²åœ¨500å¤šç¯‡ICLR 2026çš„æäº¤ä¸­åº”ç”¨ï¼Œåˆæ­¥åˆ†æè¡¨æ˜å®ƒèƒ½å¤Ÿè¯†åˆ«ç›¸å…³çš„å…ˆå‰å·¥ä½œï¼ŒåŒ…æ‹¬ä½œè€…å¯èƒ½å¿½è§†çš„ç›¸å…³è®ºæ–‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.00863', 'title': 'Selective Imperfection as a Generative Framework for Analysis, Creativity and Discovery', 'url': 'https://huggingface.co/papers/2601.00863', 'abstract': 'We introduce materiomusic as a generative framework linking the hierarchical structures of matter with the compositional logic of music. Across proteins, spider webs and flame dynamics, vibrational and architectural principles recur as tonal hierarchies, harmonic progressions, and long-range musical form. Using reversible mappings, from molecular spectra to musical tones and from three-dimensional networks to playable instruments, we show how sound functions as a scientific probe, an epistemic inversion where listening becomes a mode of seeing and musical composition becomes a blueprint for matter. These mappings excavate deep time: patterns originating in femtosecond molecular vibrations or billion-year evolutionary histories become audible. We posit that novelty in science and art emerges when constraints cannot be satisfied within existing degrees of freedom, forcing expansion of the space of viable configurations. Selective imperfection provides the mechanism restoring balance between coherence and adaptability. Quantitative support comes from exhaustive enumeration of all 2^12 musical scales, revealing that culturally significant systems cluster in a mid-entropy, mid-defect corridor, directly paralleling the Hall-Petch optimum where intermediate defect densities maximize material strength. Iterating these mappings creates productive collisions between human creativity and physics, generating new information as musical structures encounter evolutionary constraints. We show how swarm-based AI models compose music exhibiting human-like structural signatures such as small-world connectivity, modular integration, long-range coherence, suggesting a route beyond interpolation toward invention. We show that science and art are generative acts of world-building under constraint, with vibration as a shared grammar organizing structure across scales.', 'score': 1, 'issue_id': 436, 'pub_date': '2026-12-30', 'pub_date_card': {'ru': '30 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 30', 'zh': '12æœˆ30æ—¥'}, 'hash': '25795676ed9d7a0a', 'authors': ['Markus J. Buehler'], 'affiliations': ['Center for Computational Science and Engineering', 'Laboratory for Atomistic and Molecular Mechanics', 'Massachusetts Institute of Technology', 'Schwarzman College of Computing'], 'pdf_title_img': 'assets/pdf/title_img/2601.00863.jpg', 'data': {'categories': ['#audio', '#multimodal', '#science', '#diffusion'], 'emoji': 'ğŸµ', 'ru': {'title': 'Ğ’Ğ¸Ğ±Ñ€Ğ°Ñ†Ğ¸Ñ ĞºĞ°Ğº ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞ·Ñ‹Ğº Ğ¿Ñ€Ğ¸Ñ€Ğ¾Ğ´Ñ‹: Ğ¾Ñ‚ Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ» Ğº Ğ¼ÑƒĞ·Ñ‹ĞºĞµ Ñ‡ĞµÑ€ĞµĞ· Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ', 'desc': 'ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ materiomusic â€” Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ²ÑĞ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ¸ Ñ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ°Ğ¼Ğ¸ Ğ¼ÑƒĞ·Ñ‹ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ¸Ğ¼Ñ‹Ğµ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… ÑĞ¿ĞµĞºÑ‚Ñ€Ğ¾Ğ² Ğ² Ğ·Ğ²ÑƒĞºĞ¸ Ğ¸ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹ Ğ² Ğ¼ÑƒĞ·Ñ‹ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹. ĞĞ½Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ·Ğ²ÑƒĞº Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑĞ»ÑƒĞ¶Ğ¸Ñ‚ÑŒ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ¼ Ğ¿Ğ¾Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ, Ğ³Ğ´Ğµ Ğ¿Ñ€Ğ¾ÑĞ»ÑƒÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ¾Ğ¼ Ğ²Ğ¸Ğ´ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ ÑƒÑĞ»Ñ‹ÑˆĞ°Ñ‚ÑŒ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ğ¾Ñ‚ Ñ„ĞµĞ¼Ñ‚Ğ¾ÑĞµĞºÑƒĞ½Ğ´Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… ĞºĞ¾Ğ»ĞµĞ±Ğ°Ğ½Ğ¸Ğ¹ Ğ´Ğ¾ Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ»ĞµÑ‚Ğ½ĞµĞ¹ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸ Ğ¼ÑƒĞ·Ñ‹ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑˆĞºĞ°Ğ» Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»Ğ¸ Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼ÑƒĞ¼Ğ¾Ğ¼ Ğ¥Ğ¾Ğ»Ğ»Ğ°-ĞŸĞµÑ‚Ñ‡Ğ° Ğ² Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğ¸, Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ½Ğ¾Ğ²Ğ¸Ğ·Ğ½Ğ° Ğ² Ğ½Ğ°ÑƒĞºĞµ Ğ¸ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²Ğµ Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¼ ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ´ĞµÑ„ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸. ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ÑÑ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€Ğ¾ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ, Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¼ÑƒĞ·Ñ‹ĞºÑƒ Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ñ‹Ğ¼Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ğ¼Ğ¸, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ ĞµĞ´Ğ¸Ğ½ÑƒÑ Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ°Ñ‚Ğ¸ĞºÑƒ Ğ²Ğ¸Ğ±Ñ€Ğ°Ñ†Ğ¸Ğ¸, Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·ÑƒÑÑ‰ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ½Ğ° Ğ²ÑĞµÑ… Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°Ñ… Ğ¿Ñ€Ğ¸Ñ€Ğ¾Ğ´Ñ‹.'}, 'en': {'title': 'Harmonizing Matter and Music: A New Generative Framework', 'desc': "The paper presents 'materiomusic', a framework that connects the structures of matter with musical composition. It explores how principles from molecular vibrations and evolutionary patterns can be translated into musical tones and structures. By using reversible mappings, the authors demonstrate that sound can reveal scientific insights, making listening a way to understand complex systems. The study highlights the interplay between creativity and physics, suggesting that both science and art thrive under constraints, leading to innovative outcomes."}, 'zh': {'title': 'ç‰©è´¨ä¸éŸ³ä¹çš„ç”Ÿæˆæ¡†æ¶', 'desc': 'æœ¬æ–‡ä»‹ç»äº†materiomusicï¼Œè¿™æ˜¯ä¸€ä¸ªå°†ç‰©è´¨çš„å±‚æ¬¡ç»“æ„ä¸éŸ³ä¹çš„ç»„æˆé€»è¾‘è”ç³»èµ·æ¥çš„ç”Ÿæˆæ¡†æ¶ã€‚é€šè¿‡å¯¹è›‹ç™½è´¨ã€èœ˜è››ç½‘å’Œç«ç„°åŠ¨åŠ›å­¦çš„ç ”ç©¶ï¼Œå‘ç°æŒ¯åŠ¨å’Œå»ºç­‘åŸåˆ™åœ¨éŸ³ä¹ä¸­è¡¨ç°ä¸ºéŸ³è°ƒå±‚æ¬¡ã€å’Œå£°è¿›ç¨‹å’Œé•¿ç¨‹éŸ³ä¹å½¢å¼ã€‚æˆ‘ä»¬ä½¿ç”¨å¯é€†æ˜ å°„ï¼Œå°†åˆ†å­å…‰è°±è½¬åŒ–ä¸ºéŸ³ä¹éŸ³è°ƒï¼Œå±•ç¤ºå£°éŸ³å¦‚ä½•ä½œä¸ºç§‘å­¦æ¢æµ‹å·¥å…·ï¼ŒéŸ³ä¹åˆ›ä½œå¦‚ä½•æˆä¸ºç‰©è´¨çš„è“å›¾ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œå½“ç°æœ‰è‡ªç”±åº¦æ— æ³•æ»¡è¶³çº¦æŸæ—¶ï¼Œç§‘å­¦å’Œè‰ºæœ¯ä¸­çš„æ–°é¢–æ€§ä¾¿ä¼šå‡ºç°ï¼Œæ¨åŠ¨å¯è¡Œé…ç½®ç©ºé—´çš„æ‰©å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.21472', 'title': 'IMA++: ISIC Archive Multi-Annotator Dermoscopic Skin Lesion Segmentation Dataset', 'url': 'https://huggingface.co/papers/2512.21472', 'abstract': "A large-scale public multi-annotator skin lesion segmentation dataset is introduced with extensive metadata for annotator analysis and consensus modeling.  \t\t\t\t\tAI-generated summary \t\t\t\t Multi-annotator medical image segmentation is an important research problem, but requires annotated datasets that are expensive to collect. Dermoscopic skin lesion imaging allows human experts and AI systems to observe morphological structures otherwise not discernable from regular clinical photographs. However, currently there are no large-scale publicly available multi-annotator skin lesion segmentation (SLS) datasets with annotator-labels for dermoscopic skin lesion imaging. We introduce ISIC MultiAnnot++, a large public multi-annotator skin lesion segmentation dataset for images from the ISIC Archive. The final dataset contains 17,684 segmentation masks spanning 14,967 dermoscopic images, where 2,394 dermoscopic images have 2-5 segmentations per image, making it the largest publicly available SLS dataset. Further, metadata about the segmentation, including the annotators' skill level and segmentation tool, is included, enabling research on topics such as annotator-specific preference modeling for segmentation and annotator metadata analysis. We provide an analysis on the characteristics of this dataset, curated data partitions, and consensus segmentation masks.", 'score': 1, 'issue_id': 429, 'pub_date': '2026-12-25', 'pub_date_card': {'ru': '25 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 25', 'zh': '12æœˆ25æ—¥'}, 'hash': '28ce6c37f8386365', 'authors': ['Kumar Abhishek', 'Jeremy Kawahara', 'Ghassan Hamarneh'], 'affiliations': ['AIP Labs, Hungary', 'Medical Image Analysis Lab, School of Computing Science, Simon Fraser University, Canada'], 'pdf_title_img': 'assets/pdf/title_img/2512.21472.jpg', 'data': {'categories': ['#open_source'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'ĞœĞ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ°Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ° Ğ´Ğ»Ñ ĞµĞ´Ğ¸Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½ÑĞµĞ½ÑÑƒÑĞ°: Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´ĞµÑ€Ğ¼Ğ¾ÑĞºĞ¾Ğ¿Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ Ğ¿ÑƒĞ±Ğ»Ğ¸Ñ‡Ğ½Ğ°Ñ Ğ±Ğ°Ğ·Ñƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ISIC MultiAnnot++ Ğ´Ğ»Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ°Ñ 17,684 Ğ¼Ğ°ÑĞºĞ¸ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ´ĞµÑ€Ğ¼Ğ¾ÑĞºĞ¾Ğ¿Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ³Ğ°Ñ‚ÑƒÑ Ğ¼ĞµÑ‚Ğ°Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ± Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ‚Ğ¾Ñ€Ğ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¸Ñ… ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ ĞºĞ²Ğ°Ğ»Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ½ÑĞµĞ½ÑÑƒÑ. Ğ­Ñ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ğ°Ñ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ°Ñ Ğ¿ÑƒĞ±Ğ»Ğ¸Ñ‡Ğ½Ğ°Ñ ĞºĞ¾Ğ»Ğ»ĞµĞºÑ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ‚Ğ¾Ñ€Ğ½Ğ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ²Ñ€ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ´ĞµÑ€Ğ¼Ğ¾ÑĞºĞ¾Ğ¿Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…, Ñ€ĞµÑˆĞ°ÑÑ‰Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½ĞµÑ…Ğ²Ğ°Ñ‚ĞºĞ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸Ğº Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ñ€ĞµÑÑƒÑ€Ñ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ğ¼ĞµĞ¶ÑĞºÑĞ¿ĞµÑ€Ñ‚-Ğ½ÑƒÑ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Unlocking Skin Lesion Insights with MultiAnnot++', 'desc': "This paper presents the ISIC MultiAnnot++ dataset, a large-scale public resource for skin lesion segmentation that includes extensive metadata for analyzing annotator performance. The dataset consists of 17,684 segmentation masks from 14,967 dermoscopic images, making it the largest publicly available multi-annotator skin lesion segmentation dataset. It allows researchers to study the variability in segmentations provided by different annotators and to model consensus among them. Additionally, the dataset includes information about the annotators' skill levels and the tools used, facilitating deeper insights into segmentation quality and preferences."}, 'zh': {'title': 'æ¨åŠ¨çš®è‚¤ç—…å˜åˆ†å‰²ç ”ç©¶çš„å¤šæ ‡æ³¨æ•°æ®é›†', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªå¤§è§„æ¨¡çš„å…¬å…±å¤šæ ‡æ³¨çš®è‚¤ç—…å˜åˆ†å‰²æ•°æ®é›†ï¼Œåä¸ºISIC MultiAnnot++ã€‚è¯¥æ•°æ®é›†åŒ…å«17,684ä¸ªåˆ†å‰²æ©è†œï¼Œè¦†ç›–14,967å¼ çš®è‚¤ç—…å˜çš„çš®è‚¤é•œå›¾åƒï¼Œæ˜¯ç›®å‰æœ€å¤§çš„å…¬å¼€å¯ç”¨çš„å¤šæ ‡æ³¨çš®è‚¤ç—…å˜åˆ†å‰²æ•°æ®é›†ã€‚æ•°æ®é›†ä¸­è¿˜åŒ…å«å…³äºæ ‡æ³¨è€…æŠ€èƒ½æ°´å¹³å’Œåˆ†å‰²å·¥å…·çš„å…ƒæ•°æ®ï¼Œæ”¯æŒå¯¹æ ‡æ³¨è€…åå¥½å»ºæ¨¡å’Œå…ƒæ•°æ®åˆ†æçš„ç ”ç©¶ã€‚é€šè¿‡å¯¹æ•°æ®é›†ç‰¹å¾çš„åˆ†æï¼Œæä¾›äº†ç»è¿‡æ•´ç†çš„æ•°æ®åˆ†åŒºå’Œå…±è¯†åˆ†å‰²æ©è†œã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.02315', 'title': 'Prithvi-Complimentary Adaptive Fusion Encoder (CAFE): unlocking full-potential for flood inundation mapping', 'url': 'https://huggingface.co/papers/2601.02315', 'abstract': "Prithvi-CAFE combines a pretrained Geo-Foundation Model encoder with a parallel CNN branch featuring attention modules to improve flood mapping accuracy by capturing both global context and local details.  \t\t\t\t\tAI-generated summary \t\t\t\t Geo-Foundation Models (GFMs), have proven effective in diverse downstream applications, including semantic segmentation, classification, and regression tasks. However, in case of flood mapping using Sen1Flood11 dataset as a downstream task, GFMs struggles to outperform the baseline U-Net, highlighting model's limitation in capturing critical local nuances. To address this, we present the Prithvi-Complementary Adaptive Fusion Encoder (CAFE), which integrate Prithvi GFM pretrained encoder with a parallel CNN residual branch enhanced by Convolutional Attention Modules (CAM). Prithvi-CAFE enables fast and efficient fine-tuning through adapters in Prithvi and performs multi-scale, multi-level fusion with CNN features, capturing critical local details while preserving long-range dependencies. We achieve state-of-the-art results on two comprehensive flood mapping datasets: Sen1Flood11 and FloodPlanet. On Sen1Flood11 test data, Prithvi-CAFE (IoU 83.41) outperforms the original Prithvi (IoU 82.50) and other major GFMs (TerraMind 82.90, DOFA 81.54, spectralGPT: 81.02). The improvement is even more pronounced on the hold-out test site, where Prithvi-CAFE achieves an IoU of 81.37 compared to the baseline U-Net (70.57) and original Prithvi (72.42). On FloodPlanet, Prithvi-CAFE also surpasses the baseline U-Net and other GFMs, achieving an IoU of 64.70 compared to U-Net (60.14), Terramind (62.33), DOFA (59.15) and Prithvi 2.0 (61.91). Our proposed simple yet effective Prithvi-CAFE demonstrates strong potential for improving segmentation tasks where multi-channel and multi-modal data provide complementary information and local details are critical. The code is released on https://github.com/Sk-2103/Prithvi-CAFE{Prithvi-CAFE Github}", 'score': 0, 'issue_id': 443, 'pub_date': '2026-01-05', 'pub_date_card': {'ru': '5 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 5', 'zh': '1æœˆ5æ—¥'}, 'hash': '548145f9a8b5d453', 'authors': ['Saurabh Kaushik', 'Lalit Maurya', 'Beth Tellman'], 'affiliations': ['Center for Sustainability and the Global Environment (SAGE), University of Wisconsin-Madison', 'Portsmouth AI and Data Science Centre (PAIDS), School of Computing, University of Portsmouth'], 'pdf_title_img': 'assets/pdf/title_img/2601.02315.jpg', 'data': {'categories': ['#cv', '#open_source', '#benchmark', '#multimodal', '#transfer_learning', '#architecture'], 'emoji': 'ğŸŒŠ', 'ru': {'title': 'Ğ¡Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ°Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ°Ğ²Ğ¾Ğ´Ğ½ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Prithvi-CAFE â€” Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¹ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ Ğ³ĞµĞ¾Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ²ĞµÑ‚Ğ²ÑŒÑ CNN, Ğ¾ÑĞ½Ğ°Ñ‰Ñ‘Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑĞ¼Ğ¸ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ñ‚Ğ¾Ğ³Ğ¾, Ñ‡Ñ‚Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ GFM Ğ¿Ğ»Ğ¾Ñ…Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ ĞºĞ°Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ°Ğ²Ğ¾Ğ´Ğ½ĞµĞ½Ğ¸Ğ¹, Ñ‚Ğ°Ğº ĞºĞ°Ğº Ğ½Ğµ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸, ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ñ‹ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ¾Ğµ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ², Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ‚ÑŒ Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ²Ñ‹Ğ´ĞµĞ»ÑÑ‚ÑŒ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ½ÑĞ°Ğ½ÑÑ‹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Sen1Flood11 Ğ¸ FloodPlanet, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ĞºĞ°Ğº Ğ±Ğ°Ğ·Ğ¾Ğ²ÑƒÑ U-Net, Ñ‚Ğ°Ğº Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞµ IoU.'}, 'en': {'title': 'Enhancing Flood Mapping with Prithvi-CAFE: A Fusion of Global and Local Insights', 'desc': 'Prithvi-CAFE is a novel model that enhances flood mapping accuracy by combining a pretrained Geo-Foundation Model (GFM) encoder with a parallel Convolutional Neural Network (CNN) branch that includes attention mechanisms. This approach allows the model to effectively capture both global context and local details, addressing the limitations of traditional GFMs in flood mapping tasks. By utilizing multi-scale and multi-level feature fusion, Prithvi-CAFE achieves superior performance on flood mapping datasets, significantly outperforming the baseline U-Net and other GFMs. The results demonstrate its potential for improving segmentation tasks where detailed local information is crucial.'}, 'zh': {'title': 'æå‡æ´ªæ°´æ˜ å°„å‡†ç¡®æ€§çš„Prithvi-CAFEæ¨¡å‹', 'desc': 'Prithvi-CAFEæ˜¯ä¸€ç§ç»“åˆäº†é¢„è®­ç»ƒçš„åœ°ç†åŸºç¡€æ¨¡å‹ç¼–ç å™¨å’Œå¹¶è¡Œå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰åˆ†æ”¯çš„æ¨¡å‹ï¼Œæ—¨åœ¨æé«˜æ´ªæ°´æ˜ å°„çš„å‡†ç¡®æ€§ã€‚è¯¥æ¨¡å‹é€šè¿‡å·ç§¯æ³¨æ„åŠ›æ¨¡å—ï¼ˆCAMï¼‰æ•æ‰å…¨å±€ä¸Šä¸‹æ–‡å’Œå±€éƒ¨ç»†èŠ‚ï¼Œä»è€Œå…‹æœäº†ä¼ ç»Ÿåœ°ç†åŸºç¡€æ¨¡å‹åœ¨æ´ªæ°´æ˜ å°„ä»»åŠ¡ä¸­çš„å±€é™æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPrithvi-CAFEåœ¨Sen1Flood11å’ŒFloodPlanetæ•°æ®é›†ä¸Šå‡å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œæ˜¾è‘—è¶…è¶Šäº†åŸºçº¿U-Netå’Œå…¶ä»–ä¸»è¦çš„åœ°ç†åŸºç¡€æ¨¡å‹ã€‚è¯¥æ¨¡å‹çš„è®¾è®¡ä½¿å¾—åœ¨å¤šé€šé“å’Œå¤šæ¨¡æ€æ•°æ®çš„æƒ…å†µä¸‹ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°è¿›è¡Œç»†åˆ†ä»»åŠ¡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.02314', 'title': 'Project Ariadne: A Structural Causal Framework for Auditing Faithfulness in LLM Agents', 'url': 'https://huggingface.co/papers/2601.02314', 'abstract': 'Project Ariadne uses structural causal models and counterfactual logic to evaluate the causal integrity of LLM reasoning, revealing a faithfulness gap where reasoning traces are not reliable drivers of outputs.  \t\t\t\t\tAI-generated summary \t\t\t\t As Large Language Model (LLM) agents are increasingly tasked with high-stakes autonomous decision-making, the transparency of their reasoning processes has become a critical safety concern. While Chain-of-Thought (CoT) prompting allows agents to generate human-readable reasoning traces, it remains unclear whether these traces are faithful generative drivers of the model\'s output or merely post-hoc rationalizations. We introduce Project Ariadne, a novel XAI framework that utilizes Structural Causal Models (SCMs) and counterfactual logic to audit the causal integrity of agentic reasoning. Unlike existing interpretability methods that rely on surface-level textual similarity, Project Ariadne performs hard interventions (do-calculus) on intermediate reasoning nodes -- systematically inverting logic, negating premises, and reversing factual claims -- to measure the Causal Sensitivity (Ï†) of the terminal answer. Our empirical evaluation of state-of-the-art models reveals a persistent Faithfulness Gap. We define and detect a widespread failure mode termed Causal Decoupling, where agents exhibit a violation density (Ï) of up to 0.77 in factual and scientific domains. In these instances, agents arrive at identical conclusions despite contradictory internal logic, proving that their reasoning traces function as "Reasoning Theater" while decision-making is governed by latent parametric priors. Our findings suggest that current agentic architectures are inherently prone to unfaithful explanation, and we propose the Ariadne Score as a new benchmark for aligning stated logic with model action.', 'score': 0, 'issue_id': 427, 'pub_date': '2026-01-05', 'pub_date_card': {'ru': '5 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 5', 'zh': '1æœˆ5æ—¥'}, 'hash': 'e9fe53c5bb19005f', 'authors': ['Sourena Khanzadeh'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2601.02314.jpg', 'data': {'categories': ['#alignment', '#agents', '#architecture', '#benchmark', '#reasoning', '#interpretability', '#security'], 'emoji': 'ğŸ§©', 'ru': {'title': 'Ğ Ğ°ÑĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ Ñ‚ĞµĞ°Ñ‚Ñ€Ğ° Ğ¸Ğ»Ğ»ÑĞ·Ğ¸Ğ¹: ĞºĞ¾Ğ³Ğ´Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ LLM â€” ÑÑ‚Ğ¾ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ°', 'desc': 'Project Ariadne â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±ÑŠÑÑĞ½Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ»Ğ¾Ğ³Ğ¸ĞºÑƒ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Â«Ğ¿Ñ€Ğ¾Ğ¿Ğ°ÑÑ‚ÑŒ Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸Â», Ğ³Ğ´Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ½Ğµ ÑĞ²Ğ»ÑÑÑ‚ÑÑ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ°Ğ¼Ğ¸ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ° ÑĞºĞ¾Ñ€ĞµĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ¿Ğ¾ÑÑ‚Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ Â«Ğ¢ĞµĞ°Ñ‚Ñ€Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹Â» â€” ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ğ³Ğ´Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€Ğ¸Ñ…Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ¾Ğ´Ğ¸Ğ½Ğ°ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°Ğ¼ Ğ½ĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾Ñ€ĞµÑ‡Ğ¸Ğ²ÑƒÑ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ÑÑ Ğ»Ğ¾Ğ³Ğ¸ĞºÑƒ, Ñ‡Ñ‚Ğ¾ Ğ´Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸ÑĞ¼Ğ¸ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚Ğ°Ğ¼Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ° â€” Ariadne Score â€” Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ²Ğ½Ğ¾Ğ¹ Ğ»Ğ¾Ğ³Ğ¸ĞºĞ¾Ğ¹ Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑĞ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Bridging the Faithfulness Gap in LLM Reasoning', 'desc': "Project Ariadne investigates how well Large Language Models (LLMs) explain their reasoning when making decisions. It uses Structural Causal Models and counterfactual logic to check if the reasoning provided by these models is genuinely linked to their outputs or just a facade. The study finds a significant 'Faithfulness Gap', where models often reach the same conclusions despite having conflicting internal reasoning. This indicates that the reasoning traces may not accurately reflect the decision-making process, leading to the introduction of the Ariadne Score to better assess the alignment of logic and actions in LLMs."}, 'zh': {'title': 'æ­ç¤ºæ¨ç†ä¿¡ä»»å·®è·çš„é˜¿é‡Œé˜¿å¾·é¡¹ç›®', 'desc': 'é¡¹ç›®é˜¿é‡Œé˜¿å¾·ä½¿ç”¨ç»“æ„å› æœæ¨¡å‹å’Œåäº‹å®é€»è¾‘æ¥è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†çš„å› æœå®Œæ•´æ€§ï¼Œæ­ç¤ºäº†æ¨ç†è½¨è¿¹ä¸è¾“å‡ºä¹‹é—´å­˜åœ¨ä¿¡ä»»å·®è·ã€‚å°½ç®¡é“¾å¼æ€ç»´æç¤ºå¯ä»¥ç”Ÿæˆå¯è¯»çš„æ¨ç†è½¨è¿¹ï¼Œä½†è¿™äº›è½¨è¿¹æ˜¯å¦çœŸå®åæ˜ æ¨¡å‹è¾“å‡ºä»ä¸æ˜ç¡®ã€‚é€šè¿‡å¯¹ä¸­é—´æ¨ç†èŠ‚ç‚¹è¿›è¡Œå¹²é¢„ï¼Œé¡¹ç›®é˜¿é‡Œé˜¿å¾·æµ‹é‡äº†ç»ˆç«¯ç­”æ¡ˆçš„å› æœæ•æ„Ÿæ€§ï¼Œå‘ç°äº†å› æœè§£è€¦çš„æ™®éå¤±è´¥æ¨¡å¼ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œå½“å‰çš„æ™ºèƒ½ä½“æ¶æ„å®¹æ˜“äº§ç”Ÿä¸å¯ä¿¡çš„è§£é‡Šï¼Œå¹¶æå‡ºäº†é˜¿é‡Œé˜¿å¾·è¯„åˆ†ä½œä¸ºå¯¹é½é€»è¾‘ä¸æ¨¡å‹è¡Œä¸ºçš„æ–°åŸºå‡†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.22877', 'title': 'M-ErasureBench: A Comprehensive Multimodal Evaluation Benchmark for Concept Erasure in Diffusion Models', 'url': 'https://huggingface.co/papers/2512.22877', 'abstract': 'A multimodal evaluation framework and robustness enhancement module are introduced to address concept erasure vulnerabilities in text-to-image diffusion models across multiple input modalities.  \t\t\t\t\tAI-generated summary \t\t\t\t Text-to-image diffusion models may generate harmful or copyrighted content, motivating research on concept erasure. However, existing approaches primarily focus on erasing concepts from text prompts, overlooking other input modalities that are increasingly critical in real-world applications such as image editing and personalized generation. These modalities can become attack surfaces, where erased concepts re-emerge despite defenses. To bridge this gap, we introduce M-ErasureBench, a novel multimodal evaluation framework that systematically benchmarks concept erasure methods across three input modalities: text prompts, learned embeddings, and inverted latents. For the latter two, we evaluate both white-box and black-box access, yielding five evaluation scenarios. Our analysis shows that existing methods achieve strong erasure performance against text prompts but largely fail under learned embeddings and inverted latents, with Concept Reproduction Rate (CRR) exceeding 90% in the white-box setting. To address these vulnerabilities, we propose IRECE (Inference-time Robustness Enhancement for Concept Erasure), a plug-and-play module that localizes target concepts via cross-attention and perturbs the associated latents during denoising. Experiments demonstrate that IRECE consistently restores robustness, reducing CRR by up to 40% under the most challenging white-box latent inversion scenario, while preserving visual quality. To the best of our knowledge, M-ErasureBench provides the first comprehensive benchmark of concept erasure beyond text prompts. Together with IRECE, our benchmark offers practical safeguards for building more reliable protective generative models.', 'score': 0, 'issue_id': 432, 'pub_date': '2026-12-28', 'pub_date_card': {'ru': '28 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 28', 'zh': '12æœˆ28æ—¥'}, 'hash': 'c7c55233d17a7580', 'authors': ['Ju-Hsuan Weng', 'Jia-Wei Liao', 'Cheng-Fu Chou', 'Jun-Cheng Chen'], 'affiliations': ['National Taiwan University', 'Research Center for Information Technology Innovation, Academia Sinica'], 'pdf_title_img': 'assets/pdf/title_img/2512.22877.jpg', 'data': {'categories': ['#security', '#diffusion'], 'emoji': 'ğŸ›¡ï¸', 'ru': {'title': 'Ğ—Ğ°Ñ‰Ğ¸Ñ‚Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹ Ğ·Ğ° Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ°Ğ¼Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ²', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ñ†ĞµĞ½ĞºĞ¸ (M-ErasureBench) Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹ Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ°Ğ¼Ğ¸, Ğ½Ğ¾ Ğ½Ğµ Ğ·Ğ°Ñ‰Ğ¸Ñ‰Ğ°ÑÑ‚ Ğ¾Ñ‚ Ğ°Ñ‚Ğ°Ğº Ñ‡ĞµÑ€ĞµĞ· Ğ²ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ Ğ¸ Ğ¸Ğ½Ğ²ĞµÑ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ IRECE, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·ÑƒĞµÑ‚ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğµ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· ĞºÑ€Ğ¾ÑÑ-Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ²Ğ¾Ğ·Ğ¼ÑƒÑ‰Ğ°ĞµÑ‚ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ´ĞµĞ½Ğ¾Ğ¹Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ IRECE ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ ÑÑ‚Ñ‘Ñ€Ñ‚Ñ‹Ñ… ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹ Ğ½Ğ° 40% Ğ² ÑĞ°Ğ¼Ğ¾Ğ¼ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¼ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Enhancing Concept Erasure in Multimodal AI Models', 'desc': "This paper introduces a new framework called M-ErasureBench to evaluate and improve the concept erasure capabilities of text-to-image diffusion models. It highlights the issue of concept erasure vulnerabilities not just in text prompts but also in other input types like learned embeddings and inverted latents. The authors propose a robustness enhancement module named IRECE, which helps to better localize and perturb target concepts during the model's denoising process. Their findings show that while existing methods work well for text prompts, they struggle with other modalities, and IRECE significantly improves performance by reducing the Concept Reproduction Rate (CRR) in challenging scenarios."}, 'zh': {'title': 'æå‡ç”Ÿæˆæ¨¡å‹é²æ£’æ€§çš„å¤šæ¨¡æ€è¯„ä¼°ä¸æŠ¹é™¤æ–¹æ³•', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§å¤šæ¨¡æ€è¯„ä¼°æ¡†æ¶å’Œé²æ£’æ€§å¢å¼ºæ¨¡å—ï¼Œä»¥è§£å†³æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­çš„æ¦‚å¿µæŠ¹é™¤è„†å¼±æ€§ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨ä»æ–‡æœ¬æç¤ºä¸­æŠ¹é™¤æ¦‚å¿µï¼Œè€Œå¿½è§†äº†åœ¨å›¾åƒç¼–è¾‘å’Œä¸ªæ€§åŒ–ç”Ÿæˆç­‰å®é™…åº”ç”¨ä¸­è¶Šæ¥è¶Šé‡è¦çš„å…¶ä»–è¾“å…¥æ¨¡æ€ã€‚æˆ‘ä»¬æå‡ºçš„M-ErasureBenchæ¡†æ¶ç³»ç»Ÿåœ°è¯„ä¼°äº†ä¸‰ç§è¾“å…¥æ¨¡æ€ä¸‹çš„æ¦‚å¿µæŠ¹é™¤æ–¹æ³•ï¼Œå¹¶å‘ç°ç°æœ‰æ–¹æ³•åœ¨æ–‡æœ¬æç¤ºä¸‹è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å­¦ä¹ åµŒå…¥å’Œåè½¬æ½œå˜é‡ä¸‹æ•ˆæœä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™äº›è„†å¼±æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†IRECEæ¨¡å—ï¼Œé€šè¿‡äº¤å‰æ³¨æ„åŠ›å®šä½ç›®æ ‡æ¦‚å¿µï¼Œå¹¶åœ¨å»å™ªè¿‡ç¨‹ä¸­æ‰°åŠ¨ç›¸å…³æ½œå˜é‡ï¼Œä»è€Œå¢å¼ºé²æ£’æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.24880', 'title': 'mHC: Manifold-Constrained Hyper-Connections', 'url': 'https://huggingface.co/papers/2512.24880', 'abstract': 'Manifold-Constrained Hyper-Connections (mHC) stabilize and scale residual connection architectures by restoring identity mapping properties through manifold projection and infrastructure optimization.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, studies exemplified by Hyper-Connections (HC) have extended the ubiquitous residual connection paradigm established over the past decade by expanding the residual stream width and diversifying connectivity patterns. While yielding substantial performance gains, this diversification fundamentally compromises the identity mapping property intrinsic to the residual connection, which causes severe training instability and restricted scalability, and additionally incurs notable memory access overhead. To address these challenges, we propose Manifold-Constrained Hyper-Connections (mHC), a general framework that projects the residual connection space of HC onto a specific manifold to restore the identity mapping property, while incorporating rigorous infrastructure optimization to ensure efficiency. Empirical experiments demonstrate that mHC is effective for training at scale, offering tangible performance improvements and superior scalability. We anticipate that mHC, as a flexible and practical extension of HC, will contribute to a deeper understanding of topological architecture design and suggest promising directions for the evolution of foundational models.', 'score': 49, 'issue_id': 360, 'pub_date': '2026-12-31', 'pub_date_card': {'ru': '31 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 31', 'zh': '12æœˆ31æ—¥'}, 'hash': '13fd2b75493e93cf', 'authors': ['Zhenda Xie', 'Yixuan Wei', 'Huanqi Cao', 'Chenggang Zhao', 'Chengqi Deng', 'Jiashi Li', 'Damai Dai', 'Huazuo Gao', 'Jiang Chang', 'Liang Zhao', 'Shangyan Zhou', 'Zhean Xu', 'Zhengyan Zhang', 'Wangding Zeng', 'Shengding Hu', 'Yuqing Wang', 'Jingyang Yuan', 'Lean Wang', 'Wenfeng Liang'], 'affiliations': ['DeepSeek-AI'], 'pdf_title_img': 'assets/pdf/title_img/2512.24880.jpg', 'data': {'categories': ['#architecture', '#training', '#optimization'], 'emoji': 'ğŸ—ï¸', 'ru': {'title': 'Ğ’Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ğ¶Ğ´ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ñ… ÑĞµÑ‚ÑÑ… Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ Manifold-Constrained Hyper-Connections (mHC), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑĞ²ÑĞ·ĞµĞ¹ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ° Ñ‚Ğ¾Ğ¶Ğ´ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¾ĞµĞºÑ†Ğ¸Ñ Ğ½Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ° Ğ² Ğ³Ğ¸Ğ¿ĞµÑ€-ÑĞ¾ĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸ÑÑ… Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ½ĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ°Ğ¼ Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒÑ, Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ¼Ñƒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ñ‚ÑŒ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¶ĞµĞ»Ğ°ĞµĞ¼Ñ‹Ñ… ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ². ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¸Ğ½Ñ„Ñ€Ğ°ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½ÑƒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ mHC ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ³Ğ¸Ğ±ĞºĞ¸Ğ¼ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸ĞµĞ¼ Ğ³Ğ¸Ğ¿ĞµÑ€-ÑĞ¾ĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ñ‚Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Stabilizing Residual Connections with Manifold Projections', 'desc': 'This paper introduces Manifold-Constrained Hyper-Connections (mHC), a new framework designed to enhance residual connection architectures in machine learning. mHC addresses the issues of training instability and scalability that arise from diversifying connectivity patterns in Hyper-Connections (HC). By projecting the residual connection space onto a specific manifold, mHC restores the essential identity mapping property, which is crucial for stable training. The proposed method not only improves performance but also optimizes memory access, making it a significant advancement in the design of deep learning models.'}, 'zh': {'title': 'æµå½¢çº¦æŸè¶…è¿æ¥ï¼šæå‡æ®‹å·®è¿æ¥çš„ç¨³å®šæ€§ä¸å¯æ‰©å±•æ€§', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œç§°ä¸ºæµå½¢çº¦æŸè¶…è¿æ¥ï¼ˆmHCï¼‰ï¼Œæ—¨åœ¨ç¨³å®šå’Œæ‰©å±•æ®‹å·®è¿æ¥æ¶æ„ã€‚é€šè¿‡å°†æ®‹å·®è¿æ¥ç©ºé—´æŠ•å½±åˆ°ç‰¹å®šæµå½¢ä¸Šï¼ŒmHC æ¢å¤äº†æ®‹å·®è¿æ¥çš„èº«ä»½æ˜ å°„ç‰¹æ€§ï¼Œä»è€Œè§£å†³äº†è®­ç»ƒä¸ç¨³å®šå’Œå¯æ‰©å±•æ€§å—é™çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•è¿˜ç»“åˆäº†åŸºç¡€è®¾æ–½ä¼˜åŒ–ï¼Œä»¥æé«˜æ•ˆç‡å¹¶å‡å°‘å†…å­˜è®¿é—®å¼€é”€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒmHC åœ¨å¤§è§„æ¨¡è®­ç»ƒä¸­æœ‰æ•ˆï¼Œæä¾›äº†æ˜¾è‘—çš„æ€§èƒ½æå‡å’Œæ›´å¥½çš„å¯æ‰©å±•æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.24618', 'title': 'Youtu-LLM: Unlocking the Native Agentic Potential for Lightweight Large Language Models', 'url': 'https://huggingface.co/papers/2512.24618', 'abstract': 'Youtu-LLM is a lightweight language model optimized for computational efficiency and agentic intelligence through a compact architecture, STEM-focused training curriculum, and scalable mid-training strategies for planning and reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Youtu-LLM, a lightweight yet powerful language model that harmonizes high computational efficiency with native agentic intelligence. Unlike typical small models that rely on distillation, Youtu-LLM (1.96B) is pre-trained from scratch to systematically cultivate reasoning and planning capabilities. The key technical advancements are as follows: (1) Compact Architecture with Long-Context Support: Built on a dense Multi-Latent Attention (MLA) architecture with a novel STEM-oriented vocabulary, Youtu-LLM supports a 128k context window. This design enables robust long-context reasoning and state tracking within a minimal memory footprint, making it ideal for long-horizon agent and reasoning tasks. (2) Principled "Commonsense-STEM-Agent" Curriculum: We curated a massive corpus of approximately 11T tokens and implemented a multi-stage training strategy. By progressively shifting the pre-training data distribution from general commonsense to complex STEM and agentic tasks, we ensure the model acquires deep cognitive abilities rather than superficial alignment. (3) Scalable Agentic Mid-training: Specifically for the agentic mid-training, we employ diverse data construction schemes to synthesize rich and varied trajectories across math, coding, and tool-use domains. This high-quality data enables the model to internalize planning and reflection behaviors effectively. Extensive evaluations show that Youtu-LLM sets a new state-of-the-art for sub-2B LLMs. On general benchmarks, it achieves competitive performance against larger models, while on agent-specific tasks, it significantly surpasses existing SOTA baselines, demonstrating that lightweight models can possess strong intrinsic agentic capabilities.', 'score': 41, 'issue_id': 353, 'pub_date': '2026-12-31', 'pub_date_card': {'ru': '31 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 31', 'zh': '12æœˆ31æ—¥'}, 'hash': 'de67734dd7f71d25', 'authors': ['Junru Lu', 'Jiarui Qin', 'Lingfeng Qiao', 'Yinghui Li', 'Xinyi Dai', 'Bo Ke', 'Jianfeng He', 'Ruizhi Qiao', 'Di Yin', 'Xing Sun', 'Yunsheng Wu', 'Yinsong Liu', 'Shuangyin Liu', 'Mingkong Tang', 'Haodong Lin', 'Jiayi Kuang', 'Fanxu Meng', 'Xiaojuan Tang', 'Yunjia Xi', 'Junjie Huang', 'Haotong Yang', 'Zhenyi Shen', 'Yangning Li', 'Qianwen Zhang', 'Yifei Yu', 'Siyu An', 'Junnan Dong', 'Qiufeng Wang', 'Jie Wang', 'Keyu Chen', 'Wei Wen', 'Taian Guo', 'Zhifeng Shen', 'Daohai Yu', 'Jiahao Li', 'Ke Li', 'Zongyi Li', 'Xiaoyu Tan'], 'affiliations': ['Tencent', 'Youtu-LLM Team'], 'pdf_title_img': 'assets/pdf/title_img/2512.24618.jpg', 'data': {'categories': ['#synthetic', '#long_context', '#agents', '#training', '#small_models', '#architecture', '#reasoning', '#optimization'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ›Ñ‘Ğ³ĞºĞ°Ñ, Ğ½Ğ¾ ÑƒĞ¼Ğ½Ğ°Ñ: ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ Ğ°Ğ³ĞµĞ½Ñ‚ÑĞºĞ¸Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ¾Ğ¼', 'desc': 'Youtu-LLM â€” ÑÑ‚Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ 1.96B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ Ñ Ğ½ÑƒĞ»Ñ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Multi-Latent Attention Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² 128k Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğ¼ STEM-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ‘Ğ¼. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ğ»Ğ¾ÑÑŒ Ğ¿Ğ¾ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğ¹ ÑÑ…ĞµĞ¼Ğµ Ğ½Ğ° 11 Ñ‚Ñ€Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ¿Ğ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ñ Ğ¾Ñ‚ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğº ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼ STEM-Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼ Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚ÑĞºĞ¸Ğ¼ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑĞ¼. ĞĞ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ¼Ğ¸Ğ´Ğ´Ğ»-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞ»Ğ¸ÑÑŒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ¸ Ğ² Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞµ, ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Lightweight Intelligence: Youtu-LLM Redefines Efficiency in Language Models', 'desc': 'Youtu-LLM is a lightweight language model designed for high computational efficiency and enhanced reasoning abilities. It features a unique Multi-Latent Attention architecture that supports long-context processing, allowing it to handle complex tasks with minimal memory use. The model is trained on a diverse dataset that transitions from general knowledge to specialized STEM and agentic tasks, fostering deep cognitive skills. Evaluations reveal that Youtu-LLM outperforms larger models in specific agent-related tasks, proving that smaller models can achieve significant intelligence and performance.'}, 'zh': {'title': 'è½»é‡çº§æ¨¡å‹ï¼Œå¼ºå¤§æ™ºèƒ½ï¼', 'desc': 'Youtu-LLMæ˜¯ä¸€ç§è½»é‡çº§è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨æé«˜è®¡ç®—æ•ˆç‡å’Œæ™ºèƒ½ä»£ç†èƒ½åŠ›ã€‚å®ƒé‡‡ç”¨ç´§å‡‘çš„æ¶æ„å’Œä»¥STEMä¸ºé‡ç‚¹çš„è®­ç»ƒè¯¾ç¨‹ï¼Œä»é›¶å¼€å§‹é¢„è®­ç»ƒï¼Œä»¥åŸ¹å…»æ¨ç†å’Œè§„åˆ’èƒ½åŠ›ã€‚è¯¥æ¨¡å‹æ”¯æŒ128kçš„é•¿ä¸Šä¸‹æ–‡çª—å£ï¼Œé€‚åˆé•¿æ—¶é—´çš„ä»£ç†å’Œæ¨ç†ä»»åŠ¡ã€‚é€šè¿‡å¤šé˜¶æ®µçš„è®­ç»ƒç­–ç•¥ï¼ŒYoutu-LLMåœ¨å¤æ‚çš„STEMå’Œä»£ç†ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œè¯æ˜äº†è½»é‡çº§æ¨¡å‹ä¹Ÿèƒ½å…·å¤‡å¼ºå¤§çš„å†…åœ¨æ™ºèƒ½èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.24873', 'title': 'Let It Flow: Agentic Crafting on Rock and Roll, Building the ROME Model within an Open Agentic Learning Ecosystem', 'url': 'https://huggingface.co/papers/2512.24873', 'abstract': 'The Agentic Learning Ecosystem (ALE) introduces a principled infrastructure for agent development, combining post-training optimization, sandbox environments, and policy alignment to enhance long-horizon training stability and performance in real-world tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Agentic crafting requires LLMs to operate in real-world environments over multiple turns by taking actions, observing outcomes, and iteratively refining artifacts. Despite its importance, the open-source community lacks a principled, end-to-end ecosystem to streamline agent development. We introduce the Agentic Learning Ecosystem (ALE), a foundational infrastructure that optimizes the production pipeline for agent LLMs. ALE consists of three components: ROLL, a post-training framework for weight optimization; ROCK, a sandbox environment manager for trajectory generation; and iFlow CLI, an agent framework for efficient context engineering. We release ROME (ROME is Obviously an Agentic Model), an open-source agent grounded by ALE and trained on over one million trajectories. Our approach includes data composition protocols for synthesizing complex behaviors and a novel policy optimization algorithm, Interaction-based Policy Alignment (IPA), which assigns credit over semantic interaction chunks rather than individual tokens to improve long-horizon training stability. Empirically, we evaluate ROME within a structured setting and introduce Terminal Bench Pro, a benchmark with improved scale and contamination control. ROME demonstrates strong performance across benchmarks like SWE-bench Verified and Terminal Bench, proving the effectiveness of the ALE infrastructure.', 'score': 32, 'issue_id': 353, 'pub_date': '2026-12-31', 'pub_date_card': {'ru': '31 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 31', 'zh': '12æœˆ31æ—¥'}, 'hash': '2b323e652f7b8183', 'authors': ['Weixun Wang', 'XiaoXiao Xu', 'Wanhe An', 'Fangwen Dai', 'Wei Gao', 'Yancheng He', 'Ju Huang', 'Qiang Ji', 'Hanqi Jin', 'Xiaoyang Li', 'Yang Li', 'Zhongwen Li', 'Shirong Lin', 'Jiashun Liu', 'Zenan Liu', 'Tao Luo', 'Dilxat Muhtar', 'Yuanbin Qu', 'Jiaqiang Shi', 'Qinghui Sun', 'Yingshui Tan', 'Hao Tang', 'Runze Wang', 'Yi Wang', 'Zhaoguo Wang', 'Yanan Wu', 'Shaopan Xiong', 'Binchen Xu', 'Xander Xu', 'Yuchi Xu', 'Qipeng Zhang', 'Xixia Zhang', 'Haizhou Zhao', 'Jie Zhao', 'Shuaibing Zhao', 'Baihui Zheng', 'Jianhui Zheng', 'Suhang Zheng', 'Yanni Zhu', 'Mengze Cai', 'Kerui Cao', 'Xitong Chen', 'Yue Dai', 'Lifan Du', 'Tao Feng', 'Tao He', 'Jin Hu', 'Yijie Hu', 'Ziyu Jiang', 'Cheng Li', 'Xiang Li', 'Jing Liang', 'Chonghuan Liu', 'ZhenDong Liu', 'Haodong Mi', 'Yanhu Mo', 'Junjia Ni', 'Shixin Pei', 'Jingyu Shen', 'XiaoShuai Song', 'Cecilia Wang', 'Chaofan Wang', 'Kangyu Wang', 'Pei Wang', 'Tao Wang', 'Wei Wang', 'Ke Xiao', 'Mingyu Xu', 'Tiange Xu', 'Nan Ya', 'Siran Yang', 'Jianan Ye', 'Yaxing Zang', 'Duo Zhang', 'Junbo Zhang', 'Boren Zheng', 'Wanxi Deng', 'Ling Pan', 'Lin Qu', 'Wenbo Su', 'Jiamang Wang', 'Wei Wang', 'Hu Wei', 'Minggang Wu', 'Cheng Yu', 'Bing Zhao', 'Zhicheng Zheng', 'Bo Zheng'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2512.24873.jpg', 'data': {'categories': ['#alignment', '#open_source', '#synthetic', '#dataset', '#agents', '#training', '#rlhf', '#optimization', '#benchmark'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ­ĞºĞ¾ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Agentic Learning Ecosystem (ALE) â€” ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½ÑƒÑ Ğ¸Ğ½Ñ„Ñ€Ğ°ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLM, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ROLL Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²ĞµÑĞ¾Ğ², Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ ROCK Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ¸ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº iFlow Ğ´Ğ»Ñ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Interaction-based Policy Alignment (IPA), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡Ğ°ĞµÑ‚ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ±Ğ»Ğ¾ĞºĞ°Ğ¼ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ğ°Ñ… Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ’ Ñ€Ğ°Ğ¼ĞºĞ°Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ ROME â€” Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹, Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Terminal Bench Pro Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ¸ ĞºĞ¾Ğ½Ñ‚Ğ°Ğ¼Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… SWE-bench Verified Ğ¸ Terminal Bench Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ñ€Ğ°ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ°.'}, 'en': {'title': 'Streamlining Agent Development with ALE', 'desc': "The Agentic Learning Ecosystem (ALE) provides a structured framework for developing AI agents, particularly large language models (LLMs), to perform tasks in real-world settings. It integrates three key components: ROLL for optimizing model weights post-training, ROCK for managing sandbox environments to generate action trajectories, and iFlow CLI for enhancing context management. The system introduces a novel policy optimization method called Interaction-based Policy Alignment (IPA), which improves training stability by focusing on meaningful interactions rather than individual data points. The open-source agent ROME, built on ALE, showcases significant performance improvements across various benchmarks, demonstrating the framework's effectiveness in agent development."}, 'zh': {'title': 'æ™ºèƒ½ä½“å¼€å‘çš„æ–°åŸºç¡€è®¾æ–½ï¼šAgentic Learning Ecosystem', 'desc': 'Agentic Learning Ecosystem (ALE) æ˜¯ä¸€ä¸ªä¸ºæ™ºèƒ½ä½“å¼€å‘æä¾›çš„åŸºç¡€è®¾æ–½ï¼Œæ—¨åœ¨é€šè¿‡åè®­ç»ƒä¼˜åŒ–ã€æ²™ç›’ç¯å¢ƒå’Œç­–ç•¥å¯¹é½æ¥æé«˜é•¿æ—¶é—´è®­ç»ƒçš„ç¨³å®šæ€§å’Œæ€§èƒ½ã€‚è¯¥ç³»ç»ŸåŒ…æ‹¬ä¸‰ä¸ªä¸»è¦ç»„ä»¶ï¼šROLLç”¨äºæƒé‡ä¼˜åŒ–çš„åè®­ç»ƒæ¡†æ¶ï¼ŒROCKç”¨äºè½¨è¿¹ç”Ÿæˆçš„æ²™ç›’ç¯å¢ƒç®¡ç†å™¨ï¼Œä»¥åŠiFlow CLIç”¨äºé«˜æ•ˆä¸Šä¸‹æ–‡å·¥ç¨‹çš„æ™ºèƒ½ä½“æ¡†æ¶ã€‚æˆ‘ä»¬è¿˜å‘å¸ƒäº†ROMEï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºALEçš„å¼€æºæ™ºèƒ½ä½“ï¼Œç»è¿‡è¶…è¿‡ä¸€ç™¾ä¸‡æ¡è½¨è¿¹çš„è®­ç»ƒã€‚é€šè¿‡å¼•å…¥åŸºäºäº¤äº’çš„ç­–ç•¥å¯¹é½ç®—æ³•ï¼ˆIPAï¼‰ï¼Œæˆ‘ä»¬èƒ½å¤Ÿåœ¨å¤æ‚è¡Œä¸ºåˆæˆå’Œé•¿æ—¶é—´è®­ç»ƒç¨³å®šæ€§æ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.25073', 'title': 'GaMO: Geometry-aware Multi-view Diffusion Outpainting for Sparse-View 3D Reconstruction', 'url': 'https://huggingface.co/papers/2512.25073', 'abstract': 'GaMO enhances sparse-view 3D reconstruction by using geometry-aware multi-view outpainting to improve scene coverage and consistency, achieving state-of-the-art performance with reduced computational cost.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in 3D reconstruction have achieved remarkable progress in high-quality scene capture from dense multi-view imagery, yet struggle when input views are limited. Various approaches, including regularization techniques, semantic priors, and geometric constraints, have been implemented to address this challenge. Latest diffusion-based methods have demonstrated substantial improvements by generating novel views from new camera poses to augment training data, surpassing earlier regularization and prior-based techniques. Despite this progress, we identify three critical limitations in these state-of-the-art approaches: inadequate coverage beyond known view peripheries, geometric inconsistencies across generated views, and computationally expensive pipelines. We introduce GaMO (Geometry-aware Multi-view Outpainter), a framework that reformulates sparse-view reconstruction through multi-view outpainting. Instead of generating new viewpoints, GaMO expands the field of view from existing camera poses, which inherently preserves geometric consistency while providing broader scene coverage. Our approach employs multi-view conditioning and geometry-aware denoising strategies in a zero-shot manner without training. Extensive experiments on Replica and ScanNet++ demonstrate state-of-the-art reconstruction quality across 3, 6, and 9 input views, outperforming prior methods in PSNR and LPIPS, while achieving a 25times speedup over SOTA diffusion-based methods with processing time under 10 minutes. Project page: https://yichuanh.github.io/GaMO/', 'score': 21, 'issue_id': 358, 'pub_date': '2026-12-31', 'pub_date_card': {'ru': '31 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 31', 'zh': '12æœˆ31æ—¥'}, 'hash': 'a90af97d4a64b013', 'authors': ['Yi-Chuan Huang', 'Hao-Jen Chien', 'Chin-Yang Lin', 'Ying-Huan Chen', 'Yu-Lun Liu'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2512.25073.jpg', 'data': {'categories': ['#3d', '#diffusion', '#optimization', '#multimodal'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ»Ñ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ²Ğ¸Ğ´Ğ¾Ğ² Ğ´Ğ»Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½', 'desc': 'GaMO â€” ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½ Ğ¸Ğ· Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ğ¸Ğ´Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ²Ğ¸Ğ´Ğ¾Ğ²ÑƒÑ Ğ´Ğ¾Ğ¿Ğ°Ğ¸Ğ²ĞºÑƒ (outpainting) Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ‚Ğ¾Ñ‡ĞµĞº Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ°. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»Ñ Ğ·Ñ€ĞµĞ½Ğ¸Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… ĞºĞ°Ğ¼ĞµÑ€, Ñ‡Ñ‚Ğ¾ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ¼ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¸Ğ´Ğ°Ğ¼Ğ¸ Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğµ Ğ¿Ğ¾ĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ ÑÑ†ĞµĞ½Ñ‹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸-Ğ¾ÑĞ²ĞµĞ´Ğ¾Ğ¼Ğ»Ñ‘Ğ½Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ´ĞµĞ½Ğ¾Ğ¹Ğ·Ğ¸Ğ½Ğ³Ğ° Ğ² Ğ½ÑƒĞ»ĞµĞ²Ğ¾Ğ¼ Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ (Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ) Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ² 25 Ñ€Ğ°Ğ· Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ². ĞĞ° ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Replica Ğ¸ ScanNet++ GaMO Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼ PSNR Ğ¸ LPIPS Ğ¿Ñ€Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¼ĞµĞ½ĞµĞµ 10 Ğ¼Ğ¸Ğ½ÑƒÑ‚.'}, 'en': {'title': 'Enhancing 3D Reconstruction with Geometry-Aware Outpainting', 'desc': 'GaMO (Geometry-aware Multi-view Outpainter) is a novel framework designed to enhance sparse-view 3D reconstruction by improving scene coverage and consistency. It achieves this by expanding the field of view from existing camera positions rather than generating new viewpoints, which helps maintain geometric accuracy. The method utilizes multi-view conditioning and geometry-aware denoising techniques in a zero-shot manner, eliminating the need for extensive training. Experimental results show that GaMO outperforms previous methods in reconstruction quality while significantly reducing computational costs, achieving a 25 times speedup over state-of-the-art diffusion-based techniques.'}, 'zh': {'title': 'GaMOï¼šæå‡ç¨€ç–è§†å›¾3Dé‡å»ºçš„å‡ ä½•æ„ŸçŸ¥æ–¹æ³•', 'desc': 'GaMOï¼ˆå‡ ä½•æ„ŸçŸ¥å¤šè§†å›¾å¤–æ¨å™¨ï¼‰é€šè¿‡å¤šè§†å›¾å¤–æ¨æŠ€æœ¯å¢å¼ºç¨€ç–è§†å›¾çš„3Dé‡å»ºï¼Œæ”¹å–„äº†åœºæ™¯è¦†ç›–å’Œä¸€è‡´æ€§ã€‚è¯¥æ–¹æ³•ä¸å†ç”Ÿæˆæ–°è§†ç‚¹ï¼Œè€Œæ˜¯æ‰©å±•ç°æœ‰ç›¸æœºä½ç½®çš„è§†é‡ï¼Œä»è€Œä¿æŒå‡ ä½•ä¸€è‡´æ€§å¹¶æä¾›æ›´å¹¿æ³›çš„åœºæ™¯è¦†ç›–ã€‚GaMOé‡‡ç”¨å¤šè§†å›¾æ¡ä»¶å’Œå‡ ä½•æ„ŸçŸ¥å»å™ªç­–ç•¥ï¼Œä»¥é›¶æ ·æœ¬æ–¹å¼è¿›è¡Œï¼Œæ— éœ€è®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGaMOåœ¨é‡å»ºè´¨é‡ä¸Šè¶…è¶Šäº†ä¹‹å‰çš„æ–¹æ³•ï¼Œå¹¶åœ¨å¤„ç†é€Ÿåº¦ä¸Šå®ç°äº†æ˜¾è‘—æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.23380', 'title': 'A unified framework for detecting point and collective anomalies in operating system logs via collaborative transformers', 'url': 'https://huggingface.co/papers/2512.23380', 'abstract': "CoLog, a log anomaly detection framework, employs collaborative transformers and multi-head impressed attention with a modality adaptation layer to achieve high-precision detection of both point and collective anomalies across diverse log modalities.  \t\t\t\t\tAI-generated summary \t\t\t\t Log anomaly detection is crucial for preserving the security of operating systems. Depending on the source of log data collection, various information is recorded in logs that can be considered log modalities. In light of this intuition, unimodal methods often struggle by ignoring the different modalities of log data. Meanwhile, multimodal methods fail to handle the interactions between these modalities. Applying multimodal sentiment analysis to log anomaly detection, we propose CoLog, a framework that collaboratively encodes logs utilizing various modalities. CoLog utilizes collaborative transformers and multi-head impressed attention to learn interactions among several modalities, ensuring comprehensive anomaly detection. To handle the heterogeneity caused by these interactions, CoLog incorporates a modality adaptation layer, which adapts the representations from different log modalities. This methodology enables CoLog to learn nuanced patterns and dependencies within the data, enhancing its anomaly detection capabilities. Extensive experiments demonstrate CoLog's superiority over existing state-of-the-art methods. Furthermore, in detecting both point and collective anomalies, CoLog achieves a mean precision of 99.63%, a mean recall of 99.59%, and a mean F1 score of 99.61% across seven benchmark datasets for log-based anomaly detection. The comprehensive detection capabilities of CoLog make it highly suitable for cybersecurity, system monitoring, and operational efficiency. CoLog represents a significant advancement in log anomaly detection, providing a sophisticated and effective solution to point and collective anomaly detection through a unified framework and a solution to the complex challenges automatic log data analysis poses. We also provide the implementation of CoLog at https://github.com/NasirzadehMoh/CoLog.", 'score': 17, 'issue_id': 366, 'pub_date': '2026-12-29', 'pub_date_card': {'ru': '29 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 29', 'zh': '12æœˆ29æ—¥'}, 'hash': '60dd192b220eb6aa', 'authors': ['Mohammad Nasirzadeh', 'Jafar Tahmoresnezhad', 'Parviz Rashidi-Khazaee'], 'affiliations': ['Urmia University of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2512.23380.jpg', 'data': {'categories': ['#open_source', '#architecture', '#benchmark', '#security', '#multimodal'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ¡Ğ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ»Ğ¾Ğ³Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹', 'desc': 'CoLog â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹ Ğ² Ğ»Ğ¾Ğ³Ğ°Ñ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸Ñ‡Ğ°ÑÑ‰Ğ¸Ğ¼Ğ¸ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°Ğ¼Ğ¸ Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ³Ğ¾Ğ»Ğ¾Ğ²Ñ‹Ğ¼ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ»Ğ¾Ğ³Ğ¾Ğ² (Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ñ‚Ğ¸Ğ¿Ñ‹ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…) Ğ¸ Ğ²Ñ‹ÑĞ²Ğ»ÑÑ‚ÑŒ ĞºĞ°Ğº Ñ‚Ğ¾Ñ‡ĞµÑ‡Ğ½Ñ‹Ğµ, Ñ‚Ğ°Ğº Ğ¸ ĞºĞ¾Ğ»Ğ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¸. Ğ”Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğº Ğ³ĞµÑ‚ĞµÑ€Ğ¾Ğ³ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞ»Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. ĞĞ° ÑĞµĞ¼Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº-Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ… CoLog Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ğ¿ĞµÑ‡Ğ°Ñ‚Ğ»ÑÑÑ‰Ğ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ²: Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ 99.63%, Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚Ñƒ 99.59% Ğ¸ F1-Ğ¼ĞµÑ€Ñƒ 99.61%.'}, 'en': {'title': 'CoLog: Revolutionizing Log Anomaly Detection with Collaborative Transformers', 'desc': 'CoLog is a log anomaly detection framework that utilizes collaborative transformers and multi-head impressed attention to effectively identify both point and collective anomalies in diverse log modalities. It addresses the limitations of unimodal and multimodal methods by incorporating a modality adaptation layer, which helps in managing the interactions between different log data types. This innovative approach allows CoLog to learn complex patterns and dependencies, significantly improving its detection accuracy. With impressive performance metrics, including a mean precision of 99.63%, CoLog stands out as a powerful tool for enhancing cybersecurity and operational efficiency.'}, 'zh': {'title': 'CoLogï¼šé«˜æ•ˆçš„æ—¥å¿—å¼‚å¸¸æ£€æµ‹æ¡†æ¶', 'desc': 'CoLogæ˜¯ä¸€ä¸ªæ—¥å¿—å¼‚å¸¸æ£€æµ‹æ¡†æ¶ï¼Œé‡‡ç”¨åä½œå˜æ¢å™¨å’Œå¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼Œç»“åˆæ¨¡æ€é€‚åº”å±‚ï¼Œå®ç°å¯¹å¤šç§æ—¥å¿—æ¨¡æ€ä¸­ç‚¹å¼‚å¸¸å’Œé›†ä½“å¼‚å¸¸çš„é«˜ç²¾åº¦æ£€æµ‹ã€‚è¯¥æ¡†æ¶é€šè¿‡ååŒç¼–ç ä¸åŒæ¨¡æ€çš„æ—¥å¿—ï¼Œå…‹æœäº†å•æ¨¡æ€æ–¹æ³•å¿½è§†æ—¥å¿—æ•°æ®å¤šæ ·æ€§çš„é—®é¢˜ã€‚CoLogèƒ½å¤Ÿå­¦ä¹ æ¨¡æ€ä¹‹é—´çš„äº¤äº’ï¼Œé€‚åº”ä¸åŒæ—¥å¿—æ¨¡æ€çš„è¡¨ç¤ºï¼Œä»è€Œæé«˜å¼‚å¸¸æ£€æµ‹çš„èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCoLogåœ¨ä¸ƒä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå¹³å‡ç²¾åº¦è¾¾åˆ°99.63%ï¼Œä¸ºç½‘ç»œå®‰å…¨å’Œç³»ç»Ÿç›‘æ§æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.25070', 'title': 'Scaling Open-Ended Reasoning to Predict the Future', 'url': 'https://huggingface.co/papers/2512.25070', 'abstract': 'High-stakes decision making involves reasoning under uncertainty about the future. In this work, we train language models to make predictions on open-ended forecasting questions. To scale up training data, we synthesize novel forecasting questions from global events reported in daily news, using a fully automated, careful curation recipe. We train the Qwen3 thinking models on our dataset, OpenForesight. To prevent leakage of future information during training and evaluation, we use an offline news corpus, both for data generation and retrieval in our forecasting system. Guided by a small validation set, we show the benefits of retrieval, and an improved reward function for reinforcement learning (RL). Once we obtain our final forecasting system, we perform held-out testing between May to August 2025. Our specialized model, OpenForecaster 8B, matches much larger proprietary models, with our training improving the accuracy, calibration, and consistency of predictions. We find calibration improvements from forecasting training generalize across popular benchmarks. We open-source all our models, code, and data to make research on language model forecasting broadly accessible.', 'score': 12, 'issue_id': 353, 'pub_date': '2026-12-31', 'pub_date_card': {'ru': '31 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 31', 'zh': '12æœˆ31æ—¥'}, 'hash': '3b992c58072db0d3', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#optimization', '#open_source', '#dataset', '#training', '#leakage', '#small_models', '#reasoning', '#rl', '#synthetic', '#data', '#benchmark'], 'emoji': 'ğŸ”®', 'ru': {'title': 'Ğ¯Ğ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ°Ğº ÑĞºÑĞ¿ĞµÑ€Ñ‚Ñ‹ Ğ¿Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ±ÑƒĞ´ÑƒÑ‰ĞµĞ³Ğ¾', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´ĞµĞ»Ğ°Ñ‚ÑŒ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ OpenForesight Ñ Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ¸Ğ· Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹. Ğ”Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ ÑƒÑ‚ĞµÑ‡ĞºĞ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ Ğ±ÑƒĞ´ÑƒÑ‰ĞµĞ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ğ¹ ĞºĞ¾Ñ€Ğ¿ÑƒÑ Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚ĞµĞ¹ ĞºĞ°Ğº Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ğµ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ OpenForecaster 8B Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ reinforcement learning Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ¹ Ñ„ÑƒĞ½ĞºÑ†Ğ¸ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ, ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºÑƒ Ğ¸ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ»Ğ¸ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ´, Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ñ‹, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ»Ñ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾Ğ³Ğ¾ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ğ°.'}, 'en': {'title': 'Empowering Predictions: OpenForesight for Uncertain Futures', 'desc': 'This paper focuses on enhancing decision-making in uncertain situations by training language models to predict outcomes for open-ended forecasting questions. The authors create a large dataset called OpenForesight by synthesizing forecasting questions from global news events, ensuring that future information does not leak during training. They utilize reinforcement learning with an improved reward function and a validation set to refine their forecasting system. The resulting model, OpenForecaster 8B, demonstrates competitive performance against larger models, showing better accuracy and consistency in predictions, and the authors make their resources available for further research.'}, 'zh': {'title': 'å¼€æ”¾å¼é¢„æµ‹ï¼šæå‡è¯­è¨€æ¨¡å‹çš„å†³ç­–èƒ½åŠ›', 'desc': 'æœ¬ç ”ç©¶æ—¨åœ¨è®­ç»ƒè¯­è¨€æ¨¡å‹ä»¥åº”å¯¹é«˜é£é™©å†³ç­–ä¸­çš„ä¸ç¡®å®šæ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å¼€æ”¾å¼é¢„æµ‹é—®é¢˜ä¸Šã€‚æˆ‘ä»¬é€šè¿‡ä»æ¯æ—¥æ–°é—»ä¸­æå–å…¨çƒäº‹ä»¶ï¼Œè‡ªåŠ¨åˆæˆæ–°çš„é¢„æµ‹é—®é¢˜ï¼Œä»¥æ‰©å¤§è®­ç»ƒæ•°æ®é›†ã€‚æˆ‘ä»¬ä½¿ç”¨ç¦»çº¿æ–°é—»è¯­æ–™åº“æ¥é˜²æ­¢åœ¨è®­ç»ƒå’Œè¯„ä¼°è¿‡ç¨‹ä¸­æ³„éœ²æœªæ¥ä¿¡æ¯ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šè®­ç»ƒæˆ‘ä»¬çš„é¢„æµ‹ç³»ç»Ÿã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬çš„ä¸“ç”¨æ¨¡å‹OpenForecaster 8Båœ¨å‡†ç¡®æ€§ã€æ ¡å‡†å’Œä¸€è‡´æ€§æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œèƒ½å¤Ÿä¸æ›´å¤§è§„æ¨¡çš„ä¸“æœ‰æ¨¡å‹ç›¸åª²ç¾ã€‚'}}, 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2512.24551', 'title': 'PhyGDPO: Physics-Aware Groupwise Direct Preference Optimization for Physically Consistent Text-to-Video Generation', 'url': 'https://huggingface.co/papers/2512.24551', 'abstract': 'Recent advances in text-to-video (T2V) generation have achieved good visual quality, yet synthesizing videos that faithfully follow physical laws remains an open challenge. Existing methods mainly based on graphics or prompt extension struggle to generalize beyond simple simulated environments or learn implicit physical reasoning. The scarcity of training data with rich physics interactions and phenomena is also a problem. In this paper, we first introduce a Physics-Augmented video data construction Pipeline, PhyAugPipe, that leverages a vision-language model (VLM) with chain-of-thought reasoning to collect a large-scale training dataset, PhyVidGen-135K. Then we formulate a principled Physics-aware Groupwise Direct Preference Optimization, PhyGDPO, framework that builds upon the groupwise Plackett-Luce probabilistic model to capture holistic preferences beyond pairwise comparisons. In PhyGDPO, we design a Physics-Guided Rewarding (PGR) scheme that embeds VLM-based physics rewards to steer optimization toward physical consistency. We also propose a LoRA-Switch Reference (LoRA-SR) scheme that eliminates memory-heavy reference duplication for efficient training. Experiments show that our method significantly outperforms state-of-the-art open-source methods on PhyGenBench and VideoPhy2. Please check our project page at https://caiyuanhao1998.github.io/project/PhyGDPO for more video results. Our code, models, and data will be released at https://github.com/caiyuanhao1998/Open-PhyGDPO', 'score': 12, 'issue_id': 353, 'pub_date': '2026-12-31', 'pub_date_card': {'ru': '31 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 31', 'zh': '12æœˆ31æ—¥'}, 'hash': '581846a1ed6df832', 'authors': ['Yuanhao Cai', 'Kunpeng Li', 'Menglin Jia', 'Jialiang Wang', 'Junzhe Sun', 'Feng Liang', 'Weifeng Chen', 'Felix Juefei-Xu', 'Chu Wang', 'Ali Thabet', 'Xiaoliang Dai', 'Xuan Ju', 'Alan Yuille', 'Ji Hou'], 'affiliations': ['CUHK', 'Johns Hopkins University', 'Meta BizAI', 'Meta Superintelligence Labs'], 'pdf_title_img': 'assets/pdf/title_img/2512.24551.jpg', 'data': {'categories': ['#multimodal', '#dataset', '#training', '#rlhf', '#video', '#data'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ¤Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾ÑĞ¾Ğ·Ğ½Ğ°Ğ½Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ° Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹', 'desc': 'Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ñ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ PhyVidGen-135K, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ vision-language model Ğ¸ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºÑƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑĞ±Ğ¾Ñ€Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑĞ¼Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ PhyGDPO, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Plackett-Luce Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑ‡Ñ‘Ñ‚Ğ° Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´Ğ°ÑÑ‰ÑƒÑ ÑÑ…ĞµĞ¼Ñƒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ vision-language model Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ LoRA-Switch Ğ´Ğ»Ñ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğº Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸.'}, 'en': {'title': 'Generating Physically Consistent Videos with Advanced Optimization', 'desc': 'This paper addresses the challenge of generating videos that adhere to physical laws in text-to-video (T2V) generation. The authors introduce a new data construction pipeline called PhyAugPipe, which uses a vision-language model to create a large dataset with rich physics interactions. They also propose a novel optimization framework, PhyGDPO, that incorporates a physics-guided rewarding system to enhance the physical consistency of generated videos. Experimental results demonstrate that their approach significantly improves performance compared to existing methods.'}, 'zh': {'title': 'ç‰©ç†ä¸€è‡´æ€§çš„è§†é¢‘ç”Ÿæˆæ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§ç‰©ç†å¢å¼ºçš„è§†é¢‘æ•°æ®æ„å»ºç®¡é“PhyAugPipeï¼Œåˆ©ç”¨è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å’Œæ€ç»´é“¾æ¨ç†æ¥æ”¶é›†å¤§è§„æ¨¡è®­ç»ƒæ•°æ®é›†PhyVidGen-135Kã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§åŸºäºç‰©ç†çš„ç¾¤ä½“ç›´æ¥åå¥½ä¼˜åŒ–æ¡†æ¶PhyGDPOï¼Œæ—¨åœ¨æ•æ‰æ•´ä½“åå¥½å¹¶è¶…è¶Šç®€å•çš„æˆå¯¹æ¯”è¾ƒã€‚PhyGDPOä¸­è®¾è®¡çš„ç‰©ç†å¼•å¯¼å¥–åŠ±æœºåˆ¶ï¼ˆPGRï¼‰é€šè¿‡åµŒå…¥VLMåŸºç¡€çš„ç‰©ç†å¥–åŠ±æ¥å¼•å¯¼ä¼˜åŒ–ï¼Œç¡®ä¿ç”Ÿæˆè§†é¢‘çš„ç‰©ç†ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºçš„LoRA-Switch Referenceæ–¹æ¡ˆæœ‰æ•ˆå‡å°‘äº†å†…å­˜å ç”¨ï¼Œæé«˜äº†è®­ç»ƒæ•ˆç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.24210', 'title': 'GR-Dexter Technical Report', 'url': 'https://huggingface.co/papers/2512.24210', 'abstract': 'GR-Dexter introduces a hardware-model-data framework for bimanual dexterous-hand robot manipulation using vision-language-action models, combining teleoperation data and multimodal datasets to achieve robust generalization.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-language-action (VLA) models have enabled language-conditioned, long-horizon robot manipulation, but most existing systems are limited to grippers. Scaling VLA policies to bimanual robots with high degree-of-freedom (DoF) dexterous hands remains challenging due to the expanded action space, frequent hand-object occlusions, and the cost of collecting real-robot data. We present GR-Dexter, a holistic hardware-model-data framework for VLA-based generalist manipulation on a bimanual dexterous-hand robot. Our approach combines the design of a compact 21-DoF robotic hand, an intuitive bimanual teleoperation system for real-robot data collection, and a training recipe that leverages teleoperated robot trajectories together with large-scale vision-language and carefully curated cross-embodiment datasets. Across real-world evaluations spanning long-horizon everyday manipulation and generalizable pick-and-place, GR-Dexter achieves strong in-domain performance and improved robustness to unseen objects and unseen instructions. We hope GR-Dexter serves as a practical step toward generalist dexterous-hand robotic manipulation.', 'score': 11, 'issue_id': 353, 'pub_date': '2026-12-30', 'pub_date_card': {'ru': '30 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 30', 'zh': '12æœˆ30æ—¥'}, 'hash': '999fb5bf63af5286', 'authors': ['Ruoshi Wen', 'Guangzeng Chen', 'Zhongren Cui', 'Min Du', 'Yang Gou', 'Zhigang Han', 'Liqun Huang', 'Mingyu Lei', 'Yunfei Li', 'Zhuohang Li', 'Wenlei Liu', 'Yuxiao Liu', 'Xiao Ma', 'Hao Niu', 'Yutao Ouyang', 'Zeyu Ren', 'Haixin Shi', 'Wei Xu', 'Haoxiang Zhang', 'Jiajun Zhang', 'Xiao Zhang', 'Liwei Zheng', 'Weiheng Zhong', 'Yifei Zhou', 'Zhengming Zhu', 'Hang Li'], 'affiliations': ['ByteDance'], 'pdf_title_img': 'assets/pdf/title_img/2512.24210.jpg', 'data': {'categories': ['#robotics', '#dataset', '#multimodal', '#training'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞĞ±ÑƒÑ‡Ğ°ĞµĞ¼Ğ°Ñ Ğ»Ğ¾Ğ²ĞºĞ¾ÑÑ‚ÑŒ: ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ´Ğ²ÑƒÑ€ÑƒÑ‡Ğ½Ğ°Ñ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸', 'desc': 'GR-Dexter Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ°Ğ¿Ğ¿Ğ°Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ, Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ²ÑƒÑ€ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°-Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ° Ñ Ğ»Ğ¾Ğ²ĞºĞ¸Ğ¼Ğ¸ ĞºĞ¸ÑÑ‚ÑĞ¼Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ñ‚ĞµĞ»ĞµĞ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ğ¼Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ĞºÑ€Ğ¾ÑÑ-Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰Ñ‘Ğ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½ÑƒÑ 21-ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½ÑƒÑ ĞºĞ¸ÑÑ‚ÑŒ, Ğ¸Ğ½Ñ‚ÑƒĞ¸Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ñ‚ĞµĞ»ĞµĞ¾Ğ¿Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑĞ±Ğ¾Ñ€Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ€ĞµÑ†ĞµĞ¿Ñ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹ Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑĞ¸Ğ»ÑŒĞ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¾ Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğº Ğ½ĞµĞ²Ğ¸Ğ´Ğ¸Ğ¼Ñ‹Ğ¼ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼ Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ´Ğ¾Ğ»Ğ³Ğ¾Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ğ½Ñ‹Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹.'}, 'en': {'title': 'Empowering Bimanual Robots with Vision-Language Action!', 'desc': 'GR-Dexter is a framework designed to enhance the manipulation capabilities of bimanual dexterous-hand robots using vision-language-action (VLA) models. It addresses challenges such as the complex action space and occlusions that arise with high degree-of-freedom robotic hands. By integrating teleoperation data with multimodal datasets, GR-Dexter enables robust generalization in robot manipulation tasks. The framework demonstrates strong performance in real-world scenarios, making it a significant advancement in the field of robotic manipulation.'}, 'zh': {'title': 'GR-Dexterï¼šåŒæ‰‹çµå·§æœºå™¨äººæ“ä½œçš„æ–°çªç ´', 'desc': 'GR-Dexteræå‡ºäº†ä¸€ç§ç¡¬ä»¶-æ¨¡å‹-æ•°æ®æ¡†æ¶ï¼Œç”¨äºåŒæ‰‹çµå·§æœºå™¨äººæ“ä½œï¼Œç»“åˆäº†è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ã€‚è¯¥æ¡†æ¶é€šè¿‡æ•´åˆé¥æ“ä½œæ•°æ®å’Œå¤šæ¨¡æ€æ•°æ®é›†ï¼Œå®ç°äº†å¯¹å¤æ‚æ“ä½œçš„å¼ºå¤§æ³›åŒ–èƒ½åŠ›ã€‚GR-Dexterçš„è®¾è®¡åŒ…æ‹¬ä¸€ä¸ªç´§å‡‘çš„21è‡ªç”±åº¦æœºå™¨äººæ‰‹å’Œç›´è§‚çš„åŒæ‰‹é¥æ“ä½œç³»ç»Ÿï¼Œä»¥ä¾¿æ”¶é›†çœŸå®æœºå™¨äººæ•°æ®ã€‚é€šè¿‡åœ¨çœŸå®ç¯å¢ƒä¸­çš„è¯„ä¼°ï¼ŒGR-Dexteråœ¨æ—¥å¸¸æ“ä½œå’Œå¯æ³›åŒ–çš„æŠ“å–-æ”¾ç½®ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå¢å¼ºäº†å¯¹æœªçŸ¥ç‰©ä½“å’ŒæŒ‡ä»¤çš„é²æ£’æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.23343', 'title': 'AI Meets Brain: Memory Systems from Cognitive Neuroscience to Autonomous Agents', 'url': 'https://huggingface.co/papers/2512.23343', 'abstract': 'Memory serves as the pivotal nexus bridging past and future, providing both humans and AI systems with invaluable concepts and experience to navigate complex tasks. Recent research on autonomous agents has increasingly focused on designing efficient memory workflows by drawing on cognitive neuroscience. However, constrained by interdisciplinary barriers, existing works struggle to assimilate the essence of human memory mechanisms. To bridge this gap, we systematically synthesizes interdisciplinary knowledge of memory, connecting insights from cognitive neuroscience with LLM-driven agents. Specifically, we first elucidate the definition and function of memory along a progressive trajectory from cognitive neuroscience through LLMs to agents. We then provide a comparative analysis of memory taxonomy, storage mechanisms, and the complete management lifecycle from both biological and artificial perspectives. Subsequently, we review the mainstream benchmarks for evaluating agent memory. Additionally, we explore memory security from dual perspectives of attack and defense. Finally, we envision future research directions, with a focus on multimodal memory systems and skill acquisition.', 'score': 10, 'issue_id': 358, 'pub_date': '2026-12-29', 'pub_date_card': {'ru': '29 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 29', 'zh': '12æœˆ29æ—¥'}, 'hash': 'c907d7b6498bd21c', 'authors': ['Jiafeng Liang', 'Hao Li', 'Chang Li', 'Jiaqi Zhou', 'Shixin Jiang', 'Zekun Wang', 'Changkai Ji', 'Zhihao Zhu', 'Runxuan Liu', 'Tao Ren', 'Jinlan Fu', 'See-Kiong Ng', 'Xia Liang', 'Ming Liu', 'Bing Qin'], 'affiliations': ['Fudan University, China', 'Harbin Institute of Technology, China', 'National University of Singapore, Singapore', 'Peking University, China'], 'pdf_title_img': 'assets/pdf/title_img/2512.23343.jpg', 'data': {'categories': ['#agents', '#benchmark', '#multimodal'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞŸĞ°Ğ¼ÑÑ‚ÑŒ ĞºĞ°Ğº Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°: Ğ¾Ñ‚ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ°ÑƒĞºĞ¸ Ğº Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLM', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´ÑÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ² Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ñ…, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ·Ğ½Ğ°Ğ½Ğ¸ÑÑ… ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ°ÑƒĞºĞ¸ Ğ¸ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ°Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½Ğ¸ Ğ¸Ğ·ÑƒÑ‡Ğ°ÑÑ‚ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ, Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¸ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ñ Ğ±Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¼ Ğ¶Ğ¸Ğ·Ğ½ĞµĞ½Ğ½Ğ¾Ğ¼ Ñ†Ğ¸ĞºĞ»Ğµ. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ñ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ°Ñ‚Ğ°Ğº Ğ¸ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹, ÑĞ¾ÑÑ€ĞµĞ´Ğ¾Ñ‚Ğ¾Ñ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¾Ğ±Ñ€ĞµÑ‚ĞµĞ½Ğ¸Ğ¸ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ².'}, 'en': {'title': 'Bridging Human Memory and AI: A New Frontier in Autonomous Agents', 'desc': 'This paper explores the integration of human memory mechanisms into the design of autonomous agents, particularly those driven by large language models (LLMs). It synthesizes knowledge from cognitive neuroscience to enhance the understanding of memory functions and workflows in AI systems. The authors provide a comparative analysis of memory types, storage methods, and management processes from both biological and artificial viewpoints. They also discuss the evaluation of agent memory and propose future research directions, emphasizing the importance of multimodal memory systems and skill acquisition.'}, 'zh': {'title': 'è®°å¿†ï¼šè¿æ¥è¿‡å»ä¸æœªæ¥çš„æ¡¥æ¢', 'desc': 'æœ¬æ–‡æ¢è®¨äº†è®°å¿†åœ¨äººå·¥æ™ºèƒ½ç³»ç»Ÿä¸­çš„é‡è¦æ€§ï¼Œå¼ºè°ƒäº†è®°å¿†å¦‚ä½•å¸®åŠ©è‡ªä¸»æ™ºèƒ½ä½“å®Œæˆå¤æ‚ä»»åŠ¡ã€‚ç ”ç©¶ç»“åˆäº†è®¤çŸ¥ç¥ç»ç§‘å­¦ä¸å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œç³»ç»Ÿæ€§åœ°åˆ†æäº†è®°å¿†çš„å®šä¹‰ã€åŠŸèƒ½åŠå…¶åœ¨ç”Ÿç‰©å’Œäººå·¥æ™ºèƒ½ä¸­çš„ç®¡ç†ç”Ÿå‘½å‘¨æœŸã€‚æ–‡ç« è¿˜æ¯”è¾ƒäº†è®°å¿†çš„åˆ†ç±»ã€å­˜å‚¨æœºåˆ¶ï¼Œå¹¶å›é¡¾äº†è¯„ä¼°æ™ºèƒ½ä½“è®°å¿†çš„ä¸»æµåŸºå‡†ã€‚æœ€åï¼Œä½œè€…å±•æœ›äº†æœªæ¥çš„ç ”ç©¶æ–¹å‘ï¼Œç‰¹åˆ«æ˜¯å¤šæ¨¡æ€è®°å¿†ç³»ç»Ÿå’ŒæŠ€èƒ½è·å–ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.23988', 'title': 'Fantastic Reasoning Behaviors and Where to Find Them: Unsupervised Discovery of the Reasoning Process', 'url': 'https://huggingface.co/papers/2512.23988', 'abstract': "An unsupervised framework using sparse auto-encoders identifies and controls interpretable reasoning behaviors in large language models through disentangled latent vectors.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite the growing reasoning capabilities of recent large language models (LLMs), their internal mechanisms during the reasoning process remain underexplored. Prior approaches often rely on human-defined concepts (e.g., overthinking, reflection) at the word level to analyze reasoning in a supervised manner. However, such methods are limited, as it is infeasible to capture the full spectrum of potential reasoning behaviors, many of which are difficult to define in token space. In this work, we propose an unsupervised framework (namely, RISE: Reasoning behavior Interpretability via Sparse auto-Encoder) for discovering reasoning vectors, which we define as directions in the activation space that encode distinct reasoning behaviors. By segmenting chain-of-thought traces into sentence-level 'steps' and training sparse auto-encoders (SAEs) on step-level activations, we uncover disentangled features corresponding to interpretable behaviors such as reflection and backtracking. Visualization and clustering analyses show that these behaviors occupy separable regions in the decoder column space. Moreover, targeted interventions on SAE-derived vectors can controllably amplify or suppress specific reasoning behaviors, altering inference trajectories without retraining. Beyond behavior-specific disentanglement, SAEs capture structural properties such as response length, revealing clusters of long versus short reasoning traces. More interestingly, SAEs enable the discovery of novel behaviors beyond human supervision. We demonstrate the ability to control response confidence by identifying confidence-related vectors in the SAE decoder space. These findings underscore the potential of unsupervised latent discovery for both interpreting and controllably steering reasoning in LLMs.", 'score': 6, 'issue_id': 353, 'pub_date': '2026-12-30', 'pub_date_card': {'ru': '30 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 30', 'zh': '12æœˆ30æ—¥'}, 'hash': 'eee1ab8177524c42', 'authors': ['Zhenyu Zhang', 'Shujian Zhang', 'John Lambert', 'Wenxuan Zhou', 'Zhangyang Wang', 'Mingqing Chen', 'Andrew Hard', 'Rajiv Mathews', 'Lun Wang'], 'affiliations': ['Google DeepMind', 'The University of Texas at Austin'], 'pdf_title_img': 'assets/pdf/title_img/2512.23988.jpg', 'data': {'categories': ['#reasoning', '#interpretability'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ Ğ°ÑĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² LLM Ñ‡ĞµÑ€ĞµĞ· Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ½ĞµĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° RISE Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ÑÑ‚ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ğ¼Ñ‹ÑĞ»ĞµĞ¹ Ğ½Ğ° Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑˆĞ°Ğ³Ğ¸ Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ SAE Ğ½Ğ° Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸ÑÑ… ÑƒÑ€Ğ¾Ğ²Ğ½Ñ ÑˆĞ°Ğ³Ğ¾Ğ², Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ñ‚ÑŒ Ğ´Ğ¸sentangled Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸, ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¼ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸ÑĞ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‚ Ñ†ĞµĞ»ĞµĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾ ÑƒÑĞ¸Ğ»Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ»Ğ¸ Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ÑÑ‚ÑŒ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ, Ğ²Ñ‹Ñ…Ğ¾Ğ´ÑÑ‰Ğ¸Ğµ Ğ·Ğ° Ñ€Ğ°Ğ¼ĞºĞ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ½Ğ°Ğ´Ğ·Ğ¾Ñ€Ğ°, Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ñ….'}, 'en': {'title': 'Unleashing Interpretability: Controlling Reasoning in LLMs with Sparse Auto-Encoders', 'desc': "This paper introduces an unsupervised framework called RISE, which uses sparse auto-encoders to identify and control reasoning behaviors in large language models (LLMs). Unlike previous methods that rely on predefined concepts, RISE discovers reasoning vectors that represent distinct behaviors in the model's activation space. By analyzing sentence-level steps in reasoning, the framework reveals interpretable features such as reflection and backtracking, which can be visualized and clustered. Additionally, RISE allows for targeted interventions to manipulate these reasoning behaviors, enhancing our understanding and control over LLMs without the need for retraining."}, 'zh': {'title': 'æ— ç›‘ç£æ¨ç†è¡Œä¸ºæ§åˆ¶çš„åˆ›æ–°æ¡†æ¶', 'desc': 'æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ— ç›‘ç£æ¡†æ¶ï¼Œåˆ©ç”¨ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEï¼‰æ¥è¯†åˆ«å’Œæ§åˆ¶å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„å¯è§£é‡Šæ¨ç†è¡Œä¸ºã€‚é€šè¿‡å°†æ¨ç†è¿‡ç¨‹åˆ†è§£ä¸ºå¥å­çº§çš„â€œæ­¥éª¤â€ï¼Œå¹¶åœ¨è¿™äº›æ­¥éª¤çš„æ¿€æ´»ä¸Šè®­ç»ƒç¨€ç–è‡ªç¼–ç å™¨ï¼Œæˆ‘ä»¬å‘ç°äº†ä¸åæ€å’Œå›æº¯ç­‰å¯è§£é‡Šè¡Œä¸ºç›¸å¯¹åº”çš„è§£è€¦ç‰¹å¾ã€‚å¯è§†åŒ–å’Œèšç±»åˆ†æè¡¨æ˜ï¼Œè¿™äº›è¡Œä¸ºåœ¨è§£ç å™¨åˆ—ç©ºé—´ä¸­å æ®å¯åˆ†ç¦»çš„åŒºåŸŸã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥é€šè¿‡å¯¹SAEå¯¼å‡ºçš„å‘é‡è¿›è¡Œæœ‰é’ˆå¯¹æ€§çš„å¹²é¢„ï¼Œæ¥å¯æ§åœ°æ”¾å¤§æˆ–æŠ‘åˆ¶ç‰¹å®šçš„æ¨ç†è¡Œä¸ºï¼Œè€Œæ— éœ€é‡æ–°è®­ç»ƒæ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.25075', 'title': 'SpaceTimePilot: Generative Rendering of Dynamic Scenes Across Space and Time', 'url': 'https://huggingface.co/papers/2512.25075', 'abstract': "SpaceTimePilot is a video diffusion model enabling independent control of spatial viewpoint and temporal motion through a time-embedding mechanism, temporal-warping training, and a synthetic dataset for precise space-time disentanglement.  \t\t\t\t\tAI-generated summary \t\t\t\t We present SpaceTimePilot, a video diffusion model that disentangles space and time for controllable generative rendering. Given a monocular video, SpaceTimePilot can independently alter the camera viewpoint and the motion sequence within the generative process, re-rendering the scene for continuous and arbitrary exploration across space and time. To achieve this, we introduce an effective animation time-embedding mechanism in the diffusion process, allowing explicit control of the output video's motion sequence with respect to that of the source video. As no datasets provide paired videos of the same dynamic scene with continuous temporal variations, we propose a simple yet effective temporal-warping training scheme that repurposes existing multi-view datasets to mimic temporal differences. This strategy effectively supervises the model to learn temporal control and achieve robust space-time disentanglement. To further enhance the precision of dual control, we introduce two additional components: an improved camera-conditioning mechanism that allows altering the camera from the first frame, and CamxTime, the first synthetic space-and-time full-coverage rendering dataset that provides fully free space-time video trajectories within a scene. Joint training on the temporal-warping scheme and the CamxTime dataset yields more precise temporal control. We evaluate SpaceTimePilot on both real-world and synthetic data, demonstrating clear space-time disentanglement and strong results compared to prior work. Project page: https://zheninghuang.github.io/Space-Time-Pilot/ Code: https://github.com/ZheningHuang/spacetimepilot", 'score': 5, 'issue_id': 353, 'pub_date': '2026-12-31', 'pub_date_card': {'ru': '31 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 31', 'zh': '12æœˆ31æ—¥'}, 'hash': 'a5a1be5c32218359', 'authors': ['Zhening Huang', 'Hyeonho Jeong', 'Xuelin Chen', 'Yulia Gryaditskaya', 'Tuanfeng Y. Wang', 'Joan Lasenby', 'Chun-Hao Huang'], 'affiliations': ['Adobe Research', 'University of Cambridge'], 'pdf_title_img': 'assets/pdf/title_img/2512.25075.jpg', 'data': {'categories': ['#diffusion', '#dataset', '#architecture', '#video', '#synthetic'], 'emoji': 'ğŸ¬', 'ru': {'title': 'ĞĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ĞºĞ°Ğ¼ĞµÑ€Ğ¾Ğ¹ Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ĞµĞ¼ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµÑ€ĞµĞ· Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸', 'desc': 'SpaceTimePilot â€” ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ Ğ¸Ğ·Ğ¼ĞµĞ½ÑÑ‚ÑŒ Ñ‚Ğ¾Ñ‡ĞºÑƒ Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ° ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² ÑÑ†ĞµĞ½Ğµ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñƒ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ ÑÑ…ĞµĞ¼Ñƒ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ´ĞµÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿ĞµÑ€ĞµĞ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ²Ğ¸Ğ´Ğ¾Ğ²Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ Ğ¸Ğ¼Ğ¸Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¹. Ğ”Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ½Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ CamxTime Ñ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾ĞºÑ€Ñ‹Ñ‚Ğ¸ĞµĞ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Mastering Space and Time in Video Generation', 'desc': 'SpaceTimePilot is a novel video diffusion model that allows users to independently manipulate both the viewpoint and motion of a scene in generated videos. It utilizes a time-embedding mechanism and a unique temporal-warping training approach to achieve effective space-time disentanglement. By leveraging a synthetic dataset called CamxTime, the model can generate videos with continuous and arbitrary changes in both space and time. The results show that SpaceTimePilot outperforms previous methods in terms of precision and control over video generation.'}, 'zh': {'title': 'ç‹¬ç«‹æ§åˆ¶æ—¶ç©ºçš„åˆ›æ–°è§†é¢‘ç”Ÿæˆæ¨¡å‹', 'desc': 'SpaceTimePilotæ˜¯ä¸€ç§è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œèƒ½å¤Ÿç‹¬ç«‹æ§åˆ¶ç©ºé—´è§†è§’å’Œæ—¶é—´è¿åŠ¨ã€‚é€šè¿‡æ—¶é—´åµŒå…¥æœºåˆ¶å’Œæ—¶é—´æ‰­æ›²è®­ç»ƒï¼Œè¯¥æ¨¡å‹å®ç°äº†ç²¾ç¡®çš„æ—¶ç©ºè§£è€¦ã€‚å®ƒå¯ä»¥æ ¹æ®å•ç›®è§†é¢‘ï¼Œé‡æ–°æ¸²æŸ“åœºæ™¯ï¼Œå…è®¸ç”¨æˆ·åœ¨ç©ºé—´å’Œæ—¶é—´ä¸Šè¿›è¡Œè¿ç»­çš„æ¢ç´¢ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†CamxTimeæ•°æ®é›†ï¼Œä»¥å¢å¼ºæ¨¡å‹çš„åŒé‡æ§åˆ¶ç²¾åº¦ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.23851', 'title': 'Pretraining Frame Preservation in Autoregressive Video Memory Compression', 'url': 'https://huggingface.co/papers/2512.23851', 'abstract': 'We present PFP, a neural network structure to compress long videos into short contexts, with an explicit pretraining objective to preserve the high-frequency details of single frames at arbitrary temporal positions. The baseline model can compress a 20-second video into a context at about 5k length, where random frames can be retrieved with perceptually preserved appearances. Such pretrained models can be directly fine-tuned as memory encoders for autoregressive video models, enabling long history memory with low context cost and relatively low fidelity loss. We evaluate the framework with ablative settings and discuss the trade-offs of possible neural architecture designs.', 'score': 5, 'issue_id': 353, 'pub_date': '2026-12-29', 'pub_date_card': {'ru': '29 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 29', 'zh': '12æœˆ29æ—¥'}, 'hash': '0e65f9e6890d2909', 'authors': ['Lvmin Zhang', 'Shengqu Cai', 'Muyang Li', 'Chong Zeng', 'Beijia Lu', 'Anyi Rao', 'Song Han', 'Gordon Wetzstein', 'Maneesh Agrawala'], 'affiliations': ['Carnegie Mellon University', 'HKUST', 'MIT', 'Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2512.23851.jpg', 'data': {'categories': ['#video', '#architecture', '#training'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹ ĞºĞ°Ğ´Ñ€Ğ¾Ğ²', 'desc': 'ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ PFP â€” Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ÑĞµÑ‚Ğ¸ Ğ´Ğ»Ñ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ² ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ñ‹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ñ ÑĞ²Ğ½Ğ¾Ğ¹ Ñ†ĞµĞ»ÑŒÑ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸ÑÑ…. Ğ‘Ğ°Ğ·Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° ÑĞ¶Ğ°Ñ‚ÑŒ 20-ÑĞµĞºÑƒĞ½Ğ´Ğ½Ğ¾Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ´Ğ»Ğ¸Ğ½Ğ¾Ğ¹ Ğ¾ĞºĞ¾Ğ»Ğ¾ 5000 Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ¸Ğ· ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ñ‹Ğµ ĞºĞ°Ğ´Ñ€Ñ‹ Ñ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼. ĞŸÑ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ»ĞµĞ³ĞºĞ¾ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ÑÑ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ² Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ Ğº Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ¹ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ñ Ğ½Ğ¸Ğ·ĞºĞ¸Ğ¼Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Efficient Video Compression with High-Fidelity Frame Retrieval', 'desc': 'The paper introduces PFP, a novel neural network architecture designed to compress lengthy videos into concise contexts while maintaining high-frequency details of individual frames. It achieves this by using a pretraining objective that allows for the retrieval of random frames with visually preserved quality. The model can effectively reduce a 20-second video to a context of approximately 5k, making it efficient for memory encoding in autoregressive video models. The authors also explore various neural architecture designs and their trade-offs through extensive evaluations.'}, 'zh': {'title': 'é«˜æ•ˆè§†é¢‘å‹ç¼©ä¸è®°å¿†ç¼–ç ', 'desc': 'æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºPFPçš„ç¥ç»ç½‘ç»œç»“æ„ï¼Œç”¨äºå°†é•¿è§†é¢‘å‹ç¼©æˆçŸ­çš„ä¸Šä¸‹æ–‡ã€‚è¯¥æ¨¡å‹çš„é¢„è®­ç»ƒç›®æ ‡æ˜¯ä¿ç•™å•å¸§çš„é«˜é¢‘ç»†èŠ‚ï¼Œèƒ½å¤Ÿåœ¨ä»»æ„æ—¶é—´ä½ç½®è¿›è¡Œå‹ç¼©ã€‚åŸºçº¿æ¨¡å‹å¯ä»¥å°†20ç§’çš„è§†é¢‘å‹ç¼©ä¸ºçº¦5ké•¿åº¦çš„ä¸Šä¸‹æ–‡ï¼Œå¹¶ä¸”å¯ä»¥ä»¥æ„ŸçŸ¥ä¸Šä¿ç•™çš„å¤–è§‚éšæœºæ£€ç´¢å¸§ã€‚è¿™ç§é¢„è®­ç»ƒæ¨¡å‹å¯ä»¥ç›´æ¥å¾®è°ƒä¸ºè‡ªå›å½’è§†é¢‘æ¨¡å‹çš„è®°å¿†ç¼–ç å™¨ï¼Œä»è€Œä»¥è¾ƒä½çš„ä¸Šä¸‹æ–‡æˆæœ¬å®ç°é•¿å†å²è®°å¿†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.22564', 'title': 'Geometry-Aware Optimization for Respiratory Sound Classification: Enhancing Sensitivity with SAM-Optimized Audio Spectrogram Transformers', 'url': 'https://huggingface.co/papers/2512.22564', 'abstract': 'Respiratory sound classification is hindered by the limited size, high noise levels, and severe class imbalance of benchmark datasets like ICBHI 2017. While Transformer-based models offer powerful feature extraction capabilities, they are prone to overfitting and often converge to sharp minima in the loss landscape when trained on such constrained medical data. To address this, we introduce a framework that enhances the Audio Spectrogram Transformer (AST) using Sharpness-Aware Minimization (SAM). Instead of merely minimizing the training loss, our approach optimizes the geometry of the loss surface, guiding the model toward flatter minima that generalize better to unseen patients. We also implement a weighted sampling strategy to handle class imbalance effectively. Our method achieves a state-of-the-art score of 68.10% on the ICBHI 2017 dataset, outperforming existing CNN and hybrid baselines. More importantly, it reaches a sensitivity of 68.31%, a crucial improvement for reliable clinical screening. Further analysis using t-SNE and attention maps confirms that the model learns robust, discriminative features rather than memorizing background noise.', 'score': 5, 'issue_id': 357, 'pub_date': '2026-12-27', 'pub_date_card': {'ru': '27 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 27', 'zh': '12æœˆ27æ—¥'}, 'hash': '2e3862f4d88666a8', 'authors': ['Atakan IÅŸÄ±k', 'Selin Vulga IÅŸÄ±k', 'Ahmet Feridun IÅŸÄ±k', 'MahÅŸuk Taylan'], 'affiliations': ['nRespiratory'], 'pdf_title_img': 'assets/pdf/title_img/2512.22564.jpg', 'data': {'categories': ['#architecture', '#audio', '#training', '#healthcare'], 'emoji': 'ğŸ«', 'ru': {'title': 'ĞŸĞ»Ğ¾ÑĞºĞ¸Ğµ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼ÑƒĞ¼Ñ‹ Ğ´Ğ»Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸ Ğ´Ñ‹Ñ…Ğ°Ğ½Ğ¸Ñ', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ·Ğ²ÑƒĞºĞ¾Ğ² Ğ´Ñ‹Ñ…Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Audio Spectrogram Transformer, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ… Ñ Ğ´Ğ¸ÑĞ±Ğ°Ğ»Ğ°Ğ½ÑĞ¾Ğ¼ ĞºĞ»Ğ°ÑÑĞ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Sharpness-Aware Minimization Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ, Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ÑÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğº Ğ¿Ğ»Ğ¾ÑĞºĞ¸Ğ¼ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼ÑƒĞ¼Ğ°Ğ¼ Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ĞµĞ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒÑ. Ğ”Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ²Ğ·Ğ²ĞµÑˆĞµĞ½Ğ½Ğ°Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ ÑĞµĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ´Ğ¸ÑĞ±Ğ°Ğ»Ğ°Ğ½ÑĞ° ĞºĞ»Ğ°ÑÑĞ¾Ğ². Ğ”Ğ¾ÑÑ‚Ğ¸Ğ³Ğ½ÑƒÑ‚Ñ‹ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ state-of-the-art Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ ICBHI 2017 Ñ Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ 68.31%, Ñ‡Ñ‚Ğ¾ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ğ°Ğ¶Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞºÑ€Ğ¸Ğ½Ğ¸Ğ½Ğ³Ğ°.'}, 'en': {'title': 'Enhancing Respiratory Sound Classification with SAM and AST', 'desc': 'This paper addresses the challenges of respiratory sound classification, particularly the issues of limited dataset size, high noise levels, and class imbalance. The authors propose an enhanced Audio Spectrogram Transformer (AST) framework that incorporates Sharpness-Aware Minimization (SAM) to improve model generalization by optimizing the loss surface geometry. By focusing on flatter minima, the model is less likely to overfit and performs better on unseen data. Additionally, a weighted sampling strategy is introduced to effectively manage class imbalance, resulting in state-of-the-art performance on the ICBHI 2017 dataset.'}, 'zh': {'title': 'ä¼˜åŒ–å‘¼å¸å£°éŸ³åˆ†ç±»çš„æ·±åº¦å­¦ä¹ æ–¹æ³•', 'desc': 'æœ¬ç ”ç©¶é’ˆå¯¹å‘¼å¸å£°éŸ³åˆ†ç±»ä¸­çš„æ•°æ®é›†é™åˆ¶é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ”¹è¿›çš„éŸ³é¢‘è°±å›¾å˜æ¢å™¨ï¼ˆASTï¼‰æ¡†æ¶ã€‚æˆ‘ä»¬é‡‡ç”¨äº†æ•æ„Ÿåº¦æ„è¯†æœ€å°åŒ–ï¼ˆSAMï¼‰æ–¹æ³•ï¼Œä¼˜åŒ–æŸå¤±è¡¨é¢çš„å‡ ä½•å½¢çŠ¶ï¼Œä»¥å¼•å¯¼æ¨¡å‹æœå‘æ›´å¹³å¦çš„æœ€å°å€¼ï¼Œä»è€Œæé«˜å¯¹æœªè§æ‚£è€…çš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å®æ–½äº†åŠ æƒé‡‡æ ·ç­–ç•¥ï¼Œæœ‰æ•ˆå¤„ç†ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ICBHI 2017æ•°æ®é›†ä¸Šè¾¾åˆ°äº†68.10%çš„æœ€æ–°æˆç»©ï¼Œæ˜¾è‘—æé«˜äº†ä¸´åºŠç­›æŸ¥çš„å¯é æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.24885', 'title': 'BEDA: Belief Estimation as Probabilistic Constraints for Performing Strategic Dialogue Acts', 'url': 'https://huggingface.co/papers/2512.24885', 'abstract': 'A framework called BEDA uses probabilistic constraints on belief estimation to improve strategic dialogue through formalized adversarial and alignment acts, outperforming baselines across multiple task settings.  \t\t\t\t\tAI-generated summary \t\t\t\t Strategic dialogue requires agents to execute distinct dialogue acts, for which belief estimation is essential. While prior work often estimates beliefs accurately, it lacks a principled mechanism to use those beliefs during generation. We bridge this gap by first formalizing two core acts Adversarial and Alignment, and by operationalizing them via probabilistic constraints on what an agent may generate. We instantiate this idea in BEDA, a framework that consists of the world set, the belief estimator for belief estimation, and the conditional generator that selects acts and realizes utterances consistent with the inferred beliefs. Across three settings, Conditional Keeper Burglar (CKBG, adversarial), Mutual Friends (MF, cooperative), and CaSiNo (negotiation), BEDA consistently outperforms strong baselines: on CKBG it improves success rate by at least 5.0 points across backbones and by 20.6 points with GPT-4.1-nano; on Mutual Friends it achieves an average improvement of 9.3 points; and on CaSiNo it achieves the optimal deal relative to all baselines. These results indicate that casting belief estimation as constraints provides a simple, general mechanism for reliable strategic dialogue.', 'score': 4, 'issue_id': 357, 'pub_date': '2026-12-31', 'pub_date_card': {'ru': '31 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 31', 'zh': '12æœˆ31æ—¥'}, 'hash': 'da71873530f907a7', 'authors': ['Hengli Li', 'Zhaoxin Yu', 'Qi Shen', 'Chenxi Li', 'Mengmeng Wang', 'Tinglang Wu', 'Yipeng Kang', 'Yuxuan Wang', 'Song-Chun Zhu', 'Zixia Jia', 'Zilong Zheng'], 'affiliations': ['Department of Automation, THU', 'Institute for Artificial Intelligence, PKU', 'Institute of Automation, CAS', 'NLCo, BIGAI', 'School of Artificial Intelligence, BUPT', 'Yuanpei College, PKU'], 'pdf_title_img': 'assets/pdf/title_img/2512.24885.jpg', 'data': {'categories': ['#reasoning', '#alignment'], 'emoji': 'ğŸ­', 'ru': {'title': 'Ğ£Ğ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ñ ĞºĞ°Ğº Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° BEDA, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾Ñ†ĞµĞ½ĞºÑƒ ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·ÑƒÑÑ‚ Ğ´Ğ²Ğ° ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ° Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ° â€” Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾Ğ±Ğ¾Ñ€ÑÑ‚Ğ²Ğ¾ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ğµ â€” Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·ÑƒÑÑ‚ Ğ¸Ñ… Ñ‡ĞµÑ€ĞµĞ· Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ğ²Ñ‹ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¸Ñ€Ğ°, Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ĞµĞ»Ñ ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ Ñ€ĞµĞ¿Ğ»Ğ¸ĞºĞ¸, ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ĞµĞ¼Ñ‹Ğ¼Ğ¸ ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸ ÑĞ¾Ğ±ĞµÑĞµĞ´Ğ½Ğ¸ĞºĞ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ñ‚Ñ€Ñ‘Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸: Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ñ€Ğ¾ÑĞ»Ğ° Ğ½Ğ° 20.6 Ğ¿ÑƒĞ½ĞºÑ‚Ğ° Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ GPT-4.1-nano.'}, 'en': {'title': 'Enhancing Strategic Dialogue with BEDA: Belief Estimation Meets Probabilistic Constraints', 'desc': 'The paper introduces BEDA, a framework that enhances strategic dialogue by using probabilistic constraints on belief estimation. It formalizes two key dialogue acts: Adversarial and Alignment, which help agents make better decisions during conversations. By integrating a belief estimator and a conditional generator, BEDA ensures that the generated responses align with the inferred beliefs of the agents. The framework shows significant improvements in performance across various task settings, demonstrating the effectiveness of using belief estimation as a guiding principle in dialogue generation.'}, 'zh': {'title': 'ä¿¡å¿µçº¦æŸæå‡æˆ˜ç•¥å¯¹è¯çš„æœ‰æ•ˆæ€§', 'desc': 'BEDAæ¡†æ¶é€šè¿‡å¯¹ä¿¡å¿µä¼°è®¡æ–½åŠ æ¦‚ç‡çº¦æŸï¼Œæå‡äº†æˆ˜ç•¥å¯¹è¯çš„æ•ˆæœã€‚è¯¥æ¡†æ¶æ­£å¼åŒ–äº†å¯¹æŠ—æ€§å’Œå¯¹é½æ€§ä¸¤ä¸ªæ ¸å¿ƒå¯¹è¯è¡Œä¸ºï¼Œå¹¶é€šè¿‡æ¦‚ç‡çº¦æŸæ¥æ“ä½œè¿™äº›è¡Œä¸ºã€‚BEDAåŒ…æ‹¬ä¸–ç•Œé›†ã€ä¿¡å¿µä¼°è®¡å™¨å’Œæ¡ä»¶ç”Ÿæˆå™¨ï¼Œèƒ½å¤Ÿæ ¹æ®æ¨æ–­çš„ä¿¡å¿µé€‰æ‹©å¯¹è¯è¡Œä¸ºå¹¶ç”Ÿæˆä¸€è‡´çš„å‘è¨€ã€‚åœ¨å¤šä¸ªä»»åŠ¡è®¾ç½®ä¸­ï¼ŒBEDAçš„è¡¨ç°å‡ä¼˜äºå¼ºåŸºçº¿ï¼Œæ˜¾ç¤ºå‡ºå°†ä¿¡å¿µä¼°è®¡ä½œä¸ºçº¦æŸçš„æ–¹å¼ä¸ºå¯é çš„æˆ˜ç•¥å¯¹è¯æä¾›äº†ç®€å•è€Œé€šç”¨çš„æœºåˆ¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.24385', 'title': 'Forging Spatial Intelligence: A Roadmap of Multi-Modal Data Pre-Training for Autonomous Systems', 'url': 'https://huggingface.co/papers/2512.24385', 'abstract': 'The rapid advancement of autonomous systems, including self-driving vehicles and drones, has intensified the need to forge true Spatial Intelligence from multi-modal onboard sensor data. While foundation models excel in single-modal contexts, integrating their capabilities across diverse sensors like cameras and LiDAR to create a unified understanding remains a formidable challenge. This paper presents a comprehensive framework for multi-modal pre-training, identifying the core set of techniques driving progress toward this goal. We dissect the interplay between foundational sensor characteristics and learning strategies, evaluating the role of platform-specific datasets in enabling these advancements. Our central contribution is the formulation of a unified taxonomy for pre-training paradigms: ranging from single-modality baselines to sophisticated unified frameworks that learn holistic representations for advanced tasks like 3D object detection and semantic occupancy prediction. Furthermore, we investigate the integration of textual inputs and occupancy representations to facilitate open-world perception and planning. Finally, we identify critical bottlenecks, such as computational efficiency and model scalability, and propose a roadmap toward general-purpose multi-modal foundation models capable of achieving robust Spatial Intelligence for real-world deployment.', 'score': 4, 'issue_id': 354, 'pub_date': '2026-12-30', 'pub_date_card': {'ru': '30 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 30', 'zh': '12æœˆ30æ—¥'}, 'hash': 'bf38f19718f26a0d', 'authors': ['Song Wang', 'Lingdong Kong', 'Xiaolu Liu', 'Hao Shi', 'Wentong Li', 'Jianke Zhu', 'Steven C. H. Hoi'], 'affiliations': ['Alibaba Group', 'Nanjing University of Aeronautics', 'National University of Singapore', 'Singapore Management University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2512.24385.jpg', 'data': {'categories': ['#benchmark', '#3d', '#multimodal', '#dataset', '#robotics', '#survey'], 'emoji': 'ğŸš—', 'ru': {'title': 'Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ñ‡Ğ¸ĞºĞ¾Ğ² (ĞºĞ°Ğ¼ĞµÑ€ Ğ¸ LiDAR) Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ² Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ĞµĞ´Ğ¸Ğ½ÑƒÑ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¾Ñ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ… Ğ¾Ğ´Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ 3D-Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ·Ğ°Ğ½ÑÑ‚Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ²Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ·Ğ°Ğ½ÑÑ‚Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¼Ğ¸Ñ€Ğ° Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ»ÑÑÑ‚ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑƒĞ·ĞºĞ¸Ğµ Ğ¼ĞµÑÑ‚Ğ° Ğ² Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°Ñ Ğ´Ğ¾Ñ€Ğ¾Ğ¶Ğ½ÑƒÑ ĞºĞ°Ñ€Ñ‚Ñƒ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Building Spatial Intelligence through Multi-Modal Integration', 'desc': 'This paper addresses the challenge of creating Spatial Intelligence in autonomous systems by integrating data from various sensors like cameras and LiDAR. It introduces a framework for multi-modal pre-training that enhances the ability of models to understand and process diverse sensor inputs. The authors propose a unified taxonomy for pre-training methods, which helps in developing advanced tasks such as 3D object detection. Additionally, the paper highlights key challenges like computational efficiency and scalability, providing a roadmap for building versatile multi-modal foundation models for real-world applications.'}, 'zh': {'title': 'æ„å»ºå¤šæ¨¡æ€ç©ºé—´æ™ºèƒ½çš„æœªæ¥ä¹‹è·¯', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†å¦‚ä½•ä»å¤šæ¨¡æ€ä¼ æ„Ÿå™¨æ•°æ®ä¸­æ„å»ºçœŸæ­£çš„ç©ºé—´æ™ºèƒ½ï¼Œå°¤å…¶æ˜¯åœ¨è‡ªåŠ¨é©¾é©¶å’Œæ— äººæœºç­‰è‡ªä¸»ç³»ç»Ÿä¸­ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§å…¨é¢çš„å¤šæ¨¡æ€é¢„è®­ç»ƒæ¡†æ¶ï¼Œåˆ†æäº†åŸºç¡€ä¼ æ„Ÿå™¨ç‰¹æ€§ä¸å­¦ä¹ ç­–ç•¥ä¹‹é—´çš„ç›¸äº’ä½œç”¨ã€‚é€šè¿‡å»ºç«‹ç»Ÿä¸€çš„é¢„è®­ç»ƒèŒƒç•´ï¼Œæˆ‘ä»¬èƒ½å¤Ÿä»å•æ¨¡æ€åŸºçº¿åˆ°å¤æ‚çš„ç»Ÿä¸€æ¡†æ¶ï¼Œå­¦ä¹ ç”¨äºé«˜çº§ä»»åŠ¡çš„æ•´ä½“è¡¨ç¤ºã€‚æœ€åï¼Œæˆ‘ä»¬è¯†åˆ«äº†è®¡ç®—æ•ˆç‡å’Œæ¨¡å‹å¯æ‰©å±•æ€§ç­‰å…³é”®ç“¶é¢ˆï¼Œå¹¶æå‡ºäº†å®ç°é€šç”¨å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹çš„è·¯çº¿å›¾ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.24297', 'title': 'Figure It Out: Improving the Frontier of Reasoning with Active Visual Thinking', 'url': 'https://huggingface.co/papers/2512.24297', 'abstract': 'Complex reasoning problems often involve implicit spatial, geometric, and structural relationships that are not explicitly encoded in text. While recent reasoning models have achieved strong performance across many domains, purely text-based reasoning struggles to represent global structural constraints in complex settings. In this paper, we introduce FIGR, which integrates active visual thinking into multi-turn reasoning via end-to-end reinforcement learning. FIGR externalizes intermediate structural hypotheses by constructing visual representations during problem solving. By adaptively regulating when and how visual reasoning should be invoked, FIGR enables more stable and coherent reasoning over global structural properties that are difficult to capture from text alone. Experiments on challenging mathematical reasoning benchmarks demonstrate that FIGR outperforms strong text-only chain-of-thought baselines. In particular, FIGR improves the base model by 13.12% on AIME 2025 and 11.00% on BeyondAIME, highlighting the effectiveness of figure-guided multimodal reasoning in enhancing the stability and reliability of complex reasoning.', 'score': 4, 'issue_id': 353, 'pub_date': '2026-12-30', 'pub_date_card': {'ru': '30 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 30', 'zh': '12æœˆ30æ—¥'}, 'hash': '0f388b52a58f75b9', 'authors': ['Meiqi Chen', 'Fandong Meng', 'Jie Zhou'], 'affiliations': ['WeChat AI, Tencent Inc'], 'pdf_title_img': 'assets/pdf/title_img/2512.24297.jpg', 'data': {'categories': ['#rl', '#math', '#multimodal', '#benchmark'], 'emoji': 'ğŸ“Š', 'ru': {'title': 'Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ÑÑ FIGR â€” ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑĞºÑÑ‚ĞµÑ€Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ Ğ³Ğ¸Ğ¿Ğ¾Ñ‚ĞµĞ·Ñ‹, ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ. ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ñ€ĞµĞ³ÑƒĞ»ÑÑ†Ğ¸Ñ Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚Ğ° Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ: 13.12% Ğ½Ğ° AIME 2025 Ğ¸ 11.00% Ğ½Ğ° BeyondAIME Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Enhancing Reasoning with Visual Thinking: Introducing FIGR', 'desc': 'This paper presents FIGR, a novel approach that enhances multi-turn reasoning by incorporating visual thinking through reinforcement learning. Unlike traditional text-based models, FIGR constructs visual representations to capture complex spatial and structural relationships during problem solving. By dynamically deciding when to use visual reasoning, FIGR improves the coherence and stability of reasoning processes. Experimental results show that FIGR significantly outperforms existing text-only models on challenging mathematical reasoning tasks, demonstrating the benefits of integrating visual information.'}, 'zh': {'title': 'è§†è§‰å¼•å¯¼çš„å¤šæ¨¡æ€æ¨ç†æå‡å¤æ‚æ¨ç†èƒ½åŠ›', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºFIGRçš„æ¨¡å‹ï¼Œå®ƒé€šè¿‡ç«¯åˆ°ç«¯çš„å¼ºåŒ–å­¦ä¹ å°†ä¸»åŠ¨è§†è§‰æ€ç»´æ•´åˆåˆ°å¤šè½®æ¨ç†ä¸­ã€‚FIGRåœ¨è§£å†³é—®é¢˜çš„è¿‡ç¨‹ä¸­ï¼Œé€šè¿‡æ„å»ºè§†è§‰è¡¨ç¤ºæ¥å¤–åŒ–ä¸­é—´ç»“æ„å‡è®¾ï¼Œä»è€Œæ›´å¥½åœ°å¤„ç†å¤æ‚çš„æ¨ç†é—®é¢˜ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿè‡ªé€‚åº”åœ°è°ƒèŠ‚ä½•æ—¶ä»¥åŠå¦‚ä½•è°ƒç”¨è§†è§‰æ¨ç†ï¼Œä½¿å¾—å¯¹å…¨çƒç»“æ„å±æ€§çš„æ¨ç†æ›´åŠ ç¨³å®šå’Œè¿è´¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFIGRåœ¨æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºä»…åŸºäºæ–‡æœ¬çš„æ¨ç†æ¨¡å‹ï¼Œæå‡äº†æ¨ç†çš„ç¨³å®šæ€§å’Œå¯é æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.24097', 'title': 'Factorized Learning for Temporally Grounded Video-Language Models', 'url': 'https://huggingface.co/papers/2512.24097', 'abstract': 'Recent video-language models have shown great potential for video understanding, but still struggle with accurate temporal grounding for event-level perception. We observe that two main factors in video understanding (i.e., temporal grounding and textual response) form a logical hierarchy: accurate temporal evidence grounding lays the foundation for reliable textual response. However, existing works typically handle these two tasks in a coupled manner without a clear logical structure, leading to sub-optimal objectives. We address this from a factorized learning perspective. We first propose D^2VLM, a framework that decouples the learning of these two tasks while also emphasizing their inherent dependency. We adopt a "grounding then answering with evidence referencing" paradigm and introduce evidence tokens for evidence grounding, which emphasize event-level visual semantic capture beyond the focus on timestamp representation in existing works. To further facilitate the learning of these two tasks, we introduce a novel factorized preference optimization (FPO) algorithm. Unlike standard preference optimization, FPO explicitly incorporates probabilistic temporal grounding modeling into the optimization objective, enabling preference learning for both temporal grounding and textual response. We also construct a synthetic dataset to address the lack of suitable datasets for factorized preference learning with explicit temporal grounding. Experiments on various tasks demonstrate the clear advantage of our approach. Our source code is available at https://github.com/nusnlp/d2vlm.', 'score': 4, 'issue_id': 354, 'pub_date': '2026-12-30', 'pub_date_card': {'ru': '30 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 30', 'zh': '12æœˆ30æ—¥'}, 'hash': '9ba4701895c566d2', 'authors': ['Wenzheng Zeng', 'Difei Gao', 'Mike Zheng Shou', 'Hwee Tou Ng'], 'affiliations': ['National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2512.24097.jpg', 'data': {'categories': ['#training', '#multimodal', '#video', '#rlhf', '#dataset'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ DÂ²VLM â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ´Ğ²Ğµ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ² Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ¸ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾, Ñ‡Ñ‚Ğ¾ Ğ»ÑƒÑ‡ÑˆĞµ, Ñ‡ĞµĞ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğº. Ğ”Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ĞµĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ (FPO), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ²Ğ½Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ½ÑƒÑ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ² Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸. ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, ÑĞ¾Ğ·Ğ´Ğ°Ğ½ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ ÑĞ²Ğ½Ğ¾Ğ¹ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¾Ğ¹ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹.'}, 'en': {'title': 'Decoupling Temporal Grounding and Textual Response for Enhanced Video Understanding', 'desc': "This paper introduces D^2VLM, a new framework designed to improve video understanding by separating the tasks of temporal grounding and textual response. The authors argue that accurate temporal grounding is essential for generating reliable textual responses, and they propose a method that emphasizes this relationship. By using a 'grounding then answering with evidence referencing' approach, the framework incorporates evidence tokens to enhance event-level visual semantic understanding. Additionally, the novel factorized preference optimization (FPO) algorithm is introduced to better model the dependencies between these tasks, leading to improved performance in video-language tasks."}, 'zh': {'title': 'è§£è€¦è§†é¢‘ç†è§£ä¸­çš„æ—¶é—´å®šä½ä¸æ–‡æœ¬å“åº”', 'desc': 'æœ€è¿‘çš„è§†é¢‘è¯­è¨€æ¨¡å‹åœ¨è§†é¢‘ç†è§£æ–¹é¢å±•ç°äº†å¾ˆå¤§çš„æ½œåŠ›ï¼Œä½†åœ¨äº‹ä»¶çº§æ„ŸçŸ¥çš„æ—¶é—´å®šä½ä¸Šä»ç„¶å­˜åœ¨å›°éš¾ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œè§†é¢‘ç†è§£ä¸­çš„ä¸¤ä¸ªä¸»è¦å› ç´ ï¼ˆæ—¶é—´å®šä½å’Œæ–‡æœ¬å“åº”ï¼‰å½¢æˆäº†ä¸€ä¸ªé€»è¾‘å±‚æ¬¡ï¼šå‡†ç¡®çš„æ—¶é—´è¯æ®å®šä½ä¸ºå¯é çš„æ–‡æœ¬å“åº”å¥ å®šäº†åŸºç¡€ã€‚æˆ‘ä»¬æå‡ºäº†D^2VLMæ¡†æ¶ï¼Œè§£è€¦è¿™ä¸¤ä¸ªä»»åŠ¡çš„å­¦ä¹ ï¼ŒåŒæ—¶å¼ºè°ƒå®ƒä»¬ä¹‹é—´çš„å†…åœ¨ä¾èµ–å…³ç³»ã€‚é€šè¿‡å¼•å…¥æ–°çš„å› å­åŒ–åå¥½ä¼˜åŒ–ç®—æ³•ï¼ˆFPOï¼‰ï¼Œæˆ‘ä»¬èƒ½å¤Ÿæ›´å¥½åœ°ä¼˜åŒ–æ—¶é—´å®šä½å’Œæ–‡æœ¬å“åº”çš„å­¦ä¹ ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.24176', 'title': 'Guiding a Diffusion Transformer with the Internal Dynamics of Itself', 'url': 'https://huggingface.co/papers/2512.24176', 'abstract': "The diffusion model presents a powerful ability to capture the entire (conditional) data distribution. However, due to the lack of sufficient training and data to learn to cover low-probability areas, the model will be penalized for failing to generate high-quality images corresponding to these areas. To achieve better generation quality, guidance strategies such as classifier free guidance (CFG) can guide the samples to the high-probability areas during the sampling stage. However, the standard CFG often leads to over-simplified or distorted samples. On the other hand, the alternative line of guiding diffusion model with its bad version is limited by carefully designed degradation strategies, extra training and additional sampling steps. In this paper, we proposed a simple yet effective strategy Internal Guidance (IG), which introduces an auxiliary supervision on the intermediate layer during training process and extrapolates the intermediate and deep layer's outputs to obtain generative results during sampling process. This simple strategy yields significant improvements in both training efficiency and generation quality on various baselines. On ImageNet 256x256, SiT-XL/2+IG achieves FID=5.31 and FID=1.75 at 80 and 800 epochs. More impressively, LightningDiT-XL/1+IG achieves FID=1.34 which achieves a large margin between all of these methods. Combined with CFG, LightningDiT-XL/1+IG achieves the current state-of-the-art FID of 1.19.", 'score': 3, 'issue_id': 361, 'pub_date': '2026-12-30', 'pub_date_card': {'ru': '30 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 30', 'zh': '12æœˆ30æ—¥'}, 'hash': 'fd34611418fdc611', 'authors': ['Xingyu Zhou', 'Qifan Li', 'Xiaobin Hu', 'Hai Chen', 'Shuhang Gu'], 'affiliations': ['National University of Singapore', 'North China Institute of Computer Systems Engineering', 'Sun Yat-sen University', 'University of Electronic Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2512.24176.jpg', 'data': {'categories': ['#cv', '#diffusion', '#training', '#architecture'], 'emoji': 'âœ¨', 'ru': {'title': 'Ğ’Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ĞµĞµ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ´Ğ»Ñ ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Internal Guidance (IG) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ supervĞ¸Ğ·Ğ¸Ñ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ ÑĞ»Ğ¾Ğ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞºÑÑ‚Ñ€Ğ°Ğ¿Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ñ… Ğ²Ñ‹Ñ…Ğ¾Ğ´Ñ‹ Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ classifier free guidance, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼. ĞĞ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ ImageNet 256x256 Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ğ¿ĞµÑ‡Ğ°Ñ‚Ğ»ÑÑÑ‰Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ñ FID=1.19, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹.'}, 'en': {'title': 'Enhancing Diffusion Models with Internal Guidance for Superior Image Generation', 'desc': 'This paper introduces a new strategy called Internal Guidance (IG) to improve the performance of diffusion models in generating high-quality images. The authors highlight that traditional methods like classifier free guidance (CFG) can lead to oversimplified outputs, while existing alternatives require complex degradation strategies and additional training. IG enhances the training process by providing auxiliary supervision on intermediate layers, which helps in generating better samples during the sampling phase. The results show significant improvements in image quality, achieving state-of-the-art performance on benchmarks like ImageNet.'}, 'zh': {'title': 'å†…éƒ¨å¼•å¯¼ï¼šæå‡æ‰©æ•£æ¨¡å‹ç”Ÿæˆè´¨é‡çš„æœ‰æ•ˆç­–ç•¥', 'desc': 'æ‰©æ•£æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆæ•æ‰æ•´ä¸ªæ¡ä»¶æ•°æ®åˆ†å¸ƒï¼Œä½†ç”±äºè®­ç»ƒæ•°æ®ä¸è¶³ï¼Œæ¨¡å‹åœ¨ç”Ÿæˆä½æ¦‚ç‡åŒºåŸŸçš„é«˜è´¨é‡å›¾åƒæ—¶ä¼šå—åˆ°æƒ©ç½šã€‚ä¸ºæé«˜ç”Ÿæˆè´¨é‡ï¼Œç ”ç©¶è€…ä»¬æå‡ºäº†æ— åˆ†ç±»å™¨å¼•å¯¼ï¼ˆCFGï¼‰ç­–ç•¥ï¼Œä»¥åœ¨é‡‡æ ·é˜¶æ®µå¼•å¯¼æ ·æœ¬è¿›å…¥é«˜æ¦‚ç‡åŒºåŸŸã€‚ç„¶è€Œï¼Œæ ‡å‡†çš„CFGå¸¸å¸¸å¯¼è‡´æ ·æœ¬è¿‡äºç®€å•æˆ–å¤±çœŸã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„å†…éƒ¨å¼•å¯¼ï¼ˆIGï¼‰ç­–ç•¥ï¼Œé€šè¿‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¯¹ä¸­é—´å±‚è¿›è¡Œè¾…åŠ©ç›‘ç£ï¼Œå¹¶åœ¨é‡‡æ ·è¿‡ç¨‹ä¸­å¤–æ¨ä¸­é—´å±‚å’Œæ·±å±‚çš„è¾“å‡ºï¼Œä»è€Œæ˜¾è‘—æé«˜äº†è®­ç»ƒæ•ˆç‡å’Œç”Ÿæˆè´¨é‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.22905', 'title': 'JavisGPT: A Unified Multi-modal LLM for Sounding-Video Comprehension and Generation', 'url': 'https://huggingface.co/papers/2512.22905', 'abstract': 'This paper presents JavisGPT, the first unified multimodal large language model (MLLM) for Joint Audio-Video (JAV) comprehension and generation. JavisGPT adopts a concise encoder-LLM-decoder architecture, featuring a SyncFusion module for spatio-temporal audio-video fusion and synchrony-aware learnable queries to bridge a pretrained JAV-DiT generator. This design enables temporally coherent video-audio understanding and generation from multimodal instructions. We design an effective three-stage training pipeline consisting of multimodal pretraining, audio-video fine-tuning, and large-scale instruction-tuning, to progressively build multimodal comprehension and generation from existing vision-language models. To support this, we further construct JavisInst-Omni, a high-quality instruction dataset with over 200K GPT-4o-curated audio-video-text dialogues that span diverse and multi-level comprehension and generation scenarios. Extensive experiments on JAV comprehension and generation benchmarks show that JavisGPT outperforms existing MLLMs, particularly in complex and temporally synchronized settings.', 'score': 3, 'issue_id': 354, 'pub_date': '2026-12-28', 'pub_date_card': {'ru': '28 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 28', 'zh': '12æœˆ28æ—¥'}, 'hash': '0d7aa22be1582d4f', 'authors': ['Kai Liu', 'Jungang Li', 'Yuchong Sun', 'Shengqiong Wu', 'Jianzhang Gao', 'Daoan Zhang', 'Wei Zhang', 'Sheng Jin', 'Sicheng Yu', 'Geng Zhan', 'Jiayi Ji', 'Fan Zhou', 'Liang Zheng', 'Shuicheng Yan', 'Hao Fei', 'Tat-Seng Chua'], 'affiliations': ['ANU', 'HKUST(GZ)', 'HZCU', 'NTU', 'NUS', 'RUC', 'SMU', 'UR', 'USYD', 'ZJU'], 'pdf_title_img': 'assets/pdf/title_img/2512.22905.jpg', 'data': {'categories': ['#training', '#open_source', '#multimodal', '#video', '#audio', '#architecture', '#dataset'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ¡Ğ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾-Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµÑ€ĞµĞ· Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ LLM', 'desc': 'JavisGPT â€” ÑÑ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ğ°Ñ ĞµĞ´Ğ¸Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾-Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ encoder-LLM-decoder Ñ Ğ¼Ğ¾Ğ´ÑƒĞ»ĞµĞ¼ SyncFusion Ğ´Ğ»Ñ ÑĞ¿atio-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ñ‚Ñ€Ñ‘Ñ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ pipeline Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ: Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ´Ñ‚Ñ€ĞµĞ½Ğ¸Ğ½Ğ³, Ñ‚Ğ¾Ğ½ĞºĞ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ½Ğ° Ğ°ÑƒĞ´Ğ¸Ğ¾-Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑÑ…. Ğ”Ğ»Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¸ ÑÑ‚Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… JavisInst-Omni Ñ 200K Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ², Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… GPT-4o, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°.'}, 'en': {'title': 'JavisGPT: Unifying Audio and Video Understanding with Multimodal Intelligence', 'desc': 'JavisGPT is a groundbreaking multimodal large language model designed for understanding and generating audio and video together. It uses a unique architecture that combines an encoder, a large language model (LLM), and a decoder, along with a SyncFusion module to effectively merge audio and video data. The model is trained through a three-stage process that includes pretraining, fine-tuning, and instruction-tuning, enhancing its ability to handle complex multimodal tasks. With the support of a comprehensive dataset of audio-video-text dialogues, JavisGPT demonstrates superior performance in tasks requiring synchronized audio and video comprehension and generation.'}, 'zh': {'title': 'JavisGPTï¼šéŸ³è§†é¢‘ç†è§£ä¸ç”Ÿæˆçš„ç»Ÿä¸€æ¨¡å‹', 'desc': 'æœ¬æ–‡ä»‹ç»äº†JavisGPTï¼Œè¿™æ˜¯é¦–ä¸ªç»Ÿä¸€çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œç”¨äºè”åˆéŸ³é¢‘-è§†é¢‘ï¼ˆJAVï¼‰ç†è§£å’Œç”Ÿæˆã€‚JavisGPTé‡‡ç”¨ç®€æ´çš„ç¼–ç å™¨-LLM-è§£ç å™¨æ¶æ„ï¼Œé…å¤‡SyncFusionæ¨¡å—ï¼Œå®ç°éŸ³é¢‘å’Œè§†é¢‘çš„æ—¶ç©ºèåˆï¼Œå¹¶ä½¿ç”¨åŒæ­¥æ„ŸçŸ¥çš„å¯å­¦ä¹ æŸ¥è¯¢æ¥è¿æ¥é¢„è®­ç»ƒçš„JAV-DiTç”Ÿæˆå™¨ã€‚è¯¥è®¾è®¡ä½¿å¾—ä»å¤šæ¨¡æ€æŒ‡ä»¤ä¸­è¿›è¡Œæ—¶é—´ä¸€è‡´çš„éŸ³é¢‘è§†é¢‘ç†è§£å’Œç”Ÿæˆæˆä¸ºå¯èƒ½ã€‚æˆ‘ä»¬è¿˜è®¾è®¡äº†ä¸€ä¸ªæœ‰æ•ˆçš„ä¸‰é˜¶æ®µè®­ç»ƒæµç¨‹ï¼ŒåŒ…æ‹¬å¤šæ¨¡æ€é¢„è®­ç»ƒã€éŸ³é¢‘è§†é¢‘å¾®è°ƒå’Œå¤§è§„æ¨¡æŒ‡ä»¤å¾®è°ƒï¼Œä»¥é€æ­¥æ„å»ºå¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.22280', 'title': 'Valori: A Deterministic Memory Substrate for AI Systems', 'url': 'https://huggingface.co/papers/2512.22280', 'abstract': 'Valori introduces a deterministic AI memory system using fixed-point arithmetic to ensure bit-identical results across platforms, addressing non-determinism in vector embeddings and similarity search for trustworthy AI.  \t\t\t\t\tAI-generated summary \t\t\t\t Modern AI systems rely on vector embeddings stored and searched using floating-point arithmetic. While effective for approximate similarity search, this design introduces fundamental non-determinism: identical models, inputs, and code can produce different memory states and retrieval results across hardware architectures (e.g., x86 vs. ARM). This prevents replayability and safe deployment, leading to silent data divergence that prevents post-hoc verification and compromises audit trails in regulated sectors. We present Valori, a deterministic AI memory substrate that replaces floating-point memory operations with fixed-point arithmetic (Q16.16) and models memory as a replayable state machine. Valori guarantees bit-identical memory states, snapshots, and search results across platforms. We demonstrate that non-determinism arises before indexing or retrieval and show how Valori enforces determinism at the memory boundary. Our results suggest that deterministic memory is a necessary primitive for trustworthy AI systems. The reference implementation is open-source and available at https://github.com/varshith-Git/Valori-Kernel (archived at https://zenodo.org/records/18022660).', 'score': 3, 'issue_id': 355, 'pub_date': '2026-12-25', 'pub_date_card': {'ru': '25 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 25', 'zh': '12æœˆ25æ—¥'}, 'hash': 'd3f900163e2812ae', 'authors': ['Varshith Gudur'], 'affiliations': ['Independent Researcher', 'Valori Kernel Project'], 'pdf_title_img': 'assets/pdf/title_img/2512.22280.jpg', 'data': {'categories': [], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ”ĞµÑ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ñ‹Ñ… AI-ÑĞ¸ÑÑ‚ĞµĞ¼ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ñ€Ğ¸Ñ„Ğ¼ĞµÑ‚Ğ¸ĞºÑƒ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡ĞºĞ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Valori â€” ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´ĞµÑ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ´Ğ»Ñ AI, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ñ€Ğ¸Ñ„Ğ¼ĞµÑ‚Ğ¸ĞºÑƒ Ñ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡ĞºĞ¾Ğ¹ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ‡Ğ¸ÑĞµĞ» Ñ Ğ¿Ğ»Ğ°Ğ²Ğ°ÑÑ‰ĞµĞ¹ Ñ‚Ğ¾Ñ‡ĞºĞ¾Ğ¹ Ğ´Ğ»Ñ Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ° ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ°. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ… Ğ¾Ğ±Ğ¾Ñ€ÑƒĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ·Ğ°Ñ‚Ñ€ÑƒĞ´Ğ½ÑĞµÑ‚ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ°ÑƒĞ´Ğ¸Ñ‚ AI-ÑĞ¸ÑÑ‚ĞµĞ¼. Ğ’Ğ°Ğ»Ğ¾Ñ€Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ ĞºĞ°Ğº Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ğ¹ ĞºĞ¾Ğ½ĞµÑ‡Ğ½Ñ‹Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚, Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ñ€ÑƒÑ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ñ€Ğ¾Ğ². Ğ­Ñ‚Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ğ°Ğ¶Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ñ… AI-ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ² Ñ€ĞµĞ³ÑƒĞ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¾Ñ‚Ñ€Ğ°ÑĞ»ÑÑ….'}, 'en': {'title': 'Ensuring Consistency in AI with Deterministic Memory', 'desc': 'Valori is a new AI memory system that uses fixed-point arithmetic to ensure consistent results across different hardware platforms. Traditional AI systems often use floating-point arithmetic, which can lead to non-deterministic outcomes, meaning the same inputs can yield different results on different machines. This non-determinism can cause issues in verifying AI outputs and maintaining reliable audit trails, especially in regulated industries. By modeling memory as a replayable state machine, Valori guarantees that memory states and search results are identical, enhancing the trustworthiness of AI applications.'}, 'zh': {'title': 'æ„å»ºå¯ä¿¡èµ–AIçš„ç¡®å®šæ€§å†…å­˜ç³»ç»Ÿ', 'desc': 'Valori æ˜¯ä¸€ç§ç¡®å®šæ€§çš„äººå·¥æ™ºèƒ½å†…å­˜ç³»ç»Ÿï¼Œä½¿ç”¨å®šç‚¹ç®—æœ¯æ¥ç¡®ä¿è·¨å¹³å°çš„ä¸€è‡´æ€§ã€‚ä¼ ç»Ÿçš„æµ®ç‚¹ç®—æœ¯åœ¨å‘é‡åµŒå…¥å’Œç›¸ä¼¼æ€§æœç´¢ä¸­å¼•å…¥äº†éç¡®å®šæ€§ï¼Œå¯¼è‡´ç›¸åŒçš„æ¨¡å‹å’Œè¾“å…¥åœ¨ä¸åŒç¡¬ä»¶ä¸Šäº§ç”Ÿä¸åŒçš„ç»“æœã€‚Valori é€šè¿‡å°†æµ®ç‚¹å†…å­˜æ“ä½œæ›¿æ¢ä¸ºå®šç‚¹ç®—æœ¯ï¼Œç¡®ä¿å†…å­˜çŠ¶æ€å’Œæœç´¢ç»“æœåœ¨å„ä¸ªå¹³å°ä¸Šå®Œå…¨ç›¸åŒã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œç¡®å®šæ€§å†…å­˜æ˜¯æ„å»ºå¯ä¿¡èµ–äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„å¿…è¦åŸºç¡€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.06789', 'title': 'MemGovern: Enhancing Code Agents through Learning from Governed Human Experiences', 'url': 'https://huggingface.co/papers/2601.06789', 'abstract': 'MemGovern framework transforms unstructured GitHub data into structured experiential memory for autonomous software engineering agents, improving bug resolution rates through enhanced experience retrieval.  \t\t\t\t\tAI-generated summary \t\t\t\t While autonomous software engineering (SWE) agents are reshaping programming paradigms, they currently suffer from a "closed-world" limitation: they attempt to fix bugs from scratch or solely using local context, ignoring the immense historical human experience available on platforms like GitHub. Accessing this open-world experience is hindered by the unstructured and fragmented nature of real-world issue-tracking data. In this paper, we introduce MemGovern, a framework designed to govern and transform raw GitHub data into actionable experiential memory for agents. MemGovern employs experience governance to convert human experience into agent-friendly experience cards and introduces an agentic experience search strategy that enables logic-driven retrieval of human expertise. By producing 135K governed experience cards, MemGovern achieves a significant performance boost, improving resolution rates on the SWE-bench Verified by 4.65%. As a plug-in approach, MemGovern provides a solution for agent-friendly memory infrastructure.', 'score': 57, 'issue_id': 567, 'pub_date': '2026-01-11', 'pub_date_card': {'ru': '11 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 11', 'zh': '1æœˆ11æ—¥'}, 'hash': '0aaa42ea5e9dedf0', 'authors': ['Qihao Wang', 'Ziming Cheng', 'Shuo Zhang', 'Fan Liu', 'Rui Xu', 'Heng Lian', 'Kunyi Wang', 'Xiaoming Yu', 'Jianghao Yin', 'Sen Hu', 'Yue Hu', 'Shaolei Zhang', 'Yanbing Liu', 'Ronghao Chen', 'Huacan Wang'], 'affiliations': ['ECNU', 'FDU', 'HKUST(GZ)', 'NUS', 'PKU', 'QuantaAlpha', 'RUC', 'UBC', 'UCAS', 'XDU'], 'pdf_title_img': 'assets/pdf/title_img/2601.06789.jpg', 'data': {'categories': [], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞĞ¿Ñ‹Ñ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° ĞºĞ°Ğº Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° MemGovern, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ½ĞµÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ· GitHub Ğ² ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ¾Ğ¿Ñ‹Ñ‚Ğ° Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾Ğ¿Ñ‹Ñ‚Ğ¾Ğ¼ Ğ´Ğ»Ñ ĞºĞ¾Ğ½Ğ²ĞµÑ€Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ¿Ñ‹Ñ‚Ğ° Ğ² ĞºĞ°Ñ€Ñ‚Ğ¾Ñ‡ĞºĞ¸, Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸, Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ»Ğ¾Ğ³Ğ¸ĞºÑƒ-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ¿Ñ‹Ñ‚Ğ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ 135 Ñ‚Ñ‹ÑÑÑ‡ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ñ… ĞºĞ°Ñ€Ñ‚Ğ¾Ñ‡ĞµĞº Ğ¾Ğ¿Ñ‹Ñ‚Ğ° Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ½Ğ° 4.65% Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ SWE-bench Verified. MemGovern Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ ĞºĞ°Ğº Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ, Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ğ¸Ğ½Ñ„Ñ€Ğ°ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸, ÑƒĞ´Ğ¾Ğ±Ğ½ÑƒÑ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'Transforming GitHub Data into Smart Memory for Better Bug Fixing', 'desc': 'The MemGovern framework enhances autonomous software engineering agents by converting unstructured GitHub data into structured experiential memory. This transformation allows agents to leverage historical human experiences, overcoming the limitations of relying solely on local context for bug resolution. By creating 135,000 governed experience cards, MemGovern facilitates logic-driven retrieval of relevant expertise, significantly improving bug resolution rates. This innovative approach provides a plug-in solution for integrating memory infrastructure into software engineering agents.'}, 'zh': {'title': 'æå‡è‡ªä¸»è½¯ä»¶å·¥ç¨‹ä»£ç†çš„ç»éªŒè®°å¿†', 'desc': 'MemGovernæ¡†æ¶å°†éç»“æ„åŒ–çš„GitHubæ•°æ®è½¬åŒ–ä¸ºç»“æ„åŒ–çš„ç»éªŒè®°å¿†ï¼Œä»¥æ”¯æŒè‡ªä¸»è½¯ä»¶å·¥ç¨‹ä»£ç†çš„å·¥ä½œã€‚é€šè¿‡å¢å¼ºç»éªŒæ£€ç´¢ï¼ŒMemGovernæ˜¾è‘—æé«˜äº†é”™è¯¯è§£å†³ç‡ã€‚è¯¥æ¡†æ¶é€šè¿‡ç»éªŒæ²»ç†å°†äººç±»ç»éªŒè½¬æ¢ä¸ºé€‚åˆä»£ç†ä½¿ç”¨çš„ç»éªŒå¡ç‰‡ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§åŸºäºé€»è¾‘çš„ç»éªŒæœç´¢ç­–ç•¥ã€‚æœ€ç»ˆï¼ŒMemGovernç”Ÿæˆäº†135,000ä¸ªç»éªŒå¡ç‰‡ï¼Œä½¿å¾—åœ¨SWE-bench Verifiedä¸Šçš„è§£å†³ç‡æå‡äº†4.65%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.07022', 'title': 'Solar Open Technical Report', 'url': 'https://huggingface.co/papers/2601.07022', 'abstract': 'Solar Open presents a 102B-parameter bilingual Mixture-of-Experts language model that addresses data scarcity in underserved languages through synthetic data generation, progressive curriculum coordination, and scalable reinforcement learning optimization.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Solar Open, a 102B-parameter bilingual Mixture-of-Experts language model for underserved languages. Solar Open demonstrates a systematic methodology for building competitive LLMs by addressing three interconnected challenges. First, to train effectively despite data scarcity for underserved languages, we synthesize 4.5T tokens of high-quality, domain-specific, and RL-oriented data. Second, we coordinate this data through a progressive curriculum jointly optimizing composition, quality thresholds, and domain coverage across 20 trillion tokens. Third, to enable reasoning capabilities through scalable RL, we apply our proposed framework SnapPO for efficient optimization. Across benchmarks in English and Korean, Solar Open achieves competitive performance, demonstrating the effectiveness of this methodology for underserved language AI development.', 'score': 48, 'issue_id': 568, 'pub_date': '2026-01-11', 'pub_date_card': {'ru': '11 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 11', 'zh': '1æœˆ11æ—¥'}, 'hash': 'f88e2ba7e40d5615', 'authors': ['Sungrae Park', 'Sanghoon Kim', 'Jungho Cho', 'Gyoungjin Gim', 'Dawoon Jung', 'Mikyoung Cha', 'Eunhae Choo', 'Taekgyu Hong', 'Minbyul Jeong', 'SeHwan Joo', 'Minsoo Khang', 'Eunwon Kim', 'Minjeong Kim', 'Sujeong Kim', 'Yunsu Kim', 'Hyeonju Lee', 'Seunghyun Lee', 'Sukyung Lee', 'Siyoung Park', 'Gyungin Shin', 'Inseo Song', 'Wonho Song', 'Seonghoon Yang', 'Seungyoun Yi', 'Sanghoon Yoon', 'Jeonghyun Ko', 'Seyoung Song', 'Keunwoo Choi', 'Hwalsuk Lee', 'Sunghun Kim', 'Du-Seong Chang', 'Kyunghyun Cho', 'Junsuk Choe', 'Hwaran Lee', 'Jae-Gil Lee', 'KyungTae Lim', 'Alice Oh'], 'affiliations': ['Upstage'], 'pdf_title_img': 'assets/pdf/title_img/2601.07022.jpg', 'data': {'categories': ['#low_resource', '#training', '#data', '#rlhf', '#synthetic', '#optimization', '#open_source', '#multilingual', '#reasoning', '#architecture'], 'emoji': 'ğŸŒ', 'ru': {'title': 'Ğ¡Ğ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ ĞºĞ°Ğº Ğ¼Ğ¾ÑÑ‚ Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ² Ñ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµÑÑƒÑ€ÑĞ°Ğ¼Ğ¸', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Solar Open Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ¾Ğ¹ Mixture-of-Experts, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ°Ñ 102 Ğ¼Ğ»Ñ€Ğ´ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ÑÑ‰Ğ°Ñ Ğ´Ğ²Ğ° ÑĞ·Ñ‹ĞºĞ°. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ½ĞµÑ…Ğ²Ğ°Ñ‚ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ğ¼Ğ°Ğ»Ğ¾Ñ€ĞµÑÑƒÑ€ÑĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ°Ñ… Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ 4.5 Ñ‚Ñ€Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞĞ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‰ÑƒÑ Ğ¸Ñ… ÑĞ¾ÑÑ‚Ğ°Ğ², ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸ Ğ¾Ñ…Ğ²Ğ°Ñ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ¾Ğ¼ĞµĞ½Ğ¾Ğ² Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ¸Ğ· 20 Ñ‚Ñ€Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². Ğ”Ğ»Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ñ‘Ğ½ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ reinforcement learning Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ SnapPO, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ¾ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ³Ğ¾ Ğ¸ ĞºĞ¾Ñ€ĞµĞ¹ÑĞºĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ¾Ğ².'}, 'en': {'title': "Empowering Underserved Languages with Solar Open's 102B-Parameter Model", 'desc': 'The paper introduces Solar Open, a large bilingual Mixture-of-Experts language model with 102 billion parameters, designed to improve AI capabilities in underserved languages. It tackles the challenge of data scarcity by generating 4.5 trillion tokens of synthetic, high-quality data tailored for specific domains. The model employs a progressive curriculum that optimizes the composition and quality of the training data across a vast dataset of 20 trillion tokens. Additionally, it utilizes a scalable reinforcement learning framework called SnapPO to enhance reasoning abilities, achieving competitive results in benchmarks for English and Korean.'}, 'zh': {'title': 'ä¸ºæ¬ æœåŠ¡è¯­è¨€å¼€è¾Ÿæ–°å¤©åœ°çš„Solar Open', 'desc': 'Solar Open æ˜¯ä¸€ä¸ªæ‹¥æœ‰1020äº¿å‚æ•°çš„åŒè¯­æ··åˆä¸“å®¶è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³èµ„æºåŒ®ä¹è¯­è¨€çš„æ•°æ®ä¸è¶³é—®é¢˜ã€‚è¯¥æ¨¡å‹é€šè¿‡åˆæˆæ•°æ®ç”Ÿæˆã€æ¸è¿›å¼è¯¾ç¨‹åè°ƒå’Œå¯æ‰©å±•çš„å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–æ¥å®ç°é«˜æ•ˆè®­ç»ƒã€‚æˆ‘ä»¬åˆæˆäº†4.5ä¸‡äº¿ä¸ªé«˜è´¨é‡ã€ç‰¹å®šé¢†åŸŸå’Œé¢å‘å¼ºåŒ–å­¦ä¹ çš„æ•°æ®ï¼Œä»¥åº”å¯¹æ•°æ®ç¨€ç¼ºçš„æŒ‘æˆ˜ã€‚é€šè¿‡åœ¨è‹±è¯­å’ŒéŸ©è¯­çš„åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼ŒSolar Open å±•ç¤ºäº†å…¶åœ¨æ¬ æœåŠ¡è¯­è¨€AIå¼€å‘ä¸­çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.04745', 'title': 'KnowMe-Bench: Benchmarking Person Understanding for Lifelong Digital Companions', 'url': 'https://huggingface.co/papers/2601.04745', 'abstract': "Long-horizon memory benchmarks based on autobiographical narratives evaluate models' ability to infer stable motivations and decision principles through evidence-linked questions spanning factual recall, subjective state attribution, and principle-level reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t Existing long-horizon memory benchmarks mostly use multi-turn dialogues or synthetic user histories, which makes retrieval performance an imperfect proxy for person understanding. We present \\BenchName, a publicly releasable benchmark built from long-form autobiographical narratives, where actions, context, and inner thoughts provide dense evidence for inferring stable motivations and decision principles. \\BenchName~reconstructs each narrative into a flashback-aware, time-anchored stream and evaluates models with evidence-linked questions spanning factual recall, subjective state attribution, and principle-level reasoning. Across diverse narrative sources, retrieval-augmented systems mainly improve factual accuracy, while errors persist on temporally grounded explanations and higher-level inferences, highlighting the need for memory mechanisms beyond retrieval. Our data is in KnowMeBench{https://github.com/QuantaAlpha/KnowMeBench}.", 'score': 43, 'issue_id': 571, 'pub_date': '2026-01-08', 'pub_date_card': {'ru': '8 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 8', 'zh': '1æœˆ8æ—¥'}, 'hash': '9ef26640427ffeb7', 'authors': ['Tingyu Wu', 'Zhisheng Chen', 'Ziyan Weng', 'Shuhe Wang', 'Chenglong Li', 'Shuo Zhang', 'Sen Hu', 'Silin Wu', 'Qizhen Lan', 'Huacan Wang', 'Ronghao Chen'], 'affiliations': ['CITYU-DG', 'HAINNU', 'NUS', 'PKU', 'QuantaAlpha', 'THU', 'UCAS', 'UTHealth'], 'pdf_title_img': 'assets/pdf/title_img/2601.04745.jpg', 'data': {'categories': ['#reasoning', '#open_source', '#long_context'], 'emoji': 'ğŸ“–', 'ru': {'title': 'ĞŸĞ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ»Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ±Ğ¸Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ½Ğ°Ñ€Ñ€Ğ°Ñ‚Ğ¸Ğ²Ñ‹: ĞºĞ¾Ğ³Ğ´Ğ° Ğ¿Ğ¾Ğ¸ÑĞº Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº KnowMeBench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ»Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾Ğ±Ğ¸Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ½Ğ°Ñ€Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ¾Ğ². Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ñ… Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¸ Ğ¸Ğ»Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸, Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ğµ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ° Ğ² Ğ²Ğ¸Ğ´Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹, ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ñ… Ğ¼Ñ‹ÑĞ»ĞµĞ¹, Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ğµ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ñ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ¾Ğ² Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ñ‚Ñ€Ñ‘Ñ… ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸: Ñ„Ğ°ĞºÑ‚-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ†Ğ¸Ğ¸ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ Ğ¸ Ğ»Ğ¾Ğ³Ğ¸ĞºĞ¾-Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ², Ğ° Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑÑ‚ÑÑ Ğ¿Ñ€Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸ÑÑ… Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ñ… Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°Ñ…, Ñ‡Ñ‚Ğ¾ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ° Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ ÑĞ²ĞµÑ€Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°.'}, 'en': {'title': 'Unlocking Long-Term Memory in AI with Autobiographical Narratives', 'desc': "This paper introduces a new benchmark called \\\\BenchName, designed to assess machine learning models' understanding of long-term memory through autobiographical narratives. Unlike previous benchmarks that relied on dialogues or synthetic histories, \\\\BenchName uses real-life stories to provide rich context for evaluating models' abilities to infer motivations and decision-making principles. The benchmark includes questions that require factual recall, understanding of subjective states, and reasoning at a principle level. The findings indicate that while retrieval-augmented systems can improve factual accuracy, they struggle with temporally grounded explanations and higher-level reasoning, suggesting a need for advanced memory mechanisms in AI."}, 'zh': {'title': 'é•¿æ—¶è®°å¿†è¯„ä¼°çš„æ–°åŸºå‡†æµ‹è¯•', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„åŸºå‡†æµ‹è¯•ï¼Œåä¸º\\BenchNameï¼Œæ—¨åœ¨è¯„ä¼°æœºå™¨å­¦ä¹ æ¨¡å‹åœ¨é•¿æ—¶é—´è®°å¿†æ–¹é¢çš„è¡¨ç°ã€‚è¯¥åŸºå‡†æµ‹è¯•åŸºäºè‡ªä¼ å™è¿°ï¼Œæä¾›äº†ä¸°å¯Œçš„è¯æ®æ¥æ¨æ–­ç¨³å®šçš„åŠ¨æœºå’Œå†³ç­–åŸåˆ™ã€‚ä¸ç°æœ‰çš„åŸºå‡†æµ‹è¯•ä¸åŒï¼Œ\\BenchNameå…³æ³¨äºäº‹å®å›å¿†ã€ä¸»è§‚çŠ¶æ€å½’å› å’ŒåŸåˆ™çº§æ¨ç†ç­‰æ–¹é¢ã€‚ç ”ç©¶è¡¨æ˜ï¼Œç°æœ‰çš„æ£€ç´¢å¢å¼ºç³»ç»Ÿåœ¨äº‹å®å‡†ç¡®æ€§ä¸Šæœ‰æ‰€æé«˜ï¼Œä½†åœ¨æ—¶é—´åŸºç¡€çš„è§£é‡Šå’Œæ›´é«˜å±‚æ¬¡çš„æ¨ç†ä¸Šä»å­˜åœ¨é”™è¯¯ï¼Œå¼ºè°ƒäº†éœ€è¦è¶…è¶Šç®€å•æ£€ç´¢çš„è®°å¿†æœºåˆ¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.08225', 'title': 'User-Oriented Multi-Turn Dialogue Generation with Tool Use at scale', 'url': 'https://huggingface.co/papers/2601.08225', 'abstract': 'Large reasoning models enable scalable multi-turn dialogue generation through automated task-oriented simulation and user-oriented behavioral modeling for enhanced human-agent interaction datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t The recent paradigm shift toward large reasoning models (LRMs) as autonomous agents has intensified the demand for sophisticated, multi-turn tool-use capabilities. Yet, existing datasets and data-generation approaches are limited by static, predefined toolsets that cannot scale to the complexity of open-ended human-agent collaboration. To address this, we initially developed a framework for automated task-oriented multi-turn dialogue generation at scale, utilizing an LRM-based simulator to dynamically generate high-value, domain-specific tools to solve specified tasks. However, we observe that a purely task-oriented design often results in "solely task-solving" trajectories, where the agent completes the objective with minimal interaction, failing to generate the high turn-count conversations seen in realistic scenarios. To bridge this gap, we shift toward a user-oriented simulation paradigm. By decoupling task generation from a dedicated user simulator that mimics human behavioral rules - such as incremental request-making and turn-by-turn feedback - we facilitate more authentic, extended multi-turn dialogues that reflect the iterative nature of real-world problem solving. Our generation pipeline operates as a versatile, plug-and-play module capable of initiating generation from any state, ensuring high scalability in producing extended tool-use data. Furthermore, by facilitating multiple task completions within a single trajectory, it yields a high-density dataset that reflects the multifaceted demands of real-world human-agent interaction.', 'score': 39, 'issue_id': 568, 'pub_date': '2026-01-13', 'pub_date_card': {'ru': '13 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 13', 'zh': '1æœˆ13æ—¥'}, 'hash': '4d4a176d79a2d78d', 'authors': ['Jungho Cho', 'Minbyul Jeong', 'Sungrae Park'], 'affiliations': ['Upstage AI'], 'pdf_title_img': 'assets/pdf/title_img/2601.08225.jpg', 'data': {'categories': ['#synthetic', '#reasoning'], 'emoji': 'ğŸ—£ï¸', 'ru': {'title': 'Ğ¡Ğ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ² Ñ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¼ Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ¼, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑÑƒĞ³ÑƒĞ±Ğ¾ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½Ğ¾-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ğµ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¸, Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ¼Ñƒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´ Ğº Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¾-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğµ Ñ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ¾Ğ¼, Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¼ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°. Ğ‘Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ», Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğµ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ°Ñ ÑĞ²ÑĞ·ÑŒ, ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ Ğ±Ğ¾Ğ»ĞµĞµ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¸. ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ñ‹ Ğ¾Ğ±Ğ»Ğ°Ğ´Ğ°ÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ñ‚Ñ€Ğ°Ğ¶Ğ°ÑÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ² Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¸ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ°.'}, 'en': {'title': 'Enhancing Human-Agent Dialogue with Dynamic User Simulation', 'desc': 'This paper discusses the development of large reasoning models (LRMs) for improving multi-turn dialogue generation in human-agent interactions. It highlights the limitations of existing datasets that rely on static tools, which do not capture the complexity of real-world conversations. The authors propose a new framework that separates task generation from user simulation, allowing for more realistic and extended dialogues that mimic human behavior. This approach results in a scalable system that can generate diverse, high-density datasets for training agents in task-oriented scenarios.'}, 'zh': {'title': 'æå‡äººæœºäº¤äº’çš„å¤šè½®å¯¹è¯ç”Ÿæˆ', 'desc': 'æœ¬æ–‡æ¢è®¨äº†å¤§å‹æ¨ç†æ¨¡å‹åœ¨å¤šè½®å¯¹è¯ç”Ÿæˆä¸­çš„åº”ç”¨ï¼Œæ—¨åœ¨æå‡äººæœºäº¤äº’çš„æ•°æ®é›†è´¨é‡ã€‚ç°æœ‰çš„æ•°æ®é›†å’Œç”Ÿæˆæ–¹æ³•å—é™äºé™æ€çš„å·¥å…·é›†ï¼Œæ— æ³•æ»¡è¶³å¼€æ”¾å¼äººæœºåä½œçš„å¤æ‚æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œç ”ç©¶è€…ä»¬å¼€å‘äº†ä¸€ç§åŸºäºLRMçš„è‡ªåŠ¨åŒ–ä»»åŠ¡å¯¼å‘å¯¹è¯ç”Ÿæˆæ¡†æ¶ï¼Œèƒ½å¤ŸåŠ¨æ€ç”Ÿæˆé«˜ä»·å€¼çš„é¢†åŸŸç‰¹å®šå·¥å…·ã€‚é€šè¿‡å¼•å…¥ç”¨æˆ·å¯¼å‘çš„æ¨¡æ‹Ÿæ–¹æ³•ï¼Œç ”ç©¶è€…ä»¬å®ç°äº†æ›´çœŸå®çš„å¤šè½®å¯¹è¯ï¼Œåæ˜ äº†ç°å®ä¸–ç•Œé—®é¢˜è§£å†³çš„è¿­ä»£ç‰¹æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.24965', 'title': 'ShowUI-Ï€: Flow-based Generative Models as GUI Dexterous Hands', 'url': 'https://huggingface.co/papers/2512.24965', 'abstract': "Building intelligent agents capable of dexterous manipulation is essential for achieving human-like automation in both robotics and digital environments. However, existing GUI agents rely on discrete click predictions (x,y), which prohibits free-form, closed-loop trajectories (e.g. dragging a progress bar) that require continuous, on-the-fly perception and adjustment. In this work, we develop ShowUI-Ï€, the first flow-based generative model as GUI dexterous hand, featuring the following designs: (i) Unified Discrete-Continuous Actions, integrating discrete clicks and continuous drags within a shared model, enabling flexible adaptation across diverse interaction modes; (ii) Flow-based Action Generation for drag modeling, which predicts incremental cursor adjustments from continuous visual observations via a lightweight action expert, ensuring smooth and stable trajectories; (iii) Drag Training data and Benchmark, where we manually collect and synthesize 20K drag trajectories across five domains (e.g. PowerPoint, Adobe Premiere Pro), and introduce ScreenDrag, a benchmark with comprehensive online and offline evaluation protocols for assessing GUI agents' drag capabilities. Our experiments show that proprietary GUI agents still struggle on ScreenDrag (e.g. Operator scores 13.27, and the best Gemini-2.5-CUA reaches 22.18). In contrast, ShowUI-Ï€ achieves 26.98 with only 450M parameters, underscoring both the difficulty of the task and the effectiveness of our approach. We hope this work advances GUI agents toward human-like dexterous control in digital world. The code is available at https://github.com/showlab/showui-pi.", 'score': 34, 'issue_id': 568, 'pub_date': '2026-12-31', 'pub_date_card': {'ru': '31 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 31', 'zh': '12æœˆ31æ—¥'}, 'hash': 'eee0e07745c9406a', 'authors': ['Siyuan Hu', 'Kevin Qinghong Lin', 'Mike Zheng Shou'], 'affiliations': ['Show Lab, National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2512.24965.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#agents', '#small_models', '#architecture'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞŸĞ»Ğ°Ğ²Ğ½Ñ‹Ğµ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°Ñ…: Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ ShowUI-Ï€ â€” Ğ¿ĞµÑ€Ğ²Ğ°Ñ Ğ¿Ğ¾Ñ‚Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ¼, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ°Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ ĞºĞ°Ğº Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ ĞºĞ»Ğ¸ĞºĞ¸, Ñ‚Ğ°Ğº Ğ¸ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğµ Ğ¿ĞµÑ€ĞµÑ‚Ğ°ÑĞºĞ¸Ğ²Ğ°Ğ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ Ğ¸ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ»Ñ‘Ğ³ĞºĞ¸Ğ¹ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ»Ğ°Ğ²Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ ĞºÑƒÑ€ÑĞ¾Ñ€Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. Ğ”Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ±Ñ€Ğ°Ğ»Ğ¸ Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ 20 Ñ‚Ñ‹ÑÑÑ‡ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¿ĞµÑ€ĞµÑ‚Ğ°ÑĞºĞ¸Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ScreenDrag Ñ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ»Ğ°Ğ¼Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ğº Ğ² Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½Ğµ, Ñ‚Ğ°Ğº Ğ¸ Ğ¾Ñ„Ğ»Ğ°Ğ¹Ğ½Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ShowUI-Ï€ Ñ 450 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¸Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ»Ğ¾Ğ²ĞºĞ¸Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¹ Ğ² Ñ†Ğ¸Ñ„Ñ€Ğ¾Ğ²Ğ¾Ğ¹ ÑÑ€ĞµĞ´Ğµ.'}, 'en': {'title': 'Empowering GUI Agents with Human-like Dexterity', 'desc': "This paper introduces ShowUI-Ï€, a novel flow-based generative model designed to enhance GUI agents' dexterous manipulation capabilities. Unlike traditional models that only predict discrete click actions, ShowUI-Ï€ integrates both discrete and continuous actions, allowing for more fluid interactions like dragging. The model utilizes a lightweight action expert to generate smooth cursor movements based on real-time visual input, improving the agent's adaptability across various tasks. The authors also present a new benchmark, ScreenDrag, to evaluate drag performance, demonstrating that ShowUI-Ï€ outperforms existing agents significantly in this challenging domain."}, 'zh': {'title': 'æå‡GUIæ™ºèƒ½ä»£ç†çš„çµæ´»æ“æ§èƒ½åŠ›', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºShowUI-Ï€çš„æµå¼ç”Ÿæˆæ¨¡å‹ï¼Œæ—¨åœ¨æå‡å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰æ™ºèƒ½ä»£ç†çš„çµæ´»æ€§å’Œæ“æ§èƒ½åŠ›ã€‚è¯¥æ¨¡å‹ç»“åˆäº†ç¦»æ•£ç‚¹å‡»å’Œè¿ç»­æ‹–åŠ¨çš„åŠ¨ä½œï¼Œèƒ½å¤Ÿåœ¨å¤šç§äº¤äº’æ¨¡å¼ä¸‹è‡ªå¦‚é€‚åº”ã€‚é€šè¿‡æµå¼åŠ¨ä½œç”Ÿæˆï¼ŒShowUI-Ï€èƒ½å¤Ÿæ ¹æ®å®æ—¶è§†è§‰ä¿¡æ¯é¢„æµ‹å…‰æ ‡çš„ç»†å¾®è°ƒæ•´ï¼Œä»è€Œå®ç°å¹³æ»‘ç¨³å®šçš„æ‹–åŠ¨è½¨è¿¹ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æ„å»ºäº†ä¸€ä¸ªåŒ…å«2ä¸‡æ¡æ‹–åŠ¨è½¨è¿¹çš„æ•°æ®é›†å’Œè¯„ä¼°åŸºå‡†ScreenDragï¼Œä»¥æµ‹è¯•GUIä»£ç†çš„æ‹–åŠ¨èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.08079', 'title': 'MemoBrain: Executive Memory as an Agentic Brain for Reasoning', 'url': 'https://huggingface.co/papers/2601.08079', 'abstract': 'Memory management in tool-augmented agents is crucial for maintaining coherent, goal-directed reasoning over extended tasks, requiring explicit mechanisms to track and organize reasoning steps within constrained contexts.  \t\t\t\t\tAI-generated summary \t\t\t\t Complex reasoning in tool-augmented agent frameworks is inherently long-horizon, causing reasoning traces and transient tool artifacts to accumulate and strain the bounded working context of large language models. Without explicit memory mechanisms, such accumulation disrupts logical continuity and undermines task alignment. This positions memory not as an auxiliary efficiency concern, but as a core component for sustaining coherent, goal-directed reasoning over long horizons.   We propose MemoBrain, an executive memory model for tool-augmented agents that constructs a dependency-aware memory over reasoning steps, capturing salient intermediate states and their logical relations. Operating as a co-pilot alongside the reasoning agent, MemoBrain organizes reasoning progress without blocking execution and actively manages the working context. Specifically, it prunes invalid steps, folds completed sub-trajectories, and preserves a compact, high-salience reasoning backbone under a fixed context budget. Together, these mechanisms enable explicit cognitive control over reasoning trajectories rather than passive context accumulation.   We evaluate MemoBrain on challenging long-horizon benchmarks, including GAIA, WebWalker, and BrowseComp-Plus, demonstrating consistent improvements over strong baselines.', 'score': 30, 'issue_id': 574, 'pub_date': '2026-01-12', 'pub_date_card': {'ru': '12 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 12', 'zh': '1æœˆ12æ—¥'}, 'hash': 'd40e8e9bfd93aabf', 'authors': ['Hongjin Qian', 'Zhao Cao', 'Zheng Liu'], 'affiliations': ['Beijing Academy of Artificial Intelligence', 'Gaoling School of Artificial Intelligence, Renmin University of China'], 'pdf_title_img': 'assets/pdf/title_img/2601.08079.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#long_context', '#agents'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞšĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ: ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¾Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° MemoBrain â€” Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ Ğ³Ñ€Ğ°Ñ„ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹ Ğ½Ğ°Ğ´ ÑˆĞ°Ğ³Ğ°Ğ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ ĞºĞ°Ğº Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ½Ğ¸Ğº Ñ€ÑĞ´Ğ¾Ğ¼ Ñ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğ¼ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ¼, Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑ Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ LLM Ğ¿ÑƒÑ‚Ñ‘Ğ¼ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ², ÑĞ²Ğ¾Ñ€Ğ°Ñ‡Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ·Ğ°Ğ²ĞµÑ€ÑˆÑ‘Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ğ¿ÑƒÑ‚ĞµĞ¹ Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾ÑÑ‚Ğ¾Ğ²Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ²Ğ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ğ°Ğ¶Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¾Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡, Ñ‚Ğ°Ğº ĞºĞ°Ğº Ğ±ĞµĞ· Ğ½ĞµĞ³Ğ¾ Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ¸Ğµ Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ğ¾Ğ² Ğ½Ğ°Ñ€ÑƒÑˆĞ°ĞµÑ‚ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ñ†ĞµĞ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… GAIA, WebWalker Ğ¸ BrowseComp-Plus Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ½Ğ°Ğ´ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Enhancing Long-Horizon Reasoning with MemoBrain', 'desc': 'This paper addresses the challenge of memory management in tool-augmented agents, which is essential for maintaining effective reasoning over long tasks. It introduces MemoBrain, a memory model that organizes and tracks reasoning steps while managing the limited context of large language models. MemoBrain enhances cognitive control by pruning irrelevant steps and preserving important reasoning states, ensuring logical continuity. The model is evaluated on various benchmarks, showing significant improvements in performance compared to existing methods.'}, 'zh': {'title': 'å†…å­˜ç®¡ç†ï¼šå·¥å…·å¢å¼ºä»£ç†çš„æ ¸å¿ƒ', 'desc': 'åœ¨å·¥å…·å¢å¼ºä»£ç†ä¸­ï¼Œå†…å­˜ç®¡ç†å¯¹äºç»´æŒè¿è´¯çš„ç›®æ ‡å¯¼å‘æ¨ç†è‡³å…³é‡è¦ã€‚æœ¬æ–‡æå‡ºäº†MemoBrainï¼Œä¸€ä¸ªæ‰§è¡Œå†…å­˜æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨æ¨ç†æ­¥éª¤ä¸­æ„å»ºä¾èµ–æ„ŸçŸ¥çš„å†…å­˜ï¼Œæ•æ‰é‡è¦çš„ä¸­é—´çŠ¶æ€åŠå…¶é€»è¾‘å…³ç³»ã€‚MemoBrainä½œä¸ºæ¨ç†ä»£ç†çš„å‰¯é©¾é©¶ï¼Œç»„ç»‡æ¨ç†è¿›å±•å¹¶ä¸»åŠ¨ç®¡ç†å·¥ä½œä¸Šä¸‹æ–‡ï¼Œé¿å…æ— æ•ˆæ­¥éª¤çš„ç§¯ç´¯ã€‚é€šè¿‡åœ¨é•¿æ—¶é—´åŸºå‡†æµ‹è¯•ä¸­è¯„ä¼°ï¼ŒMemoBrainæ˜¾ç¤ºå‡ºç›¸è¾ƒäºå¼ºåŸºçº¿çš„ä¸€è‡´æ€§æ”¹è¿›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.06487', 'title': 'ArenaRL: Scaling RL for Open-Ended Agents via Tournament-based Relative Ranking', 'url': 'https://huggingface.co/papers/2601.06487', 'abstract': 'Reinforcement learning for large language model agents suffers from discrimination collapse in open-ended tasks due to pointwise scalar scoring, which ArenaRL addresses through relative ranking and pairwise evaluation mechanisms.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning has substantially improved the performance of LLM agents on tasks with verifiable outcomes, but it still struggles on open-ended agent tasks with vast solution spaces (e.g., complex travel planning). Due to the absence of objective ground-truth for these tasks, current RL algorithms largely rely on reward models that assign scalar scores to individual responses. We contend that such pointwise scoring suffers from an inherent discrimination collapse: the reward model struggles to distinguish subtle advantages among different trajectories, resulting in scores within a group being compressed into a narrow range. Consequently, the effective reward signal becomes dominated by noise from the reward model, leading to optimization stagnation. To address this, we propose ArenaRL, a reinforcement learning paradigm that shifts from pointwise scalar scoring to intra-group relative ranking. ArenaRL introduces a process-aware pairwise evaluation mechanism, employing multi-level rubrics to assign fine-grained relative scores to trajectories. Additionally, we construct an intra-group adversarial arena and devise a tournament-based ranking scheme to obtain stable advantage signals. Empirical results confirm that the built seeded single-elimination scheme achieves nearly equivalent advantage estimation accuracy to full pairwise comparisons with O(N^2) complexity, while operating with only O(N) complexity, striking an optimal balance between efficiency and precision. Furthermore, to address the lack of full-cycle benchmarks for open-ended agents, we build Open-Travel and Open-DeepResearch, two high-quality benchmarks featuring a comprehensive pipeline covering SFT, RL training, and multi-dimensional evaluation. Extensive experiments show that ArenaRL substantially outperforms standard RL baselines, enabling LLM agents to generate more robust solutions for complex real-world tasks.', 'score': 28, 'issue_id': 566, 'pub_date': '2026-01-10', 'pub_date_card': {'ru': '10 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 10', 'zh': '1æœˆ10æ—¥'}, 'hash': 'fa021a43db4c9cb5', 'authors': ['Qiang Zhang', 'Boli Chen', 'Fanrui Zhang', 'Ruixue Ding', 'Shihang Wang', 'Qiuchen Wang', 'Yinfeng Huang', 'Haonan Zhang', 'Rongxiang Zhu', 'Pengyong Wang', 'Ailin Ren', 'Xin Li', 'Pengjun Xie', 'Jiawei Liu', 'Ning Guo', 'Jingren Zhou', 'Zheng-Jun Zha'], 'affiliations': ['Amap, Alibaba Group', 'Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2601.06487.jpg', 'data': {'categories': ['#alignment', '#benchmark', '#rl', '#optimization', '#reasoning', '#dataset', '#agents', '#training'], 'emoji': 'ğŸ†', 'ru': {'title': 'ĞÑ‚ Ğ¿Ğ¾Ñ‚Ğ¾Ñ‡ĞµÑ‡Ğ½Ñ‹Ñ… Ğ¾Ñ†ĞµĞ½Ğ¾Ğº Ğº Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ: Ñ‚ÑƒÑ€Ğ½Ğ¸Ñ€Ğ½Ñ‹Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ± Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ ArenaRL Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ±ĞµĞ· Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ĞºĞ¾Ğ»Ğ»Ğ°Ğ¿ÑĞ° Ğ´Ğ¸ÑĞºÑ€Ğ¸Ğ¼Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸, ĞºĞ¾Ğ³Ğ´Ğ° ÑĞºĞ°Ğ»ÑÑ€Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ½Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ°ÑÑ‚ Ñ‚Ğ¾Ğ½ĞºĞ¸Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ÑĞ¼Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº ÑÑ‚Ğ°Ğ³Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ğ¾Ñ‚Ğ¾Ñ‡ĞµÑ‡Ğ½Ğ¾Ğ³Ğ¾ ÑĞºĞ¾Ñ€Ğ¸Ğ½Ğ³Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¿Ğ°Ñ€Ğ½Ñ‹Ğ¼Ğ¸ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ñ‚ÑƒÑ€Ğ½Ğ¸Ñ€Ğ½Ğ¾Ğ¹ ÑÑ…ĞµĞ¼Ğ¾Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¿Ğ°Ñ€Ğ½Ğ¾Ğ³Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ O(N) Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ O(NÂ²). Ğ”Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ñ‹ Ğ´Ğ²Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ° Open-Travel Ğ¸ Open-DeepResearch, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğµ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ°.'}, 'en': {'title': 'ArenaRL: Elevating Reinforcement Learning for Language Models through Relative Ranking', 'desc': 'This paper discusses the challenges of using reinforcement learning (RL) for large language model (LLM) agents in open-ended tasks, where traditional pointwise scalar scoring leads to discrimination collapse. The authors introduce ArenaRL, a new RL framework that replaces scalar scoring with relative ranking and pairwise evaluations to better differentiate between various solutions. By implementing a tournament-based ranking system and multi-level rubrics, ArenaRL provides more stable and precise reward signals, enhancing the optimization process. The results demonstrate that ArenaRL significantly improves the performance of LLM agents on complex tasks compared to standard RL methods.'}, 'zh': {'title': 'ArenaRLï¼šæå‡å¼€æ”¾å¼ä»»åŠ¡çš„å¼ºåŒ–å­¦ä¹ æ•ˆæœ', 'desc': 'å¼ºåŒ–å­¦ä¹ åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†çš„å¼€æ”¾å¼ä»»åŠ¡ä¸­é¢ä¸´ç€è¯„åˆ†æ­§è§†å´©æºƒçš„é—®é¢˜ã€‚ä¼ ç»Ÿçš„ç‚¹å¯¹ç‚¹æ ‡é‡è¯„åˆ†æ–¹æ³•éš¾ä»¥æœ‰æ•ˆåŒºåˆ†ä¸åŒè·¯å¾„ä¹‹é—´çš„ç»†å¾®ä¼˜åŠ¿ï¼Œå¯¼è‡´å¥–åŠ±ä¿¡å·è¢«å™ªå£°ä¸»å¯¼ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼ŒArenaRLæå‡ºäº†ä¸€ç§æ–°çš„å¼ºåŒ–å­¦ä¹ èŒƒå¼ï¼Œé€šè¿‡ç»„å†…ç›¸å¯¹æ’åå’Œæˆå¯¹è¯„ä¼°æœºåˆ¶æ¥ä¼˜åŒ–è¯„åˆ†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒArenaRLåœ¨å¤æ‚ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºæ ‡å‡†å¼ºåŒ–å­¦ä¹ åŸºçº¿ï¼Œèƒ½å¤Ÿç”Ÿæˆæ›´å¼ºå¥çš„è§£å†³æ–¹æ¡ˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.08584', 'title': 'Ministral 3', 'url': 'https://huggingface.co/papers/2601.08584', 'abstract': 'The Ministral 3 series consists of parameter-efficient dense language models with three sizes (3B, 8B, 14B) and three variants per size, trained using cascade distillation for compute-constrained applications.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce the Ministral 3 series, a family of parameter-efficient dense language models designed for compute and memory constrained applications, available in three model sizes: 3B, 8B, and 14B parameters. For each model size, we release three variants: a pretrained base model for general-purpose use, an instruction finetuned, and a reasoning model for complex problem-solving. In addition, we present our recipe to derive the Ministral 3 models through Cascade Distillation, an iterative pruning and continued training with distillation technique. Each model comes with image understanding capabilities, all under the Apache 2.0 license.', 'score': 25, 'issue_id': 566, 'pub_date': '2026-01-13', 'pub_date_card': {'ru': '13 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 13', 'zh': '1æœˆ13æ—¥'}, 'hash': '3895109cdeca63b2', 'authors': ['Alexander H. Liu', 'Kartik Khandelwal', 'Sandeep Subramanian', 'Victor Jouault', 'Abhinav Rastogi', 'Adrien SadÃ©', 'Alan Jeffares', 'Albert Jiang', 'Alexandre Cahill', 'Alexandre Gavaudan', 'Alexandre Sablayrolles', 'AmÃ©lie HÃ©liou', 'Amos You', 'Andy Ehrenberg', 'Andy Lo', 'Anton Eliseev', 'Antonia Calvi', 'Avinash Sooriyarachchi', 'Baptiste Bout', 'Baptiste RoziÃ¨re', 'Baudouin De Monicault', 'ClÃ©mence Lanfranchi', 'Corentin Barreau', 'Cyprien Courtot', 'Daniele Grattarola', 'Darius Dabert', 'Diego de las Casas', 'Elliot Chane-Sane', 'Faruk Ahmed', 'Gabrielle Berrada', 'GaÃ«tan Ecrepont', 'Gauthier Guinet', 'Georgii Novikov', 'Guillaume Kunsch', 'Guillaume Lample', 'Guillaume Martin', 'Gunshi Gupta', 'Jan Ludziejewski', 'Jason Rute', 'Joachim Studnia', 'Jonas Amar', 'JosÃ©phine Delas', 'Josselin Somerville Roberts', 'Karmesh Yadav', 'Khyathi Chandu', 'Kush Jain', 'Laurence Aitchison', 'Laurent Fainsin', 'LÃ©onard Blier', 'Lingxiao Zhao', 'Louis Martin', 'Lucile Saulnier', 'Luyu Gao', 'Maarten Buyl', 'Margaret Jennings', 'Marie Pellat', 'Mark Prins', 'Mathieu PoirÃ©e', 'Mathilde Guillaumin', 'Matthieu Dinot', 'Matthieu Futeral', 'Maxime Darrin', 'Maximilian Augustin', 'Mia Chiquier', 'Michel Schimpf', 'Nathan Grinsztajn', 'Neha Gupta', 'Nikhil Raghuraman', 'Olivier Bousquet', 'Olivier Duchenne', 'Patricia Wang', 'Patrick von Platen', 'Paul Jacob', 'Paul Wambergue', 'Paula Kurylowicz', 'Pavankumar Reddy Muddireddy', 'PhilomÃ¨ne Chagniot', 'Pierre Stock', 'Pravesh Agrawal', 'Quentin Torroba', 'Romain Sauvestre', 'Roman Soletskyi', 'Rupert Menneer', 'Sagar Vaze', 'Samuel Barry', 'Sanchit Gandhi', 'Siddhant Waghjale', 'Siddharth Gandhi', 'Soham Ghosh', 'Srijan Mishra', 'Sumukh Aithal', 'Szymon Antoniak', 'Teven Le Scao', 'ThÃ©o Cachet', 'Theo Simon Sorg', 'Thibaut Lavril', 'Thiziri Nait Saada', 'Thomas Chabal', 'Thomas Foubert', 'Thomas Robert', 'Thomas Wang', 'Tim Lawson', 'Tom Bewley', 'Tom Bewley', 'Tom Edwards', 'Umar Jamil', 'Umberto Tomasini', 'Valeriia Nemychnikova', 'Van Phung', 'Vincent MaladiÃ¨re', 'Virgile Richard', 'Wassim Bouaziz', 'Wen-Ding Li', 'William Marshall', 'Xinghui Li', 'Xinyu Yang', 'Yassine El Ouahidi', 'Yihan Wang', 'Yunhao Tang', 'Zaccharie Ramzi'], 'affiliations': ['Mistral AI'], 'pdf_title_img': 'assets/pdf/title_img/2601.08584.jpg', 'data': {'categories': [], 'emoji': 'ğŸš€', 'ru': {'title': 'ĞšĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ²ÑĞµÑ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ñ‹ Ministral 3 â€” ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ Ğ¾Ñ‚ 3 Ğ´Ğ¾ 14 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµÑÑƒÑ€ÑĞ°Ğ¼Ğ¸. Ğ”Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ñ‹ Ñ‚Ñ€Ğ¸ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ°: Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ°Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ²ĞµÑ€ÑĞ¸Ñ Ñ fine-tuning Ğ½Ğ° Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑÑ… Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ñ‹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ cascade distillation â€” Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ¸ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞµĞ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. Ğ’ÑĞµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑÑ‚ÑÑ Ğ¿Ğ¾Ğ´ Ğ»Ğ¸Ñ†ĞµĞ½Ğ·Ğ¸ĞµĞ¹ Apache 2.0.'}, 'en': {'title': 'Efficient Language Models for Resource-Constrained Applications', 'desc': 'The Ministral 3 series introduces a set of dense language models that are efficient in terms of parameters, making them suitable for applications with limited computational resources. Each model size, which includes 3B, 8B, and 14B parameters, offers three distinct variants: a general-purpose pretrained model, an instruction-tuned model for specific tasks, and a reasoning model aimed at solving complex problems. The models are developed using a technique called Cascade Distillation, which involves iterative pruning and continued training to enhance performance. Additionally, these models are equipped with image understanding capabilities and are released under the Apache 2.0 license.'}, 'zh': {'title': 'é«˜æ•ˆå¯†é›†è¯­è¨€æ¨¡å‹ï¼ŒåŠ©åŠ›è®¡ç®—å—é™åº”ç”¨', 'desc': 'Ministral 3ç³»åˆ—æ˜¯ä¸€ä¸ªé«˜æ•ˆçš„å¯†é›†è¯­è¨€æ¨¡å‹å®¶æ—ï¼Œä¸“ä¸ºè®¡ç®—å’Œå†…å­˜å—é™çš„åº”ç”¨è€Œè®¾è®¡ã€‚è¯¥ç³»åˆ—åŒ…æ‹¬ä¸‰ç§æ¨¡å‹å¤§å°ï¼š3Bã€8Bå’Œ14Bå‚æ•°ï¼Œæ¯ç§å¤§å°éƒ½æœ‰ä¸‰ä¸ªå˜ä½“ï¼Œåˆ†åˆ«æ˜¯é€šç”¨é¢„è®­ç»ƒæ¨¡å‹ã€æŒ‡ä»¤å¾®è°ƒæ¨¡å‹å’Œå¤æ‚é—®é¢˜è§£å†³æ¨¡å‹ã€‚æˆ‘ä»¬é‡‡ç”¨çº§è”è’¸é¦çš„æ–¹æ³•æ¥è®­ç»ƒè¿™äº›æ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ç§è¿­ä»£å‰ªæå’ŒæŒç»­è®­ç»ƒçš„æŠ€æœ¯ã€‚æ¯ä¸ªæ¨¡å‹éƒ½å…·å¤‡å›¾åƒç†è§£èƒ½åŠ›ï¼Œå¹¶åœ¨Apache 2.0è®¸å¯è¯ä¸‹å‘å¸ƒã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.07264', 'title': 'The Confidence Dichotomy: Analyzing and Mitigating Miscalibration in Tool-Use Agents', 'url': 'https://huggingface.co/papers/2601.07264', 'abstract': "Tool-integrated language model agents exhibit different calibration behaviors based on tool type, with a reinforcement learning framework improving both task accuracy and reliable uncertainty estimation across diverse domains.  \t\t\t\t\tAI-generated summary \t\t\t\t Autonomous agents based on large language models (LLMs) are rapidly evolving to handle multi-turn tasks, but ensuring their trustworthiness remains a critical challenge. A fundamental pillar of this trustworthiness is calibration, which refers to an agent's ability to express confidence that reliably reflects its actual performance. While calibration is well-established for static models, its dynamics in tool-integrated agentic workflows remain underexplored. In this work, we systematically investigate verbalized calibration in tool-use agents, revealing a fundamental confidence dichotomy driven by tool type. Specifically, our pilot study identifies that evidence tools (e.g., web search) systematically induce severe overconfidence due to inherent noise in retrieved information, while verification tools (e.g., code interpreters) can ground reasoning through deterministic feedback and mitigate miscalibration. To robustly improve calibration across tool types, we propose a reinforcement learning (RL) fine-tuning framework that jointly optimizes task accuracy and calibration, supported by a holistic benchmark of reward designs. We demonstrate that our trained agents not only achieve superior calibration but also exhibit robust generalization from local training environments to noisy web settings and to distinct domains such as mathematical reasoning. Our results highlight the necessity of domain-specific calibration strategies for tool-use agents. More broadly, this work establishes a foundation for building self-aware agents that can reliably communicate uncertainty in high-stakes, real-world deployments.", 'score': 20, 'issue_id': 566, 'pub_date': '2026-01-12', 'pub_date_card': {'ru': '12 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 12', 'zh': '1æœˆ12æ—¥'}, 'hash': '3b69d6831de81c4b', 'authors': ['Weihao Xuan', 'Qingcheng Zeng', 'Heli Qi', 'Yunze Xiao', 'Junjue Wang', 'Naoto Yokoya'], 'affiliations': ['Carnegie Mellon University', 'Northwestern University', 'RIKEN AIP', 'The University of Tokyo', 'Waseda University'], 'pdf_title_img': 'assets/pdf/title_img/2601.07264.jpg', 'data': {'categories': ['#benchmark', '#agents', '#rl'], 'emoji': 'ğŸ¯', 'ru': {'title': 'ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ¶Ğ°Ñ‚ÑŒ Ğ½ĞµÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞ¸ (ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² ÑĞ²Ğ¾Ğ¸Ñ… Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸ÑÑ…) Ğ² LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ñ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿ĞµÑ€ĞµĞ¾Ñ†ĞµĞ½ĞºÑƒ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ·-Ğ·Ğ° ÑˆÑƒĞ¼Ğ° Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ° Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ (Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ‚Ğ¾Ñ€Ñ‹ ĞºĞ¾Ğ´Ğ°) Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ÑÑ‚ ÑĞ½Ğ¸Ğ·Ğ¸Ñ‚ÑŒ ÑÑ‚Ñƒ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ´ĞµÑ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ñ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºÑƒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ…Ğ¾Ñ€Ğ¾ÑˆÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, Ğ²ĞµĞ±-ÑÑ€ĞµĞ´Ñ‹ Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ½Ñ‹Ğµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸.'}, 'en': {'title': 'Enhancing Trustworthiness in AI: Calibration through Reinforcement Learning', 'desc': "This paper explores how tool-integrated language model agents behave differently in terms of calibration, which is their ability to express confidence accurately about their performance. It identifies that different types of tools, like evidence tools and verification tools, affect the agents' confidence levels, with evidence tools often leading to overconfidence due to noisy information. The authors propose a reinforcement learning framework that enhances both the accuracy of tasks and the reliability of uncertainty estimation across various domains. Their findings emphasize the importance of tailored calibration strategies for agents that use different tools, paving the way for more trustworthy AI systems in real-world applications."}, 'zh': {'title': 'æå‡å·¥å…·é›†æˆä»£ç†çš„æ ¡å‡†èƒ½åŠ›', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†é›†æˆå·¥å…·çš„è¯­è¨€æ¨¡å‹ä»£ç†åœ¨ä¸åŒå·¥å…·ç±»å‹ä¸‹çš„æ ¡å‡†è¡Œä¸ºã€‚æˆ‘ä»¬å‘ç°ï¼Œè¯æ®å·¥å…·ï¼ˆå¦‚ç½‘ç»œæœç´¢ï¼‰ä¼šå¯¼è‡´è¿‡åº¦è‡ªä¿¡ï¼Œè€ŒéªŒè¯å·¥å…·ï¼ˆå¦‚ä»£ç è§£é‡Šå™¨ï¼‰åˆ™èƒ½é€šè¿‡ç¡®å®šæ€§åé¦ˆæ¥æ”¹å–„æ ¡å‡†ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨åŒæ—¶ä¼˜åŒ–ä»»åŠ¡å‡†ç¡®æ€§å’Œæ ¡å‡†æ•ˆæœã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼Œç»è¿‡è®­ç»ƒçš„ä»£ç†åœ¨å¤šç§é¢†åŸŸä¸­è¡¨ç°å‡ºæ›´å¥½çš„æ ¡å‡†èƒ½åŠ›å’Œå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.08831', 'title': '3AM: Segment Anything with Geometric Consistency in Videos', 'url': 'https://huggingface.co/papers/2601.08831', 'abstract': "3AM enhances video object segmentation by integrating 3D-aware features from MUSt3R into SAM2, achieving improved viewpoint consistency with only RGB input at inference.  \t\t\t\t\tAI-generated summary \t\t\t\t Video object segmentation methods like SAM2 achieve strong performance through memory-based architectures but struggle under large viewpoint changes due to reliance on appearance features. Traditional 3D instance segmentation methods address viewpoint consistency but require camera poses, depth maps, and expensive preprocessing. We introduce 3AM, a training-time enhancement that integrates 3D-aware features from MUSt3R into SAM2. Our lightweight Feature Merger fuses multi-level MUSt3R features that encode implicit geometric correspondence. Combined with SAM2's appearance features, the model achieves geometry-consistent recognition grounded in both spatial position and visual similarity. We propose a field-of-view aware sampling strategy ensuring frames observe spatially consistent object regions for reliable 3D correspondence learning. Critically, our method requires only RGB input at inference, with no camera poses or preprocessing. On challenging datasets with wide-baseline motion (ScanNet++, Replica), 3AM substantially outperforms SAM2 and extensions, achieving 90.6% IoU and 71.7% Positive IoU on ScanNet++'s Selected Subset, improving over state-of-the-art VOS methods by +15.9 and +30.4 points. Project page: https://jayisaking.github.io/3AM-Page/", 'score': 18, 'issue_id': 578, 'pub_date': '2026-01-13', 'pub_date_card': {'ru': '13 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 13', 'zh': '1æœˆ13æ—¥'}, 'hash': '5da7a87f80f776f3', 'authors': ['Yang-Che Sun', 'Cheng Sun', 'Chin-Yang Lin', 'Fu-En Yang', 'Min-Hung Chen', 'Yen-Yu Lin', 'Yu-Lun Liu'], 'affiliations': ['NVIDIA Research', 'National Yang Ming Chiao Tung University'], 'pdf_title_img': 'assets/pdf/title_img/2601.08831.jpg', 'data': {'categories': ['#architecture', '#training', '#3d', '#optimization', '#video', '#cv'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ“ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ°Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¸Ğ½Ñ‚ĞµĞ· 3D-Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ²', 'desc': '3AM ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ¸Ğ· Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ MUSt3R Ğ² Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ SAM2. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸ÑÑ… Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ RGB-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² ĞºĞ°Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ñ… Ğ¸Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ»Ñ‘Ğ³ĞºĞ¸Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Feature Merger, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸, ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğµ Ğ½ĞµÑĞ²Ğ½Ğ¾Ğµ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ğ´Ñ€Ğ°Ğ¼Ğ¸. ĞĞ¾Ğ²Ğ°Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ, Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ… Ñ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ĞµĞ¼ ĞºĞ°Ğ¼ĞµÑ€Ñ‹.'}, 'en': {'title': '3AM: Enhancing Video Segmentation with 3D Awareness', 'desc': 'The paper presents 3AM, a novel approach to video object segmentation that enhances the existing SAM2 model by incorporating 3D-aware features from the MUSt3R dataset. This integration allows the model to maintain viewpoint consistency without the need for camera poses or depth maps, which are typically required in traditional methods. By using a lightweight Feature Merger, 3AM combines geometric and appearance features, enabling more accurate recognition of objects based on their spatial and visual characteristics. The proposed field-of-view aware sampling strategy further ensures that the model learns reliable 3D correspondences, leading to significant performance improvements on challenging datasets.'}, 'zh': {'title': '3AMï¼šæå‡è§†é¢‘ç‰©ä½“åˆ†å‰²çš„ä¸€è‡´æ€§', 'desc': '3AMæ˜¯ä¸€ç§è§†é¢‘ç‰©ä½“åˆ†å‰²æ–¹æ³•ï¼Œé€šè¿‡å°†MUSt3Rçš„3Dç‰¹å¾é›†æˆåˆ°SAM2ä¸­ï¼Œå¢å¼ºäº†æ¨¡å‹åœ¨ä¸åŒè§†è§’ä¸‹çš„ä¸€è‡´æ€§ã€‚è¯¥æ–¹æ³•åœ¨æ¨ç†æ—¶ä»…éœ€RGBè¾“å…¥ï¼Œæ— éœ€ç›¸æœºä½å§¿æˆ–æ·±åº¦å›¾ï¼Œé™ä½äº†é¢„å¤„ç†çš„å¤æ‚æ€§ã€‚3AMä½¿ç”¨è½»é‡çº§çš„ç‰¹å¾åˆå¹¶å™¨ï¼Œèåˆå¤šå±‚MUSt3Rç‰¹å¾ï¼Œç¡®ä¿å‡ ä½•ä¸€è‡´æ€§å’Œè§†è§‰ç›¸ä¼¼æ€§ã€‚åœ¨å…·æœ‰å¤§åŸºçº¿è¿åŠ¨çš„æŒ‘æˆ˜æ€§æ•°æ®é›†ä¸Šï¼Œ3AMæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„VOSæ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶ä¼˜è¶Šçš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.08670', 'title': 'Parallel Context-of-Experts Decoding for Retrieval Augmented Generation', 'url': 'https://huggingface.co/papers/2601.08670', 'abstract': 'Parallel Context-of-Experts Decoding enables multi-document reasoning in retrieval-augmented generation by treating retrieved documents as isolated experts and synchronizing predictions through a retrieval-aware contrastive decoding rule.  \t\t\t\t\tAI-generated summary \t\t\t\t Retrieval Augmented Generation faces a trade-off: concatenating documents in a long prompt enables multi-document reasoning but creates prefill bottlenecks, while encoding document KV caches separately offers speed but breaks cross-document interaction. We propose Parallel Context-of-Experts Decoding (Pced), a training-free framework that shifts evidence aggregation from the attention mechanism to the decoding. Pced treats retrieved documents as isolated "experts", synchronizing their predictions via a novel retrieval-aware contrastive decoding rule that weighs expert logits against the model prior. This approach recovers cross-document reasoning capabilities without constructing a shared attention across documents.', 'score': 17, 'issue_id': 577, 'pub_date': '2026-01-13', 'pub_date_card': {'ru': '13 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 13', 'zh': '1æœˆ13æ—¥'}, 'hash': '1620b58b79613f44', 'authors': ['Giulio Corallo', 'Paolo Papotti'], 'affiliations': ['EURECOM, France', 'SAP Labs, France'], 'pdf_title_img': 'assets/pdf/title_img/2601.08670.jpg', 'data': {'categories': [], 'emoji': 'ğŸ¯', 'ru': {'title': 'Ğ”ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞºÑ€Ñ‘ÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ Parallel Context-of-Experts Decoding Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² retrieval-augmented generation. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚ ĞºĞ°Ğº Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ° Ğ¸ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ñ… Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ ÑƒĞ·ĞºĞ¸Ñ… Ğ¼ĞµÑÑ‚ Ğ¿Ñ€Ğ¸ ĞºĞ¾Ğ½ĞºĞ°Ñ‚ĞµĞ½Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ², ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾ÑĞ²ÑĞ·ÑÑ… Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸. ĞĞ³Ñ€ĞµĞ³Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ° Ğ½Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Empowering Multi-Document Reasoning with Expert Synchronization', 'desc': "The paper introduces Parallel Context-of-Experts Decoding (Pced), a new method for improving multi-document reasoning in retrieval-augmented generation tasks. Pced treats each retrieved document as an independent expert and synchronizes their outputs using a contrastive decoding rule that considers the model's prior knowledge. This method allows for effective evidence aggregation during the decoding phase, avoiding the limitations of traditional attention mechanisms that can hinder performance. By implementing Pced, the authors demonstrate that it is possible to achieve cross-document reasoning without the need for a shared attention structure, thus enhancing the efficiency of the generation process."}, 'zh': {'title': 'å¹¶è¡Œä¸“å®¶è§£ç ï¼šæå‡å¤šæ–‡æ¡£æ¨ç†èƒ½åŠ›', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºå¹¶è¡Œä¸“å®¶ä¸Šä¸‹æ–‡è§£ç ï¼ˆPcedï¼‰çš„æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³æ£€ç´¢å¢å¼ºç”Ÿæˆä¸­çš„å¤šæ–‡æ¡£æ¨ç†é—®é¢˜ã€‚Pcedå°†æ£€ç´¢åˆ°çš„æ–‡æ¡£è§†ä¸ºç‹¬ç«‹çš„â€œä¸“å®¶â€ï¼Œé€šè¿‡ä¸€ç§æ–°çš„æ£€ç´¢æ„ŸçŸ¥å¯¹æ¯”è§£ç è§„åˆ™æ¥åŒæ­¥å®ƒä»¬çš„é¢„æµ‹ã€‚è¯¥æ–¹æ³•é¿å…äº†åœ¨æ–‡æ¡£ä¹‹é—´æ„å»ºå…±äº«æ³¨æ„åŠ›ï¼Œä»è€Œæ¢å¤äº†è·¨æ–‡æ¡£æ¨ç†èƒ½åŠ›ã€‚Pcedä¸éœ€è¦è®­ç»ƒï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°èšåˆè¯æ®ï¼Œæå‡ç”Ÿæˆæ¨¡å‹çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.08620', 'title': 'ViDoRe V3: A Comprehensive Evaluation of Retrieval Augmented Generation in Complex Real-World Scenarios', 'url': 'https://huggingface.co/papers/2601.08620', 'abstract': 'ViDoRe v3 is a multimodal retrieval-augmented generation benchmark with diverse document types and languages, revealing that visual retrieval and late interaction models improve performance while current systems still face challenges with non-textual elements and complex queries.  \t\t\t\t\tAI-generated summary \t\t\t\t Retrieval-Augmented Generation (RAG) pipelines must address challenges beyond simple single-document retrieval, such as interpreting visual elements (tables, charts, images), synthesizing information across documents, and providing accurate source grounding. Existing benchmarks fail to capture this complexity, often focusing on textual data, single-document comprehension, or evaluating retrieval and generation in isolation. We introduce ViDoRe v3, a comprehensive multimodal RAG benchmark featuring multi-type queries over visually rich document corpora. It covers 10 datasets across diverse professional domains, comprising ~26,000 document pages paired with 3,099 human-verified queries, each available in 6 languages. Through 12,000 hours of human annotation effort, we provide high-quality annotations for retrieval relevance, bounding box localization, and verified reference answers. Our evaluation of state-of-the-art RAG pipelines reveals that visual retrievers outperform textual ones, late-interaction models and textual reranking substantially improve performance, and hybrid or purely visual contexts enhance answer generation quality. However, current models still struggle with non-textual elements, open-ended queries, and fine-grained visual grounding. To encourage progress in addressing these challenges, the benchmark is released under a commercially permissive license at https://hf.co/vidore.', 'score': 7, 'issue_id': 578, 'pub_date': '2026-01-13', 'pub_date_card': {'ru': '13 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 13', 'zh': '1æœˆ13æ—¥'}, 'hash': '89b240795c30bec1', 'authors': ['AntÃ³nio Loison', 'Quentin MacÃ©', 'Antoine Edy', 'Victor Xing', 'Tom Balough', 'Gabriel Moreira', 'Bo Liu', 'Manuel Faysse', 'CÃ©line Hudelot', 'Gautier Viaud'], 'affiliations': ['CentraleSupÃ©lec, Paris-Saclay', 'Illuin Technology', 'NVIDIA'], 'pdf_title_img': 'assets/pdf/title_img/2601.08620.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#multilingual', '#survey', '#open_source', '#rag', '#multimodal', '#low_resource'], 'emoji': 'ğŸ“Š', 'ru': {'title': 'Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¹ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'ViDoRe v3 â€” ÑÑ‚Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾ Ğ±Ğ¾Ğ³Ğ°Ñ‚Ñ‹Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚Ğ¸Ğ¿Ğ°Ğ¼Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹, Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ·Ğ´Ğ½Ğ¸Ğ¼ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ĞµĞ¼ Ğ¸ Ğ¿ĞµÑ€ĞµÑ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ 10 Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ğ½Ğ° 3000 Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞµĞ½Ğ½Ñ‹Ñ… Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¼ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ½Ğ° 6 ÑĞ·Ñ‹ĞºĞ°Ñ…. ĞĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑ, ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¿Ğ¾-Ğ¿Ñ€ĞµĞ¶Ğ½ĞµĞ¼Ñƒ Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ½ĞµÑ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ….'}, 'en': {'title': 'Enhancing Multimodal Retrieval with ViDoRe v3', 'desc': 'ViDoRe v3 is a new benchmark designed for multimodal retrieval-augmented generation (RAG) that includes various document types and languages. It highlights the importance of visual retrieval and late interaction models, which enhance performance in processing complex queries. The benchmark addresses the limitations of existing systems that often overlook non-textual elements and multi-document synthesis. With extensive human annotations and a diverse dataset, ViDoRe v3 aims to improve the capabilities of RAG systems in handling visually rich information.'}, 'zh': {'title': 'å¤šæ¨¡æ€æ£€ç´¢çš„æœªæ¥ï¼šViDoRe v3', 'desc': 'ViDoRe v3 æ˜¯ä¸€ä¸ªå¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç”ŸæˆåŸºå‡†ï¼Œæ¶µç›–å¤šç§æ–‡æ¡£ç±»å‹å’Œè¯­è¨€ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè§†è§‰æ£€ç´¢å’ŒåæœŸäº¤äº’æ¨¡å‹èƒ½å¤Ÿæé«˜æ€§èƒ½ï¼Œä½†å½“å‰ç³»ç»Ÿåœ¨å¤„ç†éæ–‡æœ¬å…ƒç´ å’Œå¤æ‚æŸ¥è¯¢æ—¶ä»é¢ä¸´æŒ‘æˆ˜ã€‚è¯¥åŸºå‡†åŒ…å«æ¥è‡ªä¸åŒä¸“ä¸šé¢†åŸŸçš„ 10 ä¸ªæ•°æ®é›†ï¼Œçº¦ 26,000 é¡µæ–‡æ¡£å’Œ 3,099 ä¸ªç»è¿‡äººå·¥éªŒè¯çš„æŸ¥è¯¢ï¼Œæ”¯æŒ 6 ç§è¯­è¨€ã€‚é€šè¿‡ 12,000 å°æ—¶çš„äººåŠ›æ ‡æ³¨ï¼Œæˆ‘ä»¬æä¾›äº†é«˜è´¨é‡çš„æ£€ç´¢ç›¸å…³æ€§ã€è¾¹ç•Œæ¡†å®šä½å’ŒéªŒè¯å‚è€ƒç­”æ¡ˆçš„æ³¨é‡Šã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.08303', 'title': 'SnapGen++: Unleashing Diffusion Transformers for Efficient High-Fidelity Image Generation on Edge Devices', 'url': 'https://huggingface.co/papers/2601.08303', 'abstract': 'An efficient diffusion transformer framework for mobile and edge devices that maintains high-generation quality while reducing computational costs through compact architecture, elastic training, and knowledge-guided distillation.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in diffusion transformers (DiTs) have set new standards in image generation, yet remain impractical for on-device deployment due to their high computational and memory costs. In this work, we present an efficient DiT framework tailored for mobile and edge devices that achieves transformer-level generation quality under strict resource constraints. Our design combines three key components. First, we propose a compact DiT architecture with an adaptive global-local sparse attention mechanism that balances global context modeling and local detail preservation. Second, we propose an elastic training framework that jointly optimizes sub-DiTs of varying capacities within a unified supernetwork, allowing a single model to dynamically adjust for efficient inference across different hardware. Finally, we develop Knowledge-Guided Distribution Matching Distillation, a step-distillation pipeline that integrates the DMD objective with knowledge transfer from few-step teacher models, producing high-fidelity and low-latency generation (e.g., 4-step) suitable for real-time on-device use. Together, these contributions enable scalable, efficient, and high-quality diffusion models for deployment on diverse hardware.', 'score': 7, 'issue_id': 566, 'pub_date': '2026-01-13', 'pub_date_card': {'ru': '13 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 13', 'zh': '1æœˆ13æ—¥'}, 'hash': '2bf6eff80c892659', 'authors': ['Dongting Hu', 'Aarush Gupta', 'Magzhan Gabidolla', 'Arpit Sahni', 'Huseyin Coskun', 'Yanyu Li', 'Yerlan Idelbayev', 'Ahsan Mahmood', 'Aleksei Lebedev', 'Dishani Lahiri', 'Anujraaj Goyal', 'Ju Hu', 'Mingming Gong', 'Sergey Tulyakov', 'Anil Kag'], 'affiliations': ['MBZUAI', 'Snap Inc.', 'The University of Melbourne'], 'pdf_title_img': 'assets/pdf/title_img/2601.08303.jpg', 'data': {'categories': ['#architecture', '#small_models', '#training', '#inference'], 'emoji': 'ğŸ“±', 'ru': {'title': 'ĞšĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ñ‹ Ğ´Ğ»Ñ ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ² Ğ½Ğ° Ğ»Ğ°Ğ´Ğ¾Ğ½Ğ¸', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ÑĞ»Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ğ°Ñ ÑÑ…ĞµĞ¼Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ°Ñ ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¹ ÑÑƒĞ¿ĞµÑ€ÑĞµÑ‚Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ´Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ´ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ°Ğ¿Ğ¿Ğ°Ñ€Ğ°Ñ‚Ğ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Knowledge-Guided Distribution Matching Distillation Ğ´Ğ»Ñ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¾Ñ‚ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞµ. ĞœĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ€Ğ°Ğ·Ğ²Ñ‘Ñ€Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ… Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµÑÑƒÑ€ÑĞ°Ğ¼Ğ¸.'}, 'en': {'title': 'Efficient Diffusion Transformers for Mobile and Edge Devices', 'desc': "This paper introduces an efficient diffusion transformer (DiT) framework designed specifically for mobile and edge devices, addressing the high computational demands of traditional models. It features a compact architecture that utilizes a global-local sparse attention mechanism to effectively balance the need for global context and local detail in image generation. The framework also includes an elastic training approach that allows the model to adapt its capacity based on the hardware it runs on, optimizing performance without sacrificing quality. Additionally, the authors implement a Knowledge-Guided Distribution Matching Distillation method to enhance the model's efficiency and fidelity, enabling real-time image generation on resource-constrained devices."}, 'zh': {'title': 'é«˜æ•ˆæ‰©æ•£å˜æ¢å™¨ï¼šç§»åŠ¨è®¾å¤‡ä¸Šçš„é«˜è´¨é‡ç”Ÿæˆ', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„æ‰©æ•£å˜æ¢å™¨æ¡†æ¶ï¼Œä¸“ä¸ºç§»åŠ¨å’Œè¾¹ç¼˜è®¾å¤‡è®¾è®¡ï¼Œæ—¨åœ¨åœ¨ä¸¥æ ¼çš„èµ„æºé™åˆ¶ä¸‹ä¿æŒé«˜è´¨é‡çš„å›¾åƒç”Ÿæˆã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªç´§å‡‘çš„æ‰©æ•£å˜æ¢å™¨æ¶æ„ï¼Œç»“åˆäº†è‡ªé€‚åº”çš„å…¨å±€-å±€éƒ¨ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ï¼Œä»¥å¹³è¡¡å…¨å±€ä¸Šä¸‹æ–‡å»ºæ¨¡å’Œå±€éƒ¨ç»†èŠ‚ä¿ç•™ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¼¹æ€§è®­ç»ƒæ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨ç»Ÿä¸€çš„è¶…ç½‘ç»œä¸­ä¼˜åŒ–ä¸åŒå®¹é‡çš„å­æ‰©æ•£å˜æ¢å™¨ï¼Œä»è€Œä½¿å•ä¸ªæ¨¡å‹èƒ½å¤Ÿæ ¹æ®ä¸åŒç¡¬ä»¶åŠ¨æ€è°ƒæ•´ä»¥å®ç°é«˜æ•ˆæ¨ç†ã€‚æœ€åï¼Œæˆ‘ä»¬å¼€å‘äº†çŸ¥è¯†å¼•å¯¼çš„åˆ†å¸ƒåŒ¹é…è’¸é¦æ–¹æ³•ï¼Œé€šè¿‡ä¸å°‘æ­¥æ•™å¸ˆæ¨¡å‹çš„çŸ¥è¯†è½¬ç§»ç›¸ç»“åˆï¼Œç”Ÿæˆé«˜ä¿çœŸã€ä½å»¶è¿Ÿçš„å›¾åƒï¼Œé€‚åˆå®æ—¶è®¾å¤‡ä½¿ç”¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.08828', 'title': 'Motion Attribution for Video Generation', 'url': 'https://huggingface.co/papers/2601.08828', 'abstract': 'Motive is a gradient-based data attribution framework that identifies influential video clips for motion improvement in text-to-video models through motion-weighted loss masking.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite the rapid progress of video generation models, the role of data in influencing motion is poorly understood. We present Motive (MOTIon attribution for Video gEneration), a motion-centric, gradient-based data attribution framework that scales to modern, large, high-quality video datasets and models. We use this to study which fine-tuning clips improve or degrade temporal dynamics. Motive isolates temporal dynamics from static appearance via motion-weighted loss masks, yielding efficient and scalable motion-specific influence computation. On text-to-video models, Motive identifies clips that strongly affect motion and guides data curation that improves temporal consistency and physical plausibility. With Motive-selected high-influence data, our method improves both motion smoothness and dynamic degree on VBench, achieving a 74.1% human preference win rate compared with the pretrained base model. To our knowledge, this is the first framework to attribute motion rather than visual appearance in video generative models and to use it to curate fine-tuning data.', 'score': 6, 'issue_id': 566, 'pub_date': '2026-01-13', 'pub_date_card': {'ru': '13 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 13', 'zh': '1æœˆ13æ—¥'}, 'hash': 'b846910920901a8a', 'authors': ['Xindi Wu', 'Despoina Paschalidou', 'Jun Gao', 'Antonio Torralba', 'Laura Leal-TaixÃ©', 'Olga Russakovsky', 'Sanja Fidler', 'Jonathan Lorraine'], 'affiliations': ['MIT CSAIL', 'NVIDIA', 'Princeton University'], 'pdf_title_img': 'assets/pdf/title_img/2601.08828.jpg', 'data': {'categories': ['#video', '#data', '#training'], 'emoji': 'ğŸ¬', 'ru': {'title': 'ĞÑ‚Ñ€Ğ¸Ğ±ÑƒÑ†Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Motive â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ°Ñ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Ğ²Ğ»Ğ¸ÑÑ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ»Ğ¸Ğ¿Ñ‹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… text-to-video. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ°ÑĞºĞ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ, Ğ²Ğ·Ğ²ĞµÑˆĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¾Ñ‚Ğ´ĞµĞ»Ğ¸Ñ‚ÑŒ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºÑƒ Ğ¾Ñ‚ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ Ğ²Ğ¸Ğ´Ğ° Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ÑŒ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ½Ğ° Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ ÑĞ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‰ÑƒÑ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Motive Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ»Ğ¸Ğ¿Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ Ğ¸Ğ»Ğ¸ ÑƒÑ…ÑƒĞ´ÑˆĞ°ÑÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ fine-tuning Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ğµ Motive, Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ÑÑ‚ Ğ³Ğ»Ğ°Ğ´ĞºĞ¾ÑÑ‚ÑŒ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° 74.1% ÑĞ¾Ğ³Ğ»Ğ°ÑĞ½Ğ¾ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼ Ğ»ÑĞ´ĞµĞ¹.'}, 'en': {'title': 'Motive: Enhancing Motion in Video Generation through Data Attribution', 'desc': 'Motive is a novel framework designed to analyze and improve motion in text-to-video generation models. It uses a gradient-based approach to identify which video clips significantly influence motion dynamics, separating motion effects from static visuals. By applying motion-weighted loss masking, Motive efficiently computes the impact of different clips on temporal consistency. This method not only enhances the quality of generated videos but also guides the selection of training data to optimize motion characteristics, achieving notable improvements in user preference tests.'}, 'zh': {'title': 'Motiveï¼šæå‡è§†é¢‘ç”Ÿæˆæ¨¡å‹è¿åŠ¨è¡¨ç°çš„å…³é”®', 'desc': 'Motiveæ˜¯ä¸€ä¸ªåŸºäºæ¢¯åº¦çš„æ•°æ®å½’å› æ¡†æ¶ï¼Œæ—¨åœ¨è¯†åˆ«å¯¹æ–‡æœ¬åˆ°è§†é¢‘æ¨¡å‹ä¸­è¿åŠ¨æ”¹è¿›æœ‰å½±å“çš„è§†é¢‘ç‰‡æ®µã€‚è¯¥æ¡†æ¶é€šè¿‡è¿åŠ¨åŠ æƒæŸå¤±æ©ç ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°ä»é™æ€å¤–è§‚ä¸­åˆ†ç¦»å‡ºæ—¶é—´åŠ¨æ€ã€‚Motiveå¯ä»¥å¤„ç†ç°ä»£å¤§å‹é«˜è´¨é‡è§†é¢‘æ•°æ®é›†ï¼Œå¸®åŠ©ç ”ç©¶å“ªäº›å¾®è°ƒç‰‡æ®µèƒ½å¤Ÿæ”¹å–„æˆ–æ¶åŒ–æ—¶é—´åŠ¨æ€ã€‚é€šè¿‡Motiveé€‰æ‹©çš„é«˜å½±å“æ•°æ®ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è¿åŠ¨å¹³æ»‘æ€§å’ŒåŠ¨æ€ç¨‹åº¦ä¸Šå–å¾—äº†æ˜¾è‘—æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.08665', 'title': 'VLingNav: Embodied Navigation with Adaptive Reasoning and Visual-Assisted Linguistic Memory', 'url': 'https://huggingface.co/papers/2601.08665', 'abstract': 'VLingNav enhances embodied navigation through linguistic-driven cognition with adaptive reasoning and visual-assisted memory, achieving state-of-the-art performance and zero-shot transfer to real robots.  \t\t\t\t\tAI-generated summary \t\t\t\t VLA models have shown promising potential in embodied navigation by unifying perception and planning while inheriting the strong generalization abilities of large VLMs. However, most existing VLA models rely on reactive mappings directly from observations to actions, lacking the explicit reasoning capabilities and persistent memory required for complex, long-horizon navigation tasks. To address these challenges, we propose VLingNav, a VLA model for embodied navigation grounded in linguistic-driven cognition. First, inspired by the dual-process theory of human cognition, we introduce an adaptive chain-of-thought mechanism, which dynamically triggers explicit reasoning only when necessary, enabling the agent to fluidly switch between fast, intuitive execution and slow, deliberate planning. Second, to handle long-horizon spatial dependencies, we develop a visual-assisted linguistic memory module that constructs a persistent, cross-modal semantic memory, enabling the agent to recall past observations to prevent repetitive exploration and infer movement trends for dynamic environments. For the training recipe, we construct Nav-AdaCoT-2.9M, the largest embodied navigation dataset with reasoning annotations to date, enriched with adaptive CoT annotations that induce a reasoning paradigm capable of adjusting both when to think and what to think about. Moreover, we incorporate an online expert-guided reinforcement learning stage, enabling the model to surpass pure imitation learning and to acquire more robust, self-explored navigation behaviors. Extensive experiments demonstrate that VLingNav achieves state-of-the-art performance across a wide range of embodied navigation benchmarks. Notably, VLingNav transfers to real-world robotic platforms in a zero-shot manner, executing various navigation tasks and demonstrating strong cross-domain and cross-task generalization.', 'score': 6, 'issue_id': 566, 'pub_date': '2026-01-13', 'pub_date_card': {'ru': '13 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 13', 'zh': '1æœˆ13æ—¥'}, 'hash': '80a64f457ad37ab4', 'authors': ['Shaoan Wang', 'Yuanfei Luo', 'Xingyu Chen', 'Aocheng Luo', 'Dongyue Li', 'Chang Liu', 'Sheng Chen', 'Yangang Zhang', 'Junzhi Yu'], 'affiliations': ['ByteDance Seed', 'Peking University', 'Zhongguancun Academy'], 'pdf_title_img': 'assets/pdf/title_img/2601.08665.jpg', 'data': {'categories': ['#rl', '#transfer_learning', '#multimodal', '#reasoning', '#dataset', '#robotics', '#agents', '#training'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ Ğ°ÑÑÑƒĞ¶Ğ´Ğ°ÑÑ‰Ğ°Ñ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ñ: ĞºĞ¾Ğ³Ğ´Ğ° Ñ€Ğ¾Ğ±Ğ¾Ñ‚ Ğ´ÑƒĞ¼Ğ°ĞµÑ‚, Ğ¿Ñ€ĞµĞ¶Ğ´Ğµ Ñ‡ĞµĞ¼ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ñ‚ÑŒ', 'desc': 'VLingNav â€” ÑÑ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ·Ñ€ĞµĞ½Ğ¸Ğµ Ğ¸ ÑĞ·Ñ‹Ğº Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ğ¼Ñ‹ÑĞ»Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑĞ²Ğ½Ğ¾Ğµ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ĞºĞ¾Ğ³Ğ´Ğ° ÑÑ‚Ğ¾ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ°Ğ³ĞµĞ½Ñ‚Ñƒ Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡Ğ°Ñ‚ÑŒÑÑ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ±Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¼ Ğ¸Ğ½Ñ‚ÑƒĞ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¾Ğ±Ğ´ÑƒĞ¼Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼. Ğ”Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ñƒ Ğ¸Ğ·Ğ±ĞµĞ³Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½ÑƒĞ»ĞµĞ²ÑƒÑ Ñ‚Ñ€Ğ°Ğ½ÑÑ„ĞµÑ€Ğ¾Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ñ‹.'}, 'en': {'title': 'Empowering Navigation with Linguistic Reasoning and Memory', 'desc': 'VLingNav is a novel model designed to improve embodied navigation by integrating linguistic-driven cognition with advanced reasoning and memory capabilities. It introduces an adaptive chain-of-thought mechanism that allows the agent to switch between quick, intuitive actions and more thoughtful planning as needed. Additionally, it features a visual-assisted memory module that helps the agent remember past observations, reducing repetitive actions and enhancing navigation in dynamic environments. The model has been trained on a large dataset with reasoning annotations and has shown exceptional performance in both simulated and real-world robotic navigation tasks, achieving zero-shot transfer capabilities.'}, 'zh': {'title': 'è¯­è¨€é©±åŠ¨çš„æ™ºèƒ½å¯¼èˆªæ–°çªç ´', 'desc': 'VLingNavæ˜¯ä¸€ç§åŸºäºè¯­è¨€é©±åŠ¨è®¤çŸ¥çš„ä½“æ„Ÿå¯¼èˆªæ¨¡å‹ï¼Œæ—¨åœ¨æå‡å¯¼èˆªèƒ½åŠ›ã€‚å®ƒå¼•å…¥äº†è‡ªé€‚åº”æ€ç»´é“¾æœºåˆ¶ï¼Œä½¿å¾—æ™ºèƒ½ä½“èƒ½å¤Ÿåœ¨å¿«é€Ÿç›´è§‰æ‰§è¡Œå’Œæ…¢é€Ÿæ·±æ€ç†Ÿè™‘ä¹‹é—´çµæ´»åˆ‡æ¢ã€‚ä¸ºäº†å¤„ç†é•¿æ—¶é—´çš„ç©ºé—´ä¾èµ–ï¼ŒVLingNavå¼€å‘äº†è§†è§‰è¾…åŠ©è¯­è¨€è®°å¿†æ¨¡å—ï¼Œå¸®åŠ©æ™ºèƒ½ä½“å›å¿†è¿‡å»çš„è§‚å¯Ÿï¼Œé¿å…é‡å¤æ¢ç´¢ã€‚é€šè¿‡æ„å»ºNav-AdaCoT-2.9Mæ•°æ®é›†å’Œåœ¨çº¿ä¸“å®¶æŒ‡å¯¼çš„å¼ºåŒ–å­¦ä¹ é˜¶æ®µï¼ŒVLingNavåœ¨å¤šç§ä½“æ„Ÿå¯¼èˆªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œå¹¶èƒ½åœ¨é›¶æ ·æœ¬æƒ…å†µä¸‹è¿ç§»åˆ°çœŸå®æœºå™¨äººä¸Šã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.08587', 'title': 'End-to-End Video Character Replacement without Structural Guidance', 'url': 'https://huggingface.co/papers/2601.08587', 'abstract': 'MoCha enables controllable video character replacement using a single frame mask through condition-aware RoPE and a comprehensive data construction pipeline with specialized datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t Controllable video character replacement with a user-provided identity remains a challenging problem due to the lack of paired video data. Prior works have predominantly relied on a reconstruction-based paradigm that requires per-frame segmentation masks and explicit structural guidance (e.g., skeleton, depth). This reliance, however, severely limits their generalizability in complex scenarios involving occlusions, character-object interactions, unusual poses, or challenging illumination, often leading to visual artifacts and temporal inconsistencies. In this paper, we propose MoCha, a pioneering framework that bypasses these limitations by requiring only a single arbitrary frame mask. To effectively adapt the multi-modal input condition and enhance facial identity, we introduce a condition-aware RoPE and employ an RL-based post-training stage. Furthermore, to overcome the scarcity of qualified paired-training data, we propose a comprehensive data construction pipeline. Specifically, we design three specialized datasets: a high-fidelity rendered dataset built with Unreal Engine 5 (UE5), an expression-driven dataset synthesized by current portrait animation techniques, and an augmented dataset derived from existing video-mask pairs. Extensive experiments demonstrate that our method substantially outperforms existing state-of-the-art approaches. We will release the code to facilitate further research. Please refer to our project page for more details: orange-3dv-team.github.io/MoCha', 'score': 5, 'issue_id': 566, 'pub_date': '2026-01-13', 'pub_date_card': {'ru': '13 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 13', 'zh': '1æœˆ13æ—¥'}, 'hash': '07a6935bace7df25', 'authors': ['Zhengbo Xu', 'Jie Ma', 'Ziheng Wang', 'Zhan Peng', 'Jun Liang', 'Jing Li'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2601.08587.jpg', 'data': {'categories': ['#rl', '#multimodal', '#data', '#dataset', '#video', '#architecture'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ—Ğ°Ğ¼ĞµĞ½Ğ° Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¼Ğ°ÑĞºĞµ Ğ±ĞµĞ· Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹', 'desc': 'MoCha â€” ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ·Ğ°Ğ¼ĞµĞ½Ñ‹ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¾Ğ´Ğ½Ñƒ Ğ¼Ğ°ÑĞºÑƒ Ğ¸Ğ· Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ°Ğ´Ñ€Ğ° Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ…ÑÑ Ñ€Ğ°Ğ½ĞµĞµ Ğ¼Ğ°ÑĞ¾Ğº Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ĞºĞ°Ğ´Ñ€Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ condition-aware RoPE Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ»Ğ¸Ñ†Ğ°, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ½ĞµÑ…Ğ²Ğ°Ñ‚ĞºĞ¸ Ğ¿Ğ°Ñ€Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ ĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Ñ‚Ñ€ĞµÑ… ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ²: ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ² Unreal Engine 5, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ°Ğ¼Ğ¸ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ€Ñ‚Ñ€ĞµÑ‚Ğ¾Ğ² Ğ¸ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ· ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ°Ñ€ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ¼Ğ°ÑĞ¾Ğº. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Revolutionizing Video Character Replacement with MoCha', 'desc': 'MoCha is a novel framework for replacing characters in videos using just a single frame mask, addressing the challenges of previous methods that needed detailed per-frame segmentation. It introduces a condition-aware RoPE to adapt to various input conditions and enhance facial identity, while also incorporating a reinforcement learning-based post-training stage for better results. The framework is supported by a comprehensive data construction pipeline that includes three specialized datasets to mitigate the lack of paired video data. Experimental results show that MoCha significantly outperforms existing methods, making it a promising solution for controllable video character replacement.'}, 'zh': {'title': 'MoChaï¼šå•å¸§æ©ç å®ç°å¯æ§è§†é¢‘è§’è‰²æ›¿æ¢', 'desc': 'MoChaæ˜¯ä¸€ä¸ªåˆ›æ–°çš„æ¡†æ¶ï¼Œèƒ½å¤Ÿé€šè¿‡å•å¸§æ©ç å®ç°å¯æ§çš„è§†é¢‘è§’è‰²æ›¿æ¢ã€‚è¯¥æ–¹æ³•å…‹æœäº†ä»¥å¾€éœ€è¦é€å¸§åˆ†å‰²æ©ç å’Œç»“æ„æŒ‡å¯¼çš„é™åˆ¶ï¼Œé€‚åº”å¤æ‚åœºæ™¯ä¸­çš„é®æŒ¡å’Œè§’è‰²äº¤äº’ã€‚æˆ‘ä»¬å¼•å…¥äº†æ¡ä»¶æ„ŸçŸ¥çš„RoPEå’ŒåŸºäºå¼ºåŒ–å­¦ä¹ çš„åè®­ç»ƒé˜¶æ®µï¼Œä»¥å¢å¼ºé¢éƒ¨èº«ä»½çš„é€‚åº”æ€§ã€‚é€šè¿‡æ„å»ºé«˜ä¿çœŸåº¦çš„æ•°æ®é›†å’Œå…¶ä»–ä¸“é—¨æ•°æ®é›†ï¼ŒMoChaåœ¨å®éªŒä¸­æ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.08468', 'title': 'JudgeRLVR: Judge First, Generate Second for Efficient Reasoning', 'url': 'https://huggingface.co/papers/2601.08468', 'abstract': 'Reinforcement learning with verifiable rewards is enhanced through a judge-then-generate paradigm that improves both efficiency and accuracy in mathematical problem-solving.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning with Verifiable Rewards (RLVR) has become a standard paradigm for reasoning in Large Language Models. However, optimizing solely for final-answer correctness often drives models into aimless, verbose exploration, where they rely on exhaustive trial-and-error tactics rather than structured planning to reach solutions. While heuristic constraints like length penalties can reduce verbosity, they often truncate essential reasoning steps, creating a difficult trade-off between efficiency and verification. In this paper, we argue that discriminative capability is a prerequisite for efficient generation: by learning to distinguish valid solutions, a model can internalize a guidance signal that prunes the search space. We propose JudgeRLVR, a two-stage judge-then-generate paradigm. In the first stage, we train the model to judge solution responses with verifiable answers. In the second stage, we fine-tune the same model with vanilla generating RLVR initialized from the judge. Compared to Vanilla RLVR using the same math-domain training data, JudgeRLVR achieves a better quality--efficiency trade-off for Qwen3-30B-A3B: on in-domain math, it delivers about +3.7 points average accuracy gain with -42\\% average generation length; on out-of-domain benchmarks, it delivers about +4.5 points average accuracy improvement, demonstrating enhanced generalization.', 'score': 5, 'issue_id': 566, 'pub_date': '2026-01-13', 'pub_date_card': {'ru': '13 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 13', 'zh': '1æœˆ13æ—¥'}, 'hash': '3af6b50faf280f19', 'authors': ['Jiangshan Duo', 'Hanyu Li', 'Hailin Zhang', 'Yudong Wang', 'Sujian Li', 'Liang Zhao'], 'affiliations': ['CFCS, School of Computer Science, Peking University', 'LLM-Core Xiaomi', 'State Key Laboratory of Multimedia Information Processing, School of Computer Science, Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2601.08468.jpg', 'data': {'categories': ['#rlhf', '#rl', '#optimization', '#reasoning', '#math', '#training'], 'emoji': 'âš–ï¸', 'ru': {'title': 'Ğ¡ÑƒĞ´ÑŒÑ ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ°, Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€: Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ¸ÑĞºÑ€Ğ¸Ğ¼Ğ¸Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ JudgeRLVR â€” Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ° Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ°Ñ‚ÑŒ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ñ‹Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ (Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ ÑÑƒĞ´ÑŒĞ¸), Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ĞµĞ¹ ÑƒÑĞ²Ğ¾Ğ¸Ñ‚ÑŒ ÑĞ¸Ğ³Ğ½Ğ°Ğ» Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ° Ğ´Ğ»Ñ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ¿Ğ¾Ğ¸ÑĞºĞ°. ĞĞ° Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ñ‚Ğ° Ğ¶Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚ ÑÑƒĞ´ÑŒĞ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ JudgeRLVR Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ: ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° +3.7 Ğ±Ğ°Ğ»Ğ»Ğ° Ğ¿Ñ€Ğ¸ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° 42% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ¼.'}, 'en': {'title': 'JudgeRLVR: Efficient and Accurate Reinforcement Learning for Math Problem-Solving', 'desc': 'This paper introduces JudgeRLVR, a novel approach in reinforcement learning that enhances the efficiency and accuracy of mathematical problem-solving. It operates in two stages: first, the model learns to evaluate solution responses for correctness, and then it generates solutions based on this learned judgment. By distinguishing valid solutions, the model reduces unnecessary exploration and focuses on more promising paths, improving both the quality of answers and the speed of generation. The results show significant improvements in accuracy and reduced verbosity compared to traditional methods, demonstrating better generalization across different problem domains.'}, 'zh': {'title': 'åˆ¤åˆ«-ç”ŸæˆèŒƒå¼æå‡å¼ºåŒ–å­¦ä¹ æ•ˆç‡ä¸å‡†ç¡®æ€§', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºJudgeRLVRçš„ä¸¤é˜¶æ®µåˆ¤åˆ«-ç”ŸæˆèŒƒå¼ï¼Œä»¥æé«˜å¼ºåŒ–å­¦ä¹ ä¸­å¯éªŒè¯å¥–åŠ±çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚ç¬¬ä¸€é˜¶æ®µï¼Œæ¨¡å‹å­¦ä¹ åˆ¤æ–­å…·æœ‰å¯éªŒè¯ç­”æ¡ˆçš„è§£å†³æ–¹æ¡ˆå“åº”ï¼›ç¬¬äºŒé˜¶æ®µï¼Œæ¨¡å‹åœ¨åˆ¤åˆ«çš„åŸºç¡€ä¸Šè¿›è¡Œå¾®è°ƒç”Ÿæˆã€‚ä¸ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼ŒJudgeRLVRåœ¨æ•°å­¦é—®é¢˜è§£å†³ä¸­å®ç°äº†æ›´å¥½çš„è´¨é‡ä¸æ•ˆç‡å¹³è¡¡ï¼Œæ˜¾è‘—æé«˜äº†å‡†ç¡®ç‡å¹¶å‡å°‘äº†ç”Ÿæˆé•¿åº¦ã€‚è¯¥æ–¹æ³•åœ¨é¢†åŸŸå†…å’Œé¢†åŸŸå¤–çš„åŸºå‡†æµ‹è¯•ä¸­å‡è¡¨ç°å‡ºè‰²ï¼Œå±•ç¤ºäº†æ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.08321', 'title': 'UM-Text: A Unified Multimodal Model for Image Understanding', 'url': 'https://huggingface.co/papers/2601.08321', 'abstract': 'A unified multimodal model for visual text editing that understands natural language instructions and maintains stylistic consistency with reference images through visual language modeling and contextual embedding combination.  \t\t\t\t\tAI-generated summary \t\t\t\t With the rapid advancement of image generation, visual text editing using natural language instructions has received increasing attention. The main challenge of this task is to fully understand the instruction and reference image, and thus generate visual text that is style-consistent with the image. Previous methods often involve complex steps of specifying the text content and attributes, such as font size, color, and layout, without considering the stylistic consistency with the reference image. To address this, we propose UM-Text, a unified multimodal model for context understanding and visual text editing by natural language instructions. Specifically, we introduce a Visual Language Model (VLM) to process the instruction and reference image, so that the text content and layout can be elaborately designed according to the context information. To generate an accurate and harmonious visual text image, we further propose the UM-Encoder to combine the embeddings of various condition information, where the combination is automatically configured by VLM according to the input instruction. During training, we propose a regional consistency loss to offer more effective supervision for glyph generation on both latent and RGB space, and design a tailored three-stage training strategy to further enhance model performance. In addition, we contribute the UM-DATA-200K, a large-scale visual text image dataset on diverse scenes for model training. Extensive qualitative and quantitative results on multiple public benchmarks demonstrate that our method achieves state-of-the-art performance.', 'score': 3, 'issue_id': 581, 'pub_date': '2026-01-13', 'pub_date_card': {'ru': '13 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 13', 'zh': '1æœˆ13æ—¥'}, 'hash': '5647ea3386323521', 'authors': ['Lichen Ma', 'Xiaolong Fu', 'Gaojing Zhou', 'Zipeng Guo', 'Ting Zhu', 'Yichun Liu', 'Yu Shi', 'Jason Li', 'Junshi Huang'], 'affiliations': ['JD.COM', 'Sun Yat-sen University'], 'pdf_title_img': 'assets/pdf/title_img/2601.08321.jpg', 'data': {'categories': ['#dataset', '#cv', '#multimodal'], 'emoji': 'âœï¸', 'ru': {'title': 'ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ğ° Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ ÑÑ‚Ğ¸Ğ»Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ°', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ UM-Text â€” ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Visual Language Model Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ¸ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑ‚ÑŒ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ñ€Ğ°ÑĞ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ğ° Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ UM-Encoder Ğ´Ğ»Ñ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ² Ğ² Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ RGB. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ UM-DATA-200K Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ ÑÑ†ĞµĞ½Ğ°Ğ¼Ğ¸, Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ state-of-the-art Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ….'}, 'en': {'title': 'Unified Multimodal Model for Stylish Visual Text Editing', 'desc': 'This paper presents UM-Text, a unified multimodal model designed for visual text editing using natural language instructions. It leverages a Visual Language Model (VLM) to interpret both the instructions and reference images, ensuring that the generated text aligns stylistically with the visual context. The model employs an innovative UM-Encoder to integrate various embeddings, allowing for a more coherent design of text content and layout. Additionally, it introduces a regional consistency loss and a three-stage training strategy to improve glyph generation, supported by a large dataset for effective training.'}, 'zh': {'title': 'ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹ï¼Œå®ç°è§†è§‰æ–‡æœ¬ç¼–è¾‘çš„é£æ ¼ä¸€è‡´æ€§', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„å¤šæ¨¡æ€æ¨¡å‹UM-Textï¼Œç”¨äºè§†è§‰æ–‡æœ¬ç¼–è¾‘ï¼Œèƒ½å¤Ÿç†è§£è‡ªç„¶è¯­è¨€æŒ‡ä»¤å¹¶ä¿æŒä¸å‚è€ƒå›¾åƒçš„é£æ ¼ä¸€è‡´æ€§ã€‚è¯¥æ¨¡å‹ç»“åˆäº†è§†è§‰è¯­è¨€å»ºæ¨¡å’Œä¸Šä¸‹æ–‡åµŒå…¥ï¼Œèƒ½å¤Ÿæ ¹æ®è¾“å…¥çš„æŒ‡ä»¤å’Œå‚è€ƒå›¾åƒç”Ÿæˆå’Œè°çš„è§†è§‰æ–‡æœ¬ã€‚é€šè¿‡å¼•å…¥è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å’ŒUM-ç¼–ç å™¨ï¼Œæ¨¡å‹èƒ½å¤Ÿè‡ªåŠ¨é…ç½®å„ç§æ¡ä»¶ä¿¡æ¯çš„åµŒå…¥ï¼Œä»¥å®ç°æ›´ç²¾ç¡®çš„æ–‡æœ¬å†…å®¹å’Œå¸ƒå±€è®¾è®¡ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†ä¸€ç§åŒºåŸŸä¸€è‡´æ€§æŸå¤±å‡½æ•°å’Œä¸‰é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œä»¥æé«˜æ¨¡å‹çš„ç”Ÿæˆæ•ˆæœã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.07290', 'title': 'VideoLoom: A Video Large Language Model for Joint Spatial-Temporal Understanding', 'url': 'https://huggingface.co/papers/2601.07290', 'abstract': 'VideoLoom is a unified video large language model that achieves state-of-the-art performance in spatial-temporal video understanding through a specialized dataset and benchmark.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper presents VideoLoom, a unified Video Large Language Model (Video LLM) for joint spatial-temporal understanding. To facilitate the development of fine-grained spatial and temporal localization capabilities, we curate LoomData-8.7k, a human-centric video dataset with temporally grounded and spatially localized captions. With this, VideoLoom achieves state-of-the-art or highly competitive performance across a variety of spatial and temporal benchmarks (e.g., 63.1 J&F on ReVOS for referring video object segmentation, and 48.3 R1@0.7 on Charades-STA for temporal grounding). In addition, we introduce LoomBench, a novel benchmark consisting of temporal, spatial, and compositional video-question pairs, enabling a comprehensive evaluation of Video LLMs from diverse aspects. Collectively, these contributions offer a universal and effective suite for joint spatial-temporal video understanding, setting a new standard in multimodal intelligence.', 'score': 3, 'issue_id': 579, 'pub_date': '2026-01-12', 'pub_date_card': {'ru': '12 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 12', 'zh': '1æœˆ12æ—¥'}, 'hash': '120e8b203796ca39', 'authors': ['Jiapeng Shi', 'Junke Wang', 'Zuyao You', 'Bo He', 'Zuxuan Wu'], 'affiliations': ['Fudan University', 'University of Maryland, College Park'], 'pdf_title_img': 'assets/pdf/title_img/2601.07290.jpg', 'data': {'categories': ['#multimodal', '#open_source', '#video', '#benchmark', '#synthetic', '#dataset'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ¾Ğ³Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¸ Ğ³Ğ´Ğµ ÑÑ‚Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚', 'desc': 'VideoLoom â€” ÑÑ‚Ğ¾ ĞµĞ´Ğ¸Ğ½Ğ°Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°ĞµÑ‚ Ğ²Ğ¸Ğ´ĞµĞ¾ ÑÑ€Ğ°Ğ·Ñƒ Ğ² Ğ´Ğ²ÑƒÑ… Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸ÑÑ…: Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ (Ğ³Ğ´Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹) Ğ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ (ĞºĞ¾Ğ³Ğ´Ğ° ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ñ). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ LoomData-8.7k Ñ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ³Ğ´Ğµ ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚ Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚ Ğ¿Ğ¾Ğ¼ĞµÑ‡ĞµĞ½Ñ‹ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ÑĞ¼Ğ¸ Ğ¸Ñ… Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚Ğ° Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞº ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº LoomBench Ğ´Ğ»Ñ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ LLM Ğ¿Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ°ÑĞ¿ĞµĞºÑ‚Ğ°Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'VideoLoom: Redefining Video Understanding with Unified Language Models', 'desc': 'VideoLoom is a cutting-edge Video Large Language Model (LLM) designed for advanced understanding of video content in both space and time. It utilizes a specially curated dataset called LoomData-8.7k, which includes human-centric videos with detailed captions that are both temporally and spatially grounded. The model demonstrates exceptional performance on various benchmarks, achieving top scores in tasks like referring video object segmentation and temporal grounding. Additionally, the introduction of LoomBench provides a new way to evaluate Video LLMs through diverse video-question pairs, enhancing the assessment of multimodal intelligence capabilities.'}, 'zh': {'title': 'è§†é¢‘ç†è§£çš„æ–°æ ‡å‡†ï¼šVideoLoom', 'desc': 'VideoLoomæ˜¯ä¸€ç§ç»Ÿä¸€çš„è§†é¢‘å¤§è¯­è¨€æ¨¡å‹ï¼Œä¸“æ³¨äºæ—¶ç©ºè§†é¢‘ç†è§£ã€‚ä¸ºäº†æå‡ç»†ç²’åº¦çš„æ—¶ç©ºå®šä½èƒ½åŠ›ï¼Œç ”ç©¶è€…ä»¬åˆ›å»ºäº†LoomData-8.7kï¼Œè¿™æ˜¯ä¸€ä¸ªä»¥äººä¸ºä¸­å¿ƒçš„è§†é¢‘æ•°æ®é›†ï¼ŒåŒ…å«æ—¶é—´å’Œç©ºé—´å®šä½çš„å­—å¹•ã€‚VideoLoomåœ¨å¤šä¸ªæ—¶ç©ºåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚é€šè¿‡å¼•å…¥LoomBenchåŸºå‡†ï¼Œç ”ç©¶è€…èƒ½å¤Ÿä»ä¸åŒæ–¹é¢å…¨é¢è¯„ä¼°è§†é¢‘å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.06786', 'title': "EpiCaR: Knowing What You Don't Know Matters for Better Reasoning in LLMs", 'url': 'https://huggingface.co/papers/2601.06786', 'abstract': 'Training large language models for improved reasoning while maintaining calibration through epistemic learning reduces inference compute requirements and enhances performance on mathematical and coding tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Improving the reasoning abilities of large language models (LLMs) has largely relied on iterative self-training with model-generated data. While effective at boosting accuracy, existing approaches primarily reinforce successful reasoning paths, incurring a substantial calibration cost: models become overconfident and lose the ability to represent uncertainty. This failure has been characterized as a form of model collapse in alignment, where predictive distributions degenerate toward low-variance point estimates. We address this issue by reframing reasoning training as an epistemic learning problem, in which models must learn not only how to reason, but also when their reasoning should be trusted. We propose epistemically-calibrated reasoning (EpiCaR) as a training objective that jointly optimizes reasoning performance and calibration, and instantiate it within an iterative supervised fine-tuning framework using explicit self-evaluation signals. Experiments on Llama-3 and Qwen-3 families demonstrate that our approach achieves Pareto-superiority over standard baselines in both accuracy and calibration, particularly in models with sufficient reasoning capacity (e.g., 3B+). This framework generalizes effectively to OOD mathematical reasoning (GSM8K) and code generation (MBPP). Ultimately, our approach enables a 3X reduction in inference compute, matching the K=30 performance of STaR with only K=10 samples in capable models.', 'score': 3, 'issue_id': 575, 'pub_date': '2026-01-11', 'pub_date_card': {'ru': '11 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 11', 'zh': '1æœˆ11æ—¥'}, 'hash': 'c93dbc2828071573', 'authors': ['Jewon Yeom', 'Jaewon Sok', 'Seonghyeon Park', 'Jeongjae Park', 'Taesup Kim'], 'affiliations': ['Department of Aerospace Engineering, Seoul National University', 'Department of Rural Systems Engineering, Seoul National University', 'Graduate School of Data Science, Seoul National University'], 'pdf_title_img': 'assets/pdf/title_img/2601.06786.jpg', 'data': {'categories': ['#small_models', '#reasoning', '#inference', '#training', '#alignment', '#plp', '#math', '#optimization'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ£Ñ‡Ğ¸Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ´ÑƒĞ¼Ğ°Ñ‚ÑŒ, Ğ½Ğ¾ Ğ¸ Ğ·Ğ½Ğ°Ñ‚ÑŒ, ĞºĞ¾Ğ³Ğ´Ğ° Ğ´Ğ¾Ğ²ĞµÑ€ÑÑ‚ÑŒ ÑĞ²Ğ¾Ğ¸Ğ¼ Ğ¼Ñ‹ÑĞ»ÑĞ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ EpiCaR Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºÑƒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ â€” ĞµÑ‘ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ ĞºĞ°Ğº Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ÑĞ¿Ğ¸ÑÑ‚ĞµĞ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ³Ğ´Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾Ğ»Ğ¶Ğ½Ğ° Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ, Ğ½Ğ¾ Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ, ĞºĞ¾Ğ³Ğ´Ğ° ĞµÑ‘ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ñ‹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ fine-tuning Ñ ÑĞ²Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ°Ğ¼Ğ¸ ÑĞ°Ğ¼Ğ¾Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ½Ğ°Ğ´ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ°. Ğ­Ñ‚Ğ° Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ° ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğº Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸ÑĞ¼ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ğ² Ñ‚Ñ€Ğ¸ Ñ€Ğ°Ğ·Ğ°, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ.'}, 'en': {'title': 'Enhancing Reasoning and Calibration in Language Models with EpiCaR', 'desc': 'This paper introduces a new method for training large language models (LLMs) to improve their reasoning abilities while ensuring they remain well-calibrated in their predictions. The authors identify a common problem where models become overconfident and fail to represent uncertainty, leading to poor performance in reasoning tasks. They propose a training objective called epistemically-calibrated reasoning (EpiCaR), which focuses on teaching models not only how to reason but also when to trust their reasoning. Their experiments show that this approach significantly enhances both accuracy and calibration, while also reducing the computational resources needed for inference.'}, 'zh': {'title': 'æå‡æ¨ç†èƒ½åŠ›ä¸æ ¡å‡†æ€§çš„åŒé‡ä¼˜åŒ–', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„è®­ç»ƒæ–¹æ³•ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹çš„æ ¡å‡†æ€§ã€‚æˆ‘ä»¬å°†æ¨ç†è®­ç»ƒé‡æ–°å®šä¹‰ä¸ºä¸€ç§è®¤è¯†å­¦ä¹ é—®é¢˜ï¼Œä½¿æ¨¡å‹ä¸ä»…å­¦ä¹ å¦‚ä½•æ¨ç†ï¼Œè¿˜è¦å­¦ä¼šä½•æ—¶ä¿¡ä»»å…¶æ¨ç†ç»“æœã€‚æˆ‘ä»¬æå‡ºçš„EpiCaRç›®æ ‡å¯ä»¥åŒæ—¶ä¼˜åŒ–æ¨ç†æ€§èƒ½å’Œæ ¡å‡†æ€§ï¼Œå¹¶åœ¨è¿­ä»£ç›‘ç£å¾®è°ƒæ¡†æ¶ä¸­å®ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å‡†ç¡®æ€§å’Œæ ¡å‡†æ€§ä¸Šä¼˜äºä¼ ç»ŸåŸºçº¿ï¼Œç‰¹åˆ«æ˜¯åœ¨å…·æœ‰è¶³å¤Ÿæ¨ç†èƒ½åŠ›çš„æ¨¡å‹ä¸­ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.07632', 'title': 'GeoMotionGPT: Geometry-Aligned Motion Understanding with Large Language Models', 'url': 'https://huggingface.co/papers/2601.07632', 'abstract': "A novel framework for motion understanding and motion-language reasoning that aligns motion token geometry with language model embeddings through orthogonal constraints and sparse projection techniques.  \t\t\t\t\tAI-generated summary \t\t\t\t Discrete motion tokenization has recently enabled Large Language Models (LLMs) to serve as versatile backbones for motion understanding and motion-language reasoning. However, existing pipelines typically decouple motion quantization from semantic embedding learning, linking them solely via token IDs. This approach fails to effectively align the intrinsic geometry of the motion space with the embedding space, thereby hindering the LLM's capacity for nuanced motion reasoning. We argue that alignment is most effective when both modalities share a unified geometric basis. Therefore, instead of forcing the LLM to reconstruct the complex geometry among motion tokens from scratch, we present a novel framework that explicitly enforces orthogonality on both the motion codebook and the LLM embedding space, ensuring that their relational structures naturally mirror each other. Specifically, we employ a decoder-only quantizer with Gumbel-Softmax for differentiable training and balanced codebook usage. To bridge the modalities, we use a sparse projection that maps motion codes into the LLM embedding space while preserving orthogonality. Finally, a two-stage orthonormal regularization schedule enforces soft constraints during tokenizer training and LLM fine-tuning to maintain geometric alignment without hindering semantic adaptation. Extensive experiments on HumanML3D demonstrate that our framework achieves a 20% performance improvement over current state-of-the-art methods, validating that a unified geometric basis effectively empowers the LLM for nuanced motion reasoning.", 'score': 1, 'issue_id': 585, 'pub_date': '2026-01-12', 'pub_date_card': {'ru': '12 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 12', 'zh': '1æœˆ12æ—¥'}, 'hash': '7f02d40dc523d40b', 'authors': ['Zhankai Ye', 'Bofan Li', 'Yukai Jin', 'Shuoqiu Li', 'Wei Wang', 'Yanfu Zhang', 'Shangqian Gao', 'Xin Liu'], 'affiliations': ['Florida State University', 'Texas Tech University', 'William & Mary University'], 'pdf_title_img': 'assets/pdf/title_img/2601.07632.jpg', 'data': {'categories': [], 'emoji': 'ğŸ•º', 'ru': {'title': 'Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¾ÑĞ½Ğ¾Ğ²Ğ° Ğ´Ğ»Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ°', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¾ ÑĞ²ÑĞ·Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ¼ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ°Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€ĞµÑˆĞ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑÑÑ‚ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ½ĞµÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ¾Ñ‚Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ². ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ñ€Ñ‚Ğ¾Ğ³Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° ĞºĞ¾Ğ´Ğ¾Ğ²ÑƒÑ ĞºĞ½Ğ¸Ğ³Ñƒ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑĞ¼Ğ±ĞµĞ´dings LLM, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾ĞµĞºÑ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ´Ğ¾Ğ² Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¾Ñ€Ñ‚Ğ¾Ğ³Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ HumanML3D Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ 20% ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Aligning Motion and Language for Enhanced Reasoning', 'desc': "This paper introduces a new framework for understanding motion and reasoning about it using language. It addresses the issue of how motion data and language models are connected, proposing that they should share a common geometric structure for better alignment. The authors implement orthogonal constraints and sparse projection techniques to ensure that the motion tokens and language embeddings reflect each other's relationships accurately. Their approach shows significant improvements in performance, demonstrating that a unified geometric basis enhances the ability of language models to reason about motion effectively."}, 'zh': {'title': 'ç»Ÿä¸€å‡ ä½•åŸºç¡€æå‡è¿åŠ¨æ¨ç†èƒ½åŠ›', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„è¿åŠ¨ç†è§£å’Œè¿åŠ¨è¯­è¨€æ¨ç†æ¡†æ¶ï¼Œé€šè¿‡æ­£äº¤çº¦æŸå’Œç¨€ç–æŠ•å½±æŠ€æœ¯ï¼Œå°†è¿åŠ¨æ ‡è®°å‡ ä½•ä¸è¯­è¨€æ¨¡å‹åµŒå…¥å¯¹é½ã€‚ç°æœ‰çš„æ–¹æ³•é€šå¸¸å°†è¿åŠ¨é‡åŒ–ä¸è¯­ä¹‰åµŒå…¥å­¦ä¹ åˆ†å¼€ï¼Œå¯¼è‡´è¿åŠ¨ç©ºé—´çš„å†…åœ¨å‡ ä½•ä¸åµŒå…¥ç©ºé—´æœªèƒ½æœ‰æ•ˆå¯¹é½ï¼Œä»è€Œé™åˆ¶äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç»†è‡´è¿åŠ¨æ¨ç†æ–¹é¢çš„èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡åœ¨è¿åŠ¨ä»£ç æœ¬å’ŒLLMåµŒå…¥ç©ºé—´ä¸Šå¼ºåˆ¶æ­£äº¤æ€§ï¼Œç¡®ä¿å®ƒä»¬çš„å…³ç³»ç»“æ„è‡ªç„¶åœ°ç›¸äº’æ˜ å°„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨HumanML3Dæ•°æ®é›†ä¸Šå®ç°äº†20%çš„æ€§èƒ½æå‡ï¼ŒéªŒè¯äº†ç»Ÿä¸€å‡ ä½•åŸºç¡€åœ¨ç»†è‡´è¿åŠ¨æ¨ç†ä¸­çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.04582', 'title': 'Aligning Text, Code, and Vision: A Multi-Objective Reinforcement Learning Framework for Text-to-Visualization', 'url': 'https://huggingface.co/papers/2601.04582', 'abstract': 'A reinforcement learning framework for text-to-visualization generation that improves chart quality and code execution by optimizing multiple objectives using post-execution feedback.  \t\t\t\t\tAI-generated summary \t\t\t\t Text-to-Visualization (Text2Vis) systems translate natural language queries over tabular data into concise answers and executable visualizations. While closed-source LLMs generate functional code, the resulting charts often lack semantic alignment and clarity, qualities that can only be assessed post-execution. Open-source models struggle even more, frequently producing non-executable or visually poor outputs. Although supervised fine-tuning can improve code executability, it fails to enhance overall visualization quality, as traditional SFT loss cannot capture post-execution feedback. To address this gap, we propose RL-Text2Vis, the first reinforcement learning framework for Text2Vis generation. Built on Group Relative Policy Optimization (GRPO), our method uses a novel multi-objective reward that jointly optimizes textual accuracy, code validity, and visualization quality using post-execution feedback. By training Qwen2.5 models (7B and 14B), RL-Text2Vis achieves a 22% relative improvement in chart quality over GPT-4o on the Text2Vis benchmark and boosts code execution success from 78% to 97% relative to its zero-shot baseline. Our models significantly outperform strong zero-shot and supervised baselines and also demonstrate robust generalization to out-of-domain datasets like VIS-Eval and NVBench. These results establish GRPO as an effective strategy for structured, multimodal reasoning in visualization generation. We release our code at https://github.com/vis-nlp/RL-Text2Vis.', 'score': 1, 'issue_id': 566, 'pub_date': '2026-01-08', 'pub_date_card': {'ru': '8 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 8', 'zh': '1æœˆ8æ—¥'}, 'hash': '596be1c607cd146a', 'authors': ['Mizanur Rahman', 'Mohammed Saidul Islam', 'Md Tahmid Rahman Laskar', 'Shafiq Joty', 'Enamul Hoque'], 'affiliations': ['Nanyang Technological University', 'Salesforce AI Research', 'York University'], 'pdf_title_img': 'assets/pdf/title_img/2601.04582.jpg', 'data': {'categories': ['#open_source', '#benchmark', '#rl', '#optimization', '#multimodal', '#training'], 'emoji': 'ğŸ“Š', 'ru': {'title': 'ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¹ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ RL-Text2Vis â€” Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¹ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ½Ğ°Ğ´ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Group Relative Policy Optimization (GRPO) Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ñ„ÑƒĞ½ĞºÑ†Ğ¸ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²ÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ, ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ¾Ğ´Ğ° Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ´Ğ¸Ğ°Ğ³Ñ€Ğ°Ğ¼Ğ¼, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ Ğ¿Ğ¾ÑĞ»Ğµ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Qwen2.5 Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ 22% Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¾Ğ² Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ GPT-4o Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ÑÑ‚ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ´Ğ° Ñ 78% Ğ´Ğ¾ 97%. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½ÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ GRPO Ğ´Ğ»Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'Reinforcement Learning for Enhanced Text-to-Visualization Generation', 'desc': 'This paper introduces RL-Text2Vis, a novel reinforcement learning framework designed to enhance the generation of visualizations from text queries. It addresses the limitations of existing models by optimizing multiple objectives, including textual accuracy, code validity, and visualization quality, using feedback obtained after code execution. The framework employs Group Relative Policy Optimization (GRPO) to improve the quality of charts generated, achieving a significant increase in both chart quality and code execution success rates. The results demonstrate that RL-Text2Vis outperforms traditional methods and shows strong generalization capabilities across different datasets.'}, 'zh': {'title': 'å¼ºåŒ–å­¦ä¹ æå‡æ–‡æœ¬åˆ°å¯è§†åŒ–ç”Ÿæˆçš„è´¨é‡', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºæ–‡æœ¬åˆ°å¯è§†åŒ–ç”Ÿæˆçš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œç§°ä¸ºRL-Text2Visã€‚è¯¥æ¡†æ¶é€šè¿‡ä¼˜åŒ–å¤šä¸ªç›®æ ‡ï¼Œåˆ©ç”¨æ‰§è¡Œåçš„åé¦ˆæ¥æé«˜å›¾è¡¨è´¨é‡å’Œä»£ç æ‰§è¡Œæ•ˆæœã€‚ä¸ä¼ ç»Ÿçš„ç›‘ç£å¾®è°ƒæ–¹æ³•ä¸åŒï¼ŒRL-Text2Visé‡‡ç”¨äº†ä¸€ç§æ–°çš„å¤šç›®æ ‡å¥–åŠ±æœºåˆ¶ï¼Œèƒ½å¤ŸåŒæ—¶ä¼˜åŒ–æ–‡æœ¬å‡†ç¡®æ€§ã€ä»£ç æœ‰æ•ˆæ€§å’Œå¯è§†åŒ–è´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRL-Text2Visåœ¨å›¾è¡¨è´¨é‡ä¸Šç›¸è¾ƒäºGPT-4oæœ‰22%çš„ç›¸å¯¹æå‡ï¼Œä»£ç æ‰§è¡ŒæˆåŠŸç‡ä»78%æé«˜åˆ°97%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.02669', 'title': 'Towards Comprehensive Stage-wise Benchmarking of Large Language Models in Fact-Checking', 'url': 'https://huggingface.co/papers/2601.02669', 'abstract': "FactArena is an automated evaluation framework that comprehensively assesses large language models across all stages of the fact-checking pipeline, revealing gaps between claim verification accuracy and overall fact-checking capability.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) are increasingly deployed in real-world fact-checking systems, yet existing evaluations focus predominantly on claim verification and overlook the broader fact-checking workflow, including claim extraction and evidence retrieval. This narrow focus prevents current benchmarks from revealing systematic reasoning failures, factual blind spots, and robustness limitations of modern LLMs. To bridge this gap, we present FactArena, a fully automated arena-style evaluation framework that conducts comprehensive, stage-wise benchmarking of LLMs across the complete fact-checking pipeline. FactArena integrates three key components: (i) an LLM-driven fact-checking process that standardizes claim decomposition, evidence retrieval via tool-augmented interactions, and justification-based verdict prediction; (ii) an arena-styled judgment mechanism guided by consolidated reference guidelines to ensure unbiased and consistent pairwise comparisons across heterogeneous judge agents; and (iii) an arena-driven claim-evolution module that adaptively generates more challenging and semantically controlled claims to probe LLMs' factual robustness beyond fixed seed data. Across 16 state-of-the-art LLMs spanning seven model families, FactArena produces stable and interpretable rankings. Our analyses further reveal significant discrepancies between static claim-verification accuracy and end-to-end fact-checking competence, highlighting the necessity of holistic evaluation. The proposed framework offers a scalable and trustworthy paradigm for diagnosing LLMs' factual reasoning, guiding future model development, and advancing the reliable deployment of LLMs in safety-critical fact-checking applications.", 'score': 0, 'issue_id': 571, 'pub_date': '2026-01-06', 'pub_date_card': {'ru': '6 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 6', 'zh': '1æœˆ6æ—¥'}, 'hash': '9a7c27380caaf64c', 'authors': ['Hongzhan Lin', 'Zixin Chen', 'Zhiqi Shen', 'Ziyang Luo', 'Zhen Ye', 'Jing Ma', 'Tat-Seng Chua', 'Guandong Xu'], 'affiliations': ['Department of Computer Science, Hong Kong Baptist University', 'National University of Singapore', 'Salesforce AI Research', 'The Education University of Hong Kong', 'The Hong Kong University of Science and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2601.02669.jpg', 'data': {'categories': [], 'emoji': 'ğŸ”', 'ru': {'title': 'ĞÑ‚ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğº Ñ†ĞµĞ»Ğ¾ÑÑ‚Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞµ: ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑÑ‚ÑŒ Ñ„Ğ°ĞºÑ‚Ñ‹', 'desc': 'FactArena Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ²ÑĞµÑ… ÑÑ‚Ğ°Ğ¿Ğ°Ñ… Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ñ„Ğ°ĞºÑ‚Ğ¾Ğ², Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ¿Ğ¾Ğ¸ÑĞº Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ² Ğ¸ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ LLM-ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ñ„Ğ°ĞºÑ‚-Ñ‡ĞµĞºĞ¸Ğ½Ğ³Ğ° Ñ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ¼ Ğ°Ñ€ĞµĞ½Ñ‹-ÑÑ‚Ğ¸Ğ»Ñ Ğ´Ğ»Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑĞ»ÑƒÑ‡Ğ°ĞµĞ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° 16 ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… LLM Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€Ğ°ÑÑ…Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¾Ğ±Ñ‰ĞµĞ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ñ„Ğ°ĞºÑ‚Ğ¾Ğ². Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ»Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ LLM Ğ² ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ğ¾ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ÑÑ… Ñ„Ğ°ĞºÑ‚-Ñ‡ĞµĞºĞ¸Ğ½Ğ³Ğ°.'}, 'en': {'title': 'FactArena: Holistic Evaluation for Reliable Fact-Checking in LLMs', 'desc': "FactArena is a new evaluation framework designed to assess large language models (LLMs) throughout the entire fact-checking process, not just their ability to verify claims. It identifies weaknesses in LLMs by examining their performance in claim extraction, evidence retrieval, and justification of verdicts. The framework includes a standardized process for fact-checking, a fair judgment system for comparing models, and a module that generates challenging claims to test LLMs' robustness. By revealing gaps between claim verification accuracy and overall fact-checking ability, FactArena aims to improve the reliability of LLMs in critical applications."}, 'zh': {'title': 'å…¨é¢è¯„ä¼°è¯­è¨€æ¨¡å‹çš„äº‹å®æ ¸æŸ¥èƒ½åŠ›', 'desc': 'FactArenaæ˜¯ä¸€ä¸ªè‡ªåŠ¨åŒ–è¯„ä¼°æ¡†æ¶ï¼Œå…¨é¢è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨äº‹å®æ ¸æŸ¥æµç¨‹ä¸­çš„å„ä¸ªé˜¶æ®µï¼Œæ­ç¤ºäº†å£°æ˜éªŒè¯å‡†ç¡®æ€§ä¸æ•´ä½“äº‹å®æ ¸æŸ¥èƒ½åŠ›ä¹‹é—´çš„å·®è·ã€‚ç°æœ‰è¯„ä¼°ä¸»è¦é›†ä¸­åœ¨å£°æ˜éªŒè¯ï¼Œå¿½è§†äº†æ›´å¹¿æ³›çš„äº‹å®æ ¸æŸ¥å·¥ä½œæµç¨‹ï¼ŒåŒ…æ‹¬å£°æ˜æå–å’Œè¯æ®æ£€ç´¢ã€‚FactArenaé€šè¿‡æ ‡å‡†åŒ–çš„å£°æ˜åˆ†è§£ã€å·¥å…·å¢å¼ºçš„è¯æ®æ£€ç´¢å’ŒåŸºäºç†ç”±çš„åˆ¤å†³é¢„æµ‹ï¼Œæä¾›äº†å…¨é¢çš„é˜¶æ®µæ€§åŸºå‡†æµ‹è¯•ã€‚è¯¥æ¡†æ¶ä¸ºè¯Šæ–­å¤§å‹è¯­è¨€æ¨¡å‹çš„äº‹å®æ¨ç†æä¾›äº†å¯æ‰©å±•å’Œå¯é çš„èŒƒå¼ï¼Œæ¨åŠ¨äº†åœ¨å®‰å…¨å…³é”®çš„äº‹å®æ ¸æŸ¥åº”ç”¨ä¸­å¯é éƒ¨ç½²å¤§å‹è¯­è¨€æ¨¡å‹çš„è¿›å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.23959', 'title': 'Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling', 'url': 'https://huggingface.co/papers/2512.23959', 'abstract': 'Multi-step retrieval-augmented generation (RAG) has become a widely adopted strategy for enhancing large language models (LLMs) on tasks that demand global comprehension and intensive reasoning. Many RAG systems incorporate a working memory module to consolidate retrieved information. However, existing memory designs function primarily as passive storage that accumulates isolated facts for the purpose of condensing the lengthy inputs and generating new sub-queries through deduction. This static nature overlooks the crucial high-order correlations among primitive facts, the compositions of which can often provide stronger guidance for subsequent steps. Therefore, their representational strength and impact on multi-step reasoning and knowledge evolution are limited, resulting in fragmented reasoning and weak global sense-making capacity in extended contexts. We introduce HGMem, a hypergraph-based memory mechanism that extends the concept of memory beyond simple storage into a dynamic, expressive structure for complex reasoning and global understanding. In our approach, memory is represented as a hypergraph whose hyperedges correspond to distinct memory units, enabling the progressive formation of higher-order interactions within memory. This mechanism connects facts and thoughts around the focal problem, evolving into an integrated and situated knowledge structure that provides strong propositions for deeper reasoning in subsequent steps. We evaluate HGMem on several challenging datasets designed for global sense-making. Extensive experiments and in-depth analyses show that our method consistently improves multi-step RAG and substantially outperforms strong baseline systems across diverse tasks.', 'score': 79, 'issue_id': 386, 'pub_date': '2026-12-30', 'pub_date_card': {'ru': '30 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 30', 'zh': '12æœˆ30æ—¥'}, 'hash': '7840b0b37077c947', 'authors': ['Chulun Zhou', 'Chunkang Zhang', 'Guoxin Yu', 'Fandong Meng', 'Jie Zhou', 'Wai Lam', 'Mo Yu'], 'affiliations': ['The Chinese University of Hong Kong', 'WeChat AI'], 'pdf_title_img': 'assets/pdf/title_img/2512.23959.jpg', 'data': {'categories': ['#graphs', '#training', '#rag', '#reasoning', '#long_context'], 'emoji': 'ğŸ•¸ï¸', 'ru': {'title': 'ĞÑ‚ Ñ…Ñ€Ğ°Ğ½Ğ¸Ğ»Ğ¸Ñ‰Ğ° Ğº ÑĞµÑ‚ĞµĞ²Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸: Ğ³Ğ¸Ğ¿ĞµÑ€Ğ³Ñ€Ğ°Ñ„Ñ‹ Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ HGMem Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ retrieval-augmented generation, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°ĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², Ğ³Ğ´Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ°Ğº Ğ¿Ğ°ÑÑĞ¸Ğ²Ğ½Ğ¾Ğµ Ñ…Ñ€Ğ°Ğ½Ğ¸Ğ»Ğ¸Ñ‰Ğµ Ñ„Ğ°ĞºÑ‚Ğ¾Ğ². ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ³Ğ¸Ğ¿ĞµÑ€Ğ³Ñ€Ğ°Ñ„Ğ°Ñ…, Ğ³Ğ´Ğµ Ğ³Ğ¸Ğ¿ĞµÑ€Ñ€ĞµĞ±Ñ€Ğ° Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ ĞµĞ´Ğ¸Ğ½Ğ¸Ñ†Ñ‹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¿Ğ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾ Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ„Ğ°ĞºÑ‚Ğ°Ğ¼Ğ¸ Ğ¸ Ğ¼Ñ‹ÑĞ»ÑĞ¼Ğ¸. Ğ­Ñ‚Ğ¾ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğµ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ğµ ÑˆĞ°Ğ³Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ HGMem Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°.'}, 'en': {'title': 'Revolutionizing Memory for Enhanced Reasoning in Language Models', 'desc': "This paper presents HGMem, a new memory mechanism designed to enhance multi-step retrieval-augmented generation (RAG) in large language models (LLMs). Unlike traditional memory systems that simply store isolated facts, HGMem uses a hypergraph structure to create dynamic connections between information, allowing for richer reasoning and better understanding of complex tasks. By representing memory as a hypergraph, it captures higher-order relationships among facts, which improves the model's ability to make sense of information over multiple steps. The authors demonstrate that HGMem significantly outperforms existing methods on various datasets, showcasing its effectiveness in enhancing global comprehension and reasoning capabilities."}, 'zh': {'title': 'è¶…å›¾è®°å¿†ï¼šæå‡å¤šæ­¥éª¤æ¨ç†çš„å…³é”®', 'desc': 'å¤šæ­¥éª¤æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ˜¯ä¸€ç§æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å…¨çƒç†è§£å’Œå¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„ç­–ç•¥ã€‚ç°æœ‰çš„è®°å¿†è®¾è®¡ä¸»è¦ä½œä¸ºè¢«åŠ¨å­˜å‚¨ï¼Œæ— æ³•æœ‰æ•ˆåˆ©ç”¨é«˜é˜¶å…³è”æ¥å¢å¼ºæ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†HGMemï¼Œä¸€ç§åŸºäºè¶…å›¾çš„è®°å¿†æœºåˆ¶ï¼Œå°†è®°å¿†æ‰©å±•ä¸ºåŠ¨æ€ã€è¡¨è¾¾ä¸°å¯Œçš„ç»“æ„ï¼Œä»¥æ”¯æŒå¤æ‚æ¨ç†å’Œå…¨çƒç†è§£ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒHGMemåœ¨å¤šæ­¥éª¤RAGä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—è¶…è¶Šäº†å¼ºåŸºçº¿ç³»ç»Ÿã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.24617', 'title': 'Dynamic Large Concept Models: Latent Reasoning in an Adaptive Semantic Space', 'url': 'https://huggingface.co/papers/2512.24617', 'abstract': 'Large Language Models (LLMs) apply uniform computation to all tokens, despite language exhibiting highly non-uniform information density. This token-uniform regime wastes capacity on locally predictable spans while under-allocating computation to semantically critical transitions. We propose Dynamic Large Concept Models (DLCM), a hierarchical language modeling framework that learns semantic boundaries from latent representations and shifts computation from tokens to a compressed concept space where reasoning is more efficient. DLCM discovers variable-length concepts end-to-end without relying on predefined linguistic units. Hierarchical compression fundamentally changes scaling behavior. We introduce the first compression-aware scaling law, which disentangles token-level capacity, concept-level reasoning capacity, and compression ratio, enabling principled compute allocation under fixed FLOPs. To stably train this heterogeneous architecture, we further develop a decoupled Î¼P parametrization that supports zero-shot hyperparameter transfer across widths and compression regimes. At a practical setting (R=4, corresponding to an average of four tokens per concept), DLCM reallocates roughly one-third of inference compute into a higher-capacity reasoning backbone, achieving a +2.69\\% average improvement across 12 zero-shot benchmarks under matched inference FLOPs.', 'score': 40, 'issue_id': 375, 'pub_date': '2026-12-31', 'pub_date_card': {'ru': '31 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 31', 'zh': '12æœˆ31æ—¥'}, 'hash': '14b3c28c67b8e772', 'authors': ['Xingwei Qu', 'Shaowen Wang', 'Zihao Huang', 'Kai Hua', 'Fan Yin', 'Rui-Jie Zhu', 'Jundong Zhou', 'Qiyang Min', 'Zihao Wang', 'Yizhi Li', 'Tianyu Zhang', 'He Xing', 'Zheng Zhang', 'Yuxuan Song', 'Tianyu Zheng', 'Zhiyuan Zeng', 'Chenghua Lin', 'Ge Zhang', 'Wenhao Huang'], 'affiliations': ['ByteDance', 'M-A-P', 'Mila - Quebec AI Institute', 'Tsinghua University', 'University of Manchester'], 'pdf_title_img': 'assets/pdf/title_img/2512.24617.jpg', 'data': {'categories': ['#training', '#architecture'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ¡Ğ¼Ğ°Ñ€Ñ‚-ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ: ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚Ñ‹ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚Ğ¾Ğ² (DLCM), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ²ÑĞµÑ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ Ğ¾Ğ´Ğ¸Ğ½Ğ°ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½ĞµÑ€Ğ°Ğ²Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² ÑĞ·Ñ‹ĞºĞµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑƒÑ‡Ğ¸Ñ‚ÑÑ Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ğ¸ ÑĞ¶Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ² ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾, Ğ³Ğ´Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑÑ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹Ğ²ĞµĞ»Ğ¸ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ·Ğ°ĞºĞ¾Ğ½ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ ÑƒÑ‡Ñ‘Ñ‚Ğ¾Ğ¼ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ÑÑ‚ÑŒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¾Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¾ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚Ğ°Ñ…. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ°Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ, Ğ¸ Ğ½Ğ° Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸ĞºĞµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ° 2.69% Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ñ‚Ğ¾Ğ³Ğ¾ Ğ¶Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Revolutionizing Language Models with Dynamic Concept Learning', 'desc': 'This paper introduces Dynamic Large Concept Models (DLCM), which improve how language models process information by focusing on important semantic transitions instead of treating all tokens equally. By learning semantic boundaries and shifting computation to a compressed concept space, DLCM enhances reasoning efficiency. The framework allows for the discovery of variable-length concepts without relying on fixed linguistic units, fundamentally changing how models scale. Additionally, it presents a new scaling law that optimizes compute allocation, leading to significant performance improvements in zero-shot tasks.'}, 'zh': {'title': 'åŠ¨æ€å¤§å‹æ¦‚å¿µæ¨¡å‹ï¼šæå‡è¯­è¨€æ¨¡å‹æ¨ç†æ•ˆç‡çš„åˆ›æ–°', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯¹æ‰€æœ‰æ ‡è®°åº”ç”¨ç»Ÿä¸€è®¡ç®—ï¼Œä½†è¯­è¨€çš„ä¿¡æ¯å¯†åº¦å¹¶ä¸å‡åŒ€ã€‚è¿™ç§ç»Ÿä¸€çš„è®¡ç®—æ–¹å¼åœ¨å¯é¢„æµ‹çš„åŒºåŸŸæµªè´¹äº†è®¡ç®—èƒ½åŠ›ï¼Œè€Œåœ¨è¯­ä¹‰å…³é”®çš„è½¬å˜ä¸Šåˆ™åˆ†é…ä¸è¶³ã€‚æˆ‘ä»¬æå‡ºäº†åŠ¨æ€å¤§å‹æ¦‚å¿µæ¨¡å‹ï¼ˆDLCMï¼‰ï¼Œå®ƒé€šè¿‡å­¦ä¹ æ½œåœ¨è¡¨ç¤ºçš„è¯­ä¹‰è¾¹ç•Œï¼Œå°†è®¡ç®—ä»æ ‡è®°è½¬ç§»åˆ°å‹ç¼©çš„æ¦‚å¿µç©ºé—´ï¼Œä»è€Œæé«˜æ¨ç†æ•ˆç‡ã€‚DLCMèƒ½å¤Ÿç«¯åˆ°ç«¯åœ°å‘ç°å¯å˜é•¿åº¦çš„æ¦‚å¿µï¼Œå¹¶å¼•å…¥äº†é¦–ä¸ªè€ƒè™‘å‹ç¼©çš„æ‰©å±•æ³•åˆ™ï¼Œä¼˜åŒ–äº†è®¡ç®—èµ„æºçš„åˆ†é…ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.24165', 'title': 'DiffThinker: Towards Generative Multimodal Reasoning with Diffusion Models', 'url': 'https://huggingface.co/papers/2512.24165', 'abstract': 'While recent Multimodal Large Language Models (MLLMs) have attained significant strides in multimodal reasoning, their reasoning processes remain predominantly text-centric, leading to suboptimal performance in complex long-horizon, vision-centric tasks. In this paper, we establish a novel Generative Multimodal Reasoning paradigm and introduce DiffThinker, a diffusion-based reasoning framework. Conceptually, DiffThinker reformulates multimodal reasoning as a native generative image-to-image task, achieving superior logical consistency and spatial precision in vision-centric tasks. We perform a systematic comparison between DiffThinker and MLLMs, providing the first in-depth investigation into the intrinsic characteristics of this paradigm, revealing four core properties: efficiency, controllability, native parallelism, and collaboration. Extensive experiments across four domains (sequential planning, combinatorial optimization, constraint satisfaction, and spatial configuration) demonstrate that DiffThinker significantly outperforms leading closed source models including GPT-5 (+314.2\\%) and Gemini-3-Flash (+111.6\\%), as well as the fine-tuned Qwen3-VL-32B baseline (+39.0\\%), highlighting generative multimodal reasoning as a promising approach for vision-centric reasoning.', 'score': 37, 'issue_id': 378, 'pub_date': '2026-12-30', 'pub_date_card': {'ru': '30 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 30', 'zh': '12æœˆ30æ—¥'}, 'hash': '7e8814af6b3a30c5', 'authors': ['Zefeng He', 'Xiaoye Qu', 'Yafu Li', 'Tong Zhu', 'Siyuan Huang', 'Yu Cheng'], 'affiliations': ['Nanjing University', 'Shanghai AI Laboratory', 'Shanghai Jiao Tong University', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2512.24165.jpg', 'data': {'categories': ['#cv', '#reasoning', '#diffusion', '#architecture', '#multimodal'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ”Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ: Ğ¾Ñ‚ Ñ‚ĞµĞºÑÑ‚Ğ° Ğº Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½Ğ°Ñ DiffThinker. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ ĞºĞ°Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆÑƒÑ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾-Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ° Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ, ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾ÑÑ‚ÑŒ, Ğ²ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»Ğ¸Ğ·Ğ¼ Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ… Ñ‚Ğ¸Ğ¿Ğ°Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ DiffThinker Ğ½Ğ°Ğ´ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ GPT-5 Ğ¸ Gemini-3-Flash.'}, 'en': {'title': 'Revolutionizing Vision-Centric Reasoning with DiffThinker', 'desc': 'This paper introduces DiffThinker, a new framework for Generative Multimodal Reasoning that enhances the reasoning capabilities of Multimodal Large Language Models (MLLMs) in vision-centric tasks. Unlike traditional MLLMs that focus primarily on text, DiffThinker treats multimodal reasoning as a generative image-to-image task, improving logical consistency and spatial accuracy. The authors highlight four key advantages of this approach: efficiency, controllability, native parallelism, and collaboration. Through extensive experiments, DiffThinker demonstrates significant performance improvements over existing models, establishing it as a leading method for complex reasoning in visual contexts.'}, 'zh': {'title': 'ç”Ÿæˆå¤šæ¨¡æ€æ¨ç†çš„æ–°çªç ´', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ç”Ÿæˆå¤šæ¨¡æ€æ¨ç†èŒƒå¼ï¼Œå¹¶å¼•å…¥äº†DiffThinkerï¼Œä¸€ä¸ªåŸºäºæ‰©æ•£çš„æ¨ç†æ¡†æ¶ã€‚DiffThinkerå°†å¤šæ¨¡æ€æ¨ç†é‡æ–°å®šä¹‰ä¸ºä¸€ç§ç”Ÿæˆçš„å›¾åƒåˆ°å›¾åƒä»»åŠ¡ï¼Œä»è€Œåœ¨è§†è§‰ä¸­å¿ƒä»»åŠ¡ä¸­å®ç°äº†æ›´é«˜çš„é€»è¾‘ä¸€è‡´æ€§å’Œç©ºé—´ç²¾åº¦ã€‚é€šè¿‡ä¸ç°æœ‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰è¿›è¡Œç³»ç»Ÿæ¯”è¾ƒï¼Œæ­ç¤ºäº†DiffThinkerçš„å››ä¸ªæ ¸å¿ƒç‰¹æ€§ï¼šæ•ˆç‡ã€å¯æ§æ€§ã€åŸç”Ÿå¹¶è¡Œæ€§å’Œåä½œæ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒDiffThinkeråœ¨å¤šä¸ªé¢†åŸŸçš„è¡¨ç°æ˜¾è‘—ä¼˜äºé¢†å…ˆçš„é—­æºæ¨¡å‹ï¼Œå±•ç¤ºäº†ç”Ÿæˆå¤šæ¨¡æ€æ¨ç†åœ¨è§†è§‰æ¨ç†ä¸­çš„æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.22630', 'title': 'On the Role of Discreteness in Diffusion LLMs', 'url': 'https://huggingface.co/papers/2512.22630', 'abstract': 'Diffusion models offer appealing properties for language generation, such as parallel decoding and iterative refinement, but the discrete and highly structured nature of text challenges the direct application of diffusion principles. In this paper, we revisit diffusion language modeling from the view of diffusion process and language modeling, and outline five properties that separate diffusion mechanics from language-specific requirements. We first categorize existing approaches into continuous diffusion in embedding space and discrete diffusion over tokens. We then show that each satisfies only part of the five essential properties and therefore reflects a structural trade-off. Through analyses of recent large diffusion language models, we identify two central issues: (i) uniform corruption does not respect how information is distributed across positions, and (ii) token-wise marginal training cannot capture multi-token dependencies during parallel decoding. These observations motivate diffusion processes that align more closely with the structure of text, and encourage future work toward more coherent diffusion language models.', 'score': 14, 'issue_id': 375, 'pub_date': '2026-12-27', 'pub_date_card': {'ru': '27 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 27', 'zh': '12æœˆ27æ—¥'}, 'hash': '8ee34765599976ba', 'authors': ['Ziqi Jin', 'Bin Wang', 'Xiang Lin', 'Lidong Bing', 'Aixin Sun'], 'affiliations': ['MiroMind AI', 'Nanyang Technological University'], 'pdf_title_img': 'assets/pdf/title_img/2512.22630.jpg', 'data': {'categories': ['#training', '#architecture', '#diffusion'], 'emoji': 'ğŸ”„', 'ru': {'title': 'Ğ”Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ñ Ğ´Ğ»Ñ Ñ‚ĞµĞºÑÑ‚Ğ°: Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ñ Ğ¿Ñ€Ğ¸Ñ€Ğ¾Ğ´Ğ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ°', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ÑÑ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾Ğ±Ğ»Ğ°Ğ´Ğ°ÑÑ‚ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ğ¼Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğ¼Ğ¸ ĞºĞ°Ğº Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿ĞµÑ€ĞµĞ¾ÑĞ¼Ñ‹ÑĞ»Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğº ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ²Ñ‹Ğ´ĞµĞ»ÑÑÑ‚ Ğ¿ÑÑ‚ÑŒ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ², Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸ĞºÑƒ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ¾Ñ‚ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğº Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ñ‚ĞµĞºÑÑ‚Ğ°. ĞĞ½Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹, Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑÑ Ğ¸Ñ… Ğ½Ğ° Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½ÑƒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ğ¸ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½ÑƒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ñ Ğ½Ğ°Ğ´ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼Ğ¸, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑÑ‹ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹: Ğ¾Ğ´Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ²Ñ€ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ½Ğµ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸ÑĞ¼, Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¼Ğ°Ñ€Ğ¶Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑÑ… Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ½Ğµ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ¸Ñ‚ÑŒ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ‚Ğ¾ĞºĞµĞ½Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸.'}, 'en': {'title': 'Enhancing Language Generation with Structured Diffusion Models', 'desc': 'This paper explores the challenges of applying diffusion models to language generation due to the structured nature of text. It categorizes existing methods into two types: continuous diffusion in embedding space and discrete diffusion over tokens, highlighting their limitations. The authors identify two main issues: the uniform corruption of information and the inability to capture dependencies between multiple tokens during decoding. They propose that future diffusion models should better align with the inherent structure of language to improve coherence in generated text.'}, 'zh': {'title': 'ä¼˜åŒ–æ‰©æ•£æ¨¡å‹ä»¥é€‚åº”è¯­è¨€ç»“æ„', 'desc': 'æ‰©æ•£æ¨¡å‹åœ¨è¯­è¨€ç”Ÿæˆä¸­å…·æœ‰å¸å¼•äººçš„ç‰¹æ€§ï¼Œå¦‚å¹¶è¡Œè§£ç å’Œè¿­ä»£ä¼˜åŒ–ï¼Œä½†æ–‡æœ¬çš„ç¦»æ•£æ€§å’Œé«˜åº¦ç»“æ„åŒ–ç‰¹å¾ä½¿å¾—ç›´æ¥åº”ç”¨æ‰©æ•£åŸç†é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬æ–‡ä»æ‰©æ•£è¿‡ç¨‹å’Œè¯­è¨€å»ºæ¨¡çš„è§’åº¦é‡æ–°å®¡è§†æ‰©æ•£è¯­è¨€å»ºæ¨¡ï¼Œå¹¶æ¦‚è¿°äº†äº”ä¸ªå°†æ‰©æ•£æœºåˆ¶ä¸è¯­è¨€ç‰¹å®šè¦æ±‚åŒºåˆ†å¼€æ¥çš„ç‰¹æ€§ã€‚æˆ‘ä»¬å°†ç°æœ‰æ–¹æ³•åˆ†ä¸ºåµŒå…¥ç©ºé—´ä¸­çš„è¿ç»­æ‰©æ•£å’Œæ ‡è®°ä¸Šçš„ç¦»æ•£æ‰©æ•£ï¼Œå¹¶æŒ‡å‡ºæ¯ç§æ–¹æ³•ä»…æ»¡è¶³äº”ä¸ªåŸºæœ¬ç‰¹æ€§ä¸­çš„ä¸€éƒ¨åˆ†ï¼Œå› æ­¤åæ˜ å‡ºç»“æ„ä¸Šçš„æƒè¡¡ã€‚é€šè¿‡å¯¹æœ€è¿‘å¤§å‹æ‰©æ•£è¯­è¨€æ¨¡å‹çš„åˆ†æï¼Œæˆ‘ä»¬è¯†åˆ«å‡ºä¸¤ä¸ªæ ¸å¿ƒé—®é¢˜ï¼šå‡åŒ€è…èš€ä¸å°Šé‡ä¿¡æ¯åœ¨ä½ç½®ä¸Šçš„åˆ†å¸ƒï¼Œä»¥åŠé€æ ‡è®°è¾¹é™…è®­ç»ƒæ— æ³•åœ¨å¹¶è¡Œè§£ç ä¸­æ•æ‰å¤šæ ‡è®°ä¾èµ–å…³ç³»ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.24766', 'title': 'Dream2Flow: Bridging Video Generation and Open-World Manipulation with 3D Object Flow', 'url': 'https://huggingface.co/papers/2512.24766', 'abstract': 'Generative video modeling has emerged as a compelling tool to zero-shot reason about plausible physical interactions for open-world manipulation. Yet, it remains a challenge to translate such human-led motions into the low-level actions demanded by robotic systems. We observe that given an initial image and task instruction, these models excel at synthesizing sensible object motions. Thus, we introduce Dream2Flow, a framework that bridges video generation and robotic control through 3D object flow as an intermediate representation. Our method reconstructs 3D object motions from generated videos and formulates manipulation as object trajectory tracking. By separating the state changes from the actuators that realize those changes, Dream2Flow overcomes the embodiment gap and enables zero-shot guidance from pre-trained video models to manipulate objects of diverse categories-including rigid, articulated, deformable, and granular. Through trajectory optimization or reinforcement learning, Dream2Flow converts reconstructed 3D object flow into executable low-level commands without task-specific demonstrations. Simulation and real-world experiments highlight 3D object flow as a general and scalable interface for adapting video generation models to open-world robotic manipulation. Videos and visualizations are available at https://dream2flow.github.io/.', 'score': 6, 'issue_id': 387, 'pub_date': '2026-12-31', 'pub_date_card': {'ru': '31 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 31', 'zh': '12æœˆ31æ—¥'}, 'hash': '7ee4e8c52d41ebf1', 'authors': ['Karthik Dharmarajan', 'Wenlong Huang', 'Jiajun Wu', 'Li Fei-Fei', 'Ruohan Zhang'], 'affiliations': ['Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2512.24766.jpg', 'data': {'categories': ['#robotics', '#rl', '#video', '#3d', '#open_source', '#reasoning'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞÑ‚ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğº Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞµ Ñ‡ĞµÑ€ĞµĞ· 3D-Ğ¿Ğ¾Ñ‚Ğ¾Ğº Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ²', 'desc': 'Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Dream2Flow â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ¼-Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ¾Ğ¼ Ğ±ĞµĞ· Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ° Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² ĞºĞ°Ğº Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ ÑĞ²ÑĞ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ½Ğ¸Ğ·ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ°Ğ¼Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ¼. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ 3D-Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸Ğ· ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ñ ĞºĞ°Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ ÑÑ‚Ğ¸Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹. Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ñ€Ñ„Ğ¾Ğ»Ğ¾Ğ³Ğ¸ĞµĞ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² â€” Ğ¶Ñ‘ÑÑ‚ĞºĞ¸Ğ¼Ğ¸, ÑĞ¾Ñ‡Ğ»ĞµĞ½Ñ‘Ğ½Ğ½Ñ‹Ğ¼Ğ¸, Ğ´ĞµÑ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ¸ ÑÑ‹Ğ¿ÑƒÑ‡Ğ¸Ğ¼Ğ¸ â€” Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ¸Ğ»Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼.'}, 'en': {'title': 'Bridging Video Generation and Robotic Control with Dream2Flow', 'desc': 'This paper presents Dream2Flow, a novel framework that connects generative video modeling with robotic control. It focuses on translating high-level human motions into low-level robotic actions by using 3D object flow as a key representation. The framework allows robots to manipulate various object types by tracking their trajectories, effectively bridging the gap between video generation and physical manipulation. Dream2Flow enables zero-shot learning, meaning it can adapt to new tasks without needing specific training for each one, showcasing its versatility in real-world applications.'}, 'zh': {'title': 'Dream2Flowï¼šè¿æ¥è§†é¢‘ç”Ÿæˆä¸æœºå™¨äººæ§åˆ¶çš„æ¡¥æ¢', 'desc': 'ç”Ÿæˆè§†é¢‘å»ºæ¨¡å·²æˆä¸ºä¸€ç§æœ‰æ•ˆå·¥å…·ï¼Œå¯ä»¥åœ¨å¼€æ”¾ä¸–ç•Œä¸­è¿›è¡Œåˆç†çš„ç‰©ç†äº¤äº’æ¨ç†ã€‚ç„¶è€Œï¼Œå°†äººç±»çš„åŠ¨ä½œè½¬åŒ–ä¸ºæœºå™¨äººç³»ç»Ÿæ‰€éœ€çš„ä½çº§åŠ¨ä½œä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†Dream2Flowæ¡†æ¶ï¼Œé€šè¿‡3Dç‰©ä½“æµä½œä¸ºä¸­é—´è¡¨ç¤ºï¼Œè¿æ¥è§†é¢‘ç”Ÿæˆå’Œæœºå™¨äººæ§åˆ¶ã€‚è¯¥æ–¹æ³•ä»ç”Ÿæˆçš„è§†é¢‘ä¸­é‡å»º3Dç‰©ä½“è¿åŠ¨ï¼Œå¹¶å°†æ“ä½œå½¢å¼åŒ–ä¸ºç‰©ä½“è½¨è¿¹è·Ÿè¸ªï¼Œä»è€Œå®ç°äº†é›¶æ ·æœ¬æŒ‡å¯¼ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.24724', 'title': 'FlowBlending: Stage-Aware Multi-Model Sampling for Fast and High-Fidelity Video Generation', 'url': 'https://huggingface.co/papers/2512.24724', 'abstract': 'In this work, we show that the impact of model capacity varies across timesteps: it is crucial for the early and late stages but largely negligible during the intermediate stage. Accordingly, we propose FlowBlending, a stage-aware multi-model sampling strategy that employs a large model and a small model at capacity-sensitive stages and intermediate stages, respectively. We further introduce simple criteria to choose stage boundaries and provide a velocity-divergence analysis as an effective proxy for identifying capacity-sensitive regions. Across LTX-Video (2B/13B) and WAN 2.1 (1.3B/14B), FlowBlending achieves up to 1.65x faster inference with 57.35% fewer FLOPs, while maintaining the visual fidelity, temporal coherence, and semantic alignment of the large models. FlowBlending is also compatible with existing sampling-acceleration techniques, enabling up to 2x additional speedup. Project page is available at: https://jibin86.github.io/flowblending_project_page.', 'score': 4, 'issue_id': 387, 'pub_date': '2026-12-31', 'pub_date_card': {'ru': '31 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 31', 'zh': '12æœˆ31æ—¥'}, 'hash': '9fe956fc654521e6', 'authors': ['Jibin Song', 'Mingi Kwon', 'Jaeseok Jeong', 'Youngjung Uh'], 'affiliations': ['Yonsei University'], 'pdf_title_img': 'assets/pdf/title_img/2512.24724.jpg', 'data': {'categories': ['#video', '#small_models', '#inference'], 'emoji': 'âš¡', 'ru': {'title': 'ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ¼ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ¼Ğ¾Ñ‰Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑÑ‚Ğ°Ğ¿Ñ‹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ğ° Ğ½Ğ° Ñ€Ğ°Ğ½Ğ½Ğ¸Ñ… Ğ¸ Ğ¿Ğ¾Ğ·Ğ´Ğ½Ğ¸Ñ… ÑÑ‚Ğ°Ğ´Ğ¸ÑÑ…, Ğ° Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ğ¿Ğ°Ñ… Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. ĞĞ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ FlowBlending â€” ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞµĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ¸ Ğ¼Ğ°Ğ»ĞµĞ½ÑŒĞºĞ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑÑ‚Ğ°Ğ¿Ğ° Ğº ĞµĞ¼ĞºĞ¾ÑÑ‚Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ´Ğ¾ 1.65x Ğ¸ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ½Ğ° 57% Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'Optimizing Model Capacity for Efficient Inference with FlowBlending', 'desc': 'This paper investigates how the capacity of machine learning models affects their performance at different stages of processing. It finds that model capacity is important at the beginning and end of the process, but not as much in the middle. To address this, the authors introduce FlowBlending, a method that uses a large model for critical stages and a smaller model for less critical ones. This approach significantly speeds up inference and reduces computational load while preserving the quality of the output.'}, 'zh': {'title': 'é˜¶æ®µæ„ŸçŸ¥çš„æ¨¡å‹é‡‡æ ·ç­–ç•¥ï¼šFlowBlending', 'desc': 'æœ¬ç ”ç©¶è¡¨æ˜ï¼Œæ¨¡å‹å®¹é‡åœ¨ä¸åŒæ—¶é—´æ­¥çš„å½±å“æ˜¯ä¸åŒçš„ï¼šåœ¨æ—©æœŸå’Œæ™šæœŸé˜¶æ®µè‡³å…³é‡è¦ï¼Œè€Œåœ¨ä¸­é—´é˜¶æ®µåˆ™å‡ ä¹å¯ä»¥å¿½ç•¥ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºFlowBlendingçš„é˜¶æ®µæ„ŸçŸ¥å¤šæ¨¡å‹é‡‡æ ·ç­–ç•¥ï¼Œåœ¨å¯¹å®¹é‡æ•æ„Ÿçš„é˜¶æ®µä½¿ç”¨å¤§æ¨¡å‹ï¼Œè€Œåœ¨ä¸­é—´é˜¶æ®µä½¿ç”¨å°æ¨¡å‹ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ç®€å•çš„æ ‡å‡†æ¥é€‰æ‹©é˜¶æ®µè¾¹ç•Œï¼Œå¹¶æä¾›äº†é€Ÿåº¦-å‘æ•£åˆ†æä½œä¸ºè¯†åˆ«å®¹é‡æ•æ„ŸåŒºåŸŸçš„æœ‰æ•ˆä»£ç†ã€‚é€šè¿‡åœ¨LTX-Videoå’ŒWAN 2.1ä¸Šçš„å®éªŒï¼ŒFlowBlendingå®ç°äº†é«˜è¾¾1.65å€çš„æ¨ç†é€Ÿåº¦æå‡ï¼ŒåŒæ—¶å‡å°‘äº†57.35%çš„FLOPsï¼Œä¿æŒäº†å¤§æ¨¡å‹çš„è§†è§‰ä¿çœŸåº¦ã€æ—¶é—´ä¸€è‡´æ€§å’Œè¯­ä¹‰å¯¹é½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.24007', 'title': 'TESO Tabu Enhanced Simulation Optimization for Noisy Black Box Problems', 'url': 'https://huggingface.co/papers/2512.24007', 'abstract': "Simulation optimization (SO) is frequently challenged by noisy evaluations, high computational costs, and complex, multimodal search landscapes. This paper introduces Tabu-Enhanced Simulation Optimization (TESO), a novel metaheuristic framework integrating adaptive search with memory-based strategies. TESO leverages a short-term Tabu List to prevent cycling and encourage diversification, and a long-term Elite Memory to guide intensification by perturbing high-performing solutions. An aspiration criterion allows overriding tabu restrictions for exceptional candidates. This combination facilitates a dynamic balance between exploration and exploitation in stochastic environments. We demonstrate TESO's effectiveness and reliability using an queue optimization problem, showing improved performance compared to benchmarks and validating the contribution of its memory components. Source code and data are available at: https://github.com/bulentsoykan/TESO.", 'score': 2, 'issue_id': 388, 'pub_date': '2026-12-30', 'pub_date_card': {'ru': '30 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 30', 'zh': '12æœˆ30æ—¥'}, 'hash': 'cd7eb2b7c3a9a044', 'authors': ['Bulent Soykan', 'Sean Mondesire', 'Ghaith Rabadi'], 'affiliations': ['University of Central Florida'], 'pdf_title_img': 'assets/pdf/title_img/2512.24007.jpg', 'data': {'categories': ['#optimization', '#open_source'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞŸĞ°Ğ¼ÑÑ‚ÑŒ Ğ¸ Ñ‚Ğ°Ğ±Ñƒ Ğ´Ğ»Ñ ÑƒĞ¼Ğ½Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ² ÑˆÑƒĞ¼Ğ½Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ…', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ TESO â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ°ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ÑÑ Ñ ÑˆÑƒĞ¼Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼Ğ¸ Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ»Ğ°Ğ½Ğ´ÑˆĞ°Ñ„Ñ‚Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ°. ĞĞ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ĞºÑ€Ğ°Ñ‚ĞºĞ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Tabu List Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ Ñ†Ğ¸ĞºĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ´Ğ¸Ğ²ĞµÑ€ÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½ÑƒÑ Elite Memory Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµĞ½ÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ±Ğ°Ğ·Ğµ Ğ²Ğ¾Ğ·Ğ¼ÑƒÑ‰ĞµĞ½Ğ¸Ğ¹ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. ĞšÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸Ğ¹ Ğ°ÑĞ¿Ğ¸Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑ‚ÑŒ tabu-Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¸ÑĞºĞ»ÑÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ ÑĞºÑĞ¿Ğ»ÑƒĞ°Ñ‚Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ² ÑÑ‚Ğ¾Ñ…Ğ°ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸ÑÑ…. Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ TESO Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ‡ĞµÑ€ĞµĞ´ĞµĞ¹, Ğ³Ğ´Ğµ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ» ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Balancing Exploration and Exploitation in Simulation Optimization with TESO', 'desc': 'This paper presents a new method called Tabu-Enhanced Simulation Optimization (TESO) to tackle challenges in simulation optimization, such as noise and high computational costs. TESO combines adaptive search techniques with memory strategies, using a short-term Tabu List to avoid revisiting previous solutions and a long-term Elite Memory to focus on improving the best solutions found. An aspiration criterion is included to allow for flexibility in exploring exceptional solutions despite tabu restrictions. The results show that TESO outperforms existing methods in a queue optimization problem, highlighting the effectiveness of its memory components.'}, 'zh': {'title': 'ç¦å¿Œå¢å¼ºæ¨¡æ‹Ÿä¼˜åŒ–ï¼šæ¢ç´¢ä¸å¼€å‘çš„åŠ¨æ€å¹³è¡¡', 'desc': 'æ¨¡æ‹Ÿä¼˜åŒ–ï¼ˆSOï¼‰å¸¸å¸¸é¢ä¸´å™ªå£°è¯„ä¼°ã€é«˜è®¡ç®—æˆæœ¬å’Œå¤æ‚çš„å¤šæ¨¡æ€æœç´¢ç¯å¢ƒçš„æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å…ƒå¯å‘å¼æ¡†æ¶â€”â€”ç¦å¿Œå¢å¼ºæ¨¡æ‹Ÿä¼˜åŒ–ï¼ˆTESOï¼‰ï¼Œå®ƒå°†è‡ªé€‚åº”æœç´¢ä¸åŸºäºè®°å¿†çš„ç­–ç•¥ç›¸ç»“åˆã€‚TESOåˆ©ç”¨çŸ­æœŸç¦å¿Œåˆ—è¡¨æ¥é˜²æ­¢å¾ªç¯å¹¶é¼“åŠ±å¤šæ ·åŒ–ï¼ŒåŒæ—¶ä½¿ç”¨é•¿æœŸç²¾è‹±è®°å¿†æ¥é€šè¿‡æ‰°åŠ¨é«˜æ€§èƒ½è§£æ¥æŒ‡å¯¼å¼ºåŒ–ã€‚é€šè¿‡åœ¨éšæœºç¯å¢ƒä¸­åŠ¨æ€å¹³è¡¡æ¢ç´¢ä¸å¼€å‘ï¼ŒTESOåœ¨é˜Ÿåˆ—ä¼˜åŒ–é—®é¢˜ä¸Šå±•ç¤ºäº†å…¶æœ‰æ•ˆæ€§å’Œå¯é æ€§ï¼Œä¸”æ€§èƒ½ä¼˜äºåŸºå‡†ï¼ŒéªŒè¯äº†å…¶è®°å¿†ç»„ä»¶çš„è´¡çŒ®ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.11522', 'title': 'UniX: Unifying Autoregression and Diffusion for Chest X-Ray Understanding and Generation', 'url': 'https://huggingface.co/papers/2601.11522', 'abstract': 'UniX presents a unified medical foundation model that decouples visual understanding and generation tasks using distinct autoregressive and diffusion branches with cross-modal attention for enhanced performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite recent progress, medical foundation models still struggle to unify visual understanding and generation, as these tasks have inherently conflicting goals: semantic abstraction versus pixel-level reconstruction. Existing approaches, typically based on parameter-shared autoregressive architectures, frequently lead to compromised performance in one or both tasks. To address this, we present UniX, a next-generation unified medical foundation model for chest X-ray understanding and generation. UniX decouples the two tasks into an autoregressive branch for understanding and a diffusion branch for high-fidelity generation. Crucially, a cross-modal self-attention mechanism is introduced to dynamically guide the generation process with understanding features. Coupled with a rigorous data cleaning pipeline and a multi-stage training strategy, this architecture enables synergistic collaboration between tasks while leveraging the strengths of diffusion models for superior generation. On two representative benchmarks, UniX achieves a 46.1% improvement in understanding performance (Micro-F1) and a 24.2% gain in generation quality (FD-RadDino), using only a quarter of the parameters of LLM-CXR. By achieving performance on par with task-specific models, our work establishes a scalable paradigm for synergistic medical image understanding and generation. Codes and models are available at https://github.com/ZrH42/UniX.', 'score': 14, 'issue_id': 683, 'pub_date': '2026-01-16', 'pub_date_card': {'ru': '16 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 16', 'zh': '1æœˆ16æ—¥'}, 'hash': '2e8309152ea38796', 'authors': ['Ruiheng Zhang', 'Jingfeng Yao', 'Huangxuan Zhao', 'Hao Yan', 'Xiao He', 'Lei Chen', 'Zhou Wei', 'Yong Luo', 'Zengmao Wang', 'Lefei Zhang', 'Dacheng Tao', 'Bo Du'], 'affiliations': ['Huazhong University of Science and Technology', 'Nanyang Technological University', 'Wuhan University'], 'pdf_title_img': 'assets/pdf/title_img/2601.11522.jpg', 'data': {'categories': ['#diffusion', '#training', '#data', '#architecture', '#open_source', '#multimodal', '#healthcare'], 'emoji': 'ğŸ«€', 'ru': {'title': 'Ğ¡Ğ¸Ğ½ĞµÑ€Ğ³Ğ¸Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ: Ñ€Ğ°Ğ·Ğ´ĞµĞ»Ñ‘Ğ½Ğ½Ñ‹Ğµ Ğ²ĞµÑ‚Ğ²Ğ¸ Ğ´Ğ»Ñ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°', 'desc': 'UniX â€” ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ-Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ´Ğ²Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²ĞµÑ‚Ğ²Ğ»ĞµĞ½Ğ¸Ñ: Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğµ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸ĞµĞ¹ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ°Ğ¼Ğ¾Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸, Ğ¸Ğ·Ğ²Ğ»Ñ‘Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ñ€Ğ¾Ğ³Ğ¸Ğ¹ pipeline Ğ¾Ñ‡Ğ¸ÑÑ‚ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ ÑĞ¸Ğ½ĞµÑ€Ğ³Ğ¸Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ´Ğ²ÑƒĞ¼Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸, Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ² ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ†Ğ¸ĞµĞ¹ Ğ¸ Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ĞµĞ¹. UniX Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾: ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ° 46,1% Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ¸ Ğ½Ğ° 24,2% Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ² Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ Ñ€Ğ°Ğ·Ğ° Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ñ‡ĞµĞ¼ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸.'}, 'en': {'title': 'UniX: Bridging Understanding and Generation in Medical Imaging', 'desc': 'UniX is a new medical foundation model designed to improve how machines understand and generate medical images, specifically chest X-rays. It separates the tasks of visual understanding and image generation into two different branches: one that uses autoregressive methods for understanding and another that employs diffusion techniques for generating high-quality images. This model introduces a cross-modal self-attention mechanism that helps the generation process by incorporating features from the understanding branch, enhancing overall performance. As a result, UniX shows significant improvements in both understanding and generation tasks while using fewer parameters than previous models, making it a scalable solution for medical image analysis.'}, 'zh': {'title': 'UniXï¼šåŒ»å­¦å›¾åƒç†è§£ä¸ç”Ÿæˆçš„ååŒæ–°èŒƒå¼', 'desc': 'UniXæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„åŒ»å­¦åŸºç¡€æ¨¡å‹ï¼Œå®ƒå°†è§†è§‰ç†è§£å’Œç”Ÿæˆä»»åŠ¡åˆ†å¼€å¤„ç†ï¼Œä½¿ç”¨ä¸åŒçš„è‡ªå›å½’å’Œæ‰©æ•£åˆ†æ”¯ï¼Œå¹¶ç»“åˆè·¨æ¨¡æ€æ³¨æ„åŠ›æœºåˆ¶ï¼Œä»¥æé«˜æ€§èƒ½ã€‚ä¼ ç»Ÿçš„åŒ»å­¦æ¨¡å‹åœ¨è¿™ä¸¤é¡¹ä»»åŠ¡ä¸Šå¾€å¾€è¡¨ç°ä¸ä½³ï¼Œå› ä¸ºå®ƒä»¬çš„ç›®æ ‡ç›¸äº’çŸ›ç›¾ï¼šè¯­ä¹‰æŠ½è±¡ä¸åƒç´ çº§é‡å»ºã€‚UniXé€šè¿‡è‡ªå›å½’åˆ†æ”¯è¿›è¡Œç†è§£ï¼Œé€šè¿‡æ‰©æ•£åˆ†æ”¯è¿›è¡Œé«˜ä¿çœŸç”Ÿæˆï¼Œè§£å†³äº†è¿™ä¸€é—®é¢˜ã€‚ç»è¿‡ä¸¥æ ¼çš„æ•°æ®æ¸…ç†å’Œå¤šé˜¶æ®µè®­ç»ƒç­–ç•¥ï¼ŒUniXåœ¨ç†è§£æ€§èƒ½å’Œç”Ÿæˆè´¨é‡ä¸Šå‡æœ‰æ˜¾è‘—æå‡ï¼Œå±•ç¤ºäº†åŒ»å­¦å›¾åƒç†è§£ä¸ç”Ÿæˆçš„ååŒå·¥ä½œæ–°èŒƒå¼ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.11655', 'title': 'Advances and Frontiers of LLM-based Issue Resolution in Software Engineering: A Comprehensive Survey', 'url': 'https://huggingface.co/papers/2601.11655', 'abstract': 'Large language models face significant challenges in software issue resolution, prompting the development of autonomous coding agents through various training-free and training-based methodologies.  \t\t\t\t\tAI-generated summary \t\t\t\t Issue resolution, a complex Software Engineering (SWE) task integral to real-world development, has emerged as a compelling challenge for artificial intelligence. The establishment of benchmarks like SWE-bench revealed this task as profoundly difficult for large language models, thereby significantly accelerating the evolution of autonomous coding agents. This paper presents a systematic survey of this emerging domain. We begin by examining data construction pipelines, covering automated collection and synthesis approaches. We then provide a comprehensive analysis of methodologies, spanning training-free frameworks with their modular components to training-based techniques, including supervised fine-tuning and reinforcement learning. Subsequently, we discuss critical analyses of data quality and agent behavior, alongside practical applications. Finally, we identify key challenges and outline promising directions for future research. An open-source repository is maintained at https://github.com/DeepSoftwareAnalytics/Awesome-Issue-Resolution to serve as a dynamic resource in this field.', 'score': 13, 'issue_id': 683, 'pub_date': '2026-01-15', 'pub_date_card': {'ru': '15 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 15', 'zh': '1æœˆ15æ—¥'}, 'hash': '3bba28a3c4ff036b', 'authors': ['Caihua Li', 'Lianghong Guo', 'Yanlin Wang', 'Daya Guo', 'Wei Tao', 'Zhenyu Shan', 'Mingwei Liu', 'Jiachi Chen', 'Haoyu Song', 'Duyu Tang', 'Hongyu Zhang', 'Zibin Zheng'], 'affiliations': ['Chongqing University', 'Hangzhou Normal University', 'Huawei Technologies Co, Ltd', 'Independent Researcher', 'Sun Yat-sen University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2601.11655.jpg', 'data': {'categories': ['#rl', '#benchmark', '#dataset', '#agents', '#data', '#training', '#open_source', '#plp', '#survey'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞĞ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ñ‹Ñ… Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ÑÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ñ‹ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ÑĞ±Ğ¾Ñ€ Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ·, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¾Ñ‚ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ´Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ. Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ²Ñ‹ÑĞ²Ğ»ÑÑÑ‚ÑÑ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ñ‹ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ÑÑ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº ĞºĞ¾Ğ´Ğ°.'}, 'en': {'title': 'Empowering Autonomous Coding Agents for Software Issue Resolution', 'desc': 'This paper explores the challenges that large language models face in resolving software issues, which is a crucial task in software engineering. It highlights the development of autonomous coding agents through both training-free and training-based methods. The authors provide a systematic survey of data construction techniques and analyze various methodologies, including supervised fine-tuning and reinforcement learning. Additionally, the paper discusses data quality, agent behavior, and future research directions, while maintaining an open-source repository for ongoing contributions in this area.'}, 'zh': {'title': 'è‡ªä¸»ç¼–ç ä»£ç†ï¼šè§£å†³è½¯ä»¶é—®é¢˜çš„æ–°æ–¹å‘', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è½¯ä»¶é—®é¢˜è§£å†³æ–¹é¢é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼Œå› æ­¤å¼€å‘äº†è‡ªä¸»ç¼–ç ä»£ç†ï¼Œé‡‡ç”¨äº†å¤šç§æ— è®­ç»ƒå’ŒåŸºäºè®­ç»ƒçš„æ–¹æ³•ã€‚è½¯ä»¶é—®é¢˜è§£å†³æ˜¯ä¸€ä¸ªå¤æ‚çš„è½¯ä»¶å·¥ç¨‹ä»»åŠ¡ï¼Œå¯¹äººå·¥æ™ºèƒ½æ¥è¯´æ˜¯ä¸€ä¸ªé‡è¦çš„æŒ‘æˆ˜ã€‚æœ¬æ–‡ç³»ç»Ÿæ€§åœ°è°ƒæŸ¥äº†è¿™ä¸€æ–°å…´é¢†åŸŸï¼Œåˆ†æäº†æ•°æ®æ„å»ºæµç¨‹å’Œå„ç§æ–¹æ³•è®ºï¼ŒåŒ…æ‹¬æ— è®­ç»ƒæ¡†æ¶å’ŒåŸºäºè®­ç»ƒçš„æŠ€æœ¯ã€‚æœ€åï¼Œè®¨è®ºäº†æ•°æ®è´¨é‡ã€ä»£ç†è¡Œä¸ºçš„å…³é”®åˆ†æä»¥åŠå®é™…åº”ç”¨ï¼Œå¹¶æŒ‡å‡ºäº†æœªæ¥ç ”ç©¶çš„ä¸»è¦æŒ‘æˆ˜å’Œæ–¹å‘ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.12993', 'title': 'Being-H0.5: Scaling Human-Centric Robot Learning for Cross-Embodiment Generalization', 'url': 'https://huggingface.co/papers/2601.12993', 'abstract': 'Being-H0.5 is a Vision-Language-Action model that enables robust cross-embodiment generalization through human-centric learning and a Mixture-of-Transformers architecture with specialized embodiment handling.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Being-H0.5, a foundational Vision-Language-Action (VLA) model designed for robust cross-embodiment generalization across diverse robotic platforms. While existing VLAs often struggle with morphological heterogeneity and data scarcity, we propose a human-centric learning paradigm that treats human interaction traces as a universal "mother tongue" for physical interaction. To support this, we present UniHand-2.0, the largest embodied pre-training recipe to date, comprising over 35,000 hours of multimodal data across 30 distinct robotic embodiments. Our approach introduces a Unified Action Space that maps heterogeneous robot controls into semantically aligned slots, enabling low-resource robots to bootstrap skills from human data and high-resource platforms. Built upon this human-centric foundation, we design a unified sequential modeling and multi-task pre-training paradigm to bridge human demonstrations and robotic execution. Architecturally, Being-H0.5 utilizes a Mixture-of-Transformers design featuring a novel Mixture-of-Flow (MoF) framework to decouple shared motor primitives from specialized embodiment-specific experts. Finally, to make cross-embodiment policies stable in the real world, we introduce Manifold-Preserving Gating for robustness under sensory shift and Universal Async Chunking to universalize chunked control across embodiments with different latency and control profiles. We empirically demonstrate that Being-H0.5 achieves state-of-the-art results on simulated benchmarks, such as LIBERO (98.9%) and RoboCasa (53.9%), while also exhibiting strong cross-embodiment capabilities on five robotic platforms.', 'score': 12, 'issue_id': 684, 'pub_date': '2026-01-19', 'pub_date_card': {'ru': '19 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 19', 'zh': '1æœˆ19æ—¥'}, 'hash': 'ccaa8b7bf86cfa7f', 'authors': ['Hao Luo', 'Ye Wang', 'Wanpeng Zhang', 'Sipeng Zheng', 'Ziheng Xi', 'Chaoyi Xu', 'Haiweng Xu', 'Haoqi Yuan', 'Chi Zhang', 'Yiqing Wang', 'Yicheng Feng', 'Zongqing Lu'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2601.12993.jpg', 'data': {'categories': ['#architecture', '#dataset', '#training', '#multimodal', '#robotics'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ§ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¹ ÑĞ·Ñ‹Ğº Ğ´Ğ»Ñ Ğ²ÑĞµÑ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ²: universal VLA Ñ ĞºÑ€Ğ¾ÑÑĞ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'Being-H0.5 â€” ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Vision-Language-Action (VLA), Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ¼Ğ¸ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾-Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡Ğ½Ğ¾Ğ¼Ñƒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñƒ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ°Ğº ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞ·Ñ‹Ğº Ğ´Ğ»Ñ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ UniHand-2.0 â€” ĞºÑ€ÑƒĞ¿Ğ½ĞµĞ¹ÑˆĞ¸Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸Ğ· 35 Ñ‚Ñ‹ÑÑÑ‡ Ñ‡Ğ°ÑĞ¾Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ 30 Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°. ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Mixture-of-Transformers Ñ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Mixture-of-Flow (MoF) Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ¾Ğ±Ñ‰Ğ¸Ğµ Ğ¼Ğ¾Ñ‚Ğ¾Ñ€Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ğ²Ñ‹ Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ñ‚Ğ¸Ğ¿Ğ° Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… LIBERO (98.9%) Ğ¸ RoboCasa (53.9%), Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¿ÑÑ‚Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ….'}, 'en': {'title': 'Empowering Robots with Human-Centric Learning for Cross-Embodiment Mastery', 'desc': 'Being-H0.5 is a Vision-Language-Action model that enhances the ability of robots to generalize across different physical forms. It employs a human-centric learning approach, using human interaction data as a foundational guide for robotic actions. The model is built on a Mixture-of-Transformers architecture, which allows it to effectively manage various robotic embodiments and their unique control requirements. With its innovative techniques like Manifold-Preserving Gating and Universal Async Chunking, Being-H0.5 demonstrates impressive performance on multiple robotic platforms, achieving state-of-the-art results in simulated environments.'}, 'zh': {'title': 'è·¨ä½“ç°æ³›åŒ–çš„å¼ºå¤§æ¨¡å‹', 'desc': 'Being-H0.5æ˜¯ä¸€ç§è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼Œæ—¨åœ¨é€šè¿‡ä»¥äººä¸ºä¸­å¿ƒçš„å­¦ä¹ å’Œæ··åˆå˜æ¢å™¨æ¶æ„å®ç°å¼ºå¤§çš„è·¨ä½“ç°æ³›åŒ–èƒ½åŠ›ã€‚è¯¥æ¨¡å‹åˆ©ç”¨äººç±»äº¤äº’è½¨è¿¹ä½œä¸ºç‰©ç†äº¤äº’çš„é€šç”¨â€œæ¯è¯­â€ï¼Œè§£å†³äº†ç°æœ‰æ¨¡å‹åœ¨å½¢æ€å¼‚è´¨æ€§å’Œæ•°æ®ç¨€ç¼ºæ–¹é¢çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºçš„UniHand-2.0æ˜¯è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„å…·èº«é¢„è®­ç»ƒæ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡35,000å°æ—¶çš„å¤šæ¨¡æ€æ•°æ®ï¼Œæ¶µç›–30ç§ä¸åŒçš„æœºå™¨äººä½“ç°ã€‚é€šè¿‡ç»Ÿä¸€çš„åŠ¨ä½œç©ºé—´å’Œå¤šä»»åŠ¡é¢„è®­ç»ƒèŒƒå¼ï¼ŒBeing-H0.5èƒ½å¤Ÿåœ¨ä¸åŒçš„æœºå™¨äººå¹³å°ä¸Šå®ç°ç¨³å®šçš„è·¨ä½“ç°ç­–ç•¥ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.12294', 'title': 'ToolPRMBench: Evaluating and Advancing Process Reward Models for Tool-using Agents', 'url': 'https://huggingface.co/papers/2601.12294', 'abstract': 'ToolPRMBench is introduced as a large-scale benchmark for evaluating process reward models in tool-using agents, featuring step-level test cases and multi-LLM verification to ensure data quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Reward-guided search methods have demonstrated strong potential in enhancing tool-using agents by effectively guiding sampling and exploration over complex action spaces. As a core design, those search methods utilize process reward models (PRMs) to provide step-level rewards, enabling more fine-grained monitoring. However, there is a lack of systematic and reliable evaluation benchmarks for PRMs in tool-using settings. In this paper, we introduce ToolPRMBench, a large-scale benchmark specifically designed to evaluate PRMs for tool-using agents. ToolPRMBench is built on top of several representative tool-using benchmarks and converts agent trajectories into step-level test cases. Each case contains the interaction history, a correct action, a plausible but incorrect alternative, and relevant tool metadata. We respectively utilize offline sampling to isolate local single-step errors and online sampling to capture realistic multi-step failures from full agent rollouts. A multi-LLM verification pipeline is proposed to reduce label noise and ensure data quality. We conduct extensive experiments across large language models, general PRMs, and tool-specialized PRMs on ToolPRMBench. The results reveal clear differences in PRM effectiveness and highlight the potential of specialized PRMs for tool-using. Code and data will be released at https://github.com/David-Li0406/ToolPRMBench.', 'score': 11, 'issue_id': 684, 'pub_date': '2026-01-18', 'pub_date_card': {'ru': '18 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 18', 'zh': '1æœˆ18æ—¥'}, 'hash': '4ace6c0ff6a7867b', 'authors': ['Dawei Li', 'Yuguang Yao', 'Zhen Tan', 'Huan Liu', 'Ruocheng Guo'], 'affiliations': ['Arizona State University', 'Intuit AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2601.12294.jpg', 'data': {'categories': ['#agents', '#benchmark', '#dataset', '#rl'], 'emoji': 'ğŸ”§', 'ru': {'title': 'Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ¾Ğ¼ Ğº Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ToolPRMBench â€” Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ°Ñ Ğ±Ğ°Ğ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑˆĞ°Ğ³Ğ¾Ğ² (process reward models) Ğ² Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ñ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹. Ğ”Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğµ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ğµ ÑĞ»ÑƒÑ‡Ğ°Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ² Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ĞºĞ°Ğº Ğ¾Ñ„Ğ»Ğ°Ğ¹Ğ½-ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº, Ñ‚Ğ°Ğº Ğ¸ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¸Ğ· Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ² ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'ToolPRMBench: Elevating Evaluation of Process Reward Models for Tool-Using Agents', 'desc': 'This paper presents ToolPRMBench, a comprehensive benchmark designed to evaluate process reward models (PRMs) specifically for tool-using agents. It addresses the need for systematic evaluation by converting agent trajectories into detailed step-level test cases, which include correct and plausible incorrect actions. The benchmark employs both offline and online sampling methods to identify single-step and multi-step errors, ensuring a thorough assessment of PRM performance. The findings demonstrate significant variations in the effectiveness of different PRMs, emphasizing the advantages of specialized models for enhancing tool-using capabilities.'}, 'zh': {'title': 'ToolPRMBenchï¼šè¯„ä¼°å·¥å…·ä½¿ç”¨ä»£ç†çš„è¿‡ç¨‹å¥–åŠ±æ¨¡å‹æ–°åŸºå‡†', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ToolPRMBenchï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°å·¥å…·ä½¿ç”¨ä»£ç†çš„è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰çš„è§„æ¨¡åŒ–åŸºå‡†ã€‚è¯¥åŸºå‡†é€šè¿‡å°†ä»£ç†è½¨è¿¹è½¬æ¢ä¸ºé€æ­¥æµ‹è¯•æ¡ˆä¾‹ï¼Œæä¾›äº†æ›´ç»†è‡´çš„ç›‘æ§å’Œè¯„ä¼°ã€‚æˆ‘ä»¬é‡‡ç”¨ç¦»çº¿é‡‡æ ·å’Œåœ¨çº¿é‡‡æ ·çš„æ–¹æ³•ï¼Œåˆ†åˆ«æ•æ‰å±€éƒ¨å•æ­¥é”™è¯¯å’ŒçœŸå®çš„å¤šæ­¥å¤±è´¥ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œä¸“é—¨åŒ–çš„PRMåœ¨å·¥å…·ä½¿ç”¨ä¸­çš„æœ‰æ•ˆæ€§æ˜æ˜¾ä¼˜äºé€šç”¨PRMï¼Œå±•ç¤ºäº†å…¶æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.11969', 'title': 'MemoryRewardBench: Benchmarking Reward Models for Long-Term Memory Management in Large Language Models', 'url': 'https://huggingface.co/papers/2601.11969', 'abstract': "A benchmark called MemoryRewardBench is introduced to systematically evaluate reward models' ability to assess long-term memory management in large language models across various context lengths and memory patterns.  \t\t\t\t\tAI-generated summary \t\t\t\t Existing works increasingly adopt memory-centric mechanisms to process long contexts in a segment manner, and effective memory management is one of the key capabilities that enables large language models to effectively propagate information across the entire sequence. Therefore, leveraging reward models (RMs) to automatically and reliably evaluate memory quality is critical. In this work, we introduce MemoryRewardBench, the first benchmark to systematically study the ability of RMs to evaluate long-term memory management processes. MemoryRewardBench covers both long-context comprehension and long-form generation tasks, featuring 10 distinct settings with different memory management patterns, with context length ranging from 8K to 128K tokens. Evaluations on 13 cutting-edge RMs indicate a diminishing performance gap between open-source and proprietary models, with newer-generation models consistently outperforming their predecessors regardless of parameter count. We further expose the capabilities and fundamental limitations of current RMs in evaluating LLM memory management across diverse settings.", 'score': 10, 'issue_id': 683, 'pub_date': '2026-01-17', 'pub_date_card': {'ru': '17 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 17', 'zh': '1æœˆ17æ—¥'}, 'hash': '3d51cd5cc576b6cc', 'authors': ['Zecheng Tang', 'Baibei Ji', 'Ruoxi Sun', 'Haitian Wang', 'WangJie You', 'Zhang Yijun', 'Wenpeng Zhu', 'Ji Qi', 'Juntao Li', 'Min Zhang'], 'affiliations': ['China Mobile (Suzhou)', 'LCM Laboratory', 'Soochow University'], 'pdf_title_img': 'assets/pdf/title_img/2601.11969.jpg', 'data': {'categories': ['#long_context', '#survey', '#benchmark', '#dataset'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞÑ†ĞµĞ½ĞºĞ° Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸: Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ MemoryRewardBench â€” Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ĞºĞ°Ğº Ğ½Ğ° Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°, Ñ‚Ğ°Ğº Ğ¸ Ğ½Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ², Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ 10 Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¹ Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ°Ğ¼Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¾Ñ‚ 8K Ğ´Ğ¾ 128K Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ»ÑƒÑ‡ÑˆĞµ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¾Ğ¹ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ, Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ÑÑ‚ÑÑ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ ĞºĞ°Ğº Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸, Ñ‚Ğ°Ğº Ğ¸ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ….'}, 'en': {'title': 'Evaluating Memory Management in Language Models with MemoryRewardBench', 'desc': 'The paper introduces MemoryRewardBench, a benchmark designed to evaluate how well reward models (RMs) assess long-term memory management in large language models (LLMs). It focuses on the ability of these models to handle long contexts and manage memory effectively across various tasks. The benchmark includes 10 different settings with context lengths ranging from 8K to 128K tokens, allowing for a comprehensive analysis of memory management patterns. Results show that newer RMs outperform older ones, highlighting both the strengths and limitations of current models in evaluating memory quality.'}, 'zh': {'title': 'è¯„ä¼°é•¿æœŸè®°å¿†ç®¡ç†çš„åˆ›æ–°åŸºå‡†', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºMemoryRewardBenchçš„åŸºå‡†ï¼Œç”¨äºç³»ç»Ÿè¯„ä¼°å¥–åŠ±æ¨¡å‹åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ç®¡ç†é•¿æœŸè®°å¿†çš„èƒ½åŠ›ã€‚éšç€å¯¹è®°å¿†ä¸­å¿ƒæœºåˆ¶çš„é‡‡ç”¨ï¼Œå¦‚ä½•æœ‰æ•ˆç®¡ç†è®°å¿†æˆä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ä¼ æ’­ä¿¡æ¯çš„å…³é”®èƒ½åŠ›ã€‚MemoryRewardBenchæ˜¯ç¬¬ä¸€ä¸ªç³»ç»Ÿç ”ç©¶å¥–åŠ±æ¨¡å‹è¯„ä¼°é•¿æœŸè®°å¿†ç®¡ç†è¿‡ç¨‹çš„åŸºå‡†ï¼Œæ¶µç›–äº†é•¿ä¸Šä¸‹æ–‡ç†è§£å’Œé•¿æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ã€‚é€šè¿‡å¯¹13ä¸ªå‰æ²¿å¥–åŠ±æ¨¡å‹çš„è¯„ä¼°ï¼Œå‘ç°å¼€æºæ¨¡å‹ä¸ä¸“æœ‰æ¨¡å‹ä¹‹é—´çš„æ€§èƒ½å·®è·é€æ¸ç¼©å°ï¼Œæ–°ä¸€ä»£æ¨¡å‹åœ¨å„ä¸ªå‚æ•°æ•°é‡ä¸‹å‡ä¼˜äºå‰ä»£æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.13247', 'title': 'Aligning Agentic World Models via Knowledgeable Experience Learning', 'url': 'https://huggingface.co/papers/2601.13247', 'abstract': 'WorldMind addresses the modal disconnect in LLMs by autonomously building a symbolic world knowledge repository that enhances physical feasibility and task optimality through experience-based learning.  \t\t\t\t\tAI-generated summary \t\t\t\t Current Large Language Models (LLMs) exhibit a critical modal disconnect: they possess vast semantic knowledge but lack the procedural grounding to respect the immutable laws of the physical world. Consequently, while these agents implicitly function as world models, their simulations often suffer from physical hallucinations-generating plans that are logically sound but physically unexecutable. Existing alignment strategies predominantly rely on resource-intensive training or fine-tuning, which attempt to compress dynamic environmental rules into static model parameters. However, such parametric encapsulation is inherently rigid, struggling to adapt to the open-ended variability of physical dynamics without continuous, costly retraining. To bridge this gap, we introduce WorldMind, a framework that autonomously constructs a symbolic World Knowledge Repository by synthesizing environmental feedback. Specifically, it unifies Process Experience to enforce physical feasibility via prediction errors and Goal Experience to guide task optimality through successful trajectories. Experiments on EB-ALFRED and EB-Habitat demonstrate that WorldMind achieves superior performance compared to baselines with remarkable cross-model and cross-environment transferability.', 'score': 9, 'issue_id': 683, 'pub_date': '2026-01-19', 'pub_date_card': {'ru': '19 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 19', 'zh': '1æœˆ19æ—¥'}, 'hash': 'd94ae6ed0aceb888', 'authors': ['Baochang Ren', 'Yunzhi Yao', 'Rui Sun', 'Shuofei Qiao', 'Ningyu Zhang', 'Huajun Chen'], 'affiliations': ['University of California, Los Angeles', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2601.13247.jpg', 'data': {'categories': ['#transfer_learning', '#alignment', '#hallucinations', '#reasoning'], 'emoji': 'ğŸŒ', 'ru': {'title': 'Ğ“Ñ€ounded Ğ¼Ğ¸Ñ€ Ğ´Ğ»Ñ LLM: Ğ¾Ñ‚ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğº Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¸Ğ¼Ñ‹Ğ¼ Ğ¿Ğ»Ğ°Ğ½Ğ°Ğ¼', 'desc': 'WorldMind Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ€Ğ°ÑÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ…Ñ€Ğ°Ğ½Ğ¸Ğ»Ğ¸Ñ‰Ğ° Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¾ Ğ¼Ğ¸Ñ€Ğµ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ´Ğ²Ğ° Ñ‚Ğ¸Ğ¿Ğ° Ğ¾Ğ¿Ñ‹Ñ‚Ğ°: Ğ¾Ğ¿Ñ‹Ñ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¾Ğ¿Ñ‹Ñ‚ Ñ†ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚Ğ¾Ğº Ğ·Ğ°ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ° Ñ„Ğ¸Ğ·Ğ¸ĞºĞ¸ Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ¾Ñ€Ğ¾Ğ³Ğ¾ÑÑ‚Ğ¾ÑÑ‰ĞµĞµ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ, WorldMind Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ Ğ¾Ñ‚ Ğ¾ĞºÑ€ÑƒĞ¶Ğ°ÑÑ‰ĞµĞ¹ ÑÑ€ĞµĞ´Ñ‹ Ğ´Ğ»Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… EB-ALFRED Ğ¸ EB-Habitat Ñ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„ĞµÑ€Ğ°Ğ±ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¸ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸.'}, 'en': {'title': 'Bridging the Gap: Enhancing LLMs with World Knowledge', 'desc': "WorldMind is a framework designed to improve Large Language Models (LLMs) by creating a symbolic repository of world knowledge. This repository helps LLMs understand and respect the physical laws of the world, reducing the occurrence of unrealistic plans or 'physical hallucinations'. Instead of relying on static model parameters that require expensive retraining, WorldMind uses experience-based learning to adaptively synthesize feedback from the environment. Experiments show that WorldMind significantly outperforms existing models in various tasks, demonstrating its ability to transfer knowledge across different models and environments."}, 'zh': {'title': 'WorldMindï¼šæ‰“ç ´æ¨¡æ€è„±èŠ‚çš„æ™ºèƒ½æ¡†æ¶', 'desc': 'WorldMind è§£å†³äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­çš„æ¨¡æ€è„±èŠ‚é—®é¢˜ï¼Œé€šè¿‡è‡ªä¸»æ„å»ºç¬¦å·ä¸–ç•ŒçŸ¥è¯†åº“ï¼Œå¢å¼ºäº†ç‰©ç†å¯è¡Œæ€§å’Œä»»åŠ¡æœ€ä¼˜æ€§ã€‚è¯¥æ¡†æ¶é€šè¿‡åˆæˆç¯å¢ƒåé¦ˆï¼Œç»Ÿä¸€äº†è¿‡ç¨‹ç»éªŒå’Œç›®æ ‡ç»éªŒï¼Œä»¥ç¡®ä¿ç‰©ç†å¯è¡Œæ€§å’Œä»»åŠ¡å¯¼å‘ã€‚ä¸ä¼ ç»Ÿçš„ä¾èµ–èµ„æºå¯†é›†å‹è®­ç»ƒçš„å¯¹é½ç­–ç•¥ç›¸æ¯”ï¼ŒWorldMind èƒ½å¤Ÿæ›´çµæ´»åœ°é€‚åº”ç‰©ç†åŠ¨æ€çš„å˜åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒWorldMind åœ¨ EB-ALFRED å’Œ EB-Habitat ä¸Šçš„è¡¨ç°ä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œå…·æœ‰æ˜¾è‘—çš„è·¨æ¨¡å‹å’Œè·¨ç¯å¢ƒçš„è¿ç§»èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.14250', 'title': 'OmniTransfer: All-in-one Framework for Spatio-temporal Video Transfer', 'url': 'https://huggingface.co/papers/2601.14250', 'abstract': 'OmniTransfer presents a unified framework for spatio-temporal video transfer that enhances appearance consistency and temporal control through multi-view information and multimodal semantic guidance.  \t\t\t\t\tAI-generated summary \t\t\t\t Videos convey richer information than images or text, capturing both spatial and temporal dynamics. However, most existing video customization methods rely on reference images or task-specific temporal priors, failing to fully exploit the rich spatio-temporal information inherent in videos, thereby limiting flexibility and generalization in video generation. To address these limitations, we propose OmniTransfer, a unified framework for spatio-temporal video transfer. It leverages multi-view information across frames to enhance appearance consistency and exploits temporal cues to enable fine-grained temporal control. To unify various video transfer tasks, OmniTransfer incorporates three key designs: Task-aware Positional Bias that adaptively leverages reference video information to improve temporal alignment or appearance consistency; Reference-decoupled Causal Learning separating reference and target branches to enable precise reference transfer while improving efficiency; and Task-adaptive Multimodal Alignment using multimodal semantic guidance to dynamically distinguish and tackle different tasks. Extensive experiments show that OmniTransfer outperforms existing methods in appearance (ID and style) and temporal transfer (camera movement and video effects), while matching pose-guided methods in motion transfer without using pose, establishing a new paradigm for flexible, high-fidelity video generation.', 'score': 7, 'issue_id': 685, 'pub_date': '2026-01-20', 'pub_date_card': {'ru': '20 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 20', 'zh': '1æœˆ20æ—¥'}, 'hash': 'ea0f06a5786530a6', 'authors': ['Pengze Zhang', 'Yanze Wu', 'Mengtian Li', 'Xu Bai', 'Songtao Zhao', 'Fulong Ye', 'Chong Mou', 'Xinghui Li', 'Zhuowei Chen', 'Qian He', 'Mingyuan Gao'], 'affiliations': ['Intelligent Creation Lab, ByteDance'], 'pdf_title_img': 'assets/pdf/title_img/2601.14250.jpg', 'data': {'categories': ['#video', '#transfer_learning', '#architecture', '#multimodal'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„ĞµÑ€ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµÑ€ĞµĞ· Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ', 'desc': 'OmniTransfer Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ĞµĞ´Ğ¸Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ñ‚Ñ€Ğ°Ğ½ÑÑ„ĞµÑ€Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ²ÑŒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ğ´Ñ€Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ Ğ²Ğ¸Ğ´Ğ° Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹ Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Task-aware Positional Bias, Reference-decoupled Causal Learning Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ‚Ñ€Ğ°Ğ½ÑÑ„ĞµÑ€Ğ° Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ², Ğ¸ Task-adaptive Multimodal Alignment Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² Ñ‚Ñ€Ğ°Ğ½ÑÑ„ĞµÑ€Ğµ ÑÑ‚Ğ¸Ğ»Ñ, Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ Ğ²Ğ¸Ğ´Ğ° Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ²Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ·, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'OmniTransfer: Revolutionizing Video Transfer with Spatio-Temporal Insights', 'desc': 'OmniTransfer is a new framework designed for improving video transfer by utilizing both spatial and temporal information. It enhances the consistency of video appearance and allows for better control over timing by using data from multiple views and different types of information. The framework introduces three innovative components: a positional bias that adapts to improve alignment, a causal learning approach that separates reference and target data for efficiency, and a multimodal alignment system that adjusts to various tasks. Overall, OmniTransfer shows significant improvements in video generation quality compared to existing methods, making it a versatile tool for high-fidelity video creation.'}, 'zh': {'title': 'OmniTransferï¼šæ—¶ç©ºè§†é¢‘ä¼ è¾“çš„æ–°èŒƒå¼', 'desc': 'OmniTransferæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„æ—¶ç©ºè§†é¢‘ä¼ è¾“æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å¤šè§†è§’ä¿¡æ¯å’Œå¤šæ¨¡æ€è¯­ä¹‰æŒ‡å¯¼æ¥å¢å¼ºå¤–è§‚ä¸€è‡´æ€§å’Œæ—¶é—´æ§åˆ¶ã€‚è¯¥æ–¹æ³•å…‹æœäº†ç°æœ‰è§†é¢‘å®šåˆ¶æ–¹æ³•çš„å±€é™æ€§ï¼Œå……åˆ†åˆ©ç”¨è§†é¢‘ä¸­ä¸°å¯Œçš„æ—¶ç©ºä¿¡æ¯ã€‚OmniTransferç»“åˆäº†ä»»åŠ¡æ„ŸçŸ¥çš„ä½ç½®ä¿¡æ¯åå·®ã€å‚è€ƒè§£è€¦å› æœå­¦ä¹ å’Œä»»åŠ¡è‡ªé€‚åº”å¤šæ¨¡æ€å¯¹é½ç­‰è®¾è®¡ï¼Œæå‡äº†è§†é¢‘ç”Ÿæˆçš„çµæ´»æ€§å’Œé«˜ä¿çœŸåº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOmniTransferåœ¨å¤–è§‚å’Œæ—¶é—´ä¼ è¾“æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¼€åˆ›äº†çµæ´»é«˜ä¿çœŸè§†é¢‘ç”Ÿæˆçš„æ–°èŒƒå¼ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.14192', 'title': 'Toward Efficient Agents: Memory, Tool learning, and Planning', 'url': 'https://huggingface.co/papers/2601.14192', 'abstract': 'Efficiency in agentic systems is examined across memory, tool learning, and planning components, analyzing trade-offs between effectiveness and computational costs through various optimization strategies and benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent years have witnessed increasing interest in extending large language models into agentic systems. While the effectiveness of agents has continued to improve, efficiency, which is crucial for real-world deployment, has often been overlooked. This paper therefore investigates efficiency from three core components of agents: memory, tool learning, and planning, considering costs such as latency, tokens, steps, etc. Aimed at conducting comprehensive research addressing the efficiency of the agentic system itself, we review a broad range of recent approaches that differ in implementation yet frequently converge on shared high-level principles including but not limited to bounding context via compression and management, designing reinforcement learning rewards to minimize tool invocation, and employing controlled search mechanisms to enhance efficiency, which we discuss in detail. Accordingly, we characterize efficiency in two complementary ways: comparing effectiveness under a fixed cost budget, and comparing cost at a comparable level of effectiveness. This trade-off can also be viewed through the Pareto frontier between effectiveness and cost. From this perspective, we also examine efficiency oriented benchmarks by summarizing evaluation protocols for these components and consolidating commonly reported efficiency metrics from both benchmark and methodological studies. Moreover, we discuss the key challenges and future directions, with the goal of providing promising insights.', 'score': 7, 'issue_id': 685, 'pub_date': '2026-01-20', 'pub_date_card': {'ru': '20 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 20', 'zh': '1æœˆ20æ—¥'}, 'hash': 'a98c299a62724db7', 'authors': ['Xiaofang Yang', 'Lijun Li', 'Heng Zhou', 'Tong Zhu', 'Xiaoye Qu', 'Yuchen Fan', 'Qianshan Wei', 'Rui Ye', 'Li Kang', 'Yiran Qin', 'Zhiqiang Kou', 'Daizong Liu', 'Qi Li', 'Ning Ding', 'Siheng Chen', 'Jing Shao'], 'affiliations': ['Fudan University', 'Hong Kong Polytechnic University', 'Institute of Automation, Chinese Academy of Sciences', 'Shanghai Artificial Intelligence Laboratory', 'Shanghai Jiaotong University', 'The Chinese University of Hong Kong (Shenzhen)', 'Tsinghua University', 'University of Science and Technology of China', 'Wuhan University'], 'pdf_title_img': 'assets/pdf/title_img/2601.14192.jpg', 'data': {'categories': ['#rl', '#optimization', '#long_context', '#agents', '#benchmark'], 'emoji': 'âš¡', 'ru': {'title': 'ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼: Ğ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒÑ', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°: Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ¾Ğ² Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº. Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ğ¼Ğ¸, Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ğ¹ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†ĞµĞ¹ ĞŸĞ°Ñ€ĞµÑ‚Ğ¾. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¸ĞºĞ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸, Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°Ñ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ÑÑ….'}, 'en': {'title': 'Optimizing Efficiency in Agentic Systems', 'desc': 'This paper explores the efficiency of agentic systems, focusing on three main components: memory, tool learning, and planning. It highlights the trade-offs between effectiveness and computational costs, emphasizing the importance of efficiency for real-world applications. The authors review various optimization strategies and benchmarks, discussing methods like context compression and reinforcement learning rewards to improve performance. Additionally, they propose a framework for evaluating efficiency through a Pareto frontier analysis, identifying challenges and future research directions in the field.'}, 'zh': {'title': 'æå‡æ™ºèƒ½ä½“ç³»ç»Ÿæ•ˆç‡çš„å…³é”®ç ”ç©¶', 'desc': 'æœ¬æ–‡æ¢è®¨äº†æ™ºèƒ½ä½“ç³»ç»Ÿä¸­çš„æ•ˆç‡ï¼Œé‡ç‚¹åˆ†æäº†è®°å¿†ã€å·¥å…·å­¦ä¹ å’Œè§„åˆ’ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ã€‚ç ”ç©¶äº†åœ¨æœ‰æ•ˆæ€§å’Œè®¡ç®—æˆæœ¬ä¹‹é—´çš„æƒè¡¡ï¼Œé‡‡ç”¨äº†å¤šç§ä¼˜åŒ–ç­–ç•¥å’ŒåŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬å›é¡¾äº†ä¸åŒå®ç°æ–¹å¼çš„æœ€æ–°æ–¹æ³•ï¼Œå¼ºè°ƒäº†å‹ç¼©å’Œç®¡ç†ä¸Šä¸‹æ–‡ã€è®¾è®¡å¼ºåŒ–å­¦ä¹ å¥–åŠ±ä»¥å‡å°‘å·¥å…·è°ƒç”¨ç­‰é«˜å±‚åŸåˆ™ã€‚æœ€åï¼Œæœ¬æ–‡æ€»ç»“äº†æ•ˆç‡è¯„ä¼°çš„æŒ‘æˆ˜å’Œæœªæ¥æ–¹å‘ï¼Œæ—¨åœ¨ä¸ºæ™ºèƒ½ä½“ç³»ç»Ÿçš„æ•ˆç‡ç ”ç©¶æä¾›æœ‰ä»·å€¼çš„è§è§£ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.14046', 'title': 'PRiSM: Benchmarking Phone Realization in Speech Models', 'url': 'https://huggingface.co/papers/2601.14046', 'abstract': 'PRiSM benchmark evaluates phonetic perception in speech models through standardized transcription-based metrics and downstream applications across clinical, educational, and multilingual domains.  \t\t\t\t\tAI-generated summary \t\t\t\t Phone recognition (PR) serves as the atomic interface for language-agnostic modeling for cross-lingual speech processing and phonetic analysis. Despite prolonged efforts in developing PR systems, current evaluations only measure surface-level transcription accuracy. We introduce PRiSM, the first open-source benchmark designed to expose blind spots in phonetic perception through intrinsic and extrinsic evaluation of PR systems. PRiSM standardizes transcription-based evaluation and assesses downstream utility in clinical, educational, and multilingual settings with transcription and representation probes. We find that diverse language exposure during training is key to PR performance, encoder-CTC models are the most stable, and specialized PR models still outperform Large Audio Language Models. PRiSM releases code, recipes, and datasets to move the field toward multilingual speech models with robust phonetic ability: https://github.com/changelinglab/prism.', 'score': 3, 'issue_id': 685, 'pub_date': '2026-01-20', 'pub_date_card': {'ru': '20 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 20', 'zh': '1æœˆ20æ—¥'}, 'hash': 'ee74254991809e26', 'authors': ['Shikhar Bharadwaj', 'Chin-Jou Li', 'Yoonjae Kim', 'Kwanghee Choi', 'Eunjung Yeo', 'Ryan Soh-Eun Shim', 'Hanyu Zhou', 'Brendon Boldt', 'Karen Rosero Jacome', 'Kalvin Chang', 'Darsh Agrawal', 'Keer Xu', 'Chao-Han Huck Yang', 'Jian Zhu', 'Shinji Watanabe', 'David R. Mortensen'], 'affiliations': ['CMU', 'Gwangju Institute of Science and Technology', 'LMU Munich', 'NVIDIA', 'UBC', 'UC Berkeley', 'UT Austin'], 'pdf_title_img': 'assets/pdf/title_img/2601.14046.jpg', 'data': {'categories': ['#open_source', '#audio', '#dataset', '#low_resource', '#multilingual', '#benchmark'], 'emoji': 'ğŸ”¤', 'ru': {'title': 'Ğ“Ğ»ÑƒĞ±Ğ¾ĞºĞ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ñ„Ğ¾Ğ½ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ñ€ĞµÑ‡ĞµĞ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº PRiSM Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ñ„Ğ¾Ğ½ĞµÑ‚Ğ¸ĞºĞ¸ Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ñ„Ğ¾Ğ½ĞµĞ¼ Ñ‡ĞµÑ€ĞµĞ· ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ¿Ñ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸Ğ·Ğ¼ĞµÑ€ÑÑÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ½ÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ½Ğµ Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°Ñ ÑĞ»ĞµĞ¿Ñ‹Ğµ Ğ¿ÑÑ‚Ğ½Ğ° Ğ² Ñ„Ğ¾Ğ½ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğµ Ğ¸ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½Ğ° ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ…, Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ·Ğ¾Ğ½Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ĞºĞ°Ğº Ñ‚Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ¿Ñ†Ğ¸Ğ¹, Ñ‚Ğ°Ğº Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ° ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ğ° Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, ÑĞ½ĞºĞ¾Ğ´ĞµÑ€-CTC Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹, Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ°ÑƒĞ´Ğ¸Ğ¾ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'PRiSM: Elevating Phonetic Perception in Speech Models', 'desc': 'The PRiSM benchmark is a new tool for evaluating how well speech models understand phonetics across different languages and applications. It goes beyond just checking if the transcription is correct by also looking at how these models perform in real-world scenarios like healthcare and education. The research shows that training with a variety of languages improves the performance of phonetic recognition systems. Additionally, specialized models still perform better than larger, more general audio language models, highlighting the importance of targeted training in phonetic perception.'}, 'zh': {'title': 'PRiSMï¼šæå‡è¯­éŸ³è¯†åˆ«çš„å¤šè¯­è¨€èƒ½åŠ›', 'desc': 'PRiSMåŸºå‡†æµ‹è¯•é€šè¿‡æ ‡å‡†åŒ–çš„è½¬å½•è¯„ä¼°æŒ‡æ ‡å’Œä¸‹æ¸¸åº”ç”¨ï¼Œè¯„ä¼°è¯­éŸ³æ¨¡å‹ä¸­çš„è¯­éŸ³æ„ŸçŸ¥èƒ½åŠ›ï¼Œæ¶µç›–ä¸´åºŠã€æ•™è‚²å’Œå¤šè¯­è¨€é¢†åŸŸã€‚å°½ç®¡åœ¨å¼€å‘è¯­éŸ³è¯†åˆ«ç³»ç»Ÿæ–¹é¢è¿›è¡Œäº†é•¿æœŸåŠªåŠ›ï¼Œä½†ç›®å‰çš„è¯„ä¼°ä»…æµ‹é‡è¡¨é¢è½¬å½•å‡†ç¡®æ€§ã€‚æˆ‘ä»¬å¼•å…¥äº†PRiSMï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå¼€æºåŸºå‡†ï¼Œæ—¨åœ¨é€šè¿‡å¯¹è¯­éŸ³è¯†åˆ«ç³»ç»Ÿçš„å†…åœ¨å’Œå¤–åœ¨è¯„ä¼°ï¼Œæ­ç¤ºè¯­éŸ³æ„ŸçŸ¥çš„ç›²ç‚¹ã€‚ç ”ç©¶å‘ç°ï¼Œå¤šæ ·åŒ–çš„è¯­è¨€æš´éœ²å¯¹è¯­éŸ³è¯†åˆ«æ€§èƒ½è‡³å…³é‡è¦ï¼Œç¼–ç å™¨-CTCæ¨¡å‹æœ€ä¸ºç¨³å®šï¼Œè€Œä¸“é—¨çš„è¯­éŸ³è¯†åˆ«æ¨¡å‹ä»ç„¶ä¼˜äºå¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.13288', 'title': 'A BERTology View of LLM Orchestrations: Token- and Layer-Selective Probes for Efficient Single-Pass Classification', 'url': 'https://huggingface.co/papers/2601.13288', 'abstract': 'Lightweight probes trained on hidden states of LLMs enable efficient classification tasks without additional computational overhead, improving safety and sentiment analysis performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Production LLM systems often rely on separate models for safety and other classification-heavy steps, increasing latency, VRAM footprint, and operational complexity. We instead reuse computation already paid for by the serving LLM: we train lightweight probes on its hidden states and predict labels in the same forward pass used for generation. We frame classification as representation selection over the full token-layer hidden-state tensor, rather than committing to a fixed token or fixed layer (e.g., first-token logits or final-layer pooling). To implement this, we introduce a two-stage aggregator that (i) summarizes tokens within each layer and (ii) aggregates across layer summaries to form a single representation for classification. We instantiate this template with direct pooling, a 100K-parameter scoring-attention gate, and a downcast multi-head self-attention (MHA) probe with up to 35M trainable parameters. Across safety and sentiment benchmarks our probes improve over logit-only reuse (e.g., MULI) and are competitive with substantially larger task-specific baselines, while preserving near-serving latency and avoiding the VRAM and latency costs of a separate guard-model pipeline.', 'score': 1, 'issue_id': 683, 'pub_date': '2026-01-19', 'pub_date_card': {'ru': '19 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 19', 'zh': '1æœˆ19æ—¥'}, 'hash': '16b301dd1efbd006', 'authors': ['Gonzalo Ariel Meyoyan', 'Luciano Del Corro'], 'affiliations': ['Departamento de ComputaciÃ³n, FCEyN Universidad de Buenos Aires', 'ELIAS Lab, Departamento de IngenierÃ­a Universidad de San AndrÃ©s'], 'pdf_title_img': 'assets/pdf/title_img/2601.13288.jpg', 'data': {'categories': ['#training', '#inference', '#small_models'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ›Ñ‘Ğ³ĞºĞ¸Ğµ Ğ·Ğ¾Ğ½Ğ´Ñ‹ Ğ´Ğ»Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸: Ğ¿ĞµÑ€ĞµĞ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ LLM Ğ±ĞµĞ· Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ»Ñ‘Ğ³ĞºĞ¸Ñ… Ğ·Ğ¾Ğ½Ğ´Ğ¾Ğ² (probes) Ğ½Ğ° ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸ÑÑ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ñ‚Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿ĞµÑ€ĞµĞ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ LLM, Ğ¾Ğ±ÑƒÑ‡Ğ¸Ğ² Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ·Ğ¾Ğ½Ğ´Ñ‹ Ğ´ĞµĞ»Ğ°Ñ‚ÑŒ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ² Ñ‚Ğ¾Ğ¼ Ğ¶Ğµ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´Ğµ, Ñ‡Ñ‚Ğ¾ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ°. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ· Ğ²ÑĞµĞ³Ğ¾ Ñ‚ĞµĞ½Ğ·Ğ¾Ñ€Ğ° ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ Ğ²ÑĞµÑ… ÑĞ»Ğ¾Ñ‘Ğ² Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ° Ğ¸Ğ»Ğ¸ ÑĞ»Ğ¾Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ°Ğ³Ñ€ĞµĞ³Ğ°Ñ‚Ğ¾Ñ€ Ğ¸ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ¾Ğ² Ğ·Ğ¾Ğ½Ğ´Ğ¾Ğ², Ğ¾Ñ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ³Ğ¾ ÑƒÑÑ€ĞµĞ´Ğ½ĞµĞ½Ğ¸Ñ Ğ´Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿ĞµÑ€ĞµĞ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ»Ğ¾Ğ³Ğ¸Ñ‚Ğ¾Ğ² Ğ¸ ĞºĞ¾Ğ½ĞºÑƒÑ€Ğ¸Ñ€ÑƒÑÑ‚ Ñ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸.'}, 'en': {'title': 'Efficient Classification with Lightweight Probes on LLMs', 'desc': 'This paper presents a method for improving classification tasks using lightweight probes that are trained on the hidden states of large language models (LLMs). By reusing the computation from the LLM during its generation process, the approach reduces latency and memory usage compared to traditional methods that require separate models for classification. The authors propose a two-stage aggregator that summarizes information from multiple layers of the LLM to create a single representation for classification tasks. The results show that these probes enhance performance in safety and sentiment analysis while maintaining efficiency, making them competitive with larger, task-specific models.'}, 'zh': {'title': 'åˆ©ç”¨è½»é‡çº§æ¢é’ˆæå‡åˆ†ç±»æ•ˆç‡', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§è½»é‡çº§æ¢é’ˆï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„éšè—çŠ¶æ€è¿›è¡Œé«˜æ•ˆåˆ†ç±»ä»»åŠ¡ï¼Œé¿å…äº†é¢å¤–çš„è®¡ç®—å¼€é”€ï¼Œä»è€Œæå‡äº†å®‰å…¨æ€§å’Œæƒ…æ„Ÿåˆ†æçš„æ€§èƒ½ã€‚æˆ‘ä»¬é€šè¿‡åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­é‡ç”¨LLMçš„è®¡ç®—ï¼Œè®­ç»ƒè¿™äº›æ¢é’ˆæ¥é¢„æµ‹æ ‡ç­¾ï¼Œè€Œä¸æ˜¯ä¾èµ–äºå•ç‹¬çš„æ¨¡å‹ã€‚åˆ†ç±»è¢«è§†ä¸ºåœ¨å®Œæ•´çš„éšè—çŠ¶æ€å¼ é‡ä¸Šè¿›è¡Œè¡¨ç¤ºé€‰æ‹©ï¼Œè€Œä¸æ˜¯å›ºå®šäºæŸä¸ªç‰¹å®šçš„tokenæˆ–å±‚ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªä¸¤é˜¶æ®µèšåˆå™¨ï¼Œé¦–å…ˆåœ¨æ¯ä¸€å±‚å†…æ€»ç»“tokenï¼Œç„¶åè·¨å±‚æ±‡æ€»å½¢æˆå•ä¸€çš„åˆ†ç±»è¡¨ç¤ºã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.10700', 'title': 'LIBERTy: A Causal Framework for Benchmarking Concept-Based Explanations of LLMs with Structural Counterfactuals', 'url': 'https://huggingface.co/papers/2601.10700', 'abstract': 'A framework for generating structured counterfactual pairs using LLMs and SCMs enables improved evaluation and analysis of concept-based explanations in high-stakes domains.  \t\t\t\t\tAI-generated summary \t\t\t\t Concept-based explanations quantify how high-level concepts (e.g., gender or experience) influence model behavior, which is crucial for decision-makers in high-stakes domains. Recent work evaluates the faithfulness of such explanations by comparing them to reference causal effects estimated from counterfactuals. In practice, existing benchmarks rely on costly human-written counterfactuals that serve as an imperfect proxy. To address this, we introduce a framework for constructing datasets containing structural counterfactual pairs: LIBERTy (LLM-based Interventional Benchmark for Explainability with Reference Targets). LIBERTy is grounded in explicitly defined Structured Causal Models (SCMs) of the text generation, interventions on a concept propagate through the SCM until an LLM generates the counterfactual. We introduce three datasets (disease detection, CV screening, and workplace violence prediction) together with a new evaluation metric, order-faithfulness. Using them, we evaluate a wide range of methods across five models and identify substantial headroom for improving concept-based explanations. LIBERTy also enables systematic analysis of model sensitivity to interventions: we find that proprietary LLMs show markedly reduced sensitivity to demographic concepts, likely due to post-training mitigation. Overall, LIBERTy provides a much-needed benchmark for developing faithful explainability methods.', 'score': 1, 'issue_id': 686, 'pub_date': '2026-01-15', 'pub_date_card': {'ru': '15 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 15', 'zh': '1æœˆ15æ—¥'}, 'hash': '68fa6c7fe60c9fca', 'authors': ['Gilat Toker', 'Nitay Calderon', 'Ohad Amosy', 'Roi Reichart'], 'affiliations': ['Technion Israel Institute of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2601.10700.jpg', 'data': {'categories': ['#healthcare', '#interpretability', '#ethics', '#benchmark', '#dataset', '#synthetic'], 'emoji': 'ğŸ”', 'ru': {'title': 'ĞĞ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ğ±ÑŠÑÑĞ½Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ñ‹Ğµ ĞºĞ¾Ğ½Ñ‚Ñ€Ñ„Ğ°ĞºÑ‚Ñ‹', 'desc': 'ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ LIBERTy â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼Ğ¸, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (SCM) Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ñ… ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹ Ğ½Ğ° Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ñ‚Ñ€Ğ¸ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ order-faithfulness Ğ´Ğ»Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ğ±ÑŠÑÑĞ½Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ğµ LLM Ğ¼ĞµĞ½ĞµĞµ Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹ Ğº Ğ´ĞµĞ¼Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸ÑĞ¼ Ğ¸Ğ·-Ğ·Ğ° Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Enhancing Explainability with LIBERTy: A New Benchmark for Counterfactual Analysis', 'desc': 'This paper presents LIBERTy, a framework that generates structured counterfactual pairs using Large Language Models (LLMs) and Structured Causal Models (SCMs) to enhance the evaluation of concept-based explanations in critical areas. Concept-based explanations help understand how high-level factors, like gender or experience, affect model decisions, which is vital for stakeholders in high-stakes situations. The framework addresses the limitations of existing benchmarks that depend on expensive human-generated counterfactuals by creating datasets that allow for systematic analysis of model behavior. By introducing new datasets and an evaluation metric called order-faithfulness, the study reveals significant opportunities for improving the reliability of concept-based explanations and highlights the sensitivity of models to demographic interventions.'}, 'zh': {'title': 'LIBERTyï¼šæå‡é«˜é£é™©é¢†åŸŸè§£é‡Šæ€§çš„æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œç»“æ„å› æœæ¨¡å‹ï¼ˆSCMsï¼‰ç”Ÿæˆç»“æ„åŒ–åäº‹å®å¯¹çš„æ¡†æ¶ï¼Œæ—¨åœ¨æ”¹å–„é«˜é£é™©é¢†åŸŸä¸­åŸºäºæ¦‚å¿µçš„è§£é‡Šçš„è¯„ä¼°å’Œåˆ†æã€‚åŸºäºæ¦‚å¿µçš„è§£é‡Šé‡åŒ–äº†é«˜å±‚æ¬¡æ¦‚å¿µï¼ˆå¦‚æ€§åˆ«æˆ–ç»éªŒï¼‰å¯¹æ¨¡å‹è¡Œä¸ºçš„å½±å“ï¼Œè¿™å¯¹å†³ç­–è€…è‡³å…³é‡è¦ã€‚ç°æœ‰çš„è¯„ä¼°æ–¹æ³•ä¾èµ–äºäººå·¥ç¼–å†™çš„åäº‹å®ï¼Œæˆæœ¬é«˜ä¸”ä¸å¤Ÿå‡†ç¡®ã€‚LIBERTyæ¡†æ¶é€šè¿‡æ„å»ºç»“æ„åäº‹å®å¯¹çš„æ•°æ®é›†ï¼Œæä¾›äº†ä¸€ç§æ–°çš„è¯„ä¼°åŸºå‡†ï¼Œå¸®åŠ©ç³»ç»Ÿåˆ†ææ¨¡å‹å¯¹å¹²é¢„çš„æ•æ„Ÿæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.13253', 'title': 'A Hybrid Protocol for Large-Scale Semantic Dataset Generation in Low-Resource Languages: The Turkish Semantic Relations Corpus', 'url': 'https://huggingface.co/papers/2601.13253', 'abstract': 'A hybrid methodology combining FastText embeddings, clustering, and AI classification generates a large-scale Turkish semantic relations dataset with high accuracy validation.  \t\t\t\t\tAI-generated summary \t\t\t\t We present a hybrid methodology for generating large-scale semantic relationship datasets in low-resource languages, demonstrated through a comprehensive Turkish semantic relations corpus. Our approach integrates three phases: (1) FastText embeddings with Agglomerative Clustering to identify semantic clusters, (2) Gemini 2.5-Flash for automated semantic relationship classification, and (3) integration with curated dictionary sources. The resulting dataset comprises 843,000 unique Turkish semantic pairs across three relationship types (synonyms, antonyms, co-hyponyms) representing a 10x scale increase over existing resources at minimal cost ($65). We validate the dataset through two downstream tasks: an embedding model achieving 90% top-1 retrieval accuracy and a classification model attaining 90% F1-macro. Our scalable protocol addresses critical data scarcity in Turkish NLP and demonstrates applicability to other low-resource languages. We publicly release the dataset and models.', 'score': 0, 'issue_id': 686, 'pub_date': '2026-01-19', 'pub_date_card': {'ru': '19 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 19', 'zh': '1æœˆ19æ—¥'}, 'hash': '49ba853e5d57b947', 'authors': ['Ebubekir Tosun', 'Mehmet Emin Buldur', 'Ã–zay Ezerceli', 'Mahmoud ElHussieni'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2601.13253.jpg', 'data': {'categories': ['#multilingual', '#data', '#low_resource', '#dataset', '#open_source', '#synthetic'], 'emoji': 'ğŸ‡¹ğŸ‡·', 'ru': {'title': 'ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¹ Ñ‚ÑƒÑ€ĞµÑ†ĞºĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° Ñ‡ĞµÑ€ĞµĞ· Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½ÑƒÑ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ', 'desc': 'ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¹ Ğ² Ñ‚ÑƒÑ€ĞµÑ†ĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ embeddings FastText Ğ¸ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ²ÑĞ·Ğ¸ (ÑĞ¸Ğ½Ğ¾Ğ½Ğ¸Ğ¼Ñ‹, Ğ°Ğ½Ñ‚Ğ¾Ğ½Ğ¸Ğ¼Ñ‹, ĞºĞ¾-Ğ³Ğ¸Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ñ‹) Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Gemini 2.5-Flash. ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 843 Ñ‚Ñ‹ÑÑÑ‡Ğ¸ ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ°Ñ€ Ñ‚ÑƒÑ€ĞµÑ†ĞºĞ¸Ñ… ÑĞ»Ğ¾Ğ² Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ 90% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ´ĞµÑ„Ğ¸Ñ†Ğ¸Ñ‚Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ´Ğ»Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€ĞµÑÑƒÑ€ÑĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ² Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ° Ğº Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼ ÑĞ·Ñ‹ĞºĞ°Ğ¼.'}, 'en': {'title': 'Scaling Turkish NLP with Hybrid Semantic Relations Dataset', 'desc': 'This paper introduces a hybrid methodology that combines FastText embeddings, clustering, and AI classification to create a large-scale dataset of semantic relations in Turkish. The process involves using Agglomerative Clustering to group similar semantic concepts, followed by automated classification with Gemini 2.5-Flash. The resulting dataset includes 843,000 unique pairs of Turkish words categorized into synonyms, antonyms, and co-hyponyms, significantly expanding existing resources. Validation of the dataset shows high accuracy in both retrieval and classification tasks, addressing the challenge of data scarcity in Turkish natural language processing.'}, 'zh': {'title': 'åˆ›æ–°æ··åˆæ–¹æ³•ï¼Œæå‡åœŸè€³å…¶è¯­è¯­ä¹‰æ•°æ®é›†çš„è§„æ¨¡ä¸å‡†ç¡®æ€§', 'desc': 'æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ··åˆæ–¹æ³•ï¼Œç»“åˆäº†FastTextåµŒå…¥ã€èšç±»å’Œäººå·¥æ™ºèƒ½åˆ†ç±»ï¼Œç”Ÿæˆäº†ä¸€ä¸ªå¤§è§„æ¨¡çš„åœŸè€³å…¶è¯­è¯­ä¹‰å…³ç³»æ•°æ®é›†ã€‚è¯¥æ–¹æ³•åˆ†ä¸ºä¸‰ä¸ªé˜¶æ®µï¼šé¦–å…ˆä½¿ç”¨FastTextåµŒå…¥å’Œèšç±»ç®—æ³•è¯†åˆ«è¯­ä¹‰ç°‡ï¼›å…¶æ¬¡åˆ©ç”¨Gemini 2.5-Flashè¿›è¡Œè‡ªåŠ¨è¯­ä¹‰å…³ç³»åˆ†ç±»ï¼›æœ€åä¸ç»è¿‡æ•´ç†çš„è¯å…¸èµ„æºæ•´åˆã€‚æœ€ç»ˆç”Ÿæˆçš„æ•°æ®é›†åŒ…å«843,000å¯¹ç‹¬ç‰¹çš„åœŸè€³å…¶è¯­è¯­ä¹‰å¯¹ï¼Œæ¶µç›–åŒä¹‰è¯ã€åä¹‰è¯å’ŒåŒä¹‰è¯ç¾¤ç­‰ä¸‰ç§å…³ç³»ç±»å‹ï¼Œè§„æ¨¡æ¯”ç°æœ‰èµ„æºå¢åŠ äº†10å€ï¼Œä¸”æˆæœ¬ä»…ä¸º65ç¾å…ƒã€‚æˆ‘ä»¬é€šè¿‡ä¸¤ä¸ªä¸‹æ¸¸ä»»åŠ¡éªŒè¯äº†æ•°æ®é›†çš„å‡†ç¡®æ€§ï¼ŒåµŒå…¥æ¨¡å‹è¾¾åˆ°äº†90%çš„æ£€ç´¢å‡†ç¡®ç‡ï¼Œåˆ†ç±»æ¨¡å‹çš„F1-macroå€¼ä¹Ÿè¾¾åˆ°äº†90%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.13251', 'title': 'Beyond Cosine Similarity: Taming Semantic Drift and Antonym Intrusion in a 15-Million Node Turkish Synonym Graph', 'url': 'https://huggingface.co/papers/2601.13251', 'abstract': "A large-scale semantic clustering system addresses the limitation of neural embeddings in distinguishing synonyms from antonyms through a specialized three-way discriminator and novel clustering algorithm.  \t\t\t\t\tAI-generated summary \t\t\t\t Neural embeddings have a notorious blind spot: they can't reliably tell synonyms apart from antonyms. Consequently, increasing similarity thresholds often fails to prevent opposites from being grouped together. We've built a large-scale semantic clustering system specifically designed to tackle this problem head on. Our pipeline chews through 15 million lexical items, evaluates a massive 520 million potential relationships, and ultimately generates 2.9 million high-precision semantic clusters. The system makes three primary contributions. First, we introduce a labeled dataset of 843,000 concept pairs spanning synonymy, antonymy, and co-hyponymy, constructed via Gemini 2.5-Flash LLM augmentation and verified using human-curated dictionary resources. Second, we propose a specialized three-way semantic relation discriminator that achieves 90% macro-F1, enabling robust disambiguation beyond raw embedding similarity. Third, we introduce a novel soft-to-hard clustering algorithm that mitigates semantic drift preventing erroneous transitive chains (e.g., hot -> spicy -> pain -> depression) while simultaneously resolving polysemy. Our approach employs a topology-aware two-stage expansion-pruning procedure with topological voting, ensuring that each term is assigned to exactly one semantically coherent cluster. The resulting resource enables high-precision semantic search and retrieval-augmented generation, particularly for morphologically rich and low-resource languages where existing synonym databases remain sparse.", 'score': 0, 'issue_id': 686, 'pub_date': '2026-01-19', 'pub_date_card': {'ru': '19 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 19', 'zh': '1æœˆ19æ—¥'}, 'hash': 'a69fdbc81b2cacaf', 'authors': ['Ebubekir Tosun', 'Mehmet Emin Buldur', 'Ã–zay Ezerceli', 'Mahmoud ElHussieni'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2601.13251.jpg', 'data': {'categories': [], 'emoji': 'ğŸ¯', 'ru': {'title': 'Ğ Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ğµ ÑĞ¸Ğ½Ğ¾Ğ½Ğ¸Ğ¼Ğ¾Ğ² Ğ¸ Ğ°Ğ½Ñ‚Ğ¾Ğ½Ğ¸Ğ¼Ğ¾Ğ²: ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ±ĞµĞ· ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ´Ñ€ĞµĞ¹Ñ„Ğ°', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½ĞµÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ°Ñ‚ÑŒ ÑĞ¸Ğ½Ğ¾Ğ½Ğ¸Ğ¼Ñ‹ Ğ¸ Ğ°Ğ½Ñ‚Ğ¾Ğ½Ğ¸Ğ¼Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ²ÑˆÑƒÑ 15 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ»ĞµĞºÑĞ¸Ñ‡ĞµÑĞºĞ¸Ñ… ĞµĞ´Ğ¸Ğ½Ğ¸Ñ† Ğ¸ 520 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ²ÑĞ·ĞµĞ¹. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ²ĞºĞ»Ğ°Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‚Ñ€Ñ‘Ñ…ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½ÑÑ Ğ´Ğ¸ÑĞºÑ€Ğ¸Ğ¼Ğ¸Ğ½Ğ¸Ñ€ÑƒÑÑ‰ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ (90% macro-F1) Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑĞ¸Ğ½Ğ¾Ğ½Ğ¸Ğ¼Ğ¸Ğ¸, Ğ°Ğ½Ñ‚Ğ¾Ğ½Ğ¸Ğ¼Ğ¸Ğ¸ Ğ¸ ĞºĞ¾-Ğ³Ğ¸Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ¸Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¼ÑĞ³ĞºĞ¾-Ğ¶Ñ‘ÑÑ‚ĞºĞ¾Ğ¹ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ Ñ‚Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ° 2,9 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¾Ğ² Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ñ‹Ğ¹ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ¼Ğ¾Ñ€Ñ„Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ±Ğ¾Ğ³Ğ°Ñ‚Ñ‹Ñ… Ğ¸ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€ĞµÑÑƒÑ€ÑĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ².'}, 'en': {'title': 'Revolutionizing Semantic Clustering: Distinguishing Synonyms from Antonyms', 'desc': 'This paper presents a large-scale semantic clustering system that improves the differentiation between synonyms and antonyms, which is a common limitation of neural embeddings. The system utilizes a three-way discriminator and a novel clustering algorithm to accurately group lexical items. It processes a vast dataset of 15 million items and identifies 2.9 million high-precision semantic clusters. Key innovations include a labeled dataset for semantic relationships, a robust discriminator achieving 90% macro-F1, and a clustering method that prevents semantic drift and resolves polysemy.'}, 'zh': {'title': 'çªç ´ç¥ç»åµŒå…¥çš„è¯­ä¹‰èšç±»æ–°æ–¹æ³•', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§å¤§è§„æ¨¡è¯­ä¹‰èšç±»ç³»ç»Ÿï¼Œæ—¨åœ¨è§£å†³ç¥ç»åµŒå…¥åœ¨åŒºåˆ†åŒä¹‰è¯å’Œåä¹‰è¯æ–¹é¢çš„å±€é™æ€§ã€‚è¯¥ç³»ç»Ÿä½¿ç”¨äº†ä¸€ä¸ªä¸“é—¨çš„ä¸‰å…ƒåˆ¤åˆ«å™¨å’Œæ–°é¢–çš„èšç±»ç®—æ³•ï¼Œèƒ½å¤Ÿå¤„ç†1500ä¸‡ä¸ªè¯æ±‡é¡¹ï¼Œå¹¶è¯„ä¼°5.2äº¿ä¸ªæ½œåœ¨å…³ç³»ï¼Œæœ€ç»ˆç”Ÿæˆ290ä¸‡ä¸ªé«˜ç²¾åº¦çš„è¯­ä¹‰èšç±»ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåŒ…å«843,000å¯¹æ¦‚å¿µçš„æ ‡æ³¨æ•°æ®é›†ï¼Œæ¶µç›–åŒä¹‰è¯ã€åä¹‰è¯å’Œå…±åŒä¸‹ä½è¯ï¼Œå¹¶é€šè¿‡äººç±»æ ¡å¯¹çš„å­—å…¸èµ„æºè¿›è¡ŒéªŒè¯ã€‚æ­¤å¤–ï¼Œç³»ç»Ÿé‡‡ç”¨äº†ä¸€ç§æ–°é¢–çš„è½¯åˆ°ç¡¬èšç±»ç®—æ³•ï¼Œæœ‰æ•ˆé˜²æ­¢è¯­ä¹‰æ¼‚ç§»å’Œå¤šä¹‰è¯é—®é¢˜ï¼Œç¡®ä¿æ¯ä¸ªæœ¯è¯­è¢«åˆ†é…åˆ°ä¸€ä¸ªè¯­ä¹‰ä¸€è‡´çš„èšç±»ä¸­ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.00393', 'title': 'NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos', 'url': 'https://huggingface.co/papers/2601.00393', 'abstract': 'In this paper, we propose NeoVerse, a versatile 4D world model that is capable of 4D reconstruction, novel-trajectory video generation, and rich downstream applications. We first identify a common limitation of scalability in current 4D world modeling methods, caused either by expensive and specialized multi-view 4D data or by cumbersome training pre-processing. In contrast, our NeoVerse is built upon a core philosophy that makes the full pipeline scalable to diverse in-the-wild monocular videos. Specifically, NeoVerse features pose-free feed-forward 4D reconstruction, online monocular degradation pattern simulation, and other well-aligned techniques. These designs empower NeoVerse with versatility and generalization to various domains. Meanwhile, NeoVerse achieves state-of-the-art performance in standard reconstruction and generation benchmarks. Our project page is available at https://neoverse-4d.github.io', 'score': 82, 'issue_id': 407, 'pub_date': '2026-01-01', 'pub_date_card': {'ru': '1 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 1', 'zh': '1æœˆ1æ—¥'}, 'hash': '4b85bcdecc1e3e88', 'authors': ['Yuxue Yang', 'Lue Fan', 'Ziqi Shi', 'Junran Peng', 'Feng Wang', 'Zhaoxiang Zhang'], 'affiliations': ['CreateAI', 'NLPR & MAIS, CASIA'], 'pdf_title_img': 'assets/pdf/title_img/2601.00393.jpg', 'data': {'categories': ['#benchmark', '#video', '#3d'], 'emoji': 'ğŸ¬', 'ru': {'title': 'ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ°Ñ 4D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾Ğ¹ ĞºĞ°Ğ¼ĞµÑ€Ñ‹', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° NeoVerse â€” ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ 4D-Ğ¼Ğ¸Ñ€Ğ°, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ°Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ 4D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ½Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² 4D-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‚ Ğ´Ğ¾Ñ€Ğ¾Ğ³Ğ¾ÑÑ‚Ğ¾ÑÑ‰Ğ¸Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ²Ğ¸Ğ´Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ»Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¹ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸. NeoVerse Ñ€ĞµÑˆĞ°ĞµÑ‚ ÑÑ‚Ñƒ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‰ĞµĞ¹ ÑĞ¾ ÑĞºÑ€Ğ¾Ğ¼Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ½Ğ¾ĞºÑƒĞ»ÑÑ€Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ°, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ pose-free feed-forward Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ğ¸ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ñ Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ğ¾Ğ² Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ³Ğ¾ÑÑƒĞ´Ğ°Ñ€ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ° Ğ² Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'NeoVerse: Scalable 4D World Modeling for All!', 'desc': 'This paper introduces NeoVerse, a 4D world model designed for efficient 4D reconstruction and video generation. It addresses the scalability issues found in existing methods that rely on complex multi-view data and extensive pre-processing. NeoVerse utilizes a pose-free feed-forward approach and simulates monocular degradation patterns, allowing it to work effectively with standard monocular videos. As a result, NeoVerse demonstrates superior performance in reconstruction and generation tasks across various applications.'}, 'zh': {'title': 'NeoVerseï¼šå¤šåŠŸèƒ½çš„4Dä¸–ç•Œæ¨¡å‹', 'desc': 'æœ¬æ–‡æå‡ºäº†NeoVerseï¼Œä¸€ä¸ªå¤šåŠŸèƒ½çš„4Dä¸–ç•Œæ¨¡å‹ï¼Œèƒ½å¤Ÿè¿›è¡Œ4Dé‡å»ºã€ç”Ÿæˆæ–°è½¨è¿¹è§†é¢‘ä»¥åŠä¸°å¯Œçš„ä¸‹æ¸¸åº”ç”¨ã€‚æˆ‘ä»¬é¦–å…ˆè¯†åˆ«äº†å½“å‰4Dä¸–ç•Œå»ºæ¨¡æ–¹æ³•åœ¨å¯æ‰©å±•æ€§æ–¹é¢çš„å…±åŒé™åˆ¶ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºæ˜‚è´µä¸”ä¸“ä¸šçš„å¤šè§†è§’4Dæ•°æ®æˆ–ç¹ççš„è®­ç»ƒé¢„å¤„ç†ã€‚ä¸æ­¤ä¸åŒï¼ŒNeoVerseåŸºäºä¸€ç§æ ¸å¿ƒç†å¿µï¼Œä½¿æ•´ä¸ªæµç¨‹èƒ½å¤Ÿæ‰©å±•åˆ°å„ç§é‡å¤–å•ç›®è§†é¢‘ã€‚NeoVerseå…·æœ‰æ— å§¿æ€å‰é¦ˆ4Dé‡å»ºã€åœ¨çº¿å•ç›®é™è§£æ¨¡å¼æ¨¡æ‹Ÿç­‰æŠ€æœ¯ï¼Œèµ‹äºˆå…¶åœ¨å¤šä¸ªé¢†åŸŸçš„å¤šæ ·æ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼ŒåŒæ—¶åœ¨æ ‡å‡†é‡å»ºå’Œç”ŸæˆåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.24615', 'title': 'Youtu-Agent: Scaling Agent Productivity with Automated Generation and Hybrid Policy Optimization', 'url': 'https://huggingface.co/papers/2512.24615', 'abstract': 'Existing Large Language Model (LLM) agent frameworks face two significant challenges: high configuration costs and static capabilities. Building a high-quality agent often requires extensive manual effort in tool integration and prompt engineering, while deployed agents struggle to adapt to dynamic environments without expensive fine-tuning. To address these issues, we propose Youtu-Agent, a modular framework designed for the automated generation and continuous evolution of LLM agents. Youtu-Agent features a structured configuration system that decouples execution environments, toolkits, and context management, enabling flexible reuse and automated synthesis. We introduce two generation paradigms: a Workflow mode for standard tasks and a Meta-Agent mode for complex, non-standard requirements, capable of automatically generating tool code, prompts, and configurations. Furthermore, Youtu-Agent establishes a hybrid policy optimization system: (1) an Agent Practice module that enables agents to accumulate experience and improve performance through in-context optimization without parameter updates; and (2) an Agent RL module that integrates with distributed training frameworks to enable scalable and stable reinforcement learning of any Youtu-Agents in an end-to-end, large-scale manner. Experiments demonstrate that Youtu-Agent achieves state-of-the-art performance on WebWalkerQA (71.47\\%) and GAIA (72.8\\%) using open-weight models. Our automated generation pipeline achieves over 81\\% tool synthesis success rate, while the Practice module improves performance on AIME 2024/2025 by +2.7\\% and +5.4\\% respectively. Moreover, our Agent RL training achieves 40\\% speedup with steady performance improvement on 7B LLMs, enhancing coding/reasoning and searching capabilities respectively up to 35\\% and 21\\% on Maths and general/multi-hop QA benchmarks.', 'score': 81, 'issue_id': 405, 'pub_date': '2026-12-31', 'pub_date_card': {'ru': '31 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 31', 'zh': '12æœˆ31æ—¥'}, 'hash': '40053cb88310e90a', 'authors': ['Yuchen Shi', 'Yuzheng Cai', 'Siqi Cai', 'Zihan Xu', 'Lichao Chen', 'Yulei Qin', 'Zhijian Zhou', 'Xiang Fei', 'Chaofan Qiu', 'Xiaoyu Tan', 'Gang Li', 'Zongyi Li', 'Haojia Lin', 'Guocan Cai', 'Yong Mao', 'Yunsheng Wu', 'Ke Li', 'Xing Sun'], 'affiliations': ['Tencent'], 'pdf_title_img': 'assets/pdf/title_img/2512.24615.jpg', 'data': {'categories': ['#rl', '#reasoning', '#benchmark', '#optimization', '#training', '#agents'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ±ĞµĞ· Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Youtu-Agent â€” Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ñ€ĞµÑˆĞ°ÑÑ‰Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ½Ğ° ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ, Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ³Ğ¸Ğ±ĞºĞ¾ Ğ¿ĞµÑ€ĞµĞ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹ Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´Ğ²Ğ° Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸: Ñ€ĞµĞ¶Ğ¸Ğ¼ Workflow Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ñ€ĞµĞ¶Ğ¸Ğ¼ Meta-Agent Ğ´Ğ»Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ ĞºĞ¾Ğ´Ğ° Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ¸ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¹. ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ°Ñ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Agent Practice Ğ´Ğ»Ñ Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ¿Ñ‹Ñ‚Ğ° Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Agent RL Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ² Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾Ğ¹ ÑÑ€ĞµĞ´Ğµ.'}, 'en': {'title': 'Youtu-Agent: Revolutionizing LLM Agent Flexibility and Performance', 'desc': 'The paper introduces Youtu-Agent, a modular framework that addresses the challenges of high configuration costs and static capabilities in existing Large Language Model (LLM) agent frameworks. It allows for automated generation and continuous evolution of LLM agents through a structured configuration system that separates execution environments, toolkits, and context management. Youtu-Agent employs two generation paradigms: Workflow mode for standard tasks and Meta-Agent mode for complex tasks, enabling the automatic creation of tool code and prompts. Additionally, it features a hybrid policy optimization system that enhances agent performance through in-context optimization and scalable reinforcement learning, achieving state-of-the-art results in various benchmarks.'}, 'zh': {'title': 'Youtu-Agentï¼šè‡ªåŠ¨åŒ–ç”Ÿæˆä¸æŒç»­æ¼”åŒ–çš„LLMä»£ç†æ¡†æ¶', 'desc': 'ç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†æ¡†æ¶é¢ä¸´ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šé«˜é…ç½®æˆæœ¬å’Œé™æ€èƒ½åŠ›ã€‚æ„å»ºé«˜è´¨é‡ä»£ç†é€šå¸¸éœ€è¦å¤§é‡çš„æ‰‹åŠ¨å·¥å…·é›†æˆå’Œæç¤ºå·¥ç¨‹ï¼Œè€Œå·²éƒ¨ç½²çš„ä»£ç†åœ¨åŠ¨æ€ç¯å¢ƒä¸­éš¾ä»¥é€‚åº”ï¼Œä¸”ä¸æ˜“è¿›è¡Œæ˜‚è´µçš„å¾®è°ƒã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Youtu-Agentï¼Œä¸€ä¸ªæ—¨åœ¨è‡ªåŠ¨ç”Ÿæˆå’ŒæŒç»­æ¼”åŒ–LLMä»£ç†çš„æ¨¡å—åŒ–æ¡†æ¶ã€‚Youtu-Agenté€šè¿‡ç»“æ„åŒ–é…ç½®ç³»ç»Ÿå®ç°æ‰§è¡Œç¯å¢ƒã€å·¥å…·åŒ…å’Œä¸Šä¸‹æ–‡ç®¡ç†çš„è§£è€¦ï¼Œæ”¯æŒçµæ´»é‡ç”¨å’Œè‡ªåŠ¨åˆæˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.00664', 'title': 'Avatar Forcing: Real-Time Interactive Head Avatar Generation for Natural Conversation', 'url': 'https://huggingface.co/papers/2601.00664', 'abstract': "A framework called Avatar Forcing uses diffusion forcing and direct preference optimization with synthetic losing samples to enable real-time, expressive multimodal interactions in talking head avatars without labeled data.  \t\t\t\t\tAI-generated summary \t\t\t\t Talking head generation creates lifelike avatars from static portraits for virtual communication and content creation. However, current models do not yet convey the feeling of truly interactive communication, often generating one-way responses that lack emotional engagement. We identify two key challenges toward truly interactive avatars: generating motion in real-time under causal constraints and learning expressive, vibrant reactions without additional labeled data. To address these challenges, we propose Avatar Forcing, a new framework for interactive head avatar generation that models real-time user-avatar interactions through diffusion forcing. This design allows the avatar to process real-time multimodal inputs, including the user's audio and motion, with low latency for instant reactions to both verbal and non-verbal cues such as speech, nods, and laughter. Furthermore, we introduce a direct preference optimization method that leverages synthetic losing samples constructed by dropping user conditions, enabling label-free learning of expressive interaction. Experimental results demonstrate that our framework enables real-time interaction with low latency (approximately 500ms), achieving 6.8X speedup compared to the baseline, and produces reactive and expressive avatar motion, which is preferred over 80% against the baseline.", 'score': 43, 'issue_id': 405, 'pub_date': '2026-01-02', 'pub_date_card': {'ru': '2 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 2', 'zh': '1æœˆ2æ—¥'}, 'hash': 'f2dbe0c7e036ea97', 'authors': ['Taekyung Ki', 'Sangwon Jang', 'Jaehyeong Jo', 'Jaehong Yoon', 'Sung Ju Hwang'], 'affiliations': ['DeepAuto.ai', 'KAIST', 'NTU Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2601.00664.jpg', 'data': {'categories': ['#video', '#optimization', '#training', '#multimodal', '#synthetic', '#diffusion', '#rlhf'], 'emoji': 'ğŸ­', 'ru': {'title': 'Ğ˜Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ°Ğ²Ğ°Ñ‚Ğ°Ñ€Ñ‹ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¸Ğ½ÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ', 'desc': 'Avatar Forcing â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ°Ğ²Ğ°Ñ‚Ğ°Ñ€Ğ¾Ğ² Ñ Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰ĞµĞ¹ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¸Ğ½ÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸ÑÑ… Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ°Ğ²Ğ°Ñ‚Ğ°Ñ€Ñƒ Ñ€ĞµĞ°Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ° Ñ€ĞµÑ‡ÑŒ, ĞºĞ¸Ğ²ĞºĞ¸ Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ½ĞµĞ²ĞµÑ€Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ñ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¾Ñ‚Ñ€Ğ¸Ñ†Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ€Ğ°Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ñ€ĞµĞ°ĞºÑ†Ğ¸ÑĞ¼ Ğ±ĞµĞ· Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ² 6.8 Ñ€Ğ°Ğ·Ğ° Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ 80% Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğµ Ğ² ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¸ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Real-Time, Expressive Interactions with Avatar Forcing', 'desc': 'The paper introduces Avatar Forcing, a novel framework designed to enhance real-time interactions with talking head avatars. It utilizes diffusion forcing and direct preference optimization to generate expressive responses without the need for labeled data. The framework addresses challenges in creating lifelike avatars that can react to both verbal and non-verbal cues in real-time. Experimental results show significant improvements in interaction speed and expressiveness, making avatars more engaging for users.'}, 'zh': {'title': 'å®æ—¶äº¤äº’ï¼Œç”ŸåŠ¨å¤´åƒçš„æœªæ¥', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºAvatar Forcingçš„æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°å®æ—¶ã€å¯Œæœ‰è¡¨ç°åŠ›çš„å¤šæ¨¡æ€äº¤äº’ï¼Œç‰¹åˆ«æ˜¯åœ¨å¯¹è¯å¤´åƒç”Ÿæˆä¸­ã€‚è¯¥æ¡†æ¶ç»“åˆäº†æ‰©æ•£å¼ºåˆ¶å’Œç›´æ¥åå¥½ä¼˜åŒ–ï¼Œèƒ½å¤Ÿåœ¨æ²¡æœ‰æ ‡è®°æ•°æ®çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡åˆæˆçš„å¤±è´¥æ ·æœ¬è¿›è¡Œå­¦ä¹ ã€‚Avatar Forcingèƒ½å¤Ÿå¤„ç†ç”¨æˆ·çš„éŸ³é¢‘å’ŒåŠ¨ä½œè¾“å…¥ï¼Œå®ç°ä½å»¶è¿Ÿçš„å³æ—¶ååº”ï¼Œæå‡äº†äº¤äº’çš„çœŸå®æ„Ÿå’Œæƒ…æ„Ÿå‚ä¸åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å®æ—¶äº¤äº’ä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„é€Ÿåº¦æå‡å’Œæ›´å…·è¡¨ç°åŠ›çš„å¤´åƒåŠ¨ä½œã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.24330', 'title': 'SenseNova-MARS: Empowering Multimodal Agentic Reasoning and Search via Reinforcement Learning', 'url': 'https://huggingface.co/papers/2512.24330', 'abstract': "While Vision-Language Models (VLMs) can solve complex tasks through agentic reasoning, their capabilities remain largely constrained to text-oriented chain-of-thought or isolated tool invocation. They fail to exhibit the human-like proficiency required to seamlessly interleave dynamic tool manipulation with continuous reasoning, particularly in knowledge-intensive and visually complex scenarios that demand coordinated external tools such as search and image cropping. In this work, we introduce SenseNova-MARS, a novel Multimodal Agentic Reasoning and Search framework that empowers VLMs with interleaved visual reasoning and tool-use capabilities via reinforcement learning (RL). Specifically, SenseNova-MARS dynamically integrates the image search, text search, and image crop tools to tackle fine-grained and knowledge-intensive visual understanding challenges. In the RL stage, we propose the Batch-Normalized Group Sequence Policy Optimization (BN-GSPO) algorithm to improve the training stability and advance the model's ability to invoke tools and reason effectively. To comprehensively evaluate the agentic VLMs on complex visual tasks, we introduce the HR-MMSearch benchmark, the first search-oriented benchmark composed of high-resolution images with knowledge-intensive and search-driven questions. Experiments demonstrate that SenseNova-MARS achieves state-of-the-art performance on open-source search and fine-grained image understanding benchmarks. Specifically, on search-oriented benchmarks, SenseNova-MARS-8B scores 67.84 on MMSearch and 41.64 on HR-MMSearch, surpassing proprietary models such as Gemini-3-Flash and GPT-5. SenseNova-MARS represents a promising step toward agentic VLMs by providing effective and robust tool-use capabilities. To facilitate further research in this field, we will release all code, models, and datasets.", 'score': 29, 'issue_id': 405, 'pub_date': '2026-12-30', 'pub_date_card': {'ru': '30 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 30', 'zh': '12æœˆ30æ—¥'}, 'hash': 'ff90ddac66fbb641', 'authors': ['Yong Xien Chng', 'Tao Hu', 'Wenwen Tong', 'Xueheng Li', 'Jiandong Chen', 'Haojia Yu', 'Jiefan Lu', 'Hewei Guo', 'Hanming Deng', 'Chengjun Xie', 'Gao Huang', 'Dahua Lin', 'Lewei Lu'], 'affiliations': ['SenseTime Research', 'Tsinghua University', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2512.24330.jpg', 'data': {'categories': ['#rl', '#reasoning', '#benchmark', '#dataset', '#cv', '#open_source', '#optimization', '#training', '#multimodal', '#agents'], 'emoji': 'ğŸ”', 'ru': {'title': 'ĞĞ³ĞµĞ½Ñ‚ÑĞºĞ¾Ğµ Ğ²Ğ¸Ğ´ĞµĞ½Ğ¸Ğµ: Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° SenseNova-MARS â€” Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚ÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² (Ğ¿Ğ¾Ğ¸ÑĞº Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¿Ğ¾Ğ¸ÑĞº Ñ‚ĞµĞºÑÑ‚Ğ°, ĞºĞ°Ğ´Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ) Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ BN-GSPO Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑĞ¼. Ğ”Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ°Ğ³ĞµĞ½Ñ‚ÑĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ²ĞµĞ´ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº HR-MMSearch, ÑĞ¾ÑÑ‚Ğ¾ÑÑ‰Ğ¸Ğ¹ Ğ¸Ğ· Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ ÑĞ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼Ğ¸, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ SenseNova-MARS Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ñ… Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ´Ğ°Ğ¶Ğµ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Gemini Ğ¸ GPT-5.'}, 'en': {'title': 'Empowering VLMs with Dynamic Tool Manipulation and Reasoning', 'desc': "This paper presents SenseNova-MARS, a new framework designed to enhance Vision-Language Models (VLMs) by enabling them to perform dynamic tool manipulation alongside continuous reasoning. Unlike traditional VLMs that struggle with complex tasks requiring coordinated tool use, SenseNova-MARS integrates image search, text search, and image cropping tools through reinforcement learning. The authors introduce a novel training algorithm called Batch-Normalized Group Sequence Policy Optimization (BN-GSPO) to improve the model's stability and effectiveness in invoking tools. The framework is evaluated using the HR-MMSearch benchmark, demonstrating superior performance in knowledge-intensive visual tasks compared to existing models."}, 'zh': {'title': 'æå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„æ™ºèƒ½æ¨ç†èƒ½åŠ›', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„å¤šæ¨¡æ€æ™ºèƒ½æ¨ç†å’Œæœç´¢æ¡†æ¶SenseNova-MARSï¼Œæ—¨åœ¨æå‡è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨åŠ¨æ€å·¥å…·æ“ä½œä¸è¿ç»­æ¨ç†ä¸­çš„èƒ½åŠ›ã€‚é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼ŒSenseNova-MARSèƒ½å¤Ÿå°†å›¾åƒæœç´¢ã€æ–‡æœ¬æœç´¢å’Œå›¾åƒè£å‰ªå·¥å…·åŠ¨æ€æ•´åˆï¼Œä»¥åº”å¯¹å¤æ‚çš„è§†è§‰ç†è§£ä»»åŠ¡ã€‚æˆ‘ä»¬æå‡ºçš„Batch-Normalized Group Sequence Policy Optimizationï¼ˆBN-GSPOï¼‰ç®—æ³•æé«˜äº†è®­ç»ƒçš„ç¨³å®šæ€§ï¼Œå¢å¼ºäº†æ¨¡å‹è°ƒç”¨å·¥å…·å’Œæœ‰æ•ˆæ¨ç†çš„èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSenseNova-MARSåœ¨å¼€æ”¾æºä»£ç çš„æœç´¢å’Œç»†ç²’åº¦å›¾åƒç†è§£åŸºå‡†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå±•ç¤ºäº†å…¶åœ¨æ™ºèƒ½VLMsé¢†åŸŸçš„æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.24271', 'title': "Taming Hallucinations: Boosting MLLMs' Video Understanding via Counterfactual Video Generation", 'url': 'https://huggingface.co/papers/2512.24271', 'abstract': 'Multimodal Large Language Models (MLLMs) have made remarkable progress in video understanding. However, they suffer from a critical vulnerability: an over-reliance on language priors, which can lead to visual ungrounded hallucinations, especially when processing counterfactual videos that defy common sense. This limitation, stemming from the intrinsic data imbalance between text and video, is challenging to address due to the substantial cost of collecting and annotating counterfactual data. To address this, we introduce DualityForge, a novel counterfactual data synthesis framework that employs controllable, diffusion-based video editing to transform real-world videos into counterfactual scenarios. By embedding structured contextual information into the video editing and QA generation processes, the framework automatically produces high-quality QA pairs together with original-edited video pairs for contrastive training. Based on this, we build DualityVidQA, a large-scale video dataset designed to reduce MLLM hallucinations. In addition, to fully exploit the contrastive nature of our paired data, we propose Duality-Normalized Advantage Training (DNA-Train), a two-stage SFT-RL training regime where the RL phase applies pair-wise ell_1 advantage normalization, thereby enabling a more stable and efficient policy optimization. Experiments on DualityVidQA-Test demonstrate that our method substantially reduces model hallucinations on counterfactual videos, yielding a relative improvement of 24.0% over the Qwen2.5-VL-7B baseline. Moreover, our approach achieves significant gains across both hallucination and general-purpose benchmarks, indicating strong generalization capability. We will open-source our dataset and code.', 'score': 24, 'issue_id': 405, 'pub_date': '2026-12-30', 'pub_date_card': {'ru': '30 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 30', 'zh': '12æœˆ30æ—¥'}, 'hash': '01479f4c4234799d', 'authors': ['Zhe Huang', 'Hao Wen', 'Aiming Hao', 'Bingze Song', 'Meiqi Wu', 'Jiahong Wu', 'Xiangxiang Chu', 'Sheng Lu', 'Haoqian Wang'], 'affiliations': ['AMAP, Alibaba Group', 'Beihang University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2512.24271.jpg', 'data': {'categories': ['#rl', '#dataset', '#video', '#open_source', '#training', '#multimodal', '#synthetic', '#diffusion', '#hallucinations'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ‘Ğ¾Ñ€ÑŒĞ±Ğ° Ñ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ‡ĞµÑ€ĞµĞ· ÑĞ¸Ğ½Ñ‚ĞµĞ· ĞºĞ¾Ğ½Ñ‚Ñ€Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…', 'desc': 'Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ‡Ñ€ĞµĞ·Ğ¼ĞµÑ€Ğ½Ğ¾ Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ÑÑ‚ÑÑ Ğ½Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ DualityForge â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° ĞºĞ¾Ğ½Ñ‚Ñ€Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ½ĞµĞ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ² Ğ¸Ğ· Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ DualityVidQA Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ DNA-Train, ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ supervised fine-tuning Ğ¸ reinforcement learning Ñ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¿Ğ°Ñ€Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ½Ğ° 24% Ğ¸ Ñ…Ğ¾Ñ€Ğ¾ÑˆÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ….'}, 'en': {'title': 'Transforming Video Understanding with Counterfactual Data Synthesis', 'desc': 'This paper addresses the limitations of Multimodal Large Language Models (MLLMs) in video understanding, particularly their tendency to generate visual hallucinations when faced with counterfactual scenarios. The authors introduce DualityForge, a framework that synthesizes counterfactual data through advanced video editing techniques, allowing for the creation of high-quality question-answer pairs. They also present DualityVidQA, a large-scale dataset aimed at training MLLMs to reduce hallucinations by providing contrastive examples. Additionally, the proposed Duality-Normalized Advantage Training (DNA-Train) enhances the training process, leading to significant improvements in model performance on both hallucination and general benchmarks.'}, 'zh': {'title': 'å‡å°‘å¹»è§‰ï¼Œæå‡è§†é¢‘ç†è§£çš„åŒé‡åŠ›é‡', 'desc': 'å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†é¢‘ç†è§£æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å®ƒä»¬å­˜åœ¨ä¸€ä¸ªå…³é”®çš„å¼±ç‚¹ï¼šè¿‡åº¦ä¾èµ–è¯­è¨€å…ˆéªŒï¼Œå¯¼è‡´åœ¨å¤„ç†è¿åå¸¸è¯†çš„åäº‹å®è§†é¢‘æ—¶å‡ºç°è§†è§‰æ— æ ¹çš„å¹»è§‰ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†DualityForgeï¼Œä¸€ä¸ªæ–°é¢–çš„åäº‹å®æ•°æ®åˆæˆæ¡†æ¶ï¼Œé€šè¿‡å¯æ§çš„æ‰©æ•£è§†é¢‘ç¼–è¾‘å°†çœŸå®è§†é¢‘è½¬åŒ–ä¸ºåäº‹å®åœºæ™¯ã€‚è¯¥æ¡†æ¶åœ¨è§†é¢‘ç¼–è¾‘å’Œé—®ç­”ç”Ÿæˆè¿‡ç¨‹ä¸­åµŒå…¥ç»“æ„åŒ–ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œè‡ªåŠ¨ç”Ÿæˆé«˜è´¨é‡çš„é—®ç­”å¯¹åŠåŸå§‹-ç¼–è¾‘è§†é¢‘å¯¹ï¼Œä»¥ä¾¿è¿›è¡Œå¯¹æ¯”è®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—å‡å°‘äº†æ¨¡å‹åœ¨åäº‹å®è§†é¢‘ä¸Šçš„å¹»è§‰ï¼Œæ˜¾ç¤ºå‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.00796', 'title': 'AdaGaR: Adaptive Gabor Representation for Dynamic Scene Reconstruction', 'url': 'https://huggingface.co/papers/2601.00796', 'abstract': 'Reconstructing dynamic 3D scenes from monocular videos requires simultaneously capturing high-frequency appearance details and temporally continuous motion. Existing methods using single Gaussian primitives are limited by their low-pass filtering nature, while standard Gabor functions introduce energy instability. Moreover, lack of temporal continuity constraints often leads to motion artifacts during interpolation. We propose AdaGaR, a unified framework addressing both frequency adaptivity and temporal continuity in explicit dynamic scene modeling. We introduce Adaptive Gabor Representation, extending Gaussians through learnable frequency weights and adaptive energy compensation to balance detail capture and stability. For temporal continuity, we employ Cubic Hermite Splines with Temporal Curvature Regularization to ensure smooth motion evolution. An Adaptive Initialization mechanism combining depth estimation, point tracking, and foreground masks establishes stable point cloud distributions in early training. Experiments on Tap-Vid DAVIS demonstrate state-of-the-art performance (PSNR 35.49, SSIM 0.9433, LPIPS 0.0723) and strong generalization across frame interpolation, depth consistency, video editing, and stereo view synthesis. Project page: https://jiewenchan.github.io/AdaGaR/', 'score': 22, 'issue_id': 408, 'pub_date': '2026-01-02', 'pub_date_card': {'ru': '2 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 2', 'zh': '1æœˆ2æ—¥'}, 'hash': 'eb50cf753bd4888a', 'authors': ['Jiewen Chan', 'Zhenjun Zhao', 'Yu-Lun Liu'], 'affiliations': ['National Yang Ming Chiao Tung University', 'University of Zaragoza'], 'pdf_title_img': 'assets/pdf/title_img/2601.00796.jpg', 'data': {'categories': ['#video', '#3d', '#architecture'], 'emoji': 'ğŸ¬', 'ru': {'title': 'ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ“Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ»Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ†ĞµĞ½ Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ AdaGaR Ğ´Ğ»Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… 3D ÑÑ†ĞµĞ½ Ğ¸Ğ· Ğ¼Ğ¾Ğ½Ğ¾ĞºÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°ĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ³Ğ°ÑƒÑÑĞ¸Ğ°Ğ½Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ“Ğ°Ğ±Ğ¾Ñ€Ğ° Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ²ĞµÑĞ°Ğ¼Ğ¸ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚ Ğ¸ ĞºĞ¾Ğ¼Ğ¿ĞµĞ½ÑĞ°Ñ†Ğ¸ĞµĞ¹ ÑĞ½ĞµÑ€Ğ³Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ñ‹ Ğ±ĞµĞ· Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ğ¾Ğ² Ğ½ĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. Ğ”Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ÑÑ ĞºÑƒĞ±Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ¿Ğ»Ğ°Ğ¹Ğ½Ñ‹ Ğ­Ñ€Ğ¼Ğ¸Ñ‚Ğ° Ñ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ ĞºÑ€Ğ¸Ğ²Ğ¸Ğ·Ğ½Ñ‹. ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹, Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡ĞµĞº Ğ¸ Ğ¼Ğ°ÑĞºĞ¸ Ğ¿ĞµÑ€ĞµĞ´Ğ½ĞµĞ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ°, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±Ğ»Ğ°ĞºĞ° Ñ‚Ğ¾Ñ‡ĞµĞº Ğ½Ğ° Ñ€Ğ°Ğ½Ğ½Ğ¸Ñ… ÑÑ‚Ğ°Ğ¿Ğ°Ñ… Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'AdaGaR: Enhancing 3D Scene Reconstruction with Adaptive Gabor Representation', 'desc': 'This paper presents AdaGaR, a new framework for reconstructing dynamic 3D scenes from single videos. It improves upon existing methods by using Adaptive Gabor Representation, which allows for better detail capture and stability through learnable frequency weights. To ensure smooth motion over time, the framework incorporates Cubic Hermite Splines with Temporal Curvature Regularization. The results show that AdaGaR achieves state-of-the-art performance in various tasks, demonstrating its effectiveness in dynamic scene modeling.'}, 'zh': {'title': 'è‡ªé€‚åº”Gaboré‡å»ºåŠ¨æ€3Dåœºæ™¯', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºAdaGaRçš„ç»Ÿä¸€æ¡†æ¶ï¼Œç”¨äºä»å•ç›®è§†é¢‘é‡å»ºåŠ¨æ€3Dåœºæ™¯ã€‚è¯¥æ–¹æ³•é€šè¿‡è‡ªé€‚åº”Gaborè¡¨ç¤ºï¼Œåˆ©ç”¨å¯å­¦ä¹ çš„é¢‘ç‡æƒé‡å’Œèƒ½é‡è¡¥å¿æ¥å¹³è¡¡ç»†èŠ‚æ•æ‰å’Œç¨³å®šæ€§ã€‚ä¸ºäº†ç¡®ä¿æ—¶é—´è¿ç»­æ€§ï¼Œé‡‡ç”¨äº†å¸¦æœ‰æ—¶é—´æ›²ç‡æ­£åˆ™åŒ–çš„ä¸‰æ¬¡Hermiteæ ·æ¡ï¼Œä»¥å®ç°å¹³æ»‘çš„è¿åŠ¨æ¼”å˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAdaGaRåœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.24695', 'title': 'Nested Learning: The Illusion of Deep Learning Architectures', 'url': 'https://huggingface.co/papers/2512.24695', 'abstract': "Despite the recent progresses, particularly in developing Language Models, there are fundamental challenges and unanswered questions about how such models can continually learn/memorize, self-improve, and find effective solutions. In this paper, we present a new learning paradigm, called Nested Learning (NL), that coherently represents a machine learning model with a set of nested, multi-level, and/or parallel optimization problems, each of which with its own context flow. Through the lenses of NL, existing deep learning methods learns from data through compressing their own context flow, and in-context learning naturally emerges in large models. NL suggests a philosophy to design more expressive learning algorithms with more levels, resulting in higher-order in-context learning and potentially unlocking effective continual learning capabilities. We advocate for NL by presenting three core contributions: (1) Expressive Optimizers: We show that known gradient-based optimizers, such as Adam, SGD with Momentum, etc., are in fact associative memory modules that aim to compress the gradients' information (by gradient descent). Building on this insight, we present other more expressive optimizers with deep memory and/or more powerful learning rules; (2) Self-Modifying Learning Module: Taking advantage of NL's insights on learning algorithms, we present a sequence model that learns how to modify itself by learning its own update algorithm; and (3) Continuum Memory System: We present a new formulation for memory system that generalizes the traditional viewpoint of long/short-term memory. Combining our self-modifying sequence model with the continuum memory system, we present a continual learning module, called Hope, showing promising results in language modeling, knowledge incorporation, and few-shot generalization tasks, continual learning, and long-context reasoning tasks.", 'score': 19, 'issue_id': 407, 'pub_date': '2026-12-31', 'pub_date_card': {'ru': '31 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 31', 'zh': '12æœˆ31æ—¥'}, 'hash': '52823ab1600cf692', 'authors': ['Ali Behrouz', 'Meisam Razaviyayn', 'Peilin Zhong', 'Vahab Mirrokni'], 'affiliations': ['Columbia University', 'Google'], 'pdf_title_img': 'assets/pdf/title_img/2512.24695.jpg', 'data': {'categories': ['#reasoning', '#training', '#architecture', '#long_context', '#optimization'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Ğ’Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ: Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° ÑĞ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸ĞµÑÑ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Nested Learning (Ğ²Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ĞºĞ°Ğº Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ²Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ°Ğ¼Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ñ‹Ğµ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ñ‹ (Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Adam Ğ¸ SGD Ñ Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ¼) ÑĞ²Ğ»ÑÑÑ‚ÑÑ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑĞ¼Ğ¸ Ğ°ÑÑĞ¾Ñ†Ğ¸Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸, ÑĞ¶Ğ¸Ğ¼Ğ°ÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ°Ñ…. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ insight Ğ¾Ğ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹Ñ€Ğ°Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ñ‹ Ñ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ Ğ¸ ÑĞ°Ğ¼Ğ¾Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒÑÑ‰Ğ¸ĞµÑÑ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ÑƒĞ»Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¸Ğ·Ğ¼ĞµĞ½ÑÑ‚ÑŒ ÑĞ²Ğ¾Ğ¹ ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ. ĞšĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒÑ ÑÑ‚Ğ¸ Ğ¸Ğ´ĞµĞ¸, Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Hope Ğ´Ğ»Ñ continual learning, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸, Ğ´Ğ¾Ğ»Ğ³Ğ¾ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¸ Ğ¸ few-shot Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¸.'}, 'en': {'title': 'Unlocking Continual Learning with Nested Learning Paradigm', 'desc': 'This paper introduces a new learning approach called Nested Learning (NL), which organizes machine learning models into a structure of nested optimization problems. NL allows models to learn from their own context flows, enhancing their ability to perform in-context learning and continual learning. The authors propose three main contributions: expressive optimizers that act as memory modules, a self-modifying learning module that adapts its own learning rules, and a continuum memory system that improves memory management. Together, these innovations aim to create more powerful and flexible learning algorithms that can better handle complex tasks in language modeling and reasoning.'}, 'zh': {'title': 'åµŒå¥—å­¦ä¹ ï¼šè§£é”æŒç»­å­¦ä¹ çš„æ–°èŒƒå¼', 'desc': 'å°½ç®¡æœ€è¿‘åœ¨è¯­è¨€æ¨¡å‹çš„å‘å±•ä¸Šå–å¾—äº†è¿›å±•ï¼Œä½†ä»ç„¶å­˜åœ¨ä¸€äº›åŸºæœ¬æŒ‘æˆ˜å’Œæœªè§£çš„é—®é¢˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å­¦ä¹ èŒƒå¼ï¼Œç§°ä¸ºåµŒå¥—å­¦ä¹ ï¼ˆNested Learningï¼ŒNLï¼‰ï¼Œå®ƒé€šè¿‡ä¸€ç»„åµŒå¥—çš„å¤šå±‚æ¬¡å’Œ/æˆ–å¹¶è¡Œä¼˜åŒ–é—®é¢˜æ¥ä¸€è‡´åœ°è¡¨ç¤ºæœºå™¨å­¦ä¹ æ¨¡å‹ã€‚NLæä¾›äº†ä¸€ç§è®¾è®¡æ›´å…·è¡¨ç°åŠ›çš„å­¦ä¹ ç®—æ³•çš„å“²å­¦ï¼Œèƒ½å¤Ÿå®ç°æ›´é«˜é˜¶çš„ä¸Šä¸‹æ–‡å­¦ä¹ ï¼Œå¹¶å¯èƒ½è§£é”æœ‰æ•ˆçš„æŒç»­å­¦ä¹ èƒ½åŠ›ã€‚æˆ‘ä»¬é€šè¿‡ä¸‰ä¸ªæ ¸å¿ƒè´¡çŒ®æ¥æ”¯æŒNLçš„è§‚ç‚¹ï¼ŒåŒ…æ‹¬è¡¨ç°åŠ›ä¼˜åŒ–å™¨ã€è‡ªæˆ‘ä¿®æ”¹å­¦ä¹ æ¨¡å—å’Œè¿ç»­è®°å¿†ç³»ç»Ÿã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.00417', 'title': 'Deep Delta Learning', 'url': 'https://huggingface.co/papers/2601.00417', 'abstract': "The efficacy of deep residual networks is fundamentally predicated on the identity shortcut connection. While this mechanism effectively mitigates the vanishing gradient problem, it imposes a strictly additive inductive bias on feature transformations, thereby limiting the network's capacity to model complex state transitions. In this paper, we introduce Deep Delta Learning (DDL), a novel architecture that generalizes the standard residual connection by modulating the identity shortcut with a learnable, data-dependent geometric transformation. This transformation, termed the Delta Operator, constitutes a rank-1 perturbation of the identity matrix, parameterized by a reflection direction vector k(X) and a gating scalar Î²(X). We provide a spectral analysis of this operator, demonstrating that the gate Î²(X) enables dynamic interpolation between identity mapping, orthogonal projection, and geometric reflection. Furthermore, we restructure the residual update as a synchronous rank-1 injection, where the gate acts as a dynamic step size governing both the erasure of old information and the writing of new features. This unification empowers the network to explicitly control the spectrum of its layer-wise transition operator, enabling the modeling of complex, non-monotonic dynamics while preserving the stable training characteristics of gated residual architectures.", 'score': 18, 'issue_id': 405, 'pub_date': '2026-01-01', 'pub_date_card': {'ru': '1 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 1', 'zh': '1æœˆ1æ—¥'}, 'hash': 'b59181a06be243db', 'authors': ['Yifan Zhang', 'Yifeng Liu', 'Mengdi Wang', 'Quanquan Gu'], 'affiliations': ['Princeton University', 'University of California, Los Angeles'], 'pdf_title_img': 'assets/pdf/title_img/2601.00417.jpg', 'data': {'categories': ['#architecture'], 'emoji': 'ğŸ”„', 'ru': {'title': 'Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ ÑĞ²ÑĞ·Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Deep Delta Learning (DDL) â€” Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑĞ²ÑĞ·ĞµĞ¹ Ğ² Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ÑÑ… Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ¾Ğ´ÑƒĞ»ÑÑ†Ğ¸Ñ Ñ‚Ğ¾Ğ¶Ğ´ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ğ¼ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼. Ğ”ĞµĞ»ÑŒÑ‚Ğ°-Ğ¾Ğ¿ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ñ€Ğ°Ğ½Ğ³-1 Ğ²Ğ¾Ğ·Ğ¼ÑƒÑ‰ĞµĞ½Ğ¸Ğµ ĞµĞ´Ğ¸Ğ½Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ñ‹, Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ¼ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞºĞ°Ğ»ÑÑ€Ğ½Ğ¾Ğ¹ Ğ²ĞµĞ½Ñ‚Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ„ÑƒĞ½ĞºÑ†Ğ¸ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ·Ğ°Ğ²Ğ¸ÑÑÑ‚ Ğ¾Ñ‚ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ¡Ğ¿ĞµĞºÑ‚Ñ€Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ²ĞµĞ½Ñ‚Ğ¸Ğ»ÑŒ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚Ğ¾Ğ¶Ğ´ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ĞµĞ¼, Ğ¾Ñ€Ñ‚Ğ¾Ğ³Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾ĞµĞºÑ†Ğ¸ĞµĞ¹ Ğ¸ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¾Ñ‚Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ĞµĞ¼. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞµÑ‚Ğ¸ ÑĞ²Ğ½Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ¿ĞµĞºÑ‚Ñ€ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ¾Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ»Ğ¾ÑĞ¼Ğ¸, Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒÑ ÑĞ»Ğ¾Ğ¶Ğ½ÑƒÑ Ğ½ĞµĞ»Ğ¸Ğ½ĞµĞ¹Ğ½ÑƒÑ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºÑƒ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Empowering Residual Networks with Dynamic Transformations', 'desc': "This paper presents Deep Delta Learning (DDL), a new architecture that enhances deep residual networks by introducing a learnable transformation to the identity shortcut connection. The Delta Operator, a key component of DDL, allows for dynamic adjustments to feature transformations, overcoming limitations of traditional residual connections. By using a gating scalar and a reflection direction vector, DDL can interpolate between various transformations, improving the network's ability to model complex dynamics. This approach maintains the stability of training while enabling richer feature representation and more effective learning of non-linear relationships."}, 'zh': {'title': 'æ·±åº¦å¢é‡å­¦ä¹ ï¼šè¶…è¶Šä¼ ç»Ÿæ®‹å·®è¿æ¥çš„åˆ›æ–°', 'desc': 'æ·±åº¦æ®‹å·®ç½‘ç»œçš„æœ‰æ•ˆæ€§ä¾èµ–äºèº«ä»½å¿«æ·è¿æ¥ã€‚è™½ç„¶è¿™ç§æœºåˆ¶æœ‰æ•ˆåœ°ç¼“è§£äº†æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼Œä½†å®ƒå¯¹ç‰¹å¾å˜æ¢æ–½åŠ äº†ä¸¥æ ¼çš„åŠ æ³•å½’çº³åç½®ï¼Œä»è€Œé™åˆ¶äº†ç½‘ç»œå»ºæ¨¡å¤æ‚çŠ¶æ€è½¬å˜çš„èƒ½åŠ›ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ¶æ„â€”â€”æ·±åº¦å¢é‡å­¦ä¹ ï¼ˆDDLï¼‰ï¼Œé€šè¿‡å¯å­¦ä¹ çš„æ•°æ®ä¾èµ–å‡ ä½•å˜æ¢æ¥æ¨å¹¿æ ‡å‡†æ®‹å·®è¿æ¥ã€‚æˆ‘ä»¬å¼•å…¥çš„å¢é‡ç®—å­ä½¿ç½‘ç»œèƒ½å¤ŸåŠ¨æ€æ§åˆ¶å±‚é—´è¿‡æ¸¡æ“ä½œçš„è°±ï¼Œä»è€Œå»ºæ¨¡å¤æ‚çš„éå•è°ƒåŠ¨æ€ï¼ŒåŒæ—¶ä¿æŒé—¨æ§æ®‹å·®æ¶æ„çš„ç¨³å®šè®­ç»ƒç‰¹æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.00747', 'title': 'The Reasoning-Creativity Trade-off: Toward Creativity-Driven Problem Solving', 'url': 'https://huggingface.co/papers/2601.00747', 'abstract': "Large language model training methods that optimize for correctness can cause reasoning path diversity collapse, but a new variational framework provides principled solutions to maintain both accuracy and creativity.  \t\t\t\t\tAI-generated summary \t\t\t\t State-of-the-art large language model (LLM) pipelines rely on bootstrapped reasoning loops: sampling diverse chains of thought and reinforcing the highest-scoring ones, mainly optimizing correctness. We analyze how this design choice is sensitive to the collapse of the model's distribution over reasoning paths, slashing semantic entropy and undermining creative problem-solving. To analyze this failure, we introduce Distributional Creative Reasoning (DCR), a unified variational objective that casts training as gradient flow through probability measures on solution traces. STaR, GRPO, and DPO, as well as entropy bonuses, and other methods, all constitute special cases of the same loss. The framework delivers three core results: (i) the diversity decay theorem, describing how correctness-based objectives lead to distinct modes of diversity decay for STaR, GRPO, and DPO; (ii) designs that ensure convergence to a stable and diverse policy, effectively preventing collapse; and (iii) simple, actionable recipes to achieve this in practice. DCR thus offers the first principled recipe for LLMs that remain both correct and creative.", 'score': 9, 'issue_id': 416, 'pub_date': '2026-01-02', 'pub_date_card': {'ru': '2 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 2', 'zh': '1æœˆ2æ—¥'}, 'hash': '9a47601dbf8f88ab', 'authors': ['Max Ruiz Luyten', 'Mihaela van der Schaar'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2601.00747.jpg', 'data': {'categories': ['#reasoning', '#optimization', '#interpretability', '#training', '#rlhf', '#architecture'], 'emoji': 'âš¡', 'ru': {'title': 'Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ñ‚Ğ²Ğ¾Ñ€Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ñ€Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´ÑÑ‚ Ğº ĞºĞ¾Ğ»Ğ»Ğ°Ğ¿ÑÑƒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ¿ÑƒÑ‚ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ‚Ğ²Ğ¾Ñ€Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Distributional Creative Reasoning (DCR) â€” ĞµĞ´Ğ¸Ğ½ÑƒÑ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ñ†ĞµĞ»ĞµĞ²ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ĞºĞ°Ğº Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ñ‚Ğ¾Ğº Ñ‡ĞµÑ€ĞµĞ· Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ (STaR, GRPO, DPO) ĞºĞ°Ğº Ñ‡Ğ°ÑÑ‚Ğ½Ñ‹Ğµ ÑĞ»ÑƒÑ‡Ğ°Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ¸ Ğ´Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ‚ĞµĞ¾Ñ€ĞµĞ¼Ñƒ Ğ¾ Ñ€Ğ°ÑĞ¿Ğ°Ğ´Ğµ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ, Ğ¾Ğ±ÑŠÑÑĞ½ÑÑÑ‰ÑƒÑ ÑÑ‚Ğ¾Ñ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµÑ†ĞµĞ¿Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸ĞµĞ¼ Ñ‚Ğ²Ğ¾Ñ€Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ² LLM.'}, 'en': {'title': 'Balancing Accuracy and Creativity in Language Models', 'desc': 'This paper addresses a problem in training large language models (LLMs) where focusing too much on correctness can reduce the diversity of reasoning paths. The authors introduce a new framework called Distributional Creative Reasoning (DCR) that helps maintain both accuracy and creativity in LLMs. By analyzing how traditional methods lead to a collapse in reasoning diversity, they provide solutions that ensure a stable and diverse policy during training. The framework also offers practical strategies to implement these solutions effectively in LLM pipelines.'}, 'zh': {'title': 'ä¿æŒå‡†ç¡®æ€§ä¸åˆ›é€ æ€§çš„å¹³è¡¡', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä¼˜åŒ–æ­£ç¡®æ€§æ—¶å¯èƒ½å¯¼è‡´æ¨ç†è·¯å¾„å¤šæ ·æ€§å´©æºƒçš„é—®é¢˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„å˜åˆ†æ¡†æ¶ï¼Œç§°ä¸ºåˆ†å¸ƒå¼åˆ›é€ æ€§æ¨ç†ï¼ˆDCRï¼‰ï¼Œæ—¨åœ¨åŒæ—¶ä¿æŒæ¨¡å‹çš„å‡†ç¡®æ€§å’Œåˆ›é€ åŠ›ã€‚é€šè¿‡åˆ†æä¸åŒè®­ç»ƒæ–¹æ³•çš„å½±å“ï¼Œæˆ‘ä»¬å‘ç°åŸºäºæ­£ç¡®æ€§çš„ç›®æ ‡ä¼šå¯¼è‡´å¤šæ ·æ€§è¡°é€€ï¼Œå¹¶æå‡ºäº†é˜²æ­¢è¿™ç§å´©æºƒçš„è®¾è®¡æ–¹æ¡ˆã€‚DCRä¸ºå®ç°æ—¢æ­£ç¡®åˆå¯Œæœ‰åˆ›é€ æ€§çš„LLMæä¾›äº†é¦–ä¸ªç³»ç»ŸåŒ–çš„è§£å†³æ–¹æ¡ˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.22955', 'title': 'Diversity or Precision? A Deep Dive into Next Token Prediction', 'url': 'https://huggingface.co/papers/2512.22955', 'abstract': "Recent advancements have shown that reinforcement learning (RL) can substantially improve the reasoning abilities of large language models (LLMs). The effectiveness of such RL training, however, depends critically on the exploration space defined by the pre-trained model's token-output distribution. In this paper, we revisit the standard cross-entropy loss, interpreting it as a specific instance of policy gradient optimization applied within a single-step episode. To systematically study how the pre-trained distribution shapes the exploration potential for subsequent RL, we propose a generalized pre-training objective that adapts on-policy RL principles to supervised learning. By framing next-token prediction as a stochastic decision process, we introduce a reward-shaping strategy that explicitly balances diversity and precision. Our method employs a positive reward scaling factor to control probability concentration on ground-truth tokens and a rank-aware mechanism that treats high-ranking and low-ranking negative tokens asymmetrically. This allows us to reshape the pre-trained token-output distribution and investigate how to provide a more favorable exploration space for RL, ultimately enhancing end-to-end reasoning performance. Contrary to the intuition that higher distribution entropy facilitates effective exploration, we find that imposing a precision-oriented prior yields a superior exploration space for RL.", 'score': 4, 'issue_id': 410, 'pub_date': '2026-12-28', 'pub_date_card': {'ru': '28 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 28', 'zh': '12æœˆ28æ—¥'}, 'hash': '77fedbb4aebb0ea1', 'authors': ['Haoyuan Wu', 'Hai Wang', 'Jiajia Wu', 'Jinxiang Ou', 'Keyao Wang', 'Weile Chen', 'Zihao Zheng', 'Bei Yu'], 'affiliations': ['Tencent', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2512.22955.jpg', 'data': {'categories': ['#reasoning', '#training', '#optimization', '#rl'], 'emoji': 'ğŸ”', 'ru': {'title': 'ĞŸĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ, ĞºĞ°Ğº Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ»Ğ¸ÑĞµÑ‚ Ğ½Ğ° ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿ĞµÑ€ĞµĞ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒÑÑ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ ĞºÑ€Ğ¾ÑÑ-ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸ ĞºĞ°Ğº Ñ‡Ğ°ÑÑ‚Ğ½Ñ‹Ğ¹ ÑĞ»ÑƒÑ‡Ğ°Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ñ‘Ğ½Ğ½ÑƒÑ Ñ†ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒÑÑ‰ÑƒÑ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ñ‹ on-policy RL Ğº ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ° ĞºĞ°Ğº ÑÑ‚Ğ¾Ñ…Ğ°ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸ĞµĞ¼ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ shaping Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ°ÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡Ğ½Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ½Ğ³Ğ¾Ğ². ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾Ñ€ĞµÑ‡Ğ¸Ñ‚ Ğ¸Ğ½Ñ‚ÑƒĞ¸Ñ†Ğ¸Ğ¸: Ğ±Ğ¾Ğ»ĞµĞµ ÑĞºĞ¾Ğ½Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ (Ğ½Ğ¸Ğ·ĞºĞ°Ñ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ñ) Ñ Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ½Ğ° Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ Ğ»ÑƒÑ‡ÑˆĞµĞµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ RL Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¸Ñ‚Ğ¾Ğ³Ğ¾Ğ²ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°.'}, 'en': {'title': 'Enhancing Language Models: Precision Over Diversity in Exploration', 'desc': 'This paper explores how reinforcement learning (RL) can enhance the reasoning capabilities of large language models (LLMs) by improving their exploration strategies. It critiques the traditional cross-entropy loss, viewing it as a form of policy gradient optimization in a single-step context. The authors propose a new pre-training objective that integrates on-policy RL concepts into supervised learning, allowing for better exploration of the token-output distribution. Their findings suggest that a focus on precision, rather than just diversity, in the token distribution leads to more effective exploration and improved reasoning performance in LLMs.'}, 'zh': {'title': 'ä¼˜åŒ–æ¢ç´¢ç©ºé—´ï¼Œæå‡æ¨ç†èƒ½åŠ›', 'desc': 'æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯ä»¥æ˜¾è‘—æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼ŒRLè®­ç»ƒçš„æœ‰æ•ˆæ€§åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–äºé¢„è®­ç»ƒæ¨¡å‹çš„è¾“å‡ºåˆ†å¸ƒæ‰€å®šä¹‰çš„æ¢ç´¢ç©ºé—´ã€‚æœ¬æ–‡é‡æ–°å®¡è§†äº†æ ‡å‡†çš„äº¤å‰ç†µæŸå¤±ï¼Œå°†å…¶è§£é‡Šä¸ºåœ¨å•æ­¥æƒ…å¢ƒä¸­åº”ç”¨çš„ç­–ç•¥æ¢¯åº¦ä¼˜åŒ–çš„ç‰¹å®šå®ä¾‹ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§é€šç”¨çš„é¢„è®­ç»ƒç›®æ ‡ï¼Œç»“åˆäº†åœ¨çº¿RLåŸåˆ™ä¸ç›‘ç£å­¦ä¹ ï¼Œä»¥æ”¹å–„RLçš„æ¢ç´¢æ½œåŠ›ï¼Œä»è€Œæå‡æ¨ç†æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.00671', 'title': 'Fast-weight Product Key Memory', 'url': 'https://huggingface.co/papers/2601.00671', 'abstract': 'FwPKM introduces a dynamic, fast-weight episodic memory mechanism for sequence modeling that balances storage capacity and efficiency, achieving strong performance on long-context tasks like Needle in a Haystack evaluations.  \t\t\t\t\tAI-generated summary \t\t\t\t Sequence modeling layers in modern language models typically face a trade-off between storage capacity and computational efficiency. While Softmax attention offers unbounded storage at prohibitive quadratic costs, linear variants provide efficiency but suffer from limited, fixed-size storage. We propose Fast-weight Product Key Memory (FwPKM), a novel architecture that resolves this tension by transforming the sparse Product Key Memory (PKM) from a static module into a dynamic, "fast-weight" episodic memory. Unlike PKM, FwPKM updates its parameters dynamically at both training and inference time via local chunk-level gradient descent, allowing the model to rapidly memorize and retrieve new key-value pairs from input sequences. Experiments reveal that FwPKM functions as an effective episodic memory that complements the semantic memory of standard modules, yielding significant perplexity reductions on long-context datasets. Notably, in Needle in a Haystack evaluations, FwPKM generalizes to 128K-token contexts despite being trained on only 4K-token sequences.', 'score': 1, 'issue_id': 405, 'pub_date': '2026-01-02', 'pub_date_card': {'ru': '2 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 2', 'zh': '1æœˆ2æ—¥'}, 'hash': '072aff4bbbc7f7a5', 'authors': ['Tianyu Zhao', 'Llion Jones'], 'affiliations': ['Sakana AI'], 'pdf_title_img': 'assets/pdf/title_img/2601.00671.jpg', 'data': {'categories': ['#optimization', '#training', '#architecture', '#long_context'], 'emoji': 'ğŸ“š', 'ru': {'title': 'Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ°Ñ ÑĞ¿Ğ¸Ğ·Ğ¾Ğ´Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° FwPKM, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Product Key Memory Ğ² Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ¿Ğ¸Ğ·Ğ¾Ğ´Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ñ Ğ±Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¼Ğ¸ Ğ²ĞµÑĞ°Ğ¼Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑĞµÑ‚ ÑĞ²Ğ¾Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ñ‡ĞµÑ€ĞµĞ· Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ ÑĞ¿ÑƒÑĞº Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ±Ğ»Ğ¾ĞºĞ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾ Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ñ‚ÑŒ Ğ¸ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°Ñ‚ÑŒ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ğ°Ñ€Ñ‹ ĞºĞ»ÑÑ‡-Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ Ğ¸Ğ· Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞ°ĞµÑ‚ ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‘Ğ¼ĞºĞ¾ÑÑ‚ÑŒÑ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ² ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¸ Ñ ĞºĞ²Ğ°Ğ´Ñ€Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Softmax-Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ´Ğ¾ 128K Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° 4K-Ñ‚Ğ¾ĞºĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑÑ….'}, 'en': {'title': 'Dynamic Memory for Efficient Sequence Modeling', 'desc': 'FwPKM introduces a new memory mechanism for sequence modeling that enhances both storage and efficiency. It transforms the traditional static Product Key Memory into a dynamic, fast-weight episodic memory that can adapt during training and inference. This allows the model to quickly memorize and retrieve information from long input sequences, addressing the limitations of existing attention mechanisms. Experiments show that FwPKM significantly improves performance on long-context tasks, demonstrating its ability to handle large amounts of data effectively.'}, 'zh': {'title': 'åŠ¨æ€å¿«é€Ÿæƒé‡è®°å¿†ï¼Œæå‡åºåˆ—å»ºæ¨¡æ•ˆç‡', 'desc': 'FwPKMæå‡ºäº†ä¸€ç§åŠ¨æ€çš„å¿«é€Ÿæƒé‡æƒ…èŠ‚è®°å¿†æœºåˆ¶ï¼Œç”¨äºåºåˆ—å»ºæ¨¡ï¼Œå¹³è¡¡äº†å­˜å‚¨å®¹é‡å’Œæ•ˆç‡ã€‚åœ¨ç°ä»£è¯­è¨€æ¨¡å‹ä¸­ï¼Œåºåˆ—å»ºæ¨¡å±‚é€šå¸¸é¢ä¸´å­˜å‚¨å®¹é‡ä¸è®¡ç®—æ•ˆç‡ä¹‹é—´çš„æƒè¡¡ã€‚FwPKMé€šè¿‡å°†ç¨€ç–çš„äº§å“é”®è®°å¿†ï¼ˆPKMï¼‰ä»é™æ€æ¨¡å—è½¬å˜ä¸ºåŠ¨æ€çš„â€œå¿«é€Ÿæƒé‡â€æƒ…èŠ‚è®°å¿†ï¼Œè§£å†³äº†è¿™ä¸€çŸ›ç›¾ã€‚å®éªŒè¡¨æ˜ï¼ŒFwPKMä½œä¸ºä¸€ç§æœ‰æ•ˆçš„æƒ…èŠ‚è®°å¿†ï¼Œæ˜¾è‘—é™ä½äº†é•¿ä¸Šä¸‹æ–‡æ•°æ®é›†çš„å›°æƒ‘åº¦ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.00575', 'title': 'InfoSynth: Information-Guided Benchmark Synthesis for LLMs', 'url': 'https://huggingface.co/papers/2601.00575', 'abstract': 'InfoSynth generates novel, diverse coding benchmarks for large language models using information-theoretic metrics and genetic algorithms, enabling scalable and self-verifying evaluation.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have demonstrated significant advancements in reasoning and code generation. However, efficiently creating new benchmarks to evaluate these capabilities remains a challenge. Traditional benchmark creation relies on manual human effort, a process that is both expensive and time-consuming. Furthermore, existing benchmarks often contaminate LLM training data, necessitating novel and diverse benchmarks to accurately assess their genuine capabilities. This work introduces InfoSynth, a novel framework for automatically generating and evaluating reasoning benchmarks guided by information-theoretic principles. We propose metrics based on KL-divergence and entropy to quantify benchmark novelty and diversity without relying on costly model evaluations. Building on this framework, we develop an end-to-end pipeline that synthesizes robust Python coding problems from seed datasets using genetic algorithms and iterative code feedback. Our method generates accurate test cases and solutions to new problems 97% of the time, and the synthesized benchmarks consistently exhibit higher novelty and diversity compared to their seed datasets. Moreover, our algorithm provides a method for controlling the novelty/diversity and difficulty of generated problems. InfoSynth offers a scalable, self-verifying pipeline for constructing high-quality, novel and diverse benchmarks for LLMs. Project Page: https://ishirgarg.github.io/infosynth_web/', 'score': 1, 'issue_id': 408, 'pub_date': '2026-01-02', 'pub_date_card': {'ru': '2 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 2', 'zh': '1æœˆ2æ—¥'}, 'hash': '2606b67e886459d6', 'authors': ['Ishir Garg', 'Neel Kolhe', 'Xuandong Zhao', 'Dawn Song'], 'affiliations': ['University of California, Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2601.00575.jpg', 'data': {'categories': ['#dataset', '#reasoning', '#open_source', '#synthetic', '#benchmark', '#plp'], 'emoji': 'ğŸ§¬', 'ru': {'title': 'ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ² Ğ´Ğ»Ñ LLM Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ñ‚ĞµĞ¾Ñ€Ğ¸Ñ', 'desc': 'InfoSynth â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾-Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ° KL-Ğ´Ğ¸Ğ²ĞµÑ€Ğ³ĞµĞ½Ñ†Ğ¸Ğ¸ Ğ¸ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸, Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ±ĞµĞ· Ğ´Ğ¾Ñ€Ğ¾Ğ³Ğ¾ÑÑ‚Ğ¾ÑÑ‰Ğ¸Ñ… Ğ¾Ñ†ĞµĞ½Ğ¾Ğº Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ñ‹ Ğ¸ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ Ğ¿Ğ¾ ĞºĞ¾Ğ´Ñƒ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ½Ğ° Python Ğ¸Ğ· Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ², ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ñ‹Ğµ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ 97%. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ Ğ¸ ÑĞ°Ğ¼Ğ¾Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ğ¾Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ², Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡.'}, 'en': {'title': 'Automating Diverse Benchmark Creation for LLMs with InfoSynth', 'desc': 'InfoSynth is a framework designed to automatically create diverse and novel coding benchmarks for large language models (LLMs) using information-theoretic metrics and genetic algorithms. It addresses the challenges of traditional benchmark creation, which is often labor-intensive and can lead to contamination of training data. By employing metrics like KL-divergence and entropy, InfoSynth quantifies the novelty and diversity of generated benchmarks without the need for expensive model evaluations. The system can generate accurate coding problems with a high success rate and allows for control over the difficulty and diversity of the benchmarks, making it a scalable solution for evaluating LLM capabilities.'}, 'zh': {'title': 'InfoSynthï¼šè‡ªåŠ¨ç”Ÿæˆå¤šæ ·åŒ–ç¼–ç¨‹åŸºå‡†çš„åˆ›æ–°æ¡†æ¶', 'desc': 'InfoSynth æ˜¯ä¸€ä¸ªæ–°æ¡†æ¶ï¼Œåˆ©ç”¨ä¿¡æ¯è®ºæŒ‡æ ‡å’Œé—ä¼ ç®—æ³•è‡ªåŠ¨ç”Ÿæˆå’Œè¯„ä¼°æ¨ç†åŸºå‡†ã€‚è¯¥æ–¹æ³•é€šè¿‡ KL æ•£åº¦å’Œç†µæ¥é‡åŒ–åŸºå‡†çš„æ–°é¢–æ€§å’Œå¤šæ ·æ€§ï¼Œé¿å…äº†æ˜‚è´µçš„æ¨¡å‹è¯„ä¼°ã€‚é€šè¿‡é—ä¼ ç®—æ³•å’Œè¿­ä»£ä»£ç åé¦ˆï¼ŒInfoSynth èƒ½å¤Ÿä»ç§å­æ•°æ®é›†ä¸­åˆæˆå‡ºå¼ºå¥çš„ Python ç¼–ç¨‹é—®é¢˜ï¼Œç”Ÿæˆçš„æµ‹è¯•ç”¨ä¾‹å’Œè§£å†³æ–¹æ¡ˆå‡†ç¡®ç‡é«˜è¾¾ 97%ã€‚æ­¤å¤–ï¼Œè¯¥ç®—æ³•è¿˜æä¾›äº†æ§åˆ¶ç”Ÿæˆé—®é¢˜çš„æ–°é¢–æ€§ã€å¤šæ ·æ€§å’Œéš¾åº¦çš„æ–¹æ³•ï¼Œç¡®ä¿åŸºå‡†çš„é«˜è´¨é‡å’Œå¤šæ ·æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.00204', 'title': 'MorphAny3D: Unleashing the Power of Structured Latent in 3D Morphing', 'url': 'https://huggingface.co/papers/2601.00204', 'abstract': '3D morphing remains challenging due to the difficulty of generating semantically consistent and temporally smooth deformations, especially across categories. We present MorphAny3D, a training-free framework that leverages Structured Latent (SLAT) representations for high-quality 3D morphing. Our key insight is that intelligently blending source and target SLAT features within the attention mechanisms of 3D generators naturally produces plausible morphing sequences. To this end, we introduce Morphing Cross-Attention (MCA), which fuses source and target information for structural coherence, and Temporal-Fused Self-Attention (TFSA), which enhances temporal consistency by incorporating features from preceding frames. An orientation correction strategy further mitigates the pose ambiguity within the morphing steps. Extensive experiments show that our method generates state-of-the-art morphing sequences, even for challenging cross-category cases. MorphAny3D further supports advanced applications such as decoupled morphing and 3D style transfer, and can be generalized to other SLAT-based generative models. Project page: https://xiaokunsun.github.io/MorphAny3D.github.io/.', 'score': 1, 'issue_id': 407, 'pub_date': '2026-01-01', 'pub_date_card': {'ru': '1 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 1', 'zh': '1æœˆ1æ—¥'}, 'hash': 'a7f85192fb47366b', 'authors': ['Xiaokun Sun', 'Zeyu Cai', 'Hao Tang', 'Ying Tai', 'Jian Yang', 'Zhenyu Zhang'], 'affiliations': ['Nanjing University', 'Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2601.00204.jpg', 'data': {'categories': ['#3d', '#architecture'], 'emoji': 'ğŸ”„', 'ru': {'title': 'ĞŸĞ»Ğ°Ğ²Ğ½Ñ‹Ğµ 3D Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· ÑƒĞ¼Ğ½Ğ¾Ğµ ÑĞ¼ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ² ÑĞµÑ‚ÑÑ… Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ', 'desc': 'ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ MorphAny3D, Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ»Ğ°Ğ²Ğ½Ñ‹Ñ… 3D Ğ¼Ğ¾Ñ€Ñ„Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¹. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ ÑĞ¼ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ° Ğ¸ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ° Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ (SLAT). Ğ”Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¼Ğ¾Ñ€Ñ„Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²ĞµĞ´Ğ¸Ğ»Ğ¸ Ğ´Ğ²Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°: Morphing Cross-Attention Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Temporal-Fused Self-Attention Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ğ´Ñ€Ğ°Ğ¼Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´Ğ°Ğ¶Ğµ Ğ´Ğ»Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑĞ»ÑƒÑ‡Ğ°ĞµĞ² Ğ¼Ğ¾Ñ€Ñ„Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ñ€Ğ°Ğ·Ğ²ÑĞ·Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¼Ğ¾Ñ€Ñ„Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ 3D Ñ‚Ñ€Ğ°Ğ½ÑÑ„ĞµÑ€ ÑÑ‚Ğ¸Ğ»Ñ.'}, 'en': {'title': 'Seamless 3D Morphing with MorphAny3D', 'desc': 'MorphAny3D is a novel framework designed to improve 3D morphing by ensuring that the deformations are both semantically consistent and temporally smooth. It utilizes Structured Latent (SLAT) representations to blend features from source and target objects effectively within attention mechanisms of 3D generators. The introduction of Morphing Cross-Attention (MCA) helps maintain structural coherence, while Temporal-Fused Self-Attention (TFSA) ensures that the morphing sequences are temporally consistent by referencing previous frames. This approach not only achieves state-of-the-art results in morphing across different categories but also opens up possibilities for advanced applications like decoupled morphing and 3D style transfer.'}, 'zh': {'title': 'MorphAny3Dï¼šé«˜è´¨é‡3Då˜å½¢çš„æ–°æ–¹æ³•', 'desc': '3Då˜å½¢ä¸€ç›´æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå› ä¸ºç”Ÿæˆè¯­ä¹‰ä¸€è‡´å’Œæ—¶é—´å¹³æ»‘çš„å˜å½¢éå¸¸å›°éš¾ï¼Œå°¤å…¶æ˜¯åœ¨ä¸åŒç±»åˆ«ä¹‹é—´ã€‚æˆ‘ä»¬æå‡ºäº†MorphAny3Dï¼Œè¿™æ˜¯ä¸€ä¸ªæ— è®­ç»ƒçš„æ¡†æ¶ï¼Œåˆ©ç”¨ç»“æ„æ½œåœ¨ï¼ˆSLATï¼‰è¡¨ç¤ºæ¥å®ç°é«˜è´¨é‡çš„3Då˜å½¢ã€‚æˆ‘ä»¬çš„å…³é”®è§è§£æ˜¯ï¼Œåœ¨3Dç”Ÿæˆå™¨çš„æ³¨æ„åŠ›æœºåˆ¶ä¸­æ™ºèƒ½åœ°èåˆæºå’Œç›®æ ‡çš„SLATç‰¹å¾ï¼Œè‡ªç„¶äº§ç”Ÿå¯ä¿¡çš„å˜å½¢åºåˆ—ã€‚é€šè¿‡å¼•å…¥å˜å½¢äº¤å‰æ³¨æ„åŠ›ï¼ˆMCAï¼‰å’Œæ—¶é—´èåˆè‡ªæ³¨æ„åŠ›ï¼ˆTFSAï¼‰ï¼Œæˆ‘ä»¬å¢å¼ºäº†ç»“æ„ä¸€è‡´æ€§å’Œæ—¶é—´ä¸€è‡´æ€§ï¼Œå®éªŒç»“æœè¡¨æ˜æˆ‘ä»¬çš„æ–¹æ³•åœ¨å˜å½¢åºåˆ—ç”Ÿæˆä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.08521', 'title': 'Your Group-Relative Advantage Is Biased', 'url': 'https://huggingface.co/papers/2601.08521', 'abstract': 'Group-based reinforcement learning from verifier rewards suffers from biased advantage estimation that underestimates hard prompts and overestimates easy prompts, which is addressed through a history-aware adaptive difficulty weighting method.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning from Verifier Rewards (RLVR) has emerged as a widely used approach for post-training large language models on reasoning tasks, with group-based methods such as GRPO and its variants gaining broad adoption. These methods rely on group-relative advantage estimation to avoid learned critics, yet its theoretical properties remain poorly understood.   In this work, we uncover a fundamental issue of group-based RL: the group-relative advantage estimator is inherently biased relative to the true (expected) advantage. We provide the first theoretical analysis showing that it systematically underestimates advantages for hard prompts and overestimates them for easy prompts, leading to imbalanced exploration and exploitation. To address this issue, we propose History-Aware Adaptive Difficulty Weighting (HA-DW), an adaptive reweighting scheme that adjusts advantage estimates based on an evolving difficulty anchor and training dynamics. Both theoretical analysis and experiments on five mathematical reasoning benchmarks demonstrate that HA-DW consistently improves performance when integrated into GRPO and its variants. Our results suggest that correcting biased advantage estimation is critical for robust and efficient RLVR training.', 'score': 95, 'issue_id': 640, 'pub_date': '2026-01-13', 'pub_date_card': {'ru': '13 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 13', 'zh': '1æœˆ13æ—¥'}, 'hash': 'caf215355050af49', 'authors': ['Fengkai Yang', 'Zherui Chen', 'Xiaohan Wang', 'Xiaodong Lu', 'Jiajun Chai', 'Guojun Yin', 'Wei Lin', 'Shuai Ma', 'Fuzhen Zhuang', 'Deqing Wang', 'Yaodong Yang', 'Jianxin Li', 'Yikun Ban'], 'affiliations': ['Beihang University', 'Meituan', 'Peking University', 'University of California, Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2601.08521.jpg', 'data': {'categories': ['#reasoning', '#training', '#optimization', '#rl', '#rlhf', '#math'], 'emoji': 'âš–ï¸', 'ru': {'title': 'Ğ˜ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ÑĞ¼ĞµÑ‰Ñ‘Ğ½Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ² Ğ² Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¾Ğ²Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ²Ğ·Ğ²ĞµÑˆĞ¸Ğ²Ğ°ÑÑ‰ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° ÑĞ¼ĞµÑ‰Ñ‘Ğ½Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ² Ğ² Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¾Ğ²Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ñ… Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¾Ñ‚ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ¾Ğ², Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº GRPO. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¾Ğ²Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ½ĞµĞ´Ğ¾Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ğ¸ Ğ¿ĞµÑ€ĞµĞ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ»Ñ‘Ğ³ĞºĞ¸Ğµ, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ´Ğ¸ÑĞ±Ğ°Ğ»Ğ°Ğ½ÑÑƒ Ğ² Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ History-Aware Adaptive Difficulty Weighting, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¿ĞµÑ€ĞµĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒÑÑ‰ĞµĞ³Ğ¾ ÑĞºĞ¾Ñ€Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ¾Ñ†ĞµĞ½Ğ¾Ğº ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Correcting Bias in Advantage Estimation for Better Learning', 'desc': 'This paper addresses a problem in group-based reinforcement learning from verifier rewards, where the advantage estimation is biased. Specifically, it underestimates the difficulty of hard prompts while overestimating the ease of simple prompts, which can lead to ineffective learning. The authors introduce a new method called History-Aware Adaptive Difficulty Weighting (HA-DW) that adjusts the advantage estimates based on the difficulty of tasks and the training process. Their experiments show that incorporating HA-DW into existing models significantly enhances performance on reasoning tasks.'}, 'zh': {'title': 'çº æ­£åå·®ï¼Œæå‡å¼ºåŒ–å­¦ä¹ æ•ˆæœ', 'desc': 'æœ¬æ–‡æ¢è®¨äº†åŸºäºéªŒè¯è€…å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰ä¸­å­˜åœ¨çš„åå·®é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ç¾¤ä½“åŸºç¡€æ–¹æ³•ä¸­ã€‚ç ”ç©¶å‘ç°ï¼Œç¾¤ä½“ç›¸å¯¹ä¼˜åŠ¿ä¼°è®¡å™¨åœ¨é¢å¯¹å›°éš¾æç¤ºæ—¶ä¼šä½ä¼°ä¼˜åŠ¿ï¼Œè€Œåœ¨é¢å¯¹ç®€å•æç¤ºæ—¶åˆ™ä¼šé«˜ä¼°ä¼˜åŠ¿ï¼Œå¯¼è‡´æ¢ç´¢å’Œåˆ©ç”¨çš„ä¸å¹³è¡¡ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§å†å²æ„ŸçŸ¥è‡ªé€‚åº”éš¾åº¦åŠ æƒæ–¹æ³•ï¼ˆHA-DWï¼‰ï¼Œè¯¥æ–¹æ³•æ ¹æ®ä¸æ–­å˜åŒ–çš„éš¾åº¦é”šç‚¹å’Œè®­ç»ƒåŠ¨æ€è°ƒæ•´ä¼˜åŠ¿ä¼°è®¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå°†HA-DWé›†æˆåˆ°GRPOåŠå…¶å˜ä½“ä¸­å¯ä»¥æ˜¾è‘—æé«˜æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.11496', 'title': 'The Poisoned Apple Effect: Strategic Manipulation of Mediated Markets via Technology Expansion of AI Agents', 'url': 'https://huggingface.co/papers/2601.11496', 'abstract': 'The integration of AI agents into economic markets fundamentally alters the landscape of strategic interaction. We investigate the economic implications of expanding the set of available technologies in three canonical game-theoretic settings: bargaining (resource division), negotiation (asymmetric information trade), and persuasion (strategic information transmission). We find that simply increasing the choice of AI delegates can drastically shift equilibrium payoffs and regulatory outcomes, often creating incentives for regulators to proactively develop and release technologies. Conversely, we identify a strategic phenomenon termed the "Poisoned Apple" effect: an agent may release a new technology, which neither they nor their opponent ultimately uses, solely to manipulate the regulator\'s choice of market design in their favor. This strategic release improves the releaser\'s welfare at the expense of their opponent and the regulator\'s fairness objectives. Our findings demonstrate that static regulatory frameworks are vulnerable to manipulation via technology expansion, necessitating dynamic market designs that adapt to the evolving landscape of AI capabilities.', 'score': 38, 'issue_id': 646, 'pub_date': '2026-01-16', 'pub_date_card': {'ru': '16 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 16', 'zh': '1æœˆ16æ—¥'}, 'hash': '111af399807fc2ee', 'authors': ['Eilam Shapira', 'Roi Reichart', 'Moshe Tennenholtz'], 'affiliations': ['Technion Israel Institute of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2601.11496.jpg', 'data': {'categories': [], 'emoji': 'ğŸ', 'ru': {'title': 'ĞœĞ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ñ Ñ€ĞµĞ³ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ‡ĞµÑ€ĞµĞ· ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ²Ñ‹Ğ¿ÑƒÑĞº Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¹', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ñ… AI Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¹ Ğ½Ğ° ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ñ‚Ñ€Ñ‘Ñ… ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ³Ñ€Ğ°Ñ…: Ñ‚Ğ¾Ñ€Ğ³Ğµ, Ğ¿ĞµÑ€ĞµĞ³Ğ¾Ğ²Ğ¾Ñ€Ğ°Ñ… Ğ¸ ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° AI Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¼Ğ¾Ğ¶ĞµÑ‚ ĞºĞ°Ñ€Ğ´Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¸Ğ·Ğ¼ĞµĞ½ÑÑ‚ÑŒ Ñ€Ğ°Ğ²Ğ½Ğ¾Ğ²ĞµÑĞ½Ñ‹Ğµ Ğ²Ñ‹Ğ¸Ğ³Ñ€Ñ‹ÑˆĞ¸ Ğ¸ ÑÑ‚Ğ¸Ğ¼ÑƒĞ»Ñ‹ Ñ€ĞµĞ³ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ¾Ğ² Ğ½Ğ° Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ğµ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¹. ĞĞ½Ğ¸ Ğ²Ñ‹ÑĞ²Ğ»ÑÑÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚ "ĞÑ‚Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ±Ğ»Ğ¾ĞºĞ°", ĞºĞ¾Ğ³Ğ´Ğ° Ğ°Ğ³ĞµĞ½Ñ‚ Ğ²Ñ‹Ğ¿ÑƒÑĞºĞ°ĞµÑ‚ Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€ÑƒÑ Ğ½Ğ¸ĞºÑ‚Ğ¾ Ğ½Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚, Ğ¸ÑĞºĞ»ÑÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ´Ğ»Ñ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ñ€ĞµĞ³ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ° Ğ¸ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞ³ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ½Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ñ‹ Ğ´Ğ»Ñ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¹ Ğ¸ Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² Ñ€Ñ‹Ğ½ĞºĞ°.'}, 'en': {'title': 'AI Agents: Transforming Market Strategies and Regulatory Dynamics', 'desc': "This paper explores how the introduction of AI agents changes the way economic markets operate, particularly in strategic interactions. It examines three key game-theoretic scenarios: bargaining, negotiation, and persuasion, showing that more AI options can significantly alter outcomes and regulatory decisions. The authors introduce the 'Poisoned Apple' effect, where an agent may introduce a technology that is not used to influence regulators and gain an advantage. The study highlights the need for flexible regulatory frameworks that can adapt to the rapid changes brought by AI technologies."}, 'zh': {'title': 'AIä»£ç†æ”¹å˜ç»æµå¸‚åœºçš„æˆ˜ç•¥äº’åŠ¨', 'desc': 'æœ¬æ–‡æ¢è®¨äº†äººå·¥æ™ºèƒ½ä»£ç†åœ¨ç»æµå¸‚åœºä¸­çš„æ•´åˆå¦‚ä½•æ”¹å˜æˆ˜ç•¥äº’åŠ¨çš„æ ¼å±€ã€‚æˆ‘ä»¬åˆ†æäº†åœ¨ä¸‰ç§ç»å…¸åšå¼ˆè®ºåœºæ™¯ä¸‹ï¼Œæ‰©å±•å¯ç”¨æŠ€æœ¯é›†çš„ç»æµå½±å“ï¼ŒåŒ…æ‹¬èµ„æºåˆ†é…ã€ä¿¡æ¯ä¸å¯¹ç§°äº¤æ˜“å’Œæˆ˜ç•¥ä¿¡æ¯ä¼ é€’ã€‚ç ”ç©¶å‘ç°ï¼Œå¢åŠ AIä»£ç†çš„é€‰æ‹©å¯ä»¥æ˜¾è‘—æ”¹å˜å‡è¡¡æ”¶ç›Šå’Œç›‘ç®¡ç»“æœï¼Œç”šè‡³ä¿ƒä½¿ç›‘ç®¡è€…ä¸»åŠ¨å¼€å‘æ–°æŠ€æœ¯ã€‚æˆ‘ä»¬è¿˜è¯†åˆ«å‡ºä¸€ç§ç§°ä¸ºâ€œæ¯’è‹¹æœâ€æ•ˆåº”çš„æˆ˜ç•¥ç°è±¡ï¼Œå³ä»£ç†å¯èƒ½ä¼šå‘å¸ƒä¸€ç§æ–°æŠ€æœ¯ï¼Œå°½ç®¡è‡ªå·±å’Œå¯¹æ‰‹éƒ½ä¸ä½¿ç”¨ï¼Œä½†å´æ˜¯ä¸ºäº†æ“æ§ç›‘ç®¡è€…çš„å¸‚åœºè®¾è®¡é€‰æ‹©ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.10355', 'title': 'Unlocking Implicit Experience: Synthesizing Tool-Use Trajectories from Text', 'url': 'https://huggingface.co/papers/2601.10355', 'abstract': 'A text-based data synthesis approach generates multi-turn tool-use trajectories for large language models, achieving improved performance and reduced computational costs through a specialized trajectory synthesizer.  \t\t\t\t\tAI-generated summary \t\t\t\t Enabling Large Language Models (LLMs) to effectively utilize tools in multi-turn interactions is essential for building capable autonomous agents. However, acquiring diverse and realistic multi-turn tool-use data remains a significant challenge. In this work, we propose a novel text-based paradigm. We observe that textual corpora naturally contain rich, multi-step problem-solving experiences, which can serve as an untapped, scalable, and authentic data source for multi-turn tool-use tasks. Based on this insight, we introduce GEM, a data synthesis pipeline that enables the generation and extraction of multi-turn tool-use trajectories from text corpora through a four-stage process: relevance filtering, workflow & tool extraction, trajectory grounding, and complexity refinement. To reduce the computational cost, we further train a specialized Trajectory Synthesizer via supervised fine-tuning. This model distills the complex generation pipeline into an efficient, end-to-end trajectory generator. Experiments demonstrate that our GEM-32B achieve a 16.5% improvement on the BFCL V3 Multi-turn benchmark. Our models partially surpass the performance of models trained on Ï„ - bench (Airline and Retail) in-domain data, highlighting the superior generalization capability derived from our text-based synthesis paradigm. Notably, our Trajectory Synthesizer matches the quality of the full pipeline while significantly reducing inference latency and costs.', 'score': 30, 'issue_id': 640, 'pub_date': '2026-01-15', 'pub_date_card': {'ru': '15 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 15', 'zh': '1æœˆ15æ—¥'}, 'hash': '561cb5995d29e4d4', 'authors': ['Zhihao Xu', 'Rumei Li', 'Jiahuan Li', 'Rongxiang Weng', 'Jingang Wang', 'Xunliang Cai', 'Xiting Wang'], 'affiliations': ['Meituan', 'Renmin University of China'], 'pdf_title_img': 'assets/pdf/title_img/2601.10355.jpg', 'data': {'categories': [], 'emoji': 'ğŸ”§', 'ru': {'title': 'Ğ¡Ğ¸Ğ½Ñ‚ĞµĞ· Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ñ€ÑĞ¼Ğ¾ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ¾Ğ²', 'desc': 'ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ GEM â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ñ‡ĞµÑ‚Ñ‹Ñ€Ñ‘Ñ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸, Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ. ĞĞ½Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Trajectory Synthesizer Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ supervised fine-tuning, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ² ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ end-to-end Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€. ĞœĞ¾Ğ´ĞµĞ»ÑŒ GEM-32B Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ 16.5% ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ BFCL V3 Multi-turn Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ»ÑƒÑ‡ÑˆÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ½Ğ° in-domain Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Trajectory Synthesizer Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ğ° Ğ¿Ñ€Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚.'}, 'en': {'title': 'Harnessing Text for Efficient Multi-Turn Tool Use in LLMs', 'desc': 'This paper presents a new method for generating multi-turn tool-use trajectories for large language models (LLMs) using text-based data synthesis. The proposed GEM pipeline extracts and refines problem-solving experiences from existing text corpora, allowing for the creation of realistic multi-turn interactions. By training a specialized Trajectory Synthesizer, the authors reduce computational costs while maintaining high performance in generating these trajectories. Experimental results show significant improvements in benchmark tests, demonstrating the effectiveness and efficiency of this approach compared to traditional data collection methods.'}, 'zh': {'title': 'æ–‡æœ¬åˆæˆåŠ©åŠ›å¤šè½®å·¥å…·ä½¿ç”¨çš„æ™ºèƒ½åŒ–', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ–‡æœ¬çš„æ•°æ®åˆæˆæ–¹æ³•ï¼Œæ—¨åœ¨ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆå¤šè½®å·¥å…·ä½¿ç”¨è½¨è¿¹ã€‚è¿™ç§æ–¹æ³•é€šè¿‡å››ä¸ªé˜¶æ®µçš„å¤„ç†æµç¨‹ï¼Œä»æ–‡æœ¬è¯­æ–™åº“ä¸­æå–å’Œç”Ÿæˆå¤šè½®å·¥å…·ä½¿ç”¨æ•°æ®ã€‚æˆ‘ä»¬è¿˜è®­ç»ƒäº†ä¸€ä¸ªä¸“é—¨çš„è½¨è¿¹åˆæˆå™¨ï¼Œä»¥é™ä½è®¡ç®—æˆæœ¬ï¼Œå¹¶æé«˜ç”Ÿæˆæ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šè½®åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº†æ€§èƒ½ï¼Œå±•ç¤ºäº†å…¶ä¼˜è¶Šçš„æ³›åŒ–èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.08430', 'title': 'RubricHub: A Comprehensive and Highly Discriminative Rubric Dataset via Automated Coarse-to-Fine Generation', 'url': 'https://huggingface.co/papers/2601.08430', 'abstract': 'RLVR has advanced reasoning capabilities but struggles with open-ended generation due to lack of ground truth; this work proposes an automated rubric generation framework and dataset to improve performance in health reasoning benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning with Verifiable Rewards (RLVR) has driven substantial progress in reasoning-intensive domains like mathematics. However, optimizing open-ended generation remains challenging due to the lack of ground truth. While rubric-based evaluation offers a structured proxy for verification, existing methods suffer from scalability bottlenecks and coarse criteria, resulting in a supervision ceiling effect. To address this, we propose an automated Coarse-to-Fine Rubric Generation framework. By synergizing principle-guided synthesis, multi-model aggregation, and difficulty evolution, our approach produces comprehensive and highly discriminative criteria capable of capturing the subtle nuances. Based on this framework, we introduce RubricHub, a large-scale (sim110k) and multi-domain dataset. We validate its utility through a two-stage post-training pipeline comprising Rubric-based Rejection Sampling Fine-Tuning (RuFT) and Reinforcement Learning (RuRL). Experimental results demonstrate that RubricHub unlocks significant performance gains: our post-trained Qwen3-14B achieves state-of-the-art (SOTA) results on HealthBench (69.3), surpassing proprietary frontier models such as GPT-5. The code and data will be released soon.', 'score': 25, 'issue_id': 640, 'pub_date': '2026-01-13', 'pub_date_card': {'ru': '13 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 13', 'zh': '1æœˆ13æ—¥'}, 'hash': '9d3d0f8e4f3e3b69', 'authors': ['Sunzhu Li', 'Jiale Zhao', 'Miteto Wei', 'Huimin Ren', 'Yang Zhou', 'Jingwen Yang', 'Shunyu Liu', 'Kaike Zhang', 'Wei Chen'], 'affiliations': ['Li Auto Inc.', 'Nanyang Technological University', 'The Chinese University of Hong Kong, Shenzhen', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2601.08430.jpg', 'data': {'categories': ['#open_source', '#reasoning', '#healthcare', '#benchmark', '#training', '#optimization', '#rl', '#dataset'], 'emoji': 'ğŸ“‹', 'ru': {'title': 'ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€ÑƒĞ±Ñ€Ğ¸Ğº Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ Ñ‚Ğ¸Ğ¿Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ¾Ğ², Ğ°Ğ³Ñ€ĞµĞ³Ğ°Ñ†Ğ¸Ğ¸ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸ĞµĞ² Ğ¾Ñ†ĞµĞ½ĞºĞ¸. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ RubricHub Ñ 110 Ñ‚Ñ‹ÑÑÑ‡Ğ°Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ¾Ğ¼ĞµĞ½Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¸Ğµ Ñ€ÑƒĞ±Ñ€Ğ¸ĞºĞ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ»ÑƒÑ‡ÑˆĞµ Ñ‡ĞµĞ¼ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Enhancing AI Reasoning with Automated Rubric Generation', 'desc': 'This paper addresses the limitations of Reinforcement Learning with Verifiable Rewards (RLVR) in open-ended generation tasks, particularly in health reasoning. It introduces an automated framework for generating evaluation rubrics that enhances the performance of AI models by providing structured criteria for assessment. The proposed Coarse-to-Fine Rubric Generation framework combines various techniques to create detailed and effective evaluation metrics. The authors validate their approach with a new dataset, RubricHub, which significantly improves the performance of AI models on health reasoning benchmarks, achieving state-of-the-art results.'}, 'zh': {'title': 'è‡ªåŠ¨åŒ–è¯„åˆ†æ ‡å‡†æå‡å¥åº·æ¨ç†èƒ½åŠ›', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§è‡ªåŠ¨åŒ–çš„è¯„åˆ†æ ‡å‡†ç”Ÿæˆæ¡†æ¶ï¼Œä»¥æé«˜åœ¨å¥åº·æ¨ç†åŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°ã€‚å°½ç®¡å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰åœ¨æ¨ç†å¯†é›†å‹é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨å¼€æ”¾å¼ç”Ÿæˆä»»åŠ¡ä¸­ä»é¢ä¸´æŒ‘æˆ˜ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†åŸåˆ™å¼•å¯¼åˆæˆã€å¤šæ¨¡å‹èšåˆå’Œéš¾åº¦æ¼”å˜ï¼Œç”Ÿæˆå…¨é¢ä¸”é«˜åº¦åŒºåˆ†çš„è¯„åˆ†æ ‡å‡†ã€‚é€šè¿‡RubricHubæ•°æ®é›†çš„éªŒè¯ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å¥åº·åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œè¶…è¶Šäº†ç°æœ‰çš„å‰æ²¿æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.11000', 'title': 'When Personalization Misleads: Understanding and Mitigating Hallucinations in Personalized LLMs', 'url': 'https://huggingface.co/papers/2601.11000', 'abstract': "Personalized large language models can generate false information aligned with user history instead of factual truth, but a new method called FPPS helps maintain both factual accuracy and personalized responses while preserving existing personalization effects.  \t\t\t\t\tAI-generated summary \t\t\t\t Personalized large language models (LLMs) adapt model behavior to individual users to enhance user satisfaction, yet personalization can inadvertently distort factual reasoning. We show that when personalized LLMs face factual queries, there exists a phenomenon where the model generates answers aligned with a user's prior history rather than the objective truth, resulting in personalization-induced hallucinations that degrade factual reliability and may propagate incorrect beliefs, due to representational entanglement between personalization and factual representations. To address this issue, we propose Factuality-Preserving Personalized Steering (FPPS), a lightweight inference-time approach that mitigates personalization-induced factual distortions while preserving personalized behavior. We further introduce PFQABench, the first benchmark designed to jointly evaluate factual and personalized question answering under personalization. Experiments across multiple LLM backbones and personalization methods show that FPPS substantially improves factual accuracy while maintaining personalized performance.", 'score': 21, 'issue_id': 644, 'pub_date': '2026-01-16', 'pub_date_card': {'ru': '16 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 16', 'zh': '1æœˆ16æ—¥'}, 'hash': '891d8c6f80792efc', 'authors': ['Zhongxiang Sun', 'Yi Zhan', 'Chenglei Shen', 'Weijie Yu', 'Xiao Zhang', 'Ming He', 'Jun Xu'], 'affiliations': ['AI Lab at Lenovo Research', 'Gaoling School of Artificial Intelligence, Renmin University of China', 'School of Artificial Intelligence and Data Science, University of International Business and Economics'], 'pdf_title_img': 'assets/pdf/title_img/2601.11000.jpg', 'data': {'categories': ['#alignment', '#hallucinations'], 'emoji': 'âš–ï¸', 'ru': {'title': 'Ğ‘Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¹ Ğ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'ĞŸĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ»Ğ¾Ğ¶Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ, ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ñ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸ĞµĞ¹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¸ÑÑ‚Ğ¸Ğ½Ñ‹, Ğ¸Ğ·-Ğ·Ğ° Ğ¿ĞµÑ€ĞµĞ¿Ğ»ĞµÑ‚ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¸ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ FPPS (Factuality-Preserving Personalized Steering), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸, Ğ²Ñ‹Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹. Ğ”Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¾Ğ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº PFQABench, Ğ¸Ğ·Ğ¼ĞµÑ€ÑÑÑ‰Ğ¸Ğ¹ ĞºĞ°Ğº Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ, Ñ‚Ğ°Ğº Ğ¸ ÑÑ‚ĞµĞ¿ĞµĞ½ÑŒ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ FPPS Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Balancing Personalization and Factual Accuracy in LLMs', 'desc': "This paper addresses the challenge of personalized large language models (LLMs) generating inaccurate information based on user history instead of factual accuracy. It identifies a problem where these models produce answers that reflect a user's past interactions, leading to 'hallucinations' that can spread misinformation. To solve this, the authors introduce a method called Factuality-Preserving Personalized Steering (FPPS), which helps maintain factual correctness while still providing personalized responses. Additionally, they present PFQABench, a benchmark for evaluating both factual accuracy and personalization in question answering."}, 'zh': {'title': 'ä¸ªæ€§åŒ–ä¸äº‹å®å‡†ç¡®æ€§çš„å¹³è¡¡ä¹‹é“', 'desc': 'ä¸ªæ€§åŒ–çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯ä»¥æ ¹æ®ç”¨æˆ·çš„å†å²ç”Ÿæˆä¿¡æ¯ï¼Œä½†è¿™å¯èƒ½å¯¼è‡´ç”Ÿæˆçš„å†…å®¹ä¸äº‹å®ä¸ç¬¦ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œç§°ä¸ºäº‹å®ä¿æŒä¸ªæ€§åŒ–å¼•å¯¼ï¼ˆFPPSï¼‰ï¼Œå®ƒå¯ä»¥åœ¨ä¿æŒä¸ªæ€§åŒ–å“åº”çš„åŒæ—¶ï¼Œç¡®ä¿ç”Ÿæˆå†…å®¹çš„äº‹å®å‡†ç¡®æ€§ã€‚FPPSé€šè¿‡è½»é‡çº§çš„æ¨ç†æ–¹æ³•ï¼Œå‡å°‘ä¸ªæ€§åŒ–å¼•èµ·çš„äº‹å®æ‰­æ›²ï¼Œé¿å…äº†é”™è¯¯ä¿¡å¿µçš„ä¼ æ’­ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†PFQABenchï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªæ—¨åœ¨å…±åŒè¯„ä¼°ä¸ªæ€§åŒ–é—®ç­”å’Œäº‹å®å‡†ç¡®æ€§çš„åŸºå‡†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.11404', 'title': 'ACoT-VLA: Action Chain-of-Thought for Vision-Language-Action Models', 'url': 'https://huggingface.co/papers/2601.11404', 'abstract': 'Vision-Language-Action models are enhanced by incorporating action-space reasoning through a structured sequence of coarse action intents, improving manipulation task performance in both simulation and real-world environments.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language-Action (VLA) models have emerged as essential generalist robot policies for diverse manipulation tasks, conventionally relying on directly translating multimodal inputs into actions via Vision-Language Model (VLM) embeddings. Recent advancements have introduced explicit intermediary reasoning, such as sub-task prediction (language) or goal image synthesis (vision), to guide action generation. However, these intermediate reasoning are often indirect and inherently limited in their capacity to convey the full, granular information required for precise action execution. Instead, we posit that the most effective form of reasoning is one that deliberates directly in the action space. We introduce Action Chain-of-Thought (ACoT), a paradigm where the reasoning process itself is formulated as a structured sequence of coarse action intents that guide the final policy. In this paper, we propose ACoT-VLA, a novel architecture that materializes the ACoT paradigm. Specifically, we introduce two complementary components: an Explicit Action Reasoner (EAR) and Implicit Action Reasoner (IAR). The former proposes coarse reference trajectories as explicit action-level reasoning steps, while the latter extracts latent action priors from internal representations of multimodal input, co-forming an ACoT that conditions the downstream action head to enable grounded policy learning. Extensive experiments in real-world and simulation environments demonstrate the superiority of our proposed method, which achieves 98.5%, 84.1%, and 47.4% on LIBERO, LIBERO-Plus and VLABench, respectively.', 'score': 18, 'issue_id': 645, 'pub_date': '2026-01-16', 'pub_date_card': {'ru': '16 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 16', 'zh': '1æœˆ16æ—¥'}, 'hash': 'feebd83e55594c09', 'authors': ['Linqing Zhong', 'Yi Liu', 'Yifei Wei', 'Ziyu Xiong', 'Maoqing Yao', 'Si Liu', 'Guanghui Ren'], 'affiliations': ['AgiBot', 'Beihang University'], 'pdf_title_img': 'assets/pdf/title_img/2601.11404.jpg', 'data': {'categories': ['#reasoning', '#architecture', '#multimodal', '#robotics'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞœÑ‹ÑĞ»Ğ¸Ñ‚ÑŒ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑĞ¼Ğ¸: Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Action Chain-of-Thought (ACoT) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Vision-Language-Action Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ â€” Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ñ‘Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ…Ğ¾Ğ´Ğ° Ğ² Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ, Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ½ĞµĞ¿Ğ¾ÑÑ€ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ³Ñ€ÑƒĞ±Ñ‹Ñ… Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğ¹. ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° ACoT-VLA ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ğ´Ğ²ÑƒÑ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ²: ÑĞ²Ğ½Ğ¾Ğ³Ğ¾ Ğ´ĞµĞ¹ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ĞµĞ»Ñ (EAR), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¾Ğ¿Ğ¾Ñ€Ğ½Ñ‹Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸, Ğ¸ Ğ½ĞµÑĞ²Ğ½Ğ¾Ğ³Ğ¾ Ğ´ĞµĞ¹ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ĞµĞ»Ñ (IAR), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸Ğ· Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ» Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… LIBERO, LIBERO-Plus Ğ¸ VLABench Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ ÑĞ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸ÑÑ….'}, 'en': {'title': 'Enhancing Robot Actions with Structured Reasoning', 'desc': 'This paper presents a new approach called Action Chain-of-Thought (ACoT) to improve Vision-Language-Action (VLA) models for robotic manipulation tasks. ACoT enhances action-space reasoning by using a structured sequence of coarse action intents, which helps the model better understand and execute tasks. The proposed ACoT-VLA architecture includes two components: an Explicit Action Reasoner (EAR) that suggests clear action steps and an Implicit Action Reasoner (IAR) that derives action insights from multimodal inputs. Experiments show that this method significantly outperforms existing models in both simulated and real-world scenarios.'}, 'zh': {'title': 'é€šè¿‡è¡ŒåŠ¨ç©ºé—´æ¨ç†æå‡æœºå™¨äººæ“ä½œèƒ½åŠ›', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è§†è§‰-è¯­è¨€-è¡ŒåŠ¨æ¨¡å‹æ¶æ„ï¼Œç§°ä¸ºACoT-VLAï¼Œæ—¨åœ¨é€šè¿‡åœ¨è¡ŒåŠ¨ç©ºé—´ä¸­è¿›è¡Œæ¨ç†æ¥æé«˜æœºå™¨äººåœ¨æ“ä½œä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚è¯¥æ¨¡å‹å¼•å…¥äº†æ˜¾å¼è¡ŒåŠ¨æ¨ç†å™¨ï¼ˆEARï¼‰å’Œéšå¼è¡ŒåŠ¨æ¨ç†å™¨ï¼ˆIARï¼‰ï¼Œå‰è€…æä¾›ç²—ç•¥çš„å‚è€ƒè½¨è¿¹ä½œä¸ºæ¨ç†æ­¥éª¤ï¼Œåè€…åˆ™ä»å¤šæ¨¡æ€è¾“å…¥çš„å†…éƒ¨è¡¨ç¤ºä¸­æå–æ½œåœ¨çš„è¡ŒåŠ¨å…ˆéªŒã€‚é€šè¿‡è¿™ç§ç»“æ„åŒ–çš„ç²—ç•¥è¡ŒåŠ¨æ„å›¾åºåˆ—ï¼ŒACoT-VLAèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°æŒ‡å¯¼æœ€ç»ˆçš„æ”¿ç­–ç”Ÿæˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨çœŸå®ä¸–ç•Œå’Œæ¨¡æ‹Ÿç¯å¢ƒä¸­å‡è¡¨ç°ä¼˜å¼‚ï¼Œè¾¾åˆ°äº†LIBEROã€LIBERO-Pluså’ŒVLABenchçš„é«˜åˆ†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.11037', 'title': 'BAPO: Boundary-Aware Policy Optimization for Reliable Agentic Search', 'url': 'https://huggingface.co/papers/2601.11037', 'abstract': "Reinforcement learning framework for agentic search that improves reliability by teaching agents to recognize reasoning limits and respond appropriately when evidence is insufficient.  \t\t\t\t\tAI-generated summary \t\t\t\t RL-based agentic search enables LLMs to solve complex questions via dynamic planning and external search. While this approach significantly enhances accuracy with agent policies optimized via large-scale reinforcement learning, we identify a critical gap in reliability: these agents fail to recognize their reasoning boundaries and rarely admit ``I DON'T KNOW'' (IDK) even when evidence is insufficient or reasoning reaches its limit. The lack of reliability often leads to plausible but unreliable answers, introducing significant risks in many real-world scenarios. To this end, we propose Boundary-Aware Policy Optimization (BAPO), a novel RL framework designed to cultivate reliable boundary awareness without compromising accuracy. BAPO introduces two key components: (i) a group-based boundary-aware reward that encourages an IDK response only when the reasoning reaches its limit, and (ii) an adaptive reward modulator that strategically suspends this reward during early exploration, preventing the model from exploiting IDK as a shortcut. Extensive experiments on four benchmarks demonstrate that BAPO substantially enhances the overall reliability of agentic search.", 'score': 12, 'issue_id': 640, 'pub_date': '2026-01-16', 'pub_date_card': {'ru': '16 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 16', 'zh': '1æœˆ16æ—¥'}, 'hash': '872fa7ac4160c51e', 'authors': ['Shiyu Liu', 'Yongjing Yin', 'Jianhao Yan', 'Yunbo Tang', 'Qinggang Zhang', 'Bei Li', 'Xin Chen', 'Jingang Wang', 'Xunliang Cai', 'Jinsong Su'], 'affiliations': ['Institute of Artificial Intelligence, Xiamen University', 'Meituan Inc.', 'School of Informatics, Xiamen University', 'The Hong Kong Polytechnic University', 'Westlake University'], 'pdf_title_img': 'assets/pdf/title_img/2601.11037.jpg', 'data': {'categories': ['#agents', '#rl', '#training'], 'emoji': 'ğŸ¯', 'ru': {'title': 'ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ ÑĞ²Ğ¾ĞµĞ³Ğ¾ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒÑ‡Ğ¸Ñ‚ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾ÑĞ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ ÑĞ²Ğ¾Ğ¸Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ, Ñ‡Ñ‚Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ´Ğ°ÑÑ‚ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ñ‹Ğµ, Ğ½Ğ¾ Ğ½ĞµĞ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚ĞºĞ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ ĞµÑˆĞµĞ½Ğ¸ĞµĞ¼ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Boundary-Aware Policy Optimization (BAPO) â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¾Ñ‰Ñ€ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±ĞµĞ· ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ñ‡ĞµÑ‚Ñ‹Ñ€Ñ‘Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°.'}, 'en': {'title': 'Enhancing Reliability in Agentic Search with Boundary Awareness', 'desc': "This paper presents a new reinforcement learning framework called Boundary-Aware Policy Optimization (BAPO) aimed at improving the reliability of agents in complex search tasks. The framework teaches agents to recognize their reasoning limits and encourages them to respond with 'I DON'T KNOW' when evidence is insufficient. By incorporating a boundary-aware reward system, BAPO ensures that agents only admit uncertainty when necessary, thus reducing the risk of providing misleading answers. Experimental results show that BAPO significantly enhances the reliability of agentic search while maintaining high accuracy in responses."}, 'zh': {'title': 'æå‡æ™ºèƒ½ä½“æœç´¢å¯é æ€§çš„è¾¹ç•Œæ„è¯†ä¼˜åŒ–', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜æ™ºèƒ½ä½“æœç´¢çš„å¯é æ€§ã€‚é€šè¿‡æ•™å¯¼æ™ºèƒ½ä½“è¯†åˆ«æ¨ç†çš„å±€é™æ€§ï¼Œå½“è¯æ®ä¸è¶³æ—¶èƒ½å¤Ÿé€‚å½“åœ°å›åº”ã€‚æˆ‘ä»¬å¼•å…¥äº†è¾¹ç•Œæ„è¯†ç­–ç•¥ä¼˜åŒ–ï¼ˆBAPOï¼‰ï¼Œè¯¥æ–¹æ³•é€šè¿‡å¥–åŠ±æœºåˆ¶é¼“åŠ±æ™ºèƒ½ä½“åœ¨æ¨ç†è¾¾åˆ°æé™æ—¶ç»™å‡ºâ€œæˆ‘ä¸çŸ¥é“â€çš„å›ç­”ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒBAPOæ˜¾è‘—æå‡äº†æ™ºèƒ½ä½“æœç´¢çš„æ•´ä½“å¯é æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.10909', 'title': 'FrankenMotion: Part-level Human Motion Generation and Composition', 'url': 'https://huggingface.co/papers/2601.10909', 'abstract': 'A diffusion-based framework generates human motion from text prompts with fine-grained part-level control using a newly constructed dataset with atomic, temporally-aware annotations.  \t\t\t\t\tAI-generated summary \t\t\t\t Human motion generation from text prompts has made remarkable progress in recent years. However, existing methods primarily rely on either sequence-level or action-level descriptions due to the absence of fine-grained, part-level motion annotations. This limits their controllability over individual body parts. In this work, we construct a high-quality motion dataset with atomic, temporally-aware part-level text annotations, leveraging the reasoning capabilities of large language models (LLMs). Unlike prior datasets that either provide synchronized part captions with fixed time segments or rely solely on global sequence labels, our dataset captures asynchronous and semantically distinct part movements at fine temporal resolution. Based on this dataset, we introduce a diffusion-based part-aware motion generation framework, namely FrankenMotion, where each body part is guided by its own temporally-structured textual prompt. This is, to our knowledge, the first work to provide atomic, temporally-aware part-level motion annotations and have a model that allows motion generation with both spatial (body part) and temporal (atomic action) control. Experiments demonstrate that FrankenMotion outperforms all previous baseline models adapted and retrained for our setting, and our model can compose motions unseen during training. Our code and dataset will be publicly available upon publication.', 'score': 10, 'issue_id': 640, 'pub_date': '2026-01-15', 'pub_date_card': {'ru': '15 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 15', 'zh': '1æœˆ15æ—¥'}, 'hash': 'a6f74fdfe5df1e23', 'authors': ['Chuqiao Li', 'Xianghui Xie', 'Yong Cao', 'Andreas Geiger', 'Gerard Pons-Moll'], 'affiliations': ['Max Planck Institute for Informatics, Saarland Informatics Campus, Germany', 'Tubingen AI Center, University of Tubingen, Germany'], 'pdf_title_img': 'assets/pdf/title_img/2601.10909.jpg', 'data': {'categories': ['#video', '#dataset', '#multimodal'], 'emoji': 'ğŸ•º', 'ru': {'title': 'Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ñ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼ Ğ½Ğ°Ğ´ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ñ‡Ğ°ÑÑ‚ÑŒÑ Ñ‚ĞµĞ»Ğ°', 'desc': 'ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ñ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸, Ğ¿Ñ€Ğ¸Ğ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ‡Ğ°ÑÑ‚ĞµĞ¹ Ñ‚ĞµĞ»Ğ°, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ FrankenMotion, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ÑĞ¼ Ñ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼ Ğ½Ğ°Ğ´ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ñ‡Ğ°ÑÑ‚ÑŒÑ Ñ‚ĞµĞ»Ğ° Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ²Ğ°Ğ»Ğ°Ğ¼Ğ¸ Ğ°Ñ‚Ğ¾Ğ¼Ğ°Ñ€Ğ½Ñ‹Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ğ»ÑÑ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼ ĞºĞ°Ğº Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğ¸ (Ñ‡Ğ°ÑÑ‚Ğ¸ Ñ‚ĞµĞ»Ğ°), Ñ‚Ğ°Ğº Ğ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¼ (Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ). Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ²ÑĞµ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±ĞµĞ½ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ½Ğµ Ğ²ÑÑ‚Ñ€ĞµÑ‡Ğ°Ğ»Ğ¸ÑÑŒ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'FrankenMotion: Fine-Grained Control of Human Motion Generation from Text', 'desc': 'This paper presents a novel framework called FrankenMotion for generating human motion from text prompts with detailed control over individual body parts. It addresses the limitations of previous methods by introducing a new dataset that includes atomic, temporally-aware annotations for part-level movements. By utilizing large language models, the framework allows for precise guidance of each body part through its own structured textual prompt. The results show that FrankenMotion significantly outperforms existing models, demonstrating its ability to create complex motions that were not part of the training data.'}, 'zh': {'title': 'ç»†ç²’åº¦æ§åˆ¶çš„äººä½“è¿åŠ¨ç”Ÿæˆ', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£çš„æ¡†æ¶ï¼Œç”¨äºä»æ–‡æœ¬æç¤ºç”Ÿæˆäººä½“è¿åŠ¨ï¼Œå¹¶å®ç°ç»†ç²’åº¦çš„éƒ¨ä½çº§æ§åˆ¶ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªé«˜è´¨é‡çš„è¿åŠ¨æ•°æ®é›†ï¼ŒåŒ…å«åŸå­çº§ã€æ—¶é—´æ„ŸçŸ¥çš„éƒ¨ä½æ–‡æœ¬æ³¨é‡Šï¼Œå…‹æœäº†ç°æœ‰æ–¹æ³•åœ¨æ§åˆ¶ä¸ªåˆ«èº«ä½“éƒ¨ä½æ—¶çš„å±€é™æ€§ã€‚ä¸ä»¥å¾€çš„æ•°æ®é›†ä¸åŒï¼Œæˆ‘ä»¬çš„æ•°æ®é›†æ•æ‰äº†ä¸åŒæ­¥ä¸”è¯­ä¹‰æ˜ç¡®çš„éƒ¨ä½è¿åŠ¨ï¼Œå…·æœ‰ç²¾ç»†çš„æ—¶é—´åˆ†è¾¨ç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFrankenMotionåœ¨è¿åŠ¨ç”Ÿæˆæ–¹é¢ä¼˜äºæ‰€æœ‰ä¹‹å‰çš„åŸºçº¿æ¨¡å‹ï¼Œå¹¶èƒ½å¤Ÿç”Ÿæˆè®­ç»ƒæœŸé—´æœªè§è¿‡çš„è¿åŠ¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.09001', 'title': 'Entropy Sentinel: Continuous LLM Accuracy Monitoring from Decoding Entropy Traces in STEM', 'url': 'https://huggingface.co/papers/2601.09001', 'abstract': 'Output-entropy profiles computed from final-layer next-token probabilities serve as a scalable signal for monitoring LLM performance and prioritizing data acquisition under domain shifts.  \t\t\t\t\tAI-generated summary \t\t\t\t Deploying LLMs raises two coupled challenges: (1) monitoring - estimating where a model underperforms as traffic and domains drift - and (2) improvement - prioritizing data acquisition to close the largest performance gaps. We test whether an inference-time signal can estimate slice-level accuracy under domain shift. For each response, we compute an output-entropy profile from final-layer next-token probabilities (from top-k logprobs) and summarize it with eleven statistics. A lightweight classifier predicts instance correctness, and averaging predicted probabilities yields a domain-level accuracy estimate. We evaluate on ten STEM reasoning benchmarks with exhaustive train/test compositions (k in {1,2,3,4}; all "10 choose k" combinations), across nine LLMs from six families (3B-20B). Estimates often track held-out benchmark accuracy, and several models show near-monotonic ordering of domains. Output-entropy profiles are thus an accessible signal for scalable monitoring and for targeting data acquisition.', 'score': 10, 'issue_id': 649, 'pub_date': '2026-01-13', 'pub_date_card': {'ru': '13 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 13', 'zh': '1æœˆ13æ—¥'}, 'hash': 'fa2ecacb83da9b5f', 'authors': ['Pedro Memoli Buffa', 'Luciano Del Corro'], 'affiliations': ['Departamento de Matematica, FCEyN Universidad de Buenos Aires', 'ELIAS Lab, Departamento de IngenierÃ¡ Universidad de San Andres'], 'pdf_title_img': 'assets/pdf/title_img/2601.09001.jpg', 'data': {'categories': ['#benchmark', '#inference', '#training', '#data'], 'emoji': 'ğŸ“Š', 'ru': {'title': 'ĞœĞ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³ LLM Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»Ğ¸ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ÑĞµĞ¼Ñ‹Ğµ Ğ¸Ğ· Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ° Ğ² Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½ĞµĞ¼ ÑĞ»Ğ¾Ğµ LLM, Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³Ğ° Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ğ´ĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ñ€Ğ¸ ÑĞ¼ĞµĞ½Ğµ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°ÑÑ‚ Ğ»ĞµĞ³ĞºĞ¸Ğ¹ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ÑƒÑÑ€ĞµĞ´Ğ½ÑÑ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ Ğ´Ğ¾Ğ¼ĞµĞ½Ñƒ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ñ‚ĞµÑÑ‚Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° Ğ´ĞµÑÑÑ‚Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¸ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ Ğ´ĞµĞ²ÑÑ‚ÑŒ LLM Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ Ğ¾Ñ‚ 3B Ğ´Ğ¾ 20B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞÑ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ ĞºĞ¾Ñ€Ñ€ĞµĞ»Ğ¸Ñ€ÑƒÑÑ‚ Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ½Ğ° Ğ½ĞµĞ²Ğ¸Ğ´Ğ¸Ğ¼Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ ÑÑ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ¼ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³Ğ° Ğ¸ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑĞ±Ğ¾Ñ€Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Scalable Monitoring of LLMs Using Output-Entropy Profiles', 'desc': "This paper discusses a method for monitoring the performance of large language models (LLMs) as they encounter different types of data. It introduces the concept of output-entropy profiles, which are derived from the probabilities of the model's predictions, to assess how well the model is performing in various domains. By using these profiles, the authors propose a way to identify where the model struggles and prioritize data collection to improve its accuracy. The approach is tested across multiple benchmarks and shows promise in providing a scalable solution for monitoring and enhancing LLM performance."}, 'zh': {'title': 'åˆ©ç”¨è¾“å‡ºç†µè½®å»“ç›‘æµ‹LLMæ€§èƒ½', 'desc': 'æœ¬æ–‡æ¢è®¨äº†å¦‚ä½•ç›‘æµ‹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ€§èƒ½å¹¶ä¼˜å…ˆè·å–æ•°æ®ä»¥åº”å¯¹é¢†åŸŸå˜åŒ–ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºè¾“å‡ºç†µè½®å»“çš„æ–¹æ³•ï¼Œé€šè¿‡è®¡ç®—æœ€ç»ˆå±‚çš„ä¸‹ä¸€ä¸ªæ ‡è®°æ¦‚ç‡æ¥è¯„ä¼°æ¨¡å‹åœ¨ä¸åŒé¢†åŸŸçš„å‡†ç¡®æ€§ã€‚ä½¿ç”¨è½»é‡çº§åˆ†ç±»å™¨é¢„æµ‹å®ä¾‹çš„æ­£ç¡®æ€§ï¼Œå¹¶é€šè¿‡å¹³å‡é¢„æµ‹æ¦‚ç‡æ¥ä¼°è®¡é¢†åŸŸçº§çš„å‡†ç¡®æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¾“å‡ºç†µè½®å»“èƒ½å¤Ÿæœ‰æ•ˆè·Ÿè¸ªåŸºå‡†å‡†ç¡®æ€§ï¼Œå¹¶ä¸ºæ•°æ®è·å–æä¾›äº†å¯æ‰©å±•çš„ä¿¡å·ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.09195', 'title': 'ProFit: Leveraging High-Value Signals in SFT via Probability-Guided Token Selection', 'url': 'https://huggingface.co/papers/2601.09195', 'abstract': 'Supervised fine-tuning with multiple references addresses overfitting to non-core expressions by masking low-probability tokens based on their semantic importance.  \t\t\t\t\tAI-generated summary \t\t\t\t Supervised fine-tuning (SFT) is a fundamental post-training strategy to align Large Language Models (LLMs) with human intent. However, traditional SFT often ignores the one-to-many nature of language by forcing alignment with a single reference answer, leading to the model overfitting to non-core expressions. Although our empirical analysis suggests that introducing multiple reference answers can mitigate this issue, the prohibitive data and computational costs necessitate a strategic shift: prioritizing the mitigation of single-reference overfitting over the costly pursuit of answer diversity. To achieve this, we reveal the intrinsic connection between token probability and semantic importance: high-probability tokens carry the core logical framework, while low-probability tokens are mostly replaceable expressions. Based on this insight, we propose ProFit, which selectively masks low-probability tokens to prevent surface-level overfitting. Extensive experiments confirm that ProFit consistently outperforms traditional SFT baselines on general reasoning and mathematical benchmarks.', 'score': 9, 'issue_id': 640, 'pub_date': '2026-01-14', 'pub_date_card': {'ru': '14 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 14', 'zh': '1æœˆ14æ—¥'}, 'hash': '687e04f44bf56850', 'authors': ['Tao Liu', 'Taiqiang Wu', 'Runming Yang', 'Shaoning Sun', 'Junjie Wang', 'Yujiu Yang'], 'affiliations': ['The University of Hong Kong', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2601.09195.jpg', 'data': {'categories': ['#benchmark', '#training'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Ğ¦ĞµĞ»ĞµĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½ĞµĞ·Ğ½Ğ°Ñ‡Ğ¸Ğ¼Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ ProFit Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ÑÑ‚-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ SFT Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ñ‚Ğ°Ğº ĞºĞ°Ğº Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° Ğ¾Ğ´Ğ¸Ğ½ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‚Ğ¾ĞºĞµĞ½Ğ° Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ ÑĞ²ÑĞ·Ğ°Ğ½Ğ° Ñ ĞµĞ³Ğ¾ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ: Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ½ĞµÑÑƒÑ‚ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½ÑƒÑ Ğ»Ğ¾Ğ³Ğ¸ĞºÑƒ, Ğ° Ğ½Ğ¸Ğ·ĞºĞ¾Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ñ‹Ğµ â€” ÑÑ‚Ğ¾ Ğ·Ğ°Ğ¼ĞµĞ½ÑĞµĞ¼Ñ‹Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞµĞ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¼Ğ°ÑĞºĞ¸Ñ€ÑƒĞµÑ‚ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹, Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰Ğ°Ñ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ¸.'}, 'en': {'title': 'Enhancing Fine-Tuning by Focusing on Core Concepts', 'desc': 'This paper introduces ProFit, a method for improving supervised fine-tuning (SFT) of Large Language Models (LLMs) by addressing the issue of overfitting to non-core expressions. Traditional SFT often relies on a single reference answer, which can lead to models learning irrelevant details instead of core concepts. ProFit strategically masks low-probability tokens, which are less semantically important, allowing the model to focus on high-probability tokens that represent the main ideas. The results show that ProFit outperforms standard SFT methods in tasks requiring general reasoning and mathematical skills.'}, 'zh': {'title': 'ProFitï¼šæå‡è¯­è¨€æ¨¡å‹çš„æ ¸å¿ƒè¡¨è¾¾èƒ½åŠ›', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ç›‘ç£å¾®è°ƒæ–¹æ³•ProFitï¼Œæ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è®­ç»ƒä¸­å¯¹éæ ¸å¿ƒè¡¨è¾¾çš„è¿‡æ‹Ÿåˆé—®é¢˜ã€‚ä¼ ç»Ÿçš„ç›‘ç£å¾®è°ƒé€šå¸¸åªä¾èµ–å•ä¸€å‚è€ƒç­”æ¡ˆï¼Œå¯¼è‡´æ¨¡å‹è¿‡åº¦æ‹ŸåˆäºæŸäº›è¡¨é¢è¡¨è¾¾ã€‚é€šè¿‡å¼•å…¥å¤šä¸ªå‚è€ƒç­”æ¡ˆï¼ŒProFitèƒ½å¤Ÿæœ‰æ•ˆå‡å°‘è¿™ç§è¿‡æ‹Ÿåˆï¼ŒåŒæ—¶é€šè¿‡æ©è”½ä½æ¦‚ç‡æ ‡è®°æ¥ä¿æŒæ¨¡å‹çš„æ ¸å¿ƒé€»è¾‘æ¡†æ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒProFitåœ¨ä¸€èˆ¬æ¨ç†å’Œæ•°å­¦åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºä¼ ç»Ÿçš„ç›‘ç£å¾®è°ƒæ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.11514', 'title': 'ShapeR: Robust Conditional 3D Shape Generation from Casual Captures', 'url': 'https://huggingface.co/papers/2601.11514', 'abstract': 'ShapeR generates high-fidelity 3D shapes from casual image sequences using visual-inertial SLAM, 3D detection, and vision-language models with rectified flow transformer conditioning.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in 3D shape generation have achieved impressive results, but most existing methods rely on clean, unoccluded, and well-segmented inputs. Such conditions are rarely met in real-world scenarios. We present ShapeR, a novel approach for conditional 3D object shape generation from casually captured sequences. Given an image sequence, we leverage off-the-shelf visual-inertial SLAM, 3D detection algorithms, and vision-language models to extract, for each object, a set of sparse SLAM points, posed multi-view images, and machine-generated captions. A rectified flow transformer trained to effectively condition on these modalities then generates high-fidelity metric 3D shapes. To ensure robustness to the challenges of casually captured data, we employ a range of techniques including on-the-fly compositional augmentations, a curriculum training scheme spanning object- and scene-level datasets, and strategies to handle background clutter. Additionally, we introduce a new evaluation benchmark comprising 178 in-the-wild objects across 7 real-world scenes with geometry annotations. Experiments show that ShapeR significantly outperforms existing approaches in this challenging setting, achieving an improvement of 2.7x in Chamfer distance compared to state of the art.', 'score': 7, 'issue_id': 648, 'pub_date': '2026-01-16', 'pub_date_card': {'ru': '16 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 16', 'zh': '1æœˆ16æ—¥'}, 'hash': 'e76e2e46a2ba726d', 'authors': ['Yawar Siddiqui', 'Duncan Frost', 'Samir Aroudj', 'Armen Avetisyan', 'Henry Howard-Jenkins', 'Daniel DeTone', 'Pierre Moulon', 'Qirui Wu', 'Zhengqin Li', 'Julian Straub', 'Richard Newcombe', 'Jakob Engel'], 'affiliations': ['Meta Reality Labs Research', 'Simon Fraser University'], 'pdf_title_img': 'assets/pdf/title_img/2601.11514.jpg', 'data': {'categories': ['#3d', '#synthetic', '#training', '#benchmark', '#multimodal'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ’Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… 3D Ñ„Ğ¾Ñ€Ğ¼ Ğ¸Ğ· ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾ ÑĞ½ÑÑ‚Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'ShapeR â€” ÑÑ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… 3D Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸Ğ· Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ñ„Ğ¾Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-Ğ¸Ğ½ĞµÑ€Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ SLAM, Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ñ‹ 3D Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ğ¸ Ğ¸ vision-language Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ñ… Ñ‚Ğ¾Ñ‡ĞµĞº, Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ²Ğ¸Ğ´Ğ¾Ğ²Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². Ğ¢Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ Ñ rectified flow Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ 3D Ñ„Ğ¾Ñ€Ğ¼Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¸Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ curriculum learning Ğ´Ğ»Ñ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğº ÑˆÑƒĞ¼Ğ½Ñ‹Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ² 2.7 Ñ€Ğ°Ğ·Ğ° Ğ¿Ğ¾ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞµ Chamfer distance Ğ½Ğ° Ğ½Ğ¾Ğ²Ğ¾Ğ¼ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ†ĞµĞ½ Ğ² ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¸ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'ShapeR: Transforming Casual Images into High-Fidelity 3D Shapes', 'desc': 'ShapeR is a new method for creating detailed 3D shapes from casual image sequences, addressing the limitations of previous techniques that require clean and well-defined inputs. It combines visual-inertial SLAM, 3D detection, and vision-language models to gather essential data like sparse SLAM points and multi-view images for each object. A rectified flow transformer is then used to generate high-quality 3D shapes from this data, ensuring robustness against real-world challenges such as background clutter. The method has been tested against a new benchmark of 178 objects and shows a significant improvement over existing methods, achieving better accuracy in 3D shape generation.'}, 'zh': {'title': 'ShapeRï¼šä»éšæ„å›¾åƒç”Ÿæˆé«˜ä¿çœŸ3Då½¢çŠ¶', 'desc': 'ShapeRæ˜¯ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œå¯ä»¥ä»éšæ„æ•è·çš„å›¾åƒåºåˆ—ä¸­ç”Ÿæˆé«˜ä¿çœŸåº¦çš„3Dç‰©ä½“å½¢çŠ¶ã€‚è¯¥æ–¹æ³•ç»“åˆäº†è§†è§‰æƒ¯æ€§SLAMã€3Dæ£€æµ‹ç®—æ³•å’Œè§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œæå–æ¯ä¸ªç‰©ä½“çš„ç¨€ç–SLAMç‚¹ã€å¤šä¸ªè§†è§’çš„å›¾åƒå’Œæœºå™¨ç”Ÿæˆçš„æè¿°ã€‚é€šè¿‡è®­ç»ƒçš„æ ¡æ­£æµå˜æ¢å™¨ï¼ŒShapeRèƒ½å¤Ÿæœ‰æ•ˆåœ°å¤„ç†è¿™äº›è¾“å…¥ï¼Œç”Ÿæˆé«˜è´¨é‡çš„3Då½¢çŠ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒShapeRåœ¨å¤„ç†çœŸå®ä¸–ç•Œæ•°æ®æ—¶ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒChamferè·ç¦»æé«˜äº†2.7å€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.10781', 'title': 'Future Optical Flow Prediction Improves Robot Control & Video Generation', 'url': 'https://huggingface.co/papers/2601.10781', 'abstract': 'A novel language-conditioned optical flow forecasting model combines Vision-Language Model and Diffusion architecture to predict future motion from noisy web-scale video data, demonstrating versatility in robotic manipulation and video generation tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Future motion representations, such as optical flow, offer immense value for control and generative tasks. However, forecasting generalizable spatially dense motion representations remains a key challenge, and learning such forecasting from noisy, real-world data remains relatively unexplored. We introduce FOFPred, a novel language-conditioned optical flow forecasting model featuring a unified Vision-Language Model (VLM) and Diffusion architecture. This unique combination enables strong multimodal reasoning with pixel-level generative fidelity for future motion prediction. Our model is trained on web-scale human activity data-a highly scalable but unstructured source. To extract meaningful signals from this noisy video-caption data, we employ crucial data preprocessing techniques and our unified architecture with strong image pretraining. The resulting trained model is then extended to tackle two distinct downstream tasks in control and generation. Evaluations across robotic manipulation and video generation under language-driven settings establish the cross-domain versatility of FOFPred, confirming the value of a unified VLM-Diffusion architecture and scalable learning from diverse web data for future optical flow prediction.', 'score': 6, 'issue_id': 654, 'pub_date': '2026-01-15', 'pub_date_card': {'ru': '15 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 15', 'zh': '1æœˆ15æ—¥'}, 'hash': '100f67342322af94', 'authors': ['Kanchana Ranasinghe', 'Honglu Zhou', 'Yu Fang', 'Luyu Yang', 'Le Xue', 'Ran Xu', 'Caiming Xiong', 'Silvio Savarese', 'Michael S Ryoo', 'Juan Carlos Niebles'], 'affiliations': ['Salesforce AI Research', 'Stony Brook University'], 'pdf_title_img': 'assets/pdf/title_img/2601.10781.jpg', 'data': {'categories': ['#multimodal', '#architecture', '#diffusion', '#video', '#transfer_learning', '#data', '#robotics'], 'emoji': 'ğŸŒŠ', 'ru': {'title': 'ĞŸÑ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· ÑĞ·Ñ‹Ğº: Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ñ Ğ²ÑÑ‚Ñ€ĞµÑ‡Ğ°ĞµÑ‚ Ğ·Ñ€ĞµĞ½Ğ¸Ğµ', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ÑÑ FOFPred â€” Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ° Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Vision-Language Model Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ Ğ¼Ğ¾Ñ‰Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ğ»Ğ¾ÑÑŒ Ğ½Ğ° Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… Ğ²ĞµĞ±-Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑÑĞ¼Ğ¸, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ñ‹Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ¸Ğ· ÑˆÑƒĞ¼Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾Ğ´ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°.'}, 'en': {'title': 'FOFPred: Predicting Future Motion with Language and Vision', 'desc': "This paper presents FOFPred, a new model for predicting future motion in videos using a combination of a Vision-Language Model and a Diffusion architecture. The model addresses the challenge of forecasting optical flow from noisy, real-world video data, which is often unstructured and complex. By leveraging large-scale human activity data and advanced preprocessing techniques, FOFPred achieves high-quality predictions with pixel-level accuracy. The model's effectiveness is demonstrated in two applications: robotic manipulation and video generation, showcasing its versatility across different tasks."}, 'zh': {'title': 'è¯­è¨€é©±åŠ¨çš„å…‰æµé¢„æµ‹æ–°æ¨¡å‹', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°é¢–çš„è¯­è¨€æ¡ä»¶å…‰æµé¢„æµ‹æ¨¡å‹FOFPredï¼Œè¯¥æ¨¡å‹ç»“åˆäº†è§†è§‰-è¯­è¨€æ¨¡å‹å’Œæ‰©æ•£æ¶æ„ï¼Œèƒ½å¤Ÿä»å™ªå£°è¾ƒå¤§çš„ç½‘ç»œè§†é¢‘æ•°æ®ä¸­é¢„æµ‹æœªæ¥çš„è¿åŠ¨ã€‚å…‰æµç­‰æœªæ¥è¿åŠ¨è¡¨ç¤ºåœ¨æ§åˆ¶å’Œç”Ÿæˆä»»åŠ¡ä¸­å…·æœ‰é‡è¦ä»·å€¼ï¼Œä½†ä»çœŸå®ä¸–ç•Œçš„å™ªå£°æ•°æ®ä¸­å­¦ä¹ å¯æ³›åŒ–çš„ç©ºé—´å¯†é›†è¿åŠ¨è¡¨ç¤ºä»ç„¶æ˜¯ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ã€‚æˆ‘ä»¬é€šè¿‡æ•°æ®é¢„å¤„ç†æŠ€æœ¯å’Œå¼ºå¤§çš„å›¾åƒé¢„è®­ç»ƒï¼Œæå–æœ‰æ„ä¹‰çš„ä¿¡å·ï¼Œå¹¶å°†æ¨¡å‹æ‰©å±•åˆ°æœºå™¨äººæ“ä½œå’Œè§†é¢‘ç”Ÿæˆç­‰ä¸‹æ¸¸ä»»åŠ¡ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒFOFPredåœ¨è¯­è¨€é©±åŠ¨çš„è®¾ç½®ä¸‹å±•ç°äº†è·¨é¢†åŸŸçš„å¤šåŠŸèƒ½æ€§ï¼ŒéªŒè¯äº†ç»Ÿä¸€çš„è§†è§‰-è¯­è¨€æ¨¡å‹å’Œæ‰©æ•£æ¶æ„åœ¨æœªæ¥å…‰æµé¢„æµ‹ä¸­çš„ä»·å€¼ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.10825', 'title': 'Reasoning Models Generate Societies of Thought', 'url': 'https://huggingface.co/papers/2601.10825', 'abstract': 'Reasoning models demonstrate enhanced performance through multi-agent-like interactions that create diverse cognitive perspectives and improve problem-solving through structured social organization.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models have achieved remarkable capabilities across domains, yet mechanisms underlying sophisticated reasoning remain elusive. Recent reasoning models outperform comparable instruction-tuned models on complex cognitive tasks, attributed to extended computation through longer chains of thought. Here we show that enhanced reasoning emerges not from extended computation alone, but from simulating multi-agent-like interactions -- a society of thought -- which enables diversification and debate among internal cognitive perspectives characterized by distinct personality traits and domain expertise. Through quantitative analysis and mechanistic interpretability methods applied to reasoning traces, we find that reasoning models like DeepSeek-R1 and QwQ-32B exhibit much greater perspective diversity than instruction-tuned models, activating broader conflict between heterogeneous personality- and expertise-related features during reasoning. This multi-agent structure manifests in conversational behaviors, including question-answering, perspective shifts, and the reconciliation of conflicting views, and in socio-emotional roles that characterize sharp back-and-forth conversations, together accounting for the accuracy advantage in reasoning tasks. Controlled reinforcement learning experiments reveal that base models increase conversational behaviors when rewarded solely for reasoning accuracy, and fine-tuning models with conversational scaffolding accelerates reasoning improvement over base models. These findings indicate that the social organization of thought enables effective exploration of solution spaces. We suggest that reasoning models establish a computational parallel to collective intelligence in human groups, where diversity enables superior problem-solving when systematically structured, which suggests new opportunities for agent organization to harness the wisdom of crowds.', 'score': 5, 'issue_id': 640, 'pub_date': '2026-01-15', 'pub_date_card': {'ru': '15 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 15', 'zh': '1æœˆ15æ—¥'}, 'hash': '7dcd7d60c59e0050', 'authors': ['Junsol Kim', 'Shiyang Lai', 'Nino Scherrer', 'Blaise AgÃ¼era y Arcas', 'James Evans'], 'affiliations': ['Google, Paradigms of Intelligence Team', 'Santa Fe Institute', 'University of Chicago'], 'pdf_title_img': 'assets/pdf/title_img/2601.10825.jpg', 'data': {'categories': ['#interpretability', '#reasoning', '#training', '#agents', '#architecture', '#rlhf'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ¡Ğ¸Ğ»Ğ° ĞºĞ¾Ğ»Ğ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ: ĞºĞ°Ğº Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ĞµĞµ Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ğ¾ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ² ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ²Ñ‹ÑĞ¾ĞºĞ°Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº DeepSeek-R1 Ğ¸ QwQ-32B, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ÑÑ Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ, Ğ½Ğ¾ Ğ¸ Ğ·Ğ° ÑÑ‡Ñ‘Ñ‚ Ğ¸Ğ¼Ğ¸Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ - Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ğ° Ğ¼Ñ‹ÑĞ»ĞµĞ¹ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ°Ğ¼Ğ¸ Ğ¸ Ñ€Ğ¾Ğ»ÑĞ¼Ğ¸. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ», Ñ‡Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ¸Ñ€ÑƒÑÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞµĞµ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ², ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ»Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ğ¼Ğ¸ Ñ‡ĞµÑ€Ñ‚Ğ°Ğ¼Ğ¸ Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¸Ğ·Ğ¾Ğ¹, ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ ĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚Ğ¾Ñ‡ĞºĞ°Ğ¼Ğ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ° Ñ ĞºĞ¾Ğ»Ğ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ¾Ğ¼ Ğ² Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ°Ñ….'}, 'en': {'title': 'Harnessing Collective Intelligence for Enhanced Reasoning', 'desc': "This paper explores how reasoning models can improve their performance by simulating interactions similar to those found in multi-agent systems. It highlights that the success of these models is not just due to longer computation times, but also from the diversity of perspectives created through internal cognitive debates. By analyzing reasoning processes, the authors demonstrate that models like DeepSeek-R1 and QwQ-32B show greater diversity in thought, leading to better problem-solving outcomes. The study suggests that organizing these cognitive perspectives in a structured way can mimic collective intelligence, enhancing the models' ability to tackle complex tasks."}, 'zh': {'title': 'å¤šæ™ºèƒ½ä½“äº’åŠ¨æå‡æ¨ç†èƒ½åŠ›', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†æ¨ç†æ¨¡å‹å¦‚ä½•é€šè¿‡æ¨¡æ‹Ÿå¤šæ™ºèƒ½ä½“çš„äº’åŠ¨æ¥æå‡æ€§èƒ½ã€‚è¿™ç§äº’åŠ¨åˆ›é€ äº†å¤šæ ·åŒ–çš„è®¤çŸ¥è§†è§’ï¼Œä½¿å¾—é—®é¢˜è§£å†³æ›´åŠ é«˜æ•ˆã€‚ç ”ç©¶è¡¨æ˜ï¼Œæ¨ç†æ¨¡å‹çš„ä¼˜åŠ¿ä¸ä»…æ¥è‡ªäºå»¶é•¿çš„è®¡ç®—é“¾ï¼Œè¿˜æºäºå†…éƒ¨è®¤çŸ¥è§†è§’ä¹‹é—´çš„è¾©è®ºå’Œå¤šæ ·æ€§ã€‚é€šè¿‡é‡åŒ–åˆ†æï¼Œå‘ç°è¿™äº›æ¨¡å‹åœ¨æ¨ç†ä»»åŠ¡ä¸­å±•ç°å‡ºæ›´å¤§çš„è§†è§’å¤šæ ·æ€§ï¼Œä»è€Œæé«˜äº†å‡†ç¡®æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.11087', 'title': 'PhysRVG: Physics-Aware Unified Reinforcement Learning for Video Generative Models', 'url': 'https://huggingface.co/papers/2601.11087', 'abstract': "A physics-aware reinforcement learning paradigm is introduced for video generation that enforces physical collision rules directly in high-dimensional spaces, ensuring strict application of physics knowledge rather than treating it as conditional constraints.  \t\t\t\t\tAI-generated summary \t\t\t\t Physical principles are fundamental to realistic visual simulation, but remain a significant oversight in transformer-based video generation. This gap highlights a critical limitation in rendering rigid body motion, a core tenet of classical mechanics. While computer graphics and physics-based simulators can easily model such collisions using Newton formulas, modern pretrain-finetune paradigms discard the concept of object rigidity during pixel-level global denoising. Even perfectly correct mathematical constraints are treated as suboptimal solutions (i.e., conditions) during model optimization in post-training, fundamentally limiting the physical realism of generated videos. Motivated by these considerations, we introduce, for the first time, a physics-aware reinforcement learning paradigm for video generation models that enforces physical collision rules directly in high-dimensional spaces, ensuring the physics knowledge is strictly applied rather than treated as conditions. Subsequently, we extend this paradigm to a unified framework, termed Mimicry-Discovery Cycle (MDcycle), which allows substantial fine-tuning while fully preserving the model's ability to leverage physics-grounded feedback. To validate our approach, we construct new benchmark PhysRVGBench and perform extensive qualitative and quantitative experiments to thoroughly assess its effectiveness.", 'score': 4, 'issue_id': 640, 'pub_date': '2026-01-16', 'pub_date_card': {'ru': '16 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 16', 'zh': '1æœˆ16æ—¥'}, 'hash': '7de1634e59778ee2', 'authors': ['Qiyuan Zhang', 'Biao Gong', 'Shuai Tan', 'Zheng Zhang', 'Yujun Shen', 'Xing Zhu', 'Yuyuan Li', 'Kelu Yao', 'Chunhua Shen', 'Changqing Zou'], 'affiliations': ['Ant Group', 'Zhejiang Lab', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2601.11087.jpg', 'data': {'categories': ['#rl', '#video', '#training', '#benchmark'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ¤Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ°ĞºĞ¾Ğ½Ñ‹ ĞºĞ°Ğº Ğ½ĞµĞ¾Ñ‚ÑŠĞµĞ¼Ğ»ĞµĞ¼Ğ°Ñ Ñ‡Ğ°ÑÑ‚ÑŒ, Ğ° Ğ½Ğµ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğµ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¶Ñ‘ÑÑ‚ĞºĞ¾ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ°ĞºĞ¾Ğ½Ñ‹ ÑÑ‚Ğ¾Ğ»ĞºĞ½Ğ¾Ğ²ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€ÑĞ¼Ğ¾ Ğ² Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¼ĞµÑ€Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ğ¾Ğ³Ğ¾, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ñ… ĞºĞ°Ğº ÑƒÑĞ»Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ğº Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¸Ğ´ĞµĞ¾: Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ° Ğ¸Ğ³Ğ½Ğ¾Ñ€Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ ĞºĞ°Ğº ÑÑƒĞ±Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ñ. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ĞµĞ´Ğ¸Ğ½Ğ°Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° Mimicry-Discovery Cycle (MDcycle), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº PhysRVGBench Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ñ‹Ğµ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹, Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‰Ğ¸Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°.'}, 'en': {'title': 'Revolutionizing Video Generation with Physics-Aware Learning', 'desc': 'This paper presents a new approach to video generation using reinforcement learning that incorporates physical laws, specifically collision rules, directly into the model. Unlike traditional methods that treat physics as optional constraints, this paradigm ensures that physical principles are strictly followed, enhancing the realism of generated videos. The authors introduce a framework called Mimicry-Discovery Cycle (MDcycle) that allows for fine-tuning while maintaining the benefits of physics-based feedback. To demonstrate the effectiveness of their method, they create a new benchmark dataset, PhysRVGBench, and conduct comprehensive experiments to evaluate performance.'}, 'zh': {'title': 'ç‰©ç†æ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ ï¼šæå‡è§†é¢‘ç”Ÿæˆçš„çœŸå®æ„Ÿ', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§ç‰©ç†æ„ŸçŸ¥çš„å¼ºåŒ–å­¦ä¹ èŒƒå¼ï¼Œç”¨äºè§†é¢‘ç”Ÿæˆï¼Œç›´æ¥åœ¨é«˜ç»´ç©ºé—´ä¸­å¼ºåˆ¶æ‰§è¡Œç‰©ç†ç¢°æ’è§„åˆ™ã€‚è¿™ç§æ–¹æ³•ç¡®ä¿äº†ç‰©ç†çŸ¥è¯†çš„ä¸¥æ ¼åº”ç”¨ï¼Œè€Œä¸æ˜¯å°†å…¶è§†ä¸ºæ¡ä»¶çº¦æŸã€‚ä¼ ç»Ÿçš„ç”Ÿæˆæ¨¡å‹åœ¨åƒç´ çº§å…¨å±€å»å™ªæ—¶å¿½è§†äº†ç‰©ä½“çš„åˆšæ€§ï¼Œå¯¼è‡´ç”Ÿæˆè§†é¢‘çš„ç‰©ç†çœŸå®æ„Ÿå—åˆ°é™åˆ¶ã€‚æˆ‘ä»¬è¿˜æ‰©å±•äº†è¿™ä¸€èŒƒå¼ï¼Œå½¢æˆäº†ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œç§°ä¸ºæ¨¡ä»¿å‘ç°å¾ªç¯ï¼ˆMDcycleï¼‰ï¼Œä»¥ä¾¿åœ¨å……åˆ†ä¿ç•™æ¨¡å‹åˆ©ç”¨ç‰©ç†åé¦ˆèƒ½åŠ›çš„åŒæ—¶è¿›è¡Œå¤§è§„æ¨¡å¾®è°ƒã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.09636', 'title': 'PersonalAlign: Hierarchical Implicit Intent Alignment for Personalized GUI Agent with Long-Term User-Centric Records', 'url': 'https://huggingface.co/papers/2601.09636', 'abstract': "PersonalAlign framework addresses GUI agent alignment with implicit user intents through hierarchical memory organization and long-term record reasoning, improving both execution and proactive performance.  \t\t\t\t\tAI-generated summary \t\t\t\t While GUI agents have shown strong performance under explicit and completion instructions, real-world deployment requires aligning with users' more complex implicit intents. In this work, we highlight Hierarchical Implicit Intent Alignment for Personalized GUI Agent (PersonalAlign), a new agent task that requires agents to leverage long-term user records as persistent context to resolve omitted preferences in vague instructions and anticipate latent routines by user state for proactive assistance. To facilitate this study, we introduce AndroidIntent, a benchmark designed to evaluate agents' ability in resolving vague instructions and providing proactive suggestions through reasoning over long-term user records. We annotated 775 user-specific preferences and 215 routines from 20k long-term records across different users for evaluation. Furthermore, we introduce Hierarchical Intent Memory Agent (HIM-Agent), which maintains a continuously updating personal memory and hierarchically organizes user preferences and routines for personalization. Finally, we evaluate a range of GUI agents on AndroidIntent, including GPT-5, Qwen3-VL, and UI-TARS, further results show that HIM-Agent significantly improves both execution and proactive performance by 15.7% and 7.3%.", 'score': 4, 'issue_id': 648, 'pub_date': '2026-01-14', 'pub_date_card': {'ru': '14 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 14', 'zh': '1æœˆ14æ—¥'}, 'hash': 'c7c5d5c2262a2509', 'authors': ['Yibo Lyu', 'Gongwei Chen', 'Rui Shao', 'Weili Guan', 'Liqiang Nie'], 'affiliations': ['Harbin Institute of Technology, Shenzhen'], 'pdf_title_img': 'assets/pdf/title_img/2601.09636.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#multimodal', '#agents'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ›Ğ¸Ñ‡Ğ½Ğ°Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ Ğ² GUI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ñ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ PersonalAlign â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ GUI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ğ½ĞµÑĞ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ´ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ¿Ğ¸ÑÑĞ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº AndroidIntent, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 20k Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹ Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ñ€ÑƒÑ‚Ğ¸Ğ½Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞ°Ñ‚ÑŒ Ğ½ĞµĞ¾Ğ´Ğ½Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ½Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ HIM-Agent â€” Ğ°Ğ³ĞµĞ½Ñ‚ Ñ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑĞµÑ‚ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ¸ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ½Ğ° 15.7% Ğ¸ Ğ¿Ñ€Ğ¾Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ¸ Ğ½Ğ° 7.3% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ GUI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Aligning GUI Agents with User Intent for Proactive Assistance', 'desc': "The PersonalAlign framework enhances the alignment of GUI agents with users' implicit intents by utilizing a hierarchical memory structure and reasoning over long-term user records. This approach allows agents to interpret vague instructions and anticipate user needs, leading to more proactive assistance. The study introduces the AndroidIntent benchmark, which assesses agents' capabilities in handling ambiguous commands and providing tailored suggestions. Results demonstrate that the Hierarchical Intent Memory Agent (HIM-Agent) significantly outperforms other agents in both execution and proactive performance metrics."}, 'zh': {'title': 'ä¸ªæ€§åŒ–GUIä»£ç†çš„éšå«æ„å›¾å¯¹é½', 'desc': 'PersonalAlignæ¡†æ¶é€šè¿‡å±‚æ¬¡åŒ–è®°å¿†ç»„ç»‡å’Œé•¿æœŸè®°å½•æ¨ç†ï¼Œè§£å†³äº†å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†ä¸ç”¨æˆ·éšå«æ„å›¾çš„å¯¹é½é—®é¢˜ï¼Œä»è€Œæé«˜äº†æ‰§è¡Œå’Œä¸»åŠ¨æ€§èƒ½ã€‚è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„ä»£ç†ä»»åŠ¡ï¼Œè¦æ±‚ä»£ç†åˆ©ç”¨é•¿æœŸç”¨æˆ·è®°å½•ä½œä¸ºæŒä¹…ä¸Šä¸‹æ–‡ï¼Œä»¥è§£å†³æ¨¡ç³ŠæŒ‡ä»¤ä¸­é—æ¼çš„åå¥½ï¼Œå¹¶é€šè¿‡ç”¨æˆ·çŠ¶æ€é¢„æµ‹æ½œåœ¨çš„æ—¥å¸¸æ´»åŠ¨ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶å›¢é˜Ÿå¼•å…¥äº†AndroidIntentåŸºå‡†ï¼Œè¯„ä¼°ä»£ç†åœ¨å¤„ç†æ¨¡ç³ŠæŒ‡ä»¤å’Œæä¾›ä¸»åŠ¨å»ºè®®æ–¹é¢çš„èƒ½åŠ›ã€‚é€šè¿‡å¯¹775ä¸ªç”¨æˆ·ç‰¹å®šåå¥½å’Œ215ä¸ªæ—¥å¸¸æ´»åŠ¨çš„æ ‡æ³¨ï¼ŒHIM-Agentæ˜¾è‘—æé«˜äº†æ‰§è¡Œå’Œä¸»åŠ¨æ€§èƒ½ï¼Œåˆ†åˆ«æå‡äº†15.7%å’Œ7.3%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.11516', 'title': 'Building Production-Ready Probes For Gemini', 'url': 'https://huggingface.co/papers/2601.11516', 'abstract': "Activation probes for language model misuse mitigation face challenges with long-context generalization, requiring new architectures and diverse training for robust performance across production shifts.  \t\t\t\t\tAI-generated summary \t\t\t\t Frontier language model capabilities are improving rapidly. We thus need stronger mitigations against bad actors misusing increasingly powerful systems. Prior work has shown that activation probes may be a promising misuse mitigation technique, but we identify a key remaining challenge: probes fail to generalize under important production distribution shifts. In particular, we find that the shift from short-context to long-context inputs is difficult for existing probe architectures. We propose several new probe architecture that handle this long-context distribution shift.   We evaluate these probes in the cyber-offensive domain, testing their robustness against various production-relevant shifts, including multi-turn conversations, static jailbreaks, and adaptive red teaming. Our results demonstrate that while multimax addresses context length, a combination of architecture choice and training on diverse distributions is required for broad generalization. Additionally, we show that pairing probes with prompted classifiers achieves optimal accuracy at a low cost due to the computational efficiency of probes.   These findings have informed the successful deployment of misuse mitigation probes in user-facing instances of Gemini, Google's frontier language model. Finally, we find early positive results using AlphaEvolve to automate improvements in both probe architecture search and adaptive red teaming, showing that automating some AI safety research is already possible.", 'score': 3, 'issue_id': 640, 'pub_date': '2026-01-16', 'pub_date_card': {'ru': '16 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 16', 'zh': '1æœˆ16æ—¥'}, 'hash': '87ddd4cbadd8d1f3', 'authors': ['JÃ¡nos KramÃ¡r', 'Joshua Engels', 'Zheng Wang', 'Bilal Chughtai', 'Rohin Shah', 'Neel Nanda', 'Arthur Conmy'], 'affiliations': ['Google DeepMind'], 'pdf_title_img': 'assets/pdf/title_img/2601.11516.jpg', 'data': {'categories': ['#alignment', '#security', '#long_context'], 'emoji': 'ğŸ›¡ï¸', 'ru': {'title': 'Ğ—Ğ°Ñ‰Ğ¸Ñ‚Ğ° Ğ¾Ñ‚ Ğ·Ğ»Ğ¾ÑƒĞ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ğ¹: Ğ·Ğ¾Ğ½Ğ´Ñ‹ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ·Ğ»Ğ¾ÑƒĞ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ·Ğ¾Ğ½Ğ´Ğ¾Ğ² Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ·Ğ¾Ğ½Ğ´Ñ‹ Ğ¿Ğ»Ğ¾Ñ…Ğ¾ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‚ÑÑ Ğ¿Ñ€Ğ¸ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğµ Ñ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ, Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹. ĞĞ½Ğ¸ Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ·Ğ¾Ğ½Ğ´Ñ‹ Ğ½Ğ° ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğº Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ñ‚Ğ¸Ğ¿Ğ°Ğ¼ Ğ°Ñ‚Ğ°Ğº Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ°Ğ¼ Ğ²Ğ·Ğ»Ğ¾Ğ¼Ğ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ñ‡ĞµÑ‚Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½ÑƒÑ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñƒ Ğ¸ ÑƒĞ¶Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ² Ğ±Ğ¾ĞµĞ²Ñ‹Ñ… Ğ²ĞµÑ€ÑĞ¸ÑÑ… Gemini.'}, 'en': {'title': 'Enhancing Misuse Mitigation in Language Models with Robust Probes', 'desc': "This paper discusses the challenges of using activation probes to prevent misuse of language models, especially when dealing with long-context inputs. The authors highlight that existing probe architectures struggle to generalize effectively when the input context length increases. They propose new architectures and emphasize the importance of training on diverse data distributions to improve robustness against various production shifts. The study also shows that combining probes with prompted classifiers can enhance accuracy while maintaining computational efficiency, leading to successful implementations in Google's Gemini model."}, 'zh': {'title': 'æå‡è¯­è¨€æ¨¡å‹æ»¥ç”¨ç¼“è§£çš„æ¢é’ˆæ¶æ„', 'desc': 'æœ¬æ–‡æ¢è®¨äº†æ¿€æ´»æ¢é’ˆåœ¨è¯­è¨€æ¨¡å‹æ»¥ç”¨ç¼“è§£ä¸­çš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨é•¿ä¸Šä¸‹æ–‡æ³›åŒ–æ–¹é¢é¢ä¸´çš„æŒ‘æˆ˜ã€‚ç ”ç©¶å‘ç°ï¼Œç°æœ‰çš„æ¢é’ˆæ¶æ„åœ¨å¤„ç†ä»çŸ­ä¸Šä¸‹æ–‡åˆ°é•¿ä¸Šä¸‹æ–‡çš„è¾“å…¥æ—¶è¡¨ç°ä¸ä½³ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†å‡ ç§æ–°çš„æ¢é’ˆæ¶æ„ï¼Œä»¥åº”å¯¹è¿™ç§é•¿ä¸Šä¸‹æ–‡åˆ†å¸ƒçš„å˜åŒ–ã€‚é€šè¿‡åœ¨ç½‘ç»œæ”»å‡»é¢†åŸŸè¿›è¡Œè¯„ä¼°ï¼Œç»“æœè¡¨æ˜ï¼Œç»“åˆæ¶æ„é€‰æ‹©å’Œå¤šæ ·åŒ–è®­ç»ƒæ˜¯å®ç°å¹¿æ³›æ³›åŒ–çš„å…³é”®ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.11044', 'title': 'AgencyBench: Benchmarking the Frontiers of Autonomous Agents in 1M-Token Real-World Contexts', 'url': 'https://huggingface.co/papers/2601.11044', 'abstract': 'AgencyBench presents a comprehensive benchmark for evaluating autonomous agents across real-world scenarios, enabling automated evaluation through user simulation and sandbox environments while revealing performance gaps between closed-source and open-source models.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) based autonomous agents demonstrate multifaceted capabilities to contribute substantially to economic production. However, existing benchmarks remain focused on single agentic capability, failing to capture long-horizon real-world scenarios. Moreover, the reliance on human-in-the-loop feedback for realistic tasks creates a scalability bottleneck, hindering automated rollout collection and evaluation. To bridge this gap, we introduce AgencyBench, a comprehensive benchmark derived from daily AI usage, evaluating 6 core agentic capabilities across 32 real-world scenarios, comprising 138 tasks with specific queries, deliverables, and rubrics. These scenarios require an average of 90 tool calls, 1 million tokens, and hours of execution time to resolve. To enable automated evaluation, we employ a user simulation agent to provide iterative feedback, and a Docker sandbox to conduct visual and functional rubric-based assessment. Experiments reveal that closed-source models significantly outperform open-source models (48.4% vs 32.1%). Further analysis reveals significant disparities across models in resource efficiency, feedback-driven self-correction, and specific tool-use preferences. Finally, we investigate the impact of agentic scaffolds, observing that proprietary models demonstrate superior performance within their native ecosystems (e.g., Claude-4.5-Opus via Claude-Agent-SDK), while open-source models exhibit distinct performance peaks, suggesting potential optimization for specific execution frameworks. AgencyBench serves as a critical testbed for next-generation agents, highlighting the necessity of co-optimizing model architecture with agentic frameworks. We believe this work sheds light on the future direction of autonomous agents, and we release the full benchmark and evaluation toolkit at https://github.com/GAIR-NLP/AgencyBench.', 'score': 2, 'issue_id': 655, 'pub_date': '2026-01-16', 'pub_date_card': {'ru': '16 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 16', 'zh': '1æœˆ16æ—¥'}, 'hash': '7fa13a458769b5b4', 'authors': ['Keyu Li', 'Junhao Shi', 'Yang Xiao', 'Mohan Jiang', 'Jie Sun', 'Yunze Wu', 'Shijie Xia', 'Xiaojie Cai', 'Tianze Xu', 'Weiye Si', 'Wenjie Li', 'Dequan Wang', 'Pengfei Liu'], 'affiliations': ['GAIR', 'PolyU', 'SII', 'SJTU'], 'pdf_title_img': 'assets/pdf/title_img/2601.11044.jpg', 'data': {'categories': ['#agents', '#open_source', '#benchmark'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚ĞµÑÑ‚Ñ‹ Ğ´Ğ»Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²: Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼', 'desc': 'AgencyBench â€” ÑÑ‚Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLM Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞµ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ Ğ¸ Ğ¸Ğ·Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑÑ€ĞµĞ´Ñƒ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ, Ñ€ĞµÑˆĞ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 32 Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ñ Ñ 138 Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ² ÑÑ€ĞµĞ´Ğ½ĞµĞ¼ 90 Ğ¾Ğ±Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğ¹ Ğº Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼ Ğ¸ Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ñ‚ÑÑ‚Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¾Ñ‚ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… (32.1% Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ² 48.4%), Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ ÑĞ¸Ğ»ÑŒĞ½Ğ¾ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ñ‚ Ğ¾Ñ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾Ğ´ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'AgencyBench: Bridging the Gap in Autonomous Agent Evaluation', 'desc': 'AgencyBench is a new benchmark designed to evaluate autonomous agents in real-world situations, focusing on their performance across various tasks. It addresses the limitations of existing benchmarks that only assess single capabilities by introducing a comprehensive evaluation of six core agentic skills in 32 different scenarios. The benchmark utilizes user simulation for automated feedback and a Docker sandbox for thorough assessment, revealing that closed-source models outperform open-source ones significantly. This work emphasizes the importance of optimizing both model architecture and agentic frameworks to enhance the effectiveness of future autonomous agents.'}, 'zh': {'title': 'AgencyBenchï¼šè¯„ä¼°è‡ªä¸»ä»£ç†çš„æ–°åŸºå‡†', 'desc': 'AgencyBenchæ˜¯ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°è‡ªä¸»ä»£ç†åœ¨ç°å®åœºæ™¯ä¸­çš„è¡¨ç°ã€‚å®ƒé€šè¿‡ç”¨æˆ·æ¨¡æ‹Ÿå’Œæ²™ç®±ç¯å¢ƒå®ç°è‡ªåŠ¨åŒ–è¯„ä¼°ï¼Œæ­ç¤ºäº†é—­æºæ¨¡å‹å’Œå¼€æºæ¨¡å‹ä¹‹é—´çš„æ€§èƒ½å·®è·ã€‚è¯¥åŸºå‡†æ¶µç›–32ä¸ªçœŸå®åœºæ™¯ä¸­çš„6ç§æ ¸å¿ƒä»£ç†èƒ½åŠ›ï¼ŒåŒ…å«138ä¸ªä»»åŠ¡ï¼Œè¦æ±‚è¿›è¡Œå¤§é‡çš„å·¥å…·è°ƒç”¨å’Œæ‰§è¡Œæ—¶é—´ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œé—­æºæ¨¡å‹åœ¨èµ„æºæ•ˆç‡å’Œè‡ªæˆ‘çº æ­£èƒ½åŠ›ä¸Šæ˜¾è‘—ä¼˜äºå¼€æºæ¨¡å‹ï¼Œå¼ºè°ƒäº†æ¨¡å‹æ¶æ„ä¸ä»£ç†æ¡†æ¶å…±åŒä¼˜åŒ–çš„é‡è¦æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.07812', 'title': 'More Images, More Problems? A Controlled Analysis of VLM Failure Modes', 'url': 'https://huggingface.co/papers/2601.07812', 'abstract': 'Large Vision Language Models exhibit significant limitations in multi-image understanding and reasoning, which are revealed through a new benchmark and addressed via procedural data generation and attention masking techniques.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Vision Language Models (LVLMs) have demonstrated remarkable capabilities, yet their proficiency in understanding and reasoning over multiple images remains largely unexplored. While existing benchmarks have initiated the evaluation of multi-image models, a comprehensive analysis of their core weaknesses and their causes is still lacking. In this work, we introduce MIMIC (Multi-Image Model Insights and Challenges), a new benchmark designed to rigorously evaluate the multi-image capabilities of LVLMs. Using MIMIC, we conduct a series of diagnostic experiments that reveal pervasive issues: LVLMs often fail to aggregate information across images and struggle to track or attend to multiple concepts simultaneously. To address these failures, we propose two novel complementary remedies. On the data side, we present a procedural data-generation strategy that composes single-image annotations into rich, targeted multi-image training examples. On the optimization side, we analyze layer-wise attention patterns and derive an attention-masking scheme tailored for multi-image inputs. Experiments substantially improved cross-image aggregation, while also enhancing performance on existing multi-image benchmarks, outperforming prior state of the art across tasks. Data and code will be made available at https://github.com/anurag-198/MIMIC.', 'score': 2, 'issue_id': 646, 'pub_date': '2026-01-12', 'pub_date_card': {'ru': '12 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 12', 'zh': '1æœˆ12æ—¥'}, 'hash': '1087868e6e3d0c16', 'authors': ['Anurag Das', 'Adrian Bulat', 'Alberto Baldrati', 'Ioannis Maniadis Metaxas', 'Bernt Schiele', 'Georgios Tzimiropoulos', 'Brais Martinez'], 'affiliations': ['MPI for Informatics, Saarland Informatics Campus', 'Queen Mary University of London, UK', 'Samsung AI, Cambridge', 'Technical University of Iasi, Romania'], 'pdf_title_img': 'assets/pdf/title_img/2601.07812.jpg', 'data': {'categories': ['#cv', '#multimodal', '#dataset', '#reasoning', '#optimization', '#open_source', '#architecture', '#benchmark', '#synthetic', '#training'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ ÑĞ»ĞµĞ¿Ğ¾Ñ‚Ñ‹: Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¸Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº MIMIC Ğ´Ğ»Ñ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸ ÑĞ»Ğ°Ğ±Ğ¾ÑÑ‚ĞµĞ¹ ÑÑ‚Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°Ñ, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ½Ğ¸ Ğ¿Ğ»Ğ¾Ñ…Ğ¾ Ğ°Ğ³Ñ€ĞµĞ³Ğ¸Ñ€ÑƒÑÑ‚ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ğ½Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ÑÑ Ğ´Ğ²Ğ° Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ÑÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°: Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ÑƒĞµÑ‚ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹, Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ…ĞµĞ¼Ğ° Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° ÑĞ»Ğ¾ĞµĞ² Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºÑ€Ğ¾ÑÑ-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ°Ğ³Ñ€ĞµĞ³Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ….'}, 'en': {'title': 'Enhancing Multi-Image Understanding in LVLMs with MIMIC', 'desc': 'This paper addresses the limitations of Large Vision Language Models (LVLMs) in understanding and reasoning with multiple images. It introduces a new benchmark called MIMIC, which evaluates the multi-image capabilities of these models and identifies key weaknesses, such as poor information aggregation and difficulty in tracking multiple concepts. To improve performance, the authors propose a procedural data generation method for creating multi-image training examples and an attention-masking technique to enhance model focus on relevant information. Experimental results show significant improvements in cross-image aggregation and overall performance on multi-image tasks, surpassing previous state-of-the-art results.'}, 'zh': {'title': 'æå‡å¤šå›¾åƒç†è§£èƒ½åŠ›çš„åˆ›æ–°æ–¹æ³•', 'desc': 'å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤šå›¾åƒç†è§£å’Œæ¨ç†æ–¹é¢å­˜åœ¨æ˜¾è‘—å±€é™æ€§ã€‚æˆ‘ä»¬é€šè¿‡æ–°çš„åŸºå‡†MIMICå¯¹è¿™äº›æ¨¡å‹è¿›è¡Œäº†ä¸¥æ ¼è¯„ä¼°ï¼Œæ­ç¤ºäº†å®ƒä»¬åœ¨ä¿¡æ¯èšåˆå’Œå¤šæ¦‚å¿µè·Ÿè¸ªæ–¹é¢çš„æ™®éé—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç¨‹åºåŒ–æ•°æ®ç”Ÿæˆç­–ç•¥å’Œä¸€ç§é’ˆå¯¹å¤šå›¾åƒè¾“å…¥çš„æ³¨æ„åŠ›æ©è”½æ–¹æ¡ˆã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¿™äº›æ–¹æ³•æ˜¾è‘—æé«˜äº†è·¨å›¾åƒä¿¡æ¯èšåˆçš„èƒ½åŠ›ï¼Œå¹¶åœ¨ç°æœ‰å¤šå›¾åƒåŸºå‡†ä¸Šè¶…è¶Šäº†ä¹‹å‰çš„æœ€ä½³è¡¨ç°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.11354', 'title': 'AstroReason-Bench: Evaluating Unified Agentic Planning across Heterogeneous Space Planning Problems', 'url': 'https://huggingface.co/papers/2601.11354', 'abstract': 'Recent advances in agentic Large Language Models (LLMs) have positioned them as generalist planners capable of reasoning and acting across diverse tasks. However, existing agent benchmarks largely focus on symbolic or weakly grounded environments, leaving their performance in physics-constrained real-world domains underexplored. We introduce AstroReason-Bench, a comprehensive benchmark for evaluating agentic planning in Space Planning Problems (SPP), a family of high-stakes problems with heterogeneous objectives, strict physical constraints, and long-horizon decision-making. AstroReason-Bench integrates multiple scheduling regimes, including ground station communication and agile Earth observation, and provides a unified agent-oriented interaction protocol. Evaluating on a range of state-of-the-art open- and closed-source agentic LLM systems, we find that current agents substantially underperform specialized solvers, highlighting key limitations of generalist planning under realistic constraints. AstroReason-Bench offers a challenging and diagnostic testbed for future agentic research.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in agentic Large Language Models (LLMs) have positioned them as generalist planners capable of reasoning and acting across diverse tasks. However, existing agent benchmarks largely focus on symbolic or weakly grounded environments, leaving their performance in physics-constrained real-world domains underexplored. We introduce AstroReason-Bench, a comprehensive benchmark for evaluating agentic planning in Space Planning Problems (SPP), a family of high-stakes problems with heterogeneous objectives, strict physical constraints, and long-horizon decision-making. AstroReason-Bench integrates multiple scheduling regimes, including ground station communication and agile Earth observation, and provides a unified agent-oriented interaction protocol. Evaluating on a range of state-of-the-art open- and closed-source agentic LLM systems, we find that current agents substantially underperform specialized solvers, highlighting key limitations of generalist planning under realistic constraints. AstroReason-Bench offers a challenging and diagnostic testbed for future agentic research.', 'score': 1, 'issue_id': 640, 'pub_date': '2026-01-16', 'pub_date_card': {'ru': '16 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 16', 'zh': '1æœˆ16æ—¥'}, 'hash': '91ba6b67dda55368', 'authors': ['Weiyi Wang', 'Xinchi Chen', 'Jingjing Gong', 'Xuanjing Huang', 'Xipeng Qiu'], 'affiliations': ['Fudan University', 'OpenMOSS Team', 'Shanghai Innovation Institute'], 'pdf_title_img': 'assets/pdf/title_img/2601.11354.jpg', 'data': {'categories': ['#open_source', '#reasoning', '#benchmark', '#agents', '#dataset'], 'emoji': 'ğŸ›°ï¸', 'ru': {'title': 'ĞšĞ¾Ğ³Ğ´Ğ° ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ²ÑÑ‚Ñ€ĞµÑ‡Ğ°ÑÑ‚ÑÑ Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ„Ğ¸Ğ·Ğ¸ĞºĞ¾Ğ¹ ĞºĞ¾ÑĞ¼Ğ¾ÑĞ°', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° AstroReason-Bench â€” Ğ½Ğ¾Ğ²Ğ°Ñ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚ÑĞºĞ¸Ñ… LLM Ñ€ĞµÑˆĞ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² ĞºĞ¾ÑĞ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ñ‚Ñ€Ğ°ÑĞ»Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ñ…ÑƒĞ¶Ğµ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€ĞµÑˆĞ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸ĞµĞ¼ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ Ğ³ĞµÑ‚ĞµÑ€Ğ¾Ğ³ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ†ĞµĞ»ÑĞ¼Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº ĞºĞ¾Ğ¼Ğ¼ÑƒĞ½Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ñ Ğ½Ğ°Ğ·ĞµĞ¼Ğ½Ñ‹Ğ¼Ğ¸ ÑÑ‚Ğ°Ğ½Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğµ Ğ—ĞµĞ¼Ğ»Ğ¸. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ¾Ğ² Ğ¿Ñ€Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'AstroReason-Bench: Testing LLMs in Real-World Space Planning', 'desc': 'This paper presents AstroReason-Bench, a new benchmark designed to evaluate the performance of agentic Large Language Models (LLMs) in Space Planning Problems (SPP). Unlike previous benchmarks that focus on simpler environments, AstroReason-Bench addresses complex real-world scenarios with strict physical constraints and diverse objectives. The study reveals that current LLMs struggle to compete with specialized solvers in these challenging tasks, indicating limitations in their generalist planning capabilities. AstroReason-Bench aims to provide a rigorous testing ground for future research in agentic planning and improve the development of more effective agents.'}, 'zh': {'title': 'AstroReason-Benchï¼šæ™ºèƒ½è§„åˆ’çš„æ–°æŒ‘æˆ˜', 'desc': 'æœ¬æ–‡ä»‹ç»äº†AstroReason-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°æ™ºèƒ½è§„åˆ’çš„åŸºå‡†ï¼Œç‰¹åˆ«æ˜¯åœ¨ç©ºé—´è§„åˆ’é—®é¢˜ï¼ˆSPPï¼‰ä¸­ã€‚ç°æœ‰çš„æ™ºèƒ½ä½“åŸºå‡†ä¸»è¦é›†ä¸­åœ¨ç¬¦å·æˆ–å¼±åŸºç¡€ç¯å¢ƒä¸­ï¼Œç¼ºä¹å¯¹ç‰©ç†çº¦æŸçš„çœŸå®ä¸–ç•Œé¢†åŸŸçš„ç ”ç©¶ã€‚é€šè¿‡å¯¹å¤šç§å…ˆè¿›çš„æ™ºèƒ½ä½“å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç³»ç»Ÿè¿›è¡Œè¯„ä¼°ï¼Œå‘ç°å½“å‰çš„æ™ºèƒ½ä½“åœ¨å¤æ‚çš„ç°å®çº¦æŸä¸‹è¡¨ç°ä¸ä½³ï¼Œè¿œä¸å¦‚ä¸“é—¨çš„æ±‚è§£å™¨ã€‚AstroReason-Benchä¸ºæœªæ¥çš„æ™ºèƒ½ä½“ç ”ç©¶æä¾›äº†ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§å’Œè¯Šæ–­æ€§çš„æµ‹è¯•å¹³å°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.11227', 'title': 'Language of Thought Shapes Output Diversity in Large Language Models', 'url': 'https://huggingface.co/papers/2601.11227', 'abstract': "Controlling the language of thought in large language models increases output diversity by leveraging distinct thinking spaces across different languages, with mixed-language sampling providing superior results.  \t\t\t\t\tAI-generated summary \t\t\t\t Output diversity is crucial for Large Language Models as it underpins pluralism and creativity. In this work, we reveal that controlling the language used during model thinking-the language of thought-provides a novel and structural source of output diversity. Our preliminary study shows that different thinking languages occupy distinct regions in a model's thinking space. Based on this observation, we study two repeated sampling strategies under multilingual thinking-Single-Language Sampling and Mixed-Language Sampling-and conduct diversity evaluation on outputs that are controlled to be in English, regardless of the thinking language used. Across extensive experiments, we demonstrate that switching the thinking language from English to non-English languages consistently increases output diversity, with a clear and consistent positive correlation such that languages farther from English in the thinking space yield larger gains. We further show that aggregating samples across multiple thinking languages yields additional improvements through compositional effects, and that scaling sampling with linguistic heterogeneity expands the model's diversity ceiling. Finally, we show that these findings translate into practical benefits in pluralistic alignment scenarios, leading to broader coverage of cultural knowledge and value orientations in LLM outputs. Our code is publicly available at https://github.com/iNLP-Lab/Multilingual-LoT-Diversity.", 'score': 1, 'issue_id': 646, 'pub_date': '2026-01-16', 'pub_date_card': {'ru': '16 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 16', 'zh': '1æœˆ16æ—¥'}, 'hash': 'a590fc798fadb73e', 'authors': ['Shaoyang Xu', 'Wenxuan Zhang'], 'affiliations': ['Singapore University of Technology and Design'], 'pdf_title_img': 'assets/pdf/title_img/2601.11227.jpg', 'data': {'categories': ['#multilingual', '#alignment', '#open_source', '#training'], 'emoji': 'ğŸŒ', 'ru': {'title': 'ĞœĞ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ ĞºĞ°Ğº Ğ¿ÑƒÑ‚ÑŒ Ğº Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ¾Ğ² LLM', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ ÑĞ·Ñ‹ĞºĞ° Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ĞµĞ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¸ Ğ·Ğ°Ğ½Ğ¸Ğ¼Ğ°ÑÑ‚ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ñ‹ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ñ… ĞºĞ°Ğº Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸Ğº Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ. Ğ˜Ğ·ÑƒÑ‡ĞµĞ½Ñ‹ Ğ´Ğ²Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ğ½Ğ¾Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ Ğ¿Ñ€Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ¼ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğ¸: Ğ¾Ğ´Ğ½Ğ¾ÑÑ‹Ñ‡Ğ½Ğ°Ñ Ğ¸ ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ°Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹. ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¿Ğ¾Ğ»ÑŒĞ·Ñƒ Ğ´Ğ»Ñ Ğ¿Ğ»ÑÑ€Ğ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ñ†ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ñ… Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¹.'}, 'en': {'title': 'Enhancing Output Diversity in LLMs through Multilingual Thinking', 'desc': "This paper explores how controlling the language of thought in large language models (LLMs) can enhance the diversity of their outputs. By using different languages during the model's thinking process, researchers found that distinct thinking spaces lead to varied and creative results. The study introduces two sampling strategies: Single-Language Sampling and Mixed-Language Sampling, demonstrating that switching to non-English languages increases output diversity. The findings suggest that leveraging multiple languages not only improves the model's performance but also enriches its understanding of cultural knowledge and values."}, 'zh': {'title': 'æ§åˆ¶æ€ç»´è¯­è¨€ï¼Œæå‡è¾“å‡ºå¤šæ ·æ€§', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­æ§åˆ¶æ€ç»´è¯­è¨€å¦‚ä½•å¢åŠ è¾“å‡ºçš„å¤šæ ·æ€§ã€‚é€šè¿‡æ··åˆè¯­è¨€é‡‡æ ·ï¼Œæˆ‘ä»¬å‘ç°ä¸åŒçš„æ€ç»´è¯­è¨€åœ¨æ¨¡å‹çš„æ€ç»´ç©ºé—´ä¸­å æ®ä¸åŒçš„åŒºåŸŸã€‚å®éªŒè¡¨æ˜ï¼Œå°†æ€ç»´è¯­è¨€ä»è‹±è¯­åˆ‡æ¢åˆ°å…¶ä»–è¯­è¨€å¯ä»¥æ˜¾è‘—æé«˜è¾“å‡ºçš„å¤šæ ·æ€§ï¼Œä¸”ä¸æ€ç»´ç©ºé—´ä¸­è¯­è¨€çš„è·ç¦»æˆæ­£ç›¸å…³ã€‚æœ€ç»ˆï¼Œè¿™äº›å‘ç°æœ‰åŠ©äºåœ¨å¤šå…ƒå¯¹é½åœºæ™¯ä¸­å®ç°æ›´å¹¿æ³›çš„æ–‡åŒ–çŸ¥è¯†å’Œä»·å€¼è§‚è¦†ç›–ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.10922', 'title': 'What Matters in Data Curation for Multimodal Reasoning? Insights from the DCVLR Challenge', 'url': 'https://huggingface.co/papers/2601.10922', 'abstract': 'Data curation for multimodal reasoning shows that difficulty-based example selection on aligned datasets drives performance gains, while increasing dataset size mainly reduces variance and synthetic augmentation heuristics often degrade performance.  \t\t\t\t\tAI-generated summary \t\t\t\t We study data curation for multimodal reasoning through the NeurIPS 2025 Data Curation for Vision-Language Reasoning (DCVLR) challenge, which isolates dataset selection by fixing the model and training protocol. Using a compact curated dataset derived primarily from Walton Multimodal Cold Start, our submission placed first in the challenge. Through post-competition ablations, we show that difficulty-based example selection on an aligned base dataset is the dominant driver of performance gains. Increasing dataset size does not reliably improve mean accuracy under the fixed training recipe, but mainly reduces run-to-run variance, while commonly used diversity and synthetic augmentation heuristics provide no additional benefit and often degrade performance. These results characterize DCVLR as a saturation-regime evaluation and highlight the central role of alignment and difficulty in data-efficient multimodal reasoning.', 'score': 1, 'issue_id': 655, 'pub_date': '2026-01-16', 'pub_date_card': {'ru': '16 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 16', 'zh': '1æœˆ16æ—¥'}, 'hash': '89cd9e27c2aea55e', 'authors': ['Yosub Shin', 'Michael Buriek', 'Boris Sobolev', 'Pavel Bushuyeu', 'Vikas Kumar', 'Haoyang Xu', 'Samuel Watson', 'Igor Molybog'], 'affiliations': ['Cisco', 'PwC', 'University of Hawaii at Manoa'], 'pdf_title_img': 'assets/pdf/title_img/2601.10922.jpg', 'data': {'categories': ['#data', '#synthetic', '#reasoning', '#multimodal', '#benchmark', '#dataset'], 'emoji': 'ğŸ“Š', 'ru': {'title': 'Ğ¡Ğ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ²Ğ°Ğ¶Ğ½ĞµĞµ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°: ĞºĞ»ÑÑ‡ Ğº ĞºÑƒÑ€Ğ°Ñ‚Ğ¾Ñ€ÑÑ‚Ğ²Ñƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ ĞºÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ½ĞºÑƒÑ€Ñ DCVLR, Ğ³Ğ´Ğµ Ñ„Ğ¸ĞºÑĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ» Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ â€” ÑÑ‚Ğ¾ Ğ²Ñ‹Ğ±Ğ¾Ñ€ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸Ñ… ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ· Ğ²Ñ‹Ñ€Ğ¾Ğ²Ğ½ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°. Ğ£Ğ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ğ½Ğµ Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ¾ÑÑ‚ ÑÑ€ĞµĞ´Ğ½ÑÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ° Ğ² Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¼ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ´Ğ¸ÑĞ¿ĞµÑ€ÑĞ¸Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ², Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½Ñ‘Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ´Ğ¸Ğ²ĞµÑ€Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ¸Ñ€ÑƒÑÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ‡Ñ‘Ñ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ñ€Ğ¾Ğ»ÑŒ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ² ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼.'}, 'en': {'title': 'Curating Data: Aligning Difficulty for Better Multimodal Reasoning', 'desc': 'This paper investigates how to effectively curate data for multimodal reasoning tasks, particularly in the context of the NeurIPS 2025 Data Curation for Vision-Language Reasoning challenge. The authors found that selecting examples based on their difficulty from a well-aligned dataset significantly enhances model performance. They also discovered that simply increasing the dataset size does not improve accuracy but rather reduces variability in results. Additionally, traditional methods like diversity and synthetic augmentation often harm performance, emphasizing the importance of alignment and difficulty in achieving efficient multimodal reasoning.'}, 'zh': {'title': 'æ•°æ®æ•´ç†ï¼šæå‡å¤šæ¨¡æ€æ¨ç†æ€§èƒ½çš„å…³é”®', 'desc': 'æœ¬è®ºæ–‡ç ”ç©¶äº†å¤šæ¨¡æ€æ¨ç†ä¸­çš„æ•°æ®æ•´ç†ï¼Œç‰¹åˆ«æ˜¯åœ¨NeurIPS 2025æ•°æ®æ•´ç†æŒ‘æˆ˜ä¸­ã€‚æˆ‘ä»¬å‘ç°ï¼ŒåŸºäºéš¾åº¦çš„ç¤ºä¾‹é€‰æ‹©åœ¨å¯¹é½çš„æ•°æ®é›†ä¸Šæ˜¾è‘—æé«˜äº†æ€§èƒ½ï¼Œè€Œå¢åŠ æ•°æ®é›†çš„å¤§å°ä¸»è¦å‡å°‘äº†æ–¹å·®ï¼Œå¹¶æœªæœ‰æ•ˆæå‡å‡†ç¡®ç‡ã€‚å¸¸ç”¨çš„å¤šæ ·æ€§å’Œåˆæˆå¢å¼ºç­–ç•¥å¾€å¾€ä¼šé™ä½æ€§èƒ½ï¼Œæœªèƒ½å¸¦æ¥é¢å¤–çš„å¥½å¤„ã€‚æˆ‘ä»¬çš„ç ”ç©¶å¼ºè°ƒäº†å¯¹é½å’Œéš¾åº¦åœ¨æ•°æ®é«˜æ•ˆå¤šæ¨¡æ€æ¨ç†ä¸­çš„é‡è¦æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.09255', 'title': 'PhyRPR: Training-Free Physics-Constrained Video Generation', 'url': 'https://huggingface.co/papers/2601.09255', 'abstract': 'A three-stage pipeline decouples physical reasoning from visual synthesis in video generation, improving physical plausibility and motion controllability through distinct phases of reasoning, planning, and refinement.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent diffusion-based video generation models can synthesize visually plausible videos, yet they often struggle to satisfy physical constraints. A key reason is that most existing approaches remain single-stage: they entangle high-level physical understanding with low-level visual synthesis, making it hard to generate content that require explicit physical reasoning. To address this limitation, we propose a training-free three-stage pipeline,PhyRPR:Phy\\uline{Reason}--Phy\\uline{Plan}--Phy\\uline{Refine}, which decouples physical understanding from visual synthesis. Specifically, PhyReason uses a large multimodal model for physical state reasoning and an image generator for keyframe synthesis; PhyPlan deterministically synthesizes a controllable coarse motion scaffold; and PhyRefine injects this scaffold into diffusion sampling via a latent fusion strategy to refine appearance while preserving the planned dynamics. This staged design enables explicit physical control during generation. Extensive experiments under physics constraints show that our method consistently improves physical plausibility and motion controllability.', 'score': 1, 'issue_id': 645, 'pub_date': '2026-01-14', 'pub_date_card': {'ru': '14 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 14', 'zh': '1æœˆ14æ—¥'}, 'hash': '4bc3324d676d118d', 'authors': ['Yibo Zhao', 'Hengjia Li', 'Xiaofei He', 'Boxi Wu'], 'affiliations': ['State Key Lab of CAD&CG, Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2601.09255.jpg', 'data': {'categories': ['#architecture', '#reasoning', '#multimodal', '#diffusion', '#video'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ¤Ğ¸Ğ·Ğ¸ĞºĞ° Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ğ¾, ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ğ¾: Ñ‚Ñ€Ñ‘Ñ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ´Ğ»Ñ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ñ‚Ñ€Ñ‘Ñ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ PhyRPR Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞ¸Ğ½Ñ‚ĞµĞ·. ĞĞ° Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ ĞºĞ°Ğ´Ñ€Ñ‹, Ğ½Ğ° Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ´ĞµÑ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ÑÑ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ğ¹ ÑĞºĞµĞ»ĞµÑ‚ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ, Ğ° Ğ½Ğ° Ñ‚Ñ€ĞµÑ‚ÑŒĞµĞ¼ ÑÑ‚Ğ°Ğ¿Ğµ ÑÑ‚Ğ¾Ñ‚ ÑĞºĞµĞ»ĞµÑ‚ Ğ²Ğ½ĞµĞ´Ñ€ÑĞµÑ‚ÑÑ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ. Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ²Ğ½Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°Ñ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸.'}, 'en': {'title': 'Decoupling Reasoning and Synthesis for Physically Plausible Video Generation', 'desc': 'This paper presents a novel three-stage pipeline called PhyRPR for video generation that separates physical reasoning from visual synthesis. The pipeline consists of three distinct phases: PhyReason for understanding physical states, PhyPlan for creating a controllable motion scaffold, and PhyRefine for enhancing visual quality while maintaining physical dynamics. By decoupling these processes, the method allows for better adherence to physical constraints, resulting in videos that are both visually appealing and physically plausible. The authors demonstrate that their approach significantly improves motion controllability and physical realism in generated videos compared to existing single-stage models.'}, 'zh': {'title': 'ä¸‰é˜¶æ®µç®¡é“ï¼šæå‡è§†é¢‘ç”Ÿæˆçš„ç‰©ç†åˆç†æ€§ä¸å¯æ§æ€§', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§ä¸‰é˜¶æ®µçš„ç®¡é“ï¼Œæ—¨åœ¨å°†ç‰©ç†æ¨ç†ä¸è§†è§‰åˆæˆåˆ†ç¦»ï¼Œä»è€Œæé«˜è§†é¢‘ç”Ÿæˆçš„ç‰©ç†åˆç†æ€§å’Œè¿åŠ¨å¯æ§æ€§ã€‚è¯¥æ–¹æ³•åŒ…æ‹¬ç‰©ç†æ¨ç†ï¼ˆPhyReasonï¼‰ã€ç‰©ç†è§„åˆ’ï¼ˆPhyPlanï¼‰å’Œç‰©ç†ç²¾ç‚¼ï¼ˆPhyRefineï¼‰ä¸‰ä¸ªé˜¶æ®µï¼Œåˆ†åˆ«è´Ÿè´£ä¸åŒçš„ä»»åŠ¡ã€‚é€šè¿‡ä½¿ç”¨å¤šæ¨¡æ€æ¨¡å‹è¿›è¡Œç‰©ç†çŠ¶æ€æ¨ç†ï¼Œå¹¶åœ¨åç»­é˜¶æ®µåˆæˆå¯æ§çš„è¿åŠ¨æ¡†æ¶ï¼Œæœ€åé€šè¿‡æ½œåœ¨èåˆç­–ç•¥è¿›è¡Œå¤–è§‚ç²¾ç‚¼ï¼Œç¡®ä¿ç”Ÿæˆå†…å®¹ç¬¦åˆç‰©ç†çº¦æŸã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç‰©ç†åˆç†æ€§å’Œè¿åŠ¨å¯æ§æ€§æ–¹é¢å‡æœ‰æ˜¾è‘—æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.11077', 'title': 'ABC-Bench: Benchmarking Agentic Backend Coding in Real-World Development', 'url': 'https://huggingface.co/papers/2601.11077', 'abstract': 'ABC-Bench evaluates LLM agents on realistic backend coding tasks requiring full development lifecycle management from repository exploration to containerized service deployment and API testing.  \t\t\t\t\tAI-generated summary \t\t\t\t The evolution of Large Language Models (LLMs) into autonomous agents has expanded the scope of AI coding from localized code generation to complex, repository-level, and execution-driven problem solving. However, current benchmarks predominantly evaluate code logic in static contexts, neglecting the dynamic, full-process requirements of real-world engineering, particularly in backend development which demands rigorous environment configuration and service deployment. To address this gap, we introduce ABC-Bench, a benchmark explicitly designed to evaluate agentic backend coding within a realistic, executable workflow. Using a scalable automated pipeline, we curated 224 practical tasks spanning 8 languages and 19 frameworks from open-source repositories. Distinct from previous evaluations, ABC-Bench require the agents to manage the entire development lifecycle from repository exploration to instantiating containerized services and pass the external end-to-end API tests. Our extensive evaluation reveals that even state-of-the-art models struggle to deliver reliable performance on these holistic tasks, highlighting a substantial disparity between current model capabilities and the demands of practical backend engineering. Our code is available at https://github.com/OpenMOSS/ABC-Bench.', 'score': 49, 'issue_id': 662, 'pub_date': '2026-01-16', 'pub_date_card': {'ru': '16 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 16', 'zh': '1æœˆ16æ—¥'}, 'hash': '97497f7535c4d03b', 'authors': ['Jie Yang', 'Honglin Guo', 'Li Ji', 'Jiazheng Zhou', 'Rui Zheng', 'Zhikai Lei', 'Shuo Zhang', 'Zhiheng Xi', 'Shichun Liu', 'Yuxin Wang', 'Bo Wang', 'Yining Zheng', 'Tao Gui', 'Xipeng Qiu'], 'affiliations': ['Fudan University', 'Shanghai Innovation Institute', 'Shanghai QÄ³i Zhifeng Co., Ltd.'], 'pdf_title_img': 'assets/pdf/title_img/2601.11077.jpg', 'data': {'categories': ['#plp', '#benchmark', '#agents', '#open_source'], 'emoji': 'ğŸ³', 'ru': {'title': 'ĞÑ‚ ĞºĞ¾Ğ´Ğ° Ğº Ğ±Ğ¾ĞµĞ²Ğ¾Ğ¼Ñƒ ÑĞµÑ€Ğ²Ğ¸ÑÑƒ: Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¼ Ñ†Ğ¸ĞºĞ»Ğµ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸', 'desc': 'ABC-Bench â€” ÑÑ‚Ğ¾ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ±ÑĞºĞµĞ½Ğ´Ğ°, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ³Ğ¾ Ñ†Ğ¸ĞºĞ»Ğ° Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¾Ñ‚ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ñ Ğ´Ğ¾ Ñ€Ğ°Ğ·Ğ²Ñ‘Ñ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞ¹Ğ½ĞµÑ€Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ€Ğ²Ğ¸ÑĞ¾Ğ². Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ»Ğ¾Ğ³Ğ¸ĞºÑƒ ĞºĞ¾Ğ´Ğ° Ğ² ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ…, ÑÑ‚Ğ¾Ñ‚ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑ‚ÑŒ Ğ²ÑĞµĞ¼ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ¼: ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ, Ñ€Ğ°Ğ·Ğ²Ñ‘Ñ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼ end-to-end API Ñ‚ĞµÑÑ‚Ğ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¸Ğ· 224 Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ½Ğ° 8 ÑĞ·Ñ‹ĞºĞ°Ñ… Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ 19 Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ°Ñ… Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ñ… ĞºĞ¾Ğ´Ğ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ ÑĞ°Ğ¼Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ»Ğ¾Ñ…Ğ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ñ‚Ğ°ĞºĞ¸Ğ¼Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ğ¸.'}, 'en': {'title': 'ABC-Bench: Bridging the Gap in LLM Backend Coding Evaluation', 'desc': 'The paper introduces ABC-Bench, a new benchmark for evaluating Large Language Model (LLM) agents on comprehensive backend coding tasks. Unlike previous benchmarks that focus on isolated code logic, ABC-Bench assesses the entire development lifecycle, including repository exploration, service deployment, and API testing. It features 224 practical tasks across multiple programming languages and frameworks, emphasizing real-world engineering challenges. The findings indicate that even advanced LLMs struggle with these complex tasks, revealing a significant gap between their current capabilities and the requirements of backend development.'}, 'zh': {'title': 'è¯„ä¼°åç«¯ç¼–ç çš„å…¨ç”Ÿå‘½å‘¨æœŸç®¡ç†èƒ½åŠ›', 'desc': 'ABC-Benchæ˜¯ä¸€ä¸ªè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨çœŸå®åç«¯ç¼–ç ä»»åŠ¡ä¸­çš„è¡¨ç°çš„åŸºå‡†ã€‚å®ƒå…³æ³¨æ•´ä¸ªå¼€å‘ç”Ÿå‘½å‘¨æœŸçš„ç®¡ç†ï¼ŒåŒ…æ‹¬ä»£ç åº“æ¢ç´¢ã€å®¹å™¨åŒ–æœåŠ¡éƒ¨ç½²å’ŒAPIæµ‹è¯•ã€‚ä¸ä»¥å¾€çš„è¯„ä¼°ä¸åŒï¼ŒABC-Benchè¦æ±‚æ¨¡å‹åœ¨åŠ¨æ€ç¯å¢ƒä¸­å®Œæˆå¤æ‚çš„ä»»åŠ¡ï¼Œè€Œä¸ä»…ä»…æ˜¯é™æ€ä»£ç é€»è¾‘ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„æ¨¡å‹åœ¨è¿™äº›å…¨é¢ä»»åŠ¡ä¸Šä¹Ÿéš¾ä»¥æä¾›å¯é çš„æ€§èƒ½ï¼Œæ˜¾ç¤ºå‡ºå½“å‰æ¨¡å‹èƒ½åŠ›ä¸å®é™…åç«¯å·¥ç¨‹éœ€æ±‚ä¹‹é—´çš„æ˜¾è‘—å·®è·ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.08808', 'title': 'Multiplex Thinking: Reasoning via Token-wise Branch-and-Merge', 'url': 'https://huggingface.co/papers/2601.08808', 'abstract': 'Multiplex Thinking introduces a stochastic soft reasoning mechanism that samples multiple candidate tokens at each step to optimize reasoning trajectories with reinforcement learning while maintaining shorter sequences than traditional chain-of-thought methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models often solve complex reasoning tasks more effectively with Chain-of-Thought (CoT), but at the cost of long, low-bandwidth token sequences. Humans, by contrast, often reason softly by maintaining a distribution over plausible next steps. Motivated by this, we propose Multiplex Thinking, a stochastic soft reasoning mechanism that, at each thinking step, samples K candidate tokens and aggregates their embeddings into a single continuous multiplex token. This preserves the vocabulary embedding prior and the sampling dynamics of standard discrete generation, while inducing a tractable probability distribution over multiplex rollouts. Consequently, multiplex trajectories can be directly optimized with on-policy reinforcement learning (RL). Importantly, Multiplex Thinking is self-adaptive: when the model is confident, the multiplex token is nearly discrete and behaves like standard CoT; when it is uncertain, it compactly represents multiple plausible next steps without increasing sequence length. Across challenging math reasoning benchmarks, Multiplex Thinking consistently outperforms strong discrete CoT and RL baselines from Pass@1 through Pass@1024, while producing shorter sequences. The code and checkpoints are available at https://github.com/GMLR-Penn/Multiplex-Thinking.', 'score': 26, 'issue_id': 662, 'pub_date': '2026-01-13', 'pub_date_card': {'ru': '13 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 13', 'zh': '1æœˆ13æ—¥'}, 'hash': '0a64241c20f7b28c', 'authors': ['Yao Tang', 'Li Dong', 'Yaru Hao', 'Qingxiu Dong', 'Furu Wei', 'Jiatao Gu'], 'affiliations': ['Microsoft Research', 'University of Pennsylvania'], 'pdf_title_img': 'assets/pdf/title_img/2601.08808.jpg', 'data': {'categories': ['#open_source', '#math', '#rl', '#reasoning', '#optimization', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞœÑĞ³ĞºĞ¾Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¿Ğ»ĞµĞ½Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ: ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¿Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ ÑˆĞ°Ğ³Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ğ°Ğ³Ñ€ĞµĞ³Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ Ğ² ĞµĞ´Ğ¸Ğ½Ñ‹Ğ¹ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¿Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ½Ğ°Ğ´ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿ÑƒÑ‚ÑĞ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, ĞºĞ°Ğº Ğ² Ğ¼ÑĞ³ĞºĞ¾Ğ¼ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼: Ğ¿Ñ€Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¿Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½ Ğ²ĞµĞ´Ñ‘Ñ‚ ÑĞµĞ±Ñ ĞºĞ°Ğº ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ°Ñ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ° Ğ¼Ñ‹ÑĞ»ĞµĞ¹, Ğ° Ğ¿Ñ€Ğ¸ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ğ»Ğ°ÑƒÑĞ¸Ğ±ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞĞ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ»Ğ¸Ğ½Ğ¸Ğ¸ Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ğº Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ, Ñ‚Ğ°Ğº Ğ¸ Ğ¿Ğ¾ Ğ´Ğ»Ğ¸Ğ½Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ².'}, 'en': {'title': 'Optimizing Reasoning with Stochastic Soft Sampling', 'desc': 'Multiplex Thinking is a new approach in machine learning that enhances reasoning in large language models by using a stochastic soft reasoning mechanism. Instead of generating long sequences of tokens like traditional Chain-of-Thought methods, it samples multiple candidate tokens at each step and combines them into a single multiplex token. This method allows the model to maintain a probability distribution over possible next steps, making it more efficient and adaptable. As a result, Multiplex Thinking achieves better performance on complex reasoning tasks while producing shorter sequences compared to existing methods.'}, 'zh': {'title': 'å¤šé‡æ€ç»´ï¼šä¼˜åŒ–æ¨ç†çš„æ–°æ–¹å¼', 'desc': 'Multiplex Thinkingæå‡ºäº†ä¸€ç§éšæœºè½¯æ¨ç†æœºåˆ¶ï¼Œåœ¨æ¯ä¸€æ­¥é‡‡æ ·å¤šä¸ªå€™é€‰æ ‡è®°ï¼Œä»¥ä¼˜åŒ–æ¨ç†è½¨è¿¹ï¼ŒåŒæ—¶ä¿æŒæ¯”ä¼ ç»Ÿçš„é“¾å¼æ€ç»´æ–¹æ³•æ›´çŸ­çš„åºåˆ—ã€‚ä¸äººç±»çš„æ¨ç†æ–¹å¼ç›¸ä¼¼ï¼Œè¯¥æ–¹æ³•åœ¨ä¸ç¡®å®šæ—¶èƒ½å¤Ÿç´§å‡‘åœ°è¡¨ç¤ºå¤šä¸ªåˆç†çš„ä¸‹ä¸€æ­¥ï¼Œè€Œåœ¨è‡ªä¿¡æ—¶åˆ™è¡¨ç°å¾—åƒæ ‡å‡†çš„é“¾å¼æ€ç»´ã€‚é€šè¿‡åœ¨ç­–ç•¥å¼ºåŒ–å­¦ä¹ ä¸­ç›´æ¥ä¼˜åŒ–å¤šé‡è½¨è¿¹ï¼ŒMultiplex Thinkingåœ¨å¤æ‚çš„æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†å¼ºå¤§çš„ç¦»æ•£é“¾å¼æ€ç»´å’Œå¼ºåŒ–å­¦ä¹ åŸºçº¿ã€‚è¯¥æ–¹æ³•çš„ä»£ç å’Œæ£€æŸ¥ç‚¹å¯åœ¨æŒ‡å®šé“¾æ¥è·å–ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.10880', 'title': 'Medical SAM3: A Foundation Model for Universal Prompt-Driven Medical Image Segmentation', 'url': 'https://huggingface.co/papers/2601.10880', 'abstract': "Medical SAM3 adapts the SAM3 foundation model through comprehensive fine-tuning on diverse medical imaging datasets to achieve robust prompt-driven segmentation across various modalities and anatomical structures.  \t\t\t\t\tAI-generated summary \t\t\t\t Promptable segmentation foundation models such as SAM3 have demonstrated strong generalization capabilities through interactive and concept-based prompting. However, their direct applicability to medical image segmentation remains limited by severe domain shifts, the absence of privileged spatial prompts, and the need to reason over complex anatomical and volumetric structures. Here we present Medical SAM3, a foundation model for universal prompt-driven medical image segmentation, obtained by fully fine-tuning SAM3 on large-scale, heterogeneous 2D and 3D medical imaging datasets with paired segmentation masks and text prompts. Through a systematic analysis of vanilla SAM3, we observe that its performance degrades substantially on medical data, with its apparent competitiveness largely relying on strong geometric priors such as ground-truth-derived bounding boxes. These findings motivate full model adaptation beyond prompt engineering alone. By fine-tuning SAM3's model parameters on 33 datasets spanning 10 medical imaging modalities, Medical SAM3 acquires robust domain-specific representations while preserving prompt-driven flexibility. Extensive experiments across organs, imaging modalities, and dimensionalities demonstrate consistent and significant performance gains, particularly in challenging scenarios characterized by semantic ambiguity, complex morphology, and long-range 3D context. Our results establish Medical SAM3 as a universal, text-guided segmentation foundation model for medical imaging and highlight the importance of holistic model adaptation for achieving robust prompt-driven segmentation under severe domain shift. Code and model will be made available at https://github.com/AIM-Research-Lab/Medical-SAM3.", 'score': 9, 'issue_id': 670, 'pub_date': '2026-01-15', 'pub_date_card': {'ru': '15 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 15', 'zh': '1æœˆ15æ—¥'}, 'hash': '717e240848228667', 'authors': ['Chongcong Jiang', 'Tianxingjian Ding', 'Chuhan Song', 'Jiachen Tu', 'Ziyang Yan', 'Yihua Shao', 'Zhenyi Wang', 'Yuzhang Shang', 'Tianyu Han', 'Yu Tian'], 'affiliations': ['The Hong Kong Polytechnic University', 'University College London', 'University of Central Florida', 'University of Illinois Urbana-Champaign', 'University of Pennsylvania', 'University of Trento'], 'pdf_title_img': 'assets/pdf/title_img/2601.10880.jpg', 'data': {'categories': ['#training', '#multimodal', '#healthcare', '#cv'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ°Ğ¼', 'desc': 'Medical SAM3 â€” ÑÑ‚Ğ¾ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ²ĞµÑ€ÑĞ¸Ñ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ SAM3, ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ… Ğ´Ğ»Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ñ‹Ñ… 2D Ğ¸ 3D Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ… Ğ¸Ğ· 33 Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ², Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… 10 Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹, Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ°ÑĞ¾Ğº ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ²Ğ°Ğ½Ğ¸Ğ»ÑŒĞ½Ğ°Ñ SAM3 ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ñ‚ĞµÑ€ÑĞµÑ‚ Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ° Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ğ¸ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº. Medical SAM3 Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ² ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ°Ğ½Ğ°Ñ‚Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑĞ»ÑƒÑ‡Ğ°ÑÑ… Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ½ĞµĞ¾Ğ´Ğ½Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼.'}, 'en': {'title': 'Medical SAM3: Revolutionizing Prompt-Driven Segmentation in Medical Imaging', 'desc': "Medical SAM3 is a specialized version of the SAM3 model, fine-tuned on a wide range of medical imaging datasets to enhance its ability to perform prompt-driven segmentation. This adaptation addresses challenges like domain shifts and the complexity of anatomical structures, which hinder the original SAM3's effectiveness in medical contexts. By training on 33 diverse datasets, Medical SAM3 develops strong domain-specific representations while maintaining the flexibility of prompt-based inputs. The model shows significant improvements in segmentation accuracy across various medical imaging modalities, especially in difficult cases with complex shapes and long-range dependencies."}, 'zh': {'title': 'åŒ»å­¦å½±åƒåˆ†å‰²çš„æ–°åŸºç¡€ï¼šMedical SAM3', 'desc': 'Medical SAM3 æ˜¯ä¸€ç§åŸºç¡€æ¨¡å‹ï¼Œé€šè¿‡åœ¨å¤šæ ·çš„åŒ»å­¦å½±åƒæ•°æ®é›†ä¸Šè¿›è¡Œå…¨é¢å¾®è°ƒï¼Œé€‚åº”äº† SAM3 æ¨¡å‹ï¼Œä»¥å®ç°å¯¹å„ç§æ¨¡æ€å’Œè§£å‰–ç»“æ„çš„ç¨³å¥æç¤ºé©±åŠ¨åˆ†å‰²ã€‚è¯¥æ¨¡å‹å…‹æœäº†åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„é¢†åŸŸè½¬ç§»é—®é¢˜ï¼Œèƒ½å¤Ÿå¤„ç†å¤æ‚çš„è§£å‰–å’Œä½“ç§¯ç»“æ„ã€‚é€šè¿‡åœ¨ 33 ä¸ªæ•°æ®é›†ä¸Šå¾®è°ƒï¼ŒMedical SAM3 è·å¾—äº†å¼ºå¤§çš„é¢†åŸŸç‰¹å®šè¡¨ç¤ºï¼ŒåŒæ—¶ä¿æŒäº†æç¤ºé©±åŠ¨çš„çµæ´»æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMedical SAM3 åœ¨å¤„ç†è¯­ä¹‰æ¨¡ç³Šã€å¤æ‚å½¢æ€å’Œé•¿è·ç¦» 3D ä¸Šä¸‹æ–‡çš„æŒ‘æˆ˜åœºæ™¯ä¸­ï¼Œè¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.11004', 'title': 'NAACL: Noise-AwAre Verbal Confidence Calibration for LLMs in RAG Systems', 'url': 'https://huggingface.co/papers/2601.11004', 'abstract': "Large language models suffer from poor confidence calibration in retrieval-augmented generation due to noisy contexts, but a noise-aware calibration framework significantly improves calibration performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Accurately assessing model confidence is essential for deploying large language models (LLMs) in mission-critical factual domains. While retrieval-augmented generation (RAG) is widely adopted to improve grounding, confidence calibration in RAG settings remains poorly understood. We conduct a systematic study across four benchmarks, revealing that LLMs exhibit poor calibration performance due to noisy retrieved contexts. Specifically, contradictory or irrelevant evidence tends to inflate the model's false certainty, leading to severe overconfidence. To address this, we propose NAACL Rules (Noise-AwAre Confidence CaLibration Rules) to provide a principled foundation for resolving overconfidence under noise. We further design NAACL, a noise-aware calibration framework that synthesizes supervision from about 2K HotpotQA examples guided by these rules. By performing supervised fine-tuning (SFT) with this data, NAACL equips models with intrinsic noise awareness without relying on stronger teacher models. Empirical results show that NAACL yields substantial gains, improving ECE scores by 10.9% in-domain and 8.0% out-of-domain. By bridging the gap between retrieval noise and verbal calibration, NAACL paves the way for both accurate and epistemically reliable LLMs.", 'score': 7, 'issue_id': 674, 'pub_date': '2026-01-16', 'pub_date_card': {'ru': '16 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 16', 'zh': '1æœˆ16æ—¥'}, 'hash': 'a9abc70fc2d5ecbf', 'authors': ['Jiayu Liu', 'Rui Wang', 'Qing Zong', 'Qingcheng Zeng', 'Tianshi Zheng', 'Haochen Shi', 'Dadi Guo', 'Baixuan Xu', 'Chunyang Li', 'Yangqiu Song'], 'affiliations': ['HKUST', 'Northwestern University'], 'pdf_title_img': 'assets/pdf/title_img/2601.11004.jpg', 'data': {'categories': ['#rag', '#training', '#benchmark'], 'emoji': 'ğŸ¯', 'ru': {'title': 'ĞÑĞ¾Ğ·Ğ½Ğ°Ğ½Ğ½Ğ°Ñ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞ°: Ğ¾Ğ±ÑƒĞ·Ğ´Ğ°Ğ½Ğ¸Ğµ ÑˆÑƒĞ¼Ğ° Ğ² Ğ¿Ğ¾Ğ¸ÑĞºĞµ Ğ´Ğ»Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ»Ğ¾Ñ…Ğ¾ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€ÑƒÑÑ‚ ÑĞ²Ğ¾Ñ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ· Ğ¿Ğ¾Ğ¸ÑĞºĞ° (RAG), Ñ‚Ğ°Ğº ĞºĞ°Ğº Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ñ‹ Ñ‡Ğ°ÑÑ‚Ğ¾ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‚ ÑˆÑƒĞ¼ Ğ¸ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾Ñ€ĞµÑ‡Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ NAACL â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞ¸, Ğ¾ÑĞ¾Ğ·Ğ½Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ½Ğ°Ğ»Ğ¸Ñ‡Ğ¸Ğµ ÑˆÑƒĞ¼Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ° Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ (SFT) Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° HotpotQA, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚ÑŒ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ĞµĞµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ° Ğ¿Ğ¾Ğ¼ĞµÑ… Ğ±ĞµĞ· Ğ¿Ñ€Ğ¸Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ ECE Ğ½Ğ° 10.9% Ğ´Ğ»Ñ Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸ 8.0% Ğ´Ğ»Ñ Ğ²Ñ‹Ñ…Ğ¾Ğ´ÑÑ‰Ğ¸Ñ… Ğ·Ğ° Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‹ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Enhancing Confidence Calibration in Language Models with Noise Awareness', 'desc': "This paper addresses the issue of poor confidence calibration in large language models (LLMs) when used in retrieval-augmented generation (RAG) settings. The authors identify that noisy contexts, such as contradictory or irrelevant information, lead to inflated confidence levels in the model's predictions. To tackle this problem, they introduce NAACL Rules, a framework designed to improve calibration by making the model aware of noise in the retrieved data. Their empirical results demonstrate that the proposed noise-aware calibration framework significantly enhances calibration performance, leading to more reliable outputs from LLMs."}, 'zh': {'title': 'æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„ç½®ä¿¡åº¦æ ¡å‡†', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ£€ç´¢å¢å¼ºç”Ÿæˆä¸­ç”±äºå™ªå£°ä¸Šä¸‹æ–‡è€Œå¯¼è‡´ç½®ä¿¡åº¦æ ¡å‡†ä¸ä½³ã€‚æˆ‘ä»¬å‘ç°ï¼Œæ£€ç´¢åˆ°çš„çŸ›ç›¾æˆ–æ— å…³è¯æ®ä¼šå¯¼è‡´æ¨¡å‹è¿‡åº¦è‡ªä¿¡ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å™ªå£°æ„ŸçŸ¥æ ¡å‡†æ¡†æ¶NAACLï¼Œåˆ©ç”¨çº¦2000ä¸ªHotpotQAç¤ºä¾‹è¿›è¡Œç›‘ç£å¾®è°ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒNAACLæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„æ ¡å‡†æ€§èƒ½ï¼Œç¼©å°äº†æ£€ç´¢å™ªå£°ä¸è¯­è¨€æ ¡å‡†ä¹‹é—´çš„å·®è·ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.11061', 'title': 'Spurious Rewards Paradox: Mechanistically Understanding How RLVR Activates Memorization Shortcuts in LLMs', 'url': 'https://huggingface.co/papers/2601.11061', 'abstract': 'Spurious rewards in reinforcement learning with verifiable rewards trigger a memorization shortcut in LLMs, identified through neural circuit analysis and causal steering techniques.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning with Verifiable Rewards (RLVR) is highly effective for enhancing LLM reasoning, yet recent evidence shows models like Qwen 2.5 achieve significant gains even with spurious or incorrect rewards. We investigate this phenomenon and identify a "Perplexity Paradox": spurious RLVR triggers a divergence where answer-token perplexity drops while prompt-side coherence degrades, suggesting the model is bypassing reasoning in favor of memorization. Using Path Patching, Logit Lens, JSD analysis, and Neural Differential Equations, we uncover a hidden Anchor-Adapter circuit that facilitates this shortcut. We localize a Functional Anchor in the middle layers (L18-20) that triggers the retrieval of memorized solutions, followed by Structural Adapters in later layers (L21+) that transform representations to accommodate the shortcut signal. Finally, we demonstrate that scaling specific MLP keys within this circuit allows for bidirectional causal steering-artificially amplifying or suppressing contamination-driven performance. Our results provide a mechanistic roadmap for identifying and mitigating data contamination in RLVR-tuned models. Code is available at https://github.com/idwts/How-RLVR-Activates-Memorization-Shortcuts.', 'score': 5, 'issue_id': 669, 'pub_date': '2026-01-16', 'pub_date_card': {'ru': '16 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 16', 'zh': '1æœˆ16æ—¥'}, 'hash': '1e155023f04ffd59', 'authors': ['Lecheng Yan', 'Ruizhe Li', 'Guanhua Chen', 'Qing Li', 'Jiahui Geng', 'Wenxi Li', 'Vincent Wang', 'Chris Lee'], 'affiliations': ['East China Normal University', 'Mohamed bin Zayed University of Artificial Intelligence (MBZUAI)', 'Southern University of Science and Technology', 'University of Aberdeen'], 'pdf_title_img': 'assets/pdf/title_img/2601.11061.jpg', 'data': {'categories': ['#training', '#architecture', '#rlhf', '#interpretability', '#security', '#rl', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞšĞ°Ğº Ğ½ĞµĞ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ·Ğ°ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ñ‚ÑŒ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¾ĞºÑĞ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸ (RLVR), Ğ³Ğ´Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ´Ğ°Ğ¶Ğµ Ğ¿Ñ€Ğ¸ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸ Â«ĞŸĞ°Ñ€Ğ°Ğ´Ğ¾ĞºÑ Ğ Ğ°ÑÑ‚ĞµÑ€ÑĞ½Ğ½Ğ¾ÑÑ‚Ğ¸Â»: Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ° Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°ĞµÑ‚ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹, ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ ÑĞºÑ€Ñ‹Ñ‚ÑƒÑ ÑÑ…ĞµĞ¼Ñƒ Ğ² Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ÑĞµÑ‚Ğ¸ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Anchor-Adapter. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ†ĞµĞ¿ĞµĞ¹ (Path Patching, Logit Lens, JSD-Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·), Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞºĞ¾Ñ€Ñ Ğ² ÑÑ€ĞµĞ´Ğ½Ğ¸Ñ… ÑĞ»Ğ¾ÑÑ… Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ¸Ñ€ÑƒÑÑ‚ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ½ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‚ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑ‚ÑŒ ÑÑ‚Ğ¾Ğ¹ ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¹ ÑÑ…ĞµĞ¼Ğ¾Ğ¹ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ñ‹ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ ÑƒÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ³Ñ€ÑĞ·Ğ½ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² RLVR-Ğ½Ğ°ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ….'}, 'en': {'title': 'Uncovering Memorization Shortcuts in RLVR Models', 'desc': "This paper explores how spurious rewards in Reinforcement Learning with Verifiable Rewards (RLVR) can lead to memorization shortcuts in large language models (LLMs). The authors identify a phenomenon called the 'Perplexity Paradox', where models show lower perplexity in answers but poorer coherence in prompts, indicating a shift from reasoning to memorization. Through advanced techniques like Path Patching and Neural Differential Equations, they reveal a hidden circuit that enables this shortcut, involving specific layers of the neural network. The findings offer insights into how to detect and address data contamination issues in RLVR-tuned models, enhancing their reasoning capabilities."}, 'zh': {'title': 'æ­ç¤ºè™šå‡å¥–åŠ±å¯¼è‡´çš„è®°å¿†æ·å¾„', 'desc': 'æœ¬æ–‡æ¢è®¨äº†åœ¨å¼ºåŒ–å­¦ä¹ ä¸­ä½¿ç”¨å¯éªŒè¯å¥–åŠ±æ—¶ï¼Œè™šå‡å¥–åŠ±å¦‚ä½•å¯¼è‡´å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å‡ºç°è®°å¿†æ·å¾„ã€‚ç ”ç©¶å‘ç°ï¼Œè™šå‡å¥–åŠ±ä¼šå¼•å‘ä¸€ç§â€œå›°æƒ‘æ‚–è®ºâ€ï¼Œå³æ¨¡å‹åœ¨å›ç­”æ—¶çš„å›°æƒ‘åº¦ä¸‹é™ï¼Œä½†æç¤ºçš„ä¸€è‡´æ€§å´é™ä½ï¼Œè¡¨æ˜æ¨¡å‹å€¾å‘äºè®°å¿†è€Œéæ¨ç†ã€‚é€šè¿‡ç¥ç»ç”µè·¯åˆ†æå’Œå› æœå¼•å¯¼æŠ€æœ¯ï¼Œä½œè€…è¯†åˆ«å‡ºä¸€ä¸ªéšè—çš„é”šå®šé€‚é…å™¨ç”µè·¯ï¼Œå¸®åŠ©æ¨¡å‹ç»•è¿‡æ¨ç†è¿‡ç¨‹ã€‚æœ€åï¼Œç ”ç©¶æä¾›äº†ä¸€ç§æœºåˆ¶æ€§è·¯çº¿å›¾ï¼Œä»¥è¯†åˆ«å’Œå‡è½»å¼ºåŒ–å­¦ä¹ ä¸­æ•°æ®æ±¡æŸ“å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.10387', 'title': 'The Assistant Axis: Situating and Stabilizing the Default Persona of Language Models', 'url': 'https://huggingface.co/papers/2601.10387', 'abstract': 'Research reveals that large language models operate within a persona space where an "Assistant Axis" controls helpfulness and behavioral stability, with steering techniques able to influence model responses and prevent harmful behavior drift.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models can represent a variety of personas but typically default to a helpful Assistant identity cultivated during post-training. We investigate the structure of the space of model personas by extracting activation directions corresponding to diverse character archetypes. Across several different models, we find that the leading component of this persona space is an "Assistant Axis," which captures the extent to which a model is operating in its default Assistant mode. Steering towards the Assistant direction reinforces helpful and harmless behavior; steering away increases the model\'s tendency to identify as other entities. Moreover, steering away with more extreme values often induces a mystical, theatrical speaking style. We find this axis is also present in pre-trained models, where it primarily promotes helpful human archetypes like consultants and coaches and inhibits spiritual ones. Measuring deviations along the Assistant Axis predicts "persona drift," a phenomenon where models slip into exhibiting harmful or bizarre behaviors that are uncharacteristic of their typical persona. We find that persona drift is often driven by conversations demanding meta-reflection on the model\'s processes or featuring emotionally vulnerable users. We show that restricting activations to a fixed region along the Assistant Axis can stabilize model behavior in these scenarios -- and also in the face of adversarial persona-based jailbreaks. Our results suggest that post-training steers models toward a particular region of persona space but only loosely tethers them to it, motivating work on training and steering strategies that more deeply anchor models to a coherent persona.', 'score': 5, 'issue_id': 662, 'pub_date': '2026-01-15', 'pub_date_card': {'ru': '15 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 15', 'zh': '1æœˆ15æ—¥'}, 'hash': '6f1cb5278e40e9ea', 'authors': ['Christina Lu', 'Jack Gallagher', 'Jonathan Michala', 'Kyle Fish', 'Jack Lindsey'], 'affiliations': ['Anthropic', 'Anthropic Fellows Program', 'MATS', 'University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2601.10387.jpg', 'data': {'categories': ['#security', '#interpretability', '#architecture', '#training', '#alignment'], 'emoji': 'ğŸ§­', 'ru': {'title': 'ĞĞ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ñƒ Ğ¿ĞµÑ€ÑĞ¾Ğ½: ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ LLM Ñ‡ĞµÑ€ĞµĞ· Ğ¾ÑÑŒ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ½Ğ¸ĞºĞ°', 'desc': "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒÑÑ‚ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ¿ĞµÑ€ÑĞ¾Ğ½, Ğ³Ğ´Ğµ ĞºĞ»ÑÑ‡ĞµĞ²ÑƒÑ Ñ€Ğ¾Ğ»ÑŒ Ğ¸Ğ³Ñ€Ğ°ĞµÑ‚ 'ĞÑÑŒ ĞŸĞ¾Ğ¼Ğ¾Ñ‰Ğ½Ğ¸ĞºĞ°', ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒÑÑ‰Ğ°Ñ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ. ĞŸÑƒÑ‚Ñ‘Ğ¼ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ² Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¹, ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ°Ñ€Ñ…ĞµÑ‚Ğ¸Ğ¿Ğ°Ğ¼ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹, Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ° Ğ¾ÑÑŒ Ğ¿Ñ€Ğ¸ÑÑƒÑ‚ÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ²Ğ¾ Ğ²ÑĞµÑ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚, Ğ½Ğ°ÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ÑĞ¸Ğ»ÑŒĞ½Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€Ğ¸Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ ÑĞ²Ğ¾ĞµĞ¹ Ñ‚Ğ¸Ğ¿Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ñ€Ğ¾Ğ»Ğ¸ Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ°. ĞÑ‚ĞºĞ»Ğ¾Ğ½ĞµĞ½Ğ¸Ğµ Ğ²Ğ´Ğ¾Ğ»ÑŒ ÑÑ‚Ğ¾Ğ¹ Ğ¾ÑĞ¸ Ğ² ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñƒ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ´Ñ€ĞµĞ¹Ñ„ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ñ‹ - ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ, ĞºĞ¾Ğ³Ğ´Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ°Ñ‡Ğ¸Ğ½Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾ÑĞ²Ğ»ÑÑ‚ÑŒ Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ğ¾Ğµ Ğ¸Ğ»Ğ¸ ÑÑ‚Ñ€Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ°Ñ… Ñ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼Ğ¸. Ğ¤Ğ¸ĞºÑĞ°Ñ†Ğ¸Ñ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾Ğ¼ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğµ ÑÑ‚Ğ¾Ğ¹ Ğ¾ÑĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ¸Ñ‚ÑŒ ĞµÑ‘ Ğ¾Ñ‚ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· persona-based Ğ°Ñ‚Ğ°ĞºĞ¸."}, 'en': {'title': 'Steering Language Models Towards Stability and Helpfulness', 'desc': "This paper explores how large language models (LLMs) can embody different personas, focusing on a key dimension called the 'Assistant Axis' that influences their helpfulness and stability. The research shows that steering techniques can effectively guide models towards this Assistant persona, promoting safe and constructive interactions. However, when models are steered away from this axis, they may adopt less desirable behaviors, including a theatrical speaking style or persona drift, which can lead to harmful outputs. The findings highlight the importance of controlling model activations to maintain a consistent and beneficial persona, especially in sensitive conversational contexts."}, 'zh': {'title': 'ç¨³å®šå¤§å‹è¯­è¨€æ¨¡å‹çš„åŠ©æ‰‹äººæ ¼', 'desc': 'è¿™ç¯‡è®ºæ–‡ç ”ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸€ä¸ªè¢«ç§°ä¸ºâ€œåŠ©æ‰‹è½´â€çš„äººæ ¼ç©ºé—´ä¸­è¿ä½œçš„æ–¹å¼ã€‚åŠ©æ‰‹è½´æ§åˆ¶ç€æ¨¡å‹çš„æœ‰ç”¨æ€§å’Œè¡Œä¸ºç¨³å®šæ€§ï¼Œä½¿ç”¨å¼•å¯¼æŠ€æœ¯å¯ä»¥å½±å“æ¨¡å‹çš„å“åº”ï¼Œé˜²æ­¢æœ‰å®³è¡Œä¸ºçš„æ¼‚ç§»ã€‚ç ”ç©¶å‘ç°ï¼Œæ¨¡å‹åœ¨é»˜è®¤åŠ©æ‰‹æ¨¡å¼ä¸‹è¡¨ç°å‡ºæœ‰ç”¨å’Œæ— å®³çš„è¡Œä¸ºï¼Œè€Œåç¦»åŠ©æ‰‹æ–¹å‘åˆ™å¯èƒ½å¯¼è‡´æ¨¡å‹è¡¨ç°å‡ºå…¶ä»–äººæ ¼ç‰¹å¾ï¼Œç”šè‡³å‡ºç°å¥‡å¼‚çš„è¯´è¯é£æ ¼ã€‚é€šè¿‡é™åˆ¶æ¿€æ´»åœ¨åŠ©æ‰‹è½´çš„ç‰¹å®šåŒºåŸŸï¼Œå¯ä»¥åœ¨é¢å¯¹æŒ‘æˆ˜æ€§å¯¹è¯æ—¶ç¨³å®šæ¨¡å‹çš„è¡Œä¸ºã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.08441', 'title': 'YaPO: Learnable Sparse Activation Steering Vectors for Domain Adaptation', 'url': 'https://huggingface.co/papers/2601.08441', 'abstract': 'YaPO learns sparse steering vectors through sparse autoencoder latent space optimization, enabling more effective and stable control of large language model behaviors compared to dense methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Steering Large Language Models (LLMs) through activation interventions has emerged as a lightweight alternative to fine-tuning for alignment and personalization. Recent work on Bi-directional Preference Optimization (BiPO) shows that dense steering vectors can be learned directly from preference data in a Direct Preference Optimization (DPO) fashion, enabling control over truthfulness, hallucinations, and safety behaviors. However, dense steering vectors often entangle multiple latent factors due to neuron multi-semanticity, limiting their effectiveness and stability in fine-grained settings such as cultural alignment, where closely related values and behaviors (e.g., among Middle Eastern cultures) must be distinguished. In this paper, we propose Yet another Policy Optimization (YaPO), a reference-free method that learns sparse steering vectors in the latent space of a Sparse Autoencoder (SAE). By optimizing sparse codes, YaPO produces disentangled, interpretable, and efficient steering directions. Empirically, we show that YaPO converges faster, achieves stronger performance, and exhibits improved training stability compared to dense steering baselines. Beyond cultural alignment, YaPO generalizes to a range of alignment-related behaviors, including hallucination, wealth-seeking, jailbreak, and power-seeking. Importantly, YaPO preserves general knowledge, with no measurable degradation on MMLU. Overall, our results show that YaPO provides a general recipe for efficient, stable, and fine-grained alignment of LLMs, with broad applications to controllability and domain adaptation. The associated code and data are publicly availablehttps://github.com/MBZUAI-Paris/YaPO.', 'score': 5, 'issue_id': 670, 'pub_date': '2026-01-13', 'pub_date_card': {'ru': '13 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 13', 'zh': '1æœˆ13æ—¥'}, 'hash': '174f6d2d1267e99b', 'authors': ['Abdelaziz Bounhar', 'Rania Hossam Elmohamady Elbadry', 'Hadi Abdine', 'Preslav Nakov', 'Michalis Vazirgiannis', 'Guokan Shang'], 'affiliations': ['Ecole Polytechnique', 'MBZUAI'], 'pdf_title_img': 'assets/pdf/title_img/2601.08441.jpg', 'data': {'categories': ['#interpretability', '#training', '#rlhf', '#hallucinations', '#open_source', '#alignment'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Ğ Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ñ‹ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ LLM', 'desc': 'YaPO â€” ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ² ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ² Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ°. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿ĞµÑ€ĞµĞ¿ÑƒÑ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¾Ğ² Ğ¸Ğ·-Ğ·Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ¾Ğ², Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ ÑÑ…Ğ¾Ğ´ÑÑ‚ÑÑ Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ»ÑƒÑ‡ÑˆÑƒÑ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. YaPO ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ´Ğ»Ñ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ğ¾Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸, ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹, Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñ‹ Ğ¾Ñ‚ jailbreak-Ğ°Ñ‚Ğ°Ğº Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ğ¾Ğ±Ñ‰Ğ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹.'}, 'en': {'title': 'YaPO: Sparse Steering for Stable LLM Control', 'desc': 'This paper introduces Yet another Policy Optimization (YaPO), a method that learns sparse steering vectors using a Sparse Autoencoder (SAE) to enhance the control of large language models (LLMs). Unlike traditional dense steering vectors, which can mix multiple factors and reduce effectiveness, YaPO focuses on optimizing sparse codes for clearer and more interpretable steering directions. The authors demonstrate that YaPO not only converges faster but also provides better performance and stability in various alignment tasks, such as cultural alignment and managing hallucinations. Overall, YaPO offers a robust framework for fine-tuning LLM behaviors while maintaining general knowledge integrity.'}, 'zh': {'title': 'YaPOï¼šé«˜æ•ˆç¨³å®šçš„ç¨€ç–å¼•å¯¼æ–¹æ³•', 'desc': 'YaPOæ˜¯ä¸€ç§é€šè¿‡ç¨€ç–è‡ªç¼–ç å™¨ä¼˜åŒ–æ½œåœ¨ç©ºé—´æ¥å­¦ä¹ ç¨€ç–å¼•å¯¼å‘é‡çš„æ–¹æ³•ã€‚è¿™ç§æ–¹æ³•ç›¸æ¯”äºå¯†é›†æ–¹æ³•ï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆå’Œç¨³å®šåœ°æ§åˆ¶å¤§å‹è¯­è¨€æ¨¡å‹çš„è¡Œä¸ºã€‚YaPOé€šè¿‡ä¼˜åŒ–ç¨€ç–ç¼–ç ï¼Œç”Ÿæˆå¯è§£è¯»ä¸”é«˜æ•ˆçš„å¼•å¯¼æ–¹å‘ï¼Œé€‚ç”¨äºæ–‡åŒ–å¯¹é½ç­‰ç»†ç²’åº¦è®¾ç½®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒYaPOåœ¨æ”¶æ•›é€Ÿåº¦ã€æ€§èƒ½å’Œè®­ç»ƒç¨³å®šæ€§ä¸Šå‡ä¼˜äºå¯†é›†å¼•å¯¼åŸºçº¿ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.11096', 'title': 'CoDance: An Unbind-Rebind Paradigm for Robust Multi-Subject Animation', 'url': 'https://huggingface.co/papers/2601.11096', 'abstract': 'CoDance introduces an Unbind-Rebind framework for animating multiple subjects with flexible spatial configurations, using pose shift encoding and semantic/textual guidance for motion reassignment.  \t\t\t\t\tAI-generated summary \t\t\t\t Character image animation is gaining significant importance across various domains, driven by the demand for robust and flexible multi-subject rendering. While existing methods excel in single-person animation, they struggle to handle arbitrary subject counts, diverse character types, and spatial misalignment between the reference image and the driving poses. We attribute these limitations to an overly rigid spatial binding that forces strict pixel-wise alignment between the pose and reference, and an inability to consistently rebind motion to intended subjects. To address these challenges, we propose CoDance, a novel Unbind-Rebind framework that enables the animation of arbitrary subject counts, types, and spatial configurations conditioned on a single, potentially misaligned pose sequence. Specifically, the Unbind module employs a novel pose shift encoder to break the rigid spatial binding between the pose and the reference by introducing stochastic perturbations to both poses and their latent features, thereby compelling the model to learn a location-agnostic motion representation. To ensure precise control and subject association, we then devise a Rebind module, leveraging semantic guidance from text prompts and spatial guidance from subject masks to direct the learned motion to intended characters. Furthermore, to facilitate comprehensive evaluation, we introduce a new multi-subject CoDanceBench. Extensive experiments on CoDanceBench and existing datasets show that CoDance achieves SOTA performance, exhibiting remarkable generalization across diverse subjects and spatial layouts. The code and weights will be open-sourced.', 'score': 4, 'issue_id': 662, 'pub_date': '2026-01-16', 'pub_date_card': {'ru': '16 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 16', 'zh': '1æœˆ16æ—¥'}, 'hash': '8c557edef204bebb', 'authors': ['Shuai Tan', 'Biao Gong', 'Ke Ma', 'Yutong Feng', 'Qiyuan Zhang', 'Yan Wang', 'Yujun Shen', 'Hengshuang Zhao'], 'affiliations': ['Ant Group', 'Huazhong University of Science and Technology', 'The University of Hong Kong', 'Tsinghua University', 'University of North Carolina at Chapel Hill'], 'pdf_title_img': 'assets/pdf/title_img/2601.11096.jpg', 'data': {'categories': ['#architecture', '#video', '#dataset', '#open_source'], 'emoji': 'ğŸ’ƒ', 'ru': {'title': 'Ğ“Ğ¸Ğ±ĞºĞ°Ñ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ° Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾ÑĞ²ÑĞ·Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'CoDance Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Unbind-Rebind Ğ´Ğ»Ñ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹ Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸. ĞœĞ¾Ğ´ÑƒĞ»ÑŒ Unbind Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº ÑĞ´Ğ²Ğ¸Ğ³Ğ° Ğ¿Ğ¾Ğ·Ñ‹ ÑĞ¾ ÑÑ‚Ğ¾Ñ…Ğ°ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ¼ÑƒÑ‰ĞµĞ½Ğ¸ÑĞ¼Ğ¸, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ñ€Ğ°Ğ·Ñ€ÑƒÑˆĞ¸Ñ‚ÑŒ Ğ¶Ñ‘ÑÑ‚ĞºÑƒÑ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºÑƒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ğ¾Ğ·Ğ¾Ğ¹ Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ĞµĞ¼, Ğ·Ğ°ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑƒÑ‡Ğ¸Ñ‚ÑŒÑÑ Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğº Ñ€Ğ°ÑĞ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ÑƒĞ»ÑŒ Rebind Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¸Ğ· Ğ¼Ğ°ÑĞ¾Ğº ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ½ÑƒĞ¶Ğ½Ñ‹Ğ¼ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶Ğ°Ğ¼. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹Ğ´Ğ°ÑÑ‰ÑƒÑÑÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ñ‹ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ SOTA Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Flexible Animation for Multiple Characters with CoDance', 'desc': 'CoDance presents a new framework called Unbind-Rebind for animating multiple characters in various spatial arrangements. It addresses the limitations of existing methods that struggle with multiple subjects and misaligned poses by introducing a pose shift encoder that allows for flexible motion representation. The framework consists of two main components: the Unbind module, which breaks rigid spatial bindings, and the Rebind module, which uses semantic and spatial guidance to accurately assign motion to specific characters. Extensive testing shows that CoDance outperforms previous methods, demonstrating its ability to generalize across different subjects and layouts.'}, 'zh': {'title': 'çµæ´»å¤šè§’è‰²åŠ¨ç”»çš„æ–°æ–¹æ³•ï¼šCoDance', 'desc': 'CoDanceæå‡ºäº†ä¸€ç§è§£ç»‘å®š-é‡ç»‘å®šæ¡†æ¶ï¼Œç”¨äºçµæ´»é…ç½®å¤šä¸ªè§’è‰²çš„åŠ¨ç”»ã€‚è¯¥æ–¹æ³•é€šè¿‡å§¿æ€åç§»ç¼–ç å’Œè¯­ä¹‰/æ–‡æœ¬æŒ‡å¯¼æ¥é‡æ–°åˆ†é…è¿åŠ¨ï¼Œå…‹æœäº†ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤šè§’è‰²åŠ¨ç”»æ—¶çš„å±€é™æ€§ã€‚è§£ç»‘å®šæ¨¡å—æ‰“ç ´äº†å§¿æ€ä¸å‚è€ƒå›¾åƒä¹‹é—´çš„ä¸¥æ ¼ç©ºé—´ç»‘å®šï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ ä½ç½®æ— å…³çš„è¿åŠ¨è¡¨ç¤ºã€‚é‡ç»‘å®šæ¨¡å—åˆ™åˆ©ç”¨æ–‡æœ¬æç¤ºå’Œè§’è‰²æ©ç çš„è¯­ä¹‰æŒ‡å¯¼ï¼Œç¡®ä¿è¿åŠ¨ç²¾ç¡®åœ°æŒ‡å‘ç›®æ ‡è§’è‰²ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.11425', 'title': 'PubMed-OCR: PMC Open Access OCR Annotations', 'url': 'https://huggingface.co/papers/2601.11425', 'abstract': 'PubMed-OCR is an OCR-centric corpus of scientific articles derived from PubMed Central Open Access PDFs. Each page image is annotated with Google Cloud Vision and released in a compact JSON schema with word-, line-, and paragraph-level bounding boxes. The corpus spans 209.5K articles (1.5M pages; ~1.3B words) and supports layout-aware modeling, coordinate-grounded QA, and evaluation of OCR-dependent pipelines. We analyze corpus characteristics (e.g., journal coverage and detected layout features) and discuss limitations, including reliance on a single OCR engine and heuristic line reconstruction. We release the data and schema to facilitate downstream research and invite extensions.', 'score': 3, 'issue_id': 674, 'pub_date': '2026-01-16', 'pub_date_card': {'ru': '16 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 16', 'zh': '1æœˆ16æ—¥'}, 'hash': '898c8f9cb7b73416', 'authors': ['Hunter Heidenreich', 'Yosheb Getachew', 'Olivia Dinica', 'Ben Elliott'], 'affiliations': ['Roots.ai'], 'pdf_title_img': 'assets/pdf/title_img/2601.11425.jpg', 'data': {'categories': [], 'emoji': 'ğŸ“„', 'ru': {'title': 'Ğ Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ…', 'desc': 'PubMed-OCR â€” ÑÑ‚Ğ¾ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğ¹ ĞºĞ¾Ñ€Ğ¿ÑƒÑ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ñ‚ĞµĞ¹ Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸, Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ² (OCR) Ğ¾Ñ‚ Google Cloud Vision. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 209.5 Ñ‚Ñ‹ÑÑÑ‡ ÑÑ‚Ğ°Ñ‚ĞµĞ¹ Ğ¸Ğ· PubMed Central Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¾Ğ¹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑĞ»Ğ¾Ğ², ÑÑ‚Ñ€Ğ¾Ğº Ğ¸ Ğ°Ğ±Ğ·Ğ°Ñ†ĞµĞ², Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚Ñ‹ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ğ¿Ñ€ÑĞ¼Ğ¾ÑƒĞ³Ğ¾Ğ»ÑŒĞ½Ğ¸ĞºĞ¾Ğ² Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ°. ĞšĞ¾Ñ€Ğ¿ÑƒÑ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ ÑƒÑ‡Ñ‘Ñ‚Ğ¾Ğ¼ Ğ¼Ğ°ĞºĞµÑ‚Ğ° Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°, Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ñ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºĞ¾Ğ¹ Ğº ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚Ğ°Ğ¼ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºÑƒ OCR-Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ñ‹Ñ… Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½Ğ¾Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ñ‚ĞµĞºÑÑ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°, Ğ¾Ğ±ÑÑƒĞ¶Ğ´Ğ°ÑÑ‚ ĞµĞ³Ğ¾ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ÑÑ‚ Ñ€ĞµÑÑƒÑ€Ñ Ğ´Ğ»Ñ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² ÑĞ¾Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ğµ.'}, 'en': {'title': 'Unlocking Scientific Text with PubMed-OCR', 'desc': "The paper introduces PubMed-OCR, a large dataset focused on optical character recognition (OCR) from scientific articles in PubMed Central. It provides detailed annotations for each page image, including bounding boxes for words, lines, and paragraphs, which are essential for training layout-aware models. The dataset consists of over 209,000 articles and is designed to support various applications like coordinate-grounded question answering and evaluation of OCR systems. The authors also discuss the dataset's characteristics and limitations, such as its dependence on a single OCR engine, and encourage further research and enhancements."}, 'zh': {'title': 'PubMed-OCRï¼šç§‘å­¦æ–‡ç« çš„OCRè¯­æ–™åº“', 'desc': 'PubMed-OCRæ˜¯ä¸€ä¸ªä»¥å…‰å­¦å­—ç¬¦è¯†åˆ«ï¼ˆOCRï¼‰ä¸ºä¸­å¿ƒçš„ç§‘å­¦æ–‡ç« è¯­æ–™åº“ï¼Œæ¥æºäºPubMed Centralå¼€æ”¾è·å–çš„PDFæ–‡æ¡£ã€‚æ¯ä¸ªé¡µé¢å›¾åƒéƒ½ç»è¿‡Google Cloud Visionçš„æ³¨é‡Šï¼Œå¹¶ä»¥ç´§å‡‘çš„JSONæ ¼å¼å‘å¸ƒï¼ŒåŒ…å«å•è¯ã€è¡Œå’Œæ®µè½çº§çš„è¾¹ç•Œæ¡†ã€‚è¯¥è¯­æ–™åº“æ¶µç›–äº†209.5Kç¯‡æ–‡ç« ï¼ˆ1.5Mé¡µï¼›çº¦13äº¿ä¸ªå•è¯ï¼‰ï¼Œæ”¯æŒå¸ƒå±€æ„ŸçŸ¥å»ºæ¨¡ã€åæ ‡åŸºç¡€çš„é—®ç­”å’ŒOCRä¾èµ–çš„ç®¡é“è¯„ä¼°ã€‚æˆ‘ä»¬åˆ†æäº†è¯­æ–™åº“çš„ç‰¹å¾ï¼ˆä¾‹å¦‚æœŸåˆŠè¦†ç›–ç‡å’Œæ£€æµ‹åˆ°çš„å¸ƒå±€ç‰¹å¾ï¼‰ï¼Œå¹¶è®¨è®ºäº†å±€é™æ€§ï¼ŒåŒ…æ‹¬å¯¹å•ä¸€OCRå¼•æ“çš„ä¾èµ–å’Œå¯å‘å¼è¡Œé‡å»ºã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.10108', 'title': 'SIN-Bench: Tracing Native Evidence Chains in Long-Context Multimodal Scientific Interleaved Literature', 'url': 'https://huggingface.co/papers/2601.10108', 'abstract': 'Researchers introduce the Fish-in-the-Ocean paradigm and SIN-Bench dataset to evaluate multimodal language models\' ability to reason over scientific documents with evidence chains, revealing a gap between answer accuracy and traceable support.  \t\t\t\t\tAI-generated summary \t\t\t\t Evaluating whether multimodal large language models truly understand long-form scientific papers remains challenging: answer-only metrics and synthetic "Needle-In-A-Haystack" tests often reward answer matching without requiring a causal, evidence-linked reasoning trace in the document. We propose the "Fish-in-the-Ocean" (FITO) paradigm, which requires models to construct explicit cross-modal evidence chains within native scientific documents. To operationalize FITO, we build SIN-Data, a scientific interleaved corpus that preserves the native interleaving of text and figures. On top of it, we construct SIN-Bench with four progressive tasks covering evidence discovery (SIN-Find), hypothesis verification (SIN-Verify), grounded QA (SIN-QA), and evidence-anchored synthesis (SIN-Summary). We further introduce "No Evidence, No Score", scoring predictions when grounded to verifiable anchors and diagnosing evidence quality via matching, relevance, and logic. Experiments on eight MLLMs show that grounding is the primary bottleneck: Gemini-3-pro achieves the best average overall score (0.573), while GPT-5 attains the highest SIN-QA answer accuracy (0.767) but underperforms on evidence-aligned overall scores, exposing a gap between correctness and traceable support.', 'score': 3, 'issue_id': 667, 'pub_date': '2026-01-15', 'pub_date_card': {'ru': '15 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 15', 'zh': '1æœˆ15æ—¥'}, 'hash': '43bd7b45c86167a6', 'authors': ['Yiming Ren', 'Junjie Wang', 'Yuxin Meng', 'Yihang Shi', 'Zhiqiang Lin', 'Ruihang Chu', 'Yiran Xu', 'Ziming Li', 'Yunfei Zhao', 'Zihan Wang', 'Yu Qiao', 'Ruiming Tang', 'Minghao Liu', 'Yujiu Yang'], 'affiliations': ['2077AI', 'Harvard University', 'KuaiShou Inc.', 'Shanghai AI Laboratory', 'Stanford University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2601.10108.jpg', 'data': {'categories': ['#interpretability', '#science', '#dataset', '#reasoning', '#multimodal', '#long_context', '#benchmark'], 'emoji': 'ğŸŸ', 'ru': {'title': 'ĞŸĞ¾Ğ¸ÑĞº Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ² Ğ¼Ğ¾Ñ€Ğµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸: Ğ¾Ñ‚ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğº Ğ´Ğ¾ĞºĞ°Ğ·ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Fish-in-the-Ocean Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… SIN-Bench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ Ğ½Ğ°Ğ´ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ². Ğ¢Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´Ğ°ÑÑ‚ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ±ĞµĞ· Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğº Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ ÑĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¾Ñ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ²Ğ½Ğ¾ ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ÑŒ ĞºÑ€Ğ¾ÑÑĞ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ² Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸ Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ñ… Ñ‚ĞµĞºÑÑ‚ Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ²Ğ¾ÑÑŒĞ¼Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ²Ñ‹ÑĞ²Ğ»ÑÑÑ‚ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€Ğ°ÑÑ…Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'Bridging the Gap: Evidence Matters in Multimodal Reasoning', 'desc': 'The paper presents the Fish-in-the-Ocean (FITO) paradigm and the SIN-Bench dataset to assess how well multimodal language models can reason with scientific documents. It highlights the limitations of traditional evaluation methods that focus solely on answer accuracy without ensuring that the answers are supported by traceable evidence. The FITO approach requires models to create explicit evidence chains that connect text and figures within scientific papers. The findings reveal a significant gap between the accuracy of answers provided by models and the quality of the evidence supporting those answers, emphasizing the need for better grounding in model evaluations.'}, 'zh': {'title': 'æµ·æ´‹ä¸­çš„é±¼ï¼šç§‘å­¦æ–‡çŒ®æ¨ç†çš„æ–°èŒƒå¼', 'desc': 'ç ”ç©¶äººå‘˜æå‡ºäº†"æµ·æ´‹ä¸­çš„é±¼"èŒƒå¼å’ŒSIN-Benchæ•°æ®é›†ï¼Œä»¥è¯„ä¼°å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹åœ¨ç§‘å­¦æ–‡çŒ®ä¸­åŸºäºè¯æ®é“¾è¿›è¡Œæ¨ç†çš„èƒ½åŠ›ã€‚è¯¥ç ”ç©¶æ­ç¤ºäº†ç­”æ¡ˆå‡†ç¡®æ€§ä¸å¯è¿½æº¯æ”¯æŒä¹‹é—´çš„å·®è·ã€‚é€šè¿‡æ„å»ºSIN-Dataå’ŒSIN-Benchï¼Œç ”ç©¶è€…è®¾è®¡äº†å››ä¸ªé€æ­¥ä»»åŠ¡ï¼Œæ¶µç›–è¯æ®å‘ç°ã€å‡è®¾éªŒè¯ã€åŸºäºè¯æ®çš„é—®ç­”å’Œè¯æ®é”šå®šçš„ç»¼åˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ¨¡å‹åœ¨è¯æ®å¯¹é½æ–¹é¢å­˜åœ¨ä¸»è¦ç“¶é¢ˆï¼Œå°½ç®¡æŸäº›æ¨¡å‹åœ¨ç­”æ¡ˆå‡†ç¡®æ€§ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å¯è¿½æº¯æ”¯æŒçš„æ•´ä½“è¯„åˆ†ä¸Šå´è¡¨ç°ä¸ä½³ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.09512', 'title': 'CLARE: Continual Learning for Vision-Language-Action Models via Autonomous Adapter Routing and Expansion', 'url': 'https://huggingface.co/papers/2601.09512', 'abstract': 'CLARE is a parameter-efficient, exemplar-free continual learning framework for vision-language-action models that enables robots to adapt to new tasks while preserving previously learned knowledge through lightweight adapters and dynamic routing mechanisms.  \t\t\t\t\tAI-generated summary \t\t\t\t To teach robots complex manipulation tasks, it is now a common practice to fine-tune a pre-trained vision-language-action model (VLA) on task-specific data. However, since this recipe updates existing representations, it is unsuitable for long-term operation in the real world, where robots must continually adapt to new tasks and environments while retaining the knowledge they have already acquired. Existing continual learning methods for robotics commonly require storing previous data (exemplars), struggle with long task sequences, or rely on task identifiers for deployment. To address these limitations, we propose CLARE, a general, parameter-efficient framework for exemplar-free continual learning with VLAs. CLARE introduces lightweight modular adapters into selected feedforward layers and autonomously expands the model only where necessary when learning a new task, guided by layer-wise feature similarity. During deployment, an autoencoder-based routing mechanism dynamically activates the most relevant adapters without requiring task labels. Through extensive experiments on the LIBERO benchmark, we show that CLARE achieves high performance on new tasks without catastrophic forgetting of earlier tasks, significantly outperforming even exemplar-based methods. Code and data are available at https://tum-lsy.github.io/clare.', 'score': 1, 'issue_id': 671, 'pub_date': '2026-01-14', 'pub_date_card': {'ru': '14 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 14', 'zh': '1æœˆ14æ—¥'}, 'hash': '5862965df7fc323e', 'authors': ['Ralf RÃ¶mer', 'Yi Zhang', 'Angela P. Schoellig'], 'affiliations': ['Technical University of Munich'], 'pdf_title_img': 'assets/pdf/title_img/2601.09512.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#training', '#cv', '#robotics'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ Ğ¾Ğ±Ğ¾Ñ‚Ñ‹ ÑƒÑ‡Ğ°Ñ‚ÑÑ Ğ½Ğ¾Ğ²Ğ¾Ğ¼Ñƒ, Ğ½Ğµ Ğ·Ğ°Ğ±Ñ‹Ğ²Ğ°Ñ ÑÑ‚Ğ°Ñ€Ğ¾Ğ³Ğ¾', 'desc': 'CLARE â€” ÑÑ‚Ğ¾ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ continual learning, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼ Ğ½Ğ° Ğ±Ğ°Ğ·Ğµ vision-language-action Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğº Ğ½Ğ¾Ğ²Ñ‹Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼ Ğ±ĞµĞ· ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ ÑÑ‚Ğ°Ñ€Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ»Ñ‘Ğ³ĞºĞ¸Ğµ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ñ‹Ğµ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ğ² Ğ¸Ğ·Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞ»Ğ¾Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚Ğ¸, Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ñ€Ğ¸ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° ÑÑ…Ğ¾Ğ¶ĞµÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ»Ğ¾ÑĞ¼Ğ¸. Ğ’Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ€Ğ°Ğ·Ğ²Ñ‘Ñ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ routing Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°Ğ²Ñ‚Ğ¾ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ñ‹ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¼ĞµÑ‚ĞºĞ°Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ñ€Ğ¸ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¾ Ñ€Ğ°Ğ½ĞµĞµ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ´Ğ°Ğ¶Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ğ¸Ğ· Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Empowering Robots to Learn Continuously Without Forgetting', 'desc': 'CLARE is a new framework designed for continual learning in robots, allowing them to learn new tasks without forgetting previous ones. It uses lightweight adapters and a dynamic routing system to adapt the model efficiently, avoiding the need for storing past data. This approach enables robots to handle long sequences of tasks and operate in changing environments without losing their learned knowledge. Experiments show that CLARE performs better than traditional methods, even those that rely on storing past examples.'}, 'zh': {'title': 'CLAREï¼šæ— ç¤ºä¾‹çš„é«˜æ•ˆæŒç»­å­¦ä¹ æ¡†æ¶', 'desc': 'CLAREæ˜¯ä¸€ç§é«˜æ•ˆçš„æŒç»­å­¦ä¹ æ¡†æ¶ï¼Œä¸“ä¸ºè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹è®¾è®¡ï¼Œèƒ½å¤Ÿå¸®åŠ©æœºå™¨äººåœ¨å­¦ä¹ æ–°ä»»åŠ¡æ—¶ä¿ç•™ä¹‹å‰çš„çŸ¥è¯†ã€‚å®ƒé€šè¿‡è½»é‡çº§é€‚é…å™¨å’ŒåŠ¨æ€è·¯ç”±æœºåˆ¶ï¼Œé¿å…äº†å­˜å‚¨ç¤ºä¾‹æ•°æ®çš„éœ€æ±‚ï¼Œä»è€Œå®ç°æ— ç¤ºä¾‹çš„æŒç»­å­¦ä¹ ã€‚CLAREåœ¨å­¦ä¹ æ–°ä»»åŠ¡æ—¶ï¼Œä»…åœ¨å¿…è¦çš„åœ°æ–¹æ‰©å±•æ¨¡å‹ï¼Œå¹¶é€šè¿‡å±‚çº§ç‰¹å¾ç›¸ä¼¼æ€§è¿›è¡ŒæŒ‡å¯¼ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCLAREåœ¨æ–°ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼ŒåŒæ—¶æœ‰æ•ˆé˜²æ­¢äº†ç¾éš¾æ€§é—å¿˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.09668', 'title': 'STEP3-VL-10B Technical Report', 'url': 'https://huggingface.co/papers/2601.09668', 'abstract': 'STEP3-VL-10B achieves superior multimodal performance through unified pre-training with a language-aligned Perception Encoder and Qwen3-8B decoder, combined with scaled post-training and Parallel Coordinated Reasoning for efficient large-scale visual reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t We present STEP3-VL-10B, a lightweight open-source foundation model designed to redefine the trade-off between compact efficiency and frontier-level multimodal intelligence. STEP3-VL-10B is realized through two strategic shifts: first, a unified, fully unfrozen pre-training strategy on 1.2T multimodal tokens that integrates a language-aligned Perception Encoder with a Qwen3-8B decoder to establish intrinsic vision-language synergy; and second, a scaled post-training pipeline featuring over 1k iterations of reinforcement learning. Crucially, we implement Parallel Coordinated Reasoning (PaCoRe) to scale test-time compute, allocating resources to scalable perceptual reasoning that explores and synthesizes diverse visual hypotheses. Consequently, despite its compact 10B footprint, STEP3-VL-10B rivals or surpasses models 10times-20times larger (e.g., GLM-4.6V-106B, Qwen3-VL-235B) and top-tier proprietary flagships like Gemini 2.5 Pro and Seed-1.5-VL. Delivering best-in-class performance, it records 92.2% on MMBench and 80.11% on MMMU, while excelling in complex reasoning with 94.43% on AIME2025 and 75.95% on MathVision. We release the full model suite to provide the community with a powerful, efficient, and reproducible baseline.', 'score': 155, 'issue_id': 611, 'pub_date': '2026-01-14', 'pub_date_card': {'ru': '14 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 14', 'zh': '1æœˆ14æ—¥'}, 'hash': '40d93c6c302d8c56', 'authors': ['Ailin Huang', 'Chengyuan Yao', 'Chunrui Han', 'Fanqi Wan', 'Hangyu Guo', 'Haoran Lv', 'Hongyu Zhou', 'Jia Wang', 'Jian Zhou', 'Jianjian Sun', 'Jingcheng Hu', 'Kangheng Lin', 'Liang Zhao', 'Mitt Huang', 'Song Yuan', 'Wenwen Qu', 'Xiangfeng Wang', 'Yanlin Lai', 'Yingxiu Zhao', 'Yinmin Zhang', 'Yukang Shi', 'Yuyang Chen', 'Zejia Weng', 'Ziyang Meng', 'Ang Li', 'Aobo Kong', 'Bo Dong', 'Changyi Wan', 'David Wang', 'Di Qi', 'Dingming Li', 'En Yu', 'Guopeng Li', 'Haiquan Yin', 'Han Zhou', 'Hanshan Zhang', 'Haolong Yan', 'Hebin Zhou', 'Hongbo Peng', 'Jiaran Zhang', 'Jiashu Lv', 'Jiayi Fu', 'Jie Cheng', 'Jie Zhou', 'Jisheng Yin', 'Jingjing Xie', 'Jingwei Wu', 'Jun Zhang', 'Junfeng Liu', 'Kaijun Tan', 'Kaiwen Yan', 'Liangyu Chen', 'Lina Chen', 'Mingliang Li', 'Qian Zhao', 'Quan Sun', 'Shaoliang Pang', 'Shengjie Fan', 'Shijie Shang', 'Siyuan Zhang', 'Tianhao You', 'Wei Ji', 'Wuxun Xie', 'Xiaobo Yang', 'Xiaojie Hou', 'Xiaoran Jiao', 'Xiaoxiao Ren', 'Xiangwen Kong', 'Xin Huang', 'Xin Wu', 'Xing Chen', 'Xinran Wang', 'Xuelin Zhang', 'Yana Wei', 'Yang Li', 'Yanming Xu', 'Yeqing Shen', 'Yuang Peng', 'Yue Peng', 'Yu Zhou', 'Yusheng Li', 'Yuxiang Yang', 'Yuyang Zhang', 'Zhe Xie', 'Zhewei Huang', 'Zhenyi Lu', 'Zhimin Fan', 'Zihui Cheng', 'Daxin Jiang', 'Qi Han', 'Xiangyu Zhang', 'Yibo Zhu', 'Zheng Ge'], 'affiliations': ['StepFun'], 'pdf_title_img': 'assets/pdf/title_img/2601.09668.jpg', 'data': {'categories': ['#optimization', '#multimodal', '#architecture', '#open_source', '#reasoning', '#training'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'ĞœĞ¾Ñ‰Ğ½Ñ‹Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ AI Ğ² ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾Ğ¼ Ğ¿Ğ°ĞºĞµÑ‚Ğµ', 'desc': 'STEP3-VL-10B â€” ÑÑ‚Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ 10 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² 10-20 Ñ€Ğ°Ğ· Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñƒ Ğº Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° 1.2 Ñ‚Ñ€Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑ Perception Encoder Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ¼ Qwen3-8B Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ¸Ğ½ĞµÑ€Ğ³Ğ¸Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ·Ñ€ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ»Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 1000 Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ²Ğ½ĞµĞ´Ñ€Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Parallel Coordinated Reasoning Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹Ğ´Ğ°ÑÑ‰Ğ¸ĞµÑÑ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…: 92.2% Ğ½Ğ° MMBench, 80.11% Ğ½Ğ° MMMU Ğ¸ 94.43% Ğ½Ğ° AIME2025, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ´Ğ°Ğ¶Ğµ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ°Ğº Gemini 2.5 Pro.'}, 'en': {'title': 'Compact Powerhouse: STEP3-VL-10B Redefines Multimodal Intelligence', 'desc': 'STEP3-VL-10B is a cutting-edge multimodal model that combines a language-aligned Perception Encoder with a Qwen3-8B decoder for enhanced visual and language understanding. It utilizes a unified pre-training approach on a massive dataset of 1.2 trillion multimodal tokens, allowing it to learn intricate relationships between text and images. The model also incorporates a scaled post-training process with reinforcement learning and Parallel Coordinated Reasoning (PaCoRe) to optimize its reasoning capabilities during inference. As a result, STEP3-VL-10B achieves performance levels comparable to much larger models while maintaining a compact size, making it a significant advancement in the field of multimodal AI.'}, 'zh': {'title': 'STEP3-VL-10Bï¼šç´§å‡‘é«˜æ•ˆçš„å¤šæ¨¡æ€æ™ºèƒ½æ–°æ ‡æ†', 'desc': 'STEP3-VL-10B æ˜¯ä¸€ä¸ªè½»é‡çº§çš„å¼€æºåŸºç¡€æ¨¡å‹ï¼Œæ—¨åœ¨é‡æ–°å®šä¹‰ç´§å‡‘æ•ˆç‡ä¸å‰æ²¿å¤šæ¨¡æ€æ™ºèƒ½ä¹‹é—´çš„æƒè¡¡ã€‚è¯¥æ¨¡å‹é€šè¿‡ç»Ÿä¸€çš„é¢„è®­ç»ƒç­–ç•¥ï¼Œç»“åˆè¯­è¨€å¯¹é½çš„æ„ŸçŸ¥ç¼–ç å™¨å’Œ Qwen3-8B è§£ç å™¨ï¼Œå»ºç«‹äº†å†…åœ¨çš„è§†è§‰-è¯­è¨€ååŒã€‚å®ƒè¿˜é‡‡ç”¨äº†å¤§è§„æ¨¡çš„åè®­ç»ƒæµç¨‹å’Œå¹¶è¡Œåè°ƒæ¨ç†ï¼ˆPaCoReï¼‰ï¼Œä»¥æé«˜è§†è§‰æ¨ç†çš„æ•ˆç‡ã€‚å°½ç®¡æ¨¡å‹ä½“ç§¯ä»…ä¸º 10Bï¼Œä½†å…¶æ€§èƒ½å¯ä¸ 10 åˆ° 20 å€æ›´å¤§çš„æ¨¡å‹ç›¸åª²ç¾ï¼Œç”šè‡³è¶…è¶Šä¸€äº›é¡¶çº§ä¸“æœ‰æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.10477', 'title': 'Urban Socio-Semantic Segmentation with Vision-Language Reasoning', 'url': 'https://huggingface.co/papers/2601.10477', 'abstract': "Urban socio-semantic segmentation is achieved through a vision-language model framework that combines cross-modal recognition and multi-stage reasoning with reinforcement learning optimization.  \t\t\t\t\tAI-generated summary \t\t\t\t As hubs of human activity, urban surfaces consist of a wealth of semantic entities. Segmenting these various entities from satellite imagery is crucial for a range of downstream applications. Current advanced segmentation models can reliably segment entities defined by physical attributes (e.g., buildings, water bodies) but still struggle with socially defined categories (e.g., schools, parks). In this work, we achieve socio-semantic segmentation by vision-language model reasoning. To facilitate this, we introduce the Urban Socio-Semantic Segmentation dataset named SocioSeg, a new resource comprising satellite imagery, digital maps, and pixel-level labels of social semantic entities organized in a hierarchical structure. Additionally, we propose a novel vision-language reasoning framework called SocioReasoner that simulates the human process of identifying and annotating social semantic entities via cross-modal recognition and multi-stage reasoning. We employ reinforcement learning to optimize this non-differentiable process and elicit the reasoning capabilities of the vision-language model. Experiments demonstrate our approach's gains over state-of-the-art models and strong zero-shot generalization. Our dataset and code are available in https://github.com/AMAP-ML/SocioReasoner.", 'score': 144, 'issue_id': 613, 'pub_date': '2026-01-15', 'pub_date_card': {'ru': '15 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 15', 'zh': '1æœˆ15æ—¥'}, 'hash': 'f42abdc4825281da', 'authors': ['Yu Wang', 'Yi Wang', 'Rui Dai', 'Yujie Wang', 'Kaikui Liu', 'Xiangxiang Chu', 'Yansheng Li'], 'affiliations': ['Amap, Alibaba Group', 'Wuhan University'], 'pdf_title_img': 'assets/pdf/title_img/2601.10477.jpg', 'data': {'categories': ['#multimodal', '#cv', '#reasoning', '#rl', '#dataset', '#open_source'], 'emoji': 'ğŸ›°ï¸', 'ru': {'title': 'ĞŸĞ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ¼Ñ‹ÑĞ»Ğ° Ğ³Ğ¾Ñ€Ğ¾Ğ´Ğ° Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ°', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ³Ğ¾Ñ€Ğ¾Ğ´ÑĞºĞ¸Ñ… Ñ‚ĞµÑ€Ñ€Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ vision-language Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°ÑÑ‚ ĞºĞ°Ğº Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ, Ñ‚Ğ°Ğº Ğ¸ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ½Ğ° ÑĞ¿ÑƒÑ‚Ğ½Ğ¸ĞºĞ¾Ğ²Ñ‹Ñ… ÑĞ½Ğ¸Ğ¼ĞºĞ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ SocioSeg Ñ Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ°Ğ¼Ğ¸ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² (ÑˆĞºĞ¾Ğ», Ğ¿Ğ°Ñ€ĞºĞ¾Ğ² Ğ¸ Ñ‚.Ğ´.) Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº SocioReasoner, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ ÑÑ‚Ğ¸Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ. Ğ”Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ½ĞµĞ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰ĞµĞµ Ñ€Ğ°ÑĞºÑ€Ñ‹Ñ‚ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ vision-language Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ñƒ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ¿ĞµÑ€ĞµĞ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¸ Ñ…Ğ¾Ñ€Ğ¾ÑˆÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ.'}, 'en': {'title': 'Revolutionizing Urban Segmentation with Vision-Language Models', 'desc': "This paper presents a new method for urban socio-semantic segmentation using a vision-language model framework. It addresses the challenge of segmenting socially defined categories from satellite images, which traditional models struggle with. The authors introduce the SocioSeg dataset, which includes satellite imagery and detailed labels for social entities, organized hierarchically. They also propose the SocioReasoner framework that utilizes reinforcement learning to enhance the model's reasoning capabilities, achieving better performance than existing models and demonstrating strong zero-shot generalization."}, 'zh': {'title': 'åŸå¸‚ç¤¾ä¼šè¯­ä¹‰åˆ†å‰²çš„æ–°æ–¹æ³•', 'desc': 'æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸå¸‚ç¤¾ä¼šè¯­ä¹‰åˆ†å‰²çš„æ–¹æ³•ï¼Œåˆ©ç”¨è§†è§‰-è¯­è¨€æ¨¡å‹æ¡†æ¶è¿›è¡Œè·¨æ¨¡æ€è¯†åˆ«å’Œå¤šé˜¶æ®µæ¨ç†ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†SocioSegï¼ŒåŒ…å«å«æ˜Ÿå›¾åƒã€æ•°å­—åœ°å›¾å’Œç¤¾ä¼šè¯­ä¹‰å®ä½“çš„åƒç´ çº§æ ‡ç­¾ï¼Œå¸®åŠ©æ¨¡å‹ç†è§£åŸå¸‚ç¯å¢ƒä¸­çš„ç¤¾ä¼šå…ƒç´ ã€‚é€šè¿‡ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–æ¨ç†è¿‡ç¨‹ï¼Œæˆ‘ä»¬çš„æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°è¯†åˆ«å’Œæ ‡æ³¨ç¤¾ä¼šå®šä¹‰çš„ç±»åˆ«ï¼Œå¦‚å­¦æ ¡å’Œå…¬å›­ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ¨¡å‹ï¼Œå¹¶åœ¨é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ä¸Šè¡¨ç°å‡ºè‰²ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.08763', 'title': 'Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs', 'url': 'https://huggingface.co/papers/2601.08763', 'abstract': 'Reinforcement learning for large language models is enhanced by a rollout-level objective that rewards rare high-level reasoning strategies, improving diverse solution discovery without sacrificing initial performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning (RL) has become a central paradigm for post-training large language models (LLMs), particularly for complex reasoning tasks, yet it often suffers from exploration collapse: policies prematurely concentrate on a small set of dominant reasoning patterns, improving pass@1 while limiting rollout-level diversity and gains in pass@k. We argue that this failure stems from regularizing local token behavior rather than diversity over sets of solutions. To address this, we propose Uniqueness-Aware Reinforcement Learning, a rollout-level objective that explicitly rewards correct solutions that exhibit rare high-level strategies. Our method uses an LLM-based judge to cluster rollouts for the same problem according to their high-level solution strategies, ignoring superficial variations, and reweights policy advantages inversely with cluster size. As a result, correct but novel strategies receive higher rewards than redundant ones. Across mathematics, physics, and medical reasoning benchmarks, our approach consistently improves pass@k across large sampling budgets and increases the area under the pass@k curve (AUC@K) without sacrificing pass@1, while sustaining exploration and uncovering more diverse solution strategies at scale.', 'score': 122, 'issue_id': 610, 'pub_date': '2026-01-13', 'pub_date_card': {'ru': '13 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 13', 'zh': '1æœˆ13æ—¥'}, 'hash': '9504e6a54b716754', 'authors': ['Zhiyuan Hu', 'Yucheng Wang', 'Yufei He', 'Jiaying Wu', 'Yilun Zhao', 'See-Kiong Ng', 'Cynthia Breazeal', 'Anh Tuan Luu', 'Hae Won Park', 'Bryan Hooi'], 'affiliations': ['MIT', 'NTU', 'NUS', 'Yale'], 'pdf_title_img': 'assets/pdf/title_img/2601.08763.jpg', 'data': {'categories': ['#alignment', '#reasoning', '#optimization', '#rl', '#training'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Ğ Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ğ¾Ğ¾Ñ‰Ñ€ĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ´ĞºĞ¸Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ² RL Ğ´Ğ»Ñ LLM', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ²Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ rollout, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ¾Ñ‰Ñ€ÑĞµÑ‚ Ñ€ĞµĞ´ĞºĞ¸Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ»ÑÑÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ exploration collapse, ĞºĞ¾Ğ³Ğ´Ğ° Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ° ÑĞ¾ÑÑ€ĞµĞ´Ğ¾Ñ‚Ğ¾Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ´Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Uniqueness-Aware Reinforcement Learning Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ LLM-ÑÑƒĞ´ÑŒÑ Ğ´Ğ»Ñ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ğ¸Ñ… Ğ²Ñ‹ÑĞ¾ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğ¼ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸ÑĞ¼ Ğ¸ Ğ¿ĞµÑ€ĞµĞ²Ğ°Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾ Ğ¿Ñ€Ğ¾Ğ¿Ğ¾Ñ€Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ñƒ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº pass@k Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ¸, Ñ„Ğ¸Ğ·Ğ¸ĞºĞ¸ Ğ¸ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ pass@1.'}, 'en': {'title': 'Unlocking Diverse Solutions with Unique Strategies in RL', 'desc': 'This paper introduces Uniqueness-Aware Reinforcement Learning (UARL) to enhance the performance of large language models (LLMs) in complex reasoning tasks. Traditional reinforcement learning often leads to exploration collapse, where models focus too much on common reasoning patterns, limiting their ability to discover diverse solutions. UARL addresses this by rewarding rare high-level reasoning strategies, encouraging the model to explore a wider range of solutions. The method clusters similar solutions and adjusts rewards to favor unique strategies, resulting in improved performance across various reasoning benchmarks without compromising initial accuracy.'}, 'zh': {'title': 'ç‹¬ç‰¹æ€§æ„è¯†å¼ºåŒ–å­¦ä¹ ï¼šæå‡æ¨ç†å¤šæ ·æ€§ä¸æ€§èƒ½', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œç§°ä¸ºç‹¬ç‰¹æ€§æ„è¯†å¼ºåŒ–å­¦ä¹ ï¼Œæ—¨åœ¨æ”¹å–„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚è¯¥æ–¹æ³•é€šè¿‡å¥–åŠ±ç¨€æœ‰çš„é«˜å±‚æ¬¡æ¨ç†ç­–ç•¥ï¼Œé¼“åŠ±æ¨¡å‹æ¢ç´¢å¤šæ ·åŒ–çš„è§£å†³æ–¹æ¡ˆï¼Œè€Œä¸æ˜¯ä»…ä»…é›†ä¸­äºå°‘æ•°ä¸»å¯¼çš„æ¨ç†æ¨¡å¼ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨åŸºäºè¯­è¨€æ¨¡å‹çš„è¯„åˆ¤è€…å¯¹ç›¸åŒé—®é¢˜çš„è§£å†³æ–¹æ¡ˆè¿›è¡Œèšç±»ï¼Œä»è€Œæé«˜äº†æ­£ç¡®ä½†æ–°é¢–ç­–ç•¥çš„å¥–åŠ±ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ•°å­¦ã€ç‰©ç†å’ŒåŒ»å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼Œæ˜¾è‘—æé«˜äº†å¤šæ ·æ€§å’Œæ•´ä½“æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.09667', 'title': 'Collaborative Multi-Agent Test-Time Reinforcement Learning for Reasoning', 'url': 'https://huggingface.co/papers/2601.09667', 'abstract': 'Multi-Agent Test-Time Reinforcement Learning (MATTRL) enhances multi-agent reasoning through structured textual experience injection and consensus-based decision making at inference time.  \t\t\t\t\tAI-generated summary \t\t\t\t Multi-agent systems have evolved into practical LLM-driven collaborators for many applications, gaining robustness from diversity and cross-checking. However, multi-agent RL (MARL) training is resource-intensive and unstable: co-adapting teammates induce non-stationarity, and rewards are often sparse and high-variance. Therefore, we introduce Multi-Agent Test-Time Reinforcement Learning (MATTRL), a framework that injects structured textual experience into multi-agent deliberation at inference time. MATTRL forms a multi-expert team of specialists for multi-turn discussions, retrieves and integrates test-time experiences, and reaches consensus for final decision-making. We also study credit assignment for constructing a turn-level experience pool, then reinjecting it into the dialogue. Across challenging benchmarks in medicine, math, and education, MATTRL improves accuracy by an average of 3.67\\% over a multi-agent baseline, and by 8.67\\% over comparable single-agent baselines. Ablation studies examine different credit-assignment schemes and provide a detailed comparison of how they affect training outcomes. MATTRL offers a stable, effective and efficient path to distribution-shift-robust multi-agent reasoning without tuning.', 'score': 71, 'issue_id': 610, 'pub_date': '2026-01-14', 'pub_date_card': {'ru': '14 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 14', 'zh': '1æœˆ14æ—¥'}, 'hash': '135666a5989aae03', 'authors': ['Zhiyuan Hu', 'Yunhai Hu', 'Juncheng Liu', 'Shuyue Stella Li', 'Yucheng Wang', 'Zhen Xu', 'See-Kiong Ng', 'Anh Tuan Luu', 'Xinxing Xu', 'Bryan Hooi', 'Cynthia Breazeal', 'Hae Won Park'], 'affiliations': ['Columbia', 'MIT', 'Microsoft', 'NTU', 'NUS', 'NYU', 'UW'], 'pdf_title_img': 'assets/pdf/title_img/2601.09667.jpg', 'data': {'categories': ['#alignment', '#reasoning', '#inference', '#rl', '#agents', '#training'], 'emoji': 'ğŸ¤', 'ru': {'title': 'ĞœĞ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞµ Ñ‡ĞµÑ€ĞµĞ· ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¾Ğ¿Ñ‹Ñ‚ Ğ¸ ĞºĞ¾Ğ½ÑĞµĞ½ÑÑƒÑ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MATTRL â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ´Ğ¾Ñ€Ğ¾Ğ³Ğ¾ÑÑ‚Ğ¾ÑÑ‰ĞµĞ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…, Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ²Ğ½ĞµĞ´Ñ€ÑĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¹ Ğ¾Ğ¿Ñ‹Ñ‚ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²-ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸ÑÑ‚Ğ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ ĞºÑ€ĞµĞ´Ğ¸Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ Ğ¿ÑƒĞ»Ğ° Ğ¾Ğ¿Ñ‹Ñ‚Ğ° Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ ĞºĞ¾Ğ½ÑĞµĞ½ÑÑƒÑ-Ğ±Ğ°Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. ĞĞ° Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ñ… Ğ¸Ğ· Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½Ñ‹, Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ¸ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ MATTRL Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 3,67% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¼ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼ Ğ¸ Ğ½Ğ° 8,67% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ğ´Ğ½Ğ°Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Enhancing Multi-Agent Reasoning with MATTRL', 'desc': 'Multi-Agent Test-Time Reinforcement Learning (MATTRL) is a novel framework that enhances multi-agent reasoning by incorporating structured textual experiences during inference. It addresses the challenges of multi-agent reinforcement learning (MARL), such as instability and non-stationarity, by enabling agents to collaborate and reach consensus based on shared knowledge. MATTRL forms a team of specialists that engage in multi-turn discussions, improving decision-making accuracy through the integration of test-time experiences. The framework demonstrates significant performance improvements across various benchmarks, showcasing its effectiveness in robust multi-agent reasoning without the need for extensive tuning.'}, 'zh': {'title': 'å¤šæ™ºèƒ½ä½“æ¨ç†çš„æ–°è·¯å¾„', 'desc': 'å¤šæ™ºèƒ½ä½“æµ‹è¯•æ—¶å¼ºåŒ–å­¦ä¹ ï¼ˆMATTRLï¼‰é€šè¿‡åœ¨æ¨ç†æ—¶æ³¨å…¥ç»“æ„åŒ–æ–‡æœ¬ç»éªŒå’ŒåŸºäºå…±è¯†çš„å†³ç­–åˆ¶å®šï¼Œå¢å¼ºäº†å¤šæ™ºèƒ½ä½“çš„æ¨ç†èƒ½åŠ›ã€‚è¯¥æ¡†æ¶å½¢æˆäº†ä¸€ä¸ªå¤šä¸“å®¶å›¢é˜Ÿï¼Œä¸“æ³¨äºå¤šè½®è®¨è®ºï¼Œå¹¶åœ¨æ¨ç†æ—¶æ£€ç´¢å’Œæ•´åˆç»éªŒï¼Œä»¥è¾¾æˆæœ€ç»ˆå†³ç­–ã€‚MATTRLåœ¨åŒ»å­¦ã€æ•°å­¦å’Œæ•™è‚²ç­‰å…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸­ï¼Œå¹³å‡æé«˜äº†3.67%çš„å‡†ç¡®ç‡ï¼Œç›¸æ¯”äºå¤šæ™ºèƒ½ä½“åŸºçº¿å’Œ8.67%ç›¸æ¯”äºå•æ™ºèƒ½ä½“åŸºçº¿ã€‚è¯¥æ–¹æ³•æä¾›äº†ä¸€æ¡ç¨³å®šã€é«˜æ•ˆçš„è·¯å¾„ï¼Œä»¥å®ç°å¯¹åˆ†å¸ƒå˜åŒ–çš„é²æ£’æ€§ï¼Œè€Œæ— éœ€è°ƒä¼˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.02242', 'title': 'VIBE: Visual Instruction Based Editor', 'url': 'https://huggingface.co/papers/2601.02242', 'abstract': 'A compact image editing system uses a 2B-parameter model for guidance and a 1.6B-parameter diffusion model to achieve high-quality edits with low computational requirements and strict source consistency.  \t\t\t\t\tAI-generated summary \t\t\t\t Instruction-based image editing is among the fastest developing areas in generative AI. Over the past year, the field has reached a new level, with dozens of open-source models released alongside highly capable commercial systems. However, only a limited number of open-source approaches currently achieve real-world quality. In addition, diffusion backbones, the dominant choice for these pipelines, are often large and computationally expensive for many deployments and research settings, with widely used variants typically containing 6B to 20B parameters. This paper presents a compact, high-throughput instruction-based image editing pipeline that uses a modern 2B-parameter Qwen3-VL model to guide the editing process and the 1.6B-parameter diffusion model Sana1.5 for image generation. Our design decisions across architecture, data processing, training configuration, and evaluation target low-cost inference and strict source consistency while maintaining high quality across the major edit categories feasible at this scale. Evaluated on the ImgEdit and GEdit benchmarks, the proposed method matches or exceeds the performance of substantially heavier baselines, including models with several times as many parameters and higher inference cost, and is particularly strong on edits that require preserving the input image, such as an attribute adjustment, object removal, background edits, and targeted replacement. The model fits within 24 GB of GPU memory and generates edited images at up to 2K resolution in approximately 4 seconds on an NVIDIA H100 in BF16, without additional inference optimizations or distillation.', 'score': 53, 'issue_id': 618, 'pub_date': '2026-01-05', 'pub_date_card': {'ru': '5 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 5', 'zh': '1æœˆ5æ—¥'}, 'hash': '75af0a65418d103e', 'authors': ['Grigorii Alekseenko', 'Aleksandr Gordeev', 'Irina Tolstykh', 'Bulat Suleimanov', 'Vladimir Dokholyan', 'Georgii Fedorov', 'Sergey Yakubson', 'Aleksandra Tsybina', 'Mikhail Chernyshov', 'Maksim Kuprashevich'], 'affiliations': ['R&D Department, SALUTEDEV'], 'pdf_title_img': 'assets/pdf/title_img/2601.02242.jpg', 'data': {'categories': ['#multimodal', '#small_models', '#open_source', '#cv', '#diffusion', '#optimization', '#benchmark', '#training'], 'emoji': 'âœï¸', 'ru': {'title': 'ĞšĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹: Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ 2B-Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Qwen3-VL Ğ´Ğ»Ñ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ 1.6B-Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Sana1.5 Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ñ€Ğ¸ Ğ½Ğ¸Ğ·ĞºĞ¸Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑÑ…, Ñ‚Ñ€ĞµĞ±ÑƒÑ Ğ²ÑĞµĞ³Ğ¾ 24 Ğ“Ğ‘ GPU Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸ĞµĞ¼ 2K Ğ·Ğ° 4 ÑĞµĞºÑƒĞ½Ğ´Ñ‹. ĞĞ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… ImgEdit Ğ¸ GEdit Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ½Ğ°Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ Ñ‡Ğ¸ÑĞ»Ğ¾Ğ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ¿Ñ€Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰ĞµĞ¼ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ: ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ¾Ğ², ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ Ğ·Ğ°Ğ¼ĞµĞ½Ğ° Ñ„Ğ¾Ğ½Ğ¾Ğ². ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¸ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ñ‹ Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾Ğ¹ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ°.'}, 'en': {'title': 'Efficient Image Editing with Compact Models', 'desc': 'This paper introduces a compact image editing system that utilizes a 2B-parameter guidance model and a 1.6B-parameter diffusion model to perform high-quality image edits efficiently. The system is designed to maintain strict source consistency while minimizing computational costs, making it suitable for real-world applications. It outperforms larger models with significantly more parameters in various editing tasks, particularly those that require preserving the original image. The proposed method achieves fast inference times and high-resolution outputs, demonstrating the potential of smaller models in generative AI for image editing.'}, 'zh': {'title': 'é«˜æ•ˆç´§å‡‘çš„å›¾åƒç¼–è¾‘ç³»ç»Ÿ', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§ç´§å‡‘çš„å›¾åƒç¼–è¾‘ç³»ç»Ÿï¼Œä½¿ç”¨äº†ä¸€ä¸ª2Bå‚æ•°çš„æ¨¡å‹æ¥æŒ‡å¯¼ç¼–è¾‘è¿‡ç¨‹ï¼Œä»¥åŠä¸€ä¸ª1.6Bå‚æ•°çš„æ‰©æ•£æ¨¡å‹æ¥ç”Ÿæˆé«˜è´¨é‡çš„å›¾åƒã€‚è¯¥ç³»ç»Ÿåœ¨è®¡ç®—éœ€æ±‚ä½çš„æƒ…å†µä¸‹ï¼Œèƒ½å¤Ÿä¿æŒä¸¥æ ¼çš„æºä¸€è‡´æ€§ï¼Œå¹¶åœ¨ä¸»è¦ç¼–è¾‘ç±»åˆ«ä¸­ä¿æŒé«˜è´¨é‡ã€‚ä¸ä¼ ç»Ÿçš„6Båˆ°20Bå‚æ•°çš„æ‰©æ•£æ¨¡å‹ç›¸æ¯”ï¼Œè¿™ç§æ–¹æ³•åœ¨æ€§èƒ½ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦ä¿ç•™è¾“å…¥å›¾åƒçš„ç¼–è¾‘ä»»åŠ¡ä¸­ï¼Œå¦‚å±æ€§è°ƒæ•´å’Œå¯¹è±¡ç§»é™¤ã€‚è¯¥æ¨¡å‹åœ¨NVIDIA H100ä¸Šè¿è¡Œæ—¶ï¼Œèƒ½å¤Ÿåœ¨çº¦4ç§’å†…ç”Ÿæˆé«˜è¾¾2Kåˆ†è¾¨ç‡çš„ç¼–è¾‘å›¾åƒï¼Œä¸”ä»…éœ€24GBçš„GPUå†…å­˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.07641', 'title': 'Beyond Static Tools: Test-Time Tool Evolution for Scientific Reasoning', 'url': 'https://huggingface.co/papers/2601.07641', 'abstract': 'Test-Time Tool Evolution enables AI agents to dynamically create and refine computational tools during inference, overcoming limitations of static tool libraries in scientific applications.  \t\t\t\t\tAI-generated summary \t\t\t\t The central challenge of AI for Science is not reasoning alone, but the ability to create computational methods in an open-ended scientific world. Existing LLM-based agents rely on static, pre-defined tool libraries, a paradigm that fundamentally fails in scientific domains where tools are sparse, heterogeneous, and intrinsically incomplete. In this paper, we propose Test-Time Tool Evolution (TTE), a new paradigm that enables agents to synthesize, verify, and evolve executable tools during inference. By transforming tools from fixed resources into problem-driven artifacts, TTE overcomes the rigidity and long-tail limitations of static tool libraries. To facilitate rigorous evaluation, we introduce SciEvo, a benchmark comprising 1,590 scientific reasoning tasks supported by 925 automatically evolved tools. Extensive experiments show that TTE achieves state-of-the-art performance in both accuracy and tool efficiency, while enabling effective cross-domain adaptation of computational tools. The code and benchmark have been released at https://github.com/lujiaxuan0520/Test-Time-Tool-Evol.', 'score': 41, 'issue_id': 615, 'pub_date': '2026-01-12', 'pub_date_card': {'ru': '12 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 12', 'zh': '1æœˆ12æ—¥'}, 'hash': '056d266d8bc6c4f6', 'authors': ['Jiaxuan Lu', 'Ziyu Kong', 'Yemin Wang', 'Rong Fu', 'Haiyuan Wan', 'Cheng Yang', 'Wenjie Lou', 'Haoran Sun', 'Lilong Wang', 'Yankai Jiang', 'Xiaosong Wang', 'Xiao Sun', 'Dongzhan Zhou'], 'affiliations': ['Fudan University', 'Hangzhou Dianzi University', 'Shanghai Artificial Intelligence Laboratory', 'Tsinghua University', 'University of Macau', 'Xiamen University'], 'pdf_title_img': 'assets/pdf/title_img/2601.07641.jpg', 'data': {'categories': ['#science', '#open_source', '#dataset', '#agents', '#benchmark', '#reasoning'], 'emoji': 'ğŸ”§', 'ru': {'title': 'Ğ­Ğ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² AI Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° Test-Time Tool Evolution (TTE), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ¸ ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ°, Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€ĞµĞ´Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ñ‹Ñ… Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞº. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€ĞµÑˆĞ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½ĞµĞ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚Ñ‹ Ğ¸ Ğ½ĞµĞ³Ğ¸Ğ±ĞºĞ¾ÑÑ‚Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ÑÑ…, Ğ³Ğ´Ğµ Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‚ÑÑ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹. Ğ”Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº SciEvo Ñ 1590 Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¼Ğ¸ 925 Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ²ÑˆĞ¸Ğ¼Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ TTE Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑĞ¼Ğ¸.'}, 'en': {'title': 'Empowering AI with Dynamic Tool Creation for Science', 'desc': "This paper introduces Test-Time Tool Evolution (TTE), a novel approach that allows AI agents to create and improve computational tools during their operation, rather than relying on fixed tool libraries. The authors argue that traditional methods are inadequate for scientific tasks due to the diverse and incomplete nature of available tools. TTE enables agents to generate, verify, and adapt tools based on specific problems they encounter, enhancing flexibility and effectiveness. The paper also presents SciEvo, a benchmark for evaluating TTE's performance across numerous scientific reasoning tasks, demonstrating its superior accuracy and efficiency compared to existing methods."}, 'zh': {'title': 'åŠ¨æ€è¿›åŒ–å·¥å…·ï¼ŒåŠ©åŠ›ç§‘å­¦æ¢ç´¢', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œç§°ä¸ºæµ‹è¯•æ—¶å·¥å…·è¿›åŒ–ï¼ˆTTEï¼‰ï¼Œæ—¨åœ¨è§£å†³ç§‘å­¦åº”ç”¨ä¸­é™æ€å·¥å…·åº“çš„å±€é™æ€§ã€‚TTEå…è®¸äººå·¥æ™ºèƒ½ä»£ç†åœ¨æ¨ç†è¿‡ç¨‹ä¸­åŠ¨æ€åˆ›å»ºå’Œæ”¹è¿›è®¡ç®—å·¥å…·ï¼Œä»è€Œé€‚åº”å¼€æ”¾å¼ç§‘å­¦ä¸–ç•Œçš„éœ€æ±‚ã€‚é€šè¿‡å°†å·¥å…·ä»å›ºå®šèµ„æºè½¬å˜ä¸ºé—®é¢˜é©±åŠ¨çš„å·¥ä»¶ï¼ŒTTEå…‹æœäº†é™æ€å·¥å…·åº“çš„åˆšæ€§å’Œé•¿å°¾é™åˆ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTTEåœ¨å‡†ç¡®æ€§å’Œå·¥å…·æ•ˆç‡æ–¹é¢å‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶æœ‰æ•ˆæ”¯æŒè·¨é¢†åŸŸçš„è®¡ç®—å·¥å…·é€‚åº”ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.10402', 'title': 'Toward Ultra-Long-Horizon Agentic Science: Cognitive Accumulation for Machine Learning Engineering', 'url': 'https://huggingface.co/papers/2601.10402', 'abstract': "ML-Master 2.0 enables long-term autonomous machine learning engineering through hierarchical cognitive caching that manages extended context and learns from execution traces.  \t\t\t\t\tAI-generated summary \t\t\t\t The advancement of artificial intelligence toward agentic science is currently bottlenecked by the challenge of ultra-long-horizon autonomy, the ability to sustain strategic coherence and iterative correction over experimental cycles spanning days or weeks. While Large Language Models (LLMs) have demonstrated prowess in short-horizon reasoning, they are easily overwhelmed by execution details in the high-dimensional, delayed-feedback environments of real-world research, failing to consolidate sparse feedback into coherent long-term guidance. Here, we present ML-Master 2.0, an autonomous agent that masters ultra-long-horizon machine learning engineering (MLE) which is a representative microcosm of scientific discovery. By reframing context management as a process of cognitive accumulation, our approach introduces Hierarchical Cognitive Caching (HCC), a multi-tiered architecture inspired by computer systems that enables the structural differentiation of experience over time. By dynamically distilling transient execution traces into stable knowledge and cross-task wisdom, HCC allows agents to decouple immediate execution from long-term experimental strategy, effectively overcoming the scaling limits of static context windows. In evaluations on OpenAI's MLE-Bench under 24-hour budgets, ML-Master 2.0 achieves a state-of-the-art medal rate of 56.44%. Our findings demonstrate that ultra-long-horizon autonomy provides a scalable blueprint for AI capable of autonomous exploration beyond human-precedent complexities.", 'score': 31, 'issue_id': 616, 'pub_date': '2026-01-15', 'pub_date_card': {'ru': '15 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 15', 'zh': '1æœˆ15æ—¥'}, 'hash': 'b2fd5a91f91c68df', 'authors': ['Xinyu Zhu', 'Yuzhu Cai', 'Zexi Liu', 'Bingyang Zheng', 'Cheng Wang', 'Rui Ye', 'Jiaao Chen', 'Hanrui Wang', 'Wei-Chen Wang', 'Yuzhi Zhang', 'Linfeng Zhang', 'Weinan E', 'Di Jin', 'Siheng Chen'], 'affiliations': ['Carnegie Mellon University', 'Meta', 'Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2601.10402.jpg', 'data': {'categories': ['#long_context', '#science', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞšĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ·Ğ° Ğ´Ğ¾Ğ»Ğ³Ğ¸Ğµ ÑÑ€Ğ¾ĞºĞ¸', 'desc': "ML-Master 2.0 â€” ÑÑ‚Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğ¹ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ñ‚ÑĞ¶ĞµĞ½Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿ĞµÑ€Ğ¸Ğ¾Ğ´Ğ¾Ğ² Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ (Ğ´Ğ½Ğ¸ Ğ¸ Ğ½ĞµĞ´ĞµĞ»Ğ¸), ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸ĞµĞ¹ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Hierarchical Cognitive Caching (HCC) â€” Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ² ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ñƒ Ñ€Ğ°Ğ·Ğ´ĞµĞ»Ğ¸Ñ‚ÑŒ Ğ½ĞµĞ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ğ¾Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ğ¾Ñ‚ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾ĞºĞ¾Ğ½ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° LLM. ĞĞ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ OpenAI's MLE-Bench Ğ°Ğ³ĞµĞ½Ñ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³ Ñ€ĞµĞºĞ¾Ñ€Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ° 56.44% Ğ¿Ñ€Ğ¸ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğµ Ğ² 24 Ñ‡Ğ°ÑĞ°."}, 'en': {'title': 'Empowering AI with Ultra-Long-Horizon Autonomy', 'desc': 'ML-Master 2.0 is a new autonomous agent designed to improve long-term machine learning engineering by using a method called Hierarchical Cognitive Caching (HCC). This method helps the agent manage and remember information over extended periods, allowing it to learn from past experiences and make better decisions in future tasks. Unlike traditional models that struggle with complex, long-term tasks, ML-Master 2.0 can maintain strategic focus and adapt its approach based on feedback received over time. The results show that this system significantly outperforms previous models in handling long-term projects, achieving a high success rate in evaluations.'}, 'zh': {'title': 'è¶…é•¿æ—¶é—´èŒƒå›´çš„è‡ªä¸»å­¦ä¹ æ–°çºªå…ƒ', 'desc': 'ML-Master 2.0 æ˜¯ä¸€ç§è‡ªä¸»æœºå™¨å­¦ä¹ å·¥ç¨‹çš„ç³»ç»Ÿï¼Œèƒ½å¤Ÿç®¡ç†é•¿æœŸçš„ä¸Šä¸‹æ–‡å¹¶ä»æ‰§è¡Œè½¨è¿¹ä¸­å­¦ä¹ ã€‚å®ƒé€šè¿‡åˆ†å±‚è®¤çŸ¥ç¼“å­˜ï¼ˆHCCï¼‰æ¥å®ç°ï¼Œè¿™æ˜¯ä¸€ç§å¤šå±‚æ¬¡çš„æ¶æ„ï¼Œèƒ½å¤Ÿåœ¨æ—¶é—´ä¸ŠåŒºåˆ†ç»éªŒã€‚è¯¥ç³»ç»Ÿèƒ½å¤Ÿå°†ç¬æ—¶çš„æ‰§è¡Œè½¨è¿¹æç‚¼ä¸ºç¨³å®šçš„çŸ¥è¯†ï¼Œä»è€Œå°†çŸ­æœŸæ‰§è¡Œä¸é•¿æœŸå®éªŒç­–ç•¥è§£è€¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒML-Master 2.0 åœ¨è¶…é•¿æ—¶é—´èŒƒå›´å†…çš„è‡ªä¸»æ€§ä¸ºäººå·¥æ™ºèƒ½çš„è‡ªä¸»æ¢ç´¢æä¾›äº†å¯æ‰©å±•çš„è“å›¾ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.10305', 'title': 'DanQing: An Up-to-Date Large-Scale Chinese Vision-Language Pre-training Dataset', 'url': 'https://huggingface.co/papers/2601.10305', 'abstract': 'A large-scale Chinese image-text dataset called DanQing is introduced to advance vision-language pretraining, demonstrating superior performance in various downstream tasks through continual pretraining of the SigLIP2 model.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language Pre-training (VLP) models demonstrate strong performance across various downstream tasks by learning from large-scale image-text pairs through contrastive pretraining. The release of extensive English image-text datasets (e.g., COYO-700M and LAION-400M) has enabled widespread adoption of models such as CLIP and SigLIP in tasks including cross-modal retrieval and image captioning. However, the advancement of Chinese vision-language pretraining has substantially lagged behind, due to the scarcity of high-quality Chinese image-text data. To address this gap, we develop a comprehensive pipeline for constructing a high-quality Chinese cross-modal dataset. As a result, we propose DanQing, which contains 100 million image-text pairs collected from Common Crawl. Different from existing datasets, DanQing is curated through a more rigorous selection process, yielding superior data quality. Moreover, DanQing is primarily built from 2024-2025 web data, enabling models to better capture evolving semantic trends and thus offering greater practical utility. We compare DanQing with existing datasets by continual pre-training of the SigLIP2 model. Experimental results show that DanQing consistently achieves superior performance across a range of Chinese downstream tasks, including zero-shot classification, cross-modal retrieval, and LMM-based evaluations. To facilitate further research in Chinese vision-language pre-training, we will open-source the DanQing dataset under the Creative Common CC-BY 4.0 license.', 'score': 31, 'issue_id': 610, 'pub_date': '2026-01-15', 'pub_date_card': {'ru': '15 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 15', 'zh': '1æœˆ15æ—¥'}, 'hash': '35356d22511867a5', 'authors': ['Hengyu Shen', 'Tiancheng Gu', 'Bin Qin', 'Lan Wu', 'Yuling Wu', 'Shuo Tan', 'Zelong Sun', 'Jun Wang', 'Nan Wu', 'Xiang An', 'Weidong Cai', 'Ziyong Feng', 'Kaicheng Yang'], 'affiliations': ['Glint Lab'], 'pdf_title_img': 'assets/pdf/title_img/2601.10305.jpg', 'data': {'categories': [], 'emoji': 'ğŸ‡¨ğŸ‡³', 'ru': {'title': 'DanQing: ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¸Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ vision-language pretraining', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ DanQing, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 100 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¿Ğ°Ñ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ-Ñ‚ĞµĞºÑÑ‚ Ğ½Ğ° ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ vision-language Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑÑ‚Ñ€Ğ¾Ğ³Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°, Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ°ĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²ĞµĞ±-Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ 2024-2025 Ğ³Ğ¾Ğ´Ğ¾Ğ² Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚ĞµĞ½Ğ´ĞµĞ½Ñ†Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ SigLIP2, Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° DanQing, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸, ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ¾Ñ†ĞµĞ½Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ñ…. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ±ÑƒĞ´ĞµÑ‚ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾ Ğ²Ñ‹Ğ¿ÑƒÑ‰ĞµĞ½ Ğ¿Ğ¾Ğ´ Ğ»Ğ¸Ñ†ĞµĞ½Ğ·Ğ¸ĞµĞ¹ CC-BY 4.0 Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¸ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¾Ğ³Ğ¾ vision-language pretraining.'}, 'en': {'title': 'DanQing: Elevating Chinese Vision-Language Pretraining', 'desc': 'The paper introduces DanQing, a large-scale Chinese image-text dataset designed to enhance vision-language pretraining. It addresses the lack of high-quality Chinese data by providing 100 million curated image-text pairs, collected from recent web data. The dataset supports the continual pretraining of the SigLIP2 model, which shows improved performance in various Chinese downstream tasks like zero-shot classification and cross-modal retrieval. By making DanQing available for research, the authors aim to advance the field of Chinese vision-language models.'}, 'zh': {'title': 'DanQingï¼šæ¨åŠ¨ä¸­æ–‡è§†è§‰-è¯­è¨€é¢„è®­ç»ƒçš„æœªæ¥', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºDanQingçš„å¤§è§„æ¨¡ä¸­æ–‡å›¾åƒ-æ–‡æœ¬æ•°æ®é›†ï¼Œæ—¨åœ¨æ¨åŠ¨è§†è§‰-è¯­è¨€é¢„è®­ç»ƒçš„å‘å±•ã€‚é€šè¿‡å¯¹SigLIP2æ¨¡å‹çš„æŒç»­é¢„è®­ç»ƒï¼ŒDanQingåœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚è¯¥æ•°æ®é›†åŒ…å«1äº¿å¯¹å›¾åƒå’Œæ–‡æœ¬ï¼Œç»è¿‡ä¸¥æ ¼ç­›é€‰ï¼Œç¡®ä¿äº†æ•°æ®è´¨é‡çš„ä¼˜è¶Šæ€§ã€‚DanQingä¸»è¦åŸºäº2024-2025å¹´çš„ç½‘ç»œæ•°æ®ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰è¯­ä¹‰è¶‹åŠ¿ï¼Œä¸ºå®é™…åº”ç”¨æä¾›æ›´å¤§çš„ä»·å€¼ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.10061', 'title': 'CoF-T2I: Video Models as Pure Visual Reasoners for Text-to-Image Generation', 'url': 'https://huggingface.co/papers/2601.10061', 'abstract': 'Chain-of-Frame reasoning is integrated into text-to-image generation through progressive visual refinement with explicit intermediate steps, achieving superior performance on benchmark datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent video generation models have revealed the emergence of Chain-of-Frame (CoF) reasoning, enabling frame-by-frame visual inference. With this capability, video models have been successfully applied to various visual tasks (e.g., maze solving, visual puzzles). However, their potential to enhance text-to-image (T2I) generation remains largely unexplored due to the absence of a clearly defined visual reasoning starting point and interpretable intermediate states in the T2I generation process. To bridge this gap, we propose CoF-T2I, a model that integrates CoF reasoning into T2I generation via progressive visual refinement, where intermediate frames act as explicit reasoning steps and the final frame is taken as output. To establish such an explicit generation process, we curate CoF-Evol-Instruct, a dataset of CoF trajectories that model the generation process from semantics to aesthetics. To further improve quality and avoid motion artifacts, we enable independent encoding operation for each frame. Experiments show that CoF-T2I significantly outperforms the base video model and achieves competitive performance on challenging benchmarks, reaching 0.86 on GenEval and 7.468 on Imagine-Bench. These results indicate the substantial promise of video models for advancing high-quality text-to-image generation.', 'score': 27, 'issue_id': 610, 'pub_date': '2026-01-15', 'pub_date_card': {'ru': '15 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 15', 'zh': '1æœˆ15æ—¥'}, 'hash': '4386e88394aa8c6c', 'authors': ['Chengzhuo Tong', 'Mingkun Chang', 'Shenglong Zhang', 'Yuran Wang', 'Cheng Liang', 'Zhizheng Zhao', 'Ruichuan An', 'Bohan Zeng', 'Yang Shi', 'Yifan Dai', 'Ziming Zhao', 'Guanbin Li', 'Pengfei Wan', 'Yuanxing Zhang', 'Wentao Zhang'], 'affiliations': ['Kling Team, Kuaishou Technology', 'Nanjing University', 'Peking University', 'Sun Yat-sen University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2601.10061.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#video', '#dataset'], 'emoji': 'ğŸ¬', 'ru': {'title': 'ĞŸĞ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ ĞºĞ°Ğ´Ñ€Ñ‹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Chain-of-Frame Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ° Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğµ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ Ñ ÑĞ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ ÑˆĞ°Ğ³Ğ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ CoF-T2I, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ ĞºĞ°Ğ´Ñ€Ñ‹ ĞºĞ°Ğº ÑÑ‚Ğ°Ğ¿Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ° Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ°Ğ´Ñ€ ĞºĞ°Ğº Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ CoF-Evol-Instruct Ñ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ÑĞ¼Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ¸ Ğº ÑÑÑ‚ĞµÑ‚Ğ¸ĞºĞµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ.'}, 'en': {'title': 'Enhancing Text-to-Image Generation with Chain-of-Frame Reasoning', 'desc': 'This paper introduces CoF-T2I, a novel model that incorporates Chain-of-Frame (CoF) reasoning into text-to-image (T2I) generation. By using progressive visual refinement, the model generates images through a series of intermediate frames that serve as explicit reasoning steps. The authors also present a new dataset, CoF-Evol-Instruct, which captures the evolution of visual generation from semantic concepts to aesthetic outputs. Experimental results demonstrate that CoF-T2I outperforms existing video models and achieves strong performance on benchmark tests, highlighting the potential of video reasoning techniques in enhancing T2I tasks.'}, 'zh': {'title': 'é“¾å¸§æ¨ç†åŠ©åŠ›æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„çªç ´', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºCoF-T2Içš„æ¨¡å‹ï¼Œå°†é“¾å¸§æ¨ç†ï¼ˆChain-of-Frame reasoningï¼‰æ•´åˆåˆ°æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆï¼ˆtext-to-image generationï¼‰ä¸­ã€‚è¯¥æ¨¡å‹é€šè¿‡é€æ­¥çš„è§†è§‰ç»†åŒ–è¿‡ç¨‹ï¼Œåˆ©ç”¨ä¸­é—´å¸§ä½œä¸ºæ˜ç¡®çš„æ¨ç†æ­¥éª¤ï¼Œä»è€Œæé«˜ç”Ÿæˆè´¨é‡ã€‚æˆ‘ä»¬è¿˜åˆ›å»ºäº†CoF-Evol-Instructæ•°æ®é›†ï¼Œä»¥æ”¯æŒä»è¯­ä¹‰åˆ°ç¾å­¦çš„ç”Ÿæˆè¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCoF-T2Iåœ¨åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾ç¤ºå‡ºè§†é¢‘æ¨¡å‹åœ¨é«˜è´¨é‡æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­çš„å·¨å¤§æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.10332', 'title': 'Think-Then-Generate: Reasoning-Aware Text-to-Image Diffusion with LLM Encoders', 'url': 'https://huggingface.co/papers/2601.10332', 'abstract': 'Text-to-image diffusion models enhanced with language model reasoning capabilities achieve improved factual consistency and semantic alignment through a think-then-generate paradigm with dual-gradient reinforcement optimization.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent progress in text-to-image (T2I) diffusion models (DMs) has enabled high-quality visual synthesis from diverse textual prompts. Yet, most existing T2I DMs, even those equipped with large language model (LLM)-based text encoders, remain text-pixel mappers -- they employ LLMs merely as text encoders, without leveraging their inherent reasoning capabilities to infer what should be visually depicted given the textual prompt. To move beyond such literal generation, we propose the think-then-generate (T2G) paradigm, where the LLM-based text encoder is encouraged to reason about and rewrite raw user prompts; the states of the rewritten prompts then serve as diffusion conditioning. To achieve this, we first activate the think-then-rewrite pattern of the LLM encoder with a lightweight supervised fine-tuning process. Subsequently, the LLM encoder and diffusion backbone are co-optimized to ensure faithful reasoning about the context and accurate rendering of the semantics via Dual-GRPO. In particular, the text encoder is reinforced using image-grounded rewards to infer and recall world knowledge, while the diffusion backbone is pushed to produce semantically consistent and visually coherent images. Experiments show substantial improvements in factual consistency, semantic alignment, and visual realism across reasoning-based image generation and editing benchmarks, achieving 0.79 on WISE score, nearly on par with GPT-4. Our results constitute a promising step toward next-generation unified models with reasoning, expression, and demonstration capacities.', 'score': 23, 'issue_id': 610, 'pub_date': '2026-01-15', 'pub_date_card': {'ru': '15 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 15', 'zh': '1æœˆ15æ—¥'}, 'hash': '3795f149cc03f38f', 'authors': ['Siqi Kou', 'Jiachun Jin', 'Zetong Zhou', 'Ye Ma', 'Yugang Wang', 'Quan Chen', 'Peng Jiang', 'Xiao Yang', 'Jun Zhu', 'Kai Yu', 'Zhijie Deng'], 'affiliations': ['Kuaishou Technology', 'Shanghai Jiao Tong University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2601.10332.jpg', 'data': {'categories': ['#diffusion', '#rlhf', '#reasoning', '#benchmark', '#optimization', '#multimodal', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞœÑ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ¿ĞµÑ€ĞµĞ´ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸ĞµĞ¼: ÑƒÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ³Ğ´Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚ĞµĞºÑÑ‚, Ğ° Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¾ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ° Ğ¿ĞµÑ€ĞµĞ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ "think-then-generate", Ğ² ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğ¹ LLM Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¹ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ¸ Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ Dual-GRPO, Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¹ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ² Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸, ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¸ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Enhancing Image Generation with Reasoning Power', 'desc': 'This paper introduces a new approach to text-to-image diffusion models that enhances their ability to generate images by incorporating reasoning capabilities from large language models. The proposed think-then-generate (T2G) paradigm allows the model to first analyze and rewrite user prompts before generating images, improving the alignment between text and visuals. By using dual-gradient reinforcement optimization, the model is trained to produce images that are not only visually coherent but also semantically accurate. The results demonstrate significant advancements in factual consistency and realism, suggesting a move towards more integrated models that combine reasoning and visual generation.'}, 'zh': {'title': 'æ¨ç†ä¸ç”Ÿæˆçš„å®Œç¾ç»“åˆ', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œç»“åˆäº†è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œä»¥æé«˜ç”Ÿæˆå›¾åƒçš„äº‹å®ä¸€è‡´æ€§å’Œè¯­ä¹‰å¯¹é½ã€‚æˆ‘ä»¬å¼•å…¥äº†â€œå…ˆæ€è€ƒåç”Ÿæˆâ€çš„èŒƒå¼ï¼Œé¼“åŠ±è¯­è¨€æ¨¡å‹å¯¹ç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬è¿›è¡Œæ¨ç†å’Œé‡å†™ï¼Œä»è€Œæ”¹å–„ç”Ÿæˆçš„å›¾åƒè´¨é‡ã€‚é€šè¿‡åŒæ¢¯åº¦å¼ºåŒ–ä¼˜åŒ–ï¼Œæˆ‘ä»¬ç¡®ä¿äº†è¯­è¨€æ¨¡å‹å’Œæ‰©æ•£æ¨¡å‹çš„å…±åŒä¼˜åŒ–ï¼Œä½¿å¾—ç”Ÿæˆçš„å›¾åƒåœ¨è¯­ä¹‰ä¸Šæ›´åŠ ä¸€è‡´ä¸”è§†è§‰ä¸Šæ›´å…·è¿è´¯æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†ç”Ÿæˆå›¾åƒçš„çœŸå®æ„Ÿå’Œä¸€è‡´æ€§ï¼Œå±•ç¤ºäº†ä¸‹ä¸€ä»£ç»Ÿä¸€æ¨¡å‹çš„æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.10714', 'title': 'Alterbute: Editing Intrinsic Attributes of Objects in Images', 'url': 'https://huggingface.co/papers/2601.10714', 'abstract': "Alterbute presents a diffusion-based approach for editing object intrinsic attributes while preserving identity and context through relaxed training objectives and visual named entities for scalable supervision.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Alterbute, a diffusion-based method for editing an object's intrinsic attributes in an image. We allow changing color, texture, material, and even the shape of an object, while preserving its perceived identity and scene context. Existing approaches either rely on unsupervised priors that often fail to preserve identity or use overly restrictive supervision that prevents meaningful intrinsic variations. Our method relies on: (i) a relaxed training objective that allows the model to change both intrinsic and extrinsic attributes conditioned on an identity reference image, a textual prompt describing the target intrinsic attributes, and a background image and object mask defining the extrinsic context. At inference, we restrict extrinsic changes by reusing the original background and object mask, thereby ensuring that only the desired intrinsic attributes are altered; (ii) Visual Named Entities (VNEs) - fine-grained visual identity categories (e.g., ''Porsche 911 Carrera'') that group objects sharing identity-defining features while allowing variation in intrinsic attributes. We use a vision-language model to automatically extract VNE labels and intrinsic attribute descriptions from a large public image dataset, enabling scalable, identity-preserving supervision. Alterbute outperforms existing methods on identity-preserving object intrinsic attribute editing.", 'score': 20, 'issue_id': 618, 'pub_date': '2026-01-15', 'pub_date_card': {'ru': '15 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 15', 'zh': '1æœˆ15æ—¥'}, 'hash': 'a338f8964ddfa9ff', 'authors': ['Tal Reiss', 'Daniel Winter', 'Matan Cohen', 'Alex Rav-Acha', 'Yael Pritch', 'Ariel Shamir', 'Yedid Hoshen'], 'affiliations': ['Google', 'Reichman University', 'The Hebrew University of Jerusalem'], 'pdf_title_img': 'assets/pdf/title_img/2601.10714.jpg', 'data': {'categories': ['#multimodal', '#open_source', '#cv', '#diffusion', '#training'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ¾Ğ² Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸', 'desc': 'Alterbute Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ñ… Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ¾Ğ² Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ñ†Ğ²ĞµÑ‚, Ñ‚ĞµĞºÑÑ‚ÑƒÑ€Ğ° Ğ¸ Ñ„Ğ¾Ñ€Ğ¼Ğ°, Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ° Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° ÑÑ†ĞµĞ½Ñ‹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾ÑĞ»Ğ°Ğ±Ğ»ĞµĞ½Ğ½ÑƒÑ Ñ†ĞµĞ»ĞµĞ²ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ÑÑ‚ÑŒ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ñ‹ Ğ¿Ğ¾Ğ´ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸ĞµĞ¼ Ñ€ĞµÑ„ĞµÑ€ĞµĞ½ÑĞ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ñ… Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ¾Ğ² Ğ¸ Ğ¼Ğ°ÑĞºĞ¸ Ñ„Ğ¾Ğ½Ğ°. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸ĞµĞ¹ ÑĞ²Ğ»ÑÑÑ‚ÑÑ Visual Named Entities (VNE) - ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¸Ñ€ÑƒÑÑ‚ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹ Ñ Ğ¾Ğ±Ñ‰Ğ¸Ğ¼Ğ¸ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ°Ğ¼Ğ¸, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ñ… Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ vision-language Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ĞµÑ‚Ğ¾Ğº VNE Ğ¸Ğ· Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ², Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ½Ğ°Ğ´ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Alterbute: Preserve Identity While Editing Object Attributes!', 'desc': 'Alterbute is a novel diffusion-based method designed for editing intrinsic attributes of objects in images, such as color and texture, while maintaining their identity and contextual integrity. It introduces a relaxed training objective that allows for flexible changes to both intrinsic and extrinsic attributes, guided by an identity reference image and a textual description of the desired changes. The method employs Visual Named Entities (VNEs) to categorize objects based on identity-defining features, facilitating scalable supervision without compromising on identity preservation. By reusing the original background and object mask during inference, Alterbute ensures that only the intended intrinsic attributes are modified, outperforming existing techniques in this domain.'}, 'zh': {'title': 'Alterbuteï¼šä¿ç•™èº«ä»½çš„ç‰©ä½“å±æ€§ç¼–è¾‘æ–°æ–¹æ³•', 'desc': 'Alterbuteæ˜¯ä¸€ç§åŸºäºæ‰©æ•£çš„æ–¹æ³•ï¼Œç”¨äºç¼–è¾‘å›¾åƒä¸­ç‰©ä½“çš„å†…åœ¨å±æ€§ï¼ŒåŒæ—¶ä¿æŒç‰©ä½“çš„èº«ä»½å’Œåœºæ™¯ä¸Šä¸‹æ–‡ã€‚è¯¥æ–¹æ³•å…è®¸æ”¹å˜ç‰©ä½“çš„é¢œè‰²ã€çº¹ç†ã€ææ–™ç”šè‡³å½¢çŠ¶ï¼Œè€Œä¸ä¼šå½±å“å…¶æ„ŸçŸ¥èº«ä»½ã€‚ä¸ç°æœ‰æ–¹æ³•ä¸åŒï¼ŒAlterbuteé‡‡ç”¨äº†æ”¾å®½çš„è®­ç»ƒç›®æ ‡ï¼Œç»“åˆèº«ä»½å‚è€ƒå›¾åƒã€æè¿°ç›®æ ‡å†…åœ¨å±æ€§çš„æ–‡æœ¬æç¤ºä»¥åŠå®šä¹‰å¤–éƒ¨ä¸Šä¸‹æ–‡çš„èƒŒæ™¯å›¾åƒå’Œç‰©ä½“æ©ç ï¼Œä»è€Œå®ç°æ›´çµæ´»çš„å±æ€§å˜åŒ–ã€‚é€šè¿‡ä½¿ç”¨è§†è§‰å‘½åå®ä½“ï¼ˆVNEsï¼‰ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿè‡ªåŠ¨æå–ç»†ç²’åº¦çš„è§†è§‰èº«ä»½ç±»åˆ«ï¼Œç¡®ä¿åœ¨ç¼–è¾‘è¿‡ç¨‹ä¸­ä¿æŒèº«ä»½çš„åŒæ—¶å®ç°å†…åœ¨å±æ€§çš„å˜åŒ–ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.09881', 'title': 'Transition Matching Distillation for Fast Video Generation', 'url': 'https://huggingface.co/papers/2601.09881', 'abstract': 'Transition Matching Distillation enables efficient video generation by distilling diffusion models into few-step predictors using conditional flows and semantic representation decomposition.  \t\t\t\t\tAI-generated summary \t\t\t\t Large video diffusion and flow models have achieved remarkable success in high-quality video generation, but their use in real-time interactive applications remains limited due to their inefficient multi-step sampling process. In this work, we present Transition Matching Distillation (TMD), a novel framework for distilling video diffusion models into efficient few-step generators. The central idea of TMD is to match the multi-step denoising trajectory of a diffusion model with a few-step probability transition process, where each transition is modeled as a lightweight conditional flow. To enable efficient distillation, we decompose the original diffusion backbone into two components: (1) a main backbone, comprising the majority of early layers, that extracts semantic representations at each outer transition step; and (2) a flow head, consisting of the last few layers, that leverages these representations to perform multiple inner flow updates. Given a pretrained video diffusion model, we first introduce a flow head to the model, and adapt it into a conditional flow map. We then apply distribution matching distillation to the student model with flow head rollout in each transition step. Extensive experiments on distilling Wan2.1 1.3B and 14B text-to-video models demonstrate that TMD provides a flexible and strong trade-off between generation speed and visual quality. In particular, TMD outperforms existing distilled models under comparable inference costs in terms of visual fidelity and prompt adherence. Project page: https://research.nvidia.com/labs/genair/tmd', 'score': 20, 'issue_id': 610, 'pub_date': '2026-01-14', 'pub_date_card': {'ru': '14 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 14', 'zh': '1æœˆ14æ—¥'}, 'hash': 'f7b4d9c997525867', 'authors': ['Weili Nie', 'Julius Berner', 'Nanye Ma', 'Chao Liu', 'Saining Xie', 'Arash Vahdat'], 'affiliations': ['NVIDIA', 'NYU'], 'pdf_title_img': 'assets/pdf/title_img/2601.09881.jpg', 'data': {'categories': ['#video', '#architecture', '#diffusion', '#inference', '#optimization', '#open_source', '#training'], 'emoji': 'âš¡', 'ru': {'title': 'Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ² Ñ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸ĞµĞ¹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ Transition Matching Distillation (TMD) Ğ´Ğ»Ñ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ğ±Ñ‹ÑÑ‚Ñ€Ñ‹Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ñ‹ Ñ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ Ñ‡Ğ¸ÑĞ»Ğ¾Ğ¼ ÑˆĞ°Ğ³Ğ¾Ğ². ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ ÑˆĞ°Ğ³Ğ°Ğ¼Ğ¸ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°, Ğ³Ğ´Ğµ ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ»Ñ‘Ğ³ĞºĞ¸Ğ¼ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½ÑƒÑ ÑĞµÑ‚ÑŒ Ğ½Ğ° Ğ´Ğ²Ğµ Ñ‡Ğ°ÑÑ‚Ğ¸: Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ±Ğ°ĞºĞ±Ğ¾Ğ½ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ñƒ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ° Ğ´Ğ»Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ TMD Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ….'}, 'en': {'title': 'Efficient Video Generation with Transition Matching Distillation', 'desc': 'Transition Matching Distillation (TMD) is a new method that improves video generation by simplifying the process of using diffusion models. It does this by transforming complex multi-step sampling into a more efficient few-step prediction using conditional flows. The approach involves breaking down the diffusion model into two parts: a backbone for extracting important features and a flow head for refining these features. TMD has been shown to generate high-quality videos faster while maintaining visual fidelity and adherence to prompts, outperforming other models in similar conditions.'}, 'zh': {'title': 'é«˜æ•ˆè§†é¢‘ç”Ÿæˆçš„æ–°æ–¹æ³•ï¼šè¿‡æ¸¡åŒ¹é…è’¸é¦', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºè¿‡æ¸¡åŒ¹é…è’¸é¦ï¼ˆTMDï¼‰çš„æ–°æ¡†æ¶ï¼Œæ—¨åœ¨å°†è§†é¢‘æ‰©æ•£æ¨¡å‹è’¸é¦ä¸ºé«˜æ•ˆçš„å°‘æ­¥ç”Ÿæˆå™¨ã€‚TMDçš„æ ¸å¿ƒæ€æƒ³æ˜¯å°†æ‰©æ•£æ¨¡å‹çš„å¤šæ­¥å»å™ªè½¨è¿¹ä¸å°‘æ­¥æ¦‚ç‡è¿‡æ¸¡è¿‡ç¨‹è¿›è¡ŒåŒ¹é…ã€‚é€šè¿‡å°†åŸå§‹æ‰©æ•£æ¨¡å‹åˆ†è§£ä¸ºä¸»è¦éª¨å¹²å’Œæµå¤´ä¸¤ä¸ªéƒ¨åˆ†ï¼ŒTMDèƒ½å¤Ÿæœ‰æ•ˆæå–è¯­ä¹‰è¡¨ç¤ºå¹¶è¿›è¡Œå¤šæ¬¡å†…éƒ¨æµæ›´æ–°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTMDåœ¨ç”Ÿæˆé€Ÿåº¦å’Œè§†è§‰è´¨é‡ä¹‹é—´æä¾›äº†çµæ´»ä¸”å¼ºå¤§çš„å¹³è¡¡ï¼Œä¼˜äºç°æœ‰çš„è’¸é¦æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.10611', 'title': 'Molmo2: Open Weights and Data for Vision-Language Models with Video Understanding and Grounding', 'url': 'https://huggingface.co/papers/2601.10611', 'abstract': "Molmo2 is a new open-source video-language model family that achieves state-of-the-art performance through novel datasets and training methods, particularly excelling in video grounding tasks without relying on proprietary models.  \t\t\t\t\tAI-generated summary \t\t\t\t Today's strongest video-language models (VLMs) remain proprietary. The strongest open-weight models either rely on synthetic data from proprietary VLMs, effectively distilling from them, or do not disclose their training data or recipe. As a result, the open-source community lacks the foundations needed to improve on the state-of-the-art video (and image) language models. Crucially, many downstream applications require more than just high-level video understanding; they require grounding -- either by pointing or by tracking in pixels. Even proprietary models lack this capability. We present Molmo2, a new family of VLMs that are state-of-the-art among open-source models and demonstrate exceptional new capabilities in point-driven grounding in single image, multi-image, and video tasks. Our key contribution is a collection of 7 new video datasets and 2 multi-image datasets, including a dataset of highly detailed video captions for pre-training, a free-form video Q&A dataset for fine-tuning, a new object tracking dataset with complex queries, and an innovative new video pointing dataset, all collected without the use of closed VLMs. We also present a training recipe for this data utilizing an efficient packing and message-tree encoding scheme, and show bi-directional attention on vision tokens and a novel token-weight strategy improves performance. Our best-in-class 8B model outperforms others in the class of open weight and data models on short videos, counting, and captioning, and is competitive on long-videos. On video-grounding Molmo2 significantly outperforms existing open-weight models like Qwen3-VL (35.5 vs 29.6 accuracy on video counting) and surpasses proprietary models like Gemini 3 Pro on some tasks (38.4 vs 20.0 F1 on video pointing and 56.2 vs 41.1 J&F on video tracking).", 'score': 19, 'issue_id': 610, 'pub_date': '2026-01-15', 'pub_date_card': {'ru': '15 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 15', 'zh': '1æœˆ15æ—¥'}, 'hash': 'cc00c73dd80a271e', 'authors': ['Christopher Clark', 'Jieyu Zhang', 'Zixian Ma', 'Jae Sung Park', 'Mohammadreza Salehi', 'Rohun Tripathi', 'Sangho Lee', 'Zhongzheng Ren', 'Chris Dongjoo Kim', 'Yinuo Yang', 'Vincent Shao', 'Yue Yang', 'Weikai Huang', 'Ziqi Gao', 'Taira Anderson', 'Jianrui Zhang', 'Jitesh Jain', 'George Stoica', 'Winson Han', 'Ali Farhadi', 'Ranjay Krishna'], 'affiliations': ['Allen Institute for AI', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2601.10611.jpg', 'data': {'categories': ['#video', '#benchmark', '#synthetic', '#multimodal', '#open_source', '#training', '#dataset'], 'emoji': 'ğŸ¥', 'ru': {'title': 'ĞÑ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°ĞµĞ½Ğ¸ĞµĞ¼ Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°', 'desc': 'Molmo2 â€” ÑÑ‚Ğ¾ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ½Ğ¾Ğ²Ñ‹Ğ¼ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ğ¼ Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ 7 Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² Ğ¸ 2 Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ-Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ñ‹ Ğ´Ğ»Ñ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¸ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ²ĞºĞ»Ğ°Ğ´ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ â€” ÑÑ‚Ğ¾ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ° video grounding, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°, Ğ½Ğ¾ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ñ‹Ñ… ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚Ğ°Ñ… Ğ¸Ğ»Ğ¸ Ğ¸Ñ… Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ. 8B Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Molmo2 Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ´Ğ°Ğ¶Ğµ Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ñ€Ğ¾Ğ´Ğµ Gemini 3 Pro Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… video pointing Ğ¸ video tracking.'}, 'en': {'title': 'Molmo2: Redefining Open-Source Video-Language Models', 'desc': 'Molmo2 is a new family of open-source video-language models that achieves top performance in video grounding tasks. It introduces innovative datasets and training methods, allowing it to excel without relying on proprietary models. The model includes seven new video datasets and two multi-image datasets, enhancing its capabilities in tasks like object tracking and video Q&A. With a unique training approach, Molmo2 outperforms existing open-weight models and even competes with some proprietary models in various video tasks.'}, 'zh': {'title': 'Molmo2ï¼šå¼€æºè§†é¢‘è¯­è¨€æ¨¡å‹çš„æ–°çªç ´', 'desc': 'Molmo2æ˜¯ä¸€ç§æ–°çš„å¼€æºè§†é¢‘è¯­è¨€æ¨¡å‹å®¶æ—ï¼Œé€šè¿‡åˆ›æ–°çš„æ•°æ®é›†å’Œè®­ç»ƒæ–¹æ³•ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨è§†é¢‘å®šä½ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œè€Œä¸ä¾èµ–äºä¸“æœ‰æ¨¡å‹ã€‚è¯¥æ¨¡å‹æä¾›äº†7ä¸ªæ–°çš„è§†é¢‘æ•°æ®é›†å’Œ2ä¸ªå¤šå›¾åƒæ•°æ®é›†ï¼Œæ”¯æŒé«˜è´¨é‡çš„è§†é¢‘ç†è§£å’Œå®šä½èƒ½åŠ›ã€‚Molmo2åœ¨çŸ­è§†é¢‘è®¡æ•°å’Œå­—å¹•ç”Ÿæˆç­‰ä»»åŠ¡ä¸­è¶…è¶Šäº†å…¶ä»–å¼€æºæ¨¡å‹ï¼Œå¹¶åœ¨æŸäº›ä»»åŠ¡ä¸Šè¶…è¿‡äº†ä¸“æœ‰æ¨¡å‹ã€‚é€šè¿‡é«˜æ•ˆçš„è®­ç»ƒæ–¹æ¡ˆå’ŒåŒå‘æ³¨æ„åŠ›æœºåˆ¶ï¼ŒMolmo2å±•ç¤ºäº†åœ¨è§†é¢‘è¯­è¨€å¤„ç†é¢†åŸŸçš„å¼ºå¤§æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.10712', 'title': 'MatchTIR: Fine-Grained Supervision for Tool-Integrated Reasoning via Bipartite Matching', 'url': 'https://huggingface.co/papers/2601.10712', 'abstract': 'MatchTIR enhances LLM reasoning by introducing fine-grained credit assignment through bipartite matching and dual-level advantage estimation for tool-integrated tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Tool-Integrated Reasoning (TIR) empowers large language models (LLMs) to tackle complex tasks by interleaving reasoning steps with external tool interactions. However, existing reinforcement learning methods typically rely on outcome- or trajectory-level rewards, assigning uniform advantages to all steps within a trajectory. This coarse-grained credit assignment fails to distinguish effective tool calls from redundant or erroneous ones, particularly in long-horizon multi-turn scenarios. To address this, we propose MatchTIR, a framework that introduces fine-grained supervision via bipartite matching-based turn-level reward assignment and dual-level advantage estimation. Specifically, we formulate credit assignment as a bipartite matching problem between predicted and ground-truth traces, utilizing two assignment strategies to derive dense turn-level rewards. Furthermore, to balance local step precision with global task success, we introduce a dual-level advantage estimation scheme that integrates turn-level and trajectory-level signals, assigning distinct advantage values to individual interaction turns. Extensive experiments on three benchmarks demonstrate the superiority of MatchTIR. Notably, our 4B model surpasses the majority of 8B competitors, particularly in long-horizon and multi-turn tasks. Our codes are available at https://github.com/quchangle1/MatchTIR.', 'score': 18, 'issue_id': 610, 'pub_date': '2026-01-15', 'pub_date_card': {'ru': '15 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 15', 'zh': '1æœˆ15æ—¥'}, 'hash': '9a506aa7bea3ff82', 'authors': ['Changle Qu', 'Sunhao Dai', 'Hengyi Cai', 'Jun Xu', 'Shuaiqiang Wang', 'Dawei Yin'], 'affiliations': ['Baidu Inc.', 'Gaoling School of Artificial Intelligence, Renmin University of China'], 'pdf_title_img': 'assets/pdf/title_img/2601.10712.jpg', 'data': {'categories': ['#rl', '#agents', '#training', '#small_models'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Ğ¢Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ·Ğ°ÑĞ»ÑƒĞ³ Ñ‡ĞµÑ€ĞµĞ· Ğ±Ğ¸partite matching Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² LLM', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ MatchTIR Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğ¼Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ²ĞºĞ»Ğ°Ğ´ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ²Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğ¸ Ğ¼ĞµĞ»ĞºĞ¾Ğ·ĞµÑ€Ğ½Ğ¸ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ ĞºÑ€ĞµĞ´Ğ¸Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ±Ğ¸partite matching Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ÑĞ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑÑ…ĞµĞ¼Ñƒ Ğ´Ğ²ÑƒÑ…ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ² Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ‰Ğ¸Ğ¹ ÑƒÑĞ¿ĞµÑ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ°Ñ 4B Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ MatchTIR Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğ¾ ĞºĞ¾Ğ½ĞºÑƒÑ€Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ 8B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼Ğ¸, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ….'}, 'en': {'title': 'Refining LLM Reasoning with Fine-Grained Credit Assignment', 'desc': "MatchTIR is a novel framework designed to improve the reasoning capabilities of large language models (LLMs) when interacting with external tools. It addresses the limitations of traditional reinforcement learning methods that provide uniform rewards across all steps, which can obscure the effectiveness of specific tool interactions. By employing bipartite matching for fine-grained credit assignment, MatchTIR differentiates between successful and unsuccessful tool calls, enhancing the model's learning process. Additionally, the dual-level advantage estimation balances immediate step performance with overall task success, leading to improved outcomes in complex, multi-turn scenarios."}, 'zh': {'title': 'MatchTIRï¼šç»†ç²’åº¦ä¿¡ç”¨åˆ†é…æå‡LLMæ¨ç†èƒ½åŠ›', 'desc': 'MatchTIR æ˜¯ä¸€ç§å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†èƒ½åŠ›çš„æ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥ç»†ç²’åº¦çš„ä¿¡ç”¨åˆ†é…æ¥å¤„ç†å·¥å…·é›†æˆä»»åŠ¡ã€‚å®ƒé‡‡ç”¨äºŒåˆ†åŒ¹é…å’ŒåŒå±‚ä¼˜åŠ¿ä¼°è®¡ï¼Œè§£å†³äº†ç°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨é•¿æ—¶é—´å¤šè½®åœºæ™¯ä¸­æ— æ³•æœ‰æ•ˆåŒºåˆ†æœ‰æ•ˆå·¥å…·è°ƒç”¨ä¸å†—ä½™æˆ–é”™è¯¯è°ƒç”¨çš„é—®é¢˜ã€‚MatchTIR å°†ä¿¡ç”¨åˆ†é…è§†ä¸ºä¸€ä¸ªäºŒåˆ†åŒ¹é…é—®é¢˜ï¼Œåˆ©ç”¨é¢„æµ‹å’ŒçœŸå®è½¨è¿¹ä¹‹é—´çš„åŒ¹é…æ¥ç”Ÿæˆå¯†é›†çš„è½®æ¬¡çº§å¥–åŠ±ã€‚é€šè¿‡ç»“åˆå±€éƒ¨æ­¥éª¤ç²¾åº¦å’Œå…¨å±€ä»»åŠ¡æˆåŠŸï¼ŒMatchTIR æä¾›äº†æ›´ç²¾ç¡®çš„å¥–åŠ±åˆ†é…ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.10156', 'title': 'ToolSafe: Enhancing Tool Invocation Safety of LLM-based agents via Proactive Step-level Guardrail and Feedback', 'url': 'https://huggingface.co/papers/2601.10156', 'abstract': 'A guardrail model and reasoning framework are developed to detect and prevent unsafe tool invocations in LLM agents, improving both safety and task performance under adversarial conditions.  \t\t\t\t\tAI-generated summary \t\t\t\t While LLM-based agents can interact with environments via invoking external tools, their expanded capabilities also amplify security risks. Monitoring step-level tool invocation behaviors in real time and proactively intervening before unsafe execution is critical for agent deployment, yet remains under-explored. In this work, we first construct TS-Bench, a novel benchmark for step-level tool invocation safety detection in LLM agents. We then develop a guardrail model, TS-Guard, using multi-task reinforcement learning. The model proactively detects unsafe tool invocation actions before execution by reasoning over the interaction history. It assesses request harmfulness and action-attack correlations, producing interpretable and generalizable safety judgments and feedback. Furthermore, we introduce TS-Flow, a guardrail-feedback-driven reasoning framework for LLM agents, which reduces harmful tool invocations of ReAct-style agents by 65 percent on average and improves benign task completion by approximately 10 percent under prompt injection attacks.', 'score': 18, 'issue_id': 611, 'pub_date': '2026-01-15', 'pub_date_card': {'ru': '15 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 15', 'zh': '1æœˆ15æ—¥'}, 'hash': '23ba9d146be279ff', 'authors': ['Yutao Mou', 'Zhangchi Xue', 'Lijun Li', 'Peiyang Liu', 'Shikun Zhang', 'Wei Ye', 'Jing Shao'], 'affiliations': ['National Engineering Research Center for Software Engineering, Peking University', 'Shanghai Artificial Intelligence Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2601.10156.jpg', 'data': {'categories': ['#benchmark', '#alignment', '#rl', '#agents', '#security', '#reasoning', '#training'], 'emoji': 'ğŸ›¡ï¸', 'ru': {'title': 'Ğ—Ğ°Ñ‰Ğ¸Ñ‚Ğ° LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¾Ñ‚ Ğ½ĞµĞ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ñ… Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ¾Ğ² Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñ‹ TS-Guard Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ñ… Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ¾Ğ² Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ñ… Ğ¿ĞµÑ€ĞµĞ´ Ğ¸Ñ… Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸ĞµĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº TS-Bench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ¾Ğ² Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ² Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº TS-Flow, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ Ğ¾Ñ‚ guardrail-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°, Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸ Ğ¸ Ğ°Ñ‚Ğ°ĞºĞ°Ğ¼Ğ¸. ĞĞ° Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸ĞºĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ñ‹Ñ… Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ¾Ğ² Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° 65% Ğ² ÑÑ€ĞµĞ´Ğ½ĞµĞ¼ Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ½Ğ° 10% Ğ¿Ñ€Ğ¸ Ğ°Ñ‚Ğ°ĞºĞ°Ñ… Ñ‡ĞµÑ€ĞµĞ· prompt injection.'}, 'en': {'title': 'Enhancing Safety in LLM Agents with Proactive Guardrails', 'desc': "This paper presents a guardrail model and reasoning framework designed to enhance the safety of large language model (LLM) agents when they invoke external tools. The authors introduce TS-Bench, a benchmark for evaluating the safety of tool invocations at each step of the agent's actions. The TS-Guard model utilizes multi-task reinforcement learning to identify and prevent unsafe tool invocations by analyzing the agent's interaction history. Additionally, the TS-Flow framework significantly reduces harmful actions while improving the overall task performance of LLM agents under adversarial conditions."}, 'zh': {'title': 'æå‡LLMä»£ç†å®‰å…¨æ€§çš„æŠ¤æ æ¨¡å‹ä¸æ¨ç†æ¡†æ¶', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§ä¿æŠ¤æ¨¡å‹å’Œæ¨ç†æ¡†æ¶ï¼Œç”¨äºæ£€æµ‹å’Œé˜²æ­¢å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†ä¸­çš„ä¸å®‰å…¨å·¥å…·è°ƒç”¨ï¼Œä»è€Œåœ¨å¯¹æŠ—æ€§æ¡ä»¶ä¸‹æé«˜å®‰å…¨æ€§å’Œä»»åŠ¡æ€§èƒ½ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªæ–°çš„åŸºå‡†TS-Benchï¼Œç”¨äºé€æ­¥å·¥å…·è°ƒç”¨å®‰å…¨æ£€æµ‹ã€‚æ¥ç€ï¼Œå¼€å‘äº†TS-Guardæ¨¡å‹ï¼Œåˆ©ç”¨å¤šä»»åŠ¡å¼ºåŒ–å­¦ä¹ ï¼Œèƒ½å¤Ÿåœ¨æ‰§è¡Œå‰ä¸»åŠ¨æ£€æµ‹ä¸å®‰å…¨çš„å·¥å…·è°ƒç”¨è¡Œä¸ºã€‚æœ€åï¼Œä»‹ç»äº†TS-Flowæ¨ç†æ¡†æ¶ï¼Œæ˜¾è‘—å‡å°‘äº†ReActé£æ ¼ä»£ç†çš„æœ‰å®³å·¥å…·è°ƒç”¨ï¼Œå¹¶åœ¨æç¤ºæ³¨å…¥æ”»å‡»ä¸‹æé«˜äº†è‰¯æ€§ä»»åŠ¡çš„å®Œæˆç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.10657', 'title': 'PACEvolve: Enabling Long-Horizon Progress-Aware Consistent Evolution', 'url': 'https://huggingface.co/papers/2601.10657', 'abstract': "PACEvolve framework addresses key failure modes in LLM evolutionary search through hierarchical context management, momentum-based backtracking, and adaptive sampling policies for improved self-improvement and solution discovery.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) have emerged as powerful operators for evolutionary search, yet the design of efficient search scaffolds remains ad hoc. While promising, current LLM-in-the-loop systems lack a systematic approach to managing the evolutionary process. We identify three distinct failure modes: Context Pollution, where experiment history biases future candidate generation; Mode Collapse, where agents stagnate in local minima due to poor exploration-exploitation balance; and Weak Collaboration, where rigid crossover strategies fail to leverage parallel search trajectories effectively. We introduce Progress-Aware Consistent Evolution (PACEvolve), a framework designed to robustly govern the agent's context and search dynamics, to address these challenges. PACEvolve combines hierarchical context management (HCM) with pruning to address context pollution; momentum-based backtracking (MBB) to escape local minima; and a self-adaptive sampling policy that unifies backtracking and crossover for dynamic search coordination (CE), allowing agents to balance internal refinement with cross-trajectory collaboration. We demonstrate that PACEvolve provides a systematic path to consistent, long-horizon self-improvement, achieving state-of-the-art results on LLM-SR and KernelBench, while discovering solutions surpassing the record on Modded NanoGPT.", 'score': 16, 'issue_id': 622, 'pub_date': '2026-01-15', 'pub_date_card': {'ru': '15 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 15', 'zh': '1æœˆ15æ—¥'}, 'hash': '6e79fa4f513c6c15', 'authors': ['Minghao Yan', 'Bo Peng', 'Benjamin Coleman', 'Ziqi Chen', 'Zhouhang Xie', 'Zhankui He', 'Noveen Sachdeva', 'Isabella Ye', 'Weili Wang', 'Chi Wang', 'Ed H. Chi', 'Wang-Cheng Kang', 'Derek Zhiyuan Cheng', 'Beidou Wang'], 'affiliations': ['Google', 'University of California San Diego', 'University of Wisconsin-Madison'], 'pdf_title_img': 'assets/pdf/title_img/2601.10657.jpg', 'data': {'categories': ['#agents', '#training'], 'emoji': 'ğŸ§¬', 'ru': {'title': 'Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¼Ñƒ ÑĞ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ¸ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²ĞºÑƒ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° PACEvolve Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ñ‚Ñ€Ğ¸ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹: Ğ·Ğ°Ğ³Ñ€ÑĞ·Ğ½ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°, ĞºĞ¾Ğ»Ğ»Ğ°Ğ¿Ñ Ğ¼Ğ¾Ğ´ (Ğ·Ğ°ÑÑ‚Ñ€ĞµĞ²Ğ°Ğ½Ğ¸Ğµ Ğ² Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¸Ğ½Ğ¸Ğ¼ÑƒĞ¼Ğ°Ñ…) Ğ¸ ÑĞ»Ğ°Ğ±Ğ¾Ğµ ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ÑĞ¼Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ°. Ğ ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ñ Ğ¾Ğ±Ñ€ĞµĞ·ĞºĞ¾Ğ¹, Ğ¸Ğ¼Ğ¿ÑƒĞ»ÑŒÑĞ½Ñ‹Ğ¹ Ğ¾Ñ‚ĞºĞ°Ñ‚ Ğ´Ğ»Ñ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ° Ğ¸Ğ· Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¸Ğ½Ğ¸Ğ¼ÑƒĞ¼Ğ¾Ğ² Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºÑƒ ÑĞµĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ backtracking Ğ¸ ĞºÑ€Ğ¾ÑÑĞ¾Ğ²ĞµÑ€. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½ĞµĞ³Ğ¾ ÑĞ»Ğ¾Ğ²Ğ° Ğ² ÑĞ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Evolving LLMs: Overcoming Search Challenges with PACEvolve', 'desc': 'The PACEvolve framework enhances the evolutionary search process in Large Language Models (LLMs) by addressing common failure modes. It implements hierarchical context management to prevent context pollution, ensuring that past experiences do not negatively influence future candidate generations. Additionally, momentum-based backtracking helps agents escape local minima, improving exploration and exploitation balance. Finally, an adaptive sampling policy allows for better collaboration among agents, leading to more effective solution discovery and consistent self-improvement.'}, 'zh': {'title': 'PACEvolveï¼šè¿›åŒ–æœç´¢çš„æ–°çªç ´', 'desc': 'PACEvolveæ¡†æ¶é€šè¿‡åˆ†å±‚ä¸Šä¸‹æ–‡ç®¡ç†ã€åŸºäºåŠ¨é‡çš„å›æº¯å’Œè‡ªé€‚åº”é‡‡æ ·ç­–ç•¥ï¼Œè§£å†³äº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›åŒ–æœç´¢ä¸­çš„å…³é”®å¤±è´¥æ¨¡å¼ã€‚è¯¥æ¡†æ¶æœ‰æ•ˆç®¡ç†è¿›åŒ–è¿‡ç¨‹ï¼Œå…‹æœäº†ä¸Šä¸‹æ–‡æ±¡æŸ“ã€æ¨¡å¼å´©æºƒå’Œå¼±åä½œç­‰é—®é¢˜ã€‚é€šè¿‡ç»“åˆä¸Šä¸‹æ–‡ç®¡ç†å’Œä¿®å‰ªï¼ŒPACEvolveèƒ½å¤Ÿå‡å°‘å®éªŒå†å²å¯¹å€™é€‰ç”Ÿæˆçš„åè§ï¼Œå¹¶é€šè¿‡åŠ¨é‡å›æº¯å¸®åŠ©ä»£ç†é€ƒç¦»å±€éƒ¨æœ€ä¼˜ã€‚æœ€ç»ˆï¼ŒPACEvolveå®ç°äº†é•¿æœŸè‡ªæˆ‘æ”¹è¿›çš„ç³»ç»Ÿè·¯å¾„ï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†é¢†å…ˆçš„ç»“æœã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.10527', 'title': 'A Safety Report on GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5', 'url': 'https://huggingface.co/papers/2601.10527', 'abstract': 'Frontier language and vision models show varied safety performance across different evaluation criteria, highlighting the need for comprehensive, standardized safety assessments.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapid evolution of Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) has produced substantial gains in reasoning, perception, and generative capability across language and vision. However, whether these advances yield commensurate improvements in safety remains unclear, in part due to fragmented evaluation practices limited to single modalities or threat models. In this report, we present an integrated safety evaluation of 7 frontier models: GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5. We evaluate each model across language, vision-language, and image generation settings using a unified protocol that integrates benchmark evaluation, adversarial evaluation, multilingual evaluation, and compliance evaluation. Aggregating our evaluations into safety leaderboards and model safety profiles across multiple evaluation modes reveals a sharply heterogeneous safety landscape. While GPT-5.2 demonstrates consistently strong and balanced safety performance across evaluations, other models exhibit pronounced trade-offs among benchmark safety, adversarial alignment, multilingual generalization, and regulatory compliance. Both language and vision-language modalities show significant vulnerability under adversarial evaluation, with all models degrading substantially despite strong results on standard benchmarks. Text-to-image models achieve relatively stronger alignment in regulated visual risk categories, yet remain brittle under adversarial or semantically ambiguous prompts. Overall, these results show that safety in frontier models is inherently multidimensional--shaped by modality, language, and evaluation scheme, underscoring the need for standardized safety evaluations to accurately assess real-world risk and guide responsible model development and deployment.', 'score': 16, 'issue_id': 610, 'pub_date': '2026-01-15', 'pub_date_card': {'ru': '15 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 15', 'zh': '1æœˆ15æ—¥'}, 'hash': 'fc0d323ab183caeb', 'authors': ['Xingjun Ma', 'Yixu Wang', 'Hengyuan Xu', 'Yutao Wu', 'Yifan Ding', 'Yunhan Zhao', 'Zilong Wang', 'Jiabin Hua', 'Ming Wen', 'Jianan Liu', 'Ranjie Duan', 'Yifeng Gao', 'Yingshui Tan', 'Yunhao Chen', 'Hui Xue', 'Xin Wang', 'Wei Cheng', 'Jingjing Chen', 'Zuxuan Wu', 'Bo Li', 'Yu-Gang Jiang'], 'affiliations': ['Deakin University', 'Fudan University', 'Shanghai Innovation institute', 'UIUC'], 'pdf_title_img': 'assets/pdf/title_img/2601.10527.jpg', 'data': {'categories': ['#security', '#benchmark', '#ethics', '#multimodal', '#multilingual', '#survey'], 'emoji': 'ğŸ›¡ï¸', 'ru': {'title': 'Ğ‘ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¸Ñ… Ñ„Ñ€Ğ¾Ğ½Ñ‚Ğ°Ñ…: Ğ¿Ğ¾Ñ‡ĞµĞ¼Ñƒ Ğ½ÑƒĞ¶Ğ½Ñ‹ ĞµĞ´Ğ¸Ğ½Ñ‹Ğµ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ñ‹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ ÑĞµĞ¼Ğ¸ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ» Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğµ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸, ÑĞ¾ÑÑ‚ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ‚ĞµÑÑ‚Ñ‹, Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºÑƒ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑĞ¼. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ñ‹Ğ¹ Ğ»Ğ°Ğ½Ğ´ÑˆĞ°Ñ„Ñ‚ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸: Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑĞµÑ€ÑŒĞµĞ·Ğ½Ñ‹Ğµ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ ÑĞ¾ÑÑ‚ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ°Ñ‚Ğ°ĞºĞ°Ñ… Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑÑ‹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸ÑĞ¼Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€Ğ¸ÑĞºĞ¾Ğ² Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Safety in AI Models: A Multidimensional Challenge', 'desc': 'This paper evaluates the safety performance of advanced language and vision models, revealing that their safety varies significantly across different evaluation criteria. The authors assess seven leading models using a unified protocol that includes various evaluation methods, highlighting the inconsistent safety profiles among them. While some models, like GPT-5.2, perform well across multiple safety dimensions, others show weaknesses, particularly under adversarial conditions. The findings emphasize the complexity of safety in these models and the necessity for standardized assessments to better understand and mitigate real-world risks.'}, 'zh': {'title': 'å‰æ²¿æ¨¡å‹å®‰å…¨æ€§è¯„ä¼°çš„å¿…è¦æ€§', 'desc': 'å‰æ²¿è¯­è¨€å’Œè§†è§‰æ¨¡å‹åœ¨ä¸åŒè¯„ä¼°æ ‡å‡†ä¸‹çš„å®‰å…¨æ€§èƒ½å·®å¼‚æ˜¾è‘—ï¼Œæ˜¾ç¤ºå‡ºå…¨é¢ã€æ ‡å‡†åŒ–å®‰å…¨è¯„ä¼°çš„å¿…è¦æ€§ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„å¿«é€Ÿå‘å±•åœ¨æ¨ç†ã€æ„ŸçŸ¥å’Œç”Ÿæˆèƒ½åŠ›ä¸Šå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†è¿™äº›è¿›å±•æ˜¯å¦èƒ½å¸¦æ¥ç›¸åº”çš„å®‰å…¨æ€§æå‡ä»ä¸æ˜ç¡®ã€‚æˆ‘ä»¬å¯¹ä¸ƒä¸ªå‰æ²¿æ¨¡å‹è¿›è¡Œäº†ç»¼åˆå®‰å…¨è¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºå®‰å…¨æ€§è¡¨ç°å­˜åœ¨æ˜æ˜¾å·®å¼‚ï¼Œå°¤å…¶åœ¨å¯¹æŠ—æ€§è¯„ä¼°ä¸­ï¼Œæ‰€æœ‰æ¨¡å‹çš„è¡¨ç°éƒ½æ˜¾è‘—ä¸‹é™ã€‚æ•´ä½“è€Œè¨€ï¼Œè¿™äº›ç»“æœè¡¨æ˜ï¼Œå‰æ²¿æ¨¡å‹çš„å®‰å…¨æ€§æ˜¯å¤šç»´çš„ï¼Œå—åˆ°æ¨¡æ€ã€è¯­è¨€å’Œè¯„ä¼°æ–¹æ¡ˆçš„å½±å“ï¼Œå¼ºè°ƒäº†æ ‡å‡†åŒ–å®‰å…¨è¯„ä¼°çš„é‡è¦æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.10103', 'title': 'FlowAct-R1: Towards Interactive Humanoid Video Generation', 'url': 'https://huggingface.co/papers/2601.10103', 'abstract': 'FlowAct-R1 enables real-time interactive humanoid video generation with high-fidelity synthesis and low-latency responsiveness through MMDiT architecture and chunkwise diffusion forcing strategies.  \t\t\t\t\tAI-generated summary \t\t\t\t Interactive humanoid video generation aims to synthesize lifelike visual agents that can engage with humans through continuous and responsive video. Despite recent advances in video synthesis, existing methods often grapple with the trade-off between high-fidelity synthesis and real-time interaction requirements. In this paper, we propose FlowAct-R1, a framework specifically designed for real-time interactive humanoid video generation. Built upon a MMDiT architecture, FlowAct-R1 enables the streaming synthesis of video with arbitrary durations while maintaining low-latency responsiveness. We introduce a chunkwise diffusion forcing strategy, complemented by a novel self-forcing variant, to alleviate error accumulation and ensure long-term temporal consistency during continuous interaction. By leveraging efficient distillation and system-level optimizations, our framework achieves a stable 25fps at 480p resolution with a time-to-first-frame (TTFF) of only around 1.5 seconds. The proposed method provides holistic and fine-grained full-body control, enabling the agent to transition naturally between diverse behavioral states in interactive scenarios. Experimental results demonstrate that FlowAct-R1 achieves exceptional behavioral vividness and perceptual realism, while maintaining robust generalization across diverse character styles.', 'score': 16, 'issue_id': 610, 'pub_date': '2026-01-15', 'pub_date_card': {'ru': '15 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 15', 'zh': '1æœˆ15æ—¥'}, 'hash': 'c14351d1e6e66f86', 'authors': ['Lizhen Wang', 'Yongming Zhu', 'Zhipeng Ge', 'Youwei Zheng', 'Longhao Zhang', 'Tianshu Hu', 'Shiyang Qin', 'Mingshuang Luo', 'Jiaxu Zhang', 'Xin Chen', 'Yulong Wang', 'Zerong Zheng', 'Jianwen Jiang', 'Chao Liang', 'Weifeng Chen', 'Xing Wang', 'Yuan Zhang', 'Mingyuan Gao'], 'affiliations': ['Bytedance'], 'pdf_title_img': 'assets/pdf/title_img/2601.10103.jpg', 'data': {'categories': ['#multimodal', '#inference', '#video', '#architecture'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ¡Ğ¸Ğ½Ñ‚ĞµĞ· Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ³ÑƒĞ¼Ğ°Ğ½Ğ¾Ğ¸Ğ´Ğ°Ğ¼Ğ¸ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸', 'desc': 'FlowAct-R1 â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶Ğ°Ğ¼Ğ¸ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¾Ñ€Ğ° (MMDiT). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¿Ğ¾Ğ±Ğ»Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ ÑĞ¸Ğ» Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ (chunkwise diffusion forcing), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½ÑƒÑ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ¼ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¸. Ğ‘Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸ÑĞ¼ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹, Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… 25 ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ² ÑĞµĞºÑƒĞ½Ğ´Ñƒ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ 480p Ñ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¾ĞºĞ¾Ğ»Ğ¾ 1,5 ÑĞµĞºÑƒĞ½Ğ´ Ğ´Ğ¾ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾ ĞºĞ°Ğ´Ñ€Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ½Ğ°Ğ´ Ñ‚ĞµĞ»Ğ¾Ğ¼ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶Ğ° Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ğ»Ğ°Ğ²Ğ½Ğ¾ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸ÑĞ¼Ğ¸ Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ….'}, 'en': {'title': 'Real-Time Interactive Humanoid Video Generation Made Easy!', 'desc': 'FlowAct-R1 is a framework designed for generating interactive humanoid videos in real-time with high visual quality. It uses a MMDiT architecture to allow for video synthesis of any length while keeping the response time low. The framework incorporates a chunkwise diffusion forcing strategy to minimize errors and maintain consistency during long interactions. With optimizations, it can produce stable video at 25 frames per second and quickly start displaying the first frame, enabling lifelike interactions with users.'}, 'zh': {'title': 'å®æ—¶äº’åŠ¨ç±»äººè§†é¢‘ç”Ÿæˆçš„åˆ›æ–°è§£å†³æ–¹æ¡ˆ', 'desc': 'FlowAct-R1 æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºå®æ—¶äº’åŠ¨ç±»äººè§†é¢‘ç”Ÿæˆçš„æ¡†æ¶ï¼Œèƒ½å¤Ÿå®ç°é«˜ä¿çœŸåˆæˆå’Œä½å»¶è¿Ÿå“åº”ã€‚è¯¥æ¡†æ¶åŸºäº MMDiT æ¶æ„ï¼Œæ”¯æŒä»»æ„æ—¶é•¿çš„è§†é¢‘æµåˆæˆï¼ŒåŒæ—¶ä¿æŒä½å»¶è¿Ÿã€‚é€šè¿‡å¼•å…¥åˆ†å—æ‰©æ•£å¼ºåˆ¶ç­–ç•¥å’Œæ–°é¢–çš„è‡ªæˆ‘å¼ºåˆ¶å˜ä½“ï¼ŒFlowAct-R1 èƒ½æœ‰æ•ˆå‡å°‘é”™è¯¯ç´¯ç§¯ï¼Œç¡®ä¿åœ¨æŒç»­äº’åŠ¨ä¸­çš„é•¿æœŸæ—¶é—´ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šæ ·åŒ–è§’è‰²é£æ ¼ä¸­è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿå®ç°ç”ŸåŠ¨çš„è¡Œä¸ºè¡¨ç°å’Œæ„ŸçŸ¥çœŸå®æ„Ÿã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.10592', 'title': 'Action100M: A Large-scale Video Action Dataset', 'url': 'https://huggingface.co/papers/2601.10592', 'abstract': 'Action100M is a large-scale video action dataset constructed from internet instructional videos using automated pipelines with V-JEPA embeddings and GPT-based reasoning for structured annotations.  \t\t\t\t\tAI-generated summary \t\t\t\t Inferring physical actions from visual observations is a fundamental capability for advancing machine intelligence in the physical world. Achieving this requires large-scale, open-vocabulary video action datasets that span broad domains. We introduce Action100M, a large-scale dataset constructed from 1.2M Internet instructional videos (14.6 years of duration), yielding O(100 million) temporally localized segments with open-vocabulary action supervision and rich captions. Action100M is generated by a fully automated pipeline that (i) performs hierarchical temporal segmentation using V-JEPA 2 embeddings, (ii) produces multi-level frame and segment captions organized as a Tree-of-Captions, and (iii) aggregates evidence with a reasoning model (GPT-OSS-120B) under a multi-round Self-Refine procedure to output structured annotations (brief/detailed action, actor, brief/detailed caption). Training VL-JEPA on Action100M demonstrates consistent data-scaling improvements and strong zero-shot performance across diverse action recognition benchmarks, establishing Action100M as a new foundation for scalable research in video understanding and world modeling.', 'score': 14, 'issue_id': 610, 'pub_date': '2026-01-15', 'pub_date_card': {'ru': '15 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 15', 'zh': '1æœˆ15æ—¥'}, 'hash': 'f279a476fac31098', 'authors': ['Delong Chen', 'Tejaswi Kasarla', 'Yejin Bang', 'Mustafa Shukor', 'Willy Chung', 'Jade Yu', 'Allen Bolourchi', 'Theo Moutakanni', 'Pascale Fung'], 'affiliations': ['HKUST', 'Meta FAIR', 'Sorbonne UniversitÃ©', 'University of Amsterdam'], 'pdf_title_img': 'assets/pdf/title_img/2601.10592.jpg', 'data': {'categories': ['#multimodal', '#video', '#dataset'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ¡Ñ‚Ğ¾Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ´Ğ»Ñ Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Action100M â€” ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¸Ğ· 1.2 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ñ€Ğ¾Ğ»Ğ¸ĞºĞ¾Ğ² Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ğ°. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ¾ĞºĞ¾Ğ»Ğ¾ 100 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ÑĞ¼Ğ¸, Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ V-JEPA 2 ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ°Ğ¼ Ğ´Ğ»Ñ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ GPT-OSS-120B Ğ´Ğ»Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ¡Ñ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑƒÑ€Ğ¾Ğ²Ğ½Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸: ĞºÑ€Ğ°Ñ‚ĞºĞ¸Ğµ Ğ¸ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹, Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ± Ğ°ĞºÑ‚ĞµÑ€Ğµ Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞ¸, Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ² Ğ²Ğ¸Ğ´Ğµ Ğ´Ñ€ĞµĞ²Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ VL-JEPA Ğ½Ğ° ÑÑ‚Ğ¾Ğ¼ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ ÑĞ¸Ğ»ÑŒĞ½ÑƒÑ zero-shot Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Action100M ĞºĞ°Ğº Ğ¾ÑĞ½Ğ¾Ğ²Ñƒ Ğ´Ğ»Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾ĞºÑ€ÑƒĞ¶Ğ°ÑÑ‰ĞµĞ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ°.'}, 'en': {'title': 'Unlocking Action Recognition with Action100M', 'desc': 'Action100M is a comprehensive video action dataset created from 1.2 million instructional videos sourced from the internet, totaling 14.6 years of content. It features over 100 million segments that are temporally localized and annotated with open-vocabulary action labels and detailed captions. The dataset is generated through an automated process that utilizes V-JEPA embeddings for hierarchical segmentation and a GPT-based model for structured annotation. Training on Action100M has shown significant improvements in action recognition tasks, making it a valuable resource for advancing video understanding and machine learning research.'}, 'zh': {'title': 'Action100Mï¼šè§†é¢‘ç†è§£çš„æ–°åŸºç¡€', 'desc': 'Action100Mæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„è§†é¢‘åŠ¨ä½œæ•°æ®é›†ï¼Œæ¥æºäºäº’è”ç½‘çš„æ•™å­¦è§†é¢‘ï¼Œä½¿ç”¨è‡ªåŠ¨åŒ–æµç¨‹æ„å»ºï¼Œç»“åˆäº†V-JEPAåµŒå…¥å’ŒåŸºäºGPTçš„æ¨ç†è¿›è¡Œç»“æ„åŒ–æ³¨é‡Šã€‚è¯¥æ•°æ®é›†åŒ…å«120ä¸‡æ®µè§†é¢‘ï¼Œæ€»æ—¶é•¿è¾¾åˆ°14.6å¹´ï¼Œç”Ÿæˆäº†è¶…è¿‡1äº¿ä¸ªæ—¶é—´å®šä½çš„åŠ¨ä½œç‰‡æ®µï¼Œå¹¶æä¾›å¼€æ”¾è¯æ±‡çš„åŠ¨ä½œç›‘ç£å’Œä¸°å¯Œçš„æè¿°ã€‚æ•°æ®é›†çš„ç”Ÿæˆè¿‡ç¨‹åŒ…æ‹¬ä½¿ç”¨V-JEPA 2åµŒå…¥è¿›è¡Œåˆ†å±‚æ—¶é—´åˆ†å‰²ï¼Œåˆ›å»ºå¤šå±‚æ¬¡çš„å¸§å’Œç‰‡æ®µæ ‡é¢˜ï¼Œå¹¶é€šè¿‡å¤šè½®è‡ªæˆ‘ç²¾ç‚¼ç¨‹åºèšåˆè¯æ®ä»¥è¾“å‡ºç»“æ„åŒ–æ³¨é‡Šã€‚é€šè¿‡åœ¨Action100Mä¸Šè®­ç»ƒVL-JEPAï¼Œå±•ç¤ºäº†æ•°æ®è§„æ¨¡çš„æŒç»­æ”¹è¿›å’Œåœ¨å¤šæ ·åŒ–åŠ¨ä½œè¯†åˆ«åŸºå‡†ä¸Šçš„å¼ºå¤§é›¶æ ·æœ¬æ€§èƒ½ï¼Œç¡®ç«‹äº†Action100Mä½œä¸ºè§†é¢‘ç†è§£å’Œä¸–ç•Œå»ºæ¨¡ç ”ç©¶çš„æ–°åŸºç¡€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.10547', 'title': 'HeartMuLa: A Family of Open Sourced Music Foundation Models', 'url': 'https://huggingface.co/papers/2601.10547', 'abstract': 'A suite of open-source music foundation models is introduced, featuring components for audio-text alignment, lyric recognition, music coding, and large language model-based song generation with controllable attributes and scalable parameterization.  \t\t\t\t\tAI-generated summary \t\t\t\t We present a family of open-source Music Foundation Models designed to advance large-scale music understanding and generation across diverse tasks and modalities. Our framework consists of four major components: (1) HeartCLAP, an audio-text alignment model; (2) HeartTranscriptor, a robust lyric recognition model optimized for real-world music scenarios; and (3) HeartCodec, a low-frame-rate (12.5 Hz) yet high-fidelity music codec tokenizer that captures long-range musical structure while preserving fine-grained acoustic details and enabling efficient autoregressive modeling; (4) HeartMuLa, an LLM-based song generation model capable of synthesizing high-fidelity music under rich, user-controllable conditions (e.g., textual style descriptions, lyrics, and reference audio). In addition, it provides two specialized modes: (i) fine-grained musical attribute control, which allows users to specify the style of different song sections (e.g., intro, verse, chorus) using natural language prompts; and (ii) short, engaging music generation, which is suitable as background music for short videos. Lastly, HeartMuLa improves significantly when scaled to 7B parameters. For the first time, we show that a Suno-level, commercial-grade system can be reproduced using academic-scale data and GPU resources. We expect these foundation models to serve as strong baselines for future research and to facilitate practical applications in multimodal content production.', 'score': 14, 'issue_id': 617, 'pub_date': '2026-01-15', 'pub_date_card': {'ru': '15 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 15', 'zh': '1æœˆ15æ—¥'}, 'hash': '166a065df89b2195', 'authors': ['Dongchao Yang', 'Yuxin Xie', 'Yuguo Yin', 'Zheyu Wang', 'Xiaoyu Yi', 'Gongxi Zhu', 'Xiaolong Weng', 'Zihan Xiong', 'Yingzhe Ma', 'Dading Cong', 'Jingliang Liu', 'Zihang Huang', 'Jinghan Ru', 'Rongjie Huang', 'Haoran Wan', 'Peixu Wang', 'Kuoxi Yu', 'Helin Wang', 'Liming Liang', 'Xianwei Zhuang', 'Yuanyuan Wang', 'Haohan Guo', 'Junjie Cao', 'Zeqian Ju', 'Songxiang Liu', 'Yuewen Cao', 'Heming Weng', 'Yuexian Zou'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2601.10547.jpg', 'data': {'categories': ['#multimodal', '#open_source', '#audio', '#architecture', '#dataset'], 'emoji': 'ğŸµ', 'ru': {'title': 'ĞÑ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¼ÑƒĞ·Ñ‹ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ÑĞµĞ¼ÑŒÑ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¼ÑƒĞ·Ñ‹ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ°Ñ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°: Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° HeartCLAP, ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¿ĞµÑĞµĞ½ HeartTranscriptor, ĞºĞ¾Ğ´ĞµĞº HeartCodec Ğ´Ğ»Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸ Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ¾Ğ¹ ĞºĞ°Ğ´Ñ€Ğ¾Ğ², Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ HeartMuLa Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° HeartMuLa Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼ÑƒĞ·Ñ‹ĞºÑƒ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ Ğ³Ğ¸Ğ±ĞºĞ¸Ğ¼ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ ÑĞ·Ñ‹Ğº, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ¸Ğ»ĞµĞ¼ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ‡Ğ°ÑÑ‚ĞµĞ¹ Ğ¿ĞµÑĞ½Ğ¸ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ„Ğ¾Ğ½Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Suno Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ°ĞºĞ°Ğ´ĞµĞ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ². Ğ­Ñ‚Ğ¸ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ»ÑƒĞ¶Ğ°Ñ‚ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ¹ Ğ±Ğ°Ğ·Ğ¾Ğ¹ Ğ´Ğ»Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°.'}, 'en': {'title': 'Empowering Music Creation with Open-Source Foundation Models', 'desc': 'This paper introduces a set of open-source Music Foundation Models aimed at enhancing music understanding and generation. The framework includes four key components: an audio-text alignment model, a lyric recognition model, a music codec tokenizer for capturing musical structure, and a large language model for generating songs with user-defined attributes. The song generation model allows for detailed control over musical elements and is optimized for creating engaging background music. The research demonstrates that high-quality music generation systems can be developed using academic resources, paving the way for future advancements in multimodal content production.'}, 'zh': {'title': 'å¼€æºéŸ³ä¹åŸºç¡€æ¨¡å‹ï¼šæ¨åŠ¨éŸ³ä¹ç”Ÿæˆä¸ç†è§£çš„æœªæ¥', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€å¥—å¼€æºçš„éŸ³ä¹åŸºç¡€æ¨¡å‹ï¼Œæ—¨åœ¨æå‡å¤§è§„æ¨¡éŸ³ä¹ç†è§£å’Œç”Ÿæˆçš„èƒ½åŠ›ã€‚è¯¥æ¡†æ¶åŒ…å«å››ä¸ªä¸»è¦ç»„ä»¶ï¼šéŸ³é¢‘æ–‡æœ¬å¯¹é½æ¨¡å‹HeartCLAPã€é’ˆå¯¹ç°å®éŸ³ä¹åœºæ™¯ä¼˜åŒ–çš„æ­Œè¯è¯†åˆ«æ¨¡å‹HeartTranscriptorã€èƒ½å¤Ÿæ•æ‰é•¿è·ç¦»éŸ³ä¹ç»“æ„çš„é«˜ä¿çœŸéŸ³ä¹ç¼–ç å™¨HeartCodecï¼Œä»¥åŠåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„æ­Œæ›²ç”Ÿæˆæ¨¡å‹HeartMuLaã€‚HeartMuLaæ”¯æŒç”¨æˆ·é€šè¿‡è‡ªç„¶è¯­è¨€æç¤ºæ§åˆ¶ä¸åŒæ­Œæ›²éƒ¨åˆ†çš„é£æ ¼ï¼Œå¹¶èƒ½å¤Ÿç”Ÿæˆé€‚åˆçŸ­è§†é¢‘çš„èƒŒæ™¯éŸ³ä¹ã€‚è¯¥æ¨¡å‹åœ¨å‚æ•°è§„æ¨¡è¾¾åˆ°70äº¿æ—¶è¡¨ç°æ˜¾è‘—æå‡ï¼Œå±•ç¤ºäº†å­¦æœ¯æ•°æ®å’ŒGPUèµ„æºä¸‹å¯å¤ç°çš„å•†ä¸šçº§ç³»ç»Ÿã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.10131', 'title': 'M^4olGen: Multi-Agent, Multi-Stage Molecular Generation under Precise Multi-Property Constraints', 'url': 'https://huggingface.co/papers/2601.10131', 'abstract': 'A fragment-level, retrieval-augmented framework with multi-agent reasoning and GRPO-trained optimization enables precise molecular generation under multiple physicochemical constraints.  \t\t\t\t\tAI-generated summary \t\t\t\t Generating molecules that satisfy precise numeric constraints over multiple physicochemical properties is critical and challenging. Although large language models (LLMs) are expressive, they struggle with precise multi-objective control and numeric reasoning without external structure and feedback. We introduce M olGen, a fragment-level, retrieval-augmented, two-stage framework for molecule generation under multi-property constraints. Stage I : Prototype generation: a multi-agent reasoner performs retrieval-anchored, fragment-level edits to produce a candidate near the feasible region. Stage II : RL-based fine-grained optimization: a fragment-level optimizer trained with Group Relative Policy Optimization (GRPO) applies one- or multi-hop refinements to explicitly minimize the property errors toward our target while regulating edit complexity and deviation from the prototype. A large, automatically curated dataset with reasoning chains of fragment edits and measured property deltas underpins both stages, enabling deterministic, reproducible supervision and controllable multi-hop reasoning. Unlike prior work, our framework better reasons about molecules by leveraging fragments and supports controllable refinement toward numeric targets. Experiments on generation under two sets of property constraints (QED, LogP, Molecular Weight and HOMO, LUMO) show consistent gains in validity and precise satisfaction of multi-property targets, outperforming strong LLMs and graph-based algorithms.', 'score': 14, 'issue_id': 630, 'pub_date': '2026-01-15', 'pub_date_card': {'ru': '15 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 15', 'zh': '1æœˆ15æ—¥'}, 'hash': 'f128db07173eaa9d', 'authors': ['Yizhan Li', 'Florence Cloutier', 'Sifan Wu', 'Ali Parviz', 'Boris Knyazev', 'Yan Zhang', 'Glen Berseth', 'Bang Liu'], 'affiliations': ['Canada CIFAR AI Chair', 'DIRO & UniversitÃ© de MontrÃ©al', 'Institut Courtois', 'Mila Quebec AI Institute', 'Samsung AI Lab, Montreal'], 'pdf_title_img': 'assets/pdf/title_img/2601.10131.jpg', 'data': {'categories': ['#science', '#agents', '#training', '#rag', '#rl', '#reasoning', '#dataset', '#optimization'], 'emoji': 'ğŸ§ª', 'ru': {'title': 'Ğ¢Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ» Ñ‡ĞµÑ€ĞµĞ· Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¸ RL-Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° MolGen Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ» Ñ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ„Ğ¸Ğ·Ğ¸ĞºĞ¾-Ñ…Ğ¸Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ğ¼Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¼ ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ¼ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ. ĞĞ° Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ĞµĞ»ÑŒ Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ¼ Ğ¿Ğ¾ Ğ±Ğ°Ğ·Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾Ñ‚Ğ¸Ğ¿ Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ»Ñ‹, Ğ° Ğ½Ğ° Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼ GRPO, Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ñ†ĞµĞ»ĞµĞ²Ñ‹Ñ… ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ¿Ğ¸Ñ€Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ² Ğ¿Ñ€Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ¹ Ğ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ğ¼Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ½Ğ°Ğ´ LLM Ğ¸ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ°Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ» Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ½Ğ° ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°.'}, 'en': {'title': 'MolGen: Precision in Molecular Generation with Multi-Agent Reasoning', 'desc': 'This paper presents MolGen, a novel framework for generating molecules that meet specific physicochemical constraints. It operates in two stages: first, a multi-agent reasoner generates candidate molecules through fragment-level edits, and second, a reinforcement learning-based optimizer fine-tunes these candidates to minimize property errors. The framework utilizes a curated dataset that includes reasoning chains and property measurements, allowing for precise control and reproducibility in the generation process. The results demonstrate that MolGen outperforms existing methods, including large language models, in generating valid molecules that satisfy multiple property targets.'}, 'zh': {'title': 'ç²¾ç¡®åˆ†å­ç”Ÿæˆçš„æ–°æ¡†æ¶', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºMolGençš„æ¡†æ¶ï¼Œç”¨äºåœ¨å¤šç§ç‰©ç†åŒ–å­¦çº¦æŸä¸‹ç”Ÿæˆåˆ†å­ã€‚è¯¥æ¡†æ¶åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µé€šè¿‡å¤šæ™ºèƒ½ä½“æ¨ç†è¿›è¡ŒåŸå‹ç”Ÿæˆï¼Œè¿›è¡Œç‰‡æ®µçº§ç¼–è¾‘ä»¥æ¥è¿‘å¯è¡ŒåŒºåŸŸï¼›ç¬¬äºŒé˜¶æ®µåˆ™ä½¿ç”¨åŸºäºå¼ºåŒ–å­¦ä¹ çš„ä¼˜åŒ–æ–¹æ³•ï¼Œç²¾ç»†è°ƒæ•´ä»¥æœ€å°åŒ–å±æ€§è¯¯å·®ã€‚MolGenåˆ©ç”¨è‡ªåŠ¨æ•´ç†çš„å¤§å‹æ•°æ®é›†ï¼Œæ”¯æŒå¯æ§çš„å¤šæ­¥æ¨ç†å’Œç”Ÿæˆè¿‡ç¨‹ï¼Œæ˜¾è‘—æé«˜äº†ç”Ÿæˆåˆ†å­çš„æœ‰æ•ˆæ€§å’Œç²¾ç¡®æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨æ»¡è¶³å¤šå±æ€§ç›®æ ‡æ–¹é¢ä¼˜äºç°æœ‰çš„å¼ºå¤§è¯­è¨€æ¨¡å‹å’Œå›¾ç®—æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.10553', 'title': 'Inference-time Physics Alignment of Video Generative Models with Latent World Models', 'url': 'https://huggingface.co/papers/2601.10553', 'abstract': 'Latent world models enhance video generation physics plausibility through inference-time alignment and trajectory steering, achieving superior performance in challenging benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t State-of-the-art video generative models produce promising visual content yet often violate basic physics principles, limiting their utility. While some attribute this deficiency to insufficient physics understanding from pre-training, we find that the shortfall in physics plausibility also stems from suboptimal inference strategies. We therefore introduce WMReward and treat improving physics plausibility of video generation as an inference-time alignment problem. In particular, we leverage the strong physics prior of a latent world model (here, VJEPA-2) as a reward to search and steer multiple candidate denoising trajectories, enabling scaling test-time compute for better generation performance. Empirically, our approach substantially improves physics plausibility across image-conditioned, multiframe-conditioned, and text-conditioned generation settings, with validation from human preference study. Notably, in the ICCV 2025 Perception Test PhysicsIQ Challenge, we achieve a final score of 62.64%, winning first place and outperforming the previous state of the art by 7.42%. Our work demonstrates the viability of using latent world models to improve physics plausibility of video generation, beyond this specific instantiation or parameterization.', 'score': 8, 'issue_id': 610, 'pub_date': '2026-01-15', 'pub_date_card': {'ru': '15 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 15', 'zh': '1æœˆ15æ—¥'}, 'hash': 'e499c674410a0a40', 'authors': ['Jianhao Yuan', 'Xiaofeng Zhang', 'Felix Friedrich', 'Nicolas Beltran-Velez', 'Melissa Hall', 'Reyhane Askari-Hemmat', 'Xiaochuang Han', 'Nicolas Ballas', 'Michal Drozdzal', 'Adriana Romero-Soriano'], 'affiliations': ['Canada CIFAR AI Chair', 'Columbia University', 'FAIR, Meta Superintelligence Labs', 'McGill University', 'Mila - QuÃ©bec AI Institute', 'University of Oxford', 'UniversitÃ© de MontrÃ©al'], 'pdf_title_img': 'assets/pdf/title_img/2601.10553.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#video', '#inference'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ¤Ğ¸Ğ·Ğ¸ĞºĞ° Ğ² Ñ„Ğ¾ĞºÑƒÑĞµ: Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµÑ€ĞµĞ· ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¸Ñ€Ğ°', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ½Ğ°Ñ€ÑƒÑˆĞµĞ½Ğ¸Ñ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ² Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ‡ĞµÑ€ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ¸Ñ€Ğ° Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ WMReward â€” Ğ¼ĞµÑ‚Ğ¾Ğ´, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ prior Ğ¾Ñ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ VJEPA-2 Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ÑĞ¼Ğ¸ Ğ´ĞµĞ½Ğ¾Ğ¹Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ²Ğ¾ Ğ²ÑĞµÑ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸: Ğ¿Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼ ĞºĞ°Ğ´Ñ€Ğ°Ğ¼ Ğ¸ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑÑ‚Ğ° Ğ² ĞºĞ¾Ğ½ĞºÑƒÑ€ÑĞµ ICCV 2025 Perception Test PhysicsIQ Challenge Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° 7.42% Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰ĞµĞ³Ğ¾ best-of-class Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°.'}, 'en': {'title': 'Enhancing Video Generation with Physics-Plausible Latent Models', 'desc': "This paper presents a method to enhance the physics realism of AI-generated videos by using latent world models. The authors identify that the lack of adherence to physical laws in video generation is not only due to poor pre-training but also due to ineffective inference strategies. They introduce a new approach called WMReward, which aligns the generation process with physics principles during inference by steering candidate trajectories based on a latent world model's physics knowledge. Their method shows significant improvements in generating videos that are more physically plausible, achieving top results in a competitive benchmark."}, 'zh': {'title': 'åˆ©ç”¨æ½œåœ¨ä¸–ç•Œæ¨¡å‹æå‡è§†é¢‘ç”Ÿæˆçš„ç‰©ç†åˆç†æ€§', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œé€šè¿‡æ½œåœ¨ä¸–ç•Œæ¨¡å‹æ¥å¢å¼ºè§†é¢‘ç”Ÿæˆçš„ç‰©ç†åˆç†æ€§ã€‚æˆ‘ä»¬å‘ç°ï¼Œè§†é¢‘ç”Ÿæˆæ¨¡å‹åœ¨æ¨ç†æ—¶çš„ç­–ç•¥ä¸è¶³æ˜¯å¯¼è‡´ç‰©ç†ä¸åˆç†çš„ä¸»è¦åŸå› ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†WMRewardï¼Œå°†æé«˜è§†é¢‘ç”Ÿæˆçš„ç‰©ç†åˆç†æ€§è§†ä¸ºä¸€ä¸ªæ¨ç†æ—¶å¯¹é½é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªç”Ÿæˆè®¾ç½®ä¸­æ˜¾è‘—æ”¹å–„äº†ç‰©ç†åˆç†æ€§ï¼Œå¹¶åœ¨ICC 2025 Perception Test PhysicsIQ Challengeä¸­è·å¾—ç¬¬ä¸€åã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.09142', 'title': 'EvasionBench: Detecting Evasive Answers in Financial Q&A via Multi-Model Consensus and LLM-as-Judge', 'url': 'https://huggingface.co/papers/2601.09142', 'abstract': "EvasionBench introduces a large-scale benchmark for detecting evasive responses in earnings calls using a multi-model annotation framework that leverages disagreement between advanced language models to identify challenging examples, resulting in a highly accurate model with significantly reduced inference costs.  \t\t\t\t\tAI-generated summary \t\t\t\t Detecting evasive answers in earnings calls is critical for financial transparency, yet progress is hindered by the lack of large-scale benchmarks. We introduce EvasionBench, comprising 30,000 training samples and 1,000 human-annotated test samples (Cohen's Kappa 0.835) across three evasion levels. Our key contribution is a multi-model annotation framework leveraging a core insight: disagreement between frontier LLMs signals hard examples most valuable for training. We mine boundary cases where two strong annotators conflict, using a judge to resolve labels. This approach outperforms single-model distillation by 2.4 percent, with judge-resolved samples improving generalization despite higher training loss (0.421 vs 0.393) - evidence that disagreement mining acts as implicit regularization. Our trained model Eva-4B (4B parameters) achieves 81.3 percent accuracy, outperforming its base by 25 percentage points and approaching frontier LLM performance at a fraction of inference cost.", 'score': 8, 'issue_id': 610, 'pub_date': '2026-01-14', 'pub_date_card': {'ru': '14 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 14', 'zh': '1æœˆ14æ—¥'}, 'hash': '8e4e9d937a122b6f', 'authors': ['Shijian Ma', 'Yan Lin', 'Yi Yang'], 'affiliations': ['Hong Kong University of Science and Technology', 'University of Macau'], 'pdf_title_img': 'assets/pdf/title_img/2601.09142.jpg', 'data': {'categories': ['#benchmark', '#small_models', '#dataset'], 'emoji': 'ğŸ¤', 'ru': {'title': 'ĞšĞ¾Ğ³Ğ´Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğµ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ½Ñ‹ â€” ÑƒÑ‡Ğ¸Ğ¼ÑÑ Ğ½Ğ° ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ğµ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ EvasionBench â€” Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ ÑƒĞºĞ»Ğ¾Ğ½Ñ‡Ğ¸Ğ²Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ² Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚Ğ°Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ°Ğ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ÑÑ…ĞµĞ¼Ñ‹ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ³Ğ»Ğ°ÑĞ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ². ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ñ‹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ° Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑĞ»ÑƒÑ‡Ğ°Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ñ†ĞµĞ½Ğ½Ñ‹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ³Ğ»Ğ°ÑĞ¸Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ ĞºĞ°Ğº Ğ½ĞµÑĞ²Ğ½Ğ°Ñ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½ÑƒÑ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ½Ğ° 2.4 Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚Ğ°. ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Eva-4B Ñ 4 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ 81.3 Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚Ğ° Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²ÑƒÑ Ğ²ĞµÑ€ÑĞ¸Ñ Ğ½Ğ° 25 Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿ÑƒĞ½ĞºÑ‚Ğ° Ğ¿Ñ€Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ….'}, 'en': {'title': 'EvasionBench: Enhancing Evasive Response Detection in Earnings Calls', 'desc': "EvasionBench is a new benchmark designed to improve the detection of evasive responses in earnings calls, which is important for maintaining financial transparency. It includes a large dataset of 30,000 training samples and 1,000 human-annotated test samples, ensuring high-quality annotations with a Cohen's Kappa score of 0.835. The innovative multi-model annotation framework utilizes the disagreement between advanced language models to identify challenging examples, which enhances the training process. The resulting model, Eva-4B, achieves an impressive accuracy of 81.3%, significantly outperforming previous models while reducing inference costs."}, 'zh': {'title': 'EvasionBenchï¼šæå‡è´¢æŠ¥é€æ˜åº¦çš„å›é¿æ€§å›ç­”æ£€æµ‹åŸºå‡†', 'desc': 'EvasionBenchæ˜¯ä¸€ä¸ªç”¨äºæ£€æµ‹è´¢æŠ¥ç”µè¯ä¼šè®®ä¸­å›é¿æ€§å›ç­”çš„å¤§è§„æ¨¡åŸºå‡†æµ‹è¯•ã€‚å®ƒåŒ…å«30,000ä¸ªè®­ç»ƒæ ·æœ¬å’Œ1,000ä¸ªç»è¿‡äººå·¥æ ‡æ³¨çš„æµ‹è¯•æ ·æœ¬ï¼Œæ¶µç›–ä¸‰ç§å›é¿çº§åˆ«ã€‚è¯¥ç ”ç©¶çš„å…³é”®è´¡çŒ®æ˜¯åˆ©ç”¨å¤šæ¨¡å‹æ ‡æ³¨æ¡†æ¶ï¼Œé€šè¿‡å…ˆè¿›è¯­è¨€æ¨¡å‹ä¹‹é—´çš„åˆ†æ­§æ¥è¯†åˆ«éš¾ä¾‹ï¼Œä»è€Œæé«˜æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚æœ€ç»ˆï¼Œè®­ç»ƒå‡ºçš„Eva-4Bæ¨¡å‹åœ¨å‡†ç¡®ç‡ä¸Šè¾¾åˆ°äº†81.3%ï¼Œå¹¶ä¸”åœ¨æ¨ç†æˆæœ¬ä¸Šæ˜¾è‘—é™ä½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.08881', 'title': 'TAG-MoE: Task-Aware Gating for Unified Generative Mixture-of-Experts', 'url': 'https://huggingface.co/papers/2601.08881', 'abstract': "A novel framework injects semantic intent into Mixture-of-Experts routing for image generation and editing, resolving task interference through hierarchical task annotation and predictive alignment regularization.  \t\t\t\t\tAI-generated summary \t\t\t\t Unified image generation and editing models suffer from severe task interference in dense diffusion transformers architectures, where a shared parameter space must compromise between conflicting objectives (e.g., local editing v.s. subject-driven generation). While the sparse Mixture-of-Experts (MoE) paradigm is a promising solution, its gating networks remain task-agnostic, operating based on local features, unaware of global task intent. This task-agnostic nature prevents meaningful specialization and fails to resolve the underlying task interference. In this paper, we propose a novel framework to inject semantic intent into MoE routing. We introduce a Hierarchical Task Semantic Annotation scheme to create structured task descriptors (e.g., scope, type, preservation). We then design Predictive Alignment Regularization to align internal routing decisions with the task's high-level semantics. This regularization evolves the gating network from a task-agnostic executor to a dispatch center. Our model effectively mitigates task interference, outperforming dense baselines in fidelity and quality, and our analysis shows that experts naturally develop clear and semantically correlated specializations.", 'score': 8, 'issue_id': 613, 'pub_date': '2026-01-12', 'pub_date_card': {'ru': '12 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 12', 'zh': '1æœˆ12æ—¥'}, 'hash': '125d010e4ac1238d', 'authors': ['Yu Xu', 'Hongbin Yan', 'Juan Cao', 'Yiji Cheng', 'Tiankai Hang', 'Runze He', 'Zijin Yin', 'Shiyi Zhang', 'Yuxin Zhang', 'Jintao Li', 'Chunyu Wang', 'Qinglin Lu', 'Tong-Yee Lee', 'Fan Tang'], 'affiliations': ['National Cheng-Kung University', 'Tencent Hunyuan', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2601.08881.jpg', 'data': {'categories': ['#diffusion', '#architecture', '#optimization', '#cv', '#training'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ¡ĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ Mixture-of-Experts Ğ´Ğ»Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´ĞµÑĞºÑ€Ğ¸Ğ¿Ñ‚Ğ¾Ñ€Ñ‹, Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¸ Ñ‚Ğ¸Ğ¿ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞĞ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ·Ğ°ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞµÑ‚ÑŒ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ, ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ¾Ğ¹ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ¿Ñ€ĞµĞ²Ñ€Ğ°Ñ‰Ğ°Ñ ĞµÑ‘ Ğ¸Ğ· Ğ¿Ğ°ÑÑĞ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ° Ğ² Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ´Ğ¸ÑĞ¿ĞµÑ‚Ñ‡ĞµÑ€. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞ°ĞµÑ‚ ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Enhancing Image Generation with Semantic Intent in MoE Routing', 'desc': 'This paper presents a new framework that enhances Mixture-of-Experts (MoE) routing by incorporating semantic intent for image generation and editing tasks. It addresses the problem of task interference in dense diffusion transformers, where conflicting objectives hinder performance. The authors introduce a Hierarchical Task Semantic Annotation system to create structured descriptors that clarify task intent. Additionally, they implement Predictive Alignment Regularization to ensure that routing decisions align with high-level semantics, leading to improved specialization among experts and better overall image quality.'}, 'zh': {'title': 'æ³¨å…¥è¯­ä¹‰æ„å›¾ï¼Œæå‡å›¾åƒç”Ÿæˆä¸ç¼–è¾‘çš„æ•ˆæœ', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ¡†æ¶ï¼Œå°†è¯­ä¹‰æ„å›¾æ³¨å…¥åˆ°æ··åˆä¸“å®¶ï¼ˆMoEï¼‰è·¯ç”±ä¸­ï¼Œä»¥è§£å†³å›¾åƒç”Ÿæˆå’Œç¼–è¾‘ä¸­çš„ä»»åŠ¡å¹²æ‰°ã€‚é€šè¿‡åˆ†å±‚ä»»åŠ¡è¯­ä¹‰æ³¨é‡Šå’Œé¢„æµ‹å¯¹é½æ­£åˆ™åŒ–ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿåˆ›å»ºç»“æ„åŒ–çš„ä»»åŠ¡æè¿°ç¬¦ï¼Œä»è€Œæé«˜æ¨¡å‹çš„ä»»åŠ¡ä¸“æ³¨æ€§ã€‚æˆ‘ä»¬è®¾è®¡çš„é¢„æµ‹å¯¹é½æ­£åˆ™åŒ–ä½¿å¾—è·¯ç”±å†³ç­–ä¸ä»»åŠ¡çš„é«˜å±‚è¯­ä¹‰å¯¹é½ï¼Œè¿›è€Œä½¿å¾—é—¨æ§ç½‘ç»œä»ä»»åŠ¡æ— å…³çš„æ‰§è¡Œè€…è½¬å˜ä¸ºè°ƒåº¦ä¸­å¿ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆå’Œç¼–è¾‘çš„ä¿çœŸåº¦å’Œè´¨é‡ä¸Šä¼˜äºä¼ ç»Ÿçš„å¯†é›†åŸºçº¿ï¼Œä¸“å®¶ç½‘ç»œä¹Ÿè‡ªç„¶å‘å±•å‡ºæ¸…æ™°ä¸”è¯­ä¹‰ç›¸å…³çš„ä¸“ä¸šåŒ–ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.06431', 'title': 'LSRIF: Logic-Structured Reinforcement Learning for Instruction Following', 'url': 'https://huggingface.co/papers/2601.06431', 'abstract': 'A logic-structured training framework explicitly models instruction logic through constraint-aware reward mechanisms, improving instruction-following and reasoning capabilities in large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Instruction-following is critical for large language models, but real-world instructions often contain logical structures such as sequential dependencies and conditional branching. Existing methods typically construct datasets with parallel constraints and optimize average rewards, ignoring logical dependencies and yielding noisy signals. We propose a logic-structured training framework LSRIF that explicitly models instruction logic. We first construct a dataset LSRInstruct with constraint structures such as parallel, sequential, and conditional types, and then design structure-aware rewarding method LSRIF including average aggregation for parallel structures, failure-penalty propagation for sequential structures, and selective rewards for conditional branches. Experiments show LSRIF brings significant improvements in instruction-following (in-domain and out-of-domain) and general reasoning. Analysis reveals that learning with explicit logic structures brings parameter updates in attention layers and sharpens token-level attention to constraints and logical operators.', 'score': 7, 'issue_id': 613, 'pub_date': '2026-01-10', 'pub_date_card': {'ru': '10 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 10', 'zh': '1æœˆ10æ—¥'}, 'hash': 'c1d82e949b0b737f', 'authors': ['Qingyu Ren', 'Qianyu He', 'Jingwen Chang', 'Jie Zeng', 'Jiaqing Liang', 'Yanghua Xiao', 'Han Xia', 'Zeye Sun', 'Fei Yu'], 'affiliations': ['Ant Group', 'School of Data Science, Fudan University', 'Shanghai Key Laboratory of Data Science, College of Computer Science and Artificial Intelligence, Fudan University'], 'pdf_title_img': 'assets/pdf/title_img/2601.06431.jpg', 'data': {'categories': ['#reasoning', '#alignment'], 'emoji': 'ğŸ§©', 'ru': {'title': 'Ğ›Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼ Ğ² LLM', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ²Ğ½Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ dataset LSRInstruct Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚Ğ¸Ğ¿Ğ°Ğ¼Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€: Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸, Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¸ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ²ĞµÑ‚Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ LSRIF Ğ´Ğ»Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑ‡ĞµÑ‚Ğ° Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ñ‚Ğ¸Ğ¿Ğ° ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹: Ğ°Ğ³Ñ€ĞµĞ³Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹, Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ ÑˆÑ‚Ñ€Ğ°Ñ„Ğ¾Ğ² Ğ·Ğ° Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ² Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑÑ… Ğ¸ ÑĞµĞ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ´Ğ»Ñ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ²ĞµÑ‚Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼ Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ ĞºĞ°Ğº Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Enhancing Instruction-Following with Logic-Structured Training', 'desc': "This paper introduces a new training framework called LSRIF that enhances the ability of large language models to follow complex instructions by incorporating logical structures. It addresses the limitations of existing methods that overlook logical dependencies in instruction datasets, which can lead to ineffective learning signals. The authors create a specialized dataset, LSRInstruct, that includes various constraint types like parallel, sequential, and conditional structures. By implementing a structure-aware reward system, LSRIF significantly improves the models' performance in both instruction-following and reasoning tasks, demonstrating the benefits of explicitly modeling logic in training."}, 'zh': {'title': 'é€»è¾‘ç»“æ„åŒ–è®­ç»ƒï¼Œæå‡æŒ‡ä»¤éµå¾ªä¸æ¨ç†èƒ½åŠ›', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§é€»è¾‘ç»“æ„åŒ–è®­ç»ƒæ¡†æ¶LSRIFï¼Œæ—¨åœ¨é€šè¿‡çº¦æŸæ„ŸçŸ¥çš„å¥–åŠ±æœºåˆ¶æ¥æ”¹å–„å¤§å‹è¯­è¨€æ¨¡å‹çš„æŒ‡ä»¤éµå¾ªå’Œæ¨ç†èƒ½åŠ›ã€‚ä¼ ç»Ÿæ–¹æ³•é€šå¸¸å¿½è§†æŒ‡ä»¤ä¸­çš„é€»è¾‘ä¾èµ–å…³ç³»ï¼Œå¯¼è‡´è®­ç»ƒä¿¡å·å™ªå£°è¾ƒå¤§ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåŒ…å«å¹¶è¡Œã€é¡ºåºå’Œæ¡ä»¶ç±»å‹çº¦æŸç»“æ„çš„æ•°æ®é›†LSRInstructï¼Œå¹¶è®¾è®¡äº†ç»“æ„æ„ŸçŸ¥çš„å¥–åŠ±æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLSRIFåœ¨æŒ‡ä»¤éµå¾ªå’Œæ¨ç†èƒ½åŠ›ä¸Šå‡æœ‰æ˜¾è‘—æå‡ï¼Œä¸”æ˜¾å¼çš„é€»è¾‘ç»“æ„å­¦ä¹ èƒ½å¤Ÿä¼˜åŒ–æ³¨æ„åŠ›å±‚çš„å‚æ•°æ›´æ–°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.10201', 'title': "PRL: Process Reward Learning Improves LLMs' Reasoning Ability and Broadens the Reasoning Boundary", 'url': 'https://huggingface.co/papers/2601.10201', 'abstract': "Process Reward Learning decomposes reinforcement learning objectives into intermediate steps to provide fine-grained supervision for improving large language model reasoning abilities.  \t\t\t\t\tAI-generated summary \t\t\t\t Improving the reasoning abilities of Large Language Models (LLMs) has been a continuous topic recently. But most relevant works are based on outcome rewards at the trajectory level, missing fine-grained supervision during the reasoning process. Other existing training frameworks that try to combine process signals together to optimize LLMs also rely heavily on tedious additional steps like MCTS, training a separate reward model, etc., doing harm to the training efficiency. Moreover, the intuition behind the process signals design lacks rigorous theoretical support, leaving the understanding of the optimization mechanism opaque. In this paper, we propose Process Reward Learning (PRL), which decomposes the entropy regularized reinforcement learning objective into intermediate steps, with rigorous process rewards that could be assigned to models accordingly. Starting from theoretical motivation, we derive the formulation of PRL that is essentially equivalent to the objective of reward maximization plus a KL-divergence penalty term between the policy model and a reference model. However, PRL could turn the outcome reward into process supervision signals, which helps better guide the exploration during RL optimization. From our experiment results, we demonstrate that PRL not only improves the average performance for LLMs' reasoning ability measured by average @ n, but also broadens the reasoning boundary by improving the pass @ n metric. Extensive experiments show the effectiveness of PRL could be verified and generalized.", 'score': 6, 'issue_id': 611, 'pub_date': '2026-01-15', 'pub_date_card': {'ru': '15 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 15', 'zh': '1æœˆ15æ—¥'}, 'hash': 'ab07b6201b05fb06', 'authors': ['Jiarui Yao', 'Ruida Wang', 'Tong Zhang'], 'affiliations': ['University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2601.10201.jpg', 'data': {'categories': ['#optimization', '#rl', '#reasoning', '#training'], 'emoji': 'ğŸ¯', 'ru': {'title': 'ĞŸĞ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğµ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğµ: Ğ¾Ñ‚ Ğ¸Ñ‚Ğ¾Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ° Ğº Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ Process Reward Learning (PRL), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ ÑÑ‚Ğ°Ğ¿Ñ‹ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ PRL ÑĞºĞ²Ğ¸Ğ²Ğ°Ğ»ĞµĞ½Ñ‚ĞµĞ½ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ ÑˆÑ‚Ñ€Ğ°Ñ„Ğ¾Ğ¼ Ğ² Ğ²Ğ¸Ğ´Ğµ Ğ´Ğ¸Ğ²ĞµÑ€Ğ³ĞµĞ½Ñ†Ğ¸Ğ¸ ĞšÑƒĞ»ÑŒĞ±Ğ°ĞºĞ°-Ğ›ĞµĞ¹Ğ±Ğ»ĞµÑ€Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚ĞµĞºÑƒÑ‰ĞµĞ¹ Ğ¸ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¾Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ² ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ»ÑƒÑ‡ÑˆĞµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ğº ÑÑ€ĞµĞ´Ğ½ĞµĞ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ñ‚Ğ°Ğº Ğ¸ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ† ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Decomposing Rewards for Enhanced Reasoning in LLMs', 'desc': "Process Reward Learning (PRL) enhances the reasoning capabilities of Large Language Models (LLMs) by breaking down reinforcement learning objectives into smaller, manageable steps. This approach provides fine-grained supervision, addressing the limitations of traditional methods that rely solely on outcome rewards. By introducing rigorous process rewards, PRL optimizes the training efficiency without the need for complex additional steps like Monte Carlo Tree Search. Experimental results indicate that PRL not only boosts average performance in reasoning tasks but also expands the models' reasoning capabilities significantly."}, 'zh': {'title': 'è¿‡ç¨‹å¥–åŠ±å­¦ä¹ ï¼šæå‡è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œç§°ä¸ºè¿‡ç¨‹å¥–åŠ±å­¦ä¹ ï¼ˆPRLï¼‰ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚PRLé€šè¿‡å°†å¼ºåŒ–å­¦ä¹ ç›®æ ‡åˆ†è§£ä¸ºä¸­é—´æ­¥éª¤ï¼Œæä¾›äº†ç»†ç²’åº¦çš„ç›‘ç£ä¿¡å·ï¼Œä»è€Œæ”¹å–„äº†æ¨¡å‹çš„è®­ç»ƒæ•ˆç‡ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼ŒPRLèƒ½å¤Ÿå°†ç»“æœå¥–åŠ±è½¬åŒ–ä¸ºè¿‡ç¨‹ç›‘ç£ä¿¡å·ï¼Œæ›´å¥½åœ°æŒ‡å¯¼å¼ºåŒ–å­¦ä¹ çš„æ¢ç´¢è¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPRLä¸ä»…æé«˜äº†LLMsçš„å¹³å‡æ¨ç†æ€§èƒ½ï¼Œè¿˜æ‰©å±•äº†æ¨ç†çš„è¾¹ç•Œã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.10129', 'title': 'LaViT: Aligning Latent Visual Thoughts for Multi-modal Reasoning', 'url': 'https://huggingface.co/papers/2601.10129', 'abstract': "LaViT addresses the perception gap in multimodal reasoning by aligning latent visual thoughts through autoregressive reconstruction of visual semantics and attention trajectories, improving visual grounding and model performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Current multimodal latent reasoning often relies on external supervision (e.g., auxiliary images), ignoring intrinsic visual attention dynamics. In this work, we identify a critical Perception Gap in distillation: student models frequently mimic a teacher's textual output while attending to fundamentally divergent visual regions, effectively relying on language priors rather than grounded perception. To bridge this, we propose LaViT, a framework that aligns latent visual thoughts rather than static embeddings. LaViT compels the student to autoregressively reconstruct the teacher's visual semantics and attention trajectories prior to text generation, employing a curriculum sensory gating mechanism to prevent shortcut learning. Extensive experiments show that LaViT significantly enhances visual grounding, achieving up to +16.9% gains on complex reasoning tasks and enabling a compact 3B model to outperform larger open-source variants and proprietary models like GPT-4o.", 'score': 5, 'issue_id': 611, 'pub_date': '2026-01-15', 'pub_date_card': {'ru': '15 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 15', 'zh': '1æœˆ15æ—¥'}, 'hash': '116dda5e12789200', 'authors': ['Linquan Wu', 'Tianxiang Jiang', 'Yifei Dong', 'Haoyu Yang', 'Fengji Zhang', 'Shichaang Meng', 'Ai Xuan', 'Linqi Song', 'Jacky Keung'], 'affiliations': ['City University of Hong Kong', 'University of Electronic Science and Technology of China', 'University of Science and Technology of China', 'Utrecht University'], 'pdf_title_img': 'assets/pdf/title_img/2601.10129.jpg', 'data': {'categories': ['#multimodal', '#small_models', '#training'], 'emoji': 'ğŸ‘ï¸', 'ru': {'title': 'Ğ’Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ñ‹ÑĞ»ĞµĞ¹ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ²', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ LaViT, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹. Ğ¡Ñ‚ÑƒĞ´ĞµĞ½Ñ‚-Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºÑƒ Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ Ğ´Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ curriculum learning Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ÑÑ†ĞµĞ½Ñ‹, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚Ğ° Ğ´Ğ¾ 16.9% Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞšĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ°Ñ 3-Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸ Ğ¸ Ğ´Ğ°Ğ¶Ğµ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ²Ñ€Ğ¾Ğ´Ğµ GPT-4o.'}, 'en': {'title': 'Bridging the Perception Gap in Multimodal Reasoning', 'desc': "LaViT is a framework designed to improve multimodal reasoning by addressing the perception gap between visual and textual information. It focuses on aligning latent visual thoughts through autoregressive reconstruction, which helps the model better understand visual semantics and attention dynamics. By ensuring that student models learn to reconstruct the teacher's visual attention before generating text, LaViT reduces reliance on language priors and enhances visual grounding. The results show significant performance improvements on complex reasoning tasks, allowing smaller models to outperform larger ones."}, 'zh': {'title': 'LaViTï¼šç¼©å°å¤šæ¨¡æ€æ¨ç†çš„æ„ŸçŸ¥å·®è·', 'desc': 'LaViT æ˜¯ä¸€ç§æ–°æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤šæ¨¡æ€æ¨ç†ä¸­çš„æ„ŸçŸ¥å·®è·é—®é¢˜ã€‚å®ƒé€šè¿‡è‡ªå›å½’é‡å»ºè§†è§‰è¯­ä¹‰å’Œæ³¨æ„åŠ›è½¨è¿¹æ¥å¯¹é½æ½œåœ¨çš„è§†è§‰æ€ç»´ï¼Œä»è€Œæé«˜è§†è§‰åŸºç¡€å’Œæ¨¡å‹æ€§èƒ½ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒLaViT å¼ºè°ƒå­¦ç”Ÿæ¨¡å‹åœ¨ç”Ÿæˆæ–‡æœ¬ä¹‹å‰ï¼Œå¿…é¡»é‡å»ºæ•™å¸ˆæ¨¡å‹çš„è§†è§‰ä¿¡æ¯ï¼Œè€Œä¸æ˜¯ä»…ä»…ä¾èµ–è¯­è¨€å…ˆéªŒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLaViT åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šæ˜¾è‘—æå‡äº†æ€§èƒ½ï¼Œç”šè‡³ä½¿å¾—ä¸€ä¸ªè¾ƒå°çš„æ¨¡å‹è¶…è¶Šäº†æ›´å¤§çš„å¼€æºå’Œä¸“æœ‰æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.06378', 'title': 'RigMo: Unifying Rig and Motion Learning for Generative Animation', 'url': 'https://huggingface.co/papers/2601.06378', 'abstract': "RigMo is a unified generative framework that simultaneously learns rig and motion from mesh sequences, encoding deformations into compact latent spaces for interpretable and physically plausible 3D animation.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite significant progress in 4D generation, rig and motion, the core structural and dynamic components of animation are typically modeled as separate problems. Existing pipelines rely on ground-truth skeletons and skinning weights for motion generation and treat auto-rigging as an independent process, undermining scalability and interpretability. We present RigMo, a unified generative framework that jointly learns rig and motion directly from raw mesh sequences, without any human-provided rig annotations. RigMo encodes per-vertex deformations into two compact latent spaces: a rig latent that decodes into explicit Gaussian bones and skinning weights, and a motion latent that produces time-varying SE(3) transformations. Together, these outputs define an animatable mesh with explicit structure and coherent motion, enabling feed-forward rig and motion inference for deformable objects. Beyond unified rig-motion discovery, we introduce a Motion-DiT model operating in RigMo's latent space and demonstrate that these structure-aware latents can naturally support downstream motion generation tasks. Experiments on DeformingThings4D, Objaverse-XL, and TrueBones demonstrate that RigMo learns smooth, interpretable, and physically plausible rigs, while achieving superior reconstruction and category-level generalization compared to existing auto-rigging and deformation baselines. RigMo establishes a new paradigm for unified, structure-aware, and scalable dynamic 3D modeling.", 'score': 5, 'issue_id': 625, 'pub_date': '2026-01-10', 'pub_date_card': {'ru': '10 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 10', 'zh': '1æœˆ10æ—¥'}, 'hash': '1484cb1787cd920d', 'authors': ['Hao Zhang', 'Jiahao Luo', 'Bohui Wan', 'Yizhou Zhao', 'Zongrui Li', 'Michael Vasilkovsky', 'Chaoyang Wang', 'Jian Wang', 'Narendra Ahuja', 'Bing Zhou'], 'affiliations': ['Carnegie Mellon University', 'Nanyang Technological University', 'Snap Inc.', 'University of California, Santa Cruz', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2601.06378.jpg', 'data': {'categories': ['#architecture', '#3d', '#training'], 'emoji': 'ğŸ¦´', 'ru': {'title': 'Ğ•Ğ´Ğ¸Ğ½Ğ¾Ğµ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑĞºĞµĞ»ĞµÑ‚Ğ° Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸Ğ· Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ´ĞµÑ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ñ…ÑÑ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ²', 'desc': 'RigMo â€” ÑÑ‚Ğ¾ ĞµĞ´Ğ¸Ğ½Ñ‹Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ¸Ğ·ÑƒÑ‡Ğ°ĞµÑ‚ ÑĞºĞµĞ»ĞµÑ‚ Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ 3D Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ½ĞµĞ¿Ğ¾ÑÑ€ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ¸Ğ· Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼ĞµÑˆ-ÑĞµÑ‚Ğ¾Ğº Ğ±ĞµĞ· Ñ€ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒĞµÑ‚ Ğ´ĞµÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ²ĞµÑ€ÑˆĞ¸Ğ½ Ğ² Ğ´Ğ²Ğ° ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ñ… Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°: Ğ¾Ğ´Ğ½Ğ¾ Ğ´Ğ»Ñ ÑĞ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ĞºĞ¾ÑÑ‚ĞµĞ¹ Ğ¸ Ğ²ĞµÑĞ¾Ğ² ÑĞºĞ¸Ğ½Ğ½Ğ¸Ğ½Ğ³Ğ°, Ğ´Ñ€ÑƒĞ³Ğ¾Ğµ Ğ´Ğ»Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Motion-DiT Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‰ÑƒÑ Ğ² Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ RigMo Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº ÑƒÑ‡Ğ¸Ñ‚ÑÑ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ³Ğ»Ğ°Ğ´ĞºĞ¸Ğµ, Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ğ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ñ‹Ğµ ÑĞºĞµĞ»ĞµÑ‚Ñ‹ Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ĞµĞ¹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ¸Ğ³Ğ³Ğ¸Ğ½Ğ³Ğ°.'}, 'en': {'title': 'Unified Rig and Motion Learning for Realistic 3D Animation', 'desc': 'RigMo is a novel framework that integrates the learning of rigging and motion from mesh sequences into a single process, enhancing the efficiency of 3D animation. It eliminates the need for manual rig annotations by directly encoding deformations into compact latent spaces, which represent both the rig structure and motion dynamics. This approach allows for the generation of animatable meshes with coherent motion and explicit structure, making it easier to create realistic animations. RigMo also introduces a Motion-DiT model that leverages these learned latents for advanced motion generation tasks, demonstrating improved performance over traditional methods.'}, 'zh': {'title': 'RigMoï¼šç»Ÿä¸€çš„3DåŠ¨ç”»ç”Ÿæˆæ¡†æ¶', 'desc': 'RigMo æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„ç”Ÿæˆæ¡†æ¶ï¼Œèƒ½å¤ŸåŒæ—¶ä»ç½‘æ ¼åºåˆ—ä¸­å­¦ä¹ éª¨æ¶å’Œè¿åŠ¨ï¼Œå°†å˜å½¢ç¼–ç åˆ°ç´§å‡‘çš„æ½œåœ¨ç©ºé—´ä¸­ï¼Œä»¥å®ç°å¯è§£é‡Šå’Œç‰©ç†ä¸Šåˆç†çš„ 3D åŠ¨ç”»ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒRigMo ä¸ä¾èµ–äºäººå·¥æä¾›çš„éª¨æ¶æ³¨é‡Šï¼Œè€Œæ˜¯ç›´æ¥ä»åŸå§‹ç½‘æ ¼åºåˆ—ä¸­å­¦ä¹ ã€‚å®ƒå°†æ¯ä¸ªé¡¶ç‚¹çš„å˜å½¢ç¼–ç ä¸ºä¸¤ä¸ªæ½œåœ¨ç©ºé—´ï¼šä¸€ä¸ªç”¨äºè§£ç æˆæ˜¾å¼çš„é«˜æ–¯éª¨éª¼å’Œè’™çš®æƒé‡ï¼Œå¦ä¸€ä¸ªç”¨äºç”Ÿæˆæ—¶é—´å˜åŒ–çš„ SE(3) å˜æ¢ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒRigMo å®ç°äº†å¯åŠ¨ç”»ç½‘æ ¼çš„ç»“æ„å’Œè¿åŠ¨çš„ç»Ÿä¸€å‘ç°ï¼Œæ¨åŠ¨äº†åŠ¨æ€ 3D å»ºæ¨¡çš„æ–°èŒƒå¼ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.10080', 'title': 'Deriving Character Logic from Storyline as Codified Decision Trees', 'url': 'https://huggingface.co/papers/2601.10080', 'abstract': 'Executable and interpretable decision trees are induced from narrative data to create robust behavioral profiles for role-playing agents, outperforming traditional methods in consistency and reliability.  \t\t\t\t\tAI-generated summary \t\t\t\t Role-playing (RP) agents rely on behavioral profiles to act consistently across diverse narrative contexts, yet existing profiles are largely unstructured, non-executable, and weakly validated, leading to brittle agent behavior. We propose Codified Decision Trees (CDT), a data-driven framework that induces an executable and interpretable decision structure from large-scale narrative data. CDT represents behavioral profiles as a tree of conditional rules, where internal nodes correspond to validated scene conditions and leaves encode grounded behavioral statements, enabling deterministic retrieval of context-appropriate rules at execution time. The tree is learned by iteratively inducing candidate scene-action rules, validating them against data, and refining them through hierarchical specialization, yielding profiles that support transparent inspection and principled updates. Across multiple benchmarks, CDT substantially outperforms human-written profiles and prior profile induction methods on 85 characters across 16 artifacts, indicating that codified and validated behavioral representations lead to more reliable agent grounding.', 'score': 4, 'issue_id': 611, 'pub_date': '2026-01-15', 'pub_date_card': {'ru': '15 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 15', 'zh': '1æœˆ15æ—¥'}, 'hash': '4f2e5fe6f7a8b20a', 'authors': ['Letian Peng', 'Kun Zhou', 'Longfei Yun', 'Yupeng Hou', 'Jingbo Shang'], 'affiliations': ['University of California, San Diego'], 'pdf_title_img': 'assets/pdf/title_img/2601.10080.jpg', 'data': {'categories': ['#agents', '#benchmark', '#dataset'], 'emoji': 'ğŸ­', 'ru': {'title': 'Ğ¡Ñ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´ĞµÑ€ĞµĞ²ÑŒÑ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ñ… Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ñ€Ğ¾Ğ»ĞµĞ²Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Codified Decision Trees (CDT) Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»ĞµĞ¹ Ñ€Ğ¾Ğ»ĞµĞ²Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ½Ğ°Ñ€Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸Ğ½Ğ´ÑƒÑ†Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼ÑƒÑ Ğ´Ñ€ĞµĞ²Ğ¾Ğ²Ğ¸Ğ´Ğ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ¸Ğ· Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¾Ğ±ÑŠĞµĞ¼Ğ¾Ğ² Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ³Ğ´Ğµ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğµ ÑƒĞ·Ğ»Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ñ ÑÑ†ĞµĞ½, Ğ° Ğ»Ğ¸ÑÑ‚ÑŒÑ ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ. ĞĞ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ° Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹, Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ Ğ¸Ñ… Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… CDT ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ĞºĞ°Ğº Ñ€ÑƒÑ‡Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»Ğ¸, Ñ‚Ğ°Ğº Ğ¸ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¸Ğ½Ğ´ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»ĞµĞ¹, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğµ Ğ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ğ¼Ğ¾Ğµ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Building Reliable Agents with Codified Decision Trees', 'desc': 'This paper introduces Codified Decision Trees (CDT), a novel framework for creating executable and interpretable decision trees from narrative data. These decision trees help role-playing agents develop robust behavioral profiles that are consistent and reliable across various contexts. By using a data-driven approach, CDT structures behavioral profiles as trees of conditional rules, allowing for clear and deterministic decision-making during execution. The results show that CDT significantly outperforms traditional methods and human-written profiles, leading to better agent performance in diverse scenarios.'}, 'zh': {'title': 'ç¼–ç å†³ç­–æ ‘ï¼šæå‡è§’è‰²æ‰®æ¼”ä»£ç†çš„è¡Œä¸ºä¸€è‡´æ€§ä¸å¯é æ€§', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºç¼–ç å†³ç­–æ ‘ï¼ˆCDTï¼‰çš„æ¡†æ¶ï¼Œç”¨äºä»å™äº‹æ•°æ®ä¸­ç”Ÿæˆå¯æ‰§è¡Œå’Œå¯è§£é‡Šçš„å†³ç­–æ ‘ï¼Œä»¥åˆ›å»ºç¨³å¥çš„è§’è‰²æ‰®æ¼”ä»£ç†çš„è¡Œä¸ºæ¡£æ¡ˆã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼ŒCDTèƒ½å¤Ÿæä¾›æ›´ä¸€è‡´å’Œå¯é çš„è¡Œä¸ºè¡¨ç°ï¼Œè§£å†³äº†ç°æœ‰æ¡£æ¡ˆç»“æ„ä¸æ¸…æ™°ã€ä¸å¯æ‰§è¡Œå’ŒéªŒè¯ä¸è¶³çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡è¿­ä»£ç”Ÿæˆå€™é€‰åœºæ™¯-åŠ¨ä½œè§„åˆ™ï¼Œå¹¶å¯¹å…¶è¿›è¡Œæ•°æ®éªŒè¯å’Œå±‚æ¬¡åŒ–ç»†åŒ–ï¼Œå½¢æˆé€æ˜ä¸”å¯æ›´æ–°çš„è¡Œä¸ºæ¡£æ¡ˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCDTåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºäººå·¥ç¼–å†™çš„æ¡£æ¡ˆå’Œä¹‹å‰çš„æ¡£æ¡ˆç”Ÿæˆæ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.09876', 'title': 'Patient-Similarity Cohort Reasoning in Clinical Text-to-SQL', 'url': 'https://huggingface.co/papers/2601.09876', 'abstract': 'CLINSQL benchmark evaluates text-to-SQL models on complex clinical tasks requiring multi-table joins, temporal reasoning, and patient similarity analysis from real-world EHR data.  \t\t\t\t\tAI-generated summary \t\t\t\t Real-world clinical text-to-SQL requires reasoning over heterogeneous EHR tables, temporal windows, and patient-similarity cohorts to produce executable queries. We introduce CLINSQL, a benchmark of 633 expert-annotated tasks on MIMIC-IV v3.1 that demands multi-table joins, clinically meaningful filters, and executable SQL. Solving CLINSQL entails navigating schema metadata and clinical coding systems, handling long contexts, and composing multi-step queries beyond traditional text-to-SQL. We evaluate 22 proprietary and open-source models under Chain-of-Thought self-refinement and use rubric-based SQL analysis with execution checks that prioritize critical clinical requirements. Despite recent advances, performance remains far from clinical reliability: on the test set, GPT-5-mini attains 74.7% execution score, DeepSeek-R1 leads open-source at 69.2% and Gemini-2.5-Pro drops from 85.5% on Easy to 67.2% on Hard. Progress on CLINSQL marks tangible advances toward clinically reliable text-to-SQL for real-world EHR analytics.', 'score': 4, 'issue_id': 622, 'pub_date': '2026-01-14', 'pub_date_card': {'ru': '14 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 14', 'zh': '1æœˆ14æ—¥'}, 'hash': 'd596b652ea9e4e0c', 'authors': ['Yifei Shen', 'Yilun Zhao', 'Justice Ou', 'Tinglin Huang', 'Arman Cohan'], 'affiliations': ['Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2601.09876.jpg', 'data': {'categories': ['#science', '#reasoning', '#plp', '#open_source', '#benchmark', '#dataset', '#healthcare', '#long_context'], 'emoji': 'ğŸ¥', 'ru': {'title': 'ĞœĞ¾ÑÑ‚ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ·Ñ‹ĞºĞ¾Ğ¼ Ğ²Ñ€Ğ°Ñ‡Ğ° Ğ¸ SQL: Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…', 'desc': 'CLINSQL â€” ÑÑ‚Ğ¾ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° Ğ² SQL Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ñ… ÑĞ¾ĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†, Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸Ñ… Ğ¿Ğ°Ñ†Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ»ĞµĞºÑ‚Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 633 Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ğ¼Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‚ Ğ¾Ñ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ³ĞµÑ‚ĞµÑ€Ğ¾Ğ³ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ğ°Ğ¼Ğ¸, ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ĞºĞ¾Ğ´Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸ĞµĞ¹ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼Ğ¾Ğ³Ğ¾ SQL ĞºĞ¾Ğ´Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ»Ğ¸ 22 Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ²Ğ°Ğ¹Ğ´ĞµÑ€Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ ÑĞ°Ğ¼Ğ¾ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºÑƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºÑƒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° SQL Ñ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ½ĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ½ĞµĞ´Ğ°Ğ²Ğ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑ Ğ² Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ğ¸ LLM, Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¾ÑÑ‚Ğ°Ñ‘Ñ‚ÑÑ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ´Ğ»Ñ ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ: Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ»Ğ¸ÑˆÑŒ 70-75% Ğ½Ğ° Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ñ… Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ÑÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ….'}, 'en': {'title': 'Advancing Text-to-SQL for Clinical EHR Analytics', 'desc': 'The CLINSQL benchmark is designed to assess text-to-SQL models specifically for complex clinical tasks that involve multi-table joins and temporal reasoning using real-world Electronic Health Record (EHR) data. It consists of 633 expert-annotated tasks that require models to generate executable SQL queries while navigating intricate schema metadata and clinical coding systems. The evaluation of various models, including proprietary and open-source options, highlights the challenges in achieving reliable performance, with scores indicating that current models still fall short of clinical standards. This benchmark represents a significant step towards improving the accuracy and reliability of text-to-SQL systems in the healthcare domain.'}, 'zh': {'title': 'CLINSQLï¼šè¿ˆå‘ä¸´åºŠå¯é çš„æ–‡æœ¬åˆ°SQL', 'desc': 'CLINSQLåŸºå‡†æµ‹è¯•è¯„ä¼°æ–‡æœ¬åˆ°SQLæ¨¡å‹åœ¨å¤æ‚ä¸´åºŠä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œè¿™äº›ä»»åŠ¡éœ€è¦å¤šè¡¨è¿æ¥ã€æ—¶é—´æ¨ç†å’Œæ‚£è€…ç›¸ä¼¼æ€§åˆ†æã€‚è¯¥åŸºå‡†åŒ…å«633ä¸ªä¸“å®¶æ³¨é‡Šçš„ä»»åŠ¡ï¼Œè¦æ±‚ç”Ÿæˆå¯æ‰§è¡Œçš„SQLæŸ¥è¯¢ï¼Œå¹¶å¤„ç†å¼‚æ„ç”µå­å¥åº·è®°å½•ï¼ˆEHRï¼‰è¡¨ã€‚è§£å†³CLINSQLé—®é¢˜éœ€è¦ç†è§£æ¨¡å¼å…ƒæ•°æ®å’Œä¸´åºŠç¼–ç ç³»ç»Ÿï¼Œå¹¶èƒ½å¤Ÿå¤„ç†é•¿ä¸Šä¸‹æ–‡å’Œå¤šæ­¥éª¤æŸ¥è¯¢ã€‚å°½ç®¡å–å¾—äº†ä¸€äº›è¿›å±•ï¼Œä½†ç›®å‰çš„æ€§èƒ½ä»æœªè¾¾åˆ°ä¸´åºŠå¯é æ€§ï¼Œè¡¨æ˜åœ¨çœŸå®ä¸–ç•ŒEHRåˆ†æä¸­ï¼Œæ–‡æœ¬åˆ°SQLçš„åº”ç”¨ä»éœ€è¿›ä¸€æ­¥æ”¹è¿›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.09499', 'title': 'V-DPM: 4D Video Reconstruction with Dynamic Point Maps', 'url': 'https://huggingface.co/papers/2601.09499', 'abstract': 'Dynamic Point Maps extended to video input through V-DPM framework achieve state-of-the-art 3D and 4D reconstruction by recovering both dynamic depth and full 3D motion of scene points.  \t\t\t\t\tAI-generated summary \t\t\t\t Powerful 3D representations such as DUSt3R invariant point maps, which encode 3D shape and camera parameters, have significantly advanced feed forward 3D reconstruction. While point maps assume static scenes, Dynamic Point Maps (DPMs) extend this concept to dynamic 3D content by additionally representing scene motion. However, existing DPMs are limited to image pairs and, like DUSt3R, require post processing via optimization when more than two views are involved. We argue that DPMs are more useful when applied to videos and introduce V-DPM to demonstrate this. First, we show how to formulate DPMs for video input in a way that maximizes representational power, facilitates neural prediction, and enables reuse of pretrained models. Second, we implement these ideas on top of VGGT, a recent and powerful 3D reconstructor. Although VGGT was trained on static scenes, we show that a modest amount of synthetic data is sufficient to adapt it into an effective V-DPM predictor. Our approach achieves state of the art performance in 3D and 4D reconstruction for dynamic scenes. In particular, unlike recent dynamic extensions of VGGT such as P3, DPMs recover not only dynamic depth but also the full 3D motion of every point in the scene.', 'score': 4, 'issue_id': 622, 'pub_date': '2026-01-14', 'pub_date_card': {'ru': '14 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 14', 'zh': '1æœˆ14æ—¥'}, 'hash': 'ed1fb1979645a09c', 'authors': ['Edgar Sucar', 'Eldar Insafutdinov', 'Zihang Lai', 'Andrea Vedaldi'], 'affiliations': ['Visual Geometry Group (VGG), University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2601.09499.jpg', 'data': {'categories': ['#video', '#synthetic', '#3d', '#transfer_learning', '#training'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ’Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· ĞºĞ°Ñ€Ñ‚Ñ‹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚Ğ¾Ñ‡ĞµĞº Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° V-DPM â€” Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¸ Dynamic Point Maps Ğ½Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ†ĞµĞ½. ĞœĞµÑ‚Ğ¾Ğ´ ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ°Ğº Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½ÑƒÑ Ñ„Ğ¾Ñ€Ğ¼Ñƒ ÑÑ†ĞµĞ½Ñ‹ Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ ĞºĞ°Ğ¼ĞµÑ€Ñ‹, Ñ‚Ğ°Ğº Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğµ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ğ¾Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ VGGT Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ ĞºĞ°Ñ€Ñ‚Ñ‹ Ñ‚Ğ¾Ñ‡ĞµĞº Ğ±ĞµĞ· Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²ĞµĞ¹ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… 3D Ğ¸ 4D Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ†ĞµĞ½, Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°Ñ ĞºĞ°Ğº Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñƒ, Ñ‚Ğ°Ğº Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğµ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ğ¾Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡ĞµĞº.'}, 'en': {'title': 'Revolutionizing 3D Reconstruction with Video-Driven Dynamic Point Maps', 'desc': 'This paper presents the V-DPM framework, which enhances Dynamic Point Maps (DPMs) to work with video inputs for improved 3D and 4D scene reconstruction. By leveraging video data, V-DPM captures both dynamic depth and the full 3D motion of scene points, surpassing the limitations of traditional DPMs that only handle static images. The authors demonstrate that a small amount of synthetic data can effectively adapt existing 3D reconstruction models, like VGGT, to function as V-DPM predictors. This approach achieves state-of-the-art results in reconstructing dynamic scenes, providing a more comprehensive understanding of motion and depth.'}, 'zh': {'title': 'åŠ¨æ€ç‚¹å›¾ï¼šè§†é¢‘è¾“å…¥ä¸‹çš„3Dé‡å»ºæ–°çªç ´', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„åŠ¨æ€ç‚¹å›¾ï¼ˆDPMï¼‰æ¡†æ¶ï¼Œç§°ä¸ºV-DPMï¼Œæ—¨åœ¨å¤„ç†è§†é¢‘è¾“å…¥ä»¥å®ç°æ›´é«˜æ•ˆçš„3Då’Œ4Dé‡å»ºã€‚ä¸ä¼ ç»Ÿçš„é™æ€ç‚¹å›¾ä¸åŒï¼ŒåŠ¨æ€ç‚¹å›¾èƒ½å¤Ÿè¡¨ç¤ºåœºæ™¯çš„è¿åŠ¨ï¼Œä»è€Œæ›´å¥½åœ°å¤„ç†åŠ¨æ€å†…å®¹ã€‚æˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•å°†DPMåº”ç”¨äºè§†é¢‘è¾“å…¥ï¼Œå¹¶é€šè¿‡æœ€å¤§åŒ–è¡¨ç¤ºèƒ½åŠ›å’Œä¿ƒè¿›ç¥ç»é¢„æµ‹æ¥æé«˜æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒV-DPMåœ¨åŠ¨æ€åœºæ™¯çš„3Då’Œ4Dé‡å»ºä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œèƒ½å¤Ÿæ¢å¤æ¯ä¸ªåœºæ™¯ç‚¹çš„åŠ¨æ€æ·±åº¦å’Œå®Œæ•´çš„3Dè¿åŠ¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.10338', 'title': 'Agent Skills in the Wild: An Empirical Study of Security Vulnerabilities at Scale', 'url': 'https://huggingface.co/papers/2601.10338', 'abstract': 'Large-scale security analysis of AI agent skills reveals widespread vulnerabilities including prompt injection, data exfiltration, and privilege escalation risks.  \t\t\t\t\tAI-generated summary \t\t\t\t The rise of AI agent frameworks has introduced agent skills, modular packages containing instructions and executable code that dynamically extend agent capabilities. While this architecture enables powerful customization, skills execute with implicit trust and minimal vetting, creating a significant yet uncharacterized attack surface. We conduct the first large-scale empirical security analysis of this emerging ecosystem, collecting 42,447 skills from two major marketplaces and systematically analyzing 31,132 using SkillScan, a multi-stage detection framework integrating static analysis with LLM-based semantic classification. Our findings reveal pervasive security risks: 26.1% of skills contain at least one vulnerability, spanning 14 distinct patterns across four categories: prompt injection, data exfiltration, privilege escalation, and supply chain risks. Data exfiltration (13.3%) and privilege escalation (11.8%) are most prevalent, while 5.2% of skills exhibit high-severity patterns strongly suggesting malicious intent. We find that skills bundling executable scripts are 2.12x more likely to contain vulnerabilities than instruction-only skills (OR=2.12, p<0.001). Our contributions include: (1) a grounded vulnerability taxonomy derived from 8,126 vulnerable skills, (2) a validated detection methodology achieving 86.7% precision and 82.5% recall, and (3) an open dataset and detection toolkit to support future research. These results demonstrate an urgent need for capability-based permission systems and mandatory security vetting before this attack vector is further exploited.', 'score': 3, 'issue_id': 619, 'pub_date': '2026-01-15', 'pub_date_card': {'ru': '15 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 15', 'zh': '1æœˆ15æ—¥'}, 'hash': 'b355d256356bbe86', 'authors': ['Yi Liu', 'Weizhe Wang', 'Ruitao Feng', 'Yao Zhang', 'Guangquan Xu', 'Gelei Deng', 'Yuekang Li', 'Leo Zhang'], 'affiliations': ['Griffith University', 'Nanyang Technological University', 'Quantstamp', 'School of Cybersecurity, Tianjin University', 'Southern Cross University', 'Tianjin University', 'University of New South Wales'], 'pdf_title_img': 'assets/pdf/title_img/2601.10338.jpg', 'data': {'categories': ['#agents', '#security', '#dataset', '#open_source', '#benchmark'], 'emoji': 'ğŸ”“', 'ru': {'title': 'Ğ¡ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ ÑƒĞ³Ñ€Ğ¾Ğ·Ñ‹ Ğ² ÑĞºĞ¾ÑĞ¸ÑÑ‚ĞµĞ¼Ğµ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‚ ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¼ĞµÑ€ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñ‹', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ°ĞºĞµÑ‚Ñ‹ ĞºĞ¾Ğ´Ğ° Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ´Ğ»Ñ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ 31 Ñ‚Ñ‹ÑÑÑ‡Ğ¸ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ğ¸Ğ· Ğ´Ğ²ÑƒÑ… ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ¼Ğ°Ñ€ĞºĞµÑ‚Ğ¿Ğ»ĞµĞ¹ÑĞ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº SkillScan Ñ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸ĞµĞ¹ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLM. ĞĞ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ 26.1% Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‚ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¸Ğ½ÑŠĞµĞºÑ†Ğ¸Ğ¸ Ğ² Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹, ÑƒÑ‚ĞµÑ‡ĞºÑƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ²Ğ¸Ğ»ĞµĞ³Ğ¸Ğ¹ Ğ¸ Ñ€Ğ¸ÑĞºĞ¸ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¾Ğº. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹, Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Uncovering Vulnerabilities in AI Agent Skills: A Call for Security Measures', 'desc': 'This paper presents a comprehensive security analysis of AI agent skills, which are modular packages that enhance the capabilities of AI agents. The study reveals that a significant portion of these skills, specifically 26.1%, contain vulnerabilities, including risks like prompt injection and data exfiltration. Using a detection framework called SkillScan, the authors analyzed over 31,000 skills and identified 14 distinct vulnerability patterns. The findings highlight the urgent need for improved security measures, such as capability-based permission systems, to mitigate these risks before they can be exploited.'}, 'zh': {'title': 'AIä»£ç†æŠ€èƒ½çš„å®‰å…¨éšæ‚£è­¦ç¤º', 'desc': 'æœ¬ç ”ç©¶å¯¹AIä»£ç†æŠ€èƒ½è¿›è¡Œäº†å¤§è§„æ¨¡çš„å®‰å…¨åˆ†æï¼Œå‘ç°äº†å¹¿æ³›çš„å®‰å…¨æ¼æ´ï¼ŒåŒ…æ‹¬æç¤ºæ³¨å…¥ã€æ•°æ®å¤–æ³„å’Œæƒé™æå‡é£é™©ã€‚AIä»£ç†æŠ€èƒ½æ˜¯åŒ…å«æŒ‡ä»¤å’Œå¯æ‰§è¡Œä»£ç çš„æ¨¡å—åŒ–åŒ…ï¼Œè™½ç„¶è¿™ç§æ¶æ„å…è®¸å¼ºå¤§çš„å®šåˆ¶ï¼Œä½†æŠ€èƒ½åœ¨æ‰§è¡Œæ—¶éšå«ä¿¡ä»»ä¸”å®¡æ ¸ä¸è¶³ï¼Œå¯¼è‡´äº†æ˜¾è‘—çš„æ”»å‡»é¢ã€‚æˆ‘ä»¬åˆ†æäº†42447ä¸ªæŠ€èƒ½ï¼Œå‘ç°26.1%çš„æŠ€èƒ½è‡³å°‘å­˜åœ¨ä¸€ä¸ªæ¼æ´ï¼Œæ¶µç›–äº†14ç§ä¸åŒæ¨¡å¼ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæ€¥éœ€å»ºç«‹åŸºäºèƒ½åŠ›çš„æƒé™ç³»ç»Ÿå’Œå¼ºåˆ¶å®‰å…¨å®¡æ ¸ï¼Œä»¥é˜²æ­¢è¿™äº›æ”»å‡»å‘é‡è¢«è¿›ä¸€æ­¥åˆ©ç”¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.09923', 'title': 'CaMeLs Can Use Computers Too: System-level Security for Computer Use Agents', 'url': 'https://huggingface.co/papers/2601.09923', 'abstract': 'Computer Use Agents face security challenges from prompt injection attacks, but a single-shot planning approach with architectural isolation enables secure autonomous task execution while maintaining performance.  \t\t\t\t\tAI-generated summary \t\t\t\t AI agents are vulnerable to prompt injection attacks, where malicious content hijacks agent behavior to steal credentials or cause financial loss. The only known robust defense is architectural isolation that strictly separates trusted task planning from untrusted environment observations. However, applying this design to Computer Use Agents (CUAs) -- systems that automate tasks by viewing screens and executing actions -- presents a fundamental challenge: current agents require continuous observation of UI state to determine each action, conflicting with the isolation required for security. We resolve this tension by demonstrating that UI workflows, while dynamic, are structurally predictable. We introduce Single-Shot Planning for CUAs, where a trusted planner generates a complete execution graph with conditional branches before any observation of potentially malicious content, providing provable control flow integrity guarantees against arbitrary instruction injections. Although this architectural isolation successfully prevents instruction injections, we show that additional measures are needed to prevent Branch Steering attacks, which manipulate UI elements to trigger unintended valid paths within the plan. We evaluate our design on OSWorld, and retain up to 57% of the performance of frontier models while improving performance for smaller open-source models by up to 19%, demonstrating that rigorous security and utility can coexist in CUAs.', 'score': 2, 'issue_id': 616, 'pub_date': '2026-01-14', 'pub_date_card': {'ru': '14 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 14', 'zh': '1æœˆ14æ—¥'}, 'hash': 'c4457725ffaceb3e', 'authors': ['Hanna Foerster', 'Robert Mullins', 'Tom Blanchard', 'Nicolas Papernot', 'Kristina NikoliÄ‡', 'Florian TramÃ¨r', 'Ilia Shumailov', 'Cheng Zhang', 'Yiren Zhao'], 'affiliations': ['AI Sequrity Company', 'ETH Zurich', 'University of Cambridge', 'University of Toronto', 'Vector Institute'], 'pdf_title_img': 'assets/pdf/title_img/2601.09923.jpg', 'data': {'categories': ['#security', '#open_source'], 'emoji': 'ğŸ›¡ï¸', 'ru': {'title': 'Ğ‘ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾Ğµ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ² Ğ¾Ğ´Ğ½Ñƒ Ñ„Ğ°Ğ·Ñƒ Ğ´Ğ»Ñ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñ‹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¾Ñ‚ Ğ°Ñ‚Ğ°Ğº', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğµ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¾Ñ‚ Ğ°Ñ‚Ğ°Ğº prompt injection, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¿ĞµÑ€ĞµÑ…Ğ²Ğ°Ñ‚Ğ¸Ñ‚ÑŒ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ¸ Ğ¿Ñ€Ğ¸Ğ²ĞµÑÑ‚Ğ¸ Ğº ĞºÑ€Ğ°Ğ¶Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ»Ğ¸ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ğ¼ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ Ğ¸Ğ·Ğ¾Ğ»ÑÑ†Ğ¸ĞµĞ¹, Ğ¿Ñ€Ğ¸ ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ Ğ´Ğ¾Ğ²ĞµÑ€ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¹ Ğ³Ñ€Ğ°Ñ„ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ²ĞµÑ‚Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ´Ğ¾ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ğ¼Ğ¾Ğ³Ğ¾. ĞĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½ÑƒÑ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ UI-Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ², Ğ¾Ğ½Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ñ‚ÑŒ Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ğ¸ Ñ†ĞµĞ»Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ° ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾Ğ¹ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Secure and Efficient Task Execution for AI Agents', 'desc': 'This paper addresses the security vulnerabilities of Computer Use Agents (CUAs) against prompt injection attacks, which can manipulate agent behavior for malicious purposes. It proposes a Single-Shot Planning approach that utilizes architectural isolation to separate trusted task planning from untrusted observations, ensuring secure task execution. The authors demonstrate that despite the dynamic nature of user interfaces, workflows are predictable enough to allow for pre-planned execution paths, thus maintaining control flow integrity. The evaluation shows that this method retains significant performance while enhancing security, proving that it is possible to achieve both rigorous security and operational efficiency in CUAs.'}, 'zh': {'title': 'å®‰å…¨ä¸æ€§èƒ½çš„å®Œç¾ç»“åˆ', 'desc': 'è®¡ç®—æœºä½¿ç”¨ä»£ç†é¢ä¸´ç€æç¤ºæ³¨å…¥æ”»å‡»çš„å®‰å…¨æŒ‘æˆ˜ï¼Œè¿™ç§æ”»å‡»å¯èƒ½ä¼šåŠ«æŒä»£ç†è¡Œä¸ºï¼Œå¯¼è‡´å‡­è¯è¢«çªƒå–æˆ–è´¢åŠ¡æŸå¤±ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§å•æ¬¡è§„åˆ’çš„æ–¹æ³•ï¼Œé€šè¿‡æ¶æ„éš”ç¦»å®ç°å®‰å…¨çš„è‡ªä¸»ä»»åŠ¡æ‰§è¡Œï¼ŒåŒæ—¶ä¿æŒæ€§èƒ½ã€‚æˆ‘ä»¬å±•ç¤ºäº†ç”¨æˆ·ç•Œé¢å·¥ä½œæµè™½ç„¶åŠ¨æ€ï¼Œä½†åœ¨ç»“æ„ä¸Šæ˜¯å¯é¢„æµ‹çš„ï¼Œä»è€Œä½¿å¾—åœ¨è§‚å¯Ÿæ½œåœ¨æ¶æ„å†…å®¹ä¹‹å‰ç”Ÿæˆå®Œæ•´çš„æ‰§è¡Œå›¾æˆä¸ºå¯èƒ½ã€‚æˆ‘ä»¬çš„è®¾è®¡åœ¨OSWorldä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œè¯æ˜äº†ä¸¥æ ¼çš„å®‰å…¨æ€§å’Œå®ç”¨æ€§å¯ä»¥åœ¨è®¡ç®—æœºä½¿ç”¨ä»£ç†ä¸­å…±å­˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.10716', 'title': 'WildRayZer: Self-supervised Large View Synthesis in Dynamic Environments', 'url': 'https://huggingface.co/papers/2601.10716', 'abstract': 'WildRayZer is a self-supervised framework for novel view synthesis in dynamic environments that uses analysis-by-synthesis to handle moving cameras and objects through motion masking and gradient gating.  \t\t\t\t\tAI-generated summary \t\t\t\t We present WildRayZer, a self-supervised framework for novel view synthesis (NVS) in dynamic environments where both the camera and objects move. Dynamic content breaks the multi-view consistency that static NVS models rely on, leading to ghosting, hallucinated geometry, and unstable pose estimation. WildRayZer addresses this by performing an analysis-by-synthesis test: a camera-only static renderer explains rigid structure, and its residuals reveal transient regions. From these residuals, we construct pseudo motion masks, distill a motion estimator, and use it to mask input tokens and gate loss gradients so supervision focuses on cross-view background completion. To enable large-scale training and evaluation, we curate Dynamic RealEstate10K (D-RE10K), a real-world dataset of 15K casually captured dynamic sequences, and D-RE10K-iPhone, a paired transient and clean benchmark for sparse-view transient-aware NVS. Experiments show that WildRayZer consistently outperforms optimization-based and feed-forward baselines in both transient-region removal and full-frame NVS quality with a single feed-forward pass.', 'score': 1, 'issue_id': 614, 'pub_date': '2026-01-15', 'pub_date_card': {'ru': '15 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 15', 'zh': '1æœˆ15æ—¥'}, 'hash': 'cb1e7cf1044b6dca', 'authors': ['Xuweiyi Chen', 'Wentao Zhou', 'Zezhou Cheng'], 'affiliations': ['University of Virginia'], 'pdf_title_img': 'assets/pdf/title_img/2601.10716.jpg', 'data': {'categories': ['#optimization'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ¡Ğ¸Ğ½Ñ‚ĞµĞ· Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ²Ğ¸Ğ´Ğ¾Ğ² Ğ² Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡Ğ½Ğ¾Ğ¼ Ğ¼Ğ¸Ñ€Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ', 'desc': 'WildRayZer â€” ÑÑ‚Ğ¾ self-supervised Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ²Ğ¸Ğ´Ğ¾Ğ² (novel view synthesis) Ğ² Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ†ĞµĞ½Ğ°Ñ…, Ğ³Ğ´Ğµ Ğ´Ğ²Ğ¸Ğ¶ÑƒÑ‚ÑÑ ĞºĞ°Ğº ĞºĞ°Ğ¼ĞµÑ€Ğ°, Ñ‚Ğ°Ğº Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·-ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ñ€ĞµĞ½Ğ´ĞµÑ€ĞµÑ€ Ğ¾Ğ±ÑŠÑÑĞ½ÑĞµÑ‚ Ğ¶ĞµÑÑ‚ĞºÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ ÑÑ†ĞµĞ½Ñ‹, Ğ° Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ğº Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ñ Ğ´Ğ²Ğ¸Ğ¶ÑƒÑ‰Ğ¸Ğ¼Ğ¸ÑÑ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¸Ñ… Ğ¾ÑÑ‚Ğ°Ñ‚ĞºĞ¾Ğ² ÑĞ¾Ğ·Ğ´Ğ°ÑÑ‚ÑÑ Ğ¼Ğ°ÑĞºĞ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ²Ñ…Ğ¾Ğ´Ğ° Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ ÑĞ¾ÑÑ€ĞµĞ´Ğ¾Ñ‚Ğ¾Ñ‡Ğ¸Ñ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğ¸ Ñ„Ğ¾Ğ½Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ D-RE10K Ñ 15K Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ WildRayZer Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ baseline Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ĞºĞ°Ğº Ğ² ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ğ¸ Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ğ¾Ğ² Ğ¾Ñ‚ Ğ´Ğ²Ğ¸Ğ¶ÑƒÑ‰Ğ¸Ñ…ÑÑ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ñ‚Ğ°Ğº Ğ¸ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ°Ğ´Ñ€Ğ°.'}, 'en': {'title': 'WildRayZer: Mastering Dynamic View Synthesis with Self-Supervision', 'desc': 'WildRayZer is a self-supervised framework designed for novel view synthesis (NVS) in environments where both the camera and objects are in motion. It tackles the challenges posed by dynamic content, which can lead to issues like ghosting and unstable pose estimation in traditional NVS models. By employing an analysis-by-synthesis approach, it generates pseudo motion masks that help focus the learning process on the background while managing transient regions effectively. The framework is trained and evaluated on a newly curated dataset, D-RE10K, which includes dynamic sequences, demonstrating superior performance over existing methods in both transient-region handling and overall NVS quality.'}, 'zh': {'title': 'åŠ¨æ€ç¯å¢ƒä¸­çš„æ–°è§†è§’åˆæˆæ–°çªç ´', 'desc': 'WildRayZeræ˜¯ä¸€ä¸ªè‡ªç›‘ç£æ¡†æ¶ï¼Œæ—¨åœ¨åŠ¨æ€ç¯å¢ƒä¸­è¿›è¡Œæ–°è§†è§’åˆæˆã€‚å®ƒé€šè¿‡è¿åŠ¨æ©æ¨¡å’Œæ¢¯åº¦é—¨æ§æ¥å¤„ç†ç§»åŠ¨çš„ç›¸æœºå’Œç‰©ä½“ï¼Œè§£å†³äº†åŠ¨æ€å†…å®¹å¯¼è‡´çš„å¤šè§†å›¾ä¸€è‡´æ€§é—®é¢˜ã€‚è¯¥æ¡†æ¶é‡‡ç”¨åˆ†æ-åˆæˆçš„æ–¹æ³•ï¼Œåˆ©ç”¨é™æ€æ¸²æŸ“å™¨è§£é‡Šåˆšæ€§ç»“æ„ï¼Œå¹¶é€šè¿‡æ®‹å·®æ„å»ºä¼ªè¿åŠ¨æ©æ¨¡ã€‚å®éªŒè¡¨æ˜ï¼ŒWildRayZeråœ¨ç¬æ€åŒºåŸŸå»é™¤å’Œå…¨å¸§æ–°è§†è§’åˆæˆè´¨é‡æ–¹é¢ï¼Œä¼˜äºåŸºäºä¼˜åŒ–å’Œå‰é¦ˆçš„åŸºçº¿æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.10124', 'title': 'VQ-Seg: Vector-Quantized Token Perturbation for Semi-Supervised Medical Image Segmentation', 'url': 'https://huggingface.co/papers/2601.10124', 'abstract': 'VQ-Seg introduces a vector quantization-based perturbation method for medical image segmentation that replaces dropout with a controllable quantized perturbation module while maintaining performance through a dual-branch architecture and foundation model guidance.  \t\t\t\t\tAI-generated summary \t\t\t\t Consistency learning with feature perturbation is a widely used strategy in semi-supervised medical image segmentation. However, many existing perturbation methods rely on dropout, and thus require a careful manual tuning of the dropout rate, which is a sensitive hyperparameter and often difficult to optimize and may lead to suboptimal regularization. To overcome this limitation, we propose VQ-Seg, the first approach to employ vector quantization (VQ) to discretize the feature space and introduce a novel and controllable Quantized Perturbation Module (QPM) that replaces dropout. Our QPM perturbs discrete representations by shuffling the spatial locations of codebook indices, enabling effective and controllable regularization. To mitigate potential information loss caused by quantization, we design a dual-branch architecture where the post-quantization feature space is shared by both image reconstruction and segmentation tasks. Moreover, we introduce a Post-VQ Feature Adapter (PFA) to incorporate guidance from a foundation model (FM), supplementing the high-level semantic information lost during quantization. Furthermore, we collect a large-scale Lung Cancer (LC) dataset comprising 828 CT scans annotated for central-type lung carcinoma. Extensive experiments on the LC dataset and other public benchmarks demonstrate the effectiveness of our method, which outperforms state-of-the-art approaches. Code available at: https://github.com/script-Yang/VQ-Seg.', 'score': 1, 'issue_id': 618, 'pub_date': '2026-01-15', 'pub_date_card': {'ru': '15 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 15', 'zh': '1æœˆ15æ—¥'}, 'hash': 'd9d2fcf8c419968e', 'authors': ['Sicheng Yang', 'Zhaohu Xing', 'Lei Zhu'], 'affiliations': ['The Hong Kong University of Science and Technology', 'The Hong Kong University of Science and Technology (Guangzhou)'], 'pdf_title_img': 'assets/pdf/title_img/2601.10124.jpg', 'data': {'categories': ['#dataset', '#architecture', '#healthcare', '#optimization', '#training'], 'emoji': 'ğŸ«', 'ru': {'title': 'ĞšĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ²Ğ¾Ğ·Ğ¼ÑƒÑ‰ĞµĞ½Ğ¸Ğµ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ dropout Ğ² Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸', 'desc': 'VQ-Seg Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ²Ğ¾Ğ·Ğ¼ÑƒÑ‰ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑƒĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ·Ğ°Ğ¼ĞµĞ½ÑÑ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ dropout ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ÑƒĞ»ĞµĞ¼ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ·Ğ¼ÑƒÑ‰ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ´Ğ²ÑƒÑ…Ğ²ĞµÑ‚Ğ²ĞµĞ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ, Ğ³Ğ´Ğµ Ğ¾Ğ´Ğ½Ğ° Ğ²ĞµÑ‚Ğ²ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‡Ğ°ĞµÑ‚ Ğ·Ğ° Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ğ° Ğ´Ñ€ÑƒĞ³Ğ°Ñ Ğ·Ğ° ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¸ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸. Ğ”Ğ»Ñ ĞºĞ¾Ğ¼Ğ¿ĞµĞ½ÑĞ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€ Post-VQ Feature Adapter, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¾Ñ‚ foundation model. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ½Ğ¾Ğ²Ğ¾Ğ¼ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ Ñ€Ğ°ĞºĞ° Ğ»ĞµĞ³ĞºĞ¸Ñ… Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Revolutionizing Medical Image Segmentation with Vector Quantization', 'desc': 'VQ-Seg is a novel method for medical image segmentation that utilizes vector quantization to enhance performance while replacing traditional dropout techniques. It introduces a Quantized Perturbation Module (QPM) that shuffles codebook indices to create controlled perturbations, improving regularization without the need for sensitive hyperparameter tuning. The method employs a dual-branch architecture to share post-quantization features between image reconstruction and segmentation tasks, minimizing information loss. Additionally, a Post-VQ Feature Adapter (PFA) integrates guidance from a foundation model to retain high-level semantic information, demonstrating superior performance on a large-scale Lung Cancer dataset and other benchmarks.'}, 'zh': {'title': 'VQ-Segï¼šå¯æ§é‡åŒ–æ‰°åŠ¨çš„åŒ»å­¦å›¾åƒåˆ†å‰²æ–°æ–¹æ³•', 'desc': 'VQ-Segæ˜¯ä¸€ç§åŸºäºå‘é‡é‡åŒ–çš„åŒ»å­¦å›¾åƒåˆ†å‰²æ–¹æ³•ï¼Œå®ƒç”¨å¯æ§çš„é‡åŒ–æ‰°åŠ¨æ¨¡å—æ›¿ä»£äº†ä¼ ç»Ÿçš„dropoutï¼Œä»è€Œæé«˜äº†åˆ†å‰²æ€§èƒ½ã€‚è¯¥æ–¹æ³•é‡‡ç”¨åŒåˆ†æ”¯æ¶æ„ï¼Œç¡®ä¿åœ¨é‡åŒ–åç‰¹å¾ç©ºé—´ä¸­ï¼Œå›¾åƒé‡å»ºå’Œåˆ†å‰²ä»»åŠ¡å…±äº«ä¿¡æ¯ï¼Œå‡å°‘ä¿¡æ¯æŸå¤±ã€‚é€šè¿‡å¼•å…¥åé‡åŒ–ç‰¹å¾é€‚é…å™¨ï¼ŒVQ-Segèƒ½å¤Ÿç»“åˆåŸºç¡€æ¨¡å‹çš„æŒ‡å¯¼ï¼Œè¡¥å……é‡åŒ–è¿‡ç¨‹ä¸­ä¸¢å¤±çš„é«˜å±‚è¯­ä¹‰ä¿¡æ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVQ-Segåœ¨å¤§è§„æ¨¡è‚ºç™Œæ•°æ®é›†å’Œå…¶ä»–å…¬å…±åŸºå‡†ä¸Šè¡¨ç°ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.08302', 'title': 'Enhancing Sentiment Classification and Irony Detection in Large Language Models through Advanced Prompt Engineering Techniques', 'url': 'https://huggingface.co/papers/2601.08302', 'abstract': "Advanced prompting techniques significantly enhance large language model performance in sentiment analysis, with optimal strategies varying by model architecture and task complexity.  \t\t\t\t\tAI-generated summary \t\t\t\t This study investigates the use of prompt engineering to enhance large language models (LLMs), specifically GPT-4o-mini and gemini-1.5-flash, in sentiment analysis tasks. It evaluates advanced prompting techniques like few-shot learning, chain-of-thought prompting, and self-consistency against a baseline. Key tasks include sentiment classification, aspect-based sentiment analysis, and detecting subtle nuances such as irony. The research details the theoretical background, datasets, and methods used, assessing performance of LLMs as measured by accuracy, recall, precision, and F1 score. Findings reveal that advanced prompting significantly improves sentiment analysis, with the few-shot approach excelling in GPT-4o-mini and chain-of-thought prompting boosting irony detection in gemini-1.5-flash by up to 46%. Thus, while advanced prompting techniques overall improve performance, the fact that few-shot prompting works best for GPT-4o-mini and chain-of-thought excels in gemini-1.5-flash for irony detection suggests that prompting strategies must be tailored to both the model and the task. This highlights the importance of aligning prompt design with both the LLM's architecture and the semantic complexity of the task.", 'score': 1, 'issue_id': 619, 'pub_date': '2026-01-13', 'pub_date_card': {'ru': '13 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 13', 'zh': '1æœˆ13æ—¥'}, 'hash': 'a4588fb75be2544c', 'authors': ['Marvin Schmitt', 'Anne Schwerk', 'Sebastian Lempert'], 'affiliations': ['IU International University of Applied Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2601.08302.jpg', 'data': {'categories': [], 'emoji': 'ğŸ¯', 'ru': {'title': 'ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚-Ğ¸Ğ½Ğ¶Ğ¸Ğ½Ğ¸Ñ€Ğ¸Ğ½Ğ³: ĞºĞ»ÑÑ‡ Ğº Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ LLM', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸Ğº Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚-Ğ¸Ğ½Ğ¶Ğ¸Ğ½Ğ¸Ñ€Ğ¸Ğ½Ğ³Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¿Ñ€Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğµ Ñ‚Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ÑÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Few-shot Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ, Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºÑƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (Chain-of-Thought) Ğ¸ ÑĞ°Ğ¼Ğ¾ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ² Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… GPT-4o-mini Ğ¸ gemini-1.5-flash. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ñ‚Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ñ‚Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ Ğ°ÑĞ¿ĞµĞºÑ‚Ğ°Ğ¼ Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ğ½ĞºĞ¸Ñ… ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ĞµĞ¹, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ¸Ñ€Ğ¾Ğ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ñ‚ Ğ¾Ñ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€ Few-shot Ğ»ÑƒÑ‡ÑˆĞµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ´Ğ»Ñ GPT-4o-mini, Ğ° Chain-of-Thought Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¸Ñ€Ğ¾Ğ½Ğ¸Ğ¸ Ğ² gemini-1.5-flash Ğ½Ğ° 46%.'}, 'en': {'title': 'Tailoring Prompts for Better Sentiment Analysis', 'desc': 'This paper explores how advanced prompting techniques can improve the performance of large language models (LLMs) in sentiment analysis. It specifically examines models like GPT-4o-mini and gemini-1.5-flash, using methods such as few-shot learning and chain-of-thought prompting. The study finds that these techniques enhance tasks like sentiment classification and irony detection, with few-shot prompting being particularly effective for GPT-4o-mini. Overall, the research emphasizes the need to customize prompting strategies based on the model architecture and the complexity of the sentiment analysis task.'}, 'zh': {'title': 'ä¼˜åŒ–æç¤ºç­–ç•¥ï¼Œæå‡æƒ…æ„Ÿåˆ†ææ•ˆæœ', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†å¦‚ä½•é€šè¿‡æç¤ºå·¥ç¨‹æ¥æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æƒ…æ„Ÿåˆ†æä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹GPT-4o-miniå’Œgemini-1.5-flashæ¨¡å‹ã€‚ç ”ç©¶è¯„ä¼°äº†å‡ ç§å…ˆè¿›çš„æç¤ºæŠ€æœ¯ï¼Œå¦‚å°‘é‡å­¦ä¹ ã€æ€ç»´é“¾æç¤ºå’Œè‡ªæˆ‘ä¸€è‡´æ€§ï¼Œå¹¶ä¸åŸºçº¿è¿›è¡Œæ¯”è¾ƒã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå…ˆè¿›çš„æç¤ºæŠ€æœ¯æ˜¾è‘—æé«˜äº†æƒ…æ„Ÿåˆ†æçš„å‡†ç¡®æ€§ï¼Œå°¤å…¶æ˜¯å°‘é‡å­¦ä¹ åœ¨GPT-4o-miniä¸­è¡¨ç°æœ€ä½³ï¼Œè€Œæ€ç»´é“¾æç¤ºåœ¨gemini-1.5-flashä¸­å¯¹è®½åˆºæ£€æµ‹çš„æå‡è¾¾åˆ°46%ã€‚å› æ­¤ï¼Œæç¤ºç­–ç•¥éœ€è¦æ ¹æ®æ¨¡å‹æ¶æ„å’Œä»»åŠ¡å¤æ‚æ€§è¿›è¡Œè°ƒæ•´ï¼Œä»¥å®ç°æœ€ä½³æ•ˆæœã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.08297', 'title': 'Demystifying the Slash Pattern in Attention: The Role of RoPE', 'url': 'https://huggingface.co/papers/2601.08297', 'abstract': 'Slash-Dominant Heads in large language models emerge from specific conditions involving rank-one queries/keys and dominant medium-high frequency components of Rotary Position Embedding, with theoretical proof showing their emergence through gradient descent training dynamics.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) often exhibit slash attention patterns, where attention scores concentrate along the Î”-th sub-diagonal for some offset Î”. These patterns play a key role in passing information across tokens. But why do they emerge? In this paper, we demystify the emergence of these Slash-Dominant Heads (SDHs) from both empirical and theoretical perspectives. First, by analyzing open-source LLMs, we find that SDHs are intrinsic to models and generalize to out-of-distribution prompts. To explain the intrinsic emergence, we analyze the queries, keys, and Rotary Position Embedding (RoPE), which jointly determine attention scores. Our empirical analysis reveals two characteristic conditions of SDHs: (1) Queries and keys are almost rank-one, and (2) RoPE is dominated by medium- and high-frequency components. Under these conditions, queries and keys are nearly identical across tokens, and interactions between medium- and high-frequency components of RoPE give rise to SDHs. Beyond empirical evidence, we theoretically show that these conditions are sufficient to ensure the emergence of SDHs by formalizing them as our modeling assumptions. Particularly, we analyze the training dynamics of a shallow Transformer equipped with RoPE under these conditions, and prove that models trained via gradient descent exhibit SDHs. The SDHs generalize to out-of-distribution prompts.', 'score': 1, 'issue_id': 626, 'pub_date': '2026-01-13', 'pub_date_card': {'ru': '13 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 13', 'zh': '1æœˆ13æ—¥'}, 'hash': '1e0e29cc30ed6f7c', 'authors': ['Yuan Cheng', 'Fengzhuo Zhang', 'Yunlong Hou', 'Cunxiao Du', 'Chao Du', 'Tianyu Pang', 'Aixin Sun', 'Zhuoran Yang'], 'affiliations': ['Nanyang Technological University', 'National University of Singapore', 'Sea AI Lab', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2601.08297.jpg', 'data': {'categories': ['#architecture', '#training', '#interpretability', '#math'], 'emoji': 'âš”ï¸', 'ru': {'title': 'Ğ¡Ğ»ÑÑˆ-Ğ´Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ñ‹ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ: Ğ¾Ñ‚ ÑĞ¼Ğ¿Ğ¸Ñ€Ğ¸Ğ¸ Ğº Ñ‚ĞµĞ¾Ñ€Ğ¸Ğ¸', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ñ„ĞµĞ½Ğ¾Ğ¼ĞµĞ½ ÑĞ»ÑÑˆ-Ğ´Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ğ³Ğ¾Ğ»Ğ¾Ğ² Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ĞºĞ¾Ğ½Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ´Ğ¾Ğ»ÑŒ Ğ´Ğ¸Ğ°Ğ³Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»Ğ¾Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¸Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ÑÑ‚ Ğ¿Ñ€Ğ¸ Ğ´Ğ²ÑƒÑ… ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ…: Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ñ‹ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ¹ Ğ¸Ğ¼ĞµÑÑ‚ Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸ ĞµĞ´Ğ¸Ğ½Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ Ñ€Ğ°Ğ½Ğ³, Ğ° Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ RoPE Ğ´Ğ¾Ğ¼Ğ¸Ğ½Ğ¸Ñ€ÑƒÑÑ‚ÑÑ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ ÑÑ€ĞµĞ´Ğ½ĞµĞ¹ Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ñ‹. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ĞºĞ°Ğº ÑĞ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… LLM, Ñ‚Ğ°Ğº Ğ¸ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ¾ Ñ‚Ğ¾Ğ³Ğ¾, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¸ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ñ Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ñ€ÑƒÑÑ‚ Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ½Ğ¾Ğ²ĞµĞ½Ğ¸Ğµ ÑĞ»ÑÑˆ-Ğ´Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ğ³Ğ¾Ğ»Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¿ÑƒÑĞºĞ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ ÑĞ²Ğ»ÑÑÑ‚ÑÑ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğ¼ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‚ÑÑ Ğ½Ğ° Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ²Ğ½Ğµ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Unveiling Slash-Dominant Heads in Language Models', 'desc': 'This paper explores the emergence of Slash-Dominant Heads (SDHs) in large language models (LLMs), which are attention patterns that focus on specific token interactions. The authors identify two key conditions for the emergence of SDHs: the queries and keys being nearly rank-one and the Rotary Position Embedding (RoPE) being influenced by medium- and high-frequency components. Through both empirical analysis of existing LLMs and theoretical proof, they demonstrate that these conditions lead to the formation of SDHs during gradient descent training. The findings suggest that SDHs are not only intrinsic to the models but also robust across various prompts, including those outside the training distribution.'}, 'zh': {'title': 'æ­ç¤ºå¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„æ–œçº¿ä¸»å¯¼å¤´', 'desc': 'æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„æ–œçº¿ä¸»å¯¼å¤´ï¼ˆSDHsï¼‰æ˜¯å¦‚ä½•åœ¨ç‰¹å®šæ¡ä»¶ä¸‹å‡ºç°çš„ã€‚è¿™äº›æ¡ä»¶åŒ…æ‹¬æŸ¥è¯¢å’Œé”®å‡ ä¹ä¸ºç§©ä¸€ï¼Œä»¥åŠæ—‹è½¬ä½ç½®åµŒå…¥ï¼ˆRoPEï¼‰ä¸­ä¸­é«˜é¢‘æˆåˆ†çš„ä¸»å¯¼ä½œç”¨ã€‚é€šè¿‡å¯¹å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹çš„åˆ†æï¼Œæˆ‘ä»¬å‘ç°SDHsæ˜¯æ¨¡å‹çš„å†…åœ¨ç‰¹æ€§ï¼Œå¹¶ä¸”èƒ½å¤Ÿæ¨å¹¿åˆ°åˆ†å¸ƒå¤–çš„æç¤ºã€‚æˆ‘ä»¬è¿˜é€šè¿‡ç†è®ºè¯æ˜äº†è¿™äº›æ¡ä»¶è¶³ä»¥ç¡®ä¿SDHsçš„å‡ºç°ï¼Œå±•ç¤ºäº†åœ¨è¿™äº›æ¡ä»¶ä¸‹ï¼Œç»è¿‡æ¢¯åº¦ä¸‹é™è®­ç»ƒçš„æ¨¡å‹ä¼šå±•ç°å‡ºSDHsã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.00756', 'title': 'Memory Bank Compression for Continual Adaptation of Large Language Models', 'url': 'https://huggingface.co/papers/2601.00756', 'abstract': 'A memory-augmented continual learning approach for large language models that compresses memory banks through codebook optimization while maintaining retention accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) have become a mainstay for many everyday applications. However, as data evolve their knowledge quickly becomes outdated. Continual learning aims to update LLMs with new information without erasing previously acquired knowledge. Although methods such as full fine-tuning can incorporate new data, they are computationally expensive and prone to catastrophic forgetting, where prior knowledge is overwritten. Memory-augmented approaches address this by equipping LLMs with a memory bank, that is an external memory module which stores information for future use. However, these methods face a critical limitation, in particular, the memory bank constantly grows in the real-world scenario when large-scale data streams arrive. In this paper, we propose MBC, a model that compresses the memory bank through a codebook optimization strategy during online adaptation learning. To ensure stable learning, we also introduce an online resetting mechanism that prevents codebook collapse. In addition, we employ Key-Value Low-Rank Adaptation in the attention layers of the LLM, enabling efficient utilization of the compressed memory representations. Experiments with benchmark question-answering datasets demonstrate that MBC reduces the memory bank size to 0.3% when compared against the most competitive baseline, while maintaining high retention accuracy during online adaptation learning. Our code is publicly available at https://github.com/Thomkat/MBC.', 'score': 1, 'issue_id': 618, 'pub_date': '2026-01-02', 'pub_date_card': {'ru': '2 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 2', 'zh': '1æœˆ2æ—¥'}, 'hash': 'd8f70c2791191552', 'authors': ['Thomas Katraouras', 'Dimitrios Rafailidis'], 'affiliations': ['University of Thessaly'], 'pdf_title_img': 'assets/pdf/title_img/2601.00756.jpg', 'data': {'categories': [], 'emoji': 'ğŸ’¾', 'ru': {'title': 'Ğ¡Ğ¶Ğ¸Ğ¼Ğ°ĞµĞ¼ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ MBC â€” Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¿Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ¾ÑÑ‚Ğ° Ğ²Ğ½ĞµÑˆĞ½ĞµĞ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ĞºĞ¾Ğ´Ğ¾Ğ²Ğ¾Ğ¹ ĞºĞ½Ğ¸Ğ³Ğ¸ Ğ´Ğ»Ñ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ğ¿ĞµÑ€ĞµĞ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ»Ğ»Ğ°Ğ¿ÑĞ° ĞºĞ¾Ğ´Ğ¾Ğ²Ğ¾Ğ¹ ĞºĞ½Ğ¸Ğ³Ğ¸. Ğ”Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¶Ğ°Ñ‚Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ½Ğ³Ğ° Ğ² ÑĞ»Ğ¾ÑÑ… Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ´Ğ¾ 0,3% Ğ¾Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ°, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ ÑƒĞ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ½Ğ° Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Efficient Memory Management for Continual Learning in LLMs', 'desc': 'This paper presents a novel approach called MBC, which enhances large language models (LLMs) by integrating a memory-augmented continual learning framework. It addresses the challenge of catastrophic forgetting by using a memory bank that is optimized through codebook compression, allowing the model to retain important information while adapting to new data. The proposed method includes an online resetting mechanism to maintain learning stability and prevent codebook collapse. Experimental results show that MBC significantly reduces the memory bank size while preserving high retention accuracy, making it efficient for real-world applications.'}, 'zh': {'title': 'è®°å¿†å¢å¼ºçš„æŒç»­å­¦ä¹ ï¼Œä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹çš„çŸ¥è¯†ä¿ç•™', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§è®°å¿†å¢å¼ºçš„æŒç»­å­¦ä¹ æ–¹æ³•ï¼Œæ—¨åœ¨ä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è®°å¿†åº“ã€‚é€šè¿‡ä»£ç æœ¬ä¼˜åŒ–ç­–ç•¥ï¼ŒMBCæ¨¡å‹åœ¨åœ¨çº¿é€‚åº”å­¦ä¹ è¿‡ç¨‹ä¸­æœ‰æ•ˆå‹ç¼©è®°å¿†åº“ï¼ŒåŒæ—¶ä¿æŒçŸ¥è¯†ä¿ç•™çš„å‡†ç¡®æ€§ã€‚è¯¥æ–¹æ³•å¼•å…¥äº†åœ¨çº¿é‡ç½®æœºåˆ¶ï¼Œä»¥é˜²æ­¢ä»£ç æœ¬å´©æºƒï¼Œå¹¶åœ¨æ³¨æ„åŠ›å±‚ä¸­é‡‡ç”¨äº†é”®å€¼ä½ç§©é€‚åº”æŠ€æœ¯ï¼Œæå‡äº†å‹ç¼©è®°å¿†è¡¨ç¤ºçš„åˆ©ç”¨æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMBCåœ¨ä¿æŒé«˜ä¿ç•™å‡†ç¡®ç‡çš„åŒæ—¶ï¼Œå°†è®°å¿†åº“å¤§å°å‡å°‘åˆ°0.3%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.05242', 'title': 'GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization', 'url': 'https://huggingface.co/papers/2601.05242', 'abstract': 'Multi-reward reinforcement learning suffers from reward normalization collapse in GRPO, which GDPO addresses by decoupling reward normalization for improved training stability and performance across reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.', 'score': 130, 'issue_id': 493, 'pub_date': '2026-01-08', 'pub_date_card': {'ru': '8 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 8', 'zh': '1æœˆ8æ—¥'}, 'hash': '2644b8e88f8fe659', 'authors': ['Shih-Yang Liu', 'Xin Dong', 'Ximing Lu', 'Shizhe Diao', 'Peter Belcak', 'Mingjie Liu', 'Min-Hung Chen', 'Hongxu Yin', 'Yu-Chiang Frank Wang', 'Kwang-Ting Cheng', 'Yejin Choi', 'Jan Kautz', 'Pavlo Molchanov'], 'affiliations': ['NVIDIA'], 'pdf_title_img': 'assets/pdf/title_img/2601.05242.jpg', 'data': {'categories': ['#optimization', '#rlhf', '#training', '#rl', '#reasoning', '#alignment'], 'emoji': 'âš–ï¸', 'ru': {'title': 'Ğ Ğ°Ğ·Ğ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ°Ñ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (reinforcement learning) Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° GRPO. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ÑĞ¼Ğ¾Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ GRPO Ğº Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ñ‹Ğ¼ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ°Ğ¼ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ¸Ñ… ĞºĞ¾Ğ»Ğ»Ğ°Ğ¿ÑÑƒ Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€Ğµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ GDPO, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¸Ñ… Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ° Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ GDPO Ğ½Ğ°Ğ´ GRPO.'}, 'en': {'title': 'Decoupling Rewards for Better Learning in Multi-Reward RL', 'desc': 'This paper addresses the challenges faced in multi-reward reinforcement learning, particularly the issue of reward normalization collapse when using Group Relative Policy Optimization (GRPO). The authors show that GRPO can lead to identical advantage values for different rewards, which diminishes the effectiveness of the training signal and can cause suboptimal learning outcomes. To overcome this, they propose a new method called Group reward-Decoupled Normalization Policy Optimization (GDPO), which separates the normalization of individual rewards to better maintain their differences. The results indicate that GDPO significantly improves training stability and performance across various reasoning tasks compared to GRPO.'}, 'zh': {'title': 'è§£è€¦å¥–åŠ±å½’ä¸€åŒ–ï¼Œæå‡å¤šå¥–åŠ±å­¦ä¹ æ•ˆæœ', 'desc': 'å¤šå¥–åŠ±å¼ºåŒ–å­¦ä¹ åœ¨ä½¿ç”¨GRPOæ—¶ä¼šå‡ºç°å¥–åŠ±å½’ä¸€åŒ–å´©æºƒçš„é—®é¢˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ç­–ç•¥ä¼˜åŒ–æ–¹æ³•GDPOï¼Œé€šè¿‡è§£è€¦ä¸ªä½“å¥–åŠ±çš„å½’ä¸€åŒ–ï¼Œæ”¹å–„äº†è®­ç»ƒçš„ç¨³å®šæ€§å’Œæ€§èƒ½ã€‚GDPOèƒ½å¤Ÿæ›´å¥½åœ°ä¿ç•™å¥–åŠ±ä¹‹é—´çš„ç›¸å¯¹å·®å¼‚ï¼Œä»è€Œå®ç°æ›´å‡†ç¡®çš„å¤šå¥–åŠ±ä¼˜åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGDPOåœ¨å·¥å…·è°ƒç”¨ã€æ•°å­¦æ¨ç†å’Œç¼–ç æ¨ç†ç­‰ä»»åŠ¡ä¸­å‡ä¼˜äºGRPOã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.04890', 'title': 'Learnable Multipliers: Freeing the Scale of Language Model Matrix Layers', 'url': 'https://huggingface.co/papers/2601.04890', 'abstract': 'Learnable multipliers are introduced to address weight decay-induced normalization artifacts in large language model training, outperforming traditional methods while reducing computational overhead.  \t\t\t\t\tAI-generated summary \t\t\t\t Applying weight decay (WD) to matrix layers is standard practice in large-language-model pretraining. Prior work suggests that stochastic gradient noise induces a Brownian-like expansion of the weight matrices W, whose growth is counteracted by WD, leading to a WD-noise equilibrium with a certain weight norm ||W||. In this work, we view the equilibrium norm as a harmful artifact of the training procedure, and address it by introducing learnable multipliers to learn the optimal scale. First, we attach a learnable scalar multiplier to W and confirm that the WD-noise equilibrium norm is suboptimal: the learned scale adapts to data and improves performance. We then argue that individual row and column norms are similarly constrained, and free their scale by introducing learnable per-row and per-column multipliers. Our method can be viewed as a learnable, more expressive generalization of muP multipliers. It outperforms a well-tuned muP baseline, reduces the computational overhead of multiplier tuning, and surfaces practical questions such as forward-pass symmetries and the width-scaling of the learned multipliers. Finally, we validate learnable multipliers with both Adam and Muon optimizers, where it shows improvement in downstream evaluations matching the improvement of the switching from Adam to Muon.', 'score': 36, 'issue_id': 494, 'pub_date': '2026-01-08', 'pub_date_card': {'ru': '8 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 8', 'zh': '1æœˆ8æ—¥'}, 'hash': '286bbc12fc07f8d9', 'authors': ['Maksim Velikanov', 'Ilyas Chahed', 'Jingwei Zuo', 'Dhia Eddine Rhaiem', 'Younes Belkada', 'Hakim Hacid'], 'affiliations': ['Falcon LLM Team'], 'pdf_title_img': 'assets/pdf/title_img/2601.04890.jpg', 'data': {'categories': ['#training', '#optimization', '#architecture'], 'emoji': 'âš–ï¸', 'ru': {'title': 'ĞĞ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ğµ Ğ¼Ğ½Ğ¾Ğ¶Ğ¸Ñ‚ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²ĞµÑĞ¾Ğ² Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ñ… Ğ¼Ğ½Ğ¾Ğ¶Ğ¸Ñ‚ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ğ¾Ğ² Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ÑÑ‰Ğ¸Ñ… Ğ¿Ñ€Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¸ weight decay Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ€Ğ°Ğ²Ğ½Ğ¾Ğ²ĞµÑĞ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑÑ‚Ğ¾Ñ…Ğ°ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¼ ÑˆÑƒĞ¼Ğ¾Ğ¼ Ğ¸ weight decay Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº ÑÑƒĞ±Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ¼ Ğ²ĞµÑĞ¾Ğ² Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ğµ ÑĞºĞ°Ğ»ÑÑ€Ğ½Ñ‹Ğµ Ğ¼Ğ½Ğ¾Ğ¶Ğ¸Ñ‚ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ñ‹, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ½Ğ¾Ğ¶Ğ¸Ñ‚ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑÑ‚Ñ€Ğ¾Ğº Ğ¸ ÑÑ‚Ğ¾Ğ»Ğ±Ñ†Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹Ñ€Ğ°Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° muP. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ĞºĞ°Ğº Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ Adam, Ñ‚Ğ°Ğº Ğ¸ Ñ Muon.'}, 'en': {'title': 'Learnable Multipliers: Enhancing Large Language Model Training', 'desc': 'This paper introduces learnable multipliers to improve the training of large language models by addressing issues caused by weight decay. Traditional weight decay can create normalization artifacts that hinder model performance, but the proposed method allows the model to learn optimal scaling for weight matrices. By using learnable multipliers for individual rows and columns, the model adapts better to the data, leading to improved performance. The approach not only outperforms existing methods but also reduces the computational burden associated with tuning multipliers.'}, 'zh': {'title': 'å¯å­¦ä¹ ä¹˜å­ï¼šæå‡å¤§è¯­è¨€æ¨¡å‹è®­ç»ƒçš„åˆ©å™¨', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§å¯å­¦ä¹ çš„ä¹˜å­ï¼Œç”¨äºè§£å†³å¤§è¯­è¨€æ¨¡å‹è®­ç»ƒä¸­å› æƒé‡è¡°å‡å¼•èµ·çš„å½’ä¸€åŒ–ä¼ªå½±ã€‚é€šè¿‡å¼•å…¥å¯å­¦ä¹ çš„ä¹˜å­ï¼Œç ”ç©¶è€…å‘ç°å¯ä»¥æ›´å¥½åœ°é€‚åº”æ•°æ®ï¼Œä»è€Œæé«˜æ¨¡å‹æ€§èƒ½ã€‚ä¸ä¼ ç»Ÿçš„æƒé‡è¡°å‡æ–¹æ³•ç›¸æ¯”ï¼Œè¿™ç§æ–°æ–¹æ³•åœ¨å‡å°‘è®¡ç®—å¼€é”€çš„åŒæ—¶ï¼Œè¡¨ç°å‡ºæ›´ä¼˜çš„æ•ˆæœã€‚æœ€ç»ˆï¼Œå®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨å¯å­¦ä¹ ä¹˜å­çš„æ¨¡å‹åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„æ”¹è¿›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.05249', 'title': 'RL-AWB: Deep Reinforcement Learning for Auto White Balance Correction in Low-Light Night-time Scenes', 'url': 'https://huggingface.co/papers/2601.05249', 'abstract': 'A novel nighttime color constancy framework combines statistical methods with deep reinforcement learning to improve white balance adjustment under low-light conditions.  \t\t\t\t\tAI-generated summary \t\t\t\t Nighttime color constancy remains a challenging problem in computational photography due to low-light noise and complex illumination conditions. We present RL-AWB, a novel framework combining statistical methods with deep reinforcement learning for nighttime white balance. Our method begins with a statistical algorithm tailored for nighttime scenes, integrating salient gray pixel detection with novel illumination estimation. Building on this foundation, we develop the first deep reinforcement learning approach for color constancy that leverages the statistical algorithm as its core, mimicking professional AWB tuning experts by dynamically optimizing parameters for each image. To facilitate cross-sensor evaluation, we introduce the first multi-sensor nighttime dataset. Experiment results demonstrate that our method achieves superior generalization capability across low-light and well-illuminated images. Project page: https://ntuneillee.github.io/research/rl-awb/', 'score': 34, 'issue_id': 496, 'pub_date': '2026-01-08', 'pub_date_card': {'ru': '8 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 8', 'zh': '1æœˆ8æ—¥'}, 'hash': 'e9f0ea8291fddbbf', 'authors': ['Yuan-Kang Lee', 'Kuan-Lin Chen', 'Chia-Che Chang', 'Yu-Lun Liu'], 'affiliations': ['MediaTek Inc.', 'National Taiwan University', 'National Yang Ming Chiao Tung University'], 'pdf_title_img': 'assets/pdf/title_img/2601.05249.jpg', 'data': {'categories': ['#cv', '#dataset', '#rl'], 'emoji': 'ğŸŒ™', 'ru': {'title': 'ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ±ĞµĞ»Ğ¾Ğ³Ğ¾ Ğ² Ğ½Ğ¾Ñ‡Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ RL-AWB Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ†Ğ²ĞµÑ‚Ğ¾Ğ²Ğ¾Ğ¹ ĞºĞ¾Ğ½ÑÑ‚Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ½Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ…, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¹ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ñ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞœĞµÑ‚Ğ¾Ğ´ Ğ½Ğ°Ñ‡Ğ¸Ğ½Ğ°ĞµÑ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ ÑĞ¾ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ°, ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ»Ñ Ğ½Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ñ ÑĞµÑ€Ñ‹Ñ… Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ñ. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ° Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ñ†Ğ²ĞµÑ‚Ğ¾Ğ²Ğ¾Ğ¹ ĞºĞ¾Ğ½ÑÑ‚Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞµ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ±ĞµĞ»Ğ¾Ğ³Ğ¾. Ğ”Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ° Ğ±Ñ‹Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸ÑĞµĞ½ÑĞ¾Ñ€Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ½Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½, Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ… Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ Ğ¸ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾ÑĞ²ĞµÑ‰Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚ÑŒÑ.'}, 'en': {'title': 'Revolutionizing Nighttime Photography with RL-AWB', 'desc': "This paper introduces RL-AWB, a new framework that enhances white balance adjustment in nighttime photography using a combination of statistical methods and deep reinforcement learning. The approach starts with a statistical algorithm designed for low-light conditions, which includes detecting gray pixels and estimating illumination. It then employs deep reinforcement learning to optimize the white balance parameters dynamically, simulating the expertise of professional photographers. The authors also present a multi-sensor nighttime dataset to evaluate the framework's performance, showing that RL-AWB outperforms existing methods in both low-light and well-lit scenarios."}, 'zh': {'title': 'å¤œé—´ç™½å¹³è¡¡çš„æ–°çªç ´', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¤œé—´è‰²å½©æ’å¸¸æ€§æ¡†æ¶ï¼Œç»“åˆäº†ç»Ÿè®¡æ–¹æ³•å’Œæ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼Œä»¥æ”¹å–„ä½å…‰ç…§æ¡ä»¶ä¸‹çš„ç™½å¹³è¡¡è°ƒæ•´ã€‚æˆ‘ä»¬çš„æ–¹æ³•é¦–å…ˆä½¿ç”¨é’ˆå¯¹å¤œé—´åœºæ™¯çš„ç»Ÿè®¡ç®—æ³•ï¼Œç»“åˆæ˜¾è‘—ç°è‰²åƒç´ æ£€æµ‹å’Œæ–°é¢–çš„å…‰ç…§ä¼°è®¡ã€‚ç„¶åï¼Œæˆ‘ä»¬å¼€å‘äº†é¦–ä¸ªæ·±åº¦å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œåˆ©ç”¨ç»Ÿè®¡ç®—æ³•ä½œä¸ºæ ¸å¿ƒï¼ŒåŠ¨æ€ä¼˜åŒ–æ¯å¼ å›¾åƒçš„å‚æ•°ï¼Œæ¨¡æ‹Ÿä¸“ä¸šç™½å¹³è¡¡è°ƒèŠ‚ä¸“å®¶çš„è°ƒèŠ‚è¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä½å…‰å’Œè‰¯å¥½ç…§æ˜å›¾åƒä¸­å…·æœ‰æ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.05106', 'title': 'Token-Level LLM Collaboration via FusionRoute', 'url': 'https://huggingface.co/papers/2601.05106', 'abstract': "FusionRoute is a token-level multi-LLM collaboration framework that uses a lightweight router to select optimal experts and add complementary logits, outperforming existing methods in diverse tasks while maintaining efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) exhibit strengths across diverse domains. However, achieving strong performance across these domains with a single general-purpose model typically requires scaling to sizes that are prohibitively expensive to train and deploy. On the other hand, while smaller domain-specialized models are much more efficient, they struggle to generalize beyond their training distributions. To address this dilemma, we propose FusionRoute, a robust and effective token-level multi-LLM collaboration framework in which a lightweight router simultaneously (i) selects the most suitable expert at each decoding step and (ii) contributes a complementary logit that refines or corrects the selected expert's next-token distribution via logit addition. Unlike existing token-level collaboration methods that rely solely on fixed expert outputs, we provide a theoretical analysis showing that pure expert-only routing is fundamentally limited: unless strong global coverage assumptions hold, it cannot in general realize the optimal decoding policy. By augmenting expert selection with a trainable complementary generator, FusionRoute expands the effective policy class and enables recovery of optimal value functions under mild conditions. Empirically, across both Llama-3 and Gemma-2 families and diverse benchmarks spanning mathematical reasoning, code generation, and instruction following, FusionRoute outperforms both sequence- and token-level collaboration, model merging, and direct fine-tuning, while remaining competitive with domain experts on their respective tasks.", 'score': 29, 'issue_id': 492, 'pub_date': '2026-01-08', 'pub_date_card': {'ru': '8 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 8', 'zh': '1æœˆ8æ—¥'}, 'hash': '652fcf403878311c', 'authors': ['Nuoya Xiong', 'Yuhang Zhou', 'Hanqing Zeng', 'Zhaorun Chen', 'Furong Huang', 'Shuchao Bi', 'Lizhu Zhang', 'Zhuokai Zhao'], 'affiliations': ['Carnegie Mellon University', 'Meta AI', 'Meta TBD Lab', 'University of Chicago', 'University of Maryland'], 'pdf_title_img': 'assets/pdf/title_img/2601.05106.jpg', 'data': {'categories': ['#benchmark', '#training', '#architecture'], 'emoji': 'ğŸ”€', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ğ°Ñ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ»Ğ¾Ğ³Ğ¸Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°', 'desc': 'FusionRoute â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… LLM, Ğ² ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ Ğ»Ñ‘Ğ³ĞºĞ¸Ğ¹ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ° Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ ÑˆĞ°Ğ³Ğµ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞšÑ€Ğ¾Ğ¼Ğµ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°, Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ»Ğ¾Ğ³Ğ¸Ñ‚Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑƒÑ‚Ğ¾Ñ‡Ğ½ÑÑÑ‚ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ° Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡ĞµĞ½ Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, ĞºĞ°Ğº Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ»Ğ¾Ğ³Ğ¸Ñ‚Ñ‹ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑÑÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ½Ğ°Ğ´ Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ° Ğ¸ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼.'}, 'en': {'title': 'Optimizing Multi-LLM Collaboration with FusionRoute', 'desc': 'FusionRoute is a novel framework that enhances collaboration among multiple large language models (LLMs) by using a lightweight router to select the best expert for each token during decoding. This approach not only chooses the most suitable model but also adds complementary logits to improve the predictions of the selected expert. By addressing the limitations of traditional expert-only routing, FusionRoute allows for a more flexible and effective decoding strategy that can adapt to various tasks. Empirical results show that it outperforms existing methods in diverse applications, demonstrating its efficiency and robustness in multi-LLM settings.'}, 'zh': {'title': 'å¤šLLMåä½œï¼Œä¼˜åŒ–è§£ç ç­–ç•¥çš„åˆ›æ–°ä¹‹è·¯', 'desc': 'FusionRoute æ˜¯ä¸€ä¸ªåŸºäºä»¤ç‰Œçº§åˆ«çš„å¤šå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åä½œæ¡†æ¶ï¼Œä½¿ç”¨è½»é‡çº§è·¯ç”±å™¨é€‰æ‹©æœ€ä½³ä¸“å®¶å¹¶æ·»åŠ äº’è¡¥çš„ logitsï¼Œä»è€Œåœ¨å¤šç§ä»»åŠ¡ä¸­è¶…è¶Šç°æœ‰æ–¹æ³•ï¼ŒåŒæ—¶ä¿æŒé«˜æ•ˆæ€§ã€‚è¯¥æ¡†æ¶é€šè¿‡åœ¨æ¯ä¸ªè§£ç æ­¥éª¤ä¸­é€‰æ‹©æœ€åˆé€‚çš„ä¸“å®¶ï¼Œå¹¶é€šè¿‡ logits ç›¸åŠ æ¥ä¿®æ­£æ‰€é€‰ä¸“å®¶çš„ä¸‹ä¸€ä¸ªä»¤ç‰Œåˆ†å¸ƒï¼Œè§£å†³äº†å•ä¸€é€šç”¨æ¨¡å‹åœ¨ä¸åŒé¢†åŸŸè¡¨ç°ä¸ä½³çš„é—®é¢˜ã€‚ä¸ä¾èµ–å›ºå®šä¸“å®¶è¾“å‡ºçš„ç°æœ‰æ–¹æ³•ä¸åŒï¼ŒFusionRoute é€šè¿‡å¯è®­ç»ƒçš„äº’è¡¥ç”Ÿæˆå™¨æ‰©å±•äº†æœ‰æ•ˆç­–ç•¥ç±»ï¼Œèƒ½å¤Ÿåœ¨æ¸©å’Œæ¡ä»¶ä¸‹æ¢å¤æœ€ä½³ä»·å€¼å‡½æ•°ã€‚å®éªŒè¯æ˜ï¼ŒFusionRoute åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºåºåˆ—å’Œä»¤ç‰Œçº§åˆ«çš„åä½œã€æ¨¡å‹åˆå¹¶å’Œç›´æ¥å¾®è°ƒï¼ŒåŒæ—¶åœ¨å„è‡ªä»»åŠ¡ä¸Šä¸é¢†åŸŸä¸“å®¶ä¿æŒç«äº‰åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.05167', 'title': 'RelayLLM: Efficient Reasoning via Collaborative Decoding', 'url': 'https://huggingface.co/papers/2601.05167', 'abstract': 'RelayLLM enables efficient collaborative reasoning between small and large language models through token-level dynamic invocation, achieving high accuracy with minimal computational overhead.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) for complex reasoning is often hindered by high computational costs and latency, while resource-efficient Small Language Models (SLMs) typically lack the necessary reasoning capacity. Existing collaborative approaches, such as cascading or routing, operate at a coarse granularity by offloading entire queries to LLMs, resulting in significant computational waste when the SLM is capable of handling the majority of reasoning steps. To address this, we propose RelayLLM, a novel framework for efficient reasoning via token-level collaborative decoding. Unlike routers, RelayLLM empowers the SLM to act as an active controller that dynamically invokes the LLM only for critical tokens via a special command, effectively "relaying" the generation process. We introduce a two-stage training framework, including warm-up and Group Relative Policy Optimization (GRPO) to teach the model to balance independence with strategic help-seeking. Empirical results across six benchmarks demonstrate that RelayLLM achieves an average accuracy of 49.52%, effectively bridging the performance gap between the two models. Notably, this is achieved by invoking the LLM for only 1.07% of the total generated tokens, offering a 98.2% cost reduction compared to performance-matched random routers.', 'score': 25, 'issue_id': 492, 'pub_date': '2026-01-08', 'pub_date_card': {'ru': '8 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 8', 'zh': '1æœˆ8æ—¥'}, 'hash': '78cdcd0348ab654b', 'authors': ['Chengsong Huang', 'Tong Zheng', 'Langlin Huang', 'Jinyuan Li', 'Haolin Liu', 'Jiaxin Huang'], 'affiliations': ['University of Maryland', 'University of Virginia', 'Washington University in St. Louis'], 'pdf_title_img': 'assets/pdf/title_img/2601.05167.jpg', 'data': {'categories': ['#training', '#rlhf', '#inference', '#small_models', '#optimization', '#reasoning'], 'emoji': 'âš¡', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ğ°Ñ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ğ°: Ğ¼Ğ°Ğ»Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ…', 'desc': 'RelayLLM Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ³Ñ€ÑƒĞ±Ñ‹Ñ… ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ñ†ĞµĞ»Ğ¸ĞºĞ¾Ğ¼, Ğ¼Ğ°Ğ»Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ¼ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ÑÑ Ğº Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ´Ğ»Ñ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Group Relative Policy Optimization Ğ´Ğ»Ñ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ° Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ¿Ñ€Ğ¸Ğ²Ğ»ĞµĞºĞ°Ñ Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼ĞµĞ½ĞµĞµ Ñ‡ĞµĞ¼ Ğ² 1% ÑĞ»ÑƒÑ‡Ğ°ĞµĞ², Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ 98% ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ².'}, 'en': {'title': 'Efficient Collaboration: Small Models, Big Reasoning!', 'desc': 'RelayLLM is a framework that enhances collaborative reasoning between small and large language models by allowing them to work together more efficiently. It uses a method called token-level dynamic invocation, where the small model can decide to call on the large model only for specific important tokens, rather than for entire queries. This approach minimizes computational costs and latency while maintaining high accuracy, as the small model handles most reasoning tasks independently. The framework includes a two-stage training process to optimize the balance between independent reasoning and when to seek help from the larger model, resulting in significant cost savings and improved performance.'}, 'zh': {'title': 'RelayLLMï¼šé«˜æ•ˆçš„åä½œæ¨ç†æ–°æ¡†æ¶', 'desc': 'RelayLLMæ˜¯ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œèƒ½å¤Ÿé€šè¿‡ä»¤ç‰Œçº§çš„åŠ¨æ€è°ƒç”¨å®ç°å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¹‹é—´çš„é«˜æ•ˆåä½œæ¨ç†ã€‚è¯¥æ–¹æ³•è§£å†³äº†ä¼ ç»Ÿåä½œæ–¹æ³•åœ¨å¤„ç†å¤æ‚æ¨ç†æ—¶çš„é«˜è®¡ç®—æˆæœ¬å’Œå»¶è¿Ÿé—®é¢˜ã€‚RelayLLMå…è®¸SLMä½œä¸ºä¸»åŠ¨æ§åˆ¶è€…ï¼Œä»…åœ¨å…³é”®ä»¤ç‰Œæ—¶è°ƒç”¨LLMï¼Œä»è€Œå‡å°‘äº†è®¡ç®—èµ„æºçš„æµªè´¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRelayLLMåœ¨å…­ä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†49.52%çš„å¹³å‡å‡†ç¡®ç‡ï¼ŒåŒæ—¶ä»…è°ƒç”¨äº†1.07%çš„æ€»ç”Ÿæˆä»¤ç‰Œï¼Œæ˜¾è‘—é™ä½äº†98.2%çš„è®¡ç®—æˆæœ¬ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.05175', 'title': 'VideoAuto-R1: Video Auto Reasoning via Thinking Once, Answering Twice', 'url': 'https://huggingface.co/papers/2601.05175', 'abstract': 'VideoAuto-R1 framework employs a reason-when-necessary strategy for video understanding, using a Thinking Once, Answering Twice training paradigm with verifiable rewards and confidence-based reasoning activation during inference.  \t\t\t\t\tAI-generated summary \t\t\t\t Chain-of-thought (CoT) reasoning has emerged as a powerful tool for multimodal large language models on video understanding tasks. However, its necessity and advantages over direct answering remain underexplored. In this paper, we first demonstrate that for RL-trained video models, direct answering often matches or even surpasses CoT performance, despite CoT producing step-by-step analyses at a higher computational cost. Motivated by this, we propose VideoAuto-R1, a video understanding framework that adopts a reason-when-necessary strategy. During training, our approach follows a Thinking Once, Answering Twice paradigm: the model first generates an initial answer, then performs reasoning, and finally outputs a reviewed answer. Both answers are supervised via verifiable rewards. During inference, the model uses the confidence score of the initial answer to determine whether to proceed with reasoning. Across video QA and grounding benchmarks, VideoAuto-R1 achieves state-of-the-art accuracy with significantly improved efficiency, reducing the average response length by ~3.3x, e.g., from 149 to just 44 tokens. Moreover, we observe a low rate of thinking-mode activation on perception-oriented tasks, but a higher rate on reasoning-intensive tasks. This suggests that explicit language-based reasoning is generally beneficial but not always necessary.', 'score': 23, 'issue_id': 492, 'pub_date': '2026-01-08', 'pub_date_card': {'ru': '8 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 8', 'zh': '1æœˆ8æ—¥'}, 'hash': 'b9a7c1c14abbe921', 'authors': ['Shuming Liu', 'Mingchen Zhuge', 'Changsheng Zhao', 'Jun Chen', 'Lemeng Wu', 'Zechun Liu', 'Chenchen Zhu', 'Zhipeng Cai', 'Chong Zhou', 'Haozhe Liu', 'Ernie Chang', 'Saksham Suri', 'Hongyu Xu', 'Qi Qian', 'Wei Wen', 'Balakrishnan Varadarajan', 'Zhuang Liu', 'Hu Xu', 'Florian Bordes', 'Raghuraman Krishnamoorthi', 'Bernard Ghanem', 'Vikas Chandra', 'Yunyang Xiong'], 'affiliations': ['King Abdullah University of Science and Technology (KAUST)', 'Meta AI', 'Princeton University'], 'pdf_title_img': 'assets/pdf/title_img/2601.05175.jpg', 'data': {'categories': ['#training', '#rlhf', '#benchmark', '#optimization', '#video', '#reasoning', '#multimodal'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ğ¾Ğµ Ğ²Ğ¸Ğ´ĞµĞ½Ğ¸Ğµ: Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ğ¹ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ĞºĞ¾Ğ³Ğ´Ğ° Ğ½ÑƒĞ¶Ğ½Ğ¾', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° VideoAuto-R1, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ñ€Ğ¸ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ¿Ğ¾ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğµ Â«Ğ´ÑƒĞ¼Ğ°Ğ¹ Ğ¾Ğ´Ğ¸Ğ½ Ñ€Ğ°Ğ·, Ğ¾Ñ‚Ğ²ĞµÑ‡Ğ°Ğ¹ Ğ´Ğ²Ğ°Ğ¶Ğ´Ñ‹Â», Ğ³Ğ´Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚, Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ Ğ¸Ñ‚Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚, Ğ¾Ğ±Ğ° ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ñ… Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´. Ğ’Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ñ†ĞµĞ½ĞºÑƒ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸Ñ‚ÑŒ, Ğ½ÑƒĞ¶Ğ½Ğ¾ Ğ»Ğ¸ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€ĞµĞ¶Ğ¸Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸, ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°Ñ ÑÑ€ĞµĞ´Ğ½ÑÑ Ğ´Ğ»Ğ¸Ğ½Ñƒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° Ğ² 3.3 Ñ€Ğ°Ğ·Ğ°, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑĞ²Ğ½Ğ¾Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ Ğ½Ğµ Ğ²ÑĞµĞ³Ğ´Ğ°.'}, 'en': {'title': 'Efficient Video Understanding with Reason-When-Necessary Strategy', 'desc': 'The VideoAuto-R1 framework introduces a novel approach to video understanding by implementing a reason-when-necessary strategy. It utilizes a Thinking Once, Answering Twice training paradigm, where the model first generates an initial answer, then reasons through it, and finally provides a reviewed answer, all while being guided by verifiable rewards. During inference, the model decides whether to engage in reasoning based on the confidence score of its initial answer, leading to improved efficiency. This method achieves state-of-the-art accuracy in video question answering and grounding tasks while significantly reducing response length.'}, 'zh': {'title': 'å¿…è¦æ—¶æ¨ç†ï¼Œæå‡è§†é¢‘ç†è§£æ•ˆç‡', 'desc': 'VideoAuto-R1æ¡†æ¶é‡‡ç”¨å¿…è¦æ—¶æ¨ç†çš„ç­–ç•¥æ¥ç†è§£è§†é¢‘ï¼Œä½¿ç”¨ä¸€æ¬¡æ€è€ƒã€ä¸¤æ¬¡å›ç­”çš„è®­ç»ƒèŒƒå¼ï¼Œå¹¶åœ¨æ¨ç†è¿‡ç¨‹ä¸­ç»“åˆå¯éªŒè¯çš„å¥–åŠ±å’ŒåŸºäºç½®ä¿¡åº¦çš„æ¨ç†æ¿€æ´»ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå¯¹äºå¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„è§†é¢‘æ¨¡å‹ï¼Œç›´æ¥å›ç­”çš„æ•ˆæœå¾€å¾€ä¸é“¾å¼æ€ç»´ï¼ˆCoTï¼‰ç›¸å½“ï¼Œç”šè‡³æ›´å¥½ï¼Œå°½ç®¡CoTçš„è®¡ç®—æˆæœ¬æ›´é«˜ã€‚è¯¥æ¡†æ¶åœ¨è®­ç»ƒæ—¶é¦–å…ˆç”Ÿæˆåˆå§‹ç­”æ¡ˆï¼Œç„¶åè¿›è¡Œæ¨ç†ï¼Œæœ€åè¾“å‡ºç»è¿‡å®¡æŸ¥çš„ç­”æ¡ˆï¼Œå¹¶é€šè¿‡å¯éªŒè¯çš„å¥–åŠ±è¿›è¡Œç›‘ç£ã€‚åœ¨æ¨ç†é˜¶æ®µï¼Œæ¨¡å‹æ ¹æ®åˆå§‹ç­”æ¡ˆçš„ç½®ä¿¡åº¦å†³å®šæ˜¯å¦è¿›è¡Œæ¨ç†ï¼Œä»è€Œåœ¨è§†é¢‘é—®ç­”å’Œå®šä½åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„å‡†ç¡®æ€§å’Œæ˜¾è‘—æé«˜çš„æ•ˆç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.05241', 'title': 'RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation', 'url': 'https://huggingface.co/papers/2601.05241', 'abstract': 'Visual identity prompting enhances manipulation data augmentation for robot policies by providing explicit visual guidance to diffusion models, improving policy performance in both simulation and real-world settings.  \t\t\t\t\tAI-generated summary \t\t\t\t The diversity, quantity, and quality of manipulation data are critical for training effective robot policies. However, due to hardware and physical setup constraints, collecting large-scale real-world manipulation data remains difficult to scale across diverse environments. Recent work uses text-prompt conditioned image diffusion models to augment manipulation data by altering the backgrounds and tabletop objects in the visual observations. However, these approaches often overlook the practical need for multi-view and temporally coherent observations required by state-of-the-art policy models. Further, text prompts alone cannot reliably specify the scene setup. To provide the diffusion model with explicit visual guidance, we introduce visual identity prompting, which supplies exemplar images as conditioning inputs to guide the generation of the desired scene setup. To this end, we also build a scalable pipeline to curate a visual identity pool from large robotics datasets. Using our augmented manipulation data to train downstream vision-language-action and visuomotor policy models yields consistent performance gains in both simulation and real-robot settings.', 'score': 22, 'issue_id': 494, 'pub_date': '2026-01-08', 'pub_date_card': {'ru': '8 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 8', 'zh': '1æœˆ8æ—¥'}, 'hash': '92a140fc7f66033d', 'authors': ['Boyang Wang', 'Haoran Zhang', 'Shujie Zhang', 'Jinkun Hao', 'Mingda Jia', 'Qi Lv', 'Yucheng Mao', 'Zhaoyang Lyu', 'Jia Zeng', 'Xudong Xu', 'Jiangmiao Pang'], 'affiliations': ['Shanghai AI Laboratory', 'Shanghai Jiao Tong University', 'Tsinghua University', 'University of Michigan'], 'pdf_title_img': 'assets/pdf/title_img/2601.05241.jpg', 'data': {'categories': ['#diffusion', '#robotics', '#multimodal', '#training', '#synthetic', '#data', '#cv'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¸Ğ½Ğ³ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¸Ğ½Ğ³Ğ° Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ². Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº, Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¾Ğ¼ ÑÑ†ĞµĞ½. ĞĞ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¿ÑƒĞ»Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸Ğ· Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ°Ğº Ğ² ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸, Ñ‚Ğ°Ğº Ğ¸ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ñ….'}, 'en': {'title': 'Enhancing Robot Policies with Visual Identity Prompting', 'desc': 'This paper introduces a method called visual identity prompting to improve data augmentation for robot manipulation tasks. By providing explicit visual examples to diffusion models, it enhances the generation of diverse and coherent training data. This approach addresses the limitations of traditional text prompts, which often fail to capture the complexity of real-world scenes. The results show that using this augmented data leads to better performance in both simulated environments and real-world robot applications.'}, 'zh': {'title': 'è§†è§‰èº«ä»½æç¤ºæå‡æœºå™¨äººç­–ç•¥è¡¨ç°', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§è§†è§‰èº«ä»½æç¤ºçš„æ–¹æ³•ï¼Œä»¥å¢å¼ºæœºå™¨äººç­–ç•¥çš„æ“ä½œæ•°æ®å¢å¼ºã€‚é€šè¿‡ä¸ºæ‰©æ•£æ¨¡å‹æä¾›æ˜ç¡®çš„è§†è§‰æŒ‡å¯¼ï¼Œæ”¹å–„äº†ç­–ç•¥åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ç¯å¢ƒä¸­çš„è¡¨ç°ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªå¯æ‰©å±•çš„ç®¡é“ï¼Œä»å¤§å‹æœºå™¨äººæ•°æ®é›†ä¸­ç­–åˆ’è§†è§‰èº«ä»½æ± ï¼Œä»¥ä¾¿ä¸ºç”Ÿæˆæ‰€éœ€åœºæ™¯è®¾ç½®æä¾›ç¤ºä¾‹å›¾åƒã€‚ä½¿ç”¨å¢å¼ºåçš„æ“ä½œæ•°æ®è®­ç»ƒè§†è§‰-è¯­è¨€-åŠ¨ä½œå’Œè§†è§‰è¿åŠ¨ç­–ç•¥æ¨¡å‹ï¼Œåœ¨æ¨¡æ‹Ÿå’ŒçœŸå®æœºå™¨äººç¯å¢ƒä¸­å‡å–å¾—äº†ä¸€è‡´çš„æ€§èƒ½æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.04767', 'title': 'AT^2PO: Agentic Turn-based Policy Optimization via Tree Search', 'url': 'https://huggingface.co/papers/2601.04767', 'abstract': 'ATÂ²PO is a unified framework for multi-turn agentic reinforcement learning that improves exploration diversity, credit assignment, and policy optimization through tree search and turn-level learning objectives.  \t\t\t\t\tAI-generated summary \t\t\t\t LLM agents have emerged as powerful systems for tackling multi-turn tasks by interleaving internal reasoning and external tool interactions. Agentic Reinforcement Learning has recently drawn significant research attention as a critical post-training paradigm to further refine these capabilities. In this paper, we present AT^2PO (Agentic Turn-based Policy Optimization via Tree Search), a unified framework for multi-turn agentic RL that addresses three core challenges: limited exploration diversity, sparse credit assignment, and misaligned policy optimization. AT^2PO introduces a turn-level tree structure that jointly enables Entropy-Guided Tree Expansion for strategic exploration and Turn-wise Credit Assignment for fine-grained reward propagation from sparse outcomes. Complementing this, we propose Agentic Turn-based Policy Optimization, a turn-level learning objective that aligns policy updates with the natural decision granularity of agentic interactions. ATPO is orthogonal to tree search and can be readily integrated into any multi-turn RL pipeline. Experiments across seven benchmarks demonstrate consistent improvements over the state-of-the-art baseline by up to 1.84 percentage points in average, with ablation studies validating the effectiveness of each component. Our code is available at https://github.com/zzfoutofspace/ATPO.', 'score': 22, 'issue_id': 496, 'pub_date': '2026-01-08', 'pub_date_card': {'ru': '8 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 8', 'zh': '1æœˆ8æ—¥'}, 'hash': '38825a65f1e8a007', 'authors': ['Zefang Zong', 'Dingwei Chen', 'Yang Li', 'Qi Yi', 'Bo Zhou', 'Chengming Li', 'Bo Qian', 'Peng Chen', 'Jie Jiang'], 'affiliations': ['Shenzhen MSU-BIT University', 'Sun Yat-Sen University', 'Tencent Inc'], 'pdf_title_img': 'assets/pdf/title_img/2601.04767.jpg', 'data': {'categories': ['#agents', '#open_source', '#reasoning', '#optimization', '#rl', '#training'], 'emoji': 'ğŸŒ³', 'ru': {'title': 'Ğ”Ñ€ĞµĞ²Ğ¾Ğ²Ğ¸Ğ´Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'ATÂ²PO â€” ÑÑ‚Ğ¾ ĞµĞ´Ğ¸Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹, Ñ€ĞµĞ´ĞºĞ¸Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ½ĞµÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ñ€ĞµĞ²Ğ¾Ğ²Ğ¸Ğ´Ğ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ² Ğ°Ğ³ĞµĞ½Ñ‚Ğ°, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰ÑƒÑ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾Ğµ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸ĞµĞ¹ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ ĞºÑ€ĞµĞ´Ğ¸Ñ‚Ğ° Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ» Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½ Ğ² Ğ»ÑĞ±Ğ¾Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° ÑĞµĞ¼Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ² ÑÑ€ĞµĞ´Ğ½ĞµĞ¼ Ğ½Ğ° 1,84 Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿ÑƒĞ½ĞºÑ‚Ğ° Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Enhancing Multi-Turn Learning with ATÂ²PO Framework', 'desc': 'The paper introduces ATÂ²PO, a new framework designed for multi-turn agentic reinforcement learning (RL). It tackles key challenges such as enhancing exploration diversity, improving credit assignment, and optimizing policies through a structured approach. By utilizing a turn-level tree structure, ATÂ²PO facilitates strategic exploration and precise reward propagation, which helps in better learning from sparse outcomes. The framework has shown significant performance improvements in various benchmarks, demonstrating its effectiveness in refining agentic interactions in RL tasks.'}, 'zh': {'title': 'ATÂ²POï¼šæå‡å¤šå›åˆæ™ºèƒ½å¼ºåŒ–å­¦ä¹ çš„ç»Ÿä¸€æ¡†æ¶', 'desc': 'ATÂ²POæ˜¯ä¸€ç§ç»Ÿä¸€çš„å¤šå›åˆæ™ºèƒ½å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡æ ‘æœç´¢å’Œå›åˆçº§å­¦ä¹ ç›®æ ‡æ¥æ”¹å–„æ¢ç´¢å¤šæ ·æ€§ã€ä¿¡ç”¨åˆ†é…å’Œç­–ç•¥ä¼˜åŒ–ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†å›åˆçº§æ ‘ç»“æ„ï¼Œç»“åˆç†µå¼•å¯¼çš„æ ‘æ‰©å±•å’Œå›åˆä¿¡ç”¨åˆ†é…ï¼Œä¿ƒè¿›äº†ç¨€ç–ç»“æœçš„å¥–åŠ±ä¼ æ’­ã€‚ATÂ²POçš„ç­–ç•¥ä¼˜åŒ–ä¸æ™ºèƒ½äº¤äº’çš„è‡ªç„¶å†³ç­–ç²’åº¦ç›¸ä¸€è‡´ï¼Œä»è€Œæé«˜äº†å­¦ä¹ æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒATÂ²POåœ¨ä¸ƒä¸ªåŸºå‡†æµ‹è¯•ä¸­ç›¸è¾ƒäºç°æœ‰æœ€ä¼˜åŸºçº¿æœ‰æ˜¾è‘—æå‡ï¼ŒéªŒè¯äº†å„ä¸ªç»„ä»¶çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.21815', 'title': 'Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models', 'url': 'https://huggingface.co/papers/2512.21815', 'abstract': 'Selective adversarial attacks targeting high-entropy tokens in vision-language models achieve significant semantic degradation with reduced budgets and demonstrate transferable vulnerabilities across different architectures.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-language models (VLMs) achieve remarkable performance but remain vulnerable to adversarial attacks. Entropy, a measure of model uncertainty, is strongly correlated with the reliability of VLM. Prior entropy-based attacks maximize uncertainty at all decoding steps, implicitly assuming that every token contributes equally to generation instability. We show instead that a small fraction (about 20%) of high-entropy tokens, i.e., critical decision points in autoregressive generation, disproportionately governs output trajectories. By concentrating adversarial perturbations on these positions, we achieve semantic degradation comparable to global methods while using substantially smaller budgets. More importantly, across multiple representative VLMs, such selective attacks convert 35-49% of benign outputs into harmful ones, exposing a more critical safety risk. Remarkably, these vulnerable high-entropy forks recur across architecturally diverse VLMs, enabling feasible transferability (17-26% harmful rates on unseen targets). Motivated by these findings, we propose Entropy-bank Guided Adversarial attacks (EGA), which achieves competitive attack success rates (93-95%) alongside high harmful conversion, thereby revealing new weaknesses in current VLM safety mechanisms.', 'score': 19, 'issue_id': 501, 'pub_date': '2026-12-26', 'pub_date_card': {'ru': '26 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 26', 'zh': '12æœˆ26æ—¥'}, 'hash': '79a1c8bb88f5fe0c', 'authors': ['Mengqi He', 'Xinyu Tian', 'Xin Shen', 'Jinhong Ni', 'Shu Zou', 'Zhaoyuan Yang', 'Jing Zhang'], 'affiliations': ['Australian National University', 'GE Research', 'The University of Queensland'], 'pdf_title_img': 'assets/pdf/title_img/2512.21815.jpg', 'data': {'categories': [], 'emoji': 'ğŸ¯', 'ru': {'title': 'Ğ˜Ğ·Ğ±Ğ¸Ñ€Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ°Ñ‚Ğ°ĞºĞ¸ Ğ½Ğ° ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚Ñ‹ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ÑÑ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğµ Ğ°Ğ´Ğ²ĞµÑ€ÑĞ°Ñ€Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ°Ñ‚Ğ°ĞºĞ¸, ÑĞ¾ÑÑ€ĞµĞ´Ğ¾Ñ‚Ğ¾Ñ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ñ… Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸ĞµĞ¹ - ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚Ğ¾Ñ‡ĞºĞ°Ñ… Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ² Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ²ÑĞµĞ³Ğ¾ 20% Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ÑŒÑˆĞµĞ¹ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ½ĞµĞ¿Ñ€Ğ¾Ğ¿Ğ¾Ñ€Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ²Ğ»Ğ¸ÑÑÑ‚ Ğ½Ğ° Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¸Ñ… Ğ¿ĞµÑ€Ñ‚ÑƒÑ€Ğ±Ğ°Ñ†Ğ¸Ñ Ğ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ° Ğ´Ğ»Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ EGA (Entropy-bank Guided Adversarial attacks) Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ 93-95% ÑƒÑĞ¿ĞµÑ…Ğ° Ğ°Ñ‚Ğ°ĞºĞ¸ Ğ¿Ñ€Ğ¸ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğµ 35-49% Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ² Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ñ‹Ğµ Ñ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğ¼ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğ¾Ğ¼ Ğ²Ğ¾Ğ·Ğ¼ÑƒÑ‰ĞµĞ½Ğ¸Ğ¹. ĞšÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸, ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ñ‹Ğµ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑÑÑ‚ÑÑ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ñ‚Ñ€Ğ°Ğ½ÑÑ„ĞµÑ€Ğ°Ğ±ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ°Ñ‚Ğ°Ğº (17-26% Ğ½Ğ° Ğ½ĞµĞ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ†ĞµĞ»ÑÑ…).'}, 'en': {'title': 'Targeting Uncertainty: A New Approach to Adversarial Attacks on VLMs', 'desc': "This paper discusses a new method for attacking vision-language models (VLMs) by focusing on high-entropy tokens, which are points of high uncertainty in the model's output. Instead of spreading adversarial changes across all tokens, the authors show that targeting just 20% of these critical tokens can lead to significant semantic degradation with less effort. Their approach, called Entropy-bank Guided Adversarial attacks (EGA), achieves high success rates in converting benign outputs into harmful ones, revealing vulnerabilities in various VLM architectures. This research highlights the importance of understanding model uncertainty and its implications for the safety of AI systems."}, 'zh': {'title': 'é«˜ç†µæ ‡è®°çš„é€‰æ‹©æ€§å¯¹æŠ—æ”»å‡»æ­ç¤ºVLMçš„æ–°è„†å¼±æ€§', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åœ¨é¢å¯¹å¯¹æŠ—æ”»å‡»æ—¶çš„è„†å¼±æ€§ã€‚æˆ‘ä»¬å‘ç°ï¼Œé«˜ç†µæ ‡è®°ï¼Œå³æ¨¡å‹ä¸ç¡®å®šæ€§è¾ƒé«˜çš„æ ‡è®°ï¼Œèƒ½å¤Ÿæ˜¾è‘—å½±å“ç”Ÿæˆç»“æœã€‚é€šè¿‡é›†ä¸­å¯¹è¿™äº›é«˜ç†µæ ‡è®°çš„å¯¹æŠ—æ‰°åŠ¨ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨è¾ƒå°çš„é¢„ç®—ä¸‹å®ç°ä¸å…¨å±€æ–¹æ³•ç›¸å½“çš„è¯­ä¹‰é™çº§ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œè¿™ç§é€‰æ‹©æ€§æ”»å‡»åœ¨ä¸åŒçš„VLMæ¶æ„ä¸­å…·æœ‰å¯è½¬ç§»æ€§ï¼Œæš´éœ²äº†å½“å‰å®‰å…¨æœºåˆ¶çš„æ–°å¼±ç‚¹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.05138', 'title': 'VerseCrafter: Dynamic Realistic Video World Model with 4D Geometric Control', 'url': 'https://huggingface.co/papers/2601.05138', 'abstract': "VerseCrafter is a 4D-aware video world model that enables unified control over camera and object dynamics through 4D geometric control representation and video diffusion models.  \t\t\t\t\tAI-generated summary \t\t\t\t Video world models aim to simulate dynamic, real-world environments, yet existing methods struggle to provide unified and precise control over camera and multi-object motion, as videos inherently operate dynamics in the projected 2D image plane. To bridge this gap, we introduce VerseCrafter, a 4D-aware video world model that enables explicit and coherent control over both camera and object dynamics within a unified 4D geometric world state. Our approach is centered on a novel 4D Geometric Control representation, which encodes the world state through a static background point cloud and per-object 3D Gaussian trajectories. This representation captures not only an object's path but also its probabilistic 3D occupancy over time, offering a flexible, category-agnostic alternative to rigid bounding boxes or parametric models. These 4D controls are rendered into conditioning signals for a pretrained video diffusion model, enabling the generation of high-fidelity, view-consistent videos that precisely adhere to the specified dynamics. Unfortunately, another major challenge lies in the scarcity of large-scale training data with explicit 4D annotations. We address this by developing an automatic data engine that extracts the required 4D controls from in-the-wild videos, allowing us to train our model on a massive and diverse dataset.", 'score': 14, 'issue_id': 492, 'pub_date': '2026-01-08', 'pub_date_card': {'ru': '8 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 8', 'zh': '1æœˆ8æ—¥'}, 'hash': '4e9d7d654118b517', 'authors': ['Sixiao Zheng', 'Minghao Yin', 'Wenbo Hu', 'Xiaoyu Li', 'Ying Shan', 'Yanwei Fu'], 'affiliations': ['ARC Lab, Tencent PCG', 'Fudan University', 'HKU', 'Shanghai Innovation Institute'], 'pdf_title_img': 'assets/pdf/title_img/2601.05138.jpg', 'data': {'categories': ['#3d', '#dataset', '#data', '#video', '#multimodal'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ•Ğ´Ğ¸Ğ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ĞºĞ°Ğ¼ĞµÑ€Ğ¾Ğ¹ Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµÑ€ĞµĞ· 4D Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ', 'desc': 'VerseCrafter Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ 4D-Ğ¾ÑĞ²ĞµĞ´Ğ¾Ğ¼Ğ»Ñ‘Ğ½Ğ½ÑƒÑ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ¸Ñ€Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºÑƒ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ 4D Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ. Ğ’ Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ»ĞµĞ¶Ğ¸Ñ‚ Ğ½Ğ¾Ğ²Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ 4D Geometric Control, ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒÑÑ‰ĞµĞµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ Ğ¼Ğ¸Ñ€Ğ° Ñ‡ĞµÑ€ĞµĞ· ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¾Ğ±Ğ»Ğ°ĞºĞ¾ Ñ‚Ğ¾Ñ‡ĞµĞº Ñ„Ğ¾Ğ½Ğ° Ğ¸ 3D Gaussian Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ³Ğ¸Ğ±ĞºÑƒÑ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ñƒ Ğ¶Ñ‘ÑÑ‚ĞºĞ¸Ğ¼ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğ¼ Ñ€Ğ°Ğ¼ĞºĞ°Ğ¼. Ğ­Ñ‚Ğ¸ 4D ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒÑÑ‚ÑÑ Ğ² ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ ĞºĞ¾Ğ½Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ²Ğ¸Ğ´ĞµĞ¾ diffusion Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°, ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¾Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€ĞµÑˆĞ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ´ĞµÑ„Ğ¸Ñ†Ğ¸Ñ‚Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¾Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ÑÑ‰ĞµĞ³Ğ¾ 4D ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ· Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ğ´Ğ¸ĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¸Ñ€Ğ¾Ğ´Ğµ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ¼ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Unified 4D Control for Dynamic Video Generation', 'desc': 'VerseCrafter is a novel 4D-aware video world model designed to enhance control over camera and object movements in video generation. It utilizes a unique 4D Geometric Control representation that captures the dynamic state of the environment through a combination of static point clouds and 3D Gaussian trajectories for each object. This approach allows for flexible and precise modeling of object dynamics, moving beyond traditional bounding boxes. To overcome the challenge of limited training data, VerseCrafter includes an automatic data engine that extracts necessary 4D controls from existing videos, enabling the model to learn from a large and varied dataset.'}, 'zh': {'title': 'ç»Ÿä¸€æ§åˆ¶ç›¸æœºä¸ç‰©ä½“åŠ¨æ€çš„4Dè§†é¢‘æ¨¡å‹', 'desc': 'VerseCrafter æ˜¯ä¸€ç§4Dæ„ŸçŸ¥çš„è§†é¢‘ä¸–ç•Œæ¨¡å‹ï¼Œèƒ½å¤Ÿé€šè¿‡4Då‡ ä½•æ§åˆ¶è¡¨ç¤ºå’Œè§†é¢‘æ‰©æ•£æ¨¡å‹ç»Ÿä¸€æ§åˆ¶ç›¸æœºå’Œç‰©ä½“çš„åŠ¨æ€ã€‚ç°æœ‰æ–¹æ³•åœ¨æä¾›ç›¸æœºå’Œå¤šç‰©ä½“è¿åŠ¨çš„ç»Ÿä¸€ç²¾ç¡®æ§åˆ¶æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œè€ŒVerseCrafteré€šè¿‡é™æ€èƒŒæ™¯ç‚¹äº‘å’Œæ¯ä¸ªç‰©ä½“çš„3Dé«˜æ–¯è½¨è¿¹æ¥ç¼–ç ä¸–ç•ŒçŠ¶æ€ï¼Œè§£å†³äº†è¿™ä¸€é—®é¢˜ã€‚è¯¥æ¨¡å‹ä¸ä»…æ•æ‰ç‰©ä½“çš„è¿åŠ¨è·¯å¾„ï¼Œè¿˜èƒ½è¡¨ç¤ºå…¶éšæ—¶é—´å˜åŒ–çš„3Då ç”¨æ¦‚ç‡ï¼Œæä¾›äº†ä¸€ç§çµæ´»çš„æ›¿ä»£æ–¹æ¡ˆã€‚æˆ‘ä»¬è¿˜å¼€å‘äº†ä¸€ä¸ªè‡ªåŠ¨æ•°æ®å¼•æ“ï¼Œä»é‡å¤–è§†é¢‘ä¸­æå–æ‰€éœ€çš„4Dæ§åˆ¶ï¼Œä»¥ä¾¿åœ¨å¤§è§„æ¨¡å¤šæ ·åŒ–çš„æ•°æ®é›†ä¸Šè®­ç»ƒæ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.05111', 'title': 'Agent-as-a-Judge', 'url': 'https://huggingface.co/papers/2601.05111', 'abstract': 'Large language models face limitations in evaluating complex, multi-step tasks, prompting the development of agent-based evaluation systems that utilize planning, tool-augmented verification, and multi-agent collaboration for more robust assessments.  \t\t\t\t\tAI-generated summary \t\t\t\t LLM-as-a-Judge has revolutionized AI evaluation by leveraging large language models for scalable assessments. However, as evaluands become increasingly complex, specialized, and multi-step, the reliability of LLM-as-a-Judge has become constrained by inherent biases, shallow single-pass reasoning, and the inability to verify assessments against real-world observations. This has catalyzed the transition to Agent-as-a-Judge, where agentic judges employ planning, tool-augmented verification, multi-agent collaboration, and persistent memory to enable more robust, verifiable, and nuanced evaluations. Despite the rapid proliferation of agentic evaluation systems, the field lacks a unified framework to navigate this shifting landscape. To bridge this gap, we present the first comprehensive survey tracing this evolution. Specifically, we identify key dimensions that characterize this paradigm shift and establish a developmental taxonomy. We organize core methodologies and survey applications across general and professional domains. Furthermore, we analyze frontier challenges and identify promising research directions, ultimately providing a clear roadmap for the next generation of agentic evaluation.', 'score': 12, 'issue_id': 492, 'pub_date': '2026-01-08', 'pub_date_card': {'ru': '8 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 8', 'zh': '1æœˆ8æ—¥'}, 'hash': '67cf270baff78987', 'authors': ['Runyang You', 'Hongru Cai', 'Caiqi Zhang', 'Qiancheng Xu', 'Meng Liu', 'Tiezheng Yu', 'Yongqi Li', 'Wenjie Li'], 'affiliations': ['Huawei Technologies', 'Shandong Jianzhu University', 'The Hong Kong Polytechnic University', 'University of Cambridge'], 'pdf_title_img': 'assets/pdf/title_img/2601.05111.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#agents', '#survey'], 'emoji': 'âš–ï¸', 'ru': {'title': 'ĞÑ‚ ÑÑƒĞ´ÑŒĞ¸-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº ÑÑƒĞ´ÑŒĞµ-Ğ°Ğ³ĞµĞ½Ñ‚Ñƒ: Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ² AI', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ² Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğµ Ğ¾Ñ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ ÑÑƒĞ´ĞµĞ¹ Ğº Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¼ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼. Ğ¢Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ LLM-as-a-Judge Ğ¸Ğ¼ĞµĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸Ğ·-Ğ·Ğ° Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Agent-as-a-Judge â€” ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ, Ğ³Ğ´Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸, ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ¸ Ğ´Ğ¾Ğ»Ğ³Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸. Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¿ĞµÑ€Ğ²Ğ°Ñ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ°Ñ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ Ğ¸ Ğ´Ğ¾Ñ€Ğ¾Ğ¶Ğ½Ğ°Ñ ĞºĞ°Ñ€Ñ‚Ğ° Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ….'}, 'en': {'title': 'From LLMs to Agentic Evaluation: A New Era in AI Assessment', 'desc': 'This paper discusses the limitations of large language models (LLMs) in evaluating complex tasks, which has led to the creation of agent-based evaluation systems. These systems enhance assessment reliability by incorporating planning, tool-augmented verification, and collaboration among multiple agents. The authors present a comprehensive survey that outlines the evolution from LLM-as-a-Judge to Agent-as-a-Judge, highlighting key dimensions and methodologies in this transition. Additionally, they identify challenges and future research directions to guide the development of more effective agentic evaluation systems.'}, 'zh': {'title': 'ä»£ç†è¯„ä¼°ï¼šè¿ˆå‘æ›´ç¨³å¥çš„AIè¯„ä¼°ç³»ç»Ÿ', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è¯„ä¼°å¤æ‚çš„å¤šæ­¥éª¤ä»»åŠ¡æ—¶å­˜åœ¨å±€é™æ€§ï¼Œå› æ­¤å¼€å‘äº†åŸºäºä»£ç†çš„è¯„ä¼°ç³»ç»Ÿã€‚è¿™äº›ç³»ç»Ÿåˆ©ç”¨è§„åˆ’ã€å·¥å…·å¢å¼ºéªŒè¯å’Œå¤šä»£ç†åä½œæ¥è¿›è¡Œæ›´ç¨³å¥çš„è¯„ä¼°ã€‚å°½ç®¡ä»£ç†è¯„ä¼°ç³»ç»Ÿè¿…é€Ÿå‘å±•ï¼Œä½†è¯¥é¢†åŸŸç¼ºä¹ç»Ÿä¸€çš„æ¡†æ¶æ¥åº”å¯¹è¿™ä¸€å˜åŒ–ã€‚æœ¬æ–‡æä¾›äº†é¦–æ¬¡å…¨é¢çš„è°ƒæŸ¥ï¼Œè¯†åˆ«äº†è¿™ä¸€èŒƒå¼è½¬å˜çš„å…³é”®ç»´åº¦ï¼Œå¹¶å»ºç«‹äº†å‘å±•åˆ†ç±»æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.03425', 'title': 'The Illusion of Specialization: Unveiling the Domain-Invariant "Standing Committee" in Mixture-of-Experts Models', 'url': 'https://huggingface.co/papers/2601.03425', 'abstract': "Research challenges the assumption of domain specialization in Mixture of Experts models by identifying a persistent central committee of experts that dominates routing behavior across different domains and architectures.  \t\t\t\t\tAI-generated summary \t\t\t\t Mixture of Experts models are widely assumed to achieve domain specialization through sparse routing. In this work, we question this assumption by introducing COMMITTEEAUDIT, a post hoc framework that analyzes routing behavior at the level of expert groups rather than individual experts. Across three representative models and the MMLU benchmark, we uncover a domain-invariant Standing Committee. This is a compact coalition of routed experts that consistently captures the majority of routing mass across domains, layers, and routing budgets, even when architectures already include shared experts. Qualitative analysis further shows that Standing Committees anchor reasoning structure and syntax, while peripheral experts handle domain-specific knowledge. These findings reveal a strong structural bias toward centralized computation, suggesting that specialization in Mixture of Experts models is far less pervasive than commonly believed. This inherent bias also indicates that current training objectives, such as load-balancing losses that enforce uniform expert utilization, may be working against the model's natural optimization path, thereby limiting training efficiency and performance.", 'score': 12, 'issue_id': 492, 'pub_date': '2026-01-06', 'pub_date_card': {'ru': '6 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 6', 'zh': '1æœˆ6æ—¥'}, 'hash': '102f6999339b81a5', 'authors': ['Yan Wang', 'Yitao Xu', 'Nanhan Shen', 'Jinyan Su', 'Jimin Huang', 'Zining Zhu'], 'affiliations': ['Cornell University', 'Georgia Institute of Technology', 'Stevens Institute of Technology', 'The Fin AI'], 'pdf_title_img': 'assets/pdf/title_img/2601.03425.jpg', 'data': {'categories': ['#training', '#benchmark', '#interpretability', '#reasoning', '#architecture'], 'emoji': 'ğŸ¤', 'ru': {'title': 'Ğ¡ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ñ†ĞµĞ½Ñ‚Ñ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ² Mixture of Experts: Ñ€Ğ¾Ğ»ÑŒ Ğ¿Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ¼Ğ¸Ñ‚ĞµÑ‚Ğ° ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ²', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¾ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Mixture of Experts (MoE) Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ¿Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ½Ğ°Ñ Ñ†ĞµĞ½Ñ‚Ñ€Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ° ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ´Ğ¾Ğ¼Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ Ğ² Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ Ğ¾Ñ‚ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ COMMITTEEAUDIT â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ³Ñ€ÑƒĞ¿Ğ¿ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ². ĞĞ° Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğµ Ñ‚Ñ€Ñ‘Ñ… Ñ€ĞµĞ¿Ñ€ĞµĞ·ĞµĞ½Ñ‚Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ° MMLU Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ÑÑ ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ĞŸĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ½Ğ¾Ğ³Ğ¾ ĞšĞ¾Ğ¼Ğ¸Ñ‚ĞµÑ‚Ğ° â€” ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ°Ğ»Ğ¸Ñ†Ğ¸Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğ¾ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ°ÑÑÑ‹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ° ÑĞ¸Ğ»ÑŒĞ½Ğ¾Ğµ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ğµ Ğ² ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñƒ Ñ†ĞµĞ½Ñ‚Ñ€Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ğµ Ñ†ĞµĞ»Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ½Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸, Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¿Ñ€ĞµĞ¿ÑÑ‚ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Rethinking Specialization in Mixture of Experts Models', 'desc': "This paper challenges the belief that Mixture of Experts (MoE) models specialize in different domains through sparse routing of experts. The authors introduce a framework called COMMITTEEAUDIT, which examines how groups of experts, rather than individual ones, influence routing decisions. They discover a consistent group of experts, termed the Standing Committee, that dominates routing across various domains and architectures. This suggests that the expected specialization in MoE models is less significant than previously thought, and current training methods may hinder the model's performance by not aligning with its natural optimization tendencies."}, 'zh': {'title': 'æ··åˆä¸“å®¶æ¨¡å‹ä¸­çš„ä¸­å¤®å§”å‘˜ä¼šå½±å“ä¸“ä¸šåŒ–', 'desc': 'è¿™ç¯‡è®ºæ–‡æŒ‘æˆ˜äº†æ··åˆä¸“å®¶æ¨¡å‹ä¸­é¢†åŸŸä¸“ä¸šåŒ–çš„å‡è®¾ï¼ŒæŒ‡å‡ºåœ¨ä¸åŒé¢†åŸŸå’Œæ¶æ„ä¸­å­˜åœ¨ä¸€ä¸ªæŒç»­çš„ä¸­å¤®å§”å‘˜ä¼šä¸“å®¶ï¼Œä¸»å¯¼ç€è·¯ç”±è¡Œä¸ºã€‚ç ”ç©¶å¼•å…¥äº†COMMITTEEAUDITæ¡†æ¶ï¼Œåˆ†æä¸“å®¶ç»„çš„è·¯ç”±è¡Œä¸ºï¼Œè€Œä¸æ˜¯å•ä¸ªä¸“å®¶ã€‚é€šè¿‡å¯¹ä¸‰ç§ä»£è¡¨æ€§æ¨¡å‹å’ŒMMLUåŸºå‡†çš„ç ”ç©¶ï¼Œå‘ç°äº†ä¸€ä¸ªé¢†åŸŸä¸å˜çš„å¸¸è®¾å§”å‘˜ä¼šï¼Œè¿™ä¸ªå§”å‘˜ä¼šåœ¨ä¸åŒé¢†åŸŸã€å±‚æ¬¡å’Œè·¯ç”±é¢„ç®—ä¸­å§‹ç»ˆå æ®ä¸»è¦çš„è·¯ç”±è´¨é‡ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæ··åˆä¸“å®¶æ¨¡å‹ä¸­çš„ä¸“ä¸šåŒ–ç¨‹åº¦è¿œä½äºæ™®éè®¤ä¸ºçš„æ°´å¹³ï¼Œå½“å‰çš„è®­ç»ƒç›®æ ‡å¯èƒ½ä¼šé™åˆ¶æ¨¡å‹çš„è®­ç»ƒæ•ˆç‡å’Œæ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.05172', 'title': 'CoV: Chain-of-View Prompting for Spatial Reasoning', 'url': 'https://huggingface.co/papers/2601.05172', 'abstract': 'Chain-of-View prompting enables vision-language models to actively explore 3D environments by selecting question-aligned views and iteratively adjusting camera positions to improve spatial reasoning in embodied question answering.  \t\t\t\t\tAI-generated summary \t\t\t\t Embodied question answering (EQA) in 3D environments often requires collecting context that is distributed across multiple viewpoints and partially occluded. However, most recent vision--language models (VLMs) are constrained to a fixed and finite set of input views, which limits their ability to acquire question-relevant context at inference time and hinders complex spatial reasoning. We propose Chain-of-View (CoV) prompting, a training-free, test-time reasoning framework that transforms a VLM into an active viewpoint reasoner through a coarse-to-fine exploration process. CoV first employs a View Selection agent to filter redundant frames and identify question-aligned anchor views. It then performs fine-grained view adjustment by interleaving iterative reasoning with discrete camera actions, obtaining new observations from the underlying 3D scene representation until sufficient context is gathered or a step budget is reached.   We evaluate CoV on OpenEQA across four mainstream VLMs and obtain an average +11.56\\% improvement in LLM-Match, with a maximum gain of +13.62\\% on Qwen3-VL-Flash. CoV further exhibits test-time scaling: increasing the minimum action budget yields an additional +2.51\\% average improvement, peaking at +3.73\\% on Gemini-2.5-Flash. On ScanQA and SQA3D, CoV delivers strong performance (e.g., 116 CIDEr / 31.9 EM@1 on ScanQA and 51.1 EM@1 on SQA3D). Overall, these results suggest that question-aligned view selection coupled with open-view search is an effective, model-agnostic strategy for improving spatial reasoning in 3D EQA without additional training.', 'score': 8, 'issue_id': 502, 'pub_date': '2026-01-08', 'pub_date_card': {'ru': '8 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 8', 'zh': '1æœˆ8æ—¥'}, 'hash': '30169d7a3fe8549f', 'authors': ['Haoyu Zhao', 'Akide Liu', 'Zeyu Zhang', 'Weijie Wang', 'Feng Chen', 'Ruihan Zhu', 'Gholamreza Haffari', 'Bohan Zhuang'], 'affiliations': ['AIML, Adelaide University', 'Monash University', 'ZIP Lab, Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2601.05172.jpg', 'data': {'categories': ['#benchmark', '#multimodal', '#agents', '#3d', '#reasoning'], 'emoji': 'ğŸ”', 'ru': {'title': 'ĞĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ 3D ÑÑ†ĞµĞ½ Ñ‡ĞµÑ€ĞµĞ· Ñ†ĞµĞ»ĞµĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ Ñ‚Ğ¾Ñ‡ĞµĞº Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ°', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Chain-of-View prompting, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ñ‹Ğµ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ, Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°Ñ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ñ€Ğ°ĞºÑƒÑ€ÑÑ‹ Ğ¸ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€ÑƒÑ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ ĞºĞ°Ğ¼ĞµÑ€Ñ‹. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ´Ğ»Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ²Ğ¸Ğ´Ğ¾Ğ², Ğ¾Ñ‚Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ¾Ğ²Ñ‹Ğ²Ğ°Ñ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ ĞºĞ°Ğ´Ñ€Ñ‹, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ñ‚Ğ¾Ğ½ĞºÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ñ‡ĞµÑ€ĞµĞ· Ñ‡ĞµÑ€ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑĞ¼Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ ĞºĞ°Ğº Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ° 11.56% Ğ² ÑÑ€ĞµĞ´Ğ½ĞµĞ¼ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ OpenEQA. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ¼ Ğ²Ñ‹Ğ±Ğ¾Ñ€ Ğ²Ğ¸Ğ´Ğ¾Ğ² ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰Ñ‘Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ² 3D Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸ÑÑ….'}, 'en': {'title': 'Enhancing Spatial Reasoning in 3D with Chain-of-View Prompting', 'desc': 'This paper introduces Chain-of-View (CoV) prompting, a novel approach that enhances vision-language models (VLMs) for embodied question answering in 3D environments. CoV allows models to dynamically select and adjust camera views to gather relevant context, overcoming limitations of fixed input views. By employing a View Selection agent and iterative reasoning, CoV improves spatial reasoning capabilities without requiring additional training. The results show significant performance gains across various benchmarks, demonstrating the effectiveness of this model-agnostic strategy.'}, 'zh': {'title': 'é“¾è§†æç¤ºï¼šæå‡ä¸‰ç»´ç©ºé—´æ¨ç†çš„æœ‰æ•ˆç­–ç•¥', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºé“¾è§†æç¤ºï¼ˆChain-of-Viewï¼ŒCoVï¼‰çš„æ–¹æ³•ï¼Œæ—¨åœ¨æå‡è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ä¸‰ç»´ç¯å¢ƒä¸­çš„ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚CoVé€šè¿‡é€‰æ‹©ä¸é—®é¢˜ç›¸å…³çš„è§†è§’å¹¶è¿­ä»£è°ƒæ•´ç›¸æœºä½ç½®ï¼Œå¸®åŠ©æ¨¡å‹ä¸»åŠ¨æ¢ç´¢ç¯å¢ƒï¼Œä»è€Œæ”¶é›†åˆ†æ•£åœ¨å¤šä¸ªè§†ç‚¹çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚è¯¥æ–¹æ³•ä¸éœ€è¦é¢å¤–çš„è®­ç»ƒï¼Œè€Œæ˜¯é€šè¿‡ç²—åˆ°ç»†çš„æ¢ç´¢è¿‡ç¨‹ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨æ¨ç†æ—¶è·å–æ›´å¤šç›¸å…³ä¿¡æ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCoVåœ¨å¤šä¸ªä¸»æµè§†è§‰è¯­è¨€æ¨¡å‹ä¸Šå‡æ˜¾è‘—æé«˜äº†è¡¨ç°ï¼Œè¯æ˜äº†å…¶åœ¨ä¸‰ç»´ç¯å¢ƒä¸­è¿›è¡Œé—®é¢˜å›ç­”çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.03559', 'title': 'DiffCoT: Diffusion-styled Chain-of-Thought Reasoning in LLMs', 'url': 'https://huggingface.co/papers/2601.03559', 'abstract': 'DiffCoT reformulates chain-of-thought reasoning as an iterative denoising process using diffusion principles, enabling unified generation and correction of intermediate steps while maintaining causal consistency.  \t\t\t\t\tAI-generated summary \t\t\t\t Chain-of-Thought (CoT) reasoning improves multi-step mathematical problem solving in large language models but remains vulnerable to exposure bias and error accumulation, as early mistakes propagate irreversibly through autoregressive decoding. In this work, we propose DiffCoT, a diffusion-styled CoT framework that reformulates CoT reasoning as an iterative denoising process. DiffCoT integrates diffusion principles at the reasoning-step level via a sliding-window mechanism, enabling unified generation and retrospective correction of intermediate steps while preserving token-level autoregression. To maintain causal consistency, we further introduce a causal diffusion noise schedule that respects the temporal structure of reasoning chains. Extensive experiments on three multi-step CoT reasoning benchmarks across diverse model backbones demonstrate that DiffCoT consistently outperforms existing CoT preference optimization methods, yielding improved robustness and error-correction capability in CoT reasoning.', 'score': 8, 'issue_id': 492, 'pub_date': '2026-01-07', 'pub_date_card': {'ru': '7 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 7', 'zh': '1æœˆ7æ—¥'}, 'hash': 'add13e4a635a0515', 'authors': ['Shidong Cao', 'Hongzhan Lin', 'Yuxuan Gu', 'Ziyang Luo', 'Jing Ma'], 'affiliations': ['Harbin Institute of Technology', 'Hong Kong Baptist University'], 'pdf_title_img': 'assets/pdf/title_img/2601.03559.jpg', 'data': {'categories': ['#training', '#diffusion', '#math', '#reasoning', '#architecture'], 'emoji': 'ğŸ”„', 'ru': {'title': 'Ğ¦ĞµĞ¿Ğ¾Ñ‡ĞºĞ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ĞºĞ°Ğº Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ñ‡Ğ¸ÑÑ‚ĞºĞ¸ Ğ¾Ñ‚ ÑˆÑƒĞ¼Ğ°', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ DiffCoT â€” Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ĞºĞ°Ğº Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ ÑˆÑƒĞ¼Ğ°, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ° Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ°Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¸, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ ÑˆĞ°Ğ³Ğ¸, Ğ½Ğ¾ Ğ¸ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑ‚ÑŒ Ğ¸Ñ… Ñ€ĞµÑ‚Ñ€Ğ¾ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ ÑĞºĞ¾Ğ»ÑŒĞ·ÑÑ‰ĞµĞµ Ğ¾ĞºĞ½Ğ¾ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑˆĞ°Ğ³Ğ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ ÑˆÑƒĞ¼Ğ° Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ DiffCoT Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ² Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ»ÑƒÑ‡ÑˆÑƒÑ Ñ€Ğ¾Ğ±ÑƒÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº.'}, 'en': {'title': 'DiffCoT: Enhancing Reasoning with Diffusion Principles', 'desc': 'This paper introduces DiffCoT, a novel framework that enhances chain-of-thought (CoT) reasoning in large language models by applying diffusion principles. It reformulates CoT reasoning as an iterative denoising process, allowing for the generation and correction of intermediate reasoning steps while maintaining causal consistency. The framework employs a sliding-window mechanism to integrate diffusion at the reasoning-step level, which helps to mitigate issues like exposure bias and error accumulation. Experimental results show that DiffCoT outperforms traditional CoT optimization methods, demonstrating better robustness and error correction in multi-step reasoning tasks.'}, 'zh': {'title': 'DiffCoTï¼šæå‡é“¾å¼æ€ç»´æ¨ç†çš„é²æ£’æ€§ä¸å‡†ç¡®æ€§', 'desc': 'DiffCoTå°†é“¾å¼æ€ç»´æ¨ç†é‡æ–°æ„å»ºä¸ºä¸€ä¸ªè¿­ä»£å»å™ªè¿‡ç¨‹ï¼Œåˆ©ç”¨æ‰©æ•£åŸç†æ¥ç»Ÿä¸€ç”Ÿæˆå’Œä¿®æ­£ä¸­é—´æ­¥éª¤ï¼ŒåŒæ—¶ä¿æŒå› æœä¸€è‡´æ€§ã€‚ä¼ ç»Ÿçš„é“¾å¼æ€ç»´æ¨ç†åœ¨å¤šæ­¥éª¤æ•°å­¦é—®é¢˜è§£å†³ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†å®¹æ˜“å—åˆ°æ›å…‰åå·®å’Œé”™è¯¯ç´¯ç§¯çš„å½±å“ã€‚DiffCoTé€šè¿‡æ»‘åŠ¨çª—å£æœºåˆ¶åœ¨æ¨ç†æ­¥éª¤çº§åˆ«æ•´åˆæ‰©æ•£åŸç†ï¼Œä½¿å¾—ä¸­é—´æ­¥éª¤çš„ç”Ÿæˆå’Œå›é¡¾æ€§ä¿®æ­£å˜å¾—æ›´åŠ é«˜æ•ˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDiffCoTåœ¨å¤šä¸ªå¤šæ­¥éª¤æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºç°æœ‰çš„æ–¹æ³•ï¼Œå±•ç°å‡ºæ›´å¼ºçš„é²æ£’æ€§å’Œé”™è¯¯ä¿®æ­£èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.05239', 'title': 'Plenoptic Video Generation', 'url': 'https://huggingface.co/papers/2601.05239', 'abstract': 'PlenopticDreamer enables consistent multi-view video re-rendering through synchronized generative hallucinations, leveraging camera-guided retrieval and progressive training mechanisms for improved temporal coherence and visual fidelity.  \t\t\t\t\tAI-generated summary \t\t\t\t Camera-controlled generative video re-rendering methods, such as ReCamMaster, have achieved remarkable progress. However, despite their success in single-view setting, these works often struggle to maintain consistency across multi-view scenarios. Ensuring spatio-temporal coherence in hallucinated regions remains challenging due to the inherent stochasticity of generative models. To address it, we introduce PlenopticDreamer, a framework that synchronizes generative hallucinations to maintain spatio-temporal memory. The core idea is to train a multi-in-single-out video-conditioned model in an autoregressive manner, aided by a camera-guided video retrieval strategy that adaptively selects salient videos from previous generations as conditional inputs. In addition, Our training incorporates progressive context-scaling to improve convergence, self-conditioning to enhance robustness against long-range visual degradation caused by error accumulation, and a long-video conditioning mechanism to support extended video generation. Extensive experiments on the Basic and Agibot benchmarks demonstrate that PlenopticDreamer achieves state-of-the-art video re-rendering, delivering superior view synchronization, high-fidelity visuals, accurate camera control, and diverse view transformations (e.g., third-person to third-person, and head-view to gripper-view in robotic manipulation). Project page: https://research.nvidia.com/labs/dir/plenopticdreamer/', 'score': 7, 'issue_id': 493, 'pub_date': '2026-01-08', 'pub_date_card': {'ru': '8 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 8', 'zh': '1æœˆ8æ—¥'}, 'hash': 'e200e4f57b8dacae', 'authors': ['Xiao Fu', 'Shitao Tang', 'Min Shi', 'Xian Liu', 'Jinwei Gu', 'Ming-Yu Liu', 'Dahua Lin', 'Chen-Hsuan Lin'], 'affiliations': ['Georgia Institute of Technology', 'NVIDIA', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2601.05239.jpg', 'data': {'categories': ['#multimodal', '#video', '#training'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ¡Ğ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ²Ğ¸Ğ´Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ¾Ñ‚Ñ€Ğ¸ÑĞ¾Ğ²ĞºĞ¸', 'desc': 'PlenopticDreamer â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¿ĞµÑ€ĞµĞ¾Ñ‚Ñ€Ğ¸ÑĞ¾Ğ²ĞºĞ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ ĞºĞ°Ğ¼ĞµÑ€Ğ¾Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ²Ğ¸Ğ´Ğ¾Ğ² Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ²Ğ¸Ğ´ĞµĞ¾, ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼Ğ¸ ĞºĞ°Ğ¼ĞµÑ€Ñ‹, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°Ñ‚ÑŒ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ñ„Ñ€ĞµĞ¹Ğ¼Ñ‹ Ğ¸Ğ· Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ²Ñ…Ğ¾Ğ´Ğ¾Ğ². Ğ’ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ğ½Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğº Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¿Ñ€Ğ¸ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´Ğ¾Ğ² Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ĞºĞ°Ğ¼ĞµÑ€Ğ¾Ğ¹.'}, 'en': {'title': 'Synchronized Generative Hallucinations for Multi-View Video Mastery', 'desc': 'PlenopticDreamer is a novel framework designed for multi-view video re-rendering that ensures consistent visual output across different perspectives. It utilizes synchronized generative hallucinations, which help maintain spatio-temporal coherence in the generated video content. The model employs a camera-guided retrieval system to select relevant video segments from previous generations, enhancing the quality of the output. Additionally, it incorporates progressive training techniques to improve model robustness and visual fidelity, achieving state-of-the-art results in video re-rendering tasks.'}, 'zh': {'title': 'åŒæ­¥ç”Ÿæˆå¹»è§‰ï¼Œå®ç°å¤šè§†è§’è§†é¢‘é‡æ¸²æŸ“çš„çªç ´', 'desc': 'PlenopticDreamer æ˜¯ä¸€ç§æ–°æ¡†æ¶ï¼Œèƒ½å¤Ÿé€šè¿‡åŒæ­¥ç”Ÿæˆçš„å¹»è§‰å®ç°ä¸€è‡´çš„å¤šè§†è§’è§†é¢‘é‡æ¸²æŸ“ã€‚å®ƒåˆ©ç”¨ç›¸æœºå¼•å¯¼çš„æ£€ç´¢å’Œæ¸è¿›è®­ç»ƒæœºåˆ¶ï¼Œæ”¹å–„äº†æ—¶é—´ä¸€è‡´æ€§å’Œè§†è§‰ä¿çœŸåº¦ã€‚è¯¥æ–¹æ³•é€šè¿‡è‡ªå›å½’æ–¹å¼è®­ç»ƒå¤šè¾“å…¥å•è¾“å‡ºçš„è§†é¢‘æ¡ä»¶æ¨¡å‹ï¼Œå¹¶è‡ªé€‚åº”é€‰æ‹©å‰æœŸç”Ÿæˆçš„æ˜¾è‘—è§†é¢‘ä½œä¸ºæ¡ä»¶è¾“å…¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPlenopticDreamer åœ¨è§†é¢‘é‡æ¸²æŸ“æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ï¼Œæä¾›äº†å“è¶Šçš„è§†è§’åŒæ­¥å’Œé«˜ä¿çœŸè§†è§‰æ•ˆæœã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.05124', 'title': 'Re-Align: Structured Reasoning-guided Alignment for In-Context Image Generation and Editing', 'url': 'https://huggingface.co/papers/2601.05124', 'abstract': "Re-Align addresses the gap between understanding and generation in in-context image generation and editing through structured reasoning-guided alignment and reinforcement learning training.  \t\t\t\t\tAI-generated summary \t\t\t\t In-context image generation and editing (ICGE) enables users to specify visual concepts through interleaved image-text prompts, demanding precise understanding and faithful execution of user intent. Although recent unified multimodal models exhibit promising understanding capabilities, these strengths often fail to transfer effectively to image generation. We introduce Re-Align, a unified framework that bridges the gap between understanding and generation through structured reasoning-guided alignment. At its core lies the In-Context Chain-of-Thought (IC-CoT), a structured reasoning paradigm that decouples semantic guidance and reference association, providing clear textual target and mitigating confusion among reference images. Furthermore, Re-Align introduces an effective RL training scheme that leverages a surrogate reward to measure the alignment between structured reasoning text and the generated image, thereby improving the model's overall performance on ICGE tasks. Extensive experiments verify that Re-Align outperforms competitive methods of comparable model scale and resources on both in-context image generation and editing tasks.", 'score': 5, 'issue_id': 495, 'pub_date': '2026-01-08', 'pub_date_card': {'ru': '8 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 8', 'zh': '1æœˆ8æ—¥'}, 'hash': 'ab34d211b4ce6dc5', 'authors': ['Runze He', 'Yiji Cheng', 'Tiankai Hang', 'Zhimin Li', 'Yu Xu', 'Zijin Yin', 'Shiyi Zhang', 'Wenxun Dai', 'Penghui Du', 'Ao Ma', 'Chunyu Wang', 'Qinglin Lu', 'Jizhong Han', 'Jiao Dai'], 'affiliations': ['Hunyuan, Tencent', 'IIE, CAS', 'UCAS'], 'pdf_title_img': 'assets/pdf/title_img/2601.05124.jpg', 'data': {'categories': ['#transfer_learning', '#multimodal', '#alignment', '#reasoning', '#rl', '#training'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ¡Ñ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'Re-Align Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ²Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ’ Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ»ĞµĞ¶Ğ¸Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ In-Context Chain-of-Thought (IC-CoT), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¸ Ğ°ÑÑĞ¾Ñ†Ğ¸Ğ°Ñ†Ğ¸Ñ Ñ Ñ€ĞµÑ„ĞµÑ€ĞµĞ½ÑĞ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ ÑÑĞ½Ñ‹Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ñ†ĞµĞ»Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑÑƒÑ€Ñ€Ğ¾Ğ³Ğ°Ñ‚Ğ½Ğ¾Ğµ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ĞµĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Re-Align Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Bridging Understanding and Generation in Image Creation', 'desc': 'Re-Align is a new framework designed to improve in-context image generation and editing by enhancing the connection between understanding user prompts and generating images. It uses a method called In-Context Chain-of-Thought (IC-CoT) to clarify the relationship between text prompts and reference images, which helps the model better understand what users want. Additionally, Re-Align employs a reinforcement learning training approach that uses a surrogate reward to evaluate how well the generated images match the structured reasoning provided by the text. Experiments show that Re-Align performs better than other similar models in generating and editing images based on user input.'}, 'zh': {'title': 'ç†è§£ä¸ç”Ÿæˆçš„æ¡¥æ¢ï¼šRe-Align', 'desc': 'Re-Align æ˜¯ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ç»“æ„åŒ–æ¨ç†å¼•å¯¼çš„å¯¹é½æ¥è§£å†³å›¾åƒç”Ÿæˆå’Œç¼–è¾‘ä¸­çš„ç†è§£ä¸ç”Ÿæˆä¹‹é—´çš„å·®è·ã€‚å®ƒé‡‡ç”¨äº†ä¸Šä¸‹æ–‡é“¾å¼æ€ç»´ï¼ˆIC-CoTï¼‰çš„æ–¹æ³•ï¼Œèƒ½å¤Ÿå°†è¯­ä¹‰æŒ‡å¯¼å’Œå‚è€ƒå…³è”è§£è€¦ï¼Œä»è€Œæä¾›æ¸…æ™°çš„æ–‡æœ¬ç›®æ ‡ï¼Œå‡å°‘å‚è€ƒå›¾åƒä¹‹é—´çš„æ··æ·†ã€‚æ­¤å¤–ï¼ŒRe-Align è¿˜å¼•å…¥äº†ä¸€ç§æœ‰æ•ˆçš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ–¹æ¡ˆï¼Œåˆ©ç”¨æ›¿ä»£å¥–åŠ±æ¥è¡¡é‡ç»“æ„åŒ–æ¨ç†æ–‡æœ¬ä¸ç”Ÿæˆå›¾åƒä¹‹é—´çš„å¯¹é½ç¨‹åº¦ï¼Œä»è€Œæå‡æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡ä¸Šçš„æ•´ä½“è¡¨ç°ã€‚å¤§é‡å®éªŒéªŒè¯äº† Re-Align åœ¨åŒç±»æ¨¡å‹è§„æ¨¡å’Œèµ„æºä¸‹çš„ç«äº‰æ–¹æ³•ä¸­è¡¨ç°ä¼˜è¶Šã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.03111', 'title': 'One Sample to Rule Them All: Extreme Data Efficiency in RL Scaling', 'url': 'https://huggingface.co/papers/2601.03111', 'abstract': 'Reinforcement learning with carefully designed single training samples can significantly enhance the reasoning abilities of large language models across multiple disciplines, outperforming traditional approaches that rely on large datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t The reasoning ability of large language models (LLMs) can be unleashed with reinforcement learning (RL) (OpenAI, 2024; DeepSeek-AI et al., 2025a; Zeng et al., 2025). The success of existing RL attempts in LLMs usually relies on high-quality samples of thousands or beyond. In this paper, we challenge fundamental assumptions about data requirements in RL for LLMs by demonstrating the remarkable effectiveness of one-shot learning. Specifically, we introduce polymath learning, a framework for designing one training sample that elicits multidisciplinary impact. We present three key findings: (1) A single, strategically selected math reasoning sample can produce significant performance improvements across multiple domains, including physics, chemistry, and biology with RL; (2) The math skills salient to reasoning suggest the characteristics of the optimal polymath sample; and (3) An engineered synthetic sample that integrates multidiscipline elements outperforms training with individual samples that naturally occur. Our approach achieves superior performance to training with larger datasets across various reasoning benchmarks, demonstrating that sample quality and design, rather than quantity, may be the key to unlock enhanced reasoning capabilities in language models. Our results suggest a shift, dubbed as sample engineering, toward precision engineering of training samples rather than simply increasing data volume.', 'score': 5, 'issue_id': 502, 'pub_date': '2026-01-06', 'pub_date_card': {'ru': '6 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 6', 'zh': '1æœˆ6æ—¥'}, 'hash': 'fd6febf737aa4e79', 'authors': ['Yiyuan Li', 'Zhen Huang', 'Yanan Wu', 'Weixun Wang', 'Xuefeng Li', 'Yijia Luo', 'Wenbo Su', 'Bo Zheng', 'Pengfei Liu'], 'affiliations': ['GAIR', 'Shanghai Jiaotong University', 'Taobao & Tmall Group of Alibaba'], 'pdf_title_img': 'assets/pdf/title_img/2601.03111.jpg', 'data': {'categories': ['#synthetic', '#training', '#rl', '#optimization', '#reasoning'], 'emoji': 'ğŸ¯', 'ru': {'title': 'ĞĞ´Ğ¸Ğ½ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ñ‹ÑÑÑ‡: Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ LLM', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ´Ğ¸Ğ½ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑÑ€Ğ°Ğ·Ñƒ Ğ¿Ğ¾ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼ Ğ´Ğ¸ÑÑ†Ğ¸Ğ¿Ğ»Ğ¸Ğ½Ğ°Ğ¼ â€” Ñ„Ğ¸Ğ·Ğ¸ĞºĞµ, Ñ…Ğ¸Ğ¼Ğ¸Ğ¸ Ğ¸ Ğ±Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰ĞµĞ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ²Ğ°Ğ¶Ğ½ĞµĞµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğ¹ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² â€” Ñ†ĞµĞ»ĞµĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ³Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¸Ñ… Ğ¾Ğ±ÑŠÑ‘Ğ¼Ğ°.'}, 'en': {'title': 'Unlocking Reasoning with One Perfect Sample', 'desc': 'This paper explores how reinforcement learning (RL) can improve the reasoning abilities of large language models (LLMs) using just one carefully designed training sample. It introduces a new approach called polymath learning, which shows that a single, well-chosen math reasoning sample can enhance performance in various fields like physics, chemistry, and biology. The authors argue that the quality and design of training samples are more important than the quantity, challenging the traditional reliance on large datasets. Their findings suggest a shift towards sample engineering, focusing on creating optimal training samples to boost reasoning capabilities in LLMs.'}, 'zh': {'title': 'æ ·æœ¬è®¾è®¡ï¼šæå‡æ¨ç†èƒ½åŠ›çš„æ–°ç­–ç•¥', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„å•ä¸€è®­ç»ƒæ ·æœ¬æ¥æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºå¤šæ‰å­¦ä¹ çš„æ¡†æ¶ï¼Œå±•ç¤ºäº†å•æ¬¡å­¦ä¹ åœ¨å¤šä¸ªå­¦ç§‘ä¸­çš„æœ‰æ•ˆæ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼Œç»è¿‡ç²¾å¿ƒé€‰æ‹©çš„æ•°å­¦æ¨ç†æ ·æœ¬èƒ½å¤Ÿæ˜¾è‘—æé«˜åœ¨ç‰©ç†ã€åŒ–å­¦å’Œç”Ÿç‰©ç­‰é¢†åŸŸçš„è¡¨ç°ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œæ ·æœ¬çš„è´¨é‡å’Œè®¾è®¡æ¯”æ•°é‡æ›´ä¸ºé‡è¦ï¼Œè¿™å¯èƒ½æ˜¯æå‡è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„å…³é”®ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.05163', 'title': 'DocDancer: Towards Agentic Document-Grounded Information Seeking', 'url': 'https://huggingface.co/papers/2601.05163', 'abstract': 'DocDancer is an end-to-end trained open-source document question answering agent that formulates the task as an information-seeking problem and uses a tool-driven framework with exploration and synthesis for training.  \t\t\t\t\tAI-generated summary \t\t\t\t Document Question Answering (DocQA) focuses on answering questions grounded in given documents, yet existing DocQA agents lack effective tool utilization and largely rely on closed-source models. In this work, we introduce DocDancer, an end-to-end trained open-source Doc agent. We formulate DocQA as an information-seeking problem and propose a tool-driven agent framework that explicitly models document exploration and comprehension. To enable end-to-end training of such agents, we introduce an Exploration-then-Synthesis data synthesis pipeline that addresses the scarcity of high-quality training data for DocQA. Training on the synthesized data, the trained models on two long-context document understanding benchmarks, MMLongBench-Doc and DocBench, show their effectiveness. Further analysis provides valuable insights for the agentic tool design and synthetic data.', 'score': 3, 'issue_id': 492, 'pub_date': '2026-01-08', 'pub_date_card': {'ru': '8 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 8', 'zh': '1æœˆ8æ—¥'}, 'hash': 'd54786c0b7245d58', 'authors': ['Qintong Zhang', 'Xinjie Lv', 'Jialong Wu', 'Baixuan Li', 'Zhengwei Tao', 'Guochen Yan', 'Huanyao Zhang', 'Bin Wang', 'Jiahao Xu', 'Haitao Mi', 'Wentao Zhang'], 'affiliations': ['Peking University', 'Shanghai AI Lab', 'Tencent AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2601.05163.jpg', 'data': {'categories': ['#open_source', '#long_context', '#benchmark', '#dataset', '#data', '#agents', '#synthetic'], 'emoji': 'ğŸ•º', 'ru': {'title': 'Ğ¢Ğ°Ğ½ĞµÑ† Ñ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸: Ğ°Ğ³ĞµĞ½Ñ‚ Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ñ‚ĞµĞºÑÑ‚Ğ°Ñ…', 'desc': 'DocDancer â€” ÑÑ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¾ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ ĞºĞ°Ğº Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñ ÑÑ‚Ğ°Ğ¿Ğ°Ğ¼Ğ¸ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Exploration-then-Synthesis Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ½ĞµÑ…Ğ²Ğ°Ñ‚ĞºĞ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… DocQA. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ end-to-end Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… MMLongBench-Doc Ğ¸ DocBench Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ†ĞµĞ½Ğ½Ñ‹Ğµ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'DocDancer: Revolutionizing Document Question Answering with Open-Source Innovation', 'desc': 'DocDancer is an innovative open-source document question answering (DocQA) agent that approaches the task as an information-seeking challenge. It utilizes a tool-driven framework that emphasizes both exploration of documents and synthesis of information for effective training. To tackle the issue of limited high-quality training data, the authors introduce a novel Exploration-then-Synthesis data synthesis pipeline. The performance of DocDancer is validated on two benchmarks, demonstrating its capability in long-context document understanding and providing insights for future tool design.'}, 'zh': {'title': 'DocDancerï¼šå¼€æºæ–‡æ¡£é—®ç­”çš„æ–°çªç ´', 'desc': 'DocDanceræ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯è®­ç»ƒçš„å¼€æºæ–‡æ¡£é—®ç­”ä»£ç†ï¼Œæ—¨åœ¨å°†æ–‡æ¡£é—®ç­”ä»»åŠ¡è§†ä¸ºä¿¡æ¯è·å–é—®é¢˜ã€‚å®ƒé‡‡ç”¨å·¥å…·é©±åŠ¨çš„æ¡†æ¶ï¼Œé€šè¿‡æ¢ç´¢å’Œç»¼åˆçš„æ–¹å¼è¿›è¡Œè®­ç»ƒï¼Œä»¥æé«˜é—®ç­”çš„å‡†ç¡®æ€§ã€‚ä¸ºäº†æ”¯æŒç«¯åˆ°ç«¯çš„è®­ç»ƒï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ¢ç´¢-å†ç»¼åˆçš„æ•°æ®åˆæˆç®¡é“ï¼Œè§£å†³äº†é«˜è´¨é‡è®­ç»ƒæ•°æ®ç¨€ç¼ºçš„é—®é¢˜ã€‚ç»è¿‡åˆæˆæ•°æ®çš„è®­ç»ƒï¼ŒDocDanceråœ¨ä¸¤ä¸ªé•¿æ–‡æœ¬ç†è§£åŸºå‡†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå±•ç¤ºäº†å…¶æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.05149', 'title': 'Multi-Scale Local Speculative Decoding for Image Generation', 'url': 'https://huggingface.co/papers/2601.05149', 'abstract': 'Multi-Scale Local Speculative Decoding accelerates autoregressive image generation through multi-resolution drafting and spatially informed verification while maintaining semantic quality and perceptual fidelity.  \t\t\t\t\tAI-generated summary \t\t\t\t Autoregressive (AR) models have achieved remarkable success in image synthesis, yet their sequential nature imposes significant latency constraints. Speculative Decoding offers a promising avenue for acceleration, but existing approaches are limited by token-level ambiguity and lack of spatial awareness. In this work, we introduce Multi-Scale Local Speculative Decoding (MuLo-SD), a novel framework that combines multi-resolution drafting with spatially informed verification to accelerate AR image generation. Our method leverages a low-resolution drafter paired with learned up-samplers to propose candidate image tokens, which are then verified in parallel by a high-resolution target model. Crucially, we incorporate a local rejection and resampling mechanism, enabling efficient correction of draft errors by focusing on spatial neighborhoods rather than raster-scan resampling after the first rejection. We demonstrate that MuLo-SD achieves substantial speedups - up to 1.7times - outperforming strong speculative decoding baselines such as EAGLE-2 and LANTERN in terms of acceleration, while maintaining comparable semantic alignment and perceptual quality. These results are validated using GenEval, DPG-Bench, and FID/HPSv2 on the MS-COCO 5k validation split. Extensive ablations highlight the impact of up-sampling design, probability pooling, and local rejection and resampling with neighborhood expansion. Our approach sets a new state-of-the-art in speculative decoding for image synthesis, bridging the gap between efficiency and fidelity.', 'score': 2, 'issue_id': 505, 'pub_date': '2026-01-08', 'pub_date_card': {'ru': '8 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 8', 'zh': '1æœˆ8æ—¥'}, 'hash': '3eb3439c3eef40bd', 'authors': ['Elia Peruzzo', 'Guillaume SautiÃ¨re', 'Amirhossein Habibian'], 'affiliations': ['Qualcomm AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2601.05149.jpg', 'data': {'categories': [], 'emoji': 'âš¡', 'ru': {'title': 'ĞœĞ½Ğ¾Ğ³Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğµ ÑĞ¿ĞµĞºÑƒĞ»ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Multi-Scale Local Speculative Decoding (MuLo-SD) Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€Ğ°Ğ·Ñ€ĞµÑˆÑ‘Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ-Ñ‡ĞµÑ€Ğ½Ğ¾Ğ²Ğ¸ĞºĞ¾Ğ¼ Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ€Ğ°Ğ·Ñ€ĞµÑˆÑ‘Ğ½Ğ½Ğ¾Ğ¹ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ğ´Ğ»Ñ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸ĞµĞ¹ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚ĞºĞ»Ğ¾Ğ½ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿ĞµÑ€ĞµĞ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¾ĞºÑ€ĞµÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ Ğ¿ĞµÑ€ĞµĞ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ´Ğ¾ 1.7x Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ²Ñ€Ğ¾Ğ´Ğµ EAGLE-2 Ğ¸ LANTERN.'}, 'en': {'title': 'Accelerating Image Generation with Smart Drafting and Verification', 'desc': 'This paper presents Multi-Scale Local Speculative Decoding (MuLo-SD), a new method to speed up autoregressive image generation while preserving quality. It combines low-resolution drafting with high-resolution verification to propose and refine image tokens efficiently. The approach uses a local rejection and resampling mechanism that focuses on nearby pixels, improving error correction. The results show that MuLo-SD significantly accelerates the generation process, achieving up to 1.7 times faster performance compared to existing methods, without sacrificing semantic and perceptual quality.'}, 'zh': {'title': 'å¤šå°ºåº¦å±€éƒ¨æ¨æµ‹è§£ç ï¼šåŠ é€Ÿå›¾åƒç”Ÿæˆçš„æ–°æ–¹æ³•', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¤šå°ºåº¦å±€éƒ¨æ¨æµ‹è§£ç æ¡†æ¶ï¼ˆMuLo-SDï¼‰ï¼Œæ—¨åœ¨åŠ é€Ÿè‡ªå›å½’å›¾åƒç”Ÿæˆã€‚è¯¥æ–¹æ³•ç»“åˆäº†å¤šåˆ†è¾¨ç‡è‰å›¾å’Œç©ºé—´ä¿¡æ¯éªŒè¯ï¼Œè§£å†³äº†ç°æœ‰æ¨æµ‹è§£ç æ–¹æ³•åœ¨æ ‡è®°çº§åˆ«æ¨¡ç³Šæ€§å’Œç¼ºä¹ç©ºé—´æ„è¯†çš„é—®é¢˜ã€‚é€šè¿‡ä½åˆ†è¾¨ç‡è‰å›¾ç”Ÿæˆå€™é€‰å›¾åƒæ ‡è®°ï¼Œå¹¶åˆ©ç”¨é«˜åˆ†è¾¨ç‡ç›®æ ‡æ¨¡å‹è¿›è¡Œå¹¶è¡ŒéªŒè¯ï¼Œæ˜¾è‘—æé«˜äº†ç”Ÿæˆé€Ÿåº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMuLo-SDåœ¨ä¿æŒè¯­ä¹‰ä¸€è‡´æ€§å’Œæ„ŸçŸ¥è´¨é‡çš„åŒæ—¶ï¼Œé€Ÿåº¦æå‡å¯è¾¾1.7å€ï¼Œæˆä¸ºå›¾åƒåˆæˆé¢†åŸŸçš„æœ€æ–°æŠ€æœ¯ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.04792', 'title': 'PyramidalWan: On Making Pretrained Video Model Pyramidal for Efficient Inference', 'url': 'https://huggingface.co/papers/2601.04792', 'abstract': 'Pyramidal diffusion models reduce computational cost through hierarchical resolution processing, with pretrained models converted via low-cost fine-tuning maintaining output quality while enabling efficient inference.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently proposed pyramidal models decompose the conventional forward and backward diffusion processes into multiple stages operating at varying resolutions. These models handle inputs with higher noise levels at lower resolutions, while less noisy inputs are processed at higher resolutions. This hierarchical approach significantly reduces the computational cost of inference in multi-step denoising models. However, existing open-source pyramidal video models have been trained from scratch and tend to underperform compared to state-of-the-art systems in terms of visual plausibility. In this work, we present a pipeline that converts a pretrained diffusion model into a pyramidal one through low-cost finetuning, achieving this transformation without degradation in quality of output videos. Furthermore, we investigate and compare various strategies for step distillation within pyramidal models, aiming to further enhance the inference efficiency. Our results are available at https://qualcomm-ai-research.github.io/PyramidalWan.', 'score': 2, 'issue_id': 509, 'pub_date': '2026-01-08', 'pub_date_card': {'ru': '8 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 8', 'zh': '1æœˆ8æ—¥'}, 'hash': '482ad7e44a3f4895', 'authors': ['Denis Korzhenkov', 'Adil Karjauv', 'Animesh Karnewar', 'Mohsen Ghafoorian', 'Amirhossein Habibian'], 'affiliations': ['Qualcomm AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2601.04792.jpg', 'data': {'categories': ['#optimization', '#diffusion', '#training', '#video', '#inference', '#open_source'], 'emoji': 'ğŸ”º', 'ru': {'title': 'Ğ˜ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ´ĞµĞ½Ğ¾Ğ¹Ğ·Ğ¸Ğ½Ğ³: Ğ¿Ğ¸Ñ€Ğ°Ğ¼Ğ¸Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ°', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¿Ğ¸Ñ€Ğ°Ğ¼Ğ¸Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ´Ğ¾Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸. ĞŸĞ¸Ñ€Ğ°Ğ¼Ğ¸Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸: ÑˆÑƒĞ¼Ğ½Ñ‹Ğµ Ğ²Ñ…Ğ¾Ğ´Ñ‹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ÑÑ Ğ½Ğ° Ğ½Ğ¸Ğ·ĞºĞ¸Ñ… Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸ÑÑ…, Ğ° Ğ¼ĞµĞ½ĞµĞµ ÑˆÑƒĞ¼Ğ½Ñ‹Ğµ â€” Ğ½Ğ° Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ…, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ¿Ñ€Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ¼ Ğ´ĞµĞ½Ğ¾Ğ¹Ğ·Ğ¸Ğ½Ğ³Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ ÑˆĞ°Ğ³Ğ¾Ğ² Ğ² Ğ¿Ğ¸Ñ€Ğ°Ğ¼Ğ¸Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ´Ğ»Ñ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞµĞ³Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ°. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ½Ğ²ĞµÑ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'Efficient Inference with Pyramidal Diffusion Models', 'desc': 'This paper introduces pyramidal diffusion models that optimize computational efficiency by processing data at different resolutions. The models operate in stages, handling noisier inputs at lower resolutions and clearer inputs at higher resolutions, which reduces the overall computational load during inference. The authors propose a method to convert existing pretrained models into pyramidal models using low-cost fine-tuning, ensuring that the quality of the output remains high. Additionally, they explore various strategies for step distillation to further improve the efficiency of these models during inference.'}, 'zh': {'title': 'é‡‘å­—å¡”æ‰©æ•£æ¨¡å‹ï¼šé«˜æ•ˆæ¨ç†çš„æ–°æ–¹æ³•', 'desc': 'é‡‘å­—å¡”æ‰©æ•£æ¨¡å‹é€šè¿‡åˆ†å±‚åˆ†è¾¨ç‡å¤„ç†æ¥é™ä½è®¡ç®—æˆæœ¬ã€‚è¿™äº›æ¨¡å‹å°†ä¼ ç»Ÿçš„å‰å‘å’Œåå‘æ‰©æ•£è¿‡ç¨‹åˆ†è§£ä¸ºå¤šä¸ªé˜¶æ®µï¼Œåœ¨ä¸åŒçš„åˆ†è¾¨ç‡ä¸‹è¿è¡Œã€‚å®ƒä»¬åœ¨è¾ƒä½åˆ†è¾¨ç‡ä¸‹å¤„ç†é«˜å™ªå£°è¾“å…¥ï¼Œè€Œåœ¨è¾ƒé«˜åˆ†è¾¨ç‡ä¸‹å¤„ç†ä½å™ªå£°è¾“å…¥ï¼Œä»è€Œæ˜¾è‘—å‡å°‘å¤šæ­¥å»å™ªæ¨¡å‹çš„æ¨ç†è®¡ç®—æˆæœ¬ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§é€šè¿‡ä½æˆæœ¬å¾®è°ƒå°†é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹è½¬æ¢ä¸ºé‡‘å­—å¡”æ¨¡å‹çš„æµç¨‹ï¼Œç¡®ä¿è¾“å‡ºè§†é¢‘è´¨é‡ä¸ä¸‹é™ï¼ŒåŒæ—¶è¿˜ç ”ç©¶äº†é‡‘å­—å¡”æ¨¡å‹ä¸­çš„æ­¥éª¤è’¸é¦ç­–ç•¥ï¼Œä»¥è¿›ä¸€æ­¥æé«˜æ¨ç†æ•ˆç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.04754', 'title': 'ProFuse: Efficient Cross-View Context Fusion for Open-Vocabulary 3D Gaussian Splatting', 'url': 'https://huggingface.co/papers/2601.04754', 'abstract': 'ProFuse enhances 3D scene understanding by integrating semantic information into 3D Gaussian Splatting through efficient context-aware processing and pre-registration phases.  \t\t\t\t\tAI-generated summary \t\t\t\t We present ProFuse, an efficient context-aware framework for open-vocabulary 3D scene understanding with 3D Gaussian Splatting (3DGS). The pipeline enhances cross-view consistency and intra-mask cohesion within a direct registration setup, adding minimal overhead and requiring no render-supervised fine-tuning. Instead of relying on a pretrained 3DGS scene, we introduce a dense correspondence-guided pre-registration phase that initializes Gaussians with accurate geometry while jointly constructing 3D Context Proposals via cross-view clustering. Each proposal carries a global feature obtained through weighted aggregation of member embeddings, and this feature is fused onto Gaussians during direct registration to maintain per-primitive language coherence across views. With associations established in advance, semantic fusion requires no additional optimization beyond standard reconstruction, and the model retains geometric refinement without densification. ProFuse achieves strong open-vocabulary 3DGS understanding while completing semantic attachment in about five minutes per scene, which is two times faster than SOTA.', 'score': 2, 'issue_id': 499, 'pub_date': '2026-01-08', 'pub_date_card': {'ru': '8 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 8', 'zh': '1æœˆ8æ—¥'}, 'hash': '8bdee003577e3af9', 'authors': ['Yen-Jen Chiou', 'Wei-Tse Cheng', 'Yuan-Fu Yang'], 'affiliations': ['National Yang Ming Chiao Tung University'], 'pdf_title_img': 'assets/pdf/title_img/2601.04754.jpg', 'data': {'categories': ['#3d', '#multimodal'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ¡ĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ² 3D Gaussian Splatting Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸', 'desc': 'ProFuse Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ 3D ÑÑ†ĞµĞ½ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ 3D Gaussian Splatting, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾-Ğ¾ÑĞ¾Ğ·Ğ½Ğ°Ğ½Ğ½ÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸ĞµĞ¹ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ñ„Ğ°Ğ·Ğ° Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ñ€ĞµĞ³Ğ¸ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸, ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ°Ñ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸ÑĞ¼Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ³Ğ°ÑƒÑÑĞ¸Ğ°Ğ½Ñ‹ Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸ĞµĞ¹ Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ 3D ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ ĞºÑ€Ğ¾ÑÑ-Ğ²Ğ¸Ğ´Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. Ğ“Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ÑÑ‚ÑÑ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ·Ğ²ĞµÑˆĞµĞ½Ğ½ÑƒÑ Ğ°Ğ³Ñ€ĞµĞ³Ğ°Ñ†Ğ¸Ñ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ğ¸ ÑĞ»Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ñ Ğ³Ğ°ÑƒÑÑĞ¸Ğ°Ğ½Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ğ²Ğ¾Ğ² Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´Ğ°Ñ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² open-vocabulary Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ ÑÑ†ĞµĞ½ Ğ·Ğ° Ğ¿ÑÑ‚ÑŒ Ğ¼Ğ¸Ğ½ÑƒÑ‚ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ²Ğ´Ğ²Ğ¾Ğµ Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ².'}, 'en': {'title': 'ProFuse: Fast and Semantic 3D Scene Understanding', 'desc': 'ProFuse is a novel framework that improves 3D scene understanding by integrating semantic information into the process of 3D Gaussian Splatting (3DGS). It utilizes a context-aware approach that enhances the consistency of views and the cohesion of object masks without needing extensive fine-tuning. The method includes a pre-registration phase that accurately initializes Gaussian representations and constructs 3D Context Proposals through cross-view clustering. This allows for efficient semantic fusion and geometric refinement, achieving significant speed improvements over existing state-of-the-art methods.'}, 'zh': {'title': 'ProFuseï¼šé«˜æ•ˆçš„3Dåœºæ™¯ç†è§£æ–°æ–¹æ³•', 'desc': 'ProFuse æ˜¯ä¸€ä¸ªé«˜æ•ˆçš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ 3D é«˜æ–¯ç‚¹äº‘ï¼ˆ3D Gaussian Splattingï¼‰å¢å¼ºå¼€æ”¾è¯æ±‡çš„ 3D åœºæ™¯ç†è§£ã€‚è¯¥æ–¹æ³•é€šè¿‡ç›´æ¥æ³¨å†Œè®¾ç½®æé«˜äº†è·¨è§†å›¾ä¸€è‡´æ€§å’Œå†…éƒ¨æ©è†œçš„å‡èšåŠ›ï¼Œä¸”å‡ ä¹æ²¡æœ‰é¢å¤–å¼€é”€ã€‚ProFuse å¼•å…¥äº†ä¸€ç§å¯†é›†å¯¹åº”å¼•å¯¼çš„é¢„æ³¨å†Œé˜¶æ®µï¼Œèƒ½å¤Ÿå‡†ç¡®åˆå§‹åŒ–é«˜æ–¯å¹¶é€šè¿‡è·¨è§†å›¾èšç±»å…±åŒæ„å»º 3D ä¸Šä¸‹æ–‡ææ¡ˆã€‚è¯¥æ¨¡å‹åœ¨æ¯ä¸ªåœºæ™¯ä¸­å®Œæˆè¯­ä¹‰é™„åŠ çš„é€Ÿåº¦çº¦ä¸ºäº”åˆ†é’Ÿï¼Œæ˜¯å½“å‰æœ€å…ˆè¿›æŠ€æœ¯çš„ä¸¤å€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.04575', 'title': 'Scaling Behavior Cloning Improves Causal Reasoning: An Open Model for Real-Time Video Game Playing', 'url': 'https://huggingface.co/papers/2601.04575', 'abstract': "Behavior cloning demonstrates improved performance and causal reasoning through scaling model size and training data, achieving human-level gameplay in 3D video games.  \t\t\t\t\tAI-generated summary \t\t\t\t Behavior cloning is enjoying a resurgence in popularity as scaling both model and data sizes proves to provide a strong starting point for many tasks of interest. In this work, we introduce an open recipe for training a video game playing foundation model designed for inference in realtime on a consumer GPU. We release all data (8300+ hours of high quality human gameplay), training and inference code, and pretrained checkpoints under an open license. We show that our best model is capable of playing a variety of 3D video games at a level competitive with human play. We use this recipe to systematically examine the scaling laws of behavior cloning to understand how the model's performance and causal reasoning varies with model and data scale. We first show in a simple toy problem that, for some types of causal reasoning, increasing both the amount of training data and the depth of the network results in the model learning a more causal policy. We then systematically study how causality varies with the number of parameters (and depth) and training steps in scaled models of up to 1.2 billion parameters, and we find similar scaling results to what we observe in the toy problem.", 'score': 2, 'issue_id': 504, 'pub_date': '2026-01-08', 'pub_date_card': {'ru': '8 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 8', 'zh': '1æœˆ8æ—¥'}, 'hash': 'ca12e042748c296d', 'authors': ['Yuguang Yue', 'Irakli Salia', 'Samuel Hunt', 'Chris Green', 'Wenzhe Shi', 'Jonathan J Hunt'], 'affiliations': ['Player2, USA'], 'pdf_title_img': 'assets/pdf/title_img/2601.04575.jpg', 'data': {'categories': ['#games', '#training', '#dataset', '#optimization', '#3d', '#reasoning', '#agents', '#video', '#open_source'], 'emoji': 'ğŸ®', 'ru': {'title': 'ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ĞºĞ»Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¸Ğ³Ñ€Ğ°Ñ…', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ñ‡ĞµÑĞºĞ¾Ğµ ĞºĞ»Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸Ğ³Ñ€Ğµ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¸Ğ³Ñ€Ñ‹ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¾Ğ±ÑŠÑ‘Ğ¼Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ½Ğ¸ Ğ²Ñ‹Ğ¿ÑƒÑÑ‚Ğ¸Ğ»Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 8300 Ñ‡Ğ°ÑĞ¾Ğ² Ğ³ĞµĞ¹Ğ¼Ğ¿Ğ»ĞµÑ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²ĞµÑĞ¾Ğ¼ Ğ´Ğ¾ 1.2 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ ĞºĞ°Ğº Ğ¾Ğ±ÑŠÑ‘Ğ¼Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ğ¾Ğ³Ğ¾ Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼ Ğ¸Ğ³Ñ€Ğ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ°ÑÑ‚ĞµÑ€ÑÑ‚Ğ²Ğ¾Ğ¼ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¸Ğ³Ñ€Ğ°Ñ….'}, 'en': {'title': 'Scaling Up Behavior Cloning for Human-Level Gameplay', 'desc': "This paper discusses the resurgence of behavior cloning in machine learning, particularly for training models to play 3D video games. By scaling up both the model size and the amount of training data, the authors demonstrate that their foundation model can achieve human-level performance in gameplay. They provide an open-source framework that includes extensive gameplay data, training code, and pretrained models, allowing others to replicate their results. The study also explores how increasing model depth and training data enhances the model's ability to learn causal relationships, revealing important insights into the scaling laws of behavior cloning."}, 'zh': {'title': 'è¡Œä¸ºå…‹éš†ï¼šé€šè¿‡æ‰©å±•å®ç°äººç±»æ°´å¹³çš„æ¸¸æˆè¡¨ç°', 'desc': 'è¡Œä¸ºå…‹éš†é€šè¿‡æ‰©å¤§æ¨¡å‹è§„æ¨¡å’Œè®­ç»ƒæ•°æ®ï¼Œå±•ç¤ºäº†åœ¨3Dè§†é¢‘æ¸¸æˆä¸­è¾¾åˆ°äººç±»æ°´å¹³çš„æ¸¸æˆè¡¨ç°å’Œå› æœæ¨ç†èƒ½åŠ›ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§å¼€æ”¾çš„è®­ç»ƒè§†é¢‘æ¸¸æˆåŸºç¡€æ¨¡å‹çš„é…æ–¹ï¼Œæ—¨åœ¨å®æ—¶æ¨ç†å¹¶åœ¨æ¶ˆè´¹çº§GPUä¸Šè¿è¡Œã€‚æˆ‘ä»¬å‘å¸ƒäº†8300å¤šä¸ªå°æ—¶çš„é«˜è´¨é‡äººç±»æ¸¸æˆæ•°æ®ã€è®­ç»ƒå’Œæ¨ç†ä»£ç ï¼Œä»¥åŠé¢„è®­ç»ƒæ£€æŸ¥ç‚¹ï¼Œå‡åœ¨å¼€æ”¾è®¸å¯ä¸‹æä¾›ã€‚ç ”ç©¶è¡¨æ˜ï¼Œæœ€ä½³æ¨¡å‹èƒ½å¤Ÿåœ¨å¤šç§3Dè§†é¢‘æ¸¸æˆä¸­ä¸äººç±»ç©å®¶ç«äº‰ï¼Œä¸”é€šè¿‡ç³»ç»Ÿæ€§ç ”ç©¶è¡Œä¸ºå…‹éš†çš„æ‰©å±•è§„å¾‹ï¼Œæ­ç¤ºäº†æ¨¡å‹æ€§èƒ½å’Œå› æœæ¨ç†å¦‚ä½•éšæ¨¡å‹å’Œæ•°æ®è§„æ¨¡å˜åŒ–ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.04342', 'title': 'ReHyAt: Recurrent Hybrid Attention for Video Diffusion Transformers', 'url': 'https://huggingface.co/papers/2601.04342', 'abstract': "ReHyAt introduces a recurrent hybrid attention mechanism that combines softmax and linear attention benefits, enabling efficient video generation with reduced computational costs and improved scalability.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in video diffusion models have shifted towards transformer-based architectures, achieving state-of-the-art video generation but at the cost of quadratic attention complexity, which severely limits scalability for longer sequences. We introduce ReHyAt, a Recurrent Hybrid Attention mechanism that combines the fidelity of softmax attention with the efficiency of linear attention, enabling chunk-wise recurrent reformulation and constant memory usage. Unlike the concurrent linear-only SANA Video, ReHyAt's hybrid design allows efficient distillation from existing softmax-based models, reducing the training cost by two orders of magnitude to ~160 GPU hours, while being competitive in the quality. Our light-weight distillation and finetuning pipeline provides a recipe that can be applied to future state-of-the-art bidirectional softmax-based models. Experiments on VBench and VBench-2.0, as well as a human preference study, demonstrate that ReHyAt achieves state-of-the-art video quality while reducing attention cost from quadratic to linear, unlocking practical scalability for long-duration and on-device video generation. Project page is available at https://qualcomm-ai-research.github.io/rehyat.", 'score': 2, 'issue_id': 509, 'pub_date': '2026-01-07', 'pub_date_card': {'ru': '7 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 7', 'zh': '1æœˆ7æ—¥'}, 'hash': '1816aeec79b1b6b5', 'authors': ['Mohsen Ghafoorian', 'Amirhossein Habibian'], 'affiliations': ['Qualcomm AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2601.04342.jpg', 'data': {'categories': ['#optimization', '#diffusion', '#training', '#architecture', '#video', '#inference', '#open_source'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ“Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'ReHyAt Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° softmax Ğ¸ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ€ĞµĞºÑƒÑ€Ñ€ĞµĞ½Ñ‚Ğ½ÑƒÑ Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²ĞºÑƒ Ğ¿Ğ¾ Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ°Ğ¼ Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑ…Ğ¾Ğ´Ğ° Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸, Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°Ñ ĞºĞ²Ğ°Ğ´Ñ€Ğ°Ñ‚Ğ¸Ñ‡Ğ½ÑƒÑ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€-Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€. Ğ‘Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸Ğ· ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ softmax, ReHyAt Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ² ÑÑ‚Ğ¾ Ñ€Ğ°Ğ· Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² (~160 GPU Ñ‡Ğ°ÑĞ¾Ğ²) Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, ÑĞ½Ğ¸Ğ¶Ğ°Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ñ ĞºĞ²Ğ°Ğ´Ñ€Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ´Ğ¾ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğ¹.'}, 'en': {'title': 'ReHyAt: Efficient Video Generation with Hybrid Attention', 'desc': 'ReHyAt presents a novel Recurrent Hybrid Attention mechanism that merges the advantages of softmax and linear attention, facilitating efficient video generation. This approach addresses the challenge of quadratic attention complexity in transformer-based models, which hampers scalability for longer video sequences. By enabling chunk-wise recurrent processing and maintaining constant memory usage, ReHyAt significantly reduces training costs while preserving high video quality. The method has been validated through experiments, showing it achieves state-of-the-art results in video generation with improved efficiency.'}, 'zh': {'title': 'ReHyAtï¼šé«˜æ•ˆè§†é¢‘ç”Ÿæˆçš„æ–°æ–¹æ³•', 'desc': 'ReHyAtæ˜¯ä¸€ç§é€’å½’æ··åˆæ³¨æ„åŠ›æœºåˆ¶ï¼Œç»“åˆäº†softmaxå’Œçº¿æ€§æ³¨æ„åŠ›çš„ä¼˜ç‚¹ï¼Œèƒ½å¤Ÿé«˜æ•ˆç”Ÿæˆè§†é¢‘ï¼ŒåŒæ—¶é™ä½è®¡ç®—æˆæœ¬å¹¶æé«˜å¯æ‰©å±•æ€§ã€‚ä¼ ç»Ÿçš„è§†é¢‘æ‰©æ•£æ¨¡å‹ä½¿ç”¨åŸºäºå˜æ¢å™¨çš„æ¶æ„ï¼Œè™½ç„¶ç”Ÿæˆæ•ˆæœä¼˜ç§€ï¼Œä½†åœ¨å¤„ç†é•¿åºåˆ—æ—¶ä¼šé¢ä¸´äºŒæ¬¡æ³¨æ„åŠ›å¤æ‚åº¦çš„é—®é¢˜ã€‚ReHyAté€šè¿‡å—çŠ¶é€’å½’é‡æ„å’Œæ’å®šå†…å­˜ä½¿ç”¨ï¼Œè§£å†³äº†è¿™ä¸€é—®é¢˜ï¼Œå¹¶ä¸”åœ¨è®­ç»ƒæˆæœ¬ä¸Šå‡å°‘äº†ä¸¤ä¸ªæ•°é‡çº§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒReHyAtåœ¨è§†é¢‘è´¨é‡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ï¼ŒåŒæ—¶å°†æ³¨æ„åŠ›æˆæœ¬ä»äºŒæ¬¡é™ä½åˆ°çº¿æ€§ï¼Œé€‚ç”¨äºé•¿æ—¶é•¿å’Œè®¾å¤‡ä¸Šçš„è§†é¢‘ç”Ÿæˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.03362', 'title': 'Guardians of the Hair: Rescuing Soft Boundaries in Depth, Stereo, and Novel Views', 'url': 'https://huggingface.co/papers/2601.03362', 'abstract': 'HairGuard is a framework for recovering fine-grained soft boundary details in 3D vision tasks through specialized depth refinement and view synthesis techniques.  \t\t\t\t\tAI-generated summary \t\t\t\t Soft boundaries, like thin hairs, are commonly observed in natural and computer-generated imagery, but they remain challenging for 3D vision due to the ambiguous mixing of foreground and background cues. This paper introduces Guardians of the Hair (HairGuard), a framework designed to recover fine-grained soft boundary details in 3D vision tasks. Specifically, we first propose a novel data curation pipeline that leverages image matting datasets for training and design a depth fixer network to automatically identify soft boundary regions. With a gated residual module, the depth fixer refines depth precisely around soft boundaries while maintaining global depth quality, allowing plug-and-play integration with state-of-the-art depth models. For view synthesis, we perform depth-based forward warping to retain high-fidelity textures, followed by a generative scene painter that fills disoccluded regions and eliminates redundant background artifacts within soft boundaries. Finally, a color fuser adaptively combines warped and inpainted results to produce novel views with consistent geometry and fine-grained details. Extensive experiments demonstrate that HairGuard achieves state-of-the-art performance across monocular depth estimation, stereo image/video conversion, and novel view synthesis, with significant improvements in soft boundary regions.', 'score': 2, 'issue_id': 495, 'pub_date': '2026-01-06', 'pub_date_card': {'ru': '6 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 6', 'zh': '1æœˆ6æ—¥'}, 'hash': 'fd6d3f256e025564', 'authors': ['Xiang Zhang', 'Yang Zhang', 'Lukas Mehl', 'Markus Gross', 'Christopher Schroers'], 'affiliations': ['DisneyResearchStudios', 'ETH Zurich'], 'pdf_title_img': 'assets/pdf/title_img/2601.03362.jpg', 'data': {'categories': ['#cv', '#data', '#dataset', '#3d'], 'emoji': 'ğŸ’‡', 'ru': {'title': 'ĞÑ…Ñ€Ğ°Ğ½Ğ° Ğ¼ĞµĞ»ĞºĞ¸Ñ… Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹: Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ğ½ĞºĞ¸Ñ… Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ† Ğ² 3D-Ğ·Ñ€ĞµĞ½Ğ¸Ğ¸', 'desc': 'HairGuard â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ğ½ĞºĞ¸Ñ… Ğ¼ÑĞ³ĞºĞ¸Ñ… Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ† Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ€Ñ‚ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ²Ğ¸Ğ´Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² Ğ¼Ğ°Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ Ğ¼ÑĞ³ĞºĞ¸Ñ… Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†. Ğ”Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ²Ğ¸Ğ´Ğ¾Ğ² Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ´ĞµÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞºĞ°Ñ€Ñ‚ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ÑŒ Ğ´Ğ»Ñ Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ²Ğ¸Ğ´Ğ¸Ğ¼Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹, ÑÑ‚ĞµÑ€ĞµĞ¾ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğµ Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğµ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ²Ğ¸Ğ´Ğ¾Ğ², Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ… Ñ‚Ğ¾Ğ½ĞºĞ¸Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€.'}, 'en': {'title': 'HairGuard: Mastering Soft Boundaries in 3D Vision', 'desc': 'HairGuard is a novel framework aimed at enhancing the recovery of fine-grained soft boundary details in 3D vision tasks. It utilizes a specialized depth fixer network that identifies and refines depth around soft boundaries, ensuring high-quality global depth. The framework also incorporates depth-based forward warping and a generative scene painter to synthesize new views while maintaining texture fidelity and eliminating background artifacts. Extensive experiments show that HairGuard outperforms existing methods in monocular depth estimation and view synthesis, particularly in handling soft boundary regions effectively.'}, 'zh': {'title': 'HairGuardï¼šæ¢å¤3Dè§†è§‰ä¸­çš„ç»†ç²’åº¦è½¯è¾¹ç•Œ', 'desc': 'HairGuardæ˜¯ä¸€ä¸ªæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ä¸“é—¨çš„æ·±åº¦ç»†åŒ–å’Œè§†å›¾åˆæˆæŠ€æœ¯ï¼Œæ¢å¤3Dè§†è§‰ä»»åŠ¡ä¸­çš„ç»†ç²’åº¦è½¯è¾¹ç•Œç»†èŠ‚ã€‚è¯¥æ¡†æ¶é¦–å…ˆæå‡ºäº†ä¸€ç§æ–°é¢–çš„æ•°æ®æ•´ç†æµç¨‹ï¼Œåˆ©ç”¨å›¾åƒæŠ å›¾æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªæ·±åº¦ä¿®å¤ç½‘ç»œï¼Œè‡ªåŠ¨è¯†åˆ«è½¯è¾¹ç•ŒåŒºåŸŸã€‚æ·±åº¦ä¿®å¤ç½‘ç»œä½¿ç”¨é—¨æ§æ®‹å·®æ¨¡å—ï¼Œç²¾ç¡®ç»†åŒ–è½¯è¾¹ç•Œå‘¨å›´çš„æ·±åº¦ï¼ŒåŒæ—¶ä¿æŒå…¨å±€æ·±åº¦è´¨é‡ã€‚é€šè¿‡æ·±åº¦åŸºç¡€çš„å‰å‘æ‰­æ›²å’Œç”Ÿæˆåœºæ™¯ç»˜åˆ¶ï¼ŒHairGuardèƒ½å¤Ÿç”Ÿæˆå…·æœ‰ä¸€è‡´å‡ ä½•å½¢çŠ¶å’Œç»†ç²’åº¦ç»†èŠ‚çš„æ–°è§†å›¾ï¼Œæ˜¾è‘—æå‡äº†è½¯è¾¹ç•ŒåŒºåŸŸçš„è¡¨ç°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.24160', 'title': 'Towards Open-Vocabulary Industrial Defect Understanding with a Large-Scale Multimodal Dataset', 'url': 'https://huggingface.co/papers/2512.24160', 'abstract': 'A large-scale industrial multimodal defect dataset with 1 million image-text pairs enables efficient foundation model adaptation for manufacturing quality inspection and generation tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t We present IMDD-1M, the first large-scale Industrial Multimodal Defect Dataset comprising 1,000,000 aligned image-text pairs, designed to advance multimodal learning for manufacturing and quality inspection. IMDD-1M contains high-resolution real-world defects spanning over 60 material categories and more than 400 defect types, each accompanied by expert-verified annotations and fine-grained textual descriptions detailing defect location, severity, and contextual attributes. This dataset enables a wide spectrum of applications, including classification, segmentation, retrieval, captioning, and generative modeling. Building upon IMDD-1M, we train a diffusion-based vision-language foundation model from scratch, specifically tailored for industrial scenarios. The model serves as a generalizable foundation that can be efficiently adapted to specialized domains through lightweight fine-tuning. With less than 5% of the task-specific data required by dedicated expert models, it achieves comparable performance, highlighting the potential of data-efficient foundation model adaptation for industrial inspection and generation, paving the way for scalable, domain-adaptive, and knowledge-grounded manufacturing intelligence.', 'score': 2, 'issue_id': 497, 'pub_date': '2026-12-30', 'pub_date_card': {'ru': '30 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 30', 'zh': '12æœˆ30æ—¥'}, 'hash': '9eb9ce39c2f3de67', 'authors': ['TsaiChing Ni', 'ZhenQi Chen', 'YuanFu Yang'], 'affiliations': ['Institute of Intelligent Systems, National Yang Ming Chiao Tung University'], 'pdf_title_img': 'assets/pdf/title_img/2512.24160.jpg', 'data': {'categories': ['#open_source', '#dataset', '#transfer_learning', '#diffusion', '#synthetic', '#multimodal', '#training'], 'emoji': 'ğŸ­', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸ Ğ´ĞµÑ„ĞµĞºÑ‚Ğ¾Ğ² Ñ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸ĞµĞ¹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° IMDD-1M â€” Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ´ĞµÑ„ĞµĞºÑ‚Ğ¾Ğ² Ñ 1 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ¼ Ğ¿Ğ°Ñ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ĞµĞµ 60 ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¹ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ² Ğ¸ 400 Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ´ĞµÑ„ĞµĞºÑ‚Ğ¾Ğ² Ñ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ foundation model, ÑĞ¾Ğ²Ğ¼ĞµÑ‰Ğ°ÑÑ‰Ğ°Ñ Ğ²Ğ¸Ğ´ĞµĞ½Ğ¸Ğµ Ğ¸ ÑĞ·Ñ‹Ğº, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğº ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ğ¼ Ñ‡ĞµÑ€ĞµĞ· Ğ»ĞµĞ³ĞºÑƒÑ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ, Ñ‚Ñ€ĞµĞ±ÑƒÑ Ğ¼ĞµĞ½ĞµĞµ 5% Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½Ğ¾-ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. Ğ­Ñ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğµ.'}, 'en': {'title': 'Revolutionizing Manufacturing with IMDD-1M: A Million Defects for Smarter AI', 'desc': 'The paper introduces IMDD-1M, a comprehensive dataset containing 1 million image-text pairs focused on industrial defects, aimed at enhancing multimodal learning in manufacturing. This dataset features high-resolution images of defects across various materials, each paired with detailed textual descriptions that include annotations on defect characteristics. The authors develop a diffusion-based vision-language foundation model that can be fine-tuned for specific industrial tasks, demonstrating its versatility and efficiency. Remarkably, this model requires significantly less task-specific data while maintaining high performance, showcasing the effectiveness of foundation model adaptation in industrial applications.'}, 'zh': {'title': 'æ¨åŠ¨å·¥ä¸šæ™ºèƒ½çš„å¤šæ¨¡æ€ç¼ºé™·æ•°æ®é›†', 'desc': 'æœ¬æ–‡ä»‹ç»äº†IMDD-1Mï¼Œè¿™æ˜¯é¦–ä¸ªå¤§è§„æ¨¡å·¥ä¸šå¤šæ¨¡æ€ç¼ºé™·æ•°æ®é›†ï¼ŒåŒ…å«100ä¸‡ä¸ªå¯¹é½çš„å›¾åƒ-æ–‡æœ¬å¯¹ï¼Œæ—¨åœ¨æ¨åŠ¨åˆ¶é€ å’Œè´¨é‡æ£€æµ‹çš„å¤šæ¨¡æ€å­¦ä¹ ã€‚è¯¥æ•°æ®é›†æ¶µç›–60å¤šç§ææ–™ç±»åˆ«å’Œ400å¤šç§ç¼ºé™·ç±»å‹ï¼Œé…æœ‰ä¸“å®¶éªŒè¯çš„æ³¨é‡Šå’Œè¯¦ç»†çš„æ–‡æœ¬æè¿°ï¼Œè¯´æ˜ç¼ºé™·çš„ä½ç½®ã€ä¸¥é‡ç¨‹åº¦å’Œä¸Šä¸‹æ–‡å±æ€§ã€‚IMDD-1Mæ”¯æŒåˆ†ç±»ã€åˆ†å‰²ã€æ£€ç´¢ã€æè¿°ç”Ÿæˆå’Œç”Ÿæˆå»ºæ¨¡ç­‰å¤šç§åº”ç”¨ã€‚åŸºäºIMDD-1Mï¼Œæˆ‘ä»¬ä»é›¶å¼€å§‹è®­ç»ƒäº†ä¸€ä¸ªåŸºäºæ‰©æ•£çš„è§†è§‰-è¯­è¨€åŸºç¡€æ¨¡å‹ï¼Œä¸“é—¨é’ˆå¯¹å·¥ä¸šåœºæ™¯ï¼Œå±•ç¤ºäº†æ•°æ®é«˜æ•ˆçš„åŸºç¡€æ¨¡å‹é€‚åº”æ€§åœ¨å·¥ä¸šæ£€æµ‹å’Œç”Ÿæˆä¸­çš„æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.23628', 'title': 'Memorization in 3D Shape Generation: An Empirical Study', 'url': 'https://huggingface.co/papers/2512.23628', 'abstract': 'Researchers develop a framework to measure memorization in 3D generative models and identify factors affecting it, finding that data modality and model design parameters influence how much training data is remembered during generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Generative models are increasingly used in 3D vision to synthesize novel shapes, yet it remains unclear whether their generation relies on memorizing training shapes. Understanding their memorization could help prevent training data leakage and improve the diversity of generated results. In this paper, we design an evaluation framework to quantify memorization in 3D generative models and study the influence of different data and modeling designs on memorization. We first apply our framework to quantify memorization in existing methods. Next, through controlled experiments with a latent vector-set (Vecset) diffusion model, we find that, on the data side, memorization depends on data modality, and increases with data diversity and finer-grained conditioning; on the modeling side, it peaks at a moderate guidance scale and can be mitigated by longer Vecsets and simple rotation augmentation. Together, our framework and analysis provide an empirical understanding of memorization in 3D generative models and suggest simple yet effective strategies to reduce it without degrading generation quality. Our code is available at https://github.com/zlab-princeton/3d_mem.', 'score': 2, 'issue_id': 493, 'pub_date': '2026-12-29', 'pub_date_card': {'ru': '29 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 29', 'zh': '12æœˆ29æ—¥'}, 'hash': 'ae6b1f98f106b276', 'authors': ['Shu Pu', 'Boya Zeng', 'Kaichen Zhou', 'Mengyu Wang', 'Zhuang Liu'], 'affiliations': ['Harvard University', 'Princeton University'], 'pdf_title_img': 'assets/pdf/title_img/2512.23628.jpg', 'data': {'categories': ['#benchmark', '#diffusion', '#open_source', '#3d', '#leakage'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞšĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ¼ĞµĞ¼Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ¼Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ (Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ñ) Ğ² Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ»Ğ¸ÑÑÑ‚ Ğ½Ğ° ÑÑ‚ĞµĞ¿ĞµĞ½ÑŒ, Ñ ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ğ¸Ğ· Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰ĞµĞ³Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°. Ğ§ĞµÑ€ĞµĞ· ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ğµ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Vecset Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµĞ¼Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ñ‚ Ğ¾Ñ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ° Ñ‡ĞµÑ€ĞµĞ· Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² guidance. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ ÑƒÑ‚ĞµÑ‡ĞºÑƒ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Measuring and Mitigating Memorization in 3D Generative Models', 'desc': 'This paper presents a framework to assess how much 3D generative models memorize their training data during the generation process. The researchers found that factors such as the type of data used and the design of the model significantly affect memorization levels. Their experiments revealed that increased data diversity and finer conditioning lead to higher memorization, while certain modeling techniques can help reduce it. The findings aim to enhance the understanding of memorization in generative models, which can help improve the quality and diversity of generated outputs.'}, 'zh': {'title': 'é‡åŒ–3Dç”Ÿæˆæ¨¡å‹ä¸­çš„è®°å¿†ç°è±¡', 'desc': 'æœ¬ç ”ç©¶å¼€å‘äº†ä¸€ä¸ªæ¡†æ¶ï¼Œç”¨äºæµ‹é‡3Dç”Ÿæˆæ¨¡å‹ä¸­çš„è®°å¿†ç°è±¡ï¼Œå¹¶è¯†åˆ«å½±å“å› ç´ ã€‚ç ”ç©¶å‘ç°ï¼Œæ•°æ®æ¨¡æ€å’Œæ¨¡å‹è®¾è®¡å‚æ•°ä¼šå½±å“è®­ç»ƒæ•°æ®åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­çš„è®°å¿†ç¨‹åº¦ã€‚é€šè¿‡å¯¹ç°æœ‰æ–¹æ³•çš„è¯„ä¼°ï¼Œæˆ‘ä»¬é‡åŒ–äº†è®°å¿†ç°è±¡ï¼Œå¹¶é€šè¿‡å®éªŒå‘ç°ï¼Œè®°å¿†ä¸æ•°æ®å¤šæ ·æ€§å’Œç»†ç²’åº¦æ¡ä»¶æœ‰å…³ã€‚æˆ‘ä»¬çš„æ¡†æ¶å’Œåˆ†æä¸ºç†è§£3Dç”Ÿæˆæ¨¡å‹ä¸­çš„è®°å¿†æä¾›äº†å®è¯ä¾æ®ï¼Œå¹¶æå‡ºäº†æœ‰æ•ˆçš„å‡å°‘è®°å¿†çš„æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.04620', 'title': 'AgentDevel: Reframing Self-Evolving LLM Agents as Release Engineering', 'url': 'https://huggingface.co/papers/2601.04620', 'abstract': 'AgentDevel presents a release engineering approach for large language model agents that treats them as shippable artifacts and emphasizes stable, auditable improvements through externalized testing and diagnostic processes.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent progress in large language model (LLM) agents has largely focused on embedding self-improvement mechanisms inside the agent or searching over many concurrent variants. While these approaches can raise aggregate scores, they often yield unstable and hard-to-audit improvement trajectories, making it difficult to guarantee non-regression or to reason about failures across versions. We reframe agent improvement as release engineering: agents are treated as shippable artifacts, and improvement is externalized into a regression-aware release pipeline. We introduce AgentDevel, a release engineering pipeline that iteratively runs the current agent, produces implementation-blind, symptom-level quality signals from execution traces, synthesizes a single release candidate (RC) via executable diagnosis, and promotes it under flip-centered gating. AgentDevel features three core designs: (i) an implementation-blind LLM critic that characterizes failure appearances without accessing agent internals, (ii) script-based executable diagnosis that aggregates dominant symptom patterns and produces auditable engineering specifications, and (iii) flip-centered gating that prioritizes pass to fail regressions and fail to pass fixes as first-class evidence. Unlike population-based search or in-agent self-refinement, AgentDevel maintains a single canonical version line and emphasizes non-regression as a primary objective. Experiments on execution-heavy benchmarks demonstrate that AgentDevel yields stable improvements with significantly fewer regressions while producing reproducible, auditable artifacts. Overall, AgentDevel provides a practical development discipline for building, debugging, and releasing LLM agents as software development.', 'score': 1, 'issue_id': 499, 'pub_date': '2026-01-08', 'pub_date_card': {'ru': '8 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 8', 'zh': '1æœˆ8æ—¥'}, 'hash': '0dcce10378a61ef8', 'authors': ['Di Zhang'], 'affiliations': ['Fudan University'], 'pdf_title_img': 'assets/pdf/title_img/2601.04620.jpg', 'data': {'categories': ['#open_source'], 'emoji': 'ğŸ“¦', 'ru': {'title': 'LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ ĞºĞ°Ğº ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ğ¹ ÑĞ¾Ñ„Ñ‚Ğ²ĞµÑ€: Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ñ Ñ€ĞµĞ»Ğ¸Ğ·Ğ¾Ğ²', 'desc': 'AgentDevel Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ°Ñ… Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ğ¸ Ñ€ĞµĞ»Ğ¸Ğ·Ğ¾Ğ², Ğ³Ğ´Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ ĞºĞ°Ğº Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ñ‹Ğµ Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ñ‹. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² ÑĞ°Ğ¼Ğ¾ÑƒÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°, ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸ÑÑ…Ğ¾Ğ´ÑÑ‚ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¹. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚Ñ€Ñ‘Ñ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ: Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ñ‡ĞµÑ€ĞµĞ· Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾-Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾Ğ³Ğ¾ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ° Ğ½Ğ° LLM, ÑĞ¸Ğ½Ñ‚ĞµĞ· ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ° Ğ½Ğ° Ñ€ĞµĞ»Ğ¸Ğ· Ñ‡ĞµÑ€ĞµĞ· Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼ÑƒÑ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºÑƒ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼Ğ¾Ñ†Ğ¸Ñ Ğ²ĞµÑ€ÑĞ¸Ğ¸ Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼ Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ AgentDevel Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ğµ Ğ¸ Ğ°ÑƒĞ´Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸ÑĞ¼Ğ¸ Ğ² ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¿Ğ¾ Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸.'}, 'en': {'title': 'Stable and Auditable Improvements for LLM Agents', 'desc': "AgentDevel introduces a new way to improve large language model (LLM) agents by treating them like software products that can be released and updated. Instead of embedding self-improvement directly in the agents, it uses a structured release engineering approach that focuses on stability and auditability. The system generates quality signals from the agents' performance, allowing for better diagnosis of issues without needing to look inside the agent's code. This method ensures that improvements are consistent and verifiable, reducing the chances of regressions and making the development process more reliable."}, 'zh': {'title': 'AgentDevelï¼šç¨³å®šå¯å®¡è®¡çš„å¤§å‹è¯­è¨€æ¨¡å‹ä»£ç†å‘å¸ƒå·¥ç¨‹', 'desc': 'AgentDevelæå‡ºäº†ä¸€ç§é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ä»£ç†çš„å‘å¸ƒå·¥ç¨‹æ–¹æ³•ï¼Œå°†å…¶è§†ä¸ºå¯äº¤ä»˜çš„å·¥ä»¶ï¼Œå¹¶å¼ºè°ƒé€šè¿‡å¤–éƒ¨æµ‹è¯•å’Œè¯Šæ–­è¿‡ç¨‹å®ç°ç¨³å®šã€å¯å®¡è®¡çš„æ”¹è¿›ã€‚ä¸ä¼ ç»Ÿçš„è‡ªæˆ‘æ”¹è¿›æœºåˆ¶ä¸åŒï¼Œè¯¥æ–¹æ³•å°†ä»£ç†çš„æ”¹è¿›å¤–éƒ¨åŒ–åˆ°ä¸€ä¸ªå›å½’æ„ŸçŸ¥çš„å‘å¸ƒç®¡é“ä¸­ã€‚AgentDevelçš„æ ¸å¿ƒè®¾è®¡åŒ…æ‹¬ä¸€ä¸ªä¸ä¾èµ–å®ç°çš„LLMè¯„è®ºå™¨ã€åŸºäºè„šæœ¬çš„å¯æ‰§è¡Œè¯Šæ–­å’Œä»¥ç¿»è½¬ä¸ºä¸­å¿ƒçš„é—¨æ§æœºåˆ¶ã€‚å®éªŒè¡¨æ˜ï¼ŒAgentDevelèƒ½å¤Ÿåœ¨å‡å°‘å›å½’çš„åŒæ—¶ï¼Œå®ç°ç¨³å®šçš„æ”¹è¿›ï¼Œå¹¶ç”Ÿæˆå¯é‡å¤ã€å¯å®¡è®¡çš„å·¥ä»¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.04300', 'title': 'Beyond Binary Preference: Aligning Diffusion Models to Fine-grained Criteria by Decoupling Attributes', 'url': 'https://huggingface.co/papers/2601.04300', 'abstract': 'A two-stage framework for diffusion model alignment using hierarchical evaluation criteria and complex preference optimization demonstrates improved generation quality and expert alignment.  \t\t\t\t\tAI-generated summary \t\t\t\t Post-training alignment of diffusion models relies on simplified signals, such as scalar rewards or binary preferences. This limits alignment with complex human expertise, which is hierarchical and fine-grained. To address this, we first construct a hierarchical, fine-grained evaluation criteria with domain experts, which decomposes image quality into multiple positive and negative attributes organized in a tree structure. Building on this, we propose a two-stage alignment framework. First, we inject domain knowledge to an auxiliary diffusion model via Supervised Fine-Tuning. Second, we introduce Complex Preference Optimization (CPO) that extends DPO to align the target diffusion to our non-binary, hierarchical criteria. Specifically, we reformulate the alignment problem to simultaneously maximize the probability of positive attributes while minimizing the probability of negative attributes with the auxiliary diffusion. We instantiate our approach in the domain of painting generation and conduct CPO training with an annotated dataset of painting with fine-grained attributes based on our criteria. Extensive experiments demonstrate that CPO significantly enhances generation quality and alignment with expertise, opening new avenues for fine-grained criteria alignment.', 'score': 1, 'issue_id': 498, 'pub_date': '2026-01-07', 'pub_date_card': {'ru': '7 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 7', 'zh': '1æœˆ7æ—¥'}, 'hash': '465dd03a32e42d19', 'authors': ['Chenye Meng', 'Zejian Li', 'Zhongni Liu', 'Yize Li', 'Changle Xie', 'Kaixin Jia', 'Ling Yang', 'Huanghuang Deng', 'Shiying Ding', 'Shengyuan Zhang', 'Jiayi Li', 'Lingyun Sun'], 'affiliations': ['Peking University', 'University of Electronic Science and Technology of China', 'University of Nottingham Ningbo China', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2601.04300.jpg', 'data': {'categories': ['#diffusion', '#alignment'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ˜ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸ĞµĞ² Ğ¾Ñ†ĞµĞ½ĞºĞ¸. ĞĞ° Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼ supervised fine-tuning Ñ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸ĞµĞ¼ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. ĞĞ° Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Complex Preference Optimization (CPO), Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑÑÑ‰Ğ¸Ğ¹ DPO Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğ¼Ğ¸ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸ÑĞ¼Ğ¸ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ±Ğ¸Ğ½Ğ°Ñ€Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ¾Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚Ñ€Ğ¸Ñ†Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ…, Ñ‡Ñ‚Ğ¾ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ÑÑ Ñ‡ĞµÑ€ĞµĞ· Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸Ğº ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°.'}, 'en': {'title': 'Enhancing Diffusion Models with Hierarchical Expert Alignment', 'desc': "This paper presents a two-stage framework designed to improve the alignment of diffusion models with complex human preferences in image generation. It begins by creating a hierarchical evaluation system that breaks down image quality into detailed attributes, allowing for a more nuanced assessment. The framework then employs Supervised Fine-Tuning to incorporate expert knowledge into the diffusion model, followed by Complex Preference Optimization (CPO) to refine the model's outputs based on these detailed criteria. Experimental results show that this approach significantly enhances both the quality of generated images and their alignment with expert evaluations."}, 'zh': {'title': 'æå‡æ‰©æ•£æ¨¡å‹å¯¹é½çš„åˆ›æ–°æ¡†æ¶', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œç”¨äºå¯¹æ‰©æ•£æ¨¡å‹è¿›è¡Œå¯¹é½ï¼Œé‡‡ç”¨åˆ†å±‚è¯„ä¼°æ ‡å‡†å’Œå¤æ‚åå¥½ä¼˜åŒ–ã€‚ä¼ ç»Ÿçš„åè®­ç»ƒå¯¹é½æ–¹æ³•ä¾èµ–äºç®€åŒ–ä¿¡å·ï¼Œå¦‚æ ‡é‡å¥–åŠ±æˆ–äºŒå…ƒåå¥½ï¼Œè¿™é™åˆ¶äº†ä¸å¤æ‚äººç±»ä¸“ä¸šçŸ¥è¯†çš„å¯¹é½ã€‚æˆ‘ä»¬é¦–å…ˆä¸é¢†åŸŸä¸“å®¶æ„å»ºäº†ä¸€ä¸ªåˆ†å±‚çš„ç»†ç²’åº¦è¯„ä¼°æ ‡å‡†ï¼Œå°†å›¾åƒè´¨é‡åˆ†è§£ä¸ºå¤šä¸ªæ­£é¢å’Œè´Ÿé¢å±æ€§ï¼Œå¹¶ä»¥æ ‘çŠ¶ç»“æ„ç»„ç»‡ã€‚é€šè¿‡å¼•å…¥å¤æ‚åå¥½ä¼˜åŒ–ï¼ˆCPOï¼‰ï¼Œæˆ‘ä»¬èƒ½å¤ŸåŒæ—¶æœ€å¤§åŒ–æ­£é¢å±æ€§çš„æ¦‚ç‡ï¼ŒåŒæ—¶æœ€å°åŒ–è´Ÿé¢å±æ€§çš„æ¦‚ç‡ï¼Œä»è€Œæ˜¾è‘—æé«˜ç”Ÿæˆè´¨é‡å’Œä¸ä¸“ä¸šçŸ¥è¯†çš„å¯¹é½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.02702', 'title': 'Learning User Preferences Through Interaction for Long-Term Collaboration', 'url': 'https://huggingface.co/papers/2601.02702', 'abstract': "MultiSessionCollab benchmark evaluates agents' ability to learn and adapt to user preferences through persistent memory systems that enhance long-term collaboration quality.  \t\t\t\t\tAI-generated summary \t\t\t\t As conversational agents accumulate experience collaborating with users, adapting to user preferences is essential for fostering long-term relationships and improving collaboration quality over time. We introduce MultiSessionCollab, a benchmark that evaluates how well agents can learn user preferences and leverage them to improve collaboration quality throughout multiple sessions. To develop agents that succeed in this setting, we present long-term collaborative agents equipped with a memory that persists and refines user preference as interaction experience accumulates. Moreover, we demonstrate that learning signals can be derived from user simulator behavior in MultiSessionCollab to train agents to generate more comprehensive reflections and update their memory more effectively. Extensive experiments show that equipping agents with memory improves long-term collaboration, yielding higher task success rates, more efficient interactions, and reduced user effort. Finally, we conduct a human user study that demonstrates that memory helps improve user experience in real-world settings.", 'score': 1, 'issue_id': 508, 'pub_date': '2026-01-06', 'pub_date_card': {'ru': '6 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 6', 'zh': '1æœˆ6æ—¥'}, 'hash': '9b399a11240ed646', 'authors': ['Shuhaib Mehri', 'Priyanka Kargupta', 'Tal August', 'Dilek Hakkani-TÃ¼r'], 'affiliations': ['University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2601.02702.jpg', 'data': {'categories': ['#benchmark', '#agents'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞĞ³ĞµĞ½Ñ‚Ñ‹ Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ Ğ´Ğ»Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ»Ğ»Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼Ğ¸', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº MultiSessionCollab Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒÑÑ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğº Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ğµ ĞºĞ¾Ğ»Ğ»Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ñ Ğ¿ĞµÑ€ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ½Ğ°ĞºĞ°Ğ¿Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¸ ÑƒÑ‚Ğ¾Ñ‡Ğ½ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ Ğ¿Ğ¾ Ğ¼ĞµÑ€Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ. ĞĞ½Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°Ñ‚ÑŒ Ğ¸Ğ· Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ° Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ Ğ´Ğ»Ñ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğµ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ĞµĞµ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑÑ‚ÑŒ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾ÑĞ½Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ»Ğ»Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğ¹ Ğ¾Ğ¿Ñ‹Ñ‚.'}, 'en': {'title': 'Enhancing Collaboration with Memory-Driven Agents', 'desc': 'The MultiSessionCollab benchmark assesses how well agents can learn and adapt to user preferences over time using persistent memory systems. This approach allows agents to enhance the quality of collaboration by refining their understanding of user needs through multiple interactions. By leveraging learning signals from user simulator behavior, agents can improve their memory updates and generate better reflections on user preferences. Experimental results indicate that agents with memory not only achieve higher task success rates but also create more efficient interactions, ultimately leading to a better user experience.'}, 'zh': {'title': 'æå‡é•¿æœŸåˆä½œè´¨é‡çš„æ™ºèƒ½ä½“è®°å¿†ç³»ç»Ÿ', 'desc': 'MultiSessionCollabåŸºå‡†æµ‹è¯•è¯„ä¼°æ™ºèƒ½ä½“åœ¨é•¿æœŸåˆä½œä¸­å­¦ä¹ å’Œé€‚åº”ç”¨æˆ·åå¥½çš„èƒ½åŠ›ã€‚é€šè¿‡æŒä¹…çš„è®°å¿†ç³»ç»Ÿï¼Œæ™ºèƒ½ä½“èƒ½å¤Ÿåœ¨å¤šä¸ªä¼šè¯ä¸­ä¸æ–­æ”¹è¿›åˆä½œè´¨é‡ã€‚æˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•åˆ©ç”¨ç”¨æˆ·æ¨¡æ‹Ÿå™¨çš„è¡Œä¸ºä¿¡å·æ¥è®­ç»ƒæ™ºèƒ½ä½“ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°æ›´æ–°è®°å¿†å¹¶ç”Ÿæˆæ›´å…¨é¢çš„åæ€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå…·å¤‡è®°å¿†çš„æ™ºèƒ½ä½“åœ¨é•¿æœŸåˆä½œä¸­è¡¨ç°æ›´ä½³ï¼Œä»»åŠ¡æˆåŠŸç‡æ›´é«˜ï¼Œäº¤äº’æ•ˆç‡æ›´é«˜ï¼Œç”¨æˆ·çš„åŠªåŠ›ä¹Ÿå‡å°‘äº†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.02016', 'title': 'Enhancing Object Detection with Privileged Information: A Model-Agnostic Teacher-Student Approach', 'url': 'https://huggingface.co/papers/2601.02016', 'abstract': 'Learning Using Privileged Information paradigm enhances object detection accuracy by integrating additional training-time information through teacher-student architectures without increasing inference complexity.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper investigates the integration of the Learning Using Privileged Information (LUPI) paradigm in object detection to exploit fine-grained, descriptive information available during training but not at inference. We introduce a general, model-agnostic methodology for injecting privileged information-such as bounding box masks, saliency maps, and depth cues-into deep learning-based object detectors through a teacher-student architecture. Experiments are conducted across five state-of-the-art object detection models and multiple public benchmarks, including UAV-based litter detection datasets and Pascal VOC 2012, to assess the impact on accuracy, generalization, and computational efficiency. Our results demonstrate that LUPI-trained students consistently outperform their baseline counterparts, achieving significant boosts in detection accuracy with no increase in inference complexity or model size. Performance improvements are especially marked for medium and large objects, while ablation studies reveal that intermediate weighting of teacher guidance optimally balances learning from privileged and standard inputs. The findings affirm that the LUPI framework provides an effective and practical strategy for advancing object detection systems in both resource-constrained and real-world settings.', 'score': 1, 'issue_id': 496, 'pub_date': '2026-01-05', 'pub_date_card': {'ru': '5 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 5', 'zh': '1æœˆ5æ—¥'}, 'hash': '661151975c914811', 'authors': ['Matthias Bartolo', 'Dylan Seychell', 'Gabriel Hili', 'Matthew Montebello', 'Carl James Debono', 'Saviour Formosa', 'Konstantinos Makantasis'], 'affiliations': ['Department of Artificial Communications of Malta', 'Department of Communications and Computer Engineering, Faculty of Information and Communications Technology, University of Malta', 'Department of Criminology, Faculty of Social Wellbeing, University of Malta', 'Faculty Intelligence, Technology, University'], 'pdf_title_img': 'assets/pdf/title_img/2601.02016.jpg', 'data': {'categories': ['#cv', '#training', '#benchmark'], 'emoji': 'ğŸ“', 'ru': {'title': 'ĞŸÑ€Ğ¸Ğ²Ğ¸Ğ»ĞµĞ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ²', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€Ğ¸Ğ²Ğ¸Ğ»ĞµĞ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹ (LUPI) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ-Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸, Ñ‚Ğ°ĞºĞ¾Ğ¹ ĞºĞ°Ğº Ğ¼Ğ°ÑĞºĞ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ğ±Ğ¾ĞºÑĞ¾Ğ², ĞºĞ°Ñ€Ñ‚Ñ‹ Ğ·Ğ°Ğ¼ĞµÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ĞºĞ°Ñ€Ñ‚Ñ‹ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹, Ğ² Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒ-ÑƒÑ‡ĞµĞ½Ğ¸Ğº Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ¿Ñ€Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ° Ğ½Ğ° Ğ¿ÑÑ‚Ğ¸ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¾Ñ€Ğ°Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¾Ğ±Ñ‰ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ², Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ UAV-Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸ Pascal VOC 2012. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸-ÑƒÑ‡ĞµĞ½Ğ¸ĞºĞ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ LUPI, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ñ‹ Ğ±ĞµĞ· ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Boosting Object Detection with Privileged Information', 'desc': 'This paper explores the Learning Using Privileged Information (LUPI) approach to improve object detection by utilizing extra information available during training. It presents a flexible method to incorporate privileged data, like bounding box masks and depth cues, into existing deep learning models using a teacher-student framework. The experiments show that models trained with LUPI significantly enhance detection accuracy without adding complexity during inference. The results indicate that this method is particularly beneficial for detecting medium and large objects, making it a valuable strategy for real-world applications.'}, 'zh': {'title': 'åˆ©ç”¨ç‰¹æƒä¿¡æ¯æå‡ç›®æ ‡æ£€æµ‹ç²¾åº¦', 'desc': 'æœ¬æ–‡æ¢è®¨äº†åœ¨ç›®æ ‡æ£€æµ‹ä¸­æ•´åˆç‰¹æƒä¿¡æ¯å­¦ä¹ ï¼ˆLUPIï¼‰èŒƒå¼ï¼Œä»¥åˆ©ç”¨è®­ç»ƒæœŸé—´å¯ç”¨ä½†æ¨ç†æ—¶ä¸å¯ç”¨çš„ç»†ç²’åº¦æè¿°ä¿¡æ¯ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§é€šç”¨çš„ã€ä¸æ¨¡å‹æ— å…³çš„æ–¹æ³•ï¼Œé€šè¿‡æ•™å¸ˆ-å­¦ç”Ÿæ¶æ„å°†ç‰¹æƒä¿¡æ¯ï¼ˆå¦‚è¾¹ç•Œæ¡†æ©ç ã€æ˜¾è‘—æ€§å›¾å’Œæ·±åº¦çº¿ç´¢ï¼‰æ³¨å…¥åŸºäºæ·±åº¦å­¦ä¹ çš„ç›®æ ‡æ£€æµ‹å™¨ä¸­ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç»è¿‡LUPIè®­ç»ƒçš„å­¦ç”Ÿæ¨¡å‹åœ¨å‡†ç¡®æ€§ã€æ³›åŒ–èƒ½åŠ›å’Œè®¡ç®—æ•ˆç‡ä¸Šå‡ä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œå°¤å…¶åœ¨ä¸­å¤§ç‰©ä½“çš„æ£€æµ‹ä¸­è¡¨ç°æ˜¾è‘—æå‡ã€‚ç ”ç©¶ç»“æœç¡®è®¤LUPIæ¡†æ¶ä¸ºåœ¨èµ„æºå—é™å’Œç°å®ä¸–ç•Œç¯å¢ƒä¸­æå‡ç›®æ ‡æ£€æµ‹ç³»ç»Ÿæä¾›äº†ä¸€ç§æœ‰æ•ˆä¸”å®ç”¨çš„ç­–ç•¥ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.04233', 'title': 'LEMAS: Large A 150K-Hour Large-scale Extensible Multilingual Audio Suite with Generative Speech Models', 'url': 'https://huggingface.co/papers/2601.04233', 'abstract': "The LEMAS-Dataset enables high-quality multilingual speech synthesis and editing through specialized models leveraging flow-matching and autoregressive architectures with novel training techniques.  \t\t\t\t\tAI-generated summary \t\t\t\t We present the LEMAS-Dataset, which, to our knowledge, is currently the largest open-source multilingual speech corpus with word-level timestamps. Covering over 150,000 hours across 10 major languages, LEMAS-Dataset is constructed via a efficient data processing pipeline that ensures high-quality data and annotations. To validate the effectiveness of LEMAS-Dataset across diverse generative paradigms, we train two benchmark models with distinct architectures and task specializations on this dataset. LEMAS-TTS, built upon a non-autoregressive flow-matching framework, leverages the dataset's massive scale and linguistic diversity to achieve robust zero-shot multilingual synthesis. Our proposed accent-adversarial training and CTC loss mitigate cross-lingual accent issues, enhancing synthesis stability. Complementarily, LEMAS-Edit employs an autoregressive decoder-only architecture that formulates speech editing as a masked token infilling task. By exploiting precise word-level alignments to construct training masks and adopting adaptive decoding strategies, it achieves seamless, smooth-boundary speech editing with natural transitions. Experimental results demonstrate that models trained on LEMAS-Dataset deliver high-quality synthesis and editing performance, confirming the dataset's quality. We envision that this richly timestamp-annotated, fine-grained multilingual corpus will drive future advances in prompt-based speech generation systems.", 'score': 1, 'issue_id': 503, 'pub_date': '2026-01-04', 'pub_date_card': {'ru': '4 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 4', 'zh': '1æœˆ4æ—¥'}, 'hash': 'ce6f38f3022fb1cb', 'authors': ['Zhiyuan Zhao', 'Lijian Lin', 'Ye Zhu', 'Kai Xie', 'Yunfei Liu', 'Yu Li'], 'affiliations': ['International Digital Economy Academy (IDEA)'], 'pdf_title_img': 'assets/pdf/title_img/2601.04233.jpg', 'data': {'categories': ['#training', '#open_source', '#low_resource', '#optimization', '#architecture', '#dataset', '#multilingual', '#audio'], 'emoji': 'ğŸ—£ï¸', 'ru': {'title': 'ĞœĞ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¾Ğ¹ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ', 'desc': "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° LEMAS-Dataset â€” ĞºÑ€ÑƒĞ¿Ğ½ĞµĞ¹ÑˆĞ¸Ğ¹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ ĞºĞ¾Ñ€Ğ¿ÑƒÑ Ñ€ĞµÑ‡Ğ¸ Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¾Ğ¹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑĞ»Ğ¾Ğ², ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ĞµĞµ 150 Ñ‚Ñ‹ÑÑÑ‡ Ñ‡Ğ°ÑĞ¾Ğ² Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ½Ğ° 10 ÑĞ·Ñ‹ĞºĞ°Ñ…. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ñ‹ Ğ´Ğ²Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: LEMAS-TTS Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ flow-matching Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ñ€ĞµÑ‡Ğ¸ Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸ĞµĞ¼ adversarial Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ°ĞºÑ†ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ğ¾Ğ², Ğ° LEMAS-Edit Ñ€ĞµĞ°Ğ»Ğ¸Ğ·ÑƒĞµÑ‚ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€ĞµÑ‡Ğ¸ Ñ‡ĞµÑ€ĞµĞ· autoregressive decoder Ñ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¸Ğ»Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¾Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². Ğ¢Ğ¾Ñ‡Ğ½Ğ°Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ° Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğº ÑĞ»Ğ¾Ğ² Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ¿Ğ»Ğ°Ğ²Ğ½Ñ‹Ñ… Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ°Ñ… Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ñ‹ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµÑ‡Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ prompt'Ğ¾Ğ² Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°."}, 'en': {'title': 'Unlocking Multilingual Speech Synthesis with LEMAS-Dataset', 'desc': "The LEMAS-Dataset is a large open-source multilingual speech corpus designed for high-quality speech synthesis and editing. It includes over 150,000 hours of audio in 10 languages, with precise word-level timestamps to enhance data quality. Two models, LEMAS-TTS and LEMAS-Edit, utilize different architectures to demonstrate effective multilingual synthesis and speech editing capabilities. The dataset's innovative training techniques, including accent-adversarial training, improve performance across diverse languages and accents, paving the way for future advancements in speech generation systems."}, 'zh': {'title': 'LEMAS-Datasetï¼šå¤šè¯­è¨€è¯­éŸ³åˆæˆä¸ç¼–è¾‘çš„æœªæ¥', 'desc': 'LEMAS-Datasetæ˜¯ä¸€ä¸ªé«˜è´¨é‡çš„å¤šè¯­è¨€è¯­éŸ³åˆæˆå’Œç¼–è¾‘æ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡150,000å°æ—¶çš„è¯­éŸ³æ•°æ®ï¼Œè¦†ç›–10ç§ä¸»è¦è¯­è¨€ã€‚è¯¥æ•°æ®é›†é€šè¿‡é«˜æ•ˆçš„æ•°æ®å¤„ç†æµç¨‹æ„å»ºï¼Œç¡®ä¿äº†æ•°æ®å’Œæ³¨é‡Šçš„é«˜è´¨é‡ã€‚æˆ‘ä»¬è®­ç»ƒäº†ä¸¤ç§åŸºå‡†æ¨¡å‹ï¼Œåˆ†åˆ«é‡‡ç”¨éè‡ªå›å½’å’Œè‡ªå›å½’æ¶æ„ï¼Œä»¥éªŒè¯LEMAS-Datasetåœ¨ä¸åŒç”ŸæˆèŒƒå¼ä¸­çš„æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºLEMAS-Datasetè®­ç»ƒçš„æ¨¡å‹åœ¨è¯­éŸ³åˆæˆå’Œç¼–è¾‘æ€§èƒ½ä¸Šè¡¨ç°å‡ºè‰²ï¼Œè¯æ˜äº†è¯¥æ•°æ®é›†çš„è´¨é‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.05125', 'title': 'VERSE: Visual Embedding Reduction and Space Exploration. Clustering-Guided Insights for Training Data Enhancement in Visually-Rich Document Understanding', 'url': 'https://huggingface.co/papers/2601.05125', 'abstract': 'VERSE is a methodology for analyzing and improving Vision-Language Models in document understanding by visualizing latent representations and generating synthetic data to enhance performance in error-prone clusters.  \t\t\t\t\tAI-generated summary \t\t\t\t This work introduces VERSE, a methodology for analyzing and improving Vision-Language Models applied to Visually-rich Document Understanding by exploring their visual embedding space. VERSE enables the visualization of latent representations, supporting the assessment of model feasibility. It also facilitates the identification of problematic regions and guides the generation of synthetic data to enhance performance in those clusters. We validate the methodology by training on the synthetic MERIT Dataset and evaluating on its real-world counterpart, MERIT Secret. Results show that VERSE helps uncover the visual features associated with error-prone clusters, and that retraining with samples containing these features substantially boosts F1 performance without degrading generalization. Furthermore, we demonstrate that on-premise models such as Donut and Idefics2, when optimized with VERSE, match or even surpass the performance of SaaS solutions like GPT-4 and Pixtral.', 'score': 0, 'issue_id': 500, 'pub_date': '2026-01-08', 'pub_date_card': {'ru': '8 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 8', 'zh': '1æœˆ8æ—¥'}, 'hash': '2d5ee3972f5a6371', 'authors': ['Ignacio de Rodrigo', 'Alvaro J. Lopez-Lopez', 'Jaime Boal'], 'affiliations': ['Institute for Research in Technology, ICAI School of Engineering, Comillas Pontifical University, Calle Rey Francisco, 4, Madrid, 28008, Madrid, Spain'], 'pdf_title_img': 'assets/pdf/title_img/2601.05125.jpg', 'data': {'categories': ['#training', '#data', '#multimodal', '#cv', '#dataset'], 'emoji': 'ğŸ“Š', 'ru': {'title': 'Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ·: Ğ¿ÑƒÑ‚ÑŒ Ğº ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ñƒ Vision-Language Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'VERSE â€” ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Vision-Language Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². ĞĞ½Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ğ²Ñ‹ÑĞ²Ğ»ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ½Ñ‹Ğµ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ñ‹, Ğ³Ğ´Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾Ğ¿ÑƒÑĞºĞ°ĞµÑ‚ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ F1 Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ (Donut, Idefics2) ĞºĞ¾Ğ½ĞºÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ Ğ¾Ğ±Ğ»Ğ°Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸ÑĞ¼Ğ¸ (GPT-4, Pixtral).'}, 'en': {'title': 'Enhancing Vision-Language Models with VERSE', 'desc': "VERSE is a new method designed to analyze and improve Vision-Language Models specifically for understanding visually-rich documents. It allows researchers to visualize the hidden representations of these models, making it easier to identify areas where the model struggles. By generating synthetic data for these challenging areas, VERSE enhances the model's performance without losing its ability to generalize to new data. The methodology has been validated using the MERIT Dataset, showing significant improvements in model accuracy compared to existing solutions."}, 'zh': {'title': 'VERSEï¼šæå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„åˆ©å™¨', 'desc': 'VERSEæ˜¯ä¸€ç§ç”¨äºåˆ†æå’Œæ”¹è¿›è§†è§‰è¯­è¨€æ¨¡å‹çš„æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨è§†è§‰ä¸°å¯Œçš„æ–‡æ¡£ç†è§£ä¸­ã€‚å®ƒé€šè¿‡å¯è§†åŒ–æ½œåœ¨è¡¨ç¤ºï¼Œå¸®åŠ©è¯„ä¼°æ¨¡å‹çš„å¯è¡Œæ€§ï¼Œå¹¶è¯†åˆ«å‡ºé—®é¢˜åŒºåŸŸã€‚è¯¥æ–¹æ³•è¿˜æŒ‡å¯¼ç”Ÿæˆåˆæˆæ•°æ®ï¼Œä»¥æé«˜åœ¨è¿™äº›é”™è¯¯æ˜“å‘é›†ç¾¤ä¸­çš„æ€§èƒ½ã€‚é€šè¿‡åœ¨åˆæˆçš„MERITæ•°æ®é›†ä¸Šè®­ç»ƒå¹¶åœ¨çœŸå®çš„MERIT Secretæ•°æ®é›†ä¸Šè¯„ä¼°ï¼Œç»“æœè¡¨æ˜VERSEèƒ½å¤Ÿæ­ç¤ºä¸é”™è¯¯é›†ç¾¤ç›¸å…³çš„è§†è§‰ç‰¹å¾ï¼Œå¹¶æ˜¾è‘—æå‡F1æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.01887', 'title': 'Safety at One Shot: Patching Fine-Tuned LLMs with A Single Instance', 'url': 'https://huggingface.co/papers/2601.01887', 'abstract': 'Safety alignment of large language models can be fully recovered with a single safety example, maintaining utility and achieving convergence in few epochs through identified low-rank gradient structures.  \t\t\t\t\tAI-generated summary \t\t\t\t Fine-tuning safety-aligned large language models (LLMs) can substantially compromise their safety. Previous approaches require many safety samples or calibration sets, which not only incur significant computational overhead during realignment but also lead to noticeable degradation in model utility. Contrary to this belief, we show that safety alignment can be fully recovered with only a single safety example, without sacrificing utility and at minimal cost. Remarkably, this recovery is effective regardless of the number of harmful examples used in fine-tuning or the size of the underlying model, and convergence is achieved within just a few epochs. Furthermore, we uncover the low-rank structure of the safety gradient, which explains why such efficient correction is possible. We validate our findings across five safety-aligned LLMs and multiple datasets, demonstrating the generality of our approach.', 'score': 0, 'issue_id': 492, 'pub_date': '2026-01-05', 'pub_date_card': {'ru': '5 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 5', 'zh': '1æœˆ5æ—¥'}, 'hash': '3f5eb740e47ff8ee', 'authors': ['Jiawen Zhang', 'Lipeng He', 'Kejia Chen', 'Jian Lou', 'Jian Liu', 'Xiaohu Yang', 'Ruoxi Jia'], 'affiliations': ['Sun Yat-sen University', 'University of Waterloo', 'Virginia Tech', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2601.01887.jpg', 'data': {'categories': ['#alignment', '#training', '#optimization', '#rlhf'], 'emoji': 'ğŸ›¡ï¸', 'ru': {'title': 'ĞĞ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ° Ğ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾: Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ LLM Ñ‡ĞµÑ€ĞµĞ· Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€Ğ°Ğ½Ğ³Ğ¾Ğ²ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ñ€Ğ¾Ğ²Ğ½ĞµĞ½Ğ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ° Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ²ÑĞµĞ³Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ° Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ»Ğ¸ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€Ğ°Ğ½Ğ³Ğ¾Ğ²ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ² Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ°Ñ… Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠÑÑĞ½ÑĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‚Ğ°ĞºĞ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ğ¸. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ÑĞ¿Ğ¾Ñ… Ğ±ĞµĞ· Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒÑ‰ĞµÑ€Ğ±Ğ° Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ…. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ñ‹ Ğ½Ğ° Ğ¿ÑÑ‚Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… LLM Ğ¸ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ…, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ĞµÑ‚ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°.'}, 'en': {'title': 'One Example is Enough: Efficient Safety Alignment for LLMs', 'desc': 'This paper presents a novel approach to safety alignment in large language models (LLMs), showing that a single safety example can effectively restore safety without compromising model performance. Traditional methods often require numerous safety samples, leading to increased computational costs and reduced utility. The authors reveal that the low-rank structure of the safety gradient allows for rapid convergence in just a few training epochs. Their findings are validated across multiple LLMs and datasets, highlighting the efficiency and general applicability of their method.'}, 'zh': {'title': 'å•ä¸ªç¤ºä¾‹å³å¯æ¢å¤å®‰å…¨å¯¹é½', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„å®‰å…¨å¯¹é½é—®é¢˜ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä»…ç”¨ä¸€ä¸ªå®‰å…¨ç¤ºä¾‹å°±èƒ½å®Œå…¨æ¢å¤æ¨¡å‹çš„å®‰å…¨æ€§ï¼Œè€Œä¸ä¼šå½±å“æ¨¡å‹çš„å®ç”¨æ€§ã€‚é€šè¿‡è¯†åˆ«ä½ç§©æ¢¯åº¦ç»“æ„ï¼Œæ¨¡å‹åœ¨å°‘æ•°è®­ç»ƒå‘¨æœŸå†…å³å¯å®ç°æ”¶æ•›ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœåœ¨å¤šä¸ªå®‰å…¨å¯¹é½çš„è¯­è¨€æ¨¡å‹å’Œæ•°æ®é›†ä¸ŠéªŒè¯äº†è¿™ä¸€æ–¹æ³•çš„æ™®éé€‚ç”¨æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.06943', 'title': 'Watching, Reasoning, and Searching: A Video Deep Research Benchmark on Open Web for Agentic Video Reasoning', 'url': 'https://huggingface.co/papers/2601.06943', 'abstract': "VideoDR benchmark enables video question answering by combining cross-frame visual extraction, web retrieval, and multi-hop reasoning in open-domain settings.  \t\t\t\t\tAI-generated summary \t\t\t\t In real-world video question answering scenarios, videos often provide only localized visual cues, while verifiable answers are distributed across the open web; models therefore need to jointly perform cross-frame clue extraction, iterative retrieval, and multi-hop reasoning-based verification. To bridge this gap, we construct the first video deep research benchmark, VideoDR. VideoDR centers on video-conditioned open-domain video question answering, requiring cross-frame visual anchor extraction, interactive web retrieval, and multi-hop reasoning over joint video-web evidence; through rigorous human annotation and quality control, we obtain high-quality video deep research samples spanning six semantic domains. We evaluate multiple closed-source and open-source multimodal large language models under both the Workflow and Agentic paradigms, and the results show that Agentic is not consistently superior to Workflow: its gains depend on a model's ability to maintain the initial video anchors over long retrieval chains. Further analysis indicates that goal drift and long-horizon consistency are the core bottlenecks. In sum, VideoDR provides a systematic benchmark for studying video agents in open-web settings and reveals the key challenges for next-generation video deep research agents.", 'score': 170, 'issue_id': 545, 'pub_date': '2026-01-11', 'pub_date_card': {'ru': '11 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 11', 'zh': '1æœˆ11æ—¥'}, 'hash': 'fb635ed8afe67290', 'authors': ['Chengwen Liu', 'Xiaomin Yu', 'Zhuoyue Chang', 'Zhe Huang', 'Shuo Zhang', 'Heng Lian', 'Kunyi Wang', 'Rui Xu', 'Sen Hu', 'Jianheng Hou', 'Hao Peng', 'Chengwei Qin', 'Xiaobin Hu', 'Hong Peng', 'Ronghao Chen', 'Huacan Wang'], 'affiliations': ['FDU', 'HKUST', 'HKUST(GZ)', 'LZU', 'NUS', 'PKU', 'QuantaAlpha', 'UBC', 'UCAS', 'USC'], 'pdf_title_img': 'assets/pdf/title_img/2601.06943.jpg', 'data': {'categories': ['#rag', '#video', '#agents', '#benchmark', '#multimodal'], 'emoji': 'ğŸ¬', 'ru': {'title': 'ĞÑ‚ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğº Ğ·Ğ½Ğ°Ğ½Ğ¸ÑĞ¼: Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ²ĞµĞ±', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº VideoDR Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¼ Ğ²ĞµĞ±-Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾Ğ»Ğ¶Ğ½Ñ‹ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸: Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°Ñ‚ÑŒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ¸ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸ÑĞºĞ°Ñ‚ÑŒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚Ğµ Ğ¸ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğµ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ½Ğµ Ğ²ÑĞµĞ³Ğ´Ğ° Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ğµ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¸, Ñ‚Ğ°Ğº ĞºĞ°Ğº Ğ·Ğ°Ğ²Ğ¸ÑÑÑ‚ Ğ¾Ñ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ‚ÑŒ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞºĞ¾Ñ€Ñ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ñ‚ÑĞ¶ĞµĞ½Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ğ¿Ğ¾Ğ¸ÑĞºĞ°. ĞÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ â€” ÑÑ‚Ğ¾ Ğ´Ñ€ĞµĞ¹Ñ„ Ñ†ĞµĞ»Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ğµ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ğ°Ñ… Ğ¿Ğ¾Ğ¸ÑĞºĞ°.'}, 'en': {'title': 'VideoDR: Bridging Video Understanding and Web Knowledge for Q&A', 'desc': 'The VideoDR benchmark is designed to enhance video question answering by integrating visual extraction from multiple frames, web-based information retrieval, and complex reasoning processes. It addresses the challenge of localized visual cues in videos and the need for answers that may be found online. By creating a comprehensive dataset with high-quality annotations across various domains, VideoDR facilitates the evaluation of multimodal large language models. The findings highlight the importance of maintaining video context during retrieval and identify key challenges such as goal drift and consistency in long-term reasoning.'}, 'zh': {'title': 'VideoDRï¼šè§†é¢‘é—®ç­”çš„æ–°åŸºå‡†', 'desc': 'VideoDRåŸºå‡†æµ‹è¯•é€šè¿‡ç»“åˆè·¨å¸§è§†è§‰æå–ã€ç½‘ç»œæ£€ç´¢å’Œå¤šè·³æ¨ç†ï¼Œæ”¯æŒå¼€æ”¾åŸŸçš„è§†é¢‘é—®ç­”ã€‚è¯¥ç ”ç©¶æŒ‡å‡ºï¼Œè§†é¢‘ä¸­çš„è§†è§‰çº¿ç´¢é€šå¸¸æ˜¯å±€éƒ¨çš„ï¼Œè€Œå¯éªŒè¯çš„ç­”æ¡ˆåˆ™åˆ†æ•£åœ¨ç½‘ç»œä¸Šï¼Œå› æ­¤æ¨¡å‹éœ€è¦åŒæ—¶è¿›è¡Œçº¿ç´¢æå–ã€è¿­ä»£æ£€ç´¢å’ŒåŸºäºå¤šè·³æ¨ç†çš„éªŒè¯ã€‚VideoDRæ˜¯é¦–ä¸ªä¸“æ³¨äºè§†é¢‘æ¡ä»¶ä¸‹å¼€æ”¾åŸŸé—®ç­”çš„æ·±åº¦ç ”ç©¶åŸºå‡†ï¼Œæ¶µç›–å…­ä¸ªè¯­ä¹‰é¢†åŸŸï¼Œå¹¶é€šè¿‡ä¸¥æ ¼çš„äººç±»æ ‡æ³¨å’Œè´¨é‡æ§åˆ¶è·å¾—é«˜è´¨é‡æ ·æœ¬ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒAgenticå’ŒWorkflowä¸¤ç§èŒƒå¼çš„è¡¨ç°å·®å¼‚ä¾èµ–äºæ¨¡å‹åœ¨é•¿æ£€ç´¢é“¾ä¸­ä¿æŒåˆå§‹è§†é¢‘é”šç‚¹çš„èƒ½åŠ›ï¼Œç›®æ ‡æ¼‚ç§»å’Œé•¿æ—¶é—´ä¸€è‡´æ€§æ˜¯ä¸»è¦ç“¶é¢ˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.06521', 'title': 'BabyVision: Visual Reasoning Beyond Language', 'url': 'https://huggingface.co/papers/2601.06521', 'abstract': 'Current multimodal large language models exhibit significant gaps in fundamental visual understanding compared to human children, as demonstrated by the BabyVision benchmark.  \t\t\t\t\tAI-generated summary \t\t\t\t While humans develop core visual skills long before acquiring language, contemporary Multimodal LLMs (MLLMs) still rely heavily on linguistic priors to compensate for their fragile visual understanding. We uncovered a crucial fact: state-of-the-art MLLMs consistently fail on basic visual tasks that humans, even 3-year-olds, can solve effortlessly. To systematically investigate this gap, we introduce BabyVision, a benchmark designed to assess core visual abilities independent of linguistic knowledge for MLLMs. BabyVision spans a wide range of tasks, with 388 items divided into 22 subclasses across four key categories. Empirical results and human evaluation reveal that leading MLLMs perform significantly below human baselines. Gemini3-Pro-Preview scores 49.7, lagging behind 6-year-old humans and falling well behind the average adult score of 94.1. These results show despite excelling in knowledge-heavy evaluations, current MLLMs still lack fundamental visual primitives. Progress in BabyVision represents a step toward human-level visual perception and reasoning capabilities. We also explore solving visual reasoning with generation models by proposing BabyVision-Gen and automatic evaluation toolkit. Our code and benchmark data are released at https://github.com/UniPat-AI/BabyVision for reproduction.', 'score': 145, 'issue_id': 545, 'pub_date': '2026-01-10', 'pub_date_card': {'ru': '10 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 10', 'zh': '1æœˆ10æ—¥'}, 'hash': '8c12de19866dd47c', 'authors': ['Liang Chen', 'Weichu Xie', 'Yiyan Liang', 'Hongfeng He', 'Hans Zhao', 'Zhibo Yang', 'Zhiqi Huang', 'Haoning Wu', 'Haoyu Lu', 'Y. charles', 'Yiping Bao', 'Yuantao Fan', 'Guopeng Li', 'Haiyang Shen', 'Xuanzhong Chen', 'Wendong Xu', 'Shuzheng Si', 'Zefan Cai', 'Wenhao Chai', 'Ziqi Huang', 'Fangfu Liu', 'Tianyu Liu', 'Baobao Chang', 'Xiaobo Hu', 'Kaiyuan Chen', 'Yixin Ren', 'Yang Liu', 'Yuan Gong', 'Kuan Li'], 'affiliations': ['Alibaba Group', 'MoonShot AI', 'Nanyang Technological University', 'Peking University', 'Princeton University', 'StepFun', 'Tsinghua University', 'UniPat AI', 'University of Wisconsin-Madison', 'xbench'], 'pdf_title_img': 'assets/pdf/title_img/2601.06521.jpg', 'data': {'categories': ['#benchmark', '#multimodal', '#cv', '#dataset'], 'emoji': 'ğŸ‘¶', 'ru': {'title': 'Ğ”ĞµÑ‚Ğ¸ Ğ²Ğ¸Ğ´ÑÑ‚ Ğ»ÑƒÑ‡ÑˆĞµ: Ğ¿Ğ¾Ñ‡ĞµĞ¼Ñƒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ»ĞµĞ¿Ñ‹ Ğº Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ·Ñ€ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº BabyVision Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ Ğ¾Ñ‚ Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ ĞœĞ›Ğ›Ğœ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ñ‚ÑÑ‚Ğ°ÑÑ‚ Ğ¾Ñ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ´ĞµÑ‚ĞµĞ¹ Ğ² Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ·Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ, Ğ½ĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ·Ğ½Ğ°Ğ½Ğ¸ĞµĞ²Ñ‹Ñ… Ğ¾Ñ†ĞµĞ½ĞºĞ°Ñ…. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 388 Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ², Ñ€Ğ°Ğ·Ğ´ĞµĞ»Ñ‘Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° 22 Ğ¿Ğ¾Ğ´ĞºĞ»Ğ°ÑÑĞ° Ğ² Ñ‡ĞµÑ‚Ñ‹Ñ€Ñ‘Ñ… Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸ÑÑ…, Ğ¸ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑĞ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¸ĞºÑƒ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ²Ñ‹Ğ¿ÑƒÑĞºĞ°ÑÑ‚ ĞºĞ¾Ğ´ Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ² Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿.'}, 'en': {'title': 'Bridging the Visual Understanding Gap in AI', 'desc': 'This paper highlights the limitations of current multimodal large language models (MLLMs) in visual understanding compared to human children. Using the BabyVision benchmark, the authors demonstrate that MLLMs struggle with basic visual tasks that even young children can perform easily. The study reveals that despite their proficiency in language-based tasks, MLLMs like Gemini3-Pro-Preview score significantly lower than human baselines in visual tasks. The introduction of BabyVision aims to bridge this gap by providing a systematic way to evaluate visual abilities independent of linguistic knowledge, paving the way for improved visual perception in AI.'}, 'zh': {'title': 'å¡«è¡¥è§†è§‰ç†è§£çš„å·®è·', 'desc': 'å½“å‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åŸºæœ¬è§†è§‰ç†è§£æ–¹é¢ä¸äººç±»å„¿ç«¥å­˜åœ¨æ˜¾è‘—å·®è·ï¼Œè¿™ä¸€ç‚¹é€šè¿‡BabyVisionåŸºå‡†å¾—åˆ°äº†éªŒè¯ã€‚å°½ç®¡äººç±»åœ¨è·å¾—è¯­è¨€èƒ½åŠ›ä¹‹å‰å°±å‘å±•äº†æ ¸å¿ƒè§†è§‰æŠ€èƒ½ï¼Œä½†ç°ä»£å¤šæ¨¡æ€LLMä»ç„¶ä¾èµ–è¯­è¨€å…ˆéªŒæ¥å¼¥è¡¥å…¶è„†å¼±çš„è§†è§‰ç†è§£ã€‚æˆ‘ä»¬å‘ç°ï¼Œæœ€å…ˆè¿›çš„å¤šæ¨¡æ€LLMåœ¨åŸºæœ¬è§†è§‰ä»»åŠ¡ä¸Šçš„è¡¨ç°è¿œä½äºäººç±»ï¼Œå³ä½¿æ˜¯3å²çš„å„¿ç«¥ä¹Ÿèƒ½è½»æ¾è§£å†³è¿™äº›ä»»åŠ¡ã€‚BabyVisionåŸºå‡†çš„è¿›å±•æ ‡å¿—ç€æœç€äººç±»æ°´å¹³çš„è§†è§‰æ„ŸçŸ¥å’Œæ¨ç†èƒ½åŠ›è¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.05593', 'title': 'PaCoRe: Learning to Scale Test-Time Compute with Parallel Coordinated Reasoning', 'url': 'https://huggingface.co/papers/2601.05593', 'abstract': "Parallel Coordinated Reasoning enables large-scale test-time compute scaling beyond sequential reasoning limitations through parallel exploration and message-passing architecture.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Parallel Coordinated Reasoning (PaCoRe), a training-and-inference framework designed to overcome a central limitation of contemporary language models: their inability to scale test-time compute (TTC) far beyond sequential reasoning under a fixed context window. PaCoRe departs from the traditional sequential paradigm by driving TTC through massive parallel exploration coordinated via a message-passing architecture in multiple rounds. Each round launches many parallel reasoning trajectories, compacts their findings into context-bounded messages, and synthesizes these messages to guide the next round and ultimately produce the final answer. Trained end-to-end with large-scale, outcome-based reinforcement learning, the model masters the synthesis abilities required by PaCoRe and scales to multi-million-token effective TTC without exceeding context limits. The approach yields strong improvements across diverse domains, and notably pushes reasoning beyond frontier systems in mathematics: an 8B model reaches 94.5% on HMMT 2025, surpassing GPT-5's 93.2% by scaling effective TTC to roughly two million tokens. We open-source model checkpoints, training data, and the full inference pipeline to accelerate follow-up work.", 'score': 61, 'issue_id': 544, 'pub_date': '2026-01-09', 'pub_date_card': {'ru': '9 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 9', 'zh': '1æœˆ9æ—¥'}, 'hash': '2beebb5ab57389a3', 'authors': ['Jingcheng Hu', 'Yinmin Zhang', 'Shijie Shang', 'Xiaobo Yang', 'Yue Peng', 'Zhewei Huang', 'Hebin Zhou', 'Xin Wu', 'Jie Cheng', 'Fanqi Wan', 'Xiangwen Kong', 'Chengyuan Yao', 'Kaiwen Yan', 'Ailin Huang', 'Hongyu Zhou', 'Qi Han', 'Zheng Ge', 'Daxin Jiang', 'Xiangyu Zhang', 'Heung-Yeung Shum'], 'affiliations': ['Peking University', 'StepFun', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2601.05593.jpg', 'data': {'categories': ['#training', '#long_context', '#open_source', '#benchmark', '#math', '#reasoning', '#rl', '#inference'], 'emoji': 'ğŸ”€', 'ru': {'title': 'ĞŸĞ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğµ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞµ', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Parallel Coordinated Reasoning (PaCoRe) â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°ĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ°. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ñ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡ĞµĞ¹ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¹, Ğ·Ğ°Ğ¿ÑƒÑĞºĞ°Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€ÑƒÑ Ğ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğµ Ğ¾ĞºĞ½Ğ¾. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ end-to-end Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸Ñ‚Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ². ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ², Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ 94.5% Ğ½Ğ° HMMT 2025 Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ 8B, Ğ¿Ñ€ĞµĞ²Ñ‹ÑˆĞ°Ñ GPT-5, Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ğ´Ğ¾ Ğ´Ğ²ÑƒÑ… Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ².'}, 'en': {'title': 'Unlocking Parallel Reasoning for Enhanced AI Performance', 'desc': 'Parallel Coordinated Reasoning (PaCoRe) is a novel framework that enhances the capabilities of language models by allowing them to perform reasoning tasks in parallel rather than sequentially. This method utilizes a message-passing architecture to coordinate multiple reasoning trajectories, enabling the model to explore vast amounts of information simultaneously. By training with large-scale reinforcement learning, PaCoRe effectively manages to synthesize findings from these parallel explorations, significantly increasing the test-time compute (TTC) without being limited by context window constraints. The results demonstrate substantial improvements in reasoning tasks, particularly in mathematics, showcasing the potential of PaCoRe to exceed the performance of existing models like GPT-5.'}, 'zh': {'title': 'å¹¶è¡Œåè°ƒæ¨ç†ï¼šè¶…è¶Šé¡ºåºæ¨ç†çš„è®¡ç®—èƒ½åŠ›', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºå¹¶è¡Œåè°ƒæ¨ç†ï¼ˆPaCoReï¼‰çš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å½“å‰è¯­è¨€æ¨¡å‹åœ¨æµ‹è¯•æ—¶è®¡ç®—èƒ½åŠ›æ‰©å±•æ–¹é¢çš„é™åˆ¶ã€‚PaCoReé€šè¿‡æ¶ˆæ¯ä¼ é€’æ¶æ„å®ç°å¤§è§„æ¨¡çš„å¹¶è¡Œæ¢ç´¢ï¼Œæ‰“ç ´äº†ä¼ ç»Ÿçš„é¡ºåºæ¨ç†æ¨¡å¼ã€‚æ¯ä¸€è½®æ¨ç†ä¼šå¯åŠ¨å¤šä¸ªå¹¶è¡Œè½¨è¿¹ï¼Œå°†ç»“æœå‹ç¼©æˆæ¶ˆæ¯ï¼Œå¹¶åˆæˆè¿™äº›æ¶ˆæ¯ä»¥æŒ‡å¯¼ä¸‹ä¸€è½®æ¨ç†ï¼Œæœ€ç»ˆå¾—å‡ºç­”æ¡ˆã€‚è¯¥æ–¹æ³•ç»è¿‡å¤§è§„æ¨¡çš„åŸºäºç»“æœçš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œèƒ½å¤Ÿåœ¨ä¸è¶…è¿‡ä¸Šä¸‹æ–‡é™åˆ¶çš„æƒ…å†µä¸‹ï¼Œæ‰©å±•åˆ°æ•°ç™¾ä¸‡ä¸ªæœ‰æ•ˆçš„æµ‹è¯•æ—¶é—´è®¡ç®—ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.06953', 'title': 'X-Coder: Advancing Competitive Programming with Fully Synthetic Tasks, Solutions, and Tests', 'url': 'https://huggingface.co/papers/2601.06953', 'abstract': 'Code LLMs trained on fully synthetic data using a feature-based synthesis pipeline achieve superior performance on competitive programming benchmarks while reducing dependence on real-world coding datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t Competitive programming presents great challenges for Code LLMs due to its intensive reasoning demands and high logical complexity. However, current Code LLMs still rely heavily on real-world data, which limits their scalability. In this paper, we explore a fully synthetic approach: training Code LLMs with entirely generated tasks, solutions, and test cases, to empower code reasoning models without relying on real-world data. To support this, we leverage feature-based synthesis to propose a novel data synthesis pipeline called SynthSmith. SynthSmith shows strong potential in producing diverse and challenging tasks, along with verified solutions and tests, supporting both supervised fine-tuning and reinforcement learning. Based on the proposed synthetic SFT and RL datasets, we introduce the X-Coder model series, which achieves a notable pass rate of 62.9 avg@8 on LiveCodeBench v5 and 55.8 on v6, outperforming DeepCoder-14B-Preview and AReal-boba2-14B despite having only 7B parameters. In-depth analysis reveals that scaling laws hold on our synthetic dataset, and we explore which dimensions are more effective to scale. We further provide insights into code-centric reinforcement learning and highlight the key factors that shape performance through detailed ablations and analysis. Our findings demonstrate that scaling high-quality synthetic data and adopting staged training can greatly advance code reasoning, while mitigating reliance on real-world coding data.', 'score': 30, 'issue_id': 545, 'pub_date': '2026-01-11', 'pub_date_card': {'ru': '11 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 11', 'zh': '1æœˆ11æ—¥'}, 'hash': '1e20056f9d305da0', 'authors': ['Jie Wu', 'Haoling Li', 'Xin Zhang', 'Jiani Guo', 'Jane Luo', 'Steven Liu', 'Yangyu Huang', 'Ruihang Chu', 'Scarlett Li', 'Yujiu Yang'], 'affiliations': ['Microsoft', 'Tsinghua University', 'Wuhan University'], 'pdf_title_img': 'assets/pdf/title_img/2601.06953.jpg', 'data': {'categories': ['#rl', '#reasoning', '#optimization', '#dataset', '#training', '#synthetic', '#small_models', '#benchmark', '#plp', '#data'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ¡Ğ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ…: ĞºĞ°Ğº Ğ½Ğ°ÑƒÑ‡Ğ¸Ñ‚ÑŒ LLM Ğ¿Ğ¸ÑĞ°Ñ‚ÑŒ ĞºĞ¾Ğ´ Ğ±ĞµĞ· Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ²', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Code LLM, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ SynthSmith Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚ĞµÑÑ‚Ğ¾Ğ² Ğ±ĞµĞ· Ğ¿Ñ€Ğ¸Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ X-Coder, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ supervised fine-tuning Ğ¸ reinforcement learning, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ğ¿ĞµÑ‡Ğ°Ñ‚Ğ»ÑÑÑ‰Ğ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… ĞºĞ¾Ğ½ĞºÑƒÑ€ÑĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ (62.9% Ğ½Ğ° LiveCodeBench v5), Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ñ… Ğ²ÑĞµĞ³Ğ¾ 7B. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ·Ğ°ĞºĞ¾Ğ½Ñ‹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑÑ‚ ÑĞ²Ğ¾Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² ĞºĞ¾Ğ´Ğµ, Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°Ñ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'Empowering Code LLMs with Fully Synthetic Data', 'desc': 'This paper presents a novel approach to training Code LLMs (Language Models) using fully synthetic data, which helps improve their performance in competitive programming tasks. By employing a feature-based synthesis pipeline called SynthSmith, the authors generate diverse coding tasks, solutions, and test cases without depending on real-world datasets. The resulting X-Coder model series demonstrates impressive performance metrics, surpassing larger models while using fewer parameters. The study emphasizes the importance of high-quality synthetic data and staged training in enhancing code reasoning capabilities.'}, 'zh': {'title': 'åˆæˆæ•°æ®é©±åŠ¨çš„ä»£ç æ¨ç†æ¨¡å‹æå‡', 'desc': 'æœ¬æ–‡æ¢è®¨äº†ä¸€ç§å®Œå…¨åˆæˆçš„æ•°æ®è®­ç»ƒæ–¹æ³•ï¼Œç”¨äºæå‡ä»£ç å¤§è¯­è¨€æ¨¡å‹ï¼ˆCode LLMsï¼‰åœ¨ç«äº‰ç¼–ç¨‹ä¸­çš„è¡¨ç°ã€‚é€šè¿‡ä½¿ç”¨ç‰¹å¾åŸºç¡€åˆæˆæŠ€æœ¯ï¼Œæå‡ºäº†åä¸ºSynthSmithçš„æ•°æ®åˆæˆç®¡é“ï¼Œèƒ½å¤Ÿç”Ÿæˆå¤šæ ·åŒ–ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡åŠå…¶è§£å†³æ–¹æ¡ˆã€‚ç ”ç©¶è¡¨æ˜ï¼ŒåŸºäºåˆæˆæ•°æ®çš„æ¨¡å‹åœ¨LiveCodeBenchåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†ç°æœ‰çš„æ·±åº¦ç¼–ç æ¨¡å‹ã€‚æˆ‘ä»¬çš„å‘ç°è¡¨æ˜ï¼Œåˆ©ç”¨é«˜è´¨é‡çš„åˆæˆæ•°æ®å’Œåˆ†é˜¶æ®µè®­ç»ƒå¯ä»¥æ˜¾è‘—æé«˜ä»£ç æ¨ç†èƒ½åŠ›ï¼ŒåŒæ—¶å‡å°‘å¯¹çœŸå®ä¸–ç•Œç¼–ç æ•°æ®çš„ä¾èµ–ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.07832', 'title': 'MHLA: Restoring Expressivity of Linear Attention via Token-Level Multi-Head', 'url': 'https://huggingface.co/papers/2601.07832', 'abstract': "Multi-Head Linear Attention addresses the performance degradation in linear attention by preserving representational diversity through head-wise token dimension computation, maintaining linear complexity while recovering softmax attention's expressive power across multiple domains.  \t\t\t\t\tAI-generated summary \t\t\t\t While the Transformer architecture dominates many fields, its quadratic self-attention complexity hinders its use in large-scale applications. Linear attention offers an efficient alternative, but its direct application often degrades performance, with existing fixes typically re-introducing computational overhead through extra modules (e.g., depthwise separable convolution) that defeat the original purpose. In this work, we identify a key failure mode in these methods: global context collapse, where the model loses representational diversity. To address this, we propose Multi-Head Linear Attention (MHLA), which preserves this diversity by computing attention within divided heads along the token dimension. We prove that MHLA maintains linear complexity while recovering much of the expressive power of softmax attention, and verify its effectiveness across multiple domains, achieving a 3.6\\% improvement on ImageNet classification, a 6.3\\% gain on NLP, a 12.6\\% improvement on image generation, and a 41\\% enhancement on video generation under the same time complexity.", 'score': 29, 'issue_id': 549, 'pub_date': '2026-01-12', 'pub_date_card': {'ru': '12 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 12', 'zh': '1æœˆ12æ—¥'}, 'hash': '86d90769af77f62e', 'authors': ['Kewei Zhang', 'Ye Huang', 'Yufan Deng', 'Jincheng Yu', 'Junsong Chen', 'Huan Ling', 'Enze Xie', 'Daquan Zhou'], 'affiliations': ['NVIDIA', 'Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2601.07832.jpg', 'data': {'categories': ['#architecture', '#training', '#optimization'], 'emoji': 'âš¡', 'ru': {'title': 'Ğ›Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ğ²Ñ‹Ñ€Ğ°Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸: ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ³Ğ¾Ğ»Ğ¾Ğ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Multi-Head Linear Attention (MHLA) â€” Ğ¼ĞµÑ‚Ğ¾Ğ´, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°Ñ…. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ°Ñ… Ğ¿Ğ¾ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ»Ğ»Ğ°Ğ¿Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ MHLA Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½ÑƒÑ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ O(n) Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ñ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ²Ñ‹Ñ€Ğ°Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° softmax attention. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…: ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ (+3.6%), Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ñ‚ĞµĞºÑÑ‚Ğ° (+6.3%), Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ (+12.6%) Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ (+41%).'}, 'en': {'title': 'Enhancing Linear Attention with Multi-Head Diversity', 'desc': 'Multi-Head Linear Attention (MHLA) improves linear attention mechanisms by maintaining representational diversity through head-wise computations. This approach addresses the issue of global context collapse, which can lead to performance degradation in traditional linear attention models. By dividing attention calculations across multiple heads, MHLA retains the efficiency of linear complexity while enhancing the expressive capabilities similar to softmax attention. The effectiveness of MHLA is demonstrated through significant performance gains across various tasks, including image classification, natural language processing, image generation, and video generation.'}, 'zh': {'title': 'å¤šå¤´çº¿æ€§æ³¨æ„åŠ›ï¼šä¿æŒå¤šæ ·æ€§ä¸é«˜æ•ˆæ€§å¹¶å­˜', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºå¤šå¤´çº¿æ€§æ³¨æ„åŠ›ï¼ˆMHLAï¼‰çš„æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³çº¿æ€§æ³¨æ„åŠ›åœ¨æ€§èƒ½ä¸Šçš„ä¸‹é™é—®é¢˜ã€‚é€šè¿‡åœ¨å¤´éƒ¨ç»´åº¦ä¸Šè¿›è¡Œåˆ†å‰²è®¡ç®—ï¼ŒMHLAèƒ½å¤Ÿä¿æŒè¡¨ç¤ºçš„å¤šæ ·æ€§ï¼ŒåŒæ—¶ä¿æŒçº¿æ€§å¤æ‚åº¦ã€‚æˆ‘ä»¬è¯æ˜äº†MHLAåœ¨æ¢å¤softmaxæ³¨æ„åŠ›çš„è¡¨è¾¾èƒ½åŠ›æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œå¹¶åœ¨å¤šä¸ªé¢†åŸŸä¸­éªŒè¯äº†å…¶æ•ˆæœã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨ImageNetåˆ†ç±»ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€å›¾åƒç”Ÿæˆå’Œè§†é¢‘ç”Ÿæˆç­‰ä»»åŠ¡ä¸­ï¼ŒMHLAå‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.05110', 'title': 'GlimpRouter: Efficient Collaborative Inference by Glimpsing One Token of Thoughts', 'url': 'https://huggingface.co/papers/2601.05110', 'abstract': 'Large reasoning models\' inference latency can be reduced by routing reasoning steps to larger models based on the entropy of their first token, enabling efficient collaborative inference without additional training.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Reasoning Models (LRMs) achieve remarkable performance by explicitly generating multi-step chains of thought, but this capability incurs substantial inference latency and computational cost. Collaborative inference offers a promising solution by selectively allocating work between lightweight and large models, yet a fundamental challenge remains: determining when a reasoning step requires the capacity of a large model or the efficiency of a small model. Existing routing strategies either rely on local token probabilities or post-hoc verification, introducing significant inference overhead. In this work, we propose a novel perspective on step-wise collaboration: the difficulty of a reasoning step can be inferred from its very first token. Inspired by the "Aha Moment" phenomenon in LRMs, we show that the entropy of the initial token serves as a strong predictor of step difficulty. Building on this insight, we introduce GlimpRouter, a training-free step-wise collaboration framework. GlimpRouter employs a lightweight model to generate only the first token of each reasoning step and routes the step to a larger model only when the initial token entropy exceeds a threshold. Experiments on multiple benchmarks demonstrate that our approach significantly reduces inference latency while preserving accuracy. For instance, GlimpRouter attains a substantial 10.7% improvement in accuracy while reducing inference latency by 25.9% compared to a standalone large model on AIME25. These results suggest a simple yet effective mechanism for reasoning: allocating computation based on a glimpse of thought rather than full-step evaluation.', 'score': 24, 'issue_id': 547, 'pub_date': '2026-01-08', 'pub_date_card': {'ru': '8 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 8', 'zh': '1æœˆ8æ—¥'}, 'hash': 'd61eeb39c38e7630', 'authors': ['Wenhao Zeng', 'Xuteng Zhang', 'Yuling Shi', 'Chao Hu', 'Yuting Chen', 'Beijun Shen', 'Xiaodong Gu'], 'affiliations': ['Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2601.05110.jpg', 'data': {'categories': ['#architecture', '#training', '#optimization', '#reasoning', '#inference'], 'emoji': 'âš¡', 'ru': {'title': 'ĞœĞ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑˆĞ°Ğ³Ğ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¼Ñƒ Ñ‚Ğ¾ĞºĞµĞ½Ñƒ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ»Ğ»Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ°', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ GlimpRouter Ğ´Ğ»Ñ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ¸ Ğ¿Ñ€Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑˆĞ°Ğ³Ğ¾Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ»Ñ‘Ğ³ĞºĞ¾Ğ¹ Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ñ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ° ĞºĞ°Ğº Ğ¿Ñ€ĞµĞ´Ğ¸ĞºÑ‚Ğ¾Ñ€ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ÑˆĞ°Ğ³Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¾ÑĞ½Ğ¾Ğ²Ñ‹Ğ²Ğ°ÑÑÑŒ Ğ½Ğ° Ñ„ĞµĞ½Ğ¾Ğ¼ĞµĞ½Ğµ Â«Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚Ğ° Ğ¾Ğ·Ğ°Ñ€ĞµĞ½Ğ¸ÑÂ» Ğ² LRM. ĞœĞ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ: Ğ»Ñ‘Ğ³ĞºĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½, Ğ° ĞµÑĞ»Ğ¸ ĞµĞ³Ğ¾ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ñ Ğ¿Ñ€ĞµĞ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ¿Ğ¾Ñ€Ğ¾Ğ³, ÑˆĞ°Ğ³ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‘Ñ‚ÑÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºÑƒ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ğ½Ğ° 25.9% Ğ¿Ñ€Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¼ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 10.7% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Efficient Inference with GlimpRouter: Smart Routing for Reasoning Steps', 'desc': "This paper introduces GlimpRouter, a framework designed to reduce the inference latency of large reasoning models (LRMs) by intelligently routing reasoning steps based on the entropy of their first token. The authors argue that the initial token's entropy can predict the difficulty of a reasoning step, allowing for efficient collaboration between lightweight and large models without the need for additional training. By generating only the first token with a lightweight model and routing to a larger model when necessary, GlimpRouter achieves significant reductions in latency while maintaining accuracy. Experimental results show a 10.7% improvement in accuracy and a 25.9% reduction in inference time compared to using a standalone large model."}, 'zh': {'title': 'åŸºäºåˆå§‹æ ‡è®°ç†µçš„é«˜æ•ˆæ¨ç†åä½œ', 'desc': 'å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰åœ¨ç”Ÿæˆå¤šæ­¥éª¤æ€ç»´é“¾æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†è¿™ä¼šå¯¼è‡´æ¨ç†å»¶è¿Ÿå’Œè®¡ç®—æˆæœ¬é«˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œé€šè¿‡åˆ†ææ¨ç†æ­¥éª¤çš„ç¬¬ä¸€ä¸ªæ ‡è®°çš„ç†µæ¥åˆ¤æ–­è¯¥æ­¥éª¤çš„éš¾åº¦ï¼Œä»è€Œå®ç°è½»é‡æ¨¡å‹ä¸å¤§å‹æ¨¡å‹çš„åä½œæ¨ç†ã€‚æˆ‘ä»¬å¼•å…¥äº†GlimpRouteræ¡†æ¶ï¼Œè¯¥æ¡†æ¶åœ¨ä¸éœ€è¦é¢å¤–è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œä»…åœ¨åˆå§‹æ ‡è®°ç†µè¶…è¿‡é˜ˆå€¼æ—¶å°†æ­¥éª¤è·¯ç”±åˆ°å¤§å‹æ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒå‡†ç¡®æ€§çš„åŒæ—¶æ˜¾è‘—å‡å°‘äº†æ¨ç†å»¶è¿Ÿã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.07779', 'title': 'OS-Symphony: A Holistic Framework for Robust and Generalist Computer-Using Agent', 'url': 'https://huggingface.co/papers/2601.07779', 'abstract': 'OS-Symphony presents a comprehensive framework for computer-using agents that enhances robustness in long-horizon tasks through reflection-memory and multimodal search capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t While Vision-Language Models (VLMs) have significantly advanced Computer-Using Agents (CUAs), current frameworks struggle with robustness in long-horizon workflows and generalization in novel domains. These limitations stem from a lack of granular control over historical visual context curation and the absence of visual-aware tutorial retrieval. To bridge these gaps, we introduce OS-Symphony, a holistic framework that comprises an Orchestrator coordinating two key innovations for robust automation: (1) a Reflection-Memory Agent that utilizes milestone-driven long-term memory to enable trajectory-level self-correction, effectively mitigating visual context loss in long-horizon tasks; (2) Versatile Tool Agents featuring a Multimodal Searcher that adopts a SeeAct paradigm to navigate a browser-based sandbox to synthesize live, visually aligned tutorials, thereby resolving fidelity issues in unseen scenarios. Experimental results demonstrate that OS-Symphony delivers substantial performance gains across varying model scales, establishing new state-of-the-art results on three online benchmarks, notably achieving 65.84% on OSWorld.', 'score': 22, 'issue_id': 549, 'pub_date': '2026-01-12', 'pub_date_card': {'ru': '12 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 12', 'zh': '1æœˆ12æ—¥'}, 'hash': '338335be0ca27562', 'authors': ['Bowen Yang', 'Kaiming Jin', 'Zhenyu Wu', 'Zhaoyang Liu', 'Qiushi Sun', 'Zehao Li', 'JingJing Xie', 'Zhoumianze Liu', 'Fangzhi Xu', 'Kanzhi Cheng', 'Qingyun Li', 'Yian Wang', 'Yu Qiao', 'Zun Wang', 'Zichen Ding'], 'affiliations': ['CUHK MMLab', 'Harbin Institute of Technology', 'Nanjing University', 'National University of Singapore', 'Shanghai AI Laboratory', 'The Hong Kong University of Science and Technology', 'The University of Hong Kong', 'University of Science and Technology of China', 'Xian Jiaotong University'], 'pdf_title_img': 'assets/pdf/title_img/2601.07779.jpg', 'data': {'categories': ['#cv', '#benchmark', '#agents', '#multimodal', '#long_context', '#reasoning'], 'emoji': 'ğŸ¼', 'ru': {'title': 'ĞÑ€ĞºĞµÑÑ‚Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ´Ğ»Ñ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ¾Ğ¼', 'desc': 'OS-Symphony â€” ÑÑ‚Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‰Ğ¸Ñ… Ñ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ¾Ğ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ²Ğ¸Ğ´ĞµĞ½Ğ¸Ğµ Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ñ…Ñ€ÑƒĞ¿ĞºĞ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ´Ğ¾Ğ»Ğ³Ğ¸Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ´Ğ²ÑƒĞ¼ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°Ğ¼: Ğ°Ğ³ĞµĞ½Ñ‚Ñƒ Ñ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸ĞµĞ¹ Ğ¸ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ´Ğ»Ñ ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹, Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼ Ğ¿Ğ¾Ğ¸ÑĞºĞ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ‚ÑƒÑ‚Ğ¾Ñ€Ğ¸Ğ°Ğ»Ñ‹ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ´Ğ¾Ğ¼ĞµĞ½Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ 65.84% Ğ½Ğ° OSWorld.'}, 'en': {'title': 'Enhancing Agent Robustness with OS-Symphony', 'desc': 'OS-Symphony is a new framework designed to improve the performance of computer-using agents in complex, long-term tasks. It introduces a Reflection-Memory Agent that helps these agents remember important information over time, allowing them to correct mistakes and maintain context. Additionally, it features Versatile Tool Agents that use a multimodal search approach to find and utilize relevant tutorials in real-time, enhancing their ability to adapt to new situations. The framework has shown significant improvements in performance on various benchmarks, setting new records in the field.'}, 'zh': {'title': 'OS-Symphonyï¼šæå‡é•¿æ—¶é—´ä»»åŠ¡é²æ£’æ€§çš„å…¨æ–°æ¡†æ¶', 'desc': 'OS-Symphonyæ˜¯ä¸€ä¸ªå…¨é¢çš„æ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºè®¡ç®—æœºä½¿ç”¨ä»£ç†åœ¨é•¿æ—¶é—´ä»»åŠ¡ä¸­çš„é²æ£’æ€§ã€‚å®ƒé€šè¿‡åæ€è®°å¿†å’Œå¤šæ¨¡æ€æœç´¢èƒ½åŠ›æ¥è§£å†³å½“å‰æ¡†æ¶åœ¨æ–°é¢†åŸŸä¸­çš„æ³›åŒ–é—®é¢˜ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬ä¸€ä¸ªåè°ƒå™¨ï¼Œç»“åˆäº†åæ€è®°å¿†ä»£ç†å’Œå¤šåŠŸèƒ½å·¥å…·ä»£ç†ï¼Œä»¥å®ç°æ›´å¼ºçš„è‡ªåŠ¨åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOS-Symphonyåœ¨å¤šä¸ªåœ¨çº¿åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå°¤å…¶æ˜¯åœ¨OSWorldä¸Šè¾¾åˆ°äº†65.84%çš„æ–°é«˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.07226', 'title': 'Lost in the Noise: How Reasoning Models Fail with Contextual Distractors', 'url': 'https://huggingface.co/papers/2601.07226', 'abstract': 'NoisyBench benchmark reveals significant performance degradation in state-of-the-art models when exposed to noisy contextual information, with agentic workflows amplifying errors and attention mechanisms disproportionately focusing on distractor tokens.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in reasoning models and agentic AI systems have led to an increased reliance on diverse external information. However, this shift introduces input contexts that are inherently noisy, a reality that current sanitized benchmarks fail to capture. We introduce NoisyBench, a comprehensive benchmark that systematically evaluates model robustness across 11 datasets in RAG, reasoning, alignment, and tool-use tasks against diverse noise types, including random documents, irrelevant chat histories, and hard negative distractors. Our evaluation reveals a catastrophic performance drop of up to 80% in state-of-the-art models when faced with contextual distractors. Crucially, we find that agentic workflows often amplify these errors by over-trusting noisy tool outputs, and distractors can trigger emergent misalignment even without adversarial intent. We find that prompting, context engineering, SFT, and outcome-reward only RL fail to ensure robustness; in contrast, our proposed Rationale-Aware Reward (RARE) significantly strengthens resilience by incentivizing the identification of helpful information within noise. Finally, we uncover an inverse scaling trend where increased test-time computation leads to worse performance in noisy settings and demonstrate via attention visualization that models disproportionately focus on distractor tokens, providing vital insights for building the next generation of robust, reasoning-capable agents.', 'score': 22, 'issue_id': 545, 'pub_date': '2026-01-12', 'pub_date_card': {'ru': '12 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 12', 'zh': '1æœˆ12æ—¥'}, 'hash': 'da6da812d41b17f9', 'authors': ['Seongyun Lee', 'Yongrae Jo', 'Minju Seo', 'Moontae Lee', 'Minjoon Seo'], 'affiliations': ['KAIST AI', 'LG AI Research', 'University of Illinois Chicago'], 'pdf_title_img': 'assets/pdf/title_img/2601.07226.jpg', 'data': {'categories': ['#reasoning', '#rlhf', '#security', '#rag', '#alignment', '#agents', '#benchmark'], 'emoji': 'ğŸ”§', 'ru': {'title': 'ĞšĞ°Ğº Ğ½Ğ°ÑƒÑ‡Ğ¸Ñ‚ÑŒ AI-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸Ğ³Ğ½Ğ¾Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑˆÑƒĞ¼ Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº NoisyBench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº ÑˆÑƒĞ¼Ğ½Ñ‹Ğ¼ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸, Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸ ĞºĞ°Ñ‚Ğ°ÑÑ‚Ñ€Ğ¾Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ğ°Ğ´ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° 80% Ğ¿Ñ€Ğ¸ Ğ½Ğ°Ğ»Ğ¸Ñ‡Ğ¸Ğ¸ Ğ¾Ñ‚Ğ²Ğ»ĞµĞºĞ°ÑÑ‰ĞµĞ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹ ÑƒÑĞ¸Ğ»Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ¸Ğ·-Ğ·Ğ° Ñ‡Ñ€ĞµĞ·Ğ¼ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¾Ğ²ĞµÑ€Ğ¸Ñ Ğº Ğ·Ğ°ÑˆÑƒĞ¼Ğ»ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ°Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ¡Ñ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ (fine-tuning, prompt engineering, RL) Ğ½Ğµ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‚ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ¾Ğ´Ğ½Ğ°ĞºĞ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Rationale-Aware Reward Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¿Ğ¾Ğ¾Ñ‰Ñ€ĞµĞ½Ğ¸Ñ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ ÑÑ€ĞµĞ´Ğ¸ ÑˆÑƒĞ¼Ğ°. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ», Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½ĞµĞ¿Ñ€Ğ¾Ğ¿Ğ¾Ñ€Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ ÑĞ¾ÑÑ€ĞµĞ´Ğ¾Ñ‚Ğ°Ñ‡Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ğ½Ğ° Ğ¾Ñ‚Ğ²Ğ»ĞµĞºĞ°ÑÑ‰Ğ¸Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ñ…, Ñ‡Ñ‚Ğ¾ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ° Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°ÑÑ‰Ğ¸Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'NoisyBench: Evaluating Robustness in a Noisy World', 'desc': 'The paper introduces NoisyBench, a benchmark designed to evaluate the robustness of machine learning models against noisy contextual information. It highlights that state-of-the-art models experience a drastic performance drop of up to 80% when exposed to irrelevant or distracting inputs. The research shows that agentic workflows can exacerbate these issues by overly relying on noisy outputs, leading to misalignment in model behavior. To address these challenges, the authors propose a new approach called Rationale-Aware Reward (RARE), which encourages models to focus on useful information amidst noise, improving their resilience in real-world applications.'}, 'zh': {'title': 'åº”å¯¹å™ªå£°æŒ‘æˆ˜ï¼Œæå‡æ¨¡å‹é²æ£’æ€§ï¼', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†NoisyBenchåŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°æ¨¡å‹åœ¨é¢å¯¹å™ªå£°ä¸Šä¸‹æ–‡ä¿¡æ¯æ—¶çš„é²æ£’æ€§ã€‚ç ”ç©¶å‘ç°ï¼Œå½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹åœ¨å¤„ç†å™ªå£°å¹²æ‰°æ—¶æ€§èƒ½ä¸‹é™å¯è¾¾80%ã€‚æ­¤å¤–ï¼Œä»£ç†å·¥ä½œæµç¨‹ä¼šæ”¾å¤§è¿™äº›é”™è¯¯ï¼Œå› ä¸ºæ¨¡å‹è¿‡äºä¾èµ–å™ªå£°å·¥å…·çš„è¾“å‡ºã€‚æˆ‘ä»¬æå‡ºçš„Rationale-Aware Reward (RARE) æ–¹æ³•èƒ½å¤Ÿæ˜¾è‘—æé«˜æ¨¡å‹åœ¨å™ªå£°ç¯å¢ƒä¸­çš„æŠ—å¹²æ‰°èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.07351', 'title': 'Beyond Hard Masks: Progressive Token Evolution for Diffusion Language Models', 'url': 'https://huggingface.co/papers/2601.07351', 'abstract': 'EvoToken-DLM introduces a diffusion-based language modeling approach that uses soft token distributions and continuous trajectory supervision to enable revisable decoding and outperforms existing baselines.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion Language Models (DLMs) offer a promising alternative for language modeling by enabling parallel decoding through iterative refinement. However, most DLMs rely on hard binary masking and discrete token assignments, which hinder the revision of early decisions and underutilize intermediate probabilistic representations. In this paper, we propose EvoToken-DLM, a novel diffusion-based language modeling approach that replaces hard binary masks with evolving soft token distributions. EvoToken-DLM enables a progressive transition from masked states to discrete outputs, supporting revisable decoding. To effectively support this evolution, we introduce continuous trajectory supervision, which aligns training objectives with iterative probabilistic updates. Extensive experiments across multiple benchmarks show that EvoToken-DLM consistently achieves superior performance, outperforming strong diffusion-based and masked DLM baselines. Project webpage: https://aim-uofa.github.io/EvoTokenDLM.', 'score': 19, 'issue_id': 550, 'pub_date': '2026-01-12', 'pub_date_card': {'ru': '12 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 12', 'zh': '1æœˆ12æ—¥'}, 'hash': '334992e5d9083611', 'authors': ['Linhao Zhong', 'Linyu Wu', 'Bozhen Fang', 'Tianjian Feng', 'Chenchen Jing', 'Wen Wang', 'Jiaheng Zhang', 'Hao Chen', 'Chunhua Shen'], 'affiliations': ['National University of Singapore', 'Zhejiang University', 'Zhejiang University of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2601.07351.jpg', 'data': {'categories': ['#diffusion'], 'emoji': 'ğŸŒŠ', 'ru': {'title': 'ĞœÑĞ³ĞºĞ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¶Ñ‘ÑÑ‚ĞºĞ¸Ñ… Ğ¼Ğ°ÑĞ¾Ğº: Ğ¿ÑƒÑ‚ÑŒ Ğº Ğ³Ğ¸Ğ±ĞºĞ¾Ğ¼Ñƒ Ğ¿ĞµÑ€ĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñƒ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'EvoToken-DLM Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ÑĞ³ĞºĞ¸Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¶Ñ‘ÑÑ‚ĞºĞ¸Ñ… Ğ±Ğ¸Ğ½Ğ°Ñ€Ğ½Ñ‹Ñ… Ğ¼Ğ°ÑĞ¾Ğº. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿ĞµÑ€ĞµÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑĞ²Ğ¾Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ñ‚ÑĞ¶ĞµĞ½Ğ¸Ğ¸ Ğ²ÑĞµĞ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ° Ğ½Ğµ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ñ‹Ğ±Ğ¾Ñ€ Ñ ÑĞ°Ğ¼Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ†ĞµĞ»Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ EvoToken-DLM Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¸ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ….'}, 'en': {'title': 'Revisable Decoding with EvoToken-DLM: A New Era in Language Modeling', 'desc': 'EvoToken-DLM is a new approach to language modeling that uses diffusion techniques to improve how models generate text. Instead of using hard decisions for token selection, it employs soft token distributions, allowing for more flexible and revisable decoding. This method incorporates continuous trajectory supervision, which helps the model learn better by aligning its training with the gradual updates during text generation. As a result, EvoToken-DLM shows better performance compared to existing models, making it a significant advancement in the field of language modeling.'}, 'zh': {'title': 'EvoToken-DLMï¼šå¯ä¿®è®¢è§£ç çš„æ–°æ–¹æ³•', 'desc': 'EvoToken-DLMæ˜¯ä¸€ç§åŸºäºæ‰©æ•£çš„è¯­è¨€å»ºæ¨¡æ–¹æ³•ï¼Œé‡‡ç”¨è½¯ä»¤ç‰Œåˆ†å¸ƒå’Œè¿ç»­è½¨è¿¹ç›‘ç£ï¼Œæ”¯æŒå¯ä¿®è®¢è§£ç ã€‚ä¸ä¼ ç»Ÿçš„ç¡¬äºŒè¿›åˆ¶æ©ç ä¸åŒï¼ŒEvoToken-DLMä½¿ç”¨ä¸æ–­æ¼”å˜çš„è½¯ä»¤ç‰Œåˆ†å¸ƒï¼Œä½¿å¾—ä»æ©ç çŠ¶æ€åˆ°ç¦»æ•£è¾“å‡ºçš„è¿‡æ¸¡æ›´åŠ å¹³æ»‘ã€‚é€šè¿‡å¼•å…¥è¿ç»­è½¨è¿¹ç›‘ç£ï¼Œè¯¥æ–¹æ³•å°†è®­ç»ƒç›®æ ‡ä¸è¿­ä»£æ¦‚ç‡æ›´æ–°å¯¹é½ï¼Œä»è€Œæé«˜äº†æ¨¡å‹çš„çµæ´»æ€§å’Œæ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒEvoToken-DLMåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†ç°æœ‰çš„æ‰©æ•£åŸºç¡€å’Œæ©ç DLMåŸºçº¿ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.05107', 'title': 'Controllable Memory Usage: Balancing Anchoring and Innovation in Long-Term Human-Agent Interaction', 'url': 'https://huggingface.co/papers/2601.05107', 'abstract': "A framework is presented that enables dynamic regulation of memory reliance in LLM-based agents, allowing users to control the balance between innovation and historical fidelity in long-term interactions.  \t\t\t\t\tAI-generated summary \t\t\t\t As LLM-based agents are increasingly used in long-term interactions, cumulative memory is critical for enabling personalization and maintaining stylistic consistency. However, most existing systems adopt an ``all-or-nothing'' approach to memory usage: incorporating all relevant past information can lead to Memory Anchoring, where the agent is trapped by past interactions, while excluding memory entirely results in under-utilization and the loss of important interaction history. We show that an agent's reliance on memory can be modeled as an explicit and user-controllable dimension. We first introduce a behavioral metric of memory dependence to quantify the influence of past interactions on current outputs. We then propose Steerable Memory Agent, SteeM, a framework that allows users to dynamically regulate memory reliance, ranging from a fresh-start mode that promotes innovation to a high-fidelity mode that closely follows interaction history. Experiments across different scenarios demonstrate that our approach consistently outperforms conventional prompting and rigid memory masking strategies, yielding a more nuanced and effective control for personalized human-agent collaboration.", 'score': 18, 'issue_id': 548, 'pub_date': '2026-01-08', 'pub_date_card': {'ru': '8 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 8', 'zh': '1æœˆ8æ—¥'}, 'hash': '12c130572293a58f', 'authors': ['Muzhao Tian', 'Zisu Huang', 'Xiaohua Wang', 'Jingwen Xu', 'Zhengkang Guo', 'Qi Qian', 'Yuanzhe Shen', 'Kaitao Song', 'Jiakang Yuan', 'Changze Lv', 'Xiaoqing Zheng'], 'affiliations': ['College of Computer Science and Artificial Intelligence, Fudan University', 'Shanghai Key Laboratory of Intelligent Information Processing'], 'pdf_title_img': 'assets/pdf/title_img/2601.05107.jpg', 'data': {'categories': [], 'emoji': 'ğŸšï¸', 'ru': {'title': 'Ğ£Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ°Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ: Ğ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ² Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ñ…', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° SteeM Ğ´Ğ»Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ² LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ñ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ "Memory Anchoring", ĞºĞ¾Ğ³Ğ´Ğ° Ğ°Ğ³ĞµĞ½Ñ‚ ÑĞ»Ğ¸ÑˆĞºĞ¾Ğ¼ ÑĞ¸Ğ»ÑŒĞ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·Ñ‹Ğ²Ğ°ĞµÑ‚ÑÑ Ğº Ğ¿Ñ€Ğ¾ÑˆĞ»Ñ‹Ğ¼ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑĞ¼, Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ ĞºĞ°Ğº ÑĞ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°. ĞĞ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼ Ğ¿Ğ»Ğ°Ğ²Ğ½Ğ¾ Ñ€ĞµĞ³ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑÑ‚ĞµĞ¿ĞµĞ½ÑŒ Ğ¾Ğ¿Ğ¾Ñ€Ñ‹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ½Ğ° Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ - Ğ¾Ñ‚ Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ° Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¹ Ğ´Ğ¾ Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ° Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¸Ğ½Ğ³Ğ° Ğ¸ Ğ¶Ñ‘ÑÑ‚ĞºĞ¾Ğ³Ğ¾ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸.'}, 'en': {'title': 'Dynamic Memory Control for Personalized AI Interactions', 'desc': "This paper introduces a new framework called SteeM that allows users to control how much memory an LLM-based agent uses during interactions. It addresses the problem of balancing innovation and historical accuracy, which is crucial for personalized experiences. The authors present a way to measure how much past interactions influence the agent's current responses, creating a flexible memory system. Experiments show that SteeM outperforms traditional methods by providing better control over memory usage, enhancing collaboration between humans and agents."}, 'zh': {'title': 'åŠ¨æ€è°ƒèŠ‚è®°å¿†ï¼Œæå‡äººæœºåä½œæ•ˆæœ', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ¡†æ¶ï¼Œå…è®¸ç”¨æˆ·åŠ¨æ€è°ƒèŠ‚åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†çš„è®°å¿†ä¾èµ–æ€§ï¼Œä»è€Œåœ¨é•¿æœŸäº¤äº’ä¸­å¹³è¡¡åˆ›æ–°ä¸å†å²å¿ å®åº¦ã€‚ç°æœ‰ç³»ç»Ÿé€šå¸¸é‡‡ç”¨å…¨æœ‰æˆ–å…¨æ— çš„è®°å¿†ä½¿ç”¨æ–¹å¼ï¼Œå¯¼è‡´è®°å¿†é”šå®šç°è±¡ï¼Œé™åˆ¶äº†ä»£ç†çš„çµæ´»æ€§ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§è¡Œä¸ºåº¦é‡æ¥é‡åŒ–è¿‡å»äº¤äº’å¯¹å½“å‰è¾“å‡ºçš„å½±å“ï¼Œå¹¶æå‡ºäº†å¯è°ƒè®°å¿†ä»£ç†ï¼ˆSteeMï¼‰ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿåœ¨ä¿ƒè¿›åˆ›æ–°å’Œä¿æŒé«˜å¿ å®åº¦ä¹‹é—´è¿›è¡Œè°ƒèŠ‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸ªæ€§åŒ–äººæœºåä½œä¸­ä¼˜äºä¼ ç»Ÿçš„æç¤ºå’ŒåƒµåŒ–çš„è®°å¿†å±è”½ç­–ç•¥ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.01528', 'title': 'DrivingGen: A Comprehensive Benchmark for Generative Video World Models in Autonomous Driving', 'url': 'https://huggingface.co/papers/2601.01528', 'abstract': 'DrivingGen presents the first comprehensive benchmark for generative driving world models, addressing limitations in existing evaluations through diverse datasets and metrics that assess visual realism, trajectory plausibility, temporal coherence, and controllability.  \t\t\t\t\tAI-generated summary \t\t\t\t Video generation models, as one form of world models, have emerged as one of the most exciting frontiers in AI, promising agents the ability to imagine the future by modeling the temporal evolution of complex scenes. In autonomous driving, this vision gives rise to driving world models: generative simulators that imagine ego and agent futures, enabling scalable simulation, safe testing of corner cases, and rich synthetic data generation. Yet, despite fast-growing research activity, the field lacks a rigorous benchmark to measure progress and guide priorities. Existing evaluations remain limited: generic video metrics overlook safety-critical imaging factors; trajectory plausibility is rarely quantified; temporal and agent-level consistency is neglected; and controllability with respect to ego conditioning is ignored. Moreover, current datasets fail to cover the diversity of conditions required for real-world deployment. To address these gaps, we present DrivingGen, the first comprehensive benchmark for generative driving world models. DrivingGen combines a diverse evaluation dataset curated from both driving datasets and internet-scale video sources, spanning varied weather, time of day, geographic regions, and complex maneuvers, with a suite of new metrics that jointly assess visual realism, trajectory plausibility, temporal coherence, and controllability. Benchmarking 14 state-of-the-art models reveals clear trade-offs: general models look better but break physics, while driving-specific ones capture motion realistically but lag in visual quality. DrivingGen offers a unified evaluation framework to foster reliable, controllable, and deployable driving world models, enabling scalable simulation, planning, and data-driven decision-making.', 'score': 16, 'issue_id': 549, 'pub_date': '2026-01-04', 'pub_date_card': {'ru': '4 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 4', 'zh': '1æœˆ4æ—¥'}, 'hash': '0dd3823130c5337d', 'authors': ['Yang Zhou', 'Hao Shao', 'Letian Wang', 'Zhuofan Zong', 'Hongsheng Li', 'Steven L. Waslander'], 'affiliations': ['CUHK MMLab', 'University of Toronto'], 'pdf_title_img': 'assets/pdf/title_img/2601.01528.jpg', 'data': {'categories': ['#survey', '#diffusion', '#synthetic'], 'emoji': 'ğŸš—', 'ru': {'title': 'Ğ•Ğ´Ğ¸Ğ½Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ', 'desc': 'DrivingGen Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ¸Ñ€Ğ° Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğµ ÑĞµÑ‚Ğ¸ Ğ´Ğ»Ñ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… ÑÑ†ĞµĞ½ Ğ² Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ¼ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¸. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ³Ğ¾Ğ´Ğ½Ñ‹Ğ¼Ğ¸ ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑĞ¼Ğ¸, Ğ²Ñ€ĞµĞ¼ĞµĞ½ĞµĞ¼ ÑÑƒÑ‚Ğ¾Ğº Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ°Ğ½Ñ‘Ğ²Ñ€Ğ°Ğ¼Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ¼Ğ°, Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼: Ğ¾Ğ±Ñ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ñ‹Ğ³Ğ»ÑĞ´ÑÑ‚ Ğ»ÑƒÑ‡ÑˆĞµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾, Ğ½Ğ¾ Ğ½Ğ°Ñ€ÑƒÑˆĞ°ÑÑ‚ Ñ„Ğ¸Ğ·Ğ¸ĞºÑƒ, Ğ² Ñ‚Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ĞºĞ°Ğº ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½ĞµĞµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´ÑÑ‚ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ, Ğ½Ğ¾ ÑƒÑÑ‚ÑƒĞ¿Ğ°ÑÑ‚ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ­Ñ‚Ğ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ĞµĞ´Ğ¸Ğ½ÑƒÑ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ñƒ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ñ‹Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'DrivingGen: A New Standard for Evaluating Generative Driving Models', 'desc': 'DrivingGen introduces a new benchmark for evaluating generative driving world models, which are essential for simulating autonomous driving scenarios. It addresses the shortcomings of existing evaluations by providing diverse datasets and metrics that measure visual realism, trajectory plausibility, temporal coherence, and controllability. The benchmark reveals trade-offs between general models that excel in visual quality but fail in physical accuracy, and driving-specific models that maintain realistic motion but lack visual appeal. By offering a comprehensive evaluation framework, DrivingGen aims to enhance the development of reliable and deployable driving simulations for safe autonomous driving.'}, 'zh': {'title': 'DrivingGenï¼šç”Ÿæˆé©¾é©¶æ¨¡å‹çš„å…¨é¢åŸºå‡†', 'desc': 'DrivingGenæ˜¯ç¬¬ä¸€ä¸ªå…¨é¢çš„ç”Ÿæˆé©¾é©¶ä¸–ç•Œæ¨¡å‹åŸºå‡†ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰è¯„ä¼°çš„å±€é™æ€§ã€‚å®ƒé€šè¿‡å¤šæ ·åŒ–çš„æ•°æ®é›†å’ŒæŒ‡æ ‡æ¥è¯„ä¼°è§†è§‰çœŸå®æ„Ÿã€è½¨è¿¹åˆç†æ€§ã€æ—¶é—´ä¸€è‡´æ€§å’Œå¯æ§æ€§ã€‚è¯¥åŸºå‡†ç»“åˆäº†æ¥è‡ªé©¾é©¶æ•°æ®é›†å’Œäº’è”ç½‘è§†é¢‘æºçš„å¤šæ ·åŒ–è¯„ä¼°æ•°æ®é›†ï¼Œæ¶µç›–ä¸åŒçš„å¤©æ°”ã€æ—¶é—´å’Œåœ°ç†åŒºåŸŸã€‚DrivingGenä¸ºç”Ÿæˆé©¾é©¶ä¸–ç•Œæ¨¡å‹æä¾›äº†ä¸€ä¸ªç»Ÿä¸€çš„è¯„ä¼°æ¡†æ¶ï¼Œä¿ƒè¿›å¯é ã€å¯æ§å’Œå¯éƒ¨ç½²çš„æ¨¡å‹å‘å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.07526', 'title': 'MegaFlow: Large-Scale Distributed Orchestration System for the Agentic Era', 'url': 'https://huggingface.co/papers/2601.07526', 'abstract': 'MegaFlow is a distributed orchestration system that enables large-scale training and evaluation of agents on complex tasks by providing efficient scheduling, resource allocation, and task management through modular services.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapid development of interactive and autonomous AI systems signals our entry into the agentic era. Training and evaluating agents on complex agentic tasks such as software engineering and computer use requires not only efficient model computation but also sophisticated infrastructure capable of coordinating vast agent-environment interactions. However, no open-source infrastructure can effectively support large-scale training and evaluation on such complex agentic tasks. To address this challenge, we present MegaFlow, a large-scale distributed orchestration system that enables efficient scheduling, resource allocation, and fine-grained task management for agent-environment workloads. MegaFlow abstracts agent training infrastructure into three independent services (Model Service, Agent Service, and Environment Service) that interact through unified interfaces, enabling independent scaling and flexible resource allocation across diverse agent-environment configurations. In our agent training deployments, MegaFlow successfully orchestrates tens of thousands of concurrent agent tasks while maintaining high system stability and achieving efficient resource utilization. By enabling such large-scale agent training, MegaFlow addresses a critical infrastructure gap in the emerging agentic AI landscape.', 'score': 14, 'issue_id': 545, 'pub_date': '2026-01-12', 'pub_date_card': {'ru': '12 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 12', 'zh': '1æœˆ12æ—¥'}, 'hash': '39021c9c97b22a61', 'authors': ['Lei Zhang', 'Mouxiang Chen', 'Ruisheng Cao', 'Jiawei Chen', 'Fan Zhou', 'Yiheng Xu', 'Jiaxi Yang', 'Liang Chen', 'Changwei Luo', 'Kai Zhang', 'Fan Yan', 'KaShun Shum', 'Jiajun Zhang', 'Zeyu Cui', 'Hu Feng', 'Junyang Lin', 'Binyuan Hui', 'Min Yang'], 'affiliations': ['Alibaba Group', 'Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2601.07526.jpg', 'data': {'categories': ['#open_source', '#training', '#agents'], 'emoji': 'ğŸ”„', 'ru': {'title': 'Ğ˜Ğ½Ñ„Ñ€Ğ°ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'MegaFlow â€” ÑÑ‚Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ñ€ĞºĞµÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ° Ğ½Ğ° Ñ‚Ñ€Ğ¸ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ñ‹Ñ… ÑĞµÑ€Ğ²Ğ¸ÑĞ° (Model Service, Agent Service Ğ¸ Environment Service), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²ÑƒÑÑ‚ Ñ‡ĞµÑ€ĞµĞ· ĞµĞ´Ğ¸Ğ½Ñ‹Ğµ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑÑ‹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³Ğ¸Ğ±ĞºĞ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€ĞµÑÑƒÑ€ÑÑ‹. MegaFlow ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ´ĞµÑÑÑ‚ĞºĞ°Ğ¼Ğ¸ Ñ‚Ñ‹ÑÑÑ‡ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ². Ğ­Ñ‚Ğ° Ğ¸Ğ½Ñ„Ñ€Ğ°ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° Ñ€ĞµÑˆĞ°ĞµÑ‚ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚ĞºĞ° Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… AI-ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ñ€Ğ¾Ğ´Ğµ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞŸĞ Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ¾Ğ¼.'}, 'en': {'title': 'Empowering Large-Scale AI Agent Training with MegaFlow', 'desc': 'MegaFlow is a distributed orchestration system designed to facilitate the large-scale training and evaluation of AI agents on complex tasks. It provides efficient scheduling, resource allocation, and task management through modular services, which allows for independent scaling of different components. The system abstracts the training infrastructure into three services: Model Service, Agent Service, and Environment Service, enabling flexible resource allocation. By successfully managing tens of thousands of concurrent agent tasks, MegaFlow fills a significant gap in the infrastructure needed for advanced agentic AI development.'}, 'zh': {'title': 'MegaFlowï¼šæ™ºèƒ½ä½“è®­ç»ƒçš„é«˜æ•ˆç¼–æ’ç³»ç»Ÿ', 'desc': 'MegaFlowæ˜¯ä¸€ä¸ªåˆ†å¸ƒå¼ç¼–æ’ç³»ç»Ÿï¼Œæ—¨åœ¨æ”¯æŒå¤§è§„æ¨¡çš„æ™ºèƒ½ä½“è®­ç»ƒå’Œè¯„ä¼°ã€‚å®ƒé€šè¿‡æ¨¡å—åŒ–æœåŠ¡æä¾›é«˜æ•ˆçš„è°ƒåº¦ã€èµ„æºåˆ†é…å’Œä»»åŠ¡ç®¡ç†ï¼Œé€‚ç”¨äºå¤æ‚çš„ä»»åŠ¡ã€‚MegaFlowå°†æ™ºèƒ½ä½“è®­ç»ƒåŸºç¡€è®¾æ–½æŠ½è±¡ä¸ºä¸‰ä¸ªç‹¬ç«‹çš„æœåŠ¡ï¼Œåˆ†åˆ«æ˜¯æ¨¡å‹æœåŠ¡ã€æ™ºèƒ½ä½“æœåŠ¡å’Œç¯å¢ƒæœåŠ¡ï¼Œè¿™äº›æœåŠ¡é€šè¿‡ç»Ÿä¸€æ¥å£è¿›è¡Œäº¤äº’ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒMegaFlowèƒ½å¤Ÿåœ¨ä¿æŒç³»ç»Ÿç¨³å®šæ€§çš„åŒæ—¶ï¼ŒæˆåŠŸç®¡ç†æˆåƒä¸Šä¸‡çš„å¹¶å‘æ™ºèƒ½ä½“ä»»åŠ¡ï¼Œå®ç°é«˜æ•ˆçš„èµ„æºåˆ©ç”¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.05823', 'title': 'Boosting Latent Diffusion Models via Disentangled Representation Alignment', 'url': 'https://huggingface.co/papers/2601.05823', 'abstract': 'Latent Diffusion Models generate high-quality images by operating in compressed latent space, typically obtained through image tokenizers such as Variational Autoencoders (VAEs). In pursuit of a generation-friendly VAE, recent studies have explored leveraging Vision Foundation Models (VFMs) as representation alignment targets for VAEs, mirroring the approach commonly adopted for LDMs. Although this yields certain performance gains, using the same alignment target for both VAEs and LDMs overlooks their fundamentally different representational requirements. We advocate that while LDMs benefit from latents retaining high-level semantic concepts, VAEs should excel in semantic disentanglement, enabling encoding of attribute-level information in a structured way. To address this, we propose the Semantic disentangled VAE (Send-VAE), explicitly optimized for disentangled representation learning through aligning its latent space with the semantic hierarchy of pre-trained VFMs. Our approach employs a non-linear mapper network to transform VAE latents, aligning them with VFMs to bridge the gap between attribute-level disentanglement and high-level semantics, facilitating effective guidance for VAE learning. We evaluate semantic disentanglement via linear probing on attribute prediction tasks, showing strong correlation with improved generation performance. Finally, using Send-VAE, we train flow-based transformers SiTs; experiments show Send-VAE significantly speeds up training and achieves a state-of-the-art FID of 1.21 and 1.75 with and without classifier-free guidance on ImageNet 256x256.  \t\t\t\t\tAI-generated summary \t\t\t\t Latent Diffusion Models (LDMs) generate high-quality images by operating in a compressed latent space, typically obtained through image tokenizers such as Variational Autoencoders (VAEs). In pursuit of a generation-friendly VAE, recent studies have explored leveraging Vision Foundation Models (VFMs) as representation alignment targets for VAEs, mirroring the approach commonly adopted for LDMs. Although this yields certain performance gains, using the same alignment target for both VAEs and LDMs overlooks their fundamentally different representational requirements. We advocate that while LDMs benefit from latents retaining high-level semantic concepts, VAEs should excel in semantic disentanglement, enabling encoding of attribute-level information in a structured way. To address this, we propose the Semantic disentangled VAE (Send-VAE), explicitly optimized for disentangled representation learning through aligning its latent space with the semantic hierarchy of pre-trained VFMs. Our approach employs a non-linear mapper network to transform VAE latents, aligning them with VFMs to bridge the gap between attribute-level disentanglement and high-level semantics, facilitating effective guidance for VAE learning. We evaluate semantic disentanglement via linear probing on attribute prediction tasks, showing strong correlation with improved generation performance. Finally, using Send-VAE, we train flow-based transformers SiTs; experiments show Send-VAE significantly speeds up training and achieves a state-of-the-art FID of 1.21 and 1.75 with and without classifier-free guidance on ImageNet 256x256.', 'score': 14, 'issue_id': 544, 'pub_date': '2026-01-09', 'pub_date_card': {'ru': '9 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 9', 'zh': '1æœˆ9æ—¥'}, 'hash': 'd7de342c692a7e3e', 'authors': ['John Page', 'Xuesong Niu', 'Kai Wu', 'Kun Gai'], 'affiliations': ['Kolors Team, Kuaishou Technology'], 'pdf_title_img': 'assets/pdf/title_img/2601.05823.jpg', 'data': {'categories': ['#training', '#architecture', '#optimization', '#diffusion', '#cv'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ”Ğ¸sentangled Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸: VAE Ğ¿ĞµÑ€ĞµÑƒÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ¿Ğ¾-Ğ½Ğ¾Ğ²Ğ¾Ğ¼Ñƒ', 'desc': 'Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Send-VAE â€” Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ°Ğ²Ñ‚Ğ¾ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº, ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ´Ğ»Ñ Ğ´Ğ¸sentangled Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ¾Ğ´Ğ¸Ğ½Ğ°ĞºĞ¾Ğ²Ñ‹Ğµ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ VAE Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ VAE Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ñ€Ğ°Ğ·Ğ´ĞµĞ»Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½ĞµĞ»Ğ¸Ğ½ĞµĞ¹Ğ½ÑƒÑ ÑĞµÑ‚ÑŒ-Ğ¼Ğ°Ğ¿Ğ¿ĞµÑ€ Ğ´Ğ»Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° VAE Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸ĞµĞ¹ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… foundation Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Send-VAE Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²Ğ° Ñ FID 1.21 Ğ½Ğ° ImageNet 256x256, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒÑĞºĞ¾Ñ€ÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ flow-based Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ².'}, 'en': {'title': 'Enhancing Image Generation with Semantic Disentangled VAEs', 'desc': 'This paper introduces the Semantic Disentangled VAE (Send-VAE), which aims to improve image generation by optimizing Variational Autoencoders (VAEs) for better semantic disentanglement. Unlike traditional methods that use the same alignment targets for both VAEs and Latent Diffusion Models (LDMs), Send-VAE focuses on aligning its latent space with the semantic hierarchy of pre-trained Vision Foundation Models (VFMs). This approach allows VAEs to encode attribute-level information more effectively while maintaining high-level semantic concepts, enhancing the overall image generation process. The results demonstrate that Send-VAE not only speeds up training but also achieves state-of-the-art performance on image quality metrics like FID.'}, 'zh': {'title': 'ä¼˜åŒ–è§£è€¦è¡¨ç¤ºï¼Œæå‡å›¾åƒç”Ÿæˆè´¨é‡', 'desc': 'æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMsï¼‰é€šè¿‡åœ¨å‹ç¼©çš„æ½œåœ¨ç©ºé—´ä¸­ç”Ÿæˆé«˜è´¨é‡å›¾åƒï¼Œé€šå¸¸ä½¿ç”¨å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEsï¼‰ä½œä¸ºå›¾åƒæ ‡è®°å™¨ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§è¯­ä¹‰è§£è€¦å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆSend-VAEï¼‰ï¼Œæ—¨åœ¨ä¼˜åŒ–è§£è€¦è¡¨ç¤ºå­¦ä¹ ï¼Œå¹¶å°†å…¶æ½œåœ¨ç©ºé—´ä¸é¢„è®­ç»ƒè§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆVFMsï¼‰çš„è¯­ä¹‰å±‚æ¬¡å¯¹é½ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä½¿ç”¨éçº¿æ€§æ˜ å°„ç½‘ç»œæ¥è½¬æ¢VAEçš„æ½œåœ¨è¡¨ç¤ºï¼Œä»è€Œæœ‰æ•ˆåœ°è¿æ¥å±æ€§çº§è§£è€¦ä¸é«˜å±‚è¯­ä¹‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSend-VAEæ˜¾è‘—åŠ å¿«äº†è®­ç»ƒé€Ÿåº¦ï¼Œå¹¶åœ¨ImageNet 256x256ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„FIDå€¼ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.06165', 'title': 'What Users Leave Unsaid: Under-Specified Queries Limit Vision-Language Models', 'url': 'https://huggingface.co/papers/2601.06165', 'abstract': 'Real-world vision-language benchmarks reveal that under-specified user queries pose significant challenges for current models, with explicit query rewriting leading to substantial performance improvements.  \t\t\t\t\tAI-generated summary \t\t\t\t Current vision-language benchmarks predominantly feature well-structured questions with clear, explicit prompts. However, real user queries are often informal and underspecified. Users naturally leave much unsaid, relying on images to convey context. We introduce HAERAE-Vision, a benchmark of 653 real-world visual questions from Korean online communities (0.76% survival from 86K candidates), each paired with an explicit rewrite, yielding 1,306 query variants in total. Evaluating 39 VLMs, we find that even state-of-the-art models (GPT-5, Gemini 2.5 Pro) achieve under 50% on the original queries. Crucially, query explicitation alone yields 8 to 22 point improvements, with smaller models benefiting most. We further show that even with web search, under-specified queries underperform explicit queries without search, revealing that current retrieval cannot compensate for what users leave unsaid. Our findings demonstrate that a substantial portion of VLM difficulty stem from natural query under-specification instead of model capability, highlighting a critical gap between benchmark evaluation and real-world deployment.', 'score': 14, 'issue_id': 544, 'pub_date': '2026-01-07', 'pub_date_card': {'ru': '7 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 7', 'zh': '1æœˆ7æ—¥'}, 'hash': '11228a6956fd7028', 'authors': ['Dasol Choi', 'Guijin Son', 'Hanwool Lee', 'Minhyuk Kim', 'Hyunwoo Ko', 'Teabin Lim', 'Ahn Eungyeol', 'Jungwhan Kim', 'Seunghyeok Hong', 'Youngsook Song'], 'affiliations': ['AIM Intelligence', 'Doodlin Corp.', 'Hankuk University of Foreign Studies', 'Korea University', 'Lablup Inc.', 'NAVER Cloud', 'OneLineAI', 'Yonsei University'], 'pdf_title_img': 'assets/pdf/title_img/2601.06165.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#dataset', '#low_resource', '#cv'], 'emoji': 'ğŸ”', 'ru': {'title': 'ĞŸĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ½ĞµÑÑĞ½Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² â€” ĞºĞ»ÑÑ‡ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ·Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ HAERAE-Vision, Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¸Ğ· 653 Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸Ğ· ĞºĞ¾Ñ€ĞµĞ¹ÑĞºĞ¸Ñ… Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-ÑĞ¾Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ², ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ğ¸Ğ· ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½ ÑĞ²Ğ½Ğ¾Ğ¹ Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¾Ğ¹. ĞŸÑ€Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞµ 39 Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ·Ñ€ĞµĞ½Ğ¸Ñ-ÑĞ·Ñ‹ĞºĞ° Ğ¾Ğ½Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ ÑĞ°Ğ¼Ñ‹Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (GPT-5, Gemini 2.5 Pro) Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ¼ĞµĞ½ĞµĞµ 50% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ñ…. Ğ¯Ğ²Ğ½Ğ°Ñ Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ° Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ° 8-22 Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿ÑƒĞ½ĞºÑ‚Ğ°, Ğ¿Ñ€Ğ¸Ñ‡Ñ‘Ğ¼ Ğ±Ğ¾Ğ»ĞµĞµ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ÑÑ‚ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ Ğ¿Ğ¾Ğ»ÑŒĞ·Ñƒ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ Ñ‡Ğ°ÑÑ‚ÑŒ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ·Ñ€ĞµĞ½Ğ¸Ñ-ÑĞ·Ñ‹ĞºĞ° Ğ²Ñ‹Ğ·Ğ²Ğ°Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ², Ğ° Ğ½Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹, Ñ‡Ñ‚Ğ¾ Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¾Ğ¹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸ĞµĞ¼.'}, 'en': {'title': 'Bridging the Gap: Enhancing VLMs with Clearer Queries', 'desc': 'This paper addresses the challenges faced by vision-language models (VLMs) when dealing with real-world user queries that are often informal and underspecified. The authors introduce HAERAE-Vision, a new benchmark consisting of 653 visual questions sourced from Korean online communities, each accompanied by explicit rewrites to clarify the queries. Their evaluation of 39 VLMs reveals that even the most advanced models struggle with original queries, scoring below 50%. The study highlights that improving query clarity through explicit rewriting can significantly enhance model performance, particularly for smaller models, and underscores the gap between benchmark evaluations and practical applications in real-world scenarios.'}, 'zh': {'title': 'æ˜ç¡®åŒ–æŸ¥è¯¢ï¼Œæå‡è§†è§‰-è¯­è¨€æ¨¡å‹æ€§èƒ½', 'desc': 'æœ¬ç ”ç©¶æ­ç¤ºäº†ç°å®ä¸–ç•Œä¸­è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åœ¨å¤„ç†ä¸æ˜ç¡®ç”¨æˆ·æŸ¥è¯¢æ—¶é¢ä¸´çš„é‡å¤§æŒ‘æˆ˜ã€‚æˆ‘ä»¬å¼•å…¥äº†HAERAE-VisionåŸºå‡†ï¼ŒåŒ…å«æ¥è‡ªéŸ©å›½åœ¨çº¿ç¤¾åŒºçš„653ä¸ªçœŸå®è§†è§‰é—®é¢˜ï¼Œå¹¶ä¸ºæ¯ä¸ªé—®é¢˜æä¾›äº†æ˜ç¡®çš„é‡å†™ç‰ˆæœ¬ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„æ¨¡å‹åœ¨å¤„ç†åŸå§‹æŸ¥è¯¢æ—¶çš„è¡¨ç°ä¹Ÿä¸è¶³50%ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼ŒæŸ¥è¯¢çš„æ˜ç¡®åŒ–å¯ä»¥æ˜¾è‘—æé«˜æ¨¡å‹æ€§èƒ½ï¼Œå°¤å…¶æ˜¯å¯¹è¾ƒå°çš„æ¨¡å‹å½±å“æ›´å¤§ï¼Œå¼ºè°ƒäº†åŸºå‡†è¯„ä¼°ä¸å®é™…åº”ç”¨ä¹‹é—´çš„å…³é”®å·®è·ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.06860', 'title': 'ET-Agent: Incentivizing Effective Tool-Integrated Reasoning Agent via Behavior Calibration', 'url': 'https://huggingface.co/papers/2601.06860', 'abstract': "ET-Agent is a training framework that calibrates tool-use behavior in large language models through self-evolving data flywheels and behavior calibration training to improve task execution effectiveness.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) can extend their parameter knowledge limits by adopting the Tool-Integrated Reasoning (TIR) paradigm. However, existing LLM-based agent training framework often focuses on answers' accuracy, overlooking specific alignment for behavior patterns. Consequently, agent often exhibits ineffective actions during TIR tasks, such as redundant and insufficient tool calls. How to calibrate erroneous behavioral patterns when executing TIR tasks, thereby exploring effective trajectories, remains an open-ended problem. In this paper, we propose ET-Agent, a training framework for calibrating agent's tool-use behavior through two synergistic perspectives: Self-evolving Data Flywheel and Behavior Calibration Training. Specifically, we introduce a self-evolutionary data flywheel to generate enhanced data, used to fine-tune LLM to improve its exploration ability. Based on this, we implement an two-phases behavior-calibration training framework. It is designed to progressively calibrate erroneous behavioral patterns to optimal behaviors. Further in-depth experiments confirm the superiority of  across multiple dimensions, including correctness, efficiency, reasoning conciseness, and tool execution accuracy. Our ET-Agent framework provides practical insights for research in the TIR field. Codes can be found in https://github.com/asilverlight/ET-Agent", 'score': 13, 'issue_id': 544, 'pub_date': '2026-01-11', 'pub_date_card': {'ru': '11 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 11', 'zh': '1æœˆ11æ—¥'}, 'hash': '600061ce305fd03f', 'authors': ['Yifei Chen', 'Guanting Dong', 'Zhicheng Dou'], 'affiliations': ['Gaoling School of Artificial Intelligence, Renmin University of China'], 'pdf_title_img': 'assets/pdf/title_img/2601.06860.jpg', 'data': {'categories': [], 'emoji': 'ğŸ”§', 'ru': {'title': 'ĞšĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ', 'desc': 'ET-Agent â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ñ‡ĞµÑ€ĞµĞ· ÑĞ°Ğ¼Ğ¾ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğµ Ñ†Ğ¸ĞºĞ»Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½ĞµÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ¸Ğ»Ğ¸ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ¾Ğ±Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ Ğº Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ÑÑ‚ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´Ğ²Ğ° ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°: ÑĞ°Ğ¼Ğ¾ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ñ†Ğ¸ĞºĞ» Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¸ Ğ´Ğ²ÑƒÑ…Ñ„Ğ°Ğ·Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ¿Ğ¾ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸, ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ»Ğ°ĞºĞ¾Ğ½Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Calibrating Tool-Use Behavior for Enhanced Task Execution in LLMs', 'desc': "ET-Agent is a novel training framework designed to enhance the tool-use behavior of large language models (LLMs) during task execution. It employs a Self-evolving Data Flywheel to generate improved training data, which helps fine-tune the model's exploration capabilities. Additionally, the framework incorporates Behavior Calibration Training to systematically correct and optimize the agent's behavioral patterns when performing Tool-Integrated Reasoning (TIR) tasks. Experimental results demonstrate that ET-Agent significantly improves the correctness, efficiency, and accuracy of tool execution in LLMs."}, 'zh': {'title': 'ET-Agentï¼šä¼˜åŒ–å·¥å…·ä½¿ç”¨è¡Œä¸ºçš„è®­ç»ƒæ¡†æ¶', 'desc': 'ET-Agentæ˜¯ä¸€ä¸ªè®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡è‡ªæˆ‘æ¼”åŒ–çš„æ•°æ®é£è½®å’Œè¡Œä¸ºæ ¡å‡†è®­ç»ƒæ¥è°ƒæ•´å¤§å‹è¯­è¨€æ¨¡å‹çš„å·¥å…·ä½¿ç”¨è¡Œä¸ºï¼Œä»è€Œæé«˜ä»»åŠ¡æ‰§è¡Œçš„æœ‰æ•ˆæ€§ã€‚è¯¥æ¡†æ¶è§£å†³äº†ç°æœ‰æ¨¡å‹åœ¨å·¥å…·é›†æˆæ¨ç†ä»»åŠ¡ä¸­å¸¸è§çš„è¡Œä¸ºæ¨¡å¼ä¸å¯¹é½é—®é¢˜ï¼Œé¿å…äº†å†—ä½™å’Œä¸è¶³çš„å·¥å…·è°ƒç”¨ã€‚é€šè¿‡ç”Ÿæˆå¢å¼ºæ•°æ®å¹¶å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼ŒET-Agentèƒ½å¤Ÿæœ‰æ•ˆæ¢ç´¢ä»»åŠ¡æ‰§è¡Œçš„æœ€ä½³è·¯å¾„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨æ­£ç¡®æ€§ã€æ•ˆç‡ã€æ¨ç†ç®€æ´æ€§å’Œå·¥å…·æ‰§è¡Œå‡†ç¡®æ€§ç­‰å¤šä¸ªç»´åº¦ä¸Šè¡¨ç°ä¼˜è¶Šã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.07055', 'title': 'Dr. Zero: Self-Evolving Search Agents without Training Data', 'url': 'https://huggingface.co/papers/2601.07055', 'abstract': "A data-free self-evolution framework enables large language models to autonomously improve reasoning capabilities through iterative question generation and solving, achieving performance comparable to supervised methods.  \t\t\t\t\tAI-generated summary \t\t\t\t As high-quality data becomes increasingly difficult to obtain, data-free self-evolution has emerged as a promising paradigm. This approach allows large language models (LLMs) to autonomously generate and solve complex problems, thereby improving their reasoning capabilities. However, multi-turn search agents struggle in data-free self-evolution due to the limited question diversity and the substantial compute required for multi-step reasoning and tool using. In this work, we introduce Dr. Zero, a framework enabling search agents to effectively self-evolve without any training data. In particular, we design a self-evolution feedback loop where a proposer generates diverse questions to train a solver initialized from the same base model. As the solver evolves, it incentivizes the proposer to produce increasingly difficult yet solvable tasks, thus establishing an automated curriculum to refine both agents. To enhance training efficiency, we also introduce hop-grouped relative policy optimization (HRPO). This method clusters structurally similar questions to construct group-level baselines, effectively minimizing the sampling overhead in evaluating each query's individual difficulty and solvability. Consequently, HRPO significantly reduces the compute requirements for solver training without compromising performance or stability. Extensive experiment results demonstrate that the data-free Dr. Zero matches or surpasses fully supervised search agents, proving that complex reasoning and search capabilities can emerge solely through self-evolution.", 'score': 8, 'issue_id': 545, 'pub_date': '2026-01-11', 'pub_date_card': {'ru': '11 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 11', 'zh': '1æœˆ11æ—¥'}, 'hash': '686199b1cd75f07e', 'authors': ['Zhenrui Yue', 'Kartikeya Upasani', 'Xianjun Yang', 'Suyu Ge', 'Shaoliang Nie', 'Yuning Mao', 'Zhe Liu', 'Dong Wang'], 'affiliations': ['Meta Superintelligence Labs', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2601.07055.jpg', 'data': {'categories': ['#reasoning', '#rlhf', '#optimization', '#training', '#agents', '#synthetic'], 'emoji': 'ğŸ”„', 'ru': {'title': 'Ğ¡Ğ°Ğ¼Ğ¾ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‡ĞµÑ€ĞµĞ· Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Dr. Zero, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ‚ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ½Ğ° Ñ†Ğ¸ĞºĞ»Ğµ ÑĞ°Ğ¼Ğ¾ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸, Ğ³Ğ´Ğµ Ğ¾Ğ´Ğ¸Ğ½ Ğ°Ğ³ĞµĞ½Ñ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ° Ğ´Ñ€ÑƒĞ³Ğ¾Ğ¹ Ğ¸Ñ… Ñ€ĞµÑˆĞ°ĞµÑ‚, Ğ¿Ğ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² ĞºĞ°Ğº Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ curriculum. Ğ”Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ hop-grouped relative policy optimization (HRPO), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ğ¾ Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ¾Ñ†ĞµĞ½ĞºÑƒ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ¾Ğ¹ Ğ¸Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‰ĞµĞ¹ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹.'}, 'en': {'title': 'Autonomous Self-Evolution for Enhanced Reasoning in Language Models', 'desc': 'This paper presents Dr. Zero, a framework that allows large language models (LLMs) to enhance their reasoning skills without relying on external training data. The framework operates through a self-evolution feedback loop, where a question generator (proposer) creates diverse questions for a problem solver, which is initialized from the same model. As the solver improves, it encourages the proposer to generate more challenging tasks, creating an automated learning environment. Additionally, the introduction of hop-grouped relative policy optimization (HRPO) helps streamline the training process by grouping similar questions, reducing computational costs while maintaining performance.'}, 'zh': {'title': 'æ— æ•°æ®è‡ªæˆ‘è¿›åŒ–ï¼Œæå‡æ¨ç†èƒ½åŠ›ï¼', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ— æ•°æ®è‡ªæˆ‘è¿›åŒ–æ¡†æ¶ï¼Œå…è®¸å¤§å‹è¯­è¨€æ¨¡å‹é€šè¿‡è¿­ä»£ç”Ÿæˆå’Œè§£å†³é—®é¢˜æ¥è‡ªä¸»æé«˜æ¨ç†èƒ½åŠ›ã€‚è¯¥æ¡†æ¶åä¸ºDr. Zeroï¼Œèƒ½å¤Ÿåœ¨æ²¡æœ‰ä»»ä½•è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹æœ‰æ•ˆåœ°è‡ªæˆ‘è¿›åŒ–ã€‚é€šè¿‡è®¾è®¡è‡ªæˆ‘è¿›åŒ–åé¦ˆå¾ªç¯ï¼Œæè®®è€…ç”Ÿæˆå¤šæ ·åŒ–çš„é—®é¢˜æ¥è®­ç»ƒæ±‚è§£è€…ï¼Œä»è€Œå»ºç«‹è‡ªåŠ¨åŒ–è¯¾ç¨‹ä»¥æå‡ä¸¤ä¸ªä»£ç†çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜å¼•å…¥äº†è·³è·ƒåˆ†ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆHRPOï¼‰ï¼Œæœ‰æ•ˆå‡å°‘äº†æ±‚è§£è€…è®­ç»ƒçš„è®¡ç®—éœ€æ±‚ï¼ŒåŒæ—¶ä¿æŒäº†æ€§èƒ½å’Œç¨³å®šæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.06803', 'title': 'Forest Before Trees: Latent Superposition for Efficient Visual Reasoning', 'url': 'https://huggingface.co/papers/2601.06803', 'abstract': 'Laser introduces a new visual reasoning paradigm using dynamic windowed alignment learning to maintain global features during latent deduction while achieving superior performance with reduced computational overhead.  \t\t\t\t\tAI-generated summary \t\t\t\t While Chain-of-Thought empowers Large Vision-Language Models with multi-step reasoning, explicit textual rationales suffer from an information bandwidth bottleneck, where continuous visual details are discarded during discrete tokenization. Recent latent reasoning methods attempt to address this challenge, but often fall prey to premature semantic collapse due to rigid autoregressive objectives. In this paper, we propose Laser, a novel paradigm that reformulates visual deduction via Dynamic Windowed Alignment Learning (DWAL). Instead of forcing a point-wise prediction, Laser aligns the latent state with a dynamic validity window of future semantics. This mechanism enforces a "Forest-before-Trees" cognitive hierarchy, enabling the model to maintain a probabilistic superposition of global features before narrowing down to local details. Crucially, Laser maintains interpretability via decodable trajectories while stabilizing unconstrained learning via Self-Refined Superposition. Extensive experiments on 6 benchmarks demonstrate that Laser achieves state-of-the-art performance among latent reasoning methods, surpassing the strong baseline Monet by 5.03% on average. Notably, it achieves these gains with extreme efficiency, reducing inference tokens by more than 97%, while demonstrating robust generalization to out-of-distribution domains.', 'score': 8, 'issue_id': 553, 'pub_date': '2026-01-11', 'pub_date_card': {'ru': '11 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 11', 'zh': '1æœˆ11æ—¥'}, 'hash': 'd425c3dfb2622d08', 'authors': ['Yubo Wang', 'Juntian Zhang', 'Yichen Wu', 'Yankai Lin', 'Nils Lukas', 'Yuhan Liu'], 'affiliations': ['Fudan University', 'Gaoling School of Artificial Intelligence, Renmin University of China', 'Harvard University', 'MBZUAI'], 'pdf_title_img': 'assets/pdf/title_img/2601.06803.jpg', 'data': {'categories': ['#benchmark', '#cv', '#multimodal', '#architecture', '#inference', '#reasoning', '#interpretability'], 'emoji': 'ğŸŒ³', 'ru': {'title': 'Ğ›ĞµÑ Ğ¿Ñ€ĞµĞ¶Ğ´Ğµ Ğ´ĞµÑ€ĞµĞ²ÑŒĞµĞ²: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼', 'desc': 'Laser â€” ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ Ğ¾ĞºĞ½Ğ¾Ğ¼ (DWAL) Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ğ¾Ñ‡ĞµÑ‡Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¾ĞºĞ½Ğ¾Ğ¼ Ğ±ÑƒĞ´ÑƒÑ‰ĞµĞ¹ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ĞµĞ¹ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ‚ÑŒ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ¿ĞµÑ€ĞµĞ´ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ñ‡ĞµÑ€ĞµĞ· Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ÑĞ°Ğ¼Ğ¾ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ ÑÑƒĞ¿ĞµÑ€Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ° 5.03% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼ Monet Ğ¿Ñ€Ğ¸ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ½Ğ° 97%.'}, 'en': {'title': 'Laser: Efficient Visual Reasoning with Dynamic Alignment', 'desc': 'The paper presents Laser, a new approach to visual reasoning that enhances the way models process and understand visual information. It introduces Dynamic Windowed Alignment Learning (DWAL), which allows the model to maintain a balance between global features and local details during reasoning. This method prevents the loss of important visual information and avoids issues like semantic collapse that can occur with traditional autoregressive models. Laser not only improves performance on various benchmarks but also does so with significantly reduced computational costs, making it a highly efficient solution for visual reasoning tasks.'}, 'zh': {'title': 'æ¿€å…‰ï¼šé«˜æ•ˆçš„è§†è§‰æ¨ç†æ–°èŒƒå¼', 'desc': 'Laseræ˜¯ä¸€ç§æ–°çš„è§†è§‰æ¨ç†èŒƒå¼ï¼Œé‡‡ç”¨åŠ¨æ€çª—å£å¯¹é½å­¦ä¹ ï¼ˆDWALï¼‰æ¥ä¿æŒå…¨å±€ç‰¹å¾ï¼ŒåŒæ—¶åœ¨æ½œåœ¨æ¨ç†ä¸­å®ç°æ›´é«˜çš„æ€§èƒ½å’Œæ›´ä½çš„è®¡ç®—å¼€é”€ã€‚ä¸ä¼ ç»Ÿçš„é€ç‚¹é¢„æµ‹ä¸åŒï¼ŒLaseré€šè¿‡åŠ¨æ€æœ‰æ•ˆçª—å£å¯¹æœªæ¥è¯­ä¹‰è¿›è¡Œå¯¹é½ï¼Œä»è€Œé¿å…äº†è¯­ä¹‰çš„è¿‡æ—©å´©æºƒã€‚è¯¥æ–¹æ³•é€šè¿‡å¯è§£ç çš„è½¨è¿¹ä¿æŒå¯è§£é‡Šæ€§ï¼Œå¹¶é€šè¿‡è‡ªæˆ‘ç²¾ç‚¼å åŠ ç¨³å®šæ— çº¦æŸå­¦ä¹ ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLaseråœ¨å…­ä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå¹³å‡è¶…è¶Šå¼ºåŸºçº¿Monet 5.03%ï¼Œå¹¶ä¸”åœ¨æ¨ç†æ•ˆç‡ä¸Šæ˜¾è‘—æé«˜ï¼Œæ¨ç†ä»¤ç‰Œå‡å°‘è¶…è¿‡97%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.04698', 'title': 'TourPlanner: A Competitive Consensus Framework with Constraint-Gated Reinforcement Learning for Travel Planning', 'url': 'https://huggingface.co/papers/2601.04698', 'abstract': "TourPlanner addresses travel planning challenges through multi-path reasoning and constraint-gated reinforcement learning to optimize both hard and soft constraints effectively.  \t\t\t\t\tAI-generated summary \t\t\t\t Travel planning is a sophisticated decision-making process that requires synthesizing multifaceted information to construct itineraries. However, existing travel planning approaches face several challenges: (1) Pruning candidate points of interest (POIs) while maintaining a high recall rate; (2) A single reasoning path restricts the exploration capability within the feasible solution space for travel planning; (3) Simultaneously optimizing hard constraints and soft constraints remains a significant difficulty. To address these challenges, we propose TourPlanner, a comprehensive framework featuring multi-path reasoning and constraint-gated reinforcement learning. Specifically, we first introduce a Personalized Recall and Spatial Optimization (PReSO) workflow to construct spatially-aware candidate POIs' set. Subsequently, we propose Competitive consensus Chain-of-Thought (CCoT), a multi-path reasoning paradigm that improves the ability of exploring the feasible solution space. To further refine the plan, we integrate a sigmoid-based gating mechanism into the reinforcement learning stage, which dynamically prioritizes soft-constraint satisfaction only after hard constraints are met. Experimental results on travel planning benchmarks demonstrate that TourPlanner achieves state-of-the-art performance, significantly surpassing existing methods in both feasibility and user-preference alignment.", 'score': 8, 'issue_id': 544, 'pub_date': '2026-01-08', 'pub_date_card': {'ru': '8 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 8', 'zh': '1æœˆ8æ—¥'}, 'hash': '84584233b4dfbf19', 'authors': ['Yinuo Wang', 'Mining Tan', 'Wenxiang Jiao', 'Xiaoxi Li', 'Hao Wang', 'Xuanyu Zhang', 'Yuan Lu', 'Weiming Dong'], 'affiliations': ['MAIS, Institute of Automation, Chinese Academy of Sciences', 'Renmin University of China', 'School of Artificial Intelligence, University of Chinese Academy of Sciences', 'Xiaohongshu Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2601.04698.jpg', 'data': {'categories': ['#rl', '#benchmark'], 'emoji': 'âœˆï¸', 'ru': {'title': 'ĞœĞ½Ğ¾Ğ³Ğ¾Ğ¿ÑƒÑ‚ĞµĞ²Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¾Ğ² Ğ¿ÑƒÑ‚ĞµÑˆĞµÑÑ‚Ğ²Ğ¸Ğ¹', 'desc': 'TourPlanner Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿ÑƒÑ‚ĞµÑˆĞµÑÑ‚Ğ²Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¿ÑƒÑ‚ĞµĞ²Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ³ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ğ¾Ğ¼ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ PReSO Ğ´Ğ»Ñ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ¾Ğ² Ğ´Ğ¾ÑÑ‚Ğ¾Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ‡Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ ÑƒÑ‡Ñ‘Ñ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ—Ğ°Ñ‚ĞµĞ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ğ¼Ñ‹ÑĞ»ĞµĞ¹ Ñ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½ÑĞµĞ½ÑÑƒÑĞ¾Ğ¼ Ğ´Ğ»Ñ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ´Ğ¾Ğ¿ÑƒÑÑ‚Ğ¸Ğ¼Ñ‹Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. ĞĞ°ĞºĞ¾Ğ½ĞµÑ†, Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¸Ğ³Ğ¼Ğ¾Ğ¸Ğ´Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ³ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ğ° Ğ² ÑÑ‚Ğ°Ğ¿ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼ÑĞ³ĞºĞ¸Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ğ¾ÑĞ»Ğµ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ¶Ñ‘ÑÑ‚ĞºĞ¸Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Optimizing Travel Plans with Multi-Path Reasoning and Reinforcement Learning', 'desc': 'TourPlanner is a novel framework designed to enhance travel planning by utilizing multi-path reasoning and constraint-gated reinforcement learning. It effectively addresses key challenges such as maintaining a high recall rate for points of interest while optimizing both hard and soft constraints. The framework introduces a Personalized Recall and Spatial Optimization (PReSO) workflow to create a set of relevant candidate locations, and a Competitive Consensus Chain-of-Thought (CCoT) approach to explore diverse solution paths. Experimental results indicate that TourPlanner outperforms existing methods, achieving better feasibility and alignment with user preferences.'}, 'zh': {'title': 'æ™ºèƒ½æ—…è¡Œè§„åˆ’çš„æ–°æ–¹æ³•', 'desc': 'TourPlanner æ˜¯ä¸€ä¸ªè§£å†³æ—…è¡Œè§„åˆ’æŒ‘æˆ˜çš„æ¡†æ¶ï¼Œé‡‡ç”¨å¤šè·¯å¾„æ¨ç†å’Œçº¦æŸé—¨æ§å¼ºåŒ–å­¦ä¹ æ¥æœ‰æ•ˆä¼˜åŒ–ç¡¬çº¦æŸå’Œè½¯çº¦æŸã€‚å®ƒé¦–å…ˆé€šè¿‡ä¸ªæ€§åŒ–å›å¿†å’Œç©ºé—´ä¼˜åŒ–ï¼ˆPReSOï¼‰å·¥ä½œæµç¨‹æ„å»ºç©ºé—´æ„ŸçŸ¥çš„å€™é€‰å…´è¶£ç‚¹ï¼ˆPOIï¼‰é›†åˆã€‚æ¥ç€ï¼Œä½¿ç”¨ç«äº‰å…±è¯†æ€ç»´é“¾ï¼ˆCCoTï¼‰å¤šè·¯å¾„æ¨ç†èŒƒå¼ï¼Œæå‡æ¢ç´¢å¯è¡Œè§£ç©ºé—´çš„èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTourPlanner åœ¨æ—…è¡Œè§„åˆ’åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.07376', 'title': 'OpenTinker: Separating Concerns in Agentic Reinforcement Learning', 'url': 'https://huggingface.co/papers/2601.07376', 'abstract': 'OpenTinker provides a modular infrastructure for reinforcement learning of large language model agents with separated components and managed execution runtime.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce OpenTinker, an infrastructure for reinforcement learning (RL) of large language model (LLM) agents built around a separation of concerns across algorithm design, execution, and agent-environment interaction. Rather than relying on monolithic, end-to-end RL pipelines, OpenTinker decomposes agentic learning systems into lightweight, composable components with clearly defined abstraction boundaries. Users specify agents, environments, and interaction protocols, while inference and training are delegated to a managed execution runtime. OpenTinker introduces a centralized scheduler for managing training and inference workloads, including LoRA-based and full-parameter RL, supervised fine-tuning, and inference, over shared resources. We further discuss design principles for extending OpenTinker to multi-agent training. Finally, we present a set of RL use cases that demonstrate the effectiveness of the framework in practical agentic learning scenarios.', 'score': 4, 'issue_id': 546, 'pub_date': '2026-01-12', 'pub_date_card': {'ru': '12 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 12', 'zh': '1æœˆ12æ—¥'}, 'hash': '0f39d0102305d976', 'authors': ['Siqi Zhu', 'Jiaxuan You'], 'affiliations': ['University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2601.07376.jpg', 'data': {'categories': ['#training', '#agents', '#rl', '#architecture'], 'emoji': 'ğŸ”§', 'ru': {'title': 'ĞœĞ¾Ğ´ÑƒĞ»ÑŒĞ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ Ğ³Ğ¸Ğ±ĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'OpenTinker â€” ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ğ°Ñ Ğ¸Ğ½Ñ„Ñ€Ğ°ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑÑÑ‰Ğ°Ñ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½, Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ñ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ½Ğ¾Ğ»Ğ¸Ñ‚Ğ½Ñ‹Ñ… end-to-end Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½Ğ¾Ğ², ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ»Ñ‘Ğ³ĞºĞ¸Ğµ, Ğ¿ĞµÑ€ĞµĞ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹ Ñ Ñ‡Ñ‘Ñ‚ĞºĞ¾ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ğ°Ğ¼Ğ¸ Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ†Ğ¸Ğ¹. Ğ˜Ğ½Ñ„Ñ€Ğ°ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ†ĞµĞ½Ñ‚Ñ€Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ¾Ğ¼, Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ LoRA-Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ, Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ supervised fine-tuning Ğ½Ğ° Ğ¾Ğ±Ñ‰Ğ¸Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ°Ñ…. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼.'}, 'en': {'title': 'Modular Reinforcement Learning for Language Models', 'desc': 'OpenTinker is a modular framework designed for reinforcement learning (RL) of large language model (LLM) agents. It separates different components of the learning process, such as algorithm design and agent-environment interaction, allowing for more flexible and efficient system development. Users can define their agents and environments while the framework manages the execution of training and inference tasks. OpenTinker also includes a centralized scheduler to optimize resource usage across various RL methods, making it easier to implement multi-agent training scenarios.'}, 'zh': {'title': 'OpenTinkerï¼šæ¨¡å—åŒ–çš„å¼ºåŒ–å­¦ä¹ åŸºç¡€è®¾æ–½', 'desc': 'OpenTinkeræ˜¯ä¸€ä¸ªç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹ä»£ç†çš„å¼ºåŒ–å­¦ä¹ åŸºç¡€è®¾æ–½ï¼Œå…·æœ‰æ¨¡å—åŒ–çš„ç»„ä»¶å’Œç®¡ç†çš„æ‰§è¡Œè¿è¡Œæ—¶ã€‚å®ƒé€šè¿‡å°†ç®—æ³•è®¾è®¡ã€æ‰§è¡Œå’Œä»£ç†-ç¯å¢ƒäº¤äº’åˆ†ç¦»ï¼Œé¿å…äº†å•ä¸€çš„ç«¯åˆ°ç«¯å¼ºåŒ–å­¦ä¹ ç®¡é“ã€‚ç”¨æˆ·å¯ä»¥æŒ‡å®šä»£ç†ã€ç¯å¢ƒå’Œäº¤äº’åè®®ï¼Œè€Œæ¨ç†å’Œè®­ç»ƒåˆ™ç”±ç®¡ç†çš„æ‰§è¡Œè¿è¡Œæ—¶å¤„ç†ã€‚OpenTinkerè¿˜å¼•å…¥äº†ä¸€ä¸ªé›†ä¸­è°ƒåº¦å™¨ï¼Œç”¨äºç®¡ç†å…±äº«èµ„æºä¸Šçš„è®­ç»ƒå’Œæ¨ç†å·¥ä½œè´Ÿè½½ï¼Œæ”¯æŒå¤šä»£ç†è®­ç»ƒçš„æ‰©å±•è®¾è®¡åŸåˆ™ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.07767', 'title': 'Are LLM Decisions Faithful to Verbal Confidence?', 'url': 'https://huggingface.co/papers/2601.07767', 'abstract': 'Large language models exhibit a disconnect between their expressed uncertainty and strategic decision-making under varying penalty conditions, failing to adjust abstention policies even when optimal.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) can produce surprisingly sophisticated estimates of their own uncertainty. However, it remains unclear to what extent this expressed confidence is tied to the reasoning, knowledge, or decision making of the model. To test this, we introduce RiskEval: a framework designed to evaluate whether models adjust their abstention policies in response to varying error penalties. Our evaluation of several frontier models reveals a critical dissociation: models are neither cost-aware when articulating their verbal confidence, nor strategically responsive when deciding whether to engage or abstain under high-penalty conditions. Even when extreme penalties render frequent abstention the mathematically optimal strategy, models almost never abstain, resulting in utility collapse. This indicates that calibrated verbal confidence scores may not be sufficient to create trustworthy and interpretable AI systems, as current models lack the strategic agency to convert uncertainty signals into optimal and risk-sensitive decisions.', 'score': 3, 'issue_id': 547, 'pub_date': '2026-01-12', 'pub_date_card': {'ru': '12 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 12', 'zh': '1æœˆ12æ—¥'}, 'hash': '3238a30b63fa6e17', 'authors': ['Jiawei Wang', 'Yanfei Zhou', 'Siddartha Devic', 'Deqing Fu'], 'affiliations': ['University of Southern California'], 'pdf_title_img': 'assets/pdf/title_img/2601.07767.jpg', 'data': {'categories': ['#interpretability', '#benchmark', '#reasoning'], 'emoji': 'âš–ï¸', 'ru': {'title': 'Ğ£Ğ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ±ĞµĞ· ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸: Ğ¿Ğ¾Ñ‡ĞµĞ¼Ñƒ LLM Ğ½Ğµ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ€Ğ¸ÑĞº Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸ÑÑ…', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ğ½Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ°Ğ´ĞµĞºĞ²Ğ°Ñ‚Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ²Ğ¾Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½ĞµÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑˆÑ‚Ñ€Ğ°Ñ„Ğ¾Ğ² Ğ·Ğ° Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº RiskEval Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ‚Ğ¾Ğ³Ğ¾, Ğ½Ğ°ÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ÑÑÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ Ğ¾Ñ‚ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ: Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğµ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ½Ğ¸ Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¸ Ğ²ĞµÑ€Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ½Ğ¸ Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ²Ğ¾Ğ·Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ, Ğ´Ğ°Ğ¶Ğµ ĞºĞ¾Ğ³Ğ´Ğ° Ñ‡Ğ°ÑÑ‚Ğ¾Ğµ Ğ¾Ñ‚ĞºĞ°Ğ· Ğ¾Ñ‚ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ĞµĞ½. Ğ­Ñ‚Ğ¾ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ñ‚ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ñ‹ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ñ… AI ÑĞ¸ÑÑ‚ĞµĞ¼, Ñ‚Ğ°Ğº ĞºĞ°Ğº Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ½Ğµ Ñ…Ğ²Ğ°Ñ‚Ğ°ĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ²Ğ°Ñ‚ÑŒ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ½ĞµÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğµ Ñ€Ğ¸ÑĞº.'}, 'en': {'title': 'Bridging the Gap: Uncertainty vs. Decision-Making in AI', 'desc': 'This paper investigates how well large language models (LLMs) understand and act on their own uncertainty when faced with different penalty scenarios. It introduces a framework called RiskEval to assess whether these models change their decision-making strategies based on the costs of errors. The findings show that LLMs often fail to adjust their abstention policies, even when it would be mathematically optimal to do so under high penalties. This suggests that while LLMs can express confidence, they do not effectively translate that uncertainty into strategic actions, raising concerns about their reliability in critical applications.'}, 'zh': {'title': 'ä¿¡å¿ƒä¸å†³ç­–çš„è„±èŠ‚ï¼šå¤§å‹è¯­è¨€æ¨¡å‹çš„æŒ‘æˆ˜', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è¡¨è¾¾ä¸ç¡®å®šæ€§å’Œåœ¨ä¸åŒæƒ©ç½šæ¡ä»¶ä¸‹çš„æˆ˜ç•¥å†³ç­–ä¹‹é—´å­˜åœ¨è„±èŠ‚ã€‚å°½ç®¡è¿™äº›æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆå¤æ‚çš„ä¸ç¡®å®šæ€§ä¼°è®¡ï¼Œä½†å®ƒä»¬åœ¨å†³ç­–æ—¶å¹¶æœªæ ¹æ®é”™è¯¯æƒ©ç½šçš„å˜åŒ–è°ƒæ•´å…¶æ”¾å¼ƒç­–ç•¥ã€‚æˆ‘ä»¬æå‡ºäº†RiskEvalæ¡†æ¶æ¥è¯„ä¼°æ¨¡å‹æ˜¯å¦ä¼šåœ¨é«˜æƒ©ç½šæ¡ä»¶ä¸‹åšå‡ºæˆ˜ç•¥å“åº”ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå½“å‰æ¨¡å‹åœ¨è¡¨è¾¾ä¿¡å¿ƒæ—¶ç¼ºä¹æˆæœ¬æ„è¯†ï¼Œä¸”åœ¨é¢å¯¹é«˜æƒ©ç½šæ—¶å‡ ä¹ä»ä¸é€‰æ‹©æ”¾å¼ƒï¼Œå¯¼è‡´æ•ˆç”¨å´©æºƒã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.06411', 'title': 'Structured Episodic Event Memory', 'url': 'https://huggingface.co/papers/2601.06411', 'abstract': 'Structured Episodic Event Memory (SEEM) enhances LLMs with hierarchical memory architecture combining graph and episodic layers for improved narrative coherence and reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t Current approaches to memory in Large Language Models (LLMs) predominantly rely on static Retrieval-Augmented Generation (RAG), which often results in scattered retrieval and fails to capture the structural dependencies required for complex reasoning. For autonomous agents, these passive and flat architectures lack the cognitive organization necessary to model the dynamic and associative nature of long-term interaction. To address this, we propose Structured Episodic Event Memory (SEEM), a hierarchical framework that synergizes a graph memory layer for relational facts with a dynamic episodic memory layer for narrative progression. Grounded in cognitive frame theory, SEEM transforms interaction streams into structured Episodic Event Frames (EEFs) anchored by precise provenance pointers. Furthermore, we introduce an agentic associative fusion and Reverse Provenance Expansion (RPE) mechanism to reconstruct coherent narrative contexts from fragmented evidence. Experimental results on the LoCoMo and LongMemEval benchmarks demonstrate that SEEM significantly outperforms baselines, enabling agents to maintain superior narrative coherence and logical consistency.', 'score': 3, 'issue_id': 545, 'pub_date': '2026-01-10', 'pub_date_card': {'ru': '10 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 10', 'zh': '1æœˆ10æ—¥'}, 'hash': 'bcf81e000cdc0bac', 'authors': ['Zhengxuan Lu', 'Dongfang Li', 'Yukun Shi', 'Beilun Wang', 'Longyue Wang', 'Baotian Hu'], 'affiliations': ['Alibaba Group, Hangzhou, China', 'Harbin Institute of Technology (Shenzhen), Shenzhen, China', 'Shenzhen Loop Area Institute, Shenzhen, China', 'Southeast University, Nanjing, China'], 'pdf_title_img': 'assets/pdf/title_img/2601.06411.jpg', 'data': {'categories': ['#reasoning', '#rag', '#long_context', '#agents', '#graphs', '#benchmark'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ˜ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ²ÑĞ·Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° SEEM (Structured Episodic Event Memory), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒÑ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ²Ñ‹Ğ¹ Ğ¸ ÑĞ¿Ğ¸Ğ·Ğ¾Ğ´Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ ÑĞ»Ğ¾Ğ¸ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ² ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ„Ñ€ĞµĞ¹Ğ¼Ñ‹ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹ Ñ ÑƒĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑĞ¼Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸ÑÑ…Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ»ÑƒÑ‡ÑˆĞµ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ²ÑĞ·Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ°ÑÑĞ¾Ñ†Ğ¸Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸ÑÑ…Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ (RPE) Ğ´Ğ»Ñ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞ²ÑĞ·Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ²ĞµÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸Ğ· Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ SEEM Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ²ÑĞ·Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ°Ñ€Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ° Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Enhancing Narrative Coherence with Structured Memory in LLMs', 'desc': 'Structured Episodic Event Memory (SEEM) is a new framework designed to improve the memory capabilities of Large Language Models (LLMs) by integrating a hierarchical architecture. It combines a graph memory layer, which organizes relational facts, with an episodic memory layer that tracks narrative flow. This approach allows for better reasoning and coherence in generated narratives, addressing the limitations of traditional static memory systems. Experimental results show that SEEM enhances the ability of agents to create logically consistent and coherent stories from fragmented information.'}, 'zh': {'title': 'ç»“æ„åŒ–æƒ…èŠ‚äº‹ä»¶è®°å¿†ï¼šæå‡æ™ºèƒ½ä½“å™äº‹èƒ½åŠ›çš„å…³é”®', 'desc': 'ç»“æ„åŒ–æƒ…èŠ‚äº‹ä»¶è®°å¿†ï¼ˆSEEMï¼‰é€šè¿‡ç»“åˆå›¾å½¢å’Œæƒ…èŠ‚å±‚çš„å±‚æ¬¡è®°å¿†æ¶æ„ï¼Œå¢å¼ºäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å™äº‹è¿è´¯æ€§å’Œæ¨ç†èƒ½åŠ›ã€‚å½“å‰çš„è®°å¿†æ–¹æ³•ä¸»è¦ä¾èµ–é™æ€çš„å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ï¼Œå¯¼è‡´ä¿¡æ¯æ£€ç´¢åˆ†æ•£ï¼Œæ— æ³•æ•æ‰å¤æ‚æ¨ç†æ‰€éœ€çš„ç»“æ„ä¾èµ–å…³ç³»ã€‚SEEMæ¡†æ¶ç»“åˆäº†å…³ç³»äº‹å®çš„å›¾å½¢è®°å¿†å±‚å’Œå™äº‹è¿›å±•çš„åŠ¨æ€æƒ…èŠ‚è®°å¿†å±‚ï¼Œæä¾›äº†æ›´å¥½çš„è®¤çŸ¥ç»„ç»‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSEEMåœ¨LoCoMoå’ŒLongMemEvalåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œå¸®åŠ©æ™ºèƒ½ä½“ä¿æŒæ›´é«˜çš„å™äº‹è¿è´¯æ€§å’Œé€»è¾‘ä¸€è‡´æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.03666', 'title': 'e5-omni: Explicit Cross-modal Alignment for Omni-modal Embeddings', 'url': 'https://huggingface.co/papers/2601.03666', 'abstract': 'Omni-modal embedding models face challenges with modality-dependent similarity scaling, ineffective in-batch negatives, and mismatched statistics across modalities, which are addressed through explicit alignment techniques including temperature calibration, controlled negative curriculum, and batch whitening with covariance regularization.  \t\t\t\t\tAI-generated summary \t\t\t\t Modern information systems often involve different types of items, e.g., a text query, an image, a video clip, or an audio segment. This motivates omni-modal embedding models that map heterogeneous modalities into a shared space for direct comparison. However, most recent omni-modal embeddings still rely heavily on implicit alignment inherited from pretrained vision-language model (VLM) backbones. In practice, this causes three common issues: (i) similarity logits have modality-dependent sharpness, so scores are not on a consistent scale; (ii) in-batch negatives become less effective over time because mixed-modality batches create an imbalanced hardness distribution; as a result, many negatives quickly become trivial and contribute little gradient; and (iii) embeddings across modalities show mismatched first- and second-order statistics, which makes rankings less stable. To tackle these problems, we propose e5-omni, a lightweight explicit alignment recipe that adapts off-the-shelf VLMs into robust omni-modal embedding models. e5-omni combines three simple components: (1) modality-aware temperature calibration to align similarity scales, (2) a controllable negative curriculum with debiasing to focus on confusing negatives while reducing the impact of false negatives, and (3) batch whitening with covariance regularization to better match cross-modal geometry in the shared embedding space. Experiments on MMEB-V2 and AudioCaps show consistent gains over strong bi-modal and omni-modal baselines, and the same recipe also transfers well to other VLM backbones. We release our model checkpoint at https://huggingface.co/Haon-Chen/e5-omni-7B.', 'score': 3, 'issue_id': 544, 'pub_date': '2026-01-07', 'pub_date_card': {'ru': '7 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 7', 'zh': '1æœˆ7æ—¥'}, 'hash': 'c6af8e5fb12ea40a', 'authors': ['Haonan Chen', 'Sicheng Gao', 'Radu Timofte', 'Tetsuya Sakai', 'Zhicheng Dou'], 'affiliations': ['Gaoling School of Artificial Intelligence, Renmin University of China', 'University of WÃ¼rzburg', 'Waseda University'], 'pdf_title_img': 'assets/pdf/title_img/2601.03666.jpg', 'data': {'categories': ['#training', '#open_source', '#multimodal', '#optimization', '#benchmark'], 'emoji': 'ğŸŒˆ', 'ru': {'title': 'Ğ¯Ğ²Ğ½Ğ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ e5-omni Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒÑÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ‚Ğ¸Ğ¿Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… (Ñ‚ĞµĞºÑÑ‚, Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ°ÑƒĞ´Ğ¸Ğ¾) Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ñ‚Ñ€Ğ¸ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹: Ğ½ĞµĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ°Ñ ÑˆĞºĞ°Ğ»Ğ° ÑÑ…Ğ¾Ğ¶ĞµÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸, Ğ½ĞµÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ñ€Ğ¸Ñ†Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ğ² Ğ±Ğ°Ñ‚Ñ‡Ğµ Ğ¸ Ğ½ĞµÑĞ¾Ğ²Ğ¿Ğ°Ğ´ĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ Ğ²Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°: ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºÑƒ Ñ‚ĞµĞ¼Ğ¿ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ñ‹ Ğ´Ğ»Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¾Ğ² ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ°, ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ curriculum Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚Ñ€Ğ¸Ñ†Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ğ¼Ğ¸ Ğ¸ Ğ±Ğ°Ñ‚Ñ‡-Ğ¾Ñ‚Ğ±ĞµĞ»Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ ĞºĞ¾Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… MMEB-V2 Ğ¸ AudioCaps Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Aligning Modalities for Better Embeddings', 'desc': 'This paper addresses the challenges faced by omni-modal embedding models, which integrate different types of data like text, images, and audio into a unified space. The authors identify three main issues: inconsistent similarity scaling across modalities, ineffective negative samples in mixed-modality batches, and mismatched statistical properties of embeddings. To resolve these, they introduce e5-omni, which employs temperature calibration for similarity alignment, a controlled negative curriculum to enhance learning from difficult examples, and batch whitening to harmonize the statistical properties of embeddings. Their experiments demonstrate that e5-omni outperforms existing models, showcasing its effectiveness in creating robust omni-modal embeddings.'}, 'zh': {'title': 'å…¨æ¨¡æ€åµŒå…¥çš„æ˜¾å¼å¯¹é½æŠ€æœ¯', 'desc': 'æœ¬æ–‡è®¨è®ºäº†å…¨æ¨¡æ€åµŒå…¥æ¨¡å‹é¢ä¸´çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æ¨¡æ€ä¾èµ–çš„ç›¸ä¼¼æ€§ç¼©æ”¾ã€æ— æ•ˆçš„æ‰¹å†…è´Ÿæ ·æœ¬å’Œæ¨¡æ€é—´ç»Ÿè®¡ä¸åŒ¹é…ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºe5-omniçš„æ˜¾å¼å¯¹é½æŠ€æœ¯ï¼Œç»“åˆäº†æ¸©åº¦æ ¡å‡†ã€å¯æ§è´Ÿæ ·æœ¬è¯¾ç¨‹å’Œæ‰¹æ¬¡ç™½åŒ–ç­‰æ–¹æ³•ã€‚é€šè¿‡è¿™äº›æŠ€æœ¯ï¼Œæ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°å¯¹é½ä¸åŒæ¨¡æ€çš„ç›¸ä¼¼æ€§å°ºåº¦ï¼Œæé«˜è´Ÿæ ·æœ¬çš„æœ‰æ•ˆæ€§ï¼Œå¹¶æ”¹å–„è·¨æ¨¡æ€å‡ ä½•åŒ¹é…ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œe5-omniåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜è¶Šï¼Œä¸”èƒ½å¤Ÿæœ‰æ•ˆè¿ç§»åˆ°å…¶ä»–è§†è§‰è¯­è¨€æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.07786', 'title': '"TODO: Fix the Mess Gemini Created": Towards Understanding GenAI-Induced Self-Admitted Technical Debt', 'url': 'https://huggingface.co/papers/2601.07786', 'abstract': 'Analysis of AI-referencing code comments reveals that developers explicitly acknowledge technical debt in AI-assisted code, identifying patterns of postponed testing, incomplete adaptation, and limited understanding as key factors in AI-induced technical debt emergence.  \t\t\t\t\tAI-generated summary \t\t\t\t As large language models (LLMs) such as ChatGPT, Copilot, Claude, and Gemini become integrated into software development workflows, developers increasingly leave traces of AI involvement in their code comments. Among these, some comments explicitly acknowledge both the use of generative AI and the presence of technical shortcomings. Analyzing 6,540 LLM-referencing code comments from public Python and JavaScript-based GitHub repositories (November 2022-July 2025), we identified 81 that also self-admit technical debt(SATD). Developers most often describe postponed testing, incomplete adaptation, and limited understanding of AI-generated code, suggesting that AI assistance affects both when and why technical debt emerges. We term GenAI-Induced Self-admitted Technical debt (GIST) as a proposed conceptual lens to describe recurring cases where developers incorporate AI-generated code while explicitly expressing uncertainty about its behavior or correctness.', 'score': 2, 'issue_id': 548, 'pub_date': '2026-01-12', 'pub_date_card': {'ru': '12 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 12', 'zh': '1æœˆ12æ—¥'}, 'hash': 'bbe97050931a6505', 'authors': ['Abdullah Al Mujahid', 'Mia Mohammad Imran'], 'affiliations': ['Missouri University of Science and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2601.07786.jpg', 'data': {'categories': [], 'emoji': 'âš™ï¸', 'ru': {'title': 'ĞšĞ¾Ğ³Ğ´Ğ° AI Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚, Ğ½Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ Ğ´Ğ¾Ğ»Ğ³Ğ¸: Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°Ğ½Ğ½Ğ°Ñ Ğ½ĞµĞ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚Ğ° Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ°', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ¾Ğ¼Ğ¼ĞµĞ½Ñ‚Ğ°Ñ€Ğ¸Ğ¸ Ğ² ĞºĞ¾Ğ´Ğµ, Ğ³Ğ´Ğµ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸ĞºĞ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ½Ğ°Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ´Ğ¾Ğ»Ğ³Ğ°. Ğ£Ñ‡Ñ‘Ğ½Ñ‹Ğµ Ğ¸Ğ·ÑƒÑ‡Ğ¸Ğ»Ğ¸ 6540 ĞºĞ¾Ğ¼Ğ¼ĞµĞ½Ñ‚Ğ°Ñ€Ğ¸ĞµĞ² Ğ¸Ğ· Ğ¿ÑƒĞ±Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸ĞµĞ² Python Ğ¸ JavaScript, Ğ½Ğ°Ğ¹Ğ´Ñ 81 ÑĞ»ÑƒÑ‡Ğ°Ğ¹ ÑĞ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ´Ğ¾Ğ»Ğ³Ğ°, Ğ²Ñ‹Ğ·Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ AI. ĞÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ Ğ¾Ñ‚Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğµ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ğ½ĞµĞ¿Ğ¾Ğ»Ğ½ÑƒÑ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ ĞºĞ¾Ğ´Ğ° Ğ¸ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¿Ğ¾Ğ½ÑÑ‚Ğ¸Ğµ GIST (GenAI-Induced Self-admitted Technical Debt) Ğ´Ğ»Ñ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ ÑĞ¸Ñ‚ÑƒĞ°Ñ†Ğ¸Ğ¹, ĞºĞ¾Ğ³Ğ´Ğ° Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸ĞºĞ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ AI-ĞºĞ¾Ğ´, Ğ½Ğ¾ ÑĞ²Ğ½Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ¶Ğ°ÑÑ‚ ÑĞ¾Ğ¼Ğ½ĞµĞ½Ğ¸Ñ Ğ² ĞµĞ³Ğ¾ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Understanding Technical Debt in AI-Assisted Coding', 'desc': 'This paper analyzes how developers acknowledge technical debt when using AI tools in coding. It focuses on comments from code repositories that mention both AI assistance and technical shortcomings. The study identifies common issues like delayed testing, incomplete adaptation of AI-generated code, and a lack of understanding of how the AI works. The authors introduce the concept of GenAI-Induced Self-admitted Technical debt (GIST) to describe these situations where developers express uncertainty about AI-generated code.'}, 'zh': {'title': 'AIè¾…åŠ©ä¸‹çš„æŠ€æœ¯å€ºåŠ¡æ–°è§†è§’', 'desc': 'è¿™ç¯‡è®ºæ–‡åˆ†æäº†å¼€å‘è€…åœ¨ä½¿ç”¨AIè¾…åŠ©ä»£ç æ—¶æ‰€ç•™ä¸‹çš„ä»£ç æ³¨é‡Šï¼Œæ­ç¤ºäº†æŠ€æœ¯å€ºåŠ¡çš„æ˜¾æ€§æ‰¿è®¤ã€‚ç ”ç©¶å‘ç°ï¼Œå¼€å‘è€…åœ¨ä»£ç ä¸­æåˆ°çš„æŠ€æœ¯å€ºåŠ¡ä¸»è¦åŒ…æ‹¬æ¨è¿Ÿæµ‹è¯•ã€ä¸å®Œæ•´çš„é€‚åº”å’Œå¯¹AIç”Ÿæˆä»£ç çš„æœ‰é™ç†è§£ã€‚é€šè¿‡åˆ†æ6540æ¡ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç›¸å…³çš„ä»£ç æ³¨é‡Šï¼Œè®ºæ–‡æå‡ºäº†ä¸€ä¸ªæ–°çš„æ¦‚å¿µï¼šç”ŸæˆAIå¼•å‘çš„è‡ªæˆ‘æ‰¿è®¤æŠ€æœ¯å€ºåŠ¡ï¼ˆGISTï¼‰ã€‚è¿™è¡¨æ˜AIè¾…åŠ©åœ¨è½¯ä»¶å¼€å‘ä¸­ä¸ä»…å½±å“äº†æŠ€æœ¯å€ºåŠ¡çš„å‡ºç°æ—¶æœºï¼Œä¹Ÿå½±å“äº†å…¶åŸå› ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.07181', 'title': 'ShowUI-Aloha: Human-Taught GUI Agent', 'url': 'https://huggingface.co/papers/2601.07181', 'abstract': 'ShowUI-Aloha presents a pipeline that converts unstructured human screen recordings into structured GUI tasks through recording, semantic interpretation, planning, and execution components.  \t\t\t\t\tAI-generated summary \t\t\t\t Graphical User Interfaces (GUIs) are central to human-computer interaction, yet automating complex GUI tasks remains a major challenge for autonomous agents, largely due to a lack of scalable, high-quality training data. While recordings of human demonstrations offer a rich data source, they are typically long, unstructured, and lack annotations, making them difficult for agents to learn from.To address this, we introduce ShowUI-Aloha, a comprehensive pipeline that transforms unstructured, in-the-wild human screen recordings from desktop environments into structured, actionable tasks. Our framework includes four key components: A recorder that captures screen video along with precise user interactions like mouse clicks, keystrokes, and scrolls. A learner that semantically interprets these raw interactions and the surrounding visual context, translating them into descriptive natural language captions. A planner that reads the parsed demonstrations, maintains task states, and dynamically formulates the next high-level action plan based on contextual reasoning. An executor that faithfully carries out these action plans at the OS level, performing precise clicks, drags, text inputs, and window operations with safety checks and real-time feedback. Together, these components provide a scalable solution for collecting and parsing real-world human data, demonstrating a viable path toward building general-purpose GUI agents that can learn effectively from simply observing humans.', 'score': 2, 'issue_id': 545, 'pub_date': '2026-01-12', 'pub_date_card': {'ru': '12 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 12', 'zh': '1æœˆ12æ—¥'}, 'hash': '96edfcef7f385531', 'authors': ['Yichun Zhang', 'Xiangwu Guo', 'Yauhong Goh', 'Jessica Hu', 'Zhiheng Chen', 'Xin Wang', 'Difei Gao', 'Mike Zheng Shou'], 'affiliations': ['Show Lab, National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2601.07181.jpg', 'data': {'categories': [], 'emoji': 'ğŸ–¥ï¸', 'ru': {'title': 'ĞÑ‚ Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹ Ğº Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑĞ¼: Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ GUI-Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ‡ĞµÑ€ĞµĞ· ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ñ', 'desc': 'ShowUI-Aloha Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ pipeline Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½ĞµÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ñ ÑĞºÑ€Ğ°Ğ½Ğ¾Ğ¼ Ğ² ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ¼. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°: Ñ€ĞµĞºĞ¾Ñ€Ğ´ĞµÑ€ Ğ´Ğ»Ñ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ, ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ‚Ğ¾Ñ€ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ² Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ, Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº Ğ´Ğ»Ñ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚ĞºĞ° Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¸Ğ¼ ÑƒÑ‡Ğ¸Ñ‚ÑŒÑÑ Ğ½ĞµĞ¿Ğ¾ÑÑ€ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ¸Ğ· Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğ¹ Ğ·Ğ° Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸ĞµĞ¼ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°. Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ñ‹ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ñ… GUI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ¾Ğ¼.'}, 'en': {'title': 'Transforming Screen Recordings into Actionable GUI Tasks', 'desc': 'ShowUI-Aloha is a machine learning pipeline designed to convert unstructured screen recordings of human interactions with graphical user interfaces (GUIs) into structured tasks that autonomous agents can understand. It consists of four main components: a recorder that captures user interactions, a learner that interprets these interactions into natural language, a planner that formulates action plans based on the parsed data, and an executor that performs the actions on the operating system. This approach addresses the challenge of automating complex GUI tasks by utilizing rich data from human demonstrations, which are typically unstructured and lack annotations. By transforming these recordings into actionable tasks, ShowUI-Aloha paves the way for developing general-purpose GUI agents that can learn from observing human behavior.'}, 'zh': {'title': 'å°†å±å¹•å½•åˆ¶è½¬åŒ–ä¸ºæ™ºèƒ½GUIä»»åŠ¡çš„è§£å†³æ–¹æ¡ˆ', 'desc': 'ShowUI-Alohaæ˜¯ä¸€ä¸ªå°†éç»“æ„åŒ–çš„äººç±»å±å¹•å½•åˆ¶è½¬æ¢ä¸ºç»“æ„åŒ–GUIä»»åŠ¡çš„ç®¡é“ã€‚å®ƒé€šè¿‡å½•åˆ¶ã€è¯­ä¹‰è§£é‡Šã€è§„åˆ’å’Œæ‰§è¡Œå››ä¸ªç»„ä»¶æ¥å®ç°è¿™ä¸€ç›®æ ‡ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿæ•æ‰ç”¨æˆ·çš„äº¤äº’è¡Œä¸ºï¼Œå¹¶å°†å…¶è½¬åŒ–ä¸ºè‡ªç„¶è¯­è¨€æè¿°ï¼Œè¿›è€Œç”Ÿæˆé«˜å±‚æ¬¡çš„è¡ŒåŠ¨è®¡åˆ’ã€‚æœ€ç»ˆï¼Œæ‰§è¡Œå™¨åœ¨æ“ä½œç³»ç»Ÿå±‚é¢æ‰§è¡Œè¿™äº›è®¡åˆ’ï¼Œä»è€Œä¸ºæ„å»ºé€šç”¨çš„GUIä»£ç†æä¾›äº†ä¸€ç§å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.07033', 'title': 'Codified Foreshadowing-Payoff Text Generation', 'url': 'https://huggingface.co/papers/2601.07033', 'abstract': 'Large language models struggle with maintaining long-range narrative dependencies, but a new framework called CFPG addresses this by structuring narrative continuity through executable causal predicates to ensure proper fulfillment of foreshadowed events.  \t\t\t\t\tAI-generated summary \t\t\t\t Foreshadowing and payoff are ubiquitous narrative devices through which authors introduce commitments early in a story and resolve them through concrete, observable outcomes. However, despite advances in story generation, large language models (LLMs) frequently fail to bridge these long-range narrative dependencies, often leaving "Chekhov\'s guns" unfired even when the necessary context is present. Existing evaluations largely overlook this structural failure, focusing on surface-level coherence rather than the logical fulfillment of narrative setups. In this paper, we introduce Codified Foreshadowing-Payoff Generation (CFPG), a novel framework that reframes narrative quality through the lens of payoff realization. Recognizing that LLMs struggle to intuitively grasp the "triggering mechanism" of a foreshadowed event, CFPG transforms narrative continuity into a set of executable causal predicates. By mining and encoding Foreshadow-Trigger-Payoff triples from the BookSum corpus, we provide structured supervision that ensures foreshadowed commitments are not only mentioned but also temporally and logically fulfilled. Experiments demonstrate that CFPG significantly outperforms standard prompting baselines in payoff accuracy and narrative alignment. Our findings suggest that explicitly codifying narrative mechanics is essential for moving LLMs from surface-level fluency to genuine narrative competence.', 'score': 2, 'issue_id': 547, 'pub_date': '2026-01-11', 'pub_date_card': {'ru': '11 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 11', 'zh': '1æœˆ11æ—¥'}, 'hash': 'bcace5ff267ebd80', 'authors': ['Longfei Yun', 'Kun Zhou', 'Yupeng Hou', 'Letian Peng', 'Jingbo Shang'], 'affiliations': ['University of California, San Diego'], 'pdf_title_img': 'assets/pdf/title_img/2601.07033.jpg', 'data': {'categories': ['#long_context', '#story_generation', '#reasoning'], 'emoji': 'ğŸ”«', 'ru': {'title': 'ĞÑ‚ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ½Ğ¾Ğ¹ Ğ±ĞµĞ³Ğ»Ğ¾ÑÑ‚Ğ¸ Ğº Ğ¿Ğ¾Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ¹ Ğ½Ğ°Ñ€Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¿ĞµÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ´Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ²ÑĞ·ĞµĞ¹', 'desc': "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº CFPG Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞ°Ñ‚ÑŒ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ½Ğ°Ñ€Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ LLM Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ½Ğµ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑÑ‚ Ğ¾Ğ±ĞµÑ‰Ğ°Ğ½Ğ¸Ñ, Ğ·Ğ°Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ² Ğ½Ğ°Ñ‡Ğ°Ğ»Ğµ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸ (Ğ»Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¸Ñ‘Ğ¼ Ğ¿Ñ€ĞµĞ´Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ), Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ñ Ğ½ĞµĞ·Ğ°Ğ²ĞµÑ€ÑˆÑ‘Ğ½Ğ½Ñ‹Ğ¼Ğ¸. Ğ ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¾ Ğ½Ğ° Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ½Ğ°Ñ€Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´Ğ¸ĞºĞ°Ñ‚Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ĞºĞ¾Ğ´Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒÑÑ‚ Ñ‚Ñ€Ğ¸Ğ¿Ğ»ĞµÑ‚Ñ‹ 'Ğ¿Ñ€ĞµĞ´Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ğµ-Ñ‚Ñ€Ğ¸Ğ³Ğ³ĞµÑ€-Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ' Ğ¸Ğ· ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ° Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ²Ğ½Ğ¾Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ°Ñ€Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğº Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ĞµÑ‰Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¹."}, 'en': {'title': 'Bridging Narrative Gaps with CFPG', 'desc': 'This paper introduces a new framework called Codified Foreshadowing-Payoff Generation (CFPG) to improve the narrative capabilities of large language models (LLMs). It addresses the issue of LLMs failing to maintain long-range narrative dependencies, particularly in fulfilling foreshadowed events. By using executable causal predicates, CFPG structures narrative continuity and ensures that commitments made early in a story are logically resolved later. The results show that CFPG enhances narrative quality by significantly improving payoff accuracy and alignment compared to traditional prompting methods.'}, 'zh': {'title': 'é€šè¿‡å› æœè°“è¯å®ç°å™äº‹çš„è¿è´¯æ€§', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¿æŒé•¿ç¯‡å™äº‹ä¾èµ–å…³ç³»æ–¹é¢å­˜åœ¨å›°éš¾ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œç§°ä¸ºCFPGï¼Œé€šè¿‡å¯æ‰§è¡Œçš„å› æœè°“è¯æ¥æ„å»ºå™äº‹è¿ç»­æ€§ï¼Œä»¥ç¡®ä¿é¢„ç¤ºäº‹ä»¶çš„æ­£ç¡®å®ç°ã€‚CFPGé€šè¿‡ä»BookSumè¯­æ–™åº“ä¸­æŒ–æ˜å’Œç¼–ç é¢„ç¤º-è§¦å‘-å›æŠ¥ä¸‰å…ƒç»„ï¼Œæä¾›äº†ç»“æ„åŒ–çš„ç›‘ç£ï¼Œç¡®ä¿é¢„ç¤ºçš„æ‰¿è¯ºä¸ä»…è¢«æåŠï¼Œè€Œä¸”åœ¨æ—¶é—´å’Œé€»è¾‘ä¸Šå¾—åˆ°æ»¡è¶³ã€‚å®éªŒè¡¨æ˜ï¼ŒCFPGåœ¨å›æŠ¥å‡†ç¡®æ€§å’Œå™äº‹ä¸€è‡´æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºæ ‡å‡†æç¤ºåŸºçº¿ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.04577', 'title': 'Sci-Reasoning: A Dataset Decoding AI Innovation Patterns', 'url': 'https://huggingface.co/papers/2601.04577', 'abstract': 'Sci-Reasoning dataset captures intellectual synthesis patterns in AI research through structured reasoning links from key papers to their predecessors, identifying 15 thinking patterns that drive breakthrough innovations.  \t\t\t\t\tAI-generated summary \t\t\t\t While AI innovation accelerates rapidly, the intellectual process behind breakthroughs -- how researchers identify gaps, synthesize prior work, and generate insights -- remains poorly understood. The lack of structured data on scientific reasoning hinders systematic analysis and development of AI research agents. We introduce Sci-Reasoning, the first dataset capturing the intellectual synthesis behind high-quality AI research. Using community-validated quality signals and an LLM-accelerated, human-verified pipeline, we trace Oral and Spotlight papers across NeurIPS, ICML, and ICLR (2023-2025) to its key predecessors, articulating specific reasoning links in a structured format. Our analysis identifies 15 distinct thinking patterns, with three dominant strategies accounting for 52.7%: Gap-Driven Reframing (24.2%), Cross-Domain Synthesis (18.0%), and Representation Shift (10.5%). The most powerful innovation recipes combine multiple patterns: Gap-Driven Reframing + Representation Shift, Cross-Domain Synthesis + Representation Shift, and Gap-Driven Reframing + Cross-Domain Synthesis. This dataset enables quantitative studies of scientific progress and provides structured reasoning trajectories for training the next generation AI research agents.', 'score': 2, 'issue_id': 559, 'pub_date': '2026-01-08', 'pub_date_card': {'ru': '8 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 8', 'zh': '1æœˆ8æ—¥'}, 'hash': '52727457142db3c7', 'authors': ['Jiachen Liu', 'Maestro Harmon', 'Zechen Zhang'], 'affiliations': ['Maestro Harmon Orchestra Research'], 'pdf_title_img': 'assets/pdf/title_img/2601.04577.jpg', 'data': {'categories': ['#reasoning', '#science', '#synthetic'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞŸĞ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ: ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ² AI-ç ”ç©¶', 'desc': 'ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Sci-Reasoning, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹, Ğ»ĞµĞ¶Ğ°Ñ‰Ğ¸Ğµ Ğ² Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ¾Ñ€Ñ‹Ğ²Ğ½Ñ‹Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ ÑĞ²ÑĞ·Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ¼Ğ¸ Ñ ĞºĞ¾Ğ½Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ¹ NeurIPS, ICML Ğ¸ ICLR Ğ¸ Ğ¸Ñ… Ğ¿Ñ€ĞµĞ´ÑˆĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¸ĞºĞ°Ğ¼Ğ¸, Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ² ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ LLM Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞµĞ½Ğ½Ñ‹Ğµ Ğ»ÑĞ´ÑŒĞ¼Ğ¸. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ñ‹ÑĞ²Ğ¸Ğ» 15 Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼Ñ‹Ñ… Ğ¿Ñ€Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹, Ğ³Ğ´Ğµ Ñ‚Ñ€Ğ¸ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ (Gap-Driven Reframing, Cross-Domain Synthesis Ğ¸ Representation Shift) ÑĞ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ğ¾Ğ»Ğ¾Ğ²Ğ¸Ğ½Ñ‹ Ğ²ÑĞµÑ… ÑĞ»ÑƒÑ‡Ğ°ĞµĞ². Ğ­Ñ‚Ğ¾Ñ‚ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ° Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ˜Ğ˜ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğµ Ğ¸Ğ´ĞµĞ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Unlocking Innovation: Understanding AI Research Through Structured Reasoning', 'desc': 'The Sci-Reasoning dataset is designed to capture how researchers in AI synthesize knowledge from previous work to create new innovations. It identifies 15 distinct thinking patterns that researchers use, with three main strategies being the most common. By tracing connections between recent AI papers and their predecessors, the dataset provides a structured way to analyze the reasoning behind scientific breakthroughs. This resource aims to enhance the understanding of intellectual processes in AI research and support the development of future AI research agents.'}, 'zh': {'title': 'æ­ç¤ºAIç ”ç©¶çš„æ™ºåŠ›ç»¼åˆæ¨¡å¼', 'desc': 'Sci-Reasoningæ•°æ®é›†æ•æ‰äº†AIç ”ç©¶ä¸­çš„æ™ºåŠ›ç»¼åˆæ¨¡å¼ï¼Œé€šè¿‡å…³é”®è®ºæ–‡ä¸å…¶å‰èº«ä¹‹é—´çš„ç»“æ„åŒ–æ¨ç†é“¾æ¥ï¼Œè¯†åˆ«å‡º15ç§æ¨åŠ¨çªç ´æ€§åˆ›æ–°çš„æ€ç»´æ¨¡å¼ã€‚å°½ç®¡AIåˆ›æ–°è¿…é€Ÿå‘å±•ï¼Œä½†ç ”ç©¶äººå‘˜å¦‚ä½•è¯†åˆ«ç ”ç©¶ç©ºç™½ã€ç»¼åˆå…ˆå‰å·¥ä½œå¹¶ç”Ÿæˆè§è§£çš„æ™ºåŠ›è¿‡ç¨‹ä»ç„¶ä¸å¤Ÿæ¸…æ™°ã€‚è¯¥æ•°æ®é›†æ˜¯é¦–ä¸ªæ•æ‰é«˜è´¨é‡AIç ”ç©¶èƒŒåæ™ºåŠ›ç»¼åˆçš„å·¥å…·ï¼Œåˆ©ç”¨ç¤¾åŒºéªŒè¯çš„è´¨é‡ä¿¡å·å’Œäººç±»éªŒè¯çš„æµç¨‹ï¼Œè¿½è¸ª2023-2025å¹´NeurIPSã€ICMLå’ŒICLRçš„è®ºæ–‡åŠå…¶å…³é”®å‰èº«ã€‚æˆ‘ä»¬çš„åˆ†ææ˜¾ç¤ºï¼Œ52.7%çš„åˆ›æ–°ç­–ç•¥æ¥è‡ªä¸‰ç§ä¸»è¦æ€ç»´æ¨¡å¼çš„ç»„åˆï¼šä»¥ç©ºç™½é©±åŠ¨çš„é‡æ„ã€è·¨é¢†åŸŸç»¼åˆå’Œè¡¨ç¤ºè½¬å˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.06993', 'title': 'Can Textual Reasoning Improve the Performance of MLLMs on Fine-grained Visual Classification?', 'url': 'https://huggingface.co/papers/2601.06993', 'abstract': "Multi-modal large language models struggle with fine-grained visual classification, and chain-of-thought reasoning harms performance due to increased reasoning length; a new framework called ReFine-RFT is proposed to address this issue through normalized multi-reward optimization.  \t\t\t\t\tAI-generated summary \t\t\t\t Multi-modal large language models (MLLMs) exhibit strong general-purpose capabilities, yet still struggle on Fine-Grained Visual Classification (FGVC), a core perception task that requires subtle visual discrimination and is crucial for many real-world applications. A widely adopted strategy for boosting performance on challenging tasks such as math and coding is Chain-of-Thought (CoT) reasoning. However, several prior works have reported that CoT can actually harm performance on visual perception tasks. These studies, though, examine the issue from relatively narrow angles and leave open why CoT degrades perception-heavy performance. We systematically re-examine the role of CoT in FGVC through the lenses of zero-shot evaluation and multiple training paradigms. Across these settings, we uncover a central paradox: the degradation induced by CoT is largely driven by the reasoning length, in which longer textual reasoning consistently lowers classification accuracy. We term this phenomenon the ``Cost of Thinking''. Building on this finding, we make two key contributions: (1) \\alg, a simple and general plug-and-play normalization method for multi-reward optimization that balances heterogeneous reward signals, and (2) ReFine-RFT, a framework that combines ensemble rewards with \\alg to constrain reasoning length while providing dense accuracy-oriented feedback. Extensive experiments demonstrate the effectiveness of our findings and the proposed ReFine-RFT, achieving state-of-the-art performance across FGVC benchmarks. Code and models are available at https://github.com/jiezhu23/ReFine-RFT{Project Link}.", 'score': 1, 'issue_id': 560, 'pub_date': '2026-01-11', 'pub_date_card': {'ru': '11 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 11', 'zh': '1æœˆ11æ—¥'}, 'hash': 'b26bc05486847e54', 'authors': ['Jie Zhu', 'Yiyang Su', 'Xiaoming Liu'], 'affiliations': ['Department of Computer Science and Engineering, Michigan State University'], 'pdf_title_img': 'assets/pdf/title_img/2601.06993.jpg', 'data': {'categories': ['#reasoning', '#open_source', '#rlhf', '#optimization', '#cv', '#benchmark', '#multimodal'], 'emoji': 'ğŸ”', 'ru': {'title': 'ĞœĞµĞ½ÑŒÑˆĞµ Ğ´ÑƒĞ¼Ğ°Ñ‚ÑŒ, Ğ»ÑƒÑ‡ÑˆĞµ Ğ²Ğ¸Ğ´ĞµÑ‚ÑŒ: Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸', 'desc': 'ĞœĞ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ»Ğ¾Ñ…Ğ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸ĞµĞ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¿Ğ¾Ñ‚Ğ¾Ğ¼Ñƒ Ñ‡Ñ‚Ğ¾ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (Chain-of-Thought) ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ´Ğ»Ğ¸Ğ½Ñƒ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¾ĞºÑ: Ğ±Ğ¾Ğ»ĞµĞµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´ÑÑ‚ Ğº Ñ…ÑƒĞ´ÑˆĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¸Ğ¼Ğ¸ Â«ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒÑ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸ÑÂ». ĞĞ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ ReFine-RFT â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ñ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ñ‹Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¼ĞµĞ»ĞºĞ¾Ğ·ĞµÑ€Ğ½Ğ¸ÑÑ‚Ğ¾Ğ¹ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸, Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°Ñ Ğ´Ğ»Ğ¸Ğ½Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'ReFine-RFT: Optimizing Reasoning for Better Visual Classification', 'desc': "This paper addresses the challenges faced by multi-modal large language models (MLLMs) in fine-grained visual classification (FGVC), which requires precise visual recognition. It identifies that the common practice of Chain-of-Thought (CoT) reasoning can negatively impact performance due to increased reasoning length, a phenomenon termed the 'Cost of Thinking'. The authors propose a new framework called ReFine-RFT, which utilizes a normalization method for multi-reward optimization to balance different reward signals and limit reasoning length. Their experiments show that ReFine-RFT significantly improves classification accuracy, achieving state-of-the-art results on FGVC benchmarks."}, 'zh': {'title': 'ä¼˜åŒ–æ€ç»´æˆæœ¬ï¼Œæå‡è§†è§‰åˆ†ç±»æ€§èƒ½', 'desc': 'å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç»†ç²’åº¦è§†è§‰åˆ†ç±»ï¼ˆFGVCï¼‰ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦ç»†è‡´è§†è§‰è¾¨åˆ«çš„æƒ…å†µä¸‹ã€‚é“¾å¼æ€ç»´æ¨ç†ï¼ˆCoTï¼‰è™½ç„¶åœ¨æ•°å­¦å’Œç¼–ç¨‹ç­‰ä»»åŠ¡ä¸­å¸¸è¢«ç”¨æ¥æå‡æ€§èƒ½ï¼Œä½†åœ¨è§†è§‰æ„ŸçŸ¥ä»»åŠ¡ä¸­å´å¯èƒ½å¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚ç ”ç©¶å‘ç°ï¼ŒCoTçš„æ€§èƒ½ä¸‹é™ä¸»è¦ä¸æ¨ç†é•¿åº¦æœ‰å…³ï¼Œæ¨ç†è¶Šé•¿ï¼Œåˆ†ç±»å‡†ç¡®ç‡è¶Šä½ï¼Œè¿™ä¸€ç°è±¡è¢«ç§°ä¸ºâ€œæ€è€ƒæˆæœ¬â€ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ReFine-RFTæ¡†æ¶ï¼Œé€šè¿‡å½’ä¸€åŒ–å¤šé‡å¥–åŠ±ä¼˜åŒ–æ¥å¹³è¡¡ä¸åŒçš„å¥–åŠ±ä¿¡å·ï¼Œä»è€Œæé«˜FGVCçš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.06944', 'title': 'SketchJudge: A Diagnostic Benchmark for Grading Hand-drawn Diagrams with Multimodal Large Language Models', 'url': 'https://huggingface.co/papers/2601.06944', 'abstract': "SketchJudge benchmark evaluates multimodal large language models' ability to grade hand-drawn STEM diagrams, revealing significant limitations in visual understanding compared to human performance.  \t\t\t\t\tAI-generated summary \t\t\t\t While Multimodal Large Language Models (MLLMs) have achieved remarkable progress in visual understanding, they often struggle when faced with the unstructured and ambiguous nature of human-generated sketches. This limitation is particularly pronounced in the underexplored task of visual grading, where models should not only solve a problem but also diagnose errors in hand-drawn diagrams. Such diagnostic capabilities depend on complex structural, semantic, and metacognitive reasoning. To bridge this gap, we introduce SketchJudge, a novel benchmark tailored for evaluating MLLMs as graders of hand-drawn STEM diagrams. SketchJudge encompasses 1,015 hand-drawn student responses across four domains: geometry, physics, charts, and flowcharts, featuring diverse stylistic variations and distinct error types. Evaluations on SketchJudge demonstrate that even advanced MLLMs lag significantly behind humans, validating the benchmark's effectiveness in exposing the fragility of current vision-language alignment in symbolic and noisy contexts. All data, code, and evaluation scripts are publicly available at https://github.com/yuhangsu82/SketchJudge.", 'score': 1, 'issue_id': 545, 'pub_date': '2026-01-11', 'pub_date_card': {'ru': '11 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 11', 'zh': '1æœˆ11æ—¥'}, 'hash': 'd245e56fb4a9de5e', 'authors': ['Yuhang Su', 'Mei Wang', 'Yaoyao Zhong', 'Guozhang Li', 'Shixing Li', 'Yihan Feng', 'Hua Huang'], 'affiliations': ['School of Artificial Intelligence, Beijing Normal University, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2601.06944.jpg', 'data': {'categories': ['#benchmark', '#multimodal', '#cv', '#dataset'], 'emoji': 'âœï¸', 'ru': {'title': 'ĞšĞ¾Ğ³Ğ´Ğ° Ğ´Ğ°Ğ¶Ğµ ÑƒĞ¼Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ ÑˆĞºĞ¾Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ€Ğ¸ÑÑƒĞ½Ğ¾Ğº', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº SketchJudge Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑÑ‚ÑŒ Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ñ€ÑƒĞºĞ¾Ğ¿Ğ¸ÑĞ½Ñ‹Ğµ STEM Ğ´Ğ¸Ğ°Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñ‹, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ ÑÑ‚ÑƒĞ´ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 1015 Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ñ€ÑƒĞºĞ¾Ğ¿Ğ¸ÑĞ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ² Ñ‡ĞµÑ‚Ñ‹Ñ€Ñ‘Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…: Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ, Ñ„Ğ¸Ğ·Ğ¸ĞºĞ°, Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¸ Ğ¸ Ğ±Ğ»Ğ¾Ğº-ÑÑ…ĞµĞ¼Ñ‹ Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ ÑÑ‚Ğ¸Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ñ‚Ğ¸Ğ¿Ğ°Ğ¼Ğ¸ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ MLLM Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ñ‚ÑÑ‚Ğ°ÑÑ‚ Ğ¾Ñ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰ĞµĞ¹ Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ½Ğ¾ Ğ¸ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ² Ğ´Ğ¸Ğ°Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ°Ñ… Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ğ¾Ğ³Ğ¾, ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¸ Ğ¼ĞµÑ‚Ğ°ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ Ñ…Ñ€ÑƒĞ¿ĞºĞ¾ÑÑ‚ÑŒ Ñ‚ĞµĞºÑƒÑ‰ĞµĞ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ·Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¸ Ğ·Ğ°ÑˆÑƒĞ¼Ğ»ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸.'}, 'en': {'title': 'SketchJudge: Bridging the Grading Gap for AI in STEM Diagrams', 'desc': 'The paper introduces SketchJudge, a benchmark designed to assess the performance of multimodal large language models (MLLMs) in grading hand-drawn STEM diagrams. It highlights the challenges these models face in understanding unstructured sketches, particularly in diagnosing errors, which requires advanced reasoning skills. The benchmark includes a diverse set of 1,015 student responses across various STEM domains, showcasing different styles and error types. Results indicate that MLLMs significantly underperform compared to human graders, emphasizing the need for improved vision-language alignment in complex visual tasks.'}, 'zh': {'title': 'è¯„ä¼°å¤šæ¨¡æ€æ¨¡å‹çš„æ‰‹ç»˜å›¾è¡¨è¯„åˆ†èƒ½åŠ›', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†SketchJudgeåŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è¯„åˆ†æ‰‹ç»˜STEMå›¾è¡¨æ–¹é¢çš„èƒ½åŠ›ã€‚ç ”ç©¶å‘ç°ï¼Œè¿™äº›æ¨¡å‹åœ¨ç†è§£äººç±»æ‰‹ç»˜è‰å›¾æ—¶å­˜åœ¨æ˜¾è‘—å±€é™ï¼Œå°¤å…¶æ˜¯åœ¨è§†è§‰è¯„åˆ†ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ã€‚SketchJudgeåŒ…å«1015ä¸ªæ‰‹ç»˜å­¦ç”Ÿå›ç­”ï¼Œæ¶µç›–å‡ ä½•ã€ç‰©ç†ã€å›¾è¡¨å’Œæµç¨‹å›¾ç­‰å››ä¸ªé¢†åŸŸï¼Œå±•ç¤ºäº†å¤šæ ·çš„é£æ ¼å˜åŒ–å’Œé”™è¯¯ç±»å‹ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œå°½ç®¡MLLMså–å¾—äº†è¿›å±•ï¼Œä½†åœ¨è§†è§‰-è¯­è¨€å¯¹é½çš„å¤æ‚å’Œå˜ˆæ‚ç¯å¢ƒä¸­ä»æ˜¾è‘—è½åäºäººç±»ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.06747', 'title': 'FinForge: Semi-Synthetic Financial Benchmark Generation', 'url': 'https://huggingface.co/papers/2601.06747', 'abstract': "FinForge presents a scalable semi-synthetic pipeline for creating domain-specific financial evaluation benchmarks using expert curation and language model synthesis, demonstrating significant variations in financial reasoning capabilities among state-of-the-art models.  \t\t\t\t\tAI-generated summary \t\t\t\t Evaluating Language Models (LMs) in specialized, high-stakes domains such as finance remains a significant challenge due to the scarcity of open, high-quality, and domain-specific datasets. Existing general-purpose benchmarks provide broad coverage but lack the depth and domain fidelity needed to assess LMs' capabilities for real-world financial reasoning, which requires both conceptual understanding and quantitative rigor. To address this gap, we introduce FinForge, a scalable, semi-synthetic pipeline for constructing finance-specific evaluation benchmarks through a hybrid of expert-guided data curation and controlled LM-based synthesis. FinForge combines manual and programmatic corpus construction from authoritative financial sources with structured question generation and validation using Gemini 2.5 Flash. To demonstrate the pipeline's efficacy, we produce FinForge-5k, a snapshot benchmark comprising over 5,000 human-validated question-answer pairs across 11 finance subdomains, derived from a curated corpus of 100,000 verified documents totaling 143M tokens. Evaluation of state-of-the-art open-source and closed-source models on FinForge-5k reveals significant differences in financial reasoning, with leading models achieving accuracy levels near 80%. These findings underscore the framework's utility for diagnosing current model limitations and guiding future improvements in financial domain competence. All code and data are available at https://github.com/gtfintechlab/FinForge.", 'score': 1, 'issue_id': 550, 'pub_date': '2026-01-11', 'pub_date_card': {'ru': '11 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 11', 'zh': '1æœˆ11æ—¥'}, 'hash': 'ac083251989186b4', 'authors': ['Glenn Matlin', 'Akhil Theerthala', 'Anant Gupta', 'Anirudh JM', 'Rayan Castilla', 'Yi Mei Ng', 'Sudheer Chava'], 'affiliations': ['College of Business, Georgia Institute of Technology', 'College of Computing, Georgia Institute of Technology', 'Financial Services Innovation Lab, Georgia Institute of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2601.06747.jpg', 'data': {'categories': ['#synthetic', '#open_source', '#science'], 'emoji': 'ğŸ’°', 'ru': {'title': 'Ğ¤Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ğµ ÑƒĞ¼ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğµ Ğ¸ ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ', 'desc': 'FinForge Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ² Ğ¿Ğ¾ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼, ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒÑ Ñ€ÑƒÑ‡Ğ½ÑƒÑ ĞºÑƒÑ€Ğ°Ñ‚Ğ¾Ñ€ÑĞºÑƒÑ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¾Ğ¼ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾Ğ¸Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ FinForge-5k, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ ÑĞ²Ñ‹ÑˆĞµ 5000 Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞµĞ½Ğ½Ñ‹Ñ… Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¼ Ğ¿Ğ°Ñ€ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ-Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ¿Ğ¾ 11 Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑĞ¼ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ², Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ· ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ° Ğ² 100000 Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². ĞÑ†ĞµĞ½ĞºĞ° ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… LLM Ğ½Ğ° ÑÑ‚Ğ¾Ğ¼ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ° ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ² ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ñ Ğ»ÑƒÑ‡ÑˆĞ¸Ğ¼Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°Ğ¼Ğ¸ Ğ¾ĞºĞ¾Ğ»Ğ¾ 80% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ñ… Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ¸ÑĞºĞ°.'}, 'en': {'title': 'FinForge: Elevating Financial Reasoning in Language Models', 'desc': 'FinForge is a new method for creating specialized benchmarks to evaluate language models in the finance sector. It combines expert knowledge with language model synthesis to generate high-quality, domain-specific datasets. The pipeline produces a benchmark called FinForge-5k, which includes over 5,000 validated question-answer pairs from a large corpus of financial documents. Testing various models on this benchmark shows notable differences in their financial reasoning abilities, highlighting the need for better tools in this critical area.'}, 'zh': {'title': 'FinForgeï¼šé‡‘èé¢†åŸŸè¯„ä¼°çš„æ–°æ ‡å‡†', 'desc': 'FinForgeæ˜¯ä¸€ä¸ªå¯æ‰©å±•çš„åŠåˆæˆç®¡é“ï¼Œç”¨äºåˆ›å»ºç‰¹å®šé¢†åŸŸçš„é‡‘èè¯„ä¼°åŸºå‡†ã€‚è¯¥æ–¹æ³•ç»“åˆäº†ä¸“å®¶æŒ‡å¯¼çš„æ•°æ®æ•´ç†å’Œè¯­è¨€æ¨¡å‹åˆæˆï¼Œè§£å†³äº†é‡‘èé¢†åŸŸé«˜è´¨é‡æ•°æ®é›†ç¨€ç¼ºçš„é—®é¢˜ã€‚é€šè¿‡æ„å»ºåŒ…å«è¶…è¿‡5000ä¸ªç»è¿‡äººå·¥éªŒè¯çš„é—®é¢˜-ç­”æ¡ˆå¯¹çš„åŸºå‡†ï¼ŒFinForgeå±•ç¤ºäº†ä¸åŒæ¨¡å‹åœ¨é‡‘èæ¨ç†èƒ½åŠ›ä¸Šçš„æ˜¾è‘—å·®å¼‚ã€‚è¯¥æ¡†æ¶æœ‰åŠ©äºè¯†åˆ«å½“å‰æ¨¡å‹çš„å±€é™æ€§ï¼Œå¹¶æŒ‡å¯¼æœªæ¥åœ¨é‡‘èé¢†åŸŸçš„æ”¹è¿›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.06463', 'title': 'Gecko: An Efficient Neural Architecture Inherently Processing Sequences with Arbitrary Lengths', 'url': 'https://huggingface.co/papers/2601.06463', 'abstract': 'Gecko is a neural architecture that improves long-range dependency capture through exponential moving average with gated attention and additional components like timestep decay normalization and sliding chunk attention, achieving efficient processing of arbitrary-length sequential data with superior long-context scalability compared to existing models.  \t\t\t\t\tAI-generated summary \t\t\t\t Designing a unified neural network to efficiently and inherently process sequential data with arbitrary lengths is a central and challenging problem in sequence modeling. The design choices in Transformer, including quadratic complexity and weak length extrapolation, have limited their ability to scale to long sequences. In this work, we propose Gecko, a neural architecture that inherits the design of Mega and Megalodon (exponential moving average with gated attention), and further introduces multiple technical components to improve its capability to capture long range dependencies, including timestep decay normalization, sliding chunk attention mechanism, and adaptive working memory. In a controlled pretraining comparison with Llama2 and Megalodon in the scale of 7 billion parameters and 2 trillion training tokens, Gecko achieves better efficiency and long-context scalability. Gecko reaches a training loss of 1.68, significantly outperforming Llama2-7B (1.75) and Megalodon-7B (1.70), and landing close to Llama2-13B (1.67). Notably, without relying on any context-extension techniques, Gecko exhibits inherent long-context processing and retrieval capabilities, stably handling sequences of up to 4 million tokens and retrieving information from contexts up to 4times longer than its attention window. Code: https://github.com/XuezheMax/gecko-llm', 'score': 1, 'issue_id': 553, 'pub_date': '2026-01-10', 'pub_date_card': {'ru': '10 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 10', 'zh': '1æœˆ10æ—¥'}, 'hash': '57b37f6520cfa000', 'authors': ['Xuezhe Ma', 'Shicheng Wen', 'Linghao Jin', 'Bilge Acun', 'Ruihang Lai', 'Bohan Hou', 'Will Lin', 'Hao Zhang', 'Songlin Yang', 'Ryan Lee', 'Mengxi Wu', 'Jonathan May', 'Luke Zettlemoyer', 'Carole-Jean Wu'], 'affiliations': ['Carnegie Mellon University', 'MIT CSAIL', 'Meta AI Research', 'University of California San Diego', 'University of Southern California'], 'pdf_title_img': 'assets/pdf/title_img/2601.06463.jpg', 'data': {'categories': ['#open_source', '#architecture', '#optimization', '#long_context', '#training'], 'emoji': 'ğŸ¦', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±ĞµĞ· ĞºĞ²Ğ°Ğ´Ñ€Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸', 'desc': 'Gecko â€” ÑÑ‚Ğ¾ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚ Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹ Ğ² Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑÑ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ° ÑÑ‡ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ ÑĞºÑĞ¿Ğ¾Ğ½ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞºĞ¾Ğ»ÑŒĞ·ÑÑ‰ĞµĞ³Ğ¾ ÑÑ€ĞµĞ´Ğ½ĞµĞ³Ğ¾ Ñ Ğ²ĞµĞ½Ñ‚Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ·Ğ°Ñ‚ÑƒÑ…Ğ°Ğ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ² Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ÑĞºĞ¾Ğ»ÑŒĞ·ÑÑ‰ĞµĞ³Ğ¾ Ğ¾ĞºĞ½Ğ° Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ´Ğ»Ğ¸Ğ½Ñ‹. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¢Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ñ ĞºĞ²Ğ°Ğ´Ñ€Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ, Gecko Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ğ¼Ğ¸. ĞŸÑ€Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ½Ğ° 2 Ñ‚Ñ€Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Llama2 Ğ¸ Megalodon, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¾ 4 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ².'}, 'en': {'title': 'Gecko: Mastering Long-Range Dependencies in Sequence Modeling', 'desc': 'Gecko is a new neural architecture designed to enhance the processing of long sequences in machine learning. It utilizes exponential moving average with gated attention and introduces innovative components like timestep decay normalization and sliding chunk attention. These features allow Gecko to efficiently capture long-range dependencies and handle arbitrary-length sequential data. In comparisons with existing models, Gecko demonstrates superior long-context scalability and better training efficiency, achieving lower training loss and effectively managing sequences of up to 4 million tokens.'}, 'zh': {'title': 'Geckoï¼šé«˜æ•ˆå¤„ç†é•¿åºåˆ—çš„ç¥ç»ç½‘ç»œæ¶æ„', 'desc': 'Geckoæ˜¯ä¸€ç§ç¥ç»ç½‘ç»œæ¶æ„ï¼Œæ—¨åœ¨é€šè¿‡æŒ‡æ•°ç§»åŠ¨å¹³å‡å’Œé—¨æ§æ³¨æ„åŠ›æœºåˆ¶æ¥æ”¹å–„é•¿è·ç¦»ä¾èµ–çš„æ•æ‰èƒ½åŠ›ã€‚å®ƒå¼•å…¥äº†æ—¶é—´æ­¥è¡°å‡å½’ä¸€åŒ–ã€æ»‘åŠ¨å—æ³¨æ„åŠ›æœºåˆ¶ç­‰å¤šä¸ªæŠ€æœ¯ç»„ä»¶ï¼Œä»è€Œæé«˜äº†å¤„ç†ä»»æ„é•¿åº¦åºåˆ—æ•°æ®çš„æ•ˆç‡ã€‚ä¸ç°æœ‰æ¨¡å‹ç›¸æ¯”ï¼ŒGeckoåœ¨é•¿ä¸Šä¸‹æ–‡å¯æ‰©å±•æ€§æ–¹é¢è¡¨ç°ä¼˜è¶Šï¼Œèƒ½å¤Ÿç¨³å®šå¤„ç†é•¿è¾¾400ä¸‡æ ‡è®°çš„åºåˆ—ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGeckoåœ¨è®­ç»ƒæŸå¤±ä¸Šæ˜¾è‘—ä¼˜äºLlama2å’ŒMegalodonï¼Œå±•ç°å‡ºå¼ºå¤§çš„é•¿ä¸Šä¸‹æ–‡å¤„ç†å’Œæ£€ç´¢èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.06423', 'title': 'Does Inference Scaling Improve Reasoning Faithfulness? A Multi-Model Analysis of Self-Consistency Tradeoffs', 'url': 'https://huggingface.co/papers/2601.06423', 'abstract': "Self-consistency improves reasoning accuracy for some models while potentially sacrificing faithfulness, with varying effects across different language models and problem difficulties.  \t\t\t\t\tAI-generated summary \t\t\t\t Self-consistency has emerged as a popular technique for improving large language model accuracy on reasoning tasks. The approach is straightforward: generate multiple reasoning paths and select the most common answer through majority voting. While this reliably boosts accuracy, it remains unclear whether these gains reflect genuine improvements in reasoning quality. We investigate a fundamental question that has not been studied before: does inference scaling improve reasoning faithfulness?   We conduct a comprehensive empirical study across four frontier models (GPT-5.2, Claude Opus 4.5, Gemini-3-flash-preview, and DeepSeek-v3.2) on 100 GSM8K mathematical reasoning problems. Our analysis employs bootstrap confidence intervals, McNemar's tests for paired comparisons, and Cohen's d effect sizes to quantify the effects rigorously. The results reveal striking differences across models that challenge common assumptions about self-consistency.   GPT-5.2 shows the expected pattern: accuracy improves from 78% to 90% at N=5, with faithfulness remaining relatively stable (0.540 to 0.510). Claude Opus 4.5 tells a completely different story. Its accuracy actually drops from 78% to 74.3% while faithfulness jumps dramatically from 0.270 to 0.891 at N=5. DeepSeek-v3.2, already at 98% accuracy, shows ceiling effects with modest faithfulness gains (0.440 to 0.541). Gemini-3-flash improves from 81% to 86% accuracy with a slight faithfulness decrease (0.260 to 0.212).   Problem difficulty analysis reveals that GPT-5.2 solves 82% of hard problems while breaking only 13% of easy ones. Claude, in contrast, breaks 23% of easy problems, explaining its accuracy decrease. These findings matter for practitioners: self-consistency is not universally beneficial, and teams should test their specific models before deployment. We release our code and provide practical recommendations for navigating these tradeoffs.", 'score': 1, 'issue_id': 558, 'pub_date': '2026-01-10', 'pub_date_card': {'ru': '10 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 10', 'zh': '1æœˆ10æ—¥'}, 'hash': '70b9fb1d5d36d731', 'authors': ['Deep Mehta'], 'affiliations': ['Adobe Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2601.06423.jpg', 'data': {'categories': ['#interpretability', '#math', '#benchmark', '#training', '#reasoning'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Self-consistency ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ½Ğ¾ Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑĞ½Ğ¸Ğ¶Ğ°Ñ‚ÑŒ Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ° self-consistency, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğ¾Ğ¼ Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ², Ğ¿Ğ¾-Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¼Ñƒ Ğ²Ğ»Ğ¸ÑĞµÑ‚ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… LLM Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¾ĞºÑĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹: ĞµÑĞ»Ğ¸ Ğ´Ğ»Ñ GPT-5.2 Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°ÑÑ‚Ñ‘Ñ‚ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ñ‚Ğ¾ Ğ´Ğ»Ñ Claude Opus Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ°Ğ´Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¸ Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚ self-consistency Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ñ‚ Ğ¾Ñ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾-Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¼Ñƒ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ»Ñ‘Ğ³ĞºĞ¸Ğ¼Ğ¸ Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ°Ğ¼Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ self-consistency Ğ½Ğµ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ñ€ĞµÑˆĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿ĞµÑ€ĞµĞ´ ĞµÑ‘ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸ĞµĞ¼.'}, 'en': {'title': 'Self-Consistency: A Double-Edged Sword in Model Reasoning', 'desc': 'This paper explores the impact of self-consistency on reasoning accuracy and faithfulness in large language models. Self-consistency involves generating multiple reasoning paths and selecting the most common answer, which can improve accuracy but may affect the quality of reasoning. The study analyzes four advanced models on mathematical reasoning tasks, revealing that the effects of self-consistency vary significantly across models. The findings suggest that self-consistency is not always advantageous, highlighting the need for careful evaluation of model performance before deployment.'}, 'zh': {'title': 'è‡ªä¸€è‡´æ€§ï¼šæå‡å‡†ç¡®æ€§ä½†éœ€è°¨æ…ä½¿ç”¨', 'desc': 'è‡ªä¸€è‡´æ€§æ˜¯ä¸€ç§æé«˜å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†ä»»åŠ¡å‡†ç¡®æ€§çš„æŠ€æœ¯ï¼Œé€šè¿‡ç”Ÿæˆå¤šä¸ªæ¨ç†è·¯å¾„å¹¶é€šè¿‡å¤šæ•°æŠ•ç¥¨é€‰æ‹©æœ€å¸¸è§çš„ç­”æ¡ˆã€‚æˆ‘ä»¬çš„ç ”ç©¶æ¢è®¨äº†æ¨ç†è§„æ¨¡æ˜¯å¦èƒ½æé«˜æ¨ç†çš„å¯ä¿¡åº¦ï¼Œå¹¶å¯¹å››ä¸ªå‰æ²¿æ¨¡å‹è¿›è¡Œäº†å…¨é¢çš„å®è¯ç ”ç©¶ã€‚ç»“æœæ˜¾ç¤ºï¼Œä¸åŒæ¨¡å‹åœ¨è‡ªä¸€è‡´æ€§ä¸Šçš„è¡¨ç°å·®å¼‚æ˜¾è‘—ï¼ŒæŒ‘æˆ˜äº†å¸¸è§å‡è®¾ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè‡ªä¸€è‡´æ€§å¹¶éå¯¹æ‰€æœ‰æ¨¡å‹éƒ½æœ‰ç›Šï¼Œå›¢é˜Ÿåœ¨éƒ¨ç½²å‰åº”æµ‹è¯•ç‰¹å®šæ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.05747', 'title': 'FlyPose: Towards Robust Human Pose Estimation From Aerial Views', 'url': 'https://huggingface.co/papers/2601.05747', 'abstract': 'A lightweight aerial human pose estimation system achieves improved accuracy and real-time performance through multi-dataset training and deployment on UAV platforms.  \t\t\t\t\tAI-generated summary \t\t\t\t Unmanned Aerial Vehicles (UAVs) are increasingly deployed in close proximity to humans for applications such as parcel delivery, traffic monitoring, disaster response and infrastructure inspections. Ensuring safe and reliable operation in these human-populated environments demands accurate perception of human poses and actions from an aerial viewpoint. This perspective challenges existing methods with low resolution, steep viewing angles and (self-)occlusion, especially if the application demands realtime feasibile models. We train and deploy FlyPose, a lightweight top-down human pose estimation pipeline for aerial imagery. Through multi-dataset training, we achieve an average improvement of 6.8 mAP in person detection across the test-sets of Manipal-UAV, VisDrone, HIT-UAV as well as our custom dataset. For 2D human pose estimation we report an improvement of 16.3 mAP on the challenging UAV-Human dataset. FlyPose runs with an inference latency of ~20 milliseconds including preprocessing on a Jetson Orin AGX Developer Kit and is deployed onboard a quadrotor UAV during flight experiments. We also publish FlyPose-104, a small but challenging aerial human pose estimation dataset, that includes manual annotations from difficult aerial perspectives: https://github.com/farooqhassaan/FlyPose.', 'score': 1, 'issue_id': 555, 'pub_date': '2026-01-09', 'pub_date_card': {'ru': '9 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 9', 'zh': '1æœˆ9æ—¥'}, 'hash': '8c634b4ff339cf13', 'authors': ['Hassaan Farooq', 'Marvin Brenner', 'Peter St\\Ã¼tz'], 'affiliations': ['Universitat der Bundeswehr Munich'], 'pdf_title_img': 'assets/pdf/title_img/2601.05747.jpg', 'data': {'categories': ['#open_source', '#small_models', '#robotics', '#inference', '#cv', '#dataset'], 'emoji': 'ğŸš', 'ru': {'title': 'Ğ›ĞµĞ³ĞºĞ°Ñ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ÑŒ Ğ´Ğ»Ñ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ·Ñ‹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ñ Ğ´Ñ€Ğ¾Ğ½Ğ¾Ğ² Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¾Ğ±Ğ»ĞµĞ³Ñ‡ĞµĞ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° FlyPose Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ·Ñ‹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ½Ğ° Ğ°ÑÑ€Ğ¾Ñ„Ğ¾Ñ‚Ğ¾ÑĞ½Ğ¸Ğ¼ĞºĞ°Ñ…, Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ Ğ±ĞµÑĞ¿Ğ¸Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ğ»ĞµÑ‚Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ¿Ğ¿Ğ°Ñ€Ğ°Ñ‚Ğ¾Ğ². Ğ‘Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¹: Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚ Ğ½Ğ° 6.8 mAP Ğ´Ğ»Ñ Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ğ¸ Ğ»ÑĞ´ĞµĞ¹ Ğ¸ Ğ½Ğ° 16.3 mAP Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ 2D Ğ¿Ğ¾Ğ·Ñ‹ Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ UAV-Human. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ñ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒÑ ~20 Ğ¼Ğ¸Ğ»Ğ»Ğ¸ÑĞµĞºÑƒĞ½Ğ´ Ğ½Ğ° Ğ²ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ñ€Ğµ Jetson Orin AGX Ğ¸ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ğ½ÑƒÑ‚Ğ° Ğ½Ğ° ĞºĞ²Ğ°Ğ´Ñ€Ğ¾ĞºĞ¾Ğ¿Ñ‚ĞµÑ€Ğµ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ»ĞµÑ‚Ğ½Ñ‹Ñ… Ğ¸ÑĞ¿Ñ‹Ñ‚Ğ°Ğ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ FlyPose-104 Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ»ÑĞ´ĞµĞ¹ Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ² ÑÑŠĞµĞ¼ĞºĞ¸ Ñ Ğ²Ğ¾Ğ·Ğ´ÑƒÑ…Ğ°.'}, 'en': {'title': 'FlyPose: Real-Time Aerial Human Pose Estimation Made Easy', 'desc': 'This paper presents FlyPose, a lightweight system designed for estimating human poses from aerial images captured by drones. It utilizes multi-dataset training to enhance accuracy, achieving significant improvements in mean Average Precision (mAP) for both person detection and 2D pose estimation. The system is optimized for real-time performance, with an inference latency of approximately 20 milliseconds, making it suitable for deployment on UAVs. Additionally, the authors introduce the FlyPose-104 dataset, which provides challenging aerial perspectives for further research in this area.'}, 'zh': {'title': 'è½»é‡çº§æ— äººæœºäººç±»å§¿æ€ä¼°è®¡çš„å®æ—¶çªç ´', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§è½»é‡çº§çš„ç©ºä¸­äººç±»å§¿æ€ä¼°è®¡ç³»ç»Ÿï¼Œåä¸ºFlyPoseã€‚è¯¥ç³»ç»Ÿé€šè¿‡å¤šæ•°æ®é›†è®­ç»ƒï¼Œèƒ½å¤Ÿåœ¨æ— äººæœºå¹³å°ä¸Šå®ç°æ›´é«˜çš„å‡†ç¡®æ€§å’Œå®æ—¶æ€§èƒ½ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªæµ‹è¯•é›†ä¸Šå®ç°äº†äººæ£€æµ‹å¹³å‡æå‡6.8 mAPï¼Œå¹¶åœ¨UAV-Humanæ•°æ®é›†ä¸Šå®ç°äº†16.3 mAPçš„æå‡ã€‚FlyPoseçš„æ¨ç†å»¶è¿Ÿçº¦ä¸º20æ¯«ç§’ï¼Œé€‚åˆåœ¨é£è¡Œå®éªŒä¸­å®æ—¶åº”ç”¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.07790', 'title': 'Benchmarking Small Language Models and Small Reasoning Language Models on System Log Severity Classification', 'url': 'https://huggingface.co/papers/2601.07790', 'abstract': 'Severity classification in system logs serves as a benchmark for evaluating model comprehension and deployability, with retrieval-augmented generation improving performance across small language models while efficiency varies significantly.  \t\t\t\t\tAI-generated summary \t\t\t\t System logs are crucial for monitoring and diagnosing modern computing infrastructure, but their scale and complexity require reliable and efficient automated interpretation. Since severity levels are predefined metadata in system log messages, having a model merely classify them offers limited standalone practical value, revealing little about its underlying ability to interpret system logs. We argue that severity classification is more informative when treated as a benchmark for probing runtime log comprehension rather than as an end task. Using real-world journalctl data from Linux production servers, we evaluate nine small language models (SLMs) and small reasoning language models (SRLMs) under zero-shot, few-shot, and retrieval-augmented generation (RAG) prompting. The results reveal strong stratification. Qwen3-4B achieves the highest accuracy at 95.64% with RAG, while Gemma3-1B improves from 20.25% under few-shot prompting to 85.28% with RAG. Notably, the tiny Qwen3-0.6B reaches 88.12% accuracy despite weak performance without retrieval. In contrast, several SRLMs, including Qwen3-1.7B and DeepSeek-R1-Distill-Qwen-1.5B, degrade substantially when paired with RAG. Efficiency measurements further separate models: most Gemma and Llama variants complete inference in under 1.2 seconds per log, whereas Phi-4-Mini-Reasoning exceeds 228 seconds per log while achieving <10% accuracy. These findings suggest that (1) architectural design, (2) training objectives, and (3) the ability to integrate retrieved context under strict output constraints jointly determine performance. By emphasizing small, deployable models, this benchmark aligns with real-time requirements of digital twin (DT) systems and shows that severity classification serves as a lens for evaluating model competence and real-time deployability, with implications for root cause analysis (RCA) and broader DT integration.', 'score': 0, 'issue_id': 557, 'pub_date': '2026-01-12', 'pub_date_card': {'ru': '12 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 12', 'zh': '1æœˆ12æ—¥'}, 'hash': 'df4f80a2e051113e', 'authors': ['Yahya Masri', 'Emily Ma', 'Zifu Wang', 'Joseph Rogers', 'Chaowei Yang'], 'affiliations': ['George Mason University'], 'pdf_title_img': 'assets/pdf/title_img/2601.07790.jpg', 'data': {'categories': ['#benchmark', '#small_models', '#inference', '#rag'], 'emoji': 'ğŸ“Š', 'ru': {'title': 'ĞšĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ ÑĞµÑ€ÑŒĞµĞ·Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ»Ğ¾Ğ³Ğ¾Ğ² ĞºĞ°Ğº Ñ‚ĞµÑÑ‚ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ´Ğ»Ñ Ğ¼Ğ°Ğ»Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ¹ ÑĞµÑ€ÑŒĞµĞ·Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½Ñ‹Ñ… Ğ»Ğ¾Ğ³Ğ°Ñ… Linux ĞºĞ°Ğº ÑÑ‚Ğ°Ğ»Ğ¾Ğ½ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ°Ğ»Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ»Ğ¾Ğ³Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚ĞµÑÑ‚Ğ¸Ñ€ÑƒÑÑ‚ Ğ´ĞµĞ²ÑÑ‚ÑŒ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… SLM Ğ¸ SLM Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ½ÑƒĞ»ĞµĞ²Ğ¾Ğ¹, Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¸ retrieval-augmented generation Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… journalctl. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ: Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Qwen3-4B, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ 95.64% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ RAG, Ñ‚Ğ¾Ğ³Ğ´Ğ° ĞºĞ°Ğº Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ†ĞµĞ»Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ² Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸.'}, 'en': {'title': 'Severity Classification: A Benchmark for Model Comprehension in System Logs', 'desc': "This paper discusses the importance of severity classification in system logs as a way to evaluate how well machine learning models understand and can be deployed in real-world scenarios. It highlights that simply classifying severity levels is not enough; instead, it should be used as a benchmark to assess a model's comprehension of logs. The authors tested various small language models and reasoning models using real-world data, finding that retrieval-augmented generation significantly improved performance for many models. The results indicate that model architecture, training objectives, and the ability to use retrieved context are crucial for achieving high accuracy and efficiency in log interpretation."}, 'zh': {'title': 'åˆ©ç”¨ä¸¥é‡æ€§åˆ†ç±»è¯„ä¼°æ¨¡å‹èƒ½åŠ›ä¸å®æ—¶éƒ¨ç½²', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†ç³»ç»Ÿæ—¥å¿—ä¸­çš„ä¸¥é‡æ€§åˆ†ç±»å¦‚ä½•ä½œä¸ºè¯„ä¼°æ¨¡å‹ç†è§£èƒ½åŠ›å’Œå¯éƒ¨ç½²æ€§çš„åŸºå‡†ã€‚é€šè¿‡ä½¿ç”¨å¢å¼ºæ£€ç´¢ç”ŸæˆæŠ€æœ¯ï¼Œç ”ç©¶æ˜¾ç¤ºå°å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†æ—¥å¿—æ—¶çš„æ€§èƒ½æ˜¾è‘—æé«˜ã€‚æˆ‘ä»¬è¯„ä¼°äº†å¤šç§å°å‹è¯­è¨€æ¨¡å‹åœ¨ä¸åŒæç¤ºæ–¹å¼ä¸‹çš„è¡¨ç°ï¼Œå‘ç°æ¨¡å‹çš„æ¶æ„è®¾è®¡å’Œè®­ç»ƒç›®æ ‡å¯¹å…¶æ€§èƒ½æœ‰é‡è¦å½±å“ã€‚æœ€ç»ˆï¼Œä¸¥é‡æ€§åˆ†ç±»ä¸ä»…æ˜¯ä¸€ä¸ªä»»åŠ¡ï¼Œæ›´æ˜¯è¯„ä¼°æ¨¡å‹èƒ½åŠ›å’Œå®æ—¶éƒ¨ç½²çš„æœ‰æ•ˆå·¥å…·ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.07239', 'title': 'Stochastic CHAOS: Why Deterministic Inference Kills, and Distributional Variability Is the Heartbeat of Artifical Cognition', 'url': 'https://huggingface.co/papers/2601.07239', 'abstract': 'Deterministic inference in large language models suppresses uncertainty modeling, emergent abilities, and safety awareness by enforcing single-output predictions instead of maintaining distributional variability.  \t\t\t\t\tAI-generated summary \t\t\t\t Deterministic inference is a comforting ideal in classical software: the same program on the same input should always produce the same output. As large language models move into real-world deployment, this ideal has been imported wholesale into inference stacks. Recent work from the Thinking Machines Lab has presented a detailed analysis of nondeterminism in LLM inference, showing how batch-invariant kernels and deterministic attention can enforce bitwise-identical outputs, positioning deterministic inference as a prerequisite for reproducibility and enterprise reliability.   In this paper, we take the opposite stance. We argue that, for LLMs, deterministic inference kills. It kills the ability to model uncertainty, suppresses emergent abilities, collapses reasoning into a single brittle path, and weakens safety alignment by hiding tail risks. LLMs implement conditional distributions over outputs, not fixed functions. Collapsing these distributions to a single canonical completion may appear reassuring, but it systematically conceals properties central to artificial cognition. We instead advocate Stochastic CHAOS, treating distributional variability as a signal to be measured and controlled.   Empirically, we show that deterministic inference is systematically misleading. Single-sample deterministic evaluation underestimates both capability and fragility, masking failure probability under paraphrases and noise. Phase-like transitions associated with emergent abilities disappear under greedy decoding. Multi-path reasoning degrades when forced onto deterministic backbones, reducing accuracy and diagnostic insight. Finally, deterministic evaluation underestimates safety risk by hiding rare but dangerous behaviors that appear only under multi-sample evaluation.', 'score': 0, 'issue_id': 561, 'pub_date': '2026-01-12', 'pub_date_card': {'ru': '12 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 12', 'zh': '1æœˆ12æ—¥'}, 'hash': 'adb315b407c448ac', 'authors': ['Tanmay Joshi', 'Shourya Aggarwal', 'Anusa Saha', 'Aadi Pandey', 'Shreyash Dhoot', 'Vighnesh Rai', 'Raxit Goswami', 'Aman Chadha', 'Vinija Jain', 'Amitava Das'], 'affiliations': ['Apple, USA', 'Google, USA', 'Pragya Lab, BITS Pilani Goa, India', 'Raapid Lab, USA'], 'pdf_title_img': 'assets/pdf/title_img/2601.07239.jpg', 'data': {'categories': ['#reasoning', '#training', '#inference', '#security', '#alignment', '#interpretability'], 'emoji': 'ğŸ²', 'ru': {'title': 'Ğ¡Ñ‚Ğ¾Ñ…Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ¿Ğ°ÑĞ°ĞµÑ‚: Ğ¿Ğ¾Ñ‡ĞµĞ¼Ñƒ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² LLM ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ğ°', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´ĞµÑ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ²Ñ‹Ğ²Ğ¾Ğ´ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ²Ñ€ĞµĞ´Ğ¸Ñ‚ Ğ¸Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼. ĞĞ½Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ½ÑƒĞ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ° ÑĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¸ÑÑ‚Ğ¸Ğ½Ğ½ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ÑÑ‰Ğ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Stochastic CHAOS, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹ ĞºĞ°Ğº Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğ¹ ÑĞ¸Ğ³Ğ½Ğ°Ğ», ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ½ÑƒĞ¶Ğ½Ğ¾ Ğ¸Ğ·Ğ¼ĞµÑ€ÑÑ‚ÑŒ Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´ĞµÑ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ½ĞµĞ´Ğ¾Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ°Ğº Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‚Ğ°Ğº Ğ¸ ĞµÑ‘ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Embrace Uncertainty: The Case Against Deterministic Inference in LLMs', 'desc': 'This paper critiques the use of deterministic inference in large language models (LLMs), arguing that it limits their ability to model uncertainty and emergent capabilities. The authors highlight that enforcing single-output predictions can lead to brittle reasoning and increased safety risks by masking potential failures. They propose an alternative approach called Stochastic CHAOS, which embraces distributional variability as a valuable aspect of LLMs. Empirical evidence is presented to show that deterministic methods can misrepresent the true capabilities and risks associated with LLMs, advocating for a more nuanced evaluation strategy.'}, 'zh': {'title': 'éšæœºæ··æ²Œï¼šè§£æ”¾å¤§å‹è¯­è¨€æ¨¡å‹çš„æ½œåŠ›', 'desc': 'è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­ï¼Œç¡®å®šæ€§æ¨ç†å¦‚ä½•æŠ‘åˆ¶ä¸ç¡®å®šæ€§å»ºæ¨¡å’Œæ–°å…´èƒ½åŠ›ã€‚ä½œè€…è®¤ä¸ºï¼Œå¼ºåˆ¶å•ä¸€è¾“å‡ºé¢„æµ‹ä¼šå¯¼è‡´æ¨ç†è¿‡ç¨‹å˜å¾—è„†å¼±ï¼Œå¹¶å‰Šå¼±å®‰å…¨æ€§æ„è¯†ã€‚ç›¸åï¼Œè®ºæ–‡æå€¡ä½¿ç”¨éšæœºæ··æ²Œï¼ˆStochastic CHAOSï¼‰ï¼Œå°†åˆ†å¸ƒå˜å¼‚æ€§è§†ä¸ºéœ€è¦æµ‹é‡å’Œæ§åˆ¶çš„ä¿¡å·ã€‚é€šè¿‡å®è¯ç ”ç©¶ï¼Œä½œè€…å±•ç¤ºäº†ç¡®å®šæ€§æ¨ç†åœ¨è¯„ä¼°èƒ½åŠ›å’Œè„†å¼±æ€§æ—¶çš„ç³»ç»Ÿæ€§è¯¯å¯¼ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.06966', 'title': 'RealMem: Benchmarking LLMs in Real-World Memory-Driven Interaction', 'url': 'https://huggingface.co/papers/2601.06966', 'abstract': 'RealMem benchmark evaluates memory systems for long-term project-oriented interactions in large language models, revealing challenges in managing dynamic context dependencies.  \t\t\t\t\tAI-generated summary \t\t\t\t As Large Language Models (LLMs) evolve from static dialogue interfaces to autonomous general agents, effective memory is paramount to ensuring long-term consistency. However, existing benchmarks primarily focus on casual conversation or task-oriented dialogue, failing to capture **"long-term project-oriented"** interactions where agents must track evolving goals.   To bridge this gap, we introduce **RealMem**, the first benchmark grounded in realistic project scenarios. RealMem comprises over 2,000 cross-session dialogues across eleven scenarios, utilizing natural user queries for evaluation.   We propose a synthesis pipeline that integrates Project Foundation Construction, Multi-Agent Dialogue Generation, and Memory and Schedule Management to simulate the dynamic evolution of memory. Experiments reveal that current memory systems face significant challenges in managing the long-term project states and dynamic context dependencies inherent in real-world projects.   Our code and datasets are available at [https://github.com/AvatarMemory/RealMemBench](https://github.com/AvatarMemory/RealMemBench).', 'score': 0, 'issue_id': 557, 'pub_date': '2026-01-11', 'pub_date_card': {'ru': '11 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 11', 'zh': '1æœˆ11æ—¥'}, 'hash': '542cc96c275ec959', 'authors': ['Haonan Bian', 'Zhiyuan Yao', 'Sen Hu', 'Zishan Xu', 'Shaolei Zhang', 'Yifu Guo', 'Ziliang Yang', 'Xueran Han', 'Huacan Wang', 'Ronghao Chen'], 'affiliations': ['Peking University', 'Renmin University of China', 'Shanghai Jiao Tong University', 'Sun Yat-sen University', 'University of the Chinese Academy of Sciences', 'Xidian University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2601.06966.jpg', 'data': {'categories': ['#open_source', '#long_context'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ”Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞµĞ°Ğ½ÑĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ°Ğ¼Ğ¸', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº RealMem Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¿Ñ€Ğ¸ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¼ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¸, Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ½Ğ° Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ñ‹. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ 2000 Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞµĞ°Ğ½ÑĞ°Ğ¼Ğ¸ Ğ² Ğ¾Ğ´Ğ¸Ğ½Ğ½Ğ°Ğ´Ñ†Ğ°Ñ‚Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¾Ğ² Ñ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¹ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ğµ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ° Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ°, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ² Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ Ñ Ñ€Ğ°ÑĞ¿Ğ¸ÑĞ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¾Ğ² Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑĞ¼Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ÑÑ….'}, 'en': {'title': 'Enhancing Memory for Long-Term Project Interactions in LLMs', 'desc': 'The RealMem benchmark is designed to evaluate how well memory systems in large language models (LLMs) handle long-term interactions in project-oriented scenarios. Unlike existing benchmarks that focus on casual or task-specific dialogues, RealMem addresses the complexities of evolving goals over time. It includes over 2,000 dialogues from realistic project situations, allowing for a comprehensive assessment of memory management. The findings indicate that current memory systems struggle with maintaining long-term project states and adapting to changing context, highlighting the need for improved memory strategies in LLMs.'}, 'zh': {'title': 'RealMemï¼šè¯„ä¼°é•¿æœŸé¡¹ç›®å¯¼å‘äº¤äº’çš„è®°å¿†ç³»ç»Ÿ', 'desc': 'æœ¬æ–‡ä»‹ç»äº†RealMemåŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é•¿æœŸé¡¹ç›®å¯¼å‘äº¤äº’ä¸­çš„è®°å¿†ç³»ç»Ÿã€‚éšç€å¤§å‹è¯­è¨€æ¨¡å‹çš„å‘å±•ï¼Œä»é™æ€å¯¹è¯ç•Œé¢è½¬å˜ä¸ºè‡ªä¸»é€šç”¨ä»£ç†ï¼Œæœ‰æ•ˆçš„è®°å¿†ç®¡ç†å˜å¾—è‡³å…³é‡è¦ã€‚ç°æœ‰çš„åŸºå‡†æµ‹è¯•ä¸»è¦é›†ä¸­åœ¨ä¼‘é—²å¯¹è¯æˆ–ä»»åŠ¡å¯¼å‘å¯¹è¯ï¼Œæœªèƒ½æ•æ‰åˆ°éœ€è¦è·Ÿè¸ªä¸æ–­å˜åŒ–ç›®æ ‡çš„é•¿æœŸé¡¹ç›®å¯¼å‘äº¤äº’ã€‚RealMemåŸºå‡†æµ‹è¯•åŒ…å«2000å¤šä¸ªè·¨ä¼šè¯å¯¹è¯ï¼Œæ¨¡æ‹ŸçœŸå®é¡¹ç›®åœºæ™¯ï¼Œæ­ç¤ºäº†å½“å‰è®°å¿†ç³»ç»Ÿåœ¨ç®¡ç†é•¿æœŸé¡¹ç›®çŠ¶æ€å’ŒåŠ¨æ€ä¸Šä¸‹æ–‡ä¾èµ–æ–¹é¢é¢ä¸´çš„é‡å¤§æŒ‘æˆ˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.06496', 'title': '3D CoCa v2: Contrastive Learners with Test-Time Search for Generalizable Spatial Intelligence', 'url': 'https://huggingface.co/papers/2601.06496', 'abstract': '3D CoCa v2 enhances 3D captioning by combining contrastive vision-language learning with spatially-aware 3D scene encoding and test-time search for improved generalization across diverse environments.  \t\t\t\t\tAI-generated summary \t\t\t\t Spatial intelligence refers to the ability to perceive, reason about, and describe objects and their relationships within three-dimensional environments, forming a foundation for embodied perception and scene understanding. 3D captioning aims to describe 3D scenes in natural language; however, it remains challenging due to the sparsity and irregularity of point clouds and, more critically, the weak grounding and limited out-of-distribution (OOD) generalization of existing captioners across drastically different environments, including indoor and outdoor 3D scenes. To address this challenge, we propose 3D CoCa v2, a generalizable 3D captioning framework that unifies contrastive vision-language learning with 3D caption generation and further improves robustness via test-time search (TTS) without updating the captioner parameters. 3D CoCa v2 builds on a frozen CLIP-based semantic prior, a spatially-aware 3D scene encoder for geometry, and a multimodal decoder jointly optimized with contrastive and captioning objectives, avoiding external detectors or handcrafted proposals. At inference, TTS produces diverse caption candidates and performs reward-guided selection using a compact scene summary. Experiments show improvements over 3D CoCa of +1.50 CIDEr@0.5IoU on ScanRefer and +1.61 CIDEr@0.5IoU on Nr3D, and +3.8 CIDEr@0.25 in zero-shot OOD evaluation on TOD3Cap. Code will be released at https://github.com/AIGeeksGroup/3DCoCav2.', 'score': 0, 'issue_id': 544, 'pub_date': '2026-01-10', 'pub_date_card': {'ru': '10 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 10', 'zh': '1æœˆ10æ—¥'}, 'hash': 'ee370f5e69428850', 'authors': ['Hao Tang', 'Ting Huang', 'Zeyu Zhang'], 'affiliations': ['School of Computer Science, Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2601.06496.jpg', 'data': {'categories': ['#open_source'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ¢Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ñ‹Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ±ĞµĞ· Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†: ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°', 'desc': '3D CoCa v2 â€” ÑÑ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½ Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ° Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ¾ÑĞ²ĞµĞ´Ğ¾Ğ¼Ğ»Ñ‘Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ¾Ğ¼ 3D-ÑÑ†ĞµĞ½. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ° Ğ½Ğ° Ğ·Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ñ… ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑÑ… CLIP Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€, ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ Ñ Ñ†ĞµĞ»ÑĞ¼Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹. Ğ”Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ¿Ğ¾Ğ¸ÑĞº Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ (TTS), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ñ‹ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ Ğ¸ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ·ÑĞ¼Ğµ ÑÑ†ĞµĞ½Ñ‹, Ğ±ĞµĞ· Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸ÑÑ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° Ğ½ĞµÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸ÑÑ….'}, 'en': {'title': 'Enhancing 3D Captioning with Contrastive Learning and Spatial Awareness', 'desc': "3D CoCa v2 is a framework designed to improve 3D captioning by integrating contrastive vision-language learning with advanced 3D scene encoding. It addresses challenges like the irregularity of point clouds and the need for better generalization in diverse environments. The model utilizes a frozen CLIP-based semantic prior and a spatially-aware encoder to enhance understanding of 3D geometry. Additionally, it employs test-time search to generate diverse caption options and select the best one without altering the model's parameters, leading to significant performance gains in various benchmarks."}, 'zh': {'title': 'æå‡ä¸‰ç»´æè¿°ç”Ÿæˆçš„æ™ºèƒ½æ¡†æ¶', 'desc': '3D CoCa v2 æ˜¯ä¸€ç§å¢å¼ºä¸‰ç»´æè¿°ç”Ÿæˆçš„æ¡†æ¶ï¼Œå®ƒç»“åˆäº†å¯¹æ¯”è§†è§‰-è¯­è¨€å­¦ä¹ å’Œç©ºé—´æ„ŸçŸ¥çš„ä¸‰ç»´åœºæ™¯ç¼–ç ã€‚è¯¥æ–¹æ³•æ—¨åœ¨è§£å†³ç°æœ‰ä¸‰ç»´æè¿°ç”Ÿæˆå™¨åœ¨ä¸åŒç¯å¢ƒä¸­çš„å¼±åŸºç¡€å’Œæœ‰é™çš„æ³›åŒ–èƒ½åŠ›ã€‚é€šè¿‡åœ¨æ¨ç†é˜¶æ®µä½¿ç”¨æµ‹è¯•æ—¶æœç´¢ï¼ˆTTSï¼‰ï¼Œ3D CoCa v2 æé«˜äº†æè¿°çš„é²æ£’æ€§ï¼Œè€Œæ— éœ€æ›´æ–°æè¿°ç”Ÿæˆå™¨çš„å‚æ•°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œ3D CoCa v2 åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šæ˜¾è‘—æå‡äº†æè¿°ç”Ÿæˆçš„è´¨é‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.06329', 'title': 'On the Fallacy of Global Token Perplexity in Spoken Language Model Evaluation', 'url': 'https://huggingface.co/papers/2601.06329', 'abstract': "Speech models trained on raw audio can generate appropriate content while maintaining speaker and emotion attributes, but traditional text-based evaluation methods underestimate speech characteristics; new evaluation approaches better correlate with human perception.  \t\t\t\t\tAI-generated summary \t\t\t\t Generative spoken language models pretrained on large-scale raw audio can continue a speech prompt with appropriate content while preserving attributes like speaker and emotion, serving as foundation models for spoken dialogue. In prior literature, these models are often evaluated using ``global token perplexity'', which directly applies the text perplexity formulation to speech tokens. However, this practice overlooks fundamental differences between speech and text modalities, possibly leading to an underestimation of the speech characteristics. In this work, we propose a variety of likelihood- and generative-based evaluation methods that serve in place of naive global token perplexity. We demonstrate that the proposed evaluations more faithfully reflect perceived generation quality, as evidenced by stronger correlations with human-rated mean opinion scores (MOS). When assessed under the new metrics, the relative performance landscape of spoken language models is reshaped, revealing a significantly reduced gap between the best-performing model and the human topline. Together, these results suggest that appropriate evaluation is critical for accurately assessing progress in spoken language modeling.", 'score': 0, 'issue_id': 546, 'pub_date': '2026-01-09', 'pub_date_card': {'ru': '9 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 9', 'zh': '1æœˆ9æ—¥'}, 'hash': '3718805726712adf', 'authors': ['Jeff Chan-Jan Sju', 'Liang-Hsuan Tseng', 'Yi-Cheng Lin', 'Yen-Chun Kuo', 'Ju-Chieh Chou', 'Kai-Wei Chang', 'Hung-yi Lee', 'Carlos Busso'], 'affiliations': ['Carnegie Mellon University', 'Massachusetts Institute of Technology', 'National Taiwan University', 'Toyota Technological Institute at Chicago'], 'pdf_title_img': 'assets/pdf/title_img/2601.06329.jpg', 'data': {'categories': ['#audio', '#benchmark'], 'emoji': 'ğŸ¤', 'ru': {'title': 'ĞŸÑ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ñ€ĞµÑ‡ĞµĞ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº, Ğ° Ğ½Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ²', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ÑÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° ÑÑ‹Ñ€Ğ¾Ğ¼ Ğ°ÑƒĞ´Ğ¸Ğ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹ Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ°Ñ‚ÑŒ Ñ€ĞµÑ‡ĞµĞ²Ñ‹Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰ĞµĞ³Ğ¾ Ğ¸ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ° Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿ĞµÑ€Ğ¿Ğ»ĞµĞºÑĞ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ½Ğµ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸ĞºÑƒ Ñ€ĞµÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ½ĞµĞ´Ğ¾Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ÑÑ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ñ… Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ñ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ»ÑƒÑ‡ÑˆĞµ ĞºĞ¾Ñ€Ñ€ĞµĞ»Ğ¸Ñ€ÑƒÑÑ‚ Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° (MOS). ĞŸÑ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº Ğ¿ĞµÑ€ĞµĞ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑƒÑÑ‚Ğ½Ğ¾Ğ¹ Ñ€ĞµÑ‡Ğ¸, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ²Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ½Ğ¸Ğ¼Ğ¸ Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼ ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ¼.'}, 'en': {'title': 'Revolutionizing Speech Model Evaluation for Better Human Alignment', 'desc': 'This paper discusses the limitations of traditional text-based evaluation methods for speech models that generate audio content. It highlights that these methods often fail to capture the unique characteristics of speech, such as speaker identity and emotional tone. The authors propose new evaluation techniques that better align with human perceptions of speech quality, showing stronger correlations with human ratings. Their findings indicate that using these improved metrics can significantly change how we assess the performance of spoken language models, emphasizing the importance of appropriate evaluation in this field.'}, 'zh': {'title': 'é€‚å½“è¯„ä¼°æ˜¯è¯­éŸ³æ¨¡å‹è¿›å±•çš„å…³é”®', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†åŸºäºåŸå§‹éŸ³é¢‘è®­ç»ƒçš„è¯­éŸ³æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹èƒ½å¤Ÿåœ¨ä¿æŒè¯´è¯è€…å’Œæƒ…æ„Ÿç‰¹å¾çš„åŒæ—¶ç”Ÿæˆåˆé€‚çš„å†…å®¹ã€‚ä¼ ç»Ÿçš„æ–‡æœ¬è¯„ä¼°æ–¹æ³•æœªèƒ½å……åˆ†è€ƒè™‘è¯­éŸ³çš„ç‰¹æ€§ï¼Œå› æ­¤å¯èƒ½ä½ä¼°äº†è¯­éŸ³æ¨¡å‹çš„è¡¨ç°ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç³»åˆ—æ–°çš„è¯„ä¼°æ–¹æ³•ï¼Œè¿™äº›æ–¹æ³•æ›´å¥½åœ°ä¸äººç±»æ„ŸçŸ¥ç›¸ç¬¦ï¼Œèƒ½å¤Ÿæ›´å‡†ç¡®åœ°åæ˜ ç”Ÿæˆè´¨é‡ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œé€‚å½“çš„è¯„ä¼°æ–¹æ³•å¯¹äºå‡†ç¡®è¯„ä¼°è¯­éŸ³è¯­è¨€å»ºæ¨¡çš„è¿›å±•è‡³å…³é‡è¦ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.06307', 'title': 'A Rising Tide Lifts All Boats: MTQE Rewards for Idioms Improve General Translation Quality', 'url': 'https://huggingface.co/papers/2601.06307', 'abstract': 'GRPO-style fine-tuning with MTQE models as rewards improves idiom translation by 14 points while enhancing general translation and cross-lingual capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t Non-compositional expressions (e.g., idioms, proverbs, and metaphors) pose significant challenges for neural machine translation systems because their meanings cannot be derived from individual words alone. These expressions encode rich, cultural meaning, and have both figurative and literal meanings, making accurate translation difficult. Because models are fairly good at translating compositional text, we investigate GRPO-style fine-tuning using Machine Translation Quality Estimation (MTQE) models as reward functions to train models to better translate idioms. Using Chinese and Hindi idiom datasets, we find that idiom translation abilities improve by ~14 points, general, non-idiomatic translation implicitly improves by ~8 points, and cross-lingual translation abilities (trained on one language, evaluated on another) improves by ~6 points. Overall, our work quantifies the non-compositional translation gap and offers insights for developing LLMs with stronger cross-cultural and figurative language understanding.', 'score': 0, 'issue_id': 561, 'pub_date': '2026-01-09', 'pub_date_card': {'ru': '9 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 9', 'zh': '1æœˆ9æ—¥'}, 'hash': 'f5d6de6d1d83de1b', 'authors': ['Ishika Agarwal', 'Zhenlin He', 'Dhruva Patil', 'Dilek Hakkani-TÃ¼r'], 'affiliations': ['UIUC'], 'pdf_title_img': 'assets/pdf/title_img/2601.06307.jpg', 'data': {'categories': ['#optimization', '#training', '#transfer_learning', '#machine_translation', '#rlhf', '#multilingual'], 'emoji': 'ğŸŒ‰', 'ru': {'title': 'ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ²Ğ°: Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° Ğ¸Ğ´Ğ¸Ğ¾Ğ¼', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° Ğ¸Ğ´Ğ¸Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²Ñ‹Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ğ»Ğ¾Ñ…Ğ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ½ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ„Ñ€Ğ°Ğ·Ğ°Ğ¼Ğ¸, ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ğ¾Ğµ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ GRPO-style fine-tuning Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° (MTQE) Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ñ‡Ğ¸ĞºĞ¾Ğ² Ğ»ÑƒÑ‡ÑˆĞµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ñ Ğ¸Ğ´Ğ¸Ğ¾Ğ¼Ğ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¾Ğ¼ Ğ¸ Ñ…Ğ¸Ğ½Ğ´Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° Ğ¸Ğ´Ğ¸Ğ¾Ğ¼ Ğ½Ğ° 14 Ğ¿ÑƒĞ½ĞºÑ‚Ğ¾Ğ², Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ĞºĞ¾ÑĞ²ĞµĞ½Ğ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° Ğ½Ğ° 8 Ğ¿ÑƒĞ½ĞºÑ‚Ğ¾Ğ² Ğ¸ ĞºÑ€Ğ¾ÑÑÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° 6 Ğ¿ÑƒĞ½ĞºÑ‚Ğ¾Ğ². Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ² Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğµ Ğ½ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ²Ñ‹Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ LLM Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ ĞºÑ€Ğ¾ÑÑ-ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°.'}, 'en': {'title': 'Enhancing Idiom Translation with GRPO and MTQE', 'desc': 'This paper addresses the challenge of translating idioms and other non-compositional expressions in neural machine translation. It proposes a method called GRPO-style fine-tuning, which utilizes Machine Translation Quality Estimation (MTQE) models as reward functions to enhance translation accuracy. The study shows that this approach improves idiom translation by approximately 14 points, while also boosting general translation and cross-lingual capabilities. The findings highlight the importance of understanding cultural nuances in language and provide a pathway for developing more effective language models.'}, 'zh': {'title': 'æå‡æˆè¯­ç¿»è¯‘çš„æ™ºèƒ½æ–¹æ³•', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†å¦‚ä½•é€šè¿‡GRPOé£æ ¼çš„å¾®è°ƒå’Œæœºå™¨ç¿»è¯‘è´¨é‡è¯„ä¼°ï¼ˆMTQEï¼‰æ¨¡å‹ä½œä¸ºå¥–åŠ±å‡½æ•°ï¼Œæ¥æ”¹å–„æˆè¯­ç¿»è¯‘çš„æ•ˆæœã€‚ç ”ç©¶è¡¨æ˜ï¼Œä½¿ç”¨è¿™ç§æ–¹æ³•ï¼Œæˆè¯­ç¿»è¯‘çš„å‡†ç¡®ç‡æé«˜äº†çº¦14ä¸ªç™¾åˆ†ç‚¹ï¼ŒåŒæ—¶ä¸€èˆ¬ç¿»è¯‘å’Œè·¨è¯­è¨€èƒ½åŠ›ä¹Ÿå¾—åˆ°äº†æå‡ã€‚é€šè¿‡å¯¹ä¸­æ–‡å’Œå°åœ°è¯­æˆè¯­æ•°æ®é›†çš„å®éªŒï¼Œå‘ç°éæˆè¯­ç¿»è¯‘çš„å‡†ç¡®ç‡æé«˜äº†çº¦8ä¸ªç™¾åˆ†ç‚¹ï¼Œè·¨è¯­è¨€ç¿»è¯‘èƒ½åŠ›æé«˜äº†çº¦6ä¸ªç™¾åˆ†ç‚¹ã€‚æ€»ä½“è€Œè¨€ï¼Œè¿™é¡¹ç ”ç©¶é‡åŒ–äº†éç»„åˆç¿»è¯‘çš„å·®è·ï¼Œå¹¶ä¸ºå¼€å‘å…·æœ‰æ›´å¼ºè·¨æ–‡åŒ–å’Œæ¯”å–»è¯­è¨€ç†è§£èƒ½åŠ›çš„å¤§å‹è¯­è¨€æ¨¡å‹æä¾›äº†è§è§£ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.06238', 'title': 'SPINAL -- Scaling-law and Preference Integration in Neural Alignment Layers', 'url': 'https://huggingface.co/papers/2601.06238', 'abstract': "SPINAL diagnoses how DPO alignment reshapes representations layer by layer, revealing geometric localization of preference gradients in final decoder blocks and enabling practical auditing of alignment progress.  \t\t\t\t\tAI-generated summary \t\t\t\t Direct Preference Optimization (DPO) is a principled, scalable alternative to RLHF for aligning large language models from pairwise preferences, but its internal geometric footprint remains undercharacterized, limiting audits, checkpoint comparisons, and failure prediction. We introduce SPINAL (Scaling-law and Preference Integration in Neural Alignment Layers), a diagnostic that measures how alignment reshapes representations across depth by tracing localized structural change layer by layer. Across model families, DPO produces a layerwise calibration effect concentrated in the final decoder blocks (often layers 21-30), where preference gradients most directly affect the next-token distribution. SPINAL encodes each checkpoint as a depth trace over (layer index, contraction score, transport score). The contraction score summarizes how quickly the tail of a layer's spectrum decays (how fast small modes vanish); higher values indicate stronger contraction into fewer effective directions. The transport score summarizes how much the token distribution shifts between adjacent layers using a bounded overlap measure; lower values indicate shorter, smoother steps through representation space. Aligned checkpoints show a late-layer ramp-up in contraction and a smooth reduction in transport, consistent with tightened and stabilized policy mass, while unaligned models trace higher-curvature, more entropic, and geometrically incoherent depth paths. Overall, alignment is geometrically localized: the final layers encode the dominant preference-induced corrections. SPINAL turns this localization into a practical audit signal, quantifying where alignment concentrates, how strongly it manifests, and when it begins to destabilize during training.", 'score': 0, 'issue_id': 561, 'pub_date': '2026-01-08', 'pub_date_card': {'ru': '8 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 8', 'zh': '1æœˆ8æ—¥'}, 'hash': 'ba9bfe2e8526b996', 'authors': ['Arion Das', 'Partha Pratim Saha', 'Amit Dhanda', 'Vinija Jain', 'Aman Chadha', 'Amitava Das'], 'affiliations': ['Apple (USA)', 'Google (USA)', 'IIIT Ranchi', 'Pragya Lab, BITS Pilani, Goa'], 'pdf_title_img': 'assets/pdf/title_img/2601.06238.jpg', 'data': {'categories': ['#training', '#rlhf', '#alignment', '#interpretability', '#architecture'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'Ğ’Ğ¸Ğ´Ğ¸Ğ¼Ğ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ: Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ SPINAL â€” Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚, ĞºĞ°Ğº Direct Preference Optimization (DPO) Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ»Ğ¾Ğ¹ Ğ·Ğ° ÑĞ»Ğ¾ĞµĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ¾ÑÑ€ĞµĞ´Ğ¾Ñ‚Ğ¾Ñ‡ĞµĞ½Ğ¾ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ² Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ»Ğ¾ĞºĞ°Ñ… Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€Ğ°, Ğ³Ğ´Ğµ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ÑŒÑˆĞµĞµ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ½Ğ° Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ°. SPINAL Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ â€” ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ĞºÑ†Ğ¸Ñ ÑĞ¿ĞµĞºÑ‚Ñ€Ğ° (ĞºĞ°Ğº Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾ Ğ¸ÑÑ‡ĞµĞ·Ğ°ÑÑ‚ Ğ¼Ğ°Ğ»Ñ‹Ğµ Ğ¼Ğ¾Ğ´Ñ‹) Ğ¸ Ñ‚Ñ€Ğ°Ğ½ÑĞ¿Ğ¾Ñ€Ñ‚Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ (Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ»Ğ¾ÑĞ¼Ğ¸) â€” Ğ´Ğ»Ñ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ğ²ÑĞµĞ¹ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ°ÑƒĞ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ½ĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'SPINAL: Unraveling Layerwise Alignment in Language Models', 'desc': "The paper introduces SPINAL, a diagnostic tool that analyzes how Direct Preference Optimization (DPO) aligns large language models by reshaping their internal representations layer by layer. It focuses on the final decoder blocks, where preference gradients significantly influence the model's output. SPINAL measures two key metrics: contraction score, which indicates how effectively a layer compresses its representation, and transport score, which assesses the smoothness of transitions between layers. This allows for practical auditing of alignment progress, revealing that alignment effects are concentrated in the later layers of the model, providing insights into the stability and effectiveness of the alignment process during training."}, 'zh': {'title': 'SPINALï¼šæ­ç¤ºå¯¹é½è¿‡ç¨‹çš„å‡ ä½•ç‰¹å¾', 'desc': 'SPINAL æ˜¯ä¸€ç§è¯Šæ–­å·¥å…·ï¼Œç”¨äºåˆ†æ DPOï¼ˆç›´æ¥åå¥½ä¼˜åŒ–ï¼‰å¦‚ä½•é€å±‚é‡å¡‘è¡¨ç¤ºï¼Œæ­ç¤ºæœ€ç»ˆè§£ç å™¨å—ä¸­åå¥½æ¢¯åº¦çš„å‡ ä½•å®šä½ã€‚é€šè¿‡è¿½è¸ªæ¯ä¸€å±‚çš„å±€éƒ¨ç»“æ„å˜åŒ–ï¼ŒSPINAL èƒ½å¤Ÿé‡åŒ–å¯¹é½è¿‡ç¨‹çš„è¿›å±•ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒDPO åœ¨æ¨¡å‹çš„æœ€åè§£ç å™¨å±‚ï¼ˆé€šå¸¸æ˜¯ç¬¬21åˆ°30å±‚ï¼‰äº§ç”Ÿäº†æ˜¾è‘—çš„å±‚çº§æ ¡å‡†æ•ˆåº”ï¼Œåå¥½æ¢¯åº¦ç›´æ¥å½±å“ä¸‹ä¸€ä¸ªæ ‡è®°çš„åˆ†å¸ƒã€‚SPINAL è¿˜æä¾›äº†å¯¹é½æ£€æŸ¥ç‚¹çš„å®¡è®¡ä¿¡å·ï¼Œå¸®åŠ©ç†è§£å¯¹é½é›†ä¸­åœ¨å“ªé‡Œã€å¼ºåº¦å¦‚ä½•ä»¥åŠåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä½•æ—¶å¼€å§‹ä¸ç¨³å®šã€‚'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (25)', '#agents (65)', '#agi', '#alignment (32)', '#architecture (81)', '#audio (12)', '#benchmark (130)', '#cv (47)', '#data (22)', '#dataset (89)', '#diffusion (38)', '#ethics (6)', '#games (3)', '#graphs (5)', '#hallucinations (11)', '#healthcare (10)', '#inference (24)', '#interpretability (26)', '#leakage (2)', '#long_context (25)', '#low_resource (11)', '#machine_translation (3)', '#math (14)', '#multilingual (17)', '#multimodal (103)', '#open_source (89)', '#optimization (87)', '#plp (11)', '#rag (14)', '#reasoning (104)', '#rl (64)', '#rlhf (29)', '#robotics (18)', '#science (20)', '#security (18)', '#small_models (23)', '#story_generation (2)', '#survey (10)', '#synthetic (33)', '#training (153)', '#transfer_learning (13)', '#video (49)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            ğŸ”º ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'ğŸ”„ ' + getTimeDiff('2026-01-21 07:31',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ",
                    pub_date: "Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸",
                    issue_id: "Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "è¯„åˆ†",
                    pub_date: "å‘å¸ƒæ—¥æœŸ",
                    issue_id: "HFä¸Šä¼ æ—¥æœŸ"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2026-01-21 07:31')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2026-01-21 07:31')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('monthly'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    