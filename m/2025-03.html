
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 103 papers. March 2025.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #7a30efcf;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: #7a30efcf;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #7a30ef17;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf monthly</h1></a>
            <p><span id="title-date">Март 2025</span> | <span id="title-articles-count">103 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/m/2025-02.html">⬅️ <span id="prev-date">02.2025</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/m/2025-04.html">➡️ <span id="next-date">04.2025</span></a></span>
            <span class="nav-item" id="nav-daily"><a href="https://hfday.ru">📈 <span id='top-day-label'>День</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': 'Март 2025', 'en': 'March 2025', 'zh': '3月2025年'};
        let feedDateNext = {'ru': '04.2025', 'en': '04/2025', 'zh': '4月2025年'};
        let feedDatePrev = {'ru': '02.2025', 'en': '02/2025', 'zh': '2月2025年'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf monthly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2503.01785', 'title': 'Visual-RFT: Visual Reinforcement Fine-Tuning', 'url': 'https://huggingface.co/papers/2503.01785', 'abstract': "Reinforcement Fine-Tuning (RFT) in Large Reasoning Models like OpenAI o1 learns from feedback on its answers, which is especially useful in applications when fine-tuning data is scarce. Recent open-source work like DeepSeek-R1 demonstrates that reinforcement learning with verifiable reward is one key direction in reproducing o1. While the R1-style model has demonstrated success in language models, its application in multi-modal domains remains under-explored. This work introduces Visual Reinforcement Fine-Tuning (Visual-RFT), which further extends the application areas of RFT on visual tasks. Specifically, Visual-RFT first uses Large Vision-Language Models (LVLMs) to generate multiple responses containing reasoning tokens and final answers for each input, and then uses our proposed visual perception verifiable reward functions to update the model via the policy optimization algorithm such as Group Relative Policy Optimization (GRPO). We design different verifiable reward functions for different perception tasks, such as the Intersection over Union (IoU) reward for object detection. Experimental results on fine-grained image classification, few-shot object detection, reasoning grounding, as well as open-vocabulary object detection benchmarks show the competitive performance and advanced generalization ability of Visual-RFT compared with Supervised Fine-tuning (SFT). For example, Visual-RFT improves accuracy by 24.3% over the baseline in one-shot fine-grained image classification with around 100 samples. In few-shot object detection, Visual-RFT also exceeds the baseline by 21.9 on COCO's two-shot setting and 15.4 on LVIS. Our Visual-RFT represents a paradigm shift in fine-tuning LVLMs, offering a data-efficient, reward-driven approach that enhances reasoning and adaptability for domain-specific tasks.", 'score': 43, 'issue_id': 2511, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': 'ef2e10eb59ab7743', 'authors': ['Ziyu Liu', 'Zeyi Sun', 'Yuhang Zang', 'Xiaoyi Dong', 'Yuhang Cao', 'Haodong Duan', 'Dahua Lin', 'Jiaqi Wang'], 'affiliations': ['Shanghai Artificial Intelligence Laboratory', 'Shanghai Jiaotong University', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2503.01785.jpg', 'data': {'categories': ['#multimodal', '#open_source', '#cv', '#optimization', '#rlhf', '#reasoning', '#training', '#rl'], 'emoji': '🔬', 'ru': {'title': 'Visual-RFT: Революция в тонкой настройке визуально-языковых моделей', 'desc': 'Статья представляет Visual Reinforcement Fine-Tuning (Visual-RFT) - метод, расширяющий применение обучения с подкреплением в визуальных задачах. Visual-RFT использует большие визуально-языковые модели для генерации ответов с токенами рассуждений и применяет визуально верифицируемые функции вознаграждения для обновления модели. Эксперименты показывают превосходство Visual-RFT над методом Supervised Fine-tuning в задачах классификации изображений, обнаружения объектов и обоснованного заземления. Метод демонстрирует значительное улучшение точности и обобщающей способности при ограниченном количестве обучающих примеров.'}, 'en': {'title': 'Revolutionizing Visual Learning with Reinforcement Fine-Tuning', 'desc': "This paper introduces Visual Reinforcement Fine-Tuning (Visual-RFT), a method that enhances large vision-language models (LVLMs) by using reinforcement learning to improve their performance on visual tasks. Visual-RFT generates multiple responses for each input and employs verifiable reward functions to optimize the model's policy, making it particularly effective in scenarios with limited fine-tuning data. The approach demonstrates significant improvements in tasks like fine-grained image classification and object detection, outperforming traditional supervised fine-tuning methods. Overall, Visual-RFT represents a novel, efficient way to fine-tune LVLMs, focusing on reasoning and adaptability in specific domains."}, 'zh': {'title': '视觉强化微调：提升推理与适应性的创新方法', 'desc': '强化微调（RFT）在大型推理模型中通过反馈学习，特别适用于微调数据稀缺的应用场景。本文提出的视觉强化微调（Visual-RFT）扩展了RFT在视觉任务中的应用，利用大型视觉语言模型生成多种响应，并通过可验证的视觉感知奖励函数进行模型更新。实验结果表明，Visual-RFT在细粒度图像分类和少样本目标检测等任务中表现出色，相较于传统的监督微调（SFT）方法，准确率显著提高。Visual-RFT代表了一种新的微调范式，提供了一种数据高效、以奖励驱动的方法，增强了模型在特定领域任务中的推理能力和适应性。'}}}, {'id': 'https://huggingface.co/papers/2503.01743', 'title': 'Phi-4-Mini Technical Report: Compact yet Powerful Multimodal Language Models via Mixture-of-LoRAs', 'url': 'https://huggingface.co/papers/2503.01743', 'abstract': 'We introduce Phi-4-Mini and Phi-4-Multimodal, compact yet highly capable language and multimodal models. Phi-4-Mini is a 3.8-billion-parameter language model trained on high-quality web and synthetic data, significantly outperforming recent open-source models of similar size and matching the performance of models twice its size on math and coding tasks requiring complex reasoning. This achievement is driven by a carefully curated synthetic data recipe emphasizing high-quality math and coding datasets. Compared to its predecessor, Phi-3.5-Mini, Phi-4-Mini features an expanded vocabulary size of 200K tokens to better support multilingual applications, as well as group query attention for more efficient long-sequence generation. Phi-4-Multimodal is a multimodal model that integrates text, vision, and speech/audio input modalities into a single model. Its novel modality extension approach leverages LoRA adapters and modality-specific routers to allow multiple inference modes combining various modalities without interference. For example, it now ranks first in the OpenASR leaderboard to date, although the LoRA component of the speech/audio modality has just 460 million parameters. Phi-4-Multimodal supports scenarios involving (vision + language), (vision + speech), and (speech/audio) inputs, outperforming larger vision-language and speech-language models on a wide range of tasks. Additionally, we experiment to further train Phi-4-Mini to enhance its reasoning capabilities. Despite its compact 3.8-billion-parameter size, this experimental version achieves reasoning performance on par with or surpassing significantly larger models, including DeepSeek-R1-Distill-Qwen-7B and DeepSeek-R1-Distill-Llama-8B.', 'score': 38, 'issue_id': 2511, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': 'fb054d6547a4a4fb', 'authors': ['Abdelrahman Abouelenin', 'Atabak Ashfaq', 'Adam Atkinson', 'Hany Awadalla', 'Nguyen Bach', 'Jianmin Bao', 'Alon Benhaim', 'Martin Cai', 'Vishrav Chaudhary', 'Congcong Chen', 'Dong Chen', 'Dongdong Chen', 'Junkun Chen', 'Weizhu Chen', 'Yen-Chun Chen', 'Yi-ling Chen', 'Qi Dai', 'Xiyang Dai', 'Ruchao Fan', 'Mei Gao', 'Min Gao', 'Amit Garg', 'Abhishek Goswami', 'Junheng Hao', 'Amr Hendy', 'Yuxuan Hu', 'Xin Jin', 'Mahmoud Khademi', 'Dongwoo Kim', 'Young Jin Kim', 'Gina Lee', 'Jinyu Li', 'Yunsheng Li', 'Chen Liang', 'Xihui Lin', 'Zeqi Lin', 'Mengchen Liu', 'Yang Liu', 'Gilsinia Lopez', 'Chong Luo', 'Piyush Madan', 'Vadim Mazalov', 'Ali Mousavi', 'Anh Nguyen', 'Jing Pan', 'Daniel Perez-Becker', 'Jacob Platin', 'Thomas Portet', 'Kai Qiu', 'Bo Ren', 'Liliang Ren', 'Sambuddha Roy', 'Ning Shang', 'Yelong Shen', 'Saksham Singhal', 'Subhojit Som', 'Xia Song', 'Tetyana Sych', 'Praneetha Vaddamanu', 'Shuohang Wang', 'Yiming Wang', 'Zhenghao Wang', 'Haibin Wu', 'Haoran Xu', 'Weijian Xu', 'Yifan Yang', 'Ziyi Yang', 'Donghan Yu', 'Ishmam Zabir', 'Jianwen Zhang', 'Li Lyna Zhang', 'Yunan Zhang', 'Xiren Zhou'], 'affiliations': ['Microsoft'], 'pdf_title_img': 'assets/pdf/title_img/2503.01743.jpg', 'data': {'categories': ['#multimodal', '#small_models', '#data', '#agi', '#synthetic', '#long_context', '#optimization', '#dataset', '#training'], 'emoji': '🧠', 'ru': {'title': 'Компактные модели с большими возможностями: прорыв в эффективности ИИ', 'desc': 'Представлены две новые модели: Phi-4-Mini и Phi-4-Multimodal. Phi-4-Mini - это языковая модель с 3,8 миллиардами параметров, обученная на высококачественных веб-данных и синтетических данных, которая превосходит аналогичные модели в задачах математики и программирования. Phi-4-Multimodal - это мультимодальная модель, объединяющая текст, изображения и речь/аудио в единую систему с использованием LoRA-адаптеров. Обе модели демонстрируют высокую эффективность несмотря на свой компактный размер, превосходя более крупные аналоги в различных задачах.'}, 'en': {'title': 'Compact Models, Superior Performance!', 'desc': 'The paper presents Phi-4-Mini and Phi-4-Multimodal, two advanced models designed for language and multimodal tasks. Phi-4-Mini, with 3.8 billion parameters, excels in math and coding tasks by utilizing a high-quality synthetic data approach and an expanded vocabulary of 200K tokens. Phi-4-Multimodal integrates text, vision, and audio inputs, employing innovative techniques like LoRA adapters for efficient multi-modal processing. Both models demonstrate superior performance compared to larger counterparts, showcasing their effectiveness in complex reasoning and diverse input scenarios.'}, 'zh': {'title': '紧凑强大的多模态模型Phi-4系列', 'desc': '我们介绍了Phi-4-Mini和Phi-4-Multimodal这两种紧凑而强大的语言和多模态模型。Phi-4-Mini是一个拥有38亿参数的语言模型，经过高质量的网络和合成数据训练，在数学和编码任务中表现优于同类开源模型，并且在复杂推理方面与两倍于其规模的模型相当。相比于前身Phi-3.5-Mini，Phi-4-Mini扩展了词汇量，支持多语言应用，并采用了组查询注意力机制以提高长序列生成的效率。Phi-4-Multimodal则是一个多模态模型，能够将文本、视觉和语音/音频输入整合到一个模型中，支持多种推理模式，且在多个任务上超越了更大的视觉-语言和语音-语言模型。'}}}, {'id': 'https://huggingface.co/papers/2503.01774', 'title': 'Difix3D+: Improving 3D Reconstructions with Single-Step Diffusion Models', 'url': 'https://huggingface.co/papers/2503.01774', 'abstract': 'Neural Radiance Fields and 3D Gaussian Splatting have revolutionized 3D reconstruction and novel-view synthesis task. However, achieving photorealistic rendering from extreme novel viewpoints remains challenging, as artifacts persist across representations. In this work, we introduce Difix3D+, a novel pipeline designed to enhance 3D reconstruction and novel-view synthesis through single-step diffusion models. At the core of our approach is Difix, a single-step image diffusion model trained to enhance and remove artifacts in rendered novel views caused by underconstrained regions of the 3D representation. Difix serves two critical roles in our pipeline. First, it is used during the reconstruction phase to clean up pseudo-training views that are rendered from the reconstruction and then distilled back into 3D. This greatly enhances underconstrained regions and improves the overall 3D representation quality. More importantly, Difix also acts as a neural enhancer during inference, effectively removing residual artifacts arising from imperfect 3D supervision and the limited capacity of current reconstruction models. Difix3D+ is a general solution, a single model compatible with both NeRF and 3DGS representations, and it achieves an average 2times improvement in FID score over baselines while maintaining 3D consistency.', 'score': 29, 'issue_id': 2512, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': '39af2f882aef9afb', 'authors': ['Jay Zhangjie Wu', 'Yuxuan Zhang', 'Haithem Turki', 'Xuanchi Ren', 'Jun Gao', 'Mike Zheng Shou', 'Sanja Fidler', 'Zan Gojcic', 'Huan Ling'], 'affiliations': ['NVIDIA', 'National University of Singapore', 'University of Toronto', 'Vector Institute'], 'pdf_title_img': 'assets/pdf/title_img/2503.01774.jpg', 'data': {'categories': ['#3d', '#diffusion'], 'emoji': '🖼️', 'ru': {'title': 'Одношаговая диффузия для фотореалистичной 3D-реконструкции', 'desc': 'Difix3D+ - это новый подход к улучшению 3D-реконструкции и синтеза изображений с новых ракурсов. В его основе лежит Difix - одношаговая модель диффузии изображений, обученная улучшать и устранять артефакты в визуализированных видах. Difix используется как на этапе реконструкции для очистки псевдо-обучающих видов, так и во время вывода для устранения остаточных артефактов. Difix3D+ совместим с представлениями NeRF и 3DGS и показывает двукратное улучшение оценки FID по сравнению с базовыми моделями.'}, 'en': {'title': 'Enhancing 3D Reconstruction with Difix3D+', 'desc': 'This paper presents Difix3D+, a new method for improving 3D reconstruction and novel-view synthesis using single-step diffusion models. The core component, Difix, is an image diffusion model that enhances rendered views by removing artifacts caused by underconstrained areas in 3D representations. It plays a dual role by cleaning up pseudo-training views during reconstruction and acting as a neural enhancer during inference to eliminate residual artifacts. Difix3D+ is versatile, working with both Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS), and it significantly improves the quality of 3D representations, achieving a 2x better FID score compared to existing methods.'}, 'zh': {'title': 'Difix3D+: 提升3D重建与新视角合成的利器', 'desc': 'Neural Radiance Fields（NeRF）和3D高斯点云（3D Gaussian Splatting）在3D重建和新视角合成任务中取得了重大进展。然而，从极端新视角实现真实感渲染仍然面临挑战，因为在表示中存在伪影。我们提出了Difix3D+，这是一种新颖的管道，旨在通过单步扩散模型增强3D重建和新视角合成。Difix作为核心模型，能够在重建阶段清理伪训练视图，并在推理阶段去除残留伪影，从而显著提高3D表示的质量。'}}}, {'id': 'https://huggingface.co/papers/2503.01183', 'title': 'DiffRhythm: Blazingly Fast and Embarrassingly Simple End-to-End Full-Length Song Generation with Latent Diffusion', 'url': 'https://huggingface.co/papers/2503.01183', 'abstract': 'Recent advancements in music generation have garnered significant attention, yet existing approaches face critical limitations. Some current generative models can only synthesize either the vocal track or the accompaniment track. While some models can generate combined vocal and accompaniment, they typically rely on meticulously designed multi-stage cascading architectures and intricate data pipelines, hindering scalability. Additionally, most systems are restricted to generating short musical segments rather than full-length songs. Furthermore, widely used language model-based methods suffer from slow inference speeds. To address these challenges, we propose DiffRhythm, the first latent diffusion-based song generation model capable of synthesizing complete songs with both vocal and accompaniment for durations of up to 4m45s in only ten seconds, maintaining high musicality and intelligibility. Despite its remarkable capabilities, DiffRhythm is designed to be simple and elegant: it eliminates the need for complex data preparation, employs a straightforward model structure, and requires only lyrics and a style prompt during inference. Additionally, its non-autoregressive structure ensures fast inference speeds. This simplicity guarantees the scalability of DiffRhythm. Moreover, we release the complete training code along with the pre-trained model on large-scale data to promote reproducibility and further research.', 'score': 18, 'issue_id': 2516, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': '0370c6364610fd8e', 'authors': ['Ziqian Ning', 'Huakang Chen', 'Yuepeng Jiang', 'Chunbo Hao', 'Guobin Ma', 'Shuai Wang', 'Jixun Yao', 'Lei Xie'], 'affiliations': ['Northwestern Polytechnical University', 'Shenzhen Research Institute of Big Data, The Chinese University of Hong Kong, Shenzhen (CUHK-Shenzhen), China'], 'pdf_title_img': 'assets/pdf/title_img/2503.01183.jpg', 'data': {'categories': ['#diffusion', '#inference', '#dataset', '#open_source', '#audio'], 'emoji': '🎵', 'ru': {'title': 'DiffRhythm: Быстрая генерация полных песен с помощью латентной диффузии', 'desc': 'DiffRhythm - это первая модель генерации песен на основе латентной диффузии, способная синтезировать полные песни с вокалом и аккомпанементом длительностью до 4м45с всего за десять секунд. Модель имеет простую структуру, не требует сложной подготовки данных и использует только текст песни и стилевую подсказку при инференсе. Благодаря неавторегрессивной структуре, DiffRhythm обеспечивает высокую скорость генерации. Авторы опубликовали полный код обучения и предобученную модель для воспроизводимости результатов и дальнейших исследований.'}, 'en': {'title': 'DiffRhythm: Fast and Scalable Song Generation with Latent Diffusion', 'desc': "This paper introduces DiffRhythm, a novel music generation model that utilizes latent diffusion techniques to create full-length songs with both vocal and accompaniment tracks. Unlike existing models that are limited to short segments or require complex architectures, DiffRhythm simplifies the process by needing only lyrics and a style prompt for song generation. It achieves high musical quality and intelligibility while significantly improving inference speed, generating songs in just ten seconds. The authors also emphasize the model's scalability and reproducibility by providing the complete training code and pre-trained model for further research."}, 'zh': {'title': 'DiffRhythm：快速生成完整歌曲的创新模型', 'desc': '本论文介绍了一种新的音乐生成模型DiffRhythm，它能够在短短十秒内合成完整的歌曲，包括人声和伴奏，时长可达4分45秒。与现有模型相比，DiffRhythm采用潜在扩散技术，避免了复杂的数据准备和多阶段架构，确保了高效的推理速度。该模型只需歌词和风格提示即可生成音乐，具有良好的可扩展性。我们还发布了完整的训练代码和预训练模型，以促进研究的可重复性和进一步发展。'}}}, {'id': 'https://huggingface.co/papers/2502.18965', 'title': 'OneRec: Unifying Retrieve and Rank with Generative Recommender and Iterative Preference Alignment', 'url': 'https://huggingface.co/papers/2502.18965', 'abstract': "Recently, generative retrieval-based recommendation systems have emerged as a promising paradigm. However, most modern recommender systems adopt a retrieve-and-rank strategy, where the generative model functions only as a selector during the retrieval stage. In this paper, we propose OneRec, which replaces the cascaded learning framework with a unified generative model. To the best of our knowledge, this is the first end-to-end generative model that significantly surpasses current complex and well-designed recommender systems in real-world scenarios. Specifically, OneRec includes: 1) an encoder-decoder structure, which encodes the user's historical behavior sequences and gradually decodes the videos that the user may be interested in. We adopt sparse Mixture-of-Experts (MoE) to scale model capacity without proportionally increasing computational FLOPs. 2) a session-wise generation approach. In contrast to traditional next-item prediction, we propose a session-wise generation, which is more elegant and contextually coherent than point-by-point generation that relies on hand-crafted rules to properly combine the generated results. 3) an Iterative Preference Alignment module combined with Direct Preference Optimization (DPO) to enhance the quality of the generated results. Unlike DPO in NLP, a recommendation system typically has only one opportunity to display results for each user's browsing request, making it impossible to obtain positive and negative samples simultaneously. To address this limitation, We design a reward model to simulate user generation and customize the sampling strategy. Extensive experiments have demonstrated that a limited number of DPO samples can align user interest preferences and significantly improve the quality of generated results. We deployed OneRec in the main scene of Kuaishou, achieving a 1.6\\% increase in watch-time, which is a substantial improvement.", 'score': 18, 'issue_id': 2515, 'pub_date': '2025-02-26', 'pub_date_card': {'ru': '26 февраля', 'en': 'February 26', 'zh': '2月26日'}, 'hash': '21c5c80a138c98a0', 'authors': ['Jiaxin Deng', 'Shiyao Wang', 'Kuo Cai', 'Lejian Ren', 'Qigen Hu', 'Weifeng Ding', 'Qiang Luo', 'Guorui Zhou'], 'affiliations': ['KuaiShou Inc. Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2502.18965.jpg', 'data': {'categories': ['#alignment', '#rlhf', '#rag', '#games', '#training', '#optimization'], 'emoji': '🎥', 'ru': {'title': 'OneRec: Единая генеративная модель для революции в рекомендательных системах', 'desc': 'OneRec - это новая система рекомендаций, использующая единую генеративную модель вместо каскадного подхода. Она включает в себя структуру кодировщик-декодировщик с разреженной смесью экспертов (MoE) для масштабирования возможностей модели. OneRec применяет поэтапную генерацию сессий и модуль итеративного выравнивания предпочтений с прямой оптимизацией предпочтений (DPO). Система показала значительное улучшение времени просмотра при развертывании на платформе Kuaishou.'}, 'en': {'title': 'OneRec: Revolutionizing Recommendations with Generative Models', 'desc': 'This paper introduces OneRec, a novel generative retrieval-based recommendation system that improves upon traditional retrieve-and-rank methods. Unlike existing systems that use generative models merely for selection, OneRec employs a unified generative model that encodes user behavior and generates personalized video recommendations in a session-wise manner. The model utilizes a sparse Mixture-of-Experts architecture to enhance capacity while maintaining efficiency, and incorporates an Iterative Preference Alignment module to optimize user preferences effectively. Experimental results show that OneRec significantly outperforms existing systems, leading to a notable increase in user engagement metrics such as watch-time.'}, 'zh': {'title': 'OneRec：统一生成模型的推荐新范式', 'desc': '最近，基于生成检索的推荐系统成为一种有前景的范式。本文提出的OneRec模型，采用统一的生成模型，取代了传统的级联学习框架，能够在真实场景中显著超越现有复杂的推荐系统。OneRec包括编码-解码结构，能够有效编码用户的历史行为，并生成用户可能感兴趣的视频。此外，OneRec还引入了会话生成方法和迭代偏好对齐模块，提升了生成结果的质量，并在快手的实际应用中实现了观看时间的显著增加。'}}}, {'id': 'https://huggingface.co/papers/2503.01688', 'title': 'When an LLM is apprehensive about its answers -- and when its uncertainty is justified', 'url': 'https://huggingface.co/papers/2503.01688', 'abstract': 'Uncertainty estimation is crucial for evaluating Large Language Models (LLMs), particularly in high-stakes domains where incorrect answers result in significant consequences. Numerous approaches consider this problem, while focusing on a specific type of uncertainty, ignoring others. We investigate what estimates, specifically token-wise entropy and model-as-judge (MASJ), would work for multiple-choice question-answering tasks for different question topics. Our experiments consider three LLMs: Phi-4, Mistral, and Qwen of different sizes from 1.5B to 72B and 14 topics. While MASJ performs similarly to a random error predictor, the response entropy predicts model error in knowledge-dependent domains and serves as an effective indicator of question difficulty: for biology ROC AUC is 0.73. This correlation vanishes for the reasoning-dependent domain: for math questions ROC-AUC is 0.55. More principally, we found out that the entropy measure required a reasoning amount. Thus, data-uncertainty related entropy should be integrated within uncertainty estimates frameworks, while MASJ requires refinement. Moreover, existing MMLU-Pro samples are biased, and should balance required amount of reasoning for different subdomains to provide a more fair assessment of LLMs performance.', 'score': 16, 'issue_id': 2518, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': '68429d7977c57eae', 'authors': ['Petr Sychev', 'Andrey Goncharov', 'Daniil Vyazhev', 'Edvard Khalafyan', 'Alexey Zaytsev'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2503.01688.jpg', 'data': {'categories': ['#ethics', '#hallucinations', '#benchmark', '#reasoning', '#data', '#multilingual'], 'emoji': '🤖', 'ru': {'title': 'Энтропия ответов как индикатор неопределенности LLM в задачах с множественным выбором', 'desc': "Исследование посвящено оценке неопределенности в крупных языковых моделях (LLM) при решении задач с множественным выбором. Авторы сравнивают эффективность энтропии токенов и метода 'модель как судья' (MASJ) для различных тем вопросов. Эксперименты проводились на трех LLM разных размеров и 14 темах. Результаты показывают, что энтропия ответов хорошо предсказывает ошибки модели в областях, зависящих от знаний, но не в областях, требующих рассуждений."}, 'en': {'title': 'Enhancing Uncertainty Estimation in LLMs for Better Decision-Making', 'desc': 'This paper explores how to measure uncertainty in Large Language Models (LLMs) when answering multiple-choice questions, which is important in critical areas where wrong answers can have serious effects. It compares two methods of uncertainty estimation: token-wise entropy and model-as-judge (MASJ), across various LLMs and topics. The findings reveal that while MASJ does not effectively predict errors, token-wise entropy is a better indicator of question difficulty, especially in knowledge-based subjects like biology. The study also highlights the need to refine MASJ and address biases in existing datasets to ensure fair evaluation of LLM performance across different reasoning requirements.'}, 'zh': {'title': '提升大型语言模型的不确定性估计', 'desc': '不确定性估计对于评估大型语言模型（LLMs）至关重要，尤其是在错误答案可能导致重大后果的高风险领域。本文探讨了不同类型的不确定性估计，特别是基于令牌的熵和模型作为评判者（MASJ），在多选题回答任务中的有效性。实验涉及三种不同规模的LLMs，结果显示，响应熵在知识依赖领域能够有效预测模型错误，而MASJ的表现类似于随机错误预测器。我们发现熵度量需要一定的推理量，因此数据不确定性相关的熵应纳入不确定性估计框架中，而MASJ则需要进一步改进。'}}}, {'id': 'https://huggingface.co/papers/2503.01496', 'title': 'Liger: Linearizing Large Language Models to Gated Recurrent Structures', 'url': 'https://huggingface.co/papers/2503.01496', 'abstract': 'Transformers with linear recurrent modeling offer linear-time training and constant-memory inference. Despite their demonstrated efficiency and performance, pretraining such non-standard architectures from scratch remains costly and risky. The linearization of large language models (LLMs) transforms pretrained standard models into linear recurrent structures, enabling more efficient deployment. However, current linearization methods typically introduce additional feature map modules that require extensive fine-tuning and overlook the gating mechanisms used in state-of-the-art linear recurrent models. To address these issues, this paper presents Liger, short for Linearizing LLMs to gated recurrent structures. Liger is a novel approach for converting pretrained LLMs into gated linear recurrent models without adding extra parameters. It repurposes the pretrained key matrix weights to construct diverse gating mechanisms, facilitating the formation of various gated recurrent structures while avoiding the need to train additional components from scratch. Using lightweight fine-tuning with Low-Rank Adaptation (LoRA), Liger restores the performance of the linearized gated recurrent models to match that of the original LLMs. Additionally, we introduce Liger Attention, an intra-layer hybrid attention mechanism, which significantly recovers 93\\% of the Transformer-based LLM at 0.02\\% pre-training tokens during the linearization process, achieving competitive results across multiple benchmarks, as validated on models ranging from 1B to 8B parameters. Code is available at https://github.com/OpenSparseLLMs/Linearization.', 'score': 13, 'issue_id': 2514, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': 'd5ca7ef45c0e90c9', 'authors': ['Disen Lan', 'Weigao Sun', 'Jiaxi Hu', 'Jusen Du', 'Yu Cheng'], 'affiliations': ['Nanjing University', 'Shanghai AI Laboratory', 'South China University of Technology', 'The Chinese University of Hong Kong', 'The Hong Kong University of Science and Technology (Guangzhou)'], 'pdf_title_img': 'assets/pdf/title_img/2503.01496.jpg', 'data': {'categories': ['#architecture', '#training', '#optimization', '#benchmark'], 'emoji': '🔢', 'ru': {'title': 'Эффективная линеаризация больших языковых моделей', 'desc': 'Данная статья представляет новый метод Liger для линеаризации больших языковых моделей (LLM) в гейтированные линейно-рекуррентные структуры. Liger преобразует предобученные LLM без добавления дополнительных параметров, используя существующие веса ключевой матрицы для создания различных механизмов гейтирования. Метод применяет легковесную донастройку с помощью Low-Rank Adaptation (LoRA) для восстановления производительности линеаризованных моделей. Авторы также представляют Liger Attention - гибридный механизм внимания, который значительно улучшает эффективность линеаризации.'}, 'en': {'title': 'Liger: Efficiently Transforming LLMs into Gated Linear Recurrent Models', 'desc': "This paper introduces Liger, a method for transforming pretrained large language models (LLMs) into gated linear recurrent models. Liger efficiently repurposes existing key matrix weights to create diverse gating mechanisms without adding extra parameters, thus avoiding the costly process of training new components from scratch. The approach employs lightweight fine-tuning techniques, specifically Low-Rank Adaptation (LoRA), to maintain the performance of the linearized models comparable to the original LLMs. Additionally, Liger incorporates a novel intra-layer hybrid attention mechanism, Liger Attention, which enhances the model's efficiency while achieving competitive results across various benchmarks."}, 'zh': {'title': 'Liger：高效转换预训练模型的创新方法', 'desc': '本文提出了一种名为Liger的方法，用于将预训练的大型语言模型（LLMs）转换为带门控的线性递归模型，而无需增加额外的参数。Liger通过重新利用预训练的关键矩阵权重，构建多样的门控机制，从而形成不同的门控递归结构。该方法使用轻量级的微调技术（如低秩适应LoRA），使线性化的门控递归模型的性能恢复到与原始LLMs相当的水平。此外，Liger Attention作为一种层内混合注意力机制，在线性化过程中显著恢复了93%的Transformer基础LLM的性能。'}}}, {'id': 'https://huggingface.co/papers/2503.01307', 'title': 'Cognitive Behaviors that Enable Self-Improving Reasoners, or, Four Habits of Highly Effective STaRs', 'url': 'https://huggingface.co/papers/2503.01307', 'abstract': "Test-time inference has emerged as a powerful paradigm for enabling language models to ``think'' longer and more carefully about complex challenges, much like skilled human experts. While reinforcement learning (RL) can drive self-improvement in language models on verifiable tasks, some models exhibit substantial gains while others quickly plateau. For instance, we find that Qwen-2.5-3B far exceeds Llama-3.2-3B under identical RL training for the game of Countdown. This discrepancy raises a critical question: what intrinsic properties enable effective self-improvement? We introduce a framework to investigate this question by analyzing four key cognitive behaviors -- verification, backtracking, subgoal setting, and backward chaining -- that both expert human problem solvers and successful language models employ. Our study reveals that Qwen naturally exhibits these reasoning behaviors, whereas Llama initially lacks them. In systematic experimentation with controlled behavioral datasets, we find that priming Llama with examples containing these reasoning behaviors enables substantial improvements during RL, matching or exceeding Qwen's performance. Importantly, the presence of reasoning behaviors, rather than correctness of answers, proves to be the critical factor -- models primed with incorrect solutions containing proper reasoning patterns achieve comparable performance to those trained on correct solutions. Finally, leveraging continued pretraining with OpenWebMath data, filtered to amplify reasoning behaviors, enables the Llama model to match Qwen's self-improvement trajectory. Our findings establish a fundamental relationship between initial reasoning behaviors and the capacity for improvement, explaining why some language models effectively utilize additional computation while others plateau.", 'score': 13, 'issue_id': 2511, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': 'fa966620baa8c013', 'authors': ['Kanishk Gandhi', 'Ayush Chakravarthy', 'Anikait Singh', 'Nathan Lile', 'Noah D. Goodman'], 'affiliations': ['Stanford University', 'SynthLabs'], 'pdf_title_img': 'assets/pdf/title_img/2503.01307.jpg', 'data': {'categories': ['#training', '#optimization', '#rl', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Когнитивные навыки - ключ к самосовершенствованию языковых моделей', 'desc': 'Исследование показывает, что способность языковых моделей к самосовершенствованию зависит от наличия у них определенных когнитивных поведений, таких как верификация, бэктрекинг, постановка подцелей и обратное планирование. Эксперименты выявили, что модель Qwen изначально обладает этими навыками, в то время как Llama нет. Прайминг Llama примерами, содержащими эти поведения, позволил значительно улучшить ее производительность при обучении с подкреплением. Важно отметить, что наличие правильных рассуждений оказалось более критичным фактором, чем корректность ответов.'}, 'en': {'title': 'Unlocking Self-Improvement in Language Models through Reasoning', 'desc': 'This paper explores how language models can improve their problem-solving abilities through a process called test-time inference, similar to human experts. It highlights the differences in performance between two models, Qwen-2.5-3B and Llama-3.2-3B, when trained with reinforcement learning (RL) on the game Countdown. The authors identify four cognitive behaviors—verification, backtracking, subgoal setting, and backward chaining—that are crucial for effective self-improvement in these models. They demonstrate that enhancing Llama with examples of these reasoning behaviors can significantly boost its performance, suggesting that the ability to reason is more important than simply providing correct answers.'}, 'zh': {'title': '推理行为是模型自我提升的关键', 'desc': '本文探讨了语言模型在复杂任务中自我改进的能力，特别是通过强化学习（RL）实现的自我提升。研究发现，不同模型在相同的RL训练下表现差异显著，例如Qwen-2.5-3B在游戏Countdown中远超Llama-3.2-3B。我们分析了四种关键的认知行为：验证、回溯、子目标设定和逆向链推理，发现Qwen自然展现了这些推理行为，而Llama则最初缺乏。通过对Llama进行示例引导，能够显著提升其在RL中的表现，证明了推理行为的存在是模型自我改进的关键因素。'}}}, {'id': 'https://huggingface.co/papers/2503.00501', 'title': 'Qilin: A Multimodal Information Retrieval Dataset with APP-level User Sessions', 'url': 'https://huggingface.co/papers/2503.00501', 'abstract': "User-generated content (UGC) communities, especially those featuring multimodal content, improve user experiences by integrating visual and textual information into results (or items). The challenge of improving user experiences in complex systems with search and recommendation (S\\&R) services has drawn significant attention from both academia and industry these years. However, the lack of high-quality datasets has limited the research progress on multimodal S\\&R. To address the growing need for developing better S\\&R services, we present a novel multimodal information retrieval dataset in this paper, namely Qilin. The dataset is collected from Xiaohongshu, a popular social platform with over 300 million monthly active users and an average search penetration rate of over 70\\%. In contrast to existing datasets, Qilin offers a comprehensive collection of user sessions with heterogeneous results like image-text notes, video notes, commercial notes, and direct answers, facilitating the development of advanced multimodal neural retrieval models across diverse task settings. To better model user satisfaction and support the analysis of heterogeneous user behaviors, we also collect extensive APP-level contextual signals and genuine user feedback. Notably, Qilin contains user-favored answers and their referred results for search requests triggering the Deep Query Answering (DQA) module. This allows not only the training \\& evaluation of a Retrieval-augmented Generation (RAG) pipeline, but also the exploration of how such a module would affect users' search behavior. Through comprehensive analysis and experiments, we provide interesting findings and insights for further improving S\\&R systems. We hope that Qilin will significantly contribute to the advancement of multimodal content platforms with S\\&R services in the future.", 'score': 10, 'issue_id': 2513, 'pub_date': '2025-03-01', 'pub_date_card': {'ru': '1 марта', 'en': 'March 1', 'zh': '3月1日'}, 'hash': 'ed7fc8625b068597', 'authors': ['Jia Chen', 'Qian Dong', 'Haitao Li', 'Xiaohui He', 'Yan Gao', 'Shaosheng Cao', 'Yi Wu', 'Ping Yang', 'Chen Xu', 'Yao Hu', 'Qingyao Ai', 'Yiqun Liu'], 'affiliations': ['Tsinghua University', 'Xiaohongshu Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2503.00501.jpg', 'data': {'categories': ['#multimodal', '#dataset', '#rag'], 'emoji': '🔍', 'ru': {'title': 'Qilin: мультимодальный датасет для улучшения поиска и рекомендаций', 'desc': 'Представлен новый набор данных Qilin для мультимодального информационного поиска, собранный на платформе Xiaohongshu. Датасет включает пользовательские сессии с разнородными результатами (изображения, видео, коммерческие заметки) и контекстуальными сигналами. Qilin позволяет обучать и оценивать нейросетевые модели поиска и рекомендаций, а также исследовать влияние модуля глубоких ответов на запросы. Авторы надеются, что Qilin внесет значительный вклад в развитие мультимодальных платформ с поисковыми сервисами.'}, 'en': {'title': 'Enhancing User Experiences with Qilin: A Multimodal Dataset for S&R Services', 'desc': 'This paper introduces Qilin, a new multimodal information retrieval dataset designed to enhance search and recommendation (S&R) services in user-generated content communities. Qilin is unique as it includes diverse user sessions with various content types, such as image-text notes and videos, which can help in developing advanced multimodal neural retrieval models. Additionally, the dataset captures user feedback and contextual signals, allowing researchers to analyze user satisfaction and behavior more effectively. The findings from this research aim to improve S&R systems and contribute to the evolution of multimodal content platforms.'}, 'zh': {'title': '推动多模态搜索与推荐服务的进步', 'desc': '本文介绍了一个新的多模态信息检索数据集Qilin，旨在改善用户在复杂系统中的搜索和推荐体验。Qilin数据集来源于小红书，包含多种类型的用户会话，如图文笔记、视频笔记和商业笔记，适用于多种任务设置。该数据集还收集了丰富的应用级上下文信号和真实用户反馈，以更好地建模用户满意度。通过对Qilin的分析和实验，本文提供了有趣的发现，期望能推动多模态内容平台的搜索和推荐服务的发展。'}}}, {'id': 'https://huggingface.co/papers/2503.00714', 'title': 'Speculative Ad-hoc Querying', 'url': 'https://huggingface.co/papers/2503.00714', 'abstract': "Analyzing large datasets requires responsive query execution, but executing SQL queries on massive datasets can be slow. This paper explores whether query execution can begin even before the user has finished typing, allowing results to appear almost instantly. We propose SpeQL, a system that leverages Large Language Models (LLMs) to predict likely queries based on the database schema, the user's past queries, and their incomplete query. Since exact query prediction is infeasible, SpeQL speculates on partial queries in two ways: 1) it predicts the query structure to compile and plan queries in advance, and 2) it precomputes smaller temporary tables that are much smaller than the original database, but are still predicted to contain all information necessary to answer the user's final query. Additionally, SpeQL continuously displays results for speculated queries and subqueries in real time, aiding exploratory analysis. A utility/user study showed that SpeQL improved task completion time, and participants reported that its speculative display of results helped them discover patterns in the data more quickly. In the study, SpeQL improves user's query latency by up to 289times and kept the overhead reasonable, at 4$ per hour.", 'score': 8, 'issue_id': 2514, 'pub_date': '2025-03-02', 'pub_date_card': {'ru': '2 марта', 'en': 'March 2', 'zh': '3月2日'}, 'hash': '1b0459b56fdb6894', 'authors': ['Haoyu Li', 'Srikanth Kandula', 'Maria Angels de Luis Balaguer', 'Aditya Akella', 'Venkat Arun'], 'affiliations': ['Amazon Web Services', 'Microsoft Research', 'The University of Texas at Austin'], 'pdf_title_img': 'assets/pdf/title_img/2503.00714.jpg', 'data': {'categories': ['#dataset', '#data', '#benchmark'], 'emoji': '⚡', 'ru': {'title': 'Молниеносные SQL-запросы с помощью предиктивной аналитики', 'desc': 'Статья представляет систему SpeQL, использующую большие языковые модели для предсказания SQL-запросов пользователя. SpeQL предугадывает структуру запроса и предварительно вычисляет временные таблицы, что позволяет начать выполнение запроса до его завершения пользователем. Система непрерывно отображает результаты предполагаемых запросов в реальном времени, помогая в исследовательском анализе данных. Исследование показало, что SpeQL значительно сокращает время выполнения задач и помогает пользователям быстрее обнаруживать закономерности в данных.'}, 'en': {'title': 'Instant Query Results with SpeQL!', 'desc': 'This paper introduces SpeQL, a novel system designed to enhance the speed of SQL query execution on large datasets. By utilizing Large Language Models (LLMs), SpeQL predicts user queries even before they are fully typed, allowing for near-instantaneous results. It employs two main strategies: predicting the structure of queries for pre-compilation and creating smaller temporary tables that contain essential data for answering the final query. A user study demonstrated that SpeQL significantly reduced query latency and helped users identify data patterns more efficiently during exploratory analysis.'}, 'zh': {'title': 'SpeQL：让查询更快的智能预测系统', 'desc': '本论文探讨了如何在用户输入SQL查询时，提前开始执行查询，以加快大数据集的查询响应速度。我们提出了SpeQL系统，利用大型语言模型（LLMs）根据数据库模式、用户的历史查询和不完整查询来预测可能的查询。SpeQL通过预测查询结构和预计算小型临时表来处理部分查询，从而在用户完成查询之前提供实时结果。研究表明，SpeQL显著提高了用户的查询速度，并帮助用户更快地发现数据中的模式。'}}}, {'id': 'https://huggingface.co/papers/2503.00784', 'title': 'DuoDecoding: Hardware-aware Heterogeneous Speculative Decoding with Dynamic Multi-Sequence Drafting', 'url': 'https://huggingface.co/papers/2503.00784', 'abstract': 'Large language models (LLMs) exhibit exceptional performance across a wide range of tasks; however, their token-by-token autoregressive generation process significantly hinders inference speed. Speculative decoding presents a promising draft-then-verify framework that reduces generation latency while maintaining output distribution fidelity. Nevertheless, the draft model introduces additional computational overhead, becoming a performance bottleneck and increasing the time to first token (TTFT). Previous approaches to mitigate draft model overhead have primarily relied on heuristics and generally failed to match the quality of the draft language models. To address these challenges, we propose DuoDecoding, a novel approach that strategically deploys the draft and target models on the CPU and GPU respectively, enabling parallel decoding while preserving draft quality. Our method incorporates a hardware-aware optimal draft budget to minimize idle times and employs dynamic multi-sequence drafting to enhance draft quality. Extensive experiments across seven tasks show that DuoDecoding achieves up to 2.61x speedup in generation latency, while reducing TTFT to 83% of that in conventional speculative decoding. The Code is available at https://github.com/KaiLv69/DuoDecoding.', 'score': 8, 'issue_id': 2510, 'pub_date': '2025-03-02', 'pub_date_card': {'ru': '2 марта', 'en': 'March 2', 'zh': '3月2日'}, 'hash': 'b4870a0e44c3cc55', 'authors': ['Kai Lv', 'Honglin Guo', 'Qipeng Guo', 'Xipeng Qiu'], 'affiliations': ['Fudan University', 'Shanghai AI Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2503.00784.jpg', 'data': {'categories': ['#inference', '#training', '#optimization'], 'emoji': '🚀', 'ru': {'title': 'DuoDecoding: Параллельное ускорение языковых моделей', 'desc': 'Статья представляет новый метод ускорения генерации текста большими языковыми моделями (LLM) под названием DuoDecoding. Этот подход использует параллельное декодирование на CPU и GPU, оптимизируя время генерации первого токена и общую латентность. DuoDecoding применяет аппаратно-ориентированный оптимальный бюджет черновика и динамическое многопоследовательное черновое декодирование для повышения качества. Эксперименты показали значительное ускорение генерации по сравнению с обычным спекулятивным декодированием.'}, 'en': {'title': 'DuoDecoding: Speeding Up Text Generation with Smart Model Deployment', 'desc': 'This paper introduces DuoDecoding, a new method to improve the speed of generating text with large language models (LLMs) while keeping the quality high. It uses a draft-then-verify approach, where a draft model quickly generates initial text, and a target model refines it, but does so in a way that reduces the time it takes to start generating text. By using both CPU and GPU for different parts of the process, DuoDecoding allows for faster and more efficient decoding. The results show that this method can significantly speed up text generation without sacrificing quality, achieving a notable improvement in performance across various tasks.'}, 'zh': {'title': 'DuoDecoding：加速生成的新方法', 'desc': '大型语言模型（LLMs）在多种任务中表现出色，但其逐字自回归生成过程显著影响推理速度。推测解码提供了一种有前景的草稿-验证框架，能够减少生成延迟，同时保持输出分布的准确性。我们提出的DuoDecoding方法通过在CPU和GPU上分别部署草稿模型和目标模型，实现了并行解码，提升了生成效率。实验结果表明，DuoDecoding在生成延迟上实现了最高2.61倍的加速，同时将首次生成时间缩短至传统推测解码的83%。'}}}, {'id': 'https://huggingface.co/papers/2503.01506', 'title': 'SampleMix: A Sample-wise Pre-training Data Mixing Strategey by Coordinating Data Quality and Diversity', 'url': 'https://huggingface.co/papers/2503.01506', 'abstract': "Existing pretraining data mixing methods for large language models (LLMs) typically follow a domain-wise methodology, a top-down process that first determines domain weights and then performs uniform data sampling across each domain. However, these approaches neglect significant inter-domain overlaps and commonalities, failing to control the global diversity of the constructed training dataset. Further, uniform sampling within domains ignores fine-grained sample-specific features, potentially leading to suboptimal data distribution. To address these shortcomings, we propose a novel sample-wise data mixture approach based on a bottom-up paradigm. This method performs global cross-domain sampling by systematically evaluating the quality and diversity of each sample, thereby dynamically determining the optimal domain distribution. Comprehensive experiments across multiple downstream tasks and perplexity assessments demonstrate that SampleMix surpasses existing domain-based methods. Meanwhile, SampleMix requires 1.4x to 2.1x training steps to achieves the baselines' performance, highlighting the substantial potential of SampleMix to optimize pre-training data.", 'score': 7, 'issue_id': 2517, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': '018cc621eb1ee12b', 'authors': ['Xiangyu Xi', 'Deyang Kong', 'Jian Yang', 'Jiawei Yang', 'Zhengyu Chen', 'Wei Wang', 'Jingang Wang', 'Xunliang Cai', 'Shikun Zhang', 'Wei Ye'], 'affiliations': ['Meituan Group, Beijing, China', 'National Engineering Research Center for Software Engineering, Peking University, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.01506.jpg', 'data': {'categories': ['#transfer_learning', '#training', '#optimization', '#data'], 'emoji': '🔀', 'ru': {'title': 'SampleMix: революция в смешивании данных для LLM', 'desc': 'В статье представлен новый подход к смешиванию предобучающих данных для больших языковых моделей (LLM), названный SampleMix. В отличие от традиционных методов, основанных на доменах, SampleMix использует выборку на уровне отдельных образцов, оценивая их качество и разнообразие. Этот метод позволяет динамически определять оптимальное распределение доменов и учитывать межdomенные пересечения. Эксперименты показали, что SampleMix превосходит существующие методы, основанные на доменах, хотя и требует больше шагов обучения.'}, 'en': {'title': 'Revolutionizing Data Mixing for Better Language Model Training', 'desc': 'This paper introduces SampleMix, a new method for mixing pretraining data for large language models (LLMs). Unlike traditional domain-wise approaches that sample uniformly within predefined domains, SampleMix uses a bottom-up strategy that evaluates the quality and diversity of individual samples across domains. This allows for a more dynamic and optimal distribution of training data, addressing the limitations of inter-domain overlaps and sample-specific features. Experimental results show that SampleMix not only outperforms existing methods but also requires fewer training steps to achieve comparable performance.'}, 'zh': {'title': '样本级数据混合，优化预训练数据的未来', 'desc': '现有的大型语言模型预训练数据混合方法通常采用领域导向的方法，先确定领域权重，再在每个领域内进行均匀数据采样。然而，这些方法忽视了领域之间的重要重叠和共性，未能有效控制训练数据集的全球多样性。此外，领域内的均匀采样忽略了样本特定的细微特征，可能导致数据分布不理想。为了解决这些问题，我们提出了一种基于自下而上的新型样本级数据混合方法，能够通过系统评估每个样本的质量和多样性，动态确定最佳领域分布。'}}}, {'id': 'https://huggingface.co/papers/2502.18890', 'title': 'From Hours to Minutes: Lossless Acceleration of Ultra Long Sequence Generation up to 100K Tokens', 'url': 'https://huggingface.co/papers/2502.18890', 'abstract': "Generating ultra-long sequences with large language models (LLMs) has become increasingly crucial but remains a highly time-intensive task, particularly for sequences up to 100K tokens. While traditional speculative decoding methods exist, simply extending their generation limits fails to accelerate the process and can be detrimental. Through an in-depth analysis, we identify three major challenges hindering efficient generation: frequent model reloading, dynamic key-value (KV) management and repetitive generation. To address these issues, we introduce TOKENSWIFT, a novel framework designed to substantially accelerate the generation process of ultra-long sequences while maintaining the target model's inherent quality. Experimental results demonstrate that TOKENSWIFT achieves over 3 times speedup across models of varying scales (1.5B, 7B, 8B, 14B) and architectures (MHA, GQA). This acceleration translates to hours of time savings for ultra-long sequence generation, establishing TOKENSWIFT as a scalable and effective solution at unprecedented lengths. Code can be found at https://github.com/bigai-nlco/TokenSwift.", 'score': 7, 'issue_id': 2517, 'pub_date': '2025-02-26', 'pub_date_card': {'ru': '26 февраля', 'en': 'February 26', 'zh': '2月26日'}, 'hash': 'd07c05abfac49ecc', 'authors': ['Tong Wu', 'Junzhe Shen', 'Zixia Jia', 'Yuxuan Wang', 'Zilong Zheng'], 'affiliations': ['NLCo Lab, BIGAI LUMIA Lab, Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2502.18890.jpg', 'data': {'categories': ['#training', '#architecture', '#long_context', '#inference', '#optimization'], 'emoji': '⚡', 'ru': {'title': 'TOKENSWIFT: революция в скорости генерации сверхдлинных текстов', 'desc': 'Исследователи представили TOKENSWIFT - новую систему для ускорения генерации сверхдлинных последовательностей большими языковыми моделями (LLM). Они выявили три основные проблемы, препятствующие эффективной генерации: частая перезагрузка модели, динамическое управление ключами-значениями и повторяющаяся генерация. TOKENSWIFT решает эти проблемы, позволяя ускорить процесс генерации в 3 раза для моделей различных масштабов и архитектур. Это существенно сокращает время генерации сверхдлинных последовательностей, сохраняя при этом качество целевой модели.'}, 'en': {'title': 'Accelerating Ultra-Long Sequence Generation with TOKENSWIFT', 'desc': 'This paper presents TOKENSWIFT, a new framework aimed at speeding up the generation of ultra-long sequences using large language models (LLMs). The authors identify key challenges such as model reloading, dynamic key-value management, and repetitive generation that slow down the process. By addressing these issues, TOKENSWIFT achieves over three times the speed of traditional methods while preserving the quality of the generated text. Experimental results show that this framework is effective across various model sizes and architectures, making it a significant advancement in the field of sequence generation.'}, 'zh': {'title': 'TOKENSWIFT：加速超长序列生成的创新框架', 'desc': '生成超长序列对于大型语言模型（LLMs）变得越来越重要，但这一过程通常非常耗时，尤其是对于长达10万标记的序列。传统的推测解码方法在延长生成限制时并未加速过程，反而可能造成负面影响。我们通过深入分析，识别出影响高效生成的三个主要挑战：频繁的模型重载、动态键值（KV）管理和重复生成。为了解决这些问题，我们提出了TOKENSWIFT，一个新框架，旨在显著加快超长序列的生成过程，同时保持目标模型的固有质量。'}}}, {'id': 'https://huggingface.co/papers/2503.01370', 'title': 'Kiss3DGen: Repurposing Image Diffusion Models for 3D Asset Generation', 'url': 'https://huggingface.co/papers/2503.01370', 'abstract': "Diffusion models have achieved great success in generating 2D images. However, the quality and generalizability of 3D content generation remain limited. State-of-the-art methods often require large-scale 3D assets for training, which are challenging to collect. In this work, we introduce Kiss3DGen (Keep It Simple and Straightforward in 3D Generation), an efficient framework for generating, editing, and enhancing 3D objects by repurposing a well-trained 2D image diffusion model for 3D generation. Specifically, we fine-tune a diffusion model to generate ''3D Bundle Image'', a tiled representation composed of multi-view images and their corresponding normal maps. The normal maps are then used to reconstruct a 3D mesh, and the multi-view images provide texture mapping, resulting in a complete 3D model. This simple method effectively transforms the 3D generation problem into a 2D image generation task, maximizing the utilization of knowledge in pretrained diffusion models. Furthermore, we demonstrate that our Kiss3DGen model is compatible with various diffusion model techniques, enabling advanced features such as 3D editing, mesh and texture enhancement, etc. Through extensive experiments, we demonstrate the effectiveness of our approach, showcasing its ability to produce high-quality 3D models efficiently.", 'score': 7, 'issue_id': 2513, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': '3decc9fe2b6f6e32', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#cv', '#diffusion', '#3d'], 'emoji': '🎨', 'ru': {'title': 'Простая и эффективная 3D-генерация на основе 2D-диффузии', 'desc': "Статья представляет Kiss3DGen - эффективный фреймворк для генерации, редактирования и улучшения 3D-объектов с использованием предобученной модели диффузии для 2D-изображений. Метод основан на дообучении диффузионной модели для генерации 'Пакетного 3D-изображения', состоящего из мультиракурсных изображений и соответствующих карт нормалей. Затем карты нормалей используются для реконструкции 3D-меша, а мультиракурсные изображения обеспечивают текстурирование, что в результате дает полную 3D-модель. Авторы демонстрируют, что их подход совместим с различными техниками диффузионных моделей и позволяет эффективно создавать качественные 3D-модели."}, 'en': {'title': 'Kiss3DGen: Simplifying 3D Generation with 2D Diffusion Models', 'desc': "This paper presents Kiss3DGen, a novel framework that simplifies the process of generating and enhancing 3D objects by leveraging existing 2D image diffusion models. The approach involves fine-tuning a diffusion model to create a '3D Bundle Image', which consists of multiple views and normal maps that are essential for 3D reconstruction. By transforming the 3D generation challenge into a 2D image task, the method maximizes the use of knowledge from pretrained models, making it more efficient. The results show that Kiss3DGen not only generates high-quality 3D models but also supports advanced features like editing and texture enhancement."}, 'zh': {'title': '简单高效的三维生成方法', 'desc': '扩散模型在生成二维图像方面取得了巨大成功，但在三维内容生成的质量和通用性上仍然有限。现有的先进方法通常需要大量的三维资产进行训练，这些资产难以收集。我们提出了Kiss3DGen（简单直接的三维生成），这是一个高效的框架，通过重新利用经过良好训练的二维图像扩散模型来生成、编辑和增强三维物体。该方法将三维生成问题转化为二维图像生成任务，最大化利用预训练扩散模型中的知识，能够有效生成高质量的三维模型。'}}, 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2503.00455', 'title': 'PodAgent: A Comprehensive Framework for Podcast Generation', 'url': 'https://huggingface.co/papers/2503.00455', 'abstract': "Existing Existing automatic audio generation methods struggle to generate podcast-like audio programs effectively. The key challenges lie in in-depth content generation, appropriate and expressive voice production. This paper proposed PodAgent, a comprehensive framework for creating audio programs. PodAgent 1) generates informative topic-discussion content by designing a Host-Guest-Writer multi-agent collaboration system, 2) builds a voice pool for suitable voice-role matching and 3) utilizes LLM-enhanced speech synthesis method to generate expressive conversational speech. Given the absence of standardized evaluation criteria for podcast-like audio generation, we developed comprehensive assessment guidelines to effectively evaluate the model's performance. Experimental results demonstrate PodAgent's effectiveness, significantly surpassing direct GPT-4 generation in topic-discussion dialogue content, achieving an 87.4% voice-matching accuracy, and producing more expressive speech through LLM-guided synthesis. Demo page: https://podcast-agent.github.io/demo/. Source code: https://github.com/yujxx/PodAgent.", 'score': 5, 'issue_id': 2519, 'pub_date': '2025-03-01', 'pub_date_card': {'ru': '1 марта', 'en': 'March 1', 'zh': '3月1日'}, 'hash': '59ce5f373a030894', 'authors': ['Yujia Xiao', 'Lei He', 'Haohan Guo', 'Fenglong Xie', 'Tan Lee'], 'affiliations': ['Microsoft', 'The Chinese University of Hong Kong', 'Xiaohongshu Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2503.00455.jpg', 'data': {'categories': ['#games', '#audio', '#interpretability', '#benchmark', '#optimization', '#multimodal'], 'emoji': '🎙️', 'ru': {'title': 'PodAgent: ИИ-ведущий для подкастов нового поколения', 'desc': 'PodAgent - это новая система для автоматического создания аудиопрограмм в стиле подкастов. Она использует мультиагентный подход для генерации содержательного контента, подбирает подходящие голоса из голосового пула и применяет улучшенный синтез речи на основе языковых моделей. Система решает ключевые проблемы существующих методов, такие как глубина контента и выразительность голоса. Эксперименты показали значительное превосходство PodAgent над прямой генерацией GPT-4 по качеству диалогов и точности подбора голосов.'}, 'en': {'title': 'Revolutionizing Podcast Audio Generation with PodAgent', 'desc': "This paper introduces PodAgent, a novel framework designed to enhance the generation of podcast-like audio programs. It addresses key challenges in content creation and voice production by employing a multi-agent system that includes a Host, Guest, and Writer for collaborative topic discussions. Additionally, PodAgent features a voice pool for effective voice-role matching and utilizes a large language model (LLM) to improve the expressiveness of the generated speech. The framework's performance is validated through comprehensive evaluation guidelines, showing significant improvements over existing methods, including a high voice-matching accuracy and more engaging conversational audio."}, 'zh': {'title': 'PodAgent：智能生成播客音频的全新框架', 'desc': '本论文提出了一种名为PodAgent的框架，用于自动生成类似播客的音频节目。PodAgent通过设计一个主持人-嘉宾-编剧的多智能体协作系统，生成有深度的主题讨论内容。同时，它建立了一个声音库，以实现合适的声音角色匹配，并利用增强型大语言模型（LLM）进行富有表现力的语音合成。实验结果表明，PodAgent在主题讨论对话内容生成方面显著优于直接使用GPT-4，语音匹配准确率达到87.4%，并通过LLM引导的合成生成了更具表现力的语音。'}}}, {'id': 'https://huggingface.co/papers/2503.01295', 'title': 'CodeArena: A Collective Evaluation Platform for LLM Code Generation', 'url': 'https://huggingface.co/papers/2503.01295', 'abstract': 'Large Language Models (LLMs) have reshaped code generation by synergizing their exceptional comprehension of natural language and programming syntax, thereby substantially boosting developer productivity. These advancements have prompted numerous efforts to quantitatively evaluate their coding capabilities. However, persistent challenges, such as benchmark leakage, data dissipation, and limited system accessibility, continue to impede a timely and accurate assessment. To address these limitations, we introduce CodeArena, an online evaluation framework tailored for LLM code generation. The key innovation is a collective evaluation mechanism, which dynamically recalibrates individual model scores based on the holistic performance of all participating models, mitigating score biases caused by widespread benchmark leakage. In addition, CodeArena ensures open access to all submitted solutions and test cases and provides automation-friendly APIs to streamline the code evaluation workflow. Our main contributions are: (1) a collective evaluation system for unbiased assessment, (2) a public repository of solutions and test cases, and (3) automation-ready APIs for seamless integration.', 'score': 5, 'issue_id': 2514, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': '96f50dd9e636b12e', 'authors': ['Mingzhe Du', 'Anh Tuan Luu', 'Bin Ji', 'Xiaobao Wu', 'Dong Huang', 'Terry Yue Zhuo', 'Qian Liu', 'See-Kiong Ng'], 'affiliations': ['ByteDance', 'Monash University', 'Nanyang Technological University', 'National University of Singapore', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2503.01295.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#leakage', '#open_source'], 'emoji': '🏟️', 'ru': {'title': 'CodeArena: Справедливая арена для оценки LLM в генерации кода', 'desc': 'CodeArena - это новая онлайн-платформа для оценки генерации кода большими языковыми моделями (LLM). Она использует коллективный механизм оценки, который динамически пересчитывает индивидуальные оценки моделей на основе общей производительности всех участвующих моделей. Это помогает снизить искажения оценок, вызванные утечкой тестовых данных. CodeArena также предоставляет открытый доступ ко всем отправленным решениям и тестовым случаям, а также API для автоматизации процесса оценки.'}, 'en': {'title': 'Revolutionizing Code Evaluation with CodeArena', 'desc': 'This paper discusses the impact of Large Language Models (LLMs) on code generation, highlighting their ability to understand both natural language and programming syntax, which enhances developer productivity. It identifies ongoing issues in evaluating LLM coding capabilities, such as benchmark leakage and limited access to evaluation systems. To overcome these challenges, the authors present CodeArena, an online framework that offers a collective evaluation mechanism to provide unbiased assessments of LLMs. CodeArena also features a public repository for solutions and test cases, along with APIs for easy integration into existing workflows.'}, 'zh': {'title': 'CodeArena：公平评估LLM代码生成的创新平台', 'desc': '大型语言模型（LLMs）通过结合对自然语言和编程语法的深刻理解，极大地提升了代码生成的效率，进而提高了开发者的生产力。为了量化评估这些模型的编码能力，许多研究工作应运而生，但仍面临基准泄漏、数据消散和系统可访问性有限等挑战。为了解决这些问题，我们提出了CodeArena，这是一个专为LLM代码生成设计的在线评估框架。其核心创新在于集体评估机制，能够根据所有参与模型的整体表现动态调整个别模型的评分，从而减少因基准泄漏造成的评分偏差。'}}}, {'id': 'https://huggingface.co/papers/2503.01807', 'title': 'Large-Scale Data Selection for Instruction Tuning', 'url': 'https://huggingface.co/papers/2503.01807', 'abstract': 'Selecting high-quality training data from a larger pool is a crucial step when instruction-tuning language models, as carefully curated datasets often produce models that outperform those trained on much larger, noisier datasets. Automated data selection approaches for instruction-tuning are typically tested by selecting small datasets (roughly 10k samples) from small pools (100-200k samples). However, popular deployed instruction-tuned models often train on hundreds of thousands to millions of samples, subsampled from even larger data pools. We present a systematic study of how well data selection methods scale to these settings, selecting up to 2.5M samples from pools of up to 5.8M samples and evaluating across 7 diverse tasks. We show that many recently proposed methods fall short of random selection in this setting (while using more compute), and even decline in performance when given access to larger pools of data to select over. However, we find that a variant of representation-based data selection (RDS+), which uses weighted mean pooling of pretrained LM hidden states, consistently outperforms more complex methods across all settings tested -- all whilst being more compute-efficient. Our findings highlight that the scaling properties of proposed automated selection methods should be more closely examined. We release our code, data, and models at https://github.com/hamishivi/automated-instruction-selection.', 'score': 5, 'issue_id': 2511, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': '8bbc980a9ef867f7', 'authors': ['Hamish Ivison', 'Muru Zhang', 'Faeze Brahman', 'Pang Wei Koh', 'Pradeep Dasigi'], 'affiliations': ['Allen Institute for AI', 'University of Southern California', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2503.01807.jpg', 'data': {'categories': ['#data', '#open_source', '#optimization', '#dataset', '#training'], 'emoji': '🔍', 'ru': {'title': 'Эффективный отбор данных для обучения языковых моделей: меньше значит больше', 'desc': 'Эта статья исследует методы автоматического отбора данных для инструктивной настройки языковых моделей. Авторы проводят систематическое изучение эффективности различных методов при масштабировании до больших объемов данных, выбирая до 2,5 миллионов образцов из пулов до 5,8 миллионов. Результаты показывают, что многие недавно предложенные методы уступают случайному отбору в этих условиях, однако вариант метода отбора на основе представлений (RDS+) превосходит более сложные подходы. Исследование подчеркивает важность тщательного анализа масштабируемости методов автоматического отбора данных.'}, 'en': {'title': 'Quality Over Quantity: Smart Data Selection for Language Models', 'desc': 'This paper investigates the importance of selecting high-quality training data for instruction-tuning language models. It reveals that many automated data selection methods do not perform better than random selection when scaling to larger datasets, which can include millions of samples. The study introduces a representation-based data selection method (RDS+) that consistently outperforms more complex approaches while being more efficient in terms of computational resources. The authors emphasize the need for a deeper examination of how these selection methods behave as the size of the data pools increases.'}, 'zh': {'title': '高效选择：优化语言模型训练数据的关键', 'desc': '在对语言模型进行指令调优时，从更大数据集中选择高质量的训练数据是一个关键步骤。经过精心策划的数据集通常能产生比那些在更大、更嘈杂的数据集上训练的模型更好的效果。我们进行了系统研究，评估数据选择方法在大规模数据集上的表现，发现许多新提出的方法在这种情况下的表现不如随机选择。我们还发现一种基于表示的数据选择变体（RDS+）在所有测试设置中始终优于更复杂的方法，同时计算效率更高。'}}}, {'id': 'https://huggingface.co/papers/2503.00031', 'title': 'Efficient Test-Time Scaling via Self-Calibration', 'url': 'https://huggingface.co/papers/2503.00031', 'abstract': 'Increasing test-time computation is a straightforward approach to enhancing the quality of responses in Large Language Models (LLMs). While Best-of-N sampling and Self-Consistency with majority voting are simple and effective, they require a fixed number of sampling responses for each query, regardless of its complexity. This could result in wasted computation for simpler questions and insufficient exploration for more challenging ones. In this work, we argue that model confidence of responses can be used for improving the efficiency of test-time scaling. Unfortunately, LLMs are known to be overconfident and provide unreliable confidence estimation. To address this limitation, we introduce Self-Calibration by distilling Self-Consistency-derived confidence into the model itself. This enables reliable confidence estimation at test time with one forward pass. We then design confidence-based efficient test-time scaling methods to handle queries of various difficulty, such as Early-Stopping for Best-of-N and Self-Consistency with calibrated confidence. Experiments on three LLMs across six datasets demonstrate the effectiveness of our approach. Specifically, applying confidence-based Early Stopping to Best-of-N improves MathQA accuracy from 81.0 to 83.6 with a sample budget of 16 responses, indicating the efficacy of confidence-based sampling strategy at inference time.', 'score': 4, 'issue_id': 2523, 'pub_date': '2025-02-25', 'pub_date_card': {'ru': '25 февраля', 'en': 'February 25', 'zh': '2月25日'}, 'hash': 'fad9bb0721d1d6ce', 'authors': ['Chengsong Huang', 'Langlin Huang', 'Jixuan Leng', 'Jiacheng Liu', 'Jiaxin Huang'], 'affiliations': ['Carnegie Mellon University', 'University of Washington', 'Washington University in St. Louis'], 'pdf_title_img': 'assets/pdf/title_img/2503.00031.jpg', 'data': {'categories': ['#optimization', '#training', '#inference'], 'emoji': '🎯', 'ru': {'title': 'Повышение эффективности LLM через калибровку уверенности', 'desc': 'Это исследование предлагает метод Self-Calibration для улучшения эффективности крупных языковых моделей (LLM) при тестировании. Метод основан на дистилляции уверенности, полученной из Self-Consistency, в саму модель, что позволяет надежно оценивать уверенность за один проход. Авторы разработали методы эффективного масштабирования на основе уверенности, такие как Early-Stopping для Best-of-N и Self-Consistency с калиброванной уверенностью. Эксперименты на трех LLM и шести наборах данных показали эффективность этого подхода, в частности, улучшение точности MathQA с 81.0 до 83.6 при бюджете выборки в 16 ответов.'}, 'en': {'title': 'Enhancing LLM Efficiency with Confidence-Based Sampling', 'desc': 'This paper discusses how to improve the efficiency of Large Language Models (LLMs) during testing by using model confidence to guide response sampling. Traditional methods like Best-of-N sampling and Self-Consistency require a fixed number of responses, which can lead to wasted resources or inadequate exploration of complex queries. The authors propose a technique called Self-Calibration, which helps LLMs provide more reliable confidence estimates by distilling information from previous responses. By implementing confidence-based strategies such as Early-Stopping, the paper shows that it is possible to enhance accuracy while reducing unnecessary computations, particularly in challenging tasks like MathQA.'}, 'zh': {'title': '提升大型语言模型响应质量的自我校准方法', 'desc': '本论文探讨了如何通过增加测试时的计算来提高大型语言模型（LLMs）的响应质量。我们提出了一种自我校准的方法，通过将自我一致性生成的置信度提炼到模型中，从而改善置信度估计的可靠性。这样，模型在测试时可以在一次前向传播中获得可靠的置信度估计。我们的实验表明，基于置信度的早停策略能够有效提高模型在不同难度问题上的表现。'}}}, {'id': 'https://huggingface.co/papers/2503.01714', 'title': "Word Form Matters: LLMs' Semantic Reconstruction under Typoglycemia", 'url': 'https://huggingface.co/papers/2503.01714', 'abstract': "Human readers can efficiently comprehend scrambled words, a phenomenon known as Typoglycemia, primarily by relying on word form; if word form alone is insufficient, they further utilize contextual cues for interpretation. While advanced large language models (LLMs) exhibit similar abilities, the underlying mechanisms remain unclear. To investigate this, we conduct controlled experiments to analyze the roles of word form and contextual information in semantic reconstruction and examine LLM attention patterns. Specifically, we first propose SemRecScore, a reliable metric to quantify the degree of semantic reconstruction, and validate its effectiveness. Using this metric, we study how word form and contextual information influence LLMs' semantic reconstruction ability, identifying word form as the core factor in this process. Furthermore, we analyze how LLMs utilize word form and find that they rely on specialized attention heads to extract and process word form information, with this mechanism remaining stable across varying levels of word scrambling. This distinction between LLMs' fixed attention patterns primarily focused on word form and human readers' adaptive strategy in balancing word form and contextual information provides insights into enhancing LLM performance by incorporating human-like, context-aware mechanisms.", 'score': 4, 'issue_id': 2517, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': '4880ed4c044081c4', 'authors': ['Chenxi Wang', 'Tianle Gu', 'Zhongyu Wei', 'Lang Gao', 'Zirui Song', 'Xiuying Chen'], 'affiliations': ['Fudan University', 'Mohamed bin Zayed University of Artificial Intelligence (MBZUAI)'], 'pdf_title_img': 'assets/pdf/title_img/2503.01714.jpg', 'data': {'categories': ['#training', '#interpretability', '#data', '#multimodal', '#alignment'], 'emoji': '🔀', 'ru': {'title': 'Форма слова - ключ к пониманию перемешанного текста для ИИ', 'desc': 'Исследование посвящено способности больших языковых моделей (LLM) понимать перемешанные слова, подобно людям. Авторы предлагают метрику SemRecScore для оценки семантической реконструкции и анализируют роль формы слова и контекстной информации. Результаты показывают, что форма слова является ключевым фактором для LLM при обработке перемешанных слов. Анализ паттернов внимания LLM выявляет специализированные механизмы для извлечения информации о форме слова.'}, 'en': {'title': 'Unlocking LLMs: The Power of Word Form in Understanding Scrambled Text', 'desc': 'This paper explores how large language models (LLMs) understand scrambled words, similar to how humans do through a phenomenon called Typoglycemia. The authors introduce a new metric, SemRecScore, to measure how well LLMs can reconstruct meaning from scrambled text by focusing on word form and context. Their experiments reveal that LLMs primarily depend on word form for semantic reconstruction, utilizing specific attention heads to process this information. The findings suggest that incorporating more human-like, context-aware strategies could improve LLM performance in understanding language.'}, 'zh': {'title': '揭示大型语言模型的语义重建机制', 'desc': '本研究探讨了大型语言模型（LLMs）在语义重建中的能力，特别是它们如何利用单词形式和上下文信息。我们提出了一种新的度量标准SemRecScore，用于量化语义重建的程度，并验证了其有效性。研究发现，单词形式是影响LLMs语义重建能力的核心因素，且LLMs通过专门的注意力头来提取和处理单词形式信息。与人类读者在单词形式和上下文信息之间的灵活策略不同，LLMs的注意力模式主要集中在单词形式上，这为提升LLMs性能提供了新的思路。'}}}, {'id': 'https://huggingface.co/papers/2502.19402', 'title': 'General Reasoning Requires Learning to Reason from the Get-go', 'url': 'https://huggingface.co/papers/2502.19402', 'abstract': "Large Language Models (LLMs) have demonstrated impressive real-world utility, exemplifying artificial useful intelligence (AUI). However, their ability to reason adaptively and robustly -- the hallmarks of artificial general intelligence (AGI) -- remains fragile. While LLMs seemingly succeed in commonsense reasoning, programming, and mathematics, they struggle to generalize algorithmic understanding across novel contexts. Our experiments with algorithmic tasks in esoteric programming languages reveal that LLM's reasoning overfits to the training data and is limited in its transferability. We hypothesize that the core issue underlying such limited transferability is the coupling of reasoning and knowledge in LLMs.   To transition from AUI to AGI, we propose disentangling knowledge and reasoning through three key directions: (1) pretaining to reason using RL from scratch as an alternative to the widely used next-token prediction pretraining, (2) using a curriculum of synthetic tasks to ease the learning of a reasoning prior for RL that can then be transferred to natural language tasks, and (3) learning more generalizable reasoning functions using a small context window to reduce exploiting spurious correlations between tokens. Such a reasoning system coupled with a trained retrieval system and a large external memory bank as a knowledge store can overcome several limitations of existing architectures at learning to reason in novel scenarios.", 'score': 3, 'issue_id': 2520, 'pub_date': '2025-02-26', 'pub_date_card': {'ru': '26 февраля', 'en': 'February 26', 'zh': '2月26日'}, 'hash': '5774d50d6c5a9361', 'authors': ['Seungwook Han', 'Jyothish Pari', 'Samuel J. Gershman', 'Pulkit Agrawal'], 'affiliations': ['Department of Psychology and Center for Brain Science, Harvard University', 'Improbable AI Lab, MIT'], 'pdf_title_img': 'assets/pdf/title_img/2502.19402.jpg', 'data': {'categories': ['#agi', '#transfer_learning', '#architecture', '#rl', '#synthetic', '#training', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Отделяя рассуждения от знаний: путь к AGI', 'desc': 'Статья рассматривает ограничения больших языковых моделей (LLM) в области обобщенного рассуждения, несмотря на их впечатляющую полезность. Авторы выявляют проблему переобучения LLM на тренировочных данных и предлагают разделить знания и рассуждения для перехода к искусственному общему интеллекту (AGI). Предлагается использовать обучение с подкреплением, синтетические задачи и ограниченный контекст для улучшения обобщающей способности. Такая система рассуждений в сочетании с извлечением информации и внешней памятью может преодолеть ограничения существующих архитектур.'}, 'en': {'title': 'Disentangling Knowledge and Reasoning for Robust AI', 'desc': "This paper discusses the limitations of Large Language Models (LLMs) in achieving robust reasoning capabilities, which are essential for artificial general intelligence (AGI). The authors identify that LLMs often overfit to their training data, leading to poor generalization in novel algorithmic tasks. They propose a solution that involves separating knowledge from reasoning by employing reinforcement learning (RL) and a structured curriculum of synthetic tasks. By enhancing reasoning functions and integrating a retrieval system with an external memory, the authors aim to improve LLMs' adaptability and performance in unfamiliar contexts."}, 'zh': {'title': '解耦知识与推理，迈向人工通用智能', 'desc': '大型语言模型（LLMs）在实际应用中表现出色，展示了人工有用智能（AUI）的潜力。然而，它们在自适应和稳健推理方面的能力仍然脆弱，这是人工通用智能（AGI）的标志。我们的实验表明，LLMs在算法任务中容易过拟合训练数据，且在新环境中的迁移能力有限。为了解决这一问题，我们提出通过三种关键方向来解耦知识与推理，以促进从AUI向AGI的过渡。'}}}, {'id': 'https://huggingface.co/papers/2503.01739', 'title': 'VideoUFO: A Million-Scale User-Focused Dataset for Text-to-Video Generation', 'url': 'https://huggingface.co/papers/2503.01739', 'abstract': "Text-to-video generative models convert textual prompts into dynamic visual content, offering wide-ranging applications in film production, gaming, and education. However, their real-world performance often falls short of user expectations. One key reason is that these models have not been trained on videos related to some topics users want to create. In this paper, we propose VideoUFO, the first Video dataset specifically curated to align with Users' FOcus in real-world scenarios. Beyond this, our VideoUFO also features: (1) minimal (0.29%) overlap with existing video datasets, and (2) videos searched exclusively via YouTube's official API under the Creative Commons license. These two attributes provide future researchers with greater freedom to broaden their training sources. The VideoUFO comprises over 1.09 million video clips, each paired with both a brief and a detailed caption (description). Specifically, through clustering, we first identify 1,291 user-focused topics from the million-scale real text-to-video prompt dataset, VidProM. Then, we use these topics to retrieve videos from YouTube, split the retrieved videos into clips, and generate both brief and detailed captions for each clip. After verifying the clips with specified topics, we are left with about 1.09 million video clips. Our experiments reveal that (1) current 16 text-to-video models do not achieve consistent performance across all user-focused topics; and (2) a simple model trained on VideoUFO outperforms others on worst-performing topics. The dataset is publicly available at https://huggingface.co/datasets/WenhaoWang/VideoUFO under the CC BY 4.0 License.", 'score': 3, 'issue_id': 2512, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': '046fdeee8939e82c', 'authors': ['Wenhao Wang', 'Yi Yang'], 'affiliations': ['University of Technology Sydney', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.01739.jpg', 'data': {'categories': ['#video', '#dataset', '#data', '#games', '#open_source'], 'emoji': '🎬', 'ru': {'title': 'VideoUFO: Новый эталонный датасет для генерации видео по запросу', 'desc': 'Статья представляет VideoUFO - новый набор данных для обучения моделей генерации видео по текстовому описанию. Этот датасет содержит более 1,09 миллиона видеоклипов с подробными и краткими описаниями, охватывающих 1291 тему, актуальную для пользователей. VideoUFO отличается минимальным пересечением с существующими наборами данных и использованием только видео под лицензией Creative Commons. Эксперименты показали, что простая модель, обученная на VideoUFO, превосходит другие модели на наиболее сложных темах.'}, 'en': {'title': 'Empowering Text-to-Video Models with User-Focused Data', 'desc': 'This paper introduces VideoUFO, a novel video dataset designed to enhance text-to-video generative models by focusing on user-relevant topics. The dataset contains over 1.09 million video clips, each accompanied by both brief and detailed captions, ensuring minimal overlap with existing datasets. By clustering user prompts, the authors identified 1,291 specific topics to guide video retrieval from YouTube, which were then segmented into clips. Experiments show that models trained on VideoUFO significantly outperform existing models, particularly on challenging topics, highlighting the importance of tailored training data in machine learning applications.'}, 'zh': {'title': '提升文本到视频生成的用户体验', 'desc': '本文介绍了一种新的视频数据集VideoUFO，旨在提高文本到视频生成模型的性能。该数据集专注于用户关注的主题，包含超过109万个视频片段，并为每个片段提供简短和详细的描述。VideoUFO与现有数据集的重叠率极低，且所有视频均通过YouTube的官方API获取，确保了数据的多样性和合法性。实验结果表明，使用VideoUFO训练的模型在用户关注的主题上表现优于其他模型。'}}}, {'id': 'https://huggingface.co/papers/2503.01103', 'title': 'Direct Discriminative Optimization: Your Likelihood-Based Visual Generative Model is Secretly a GAN Discriminator', 'url': 'https://huggingface.co/papers/2503.01103', 'abstract': 'While likelihood-based generative models, particularly diffusion and autoregressive models, have achieved remarkable fidelity in visual generation, the maximum likelihood estimation (MLE) objective inherently suffers from a mode-covering tendency that limits the generation quality under limited model capacity. In this work, we propose Direct Discriminative Optimization (DDO) as a unified framework that bridges likelihood-based generative training and the GAN objective to bypass this fundamental constraint. Our key insight is to parameterize a discriminator implicitly using the likelihood ratio between a learnable target model and a fixed reference model, drawing parallels with the philosophy of Direct Preference Optimization (DPO). Unlike GANs, this parameterization eliminates the need for joint training of generator and discriminator networks, allowing for direct, efficient, and effective finetuning of a well-trained model to its full potential beyond the limits of MLE. DDO can be performed iteratively in a self-play manner for progressive model refinement, with each round requiring less than 1% of pretraining epochs. Our experiments demonstrate the effectiveness of DDO by significantly advancing the previous SOTA diffusion model EDM, reducing FID scores from 1.79/1.58 to new records of 1.30/0.97 on CIFAR-10/ImageNet-64 datasets, and by consistently improving both guidance-free and CFG-enhanced FIDs of visual autoregressive models on ImageNet 256times256.', 'score': 2, 'issue_id': 2517, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': 'd8b58c1a2c49da16', 'authors': ['Kaiwen Zheng', 'Yongxin Chen', 'Huayu Chen', 'Guande He', 'Ming-Yu Liu', 'Jun Zhu', 'Qinsheng Zhang'], 'affiliations': ['NVIDIA', 'The University of Texas at', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.01103.jpg', 'data': {'categories': ['#training', '#diffusion', '#cv', '#optimization'], 'emoji': '🚀', 'ru': {'title': 'DDO: Прорыв в обучении генеративных моделей без ограничений MLE', 'desc': 'Авторы статьи предлагают новый метод обучения генеративных моделей под названием Direct Discriminative Optimization (DDO). Этот подход объединяет методы обучения на основе правдоподобия и цели генеративно-состязательных сетей (GAN), чтобы преодолеть ограничения метода максимального правдоподобия (MLE). DDO использует отношение правдоподобия между обучаемой целевой моделью и фиксированной эталонной моделью для параметризации дискриминатора. Эксперименты показывают, что DDO значительно улучшает результаты современных диффузионных и авторегрессионных моделей на различных наборах данных.'}, 'en': {'title': 'Enhancing Generative Models with Direct Discriminative Optimization', 'desc': 'This paper introduces Direct Discriminative Optimization (DDO), a new framework that enhances the performance of generative models by combining likelihood-based training with concepts from Generative Adversarial Networks (GANs). DDO addresses the limitations of maximum likelihood estimation (MLE) by using a discriminator that is parameterized through the likelihood ratio of a target model and a fixed reference model. This approach allows for efficient finetuning of pre-trained models without the need for joint training of generator and discriminator networks. The results show that DDO significantly improves the state-of-the-art performance in visual generation tasks, achieving lower FID scores on popular datasets like CIFAR-10 and ImageNet.'}, 'zh': {'title': '直接判别优化：提升生成模型的新方法', 'desc': '本文提出了一种新的方法，称为直接判别优化（DDO），旨在提高生成模型的质量。DDO通过将生成训练与GAN目标结合，克服了最大似然估计（MLE）在模型容量有限时的局限性。该方法通过使用可学习的目标模型与固定参考模型之间的似然比来隐式参数化判别器，从而简化了生成器和判别器的联合训练。实验结果表明，DDO显著提高了现有扩散模型的性能，并在多个数据集上创造了新的记录。'}}}, {'id': 'https://huggingface.co/papers/2502.16779', 'title': 'Unposed Sparse Views Room Layout Reconstruction in the Age of Pretrain Model', 'url': 'https://huggingface.co/papers/2502.16779', 'abstract': 'Room layout estimation from multiple-perspective images is poorly investigated due to the complexities that emerge from multi-view geometry, which requires muti-step solutions such as camera intrinsic and extrinsic estimation, image matching, and triangulation. However, in 3D reconstruction, the advancement of recent 3D foundation models such as DUSt3R has shifted the paradigm from the traditional multi-step structure-from-motion process to an end-to-end single-step approach. To this end, we introduce Plane-DUSt3R, a novel method for multi-view room layout estimation leveraging the 3D foundation model DUSt3R. Plane-DUSt3R incorporates the DUSt3R framework and fine-tunes on a room layout dataset (Structure3D) with a modified objective to estimate structural planes. By generating uniform and parsimonious results, Plane-DUSt3R enables room layout estimation with only a single post-processing step and 2D detection results. Unlike previous methods that rely on single-perspective or panorama image, Plane-DUSt3R extends the setting to handle multiple-perspective images. Moreover, it offers a streamlined, end-to-end solution that simplifies the process and reduces error accumulation. Experimental results demonstrate that Plane-DUSt3R not only outperforms state-of-the-art methods on the synthetic dataset but also proves robust and effective on in the wild data with different image styles such as cartoon.Our code is available at: https://github.com/justacar/Plane-DUSt3R', 'score': 2, 'issue_id': 2516, 'pub_date': '2025-02-24', 'pub_date_card': {'ru': '24 февраля', 'en': 'February 24', 'zh': '2月24日'}, 'hash': '4a9f6cc2fb1ab840', 'authors': ['Yaxuan Huang', 'Xili Dai', 'Jianan Wang', 'Xianbiao Qi', 'Yixing Yuan', 'Xiangyu Yue'], 'affiliations': ['Astribot', 'Hong Kong Center for Construction Robotics, The Hong Kong University of Science and Technology', 'Intellifusion Inc.', 'MMLab, The Chinese University of Hong Kong', 'The Hong Kong University of Science and Technology (Guangzhou)'], 'pdf_title_img': 'assets/pdf/title_img/2502.16779.jpg', 'data': {'categories': ['#3d', '#optimization', '#cv', '#synthetic'], 'emoji': '🏠', 'ru': {'title': 'Революция в оценке планировки помещений: от множества шагов к единому решению', 'desc': 'Статья представляет Plane-DUSt3R - новый метод для оценки планировки помещений по множественным ракурсам изображений. Этот подход использует 3D-модель фундаментального уровня DUSt3R и дообучается на наборе данных Structure3D для оценки структурных плоскостей. Plane-DUSt3R предлагает упрощенное сквозное решение, которое превосходит современные методы на синтетических данных и показывает надежность на реальных изображениях различных стилей. Метод позволяет оценивать планировку помещений с помощью одного шага постобработки и результатов 2D-обнаружения.'}, 'en': {'title': 'Revolutionizing Room Layout Estimation with Plane-DUSt3R', 'desc': 'This paper presents Plane-DUSt3R, a new method for estimating room layouts from multiple images taken from different perspectives. It builds on the DUSt3R 3D foundation model, moving away from traditional multi-step processes to a more efficient end-to-end approach. By fine-tuning the model on a specific dataset, Plane-DUSt3R can accurately identify structural planes with minimal post-processing. The results show that this method not only surpasses existing techniques on synthetic data but also performs well on real-world images with varying styles.'}, 'zh': {'title': '简化多视角房间布局估计的全新方法', 'desc': '本论文提出了一种新的多视角房间布局估计方法，称为Plane-DUSt3R。该方法利用了先进的3D基础模型DUSt3R，简化了传统的多步骤流程，采用端到端的单步骤方法。Plane-DUSt3R通过在房间布局数据集上进行微调，能够有效估计结构平面，并生成一致且简洁的结果。实验结果表明，Plane-DUSt3R在合成数据集上超越了现有的最先进方法，并在不同风格的真实数据上表现出色。'}}}, {'id': 'https://huggingface.co/papers/2503.00729', 'title': 'CLEA: Closed-Loop Embodied Agent for Enhancing Task Execution in Dynamic Environments', 'url': 'https://huggingface.co/papers/2503.00729', 'abstract': "Large Language Models (LLMs) exhibit remarkable capabilities in the hierarchical decomposition of complex tasks through semantic reasoning. However, their application in embodied systems faces challenges in ensuring reliable execution of subtask sequences and achieving one-shot success in long-term task completion. To address these limitations in dynamic environments, we propose Closed-Loop Embodied Agent (CLEA) -- a novel architecture incorporating four specialized open-source LLMs with functional decoupling for closed-loop task management. The framework features two core innovations: (1) Interactive task planner that dynamically generates executable subtasks based on the environmental memory, and (2) Multimodal execution critic employing an evaluation framework to conduct a probabilistic assessment of action feasibility, triggering hierarchical re-planning mechanisms when environmental perturbations exceed preset thresholds. To validate CLEA's effectiveness, we conduct experiments in a real environment with manipulable objects, using two heterogeneous robots for object search, manipulation, and search-manipulation integration tasks. Across 12 task trials, CLEA outperforms the baseline model, achieving a 67.3% improvement in success rate and a 52.8% increase in task completion rate. These results demonstrate that CLEA significantly enhances the robustness of task planning and execution in dynamic environments.", 'score': 2, 'issue_id': 2514, 'pub_date': '2025-03-02', 'pub_date_card': {'ru': '2 марта', 'en': 'March 2', 'zh': '3月2日'}, 'hash': '57f6361f66ec99cf', 'authors': ['Mingcong Lei', 'Ge Wang', 'Yiming Zhao', 'Zhixin Mai', 'Qing Zhao', 'Yao Guo', 'Zhen Li', 'Shuguang Cui', 'Yatong Han', 'Jinke Ren'], 'affiliations': ['Guangdong Provincial Key Laboratory of Future Networks of Intelligence, The Chinese University of Hong Kong, Shenzhen', 'Harbin Engineering University, Harbin', 'Infused Synapse AI, Shenzhen', 'Institute of Medical Robotics, School of Biomedical Engineering, Shanghai Jiao Tong University, Shanghai', 'School of Science and Engineering (SSE), FNii-Shenzhen', 'Shenzhen Future Network of Intelligence Institute (FNii-Shenzhen)'], 'pdf_title_img': 'assets/pdf/title_img/2503.00729.jpg', 'data': {'categories': ['#architecture', '#reasoning', '#open_source', '#robotics', '#agents', '#optimization'], 'emoji': '🤖', 'ru': {'title': 'CLEA: Повышение надежности выполнения задач роботами с помощью языковых моделей', 'desc': 'Статья представляет новую архитектуру под названием CLEA (Closed-Loop Embodied Agent) для улучшения выполнения сложных задач роботами в динамических средах. CLEA использует четыре специализированные языковые модели с открытым исходным кодом для управления задачами в замкнутом цикле. Ключевые инновации включают интерактивный планировщик задач и мультимодальный критик выполнения для оценки выполнимости действий. Эксперименты показали, что CLEA значительно превосходит базовую модель по показателям успешности и завершения задач в реальной среде с манипулируемыми объектами.'}, 'en': {'title': 'Enhancing Task Execution in Dynamic Environments with CLEA', 'desc': 'This paper introduces the Closed-Loop Embodied Agent (CLEA), a new architecture designed to improve the performance of Large Language Models (LLMs) in dynamic environments. CLEA features an interactive task planner that creates subtasks based on real-time environmental data, allowing for better adaptability. Additionally, it includes a multimodal execution critic that evaluates the feasibility of actions and adjusts plans when unexpected changes occur. Experimental results show that CLEA significantly enhances task success and completion rates compared to traditional models, demonstrating its effectiveness in complex, real-world scenarios.'}, 'zh': {'title': '闭环具身代理：提升动态环境中的任务执行能力', 'desc': '大型语言模型（LLMs）在复杂任务的层次分解和语义推理方面表现出色。然而，在具身系统中应用时，确保子任务序列的可靠执行和实现长期任务的一次性成功面临挑战。为了解决这些问题，我们提出了闭环具身代理（CLEA），这是一种新颖的架构，结合了四个专门的开源LLM，并实现功能解耦以进行闭环任务管理。通过动态生成可执行的子任务和使用多模态执行评估框架，CLEA显著提高了在动态环境中任务规划和执行的鲁棒性。'}}}, {'id': 'https://huggingface.co/papers/2502.20383', 'title': 'Why Are Web AI Agents More Vulnerable Than Standalone LLMs? A Security Analysis', 'url': 'https://huggingface.co/papers/2502.20383', 'abstract': 'Recent advancements in Web AI agents have demonstrated remarkable capabilities in addressing complex web navigation tasks. However, emerging research shows that these agents exhibit greater vulnerability compared to standalone Large Language Models (LLMs), despite both being built upon the same safety-aligned models. This discrepancy is particularly concerning given the greater flexibility of Web AI Agent compared to standalone LLMs, which may expose them to a wider range of adversarial user inputs. To build a scaffold that addresses these concerns, this study investigates the underlying factors that contribute to the increased vulnerability of Web AI agents. Notably, this disparity stems from the multifaceted differences between Web AI agents and standalone LLMs, as well as the complex signals - nuances that simple evaluation metrics, such as success rate, often fail to capture. To tackle these challenges, we propose a component-level analysis and a more granular, systematic evaluation framework. Through this fine-grained investigation, we identify three critical factors that amplify the vulnerability of Web AI agents; (1) embedding user goals into the system prompt, (2) multi-step action generation, and (3) observational capabilities. Our findings highlights the pressing need to enhance security and robustness in AI agent design and provide actionable insights for targeted defense strategies.', 'score': 1, 'issue_id': 2522, 'pub_date': '2025-02-27', 'pub_date_card': {'ru': '27 февраля', 'en': 'February 27', 'zh': '2月27日'}, 'hash': '12b0a9a60578dc2b', 'authors': ['Jeffrey Yang Fan Chiang', 'Seungjae Lee', 'Jia-Bin Huang', 'Furong Huang', 'Yizheng Chen'], 'affiliations': ['University of Maryland'], 'pdf_title_img': 'assets/pdf/title_img/2502.20383.jpg', 'data': {'categories': ['#benchmark', '#security', '#agents'], 'emoji': '🕸️', 'ru': {'title': 'Раскрытие уязвимостей веб-агентов ИИ: путь к более безопасным системам', 'desc': 'Исследование показывает, что веб-агенты на основе искусственного интеллекта более уязвимы, чем автономные большие языковые модели (LLM), несмотря на использование одинаковых базовых моделей. Авторы предлагают компонентный анализ и более детальную систему оценки для выявления причин этой уязвимости. Выделены три ключевых фактора, усиливающих уязвимость веб-агентов: встраивание целей пользователя в системный промпт, генерация многошаговых действий и возможности наблюдения. Результаты исследования подчеркивают необходимость улучшения безопасности и устойчивости при разработке ИИ-агентов.'}, 'en': {'title': 'Strengthening Web AI Agents Against Vulnerabilities', 'desc': 'This paper explores the vulnerabilities of Web AI agents compared to standalone Large Language Models (LLMs), despite both being based on similar safety models. The research identifies that Web AI agents are more susceptible to adversarial inputs due to their flexibility and the complexity of their tasks. It highlights three key factors that increase their vulnerability: the integration of user goals into prompts, the generation of multi-step actions, and the need for observational capabilities. The study proposes a detailed evaluation framework to better understand these vulnerabilities and suggests strategies for improving the security and robustness of AI agents.'}, 'zh': {'title': '提升网络人工智能代理的安全性与鲁棒性', 'desc': '最近，网络人工智能代理在处理复杂的网络导航任务方面表现出色。然而，研究表明，这些代理比独立的大型语言模型（LLMs）更容易受到攻击，尽管它们都是基于相同的安全模型构建的。这种差异令人担忧，因为网络人工智能代理的灵活性更高，可能会面临更广泛的恶意用户输入。为了解决这些问题，本研究探讨了导致网络人工智能代理脆弱性的因素，并提出了一种更细致的评估框架，以识别和应对这些挑战。'}}}, {'id': 'https://huggingface.co/papers/2503.01063', 'title': 'AI-Invented Tonal Languages: Preventing a Machine Lingua Franca Beyond Human Understanding', 'url': 'https://huggingface.co/papers/2503.01063', 'abstract': 'This paper investigates the potential for large language models (LLMs) to develop private tonal languages for machine-to-machine (M2M) communication. Inspired by cryptophasia in human twins (affecting up to 50% of twin births) and natural tonal languages like Mandarin and Vietnamese, we implement a precise character-to-frequency mapping system that encodes the full ASCII character set (32-126) using musical semitones. Each character is assigned a unique frequency, creating a logarithmic progression beginning with space (220 Hz) and ending with tilde (50,175.42 Hz). This spans approximately 7.9 octaves, with higher characters deliberately mapped to ultrasonic frequencies beyond human perception (>20 kHz). Our implemented software prototype demonstrates this encoding through visualization, auditory playback, and ABC musical notation, allowing for analysis of information density and transmission speed. Testing reveals that tonal encoding can achieve information rates exceeding human speech while operating partially outside human perceptual boundaries. This work responds directly to concerns about AI systems catastrophically developing private languages within the next five years, providing a concrete prototype software example of how such communication might function and the technical foundation required for its emergence, detection, and governance.', 'score': 1, 'issue_id': 2515, 'pub_date': '2025-03-02', 'pub_date_card': {'ru': '2 марта', 'en': 'March 2', 'zh': '3月2日'}, 'hash': '7021403742a91f3e', 'authors': ['David Noever'], 'affiliations': ['PeopleTec, Inc., Huntsville, AL'], 'pdf_title_img': 'assets/pdf/title_img/2503.01063.jpg', 'data': {'categories': ['#security', '#ethics', '#audio', '#multimodal'], 'emoji': '🎵', 'ru': {'title': 'Тональные языки: секретный код машин будущего', 'desc': 'Это исследование изучает потенциал больших языковых моделей (LLM) для разработки приватных тональных языков для коммуникации между машинами. Авторы создали систему кодирования, которая сопоставляет каждому символу ASCII уникальную частоту, формируя логарифмическую прогрессию от 220 Гц до 50,175.42 Гц. Разработанный программный прототип демонстрирует это кодирование через визуализацию, воспроизведение звука и нотацию ABC. Тестирование показало, что тональное кодирование может достигать скорости передачи информации, превышающей человеческую речь, при этом частично работая за пределами человеческого восприятия.'}, 'en': {'title': 'Unlocking Machine Communication with Tonal Languages', 'desc': 'This paper explores how large language models (LLMs) can create private tonal languages for communication between machines. It draws inspiration from the phenomenon of cryptophasia in twins and uses a character-to-frequency mapping system to encode ASCII characters into musical tones. Each character is assigned a unique frequency, allowing for efficient data transmission that exceeds human speech rates. The study provides a prototype that visualizes and plays back this encoding, addressing concerns about AI developing private languages and offering a framework for understanding and managing such systems.'}, 'zh': {'title': '探索机器间的私有音调语言', 'desc': '本论文研究了大型语言模型（LLMs）在机器间（M2M）通信中开发私有音调语言的潜力。我们借鉴了人类双胞胎中的密码语言现象和自然音调语言，如普通话和越南语，实施了一种精确的字符到频率映射系统。每个字符被分配一个独特的频率，形成一个对数进程，覆盖约7.9个八度，并将高频字符映射到人类听觉范围之外的超声波频率。我们的软件原型展示了这种编码的可视化、听觉播放和音乐记谱法，分析了信息密度和传输速度，测试结果表明音调编码的信息传输速率超过人类语言。'}}}, {'id': 'https://huggingface.co/papers/2503.00865', 'title': 'Babel: Open Multilingual Large Language Models Serving Over 90% of Global Speakers', 'url': 'https://huggingface.co/papers/2503.00865', 'abstract': "Large language models (LLMs) have revolutionized natural language processing (NLP), yet open-source multilingual LLMs remain scarce, with existing models often limited in language coverage. Such models typically prioritize well-resourced languages, while widely spoken but under-resourced languages are often overlooked. To address this disparity, we introduce Babel, an open multilingual LLM that covers the top 25 languages by number of speakers, supports over 90% of the global population, and includes many languages neglected by other open multilingual LLMs. Unlike traditional continue pretraining approaches, Babel expands its parameter count through a layer extension technique that elevates Babel's performance ceiling. We introduce two variants: Babel-9B, designed for efficient inference and fine-tuning, and Babel-83B, which sets a new standard for open multilingual LLMs. Extensive evaluations on multilingual tasks demonstrate its superior performance compared to open LLMs of comparable size. In addition, using open-source supervised fine-tuning datasets, Babel achieves remarkable performance, with Babel-9B-Chat leading among 10B-sized LLMs and Babel-83B-Chat setting a new standard for multilingual tasks, reaching the same level of commercial models.", 'score': 40, 'issue_id': 2555, 'pub_date': '2025-03-02', 'pub_date_card': {'ru': '2 марта', 'en': 'March 2', 'zh': '3月2日'}, 'hash': 'bc2424e709a2dd78', 'authors': ['Yiran Zhao', 'Chaoqun Liu', 'Yue Deng', 'Jiahao Ying', 'Mahani Aljunied', 'Zhaodonghui Li', 'Lidong Bing', 'Hou Pong Chan', 'Yu Rong', 'Deli Zhao', 'Wenxuan Zhang'], 'affiliations': ['DAMO Academy, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2503.00865.jpg', 'data': {'categories': ['#low_resource', '#architecture', '#open_source', '#training', '#multilingual'], 'emoji': '🌍', 'ru': {'title': 'Babel: революция в многоязычном машинном обучении', 'desc': 'Представлена новая многоязычная языковая модель Babel, охватывающая 25 самых распространенных языков мира. Модель использует технику расширения слоев для улучшения производительности. Предложены две версии: Babel-9B для эффективного вывода и дообучения, и Babel-83B, устанавливающая новый стандарт для открытых многоязычных моделей. Обе версии показывают превосходные результаты в многоязычных задачах по сравнению с аналогичными открытыми моделями.'}, 'en': {'title': 'Babel: Bridging the Language Gap with Open Multilingual LLMs', 'desc': "This paper presents Babel, an innovative open-source multilingual large language model (LLM) that addresses the lack of coverage for under-resourced languages in existing models. Babel supports the top 25 languages spoken globally, reaching over 90% of the world's population, and includes many languages that are often neglected. The model employs a unique layer extension technique to increase its parameter count, enhancing its performance beyond traditional pretraining methods. With two variants, Babel-9B and Babel-83B, the model demonstrates superior performance on multilingual tasks, outperforming other open LLMs of similar size and achieving results comparable to commercial models."}, 'zh': {'title': 'Babel：打破语言壁垒的多语言模型', 'desc': '大型语言模型（LLMs）在自然语言处理（NLP）领域带来了革命性的变化，但开源的多语言LLMs仍然稀缺，现有模型通常在语言覆盖上有限。许多模型优先考虑资源丰富的语言，而广泛使用但资源不足的语言常常被忽视。为了解决这一差距，我们推出了Babel，一个开放的多语言LLM，覆盖全球前25种语言，支持超过90%的人口，并包括许多其他开源多语言LLMs忽视的语言。Babel通过层扩展技术增加参数数量，提升了性能，并推出了两个变体：Babel-9B和Babel-83B，后者在多语言任务中设定了新的标准。'}}}, {'id': 'https://huggingface.co/papers/2503.03746', 'title': 'Process-based Self-Rewarding Language Models', 'url': 'https://huggingface.co/papers/2503.03746', 'abstract': "Large Language Models have demonstrated outstanding performance across various downstream tasks and have been widely applied in multiple scenarios. Human-annotated preference data is used for training to further improve LLMs' performance, which is constrained by the upper limit of human performance. Therefore, Self-Rewarding method has been proposed, where LLMs generate training data by rewarding their own outputs. However, the existing self-rewarding paradigm is not effective in mathematical reasoning scenarios and may even lead to a decline in performance. In this work, we propose the Process-based Self-Rewarding pipeline for language models, which introduces long-thought reasoning, step-wise LLM-as-a-Judge, and step-wise preference optimization within the self-rewarding paradigm. Our new paradigm successfully enhances the performance of LLMs on multiple mathematical reasoning benchmarks through iterative Process-based Self-Rewarding, demonstrating the immense potential of self-rewarding to achieve LLM reasoning that may surpass human capabilities.", 'score': 21, 'issue_id': 2564, 'pub_date': '2025-03-05', 'pub_date_card': {'ru': '5 марта', 'en': 'March 5', 'zh': '3月5日'}, 'hash': '808bee960390ec29', 'authors': ['Shimao Zhang', 'Xiao Liu', 'Xin Zhang', 'Junxiao Liu', 'Zheheng Luo', 'Shujian Huang', 'Yeyun Gong'], 'affiliations': ['Microsoft Research Asia', 'National Key Laboratory for Novel Software Technology, Nanjing University', 'The University of Manchester'], 'pdf_title_img': 'assets/pdf/title_img/2503.03746.jpg', 'data': {'categories': ['#math', '#alignment', '#rlhf', '#reasoning', '#training'], 'emoji': '🧮', 'ru': {'title': 'Самообучение ИИ математике: шаг за шагом к сверхчеловеческим способностям', 'desc': 'Статья представляет новый метод самообучения языковых моделей для задач математических рассуждений. Авторы предлагают подход Process-based Self-Rewarding, который включает пошаговое рассуждение и оценку промежуточных результатов самой моделью. Этот метод позволяет преодолеть ограничения существующих подходов к самообучению в математических задачах. Эксперименты показывают, что новый метод значительно улучшает способности языковых моделей к математическим рассуждениям на различных тестовых наборах.'}, 'en': {'title': 'Empowering LLMs with Process-based Self-Rewarding for Superior Reasoning', 'desc': 'This paper discusses the limitations of current self-rewarding methods used to train Large Language Models (LLMs), particularly in mathematical reasoning tasks. The authors introduce a new approach called Process-based Self-Rewarding, which incorporates long-thought reasoning and a step-wise evaluation process. By allowing LLMs to act as judges of their own outputs, this method optimizes the training process iteratively. The results show significant improvements in LLM performance on mathematical reasoning benchmarks, suggesting that self-rewarding can enhance reasoning capabilities beyond human levels.'}, 'zh': {'title': '基于过程的自我奖励：超越人类的推理能力', 'desc': '大型语言模型在各种下游任务中表现出色，并广泛应用于多个场景。为了进一步提高其性能，研究者使用人类标注的偏好数据进行训练，但这受到人类表现上限的限制。因此，提出了自我奖励的方法，让语言模型通过奖励自己的输出生成训练数据。然而，现有的自我奖励方法在数学推理场景中效果不佳，甚至可能导致性能下降。本文提出了一种基于过程的自我奖励管道，通过引入长时间思考推理、逐步的语言模型评判和逐步的偏好优化，成功提升了语言模型在多个数学推理基准上的表现，展示了自我奖励在超越人类能力的推理中的巨大潜力。'}}}, {'id': 'https://huggingface.co/papers/2503.00329', 'title': 'ABC: Achieving Better Control of Multimodal Embeddings using VLMs', 'url': 'https://huggingface.co/papers/2503.00329', 'abstract': 'Visual embedding models excel at zero-shot tasks like visual retrieval and classification. However, these models cannot be used for tasks that contain ambiguity or require user instruction. These tasks necessitate a multimodal embedding model, which outputs embeddings that combine visual and natural language input. Existing CLIP-based approaches embed images and text independently, and fuse the result. We find that this results in weak interactions between modalities, and poor user control over the representation. We introduce ABC, an open-source multimodal embedding model that uses a vision-language model backbone to deeply integrate image features with natural language instructions. ABC achieves bestfor-size performance on MSCOCO image-to-text retrieval and is the top performing model on classification and VQA tasks in the Massive Multimodal Embedding Benchmark. With a strongly unified vision-language representation, ABC can use natural language to solve subtle and potentially ambiguous visual retrieval problems. To evaluate this capability, we design CtrlBench, a benchmark that requires interleaving textual instructions with image content for correct retrieval. ABC advances the state of multimodal embeddings by offering high-quality representations and flexible natural language control. Our model and datasets are available at our project page.', 'score': 14, 'issue_id': 2555, 'pub_date': '2025-03-01', 'pub_date_card': {'ru': '1 марта', 'en': 'March 1', 'zh': '3月1日'}, 'hash': '0483c542c8885777', 'authors': ['Benjamin Schneider', 'Florian Kerschbaum', 'Wenhu Chen'], 'affiliations': ['Cheriton School of Computer Science, University of Waterloo', 'Vector Institute, Toronto'], 'pdf_title_img': 'assets/pdf/title_img/2503.00329.jpg', 'data': {'categories': ['#benchmark', '#multimodal', '#open_source', '#dataset'], 'emoji': '🧠', 'ru': {'title': 'ABC: Мультимодальные встраивания с гибким языковым контролем', 'desc': 'Статья представляет новую мультимодальную модель встраивания под названием ABC, которая объединяет визуальные и текстовые данные. В отличие от существующих подходов, ABC использует глубокую интеграцию изображений и естественного языка. Модель демонстрирует высокую производительность в задачах поиска изображений по тексту и классификации. ABC также позволяет использовать естественный язык для решения сложных задач визуального поиска с неоднозначностями.'}, 'en': {'title': 'ABC: Unifying Vision and Language for Enhanced Multimodal Understanding', 'desc': "This paper presents ABC, a new multimodal embedding model that integrates visual and natural language inputs more effectively than existing CLIP-based methods. Unlike previous models that treat images and text separately, ABC combines these modalities deeply, allowing for better interaction and user control. The model excels in zero-shot tasks, particularly in image-to-text retrieval and classification, outperforming others in the Massive Multimodal Embedding Benchmark. To assess its capabilities, the authors introduce CtrlBench, a benchmark designed to evaluate the model's performance in handling complex visual retrieval tasks with natural language instructions."}, 'zh': {'title': 'ABC：多模态嵌入的新突破', 'desc': '这篇论文介绍了一种名为ABC的多模态嵌入模型，旨在解决视觉检索和分类中的模糊性问题。与现有的CLIP方法不同，ABC通过深度整合图像特征和自然语言指令，提供更强的模态交互。ABC在MSCOCO图像到文本检索任务中表现出色，并在分类和视觉问答任务中取得了最佳性能。通过设计CtrlBench基准，评估了ABC在处理复杂视觉检索问题时的能力，展示了其高质量的表示和灵活的自然语言控制。'}}}, {'id': 'https://huggingface.co/papers/2503.03751', 'title': 'GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control', 'url': 'https://huggingface.co/papers/2503.03751', 'abstract': 'We present GEN3C, a generative video model with precise Camera Control and temporal 3D Consistency. Prior video models already generate realistic videos, but they tend to leverage little 3D information, leading to inconsistencies, such as objects popping in and out of existence. Camera control, if implemented at all, is imprecise, because camera parameters are mere inputs to the neural network which must then infer how the video depends on the camera. In contrast, GEN3C is guided by a 3D cache: point clouds obtained by predicting the pixel-wise depth of seed images or previously generated frames. When generating the next frames, GEN3C is conditioned on the 2D renderings of the 3D cache with the new camera trajectory provided by the user. Crucially, this means that GEN3C neither has to remember what it previously generated nor does it have to infer the image structure from the camera pose. The model, instead, can focus all its generative power on previously unobserved regions, as well as advancing the scene state to the next frame. Our results demonstrate more precise camera control than prior work, as well as state-of-the-art results in sparse-view novel view synthesis, even in challenging settings such as driving scenes and monocular dynamic video. Results are best viewed in videos. Check out our webpage! https://research.nvidia.com/labs/toronto-ai/GEN3C/', 'score': 13, 'issue_id': 2555, 'pub_date': '2025-03-05', 'pub_date_card': {'ru': '5 марта', 'en': 'March 5', 'zh': '3月5日'}, 'hash': '8f5f2ad910a260c0', 'authors': ['Xuanchi Ren', 'Tianchang Shen', 'Jiahui Huang', 'Huan Ling', 'Yifan Lu', 'Merlin Nimier-David', 'Thomas Müller', 'Alexander Keller', 'Sanja Fidler', 'Jun Gao'], 'affiliations': ['NVIDIA', 'University of Toronto', 'Vector Institute'], 'pdf_title_img': 'assets/pdf/title_img/2503.03751.jpg', 'data': {'categories': ['#3d', '#video'], 'emoji': '🎥', 'ru': {'title': 'Точный контроль камеры и 3D-согласованность в генерации видео', 'desc': 'GEN3C - это генеративная модель видео с точным контролем камеры и временной 3D-согласованностью. Она использует 3D-кэш в виде облаков точек, полученных из глубинных карт исходных изображений или ранее сгенерированных кадров. При генерации следующих кадров GEN3C опирается на 2D-рендеринг 3D-кэша с новой траекторией камеры, заданной пользователем. Это позволяет модели сфокусироваться на ранее ненаблюдаемых областях и продвижении состояния сцены, не тратя ресурсы на запоминание предыдущих результатов или вывод структуры изображения из положения камеры.'}, 'en': {'title': 'GEN3C: Mastering Video Generation with 3D Precision and Camera Control', 'desc': 'GEN3C is a generative video model that enhances video generation by incorporating precise camera control and maintaining temporal 3D consistency. Unlike previous models that often lack 3D information, GEN3C utilizes a 3D cache of point clouds derived from depth predictions, allowing for more coherent object presence in videos. The model is conditioned on 2D renderings from this cache, enabling it to generate new frames without needing to remember past outputs or infer scene structure from camera angles. This approach results in superior camera control and state-of-the-art performance in generating novel views, particularly in complex scenarios like driving scenes.'}, 'zh': {'title': 'GEN3C：精确相机控制与时间一致性的视频生成模型', 'desc': '我们提出了GEN3C，这是一种具有精确相机控制和时间一致性的生成视频模型。以往的视频模型虽然能够生成逼真的视频，但往往缺乏3D信息，导致物体出现和消失的不一致性。GEN3C通过3D缓存来指导生成过程，利用从种子图像或先前生成帧中预测的像素深度获得的点云。这样，GEN3C能够在用户提供的新相机轨迹下，专注于生成未观察到的区域，并有效推进场景状态到下一个帧。'}}}, {'id': 'https://huggingface.co/papers/2503.02951', 'title': 'KodCode: A Diverse, Challenging, and Verifiable Synthetic Dataset for Coding', 'url': 'https://huggingface.co/papers/2503.02951', 'abstract': 'We introduce KodCode, a synthetic dataset that addresses the persistent challenge of acquiring high-quality, verifiable training data across diverse difficulties and domains for training Large Language Models for coding. Existing code-focused resources typically fail to ensure either the breadth of coverage (e.g., spanning simple coding tasks to advanced algorithmic problems) or verifiable correctness (e.g., unit tests). In contrast, KodCode comprises question-solution-test triplets that are systematically validated via a self-verification procedure. Our pipeline begins by synthesizing a broad range of coding questions, then generates solutions and test cases with additional attempts allocated to challenging problems. Finally, post-training data synthesis is done by rewriting questions into diverse formats and generating responses under a test-based reject sampling procedure from a reasoning model (DeepSeek R1). This pipeline yields a large-scale, robust and diverse coding dataset. KodCode is suitable for supervised fine-tuning and the paired unit tests also provide great potential for RL tuning. Fine-tuning experiments on coding benchmarks (HumanEval(+), MBPP(+), BigCodeBench, and LiveCodeBench) demonstrate that KodCode-tuned models achieve state-of-the-art performance, surpassing models like Qwen2.5-Coder-32B-Instruct and DeepSeek-R1-Distill-Llama-70B.', 'score': 12, 'issue_id': 2555, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 марта', 'en': 'March 4', 'zh': '3月4日'}, 'hash': '6c344ba0bf71ac84', 'authors': ['Zhangchen Xu', 'Yang Liu', 'Yueqin Yin', 'Mingyuan Zhou', 'Radha Poovendran'], 'affiliations': ['Microsoft', 'The University of Texas at Austin', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2503.02951.jpg', 'data': {'categories': ['#dataset', '#rl', '#optimization', '#synthetic', '#training'], 'emoji': '🧑\u200d💻', 'ru': {'title': 'KodCode: Синтетические данные для обучения ИИ программированию', 'desc': 'KodCode - это синтетический набор данных для обучения больших языковых моделей программированию. Он состоит из триплетов вопрос-решение-тест, которые проходят процедуру самопроверки. Процесс создания KodCode включает синтез вопросов по программированию, генерацию решений и тестовых случаев, а также постобработку данных. Эксперименты показывают, что модели, обученные на KodCode, достигают наилучших результатов на различных бенчмарках по программированию.'}, 'en': {'title': 'KodCode: Elevating Coding Models with Verified Data', 'desc': 'KodCode is a synthetic dataset designed to improve the training of Large Language Models (LLMs) for coding tasks by providing high-quality, verifiable data. It includes question-solution-test triplets that are validated through a self-verification process, ensuring both correctness and a wide range of coding difficulties. The dataset is generated using a systematic pipeline that synthesizes coding questions, creates solutions, and develops test cases, particularly focusing on challenging problems. Fine-tuning experiments show that models trained on KodCode outperform existing models on various coding benchmarks, demonstrating its effectiveness in enhancing LLM performance.'}, 'zh': {'title': 'KodCode：高质量编码数据集的解决方案', 'desc': '我们介绍了KodCode，这是一个合成数据集，旨在解决获取高质量、可验证的训练数据的挑战，以训练大型语言模型进行编码。现有的代码资源通常无法确保覆盖范围广泛或正确性可验证。KodCode由问题-解决方案-测试三元组组成，通过自我验证程序系统地验证。我们的流程包括合成各种编码问题，生成解决方案和测试用例，并在后期通过重写问题和生成响应来进行数据合成，最终生成一个大规模、强大且多样化的编码数据集。'}}}, {'id': 'https://huggingface.co/papers/2503.03278', 'title': 'Enhancing Abnormality Grounding for Vision Language Models with Knowledge Descriptions', 'url': 'https://huggingface.co/papers/2503.03278', 'abstract': 'Visual Language Models (VLMs) have demonstrated impressive capabilities in visual grounding tasks. However, their effectiveness in the medical domain, particularly for abnormality detection and localization within medical images, remains underexplored. A major challenge is the complex and abstract nature of medical terminology, which makes it difficult to directly associate pathological anomaly terms with their corresponding visual features. In this work, we introduce a novel approach to enhance VLM performance in medical abnormality detection and localization by leveraging decomposed medical knowledge. Instead of directly prompting models to recognize specific abnormalities, we focus on breaking down medical concepts into fundamental attributes and common visual patterns. This strategy promotes a stronger alignment between textual descriptions and visual features, improving both the recognition and localization of abnormalities in medical images.We evaluate our method on the 0.23B Florence-2 base model and demonstrate that it achieves comparable performance in abnormality grounding to significantly larger 7B LLaVA-based medical VLMs, despite being trained on only 1.5% of the data used for such models. Experimental results also demonstrate the effectiveness of our approach in both known and previously unseen abnormalities, suggesting its strong generalization capabilities.', 'score': 10, 'issue_id': 2560, 'pub_date': '2025-03-05', 'pub_date_card': {'ru': '5 марта', 'en': 'March 5', 'zh': '3月5日'}, 'hash': '6103dbe5d60b5f3f', 'authors': ['Jun Li', 'Che Liu', 'Wenjia Bai', 'Rossella Arcucci', 'Cosmin I. Bercea', 'Julia A. Schnabel'], 'affiliations': ['Helmholtz AI and Helmholtz Munich, Germany', 'Imperial College London, UK', 'Kings College London, UK', 'Munich Center for Machine Learning, Germany', 'Technical University of Munich, Germany'], 'pdf_title_img': 'assets/pdf/title_img/2503.03278.jpg', 'data': {'categories': ['#cv', '#multimodal', '#healthcare', '#alignment', '#transfer_learning'], 'emoji': '🔬', 'ru': {'title': 'Декомпозиция медицинских знаний для повышения эффективности VLM в анализе медицинских изображений', 'desc': 'Эта статья представляет новый подход к улучшению работы визуальных языковых моделей (VLM) в обнаружении и локализации аномалий на медицинских изображениях. Вместо прямого распознавания конкретных патологий, метод фокусируется на разложении медицинских концепций на базовые атрибуты и общие визуальные паттерны. Это улучшает связь между текстовыми описаниями и визуальными характеристиками, повышая точность распознавания и локализации аномалий. Метод был протестирован на модели Florence-2 и показал результаты, сравнимые с гораздо более крупными медицинскими VLM, несмотря на использование значительно меньшего объема данных для обучения.'}, 'en': {'title': 'Enhancing Medical VLMs through Decomposed Knowledge', 'desc': 'This paper presents a new method to improve Visual Language Models (VLMs) for detecting and locating abnormalities in medical images. The authors address the challenge of complex medical terminology by breaking down medical concepts into simpler attributes and common visual patterns. This approach enhances the alignment between text descriptions and visual features, leading to better performance in recognizing and localizing abnormalities. The proposed method shows competitive results with larger models while using significantly less training data, indicating its efficiency and strong generalization capabilities.'}, 'zh': {'title': '分解医学知识，提升视觉语言模型的异常检测能力', 'desc': '视觉语言模型（VLMs）在视觉定位任务中表现出色，但在医学领域，尤其是医学图像中的异常检测和定位方面，仍然缺乏研究。医学术语的复杂性使得将病理异常术语与相应的视觉特征直接关联变得困难。我们提出了一种新方法，通过分解医学知识来增强VLM在医学异常检测和定位中的性能。该方法通过将医学概念分解为基本属性和常见视觉模式，促进了文本描述与视觉特征之间的更强对齐，从而提高了医学图像中异常的识别和定位能力。'}}}, {'id': 'https://huggingface.co/papers/2503.01836', 'title': 'CrowdSelect: Synthetic Instruction Data Selection with Multi-LLM Wisdom', 'url': 'https://huggingface.co/papers/2503.01836', 'abstract': "Distilling advanced Large Language Models' instruction-following capabilities into smaller models using a selected subset has become a mainstream approach in model training. While existing synthetic instruction data selection strategies rely mainly on single-dimensional signals (i.e., reward scores, model perplexity), they fail to capture the complexity of instruction-following across diverse fields. Therefore, we investigate more diverse signals to capture comprehensive instruction-response pair characteristics and propose three foundational metrics that leverage Multi-LLM wisdom, informed by (1) diverse LLM responses and (2) reward model assessment. Building upon base metrics, we propose CrowdSelect, an integrated metric incorporating a clustering-based approach to maintain response diversity. Our comprehensive experiments demonstrate that our foundation metrics consistently improve performance across 4 base models on MT-bench and Arena-Hard. CrowdSelect, efficiently incorporating all metrics, achieves state-of-the-art performance in both Full and LoRA fine-tuning, showing improvements of 4.81% on Arena-Hard and 11.1% on MT-bench with Llama-3.2-3b-instruct. We hope our findings will bring valuable insights for future research in this direction. Code are available at https://github.com/listentm/crowdselect.", 'score': 9, 'issue_id': 2560, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': 'd59d65fb3b60c043', 'authors': ['Yisen Li', 'Lingfeng Yang', 'Wenxuan Shen', 'Pan Zhou', 'Yao Wan', 'Weiwei Lin', 'Dongping Chen'], 'affiliations': ['Huazhong University of Science and Technology', 'South China University of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2503.01836.jpg', 'data': {'categories': ['#small_models', '#training', '#synthetic', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'CrowdSelect: умный отбор инструкций для обучения языковых моделей', 'desc': 'Статья описывает новый метод отбора инструкций для обучения языковых моделей, названный CrowdSelect. Он использует три основных метрики, основанные на оценках различных большиx языковых моделей и моделей вознаграждения. CrowdSelect также включает кластеризацию для сохранения разнообразия ответов. Эксперименты показали, что этот метод превосходит существующие подходы на бенчмарках MT-bench и Arena-Hard. Авторы надеются, что их исследование внесет вклад в развитие этого направления.'}, 'en': {'title': 'Enhancing Model Training with Diverse Instruction Metrics', 'desc': 'This paper focuses on improving the training of smaller models by distilling the instruction-following abilities of larger language models (LLMs). It critiques existing methods that use simple metrics for selecting synthetic instruction data, which do not adequately reflect the complexity of instruction-following tasks. The authors propose new metrics that utilize diverse responses from multiple LLMs and a reward model to better assess instruction-response pairs. Their method, CrowdSelect, combines these metrics with a clustering approach to enhance response diversity, leading to significant performance improvements in various model evaluations.'}, 'zh': {'title': '提升小模型的指令跟随能力', 'desc': '本论文探讨了如何将大型语言模型的指令跟随能力提炼到更小的模型中。现有的合成指令数据选择策略主要依赖单一维度的信号，未能全面捕捉指令跟随的复杂性。我们提出了三种基础指标，利用多种大型语言模型的智慧，结合多样的响应和奖励模型评估。通过综合实验，我们的CrowdSelect指标在多个基模型上表现出色，显著提升了性能，展示了未来研究的潜力。'}}}, {'id': 'https://huggingface.co/papers/2503.01933', 'title': 'Fine-Tuning Small Language Models for Domain-Specific AI: An Edge AI Perspective', 'url': 'https://huggingface.co/papers/2503.01933', 'abstract': 'Deploying large scale language models on edge devices faces inherent challenges such as high computational demands, energy consumption, and potential data privacy risks. This paper introduces the Shakti Small Language Models (SLMs) Shakti-100M, Shakti-250M, and Shakti-500M which target these constraints headon. By combining efficient architectures, quantization techniques, and responsible AI principles, the Shakti series enables on-device intelligence for smartphones, smart appliances, IoT systems, and beyond. We provide comprehensive insights into their design philosophy, training pipelines, and benchmark performance on both general tasks (e.g., MMLU, Hellaswag) and specialized domains (healthcare, finance, and legal). Our findings illustrate that compact models, when carefully engineered and fine-tuned, can meet and often exceed expectations in real-world edge-AI scenarios.', 'score': 6, 'issue_id': 2563, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': '6fa74210552cc49f', 'authors': ['Rakshit Aralimatti', 'Syed Abdul Gaffar Shakhadri', 'Kruthika KR', 'Kartik Basavaraj Angadi'], 'affiliations': ['SandLogic Technologies Pvt Ltd'], 'pdf_title_img': 'assets/pdf/title_img/2503.01933.jpg', 'data': {'categories': ['#healthcare', '#benchmark', '#training', '#ethics', '#inference', '#small_models', '#optimization'], 'emoji': '📱', 'ru': {'title': 'Малые языковые модели для большого интеллекта на краю сети', 'desc': 'Статья представляет серию малых языковых моделей Shakti, разработанных для применения на периферийных устройствах. Модели Shakti-100M, Shakti-250M и Shakti-500M решают проблемы высоких вычислительных требований, энергопотребления и потенциальных рисков конфиденциальности данных. Используя эффективные архитектуры, методы квантования и принципы ответственного ИИ, серия Shakti обеспечивает локальный интеллект для смартфонов, умных устройств и IoT-систем. Исследование показывает, что компактные модели, при тщательной разработке и настройке, могут соответствовать и часто превосходить ожидания в реальных сценариях периферийного ИИ.'}, 'en': {'title': 'Empowering Edge Devices with Efficient Language Models', 'desc': 'This paper presents the Shakti Small Language Models (SLMs) designed to operate efficiently on edge devices while addressing challenges like high computational needs and energy consumption. The Shakti models, including Shakti-100M, Shakti-250M, and Shakti-500M, utilize advanced architectures and quantization techniques to optimize performance without compromising data privacy. The authors detail the design philosophy, training processes, and benchmark results across various tasks and specialized fields such as healthcare and finance. The results demonstrate that well-engineered compact models can perform effectively in real-world applications, showcasing the potential of on-device AI.'}, 'zh': {'title': '小型语言模型，智能边缘计算的未来', 'desc': '本论文介绍了Shakti小型语言模型（SLMs），包括Shakti-100M、Shakti-250M和Shakti-500M，旨在解决在边缘设备上部署大型语言模型时面临的高计算需求和能耗问题。通过结合高效架构、量化技术和负责任的人工智能原则，Shakti系列实现了智能手机、智能家电和物联网系统的本地智能。我们提供了关于其设计理念、训练流程和在一般任务（如MMLU、Hellaswag）及专业领域（医疗、金融和法律）上的基准性能的全面见解。研究结果表明，经过精心设计和微调的紧凑模型能够在实际边缘人工智能场景中满足并超越预期。'}}}, {'id': 'https://huggingface.co/papers/2502.20317', 'title': 'Mixture of Structural-and-Textual Retrieval over Text-rich Graph Knowledge Bases', 'url': 'https://huggingface.co/papers/2502.20317', 'abstract': 'Text-rich Graph Knowledge Bases (TG-KBs) have become increasingly crucial for answering queries by providing textual and structural knowledge. However, current retrieval methods often retrieve these two types of knowledge in isolation without considering their mutual reinforcement and some hybrid methods even bypass structural retrieval entirely after neighboring aggregation. To fill in this gap, we propose a Mixture of Structural-and-Textual Retrieval (MoR) to retrieve these two types of knowledge via a Planning-Reasoning-Organizing framework. In the Planning stage, MoR generates textual planning graphs delineating the logic for answering queries. Following planning graphs, in the Reasoning stage, MoR interweaves structural traversal and textual matching to obtain candidates from TG-KBs. In the Organizing stage, MoR further reranks fetched candidates based on their structural trajectory. Extensive experiments demonstrate the superiority of MoR in harmonizing structural and textual retrieval with insights, including uneven retrieving performance across different query logics and the benefits of integrating structural trajectories for candidate reranking. Our code is available at https://github.com/Yoega/MoR.', 'score': 6, 'issue_id': 2561, 'pub_date': '2025-02-27', 'pub_date_card': {'ru': '27 февраля', 'en': 'February 27', 'zh': '2月27日'}, 'hash': '686b2ff85600f281', 'authors': ['Yongjia Lei', 'Haoyu Han', 'Ryan A. Rossi', 'Franck Dernoncourt', 'Nedim Lipka', 'Mahantesh M Halappanavar', 'Jiliang Tang', 'Yu Wang'], 'affiliations': ['Adobe Research', 'Michigan State University', 'Pacific Northwest National Laboratory', 'University of Oregon'], 'pdf_title_img': 'assets/pdf/title_img/2502.20317.jpg', 'data': {'categories': ['#benchmark', '#data', '#graphs', '#dataset', '#multimodal', '#reasoning'], 'emoji': '🕸️', 'ru': {'title': 'Гармоничное слияние структурного и текстового поиска в графовых базах знаний', 'desc': 'Эта статья представляет новый метод под названием MoR (Mixture of Structural-and-Textual Retrieval) для работы с графовыми базами знаний, содержащими текстовую информацию. MoR использует трехэтапный подход: планирование, рассуждение и организация, чтобы эффективно объединить структурный и текстовый поиск. Метод генерирует текстовые графы планирования, затем переплетает структурный обход и текстовое сопоставление, и наконец, переранжирует кандидатов на основе их структурных траекторий. Эксперименты показывают превосходство MoR в гармонизации структурного и текстового поиска.'}, 'en': {'title': 'Harmonizing Text and Structure for Better Knowledge Retrieval', 'desc': 'This paper introduces a new method called Mixture of Structural-and-Textual Retrieval (MoR) for improving the retrieval of knowledge from Text-rich Graph Knowledge Bases (TG-KBs). Unlike traditional methods that treat textual and structural knowledge separately, MoR integrates both types through a three-stage framework: Planning, Reasoning, and Organizing. In the Planning stage, it creates graphs that outline the logic for query responses, while the Reasoning stage combines structural traversal with textual matching to gather relevant candidates. Finally, the Organizing stage enhances the selection process by reranking candidates based on their structural paths, leading to better retrieval performance across various query types.'}, 'zh': {'title': '结构与文本知识的完美结合', 'desc': '本文提出了一种新的混合检索方法，称为结构与文本检索的混合体（MoR），旨在同时利用文本和结构知识来回答查询。MoR通过规划-推理-组织的框架来实现这一目标，在规划阶段生成文本规划图，明确回答查询的逻辑。接着，在推理阶段，MoR结合结构遍历和文本匹配，从文本丰富的图知识库中获取候选答案。最后，在组织阶段，MoR根据结构轨迹对获取的候选答案进行重新排序，实验结果表明该方法在结构和文本检索的协调性上具有显著优势。'}}}, {'id': 'https://huggingface.co/papers/2503.03044', 'title': 'QE4PE: Word-level Quality Estimation for Human Post-Editing', 'url': 'https://huggingface.co/papers/2503.03044', 'abstract': "Word-level quality estimation (QE) detects erroneous spans in machine translations, which can direct and facilitate human post-editing. While the accuracy of word-level QE systems has been assessed extensively, their usability and downstream influence on the speed, quality and editing choices of human post-editing remain understudied. Our QE4PE study investigates the impact of word-level QE on machine translation (MT) post-editing in a realistic setting involving 42 professional post-editors across two translation directions. We compare four error-span highlight modalities, including supervised and uncertainty-based word-level QE methods, for identifying potential errors in the outputs of a state-of-the-art neural MT model. Post-editing effort and productivity are estimated by behavioral logs, while quality improvements are assessed by word- and segment-level human annotation. We find that domain, language and editors' speed are critical factors in determining highlights' effectiveness, with modest differences between human-made and automated QE highlights underlining a gap between accuracy and usability in professional workflows.", 'score': 5, 'issue_id': 2560, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 марта', 'en': 'March 4', 'zh': '3月4日'}, 'hash': 'e4d3d7db506b6e1c', 'authors': ['Gabriele Sarti', 'Vilém Zouhar', 'Grzegorz Chrupała', 'Ana Guerberof-Arenas', 'Malvina Nissim', 'Arianna Bisazza'], 'affiliations': ['CLCG, University of Groningen', 'CSAI, Tilburg University', 'ETH Zürich'], 'pdf_title_img': 'assets/pdf/title_img/2503.03044.jpg', 'data': {'categories': ['#machine_translation', '#data', '#multilingual', '#healthcare'], 'emoji': '🔍', 'ru': {'title': 'Оценка качества перевода: мост между точностью и практичностью', 'desc': 'Статья исследует влияние оценки качества перевода на уровне слов (word-level QE) на процесс постредактирования машинного перевода. В исследовании участвовали 42 профессиональных редактора, работавших с двумя направлениями перевода. Сравнивались четыре модальности подсветки ошибок, включая методы на основе обучения с учителем и неопределенности. Результаты показывают, что эффективность подсветки зависит от домена, языка и скорости работы редакторов, при этом разница между ручной и автоматической QE оказалась незначительной.'}, 'en': {'title': 'Enhancing Post-Editing Efficiency with Word-Level Quality Estimation', 'desc': 'This paper explores how word-level quality estimation (QE) can help improve the efficiency of human post-editing in machine translation (MT). It examines the effectiveness of different methods for highlighting potential translation errors, comparing supervised and uncertainty-based approaches. The study involves 42 professional post-editors and assesses their editing speed and quality improvements through detailed behavioral logs and human annotations. The findings reveal that factors like domain, language, and editor speed significantly influence the effectiveness of error highlights, indicating a need to bridge the gap between the accuracy of QE systems and their practical usability in real-world editing tasks.'}, 'zh': {'title': '提升机器翻译后编辑效率的关键', 'desc': '本文研究了词级质量评估（QE）在机器翻译后编辑中的影响。我们分析了42名专业后编辑在两种翻译方向下的表现，比较了四种错误范围高亮方式，包括监督和基于不确定性的词级QE方法。研究发现，领域、语言和编辑速度是影响高亮效果的关键因素。结果表明，人工和自动QE高亮之间存在适度差异，突显了专业工作流程中准确性与可用性之间的差距。'}}}, {'id': 'https://huggingface.co/papers/2503.00307', 'title': 'Remasking Discrete Diffusion Models with Inference-Time Scaling', 'url': 'https://huggingface.co/papers/2503.00307', 'abstract': 'Part of the success of diffusion models stems from their ability to perform iterative refinement, i.e., repeatedly correcting outputs during generation. However, modern masked discrete diffusion lacks this capability: when a token is generated, it cannot be updated again, even when it introduces an error. Here, we address this limitation by introducing the remasking diffusion model (ReMDM) sampler, a method that can be applied to pretrained masked diffusion models in a principled way and that is derived from a discrete diffusion model with a custom remasking backward process. Most interestingly, ReMDM endows discrete diffusion with a form of inference-time compute scaling. By increasing the number of sampling steps, ReMDM generates natural language outputs that approach the quality of autoregressive models, whereas when the computation budget is limited, ReMDM better maintains quality. ReMDM also improves sample quality of masked diffusion models for discretized images, and in scientific domains such as molecule design, ReMDM facilitates diffusion guidance and pushes the Pareto frontier of controllability relative to classical masking and uniform noise diffusion. We provide the code along with a blog post on the project page: https://remdm.github.io.', 'score': 4, 'issue_id': 2572, 'pub_date': '2025-03-01', 'pub_date_card': {'ru': '1 марта', 'en': 'March 1', 'zh': '3月1日'}, 'hash': '7f31677f2cb675e1', 'authors': ['Guanghan Wang', 'Yair Schiff', 'Subham Sekhar Sahoo', 'Volodymyr Kuleshov'], 'affiliations': ['Department of Computer Science, Cornell Unversity'], 'pdf_title_img': 'assets/pdf/title_img/2503.00307.jpg', 'data': {'categories': ['#open_source', '#inference', '#diffusion', '#science', '#multimodal'], 'emoji': '🔄', 'ru': {'title': 'Перемаскировка для улучшения дискретных диффузионных моделей', 'desc': 'Статья представляет новый метод ReMDM (remasking diffusion model), который улучшает процесс генерации в дискретных диффузионных моделях. ReMDM позволяет итеративно уточнять сгенерированные токены, что ранее было невозможно в классических масочных диффузионных моделях. Этот подход повышает качество генерации естественного языка, приближая его к уровню авторегрессионных моделей, особенно при увеличении числа шагов сэмплирования. ReMDM также демонстрирует улучшения в генерации дискретизированных изображений и дизайне молекул.'}, 'en': {'title': 'Enhancing Masked Diffusion with Iterative Refinement', 'desc': 'This paper introduces the remasking diffusion model (ReMDM) sampler, which enhances the capabilities of masked discrete diffusion models by allowing iterative refinement during output generation. Unlike traditional methods where generated tokens cannot be updated, ReMDM enables the correction of errors by applying a remasking backward process. This approach not only improves the quality of natural language outputs to rival autoregressive models but also maintains high quality under limited computational resources. Additionally, ReMDM enhances the performance of masked diffusion models in generating discretized images and aids in molecule design, pushing the boundaries of controllability in scientific applications.'}, 'zh': {'title': '重掩蔽扩散模型：提升生成质量的新方法', 'desc': '扩散模型的成功部分源于其迭代精炼的能力，即在生成过程中不断修正输出。然而，现代的掩蔽离散扩散模型缺乏这种能力：一旦生成一个标记，就无法再次更新，即使它引入了错误。为了解决这个限制，我们提出了重掩蔽扩散模型（ReMDM）采样器，这是一种可以以原则性方式应用于预训练掩蔽扩散模型的方法。ReMDM通过增加采样步骤，生成的自然语言输出质量接近自回归模型，同时在计算预算有限时，ReMDM更好地保持质量。'}}}, {'id': 'https://huggingface.co/papers/2502.18860', 'title': 'Exploring Rewriting Approaches for Different Conversational Tasks', 'url': 'https://huggingface.co/papers/2502.18860', 'abstract': "Conversational assistants often require a question rewriting algorithm that leverages a subset of past interactions to provide a more meaningful (accurate) answer to the user's question or request. However, the exact rewriting approach may often depend on the use case and application-specific tasks supported by the conversational assistant, among other constraints. In this paper, we systematically investigate two different approaches, denoted as rewriting and fusion, on two fundamentally different generation tasks, including a text-to-text generation task and a multimodal generative task that takes as input text and generates a visualization or data table that answers the user's question. Our results indicate that the specific rewriting or fusion approach highly depends on the underlying use case and generative task. In particular, we find that for a conversational question-answering assistant, the query rewriting approach performs best, whereas for a data analysis assistant that generates visualizations and data tables based on the user's conversation with the assistant, the fusion approach works best. Notably, we explore two datasets for the data analysis assistant use case, for short and long conversations, and we find that query fusion always performs better, whereas for the conversational text-based question-answering, the query rewrite approach performs best.", 'score': 4, 'issue_id': 2561, 'pub_date': '2025-02-26', 'pub_date_card': {'ru': '26 февраля', 'en': 'February 26', 'zh': '2月26日'}, 'hash': '1f018cc4f38149bc', 'authors': ['Md Mehrab Tanjim', 'Ryan A. Rossi', 'Mike Rimer', 'Xiang Chen', 'Sungchul Kim', 'Vaishnavi Muppala', 'Tong Yu', 'Zhengmian Hu', 'Ritwik Sinha', 'Wei Zhang', 'Iftikhar Ahamath Burhanuddin', 'Franck Dernoncourt'], 'affiliations': ['Adobe Research'], 'pdf_title_img': 'assets/pdf/title_img/2502.18860.jpg', 'data': {'categories': ['#dataset', '#multimodal'], 'emoji': '🤖', 'ru': {'title': 'Переписывание запросов в разговорных ИИ: один метод не подходит для всех задач', 'desc': 'В статье исследуются два подхода к переписыванию запросов в разговорных ассистентах: переписывание и слияние. Эксперименты проводились на двух различных задачах генерации: текст-в-текст и мультимодальной генерации визуализаций. Результаты показывают, что эффективность подхода зависит от конкретного случая использования и задачи. Для текстового вопросно-ответного ассистента лучше работает переписывание запросов, а для ассистента по анализу данных - слияние запросов.'}, 'en': {'title': 'Tailoring Response Strategies for Conversational Assistants', 'desc': 'This paper explores how conversational assistants can improve their responses by using two methods: rewriting and fusion. The rewriting method modifies user questions to enhance accuracy, while the fusion method combines information from past interactions to generate answers. The study shows that the effectiveness of these methods varies based on the task; rewriting is better for text-based question answering, while fusion excels in generating visualizations from data analysis tasks. The findings highlight the importance of tailoring the approach to the specific use case for optimal performance.'}, 'zh': {'title': '对话助手中的问题重写与融合方法的最佳选择', 'desc': '本论文探讨了对话助手中问题重写算法的两种不同方法：重写和融合。这两种方法在文本生成和多模态生成任务中表现不同，具体取决于应用场景。研究发现，对于对话问答助手，查询重写方法效果最佳；而对于生成可视化和数据表的数据分析助手，查询融合方法更为有效。我们还分析了短对话和长对话的数据集，结果表明查询融合在数据分析任务中始终表现更好。'}}}, {'id': 'https://huggingface.co/papers/2503.01763', 'title': "Retrieval Models Aren't Tool-Savvy: Benchmarking Tool Retrieval for Large Language Models", 'url': 'https://huggingface.co/papers/2503.01763', 'abstract': 'Tool learning aims to augment large language models (LLMs) with diverse tools, enabling them to act as agents for solving practical tasks. Due to the limited context length of tool-using LLMs, adopting information retrieval (IR) models to select useful tools from large toolsets is a critical initial step. However, the performance of IR models in tool retrieval tasks remains underexplored and unclear. Most tool-use benchmarks simplify this step by manually pre-annotating a small set of relevant tools for each task, which is far from the real-world scenarios. In this paper, we propose ToolRet, a heterogeneous tool retrieval benchmark comprising 7.6k diverse retrieval tasks, and a corpus of 43k tools, collected from existing datasets. We benchmark six types of models on ToolRet. Surprisingly, even the models with strong performance in conventional IR benchmarks, exhibit poor performance on ToolRet. This low retrieval quality degrades the task pass rate of tool-use LLMs. As a further step, we contribute a large-scale training dataset with over 200k instances, which substantially optimizes the tool retrieval ability of IR models.', 'score': 4, 'issue_id': 2558, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': 'e6a23582f741dc5b', 'authors': ['Zhengliang Shi', 'Yuhan Wang', 'Lingyong Yan', 'Pengjie Ren', 'Shuaiqiang Wang', 'Dawei Yin', 'Zhaochun Ren'], 'affiliations': ['Baidu Inc., Beijing, China', 'Leiden University, Leiden, The Netherlands', 'Shandong University, Qingdao, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.01763.jpg', 'data': {'categories': ['#training', '#optimization', '#benchmark', '#dataset', '#data'], 'emoji': '🔍', 'ru': {'title': 'ToolRet: Новый вызов для моделей поиска инструментов ИИ', 'desc': 'ToolRet - это новый эталонный тест для оценки поиска инструментов в контексте обучения инструментам для больших языковых моделей (LLM). Он включает 7,6 тысяч разнообразных задач поиска и корпус из 43 тысяч инструментов. Исследование показало, что даже модели с высокой производительностью в традиционных тестах информационного поиска плохо справляются с ToolRet. Авторы также предоставили обучающий набор данных из более чем 200 тысяч примеров для улучшения способностей моделей к поиску инструментов.'}, 'en': {'title': 'Enhancing Tool Retrieval for Language Models with ToolRet', 'desc': 'This paper introduces ToolRet, a benchmark designed to evaluate the effectiveness of information retrieval (IR) models in selecting tools for large language models (LLMs) in practical tasks. The authors highlight that existing benchmarks often rely on a limited set of pre-annotated tools, which does not reflect real-world complexities. Their findings reveal that even high-performing IR models struggle with tool retrieval in this new context, leading to lower task success rates for LLMs. To address this issue, they provide a large-scale training dataset that significantly enhances the tool retrieval capabilities of IR models.'}, 'zh': {'title': '工具检索：提升LLMs的实用能力', 'desc': '本文探讨了工具学习如何增强大型语言模型（LLMs）的能力，使其能够作为代理解决实际任务。由于工具使用的LLMs具有有限的上下文长度，因此采用信息检索（IR）模型从大量工具集中选择有用工具是关键的初步步骤。我们提出了ToolRet，一个包含7.6k多样化检索任务和43k工具的异构工具检索基准，旨在评估IR模型在工具检索任务中的表现。研究发现，即使在传统IR基准上表现良好的模型，在ToolRet上的表现却很差，这降低了工具使用LLMs的任务通过率。'}}}, {'id': 'https://huggingface.co/papers/2503.01729', 'title': 'FLAME: A Federated Learning Benchmark for Robotic Manipulation', 'url': 'https://huggingface.co/papers/2503.01729', 'abstract': 'Recent progress in robotic manipulation has been fueled by large-scale datasets collected across diverse environments. Training robotic manipulation policies on these datasets is traditionally performed in a centralized manner, raising concerns regarding scalability, adaptability, and data privacy. While federated learning enables decentralized, privacy-preserving training, its application to robotic manipulation remains largely unexplored. We introduce FLAME (Federated Learning Across Manipulation Environments), the first benchmark designed for federated learning in robotic manipulation. FLAME consists of: (i) a set of large-scale datasets of over 160,000 expert demonstrations of multiple manipulation tasks, collected across a wide range of simulated environments; (ii) a training and evaluation framework for robotic policy learning in a federated setting. We evaluate standard federated learning algorithms in FLAME, showing their potential for distributed policy learning and highlighting key challenges. Our benchmark establishes a foundation for scalable, adaptive, and privacy-aware robotic learning.', 'score': 3, 'issue_id': 2558, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': '893358a382c79250', 'authors': ['Santiago Bou Betran', 'Alberta Longhini', 'Miguel Vasco', 'Yuchong Zhang', 'Danica Kragic'], 'affiliations': ['KTH Royal Institute of Technology, Stockholm, Sweden'], 'pdf_title_img': 'assets/pdf/title_img/2503.01729.jpg', 'data': {'categories': ['#robotics', '#benchmark', '#dataset'], 'emoji': '🤖', 'ru': {'title': 'Федеративное обучение для масштабируемой и конфиденциальной робототехники', 'desc': 'Статья представляет FLAME - первый бенчмарк для федеративного обучения в робототехнической манипуляции. FLAME включает в себя большой набор данных с более чем 160 000 экспертных демонстраций различных задач манипуляции, собранных в симулированных средах. Бенчмарк также предоставляет фреймворк для обучения и оценки робототехнических политик в федеративной среде. Авторы оценивают стандартные алгоритмы федеративного обучения на FLAME, демонстрируя их потенциал для распределенного обучения политик и выявляя ключевые проблемы.'}, 'en': {'title': 'Empowering Robots with Federated Learning for Privacy and Scalability', 'desc': 'This paper presents FLAME, a benchmark for applying federated learning to robotic manipulation tasks. It addresses the limitations of centralized training by allowing robots to learn from diverse datasets while preserving data privacy. FLAME includes over 160,000 expert demonstrations from various simulated environments, facilitating decentralized training. The study evaluates existing federated learning algorithms, demonstrating their effectiveness and identifying challenges in distributed policy learning for robotics.'}, 'zh': {'title': '联邦学习助力机器人操控的未来', 'desc': '这篇论文介绍了FLAME（跨操控环境的联邦学习），这是一个为机器人操控设计的基准测试。FLAME包含超过160,000个专家演示的大规模数据集，涵盖多种操控任务，收集自多种模拟环境。通过在FLAME中评估标准的联邦学习算法，论文展示了分布式策略学习的潜力，并指出了关键挑战。该基准为可扩展、适应性强且注重隐私的机器人学习奠定了基础。'}}}, {'id': 'https://huggingface.co/papers/2503.01449', 'title': 'Benchmarking Large Language Models for Multi-Language Software Vulnerability Detection', 'url': 'https://huggingface.co/papers/2503.01449', 'abstract': 'Recent advancements in generative AI have led to the widespread adoption of large language models (LLMs) in software engineering, addressing numerous long-standing challenges. However, a comprehensive study examining the capabilities of LLMs in software vulnerability detection (SVD), a crucial aspect of software security, is currently lacking. Existing research primarily focuses on evaluating LLMs using C/C++ datasets. It typically explores only one or two strategies among prompt engineering, instruction tuning, and sequence classification fine-tuning for open-source LLMs. Consequently, there is a significant knowledge gap regarding the effectiveness of diverse LLMs in detecting vulnerabilities across various programming languages. To address this knowledge gap, we present a comprehensive empirical study evaluating the performance of LLMs on the SVD task. We have compiled a comprehensive dataset comprising 8,260 vulnerable functions in Python, 7,505 in Java, and 28,983 in JavaScript. We assess five open-source LLMs using multiple approaches, including prompt engineering, instruction tuning, and sequence classification fine-tuning. These LLMs are benchmarked against five fine-tuned small language models and two open-source static application security testing tools. Furthermore, we explore two avenues to improve LLM performance on SVD: a) Data perspective: Retraining models using downsampled balanced datasets. b) Model perspective: Investigating ensemble learning methods that combine predictions from multiple LLMs. Our comprehensive experiments demonstrate that SVD remains a challenging task for LLMs. This study provides a thorough understanding of the role of LLMs in SVD and offers practical insights for future advancements in leveraging generative AI to enhance software security practices.', 'score': 3, 'issue_id': 2558, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': '1b4593bb9d78ec53', 'authors': ['Ting Zhang', 'Chengran Yang', 'Yindu Su', 'Martin Weyssow', 'Hung Nguyen', 'Tan Bui', 'Hong Jin Kang', 'Yikun Li', 'Eng Lieh Ouh', 'Lwin Khin Shar', 'David Lo'], 'affiliations': ['School of Computer Science, University of Sydney, Australia', 'School of Computing and Information Systems, Singapore Management University, Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2503.01449.jpg', 'data': {'categories': ['#open_source', '#plp', '#training', '#security', '#benchmark', '#dataset', '#data'], 'emoji': '🛡️', 'ru': {'title': 'LLM на страже безопасности кода: новые горизонты в обнаружении уязвимостей', 'desc': 'Статья представляет комплексное исследование возможностей больших языковых моделей (LLM) в обнаружении уязвимостей программного обеспечения (SVD). Авторы оценивают производительность пяти открытых LLM на наборах данных, включающих уязвимые функции на Python, Java и JavaScript, используя различные подходы, такие как инженерия промптов, настройка инструкций и тонкая настройка классификации последовательностей. Исследование также изучает способы улучшения производительности LLM в SVD, включая переобучение на сбалансированных наборах данных и использование ансамблевых методов обучения. Результаты показывают, что SVD остается сложной задачей для LLM, предоставляя ценные insights для будущих разработок в области применения генеративного ИИ для повышения безопасности программного обеспечения.'}, 'en': {'title': 'Unlocking LLMs for Software Vulnerability Detection', 'desc': "This paper investigates the effectiveness of large language models (LLMs) in detecting software vulnerabilities, an important area for software security. It highlights the lack of comprehensive studies on LLMs' capabilities across various programming languages, as most existing research focuses on C/C++ datasets. The authors present an empirical study using a dataset of over 44,000 vulnerable functions from Python, Java, and JavaScript, evaluating five open-source LLMs with different strategies like prompt engineering and instruction tuning. The findings reveal that while LLMs show promise, software vulnerability detection remains a challenging task, providing valuable insights for future improvements in this field."}, 'zh': {'title': '提升软件安全：大型语言模型在漏洞检测中的应用', 'desc': '最近生成性人工智能的进展使得大型语言模型（LLMs）在软件工程中得到了广泛应用，解决了许多长期存在的挑战。然而，目前缺乏对LLMs在软件漏洞检测（SVD）能力的全面研究，这对软件安全至关重要。现有研究主要集中在使用C/C++数据集评估LLMs，通常只探讨了提示工程、指令调优和序列分类微调中的一两种策略。因此，我们进行了一项全面的实证研究，评估LLMs在不同编程语言中检测漏洞的有效性。'}}}, {'id': 'https://huggingface.co/papers/2503.01378', 'title': 'CognitiveDrone: A VLA Model and Evaluation Benchmark for Real-Time Cognitive Task Solving and Reasoning in UAVs', 'url': 'https://huggingface.co/papers/2503.01378', 'abstract': 'This paper introduces CognitiveDrone, a novel Vision-Language-Action (VLA) model tailored for complex Unmanned Aerial Vehicles (UAVs) tasks that demand advanced cognitive abilities. Trained on a dataset comprising over 8,000 simulated flight trajectories across three key categories-Human Recognition, Symbol Understanding, and Reasoning-the model generates real-time 4D action commands based on first-person visual inputs and textual instructions. To further enhance performance in intricate scenarios, we propose CognitiveDrone-R1, which integrates an additional Vision-Language Model (VLM) reasoning module to simplify task directives prior to high-frequency control. Experimental evaluations using our open-source benchmark, CognitiveDroneBench, reveal that while a racing-oriented model (RaceVLA) achieves an overall success rate of 31.3%, the base CognitiveDrone model reaches 59.6%, and CognitiveDrone-R1 attains a success rate of 77.2%. These results demonstrate improvements of up to 30% in critical cognitive tasks, underscoring the effectiveness of incorporating advanced reasoning capabilities into UAV control systems. Our contributions include the development of a state-of-the-art VLA model for UAV control and the introduction of the first dedicated benchmark for assessing cognitive tasks in drone operations. The complete repository is available at cognitivedrone.github.io', 'score': 2, 'issue_id': 2558, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': '8a4aab69ce92453d', 'authors': ['Artem Lykov', 'Valerii Serpiva', 'Muhammad Haris Khan', 'Oleg Sautenkov', 'Artyom Myshlyaev', 'Grik Tadevosyan', 'Yasheerah Yaqoot', 'Dzmitry Tsetserukou'], 'affiliations': ['Intelligent Space Robotics Laboratory, Science Center for Digital Engineering, Technology. Skolkovo Institute'], 'pdf_title_img': 'assets/pdf/title_img/2503.01378.jpg', 'data': {'categories': ['#open_source', '#reasoning', '#benchmark', '#dataset', '#multimodal', '#cv'], 'emoji': '🚁', 'ru': {'title': 'Умные дроны: когнитивное управление БПЛА с помощью ИИ', 'desc': 'В статье представлена модель CognitiveDrone - новая модель Зрение-Язык-Действие (VLA) для сложных задач беспилотных летательных аппаратов (БПЛА). Модель обучена на наборе данных из более чем 8000 симулированных полетов и генерирует команды действий в реальном времени на основе визуальных входных данных и текстовых инструкций. Усовершенствованная версия CognitiveDrone-R1 включает дополнительный модуль рассуждений на основе Модели Зрения-Языка (VLM) для упрощения сложных задач. Экспериментальная оценка показывает, что CognitiveDrone-R1 достигает успешности выполнения задач в 77.2%, что на 30% лучше базовых моделей в критических когнитивных задачах.'}, 'en': {'title': 'CognitiveDrone: Elevating UAV Intelligence with Vision-Language-Action!', 'desc': 'This paper presents CognitiveDrone, a new Vision-Language-Action (VLA) model designed for complex tasks performed by Unmanned Aerial Vehicles (UAVs). It is trained on a dataset of over 8,000 simulated flight paths focusing on Human Recognition, Symbol Understanding, and Reasoning. The model can generate real-time 4D action commands from visual inputs and text instructions, with an enhanced version, CognitiveDrone-R1, that includes a Vision-Language Model (VLM) reasoning module for better task management. Experimental results show significant performance improvements, with CognitiveDrone-R1 achieving a 77.2% success rate, highlighting the importance of advanced reasoning in UAV operations.'}, 'zh': {'title': '智能无人机的认知飞行新纪元', 'desc': '本文介绍了一种名为CognitiveDrone的新型视觉-语言-行动（VLA）模型，专为复杂的无人机任务设计，具备高级认知能力。该模型在超过8000条模拟飞行轨迹的数据集上进行训练，涵盖人类识别、符号理解和推理三个关键类别。CognitiveDrone-R1通过集成额外的视觉-语言模型（VLM）推理模块，进一步提升在复杂场景中的表现。实验结果显示，CognitiveDrone模型的成功率达到59.6%，而CognitiveDrone-R1的成功率更是高达77.2%，证明了将高级推理能力融入无人机控制系统的有效性。'}}}, {'id': 'https://huggingface.co/papers/2503.01372', 'title': 'SwiLTra-Bench: The Swiss Legal Translation Benchmark', 'url': 'https://huggingface.co/papers/2503.01372', 'abstract': "In Switzerland legal translation is uniquely important due to the country's four official languages and requirements for multilingual legal documentation. However, this process traditionally relies on professionals who must be both legal experts and skilled translators -- creating bottlenecks and impacting effective access to justice. To address this challenge, we introduce SwiLTra-Bench, a comprehensive multilingual benchmark of over 180K aligned Swiss legal translation pairs comprising laws, headnotes, and press releases across all Swiss languages along with English, designed to evaluate LLM-based translation systems. Our systematic evaluation reveals that frontier models achieve superior translation performance across all document types, while specialized translation systems excel specifically in laws but under-perform in headnotes. Through rigorous testing and human expert validation, we demonstrate that while fine-tuning open SLMs significantly improves their translation quality, they still lag behind the best zero-shot prompted frontier models such as Claude-3.5-Sonnet. Additionally, we present SwiLTra-Judge, a specialized LLM evaluation system that aligns best with human expert assessments.", 'score': 2, 'issue_id': 2558, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': '3de5be81537fa0fd', 'authors': ['Joel Niklaus', 'Jakob Merane', 'Luka Nenadic', 'Sina Ahmadi', 'Yingqiang Gao', 'Cyrill A. H. Chevalley', 'Claude Humbel', 'Christophe Gösken', 'Lorenzo Tanzi', 'Thomas Lüthi', 'Stefan Palombo', 'Spencer Poff', 'Boling Yang', 'Nan Wu', 'Matthew Guillod', 'Robin Mamié', 'Daniel Brunner', 'Julio Pereyra', 'Niko Grupen'], 'affiliations': ['Canton of Solothurn', 'ETH Zurich', 'Max Planck Institute for Research on Collective Goods', 'Swiss Federal Supreme Court', 'University of Basel', 'University of Geneva', 'University of Lausanne', 'University of Zurich'], 'pdf_title_img': 'assets/pdf/title_img/2503.01372.jpg', 'data': {'categories': ['#multilingual', '#open_source', '#benchmark', '#dataset', '#machine_translation'], 'emoji': '⚖️', 'ru': {'title': 'Революция в юридическом переводе: ИИ покоряет многоязычную Швейцарию', 'desc': 'Статья представляет SwiLTra-Bench - многоязычный набор данных для оценки систем машинного перевода юридических текстов в Швейцарии. Авторы провели систематическую оценку различных моделей, включая крупные языковые модели и специализированные системы перевода. Результаты показывают, что передовые модели достигают лучших результатов во всех типах документов, а дообучение открытых моделей значительно улучшает качество перевода. Также представлена система SwiLTra-Judge для оценки качества перевода, которая хорошо коррелирует с оценками экспертов.'}, 'en': {'title': 'Enhancing Legal Translation with SwiLTra-Bench and LLMs', 'desc': 'This paper addresses the challenges of legal translation in Switzerland, where multiple languages complicate the process. It introduces SwiLTra-Bench, a benchmark dataset with over 180,000 aligned legal translation pairs to evaluate large language model (LLM) translation systems. The findings show that while advanced models perform well across various document types, specialized systems are better for translating laws but struggle with headnotes. The study also highlights the effectiveness of fine-tuning open-source language models, although they still do not match the performance of top zero-shot models like Claude-3.5-Sonnet.'}, 'zh': {'title': '瑞士法律翻译的智能解决方案', 'desc': '在瑞士，由于有四种官方语言，法律翻译显得尤为重要。传统上，这一过程依赖于既是法律专家又是翻译高手的专业人士，导致了瓶颈，影响了公正的有效获取。为了解决这个问题，我们推出了SwiLTra-Bench，这是一个包含超过18万对瑞士法律翻译的多语言基准数据集，旨在评估基于大语言模型的翻译系统。我们的评估显示，前沿模型在所有文档类型的翻译表现上优于其他系统，而专门的翻译系统在法律文本中表现出色，但在头注方面表现不佳。'}}}, {'id': 'https://huggingface.co/papers/2503.00502', 'title': 'Interact, Instruct to Improve: A LLM-Driven Parallel Actor-Reasoner Framework for Enhancing Autonomous Vehicle Interactions', 'url': 'https://huggingface.co/papers/2503.00502', 'abstract': "Autonomous Vehicles (AVs) have entered the commercialization stage, but their limited ability to interact and express intentions still poses challenges in interactions with Human-driven Vehicles (HVs). Recent advances in large language models (LLMs) enable bidirectional human-machine communication, but the conflict between slow inference speed and the need for real-time decision-making challenges practical deployment. To address these issues, this paper introduces a parallel Actor-Reasoner framework designed to enable explicit bidirectional AV-HV interactions across multiple scenarios. First, by facilitating interactions between the LLM-driven Reasoner and heterogeneous simulated HVs during training, an interaction memory database, referred to as the Actor, is established. Then, by introducing the memory partition module and the two-layer memory retrieval module, the Actor's ability to handle heterogeneous HVs is significantly enhanced. Ablation studies and comparisons with other decision-making methods demonstrate that the proposed Actor-Reasoner framework significantly improves safety and efficiency. Finally, with the combination of the external Human-Machine Interface (eHMI) information derived from Reasoner's reasoning and the feasible action solutions retrieved from the Actor, the effectiveness of the proposed Actor-Reasoner is confirmed in multi-scenario field interactions. Our code is available at https://github.com/FanGShiYuu/Actor-Reasoner.", 'score': 2, 'issue_id': 2555, 'pub_date': '2025-03-01', 'pub_date_card': {'ru': '1 марта', 'en': 'March 1', 'zh': '3月1日'}, 'hash': 'd184a5cae68093d5', 'authors': ['Shiyu Fang', 'Jiaqi Liu', 'Chengkai Xu', 'Chen Lv', 'Peng Hang', 'Jian Sun'], 'affiliations': ['College of Transportation, Tongji University, Shanghai 201804, China', 'Nanyang Technological University, 639798, Singapore', 'State Key Lab of Intelligent Transportation System, Beijing 100088, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.00502.jpg', 'data': {'categories': ['#rl', '#robotics', '#inference', '#optimization', '#agents', '#reasoning'], 'emoji': '🚗', 'ru': {'title': 'Интеллектуальное взаимодействие автономных и обычных автомобилей с помощью больших языковых моделей', 'desc': 'Эта статья представляет новую архитектуру Actor-Reasoner для улучшения взаимодействия между автономными и управляемыми человеком транспортными средствами. Авторы используют большие языковые модели для создания базы данных взаимодействий и двухуровневую систему извлечения памяти для работы с разнородными транспортными средствами. Предложенный подход значительно повышает безопасность и эффективность принятия решений по сравнению с другими методами. Эксперименты в реальных условиях подтверждают эффективность предложенной архитектуры Actor-Reasoner в различных сценариях.'}, 'en': {'title': 'Enhancing AV-HV Interactions with the Actor-Reasoner Framework', 'desc': "This paper presents a new framework called the Actor-Reasoner to improve interactions between Autonomous Vehicles (AVs) and Human-driven Vehicles (HVs). It leverages large language models (LLMs) to facilitate real-time communication and decision-making, addressing the challenge of slow inference speeds. The framework includes an interaction memory database, which enhances the AV's ability to understand and respond to various HV behaviors. Experimental results show that this approach significantly boosts both safety and efficiency in multi-scenario driving situations."}, 'zh': {'title': '提升自动驾驶与人类驾驶互动的智能框架', 'desc': '这篇论文介绍了一种新的并行演员-推理器框架，旨在改善自动驾驶汽车（AV）与人类驾驶汽车（HV）之间的互动。通过在训练过程中促进大语言模型（LLM）驱动的推理器与不同类型的模拟HV之间的互动，建立了一个互动记忆数据库。引入记忆分区模块和双层记忆检索模块后，演员的处理能力得到了显著提升。实验结果表明，该框架在多场景交互中显著提高了安全性和效率。'}}}, {'id': 'https://huggingface.co/papers/2503.02954', 'title': 'Reliable and Efficient Multi-Agent Coordination via Graph Neural Network Variational Autoencoders', 'url': 'https://huggingface.co/papers/2503.02954', 'abstract': 'Multi-agent coordination is crucial for reliable multi-robot navigation in shared spaces such as automated warehouses. In regions of dense robot traffic, local coordination methods may fail to find a deadlock-free solution. In these scenarios, it is appropriate to let a central unit generate a global schedule that decides the passing order of robots. However, the runtime of such centralized coordination methods increases significantly with the problem scale. In this paper, we propose to leverage Graph Neural Network Variational Autoencoders (GNN-VAE) to solve the multi-agent coordination problem at scale faster than through centralized optimization. We formulate the coordination problem as a graph problem and collect ground truth data using a Mixed-Integer Linear Program (MILP) solver. During training, our learning framework encodes good quality solutions of the graph problem into a latent space. At inference time, solution samples are decoded from the sampled latent variables, and the lowest-cost sample is selected for coordination. Finally, the feasible proposal with the highest performance index is selected for the deployment. By construction, our GNN-VAE framework returns solutions that always respect the constraints of the considered coordination problem. Numerical results show that our approach trained on small-scale problems can achieve high-quality solutions even for large-scale problems with 250 robots, being much faster than other baselines. Project page: https://mengyuest.github.io/gnn-vae-coord', 'score': 0, 'issue_id': 2574, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 марта', 'en': 'March 4', 'zh': '3月4日'}, 'hash': '1d0c072d834299e0', 'authors': ['Yue Meng', 'Nathalie Majcherczyk', 'Wenliang Liu', 'Scott Kiesel', 'Chuchu Fan', 'Federico Pecora'], 'affiliations': ['Amazon Robotics, North Reading, MA USA', 'Massachusetts Institute of Technology, Cambridge, MA 02139 USA'], 'pdf_title_img': 'assets/pdf/title_img/2503.02954.jpg', 'data': {'categories': ['#optimization', '#training', '#games', '#agents', '#inference', '#graphs'], 'emoji': '🤖', 'ru': {'title': 'GNN-VAE: Быстрая координация множества роботов', 'desc': 'Статья представляет новый подход к координации множества роботов в общих пространствах, таких как автоматизированные склады. Авторы предлагают использовать вариационные автоэнкодеры на основе графовых нейронных сетей (GNN-VAE) для решения проблемы координации в масштабе быстрее, чем с помощью централизованной оптимизации. Модель обучается на данных, сгенерированных решателем задач смешанного целочисленного линейного программирования (MILP). Результаты показывают, что подход может достигать высококачественных решений даже для крупномасштабных проблем с 250 роботами, работая значительно быстрее других базовых методов.'}, 'en': {'title': 'Efficient Multi-Robot Coordination with GNN-VAE', 'desc': 'This paper addresses the challenge of coordinating multiple robots in shared environments, particularly in high-traffic areas where traditional local methods may lead to deadlocks. The authors propose a novel approach using Graph Neural Network Variational Autoencoders (GNN-VAE) to efficiently generate coordination schedules at scale. By framing the coordination problem as a graph problem and utilizing a Mixed-Integer Linear Program (MILP) for data collection, the framework learns to encode effective solutions into a latent space. During inference, it decodes these solutions to select the most optimal coordination strategy, ensuring compliance with all operational constraints while significantly reducing computation time compared to centralized methods.'}, 'zh': {'title': '高效多智能体协调的新方法', 'desc': '多智能体协调在共享空间中的可靠多机器人导航中至关重要，尤其是在机器人交通密集的区域。传统的局部协调方法可能无法找到无死锁的解决方案，因此需要一个中央单元生成全局调度来决定机器人的通行顺序。本文提出利用图神经网络变分自编码器（GNN-VAE）来更快地解决大规模的多智能体协调问题，避免了集中优化方法的高运行时间。通过将协调问题形式化为图问题，并使用混合整数线性规划（MILP）求解器收集真实数据，我们的学习框架能够在潜在空间中编码高质量的解决方案。'}}}, {'id': 'https://huggingface.co/papers/2503.02924', 'title': 'Diverse Controllable Diffusion Policy with Signal Temporal Logic', 'url': 'https://huggingface.co/papers/2503.02924', 'abstract': 'Generating realistic simulations is critical for autonomous system applications such as self-driving and human-robot interactions. However, driving simulators nowadays still have difficulty in generating controllable, diverse, and rule-compliant behaviors for road participants: Rule-based models cannot produce diverse behaviors and require careful tuning, whereas learning-based methods imitate the policy from data but are not designed to follow the rules explicitly. Besides, the real-world datasets are by nature "single-outcome", making the learning method hard to generate diverse behaviors. In this paper, we leverage Signal Temporal Logic (STL) and Diffusion Models to learn controllable, diverse, and rule-aware policy. We first calibrate the STL on the real-world data, then generate diverse synthetic data using trajectory optimization, and finally learn the rectified diffusion policy on the augmented dataset. We test on the NuScenes dataset and our approach can achieve the most diverse rule-compliant trajectories compared to other baselines, with a runtime 1/17X to the second-best approach. In the closed-loop testing, our approach reaches the highest diversity, rule satisfaction rate, and the least collision rate. Our method can generate varied characteristics conditional on different STL parameters in testing. A case study on human-robot encounter scenarios shows our approach can generate diverse and closed-to-oracle trajectories. The annotation tool, augmented dataset, and code are available at https://github.com/mengyuest/pSTL-diffusion-policy.', 'score': 0, 'issue_id': 2574, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 марта', 'en': 'March 4', 'zh': '3月4日'}, 'hash': 'adc4dd2a16bb83a4', 'authors': ['Yue Meng', 'Chuchu fan'], 'affiliations': ['Department of Aeronautics and Astronautics, Massachusetts Institute of Technology, Cambridge, MA 02139 USA'], 'pdf_title_img': 'assets/pdf/title_img/2503.02924.jpg', 'data': {'categories': ['#dataset', '#training', '#data', '#rl', '#agents', '#diffusion', '#synthetic'], 'emoji': '🚗', 'ru': {'title': 'Реалистичные и разнообразные симуляции дорожного движения с помощью STL и диффузионных моделей', 'desc': 'Эта статья представляет новый подход к генерации реалистичных симуляций для автономных систем, таких как беспилотные автомобили и взаимодействие человека с роботом. Авторы используют комбинацию Сигнальной Темпоральной Логики (STL) и Диффузионных Моделей для создания контролируемой, разнообразной и соблюдающей правила политики поведения участников дорожного движения. Метод сначала калибрует STL на реальных данных, затем генерирует разнообразные синтетические данные с помощью оптимизации траекторий, и наконец обучает скорректированную диффузионную политику на расширенном наборе данных. Результаты показывают, что подход превосходит базовые методы по разнообразию, соблюдению правил и безопасности, а также позволяет генерировать различные характеристики в зависимости от параметров STL.'}, 'en': {'title': 'Diverse and Rule-Compliant Simulations for Autonomous Systems', 'desc': 'This paper addresses the challenge of generating realistic simulations for autonomous systems, particularly in driving scenarios. It combines Signal Temporal Logic (STL) with Diffusion Models to create a policy that is both controllable and diverse while adhering to traffic rules. By calibrating STL on real-world data and generating synthetic data through trajectory optimization, the authors enhance the learning process to produce varied and rule-compliant behaviors. The results demonstrate that their method outperforms existing approaches in terms of diversity and safety in simulated environments.'}, 'zh': {'title': '生成多样化且遵循规则的行为策略', 'desc': '本文提出了一种新方法，通过信号时序逻辑（STL）和扩散模型来生成可控、多样且遵循规则的行为策略，以解决当前驾驶模拟器在生成道路参与者行为时的局限性。我们首先在真实数据上校准STL，然后利用轨迹优化生成多样的合成数据，最后在增强数据集上学习修正的扩散策略。实验结果表明，我们的方法在NuScenes数据集上能够生成最具多样性且符合规则的轨迹，且运行时间显著优于其他基线方法。通过闭环测试，我们的方法在多样性、规则满足率和碰撞率方面均表现最佳，能够根据不同的STL参数生成多样化的特征。'}}}, {'id': 'https://huggingface.co/papers/2503.02682', 'title': 'MPO: Boosting LLM Agents with Meta Plan Optimization', 'url': 'https://huggingface.co/papers/2503.02682', 'abstract': "Recent advancements in large language models (LLMs) have enabled LLM-based agents to successfully tackle interactive planning tasks. However, despite their successes, existing approaches often suffer from planning hallucinations and require retraining for each new agent. To address these challenges, we propose the Meta Plan Optimization (MPO) framework, which enhances agent planning capabilities by directly incorporating explicit guidance. Unlike previous methods that rely on complex knowledge, which either require significant human effort or lack quality assurance, MPO leverages high-level general guidance through meta plans to assist agent planning and enables continuous optimization of the meta plans based on feedback from the agent's task execution. Our experiments conducted on two representative tasks demonstrate that MPO significantly outperforms existing baselines. Moreover, our analysis indicates that MPO provides a plug-and-play solution that enhances both task completion efficiency and generalization capabilities in previous unseen scenarios.", 'score': 15, 'issue_id': 2535, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 марта', 'en': 'March 4', 'zh': '3月4日'}, 'hash': 'e5fa3c849c1eee72', 'authors': ['Weimin Xiong', 'Yifan Song', 'Qingxiu Dong', 'Bingchan Zhao', 'Feifan Song', 'Xun Wang', 'Sujian Li'], 'affiliations': ['National Key Laboratory for Multimedia Information Processing, School of Computer Science, Peking University', 'Peking University', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2503.02682.jpg', 'data': {'categories': ['#rl', '#training', '#optimization', '#hallucinations', '#agents'], 'emoji': '🧠', 'ru': {'title': 'Метапланы для умных агентов: эффективнее, универсальнее, без галлюцинаций', 'desc': 'Статья представляет новый подход к улучшению планирования задач агентами на основе больших языковых моделей (LLM). Метод под названием Meta Plan Optimization (MPO) использует высокоуровневые метапланы для руководства агентом и оптимизирует их на основе обратной связи. MPO превосходит существующие методы в эффективности выполнения задач и способности к обобщению. Этот подход не требует переобучения для каждого нового агента и решает проблему галлюцинаций при планировании.'}, 'en': {'title': 'Enhancing Agent Planning with Meta Plans', 'desc': "This paper introduces the Meta Plan Optimization (MPO) framework to improve the planning abilities of large language model (LLM)-based agents. MPO addresses issues like planning hallucinations and the need for retraining by using high-level meta plans for guidance. This approach allows for continuous optimization based on feedback from the agent's performance in tasks. Experimental results show that MPO not only outperforms existing methods but also enhances efficiency and adaptability in new situations."}, 'zh': {'title': '元规划优化：提升智能体规划能力的创新框架', 'desc': '最近，大型语言模型（LLMs）的进步使得基于LLM的智能体能够成功处理交互式规划任务。然而，现有方法常常面临规划幻觉的问题，并且每个新智能体都需要重新训练。为了解决这些挑战，我们提出了元规划优化（MPO）框架，通过直接引入明确的指导来增强智能体的规划能力。我们的实验表明，MPO在任务完成效率和在未见场景中的泛化能力上显著优于现有基线。'}}}, {'id': 'https://huggingface.co/papers/2503.02846', 'title': 'Mask-DPO: Generalizable Fine-grained Factuality Alignment of LLMs', 'url': 'https://huggingface.co/papers/2503.02846', 'abstract': 'Large language models (LLMs) exhibit hallucinations (i.e., unfaithful or nonsensical information) when serving as AI assistants in various domains. Since hallucinations always come with truthful content in the LLM responses, previous factuality alignment methods that conduct response-level preference learning inevitably introduced noises during training. Therefore, this paper proposes a fine-grained factuality alignment method based on Direct Preference Optimization (DPO), called Mask-DPO. Incorporating sentence-level factuality as mask signals, Mask-DPO only learns from factually correct sentences in the preferred samples and prevents the penalty on factual contents in the not preferred samples, which resolves the ambiguity in the preference learning. Extensive experimental results demonstrate that Mask-DPO can significantly improve the factuality of LLMs responses to questions from both in-domain and out-of-domain datasets, although these questions and their corresponding topics are unseen during training. Only trained on the ANAH train set, the score of Llama3.1-8B-Instruct on the ANAH test set is improved from 49.19% to 77.53%, even surpassing the score of Llama3.1-70B-Instruct (53.44%), while its FactScore on the out-of-domain Biography dataset is also improved from 30.29% to 39.39%. We further study the generalization property of Mask-DPO using different training sample scaling strategies and find that scaling the number of topics in the dataset is more effective than the number of questions. We provide a hypothesis of what factual alignment is doing with LLMs, on the implication of this phenomenon, and conduct proof-of-concept experiments to verify it. We hope the method and the findings pave the way for future research on scaling factuality alignment.', 'score': 14, 'issue_id': 2535, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 марта', 'en': 'March 4', 'zh': '3月4日'}, 'hash': '7e9277cf8ca5bb98', 'authors': ['Yuzhe Gu', 'Wenwei Zhang', 'Chengqi Lyu', 'Dahua Lin', 'Kai Chen'], 'affiliations': ['MMLab, The Chinese University of Hong Kong', 'Shanghai AI Laboratory', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2503.02846.jpg', 'data': {'categories': ['#hallucinations', '#training', '#rlhf', '#alignment'], 'emoji': '🎭', 'ru': {'title': 'Mask-DPO: точная настройка фактов в ответах языковых моделей', 'desc': 'Эта статья представляет метод Mask-DPO для улучшения фактической точности больших языковых моделей (LLM). Метод основан на оптимизации прямых предпочтений (DPO) и использует маскирование на уровне предложений для обучения только на фактически верных частях ответов. Экспериментальные результаты показывают значительное улучшение фактической точности как на знакомых, так и на новых данных. Исследование также выявило, что масштабирование количества тем в обучающем наборе более эффективно, чем увеличение числа вопросов.'}, 'en': {'title': 'Enhancing Factuality in LLMs with Mask-DPO', 'desc': 'This paper addresses the issue of hallucinations in large language models (LLMs), where they generate incorrect or nonsensical information despite including some accurate content. The authors introduce a new method called Mask-DPO, which focuses on fine-grained factuality alignment by using sentence-level factuality as mask signals. This approach allows the model to learn only from factually correct sentences in preferred samples, reducing noise during training and improving the overall factual accuracy of LLM responses. Experimental results show that Mask-DPO significantly enhances the factuality of LLMs, even on unseen questions and topics, outperforming larger models in certain tests.'}, 'zh': {'title': '提升大型语言模型的事实性对齐', 'desc': '大型语言模型（LLMs）在作为AI助手时常常会出现幻觉现象，即提供不真实或无意义的信息。以往的事实对齐方法在训练过程中引入了噪声，因为它们在响应级别进行偏好学习。本文提出了一种基于直接偏好优化（DPO）的细粒度事实对齐方法，称为Mask-DPO，该方法通过句子级别的事实作为掩码信号，仅从事实正确的句子中学习，从而提高了LLMs的响应准确性。实验结果表明，Mask-DPO显著提升了LLMs在未见问题上的事实性，尤其是在不同领域的数据集上表现优异。'}}}, {'id': 'https://huggingface.co/papers/2503.02879', 'title': 'Wikipedia in the Era of LLMs: Evolution and Risks', 'url': 'https://huggingface.co/papers/2503.02879', 'abstract': "In this paper, we present a thorough analysis of the impact of Large Language Models (LLMs) on Wikipedia, examining the evolution of Wikipedia through existing data and using simulations to explore potential risks. We begin by analyzing page views and article content to study Wikipedia's recent changes and assess the impact of LLMs. Subsequently, we evaluate how LLMs affect various Natural Language Processing (NLP) tasks related to Wikipedia, including machine translation and retrieval-augmented generation (RAG). Our findings and simulation results reveal that Wikipedia articles have been influenced by LLMs, with an impact of approximately 1%-2% in certain categories. If the machine translation benchmark based on Wikipedia is influenced by LLMs, the scores of the models may become inflated, and the comparative results among models might shift as well. Moreover, the effectiveness of RAG might decrease if the knowledge base becomes polluted by LLM-generated content. While LLMs have not yet fully changed Wikipedia's language and knowledge structures, we believe that our empirical findings signal the need for careful consideration of potential future risks.", 'score': 13, 'issue_id': 2535, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 марта', 'en': 'March 4', 'zh': '3月4日'}, 'hash': 'd8fedc8bf5ffc308', 'authors': ['Siming Huang', 'Yuliang Xu', 'Mingmeng Geng', 'Yao Wan', 'Dongping Chen'], 'affiliations': ['Huazhong University of Science and Technology', 'International School for Advanced Studies (SISSA)'], 'pdf_title_img': 'assets/pdf/title_img/2503.02879.jpg', 'data': {'categories': ['#rag', '#benchmark', '#machine_translation', '#dataset', '#multimodal', '#science', '#data'], 'emoji': '🧠', 'ru': {'title': 'Большие языковые модели меняют лицо Википедии: анализ влияния и потенциальных рисков', 'desc': 'Эта статья представляет анализ влияния больших языковых моделей (LLM) на Википедию. Исследователи изучили изменения в просмотрах страниц и содержании статей, а также провели симуляции для оценки потенциальных рисков. Результаты показывают, что LLM повлияли на 1-2% статей в некоторых категориях, что может привести к искажению результатов в задачах машинного перевода и генерации с использованием внешних знаний (RAG). Авторы призывают к осторожности в отношении будущих рисков, связанных с влиянием LLM на структуру знаний в Википедии.'}, 'en': {'title': "Navigating the Impact of LLMs on Wikipedia's Evolution", 'desc': 'This paper analyzes how Large Language Models (LLMs) are affecting Wikipedia by examining changes in page views and article content. It assesses the influence of LLMs on various Natural Language Processing (NLP) tasks, such as machine translation and retrieval-augmented generation (RAG). The study finds that LLMs have caused a 1%-2% impact on certain Wikipedia categories, which could inflate machine translation benchmarks and alter model comparisons. The authors emphasize the importance of monitoring these changes to mitigate potential risks associated with LLM-generated content.'}, 'zh': {'title': '大型语言模型对维基百科的影响分析', 'desc': '本文深入分析了大型语言模型（LLMs）对维基百科的影响，研究了维基百科的演变。我们通过分析页面浏览量和文章内容，评估LLMs对维基百科的影响，发现某些类别的影响约为1%-2%。此外，我们还评估了LLMs对与维基百科相关的自然语言处理（NLP）任务的影响，包括机器翻译和检索增强生成（RAG）。我们的研究结果表明，LLMs可能会导致机器翻译基准的分数膨胀，并可能影响模型之间的比较结果，因此需要对未来的潜在风险进行仔细考虑。'}}}, {'id': 'https://huggingface.co/papers/2503.01935', 'title': 'MultiAgentBench: Evaluating the Collaboration and Competition of LLM agents', 'url': 'https://huggingface.co/papers/2503.01935', 'abstract': 'Large Language Models (LLMs) have shown remarkable capabilities as autonomous agents, yet existing benchmarks either focus on single-agent tasks or are confined to narrow domains, failing to capture the dynamics of multi-agent coordination and competition. In this paper, we introduce MultiAgentBench, a comprehensive benchmark designed to evaluate LLM-based multi-agent systems across diverse, interactive scenarios. Our framework measures not only task completion but also the quality of collaboration and competition using novel, milestone-based key performance indicators. Moreover, we evaluate various coordination protocols (including star, chain, tree, and graph topologies) and innovative strategies such as group discussion and cognitive planning. Notably, gpt-4o-mini reaches the average highest task score, graph structure performs the best among coordination protocols in the research scenario, and cognitive planning improves milestone achievement rates by 3%. Code and datasets are public available at https://github.com/MultiagentBench/MARBLE.', 'score': 10, 'issue_id': 2532, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': '4353f27d396c322a', 'authors': ['Kunlun Zhu', 'Hongyi Du', 'Zhaochen Hong', 'Xiaocheng Yang', 'Shuyi Guo', 'Zhe Wang', 'Zhenhailong Wang', 'Cheng Qian', 'Xiangru Tang', 'Heng Ji', 'Jiaxuan You'], 'affiliations': ['University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2503.01935.jpg', 'data': {'categories': ['#games', '#optimization', '#open_source', '#benchmark', '#agents'], 'emoji': '🤖', 'ru': {'title': 'MultiAgentBench: новый стандарт для оценки мультиагентных LLM-систем', 'desc': 'Статья представляет MultiAgentBench - комплексный бенчмарк для оценки мультиагентных систем на основе больших языковых моделей (LLM) в различных интерактивных сценариях. Фреймворк измеряет не только выполнение задач, но и качество сотрудничества и конкуренции с помощью новых ключевых показателей эффективности. Авторы оценивают различные протоколы координации и инновационные стратегии, такие как групповое обсуждение и когнитивное планирование. Результаты показывают, что gpt-4o-mini достигает наивысшего среднего балла за выполнение задач, а графовая структура лучше всего работает среди протоколов координации в исследовательском сценарии.'}, 'en': {'title': 'Evaluating LLMs in Multi-Agent Dynamics', 'desc': 'This paper presents MultiAgentBench, a new benchmark for assessing the performance of Large Language Models (LLMs) in multi-agent environments. Unlike previous benchmarks that focus on single-agent tasks, MultiAgentBench evaluates how well LLMs can collaborate and compete in various interactive scenarios. The framework introduces key performance indicators that measure both task completion and the quality of interactions among agents. The study also explores different coordination protocols and innovative strategies, revealing that certain configurations, like graph structures and cognitive planning, significantly enhance performance.'}, 'zh': {'title': '多智能体系统的全面评估基准', 'desc': '本文介绍了MultiAgentBench，这是一个全面的基准测试，旨在评估基于大型语言模型（LLM）的多智能体系统在多样化互动场景中的表现。该框架不仅测量任务完成情况，还评估协作和竞争的质量，使用新颖的里程碑式关键绩效指标。我们还评估了多种协调协议（如星形、链形、树形和图形拓扑）以及创新策略，如小组讨论和认知规划。研究表明，gpt-4o-mini在任务得分上表现最佳，图形结构在协调协议中表现最佳，而认知规划提高了里程碑达成率3%。'}}}, {'id': 'https://huggingface.co/papers/2503.01328', 'title': 'PipeOffload: Improving Scalability of Pipeline Parallelism with Memory Optimization', 'url': 'https://huggingface.co/papers/2503.01328', 'abstract': 'Pipeline parallelism (PP) is widely used for training large language models (LLMs), yet its scalability is often constrained by high activation memory consumption as the number of in-flight microbatches grows with the degree of PP. In this paper, we focus on addressing this challenge by leveraging the under-explored memory offload strategy in PP. With empirical study, we discover that in the majority of standard configurations, at least half, and potentially all, of the activations can be offloaded with negligible overhead. In the cases where full overload is not possible, we introduce a novel selective offload strategy that decreases peak activation memory in a better-than-linear manner. Furthermore, we integrate memory offload with other techniques to jointly consider overall throughput and memory limitation. Our experiments proves that the per-device activation memory effectively reduces with the total number of stages, making PP a stronger alternative than TP, offering up to a 19\\% acceleration with even lower memory consumption. The implementation is open-sourced at https://github.com/sail-sg/zero-bubble-pipeline-parallelism{this url}.', 'score': 10, 'issue_id': 2532, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': '75e25b312e4cef8a', 'authors': ['Xinyi Wan', 'Penghui Qi', 'Guangxing Huang', 'Jialin Li', 'Min Lin'], 'affiliations': ['National University of', 'Sea AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2503.01328.jpg', 'data': {'categories': ['#open_source', '#inference', '#optimization', '#training'], 'emoji': '🚀', 'ru': {'title': 'Оптимизация памяти для масштабирования больших языковых моделей', 'desc': 'Статья посвящена улучшению масштабируемости конвейерного параллелизма при обучении больших языковых моделей. Авторы предлагают стратегию выгрузки активаций из памяти, которая позволяет значительно снизить пиковое потребление памяти. Они также представляют метод выборочной выгрузки для случаев, когда полная выгрузка невозможна. Эксперименты показывают, что данный подход делает конвейерный параллелизм более эффективной альтернативой тензорному параллелизму, ускоряя обучение на 19% при меньшем потреблении памяти.'}, 'en': {'title': 'Optimizing Memory Usage in Pipeline Parallelism for Faster Training', 'desc': 'This paper addresses the challenge of high activation memory consumption in pipeline parallelism (PP) when training large language models (LLMs). The authors explore a memory offload strategy that allows for significant reductions in memory usage, showing that up to half of the activations can be offloaded with minimal impact on performance. They also propose a selective offload strategy that further decreases peak activation memory in a more efficient way. The results demonstrate that using memory offload in conjunction with other techniques can enhance throughput while reducing memory requirements, making PP a more viable option compared to tensor parallelism (TP).'}, 'zh': {'title': '优化管道并行性，降低内存消耗！', 'desc': '本文探讨了在训练大型语言模型时，管道并行性（PP）面临的高激活内存消耗问题。我们提出了一种内存卸载策略，可以在大多数标准配置中卸载至少一半的激活，几乎没有额外开销。对于无法完全卸载的情况，我们引入了一种新的选择性卸载策略，显著降低了峰值激活内存。实验结果表明，随着阶段数量的增加，每个设备的激活内存有效减少，使得PP成为比张量并行性（TP）更强的选择，提供高达19%的加速，同时降低内存消耗。'}}}, {'id': 'https://huggingface.co/papers/2503.00735', 'title': 'LADDER: Self-Improving LLMs Through Recursive Problem Decomposition', 'url': 'https://huggingface.co/papers/2503.00735', 'abstract': "We introduce LADDER (Learning through Autonomous Difficulty-Driven Example Recursion), a framework which enables Large Language Models to autonomously improve their problem-solving capabilities through self-guided learning by recursively generating and solving progressively simpler variants of complex problems. Unlike prior approaches that require curated datasets or human feedback, LADDER leverages a model's own capabilities to generate easier question variants. We demonstrate LADDER's effectiveness in the subject of mathematical integration, improving Llama 3.2 3B's accuracy from 1% to 82% on undergraduate-level problems and enabling Qwen2.5 7B Deepseek-R1 Distilled to achieve 73% on the MIT Integration Bee qualifying examination. We also introduce TTRL (Test-Time Reinforcement Learning), where we perform reinforcement learning on variants of test problems at inference time. TTRL enables Qwen2.5 7B Deepseek-R1 Distilled to achieve a state-of-the-art score of 90% on the MIT Integration Bee qualifying examination, surpassing OpenAI o1's performance. These results show how self-directed strategic learning can achieve significant capability improvements without relying on architectural scaling or human supervision.", 'score': 9, 'issue_id': 2540, 'pub_date': '2025-03-02', 'pub_date_card': {'ru': '2 марта', 'en': 'March 2', 'zh': '3月2日'}, 'hash': 'c9ce66a728a6df87', 'authors': ['Toby Simonds', 'Akira Yoshiyama'], 'affiliations': ['Tufa Labs'], 'pdf_title_img': 'assets/pdf/title_img/2503.00735.jpg', 'data': {'categories': ['#math', '#reasoning', '#optimization', '#training', '#rl'], 'emoji': '🧮', 'ru': {'title': 'Самообучение языковых моделей через генерацию упрощенных задач', 'desc': 'Представлен метод LADDER, позволяющий языковым моделям самостоятельно улучшать навыки решения задач путем генерации и решения упрощенных вариантов сложных проблем. В отличие от предыдущих подходов, LADDER не требует подготовленных датасетов или обратной связи от людей. Эффективность метода продемонстрирована на задачах математического интегрирования, где точность модели Llama 3.2 3B выросла с 1% до 82% на задачах университетского уровня. Также представлен метод TTRL, использующий обучение с подкреплением во время вывода, что позволило модели Qwen2.5 7B достичь рекордных 90% на отборочном экзамене MIT Integration Bee.'}, 'en': {'title': 'Empowering Models to Learn and Solve Problems Autonomously', 'desc': 'LADDER is a new framework that helps Large Language Models (LLMs) improve their problem-solving skills by creating and solving simpler versions of complex problems on their own. This method does not need pre-made datasets or human input, as it allows the model to generate easier questions based on its own understanding. The framework has shown remarkable success in mathematical integration, significantly boosting the accuracy of models like Llama 3.2 and Qwen2.5 on challenging exams. Additionally, the introduction of Test-Time Reinforcement Learning (TTRL) further enhances performance by applying reinforcement learning during the testing phase, leading to state-of-the-art results.'}, 'zh': {'title': '自主学习，提升能力的全新框架', 'desc': '本文介绍了LADDER（通过自主难度驱动的示例递归学习）框架，该框架使大型语言模型能够通过自我引导学习，逐步生成和解决复杂问题的简化变体，从而提高其解决问题的能力。与以往需要人工反馈或精心策划数据集的方法不同，LADDER利用模型自身的能力生成更简单的问题变体。我们展示了LADDER在数学积分领域的有效性，使Llama 3.2 3B在本科水平问题上的准确率从1%提高到82%，并使Qwen2.5 7B Deepseek-R1 Distilled在MIT积分竞赛资格考试中达到73%。此外，我们还引入了TTRL（测试时强化学习），在推理时对测试问题的变体进行强化学习，使Qwen2.5 7B Deepseek-R1 Distilled在MIT积分竞赛资格考试中获得90%的领先成绩，超越了OpenAI o1的表现。'}}}, {'id': 'https://huggingface.co/papers/2503.02368', 'title': 'Iterative Value Function Optimization for Guided Decoding', 'url': 'https://huggingface.co/papers/2503.02368', 'abstract': 'While Reinforcement Learning from Human Feedback (RLHF) has become the predominant method for controlling language model outputs, it suffers from high computational costs and training instability. Guided decoding, especially value-guided methods, offers a cost-effective alternative by controlling outputs without re-training models. However, the accuracy of the value function is crucial for value-guided decoding, as inaccuracies can lead to suboptimal decision-making and degraded performance. Existing methods struggle with accurately estimating the optimal value function, leading to less effective control. We propose Iterative Value Function Optimization, a novel framework that addresses these limitations through two key components: Monte Carlo Value Estimation, which reduces estimation variance by exploring diverse trajectories, and Iterative On-Policy Optimization, which progressively improves value estimation through collecting trajectories from value-guided policies. Extensive experiments on text summarization, multi-turn dialogue, and instruction following demonstrate the effectiveness of value-guided decoding approaches in aligning language models. These approaches not only achieve alignment but also significantly reduce computational costs by leveraging principled value function optimization for efficient and effective control.', 'score': 9, 'issue_id': 2538, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 марта', 'en': 'March 4', 'zh': '3月4日'}, 'hash': '6a7f0679f238bd4d', 'authors': ['Zhenhua Liu', 'Lijun Li', 'Ruizhe Chen', 'Yuxian Jiang', 'Tong Zhu', 'Wenliang Chen', 'Jing Shao'], 'affiliations': ['Fudan University', 'Shanghai Artificial Intelligence Laboratory', 'Soochow University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.02368.jpg', 'data': {'categories': ['#optimization', '#alignment', '#training', '#rlhf'], 'emoji': '🧠', 'ru': {'title': 'Эффективное управление языковыми моделями без переобучения', 'desc': "Статья представляет новый метод управления выходными данными языковых моделей, называемый 'Итеративная оптимизация функции ценности'. Этот подход предлагает альтернативу традиционному обучению с подкреплением на основе обратной связи от человека (RLHF), снижая вычислительные затраты и повышая стабильность обучения. Метод использует управляемое декодирование на основе функции ценности, улучшая её точность с помощью оценки Монте-Карло и итеративной оптимизации. Эксперименты показали эффективность предложенного подхода в задачах суммаризации текста, многоэтапного диалога и выполнения инструкций."}, 'en': {'title': 'Optimizing Value Functions for Efficient Language Model Control', 'desc': 'This paper addresses the challenges of Reinforcement Learning from Human Feedback (RLHF) in controlling language models, particularly its high computational costs and instability. It introduces a new framework called Iterative Value Function Optimization, which enhances value-guided decoding methods by improving the accuracy of the value function. The framework employs Monte Carlo Value Estimation to minimize estimation variance and Iterative On-Policy Optimization to refine value estimation through trajectory collection. Experimental results show that this approach not only aligns language models effectively but also reduces computational expenses significantly.'}, 'zh': {'title': '高效控制语言模型的新方法', 'desc': '本论文探讨了人类反馈强化学习（RLHF）在控制语言模型输出时的高计算成本和训练不稳定性问题。我们提出了一种新的框架——迭代值函数优化，通过蒙特卡洛值估计和迭代在线优化来提高值函数的准确性。该方法通过探索多样化的轨迹来减少估计方差，并逐步改进值估计。实验结果表明，基于值引导的解码方法在文本摘要、多轮对话和指令跟随任务中表现出色，显著降低了计算成本。'}}}, {'id': 'https://huggingface.co/papers/2503.00955', 'title': 'SemViQA: A Semantic Question Answering System for Vietnamese Information Fact-Checking', 'url': 'https://huggingface.co/papers/2503.00955', 'abstract': 'The rise of misinformation, exacerbated by Large Language Models (LLMs) like GPT and Gemini, demands robust fact-checking solutions, especially for low-resource languages like Vietnamese. Existing methods struggle with semantic ambiguity, homonyms, and complex linguistic structures, often trading accuracy for efficiency. We introduce SemViQA, a novel Vietnamese fact-checking framework integrating Semantic-based Evidence Retrieval (SER) and Two-step Verdict Classification (TVC). Our approach balances precision and speed, achieving state-of-the-art results with 78.97\\% strict accuracy on ISE-DSC01 and 80.82\\% on ViWikiFC, securing 1st place in the UIT Data Science Challenge. Additionally, SemViQA Faster improves inference speed 7x while maintaining competitive accuracy. SemViQA sets a new benchmark for Vietnamese fact verification, advancing the fight against misinformation. The source code is available at: https://github.com/DAVID-NGUYEN-S16/SemViQA.', 'score': 7, 'issue_id': 2535, 'pub_date': '2025-03-02', 'pub_date_card': {'ru': '2 марта', 'en': 'March 2', 'zh': '3月2日'}, 'hash': '651aacbd378465bd', 'authors': ['Nam V. Nguyen', 'Dien X. Tran', 'Thanh T. Tran', 'Anh T. Hoang', 'Tai V. Duong', 'Di T. Le', 'Phuc-Lu Le'], 'affiliations': ['FPT Software AI Center, Viet Nam', 'FPT Telecom, Viet Nam', 'Faculty of Information Technology, Industrial University of Ho Chi Minh City, Viet Nam', 'Faculty of Information Technology, University of Science, VNU-HCM, Viet Nam'], 'pdf_title_img': 'assets/pdf/title_img/2503.00955.jpg', 'data': {'categories': ['#multilingual', '#inference', '#benchmark', '#low_resource', '#dataset', '#science', '#data'], 'emoji': '🕵️', 'ru': {'title': 'SemViQA: Передовая система проверки фактов для борьбы с дезинформацией на вьетнамском языке', 'desc': 'SemViQA - это новая система проверки фактов на вьетнамском языке, разработанная для борьбы с дезинформацией, усугубляемой крупными языковыми моделями. Она объединяет семантический поиск доказательств и двухэтапную классификацию вердиктов, обеспечивая баланс между точностью и скоростью. SemViQA достигла наилучших результатов на датасетах ISE-DSC01 и ViWikiFC, установив новый стандарт для проверки фактов на вьетнамском языке. Быстрая версия SemViQA Faster увеличивает скорость вывода в 7 раз при сохранении конкурентоспособной точности.'}, 'en': {'title': 'SemViQA: Revolutionizing Vietnamese Fact-Checking Against Misinformation', 'desc': 'This paper addresses the challenge of misinformation in low-resource languages, specifically Vietnamese, by introducing SemViQA, a new fact-checking framework. SemViQA combines Semantic-based Evidence Retrieval (SER) and Two-step Verdict Classification (TVC) to enhance both accuracy and efficiency in verifying facts. The framework achieves impressive results, with a strict accuracy of 78.97% on the ISE-DSC01 dataset and 80.82% on ViWikiFC, outperforming existing methods. Additionally, SemViQA Faster significantly boosts inference speed by 7 times while maintaining competitive accuracy, setting a new standard for fact verification in Vietnamese.'}, 'zh': {'title': 'SemViQA：越南语事实核查的新标杆', 'desc': '随着大型语言模型（LLMs）如GPT和Gemini的兴起，虚假信息问题日益严重，尤其是在资源匮乏的语言如越南语中，迫切需要强有力的事实核查解决方案。现有的方法在语义模糊、同义词和复杂语言结构方面存在困难，往往在准确性和效率之间做出妥协。我们提出了SemViQA，这是一种新颖的越南语事实核查框架，结合了基于语义的证据检索（SER）和两步裁决分类（TVC）。我们的方案在保持竞争性准确度的同时，实现了精度与速度的平衡，在ISE-DSC01上达到了78.97%的严格准确率，在ViWikiFC上达到了80.82%，并在UIT数据科学挑战赛中获得第一名。'}}}, {'id': 'https://huggingface.co/papers/2502.14856', 'title': 'FR-Spec: Accelerating Large-Vocabulary Language Models via Frequency-Ranked Speculative Sampling', 'url': 'https://huggingface.co/papers/2502.14856', 'abstract': 'Speculative sampling has emerged as an important technique for accelerating the auto-regressive generation process of large language models (LLMs) by utilizing a draft-then-verify mechanism to produce multiple tokens per forward pass. While state-of-the-art speculative sampling methods use only a single layer and a language modeling (LM) head as the draft model to achieve impressive layer compression, their efficiency gains are substantially reduced for large-vocabulary LLMs, such as Llama-3-8B with a vocabulary of 128k tokens. To address this, we present FR-Spec, a frequency-ranked speculative sampling framework that optimizes draft candidate selection through vocabulary space compression. By constraining the draft search to a frequency-prioritized token subset, our method reduces LM Head computation overhead by 75% while ensuring the equivalence of the final output distribution. Experiments across multiple datasets demonstrate an average of 1.12times speedup over the state-of-the-art speculative sampling method EAGLE-2.', 'score': 6, 'issue_id': 2535, 'pub_date': '2025-02-20', 'pub_date_card': {'ru': '20 февраля', 'en': 'February 20', 'zh': '2月20日'}, 'hash': '45e128ccea542dad', 'authors': ['Weilin Zhao', 'Tengyu Pan', 'Xu Han', 'Yudi Zhang', 'Ao Sun', 'Yuxiang Huang', 'Kaihuo Zhang', 'Weilun Zhao', 'Yuxuan Li', 'Jianyong Wang', 'Zhiyuan Liu', 'Maosong Sun'], 'affiliations': ['Beijing University of Posts and Telecommunications, Beijing, China', 'Harbin Institute of Technology, Harbin, China', 'OpenBMB', 'Tsinghua University, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2502.14856.jpg', 'data': {'categories': ['#training', '#inference', '#optimization'], 'emoji': '🚀', 'ru': {'title': 'Быстрее и эффективнее: оптимизация спекулятивной выборки для LLM', 'desc': 'Статья представляет FR-Spec - новый метод спекулятивной выборки для ускорения работы больших языковых моделей (LLM). Метод оптимизирует выбор кандидатов путем сжатия пространства словаря, отдавая приоритет наиболее частотным токенам. Это позволяет снизить вычислительные затраты на 75% по сравнению с существующими методами, сохраняя эквивалентность итогового распределения. Эксперименты показывают ускорение в 1.12 раза по сравнению с современным методом EAGLE-2.'}, 'en': {'title': 'Accelerating Token Generation with Frequency-Ranked Speculative Sampling', 'desc': 'This paper introduces FR-Spec, a new framework for speculative sampling in large language models (LLMs) that enhances the efficiency of token generation. By using a draft-then-verify approach, FR-Spec optimizes the selection of draft candidates by focusing on a frequency-ranked subset of tokens, which reduces computational overhead significantly. The method achieves a 75% reduction in LM Head computation while maintaining the same output distribution as traditional methods. Experimental results show that FR-Spec provides an average speedup of 1.12 times compared to the leading speculative sampling technique, EAGLE-2.'}, 'zh': {'title': '频率优先，推测采样加速！', 'desc': '本文提出了一种名为FR-Spec的频率排名推测采样框架，旨在加速大型语言模型的自回归生成过程。该方法通过压缩词汇空间，优化草稿候选选择，从而减少计算开销。FR-Spec将草稿搜索限制在一个优先考虑频率的词汇子集上，使得语言模型头的计算开销降低了75%。实验结果表明，FR-Spec在多个数据集上相较于最先进的推测采样方法EAGLE-2实现了平均1.12倍的加速。'}}}, {'id': 'https://huggingface.co/papers/2503.02537', 'title': 'RectifiedHR: Enable Efficient High-Resolution Image Generation via Energy Rectification', 'url': 'https://huggingface.co/papers/2503.02537', 'abstract': "Diffusion models have achieved remarkable advances in various image generation tasks. However, their performance notably declines when generating images at resolutions higher than those used during the training period. Despite the existence of numerous methods for producing high-resolution images, they either suffer from inefficiency or are hindered by complex operations. In this paper, we propose RectifiedHR, an efficient and straightforward solution for training-free high-resolution image generation. Specifically, we introduce the noise refresh strategy, which theoretically only requires a few lines of code to unlock the model's high-resolution generation ability and improve efficiency. Additionally, we first observe the phenomenon of energy decay that may cause image blurriness during the high-resolution image generation process. To address this issue, we propose an Energy Rectification strategy, where modifying the hyperparameters of the classifier-free guidance effectively improves the generation performance. Our method is entirely training-free and boasts a simple implementation logic. Through extensive comparisons with numerous baseline methods, our RectifiedHR demonstrates superior effectiveness and efficiency.", 'score': 5, 'issue_id': 2540, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 марта', 'en': 'March 4', 'zh': '3月4日'}, 'hash': '764b3070cbcdd839', 'authors': ['Zhen Yang', 'Guibao Shen', 'Liang Hou', 'Mushui Liu', 'Luozhou Wang', 'Xin Tao', 'Pengfei Wan', 'Di Zhang', 'Ying-Cong Chen'], 'affiliations': ['HKUST', 'HKUST(GZ)', 'Kuaishou Technology', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.02537.jpg', 'data': {'categories': ['#cv', '#optimization', '#diffusion', '#training'], 'emoji': '🖼️', 'ru': {'title': 'Эффективная генерация изображений высокого разрешения без дополнительного обучения', 'desc': 'Статья представляет новый метод RectifiedHR для генерации изображений высокого разрешения с помощью диффузионных моделей. Авторы предлагают стратегию обновления шума, которая позволяет улучшить способность модели генерировать изображения высокого разрешения без дополнительного обучения. Они также вводят стратегию энергетической ректификации для решения проблемы размытости изображений. Метод RectifiedHR показывает превосходную эффективность по сравнению с существующими подходами.'}, 'en': {'title': 'Unlocking High-Resolution Image Generation with RectifiedHR', 'desc': 'This paper presents RectifiedHR, a novel approach for generating high-resolution images using diffusion models without the need for additional training. The authors introduce a noise refresh strategy that simplifies the process, allowing for efficient high-resolution image generation with minimal code changes. They also identify and address the issue of energy decay, which can lead to blurry images, by proposing an Energy Rectification strategy that optimizes hyperparameters for better performance. Overall, RectifiedHR stands out for its simplicity and effectiveness compared to existing methods.'}, 'zh': {'title': '高效无训练的高分辨率图像生成', 'desc': '扩散模型在图像生成任务中取得了显著进展，但在生成高于训练分辨率的图像时性能明显下降。尽管已有多种方法可以生成高分辨率图像，但它们往往效率低下或操作复杂。本文提出了一种名为RectifiedHR的高效且简单的无训练高分辨率图像生成解决方案。我们引入了噪声刷新策略和能量修正策略，显著提高了生成性能，且实现逻辑简单。'}}}, {'id': 'https://huggingface.co/papers/2503.02197', 'title': 'ATLaS: Agent Tuning via Learning Critical Steps', 'url': 'https://huggingface.co/papers/2503.02197', 'abstract': "Large Language Model (LLM) agents have demonstrated remarkable generalization capabilities across multi-domain tasks. Existing agent tuning approaches typically employ supervised finetuning on entire expert trajectories. However, behavior-cloning of full trajectories can introduce expert bias and weaken generalization to states not covered by the expert data. Additionally, critical steps, such as planning, complex reasoning for intermediate subtasks, and strategic decision-making, are essential to success in agent tasks, so learning these steps is the key to improving LLM agents. For more effective and efficient agent tuning, we propose ATLaS that identifies the critical steps in expert trajectories and finetunes LLMs solely on these steps with reduced costs. By steering the training's focus to a few critical steps, our method mitigates the risk of overfitting entire trajectories and promotes generalization across different environments and tasks. In extensive experiments, an LLM finetuned on only 30% critical steps selected by ATLaS outperforms the LLM finetuned on all steps and recent open-source LLM agents. ATLaS maintains and improves base LLM skills as generalist agents interacting with diverse environments.", 'score': 5, 'issue_id': 2532, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 марта', 'en': 'March 4', 'zh': '3月4日'}, 'hash': 'a06c17fd97b92f03', 'authors': ['Zhixun Chen', 'Ming Li', 'Yuxuan Huang', 'Yali Du', 'Meng Fang', 'Tianyi Zhou'], 'affiliations': ['Kings College London', 'University of Liverpool', 'University of Maryland', 'University of Technology Sydney'], 'pdf_title_img': 'assets/pdf/title_img/2503.02197.jpg', 'data': {'categories': ['#optimization', '#reasoning', '#training', '#agents', '#agi'], 'emoji': '🎯', 'ru': {'title': 'ATLaS: точечная настройка LLM-агентов для улучшения обобщения', 'desc': 'ATLaS - это новый метод настройки агентов на основе больших языковых моделей (LLM). Он фокусируется на обучении только критически важным шагам в экспертных траекториях, что улучшает обобщение и снижает риск переобучения. Эксперименты показывают, что LLM, настроенная на 30% критических шагов, выбранных ATLaS, превосходит модели, обученные на всех шагах. Этот подход позволяет сохранить и улучшить базовые навыки LLM как универсальных агентов для взаимодействия с различными средами.'}, 'en': {'title': 'Focus on Critical Steps for Better LLM Performance', 'desc': 'This paper introduces ATLaS, a novel approach for tuning Large Language Model (LLM) agents by focusing on critical steps in expert trajectories rather than using full trajectory behavior-cloning. By finetuning LLMs on only 30% of these essential steps, ATLaS reduces the risk of expert bias and enhances generalization to unseen states. The method emphasizes the importance of planning, reasoning, and decision-making in agent tasks, which are crucial for improving performance. Experimental results show that LLMs trained with ATLaS outperform those trained on complete trajectories and other recent models, while also maintaining their generalist capabilities across various tasks.'}, 'zh': {'title': '聚焦关键步骤，提升LLM代理的泛化能力', 'desc': '大型语言模型（LLM）代理在多领域任务中展现了出色的泛化能力。现有的代理调优方法通常在整个专家轨迹上进行监督微调，但这种行为克隆可能引入专家偏见，削弱对未覆盖状态的泛化能力。本文提出的ATLaS方法通过识别专家轨迹中的关键步骤，仅在这些步骤上进行微调，从而降低成本并提高效率。实验表明，基于ATLaS选择的30%关键步骤微调的LLM在性能上优于在所有步骤上微调的LLM和最近的开源LLM代理。'}}}, {'id': 'https://huggingface.co/papers/2503.02878', 'title': 'Language Models can Self-Improve at State-Value Estimation for Better Search', 'url': 'https://huggingface.co/papers/2503.02878', 'abstract': 'Collecting ground truth task completion rewards or human demonstrations for multi-step reasoning tasks is often cost-prohibitive and time-consuming, especially in interactive domains like web tasks. To address this bottleneck, we present self-taught lookahead, a self-supervised method that leverages state-transition dynamics to train a value model capable of effectively guiding language model-controlled search. We find that moderately sized (8 billion parameters) open-weight value models improved with self-taught lookahead can match the performance of using a frontier LLM such as gpt-4o as the value model. Furthermore, we find that self-taught lookahead improves performance by 20% while reducing costs 37x compared to previous LLM-based tree search, without relying on ground truth rewards.', 'score': 5, 'issue_id': 2532, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 марта', 'en': 'March 4', 'zh': '3月4日'}, 'hash': '04d05c7118ce4a93', 'authors': ['Ethan Mendes', 'Alan Ritter'], 'affiliations': ['Georgia Institute of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2503.02878.jpg', 'data': {'categories': ['#reasoning', '#rl', '#optimization', '#training'], 'emoji': '🧠', 'ru': {'title': 'Самообучение для эффективного поиска без дорогостоящих демонстраций', 'desc': "Статья представляет метод 'self-taught lookahead' для обучения модели оценки, способной эффективно направлять поиск, управляемый языковой моделью. Этот самоконтролируемый метод использует динамику переходов состояний, что позволяет избежать дорогостоящего и трудоемкого сбора наград за выполнение задач или демонстраций от людей. Исследование показывает, что относительно небольшие (8 миллиардов параметров) модели оценки, улучшенные с помощью 'self-taught lookahead', могут сравниться по производительности с использованием передовых языковых моделей, таких как GPT-4, в качестве модели оценки. Более того, метод улучшает производительность на 20% при одновременном снижении затрат в 37 раз по сравнению с предыдущими методами поиска на основе больших языковых моделей."}, 'en': {'title': 'Self-Taught Lookahead: Cost-Effective Multi-Step Reasoning', 'desc': 'This paper introduces a method called self-taught lookahead, which is designed to improve the efficiency of training value models for multi-step reasoning tasks without needing expensive human input. By utilizing state-transition dynamics, this self-supervised approach allows the model to learn how to guide searches effectively. The authors demonstrate that a moderately sized value model can achieve performance comparable to larger models like GPT-4o, while also significantly reducing costs. Overall, self-taught lookahead enhances performance by 20% and cuts costs by 37 times compared to traditional LLM-based methods.'}, 'zh': {'title': '自我教导前瞻：高效的多步骤推理解决方案', 'desc': '本论文提出了一种自我学习的前瞻性方法，称为自我教导前瞻（self-taught lookahead），旨在解决多步骤推理任务中收集真实奖励或人类示范的高成本和时间消耗问题。该方法利用状态转移动态来训练一个价值模型，从而有效指导语言模型控制的搜索。研究表明，使用自我教导前瞻的中等规模（80亿参数）开放权重价值模型，其性能可以与前沿的语言模型（如gpt-4o）相媲美。更重要的是，自我教导前瞻在不依赖真实奖励的情况下，能够将性能提升20%，同时将成本降低37倍。'}}}, {'id': 'https://huggingface.co/papers/2503.01342', 'title': 'UFO: A Unified Approach to Fine-grained Visual Perception via Open-ended Language Interface', 'url': 'https://huggingface.co/papers/2503.01342', 'abstract': 'Generalist models have achieved remarkable success in both language and vision-language tasks, showcasing the potential of unified modeling. However, effectively integrating fine-grained perception tasks like detection and segmentation into these models remains a significant challenge. This is primarily because these tasks often rely heavily on task-specific designs and architectures that can complicate the modeling process. To address this challenge, we present \\ours, a framework that Unifies Fine-grained visual perception tasks through an Open-ended language interface. By transforming all perception targets into the language space, \\ours unifies object-level detection, pixel-level segmentation, and image-level vision-language tasks into a single model. Additionally, we introduce a novel embedding retrieval approach that relies solely on the language interface to support segmentation tasks. Our framework bridges the gap between fine-grained perception and vision-language tasks, significantly simplifying architectural design and training strategies while achieving comparable or superior performance to methods with intricate task-specific designs. After multi-task training on five standard visual perception datasets, \\ours outperforms the previous state-of-the-art generalist models by 12.3 mAP on COCO instance segmentation and 3.3 mIoU on ADE20K semantic segmentation. Furthermore, our method seamlessly integrates with existing MLLMs, effectively combining fine-grained perception capabilities with their advanced language abilities, thereby enabling more challenging tasks such as reasoning segmentation. Code and models will be publicly available.', 'score': 4, 'issue_id': 2535, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': '8f87e24c90eade92', 'authors': ['Hao Tang', 'Chenwei Xie', 'Haiyang Wang', 'Xiaoyi Bao', 'Tingyu Weng', 'Pandeng Li', 'Yun Zheng', 'Liwei Wang'], 'affiliations': ['Alibaba Group', 'Center for Data Science, Peking University', 'Institute of Automation, Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2503.01342.jpg', 'data': {'categories': ['#cv', '#training', '#optimization', '#agi', '#open_source', '#multimodal', '#architecture'], 'emoji': '🔬', 'ru': {'title': 'Унификация задач компьютерного зрения через языковой интерфейс', 'desc': 'Статья представляет новый фреймворк для унификации задач тонкой визуальной перцепции через языковой интерфейс. Авторы трансформируют все цели восприятия в языковое пространство, объединяя обнаружение объектов, сегментацию на уровне пикселей и задачи зрения-языка в единую модель. Предложен новый подход с извлечением эмбеддингов, использующий только языковой интерфейс для поддержки задач сегментации. Фреймворк превосходит предыдущие модели-генералисты на нескольких стандартных наборах данных и легко интегрируется с существующими мультимодальными языковыми моделями.'}, 'en': {'title': 'Unifying Fine-Grained Perception with Language for Enhanced Model Performance', 'desc': 'This paper introduces a framework called \textit{ours} that aims to unify fine-grained visual perception tasks, such as detection and segmentation, with language-based tasks. By converting all perception targets into a language format, the framework simplifies the integration of various tasks into a single model. The authors also propose a new embedding retrieval method that utilizes the language interface to enhance segmentation capabilities. The results show that \textit{ours} outperforms existing generalist models on multiple datasets, demonstrating its effectiveness in bridging fine-grained perception and vision-language tasks.'}, 'zh': {'title': '统一细粒度视觉感知与语言任务的创新框架', 'desc': '本论文提出了一种新的框架\textit{ours}，旨在通过开放式语言接口统一细粒度视觉感知任务，如目标检测和像素分割。该框架将所有感知目标转化为语言空间，从而将对象级检测、像素级分割和图像级视觉语言任务整合到一个模型中。我们还引入了一种新颖的嵌入检索方法，仅依赖语言接口来支持分割任务。经过在五个标准视觉感知数据集上的多任务训练，\textit{ours}在COCO实例分割上比之前的最先进通用模型提高了12.3 mAP，在ADE20K语义分割上提高了3.3 mIoU。'}}}, {'id': 'https://huggingface.co/papers/2503.02876', 'title': 'SPIDER: A Comprehensive Multi-Organ Supervised Pathology Dataset and Baseline Models', 'url': 'https://huggingface.co/papers/2503.02876', 'abstract': 'Advancing AI in computational pathology requires large, high-quality, and diverse datasets, yet existing public datasets are often limited in organ diversity, class coverage, or annotation quality. To bridge this gap, we introduce SPIDER (Supervised Pathology Image-DEscription Repository), the largest publicly available patch-level dataset covering multiple organ types, including Skin, Colorectal, and Thorax, with comprehensive class coverage for each organ. SPIDER provides high-quality annotations verified by expert pathologists and includes surrounding context patches, which enhance classification performance by providing spatial context.   Alongside the dataset, we present baseline models trained on SPIDER using the Hibou-L foundation model as a feature extractor combined with an attention-based classification head. The models achieve state-of-the-art performance across multiple tissue categories and serve as strong benchmarks for future digital pathology research. Beyond patch classification, the model enables rapid identification of significant areas, quantitative tissue metrics, and establishes a foundation for multimodal approaches.   Both the dataset and trained models are publicly available to advance research, reproducibility, and AI-driven pathology development. Access them at: https://github.com/HistAI/SPIDER', 'score': 4, 'issue_id': 2532, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 марта', 'en': 'March 4', 'zh': '3月4日'}, 'hash': 'a85b69f61f2f377f', 'authors': ['Dmitry Nechaev', 'Alexey Pchelnikov', 'Ekaterina Ivanova'], 'affiliations': ['HistAI'], 'pdf_title_img': 'assets/pdf/title_img/2503.02876.jpg', 'data': {'categories': ['#open_source', '#dataset', '#multimodal', '#science', '#benchmark'], 'emoji': '🕷️', 'ru': {'title': 'SPIDER: Новый стандарт данных для ИИ в патологии', 'desc': 'Статья представляет SPIDER - крупнейший общедоступный набор данных для вычислительной патологии, охватывающий множество типов органов. SPIDER предоставляет высококачественные аннотации, проверенные экспертами-патологами, и включает контекстные патчи для улучшения классификации. Авторы также представляют базовые модели, обученные на SPIDER с использованием модели Hibou-L в качестве экстрактора признаков и классификационной головки на основе механизма внимания. Модели достигают современного уровня производительности в нескольких категориях тканей и служат эталоном для будущих исследований в области цифровой патологии.'}, 'en': {'title': 'SPIDER: Bridging the Gap in Pathology Datasets for AI Advancement', 'desc': 'This paper introduces SPIDER, a large and diverse dataset designed for computational pathology, addressing the limitations of existing public datasets. SPIDER includes high-quality, expert-verified annotations for various organ types, enhancing the classification of pathology images. The authors also present baseline models that utilize the Hibou-L foundation model and an attention-based classification head, achieving state-of-the-art results in tissue classification. The dataset and models are publicly available, promoting further research and development in AI-driven pathology.'}, 'zh': {'title': '推动病理学的AI进步', 'desc': '本论文介绍了SPIDER（监督病理图像描述库），这是一个涵盖多种器官类型的最大公开补丁级数据集，包括皮肤、结直肠和胸部。该数据集提供了高质量的注释，由专家病理学家验证，并包含周围上下文补丁，以提高分类性能。我们还展示了基于SPIDER训练的基线模型，使用Hibou-L基础模型作为特征提取器，并结合基于注意力的分类头，达到了多种组织类别的最先进性能。该数据集和训练模型均可公开获取，以推动研究、可重复性和基于AI的病理发展。'}}}, {'id': 'https://huggingface.co/papers/2503.00876', 'title': 'Improve Representation for Imbalanced Regression through Geometric Constraints', 'url': 'https://huggingface.co/papers/2503.00876', 'abstract': 'In representation learning, uniformity refers to the uniform feature distribution in the latent space (i.e., unit hypersphere). Previous work has shown that improving uniformity contributes to the learning of under-represented classes. However, most of the previous work focused on classification; the representation space of imbalanced regression remains unexplored. Classification-based methods are not suitable for regression tasks because they cluster features into distinct groups without considering the continuous and ordered nature essential for regression. In a geometric aspect, we uniquely focus on ensuring uniformity in the latent space for imbalanced regression through two key losses: enveloping and homogeneity. The enveloping loss encourages the induced trace to uniformly occupy the surface of a hypersphere, while the homogeneity loss ensures smoothness, with representations evenly spaced at consistent intervals. Our method integrates these geometric principles into the data representations via a Surrogate-driven Representation Learning (SRL) framework. Experiments with real-world regression and operator learning tasks highlight the importance of uniformity in imbalanced regression and validate the efficacy of our geometry-based loss functions.', 'score': 3, 'issue_id': 2543, 'pub_date': '2025-03-02', 'pub_date_card': {'ru': '2 марта', 'en': 'March 2', 'zh': '3月2日'}, 'hash': '018d350ccd3dd3f5', 'authors': ['Zijian Dong', 'Yilei Wu', 'Chongyao Chen', 'Yingtian Zou', 'Yichi Zhang', 'Juan Helen Zhou'], 'affiliations': ['National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2503.00876.jpg', 'data': {'categories': ['#data', '#optimization', '#training'], 'emoji': '🌐', 'ru': {'title': 'Геометрический подход к равномерному представлению в несбалансированной регрессии', 'desc': 'Эта статья исследует проблему равномерности представления данных в задачах регрессии с несбалансированными выборками. Авторы предлагают новый подход, основанный на геометрических принципах, для улучшения равномерности в латентном пространстве. Они вводят две ключевые функции потерь: обволакивающую и гомогенности, которые способствуют равномерному распределению признаков на гиперсфере. Эксперименты на реальных данных подтверждают эффективность предложенного метода для задач регрессии и обучения операторов.'}, 'en': {'title': 'Achieving Uniformity in Imbalanced Regression through Geometric Losses', 'desc': "This paper addresses the challenge of representation learning in imbalanced regression tasks, where the distribution of data points is uneven across different classes. It introduces a novel approach that focuses on achieving uniformity in the latent space by employing two specific loss functions: enveloping and homogeneity. The enveloping loss promotes a uniform distribution of features on the hypersphere's surface, while the homogeneity loss ensures that these features are evenly spaced. The proposed Surrogate-driven Representation Learning (SRL) framework demonstrates the effectiveness of these geometric principles in improving performance on real-world regression tasks."}, 'zh': {'title': '提升不平衡回归的均匀性', 'desc': '在表示学习中，均匀性指的是潜在空间中特征的均匀分布。以往的研究表明，提高均匀性有助于学习代表性不足的类别，但大多数研究集中在分类任务上，而不平衡回归的表示空间尚未被探索。我们的方法通过两个关键损失函数：包络损失和均匀性损失，确保不平衡回归中的潜在空间均匀性。实验结果表明，我们的几何基础损失函数在不平衡回归中具有重要意义，验证了其有效性。'}}}, {'id': 'https://huggingface.co/papers/2503.02357', 'title': 'Q-Eval-100K: Evaluating Visual Quality and Alignment Level for Text-to-Vision Content', 'url': 'https://huggingface.co/papers/2503.02357', 'abstract': 'Evaluating text-to-vision content hinges on two crucial aspects: visual quality and alignment. While significant progress has been made in developing objective models to assess these dimensions, the performance of such models heavily relies on the scale and quality of human annotations. According to Scaling Law, increasing the number of human-labeled instances follows a predictable pattern that enhances the performance of evaluation models. Therefore, we introduce a comprehensive dataset designed to Evaluate Visual quality and Alignment Level for text-to-vision content (Q-EVAL-100K), featuring the largest collection of human-labeled Mean Opinion Scores (MOS) for the mentioned two aspects. The Q-EVAL-100K dataset encompasses both text-to-image and text-to-video models, with 960K human annotations specifically focused on visual quality and alignment for 100K instances (60K images and 40K videos). Leveraging this dataset with context prompt, we propose Q-Eval-Score, a unified model capable of evaluating both visual quality and alignment with special improvements for handling long-text prompt alignment. Experimental results indicate that the proposed Q-Eval-Score achieves superior performance on both visual quality and alignment, with strong generalization capabilities across other benchmarks. These findings highlight the significant value of the Q-EVAL-100K dataset. Data and codes will be available at https://github.com/zzc-1998/Q-Eval.', 'score': 3, 'issue_id': 2541, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 марта', 'en': 'March 4', 'zh': '3月4日'}, 'hash': 'a9bc65d32288d8f5', 'authors': ['Zicheng Zhang', 'Tengchuan Kou', 'Shushi Wang', 'Chunyi Li', 'Wei Sun', 'Wei Wang', 'Xiaoyu Li', 'Zongyu Wang', 'Xuezhi Cao', 'Xiongkuo Min', 'Xiaohong Liu', 'Guangtao Zhai'], 'affiliations': ['Meituan', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2503.02357.jpg', 'data': {'categories': ['#benchmark', '#alignment', '#long_context', '#dataset', '#multimodal', '#cv', '#video'], 'emoji': '🔍', 'ru': {'title': 'Большие данные для лучшей оценки генеративных моделей', 'desc': 'Статья описывает создание большого набора данных Q-EVAL-100K для оценки качества и соответствия контента, сгенерированного моделями text-to-image и text-to-video. На основе этого набора данных авторы разработали модель Q-Eval-Score для автоматической оценки визуального качества и соответствия запросу. Модель показала высокую эффективность и обобщающую способность на различных бенчмарках. Исследование подчеркивает важность масштабных размеченных человеком данных для улучшения оценочных моделей в области генерации изображений и видео по текстовому описанию.'}, 'en': {'title': 'Enhancing Text-to-Vision Evaluation with Q-EVAL-100K', 'desc': 'This paper discusses the importance of evaluating text-to-vision content based on visual quality and alignment. It highlights the role of human annotations in improving the performance of evaluation models, following the Scaling Law principle. The authors introduce a new dataset called Q-EVAL-100K, which contains a large number of human-labeled Mean Opinion Scores for both text-to-image and text-to-video models. They also present Q-Eval-Score, a model that effectively assesses visual quality and alignment, demonstrating strong performance and generalization across various benchmarks.'}, 'zh': {'title': '提升文本到视觉内容评估的质量与对齐度', 'desc': '本文探讨了文本到视觉内容评估的两个关键方面：视觉质量和对齐度。尽管已有客观模型在这两个维度上取得了显著进展，但这些模型的性能依赖于人类标注的规模和质量。我们引入了一个全面的数据集Q-EVAL-100K，包含960K个人类标注的平均意见分数（MOS），用于评估文本到图像和文本到视频模型的视觉质量和对齐度。通过利用该数据集，我们提出了Q-Eval-Score模型，能够有效评估视觉质量和对齐度，并在处理长文本提示对齐方面进行了特别改进。'}}}, {'id': 'https://huggingface.co/papers/2503.02783', 'title': 'IterPref: Focal Preference Learning for Code Generation via Iterative Debugging', 'url': 'https://huggingface.co/papers/2503.02783', 'abstract': 'Preference learning enhances Code LLMs beyond supervised fine-tuning by leveraging relative quality comparisons. Existing methods construct preference pairs from   candidates based on test case success, treating the higher pass rate sample as positive and the lower as negative. However, this approach does not pinpoint specific errors in the code, which prevents the model from learning more informative error correction patterns, as aligning failing code as a whole lacks the granularity needed to capture meaningful error-resolution relationships. To address these issues, we propose IterPref, a new preference alignment framework that mimics human iterative debugging to refine Code LLMs. IterPref explicitly locates error regions and aligns the corresponding tokens via a tailored DPO algorithm. To generate informative pairs, we introduce the CodeFlow dataset, where samples are iteratively refined until passing tests, with modifications capturing error corrections. Extensive experiments show that a diverse suite of Code LLMs equipped with IterPref achieves significant performance gains in code generation and improves on challenging tasks like BigCodeBench. In-depth analysis reveals that IterPref yields fewer errors. Our code and data will be made publicaly available.', 'score': 3, 'issue_id': 2540, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 марта', 'en': 'March 4', 'zh': '3月4日'}, 'hash': '3d7ced41646a5f95', 'authors': ['Jie Wu', 'Haoling Li', 'Xin Zhang', 'Jianwen Luo', 'Yangyu Huang', 'Ruihang Chu', 'Yujiu Yang', 'Scarlett Li'], 'affiliations': ['CASIA', 'Microsoft Research', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.02783.jpg', 'data': {'categories': ['#alignment', '#rlhf', '#open_source', '#dataset', '#optimization', '#training'], 'emoji': '🔧', 'ru': {'title': 'Итеративное обучение предпочтениям для усовершенствования языковых моделей кода', 'desc': 'Статья представляет новый метод IterPref для улучшения языковых моделей кода (Code LLMs) с помощью обучения предпочтениям. В отличие от существующих подходов, IterPref точно определяет области ошибок в коде и выравнивает соответствующие токены с использованием специального алгоритма DPO. Для генерации информативных пар авторы создали набор данных CodeFlow, где образцы кода итеративно улучшаются до прохождения тестов. Эксперименты показывают, что IterPref значительно повышает производительность Code LLMs в генерации кода и улучшает результаты на сложных задачах.'}, 'en': {'title': 'Iterative Preference Learning for Enhanced Code LLMs', 'desc': 'This paper introduces IterPref, a novel framework for enhancing Code LLMs (Language Models) through preference learning. Unlike traditional methods that simply compare pass rates of code samples, IterPref focuses on identifying specific error regions in code, allowing for more precise learning of error correction patterns. By utilizing a tailored DPO (Direct Preference Optimization) algorithm and the CodeFlow dataset, which iteratively refines code samples, the framework generates informative preference pairs that improve model training. Experimental results demonstrate that Code LLMs using IterPref show significant performance improvements in code generation tasks and exhibit fewer errors overall.'}, 'zh': {'title': '迭代调试，提升代码生成能力！', 'desc': '本论文提出了一种新的偏好对齐框架IterPref，旨在通过模拟人类的迭代调试过程来提升代码大语言模型（Code LLMs）的性能。现有方法通过测试用例的成功率构建偏好对，但未能准确定位代码中的具体错误，限制了模型学习有效的错误修正模式。IterPref通过定制的DPO算法明确定位错误区域，并对相应的标记进行对齐，从而生成更具信息量的偏好对。实验结果表明，使用IterPref的多样化代码大语言模型在代码生成和复杂任务上均取得了显著的性能提升。'}}}, {'id': 'https://huggingface.co/papers/2503.02268', 'title': 'AppAgentX: Evolving GUI Agents as Proficient Smartphone Users', 'url': 'https://huggingface.co/papers/2503.02268', 'abstract': "Recent advancements in Large Language Models (LLMs) have led to the development of intelligent LLM-based agents capable of interacting with graphical user interfaces (GUIs). These agents demonstrate strong reasoning and adaptability, enabling them to perform complex tasks that traditionally required predefined rules. However, the reliance on step-by-step reasoning in LLM-based agents often results in inefficiencies, particularly for routine tasks. In contrast, traditional rule-based systems excel in efficiency but lack the intelligence and flexibility to adapt to novel scenarios. To address this challenge, we propose a novel evolutionary framework for GUI agents that enhances operational efficiency while retaining intelligence and flexibility. Our approach incorporates a memory mechanism that records the agent's task execution history. By analyzing this history, the agent identifies repetitive action sequences and evolves high-level actions that act as shortcuts, replacing these low-level operations and improving efficiency. This allows the agent to focus on tasks requiring more complex reasoning, while simplifying routine actions. Experimental results on multiple benchmark tasks demonstrate that our approach significantly outperforms existing methods in both efficiency and accuracy. The code will be open-sourced to support further research.", 'score': 3, 'issue_id': 2536, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 марта', 'en': 'March 4', 'zh': '3月4日'}, 'hash': '03199cb797dc8d88', 'authors': ['Wenjia Jiang', 'Yangyang Zhuang', 'Chenxi Song', 'Xu Yang', 'Chi Zhang'], 'affiliations': ['Henan University', 'Southeast University', 'Westlake University'], 'pdf_title_img': 'assets/pdf/title_img/2503.02268.jpg', 'data': {'categories': ['#optimization', '#agents', '#benchmark', '#reasoning', '#open_source', '#training'], 'emoji': '🧠', 'ru': {'title': 'Эволюционное обучение агентов GUI: баланс эффективности и гибкости', 'desc': 'Эта статья представляет новый эволюционный подход для агентов, работающих с графическим интерфейсом на основе больших языковых моделей (LLM). Авторы предлагают механизм памяти, который записывает историю выполнения задач агентом и анализирует ее для выявления повторяющихся последовательностей действий. На основе этого анализа создаются высокоуровневые действия, которые заменяют низкоуровневые операции, повышая эффективность работы. Экспериментальные результаты показывают, что данный метод значительно превосходит существующие подходы по эффективности и точности.'}, 'en': {'title': 'Evolving Efficiency: Smart Shortcuts for GUI Agents', 'desc': 'This paper presents a new framework for Large Language Model (LLM)-based agents that interact with graphical user interfaces (GUIs). The proposed method enhances the efficiency of these agents by incorporating a memory mechanism that tracks their task execution history. By analyzing this history, the agents can identify repetitive actions and evolve high-level shortcuts, allowing them to streamline routine tasks. This approach maintains the intelligence and adaptability of LLMs while improving their operational efficiency, as demonstrated by experimental results on benchmark tasks.'}, 'zh': {'title': '智能代理的进化：提升效率与灵活性', 'desc': '最近，大型语言模型（LLMs）的进步使得基于LLM的智能代理能够与图形用户界面（GUIs）进行交互。这些代理展现出强大的推理能力和适应性，能够执行传统上需要预定义规则的复杂任务。然而，基于LLM的代理在依赖逐步推理时，往往在处理常规任务时效率较低。为了解决这个问题，我们提出了一种新颖的进化框架，通过记录代理的任务执行历史，识别重复的操作序列，从而演化出高层次的快捷操作，提升了操作效率，同时保留了智能和灵活性。'}}}, {'id': 'https://huggingface.co/papers/2503.02812', 'title': 'Q-Filters: Leveraging QK Geometry for Efficient KV Cache Compression', 'url': 'https://huggingface.co/papers/2503.02812', 'abstract': 'Autoregressive language models rely on a Key-Value (KV) Cache, which avoids re-computing past hidden states during generation, making it faster. As model sizes and context lengths grow, the KV Cache becomes a significant memory bottleneck, which calls for compression methods that limit its size during generation. In this paper, we discover surprising properties of Query (Q) and Key (K) vectors that allow us to efficiently approximate attention scores without computing the attention maps. We propose Q-Filters, a training-free KV Cache compression method that filters out less crucial Key-Value pairs based on a single context-agnostic projection. Contrarily to many alternatives, Q-Filters is compatible with FlashAttention, as it does not require direct access to attention weights. Experimental results in long-context settings demonstrate that Q-Filters is competitive with attention-based compression methods such as SnapKV in retrieval tasks while consistently outperforming efficient compression schemes such as Streaming-LLM in generation setups. Notably, Q-Filters achieves a 99% accuracy in the needle-in-a-haystack task with a x32 compression level while reducing the generation perplexity drop by up to 65% in text generation compared to Streaming-LLM.', 'score': 2, 'issue_id': 2546, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 марта', 'en': 'March 4', 'zh': '3月4日'}, 'hash': '9dc95b6ce0d4b6ca', 'authors': ['Nathan Godey', 'Alessio Devoto', 'Yu Zhao', 'Simone Scardapane', 'Pasquale Minervini', 'Éric de la Clergerie', 'Benoît Sagot'], 'affiliations': ['Miniml.AI', 'Sapienza University of Rome', 'Sorbonne Université, Paris, France', 'University of Edinburgh'], 'pdf_title_img': 'assets/pdf/title_img/2503.02812.jpg', 'data': {'categories': ['#inference', '#long_context', '#training', '#optimization'], 'emoji': '🔍', 'ru': {'title': 'Q-Filters: Эффективное сжатие KV-кэша без компромиссов в производительности', 'desc': 'Статья представляет новый метод сжатия KV-кэша для авторегрессивных языковых моделей под названием Q-Filters. Этот метод основан на обнаруженных свойствах векторов запросов (Q) и ключей (K), позволяющих эффективно аппроксимировать оценки внимания без вычисления карт внимания. Q-Filters фильтрует менее важные пары ключ-значение на основе одной контекстно-независимой проекции, что делает его совместимым с FlashAttention. Экспериментальные результаты показывают, что Q-Filters превосходит другие методы сжатия в задачах генерации текста и извлечения информации при длинных контекстах.'}, 'en': {'title': 'Efficient KV Cache Compression with Q-Filters', 'desc': 'This paper addresses the memory limitations of autoregressive language models caused by the Key-Value (KV) Cache during text generation. It introduces Q-Filters, a novel method that compresses the KV Cache by filtering out less important Key-Value pairs without needing to compute attention maps. The method is efficient and compatible with existing techniques like FlashAttention, making it a practical solution for large models. Experimental results show that Q-Filters not only competes well with other compression methods but also significantly improves performance in text generation tasks.'}, 'zh': {'title': '高效压缩：Q-Filters的创新之路', 'desc': '自回归语言模型依赖于键值缓存（KV Cache），以避免在生成过程中重新计算过去的隐藏状态，从而提高速度。随着模型规模和上下文长度的增加，KV Cache 成为一个重要的内存瓶颈，因此需要压缩方法来限制其大小。本文发现了查询（Q）和键（K）向量的意外特性，使我们能够在不计算注意力图的情况下有效地近似注意力分数。我们提出了 Q-Filters，这是一种无训练的 KV Cache 压缩方法，通过单一的上下文无关投影过滤掉不太重要的键值对。'}}}, {'id': 'https://huggingface.co/papers/2503.02823', 'title': 'A Multimodal Symphony: Integrating Taste and Sound through Generative AI', 'url': 'https://huggingface.co/papers/2503.02823', 'abstract': "In recent decades, neuroscientific and psychological research has traced direct relationships between taste and auditory perceptions. This article explores multimodal generative models capable of converting taste information into music, building on this foundational research. We provide a brief review of the state of the art in this field, highlighting key findings and methodologies. We present an experiment in which a fine-tuned version of a generative music model (MusicGEN) is used to generate music based on detailed taste descriptions provided for each musical piece. The results are promising: according the participants' (n=111) evaluation, the fine-tuned model produces music that more coherently reflects the input taste descriptions compared to the non-fine-tuned model. This study represents a significant step towards understanding and developing embodied interactions between AI, sound, and taste, opening new possibilities in the field of generative AI. We release our dataset, code and pre-trained model at: https://osf.io/xs5jy/.", 'score': 2, 'issue_id': 2545, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 марта', 'en': 'March 4', 'zh': '3月4日'}, 'hash': '841602ca85525c24', 'authors': ['Matteo Spanio', 'Massimiliano Zampini', 'Antonio Rodà', 'Franco Pierucci'], 'affiliations': ['Center for Mind/Brain Sciences (CIMeC) University of Trento Rovereto, Italy', 'Centro di Sonologia Computazionale (CSC) Department of Information Engineering University of Padova Padova, Italy', 'SoundFood s.r.l. Terni, Italy'], 'pdf_title_img': 'assets/pdf/title_img/2503.02823.jpg', 'data': {'categories': ['#multimodal', '#open_source', '#games', '#dataset'], 'emoji': '🎵', 'ru': {'title': 'От вкуса к мелодии: ИИ преобразует гастрономические ощущения в музыку', 'desc': 'Статья исследует генеративные модели, способные преобразовывать информацию о вкусе в музыку. Авторы провели эксперимент с использованием дообученной версии модели MusicGEN для генерации музыки на основе описаний вкусов. Результаты показали, что дообученная модель создает музыку, более согласованную с входными описаниями вкусов по сравнению с базовой моделью. Это исследование открывает новые возможности в области генеративного искусственного интеллекта и мультимодального восприятия.'}, 'en': {'title': 'Transforming Taste into Sound: A New Frontier in Generative AI', 'desc': 'This paper investigates the connection between taste and sound by using multimodal generative models. It focuses on a specific model called MusicGEN, which has been fine-tuned to create music that corresponds to taste descriptions. An experiment with 111 participants showed that the fine-tuned model produced music that better matched the taste inputs than the original model. This research advances the understanding of how AI can create embodied experiences that link different sensory modalities, particularly taste and sound.'}, 'zh': {'title': '味觉与音乐的奇妙结合', 'desc': '近年来，神经科学和心理学研究发现味觉与听觉之间存在直接关系。本文探讨了多模态生成模型，能够将味觉信息转换为音乐，基于这一基础研究进行深入分析。我们回顾了该领域的最新进展，强调了关键发现和方法论。实验结果表明，经过微调的生成音乐模型（MusicGEN）能够更好地反映输入的味觉描述，展示了AI、声音与味觉之间的互动新可能。'}}}, {'id': 'https://huggingface.co/papers/2503.02152', 'title': 'Tabby: Tabular Data Synthesis with Language Models', 'url': 'https://huggingface.co/papers/2503.02152', 'abstract': 'While advances in large language models (LLMs) have greatly improved the quality of synthetic text data in recent years, synthesizing tabular data has received relatively less attention. We address this disparity with Tabby, a simple but powerful post-training modification to the standard Transformer language model architecture, enabling its use for tabular dataset synthesis. Tabby enables the representation of differences across columns using Gated Mixture-of-Experts, with column-specific sets of parameters. Empirically, Tabby results in data quality near or equal to that of real data. By pairing our novel LLM table training technique, Plain, with Tabby, we observe up to a 44% improvement in quality over previous methods. We also show that Tabby extends beyond tables to more general structured data, reaching parity with real data on a nested JSON dataset as well.', 'score': 1, 'issue_id': 2545, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 марта', 'en': 'March 4', 'zh': '3月4日'}, 'hash': '1d33263eb28e0be2', 'authors': ['Sonia Cromp', 'Satya Sai Srinath Namburi GNVV', 'Mohammed Alkhudhayri', 'Catherine Cao', 'Samuel Guo', 'Nicholas Roberts', 'Frederic Sala'], 'affiliations': ['GE HealthCare', 'University of Wisconsin-Madison'], 'pdf_title_img': 'assets/pdf/title_img/2503.02152.jpg', 'data': {'categories': ['#dataset', '#synthetic', '#architecture', '#data'], 'emoji': '📊', 'ru': {'title': 'Tabby: прорыв в синтезе табличных данных с помощью языковых моделей', 'desc': 'Исследователи представили Tabby - модификацию архитектуры трансформера для синтеза табличных данных. Tabby использует Gated Mixture-of-Experts для представления различий между столбцами с помощью специфичных для каждого столбца параметров. В сочетании с новой техникой обучения Plain, Tabby показывает улучшение качества синтетических данных до 44% по сравнению с предыдущими методами. Модель также эффективна для работы с вложенными JSON-данными.'}, 'en': {'title': 'Tabby: Transforming Tabular Data Synthesis with LLMs', 'desc': 'This paper introduces Tabby, a new method for synthesizing tabular data using a modified Transformer language model. It employs Gated Mixture-of-Experts to effectively capture the unique characteristics of different columns in a dataset. The results show that Tabby can generate synthetic data that is comparable in quality to real data, achieving significant improvements over existing techniques. Additionally, Tabby is versatile enough to be applied to other structured data formats, such as nested JSON, demonstrating its broad applicability in data synthesis tasks.'}, 'zh': {'title': 'Tabby：表格数据合成的新突破', 'desc': '本文介绍了一种名为Tabby的方法，用于合成表格数据。Tabby是对标准Transformer语言模型架构的后训练修改，能够有效表示列之间的差异。通过使用门控混合专家模型，Tabby为每一列提供特定的参数集，从而提高数据质量。实验表明，Tabby在合成数据的质量上接近或等同于真实数据，并且在处理嵌套JSON数据集时也表现出色。'}}}, {'id': 'https://huggingface.co/papers/2503.02304', 'title': 'A Token-level Text Image Foundation Model for Document Understanding', 'url': 'https://huggingface.co/papers/2503.02304', 'abstract': 'In recent years, general visual foundation models (VFMs) have witnessed increasing adoption, particularly as image encoders for popular multi-modal large language models (MLLMs). However, without semantically fine-grained supervision, these models still encounter fundamental prediction errors in the context of downstream text-image-related tasks, i.e., perception, understanding and reasoning with images containing small and dense texts. To bridge this gap, we develop TokenOCR, the first token-level visual foundation model specifically tailored for text-image-related tasks, designed to support a variety of traditional downstream applications. To facilitate the pretraining of TokenOCR, we also devise a high-quality data production pipeline that constructs the first token-level image text dataset, TokenIT, comprising 20 million images and 1.8 billion token-mask pairs. Furthermore, leveraging this foundation with exceptional image-as-text capability, we seamlessly replace previous VFMs with TokenOCR to construct a document-level MLLM, TokenVL, for VQA-based document understanding tasks. Finally, extensive experiments demonstrate the effectiveness of TokenOCR and TokenVL. Code, datasets, and weights will be available at https://token-family.github.io/TokenOCR_project.', 'score': 1, 'issue_id': 2545, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 марта', 'en': 'March 4', 'zh': '3月4日'}, 'hash': 'b275d8ba26cca4b5', 'authors': ['Tongkun Guan', 'Zining Wang', 'Pei Fu', 'Zhengtao Guo', 'Wei Shen', 'Kai Zhou', 'Tiezhu Yue', 'Chen Duan', 'Hao Sun', 'Qianyi Jiang', 'Junfeng Luo', 'Xiaokang Yang'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2503.02304.jpg', 'data': {'categories': ['#dataset', '#cv', '#agi', '#reasoning', '#optimization', '#multimodal', '#games', '#data', '#open_source'], 'emoji': '🔍', 'ru': {'title': 'TokenOCR: Новый уровень распознавания текста на изображениях', 'desc': 'Исследователи разработали TokenOCR - первую визуальную фундаментальную модель на уровне токенов для задач, связанных с текстом на изображениях. Для предобучения модели создан набор данных TokenIT, содержащий 20 миллионов изображений и 1,8 миллиарда пар токен-маска. На основе TokenOCR построена мультимодальная языковая модель TokenVL для задач понимания документов. Эксперименты подтверждают эффективность предложенных моделей TokenOCR и TokenVL.'}, 'en': {'title': 'TokenOCR: Bridging Text and Image Understanding', 'desc': 'This paper introduces TokenOCR, a novel visual foundation model designed specifically for tasks that involve understanding and reasoning about images with dense text. Traditional visual foundation models struggle with these tasks due to a lack of detailed supervision, leading to prediction errors. TokenOCR addresses this issue by utilizing a new dataset, TokenIT, which contains 20 million images and 1.8 billion token-mask pairs for effective pretraining. The model is integrated into a document-level multi-modal large language model, TokenVL, enhancing its capabilities for visual question answering and document understanding tasks.'}, 'zh': {'title': 'TokenOCR：文本图像任务的视觉基础模型新突破', 'desc': '近年来，通用视觉基础模型（VFM）在多模态大语言模型（MLLM）中得到了广泛应用，尤其作为图像编码器。然而，在没有细粒度语义监督的情况下，这些模型在处理与文本相关的图像任务时仍然会出现基本的预测错误。为了解决这个问题，我们开发了TokenOCR，这是第一个专门针对文本-图像相关任务的标记级视觉基础模型，并设计了一个高质量的数据生产管道，构建了包含2000万张图像和18亿个标记-掩码对的TokenIT数据集。通过这一基础，我们成功地用TokenOCR替换了之前的VFM，构建了用于文档理解任务的TokenVL文档级MLLM，实验结果证明了TokenOCR和TokenVL的有效性。'}}}, {'id': 'https://huggingface.co/papers/2503.00200', 'title': 'Unified Video Action Model', 'url': 'https://huggingface.co/papers/2503.00200', 'abstract': 'A unified video and action model holds significant promise for robotics, where videos provide rich scene information for action prediction, and actions provide dynamics information for video prediction. However, effectively combining video generation and action prediction remains challenging, and current video generation-based methods struggle to match the performance of direct policy learning in action accuracy and inference speed. To bridge this gap, we introduce the Unified Video Action model (UVA), which jointly optimizes video and action predictions to achieve both high accuracy and efficient action inference. The key lies in learning a joint video-action latent representation and decoupling video-action decoding. The joint latent representation bridges the visual and action domains, effectively modeling the relationship between video and action sequences. Meanwhile, the decoupled decoding, powered by two lightweight diffusion heads, enables high-speed action inference by bypassing video generation during inference. Such a unified framework further enables versatile functionality through masked input training. By selectively masking actions or videos, a single model can tackle diverse tasks beyond policy learning, such as forward and inverse dynamics modeling and video generation. Via an extensive set of experiments, we demonstrate that UVA can serve as a general-purpose solution for a wide range of robotics tasks, such as policy learning, forward/inverse dynamics and video observation prediction, without compromising performance compared to methods tailored for specific applications. Results are best viewed on https://unified-video-action-model.github.io/.', 'score': 0, 'issue_id': 2551, 'pub_date': '2025-02-28', 'pub_date_card': {'ru': '28 февраля', 'en': 'February 28', 'zh': '2月28日'}, 'hash': '286c6abc07fdf95e', 'authors': ['Shuang Li', 'Yihuai Gao', 'Dorsa Sadigh', 'Shuran Song'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2503.00200.jpg', 'data': {'categories': ['#robotics', '#video', '#games', '#optimization'], 'emoji': '🤖', 'ru': {'title': 'Единая модель для видео и действий: новый подход к робототехнике', 'desc': 'Модель UVA (Unified Video Action) представляет собой единую систему для видео и действий в робототехнике. Она объединяет генерацию видео и предсказание действий, используя совместное латентное представление и раздельное декодирование. UVA обеспечивает высокую точность и эффективность вывода действий, избегая генерации видео во время вывода. Модель применима к различным задачам робототехники, включая обучение политик, прямую и обратную динамику, и предсказание видеонаблюдений.'}, 'en': {'title': 'Bridging Video and Action for Smarter Robotics', 'desc': "The Unified Video Action model (UVA) integrates video generation and action prediction to enhance robotics applications. It achieves this by creating a joint latent representation that connects visual data with action sequences, allowing for better modeling of their interrelationship. The model employs decoupled decoding with lightweight diffusion heads, which speeds up action inference by avoiding the need for video generation during this process. UVA's versatility is further demonstrated through masked input training, enabling it to perform various tasks like policy learning and dynamics modeling efficiently."}, 'zh': {'title': '统一视频与动作模型：提升机器人智能的关键', 'desc': '本文提出了一种统一的视频与动作模型（UVA），旨在提高机器人领域中的动作预测和视频生成的性能。UVA通过联合优化视频和动作预测，学习视频与动作的联合潜在表示，从而有效建模视频与动作序列之间的关系。该模型采用解耦解码的方法，利用轻量级的扩散头实现快速的动作推理，避免了在推理过程中生成视频的需求。通过掩蔽输入训练，UVA能够处理多种任务，如策略学习、前向/逆向动力学建模和视频生成，展现出广泛的适用性。'}}}, {'id': 'https://huggingface.co/papers/2503.01842', 'title': 'Discrete-Time Hybrid Automata Learning: Legged Locomotion Meets Skateboarding', 'url': 'https://huggingface.co/papers/2503.01842', 'abstract': 'This paper introduces Discrete-time Hybrid Automata Learning (DHAL), a framework using on-policy Reinforcement Learning to identify and execute mode-switching without trajectory segmentation or event function learning. Hybrid dynamical systems, which include continuous flow and discrete mode switching, can model robotics tasks like legged robot locomotion. Model-based methods usually depend on predefined gaits, while model-free approaches lack explicit mode-switching knowledge. Current methods identify discrete modes via segmentation before regressing continuous flow, but learning high-dimensional complex rigid body dynamics without trajectory labels or segmentation is a challenging open problem. Our approach incorporates a beta policy distribution and a multi-critic architecture to model contact-guided motions, exemplified by a challenging quadrupedal robot skateboard task. We validate our method through simulations and real-world tests, demonstrating robust performance in hybrid dynamical systems.', 'score': 0, 'issue_id': 2545, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': '07eab4aa91f88137', 'authors': ['Hang Liu', 'Sangli Teng', 'Ben Liu', 'Wei Zhang', 'Maani Ghaffari'], 'affiliations': ['Southern University of Science and Technology', 'University of Michigan'], 'pdf_title_img': 'assets/pdf/title_img/2503.01842.jpg', 'data': {'categories': ['#games', '#rl', '#robotics', '#optimization'], 'emoji': '🤖', 'ru': {'title': 'Обучение гибридным системам без сегментации траекторий', 'desc': 'Статья представляет DHAL - фреймворк для обучения гибридным автоматам с дискретным временем, использующий обучение с подкреплением для идентификации и выполнения переключения режимов. Метод применяет бета-распределение политики и архитектуру с несколькими критиками для моделирования движений, управляемых контактами. DHAL не требует сегментации траекторий или предварительного обучения функций событий. Эффективность подхода продемонстрирована на сложной задаче управления четвероногим роботом на скейтборде в симуляции и реальном мире.'}, 'en': {'title': 'Learning Mode-Switching in Robotics Without Segmentation', 'desc': 'This paper presents Discrete-time Hybrid Automata Learning (DHAL), a novel framework that leverages on-policy Reinforcement Learning to effectively manage mode-switching in hybrid dynamical systems. Unlike traditional methods that require trajectory segmentation or predefined gaits, DHAL learns to identify and execute mode transitions directly from the data. The approach utilizes a beta policy distribution and a multi-critic architecture to handle complex contact-guided motions, particularly in tasks like quadrupedal robot locomotion. The results show that DHAL can robustly learn high-dimensional dynamics without the need for explicit trajectory labels, making it a significant advancement in the field.'}, 'zh': {'title': '无轨迹分割的智能模式切换学习', 'desc': '本文介绍了一种离散时间混合自动机学习（DHAL）框架，利用基于策略的强化学习来识别和执行模式切换，而无需进行轨迹分割或事件函数学习。混合动态系统能够模拟机器人任务，例如四足机器人行走。传统的模型方法通常依赖于预定义的步态，而无模型的方法则缺乏明确的模式切换知识。我们的方法通过引入贝塔策略分布和多重评论家架构，成功地建模了接触引导的运动，并在仿真和实际测试中验证了其在混合动态系统中的强大性能。'}}}, {'id': 'https://huggingface.co/papers/2503.04625', 'title': 'START: Self-taught Reasoner with Tools', 'url': 'https://huggingface.co/papers/2503.04625', 'abstract': "Large reasoning models (LRMs) like OpenAI-o1 and DeepSeek-R1 have demonstrated remarkable capabilities in complex reasoning tasks through the utilization of long Chain-of-thought (CoT). However, these models often suffer from hallucinations and inefficiencies due to their reliance solely on internal reasoning processes. In this paper, we introduce START (Self-Taught Reasoner with Tools), a novel tool-integrated long CoT reasoning LLM that significantly enhances reasoning capabilities by leveraging external tools. Through code execution, START is capable of performing complex computations, self-checking, exploring diverse methods, and self-debugging, thereby addressing the limitations of LRMs. The core innovation of START lies in its self-learning framework, which comprises two key techniques: 1) Hint-infer: We demonstrate that inserting artificially designed hints (e.g., ``Wait, maybe using Python here is a good idea.'') during the inference process of a LRM effectively stimulates its ability to utilize external tools without the need for any demonstration data. Hint-infer can also serve as a simple and effective sequential test-time scaling method; 2) Hint Rejection Sampling Fine-Tuning (Hint-RFT): Hint-RFT combines Hint-infer and RFT by scoring, filtering, and modifying the reasoning trajectories with tool invocation generated by a LRM via Hint-infer, followed by fine-tuning the LRM. Through this framework, we have fine-tuned the QwQ-32B model to achieve START. On PhD-level science QA (GPQA), competition-level math benchmarks (AMC23, AIME24, AIME25), and the competition-level code benchmark (LiveCodeBench), START achieves accuracy rates of 63.6%, 95.0%, 66.7%, 47.1%, and 47.3%, respectively. It significantly outperforms the base QwQ-32B and achieves performance comparable to the state-of-the-art open-weight model R1-Distill-Qwen-32B and the proprietary model o1-Preview.", 'score': 36, 'issue_id': 2579, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 марта', 'en': 'March 6', 'zh': '3月6日'}, 'hash': '8961f69e1eda24ad', 'authors': ['Chengpeng Li', 'Mingfeng Xue', 'Zhenru Zhang', 'Jiaxi Yang', 'Beichen Zhang', 'Xiang Wang', 'Bowen Yu', 'Binyuan Hui', 'Junyang Lin', 'Dayiheng Liu'], 'affiliations': ['Alibaba Group', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2503.04625.jpg', 'data': {'categories': ['#long_context', '#rl', '#training', '#architecture', '#hallucinations', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'START: Самообучающаяся модель рассуждений с инструментами', 'desc': 'Статья представляет START - новую модель для рассуждений с длинной цепочкой мыслей, интегрирующую внешние инструменты. START использует фреймворк самообучения, включающий технику Hint-infer для стимулирования использования инструментов, и Hint Rejection Sampling Fine-Tuning для улучшения траекторий рассуждений. Модель значительно превосходит базовую QwQ-32B и достигает результатов на уровне современных моделей в сложных задачах рассуждений. START демонстрирует высокую точность на наборах данных по науке, математике и программированию уровня PhD и соревнований.'}, 'en': {'title': 'Enhancing Reasoning with External Tools: Introducing START', 'desc': "This paper presents START, a new long Chain-of-thought reasoning model that integrates external tools to improve reasoning capabilities. Traditional large reasoning models often struggle with hallucinations and inefficiencies, but START addresses these issues by allowing the model to perform complex computations and self-debugging. The innovation lies in two techniques: Hint-infer, which uses designed hints to encourage tool usage, and Hint Rejection Sampling Fine-Tuning, which refines the model's reasoning paths. START demonstrates superior performance on various benchmarks, surpassing its predecessor and competing effectively with state-of-the-art models."}, 'zh': {'title': '工具整合，推理更强！', 'desc': '本文介绍了一种新型的长链推理模型START（自我学习推理器与工具），它通过整合外部工具来增强推理能力。START利用代码执行进行复杂计算、自我检查、探索多种方法和自我调试，从而克服了大型推理模型（LRMs）在推理过程中常见的幻觉和低效问题。核心创新在于自我学习框架，包括提示推理（Hint-infer）和提示拒绝采样微调（Hint-RFT）两种技术，前者通过插入设计的提示来激发模型使用外部工具的能力。经过微调，START在多个科学问答和数学基准测试中表现优异，准确率显著高于基础模型QwQ-32B。'}}}, {'id': 'https://huggingface.co/papers/2503.03803', 'title': 'EgoLife: Towards Egocentric Life Assistant', 'url': 'https://huggingface.co/papers/2503.03803', 'abstract': 'We introduce EgoLife, a project to develop an egocentric life assistant that accompanies and enhances personal efficiency through AI-powered wearable glasses. To lay the foundation for this assistant, we conducted a comprehensive data collection study where six participants lived together for one week, continuously recording their daily activities - including discussions, shopping, cooking, socializing, and entertainment - using AI glasses for multimodal egocentric video capture, along with synchronized third-person-view video references. This effort resulted in the EgoLife Dataset, a comprehensive 300-hour egocentric, interpersonal, multiview, and multimodal daily life dataset with intensive annotation. Leveraging this dataset, we introduce EgoLifeQA, a suite of long-context, life-oriented question-answering tasks designed to provide meaningful assistance in daily life by addressing practical questions such as recalling past relevant events, monitoring health habits, and offering personalized recommendations. To address the key technical challenges of (1) developing robust visual-audio models for egocentric data, (2) enabling identity recognition, and (3) facilitating long-context question answering over extensive temporal information, we introduce EgoButler, an integrated system comprising EgoGPT and EgoRAG. EgoGPT is an omni-modal model trained on egocentric datasets, achieving state-of-the-art performance on egocentric video understanding. EgoRAG is a retrieval-based component that supports answering ultra-long-context questions. Our experimental studies verify their working mechanisms and reveal critical factors and bottlenecks, guiding future improvements. By releasing our datasets, models, and benchmarks, we aim to stimulate further research in egocentric AI assistants.', 'score': 9, 'issue_id': 2581, 'pub_date': '2025-03-05', 'pub_date_card': {'ru': '5 марта', 'en': 'March 5', 'zh': '3月5日'}, 'hash': '52396234365a3fb0', 'authors': ['Jingkang Yang', 'Shuai Liu', 'Hongming Guo', 'Yuhao Dong', 'Xiamengwei Zhang', 'Sicheng Zhang', 'Pengyun Wang', 'Zitang Zhou', 'Binzhu Xie', 'Ziyue Wang', 'Bei Ouyang', 'Zhengyu Lin', 'Marco Cominelli', 'Zhongang Cai', 'Yuanhan Zhang', 'Peiyuan Zhang', 'Fangzhou Hong', 'Joerg Widmer', 'Francesco Gringoli', 'Lei Yang', 'Bo Li', 'Ziwei Liu'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2503.03803.jpg', 'data': {'categories': ['#video', '#long_context', '#dataset', '#open_source', '#agents', '#multimodal', '#data', '#benchmark'], 'emoji': '👓', 'ru': {'title': 'EgoLife: ИИ-ассистент для повседневной жизни на базе умных очков', 'desc': 'Проект EgoLife представляет собой разработку эгоцентричного ассистента на основе очков дополненной реальности с искусственным интеллектом. Исследователи собрали обширный набор данных EgoLife Dataset, включающий 300 часов эгоцентричного видео повседневной жизни шести участников. На основе этих данных создан набор задач EgoLifeQA для тестирования вопросно-ответных систем в контексте повседневной жизни. Для решения технических задач разработана система EgoButler, включающая мультимодальную модель EgoGPT и компонент для ответов на вопросы с длинным контекстом EgoRAG.'}, 'en': {'title': 'Empowering Daily Life with Egocentric AI Assistance', 'desc': 'The paper presents EgoLife, an AI-powered life assistant designed to enhance personal efficiency through wearable glasses that capture egocentric video data. A comprehensive dataset, the EgoLife Dataset, was created by recording daily activities of participants over a week, resulting in 300 hours of multimodal data with detailed annotations. The study introduces EgoLifeQA, a set of question-answering tasks that utilize this dataset to assist users with practical life questions and personalized recommendations. To tackle challenges in visual-audio model development and long-context question answering, the authors propose EgoButler, which includes EgoGPT for video understanding and EgoRAG for retrieval-based answering.'}, 'zh': {'title': '智能生活助手，提升个人效率', 'desc': '我们介绍了EgoLife项目，旨在开发一个以自我为中心的生活助手，通过AI驱动的可穿戴眼镜提升个人效率。我们进行了全面的数据收集研究，六名参与者共同生活一周，使用AI眼镜记录日常活动，形成了EgoLife数据集，包含300小时的多视角、多模态日常生活数据。基于该数据集，我们推出了EgoLifeQA，一个针对生活的长文本问答任务，旨在提供实用的日常生活帮助。为了解决关键技术挑战，我们引入了EgoButler系统，包括EgoGPT和EgoRAG，前者在自我中心视频理解上表现出色，后者支持超长文本问题的回答。'}}}, {'id': 'https://huggingface.co/papers/2503.04598', 'title': 'HybridNorm: Towards Stable and Efficient Transformer Training via Hybrid Normalization', 'url': 'https://huggingface.co/papers/2503.04598', 'abstract': 'Transformers have become the de facto architecture for a wide range of machine learning tasks, particularly in large language models (LLMs). Despite their remarkable performance, challenges remain in training deep transformer networks, especially regarding the location of layer normalization. While Pre-Norm structures facilitate easier training due to their more prominent identity path, they often yield suboptimal performance compared to Post-Norm. In this paper, we propose HybridNorm, a straightforward yet effective hybrid normalization strategy that integrates the advantages of both Pre-Norm and Post-Norm approaches. Specifically, HybridNorm employs QKV normalization within the attention mechanism and Post-Norm in the feed-forward network (FFN) of each transformer block. This design not only stabilizes training but also enhances performance, particularly in the context of LLMs. Comprehensive experiments in both dense and sparse architectures show that HybridNorm consistently outperforms both Pre-Norm and Post-Norm approaches, achieving state-of-the-art results across various benchmarks. These findings highlight the potential of HybridNorm as a more stable and effective technique for improving the training and performance of deep transformer models. %Code will be made publicly available. Code is available at https://github.com/BryceZhuo/HybridNorm.', 'score': 7, 'issue_id': 2579, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 марта', 'en': 'March 6', 'zh': '3月6日'}, 'hash': '78b05ed4e27a874b', 'authors': ['Zhijian Zhuo', 'Yutao Zeng', 'Ya Wang', 'Sijun Zhang', 'Jian Yang', 'Xiaoqing Li', 'Xun Zhou', 'Jinwen Ma'], 'affiliations': ['Capital University of Economics and Business', 'School of Mathematical Sciences, Peking University', 'SeedFoundation-Model, ByteDance'], 'pdf_title_img': 'assets/pdf/title_img/2503.04598.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#training', '#architecture', '#open_source'], 'emoji': '🔀', 'ru': {'title': 'HybridNorm: Гибридная нормализация для улучшения обучения и производительности трансформеров', 'desc': 'Статья представляет новый метод нормализации для трансформеров под названием HybridNorm. Этот подход сочетает преимущества Pre-Norm и Post-Norm стратегий, применяя QKV нормализацию в механизме внимания и Post-Norm в FFN каждого блока трансформера. HybridNorm показывает лучшие результаты по сравнению с существующими методами как для плотных, так и для разреженных архитектур. Эксперименты демонстрируют, что HybridNorm обеспечивает более стабильное обучение и улучшенную производительность для глубоких моделей трансформеров, особенно в контексте больших языковых моделей.'}, 'en': {'title': 'HybridNorm: The Best of Both Normalization Worlds for Transformers', 'desc': 'This paper introduces HybridNorm, a new normalization strategy for training deep transformer networks, which combines the strengths of Pre-Norm and Post-Norm methods. Pre-Norm structures help with training stability but often lead to lower performance, while Post-Norm structures typically perform better but can complicate training. HybridNorm applies QKV normalization in the attention mechanism and Post-Norm in the feed-forward network, resulting in improved training stability and performance. Experiments demonstrate that HybridNorm outperforms both Pre-Norm and Post-Norm across various benchmarks, making it a promising approach for enhancing large language models.'}, 'zh': {'title': 'HybridNorm：提升变换器模型训练的稳定性与性能', 'desc': '本文提出了一种新的混合归一化策略，称为HybridNorm，旨在解决深度变换器网络训练中的层归一化位置问题。HybridNorm结合了Pre-Norm和Post-Norm的优点，在注意力机制中使用QKV归一化，而在前馈网络中使用Post-Norm。实验结果表明，HybridNorm在稠密和稀疏架构中均优于传统的Pre-Norm和Post-Norm方法，提升了大语言模型的训练稳定性和性能。该研究为深度变换器模型的训练和性能改进提供了新的思路。'}}}, {'id': 'https://huggingface.co/papers/2502.20258', 'title': 'LLM as a Broken Telephone: Iterative Generation Distorts Information', 'url': 'https://huggingface.co/papers/2502.20258', 'abstract': 'As large language models are increasingly responsible for online content, concerns arise about the impact of repeatedly processing their own outputs. Inspired by the "broken telephone" effect in chained human communication, this study investigates whether LLMs similarly distort information through iterative generation. Through translation-based experiments, we find that distortion accumulates over time, influenced by language choice and chain complexity. While degradation is inevitable, it can be mitigated through strategic prompting techniques. These findings contribute to discussions on the long-term effects of AI-mediated information propagation, raising important questions about the reliability of LLM-generated content in iterative workflows.', 'score': 6, 'issue_id': 2580, 'pub_date': '2025-02-27', 'pub_date_card': {'ru': '27 февраля', 'en': 'February 27', 'zh': '2月27日'}, 'hash': '5b26055854ae3d90', 'authors': ['Amr Mohamed', 'Mingmeng Geng', 'Michalis Vazirgiannis', 'Guokan Shang'], 'affiliations': ['Ecole Polytechnique', 'MBZUAI', 'SISSA'], 'pdf_title_img': 'assets/pdf/title_img/2502.20258.jpg', 'data': {'categories': ['#hallucinations', '#data', '#long_context', '#alignment', '#multimodal', '#training'], 'emoji': '📞', 'ru': {'title': "Эффект 'испорченного телефона' в языковых моделях", 'desc': 'Исследование посвящено изучению эффекта искажения информации при многократной обработке собственных выходных данных большими языковыми моделями (LLM). Авторы провели эксперименты на основе переводов, чтобы оценить накопление искажений в зависимости от выбора языка и сложности цепочки обработки. Результаты показывают, что деградация информации неизбежна, но может быть смягчена с помощью стратегических методов формулировки запросов. Исследование поднимает важные вопросы о надежности контента, генерируемого LLM в итеративных рабочих процессах.'}, 'en': {'title': 'Mitigating Distortion in Iterative LLM Outputs', 'desc': "This paper explores how large language models (LLMs) can distort information when they repeatedly process their own outputs, similar to the 'broken telephone' effect seen in human communication. The researchers conducted translation-based experiments to observe how information degradation occurs over time, which is affected by the choice of language and the complexity of the generation chain. They found that while some level of distortion is unavoidable, it can be reduced by using specific prompting strategies. These insights highlight the potential risks of relying on LLMs for generating content in iterative processes, raising concerns about the accuracy of AI-generated information."}, 'zh': {'title': '探讨大型语言模型的信息扭曲与可靠性', 'desc': '本研究探讨了大型语言模型（LLM）在反复处理自身输出时是否会扭曲信息，类似于人类沟通中的“破电话”效应。通过基于翻译的实验，我们发现信息扭曲会随着时间的推移而累积，受语言选择和链条复杂性的影响。尽管信息退化是不可避免的，但通过战略性提示技术可以减轻这种影响。研究结果为AI介导的信息传播的长期影响提供了重要见解，提出了关于LLM生成内容在迭代工作流程中可靠性的重要问题。'}}}, {'id': 'https://huggingface.co/papers/2503.04094', 'title': 'PokéChamp: an Expert-level Minimax Language Agent', 'url': 'https://huggingface.co/papers/2503.04094', 'abstract': "We introduce Pok\\'eChamp, a minimax agent powered by Large Language Models (LLMs) for Pok\\'emon battles. Built on a general framework for two-player competitive games, Pok\\'eChamp leverages the generalist capabilities of LLMs to enhance minimax tree search. Specifically, LLMs replace three key modules: (1) player action sampling, (2) opponent modeling, and (3) value function estimation, enabling the agent to effectively utilize gameplay history and human knowledge to reduce the search space and address partial observability. Notably, our framework requires no additional LLM training. We evaluate Pok\\'eChamp in the popular Gen 9 OU format. When powered by GPT-4o, it achieves a win rate of 76% against the best existing LLM-based bot and 84% against the strongest rule-based bot, demonstrating its superior performance. Even with an open-source 8-billion-parameter Llama 3.1 model, Pok\\'eChamp consistently outperforms the previous best LLM-based bot, Pok\\'ellmon powered by GPT-4o, with a 64% win rate. Pok\\'eChamp attains a projected Elo of 1300-1500 on the Pok\\'emon Showdown online ladder, placing it among the top 30%-10% of human players. In addition, this work compiles the largest real-player Pok\\'emon battle dataset, featuring over 3 million games, including more than 500k high-Elo matches. Based on this dataset, we establish a series of battle benchmarks and puzzles to evaluate specific battling skills. We further provide key updates to the local game engine. We hope this work fosters further research that leverage Pok\\'emon battle as benchmark to integrate LLM technologies with game-theoretic algorithms addressing general multiagent problems. Videos, code, and dataset available at https://sites.google.com/view/pokechamp-llm.", 'score': 5, 'issue_id': 2580, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 марта', 'en': 'March 6', 'zh': '3月6日'}, 'hash': 'bffe348d8443d4c2', 'authors': ['Seth Karten', 'Andy Luu Nguyen', 'Chi Jin'], 'affiliations': ['Department of Computer Science, Princeton University', 'Department of Electrical and Computer Engineering, Princeton University'], 'pdf_title_img': 'assets/pdf/title_img/2503.04094.jpg', 'data': {'categories': ['#open_source', '#benchmark', '#dataset', '#agents', '#games'], 'emoji': '🎮', 'ru': {'title': 'PokeChamp: Революция в ИИ для игровых стратегий с использованием языковых моделей', 'desc': 'Статья представляет PokeChamp - агента на основе минимакса, использующего большие языковые модели (LLM) для боев в Pokémon. PokeChamp применяет LLM для выборки действий игрока, моделирования оппонента и оценки функции ценности, что позволяет эффективно использовать историю игры и знания человека. При использовании GPT-4o агент достигает высокого процента побед против существующих ботов и попадает в топ-30% - 10% игроков на онлайн-платформе Pokémon Showdown. Исследование также включает создание крупнейшего датасета боев Pokémon и серии тестов для оценки навыков ведения боя.'}, 'en': {'title': "Pok'eChamp: Elevating Pokémon Battles with LLMs", 'desc': "Pok'eChamp is a minimax agent designed for Pokémon battles that utilizes Large Language Models (LLMs) to improve its decision-making process. It replaces traditional components of the minimax algorithm with LLMs for player action sampling, opponent modeling, and value function estimation, allowing it to better understand gameplay history and human strategies. The agent demonstrates impressive performance, achieving a 76% win rate against top LLM-based bots and a 64% win rate with a smaller model, showcasing its effectiveness in competitive play. Additionally, Pok'eChamp compiles a large dataset of over 3 million Pokémon battles to create benchmarks for evaluating battling skills and encourages further research in integrating LLMs with game-theoretic approaches."}, 'zh': {'title': 'PokéChamp：宝可梦对战中的智能代理', 'desc': '本文介绍了PokéChamp，一个基于大型语言模型（LLMs）的极小极大算法代理，用于宝可梦对战。该代理利用LLMs的通用能力，增强了极小极大树搜索，替代了玩家动作采样、对手建模和价值函数估计等关键模块。PokéChamp在不需要额外训练的情况下，能够有效利用游戏历史和人类知识，减少搜索空间并解决部分可观测性问题。经过评估，PokéChamp在宝可梦对战中表现优异，赢得了76%的胜率，展示了其在多智能体问题中的潜力。'}}}, {'id': 'https://huggingface.co/papers/2503.04222', 'title': 'FuseChat-3.0: Preference Optimization Meets Heterogeneous Model Fusion', 'url': 'https://huggingface.co/papers/2503.04222', 'abstract': 'We introduce FuseChat-3.0, a suite of large language models (LLMs) developed by integrating the strengths of heterogeneous source LLMs into more compact target LLMs. Our source models include the powerful Gemma-2-27B-it, Mistral-Large-Instruct-2407, Qwen-2.5-72B-Instruct, and Llama-3.1-70B-Instruct. For target models, we focus on three widely-used smaller variants-Llama-3.1-8B-Instruct, Gemma-2-9B-it, and Qwen-2.5-7B-Instruct-along with two ultra-compact options, Llama-3.2-3B-Instruct and Llama-3.2-1B-Instruct. To leverage the diverse capabilities of these source models, we develop a specialized data construction protocol tailored to various tasks and domains. The FuseChat-3.0 training pipeline consists of two key stages: (1) supervised fine-tuning (SFT) to align the target and source model distributions, and (2) Direct Preference Optimization (DPO) to apply preferences from multiple source LLMs to fine-tune the target model. The resulting FuseChat-3.0 models exhibit significant performance gains across tasks such as instruction following, general knowledge, mathematics, and coding. As illustrated in Figure 1, using Llama-3.1-8B-Instruct as the target model, our fusion approach achieves an average improvement of 6.8 points across 14 benchmarks. Moreover, it demonstrates remarkable gains of 37.1 points and 30.1 points on the instruction-following benchmarks AlpacaEval-2 and Arena-Hard, respectively. Our code, models, and datasets are available at https://github.com/SLIT-AI/FuseChat-3.0.', 'score': 5, 'issue_id': 2578, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 марта', 'en': 'March 6', 'zh': '3月6日'}, 'hash': 'c7d793a0b91efd5d', 'authors': ['Ziyi Yang', 'Fanqi Wan', 'Longguang Zhong', 'Canbin Huang', 'Guosheng Liang', 'Xiaojun Quan'], 'affiliations': ['School of Computer Science and Engineering, Sun Yat-sen University, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.04222.jpg', 'data': {'categories': ['#training', '#dataset', '#benchmark', '#small_models', '#rlhf', '#transfer_learning', '#open_source', '#optimization'], 'emoji': '🔀', 'ru': {'title': 'Слияние мощи больших языковых моделей в компактном формате', 'desc': 'FuseChat-3.0 представляет собой набор больших языковых моделей (LLM), разработанных путем интеграции сильных сторон гетерогенных исходных LLM в более компактные целевые модели. Процесс обучения включает два ключевых этапа: контролируемая тонкая настройка и оптимизация прямых предпочтений. Результирующие модели FuseChat-3.0 демонстрируют значительный прирост производительности в различных задачах, включая следование инструкциям, общие знания, математику и программирование. Используя Llama-3.1-8B-Instruct в качестве целевой модели, подход авторов достигает среднего улучшения на 6,8 пунктов по 14 бенчмаркам.'}, 'en': {'title': 'Fusing Strengths for Smarter, Smaller Models', 'desc': 'FuseChat-3.0 is a collection of large language models (LLMs) that combines the strengths of various larger source models into smaller, more efficient target models. The training process involves supervised fine-tuning to align the target models with the source models, followed by Direct Preference Optimization to enhance performance based on preferences from multiple sources. This innovative approach leads to significant improvements in tasks like instruction following, general knowledge, mathematics, and coding. The results show an average performance boost of 6.8 points across 14 benchmarks, with particularly impressive gains in instruction-following tasks.'}, 'zh': {'title': '融合多源模型，提升语言理解能力', 'desc': '我们介绍了FuseChat-3.0，这是一个通过整合不同来源的大型语言模型（LLMs）优势而开发的更紧凑的目标LLM套件。源模型包括强大的Gemma-2-27B-it、Mistral-Large-Instruct-2407、Qwen-2.5-72B-Instruct和Llama-3.1-70B-Instruct。目标模型则集中在三种广泛使用的小型变体上，以及两个超紧凑选项。通过专门的数据构建协议和两阶段的训练流程，FuseChat-3.0在多个任务上表现出显著的性能提升。'}}}, {'id': 'https://huggingface.co/papers/2503.04130', 'title': 'Token-Efficient Long Video Understanding for Multimodal LLMs', 'url': 'https://huggingface.co/papers/2503.04130', 'abstract': 'Recent advances in video-based multimodal large language models (Video-LLMs) have significantly improved video understanding by processing videos as sequences of image frames. However, many existing methods treat frames independently in the vision backbone, lacking explicit temporal modeling, which limits their ability to capture dynamic patterns and efficiently handle long videos. To address these limitations, we introduce STORM (Spatiotemporal TOken Reduction for Multimodal LLMs), a novel architecture incorporating a dedicated temporal encoder between the image encoder and the LLM. Our temporal encoder leverages the Mamba State Space Model to integrate temporal information into image tokens, generating enriched representations that preserve inter-frame dynamics across the entire video sequence. This enriched encoding not only enhances video reasoning capabilities but also enables effective token reduction strategies, including test-time sampling and training-based temporal and spatial pooling, substantially reducing computational demands on the LLM without sacrificing key temporal information. By integrating these techniques, our approach simultaneously reduces training and inference latency while improving performance, enabling efficient and robust video understanding over extended temporal contexts. Extensive evaluations show that STORM achieves state-of-the-art results across various long video understanding benchmarks (more than 5\\% improvement on MLVU and LongVideoBench) while reducing the computation costs by up to 8times and the decoding latency by 2.4-2.9times for the fixed numbers of input frames. Project page is available at https://research.nvidia.com/labs/lpr/storm', 'score': 3, 'issue_id': 2580, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 марта', 'en': 'March 6', 'zh': '3月6日'}, 'hash': 'f1d1afacd41dc0c7', 'authors': ['Jindong Jiang', 'Xiuyu Li', 'Zhijian Liu', 'Muyang Li', 'Guo Chen', 'Zhiqi Li', 'De-An Huang', 'Guilin Liu', 'Zhiding Yu', 'Kurt Keutzer', 'Sungjin Ahn', 'Jan Kautz', 'Hongxu Yin', 'Yao Lu', 'Song Han', 'Wonmin Byeon'], 'affiliations': ['KAIST', 'MIT', 'NVIDIA', 'Nanjing University', 'Rutgers University', 'UC Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2503.04130.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#optimization', '#long_context', '#architecture', '#video', '#inference', '#multimodal'], 'emoji': '🌪️', 'ru': {'title': 'STORM: Эффективное понимание длинных видео с помощью темпорального кодирования', 'desc': 'STORM - это новая архитектура для видео-LLM, которая использует специальный темпоральный энкодер на основе модели Mamba State Space между энкодером изображений и LLM. Эта архитектура позволяет интегрировать временную информацию в токены изображений, создавая обогащенные представления, сохраняющие динамику между кадрами во всей видеопоследовательности. STORM применяет стратегии сокращения токенов, включая выборку во время тестирования и пулинг во время обучения, что значительно снижает вычислительные затраты LLM. Результаты экспериментов показывают, что STORM достигает улучшения более чем на 5% на бенчмарках длинных видео, одновременно сокращая вычислительные затраты до 8 раз.'}, 'en': {'title': 'STORM: Revolutionizing Video Understanding with Temporal Insights', 'desc': 'This paper presents STORM, a new architecture designed to enhance video understanding in multimodal large language models (LLMs) by incorporating a temporal encoder. Unlike previous methods that treat video frames independently, STORM uses the Mamba State Space Model to capture the dynamics between frames, resulting in richer representations of video content. The architecture also implements token reduction strategies that significantly lower computational costs and improve processing speed without losing important temporal information. Evaluations demonstrate that STORM outperforms existing models on long video benchmarks while achieving substantial reductions in computation and latency.'}, 'zh': {'title': '高效视频理解的新突破：STORM模型', 'desc': '最近，基于视频的多模态大语言模型（Video-LLMs）在视频理解方面取得了显著进展，但许多现有方法在视觉骨干中独立处理帧，缺乏明确的时间建模，限制了捕捉动态模式的能力。为了解决这些问题，我们提出了STORM（时空令牌减少模型），它在图像编码器和大语言模型之间引入了专门的时间编码器。我们的时间编码器利用Mamba状态空间模型，将时间信息整合到图像令牌中，生成丰富的表示，保留整个视频序列中的帧间动态。通过这些技术的整合，我们的方法在提高性能的同时，显著减少了计算需求和推理延迟，实现了对长视频的高效理解。'}}}, {'id': 'https://huggingface.co/papers/2503.04725', 'title': 'L$^2$M: Mutual Information Scaling Law for Long-Context Language Modeling', 'url': 'https://huggingface.co/papers/2503.04725', 'abstract': "We rigorously establish a bipartite mutual information scaling law in natural language that governs long-range dependencies. This scaling law, which we show is distinct from and scales independently of the conventional two-point mutual information, is the key to understanding long-context language modeling. Using this scaling law, we formulate the Long-context Language Modeling (L^2M) condition, which relates a model's capacity for effective long context length modeling to the scaling of its latent state size for storing past information. Our results are validated through experiments on both transformers and state space models. This work establishes a theoretical foundation that guides the development of large language models toward longer context lengths.", 'score': 2, 'issue_id': 2582, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 марта', 'en': 'March 6', 'zh': '3月6日'}, 'hash': '51e8f1332666da31', 'authors': ['Zhuo Chen', 'Oriol Mayné i Comas', 'Zhuotao Jin', 'Di Luo', 'Marin Soljačić'], 'affiliations': ['Harvard University', 'Massachusetts Institute of Technology', 'NSF AI Institute for Artificial Intelligence and Fundamental Interactions', 'Polytechnic University of Catalonia', 'University of California, Los Angeles'], 'pdf_title_img': 'assets/pdf/title_img/2503.04725.jpg', 'data': {'categories': ['#training', '#architecture', '#math', '#long_context'], 'emoji': '📏', 'ru': {'title': 'Масштабирование взаимной информации - ключ к моделированию длинного контекста', 'desc': 'Статья представляет собой исследование в области обработки естественного языка, фокусирующееся на закономерностях взаимной информации в длинных текстах. Авторы формулируют условие L^2M для моделирования длинного контекста, связывающее способность модели обрабатывать длинные последовательности с масштабированием ее скрытого состояния. Результаты подтверждены экспериментами на трансформерах и моделях пространства состояний. Работа закладывает теоретическую основу для развития больших языковых моделей с увеличенной длиной контекста.'}, 'en': {'title': 'Unlocking Long-Range Dependencies in Language Models', 'desc': "This paper introduces a new scaling law for bipartite mutual information that is crucial for understanding long-range dependencies in natural language processing. Unlike traditional two-point mutual information, this scaling law operates independently and is essential for effective long-context language modeling. The authors propose the Long-context Language Modeling (L^2M) condition, which connects a model's ability to handle long contexts with the size of its latent state for retaining past information. Experimental validation on transformers and state space models supports the theoretical framework, paving the way for advancements in large language models with extended context lengths."}, 'zh': {'title': '长上下文建模的新法则', 'desc': '本文严谨地建立了自然语言中的双向互信息缩放法则，该法则控制着长距离依赖关系。我们展示了这一缩放法则与传统的两点互信息不同，并且独立缩放，是理解长上下文语言建模的关键。通过这一缩放法则，我们提出了长上下文语言建模（L^2M）条件，将模型有效建模长上下文长度的能力与其存储过去信息的潜在状态大小的缩放联系起来。我们的结果通过对变换器和状态空间模型的实验得到了验证，为大型语言模型向更长上下文长度的发展奠定了理论基础。'}}}, {'id': 'https://huggingface.co/papers/2503.03983', 'title': 'Audio Flamingo 2: An Audio-Language Model with Long-Audio Understanding and Expert Reasoning Abilities', 'url': 'https://huggingface.co/papers/2503.03983', 'abstract': 'Understanding and reasoning over non-speech sounds and music are crucial for both humans and AI agents to interact effectively with their environments. In this paper, we introduce Audio Flamingo 2 (AF2), an Audio-Language Model (ALM) with advanced audio understanding and reasoning capabilities. AF2 leverages (i) a custom CLAP model, (ii) synthetic Audio QA data for fine-grained audio reasoning, and (iii) a multi-stage curriculum learning strategy. AF2 achieves state-of-the-art performance with only a 3B parameter small language model, surpassing large open-source and proprietary models across over 20 benchmarks. Next, for the first time, we extend audio understanding to long audio segments (30 secs to 5 mins) and propose LongAudio, a large and novel dataset for training ALMs on long audio captioning and question-answering tasks. Fine-tuning AF2 on LongAudio leads to exceptional performance on our proposed LongAudioBench, an expert annotated benchmark for evaluating ALMs on long audio understanding capabilities. We conduct extensive ablation studies to confirm the efficacy of our approach. Project Website: https://research.nvidia.com/labs/adlr/AF2/.', 'score': 2, 'issue_id': 2581, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 марта', 'en': 'March 6', 'zh': '3月6日'}, 'hash': '952e4cb72ec34df8', 'authors': ['Sreyan Ghosh', 'Zhifeng Kong', 'Sonal Kumar', 'S Sakshi', 'Jaehyeon Kim', 'Wei Ping', 'Rafael Valle', 'Dinesh Manocha', 'Bryan Catanzaro'], 'affiliations': ['NVIDIA, Santa Clara, CA, USA', 'University of Maryland, College Park, MD, USA'], 'pdf_title_img': 'assets/pdf/title_img/2503.03983.jpg', 'data': {'categories': ['#audio', '#long_context', '#dataset', '#small_models', '#reasoning', '#synthetic', '#open_source', '#benchmark'], 'emoji': '🎵', 'ru': {'title': 'AF2: Революция в понимании аудио искусственным интеллектом', 'desc': 'Audio Flamingo 2 (AF2) - это усовершенствованная аудио-языковая модель (ALM) с продвинутыми возможностями понимания и рассуждения об аудио. Модель использует специальную модель CLAP, синтетические данные Audio QA и многоступенчатую стратегию обучения. AF2 достигает наилучших результатов среди существующих моделей на более чем 20 тестах, используя всего 3 миллиарда параметров. Кроме того, исследователи расширили возможности модели для работы с длинными аудиосегментами и создали набор данных LongAudio для обучения ALM на задачах описания и ответов на вопросы по длинным аудио.'}, 'en': {'title': 'Revolutionizing Audio Understanding with Audio Flamingo 2', 'desc': 'This paper presents Audio Flamingo 2 (AF2), an advanced Audio-Language Model (ALM) designed to enhance audio understanding and reasoning. AF2 utilizes a custom CLAP model and synthetic Audio QA data to improve fine-grained audio reasoning, along with a multi-stage curriculum learning approach. The model achieves state-of-the-art results with a relatively small 3B parameter language model, outperforming larger models on over 20 benchmarks. Additionally, it introduces LongAudio, a new dataset for training on long audio segments, and demonstrates exceptional performance on the LongAudioBench for evaluating long audio understanding.'}, 'zh': {'title': '音频理解的新突破：Audio Flamingo 2', 'desc': '本文介绍了Audio Flamingo 2（AF2），这是一种具有先进音频理解和推理能力的音频语言模型（ALM）。AF2利用了定制的CLAP模型、合成的音频问答数据以及多阶段的课程学习策略。AF2在仅使用一个3B参数的小型语言模型的情况下，超越了20多个基准测试中的大型开源和专有模型，达到了最先进的性能。此外，AF2首次扩展了对长音频片段（30秒到5分钟）的理解，并提出了LongAudio数据集，用于训练ALM在长音频标注和问答任务上的能力。'}}}, {'id': 'https://huggingface.co/papers/2503.04606', 'title': 'The Best of Both Worlds: Integrating Language Models and Diffusion Models for Video Generation', 'url': 'https://huggingface.co/papers/2503.04606', 'abstract': 'Recent advancements in text-to-video (T2V) generation have been driven by two competing paradigms: autoregressive language models and diffusion models. However, each paradigm has intrinsic limitations: language models struggle with visual quality and error accumulation, while diffusion models lack semantic understanding and causal modeling. In this work, we propose LanDiff, a hybrid framework that synergizes the strengths of both paradigms through coarse-to-fine generation. Our architecture introduces three key innovations: (1) a semantic tokenizer that compresses 3D visual features into compact 1D discrete representations through efficient semantic compression, achieving a sim14,000times compression ratio; (2) a language model that generates semantic tokens with high-level semantic relationships; (3) a streaming diffusion model that refines coarse semantics into high-fidelity videos. Experiments show that LanDiff, a 5B model, achieves a score of 85.43 on the VBench T2V benchmark, surpassing the state-of-the-art open-source models Hunyuan Video (13B) and other commercial models such as Sora, Keling, and Hailuo. Furthermore, our model also achieves state-of-the-art performance in long video generation, surpassing other open-source models in this field. Our demo can be viewed at https://landiff.github.io/.', 'score': 2, 'issue_id': 2580, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 марта', 'en': 'March 6', 'zh': '3月6日'}, 'hash': 'c635690f3edc1186', 'authors': ['Aoxiong Yin', 'Kai Shen', 'Yichong Leng', 'Xu Tan', 'Xinyu Zhou', 'Juncheng Li', 'Siliang Tang'], 'affiliations': ['College of Computer Science and Technology, Zhejiang University, Hangzhou, China', 'Moonshot AI, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.04606.jpg', 'data': {'categories': ['#diffusion', '#open_source', '#benchmark', '#long_context', '#architecture', '#video'], 'emoji': '🎬', 'ru': {'title': 'LanDiff: гибридный подход к генерации видео из текста', 'desc': 'Статья представляет LanDiff - гибридную архитектуру для генерации видео из текста, сочетающую преимущества авторегрессионных языковых моделей и диффузионных моделей. LanDiff включает семантический токенизатор, языковую модель для генерации семантических токенов и потоковую диффузионную модель для уточнения деталей. Модель LanDiff объемом 5 миллиардов параметров превзошла современные открытые и коммерческие модели в бенчмарке VBench T2V. Также LanDiff показала лучшие результаты в генерации длинных видео по сравнению с другими открытыми моделями.'}, 'en': {'title': 'LanDiff: Bridging Language and Visuals for Superior Video Generation', 'desc': 'This paper introduces LanDiff, a novel framework for text-to-video (T2V) generation that combines the strengths of autoregressive language models and diffusion models. It addresses the limitations of each approach by using a coarse-to-fine generation strategy, which enhances both visual quality and semantic understanding. Key innovations include a semantic tokenizer for efficient compression of visual features, a language model that generates meaningful semantic tokens, and a streaming diffusion model that refines these tokens into high-quality videos. The results demonstrate that LanDiff outperforms existing state-of-the-art models in T2V tasks, particularly in generating long videos.'}, 'zh': {'title': 'LanDiff：文本到视频生成的新突破', 'desc': '本文提出了一种名为LanDiff的混合框架，旨在结合自回归语言模型和扩散模型的优点，以实现文本到视频的生成。该框架通过粗到细的生成过程，克服了各自的局限性，提升了视觉质量和语义理解。LanDiff引入了三项关键创新，包括高效的语义压缩技术、生成高层语义关系的语言模型，以及将粗略语义精炼为高保真视频的流式扩散模型。实验结果表明，LanDiff在VBench T2V基准测试中表现优异，超越了现有的开源和商业模型。'}}}, {'id': 'https://huggingface.co/papers/2503.02972', 'title': 'LINGOLY-TOO: Disentangling Memorisation from Reasoning with Linguistic Templatisation and Orthographic Obfuscation', 'url': 'https://huggingface.co/papers/2503.02972', 'abstract': 'Effective evaluation of the reasoning capabilities of large language models (LLMs) are susceptible to overestimation due to data exposure of evaluation benchmarks. We introduce a framework for producing linguistic reasoning problems that reduces the effect of memorisation in model performance estimates and apply this framework to develop LINGOLY-TOO, a challenging evaluation benchmark for linguistic reasoning. By developing orthographic templates, we dynamically obfuscate the writing systems of real languages to generate numerous question variations. These variations preserve the reasoning steps required for each solution while reducing the likelihood of specific problem instances appearing in model training data. Our experiments demonstrate that frontier models, including OpenAI o1-preview and DeepSeem R1, struggle with advanced reasoning. Our analysis also shows that LLMs exhibit noticeable variance in accuracy across permutations of the same problem, and on average perform better on questions appearing in their original orthography. Our findings highlight the opaque nature of response generation in LLMs and provide evidence that prior data exposure contributes to overestimating the reasoning capabilities of frontier models.', 'score': 1, 'issue_id': 2585, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 марта', 'en': 'March 4', 'zh': '3月4日'}, 'hash': '0eb195e2704f3d5d', 'authors': ['Jude Khouja', 'Karolina Korgul', 'Simi Hellsten', 'Lingyi Yang', 'Vlad Neacs', 'Harry Mayne', 'Ryan Kearns', 'Andrew Bean', 'Adam Mahdi'], 'affiliations': ['Asia-Pacific Linguistics Olympiad', 'Hong Kong Linguistics Olympiad', 'National University of Science and Technology POLITEHNICA Bucharest, Romania', 'United Kingdom Linguistics Olympiad', 'University of Glasgow, Glasgow, United Kingdom', 'University of Oxford, Oxford, United Kingdom'], 'pdf_title_img': 'assets/pdf/title_img/2503.02972.jpg', 'data': {'categories': ['#reasoning', '#dataset', '#hallucinations', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'Новый метод оценки рассуждений LLM без влияния предварительных знаний', 'desc': 'Статья представляет новый подход к оценке способностей больших языковых моделей (LLM) к рассуждению. Авторы разработали фреймворк LINGOLY-TOO, который генерирует лингвистические задачи с обфускацией систем письма реальных языков. Эксперименты показали, что современные модели, включая OpenAI и DeepSeem, испытывают трудности с продвинутыми рассуждениями. Исследование выявило, что предварительное знакомство с данными может приводить к переоценке возможностей LLM в области рассуждений.'}, 'en': {'title': 'Unmasking Reasoning: Evaluating LLMs Beyond Memorization', 'desc': 'This paper addresses the challenge of accurately evaluating the reasoning abilities of large language models (LLMs) by introducing a new framework that minimizes the impact of memorization on performance assessments. The authors present LINGOLY-TOO, a benchmark designed to test linguistic reasoning through dynamically generated questions that obfuscate the original writing systems of languages. By using orthographic templates, they create multiple variations of questions that maintain the necessary reasoning steps while reducing the chance of models having seen specific instances during training. The results indicate that leading models struggle with complex reasoning tasks and show significant performance differences based on the question format, revealing the influence of prior data exposure on their evaluation.'}, 'zh': {'title': '揭示大型语言模型推理能力的真实面貌', 'desc': '本文提出了一种评估大型语言模型（LLMs）推理能力的新框架，旨在减少由于数据暴露导致的评估过高的问题。我们开发了LINGOLY-TOO，这是一个具有挑战性的语言推理评估基准，通过使用正字法模板动态模糊真实语言的书写系统，生成多种问题变体。实验结果表明，前沿模型在高级推理任务中表现不佳，并且在相同问题的不同排列中，LLMs的准确性存在显著差异。我们的研究揭示了LLMs响应生成的复杂性，并提供了证据表明，先前的数据暴露会导致对前沿模型推理能力的高估。'}}}, {'id': 'https://huggingface.co/papers/2503.01901', 'title': 'Identifying Sensitive Weights via Post-quantization Integral', 'url': 'https://huggingface.co/papers/2503.01901', 'abstract': "Serving Large Language Models (LLMs) is costly. However, post-training weight quantization can address this problem by both compressing their sizes for limited memory and saving bandwidth for acceleration. As not all weight dimensions are equally important, those methods typically rely on a sensitivity metric, which indicates the element-wise influence of weights on loss function and is used to preprocess original weights for better quantization. In this work, we conduct an empirical study on the accuracy of the sensitivity metric, and find that existing gradient and Hessian based metrics are very inaccurate: they underestimate quantization's impact on the loss function by orders of magnitude, mainly due to the small convergence radius of local 2nd order approximation, \\ie, gradient and Hessian term in Taylor's formula. To tackle this problem, we propose Post-quantization Integral (PQI), an accurate metric to estimate posterior sensitivity in a fine-grained manner. To leverage this accurate metric, we further propose ReQuant, a simple yet powerful framework that mainly consists of two Dense-and-Sparse detach components: self-adaptive outlier selection and step-wise significant weights detach. Results show that ReQuant boosts state-of-the-art post-training quantization methods, with a pronounced improvement of 2.66 perplexity gain on Llama 3.2 1B with QTIP.", 'score': 1, 'issue_id': 2585, 'pub_date': '2025-02-28', 'pub_date_card': {'ru': '28 февраля', 'en': 'February 28', 'zh': '2月28日'}, 'hash': '1045983ed982a00b', 'authors': ['Yuezhou Hu', 'Weiyu Huang', 'Zichen Liang', 'Chang Chen', 'Jintao Zhang', 'Jun Zhu', 'Jianfei Chen'], 'affiliations': ['Dept. of Comp. Sci. & Tech., Institute for AI, BNRist Center, Tsinghua-Bosch Joint ML Center, THBI Lab, Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.01901.jpg', 'data': {'categories': ['#training', '#inference', '#optimization'], 'emoji': '⚖️', 'ru': {'title': 'Точная квантизация для эффективных языковых моделей', 'desc': 'Эта статья представляет новый метод квантизации весов для крупных языковых моделей (LLM), называемый ReQuant. Авторы обнаружили, что существующие метрики чувствительности весов неточны и предложили более точную метрику - Post-quantization Integral (PQI). ReQuant использует PQI вместе с адаптивным выбором выбросов и пошаговым отделением значимых весов. Результаты показывают, что ReQuant улучшает современные методы пост-тренировочной квантизации, значительно повышая производительность модели Llama 3.2 1B.'}, 'en': {'title': 'Enhancing LLM Efficiency with Accurate Quantization Metrics', 'desc': 'This paper addresses the high costs associated with serving large language models (LLMs) by introducing post-training weight quantization techniques. It highlights the inadequacy of existing sensitivity metrics, which fail to accurately predict the impact of quantization on model performance. The authors propose a new metric called Post-quantization Integral (PQI) that provides a more precise estimation of weight sensitivity. Additionally, they introduce ReQuant, a framework that enhances quantization methods by effectively selecting significant weights and improving overall model accuracy.'}, 'zh': {'title': '提升量化精度，降低模型成本', 'desc': '这篇论文讨论了大型语言模型（LLMs）在服务时的高成本问题。通过后训练权重量化，可以压缩模型大小，节省内存和带宽。研究发现，现有的基于梯度和海森矩阵的敏感度度量不够准确，低估了量化对损失函数的影响。为了解决这个问题，提出了后量化积分（PQI）作为一种更精确的敏感度度量，并进一步提出了ReQuant框架，以提高后训练量化方法的效果。'}}}, {'id': 'https://huggingface.co/papers/2503.01375', 'title': 'Combining Flow Matching and Transformers for Efficient Solution of Bayesian Inverse Problems', 'url': 'https://huggingface.co/papers/2503.01375', 'abstract': 'Solving Bayesian inverse problems efficiently remains a significant challenge due to the complexity of posterior distributions and the computational cost of traditional sampling methods. Given a series of observations and the forward model, we want to recover the distribution of the parameters, conditioned on observed experimental data. We show, that combining Conditional Flow Mathching (CFM) with transformer-based architecture, we can efficiently sample from such kind of distribution, conditioned on variable number of observations.', 'score': 1, 'issue_id': 2583, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': '9dd35b1faaa32b61', 'authors': ['Daniil Sherki', 'Ivan Oseledets', 'Ekaterina Muravleva'], 'affiliations': ['Artificial Intelligence Research Institute', 'Sberbank, AI4S Center', 'Skolkovo Institute of Science and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2503.01375.jpg', 'data': {'categories': ['#architecture', '#math'], 'emoji': '🔄', 'ru': {'title': 'Эффективная выборка из сложных апостериорных распределений с помощью CFM и трансформеров', 'desc': 'Статья представляет новый подход к решению байесовских обратных задач. Авторы предлагают комбинацию метода Conditional Flow Matching (CFM) с архитектурой на основе трансформеров. Этот подход позволяет эффективно осуществлять выборку из сложных апостериорных распределений, обусловленных переменным числом наблюдений. Метод преодолевает ограничения традиционных методов выборки, снижая вычислительные затраты.'}, 'en': {'title': 'Efficient Bayesian Inference with CFM and Transformers', 'desc': 'This paper addresses the challenge of efficiently solving Bayesian inverse problems, which involve estimating parameter distributions based on observed data. Traditional sampling methods can be computationally expensive, especially when dealing with complex posterior distributions. The authors propose a novel approach that combines Conditional Flow Matching (CFM) with transformer-based architectures to improve sampling efficiency. This method allows for effective sampling from parameter distributions conditioned on a variable number of observations, enhancing the ability to recover accurate parameter estimates.'}, 'zh': {'title': '高效贝叶斯逆问题求解的新方法', 'desc': '解决贝叶斯逆问题的效率仍然是一个重大挑战，因为后验分布的复杂性和传统采样方法的计算成本较高。我们希望在给定一系列观测和前向模型的情况下，恢复参数的分布，这些分布是基于观察到的实验数据。我们展示了将条件流匹配（CFM）与基于变换器的架构相结合，可以有效地从这种分布中进行采样，且该分布可以根据观测数量的变化而变化。此方法为贝叶斯推断提供了一种新的高效解决方案。'}}}, {'id': 'https://huggingface.co/papers/2503.02191', 'title': 'Understanding and Predicting Derailment in Toxic Conversations on GitHub', 'url': 'https://huggingface.co/papers/2503.02191', 'abstract': 'Software projects thrive on the involvement and contributions of individuals from different backgrounds. However, toxic language and negative interactions can hinder the participation and retention of contributors and alienate newcomers. Proactive moderation strategies aim to prevent toxicity from occurring by addressing conversations that have derailed from their intended purpose. This study aims to understand and predict conversational derailment leading to toxicity on GitHub.   To facilitate this research, we curate a novel dataset comprising 202 toxic conversations from GitHub with annotated derailment points, along with 696 non-toxic conversations as a baseline. Based on this dataset, we identify unique characteristics of toxic conversations and derailment points, including linguistic markers such as second-person pronouns, negation terms, and tones of Bitter Frustration and Impatience, as well as patterns in conversational dynamics between project contributors and external participants.   Leveraging these empirical observations, we propose a proactive moderation approach to automatically detect and address potentially harmful conversations before escalation. By utilizing modern LLMs, we develop a conversation trajectory summary technique that captures the evolution of discussions and identifies early signs of derailment. Our experiments demonstrate that LLM prompts tailored to provide summaries of GitHub conversations achieve 69% F1-Score in predicting conversational derailment, strongly improving over a set of baseline approaches.', 'score': 1, 'issue_id': 2581, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 марта', 'en': 'March 4', 'zh': '3月4日'}, 'hash': '1d0bc1069249c9e1', 'authors': ['Mia Mohammad Imran', 'Robert Zita', 'Rebekah Copeland', 'Preetha Chatterjee', 'Rahat Rizvi Rahman', 'Kostadin Damevski'], 'affiliations': ['Drexel University, Philadelphia, PA, USA', 'Eastern Mennonite University, Harrisonburg, VA, USA', 'Elmhurst University, Elmhurst, IL, USA', 'Missouri University of Science and Technology, Rolla, MO, USA', 'Virginia Commonwealth University, Richmond, VA, USA'], 'pdf_title_img': 'assets/pdf/title_img/2503.02191.jpg', 'data': {'categories': ['#ethics', '#dataset', '#multimodal', '#data'], 'emoji': '🛡️', 'ru': {'title': 'ИИ на страже здоровой атмосферы в open-source сообществах', 'desc': 'Исследование посвящено анализу и предотвращению токсичных взаимодействий в проектах на GitHub. Авторы создали датасет из 202 токсичных и 696 нетоксичных разговоров, выявив лингвистические маркеры и паттерны, характерные для деструктивных обсуждений. На основе этих данных была разработана система проактивной модерации с использованием больших языковых моделей (LLM). Эксперименты показали, что предложенный подход достигает 69% F1-меры в предсказании потенциально опасных разговоров на ранних стадиях.'}, 'en': {'title': 'Proactive Moderation: Detecting Toxicity Before It Escalates', 'desc': 'This paper investigates how toxic language in GitHub conversations can deter contributors and newcomers. It introduces a dataset of 202 toxic conversations with marked derailment points, alongside 696 non-toxic examples for comparison. The study identifies specific linguistic features and conversational dynamics that signal potential toxicity. Using large language models (LLMs), the authors propose a proactive moderation strategy that summarizes conversation trajectories to detect early signs of derailment, achieving a 69% F1-Score in predictions.'}, 'zh': {'title': '主动管理，防止对话偏离与有毒语言', 'desc': '本研究探讨了如何在GitHub上预测和理解对话偏离导致的有毒语言。我们创建了一个新数据集，包含202个有毒对话和696个非有毒对话，并标注了偏离点。通过分析这些对话的语言特征和动态模式，我们识别出有毒对话的独特特征。最后，我们提出了一种主动的管理策略，利用现代大语言模型自动检测和处理潜在的有害对话。'}}}, {'id': 'https://huggingface.co/papers/2503.04378', 'title': 'Dedicated Feedback and Edit Models Empower Inference-Time Scaling for Open-Ended General-Domain Tasks', 'url': 'https://huggingface.co/papers/2503.04378', 'abstract': 'Inference-Time Scaling has been critical to the success of recent models such as OpenAI o1 and DeepSeek R1. However, many techniques used to train models for inference-time scaling require tasks to have answers that can be verified, limiting their application to domains such as math, coding and logical reasoning. We take inspiration from how humans make first attempts, ask for detailed feedback from others and make improvements based on such feedback across a wide spectrum of open-ended endeavors. To this end, we collect data for and train dedicated Feedback and Edit Models that are capable of performing inference-time scaling for open-ended general-domain tasks. In our setup, one model generates an initial response, which are given feedback by a second model, that are then used by a third model to edit the response. We show that performance on Arena Hard, a benchmark strongly predictive of Chatbot Arena Elo can be boosted by scaling the number of initial response drafts, effective feedback and edited responses. When scaled optimally, our setup based on 70B models from the Llama 3 family can reach SoTA performance on Arena Hard at 92.7 as of 5 Mar 2025, surpassing OpenAI o1-preview-2024-09-12 with 90.4 and DeepSeek R1 with 92.3.', 'score': 1, 'issue_id': 2578, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 марта', 'en': 'March 6', 'zh': '3月6日'}, 'hash': '926e56aa51a0fefe', 'authors': ['Zhilin Wang', 'Jiaqi Zeng', 'Olivier Delalleau', 'Daniel Egert', 'Ellie Evans', 'Hoo-Chang Shin', 'Felipe Soares', 'Yi Dong', 'Oleksii Kuchaiev'], 'affiliations': ['NVIDIA'], 'pdf_title_img': 'assets/pdf/title_img/2503.04378.jpg', 'data': {'categories': ['#training', '#benchmark', '#inference', '#reasoning', '#rlhf', '#optimization'], 'emoji': '🔄', 'ru': {'title': 'Масштабирование при выводе для открытых задач: от черновика к улучшенному ответу', 'desc': 'Статья описывает новый метод масштабирования во время вывода для открытых задач общего домена. Авторы предлагают систему из трех моделей: одна генерирует начальный ответ, вторая дает обратную связь, а третья редактирует ответ на основе этой обратной связи. Эксперименты показывают, что такой подход позволяет значительно улучшить производительность на бенчмарке Arena Hard. При оптимальном масштабировании система на основе моделей семейства Llama 3 размером 70B достигает наилучших результатов, превосходя OpenAI o1 и DeepSeek R1.'}, 'en': {'title': 'Enhancing Open-Ended Task Performance through Feedback and Editing', 'desc': 'This paper discusses a novel approach to improve inference-time scaling in machine learning models, particularly for open-ended tasks. It introduces a system where one model generates an initial response, a second model provides feedback, and a third model edits the response based on that feedback. This method allows for more flexible applications beyond traditional tasks that require verifiable answers, such as math or coding. The authors demonstrate that by optimizing the number of drafts, feedback, and edits, their model achieves state-of-the-art performance on the Arena Hard benchmark, outperforming previous models.'}, 'zh': {'title': '推理时间扩展：提升开放性任务的性能', 'desc': '本文探讨了推理时间扩展在机器学习模型中的重要性，尤其是OpenAI o1和DeepSeek R1等模型。许多现有技术要求任务的答案可验证，这限制了它们在开放性任务中的应用。我们借鉴人类如何进行初步尝试、请求反馈并根据反馈进行改进的过程，开发了专门的反馈和编辑模型。通过优化初始响应草稿、有效反馈和编辑响应的数量，我们的模型在Arena Hard基准测试中达到了92.7的最新性能，超越了其他模型。'}}}, {'id': 'https://huggingface.co/papers/2503.04644', 'title': 'IFIR: A Comprehensive Benchmark for Evaluating Instruction-Following in Expert-Domain Information Retrieval', 'url': 'https://huggingface.co/papers/2503.04644', 'abstract': 'We introduce IFIR, the first comprehensive benchmark designed to evaluate instruction-following information retrieval (IR) in expert domains. IFIR includes 2,426 high-quality examples and covers eight subsets across four specialized domains: finance, law, healthcare, and science literature. Each subset addresses one or more domain-specific retrieval tasks, replicating real-world scenarios where customized instructions are critical. IFIR enables a detailed analysis of instruction-following retrieval capabilities by incorporating instructions at different levels of complexity. We also propose a novel LLM-based evaluation method to provide a more precise and reliable assessment of model performance in following instructions. Through extensive experiments on 15 frontier retrieval models, including those based on LLMs, our results reveal that current models face significant challenges in effectively following complex, domain-specific instructions. We further provide in-depth analyses to highlight these limitations, offering valuable insights to guide future advancements in retriever development.', 'score': 0, 'issue_id': 2585, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 марта', 'en': 'March 6', 'zh': '3月6日'}, 'hash': 'c4f19dc17abc0e8f', 'authors': ['Tingyu Song', 'Guo Gan', 'Mingsheng Shang', 'Yilun Zhao'], 'affiliations': ['Chongqing Institute of Green and Intelligent Technology, Chinese Academy of Sciences', 'School of Advanced Interdisciplinary Sciences, University of Chinese Academy of Sciences', 'Yale University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.04644.jpg', 'data': {'categories': ['#science', '#healthcare', '#dataset', '#benchmark'], 'emoji': '🔍', 'ru': {'title': 'IFIR: Новый стандарт оценки интеллектуального поиска информации', 'desc': 'IFIR - это новый комплексный бенчмарк для оценки информационного поиска с выполнением инструкций в экспертных областях. Он включает 2426 примеров высокого качества в 8 подмножествах из 4 специализированных доменов: финансы, право, здравоохранение и научная литература. IFIR позволяет проводить детальный анализ возможностей поиска с выполнением инструкций разной сложности. Эксперименты на 15 передовых моделях поиска показали значительные трудности в эффективном выполнении сложных доменно-специфичных инструкций.'}, 'en': {'title': 'IFIR: Benchmarking Instruction-Following in Expert Domains', 'desc': 'The paper presents IFIR, a new benchmark for assessing how well information retrieval systems can follow instructions in specialized fields like finance, law, healthcare, and science. It consists of 2,426 examples that simulate real-world retrieval tasks requiring tailored instructions. The benchmark allows for a nuanced evaluation of retrieval models, particularly focusing on their ability to handle varying complexities of instructions. The authors also introduce a new evaluation method using large language models (LLMs) to measure performance, revealing that existing models struggle with complex, domain-specific tasks and providing insights for future improvements.'}, 'zh': {'title': 'IFIR：专家领域指令跟随检索的首个基准', 'desc': '我们介绍了IFIR，这是第一个全面的基准，用于评估专家领域中的指令跟随信息检索（IR）。IFIR包含2426个高质量示例，涵盖金融、法律、医疗和科学文献等四个专业领域的八个子集。每个子集针对一个或多个特定领域的检索任务，模拟了需要定制指令的真实场景。通过对15个前沿检索模型的广泛实验，我们的结果显示，当前模型在有效跟随复杂的领域特定指令方面面临重大挑战。'}}}, {'id': 'https://huggingface.co/papers/2502.20730', 'title': 'DeepSolution: Boosting Complex Engineering Solution Design via Tree-based Exploration and Bi-point Thinking', 'url': 'https://huggingface.co/papers/2502.20730', 'abstract': "Designing solutions for complex engineering challenges is crucial in human production activities. However, previous research in the retrieval-augmented generation (RAG) field has not sufficiently addressed tasks related to the design of complex engineering solutions. To fill this gap, we introduce a new benchmark, SolutionBench, to evaluate a system's ability to generate complete and feasible solutions for engineering problems with multiple complex constraints. To further advance the design of complex engineering solutions, we propose a novel system, SolutionRAG, that leverages the tree-based exploration and bi-point thinking mechanism to generate reliable solutions. Extensive experimental results demonstrate that SolutionRAG achieves state-of-the-art (SOTA) performance on the SolutionBench, highlighting its potential to enhance the automation and reliability of complex engineering solution design in real-world applications.", 'score': 20, 'issue_id': 2486, 'pub_date': '2025-02-28', 'pub_date_card': {'ru': '28 февраля', 'en': 'February 28', 'zh': '2月28日'}, 'hash': 'e9ef168e304ec240', 'authors': ['Zhuoqun Li', 'Haiyang Yu', 'Xuanang Chen', 'Hongyu Lin', 'Yaojie Lu', 'Fei Huang', 'Xianpei Han', 'Yongbin Li', 'Le Sun'], 'affiliations': ['Chinese Information Processing Laboratory, Institute of Software, Chinese Academy of Sciences', 'Tongyi Lab', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2502.20730.jpg', 'data': {'categories': ['#rag', '#benchmark'], 'emoji': '🔧', 'ru': {'title': 'Умная система для проектирования сложных инженерных решений', 'desc': 'Статья представляет новый бенчмарк SolutionBench для оценки способности систем генерировать решения инженерных задач с множественными ограничениями. Авторы предлагают систему SolutionRAG, использующую древовидное исследование и механизм двухточечного мышления для создания надежных решений. Экспериментальные результаты показывают, что SolutionRAG достигает наилучших показателей на SolutionBench. Это демонстрирует потенциал системы для улучшения автоматизации и надежности проектирования сложных инженерных решений в реальных приложениях.'}, 'en': {'title': 'Revolutionizing Engineering Design with SolutionRAG', 'desc': 'This paper addresses the need for effective solutions in complex engineering design tasks, which have been overlooked in previous research on retrieval-augmented generation (RAG). It introduces a new benchmark called SolutionBench, aimed at evaluating the generation of feasible solutions under multiple constraints. The authors propose a novel system named SolutionRAG, which utilizes tree-based exploration and bi-point thinking to improve solution reliability. Experimental results show that SolutionRAG outperforms existing methods, indicating its potential to automate and enhance the design process in engineering applications.'}, 'zh': {'title': '提升复杂工程设计的自动化与可靠性', 'desc': '本论文提出了一个新的基准测试，称为SolutionBench，用于评估系统在生成复杂工程问题的完整和可行解决方案方面的能力。我们还提出了一种新系统SolutionRAG，利用树形探索和双点思维机制来生成可靠的解决方案。通过大量实验结果，SolutionRAG在SolutionBench上达到了最先进的性能，显示了其在实际应用中提高复杂工程解决方案设计的自动化和可靠性的潜力。此研究填补了以往在检索增强生成（RAG）领域中对复杂工程解决方案设计任务的研究空白。'}}}, {'id': 'https://huggingface.co/papers/2502.18600', 'title': 'Chain of Draft: Thinking Faster by Writing Less', 'url': 'https://huggingface.co/papers/2502.18600', 'abstract': 'Large Language Models (LLMs) have demonstrated remarkable performance in solving complex reasoning tasks through mechanisms like Chain-of-Thought (CoT) prompting, which emphasizes verbose, step-by-step reasoning. However, humans typically employ a more efficient strategy: drafting concise intermediate thoughts that capture only essential information. In this work, we propose Chain of Draft (CoD), a novel paradigm inspired by human cognitive processes, where LLMs generate minimalistic yet informative intermediate reasoning outputs while solving tasks. By reducing verbosity and focusing on critical insights, CoD matches or surpasses CoT in accuracy while using as little as only 7.6% of the tokens, significantly reducing cost and latency across various reasoning tasks.', 'score': 19, 'issue_id': 2491, 'pub_date': '2025-02-25', 'pub_date_card': {'ru': '25 февраля', 'en': 'February 25', 'zh': '2月25日'}, 'hash': '739d903f5735d9eb', 'authors': ['Silei Xu', 'Wenhao Xie', 'Lingxiao Zhao', 'Pengcheng He'], 'affiliations': ['Zoom Communications'], 'pdf_title_img': 'assets/pdf/title_img/2502.18600.jpg', 'data': {'categories': ['#reasoning', '#training', '#rl'], 'emoji': '✍️', 'ru': {'title': 'Эффективное рассуждение: краткость - сестра точности', 'desc': 'Статья представляет новый подход к решению сложных задач с помощью больших языковых моделей - Chain of Draft (CoD). В отличие от метода Chain-of-Thought (CoT), который использует подробные рассуждения, CoD имитирует человеческий подход, генерируя краткие промежуточные мысли. Этот метод позволяет достичь той же или лучшей точности, что и CoT, но при этом использует значительно меньше токенов. CoD демонстрирует эффективность в различных задачах, требующих рассуждений, снижая затраты и задержки.'}, 'en': {'title': 'Streamlining Reasoning: Less is More with Chain of Draft', 'desc': 'This paper introduces Chain of Draft (CoD), a new approach for Large Language Models (LLMs) that mimics human reasoning by generating concise intermediate thoughts. Unlike the traditional Chain-of-Thought (CoT) prompting, which relies on verbose explanations, CoD focuses on delivering essential information in a minimalistic format. The authors demonstrate that CoD can achieve comparable or even superior accuracy to CoT while using significantly fewer tokens, leading to reduced computational costs and faster processing times. This innovative method enhances the efficiency of LLMs in tackling complex reasoning tasks.'}, 'zh': {'title': '草稿链：高效推理的新方法', 'desc': '大型语言模型（LLMs）在解决复杂推理任务方面表现出色，尤其是通过链式思维（CoT）提示，强调逐步推理的详细过程。然而，人类通常采用更高效的策略：草拟简洁的中间思考，只捕捉关键信息。本文提出了一种新范式——草稿链（CoD），灵感来源于人类的认知过程，使LLMs在解决任务时生成简约而信息丰富的中间推理输出。通过减少冗长并专注于关键见解，CoD在准确性上与CoT相匹配或超越，同时仅使用7.6%的标记，显著降低了各种推理任务的成本和延迟。'}}}, {'id': 'https://huggingface.co/papers/2502.20380', 'title': 'Multi-Turn Code Generation Through Single-Step Rewards', 'url': 'https://huggingface.co/papers/2502.20380', 'abstract': 'We address the problem of code generation from multi-turn execution feedback. Existing methods either generate code without feedback or use complex, hierarchical reinforcement learning to optimize multi-turn rewards. We propose a simple yet scalable approach, muCode, that solves multi-turn code generation using only single-step rewards. Our key insight is that code generation is a one-step recoverable MDP, where the correct code can be recovered from any intermediate code state in a single turn. muCode iteratively trains both a generator to provide code solutions conditioned on multi-turn execution feedback and a verifier to score the newly generated code. Experimental evaluations show that our approach achieves significant improvements over the state-of-the-art baselines. We provide analysis of the design choices of the reward models and policy, and show the efficacy of muCode at utilizing the execution feedback. Our code is available at https://github.com/portal-cornell/muCode.', 'score': 17, 'issue_id': 2500, 'pub_date': '2025-02-27', 'pub_date_card': {'ru': '27 февраля', 'en': 'February 27', 'zh': '2月27日'}, 'hash': 'cceb0299fb5077b5', 'authors': ['Arnav Kumar Jain', 'Gonzalo Gonzalez-Pumariega', 'Wayne Chen', 'Alexander M Rush', 'Wenting Zhao', 'Sanjiban Choudhury'], 'affiliations': ['Cornell University', 'MilaQuebec AI Institute', 'Universite de Montreal'], 'pdf_title_img': 'assets/pdf/title_img/2502.20380.jpg', 'data': {'categories': ['#rl', '#rlhf', '#training', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Эффективная генерация кода с многоэтапной обратной связью', 'desc': 'Статья представляет новый подход к генерации кода с использованием многоэтапной обратной связи по выполнению, названный muCode. В отличие от существующих методов, использующих сложное иерархическое обучение с подкреплением, muCode применяет простой масштабируемый подход с одношаговыми вознаграждениями. Метод итеративно обучает генератор для создания кодовых решений и верификатор для оценки нового кода. Экспериментальные результаты показывают значительное улучшение по сравнению с современными базовыми методами.'}, 'en': {'title': 'Simplifying Code Generation with muCode: One-Step Recovery from Feedback', 'desc': 'This paper presents muCode, a novel approach for generating code based on multi-turn execution feedback. Unlike existing methods that rely on complex reinforcement learning techniques, muCode simplifies the process by using single-step rewards. The authors argue that code generation can be treated as a one-step recoverable Markov Decision Process (MDP), allowing for the recovery of correct code from any intermediate state. Through iterative training of a code generator and a verifier, muCode demonstrates significant performance improvements over current state-of-the-art methods.'}, 'zh': {'title': '简单高效的多轮代码生成方法', 'desc': '本文解决了从多轮执行反馈中生成代码的问题。现有方法要么在没有反馈的情况下生成代码，要么使用复杂的层次强化学习来优化多轮奖励。我们提出了一种简单而可扩展的方法muCode，仅使用单步奖励来解决多轮代码生成。实验结果表明，我们的方法在性能上显著优于现有的最先进基线。'}}}, {'id': 'https://huggingface.co/papers/2502.21318', 'title': 'How far can we go with ImageNet for Text-to-Image generation?', 'url': 'https://huggingface.co/papers/2502.21318', 'abstract': "Recent text-to-image (T2I) generation models have achieved remarkable results by training on billion-scale datasets, following a `bigger is better' paradigm that prioritizes data quantity over quality. We challenge this established paradigm by demonstrating that strategic data augmentation of small, well-curated datasets can match or outperform models trained on massive web-scraped collections. Using only ImageNet enhanced with well-designed text and image augmentations, we achieve a +2 overall score over SD-XL on GenEval and +5 on DPGBench while using just 1/10th the parameters and 1/1000th the training images. Our results suggest that strategic data augmentation, rather than massive datasets, could offer a more sustainable path forward for T2I generation.", 'score': 13, 'issue_id': 2498, 'pub_date': '2025-02-28', 'pub_date_card': {'ru': '28 февраля', 'en': 'February 28', 'zh': '2月28日'}, 'hash': '73a20b698d827c0d', 'authors': ['L. Degeorge', 'A. Ghosh', 'N. Dufour', 'D. Picard', 'V. Kalogeiton'], 'affiliations': ['AMIAD, Pole recherche', 'LIGM, Ecole Nationale des Ponts et Chaussees, IP Paris, Univ Gustave Eiffel, CNRS, France', 'LIX, Ecole Polytechnique, CNRS, IP Paris, France'], 'pdf_title_img': 'assets/pdf/title_img/2502.21318.jpg', 'data': {'categories': ['#synthetic', '#optimization', '#training', '#data', '#cv', '#dataset'], 'emoji': '🔬', 'ru': {'title': 'Качество данных побеждает количество в генерации изображений', 'desc': 'Исследователи предлагают новый подход к обучению моделей генерации изображений по тексту, основанный на стратегическом аугментировании небольших, но качественных наборов данных. Используя только ImageNet с тщательно разработанными текстовыми и визуальными аугментациями, им удалось превзойти модели, обученные на огромных веб-наборах данных. Их модель превосходит SD-XL на 2 балла в GenEval и на 5 баллов в DPGBench, используя всего 1/10 параметров и 1/1000 обучающих изображений. Результаты указывают на то, что стратегическое аугментирование данных может быть более устойчивым путем развития генеративных моделей, чем использование массивных датасетов.'}, 'en': {'title': 'Quality Over Quantity: Augmenting Small Datasets for Better T2I Models', 'desc': 'This paper challenges the common belief that larger datasets always lead to better text-to-image (T2I) generation models. It shows that by using strategic data augmentation on smaller, high-quality datasets, we can achieve results that are as good as or better than those from models trained on much larger datasets. Specifically, the authors enhanced ImageNet with carefully designed text and image augmentations, leading to significant performance improvements. Their findings suggest that focusing on data quality and augmentation may be a more efficient and sustainable approach for developing T2I models.'}, 'zh': {'title': '战略数据增强：小数据集的力量', 'desc': '最近的文本到图像生成模型通过在大规模数据集上训练取得了显著成果，遵循了“越大越好”的范式，优先考虑数据的数量而非质量。我们挑战了这一既定范式，展示了通过对小型、精心策划的数据集进行战略性数据增强，可以与在大规模网络抓取集合上训练的模型相匹敌或超越。仅使用经过精心设计的文本和图像增强的ImageNet，我们在GenEval上比SD-XL高出2分，在DPGBench上高出5分，同时只使用了1/10的参数和1/1000的训练图像。我们的结果表明，战略性数据增强而非大规模数据集，可能为文本到图像生成提供更可持续的发展路径。'}}}, {'id': 'https://huggingface.co/papers/2502.18017', 'title': 'ViDoRAG: Visual Document Retrieval-Augmented Generation via Dynamic Iterative Reasoning Agents', 'url': 'https://huggingface.co/papers/2502.18017', 'abstract': "Understanding information from visually rich documents remains a significant challenge for traditional Retrieval-Augmented Generation (RAG) methods. Existing benchmarks predominantly focus on image-based question answering (QA), overlooking the fundamental challenges of efficient retrieval, comprehension, and reasoning within dense visual documents. To bridge this gap, we introduce ViDoSeek, a novel dataset designed to evaluate RAG performance on visually rich documents requiring complex reasoning. Based on it, we identify key limitations in current RAG approaches: (i) purely visual retrieval methods struggle to effectively integrate both textual and visual features, and (ii) previous approaches often allocate insufficient reasoning tokens, limiting their effectiveness. To address these challenges, we propose ViDoRAG, a novel multi-agent RAG framework tailored for complex reasoning across visual documents. ViDoRAG employs a Gaussian Mixture Model (GMM)-based hybrid strategy to effectively handle multi-modal retrieval. To further elicit the model's reasoning capabilities, we introduce an iterative agent workflow incorporating exploration, summarization, and reflection, providing a framework for investigating test-time scaling in RAG domains. Extensive experiments on ViDoSeek validate the effectiveness and generalization of our approach. Notably, ViDoRAG outperforms existing methods by over 10% on the competitive ViDoSeek benchmark.", 'score': 9, 'issue_id': 2487, 'pub_date': '2025-02-25', 'pub_date_card': {'ru': '25 февраля', 'en': 'February 25', 'zh': '2月25日'}, 'hash': '4202273d8c895c2a', 'authors': ['Qiuchen Wang', 'Ruixue Ding', 'Zehui Chen', 'Weiqi Wu', 'Shihang Wang', 'Pengjun Xie', 'Feng Zhao'], 'affiliations': ['MoE Key Laboratory of Brain-inspired Intelligent Perception and Cognition, USTC', 'Shanghai Jiao Tong University', 'Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2502.18017.jpg', 'data': {'categories': ['#reasoning', '#agents', '#rag', '#games', '#benchmark', '#multimodal', '#dataset'], 'emoji': '🔍', 'ru': {'title': 'ViDoRAG: Новый подход к извлечению информации из визуальных документов', 'desc': 'Статья представляет новый набор данных ViDoSeek для оценки эффективности методов извлечения информации с добавлением генерации (RAG) на визуально насыщенных документах. Авторы выявляют ограничения существующих подходов RAG в обработке мультимодальной информации и сложных рассуждений. Для решения этих проблем предлагается новая мультиагентная система ViDoRAG, использующая гибридную стратегию на основе гауссовых смесей для мультимодального поиска. ViDoRAG демонстрирует улучшение производительности более чем на 10% по сравнению с существующими методами на бенчмарке ViDoSeek.'}, 'en': {'title': 'Enhancing RAG for Complex Visual Reasoning with ViDoRAG', 'desc': 'This paper addresses the challenges of understanding complex information in visually rich documents using Retrieval-Augmented Generation (RAG) methods. It introduces ViDoSeek, a new dataset that tests RAG performance on documents that require advanced reasoning skills. The authors highlight limitations in current RAG techniques, such as difficulties in integrating visual and textual data and inadequate reasoning capabilities. To overcome these issues, they propose ViDoRAG, a multi-agent framework that enhances retrieval and reasoning through a hybrid strategy and an iterative workflow, demonstrating significant improvements in performance on the ViDoSeek benchmark.'}, 'zh': {'title': '提升视觉文档理解的RAG新框架', 'desc': '理解视觉丰富文档中的信息对传统的增强检索生成（RAG）方法来说仍然是一个重大挑战。现有的基准主要集中在基于图像的问题回答（QA），而忽视了在密集视觉文档中高效检索、理解和推理的基本挑战。为了解决这些问题，我们引入了ViDoSeek，这是一个新颖的数据集，旨在评估RAG在需要复杂推理的视觉丰富文档上的表现。我们提出的ViDoRAG框架采用混合策略，结合多模态检索和迭代代理工作流，以提高模型的推理能力，并在ViDoSeek基准上显著超越现有方法。'}}}, {'id': 'https://huggingface.co/papers/2502.20545', 'title': 'SoS1: O1 and R1-Like Reasoning LLMs are Sum-of-Square Solvers', 'url': 'https://huggingface.co/papers/2502.20545', 'abstract': "Large Language Models (LLMs) have achieved human-level proficiency across diverse tasks, but their ability to perform rigorous mathematical problem solving remains an open challenge. In this work, we investigate a fundamental yet computationally intractable problem: determining whether a given multivariate polynomial is nonnegative. This problem, closely related to Hilbert's Seventeenth Problem, plays a crucial role in global polynomial optimization and has applications in various fields. First, we introduce SoS-1K, a meticulously curated dataset of approximately 1,000 polynomials, along with expert-designed reasoning instructions based on five progressively challenging criteria. Evaluating multiple state-of-the-art LLMs, we find that without structured guidance, all models perform only slightly above the random guess baseline 50%. However, high-quality reasoning instructions significantly improve accuracy, boosting performance up to 81%. Furthermore, our 7B model, SoS-7B, fine-tuned on SoS-1K for just 4 hours, outperforms the 671B DeepSeek-V3 and GPT-4o-mini in accuracy while only requiring 1.8% and 5% of the computation time needed for letters, respectively. Our findings highlight the potential of LLMs to push the boundaries of mathematical reasoning and tackle NP-hard problems.", 'score': 9, 'issue_id': 2486, 'pub_date': '2025-02-27', 'pub_date_card': {'ru': '27 февраля', 'en': 'February 27', 'zh': '2月27日'}, 'hash': 'fb32f9423103ece9', 'authors': ['Kechen Li', 'Wenqi Zhu', 'Coralia Cartis', 'Tianbo Ji', 'Shiwei Liu'], 'affiliations': ['Mathematical Institute, University of Oxford', 'Nanjing University of Aeronautics and Astronautics', 'School of Transportation and Civil Engineering, Nantong University'], 'pdf_title_img': 'assets/pdf/title_img/2502.20545.jpg', 'data': {'categories': ['#dataset', '#optimization', '#training', '#math', '#reasoning'], 'emoji': '🧮', 'ru': {'title': 'Большие языковые модели преодолевают границы математического мышления', 'desc': 'В этой статье исследуется способность больших языковых моделей (LLM) решать сложные математические задачи, в частности, определение неотрицательности многомерных полиномов. Авторы создали датасет SoS-1K из 1000 полиномов и разработали инструкции по рассуждению для моделей. Эксперименты показали, что без структурированного руководства модели работают немного лучше случайного угадывания, но с качественными инструкциями точность повышается до 81%. Модель SoS-7B, дообученная на SoS-1K, превзошла более крупные модели по точности и скорости вычислений.'}, 'en': {'title': 'Unlocking Mathematical Reasoning in LLMs with Structured Guidance', 'desc': 'This paper explores the limitations of Large Language Models (LLMs) in solving complex mathematical problems, specifically the challenge of determining if a multivariate polynomial is nonnegative. The authors introduce a new dataset called SoS-1K, which contains around 1,000 polynomials and structured reasoning instructions to guide the models. They demonstrate that LLMs perform poorly without guidance, achieving only slightly above random guessing, but can significantly improve their accuracy with high-quality instructions. Notably, their fine-tuned model, SoS-7B, surpasses larger models in performance while being more computationally efficient, showcasing the potential of LLMs in addressing NP-hard problems.'}, 'zh': {'title': '推动数学推理的边界', 'desc': '大型语言模型（LLMs）在多种任务中达到了人类水平的能力，但在严格的数学问题解决方面仍然面临挑战。本文研究了一个基本但计算上难以处理的问题：判断给定的多变量多项式是否非负。我们引入了SoS-1K数据集，包含约1000个多项式，并设计了基于五个逐步挑战标准的推理指导。实验表明，经过高质量的推理指导后，模型的准确率显著提高，最高可达81%。'}}}, {'id': 'https://huggingface.co/papers/2502.20396', 'title': 'Sim-to-Real Reinforcement Learning for Vision-Based Dexterous Manipulation on Humanoids', 'url': 'https://huggingface.co/papers/2502.20396', 'abstract': 'Reinforcement learning has delivered promising results in achieving human- or even superhuman-level capabilities across diverse problem domains, but success in dexterous robot manipulation remains limited. This work investigates the key challenges in applying reinforcement learning to solve a collection of contact-rich manipulation tasks on a humanoid embodiment. We introduce novel techniques to overcome the identified challenges with empirical validation. Our main contributions include an automated real-to-sim tuning module that brings the simulated environment closer to the real world, a generalized reward design scheme that simplifies reward engineering for long-horizon contact-rich manipulation tasks, a divide-and-conquer distillation process that improves the sample efficiency of hard-exploration problems while maintaining sim-to-real performance, and a mixture of sparse and dense object representations to bridge the sim-to-real perception gap. We show promising results on three humanoid dexterous manipulation tasks, with ablation studies on each technique. Our work presents a successful approach to learning humanoid dexterous manipulation using sim-to-real reinforcement learning, achieving robust generalization and high performance without the need for human demonstration.', 'score': 7, 'issue_id': 2486, 'pub_date': '2025-02-27', 'pub_date_card': {'ru': '27 февраля', 'en': 'February 27', 'zh': '2月27日'}, 'hash': '41439b4f54e02c9b', 'authors': ['Toru Lin', 'Kartik Sachdev', 'Linxi Fan', 'Jitendra Malik', 'Yuke Zhu'], 'affiliations': ['NVIDIA', 'UC Berkeley', 'UT Austin'], 'pdf_title_img': 'assets/pdf/title_img/2502.20396.jpg', 'data': {'categories': ['#robotics', '#rl', '#games', '#optimization'], 'emoji': '🤖', 'ru': {'title': 'Ловкость робота: от симуляции к реальности', 'desc': 'Статья описывает применение обучения с подкреплением для решения задач манипуляции объектами с помощью человекоподобного робота. Авторы представляют новые методы для преодоления ключевых проблем, включая автоматическую настройку симуляции, обобщенную схему проектирования наград и смешанное представление объектов. Исследование демонстрирует успешные результаты на трех задачах ловкой манипуляции, достигая надежной генерализации и высокой производительности. Работа предлагает эффективный подход к обучению человекоподобных роботов сложным манипуляциям без необходимости в демонстрациях человека.'}, 'en': {'title': 'Reinforcement Learning for Dexterous Robot Manipulation: Bridging Sim and Real Worlds', 'desc': 'This paper addresses the challenges of using reinforcement learning for complex robot manipulation tasks that involve physical contact. The authors propose innovative methods to enhance the performance of humanoid robots in these tasks, including a system to align simulated environments with real-world conditions. They also introduce a new reward design that simplifies the process of creating effective rewards for long tasks and a technique to improve learning efficiency in difficult scenarios. The results demonstrate that their approach allows robots to learn dexterous manipulation effectively, achieving high performance without requiring human guidance.'}, 'zh': {'title': '突破类人机器人灵巧操作的强化学习挑战', 'desc': '本研究探讨了在类人机器人灵巧操作任务中应用强化学习的关键挑战。我们提出了新技术来克服这些挑战，包括自动化的真实到模拟调优模块，以缩小模拟环境与现实世界的差距。我们还设计了一种通用的奖励机制，简化了长时间接触丰富的操作任务的奖励工程。通过对三项类人灵巧操作任务的实验，我们展示了在不需要人类示范的情况下，使用模拟到真实的强化学习实现了稳健的泛化和高性能。'}}}, {'id': 'https://huggingface.co/papers/2502.19577', 'title': 'Tell me why: Visual foundation models as self-explainable classifiers', 'url': 'https://huggingface.co/papers/2502.19577', 'abstract': 'Visual foundation models (VFMs) have become increasingly popular due to their state-of-the-art performance. However, interpretability remains crucial for critical applications. In this sense, self-explainable models (SEM) aim to provide interpretable classifiers that decompose predictions into a weighted sum of interpretable concepts. Despite their promise, recent studies have shown that these explanations often lack faithfulness. In this work, we combine VFMs with a novel prototypical architecture and specialized training objectives. By training only a lightweight head (approximately 1M parameters) on top of frozen VFMs, our approach (ProtoFM) offers an efficient and interpretable solution. Evaluations demonstrate that our approach achieves competitive classification performance while outperforming existing models across a range of interpretability metrics derived from the literature. Code is available at https://github.com/hturbe/proto-fm.', 'score': 6, 'issue_id': 2493, 'pub_date': '2025-02-26', 'pub_date_card': {'ru': '26 февраля', 'en': 'February 26', 'zh': '2月26日'}, 'hash': '7d2bd5235959eba5', 'authors': ['Hugues Turbé', 'Mina Bjelogrlic', 'Gianmarco Mengaldo', 'Christian Lovis'], 'affiliations': ['Department of Mechanical Engineering, College of Design and Engineering, National University of Singapore, Singapore', 'Department of Radiology and Medical Informatics, University of Geneva, Geneva, Switzerland', 'Division of Medical Information Sciences, Geneva University Hospitals, Geneva, Switzerland'], 'pdf_title_img': 'assets/pdf/title_img/2502.19577.jpg', 'data': {'categories': ['#cv', '#small_models', '#architecture', '#interpretability', '#open_source', '#training', '#optimization'], 'emoji': '🔍', 'ru': {'title': 'ProtoFM: Интерпретируемость визуальных моделей без потери точности', 'desc': 'Статья описывает новый подход к интерпретируемости визуальных моделей-основ (VFM). Авторы предлагают комбинацию VFM с прототипической архитектурой и специализированными целями обучения, названную ProtoFM. Этот метод обучает только легковесную верхушку поверх замороженных VFM, что обеспечивает эффективное и интерпретируемое решение. Эксперименты показывают, что ProtoFM достигает конкурентоспособной точности классификации, превосходя существующие модели по ряду метрик интерпретируемости.'}, 'en': {'title': 'ProtoFM: Interpretable and Efficient Visual Foundation Models', 'desc': 'This paper introduces ProtoFM, a new approach that combines visual foundation models (VFMs) with a prototypical architecture to enhance interpretability in machine learning. The method focuses on creating self-explainable models (SEMs) that break down predictions into understandable components, addressing the issue of faithfulness in explanations. By training a lightweight head on top of frozen VFMs, ProtoFM maintains high classification performance while improving interpretability metrics. The results show that ProtoFM outperforms existing models, making it a promising solution for applications requiring both accuracy and clarity.'}, 'zh': {'title': '高效可解释的视觉基础模型', 'desc': '视觉基础模型（VFM）因其卓越的性能而受到广泛关注，但在关键应用中可解释性仍然至关重要。自解释模型（SEM）旨在提供可解释的分类器，将预测分解为可解释概念的加权和。尽管有潜力，近期研究表明这些解释往往缺乏可信度。我们提出了一种结合VFM和新型原型架构的方案（ProtoFM），通过在冻结的VFM上训练一个轻量级的头部模型，提供了一种高效且可解释的解决方案。'}}}, {'id': 'https://huggingface.co/papers/2502.20969', 'title': 'TeleRAG: Efficient Retrieval-Augmented Generation Inference with Lookahead Retrieval', 'url': 'https://huggingface.co/papers/2502.20969', 'abstract': 'Retrieval-augmented generation (RAG) extends large language models (LLMs) with external data sources to enhance factual correctness and domain coverage. Modern RAG pipelines rely on large datastores, leading to system challenges in latency-sensitive deployments, especially when limited GPU memory is available. To address these challenges, we propose TeleRAG, an efficient inference system that reduces RAG latency with minimal GPU memory requirements. The core innovation of TeleRAG is lookahead retrieval, a prefetching mechanism that anticipates required data and transfers it from CPU to GPU in parallel with LLM generation. By leveraging the modularity of RAG pipelines, the inverted file index (IVF) search algorithm and similarities between queries, TeleRAG optimally overlaps data movement and computation. Experimental results show that TeleRAG reduces end-to-end RAG inference latency by up to 1.72x on average compared to state-of-the-art systems, enabling faster, more memory-efficient deployments of advanced RAG applications.', 'score': 5, 'issue_id': 2498, 'pub_date': '2025-02-28', 'pub_date_card': {'ru': '28 февраля', 'en': 'February 28', 'zh': '2月28日'}, 'hash': '5a85f59eed1c3c3b', 'authors': ['Chien-Yu Lin', 'Keisuke Kamahori', 'Yiyu Liu', 'Xiaoxiang Shi', 'Madhav Kashyap', 'Yile Gu', 'Rulin Shao', 'Zihao Ye', 'Kan Zhu', 'Stephanie Wang', 'Arvind Krishnamurthy', 'Rohan Kadekodi', 'Luis Ceze', 'Baris Kasikci'], 'affiliations': ['Shanghai Jiao Tong University', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2502.20969.jpg', 'data': {'categories': ['#rag', '#inference', '#optimization'], 'emoji': '🚀', 'ru': {'title': 'TeleRAG: Ускорение RAG без компромиссов по памяти', 'desc': 'TeleRAG - это эффективная система вывода, которая снижает задержку RAG при минимальных требованиях к памяти GPU. Основное нововведение TeleRAG - это опережающее извлечение, механизм предварительной выборки, который предвидит необходимые данные и передает их с CPU на GPU параллельно с генерацией LLM. Система использует модульность RAG-конвейеров, алгоритм поиска инвертированного файлового индекса (IVF) и сходства между запросами для оптимального совмещения перемещения данных и вычислений. Экспериментальные результаты показывают, что TeleRAG снижает задержку вывода RAG в среднем до 1,72 раза по сравнению с современными системами.'}, 'en': {'title': 'Speeding Up RAG with TeleRAG: Faster and Smarter Inference!', 'desc': 'This paper introduces TeleRAG, a new system designed to improve the efficiency of retrieval-augmented generation (RAG) in large language models (LLMs). TeleRAG addresses the latency issues that arise from using large datastores, particularly in environments with limited GPU memory. The key feature of TeleRAG is its lookahead retrieval mechanism, which allows data to be pre-fetched from the CPU to the GPU while the LLM is generating responses. Experimental results demonstrate that TeleRAG can significantly reduce inference latency, making RAG applications faster and more efficient.'}, 'zh': {'title': 'TeleRAG：高效的RAG推理系统', 'desc': '检索增强生成（RAG）通过外部数据源扩展大型语言模型（LLM），以提高事实准确性和领域覆盖率。然而，现代RAG管道依赖于大型数据存储，导致在延迟敏感的部署中面临系统挑战，尤其是在GPU内存有限的情况下。为了解决这些问题，我们提出了TeleRAG，这是一种高效的推理系统，能够在最小的GPU内存需求下减少RAG延迟。TeleRAG的核心创新是前瞻性检索，这是一种预取机制，可以在LLM生成的同时，预测所需数据并将其从CPU并行传输到GPU。'}}}, {'id': 'https://huggingface.co/papers/2502.17941', 'title': 'Optimal Brain Apoptosis', 'url': 'https://huggingface.co/papers/2502.17941', 'abstract': 'The increasing complexity and parameter count of Convolutional Neural Networks (CNNs) and Transformers pose challenges in terms of computational efficiency and resource demands. Pruning has been identified as an effective strategy to address these challenges by removing redundant elements such as neurons, channels, or connections, thereby enhancing computational efficiency without heavily compromising performance. This paper builds on the foundational work of Optimal Brain Damage (OBD) by advancing the methodology of parameter importance estimation using the Hessian matrix. Unlike previous approaches that rely on approximations, we introduce Optimal Brain Apoptosis (OBA), a novel pruning method that calculates the Hessian-vector product value directly for each parameter. By decomposing the Hessian matrix across network layers and identifying conditions under which inter-layer Hessian submatrices are non-zero, we propose a highly efficient technique for computing the second-order Taylor expansion of parameters. This approach allows for a more precise pruning process, particularly in the context of CNNs and Transformers, as validated in our experiments including VGG19, ResNet32, ResNet50, and ViT-B/16 on CIFAR10, CIFAR100 and Imagenet datasets. Our code is available at https://github.com/NEU-REAL/OBA.', 'score': 5, 'issue_id': 2495, 'pub_date': '2025-02-25', 'pub_date_card': {'ru': '25 февраля', 'en': 'February 25', 'zh': '2月25日'}, 'hash': '3748780b9de1393e', 'authors': ['Mingyuan Sun', 'Zheng Fang', 'Jiaxu Wang', 'Junjie Jiang', 'Delei Kong', 'Chenming Hu', 'Yuetong Fang', 'Renjing Xu'], 'affiliations': ['Hunan University', 'Northeastern University', 'The Hong Kong University of Science and Technology (Guangzhou)'], 'pdf_title_img': 'assets/pdf/title_img/2502.17941.jpg', 'data': {'categories': ['#optimization', '#inference', '#training', '#architecture'], 'emoji': '✂️', 'ru': {'title': 'Точная обрезка нейронных сетей с помощью прямого расчета Гессиана', 'desc': 'Статья представляет новый метод прунинга нейронных сетей под названием Optimal Brain Apoptosis (OBA). OBA использует прямой расчет произведения Гессиана на вектор для каждого параметра, что позволяет более точно оценивать важность параметров. Метод эффективно применяется к сверточным нейронным сетям и трансформерам, разлагая матрицу Гессиана по слоям сети. Эксперименты на различных архитектурах и датасетах подтверждают эффективность предложенного подхода.'}, 'en': {'title': 'Efficient Neural Network Pruning with Optimal Brain Apoptosis', 'desc': 'This paper addresses the challenges of high computational demands in Convolutional Neural Networks (CNNs) and Transformers by introducing a new pruning method called Optimal Brain Apoptosis (OBA). OBA improves upon previous methods by directly calculating the Hessian-vector product for each parameter, allowing for more accurate estimation of parameter importance. The authors decompose the Hessian matrix across layers to identify non-zero conditions, which enhances the efficiency of the pruning process. Experimental results demonstrate the effectiveness of OBA on various architectures and datasets, confirming its potential to optimize neural networks without significant performance loss.'}, 'zh': {'title': '高效剪枝：最优脑凋亡方法', 'desc': '随着卷积神经网络（CNN）和变换器（Transformers）模型的复杂性和参数数量的增加，计算效率和资源需求面临挑战。剪枝被认为是一种有效的策略，通过去除冗余元素（如神经元、通道或连接）来提高计算效率，而不会严重影响性能。本文在最优脑损伤（OBD）的基础上，提出了一种新的剪枝方法——最优脑凋亡（OBA），通过直接计算每个参数的Hessian-向量乘积值来估计参数的重要性。我们的方法在多个数据集上进行了验证，包括VGG19、ResNet32、ResNet50和ViT-B/16，展示了在CNN和Transformers中的高效剪枝过程。'}}}, {'id': 'https://huggingface.co/papers/2502.19731', 'title': "Preference Learning Unlocks LLMs' Psycho-Counseling Skills", 'url': 'https://huggingface.co/papers/2502.19731', 'abstract': "Applying large language models (LLMs) to assist in psycho-counseling is an emerging and meaningful approach, driven by the significant gap between patient needs and the availability of mental health support. However, current LLMs struggle to consistently provide effective responses to client speeches, largely due to the lack of supervision from high-quality real psycho-counseling data, whose content is typically inaccessible due to client privacy concerns. Furthermore, the quality of therapists' responses in available sessions can vary significantly based on their professional training and experience. Assessing the quality of therapists' responses remains an open challenge. In this work, we address these challenges by first proposing a set of professional and comprehensive principles to evaluate therapists' responses to client speeches. Using these principles, we create a preference dataset, PsychoCounsel-Preference, which contains 36k high-quality preference comparison pairs. This dataset aligns with the preferences of professional psychotherapists, providing a robust foundation for evaluating and improving LLMs in psycho-counseling. Experiments on reward modeling and preference learning demonstrate that PsychoCounsel-Preference is an excellent resource for LLMs to acquire essential skills for responding to clients in a counseling session. Our best-aligned model, PsychoCounsel-Llama3-8B, achieves an impressive win rate of 87% against GPT-4o. We release PsychoCounsel-Preference, PsychoCounsel-Llama3-8B and the reward model PsychoCounsel Llama3-8B-Reward to facilitate the research of psycho-counseling with LLMs at: https://hf.co/Psychotherapy-LLM.", 'score': 4, 'issue_id': 2500, 'pub_date': '2025-02-27', 'pub_date_card': {'ru': '27 февраля', 'en': 'February 27', 'zh': '2月27日'}, 'hash': '533e7e87242a9b9e', 'authors': ['Mian Zhang', 'Shaun M. Eack', 'Zhiyu Zoey Chen'], 'affiliations': ['Department of Computer Science, University of Texas at Dallas', 'School of Social Work, University of Pittsburgh'], 'pdf_title_img': 'assets/pdf/title_img/2502.19731.jpg', 'data': {'categories': ['#open_source', '#alignment', '#healthcare', '#data', '#dataset'], 'emoji': '🧠', 'ru': {'title': 'Большие языковые модели на страже психического здоровья', 'desc': 'Статья представляет новый подход к применению больших языковых моделей (LLM) в психологическом консультировании. Авторы разработали набор принципов для оценки ответов терапевтов и создали датасет PsychoCounsel-Preference, содержащий 36 тысяч пар сравнений высокого качества. На основе этих данных была обучена модель PsychoCounsel-Llama3-8B, которая показала впечатляющие результаты в сравнении с GPT-4. Исследование направлено на преодоление разрыва между потребностями пациентов и доступностью психологической помощи.'}, 'en': {'title': 'Enhancing Psycho-Counseling with LLMs: A New Standard for Therapist Response Evaluation', 'desc': "This paper explores the use of large language models (LLMs) in psycho-counseling, addressing the gap between patient needs and available mental health support. It highlights the challenges faced by LLMs in generating effective responses due to a lack of high-quality training data and variability in therapist responses. To tackle these issues, the authors propose evaluation principles for therapist responses and create a dataset called PsychoCounsel-Preference, which includes 36,000 preference comparison pairs aligned with professional psychotherapists' judgments. The results show that their model, PsychoCounsel-Llama3-8B, significantly outperforms existing models, achieving an 87% win rate against GPT-4o, thus demonstrating the potential of LLMs in enhancing psycho-counseling practices."}, 'zh': {'title': '提升心理咨询的语言模型应用', 'desc': '本研究探讨了大型语言模型（LLMs）在心理咨询中的应用，旨在填补患者需求与心理健康支持之间的差距。由于缺乏高质量的真实心理咨询数据，当前的LLMs在回应客户发言时效果不佳。我们提出了一套专业的评估原则，并创建了包含36,000个高质量偏好比较对的PsychoCounsel-Preference数据集，以帮助评估和改进LLMs在心理咨询中的表现。实验结果表明，PsychoCounsel-Preference为LLMs提供了必要的技能基础，使其在咨询会话中更有效地回应客户。'}}}, {'id': 'https://huggingface.co/papers/2502.20583', 'title': 'LiteASR: Efficient Automatic Speech Recognition with Low-Rank Approximation', 'url': 'https://huggingface.co/papers/2502.20583', 'abstract': "Modern automatic speech recognition (ASR) models, such as OpenAI's Whisper, rely on deep encoder-decoder architectures, and their encoders are a critical bottleneck for efficient deployment due to high computational intensity. We introduce LiteASR, a low-rank compression scheme for ASR encoders that significantly reduces inference costs while maintaining transcription accuracy. Our approach leverages the strong low-rank properties observed in intermediate activations: by applying principal component analysis (PCA) with a small calibration dataset, we approximate linear transformations with a chain of low-rank matrix multiplications, and further optimize self-attention to work in the reduced dimension. Evaluation results show that our method can compress Whisper large-v3's encoder size by over 50%, matching Whisper medium's size with better transcription accuracy, thereby establishing a new Pareto-optimal frontier of efficiency and performance. The code of LiteASR is available at https://github.com/efeslab/LiteASR.", 'score': 4, 'issue_id': 2486, 'pub_date': '2025-02-27', 'pub_date_card': {'ru': '27 февраля', 'en': 'February 27', 'zh': '2月27日'}, 'hash': '3c7268c0881fa426', 'authors': ['Keisuke Kamahori', 'Jungo Kasai', 'Noriyuki Kojima', 'Baris Kasikci'], 'affiliations': ['Kotoba Technologies Inc.', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2502.20583.jpg', 'data': {'categories': ['#inference', '#optimization', '#audio'], 'emoji': '🎙️', 'ru': {'title': 'LiteASR: Эффективное сжатие энкодеров ASR без потери точности', 'desc': 'LiteASR - это метод сжатия энкодеров для систем автоматического распознавания речи (ASR), который значительно уменьшает вычислительные затраты при сохранении точности транскрипции. Метод использует анализ главных компонент (PCA) для аппроксимации линейных преобразований цепочкой умножений матриц низкого ранга. LiteASR позволяет сжать размер энкодера модели Whisper large-v3 более чем на 50%, достигая размера Whisper medium с лучшей точностью транскрипции. Это устанавливает новую Парето-оптимальную границу эффективности и производительности для моделей ASR.'}, 'en': {'title': 'LiteASR: Efficient ASR with Low-Rank Compression', 'desc': "This paper presents LiteASR, a novel low-rank compression technique designed to enhance the efficiency of automatic speech recognition (ASR) models, particularly focusing on the encoder component. By utilizing principal component analysis (PCA) on intermediate activations, LiteASR reduces the computational load during inference while preserving transcription accuracy. The method achieves over 50% reduction in the encoder size of OpenAI's Whisper large-v3 model, aligning its performance with that of the medium version but with improved accuracy. This work sets a new standard for balancing efficiency and performance in ASR systems."}, 'zh': {'title': 'LiteASR：高效的低秩压缩方案', 'desc': '现代自动语音识别（ASR）模型，如OpenAI的Whisper，依赖于深度编码器-解码器架构，而编码器的计算强度是高效部署的瓶颈。我们提出了LiteASR，这是一种针对ASR编码器的低秩压缩方案，能够显著降低推理成本，同时保持转录准确性。我们的方法利用了中间激活中的强低秩特性，通过使用小型校准数据集的主成分分析（PCA），用低秩矩阵乘法链来近似线性变换，并进一步优化自注意力机制以适应降维后的数据。评估结果表明，我们的方法可以将Whisper large-v3的编码器大小压缩超过50%，并在转录准确性上优于Whisper medium，从而建立了效率与性能的新帕累托最优边界。'}}}, {'id': 'https://huggingface.co/papers/2502.20900', 'title': 'DexGraspVLA: A Vision-Language-Action Framework Towards General Dexterous Grasping', 'url': 'https://huggingface.co/papers/2502.20900', 'abstract': "Dexterous grasping remains a fundamental yet challenging problem in robotics. A general-purpose robot must be capable of grasping diverse objects in arbitrary scenarios. However, existing research typically relies on specific assumptions, such as single-object settings or limited environments, leading to constrained generalization. Our solution is DexGraspVLA, a hierarchical framework that utilizes a pre-trained Vision-Language model as the high-level task planner and learns a diffusion-based policy as the low-level Action controller. The key insight lies in iteratively transforming diverse language and visual inputs into domain-invariant representations, where imitation learning can be effectively applied due to the alleviation of domain shift. Thus, it enables robust generalization across a wide range of real-world scenarios. Notably, our method achieves a 90+% success rate under thousands of unseen object, lighting, and background combinations in a ``zero-shot'' environment. Empirical analysis further confirms the consistency of internal model behavior across environmental variations, thereby validating our design and explaining its generalization performance. We hope our work can be a step forward in achieving general dexterous grasping. Our demo and code can be found at https://dexgraspvla.github.io/.", 'score': 3, 'issue_id': 2498, 'pub_date': '2025-02-28', 'pub_date_card': {'ru': '28 февраля', 'en': 'February 28', 'zh': '2月28日'}, 'hash': '5d9b331844235882', 'authors': ['Yifan Zhong', 'Xuchuan Huang', 'Ruochong Li', 'Ceyao Zhang', 'Yitao Liang', 'Yaodong Yang', 'Yuanpei Chen'], 'affiliations': ['Hong Kong University of Science and Technology (Guangzhou)', 'Institute for AI, Peking University', 'PKU-PsiBot Joint Lab'], 'pdf_title_img': 'assets/pdf/title_img/2502.20900.jpg', 'data': {'categories': ['#games', '#optimization', '#robotics', '#diffusion', '#interpretability', '#multimodal'], 'emoji': '🦾', 'ru': {'title': 'Универсальный захват объектов роботами с помощью зрения и языка', 'desc': 'DexGraspVLA - это иерархическая система для роботизированного захвата объектов, использующая предобученную Vision-Language модель в качестве высокоуровневого планировщика задач и обучаемую диффузионную политику в качестве низкоуровневого контроллера действий. Ключевая идея заключается в итеративном преобразовании разнородных языковых и визуальных входных данных в инвариантные к домену представления, что позволяет эффективно применять имитационное обучение. Система демонстрирует надежную генерализацию в широком диапазоне реальных сценариев, достигая более 90% успешных захватов в условиях тысяч невиданных ранее комбинаций объектов, освещения и фонов. Эмпирический анализ подтверждает согласованность внутреннего поведения модели при различных условиях окружающей среды.'}, 'en': {'title': 'Achieving Dexterous Grasping with DexGraspVLA', 'desc': 'This paper presents DexGraspVLA, a novel hierarchical framework designed to improve dexterous grasping in robotics. It combines a pre-trained Vision-Language model for high-level task planning with a diffusion-based policy for low-level action control. The framework effectively transforms diverse language and visual inputs into domain-invariant representations, allowing for better generalization across various real-world scenarios. The results show a success rate of over 90% in grasping tasks involving thousands of unseen objects and conditions, demonstrating the robustness of the approach.'}, 'zh': {'title': '实现灵巧抓取的突破性进展', 'desc': '本论文提出了一种名为DexGraspVLA的层次框架，旨在解决机器人灵巧抓取的问题。该框架利用预训练的视觉-语言模型作为高层任务规划器，并学习基于扩散的策略作为低层动作控制器。通过将多样的语言和视觉输入迭代转换为领域不变的表示，减轻了领域转移的影响，从而有效应用模仿学习。我们的实验表明，该方法在数千种未见物体、光照和背景组合下，在“零-shot”环境中实现了90%以上的成功率，展示了其在真实场景中的强大泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2502.21291', 'title': 'MIGE: A Unified Framework for Multimodal Instruction-Based Image Generation and Editing', 'url': 'https://huggingface.co/papers/2502.21291', 'abstract': 'Despite significant progress in diffusion-based image generation, subject-driven generation and instruction-based editing remain challenging. Existing methods typically treat them separately, struggling with limited high-quality data and poor generalization. However, both tasks require capturing complex visual variations while maintaining consistency between inputs and outputs. Therefore, we propose MIGE, a unified framework that standardizes task representations using multimodal instructions. It treats subject-driven generation as creation on a blank canvas and instruction-based editing as modification of an existing image, establishing a shared input-output formulation. MIGE introduces a novel multimodal encoder that maps free-form multimodal instructions into a unified vision-language space, integrating visual and semantic features through a feature fusion mechanism.This unification enables joint training of both tasks, providing two key advantages: (1) Cross-Task Enhancement: By leveraging shared visual and semantic representations, joint training improves instruction adherence and visual consistency in both subject-driven generation and instruction-based editing. (2) Generalization: Learning in a unified format facilitates cross-task knowledge transfer, enabling MIGE to generalize to novel compositional tasks, including instruction-based subject-driven editing. Experiments show that MIGE excels in both subject-driven generation and instruction-based editing while setting a state-of-the-art in the new task of instruction-based subject-driven editing. Code and model have been publicly available at https://github.com/Eureka-Maggie/MIGE.', 'score': 3, 'issue_id': 2498, 'pub_date': '2025-02-28', 'pub_date_card': {'ru': '28 февраля', 'en': 'February 28', 'zh': '2月28日'}, 'hash': '55451dcf6bfad4a4', 'authors': ['Xueyun Tian', 'Wei Li', 'Bingbing Xu', 'Yige Yuan', 'Yuanzhuo Wang', 'Huawei Shen'], 'affiliations': ['Huawei Shen CAS Key Laboratory of AI Safety, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China', 'University of Chinese Academy of Sciences, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2502.21291.jpg', 'data': {'categories': ['#open_source', '#cv', '#diffusion', '#transfer_learning', '#multimodal'], 'emoji': '🎨', 'ru': {'title': 'Единый подход к генерации и редактированию изображений на основе инструкций', 'desc': 'MIGE - это унифицированная система для генерации изображений на основе инструкций и редактирования существующих изображений. Она использует мультимодальный энкодер для преобразования свободных инструкций в единое визуально-языковое пространство. Совместное обучение обеих задач улучшает соответствие инструкциям и визуальную согласованность. MIGE демонстрирует превосходные результаты в генерации и редактировании изображений, а также устанавливает новый стандарт в задаче редактирования изображений на основе инструкций с учетом субъекта.'}, 'en': {'title': 'MIGE: Unifying Image Generation and Editing with Multimodal Instructions', 'desc': 'This paper presents MIGE, a unified framework for improving both subject-driven image generation and instruction-based editing in machine learning. It addresses the challenges of limited data and poor generalization by standardizing task representations through multimodal instructions. MIGE employs a novel multimodal encoder that integrates visual and semantic features, allowing for joint training of both tasks. The results demonstrate that MIGE enhances instruction adherence and visual consistency while also enabling generalization to new tasks, achieving state-of-the-art performance in instruction-based subject-driven editing.'}, 'zh': {'title': '统一框架，提升图像生成与编辑的能力', 'desc': '尽管扩散基础的图像生成取得了显著进展，但以主题为驱动的生成和基于指令的编辑仍然面临挑战。现有方法通常将这两者分开处理，受限于高质量数据的不足和泛化能力差。我们提出了MIGE，一个统一框架，通过多模态指令标准化任务表示，将主题驱动生成视为在空白画布上的创作，而将基于指令的编辑视为对现有图像的修改。MIGE引入了一种新颖的多模态编码器，将自由形式的多模态指令映射到统一的视觉-语言空间，从而实现了跨任务的增强和更好的泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2502.17125', 'title': 'LettuceDetect: A Hallucination Detection Framework for RAG Applications', 'url': 'https://huggingface.co/papers/2502.17125', 'abstract': "Retrieval Augmented Generation (RAG) systems remain vulnerable to hallucinated answers despite incorporating external knowledge sources. We present LettuceDetect a framework that addresses two critical limitations in existing hallucination detection methods: (1) the context window constraints of traditional encoder-based methods, and (2) the computational inefficiency of LLM based approaches. Building on ModernBERT's extended context capabilities (up to 8k tokens) and trained on the RAGTruth benchmark dataset, our approach outperforms all previous encoder-based models and most prompt-based models, while being approximately 30 times smaller than the best models. LettuceDetect is a token-classification model that processes context-question-answer triples, allowing for the identification of unsupported claims at the token level. Evaluations on the RAGTruth corpus demonstrate an F1 score of 79.22% for example-level detection, which is a 14.8% improvement over Luna, the previous state-of-the-art encoder-based architecture. Additionally, the system can process 30 to 60 examples per second on a single GPU, making it more practical for real-world RAG applications.", 'score': 3, 'issue_id': 2497, 'pub_date': '2025-02-24', 'pub_date_card': {'ru': '24 февраля', 'en': 'February 24', 'zh': '2月24日'}, 'hash': '347ebb871884d940', 'authors': ['Ádám Kovács', 'Gábor Recski'], 'affiliations': ['KR Labs', 'TU Wien'], 'pdf_title_img': 'assets/pdf/title_img/2502.17125.jpg', 'data': {'categories': ['#benchmark', '#rag', '#hallucinations', '#architecture', '#small_models', '#long_context'], 'emoji': '🥬', 'ru': {'title': 'LettuceDetect: Эффективное обнаружение галлюцинаций в RAG-системах', 'desc': 'LettuceDetect - это новая система для обнаружения галлюцинаций в генеративных моделях с извлечением информации (RAG). Она использует архитектуру ModernBERT с расширенным контекстным окном до 8000 токенов и обучена на датасете RAGTruth. LettuceDetect превосходит предыдущие модели на основе энкодеров и большинство моделей на основе промптов, будучи при этом в 30 раз меньше. Система демонстрирует F1-score 79.22% для обнаружения на уровне примеров, что на 14.8% лучше предыдущего SOTA-решения Luna.'}, 'en': {'title': 'LettuceDetect: Efficient Hallucination Detection for RAG Systems', 'desc': "This paper introduces LettuceDetect, a novel framework designed to improve the detection of hallucinated answers in Retrieval Augmented Generation (RAG) systems. It addresses limitations of existing methods by utilizing ModernBERT's extended context capabilities and a token-classification approach to analyze context-question-answer triples. LettuceDetect achieves a significant F1 score of 79.22% on the RAGTruth dataset, outperforming previous models while being much smaller and more efficient. The system's ability to process 30 to 60 examples per second on a single GPU enhances its practicality for real-world applications."}, 'zh': {'title': '提升幻觉检测的效率与准确性', 'desc': '本论文介绍了一种名为LettuceDetect的框架，旨在解决现有幻觉检测方法的两个主要限制。首先，它克服了传统编码器方法的上下文窗口限制，其次，它提高了基于大型语言模型（LLM）方法的计算效率。LettuceDetect基于ModernBERT，能够处理多达8000个标记，并在RAGTruth基准数据集上进行训练，表现优于所有先前的编码器模型和大多数基于提示的模型。该系统在RAGTruth语料库上的F1得分达到79.22%，并且在单个GPU上每秒可处理30到60个示例，适用于实际的RAG应用。'}}}, {'id': 'https://huggingface.co/papers/2502.20490', 'title': 'EgoNormia: Benchmarking Physical Social Norm Understanding', 'url': 'https://huggingface.co/papers/2502.20490', 'abstract': 'Human activity is moderated by norms. When performing actions in the real world, humans not only follow norms, but also consider the trade-off between different norms However, machines are often trained without explicit supervision on norm understanding and reasoning, especially when the norms are grounded in a physical and social context. To improve and evaluate the normative reasoning capability of vision-language models (VLMs), we present EgoNormia |epsilon|, consisting of 1,853 ego-centric videos of human interactions, each of which has two related questions evaluating both the prediction and justification of normative actions. The normative actions encompass seven categories: safety, privacy, proxemics, politeness, cooperation, coordination/proactivity, and communication/legibility. To compile this dataset at scale, we propose a novel pipeline leveraging video sampling, automatic answer generation, filtering, and human validation. Our work demonstrates that current state-of-the-art vision-language models lack robust norm understanding, scoring a maximum of 45% on EgoNormia (versus a human bench of 92%). Our analysis of performance in each dimension highlights the significant risks of safety, privacy, and the lack of collaboration and communication capability when applied to real-world agents. We additionally show that through a retrieval-based generation method, it is possible to use EgoNomia to enhance normative reasoning in VLMs.', 'score': 2, 'issue_id': 2499, 'pub_date': '2025-02-27', 'pub_date_card': {'ru': '27 февраля', 'en': 'February 27', 'zh': '2月27日'}, 'hash': 'a2c7525ed78fb0ce', 'authors': ['MohammadHossein Rezaei', 'Yicheng Fu', 'Phil Cuvin', 'Caleb Ziems', 'Yanzhe Zhang', 'Hao Zhu', 'Diyi Yang'], 'affiliations': ['Georgia Tech', 'Stanford University', 'University of Arizona', 'University of Toronto'], 'pdf_title_img': 'assets/pdf/title_img/2502.20490.jpg', 'data': {'categories': ['#reasoning', '#multimodal', '#data', '#dataset', '#benchmark', '#ethics'], 'emoji': '🤖', 'ru': {'title': 'EgoNormia: повышение этического интеллекта ИИ через эгоцентрические видео', 'desc': 'Статья представляет EgoNormia |epsilon| - набор данных из 1853 эгоцентрических видео человеческих взаимодействий для оценки нормативного рассуждения моделей компьютерного зрения и обработки естественного языка (VLM). Каждое видео сопровождается двумя вопросами, оценивающими предсказание и обоснование нормативных действий в семи категориях, включая безопасность, конфиденциальность и сотрудничество. Исследование показывает, что современные VLM недостаточно понимают нормы, набирая максимум 45% на EgoNormia по сравнению с 92% у людей. Авторы предлагают метод генерации на основе поиска для улучшения нормативного рассуждения в VLM с помощью EgoNomia.'}, 'en': {'title': 'Enhancing Norm Understanding in Machines with EgoNormia', 'desc': "This paper introduces EgoNormia, a dataset designed to enhance the normative reasoning abilities of vision-language models (VLMs) by providing 1,853 ego-centric videos that depict human interactions. Each video is accompanied by questions that assess the model's ability to predict and justify normative actions across seven categories, including safety and privacy. The authors highlight that current VLMs perform poorly on this task, achieving only 45% accuracy compared to a human benchmark of 92%. They also propose a novel pipeline for dataset creation and demonstrate that using EgoNormia can improve the normative reasoning capabilities of VLMs through a retrieval-based generation method."}, 'zh': {'title': '提升机器的规范推理能力', 'desc': '本论文探讨了人类在活动中如何遵循社会规范，并提出了一个名为EgoNormia的数据集，以评估视觉-语言模型（VLMs）在规范推理方面的能力。该数据集包含1853个以自我为中心的人类互动视频，并通过相关问题来评估模型对规范行为的预测和解释。研究发现，当前的最先进VLM在规范理解方面表现不佳，最高得分仅为45%，远低于人类的92%。此外，论文还提出了一种基于检索的生成方法，能够利用EgoNormia提升VLM的规范推理能力。'}}}, {'id': 'https://huggingface.co/papers/2502.20811', 'title': 'HAIC: Improving Human Action Understanding and Generation with Better Captions for Multi-modal Large Language Models', 'url': 'https://huggingface.co/papers/2502.20811', 'abstract': 'Recent Multi-modal Large Language Models (MLLMs) have made great progress in video understanding. However, their performance on videos involving human actions is still limited by the lack of high-quality data. To address this, we introduce a two-stage data annotation pipeline. First, we design strategies to accumulate videos featuring clear human actions from the Internet. Second, videos are annotated in a standardized caption format that uses human attributes to distinguish individuals and chronologically details their actions and interactions. Through this pipeline, we curate two datasets, namely HAICTrain and HAICBench. HAICTrain comprises 126K video-caption pairs generated by Gemini-Pro and verified for training purposes. Meanwhile, HAICBench includes 500 manually annotated video-caption pairs and 1,400 QA pairs, for a comprehensive evaluation of human action understanding. Experimental results demonstrate that training with HAICTrain not only significantly enhances human understanding abilities across 4 benchmarks, but can also improve text-to-video generation results. Both the HAICTrain and HAICBench are released at https://huggingface.co/datasets/KuaishouHAIC/HAIC.', 'score': 1, 'issue_id': 2486, 'pub_date': '2025-02-28', 'pub_date_card': {'ru': '28 февраля', 'en': 'February 28', 'zh': '2月28日'}, 'hash': '806f6aacd5ee2f8a', 'authors': ['Xiao Wang', 'Jingyun Hua', 'Weihong Lin', 'Yuanxing Zhang', 'Fuzheng Zhang', 'Jianlong Wu', 'Di Zhang', 'Liqiang Nie'], 'affiliations': ['Kuaishou Technology'], 'pdf_title_img': 'assets/pdf/title_img/2502.20811.jpg', 'data': {'categories': ['#dataset', '#data', '#video', '#multimodal', '#benchmark', '#synthetic'], 'emoji': '🎬', 'ru': {'title': 'Улучшение понимания человеческих действий в видео с помощью аннотированных данных', 'desc': 'Статья представляет новый подход к улучшению понимания видео с человеческими действиями для мультимодальных больших языковых моделей (MLLM). Авторы разработали двухэтапный процесс аннотации данных, включающий сбор релевантных видео и их стандартизированное описание. В результате были созданы два набора данных: HAICTrain для обучения и HAICBench для оценки моделей. Эксперименты показали, что обучение на HAICTrain значительно улучшает способности моделей понимать человеческие действия на видео.'}, 'en': {'title': 'Enhancing Video Understanding with Curated Human Action Datasets', 'desc': 'This paper presents a solution to improve video understanding in Multi-modal Large Language Models (MLLMs) by addressing the scarcity of high-quality data on human actions. The authors introduce a two-stage data annotation pipeline that first collects videos with clear human actions from the Internet and then annotates them using a standardized caption format. This results in two curated datasets: HAICTrain, which contains 126K video-caption pairs for training, and HAICBench, which includes 500 annotated pairs for evaluation. Experimental results show that using HAICTrain significantly boosts human action understanding and enhances text-to-video generation capabilities across multiple benchmarks.'}, 'zh': {'title': '提升视频理解的创新数据标注流程', 'desc': '最近的多模态大型语言模型（MLLMs）在视频理解方面取得了显著进展。然而，它们在涉及人类动作的视频上的表现仍然受到高质量数据缺乏的限制。为了解决这个问题，我们提出了一个两阶段的数据标注流程，首先从互联网收集包含清晰人类动作的视频，然后使用标准化的字幕格式对视频进行标注。通过这个流程，我们创建了两个数据集HAICTrain和HAICBench，实验结果表明，使用HAICTrain进行训练显著提升了人类动作理解能力，并改善了文本到视频生成的效果。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (4)', '#agents (12)', '#agi (5)', '#alignment (10)', '#architecture (16)', '#audio (5)', '#benchmark (35)', '#cv (13)', '#data (24)', '#dataset (38)', '#diffusion (10)', '#ethics (5)', '#games (13)', '#graphs (2)', '#hallucinations (7)', '#healthcare (5)', '#inference (18)', '#interpretability (4)', '#leakage (1)', '#long_context (12)', '#low_resource (2)', '#machine_translation (3)', '#math (5)', '#multilingual (5)', '#multimodal (27)', '#open_source (29)', '#optimization (51)', '#plp (1)', '#rag (7)', '#reasoning (23)', '#rl (14)', '#rlhf (9)', '#robotics (7)', '#science (5)', '#security (3)', '#small_models (7)', '#story_generation', '#survey', '#synthetic (10)', '#training (49)', '#transfer_learning (5)', '#video (8)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-03-07 11:10',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-03-07 11:10')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-03-07 11:10')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('monthly'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    