
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 348 papers. March 2025.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #7a30efcf;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: #7a30efcf;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #7a30ef17;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf monthly</h1></a>
            <p><span id="title-date">Март 2025</span> | <span id="title-articles-count">348 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/m/2025-02.html">⬅️ <span id="prev-date">02.2025</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/m/2025-04.html">➡️ <span id="next-date">04.2025</span></a></span>
            <span class="nav-item" id="nav-daily"><a href="https://hfday.ru">📈 <span id='top-day-label'>День</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': 'Март 2025', 'en': 'March 2025', 'zh': '3月2025年'};
        let feedDateNext = {'ru': '04.2025', 'en': '04/2025', 'zh': '4月2025年'};
        let feedDatePrev = {'ru': '02.2025', 'en': '02/2025', 'zh': '2月2025年'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf monthly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2503.01785', 'title': 'Visual-RFT: Visual Reinforcement Fine-Tuning', 'url': 'https://huggingface.co/papers/2503.01785', 'abstract': "Reinforcement Fine-Tuning (RFT) in Large Reasoning Models like OpenAI o1 learns from feedback on its answers, which is especially useful in applications when fine-tuning data is scarce. Recent open-source work like DeepSeek-R1 demonstrates that reinforcement learning with verifiable reward is one key direction in reproducing o1. While the R1-style model has demonstrated success in language models, its application in multi-modal domains remains under-explored. This work introduces Visual Reinforcement Fine-Tuning (Visual-RFT), which further extends the application areas of RFT on visual tasks. Specifically, Visual-RFT first uses Large Vision-Language Models (LVLMs) to generate multiple responses containing reasoning tokens and final answers for each input, and then uses our proposed visual perception verifiable reward functions to update the model via the policy optimization algorithm such as Group Relative Policy Optimization (GRPO). We design different verifiable reward functions for different perception tasks, such as the Intersection over Union (IoU) reward for object detection. Experimental results on fine-grained image classification, few-shot object detection, reasoning grounding, as well as open-vocabulary object detection benchmarks show the competitive performance and advanced generalization ability of Visual-RFT compared with Supervised Fine-tuning (SFT). For example, Visual-RFT improves accuracy by 24.3% over the baseline in one-shot fine-grained image classification with around 100 samples. In few-shot object detection, Visual-RFT also exceeds the baseline by 21.9 on COCO's two-shot setting and 15.4 on LVIS. Our Visual-RFT represents a paradigm shift in fine-tuning LVLMs, offering a data-efficient, reward-driven approach that enhances reasoning and adaptability for domain-specific tasks.", 'score': 43, 'issue_id': 2511, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': 'ef2e10eb59ab7743', 'authors': ['Ziyu Liu', 'Zeyi Sun', 'Yuhang Zang', 'Xiaoyi Dong', 'Yuhang Cao', 'Haodong Duan', 'Dahua Lin', 'Jiaqi Wang'], 'affiliations': ['Shanghai Artificial Intelligence Laboratory', 'Shanghai Jiaotong University', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2503.01785.jpg', 'data': {'categories': ['#multimodal', '#open_source', '#cv', '#optimization', '#rlhf', '#reasoning', '#training', '#rl'], 'emoji': '🔬', 'ru': {'title': 'Visual-RFT: Революция в тонкой настройке визуально-языковых моделей', 'desc': 'Статья представляет Visual Reinforcement Fine-Tuning (Visual-RFT) - метод, расширяющий применение обучения с подкреплением в визуальных задачах. Visual-RFT использует большие визуально-языковые модели для генерации ответов с токенами рассуждений и применяет визуально верифицируемые функции вознаграждения для обновления модели. Эксперименты показывают превосходство Visual-RFT над методом Supervised Fine-tuning в задачах классификации изображений, обнаружения объектов и обоснованного заземления. Метод демонстрирует значительное улучшение точности и обобщающей способности при ограниченном количестве обучающих примеров.'}, 'en': {'title': 'Revolutionizing Visual Learning with Reinforcement Fine-Tuning', 'desc': "This paper introduces Visual Reinforcement Fine-Tuning (Visual-RFT), a method that enhances large vision-language models (LVLMs) by using reinforcement learning to improve their performance on visual tasks. Visual-RFT generates multiple responses for each input and employs verifiable reward functions to optimize the model's policy, making it particularly effective in scenarios with limited fine-tuning data. The approach demonstrates significant improvements in tasks like fine-grained image classification and object detection, outperforming traditional supervised fine-tuning methods. Overall, Visual-RFT represents a novel, efficient way to fine-tune LVLMs, focusing on reasoning and adaptability in specific domains."}, 'zh': {'title': '视觉强化微调：提升推理与适应性的创新方法', 'desc': '强化微调（RFT）在大型推理模型中通过反馈学习，特别适用于微调数据稀缺的应用场景。本文提出的视觉强化微调（Visual-RFT）扩展了RFT在视觉任务中的应用，利用大型视觉语言模型生成多种响应，并通过可验证的视觉感知奖励函数进行模型更新。实验结果表明，Visual-RFT在细粒度图像分类和少样本目标检测等任务中表现出色，相较于传统的监督微调（SFT）方法，准确率显著提高。Visual-RFT代表了一种新的微调范式，提供了一种数据高效、以奖励驱动的方法，增强了模型在特定领域任务中的推理能力和适应性。'}}}, {'id': 'https://huggingface.co/papers/2503.01743', 'title': 'Phi-4-Mini Technical Report: Compact yet Powerful Multimodal Language Models via Mixture-of-LoRAs', 'url': 'https://huggingface.co/papers/2503.01743', 'abstract': 'We introduce Phi-4-Mini and Phi-4-Multimodal, compact yet highly capable language and multimodal models. Phi-4-Mini is a 3.8-billion-parameter language model trained on high-quality web and synthetic data, significantly outperforming recent open-source models of similar size and matching the performance of models twice its size on math and coding tasks requiring complex reasoning. This achievement is driven by a carefully curated synthetic data recipe emphasizing high-quality math and coding datasets. Compared to its predecessor, Phi-3.5-Mini, Phi-4-Mini features an expanded vocabulary size of 200K tokens to better support multilingual applications, as well as group query attention for more efficient long-sequence generation. Phi-4-Multimodal is a multimodal model that integrates text, vision, and speech/audio input modalities into a single model. Its novel modality extension approach leverages LoRA adapters and modality-specific routers to allow multiple inference modes combining various modalities without interference. For example, it now ranks first in the OpenASR leaderboard to date, although the LoRA component of the speech/audio modality has just 460 million parameters. Phi-4-Multimodal supports scenarios involving (vision + language), (vision + speech), and (speech/audio) inputs, outperforming larger vision-language and speech-language models on a wide range of tasks. Additionally, we experiment to further train Phi-4-Mini to enhance its reasoning capabilities. Despite its compact 3.8-billion-parameter size, this experimental version achieves reasoning performance on par with or surpassing significantly larger models, including DeepSeek-R1-Distill-Qwen-7B and DeepSeek-R1-Distill-Llama-8B.', 'score': 38, 'issue_id': 2511, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': 'fb054d6547a4a4fb', 'authors': ['Abdelrahman Abouelenin', 'Atabak Ashfaq', 'Adam Atkinson', 'Hany Awadalla', 'Nguyen Bach', 'Jianmin Bao', 'Alon Benhaim', 'Martin Cai', 'Vishrav Chaudhary', 'Congcong Chen', 'Dong Chen', 'Dongdong Chen', 'Junkun Chen', 'Weizhu Chen', 'Yen-Chun Chen', 'Yi-ling Chen', 'Qi Dai', 'Xiyang Dai', 'Ruchao Fan', 'Mei Gao', 'Min Gao', 'Amit Garg', 'Abhishek Goswami', 'Junheng Hao', 'Amr Hendy', 'Yuxuan Hu', 'Xin Jin', 'Mahmoud Khademi', 'Dongwoo Kim', 'Young Jin Kim', 'Gina Lee', 'Jinyu Li', 'Yunsheng Li', 'Chen Liang', 'Xihui Lin', 'Zeqi Lin', 'Mengchen Liu', 'Yang Liu', 'Gilsinia Lopez', 'Chong Luo', 'Piyush Madan', 'Vadim Mazalov', 'Ali Mousavi', 'Anh Nguyen', 'Jing Pan', 'Daniel Perez-Becker', 'Jacob Platin', 'Thomas Portet', 'Kai Qiu', 'Bo Ren', 'Liliang Ren', 'Sambuddha Roy', 'Ning Shang', 'Yelong Shen', 'Saksham Singhal', 'Subhojit Som', 'Xia Song', 'Tetyana Sych', 'Praneetha Vaddamanu', 'Shuohang Wang', 'Yiming Wang', 'Zhenghao Wang', 'Haibin Wu', 'Haoran Xu', 'Weijian Xu', 'Yifan Yang', 'Ziyi Yang', 'Donghan Yu', 'Ishmam Zabir', 'Jianwen Zhang', 'Li Lyna Zhang', 'Yunan Zhang', 'Xiren Zhou'], 'affiliations': ['Microsoft'], 'pdf_title_img': 'assets/pdf/title_img/2503.01743.jpg', 'data': {'categories': ['#multimodal', '#small_models', '#data', '#agi', '#synthetic', '#long_context', '#optimization', '#dataset', '#training'], 'emoji': '🧠', 'ru': {'title': 'Компактные модели с большими возможностями: прорыв в эффективности ИИ', 'desc': 'Представлены две новые модели: Phi-4-Mini и Phi-4-Multimodal. Phi-4-Mini - это языковая модель с 3,8 миллиардами параметров, обученная на высококачественных веб-данных и синтетических данных, которая превосходит аналогичные модели в задачах математики и программирования. Phi-4-Multimodal - это мультимодальная модель, объединяющая текст, изображения и речь/аудио в единую систему с использованием LoRA-адаптеров. Обе модели демонстрируют высокую эффективность несмотря на свой компактный размер, превосходя более крупные аналоги в различных задачах.'}, 'en': {'title': 'Compact Models, Superior Performance!', 'desc': 'The paper presents Phi-4-Mini and Phi-4-Multimodal, two advanced models designed for language and multimodal tasks. Phi-4-Mini, with 3.8 billion parameters, excels in math and coding tasks by utilizing a high-quality synthetic data approach and an expanded vocabulary of 200K tokens. Phi-4-Multimodal integrates text, vision, and audio inputs, employing innovative techniques like LoRA adapters for efficient multi-modal processing. Both models demonstrate superior performance compared to larger counterparts, showcasing their effectiveness in complex reasoning and diverse input scenarios.'}, 'zh': {'title': '紧凑强大的多模态模型Phi-4系列', 'desc': '我们介绍了Phi-4-Mini和Phi-4-Multimodal这两种紧凑而强大的语言和多模态模型。Phi-4-Mini是一个拥有38亿参数的语言模型，经过高质量的网络和合成数据训练，在数学和编码任务中表现优于同类开源模型，并且在复杂推理方面与两倍于其规模的模型相当。相比于前身Phi-3.5-Mini，Phi-4-Mini扩展了词汇量，支持多语言应用，并采用了组查询注意力机制以提高长序列生成的效率。Phi-4-Multimodal则是一个多模态模型，能够将文本、视觉和语音/音频输入整合到一个模型中，支持多种推理模式，且在多个任务上超越了更大的视觉-语言和语音-语言模型。'}}}, {'id': 'https://huggingface.co/papers/2503.01774', 'title': 'Difix3D+: Improving 3D Reconstructions with Single-Step Diffusion Models', 'url': 'https://huggingface.co/papers/2503.01774', 'abstract': 'Neural Radiance Fields and 3D Gaussian Splatting have revolutionized 3D reconstruction and novel-view synthesis task. However, achieving photorealistic rendering from extreme novel viewpoints remains challenging, as artifacts persist across representations. In this work, we introduce Difix3D+, a novel pipeline designed to enhance 3D reconstruction and novel-view synthesis through single-step diffusion models. At the core of our approach is Difix, a single-step image diffusion model trained to enhance and remove artifacts in rendered novel views caused by underconstrained regions of the 3D representation. Difix serves two critical roles in our pipeline. First, it is used during the reconstruction phase to clean up pseudo-training views that are rendered from the reconstruction and then distilled back into 3D. This greatly enhances underconstrained regions and improves the overall 3D representation quality. More importantly, Difix also acts as a neural enhancer during inference, effectively removing residual artifacts arising from imperfect 3D supervision and the limited capacity of current reconstruction models. Difix3D+ is a general solution, a single model compatible with both NeRF and 3DGS representations, and it achieves an average 2times improvement in FID score over baselines while maintaining 3D consistency.', 'score': 29, 'issue_id': 2512, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': '39af2f882aef9afb', 'authors': ['Jay Zhangjie Wu', 'Yuxuan Zhang', 'Haithem Turki', 'Xuanchi Ren', 'Jun Gao', 'Mike Zheng Shou', 'Sanja Fidler', 'Zan Gojcic', 'Huan Ling'], 'affiliations': ['NVIDIA', 'National University of Singapore', 'University of Toronto', 'Vector Institute'], 'pdf_title_img': 'assets/pdf/title_img/2503.01774.jpg', 'data': {'categories': ['#3d', '#diffusion'], 'emoji': '🖼️', 'ru': {'title': 'Одношаговая диффузия для фотореалистичной 3D-реконструкции', 'desc': 'Difix3D+ - это новый подход к улучшению 3D-реконструкции и синтеза изображений с новых ракурсов. В его основе лежит Difix - одношаговая модель диффузии изображений, обученная улучшать и устранять артефакты в визуализированных видах. Difix используется как на этапе реконструкции для очистки псевдо-обучающих видов, так и во время вывода для устранения остаточных артефактов. Difix3D+ совместим с представлениями NeRF и 3DGS и показывает двукратное улучшение оценки FID по сравнению с базовыми моделями.'}, 'en': {'title': 'Enhancing 3D Reconstruction with Difix3D+', 'desc': 'This paper presents Difix3D+, a new method for improving 3D reconstruction and novel-view synthesis using single-step diffusion models. The core component, Difix, is an image diffusion model that enhances rendered views by removing artifacts caused by underconstrained areas in 3D representations. It plays a dual role by cleaning up pseudo-training views during reconstruction and acting as a neural enhancer during inference to eliminate residual artifacts. Difix3D+ is versatile, working with both Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS), and it significantly improves the quality of 3D representations, achieving a 2x better FID score compared to existing methods.'}, 'zh': {'title': 'Difix3D+: 提升3D重建与新视角合成的利器', 'desc': 'Neural Radiance Fields（NeRF）和3D高斯点云（3D Gaussian Splatting）在3D重建和新视角合成任务中取得了重大进展。然而，从极端新视角实现真实感渲染仍然面临挑战，因为在表示中存在伪影。我们提出了Difix3D+，这是一种新颖的管道，旨在通过单步扩散模型增强3D重建和新视角合成。Difix作为核心模型，能够在重建阶段清理伪训练视图，并在推理阶段去除残留伪影，从而显著提高3D表示的质量。'}}}, {'id': 'https://huggingface.co/papers/2503.01183', 'title': 'DiffRhythm: Blazingly Fast and Embarrassingly Simple End-to-End Full-Length Song Generation with Latent Diffusion', 'url': 'https://huggingface.co/papers/2503.01183', 'abstract': 'Recent advancements in music generation have garnered significant attention, yet existing approaches face critical limitations. Some current generative models can only synthesize either the vocal track or the accompaniment track. While some models can generate combined vocal and accompaniment, they typically rely on meticulously designed multi-stage cascading architectures and intricate data pipelines, hindering scalability. Additionally, most systems are restricted to generating short musical segments rather than full-length songs. Furthermore, widely used language model-based methods suffer from slow inference speeds. To address these challenges, we propose DiffRhythm, the first latent diffusion-based song generation model capable of synthesizing complete songs with both vocal and accompaniment for durations of up to 4m45s in only ten seconds, maintaining high musicality and intelligibility. Despite its remarkable capabilities, DiffRhythm is designed to be simple and elegant: it eliminates the need for complex data preparation, employs a straightforward model structure, and requires only lyrics and a style prompt during inference. Additionally, its non-autoregressive structure ensures fast inference speeds. This simplicity guarantees the scalability of DiffRhythm. Moreover, we release the complete training code along with the pre-trained model on large-scale data to promote reproducibility and further research.', 'score': 18, 'issue_id': 2516, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': '0370c6364610fd8e', 'authors': ['Ziqian Ning', 'Huakang Chen', 'Yuepeng Jiang', 'Chunbo Hao', 'Guobin Ma', 'Shuai Wang', 'Jixun Yao', 'Lei Xie'], 'affiliations': ['Northwestern Polytechnical University', 'Shenzhen Research Institute of Big Data, The Chinese University of Hong Kong, Shenzhen (CUHK-Shenzhen), China'], 'pdf_title_img': 'assets/pdf/title_img/2503.01183.jpg', 'data': {'categories': ['#diffusion', '#inference', '#dataset', '#open_source', '#audio'], 'emoji': '🎵', 'ru': {'title': 'DiffRhythm: Быстрая генерация полных песен с помощью латентной диффузии', 'desc': 'DiffRhythm - это первая модель генерации песен на основе латентной диффузии, способная синтезировать полные песни с вокалом и аккомпанементом длительностью до 4м45с всего за десять секунд. Модель имеет простую структуру, не требует сложной подготовки данных и использует только текст песни и стилевую подсказку при инференсе. Благодаря неавторегрессивной структуре, DiffRhythm обеспечивает высокую скорость генерации. Авторы опубликовали полный код обучения и предобученную модель для воспроизводимости результатов и дальнейших исследований.'}, 'en': {'title': 'DiffRhythm: Fast and Scalable Song Generation with Latent Diffusion', 'desc': "This paper introduces DiffRhythm, a novel music generation model that utilizes latent diffusion techniques to create full-length songs with both vocal and accompaniment tracks. Unlike existing models that are limited to short segments or require complex architectures, DiffRhythm simplifies the process by needing only lyrics and a style prompt for song generation. It achieves high musical quality and intelligibility while significantly improving inference speed, generating songs in just ten seconds. The authors also emphasize the model's scalability and reproducibility by providing the complete training code and pre-trained model for further research."}, 'zh': {'title': 'DiffRhythm：快速生成完整歌曲的创新模型', 'desc': '本论文介绍了一种新的音乐生成模型DiffRhythm，它能够在短短十秒内合成完整的歌曲，包括人声和伴奏，时长可达4分45秒。与现有模型相比，DiffRhythm采用潜在扩散技术，避免了复杂的数据准备和多阶段架构，确保了高效的推理速度。该模型只需歌词和风格提示即可生成音乐，具有良好的可扩展性。我们还发布了完整的训练代码和预训练模型，以促进研究的可重复性和进一步发展。'}}}, {'id': 'https://huggingface.co/papers/2502.18965', 'title': 'OneRec: Unifying Retrieve and Rank with Generative Recommender and Iterative Preference Alignment', 'url': 'https://huggingface.co/papers/2502.18965', 'abstract': "Recently, generative retrieval-based recommendation systems have emerged as a promising paradigm. However, most modern recommender systems adopt a retrieve-and-rank strategy, where the generative model functions only as a selector during the retrieval stage. In this paper, we propose OneRec, which replaces the cascaded learning framework with a unified generative model. To the best of our knowledge, this is the first end-to-end generative model that significantly surpasses current complex and well-designed recommender systems in real-world scenarios. Specifically, OneRec includes: 1) an encoder-decoder structure, which encodes the user's historical behavior sequences and gradually decodes the videos that the user may be interested in. We adopt sparse Mixture-of-Experts (MoE) to scale model capacity without proportionally increasing computational FLOPs. 2) a session-wise generation approach. In contrast to traditional next-item prediction, we propose a session-wise generation, which is more elegant and contextually coherent than point-by-point generation that relies on hand-crafted rules to properly combine the generated results. 3) an Iterative Preference Alignment module combined with Direct Preference Optimization (DPO) to enhance the quality of the generated results. Unlike DPO in NLP, a recommendation system typically has only one opportunity to display results for each user's browsing request, making it impossible to obtain positive and negative samples simultaneously. To address this limitation, We design a reward model to simulate user generation and customize the sampling strategy. Extensive experiments have demonstrated that a limited number of DPO samples can align user interest preferences and significantly improve the quality of generated results. We deployed OneRec in the main scene of Kuaishou, achieving a 1.6\\% increase in watch-time, which is a substantial improvement.", 'score': 18, 'issue_id': 2515, 'pub_date': '2025-02-26', 'pub_date_card': {'ru': '26 февраля', 'en': 'February 26', 'zh': '2月26日'}, 'hash': '21c5c80a138c98a0', 'authors': ['Jiaxin Deng', 'Shiyao Wang', 'Kuo Cai', 'Lejian Ren', 'Qigen Hu', 'Weifeng Ding', 'Qiang Luo', 'Guorui Zhou'], 'affiliations': ['KuaiShou Inc. Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2502.18965.jpg', 'data': {'categories': ['#alignment', '#rlhf', '#rag', '#games', '#training', '#optimization'], 'emoji': '🎥', 'ru': {'title': 'OneRec: Единая генеративная модель для революции в рекомендательных системах', 'desc': 'OneRec - это новая система рекомендаций, использующая единую генеративную модель вместо каскадного подхода. Она включает в себя структуру кодировщик-декодировщик с разреженной смесью экспертов (MoE) для масштабирования возможностей модели. OneRec применяет поэтапную генерацию сессий и модуль итеративного выравнивания предпочтений с прямой оптимизацией предпочтений (DPO). Система показала значительное улучшение времени просмотра при развертывании на платформе Kuaishou.'}, 'en': {'title': 'OneRec: Revolutionizing Recommendations with Generative Models', 'desc': 'This paper introduces OneRec, a novel generative retrieval-based recommendation system that improves upon traditional retrieve-and-rank methods. Unlike existing systems that use generative models merely for selection, OneRec employs a unified generative model that encodes user behavior and generates personalized video recommendations in a session-wise manner. The model utilizes a sparse Mixture-of-Experts architecture to enhance capacity while maintaining efficiency, and incorporates an Iterative Preference Alignment module to optimize user preferences effectively. Experimental results show that OneRec significantly outperforms existing systems, leading to a notable increase in user engagement metrics such as watch-time.'}, 'zh': {'title': 'OneRec：统一生成模型的推荐新范式', 'desc': '最近，基于生成检索的推荐系统成为一种有前景的范式。本文提出的OneRec模型，采用统一的生成模型，取代了传统的级联学习框架，能够在真实场景中显著超越现有复杂的推荐系统。OneRec包括编码-解码结构，能够有效编码用户的历史行为，并生成用户可能感兴趣的视频。此外，OneRec还引入了会话生成方法和迭代偏好对齐模块，提升了生成结果的质量，并在快手的实际应用中实现了观看时间的显著增加。'}}}, {'id': 'https://huggingface.co/papers/2503.01688', 'title': 'When an LLM is apprehensive about its answers -- and when its uncertainty is justified', 'url': 'https://huggingface.co/papers/2503.01688', 'abstract': 'Uncertainty estimation is crucial for evaluating Large Language Models (LLMs), particularly in high-stakes domains where incorrect answers result in significant consequences. Numerous approaches consider this problem, while focusing on a specific type of uncertainty, ignoring others. We investigate what estimates, specifically token-wise entropy and model-as-judge (MASJ), would work for multiple-choice question-answering tasks for different question topics. Our experiments consider three LLMs: Phi-4, Mistral, and Qwen of different sizes from 1.5B to 72B and 14 topics. While MASJ performs similarly to a random error predictor, the response entropy predicts model error in knowledge-dependent domains and serves as an effective indicator of question difficulty: for biology ROC AUC is 0.73. This correlation vanishes for the reasoning-dependent domain: for math questions ROC-AUC is 0.55. More principally, we found out that the entropy measure required a reasoning amount. Thus, data-uncertainty related entropy should be integrated within uncertainty estimates frameworks, while MASJ requires refinement. Moreover, existing MMLU-Pro samples are biased, and should balance required amount of reasoning for different subdomains to provide a more fair assessment of LLMs performance.', 'score': 16, 'issue_id': 2518, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': '68429d7977c57eae', 'authors': ['Petr Sychev', 'Andrey Goncharov', 'Daniil Vyazhev', 'Edvard Khalafyan', 'Alexey Zaytsev'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2503.01688.jpg', 'data': {'categories': ['#ethics', '#hallucinations', '#benchmark', '#reasoning', '#data', '#multilingual'], 'emoji': '🤖', 'ru': {'title': 'Энтропия ответов как индикатор неопределенности LLM в задачах с множественным выбором', 'desc': "Исследование посвящено оценке неопределенности в крупных языковых моделях (LLM) при решении задач с множественным выбором. Авторы сравнивают эффективность энтропии токенов и метода 'модель как судья' (MASJ) для различных тем вопросов. Эксперименты проводились на трех LLM разных размеров и 14 темах. Результаты показывают, что энтропия ответов хорошо предсказывает ошибки модели в областях, зависящих от знаний, но не в областях, требующих рассуждений."}, 'en': {'title': 'Enhancing Uncertainty Estimation in LLMs for Better Decision-Making', 'desc': 'This paper explores how to measure uncertainty in Large Language Models (LLMs) when answering multiple-choice questions, which is important in critical areas where wrong answers can have serious effects. It compares two methods of uncertainty estimation: token-wise entropy and model-as-judge (MASJ), across various LLMs and topics. The findings reveal that while MASJ does not effectively predict errors, token-wise entropy is a better indicator of question difficulty, especially in knowledge-based subjects like biology. The study also highlights the need to refine MASJ and address biases in existing datasets to ensure fair evaluation of LLM performance across different reasoning requirements.'}, 'zh': {'title': '提升大型语言模型的不确定性估计', 'desc': '不确定性估计对于评估大型语言模型（LLMs）至关重要，尤其是在错误答案可能导致重大后果的高风险领域。本文探讨了不同类型的不确定性估计，特别是基于令牌的熵和模型作为评判者（MASJ），在多选题回答任务中的有效性。实验涉及三种不同规模的LLMs，结果显示，响应熵在知识依赖领域能够有效预测模型错误，而MASJ的表现类似于随机错误预测器。我们发现熵度量需要一定的推理量，因此数据不确定性相关的熵应纳入不确定性估计框架中，而MASJ则需要进一步改进。'}}}, {'id': 'https://huggingface.co/papers/2503.01496', 'title': 'Liger: Linearizing Large Language Models to Gated Recurrent Structures', 'url': 'https://huggingface.co/papers/2503.01496', 'abstract': 'Transformers with linear recurrent modeling offer linear-time training and constant-memory inference. Despite their demonstrated efficiency and performance, pretraining such non-standard architectures from scratch remains costly and risky. The linearization of large language models (LLMs) transforms pretrained standard models into linear recurrent structures, enabling more efficient deployment. However, current linearization methods typically introduce additional feature map modules that require extensive fine-tuning and overlook the gating mechanisms used in state-of-the-art linear recurrent models. To address these issues, this paper presents Liger, short for Linearizing LLMs to gated recurrent structures. Liger is a novel approach for converting pretrained LLMs into gated linear recurrent models without adding extra parameters. It repurposes the pretrained key matrix weights to construct diverse gating mechanisms, facilitating the formation of various gated recurrent structures while avoiding the need to train additional components from scratch. Using lightweight fine-tuning with Low-Rank Adaptation (LoRA), Liger restores the performance of the linearized gated recurrent models to match that of the original LLMs. Additionally, we introduce Liger Attention, an intra-layer hybrid attention mechanism, which significantly recovers 93\\% of the Transformer-based LLM at 0.02\\% pre-training tokens during the linearization process, achieving competitive results across multiple benchmarks, as validated on models ranging from 1B to 8B parameters. Code is available at https://github.com/OpenSparseLLMs/Linearization.', 'score': 13, 'issue_id': 2514, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': 'd5ca7ef45c0e90c9', 'authors': ['Disen Lan', 'Weigao Sun', 'Jiaxi Hu', 'Jusen Du', 'Yu Cheng'], 'affiliations': ['Nanjing University', 'Shanghai AI Laboratory', 'South China University of Technology', 'The Chinese University of Hong Kong', 'The Hong Kong University of Science and Technology (Guangzhou)'], 'pdf_title_img': 'assets/pdf/title_img/2503.01496.jpg', 'data': {'categories': ['#architecture', '#training', '#optimization', '#benchmark'], 'emoji': '🔢', 'ru': {'title': 'Эффективная линеаризация больших языковых моделей', 'desc': 'Данная статья представляет новый метод Liger для линеаризации больших языковых моделей (LLM) в гейтированные линейно-рекуррентные структуры. Liger преобразует предобученные LLM без добавления дополнительных параметров, используя существующие веса ключевой матрицы для создания различных механизмов гейтирования. Метод применяет легковесную донастройку с помощью Low-Rank Adaptation (LoRA) для восстановления производительности линеаризованных моделей. Авторы также представляют Liger Attention - гибридный механизм внимания, который значительно улучшает эффективность линеаризации.'}, 'en': {'title': 'Liger: Efficiently Transforming LLMs into Gated Linear Recurrent Models', 'desc': "This paper introduces Liger, a method for transforming pretrained large language models (LLMs) into gated linear recurrent models. Liger efficiently repurposes existing key matrix weights to create diverse gating mechanisms without adding extra parameters, thus avoiding the costly process of training new components from scratch. The approach employs lightweight fine-tuning techniques, specifically Low-Rank Adaptation (LoRA), to maintain the performance of the linearized models comparable to the original LLMs. Additionally, Liger incorporates a novel intra-layer hybrid attention mechanism, Liger Attention, which enhances the model's efficiency while achieving competitive results across various benchmarks."}, 'zh': {'title': 'Liger：高效转换预训练模型的创新方法', 'desc': '本文提出了一种名为Liger的方法，用于将预训练的大型语言模型（LLMs）转换为带门控的线性递归模型，而无需增加额外的参数。Liger通过重新利用预训练的关键矩阵权重，构建多样的门控机制，从而形成不同的门控递归结构。该方法使用轻量级的微调技术（如低秩适应LoRA），使线性化的门控递归模型的性能恢复到与原始LLMs相当的水平。此外，Liger Attention作为一种层内混合注意力机制，在线性化过程中显著恢复了93%的Transformer基础LLM的性能。'}}}, {'id': 'https://huggingface.co/papers/2503.01307', 'title': 'Cognitive Behaviors that Enable Self-Improving Reasoners, or, Four Habits of Highly Effective STaRs', 'url': 'https://huggingface.co/papers/2503.01307', 'abstract': "Test-time inference has emerged as a powerful paradigm for enabling language models to ``think'' longer and more carefully about complex challenges, much like skilled human experts. While reinforcement learning (RL) can drive self-improvement in language models on verifiable tasks, some models exhibit substantial gains while others quickly plateau. For instance, we find that Qwen-2.5-3B far exceeds Llama-3.2-3B under identical RL training for the game of Countdown. This discrepancy raises a critical question: what intrinsic properties enable effective self-improvement? We introduce a framework to investigate this question by analyzing four key cognitive behaviors -- verification, backtracking, subgoal setting, and backward chaining -- that both expert human problem solvers and successful language models employ. Our study reveals that Qwen naturally exhibits these reasoning behaviors, whereas Llama initially lacks them. In systematic experimentation with controlled behavioral datasets, we find that priming Llama with examples containing these reasoning behaviors enables substantial improvements during RL, matching or exceeding Qwen's performance. Importantly, the presence of reasoning behaviors, rather than correctness of answers, proves to be the critical factor -- models primed with incorrect solutions containing proper reasoning patterns achieve comparable performance to those trained on correct solutions. Finally, leveraging continued pretraining with OpenWebMath data, filtered to amplify reasoning behaviors, enables the Llama model to match Qwen's self-improvement trajectory. Our findings establish a fundamental relationship between initial reasoning behaviors and the capacity for improvement, explaining why some language models effectively utilize additional computation while others plateau.", 'score': 13, 'issue_id': 2511, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': 'fa966620baa8c013', 'authors': ['Kanishk Gandhi', 'Ayush Chakravarthy', 'Anikait Singh', 'Nathan Lile', 'Noah D. Goodman'], 'affiliations': ['Stanford University', 'SynthLabs'], 'pdf_title_img': 'assets/pdf/title_img/2503.01307.jpg', 'data': {'categories': ['#training', '#optimization', '#rl', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Когнитивные навыки - ключ к самосовершенствованию языковых моделей', 'desc': 'Исследование показывает, что способность языковых моделей к самосовершенствованию зависит от наличия у них определенных когнитивных поведений, таких как верификация, бэктрекинг, постановка подцелей и обратное планирование. Эксперименты выявили, что модель Qwen изначально обладает этими навыками, в то время как Llama нет. Прайминг Llama примерами, содержащими эти поведения, позволил значительно улучшить ее производительность при обучении с подкреплением. Важно отметить, что наличие правильных рассуждений оказалось более критичным фактором, чем корректность ответов.'}, 'en': {'title': 'Unlocking Self-Improvement in Language Models through Reasoning', 'desc': 'This paper explores how language models can improve their problem-solving abilities through a process called test-time inference, similar to human experts. It highlights the differences in performance between two models, Qwen-2.5-3B and Llama-3.2-3B, when trained with reinforcement learning (RL) on the game Countdown. The authors identify four cognitive behaviors—verification, backtracking, subgoal setting, and backward chaining—that are crucial for effective self-improvement in these models. They demonstrate that enhancing Llama with examples of these reasoning behaviors can significantly boost its performance, suggesting that the ability to reason is more important than simply providing correct answers.'}, 'zh': {'title': '推理行为是模型自我提升的关键', 'desc': '本文探讨了语言模型在复杂任务中自我改进的能力，特别是通过强化学习（RL）实现的自我提升。研究发现，不同模型在相同的RL训练下表现差异显著，例如Qwen-2.5-3B在游戏Countdown中远超Llama-3.2-3B。我们分析了四种关键的认知行为：验证、回溯、子目标设定和逆向链推理，发现Qwen自然展现了这些推理行为，而Llama则最初缺乏。通过对Llama进行示例引导，能够显著提升其在RL中的表现，证明了推理行为的存在是模型自我改进的关键因素。'}}}, {'id': 'https://huggingface.co/papers/2503.00501', 'title': 'Qilin: A Multimodal Information Retrieval Dataset with APP-level User Sessions', 'url': 'https://huggingface.co/papers/2503.00501', 'abstract': "User-generated content (UGC) communities, especially those featuring multimodal content, improve user experiences by integrating visual and textual information into results (or items). The challenge of improving user experiences in complex systems with search and recommendation (S\\&R) services has drawn significant attention from both academia and industry these years. However, the lack of high-quality datasets has limited the research progress on multimodal S\\&R. To address the growing need for developing better S\\&R services, we present a novel multimodal information retrieval dataset in this paper, namely Qilin. The dataset is collected from Xiaohongshu, a popular social platform with over 300 million monthly active users and an average search penetration rate of over 70\\%. In contrast to existing datasets, Qilin offers a comprehensive collection of user sessions with heterogeneous results like image-text notes, video notes, commercial notes, and direct answers, facilitating the development of advanced multimodal neural retrieval models across diverse task settings. To better model user satisfaction and support the analysis of heterogeneous user behaviors, we also collect extensive APP-level contextual signals and genuine user feedback. Notably, Qilin contains user-favored answers and their referred results for search requests triggering the Deep Query Answering (DQA) module. This allows not only the training \\& evaluation of a Retrieval-augmented Generation (RAG) pipeline, but also the exploration of how such a module would affect users' search behavior. Through comprehensive analysis and experiments, we provide interesting findings and insights for further improving S\\&R systems. We hope that Qilin will significantly contribute to the advancement of multimodal content platforms with S\\&R services in the future.", 'score': 10, 'issue_id': 2513, 'pub_date': '2025-03-01', 'pub_date_card': {'ru': '1 марта', 'en': 'March 1', 'zh': '3月1日'}, 'hash': 'ed7fc8625b068597', 'authors': ['Jia Chen', 'Qian Dong', 'Haitao Li', 'Xiaohui He', 'Yan Gao', 'Shaosheng Cao', 'Yi Wu', 'Ping Yang', 'Chen Xu', 'Yao Hu', 'Qingyao Ai', 'Yiqun Liu'], 'affiliations': ['Tsinghua University', 'Xiaohongshu Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2503.00501.jpg', 'data': {'categories': ['#multimodal', '#dataset', '#rag'], 'emoji': '🔍', 'ru': {'title': 'Qilin: мультимодальный датасет для улучшения поиска и рекомендаций', 'desc': 'Представлен новый набор данных Qilin для мультимодального информационного поиска, собранный на платформе Xiaohongshu. Датасет включает пользовательские сессии с разнородными результатами (изображения, видео, коммерческие заметки) и контекстуальными сигналами. Qilin позволяет обучать и оценивать нейросетевые модели поиска и рекомендаций, а также исследовать влияние модуля глубоких ответов на запросы. Авторы надеются, что Qilin внесет значительный вклад в развитие мультимодальных платформ с поисковыми сервисами.'}, 'en': {'title': 'Enhancing User Experiences with Qilin: A Multimodal Dataset for S&R Services', 'desc': 'This paper introduces Qilin, a new multimodal information retrieval dataset designed to enhance search and recommendation (S&R) services in user-generated content communities. Qilin is unique as it includes diverse user sessions with various content types, such as image-text notes and videos, which can help in developing advanced multimodal neural retrieval models. Additionally, the dataset captures user feedback and contextual signals, allowing researchers to analyze user satisfaction and behavior more effectively. The findings from this research aim to improve S&R systems and contribute to the evolution of multimodal content platforms.'}, 'zh': {'title': '推动多模态搜索与推荐服务的进步', 'desc': '本文介绍了一个新的多模态信息检索数据集Qilin，旨在改善用户在复杂系统中的搜索和推荐体验。Qilin数据集来源于小红书，包含多种类型的用户会话，如图文笔记、视频笔记和商业笔记，适用于多种任务设置。该数据集还收集了丰富的应用级上下文信号和真实用户反馈，以更好地建模用户满意度。通过对Qilin的分析和实验，本文提供了有趣的发现，期望能推动多模态内容平台的搜索和推荐服务的发展。'}}}, {'id': 'https://huggingface.co/papers/2503.00714', 'title': 'Speculative Ad-hoc Querying', 'url': 'https://huggingface.co/papers/2503.00714', 'abstract': "Analyzing large datasets requires responsive query execution, but executing SQL queries on massive datasets can be slow. This paper explores whether query execution can begin even before the user has finished typing, allowing results to appear almost instantly. We propose SpeQL, a system that leverages Large Language Models (LLMs) to predict likely queries based on the database schema, the user's past queries, and their incomplete query. Since exact query prediction is infeasible, SpeQL speculates on partial queries in two ways: 1) it predicts the query structure to compile and plan queries in advance, and 2) it precomputes smaller temporary tables that are much smaller than the original database, but are still predicted to contain all information necessary to answer the user's final query. Additionally, SpeQL continuously displays results for speculated queries and subqueries in real time, aiding exploratory analysis. A utility/user study showed that SpeQL improved task completion time, and participants reported that its speculative display of results helped them discover patterns in the data more quickly. In the study, SpeQL improves user's query latency by up to 289times and kept the overhead reasonable, at 4$ per hour.", 'score': 8, 'issue_id': 2514, 'pub_date': '2025-03-02', 'pub_date_card': {'ru': '2 марта', 'en': 'March 2', 'zh': '3月2日'}, 'hash': '1b0459b56fdb6894', 'authors': ['Haoyu Li', 'Srikanth Kandula', 'Maria Angels de Luis Balaguer', 'Aditya Akella', 'Venkat Arun'], 'affiliations': ['Amazon Web Services', 'Microsoft Research', 'The University of Texas at Austin'], 'pdf_title_img': 'assets/pdf/title_img/2503.00714.jpg', 'data': {'categories': ['#dataset', '#data', '#benchmark'], 'emoji': '⚡', 'ru': {'title': 'Молниеносные SQL-запросы с помощью предиктивной аналитики', 'desc': 'Статья представляет систему SpeQL, использующую большие языковые модели для предсказания SQL-запросов пользователя. SpeQL предугадывает структуру запроса и предварительно вычисляет временные таблицы, что позволяет начать выполнение запроса до его завершения пользователем. Система непрерывно отображает результаты предполагаемых запросов в реальном времени, помогая в исследовательском анализе данных. Исследование показало, что SpeQL значительно сокращает время выполнения задач и помогает пользователям быстрее обнаруживать закономерности в данных.'}, 'en': {'title': 'Instant Query Results with SpeQL!', 'desc': 'This paper introduces SpeQL, a novel system designed to enhance the speed of SQL query execution on large datasets. By utilizing Large Language Models (LLMs), SpeQL predicts user queries even before they are fully typed, allowing for near-instantaneous results. It employs two main strategies: predicting the structure of queries for pre-compilation and creating smaller temporary tables that contain essential data for answering the final query. A user study demonstrated that SpeQL significantly reduced query latency and helped users identify data patterns more efficiently during exploratory analysis.'}, 'zh': {'title': 'SpeQL：让查询更快的智能预测系统', 'desc': '本论文探讨了如何在用户输入SQL查询时，提前开始执行查询，以加快大数据集的查询响应速度。我们提出了SpeQL系统，利用大型语言模型（LLMs）根据数据库模式、用户的历史查询和不完整查询来预测可能的查询。SpeQL通过预测查询结构和预计算小型临时表来处理部分查询，从而在用户完成查询之前提供实时结果。研究表明，SpeQL显著提高了用户的查询速度，并帮助用户更快地发现数据中的模式。'}}}, {'id': 'https://huggingface.co/papers/2503.00784', 'title': 'DuoDecoding: Hardware-aware Heterogeneous Speculative Decoding with Dynamic Multi-Sequence Drafting', 'url': 'https://huggingface.co/papers/2503.00784', 'abstract': 'Large language models (LLMs) exhibit exceptional performance across a wide range of tasks; however, their token-by-token autoregressive generation process significantly hinders inference speed. Speculative decoding presents a promising draft-then-verify framework that reduces generation latency while maintaining output distribution fidelity. Nevertheless, the draft model introduces additional computational overhead, becoming a performance bottleneck and increasing the time to first token (TTFT). Previous approaches to mitigate draft model overhead have primarily relied on heuristics and generally failed to match the quality of the draft language models. To address these challenges, we propose DuoDecoding, a novel approach that strategically deploys the draft and target models on the CPU and GPU respectively, enabling parallel decoding while preserving draft quality. Our method incorporates a hardware-aware optimal draft budget to minimize idle times and employs dynamic multi-sequence drafting to enhance draft quality. Extensive experiments across seven tasks show that DuoDecoding achieves up to 2.61x speedup in generation latency, while reducing TTFT to 83% of that in conventional speculative decoding. The Code is available at https://github.com/KaiLv69/DuoDecoding.', 'score': 8, 'issue_id': 2510, 'pub_date': '2025-03-02', 'pub_date_card': {'ru': '2 марта', 'en': 'March 2', 'zh': '3月2日'}, 'hash': 'b4870a0e44c3cc55', 'authors': ['Kai Lv', 'Honglin Guo', 'Qipeng Guo', 'Xipeng Qiu'], 'affiliations': ['Fudan University', 'Shanghai AI Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2503.00784.jpg', 'data': {'categories': ['#inference', '#training', '#optimization'], 'emoji': '🚀', 'ru': {'title': 'DuoDecoding: Параллельное ускорение языковых моделей', 'desc': 'Статья представляет новый метод ускорения генерации текста большими языковыми моделями (LLM) под названием DuoDecoding. Этот подход использует параллельное декодирование на CPU и GPU, оптимизируя время генерации первого токена и общую латентность. DuoDecoding применяет аппаратно-ориентированный оптимальный бюджет черновика и динамическое многопоследовательное черновое декодирование для повышения качества. Эксперименты показали значительное ускорение генерации по сравнению с обычным спекулятивным декодированием.'}, 'en': {'title': 'DuoDecoding: Speeding Up Text Generation with Smart Model Deployment', 'desc': 'This paper introduces DuoDecoding, a new method to improve the speed of generating text with large language models (LLMs) while keeping the quality high. It uses a draft-then-verify approach, where a draft model quickly generates initial text, and a target model refines it, but does so in a way that reduces the time it takes to start generating text. By using both CPU and GPU for different parts of the process, DuoDecoding allows for faster and more efficient decoding. The results show that this method can significantly speed up text generation without sacrificing quality, achieving a notable improvement in performance across various tasks.'}, 'zh': {'title': 'DuoDecoding：加速生成的新方法', 'desc': '大型语言模型（LLMs）在多种任务中表现出色，但其逐字自回归生成过程显著影响推理速度。推测解码提供了一种有前景的草稿-验证框架，能够减少生成延迟，同时保持输出分布的准确性。我们提出的DuoDecoding方法通过在CPU和GPU上分别部署草稿模型和目标模型，实现了并行解码，提升了生成效率。实验结果表明，DuoDecoding在生成延迟上实现了最高2.61倍的加速，同时将首次生成时间缩短至传统推测解码的83%。'}}}, {'id': 'https://huggingface.co/papers/2503.01506', 'title': 'SampleMix: A Sample-wise Pre-training Data Mixing Strategey by Coordinating Data Quality and Diversity', 'url': 'https://huggingface.co/papers/2503.01506', 'abstract': "Existing pretraining data mixing methods for large language models (LLMs) typically follow a domain-wise methodology, a top-down process that first determines domain weights and then performs uniform data sampling across each domain. However, these approaches neglect significant inter-domain overlaps and commonalities, failing to control the global diversity of the constructed training dataset. Further, uniform sampling within domains ignores fine-grained sample-specific features, potentially leading to suboptimal data distribution. To address these shortcomings, we propose a novel sample-wise data mixture approach based on a bottom-up paradigm. This method performs global cross-domain sampling by systematically evaluating the quality and diversity of each sample, thereby dynamically determining the optimal domain distribution. Comprehensive experiments across multiple downstream tasks and perplexity assessments demonstrate that SampleMix surpasses existing domain-based methods. Meanwhile, SampleMix requires 1.4x to 2.1x training steps to achieves the baselines' performance, highlighting the substantial potential of SampleMix to optimize pre-training data.", 'score': 7, 'issue_id': 2517, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': '018cc621eb1ee12b', 'authors': ['Xiangyu Xi', 'Deyang Kong', 'Jian Yang', 'Jiawei Yang', 'Zhengyu Chen', 'Wei Wang', 'Jingang Wang', 'Xunliang Cai', 'Shikun Zhang', 'Wei Ye'], 'affiliations': ['Meituan Group, Beijing, China', 'National Engineering Research Center for Software Engineering, Peking University, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.01506.jpg', 'data': {'categories': ['#transfer_learning', '#training', '#optimization', '#data'], 'emoji': '🔀', 'ru': {'title': 'SampleMix: революция в смешивании данных для LLM', 'desc': 'В статье представлен новый подход к смешиванию предобучающих данных для больших языковых моделей (LLM), названный SampleMix. В отличие от традиционных методов, основанных на доменах, SampleMix использует выборку на уровне отдельных образцов, оценивая их качество и разнообразие. Этот метод позволяет динамически определять оптимальное распределение доменов и учитывать межdomенные пересечения. Эксперименты показали, что SampleMix превосходит существующие методы, основанные на доменах, хотя и требует больше шагов обучения.'}, 'en': {'title': 'Revolutionizing Data Mixing for Better Language Model Training', 'desc': 'This paper introduces SampleMix, a new method for mixing pretraining data for large language models (LLMs). Unlike traditional domain-wise approaches that sample uniformly within predefined domains, SampleMix uses a bottom-up strategy that evaluates the quality and diversity of individual samples across domains. This allows for a more dynamic and optimal distribution of training data, addressing the limitations of inter-domain overlaps and sample-specific features. Experimental results show that SampleMix not only outperforms existing methods but also requires fewer training steps to achieve comparable performance.'}, 'zh': {'title': '样本级数据混合，优化预训练数据的未来', 'desc': '现有的大型语言模型预训练数据混合方法通常采用领域导向的方法，先确定领域权重，再在每个领域内进行均匀数据采样。然而，这些方法忽视了领域之间的重要重叠和共性，未能有效控制训练数据集的全球多样性。此外，领域内的均匀采样忽略了样本特定的细微特征，可能导致数据分布不理想。为了解决这些问题，我们提出了一种基于自下而上的新型样本级数据混合方法，能够通过系统评估每个样本的质量和多样性，动态确定最佳领域分布。'}}}, {'id': 'https://huggingface.co/papers/2502.18890', 'title': 'From Hours to Minutes: Lossless Acceleration of Ultra Long Sequence Generation up to 100K Tokens', 'url': 'https://huggingface.co/papers/2502.18890', 'abstract': "Generating ultra-long sequences with large language models (LLMs) has become increasingly crucial but remains a highly time-intensive task, particularly for sequences up to 100K tokens. While traditional speculative decoding methods exist, simply extending their generation limits fails to accelerate the process and can be detrimental. Through an in-depth analysis, we identify three major challenges hindering efficient generation: frequent model reloading, dynamic key-value (KV) management and repetitive generation. To address these issues, we introduce TOKENSWIFT, a novel framework designed to substantially accelerate the generation process of ultra-long sequences while maintaining the target model's inherent quality. Experimental results demonstrate that TOKENSWIFT achieves over 3 times speedup across models of varying scales (1.5B, 7B, 8B, 14B) and architectures (MHA, GQA). This acceleration translates to hours of time savings for ultra-long sequence generation, establishing TOKENSWIFT as a scalable and effective solution at unprecedented lengths. Code can be found at https://github.com/bigai-nlco/TokenSwift.", 'score': 7, 'issue_id': 2517, 'pub_date': '2025-02-26', 'pub_date_card': {'ru': '26 февраля', 'en': 'February 26', 'zh': '2月26日'}, 'hash': 'd07c05abfac49ecc', 'authors': ['Tong Wu', 'Junzhe Shen', 'Zixia Jia', 'Yuxuan Wang', 'Zilong Zheng'], 'affiliations': ['NLCo Lab, BIGAI LUMIA Lab, Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2502.18890.jpg', 'data': {'categories': ['#training', '#architecture', '#long_context', '#inference', '#optimization'], 'emoji': '⚡', 'ru': {'title': 'TOKENSWIFT: революция в скорости генерации сверхдлинных текстов', 'desc': 'Исследователи представили TOKENSWIFT - новую систему для ускорения генерации сверхдлинных последовательностей большими языковыми моделями (LLM). Они выявили три основные проблемы, препятствующие эффективной генерации: частая перезагрузка модели, динамическое управление ключами-значениями и повторяющаяся генерация. TOKENSWIFT решает эти проблемы, позволяя ускорить процесс генерации в 3 раза для моделей различных масштабов и архитектур. Это существенно сокращает время генерации сверхдлинных последовательностей, сохраняя при этом качество целевой модели.'}, 'en': {'title': 'Accelerating Ultra-Long Sequence Generation with TOKENSWIFT', 'desc': 'This paper presents TOKENSWIFT, a new framework aimed at speeding up the generation of ultra-long sequences using large language models (LLMs). The authors identify key challenges such as model reloading, dynamic key-value management, and repetitive generation that slow down the process. By addressing these issues, TOKENSWIFT achieves over three times the speed of traditional methods while preserving the quality of the generated text. Experimental results show that this framework is effective across various model sizes and architectures, making it a significant advancement in the field of sequence generation.'}, 'zh': {'title': 'TOKENSWIFT：加速超长序列生成的创新框架', 'desc': '生成超长序列对于大型语言模型（LLMs）变得越来越重要，但这一过程通常非常耗时，尤其是对于长达10万标记的序列。传统的推测解码方法在延长生成限制时并未加速过程，反而可能造成负面影响。我们通过深入分析，识别出影响高效生成的三个主要挑战：频繁的模型重载、动态键值（KV）管理和重复生成。为了解决这些问题，我们提出了TOKENSWIFT，一个新框架，旨在显著加快超长序列的生成过程，同时保持目标模型的固有质量。'}}}, {'id': 'https://huggingface.co/papers/2503.01370', 'title': 'Kiss3DGen: Repurposing Image Diffusion Models for 3D Asset Generation', 'url': 'https://huggingface.co/papers/2503.01370', 'abstract': "Diffusion models have achieved great success in generating 2D images. However, the quality and generalizability of 3D content generation remain limited. State-of-the-art methods often require large-scale 3D assets for training, which are challenging to collect. In this work, we introduce Kiss3DGen (Keep It Simple and Straightforward in 3D Generation), an efficient framework for generating, editing, and enhancing 3D objects by repurposing a well-trained 2D image diffusion model for 3D generation. Specifically, we fine-tune a diffusion model to generate ''3D Bundle Image'', a tiled representation composed of multi-view images and their corresponding normal maps. The normal maps are then used to reconstruct a 3D mesh, and the multi-view images provide texture mapping, resulting in a complete 3D model. This simple method effectively transforms the 3D generation problem into a 2D image generation task, maximizing the utilization of knowledge in pretrained diffusion models. Furthermore, we demonstrate that our Kiss3DGen model is compatible with various diffusion model techniques, enabling advanced features such as 3D editing, mesh and texture enhancement, etc. Through extensive experiments, we demonstrate the effectiveness of our approach, showcasing its ability to produce high-quality 3D models efficiently.", 'score': 7, 'issue_id': 2513, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': '3decc9fe2b6f6e32', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#cv', '#diffusion', '#3d'], 'emoji': '🎨', 'ru': {'title': 'Простая и эффективная 3D-генерация на основе 2D-диффузии', 'desc': "Статья представляет Kiss3DGen - эффективный фреймворк для генерации, редактирования и улучшения 3D-объектов с использованием предобученной модели диффузии для 2D-изображений. Метод основан на дообучении диффузионной модели для генерации 'Пакетного 3D-изображения', состоящего из мультиракурсных изображений и соответствующих карт нормалей. Затем карты нормалей используются для реконструкции 3D-меша, а мультиракурсные изображения обеспечивают текстурирование, что в результате дает полную 3D-модель. Авторы демонстрируют, что их подход совместим с различными техниками диффузионных моделей и позволяет эффективно создавать качественные 3D-модели."}, 'en': {'title': 'Kiss3DGen: Simplifying 3D Generation with 2D Diffusion Models', 'desc': "This paper presents Kiss3DGen, a novel framework that simplifies the process of generating and enhancing 3D objects by leveraging existing 2D image diffusion models. The approach involves fine-tuning a diffusion model to create a '3D Bundle Image', which consists of multiple views and normal maps that are essential for 3D reconstruction. By transforming the 3D generation challenge into a 2D image task, the method maximizes the use of knowledge from pretrained models, making it more efficient. The results show that Kiss3DGen not only generates high-quality 3D models but also supports advanced features like editing and texture enhancement."}, 'zh': {'title': '简单高效的三维生成方法', 'desc': '扩散模型在生成二维图像方面取得了巨大成功，但在三维内容生成的质量和通用性上仍然有限。现有的先进方法通常需要大量的三维资产进行训练，这些资产难以收集。我们提出了Kiss3DGen（简单直接的三维生成），这是一个高效的框架，通过重新利用经过良好训练的二维图像扩散模型来生成、编辑和增强三维物体。该方法将三维生成问题转化为二维图像生成任务，最大化利用预训练扩散模型中的知识，能够有效生成高质量的三维模型。'}}, 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2503.00455', 'title': 'PodAgent: A Comprehensive Framework for Podcast Generation', 'url': 'https://huggingface.co/papers/2503.00455', 'abstract': "Existing Existing automatic audio generation methods struggle to generate podcast-like audio programs effectively. The key challenges lie in in-depth content generation, appropriate and expressive voice production. This paper proposed PodAgent, a comprehensive framework for creating audio programs. PodAgent 1) generates informative topic-discussion content by designing a Host-Guest-Writer multi-agent collaboration system, 2) builds a voice pool for suitable voice-role matching and 3) utilizes LLM-enhanced speech synthesis method to generate expressive conversational speech. Given the absence of standardized evaluation criteria for podcast-like audio generation, we developed comprehensive assessment guidelines to effectively evaluate the model's performance. Experimental results demonstrate PodAgent's effectiveness, significantly surpassing direct GPT-4 generation in topic-discussion dialogue content, achieving an 87.4% voice-matching accuracy, and producing more expressive speech through LLM-guided synthesis. Demo page: https://podcast-agent.github.io/demo/. Source code: https://github.com/yujxx/PodAgent.", 'score': 5, 'issue_id': 2519, 'pub_date': '2025-03-01', 'pub_date_card': {'ru': '1 марта', 'en': 'March 1', 'zh': '3月1日'}, 'hash': '59ce5f373a030894', 'authors': ['Yujia Xiao', 'Lei He', 'Haohan Guo', 'Fenglong Xie', 'Tan Lee'], 'affiliations': ['Microsoft', 'The Chinese University of Hong Kong', 'Xiaohongshu Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2503.00455.jpg', 'data': {'categories': ['#games', '#audio', '#interpretability', '#benchmark', '#optimization', '#multimodal'], 'emoji': '🎙️', 'ru': {'title': 'PodAgent: ИИ-ведущий для подкастов нового поколения', 'desc': 'PodAgent - это новая система для автоматического создания аудиопрограмм в стиле подкастов. Она использует мультиагентный подход для генерации содержательного контента, подбирает подходящие голоса из голосового пула и применяет улучшенный синтез речи на основе языковых моделей. Система решает ключевые проблемы существующих методов, такие как глубина контента и выразительность голоса. Эксперименты показали значительное превосходство PodAgent над прямой генерацией GPT-4 по качеству диалогов и точности подбора голосов.'}, 'en': {'title': 'Revolutionizing Podcast Audio Generation with PodAgent', 'desc': "This paper introduces PodAgent, a novel framework designed to enhance the generation of podcast-like audio programs. It addresses key challenges in content creation and voice production by employing a multi-agent system that includes a Host, Guest, and Writer for collaborative topic discussions. Additionally, PodAgent features a voice pool for effective voice-role matching and utilizes a large language model (LLM) to improve the expressiveness of the generated speech. The framework's performance is validated through comprehensive evaluation guidelines, showing significant improvements over existing methods, including a high voice-matching accuracy and more engaging conversational audio."}, 'zh': {'title': 'PodAgent：智能生成播客音频的全新框架', 'desc': '本论文提出了一种名为PodAgent的框架，用于自动生成类似播客的音频节目。PodAgent通过设计一个主持人-嘉宾-编剧的多智能体协作系统，生成有深度的主题讨论内容。同时，它建立了一个声音库，以实现合适的声音角色匹配，并利用增强型大语言模型（LLM）进行富有表现力的语音合成。实验结果表明，PodAgent在主题讨论对话内容生成方面显著优于直接使用GPT-4，语音匹配准确率达到87.4%，并通过LLM引导的合成生成了更具表现力的语音。'}}}, {'id': 'https://huggingface.co/papers/2503.01295', 'title': 'CodeArena: A Collective Evaluation Platform for LLM Code Generation', 'url': 'https://huggingface.co/papers/2503.01295', 'abstract': 'Large Language Models (LLMs) have reshaped code generation by synergizing their exceptional comprehension of natural language and programming syntax, thereby substantially boosting developer productivity. These advancements have prompted numerous efforts to quantitatively evaluate their coding capabilities. However, persistent challenges, such as benchmark leakage, data dissipation, and limited system accessibility, continue to impede a timely and accurate assessment. To address these limitations, we introduce CodeArena, an online evaluation framework tailored for LLM code generation. The key innovation is a collective evaluation mechanism, which dynamically recalibrates individual model scores based on the holistic performance of all participating models, mitigating score biases caused by widespread benchmark leakage. In addition, CodeArena ensures open access to all submitted solutions and test cases and provides automation-friendly APIs to streamline the code evaluation workflow. Our main contributions are: (1) a collective evaluation system for unbiased assessment, (2) a public repository of solutions and test cases, and (3) automation-ready APIs for seamless integration.', 'score': 5, 'issue_id': 2514, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': '96f50dd9e636b12e', 'authors': ['Mingzhe Du', 'Anh Tuan Luu', 'Bin Ji', 'Xiaobao Wu', 'Dong Huang', 'Terry Yue Zhuo', 'Qian Liu', 'See-Kiong Ng'], 'affiliations': ['ByteDance', 'Monash University', 'Nanyang Technological University', 'National University of Singapore', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2503.01295.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#leakage', '#open_source'], 'emoji': '🏟️', 'ru': {'title': 'CodeArena: Справедливая арена для оценки LLM в генерации кода', 'desc': 'CodeArena - это новая онлайн-платформа для оценки генерации кода большими языковыми моделями (LLM). Она использует коллективный механизм оценки, который динамически пересчитывает индивидуальные оценки моделей на основе общей производительности всех участвующих моделей. Это помогает снизить искажения оценок, вызванные утечкой тестовых данных. CodeArena также предоставляет открытый доступ ко всем отправленным решениям и тестовым случаям, а также API для автоматизации процесса оценки.'}, 'en': {'title': 'Revolutionizing Code Evaluation with CodeArena', 'desc': 'This paper discusses the impact of Large Language Models (LLMs) on code generation, highlighting their ability to understand both natural language and programming syntax, which enhances developer productivity. It identifies ongoing issues in evaluating LLM coding capabilities, such as benchmark leakage and limited access to evaluation systems. To overcome these challenges, the authors present CodeArena, an online framework that offers a collective evaluation mechanism to provide unbiased assessments of LLMs. CodeArena also features a public repository for solutions and test cases, along with APIs for easy integration into existing workflows.'}, 'zh': {'title': 'CodeArena：公平评估LLM代码生成的创新平台', 'desc': '大型语言模型（LLMs）通过结合对自然语言和编程语法的深刻理解，极大地提升了代码生成的效率，进而提高了开发者的生产力。为了量化评估这些模型的编码能力，许多研究工作应运而生，但仍面临基准泄漏、数据消散和系统可访问性有限等挑战。为了解决这些问题，我们提出了CodeArena，这是一个专为LLM代码生成设计的在线评估框架。其核心创新在于集体评估机制，能够根据所有参与模型的整体表现动态调整个别模型的评分，从而减少因基准泄漏造成的评分偏差。'}}}, {'id': 'https://huggingface.co/papers/2503.01807', 'title': 'Large-Scale Data Selection for Instruction Tuning', 'url': 'https://huggingface.co/papers/2503.01807', 'abstract': 'Selecting high-quality training data from a larger pool is a crucial step when instruction-tuning language models, as carefully curated datasets often produce models that outperform those trained on much larger, noisier datasets. Automated data selection approaches for instruction-tuning are typically tested by selecting small datasets (roughly 10k samples) from small pools (100-200k samples). However, popular deployed instruction-tuned models often train on hundreds of thousands to millions of samples, subsampled from even larger data pools. We present a systematic study of how well data selection methods scale to these settings, selecting up to 2.5M samples from pools of up to 5.8M samples and evaluating across 7 diverse tasks. We show that many recently proposed methods fall short of random selection in this setting (while using more compute), and even decline in performance when given access to larger pools of data to select over. However, we find that a variant of representation-based data selection (RDS+), which uses weighted mean pooling of pretrained LM hidden states, consistently outperforms more complex methods across all settings tested -- all whilst being more compute-efficient. Our findings highlight that the scaling properties of proposed automated selection methods should be more closely examined. We release our code, data, and models at https://github.com/hamishivi/automated-instruction-selection.', 'score': 5, 'issue_id': 2511, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': '8bbc980a9ef867f7', 'authors': ['Hamish Ivison', 'Muru Zhang', 'Faeze Brahman', 'Pang Wei Koh', 'Pradeep Dasigi'], 'affiliations': ['Allen Institute for AI', 'University of Southern California', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2503.01807.jpg', 'data': {'categories': ['#data', '#open_source', '#optimization', '#dataset', '#training'], 'emoji': '🔍', 'ru': {'title': 'Эффективный отбор данных для обучения языковых моделей: меньше значит больше', 'desc': 'Эта статья исследует методы автоматического отбора данных для инструктивной настройки языковых моделей. Авторы проводят систематическое изучение эффективности различных методов при масштабировании до больших объемов данных, выбирая до 2,5 миллионов образцов из пулов до 5,8 миллионов. Результаты показывают, что многие недавно предложенные методы уступают случайному отбору в этих условиях, однако вариант метода отбора на основе представлений (RDS+) превосходит более сложные подходы. Исследование подчеркивает важность тщательного анализа масштабируемости методов автоматического отбора данных.'}, 'en': {'title': 'Quality Over Quantity: Smart Data Selection for Language Models', 'desc': 'This paper investigates the importance of selecting high-quality training data for instruction-tuning language models. It reveals that many automated data selection methods do not perform better than random selection when scaling to larger datasets, which can include millions of samples. The study introduces a representation-based data selection method (RDS+) that consistently outperforms more complex approaches while being more efficient in terms of computational resources. The authors emphasize the need for a deeper examination of how these selection methods behave as the size of the data pools increases.'}, 'zh': {'title': '高效选择：优化语言模型训练数据的关键', 'desc': '在对语言模型进行指令调优时，从更大数据集中选择高质量的训练数据是一个关键步骤。经过精心策划的数据集通常能产生比那些在更大、更嘈杂的数据集上训练的模型更好的效果。我们进行了系统研究，评估数据选择方法在大规模数据集上的表现，发现许多新提出的方法在这种情况下的表现不如随机选择。我们还发现一种基于表示的数据选择变体（RDS+）在所有测试设置中始终优于更复杂的方法，同时计算效率更高。'}}}, {'id': 'https://huggingface.co/papers/2503.00031', 'title': 'Efficient Test-Time Scaling via Self-Calibration', 'url': 'https://huggingface.co/papers/2503.00031', 'abstract': 'Increasing test-time computation is a straightforward approach to enhancing the quality of responses in Large Language Models (LLMs). While Best-of-N sampling and Self-Consistency with majority voting are simple and effective, they require a fixed number of sampling responses for each query, regardless of its complexity. This could result in wasted computation for simpler questions and insufficient exploration for more challenging ones. In this work, we argue that model confidence of responses can be used for improving the efficiency of test-time scaling. Unfortunately, LLMs are known to be overconfident and provide unreliable confidence estimation. To address this limitation, we introduce Self-Calibration by distilling Self-Consistency-derived confidence into the model itself. This enables reliable confidence estimation at test time with one forward pass. We then design confidence-based efficient test-time scaling methods to handle queries of various difficulty, such as Early-Stopping for Best-of-N and Self-Consistency with calibrated confidence. Experiments on three LLMs across six datasets demonstrate the effectiveness of our approach. Specifically, applying confidence-based Early Stopping to Best-of-N improves MathQA accuracy from 81.0 to 83.6 with a sample budget of 16 responses, indicating the efficacy of confidence-based sampling strategy at inference time.', 'score': 4, 'issue_id': 2523, 'pub_date': '2025-02-25', 'pub_date_card': {'ru': '25 февраля', 'en': 'February 25', 'zh': '2月25日'}, 'hash': 'fad9bb0721d1d6ce', 'authors': ['Chengsong Huang', 'Langlin Huang', 'Jixuan Leng', 'Jiacheng Liu', 'Jiaxin Huang'], 'affiliations': ['Carnegie Mellon University', 'University of Washington', 'Washington University in St. Louis'], 'pdf_title_img': 'assets/pdf/title_img/2503.00031.jpg', 'data': {'categories': ['#optimization', '#training', '#inference'], 'emoji': '🎯', 'ru': {'title': 'Повышение эффективности LLM через калибровку уверенности', 'desc': 'Это исследование предлагает метод Self-Calibration для улучшения эффективности крупных языковых моделей (LLM) при тестировании. Метод основан на дистилляции уверенности, полученной из Self-Consistency, в саму модель, что позволяет надежно оценивать уверенность за один проход. Авторы разработали методы эффективного масштабирования на основе уверенности, такие как Early-Stopping для Best-of-N и Self-Consistency с калиброванной уверенностью. Эксперименты на трех LLM и шести наборах данных показали эффективность этого подхода, в частности, улучшение точности MathQA с 81.0 до 83.6 при бюджете выборки в 16 ответов.'}, 'en': {'title': 'Enhancing LLM Efficiency with Confidence-Based Sampling', 'desc': 'This paper discusses how to improve the efficiency of Large Language Models (LLMs) during testing by using model confidence to guide response sampling. Traditional methods like Best-of-N sampling and Self-Consistency require a fixed number of responses, which can lead to wasted resources or inadequate exploration of complex queries. The authors propose a technique called Self-Calibration, which helps LLMs provide more reliable confidence estimates by distilling information from previous responses. By implementing confidence-based strategies such as Early-Stopping, the paper shows that it is possible to enhance accuracy while reducing unnecessary computations, particularly in challenging tasks like MathQA.'}, 'zh': {'title': '提升大型语言模型响应质量的自我校准方法', 'desc': '本论文探讨了如何通过增加测试时的计算来提高大型语言模型（LLMs）的响应质量。我们提出了一种自我校准的方法，通过将自我一致性生成的置信度提炼到模型中，从而改善置信度估计的可靠性。这样，模型在测试时可以在一次前向传播中获得可靠的置信度估计。我们的实验表明，基于置信度的早停策略能够有效提高模型在不同难度问题上的表现。'}}}, {'id': 'https://huggingface.co/papers/2503.01714', 'title': "Word Form Matters: LLMs' Semantic Reconstruction under Typoglycemia", 'url': 'https://huggingface.co/papers/2503.01714', 'abstract': "Human readers can efficiently comprehend scrambled words, a phenomenon known as Typoglycemia, primarily by relying on word form; if word form alone is insufficient, they further utilize contextual cues for interpretation. While advanced large language models (LLMs) exhibit similar abilities, the underlying mechanisms remain unclear. To investigate this, we conduct controlled experiments to analyze the roles of word form and contextual information in semantic reconstruction and examine LLM attention patterns. Specifically, we first propose SemRecScore, a reliable metric to quantify the degree of semantic reconstruction, and validate its effectiveness. Using this metric, we study how word form and contextual information influence LLMs' semantic reconstruction ability, identifying word form as the core factor in this process. Furthermore, we analyze how LLMs utilize word form and find that they rely on specialized attention heads to extract and process word form information, with this mechanism remaining stable across varying levels of word scrambling. This distinction between LLMs' fixed attention patterns primarily focused on word form and human readers' adaptive strategy in balancing word form and contextual information provides insights into enhancing LLM performance by incorporating human-like, context-aware mechanisms.", 'score': 4, 'issue_id': 2517, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': '4880ed4c044081c4', 'authors': ['Chenxi Wang', 'Tianle Gu', 'Zhongyu Wei', 'Lang Gao', 'Zirui Song', 'Xiuying Chen'], 'affiliations': ['Fudan University', 'Mohamed bin Zayed University of Artificial Intelligence (MBZUAI)'], 'pdf_title_img': 'assets/pdf/title_img/2503.01714.jpg', 'data': {'categories': ['#training', '#interpretability', '#data', '#multimodal', '#alignment'], 'emoji': '🔀', 'ru': {'title': 'Форма слова - ключ к пониманию перемешанного текста для ИИ', 'desc': 'Исследование посвящено способности больших языковых моделей (LLM) понимать перемешанные слова, подобно людям. Авторы предлагают метрику SemRecScore для оценки семантической реконструкции и анализируют роль формы слова и контекстной информации. Результаты показывают, что форма слова является ключевым фактором для LLM при обработке перемешанных слов. Анализ паттернов внимания LLM выявляет специализированные механизмы для извлечения информации о форме слова.'}, 'en': {'title': 'Unlocking LLMs: The Power of Word Form in Understanding Scrambled Text', 'desc': 'This paper explores how large language models (LLMs) understand scrambled words, similar to how humans do through a phenomenon called Typoglycemia. The authors introduce a new metric, SemRecScore, to measure how well LLMs can reconstruct meaning from scrambled text by focusing on word form and context. Their experiments reveal that LLMs primarily depend on word form for semantic reconstruction, utilizing specific attention heads to process this information. The findings suggest that incorporating more human-like, context-aware strategies could improve LLM performance in understanding language.'}, 'zh': {'title': '揭示大型语言模型的语义重建机制', 'desc': '本研究探讨了大型语言模型（LLMs）在语义重建中的能力，特别是它们如何利用单词形式和上下文信息。我们提出了一种新的度量标准SemRecScore，用于量化语义重建的程度，并验证了其有效性。研究发现，单词形式是影响LLMs语义重建能力的核心因素，且LLMs通过专门的注意力头来提取和处理单词形式信息。与人类读者在单词形式和上下文信息之间的灵活策略不同，LLMs的注意力模式主要集中在单词形式上，这为提升LLMs性能提供了新的思路。'}}}, {'id': 'https://huggingface.co/papers/2502.19402', 'title': 'General Reasoning Requires Learning to Reason from the Get-go', 'url': 'https://huggingface.co/papers/2502.19402', 'abstract': "Large Language Models (LLMs) have demonstrated impressive real-world utility, exemplifying artificial useful intelligence (AUI). However, their ability to reason adaptively and robustly -- the hallmarks of artificial general intelligence (AGI) -- remains fragile. While LLMs seemingly succeed in commonsense reasoning, programming, and mathematics, they struggle to generalize algorithmic understanding across novel contexts. Our experiments with algorithmic tasks in esoteric programming languages reveal that LLM's reasoning overfits to the training data and is limited in its transferability. We hypothesize that the core issue underlying such limited transferability is the coupling of reasoning and knowledge in LLMs.   To transition from AUI to AGI, we propose disentangling knowledge and reasoning through three key directions: (1) pretaining to reason using RL from scratch as an alternative to the widely used next-token prediction pretraining, (2) using a curriculum of synthetic tasks to ease the learning of a reasoning prior for RL that can then be transferred to natural language tasks, and (3) learning more generalizable reasoning functions using a small context window to reduce exploiting spurious correlations between tokens. Such a reasoning system coupled with a trained retrieval system and a large external memory bank as a knowledge store can overcome several limitations of existing architectures at learning to reason in novel scenarios.", 'score': 3, 'issue_id': 2520, 'pub_date': '2025-02-26', 'pub_date_card': {'ru': '26 февраля', 'en': 'February 26', 'zh': '2月26日'}, 'hash': '5774d50d6c5a9361', 'authors': ['Seungwook Han', 'Jyothish Pari', 'Samuel J. Gershman', 'Pulkit Agrawal'], 'affiliations': ['Department of Psychology and Center for Brain Science, Harvard University', 'Improbable AI Lab, MIT'], 'pdf_title_img': 'assets/pdf/title_img/2502.19402.jpg', 'data': {'categories': ['#agi', '#transfer_learning', '#architecture', '#rl', '#synthetic', '#training', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Отделяя рассуждения от знаний: путь к AGI', 'desc': 'Статья рассматривает ограничения больших языковых моделей (LLM) в области обобщенного рассуждения, несмотря на их впечатляющую полезность. Авторы выявляют проблему переобучения LLM на тренировочных данных и предлагают разделить знания и рассуждения для перехода к искусственному общему интеллекту (AGI). Предлагается использовать обучение с подкреплением, синтетические задачи и ограниченный контекст для улучшения обобщающей способности. Такая система рассуждений в сочетании с извлечением информации и внешней памятью может преодолеть ограничения существующих архитектур.'}, 'en': {'title': 'Disentangling Knowledge and Reasoning for Robust AI', 'desc': "This paper discusses the limitations of Large Language Models (LLMs) in achieving robust reasoning capabilities, which are essential for artificial general intelligence (AGI). The authors identify that LLMs often overfit to their training data, leading to poor generalization in novel algorithmic tasks. They propose a solution that involves separating knowledge from reasoning by employing reinforcement learning (RL) and a structured curriculum of synthetic tasks. By enhancing reasoning functions and integrating a retrieval system with an external memory, the authors aim to improve LLMs' adaptability and performance in unfamiliar contexts."}, 'zh': {'title': '解耦知识与推理，迈向人工通用智能', 'desc': '大型语言模型（LLMs）在实际应用中表现出色，展示了人工有用智能（AUI）的潜力。然而，它们在自适应和稳健推理方面的能力仍然脆弱，这是人工通用智能（AGI）的标志。我们的实验表明，LLMs在算法任务中容易过拟合训练数据，且在新环境中的迁移能力有限。为了解决这一问题，我们提出通过三种关键方向来解耦知识与推理，以促进从AUI向AGI的过渡。'}}}, {'id': 'https://huggingface.co/papers/2503.01739', 'title': 'VideoUFO: A Million-Scale User-Focused Dataset for Text-to-Video Generation', 'url': 'https://huggingface.co/papers/2503.01739', 'abstract': "Text-to-video generative models convert textual prompts into dynamic visual content, offering wide-ranging applications in film production, gaming, and education. However, their real-world performance often falls short of user expectations. One key reason is that these models have not been trained on videos related to some topics users want to create. In this paper, we propose VideoUFO, the first Video dataset specifically curated to align with Users' FOcus in real-world scenarios. Beyond this, our VideoUFO also features: (1) minimal (0.29%) overlap with existing video datasets, and (2) videos searched exclusively via YouTube's official API under the Creative Commons license. These two attributes provide future researchers with greater freedom to broaden their training sources. The VideoUFO comprises over 1.09 million video clips, each paired with both a brief and a detailed caption (description). Specifically, through clustering, we first identify 1,291 user-focused topics from the million-scale real text-to-video prompt dataset, VidProM. Then, we use these topics to retrieve videos from YouTube, split the retrieved videos into clips, and generate both brief and detailed captions for each clip. After verifying the clips with specified topics, we are left with about 1.09 million video clips. Our experiments reveal that (1) current 16 text-to-video models do not achieve consistent performance across all user-focused topics; and (2) a simple model trained on VideoUFO outperforms others on worst-performing topics. The dataset is publicly available at https://huggingface.co/datasets/WenhaoWang/VideoUFO under the CC BY 4.0 License.", 'score': 3, 'issue_id': 2512, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': '046fdeee8939e82c', 'authors': ['Wenhao Wang', 'Yi Yang'], 'affiliations': ['University of Technology Sydney', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.01739.jpg', 'data': {'categories': ['#video', '#dataset', '#data', '#games', '#open_source'], 'emoji': '🎬', 'ru': {'title': 'VideoUFO: Новый эталонный датасет для генерации видео по запросу', 'desc': 'Статья представляет VideoUFO - новый набор данных для обучения моделей генерации видео по текстовому описанию. Этот датасет содержит более 1,09 миллиона видеоклипов с подробными и краткими описаниями, охватывающих 1291 тему, актуальную для пользователей. VideoUFO отличается минимальным пересечением с существующими наборами данных и использованием только видео под лицензией Creative Commons. Эксперименты показали, что простая модель, обученная на VideoUFO, превосходит другие модели на наиболее сложных темах.'}, 'en': {'title': 'Empowering Text-to-Video Models with User-Focused Data', 'desc': 'This paper introduces VideoUFO, a novel video dataset designed to enhance text-to-video generative models by focusing on user-relevant topics. The dataset contains over 1.09 million video clips, each accompanied by both brief and detailed captions, ensuring minimal overlap with existing datasets. By clustering user prompts, the authors identified 1,291 specific topics to guide video retrieval from YouTube, which were then segmented into clips. Experiments show that models trained on VideoUFO significantly outperform existing models, particularly on challenging topics, highlighting the importance of tailored training data in machine learning applications.'}, 'zh': {'title': '提升文本到视频生成的用户体验', 'desc': '本文介绍了一种新的视频数据集VideoUFO，旨在提高文本到视频生成模型的性能。该数据集专注于用户关注的主题，包含超过109万个视频片段，并为每个片段提供简短和详细的描述。VideoUFO与现有数据集的重叠率极低，且所有视频均通过YouTube的官方API获取，确保了数据的多样性和合法性。实验结果表明，使用VideoUFO训练的模型在用户关注的主题上表现优于其他模型。'}}}, {'id': 'https://huggingface.co/papers/2503.01103', 'title': 'Direct Discriminative Optimization: Your Likelihood-Based Visual Generative Model is Secretly a GAN Discriminator', 'url': 'https://huggingface.co/papers/2503.01103', 'abstract': 'While likelihood-based generative models, particularly diffusion and autoregressive models, have achieved remarkable fidelity in visual generation, the maximum likelihood estimation (MLE) objective inherently suffers from a mode-covering tendency that limits the generation quality under limited model capacity. In this work, we propose Direct Discriminative Optimization (DDO) as a unified framework that bridges likelihood-based generative training and the GAN objective to bypass this fundamental constraint. Our key insight is to parameterize a discriminator implicitly using the likelihood ratio between a learnable target model and a fixed reference model, drawing parallels with the philosophy of Direct Preference Optimization (DPO). Unlike GANs, this parameterization eliminates the need for joint training of generator and discriminator networks, allowing for direct, efficient, and effective finetuning of a well-trained model to its full potential beyond the limits of MLE. DDO can be performed iteratively in a self-play manner for progressive model refinement, with each round requiring less than 1% of pretraining epochs. Our experiments demonstrate the effectiveness of DDO by significantly advancing the previous SOTA diffusion model EDM, reducing FID scores from 1.79/1.58 to new records of 1.30/0.97 on CIFAR-10/ImageNet-64 datasets, and by consistently improving both guidance-free and CFG-enhanced FIDs of visual autoregressive models on ImageNet 256times256.', 'score': 2, 'issue_id': 2517, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': 'd8b58c1a2c49da16', 'authors': ['Kaiwen Zheng', 'Yongxin Chen', 'Huayu Chen', 'Guande He', 'Ming-Yu Liu', 'Jun Zhu', 'Qinsheng Zhang'], 'affiliations': ['NVIDIA', 'The University of Texas at', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.01103.jpg', 'data': {'categories': ['#training', '#diffusion', '#cv', '#optimization'], 'emoji': '🚀', 'ru': {'title': 'DDO: Прорыв в обучении генеративных моделей без ограничений MLE', 'desc': 'Авторы статьи предлагают новый метод обучения генеративных моделей под названием Direct Discriminative Optimization (DDO). Этот подход объединяет методы обучения на основе правдоподобия и цели генеративно-состязательных сетей (GAN), чтобы преодолеть ограничения метода максимального правдоподобия (MLE). DDO использует отношение правдоподобия между обучаемой целевой моделью и фиксированной эталонной моделью для параметризации дискриминатора. Эксперименты показывают, что DDO значительно улучшает результаты современных диффузионных и авторегрессионных моделей на различных наборах данных.'}, 'en': {'title': 'Enhancing Generative Models with Direct Discriminative Optimization', 'desc': 'This paper introduces Direct Discriminative Optimization (DDO), a new framework that enhances the performance of generative models by combining likelihood-based training with concepts from Generative Adversarial Networks (GANs). DDO addresses the limitations of maximum likelihood estimation (MLE) by using a discriminator that is parameterized through the likelihood ratio of a target model and a fixed reference model. This approach allows for efficient finetuning of pre-trained models without the need for joint training of generator and discriminator networks. The results show that DDO significantly improves the state-of-the-art performance in visual generation tasks, achieving lower FID scores on popular datasets like CIFAR-10 and ImageNet.'}, 'zh': {'title': '直接判别优化：提升生成模型的新方法', 'desc': '本文提出了一种新的方法，称为直接判别优化（DDO），旨在提高生成模型的质量。DDO通过将生成训练与GAN目标结合，克服了最大似然估计（MLE）在模型容量有限时的局限性。该方法通过使用可学习的目标模型与固定参考模型之间的似然比来隐式参数化判别器，从而简化了生成器和判别器的联合训练。实验结果表明，DDO显著提高了现有扩散模型的性能，并在多个数据集上创造了新的记录。'}}}, {'id': 'https://huggingface.co/papers/2502.16779', 'title': 'Unposed Sparse Views Room Layout Reconstruction in the Age of Pretrain Model', 'url': 'https://huggingface.co/papers/2502.16779', 'abstract': 'Room layout estimation from multiple-perspective images is poorly investigated due to the complexities that emerge from multi-view geometry, which requires muti-step solutions such as camera intrinsic and extrinsic estimation, image matching, and triangulation. However, in 3D reconstruction, the advancement of recent 3D foundation models such as DUSt3R has shifted the paradigm from the traditional multi-step structure-from-motion process to an end-to-end single-step approach. To this end, we introduce Plane-DUSt3R, a novel method for multi-view room layout estimation leveraging the 3D foundation model DUSt3R. Plane-DUSt3R incorporates the DUSt3R framework and fine-tunes on a room layout dataset (Structure3D) with a modified objective to estimate structural planes. By generating uniform and parsimonious results, Plane-DUSt3R enables room layout estimation with only a single post-processing step and 2D detection results. Unlike previous methods that rely on single-perspective or panorama image, Plane-DUSt3R extends the setting to handle multiple-perspective images. Moreover, it offers a streamlined, end-to-end solution that simplifies the process and reduces error accumulation. Experimental results demonstrate that Plane-DUSt3R not only outperforms state-of-the-art methods on the synthetic dataset but also proves robust and effective on in the wild data with different image styles such as cartoon.Our code is available at: https://github.com/justacar/Plane-DUSt3R', 'score': 2, 'issue_id': 2516, 'pub_date': '2025-02-24', 'pub_date_card': {'ru': '24 февраля', 'en': 'February 24', 'zh': '2月24日'}, 'hash': '4a9f6cc2fb1ab840', 'authors': ['Yaxuan Huang', 'Xili Dai', 'Jianan Wang', 'Xianbiao Qi', 'Yixing Yuan', 'Xiangyu Yue'], 'affiliations': ['Astribot', 'Hong Kong Center for Construction Robotics, The Hong Kong University of Science and Technology', 'Intellifusion Inc.', 'MMLab, The Chinese University of Hong Kong', 'The Hong Kong University of Science and Technology (Guangzhou)'], 'pdf_title_img': 'assets/pdf/title_img/2502.16779.jpg', 'data': {'categories': ['#3d', '#optimization', '#cv', '#synthetic'], 'emoji': '🏠', 'ru': {'title': 'Революция в оценке планировки помещений: от множества шагов к единому решению', 'desc': 'Статья представляет Plane-DUSt3R - новый метод для оценки планировки помещений по множественным ракурсам изображений. Этот подход использует 3D-модель фундаментального уровня DUSt3R и дообучается на наборе данных Structure3D для оценки структурных плоскостей. Plane-DUSt3R предлагает упрощенное сквозное решение, которое превосходит современные методы на синтетических данных и показывает надежность на реальных изображениях различных стилей. Метод позволяет оценивать планировку помещений с помощью одного шага постобработки и результатов 2D-обнаружения.'}, 'en': {'title': 'Revolutionizing Room Layout Estimation with Plane-DUSt3R', 'desc': 'This paper presents Plane-DUSt3R, a new method for estimating room layouts from multiple images taken from different perspectives. It builds on the DUSt3R 3D foundation model, moving away from traditional multi-step processes to a more efficient end-to-end approach. By fine-tuning the model on a specific dataset, Plane-DUSt3R can accurately identify structural planes with minimal post-processing. The results show that this method not only surpasses existing techniques on synthetic data but also performs well on real-world images with varying styles.'}, 'zh': {'title': '简化多视角房间布局估计的全新方法', 'desc': '本论文提出了一种新的多视角房间布局估计方法，称为Plane-DUSt3R。该方法利用了先进的3D基础模型DUSt3R，简化了传统的多步骤流程，采用端到端的单步骤方法。Plane-DUSt3R通过在房间布局数据集上进行微调，能够有效估计结构平面，并生成一致且简洁的结果。实验结果表明，Plane-DUSt3R在合成数据集上超越了现有的最先进方法，并在不同风格的真实数据上表现出色。'}}}, {'id': 'https://huggingface.co/papers/2503.00729', 'title': 'CLEA: Closed-Loop Embodied Agent for Enhancing Task Execution in Dynamic Environments', 'url': 'https://huggingface.co/papers/2503.00729', 'abstract': "Large Language Models (LLMs) exhibit remarkable capabilities in the hierarchical decomposition of complex tasks through semantic reasoning. However, their application in embodied systems faces challenges in ensuring reliable execution of subtask sequences and achieving one-shot success in long-term task completion. To address these limitations in dynamic environments, we propose Closed-Loop Embodied Agent (CLEA) -- a novel architecture incorporating four specialized open-source LLMs with functional decoupling for closed-loop task management. The framework features two core innovations: (1) Interactive task planner that dynamically generates executable subtasks based on the environmental memory, and (2) Multimodal execution critic employing an evaluation framework to conduct a probabilistic assessment of action feasibility, triggering hierarchical re-planning mechanisms when environmental perturbations exceed preset thresholds. To validate CLEA's effectiveness, we conduct experiments in a real environment with manipulable objects, using two heterogeneous robots for object search, manipulation, and search-manipulation integration tasks. Across 12 task trials, CLEA outperforms the baseline model, achieving a 67.3% improvement in success rate and a 52.8% increase in task completion rate. These results demonstrate that CLEA significantly enhances the robustness of task planning and execution in dynamic environments.", 'score': 2, 'issue_id': 2514, 'pub_date': '2025-03-02', 'pub_date_card': {'ru': '2 марта', 'en': 'March 2', 'zh': '3月2日'}, 'hash': '57f6361f66ec99cf', 'authors': ['Mingcong Lei', 'Ge Wang', 'Yiming Zhao', 'Zhixin Mai', 'Qing Zhao', 'Yao Guo', 'Zhen Li', 'Shuguang Cui', 'Yatong Han', 'Jinke Ren'], 'affiliations': ['Guangdong Provincial Key Laboratory of Future Networks of Intelligence, The Chinese University of Hong Kong, Shenzhen', 'Harbin Engineering University, Harbin', 'Infused Synapse AI, Shenzhen', 'Institute of Medical Robotics, School of Biomedical Engineering, Shanghai Jiao Tong University, Shanghai', 'School of Science and Engineering (SSE), FNii-Shenzhen', 'Shenzhen Future Network of Intelligence Institute (FNii-Shenzhen)'], 'pdf_title_img': 'assets/pdf/title_img/2503.00729.jpg', 'data': {'categories': ['#architecture', '#reasoning', '#open_source', '#robotics', '#agents', '#optimization'], 'emoji': '🤖', 'ru': {'title': 'CLEA: Повышение надежности выполнения задач роботами с помощью языковых моделей', 'desc': 'Статья представляет новую архитектуру под названием CLEA (Closed-Loop Embodied Agent) для улучшения выполнения сложных задач роботами в динамических средах. CLEA использует четыре специализированные языковые модели с открытым исходным кодом для управления задачами в замкнутом цикле. Ключевые инновации включают интерактивный планировщик задач и мультимодальный критик выполнения для оценки выполнимости действий. Эксперименты показали, что CLEA значительно превосходит базовую модель по показателям успешности и завершения задач в реальной среде с манипулируемыми объектами.'}, 'en': {'title': 'Enhancing Task Execution in Dynamic Environments with CLEA', 'desc': 'This paper introduces the Closed-Loop Embodied Agent (CLEA), a new architecture designed to improve the performance of Large Language Models (LLMs) in dynamic environments. CLEA features an interactive task planner that creates subtasks based on real-time environmental data, allowing for better adaptability. Additionally, it includes a multimodal execution critic that evaluates the feasibility of actions and adjusts plans when unexpected changes occur. Experimental results show that CLEA significantly enhances task success and completion rates compared to traditional models, demonstrating its effectiveness in complex, real-world scenarios.'}, 'zh': {'title': '闭环具身代理：提升动态环境中的任务执行能力', 'desc': '大型语言模型（LLMs）在复杂任务的层次分解和语义推理方面表现出色。然而，在具身系统中应用时，确保子任务序列的可靠执行和实现长期任务的一次性成功面临挑战。为了解决这些问题，我们提出了闭环具身代理（CLEA），这是一种新颖的架构，结合了四个专门的开源LLM，并实现功能解耦以进行闭环任务管理。通过动态生成可执行的子任务和使用多模态执行评估框架，CLEA显著提高了在动态环境中任务规划和执行的鲁棒性。'}}}, {'id': 'https://huggingface.co/papers/2502.20383', 'title': 'Why Are Web AI Agents More Vulnerable Than Standalone LLMs? A Security Analysis', 'url': 'https://huggingface.co/papers/2502.20383', 'abstract': 'Recent advancements in Web AI agents have demonstrated remarkable capabilities in addressing complex web navigation tasks. However, emerging research shows that these agents exhibit greater vulnerability compared to standalone Large Language Models (LLMs), despite both being built upon the same safety-aligned models. This discrepancy is particularly concerning given the greater flexibility of Web AI Agent compared to standalone LLMs, which may expose them to a wider range of adversarial user inputs. To build a scaffold that addresses these concerns, this study investigates the underlying factors that contribute to the increased vulnerability of Web AI agents. Notably, this disparity stems from the multifaceted differences between Web AI agents and standalone LLMs, as well as the complex signals - nuances that simple evaluation metrics, such as success rate, often fail to capture. To tackle these challenges, we propose a component-level analysis and a more granular, systematic evaluation framework. Through this fine-grained investigation, we identify three critical factors that amplify the vulnerability of Web AI agents; (1) embedding user goals into the system prompt, (2) multi-step action generation, and (3) observational capabilities. Our findings highlights the pressing need to enhance security and robustness in AI agent design and provide actionable insights for targeted defense strategies.', 'score': 1, 'issue_id': 2522, 'pub_date': '2025-02-27', 'pub_date_card': {'ru': '27 февраля', 'en': 'February 27', 'zh': '2月27日'}, 'hash': '12b0a9a60578dc2b', 'authors': ['Jeffrey Yang Fan Chiang', 'Seungjae Lee', 'Jia-Bin Huang', 'Furong Huang', 'Yizheng Chen'], 'affiliations': ['University of Maryland'], 'pdf_title_img': 'assets/pdf/title_img/2502.20383.jpg', 'data': {'categories': ['#benchmark', '#security', '#agents'], 'emoji': '🕸️', 'ru': {'title': 'Раскрытие уязвимостей веб-агентов ИИ: путь к более безопасным системам', 'desc': 'Исследование показывает, что веб-агенты на основе искусственного интеллекта более уязвимы, чем автономные большие языковые модели (LLM), несмотря на использование одинаковых базовых моделей. Авторы предлагают компонентный анализ и более детальную систему оценки для выявления причин этой уязвимости. Выделены три ключевых фактора, усиливающих уязвимость веб-агентов: встраивание целей пользователя в системный промпт, генерация многошаговых действий и возможности наблюдения. Результаты исследования подчеркивают необходимость улучшения безопасности и устойчивости при разработке ИИ-агентов.'}, 'en': {'title': 'Strengthening Web AI Agents Against Vulnerabilities', 'desc': 'This paper explores the vulnerabilities of Web AI agents compared to standalone Large Language Models (LLMs), despite both being based on similar safety models. The research identifies that Web AI agents are more susceptible to adversarial inputs due to their flexibility and the complexity of their tasks. It highlights three key factors that increase their vulnerability: the integration of user goals into prompts, the generation of multi-step actions, and the need for observational capabilities. The study proposes a detailed evaluation framework to better understand these vulnerabilities and suggests strategies for improving the security and robustness of AI agents.'}, 'zh': {'title': '提升网络人工智能代理的安全性与鲁棒性', 'desc': '最近，网络人工智能代理在处理复杂的网络导航任务方面表现出色。然而，研究表明，这些代理比独立的大型语言模型（LLMs）更容易受到攻击，尽管它们都是基于相同的安全模型构建的。这种差异令人担忧，因为网络人工智能代理的灵活性更高，可能会面临更广泛的恶意用户输入。为了解决这些问题，本研究探讨了导致网络人工智能代理脆弱性的因素，并提出了一种更细致的评估框架，以识别和应对这些挑战。'}}}, {'id': 'https://huggingface.co/papers/2503.01063', 'title': 'AI-Invented Tonal Languages: Preventing a Machine Lingua Franca Beyond Human Understanding', 'url': 'https://huggingface.co/papers/2503.01063', 'abstract': 'This paper investigates the potential for large language models (LLMs) to develop private tonal languages for machine-to-machine (M2M) communication. Inspired by cryptophasia in human twins (affecting up to 50% of twin births) and natural tonal languages like Mandarin and Vietnamese, we implement a precise character-to-frequency mapping system that encodes the full ASCII character set (32-126) using musical semitones. Each character is assigned a unique frequency, creating a logarithmic progression beginning with space (220 Hz) and ending with tilde (50,175.42 Hz). This spans approximately 7.9 octaves, with higher characters deliberately mapped to ultrasonic frequencies beyond human perception (>20 kHz). Our implemented software prototype demonstrates this encoding through visualization, auditory playback, and ABC musical notation, allowing for analysis of information density and transmission speed. Testing reveals that tonal encoding can achieve information rates exceeding human speech while operating partially outside human perceptual boundaries. This work responds directly to concerns about AI systems catastrophically developing private languages within the next five years, providing a concrete prototype software example of how such communication might function and the technical foundation required for its emergence, detection, and governance.', 'score': 1, 'issue_id': 2515, 'pub_date': '2025-03-02', 'pub_date_card': {'ru': '2 марта', 'en': 'March 2', 'zh': '3月2日'}, 'hash': '7021403742a91f3e', 'authors': ['David Noever'], 'affiliations': ['PeopleTec, Inc., Huntsville, AL'], 'pdf_title_img': 'assets/pdf/title_img/2503.01063.jpg', 'data': {'categories': ['#security', '#ethics', '#audio', '#multimodal'], 'emoji': '🎵', 'ru': {'title': 'Тональные языки: секретный код машин будущего', 'desc': 'Это исследование изучает потенциал больших языковых моделей (LLM) для разработки приватных тональных языков для коммуникации между машинами. Авторы создали систему кодирования, которая сопоставляет каждому символу ASCII уникальную частоту, формируя логарифмическую прогрессию от 220 Гц до 50,175.42 Гц. Разработанный программный прототип демонстрирует это кодирование через визуализацию, воспроизведение звука и нотацию ABC. Тестирование показало, что тональное кодирование может достигать скорости передачи информации, превышающей человеческую речь, при этом частично работая за пределами человеческого восприятия.'}, 'en': {'title': 'Unlocking Machine Communication with Tonal Languages', 'desc': 'This paper explores how large language models (LLMs) can create private tonal languages for communication between machines. It draws inspiration from the phenomenon of cryptophasia in twins and uses a character-to-frequency mapping system to encode ASCII characters into musical tones. Each character is assigned a unique frequency, allowing for efficient data transmission that exceeds human speech rates. The study provides a prototype that visualizes and plays back this encoding, addressing concerns about AI developing private languages and offering a framework for understanding and managing such systems.'}, 'zh': {'title': '探索机器间的私有音调语言', 'desc': '本论文研究了大型语言模型（LLMs）在机器间（M2M）通信中开发私有音调语言的潜力。我们借鉴了人类双胞胎中的密码语言现象和自然音调语言，如普通话和越南语，实施了一种精确的字符到频率映射系统。每个字符被分配一个独特的频率，形成一个对数进程，覆盖约7.9个八度，并将高频字符映射到人类听觉范围之外的超声波频率。我们的软件原型展示了这种编码的可视化、听觉播放和音乐记谱法，分析了信息密度和传输速度，测试结果表明音调编码的信息传输速率超过人类语言。'}}}, {'id': 'https://huggingface.co/papers/2503.00865', 'title': 'Babel: Open Multilingual Large Language Models Serving Over 90% of Global Speakers', 'url': 'https://huggingface.co/papers/2503.00865', 'abstract': "Large language models (LLMs) have revolutionized natural language processing (NLP), yet open-source multilingual LLMs remain scarce, with existing models often limited in language coverage. Such models typically prioritize well-resourced languages, while widely spoken but under-resourced languages are often overlooked. To address this disparity, we introduce Babel, an open multilingual LLM that covers the top 25 languages by number of speakers, supports over 90% of the global population, and includes many languages neglected by other open multilingual LLMs. Unlike traditional continue pretraining approaches, Babel expands its parameter count through a layer extension technique that elevates Babel's performance ceiling. We introduce two variants: Babel-9B, designed for efficient inference and fine-tuning, and Babel-83B, which sets a new standard for open multilingual LLMs. Extensive evaluations on multilingual tasks demonstrate its superior performance compared to open LLMs of comparable size. In addition, using open-source supervised fine-tuning datasets, Babel achieves remarkable performance, with Babel-9B-Chat leading among 10B-sized LLMs and Babel-83B-Chat setting a new standard for multilingual tasks, reaching the same level of commercial models.", 'score': 40, 'issue_id': 2555, 'pub_date': '2025-03-02', 'pub_date_card': {'ru': '2 марта', 'en': 'March 2', 'zh': '3月2日'}, 'hash': 'bc2424e709a2dd78', 'authors': ['Yiran Zhao', 'Chaoqun Liu', 'Yue Deng', 'Jiahao Ying', 'Mahani Aljunied', 'Zhaodonghui Li', 'Lidong Bing', 'Hou Pong Chan', 'Yu Rong', 'Deli Zhao', 'Wenxuan Zhang'], 'affiliations': ['DAMO Academy, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2503.00865.jpg', 'data': {'categories': ['#low_resource', '#architecture', '#open_source', '#training', '#multilingual'], 'emoji': '🌍', 'ru': {'title': 'Babel: революция в многоязычном машинном обучении', 'desc': 'Представлена новая многоязычная языковая модель Babel, охватывающая 25 самых распространенных языков мира. Модель использует технику расширения слоев для улучшения производительности. Предложены две версии: Babel-9B для эффективного вывода и дообучения, и Babel-83B, устанавливающая новый стандарт для открытых многоязычных моделей. Обе версии показывают превосходные результаты в многоязычных задачах по сравнению с аналогичными открытыми моделями.'}, 'en': {'title': 'Babel: Bridging the Language Gap with Open Multilingual LLMs', 'desc': "This paper presents Babel, an innovative open-source multilingual large language model (LLM) that addresses the lack of coverage for under-resourced languages in existing models. Babel supports the top 25 languages spoken globally, reaching over 90% of the world's population, and includes many languages that are often neglected. The model employs a unique layer extension technique to increase its parameter count, enhancing its performance beyond traditional pretraining methods. With two variants, Babel-9B and Babel-83B, the model demonstrates superior performance on multilingual tasks, outperforming other open LLMs of similar size and achieving results comparable to commercial models."}, 'zh': {'title': 'Babel：打破语言壁垒的多语言模型', 'desc': '大型语言模型（LLMs）在自然语言处理（NLP）领域带来了革命性的变化，但开源的多语言LLMs仍然稀缺，现有模型通常在语言覆盖上有限。许多模型优先考虑资源丰富的语言，而广泛使用但资源不足的语言常常被忽视。为了解决这一差距，我们推出了Babel，一个开放的多语言LLM，覆盖全球前25种语言，支持超过90%的人口，并包括许多其他开源多语言LLMs忽视的语言。Babel通过层扩展技术增加参数数量，提升了性能，并推出了两个变体：Babel-9B和Babel-83B，后者在多语言任务中设定了新的标准。'}}}, {'id': 'https://huggingface.co/papers/2503.03746', 'title': 'Process-based Self-Rewarding Language Models', 'url': 'https://huggingface.co/papers/2503.03746', 'abstract': "Large Language Models have demonstrated outstanding performance across various downstream tasks and have been widely applied in multiple scenarios. Human-annotated preference data is used for training to further improve LLMs' performance, which is constrained by the upper limit of human performance. Therefore, Self-Rewarding method has been proposed, where LLMs generate training data by rewarding their own outputs. However, the existing self-rewarding paradigm is not effective in mathematical reasoning scenarios and may even lead to a decline in performance. In this work, we propose the Process-based Self-Rewarding pipeline for language models, which introduces long-thought reasoning, step-wise LLM-as-a-Judge, and step-wise preference optimization within the self-rewarding paradigm. Our new paradigm successfully enhances the performance of LLMs on multiple mathematical reasoning benchmarks through iterative Process-based Self-Rewarding, demonstrating the immense potential of self-rewarding to achieve LLM reasoning that may surpass human capabilities.", 'score': 21, 'issue_id': 2564, 'pub_date': '2025-03-05', 'pub_date_card': {'ru': '5 марта', 'en': 'March 5', 'zh': '3月5日'}, 'hash': '808bee960390ec29', 'authors': ['Shimao Zhang', 'Xiao Liu', 'Xin Zhang', 'Junxiao Liu', 'Zheheng Luo', 'Shujian Huang', 'Yeyun Gong'], 'affiliations': ['Microsoft Research Asia', 'National Key Laboratory for Novel Software Technology, Nanjing University', 'The University of Manchester'], 'pdf_title_img': 'assets/pdf/title_img/2503.03746.jpg', 'data': {'categories': ['#math', '#alignment', '#rlhf', '#reasoning', '#training'], 'emoji': '🧮', 'ru': {'title': 'Самообучение ИИ математике: шаг за шагом к сверхчеловеческим способностям', 'desc': 'Статья представляет новый метод самообучения языковых моделей для задач математических рассуждений. Авторы предлагают подход Process-based Self-Rewarding, который включает пошаговое рассуждение и оценку промежуточных результатов самой моделью. Этот метод позволяет преодолеть ограничения существующих подходов к самообучению в математических задачах. Эксперименты показывают, что новый метод значительно улучшает способности языковых моделей к математическим рассуждениям на различных тестовых наборах.'}, 'en': {'title': 'Empowering LLMs with Process-based Self-Rewarding for Superior Reasoning', 'desc': 'This paper discusses the limitations of current self-rewarding methods used to train Large Language Models (LLMs), particularly in mathematical reasoning tasks. The authors introduce a new approach called Process-based Self-Rewarding, which incorporates long-thought reasoning and a step-wise evaluation process. By allowing LLMs to act as judges of their own outputs, this method optimizes the training process iteratively. The results show significant improvements in LLM performance on mathematical reasoning benchmarks, suggesting that self-rewarding can enhance reasoning capabilities beyond human levels.'}, 'zh': {'title': '基于过程的自我奖励：超越人类的推理能力', 'desc': '大型语言模型在各种下游任务中表现出色，并广泛应用于多个场景。为了进一步提高其性能，研究者使用人类标注的偏好数据进行训练，但这受到人类表现上限的限制。因此，提出了自我奖励的方法，让语言模型通过奖励自己的输出生成训练数据。然而，现有的自我奖励方法在数学推理场景中效果不佳，甚至可能导致性能下降。本文提出了一种基于过程的自我奖励管道，通过引入长时间思考推理、逐步的语言模型评判和逐步的偏好优化，成功提升了语言模型在多个数学推理基准上的表现，展示了自我奖励在超越人类能力的推理中的巨大潜力。'}}}, {'id': 'https://huggingface.co/papers/2503.00329', 'title': 'ABC: Achieving Better Control of Multimodal Embeddings using VLMs', 'url': 'https://huggingface.co/papers/2503.00329', 'abstract': 'Visual embedding models excel at zero-shot tasks like visual retrieval and classification. However, these models cannot be used for tasks that contain ambiguity or require user instruction. These tasks necessitate a multimodal embedding model, which outputs embeddings that combine visual and natural language input. Existing CLIP-based approaches embed images and text independently, and fuse the result. We find that this results in weak interactions between modalities, and poor user control over the representation. We introduce ABC, an open-source multimodal embedding model that uses a vision-language model backbone to deeply integrate image features with natural language instructions. ABC achieves bestfor-size performance on MSCOCO image-to-text retrieval and is the top performing model on classification and VQA tasks in the Massive Multimodal Embedding Benchmark. With a strongly unified vision-language representation, ABC can use natural language to solve subtle and potentially ambiguous visual retrieval problems. To evaluate this capability, we design CtrlBench, a benchmark that requires interleaving textual instructions with image content for correct retrieval. ABC advances the state of multimodal embeddings by offering high-quality representations and flexible natural language control. Our model and datasets are available at our project page.', 'score': 14, 'issue_id': 2555, 'pub_date': '2025-03-01', 'pub_date_card': {'ru': '1 марта', 'en': 'March 1', 'zh': '3月1日'}, 'hash': '0483c542c8885777', 'authors': ['Benjamin Schneider', 'Florian Kerschbaum', 'Wenhu Chen'], 'affiliations': ['Cheriton School of Computer Science, University of Waterloo', 'Vector Institute, Toronto'], 'pdf_title_img': 'assets/pdf/title_img/2503.00329.jpg', 'data': {'categories': ['#benchmark', '#multimodal', '#open_source', '#dataset'], 'emoji': '🧠', 'ru': {'title': 'ABC: Мультимодальные встраивания с гибким языковым контролем', 'desc': 'Статья представляет новую мультимодальную модель встраивания под названием ABC, которая объединяет визуальные и текстовые данные. В отличие от существующих подходов, ABC использует глубокую интеграцию изображений и естественного языка. Модель демонстрирует высокую производительность в задачах поиска изображений по тексту и классификации. ABC также позволяет использовать естественный язык для решения сложных задач визуального поиска с неоднозначностями.'}, 'en': {'title': 'ABC: Unifying Vision and Language for Enhanced Multimodal Understanding', 'desc': "This paper presents ABC, a new multimodal embedding model that integrates visual and natural language inputs more effectively than existing CLIP-based methods. Unlike previous models that treat images and text separately, ABC combines these modalities deeply, allowing for better interaction and user control. The model excels in zero-shot tasks, particularly in image-to-text retrieval and classification, outperforming others in the Massive Multimodal Embedding Benchmark. To assess its capabilities, the authors introduce CtrlBench, a benchmark designed to evaluate the model's performance in handling complex visual retrieval tasks with natural language instructions."}, 'zh': {'title': 'ABC：多模态嵌入的新突破', 'desc': '这篇论文介绍了一种名为ABC的多模态嵌入模型，旨在解决视觉检索和分类中的模糊性问题。与现有的CLIP方法不同，ABC通过深度整合图像特征和自然语言指令，提供更强的模态交互。ABC在MSCOCO图像到文本检索任务中表现出色，并在分类和视觉问答任务中取得了最佳性能。通过设计CtrlBench基准，评估了ABC在处理复杂视觉检索问题时的能力，展示了其高质量的表示和灵活的自然语言控制。'}}}, {'id': 'https://huggingface.co/papers/2503.03751', 'title': 'GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control', 'url': 'https://huggingface.co/papers/2503.03751', 'abstract': 'We present GEN3C, a generative video model with precise Camera Control and temporal 3D Consistency. Prior video models already generate realistic videos, but they tend to leverage little 3D information, leading to inconsistencies, such as objects popping in and out of existence. Camera control, if implemented at all, is imprecise, because camera parameters are mere inputs to the neural network which must then infer how the video depends on the camera. In contrast, GEN3C is guided by a 3D cache: point clouds obtained by predicting the pixel-wise depth of seed images or previously generated frames. When generating the next frames, GEN3C is conditioned on the 2D renderings of the 3D cache with the new camera trajectory provided by the user. Crucially, this means that GEN3C neither has to remember what it previously generated nor does it have to infer the image structure from the camera pose. The model, instead, can focus all its generative power on previously unobserved regions, as well as advancing the scene state to the next frame. Our results demonstrate more precise camera control than prior work, as well as state-of-the-art results in sparse-view novel view synthesis, even in challenging settings such as driving scenes and monocular dynamic video. Results are best viewed in videos. Check out our webpage! https://research.nvidia.com/labs/toronto-ai/GEN3C/', 'score': 13, 'issue_id': 2555, 'pub_date': '2025-03-05', 'pub_date_card': {'ru': '5 марта', 'en': 'March 5', 'zh': '3月5日'}, 'hash': '8f5f2ad910a260c0', 'authors': ['Xuanchi Ren', 'Tianchang Shen', 'Jiahui Huang', 'Huan Ling', 'Yifan Lu', 'Merlin Nimier-David', 'Thomas Müller', 'Alexander Keller', 'Sanja Fidler', 'Jun Gao'], 'affiliations': ['NVIDIA', 'University of Toronto', 'Vector Institute'], 'pdf_title_img': 'assets/pdf/title_img/2503.03751.jpg', 'data': {'categories': ['#3d', '#video'], 'emoji': '🎥', 'ru': {'title': 'Точный контроль камеры и 3D-согласованность в генерации видео', 'desc': 'GEN3C - это генеративная модель видео с точным контролем камеры и временной 3D-согласованностью. Она использует 3D-кэш в виде облаков точек, полученных из глубинных карт исходных изображений или ранее сгенерированных кадров. При генерации следующих кадров GEN3C опирается на 2D-рендеринг 3D-кэша с новой траекторией камеры, заданной пользователем. Это позволяет модели сфокусироваться на ранее ненаблюдаемых областях и продвижении состояния сцены, не тратя ресурсы на запоминание предыдущих результатов или вывод структуры изображения из положения камеры.'}, 'en': {'title': 'GEN3C: Mastering Video Generation with 3D Precision and Camera Control', 'desc': 'GEN3C is a generative video model that enhances video generation by incorporating precise camera control and maintaining temporal 3D consistency. Unlike previous models that often lack 3D information, GEN3C utilizes a 3D cache of point clouds derived from depth predictions, allowing for more coherent object presence in videos. The model is conditioned on 2D renderings from this cache, enabling it to generate new frames without needing to remember past outputs or infer scene structure from camera angles. This approach results in superior camera control and state-of-the-art performance in generating novel views, particularly in complex scenarios like driving scenes.'}, 'zh': {'title': 'GEN3C：精确相机控制与时间一致性的视频生成模型', 'desc': '我们提出了GEN3C，这是一种具有精确相机控制和时间一致性的生成视频模型。以往的视频模型虽然能够生成逼真的视频，但往往缺乏3D信息，导致物体出现和消失的不一致性。GEN3C通过3D缓存来指导生成过程，利用从种子图像或先前生成帧中预测的像素深度获得的点云。这样，GEN3C能够在用户提供的新相机轨迹下，专注于生成未观察到的区域，并有效推进场景状态到下一个帧。'}}}, {'id': 'https://huggingface.co/papers/2503.02951', 'title': 'KodCode: A Diverse, Challenging, and Verifiable Synthetic Dataset for Coding', 'url': 'https://huggingface.co/papers/2503.02951', 'abstract': 'We introduce KodCode, a synthetic dataset that addresses the persistent challenge of acquiring high-quality, verifiable training data across diverse difficulties and domains for training Large Language Models for coding. Existing code-focused resources typically fail to ensure either the breadth of coverage (e.g., spanning simple coding tasks to advanced algorithmic problems) or verifiable correctness (e.g., unit tests). In contrast, KodCode comprises question-solution-test triplets that are systematically validated via a self-verification procedure. Our pipeline begins by synthesizing a broad range of coding questions, then generates solutions and test cases with additional attempts allocated to challenging problems. Finally, post-training data synthesis is done by rewriting questions into diverse formats and generating responses under a test-based reject sampling procedure from a reasoning model (DeepSeek R1). This pipeline yields a large-scale, robust and diverse coding dataset. KodCode is suitable for supervised fine-tuning and the paired unit tests also provide great potential for RL tuning. Fine-tuning experiments on coding benchmarks (HumanEval(+), MBPP(+), BigCodeBench, and LiveCodeBench) demonstrate that KodCode-tuned models achieve state-of-the-art performance, surpassing models like Qwen2.5-Coder-32B-Instruct and DeepSeek-R1-Distill-Llama-70B.', 'score': 12, 'issue_id': 2555, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 марта', 'en': 'March 4', 'zh': '3月4日'}, 'hash': '6c344ba0bf71ac84', 'authors': ['Zhangchen Xu', 'Yang Liu', 'Yueqin Yin', 'Mingyuan Zhou', 'Radha Poovendran'], 'affiliations': ['Microsoft', 'The University of Texas at Austin', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2503.02951.jpg', 'data': {'categories': ['#dataset', '#rl', '#optimization', '#synthetic', '#training'], 'emoji': '🧑\u200d💻', 'ru': {'title': 'KodCode: Синтетические данные для обучения ИИ программированию', 'desc': 'KodCode - это синтетический набор данных для обучения больших языковых моделей программированию. Он состоит из триплетов вопрос-решение-тест, которые проходят процедуру самопроверки. Процесс создания KodCode включает синтез вопросов по программированию, генерацию решений и тестовых случаев, а также постобработку данных. Эксперименты показывают, что модели, обученные на KodCode, достигают наилучших результатов на различных бенчмарках по программированию.'}, 'en': {'title': 'KodCode: Elevating Coding Models with Verified Data', 'desc': 'KodCode is a synthetic dataset designed to improve the training of Large Language Models (LLMs) for coding tasks by providing high-quality, verifiable data. It includes question-solution-test triplets that are validated through a self-verification process, ensuring both correctness and a wide range of coding difficulties. The dataset is generated using a systematic pipeline that synthesizes coding questions, creates solutions, and develops test cases, particularly focusing on challenging problems. Fine-tuning experiments show that models trained on KodCode outperform existing models on various coding benchmarks, demonstrating its effectiveness in enhancing LLM performance.'}, 'zh': {'title': 'KodCode：高质量编码数据集的解决方案', 'desc': '我们介绍了KodCode，这是一个合成数据集，旨在解决获取高质量、可验证的训练数据的挑战，以训练大型语言模型进行编码。现有的代码资源通常无法确保覆盖范围广泛或正确性可验证。KodCode由问题-解决方案-测试三元组组成，通过自我验证程序系统地验证。我们的流程包括合成各种编码问题，生成解决方案和测试用例，并在后期通过重写问题和生成响应来进行数据合成，最终生成一个大规模、强大且多样化的编码数据集。'}}}, {'id': 'https://huggingface.co/papers/2503.03278', 'title': 'Enhancing Abnormality Grounding for Vision Language Models with Knowledge Descriptions', 'url': 'https://huggingface.co/papers/2503.03278', 'abstract': 'Visual Language Models (VLMs) have demonstrated impressive capabilities in visual grounding tasks. However, their effectiveness in the medical domain, particularly for abnormality detection and localization within medical images, remains underexplored. A major challenge is the complex and abstract nature of medical terminology, which makes it difficult to directly associate pathological anomaly terms with their corresponding visual features. In this work, we introduce a novel approach to enhance VLM performance in medical abnormality detection and localization by leveraging decomposed medical knowledge. Instead of directly prompting models to recognize specific abnormalities, we focus on breaking down medical concepts into fundamental attributes and common visual patterns. This strategy promotes a stronger alignment between textual descriptions and visual features, improving both the recognition and localization of abnormalities in medical images.We evaluate our method on the 0.23B Florence-2 base model and demonstrate that it achieves comparable performance in abnormality grounding to significantly larger 7B LLaVA-based medical VLMs, despite being trained on only 1.5% of the data used for such models. Experimental results also demonstrate the effectiveness of our approach in both known and previously unseen abnormalities, suggesting its strong generalization capabilities.', 'score': 10, 'issue_id': 2560, 'pub_date': '2025-03-05', 'pub_date_card': {'ru': '5 марта', 'en': 'March 5', 'zh': '3月5日'}, 'hash': '6103dbe5d60b5f3f', 'authors': ['Jun Li', 'Che Liu', 'Wenjia Bai', 'Rossella Arcucci', 'Cosmin I. Bercea', 'Julia A. Schnabel'], 'affiliations': ['Helmholtz AI and Helmholtz Munich, Germany', 'Imperial College London, UK', 'Kings College London, UK', 'Munich Center for Machine Learning, Germany', 'Technical University of Munich, Germany'], 'pdf_title_img': 'assets/pdf/title_img/2503.03278.jpg', 'data': {'categories': ['#cv', '#multimodal', '#healthcare', '#alignment', '#transfer_learning'], 'emoji': '🔬', 'ru': {'title': 'Декомпозиция медицинских знаний для повышения эффективности VLM в анализе медицинских изображений', 'desc': 'Эта статья представляет новый подход к улучшению работы визуальных языковых моделей (VLM) в обнаружении и локализации аномалий на медицинских изображениях. Вместо прямого распознавания конкретных патологий, метод фокусируется на разложении медицинских концепций на базовые атрибуты и общие визуальные паттерны. Это улучшает связь между текстовыми описаниями и визуальными характеристиками, повышая точность распознавания и локализации аномалий. Метод был протестирован на модели Florence-2 и показал результаты, сравнимые с гораздо более крупными медицинскими VLM, несмотря на использование значительно меньшего объема данных для обучения.'}, 'en': {'title': 'Enhancing Medical VLMs through Decomposed Knowledge', 'desc': 'This paper presents a new method to improve Visual Language Models (VLMs) for detecting and locating abnormalities in medical images. The authors address the challenge of complex medical terminology by breaking down medical concepts into simpler attributes and common visual patterns. This approach enhances the alignment between text descriptions and visual features, leading to better performance in recognizing and localizing abnormalities. The proposed method shows competitive results with larger models while using significantly less training data, indicating its efficiency and strong generalization capabilities.'}, 'zh': {'title': '分解医学知识，提升视觉语言模型的异常检测能力', 'desc': '视觉语言模型（VLMs）在视觉定位任务中表现出色，但在医学领域，尤其是医学图像中的异常检测和定位方面，仍然缺乏研究。医学术语的复杂性使得将病理异常术语与相应的视觉特征直接关联变得困难。我们提出了一种新方法，通过分解医学知识来增强VLM在医学异常检测和定位中的性能。该方法通过将医学概念分解为基本属性和常见视觉模式，促进了文本描述与视觉特征之间的更强对齐，从而提高了医学图像中异常的识别和定位能力。'}}}, {'id': 'https://huggingface.co/papers/2503.01836', 'title': 'CrowdSelect: Synthetic Instruction Data Selection with Multi-LLM Wisdom', 'url': 'https://huggingface.co/papers/2503.01836', 'abstract': "Distilling advanced Large Language Models' instruction-following capabilities into smaller models using a selected subset has become a mainstream approach in model training. While existing synthetic instruction data selection strategies rely mainly on single-dimensional signals (i.e., reward scores, model perplexity), they fail to capture the complexity of instruction-following across diverse fields. Therefore, we investigate more diverse signals to capture comprehensive instruction-response pair characteristics and propose three foundational metrics that leverage Multi-LLM wisdom, informed by (1) diverse LLM responses and (2) reward model assessment. Building upon base metrics, we propose CrowdSelect, an integrated metric incorporating a clustering-based approach to maintain response diversity. Our comprehensive experiments demonstrate that our foundation metrics consistently improve performance across 4 base models on MT-bench and Arena-Hard. CrowdSelect, efficiently incorporating all metrics, achieves state-of-the-art performance in both Full and LoRA fine-tuning, showing improvements of 4.81% on Arena-Hard and 11.1% on MT-bench with Llama-3.2-3b-instruct. We hope our findings will bring valuable insights for future research in this direction. Code are available at https://github.com/listentm/crowdselect.", 'score': 9, 'issue_id': 2560, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': 'd59d65fb3b60c043', 'authors': ['Yisen Li', 'Lingfeng Yang', 'Wenxuan Shen', 'Pan Zhou', 'Yao Wan', 'Weiwei Lin', 'Dongping Chen'], 'affiliations': ['Huazhong University of Science and Technology', 'South China University of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2503.01836.jpg', 'data': {'categories': ['#small_models', '#training', '#synthetic', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'CrowdSelect: умный отбор инструкций для обучения языковых моделей', 'desc': 'Статья описывает новый метод отбора инструкций для обучения языковых моделей, названный CrowdSelect. Он использует три основных метрики, основанные на оценках различных большиx языковых моделей и моделей вознаграждения. CrowdSelect также включает кластеризацию для сохранения разнообразия ответов. Эксперименты показали, что этот метод превосходит существующие подходы на бенчмарках MT-bench и Arena-Hard. Авторы надеются, что их исследование внесет вклад в развитие этого направления.'}, 'en': {'title': 'Enhancing Model Training with Diverse Instruction Metrics', 'desc': 'This paper focuses on improving the training of smaller models by distilling the instruction-following abilities of larger language models (LLMs). It critiques existing methods that use simple metrics for selecting synthetic instruction data, which do not adequately reflect the complexity of instruction-following tasks. The authors propose new metrics that utilize diverse responses from multiple LLMs and a reward model to better assess instruction-response pairs. Their method, CrowdSelect, combines these metrics with a clustering approach to enhance response diversity, leading to significant performance improvements in various model evaluations.'}, 'zh': {'title': '提升小模型的指令跟随能力', 'desc': '本论文探讨了如何将大型语言模型的指令跟随能力提炼到更小的模型中。现有的合成指令数据选择策略主要依赖单一维度的信号，未能全面捕捉指令跟随的复杂性。我们提出了三种基础指标，利用多种大型语言模型的智慧，结合多样的响应和奖励模型评估。通过综合实验，我们的CrowdSelect指标在多个基模型上表现出色，显著提升了性能，展示了未来研究的潜力。'}}}, {'id': 'https://huggingface.co/papers/2503.01933', 'title': 'Fine-Tuning Small Language Models for Domain-Specific AI: An Edge AI Perspective', 'url': 'https://huggingface.co/papers/2503.01933', 'abstract': 'Deploying large scale language models on edge devices faces inherent challenges such as high computational demands, energy consumption, and potential data privacy risks. This paper introduces the Shakti Small Language Models (SLMs) Shakti-100M, Shakti-250M, and Shakti-500M which target these constraints headon. By combining efficient architectures, quantization techniques, and responsible AI principles, the Shakti series enables on-device intelligence for smartphones, smart appliances, IoT systems, and beyond. We provide comprehensive insights into their design philosophy, training pipelines, and benchmark performance on both general tasks (e.g., MMLU, Hellaswag) and specialized domains (healthcare, finance, and legal). Our findings illustrate that compact models, when carefully engineered and fine-tuned, can meet and often exceed expectations in real-world edge-AI scenarios.', 'score': 6, 'issue_id': 2563, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': '6fa74210552cc49f', 'authors': ['Rakshit Aralimatti', 'Syed Abdul Gaffar Shakhadri', 'Kruthika KR', 'Kartik Basavaraj Angadi'], 'affiliations': ['SandLogic Technologies Pvt Ltd'], 'pdf_title_img': 'assets/pdf/title_img/2503.01933.jpg', 'data': {'categories': ['#healthcare', '#benchmark', '#training', '#ethics', '#inference', '#small_models', '#optimization'], 'emoji': '📱', 'ru': {'title': 'Малые языковые модели для большого интеллекта на краю сети', 'desc': 'Статья представляет серию малых языковых моделей Shakti, разработанных для применения на периферийных устройствах. Модели Shakti-100M, Shakti-250M и Shakti-500M решают проблемы высоких вычислительных требований, энергопотребления и потенциальных рисков конфиденциальности данных. Используя эффективные архитектуры, методы квантования и принципы ответственного ИИ, серия Shakti обеспечивает локальный интеллект для смартфонов, умных устройств и IoT-систем. Исследование показывает, что компактные модели, при тщательной разработке и настройке, могут соответствовать и часто превосходить ожидания в реальных сценариях периферийного ИИ.'}, 'en': {'title': 'Empowering Edge Devices with Efficient Language Models', 'desc': 'This paper presents the Shakti Small Language Models (SLMs) designed to operate efficiently on edge devices while addressing challenges like high computational needs and energy consumption. The Shakti models, including Shakti-100M, Shakti-250M, and Shakti-500M, utilize advanced architectures and quantization techniques to optimize performance without compromising data privacy. The authors detail the design philosophy, training processes, and benchmark results across various tasks and specialized fields such as healthcare and finance. The results demonstrate that well-engineered compact models can perform effectively in real-world applications, showcasing the potential of on-device AI.'}, 'zh': {'title': '小型语言模型，智能边缘计算的未来', 'desc': '本论文介绍了Shakti小型语言模型（SLMs），包括Shakti-100M、Shakti-250M和Shakti-500M，旨在解决在边缘设备上部署大型语言模型时面临的高计算需求和能耗问题。通过结合高效架构、量化技术和负责任的人工智能原则，Shakti系列实现了智能手机、智能家电和物联网系统的本地智能。我们提供了关于其设计理念、训练流程和在一般任务（如MMLU、Hellaswag）及专业领域（医疗、金融和法律）上的基准性能的全面见解。研究结果表明，经过精心设计和微调的紧凑模型能够在实际边缘人工智能场景中满足并超越预期。'}}}, {'id': 'https://huggingface.co/papers/2502.20317', 'title': 'Mixture of Structural-and-Textual Retrieval over Text-rich Graph Knowledge Bases', 'url': 'https://huggingface.co/papers/2502.20317', 'abstract': 'Text-rich Graph Knowledge Bases (TG-KBs) have become increasingly crucial for answering queries by providing textual and structural knowledge. However, current retrieval methods often retrieve these two types of knowledge in isolation without considering their mutual reinforcement and some hybrid methods even bypass structural retrieval entirely after neighboring aggregation. To fill in this gap, we propose a Mixture of Structural-and-Textual Retrieval (MoR) to retrieve these two types of knowledge via a Planning-Reasoning-Organizing framework. In the Planning stage, MoR generates textual planning graphs delineating the logic for answering queries. Following planning graphs, in the Reasoning stage, MoR interweaves structural traversal and textual matching to obtain candidates from TG-KBs. In the Organizing stage, MoR further reranks fetched candidates based on their structural trajectory. Extensive experiments demonstrate the superiority of MoR in harmonizing structural and textual retrieval with insights, including uneven retrieving performance across different query logics and the benefits of integrating structural trajectories for candidate reranking. Our code is available at https://github.com/Yoega/MoR.', 'score': 6, 'issue_id': 2561, 'pub_date': '2025-02-27', 'pub_date_card': {'ru': '27 февраля', 'en': 'February 27', 'zh': '2月27日'}, 'hash': '686b2ff85600f281', 'authors': ['Yongjia Lei', 'Haoyu Han', 'Ryan A. Rossi', 'Franck Dernoncourt', 'Nedim Lipka', 'Mahantesh M Halappanavar', 'Jiliang Tang', 'Yu Wang'], 'affiliations': ['Adobe Research', 'Michigan State University', 'Pacific Northwest National Laboratory', 'University of Oregon'], 'pdf_title_img': 'assets/pdf/title_img/2502.20317.jpg', 'data': {'categories': ['#benchmark', '#data', '#graphs', '#dataset', '#multimodal', '#reasoning'], 'emoji': '🕸️', 'ru': {'title': 'Гармоничное слияние структурного и текстового поиска в графовых базах знаний', 'desc': 'Эта статья представляет новый метод под названием MoR (Mixture of Structural-and-Textual Retrieval) для работы с графовыми базами знаний, содержащими текстовую информацию. MoR использует трехэтапный подход: планирование, рассуждение и организация, чтобы эффективно объединить структурный и текстовый поиск. Метод генерирует текстовые графы планирования, затем переплетает структурный обход и текстовое сопоставление, и наконец, переранжирует кандидатов на основе их структурных траекторий. Эксперименты показывают превосходство MoR в гармонизации структурного и текстового поиска.'}, 'en': {'title': 'Harmonizing Text and Structure for Better Knowledge Retrieval', 'desc': 'This paper introduces a new method called Mixture of Structural-and-Textual Retrieval (MoR) for improving the retrieval of knowledge from Text-rich Graph Knowledge Bases (TG-KBs). Unlike traditional methods that treat textual and structural knowledge separately, MoR integrates both types through a three-stage framework: Planning, Reasoning, and Organizing. In the Planning stage, it creates graphs that outline the logic for query responses, while the Reasoning stage combines structural traversal with textual matching to gather relevant candidates. Finally, the Organizing stage enhances the selection process by reranking candidates based on their structural paths, leading to better retrieval performance across various query types.'}, 'zh': {'title': '结构与文本知识的完美结合', 'desc': '本文提出了一种新的混合检索方法，称为结构与文本检索的混合体（MoR），旨在同时利用文本和结构知识来回答查询。MoR通过规划-推理-组织的框架来实现这一目标，在规划阶段生成文本规划图，明确回答查询的逻辑。接着，在推理阶段，MoR结合结构遍历和文本匹配，从文本丰富的图知识库中获取候选答案。最后，在组织阶段，MoR根据结构轨迹对获取的候选答案进行重新排序，实验结果表明该方法在结构和文本检索的协调性上具有显著优势。'}}}, {'id': 'https://huggingface.co/papers/2503.03044', 'title': 'QE4PE: Word-level Quality Estimation for Human Post-Editing', 'url': 'https://huggingface.co/papers/2503.03044', 'abstract': "Word-level quality estimation (QE) detects erroneous spans in machine translations, which can direct and facilitate human post-editing. While the accuracy of word-level QE systems has been assessed extensively, their usability and downstream influence on the speed, quality and editing choices of human post-editing remain understudied. Our QE4PE study investigates the impact of word-level QE on machine translation (MT) post-editing in a realistic setting involving 42 professional post-editors across two translation directions. We compare four error-span highlight modalities, including supervised and uncertainty-based word-level QE methods, for identifying potential errors in the outputs of a state-of-the-art neural MT model. Post-editing effort and productivity are estimated by behavioral logs, while quality improvements are assessed by word- and segment-level human annotation. We find that domain, language and editors' speed are critical factors in determining highlights' effectiveness, with modest differences between human-made and automated QE highlights underlining a gap between accuracy and usability in professional workflows.", 'score': 5, 'issue_id': 2560, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 марта', 'en': 'March 4', 'zh': '3月4日'}, 'hash': 'e4d3d7db506b6e1c', 'authors': ['Gabriele Sarti', 'Vilém Zouhar', 'Grzegorz Chrupała', 'Ana Guerberof-Arenas', 'Malvina Nissim', 'Arianna Bisazza'], 'affiliations': ['CLCG, University of Groningen', 'CSAI, Tilburg University', 'ETH Zürich'], 'pdf_title_img': 'assets/pdf/title_img/2503.03044.jpg', 'data': {'categories': ['#machine_translation', '#data', '#multilingual', '#healthcare'], 'emoji': '🔍', 'ru': {'title': 'Оценка качества перевода: мост между точностью и практичностью', 'desc': 'Статья исследует влияние оценки качества перевода на уровне слов (word-level QE) на процесс постредактирования машинного перевода. В исследовании участвовали 42 профессиональных редактора, работавших с двумя направлениями перевода. Сравнивались четыре модальности подсветки ошибок, включая методы на основе обучения с учителем и неопределенности. Результаты показывают, что эффективность подсветки зависит от домена, языка и скорости работы редакторов, при этом разница между ручной и автоматической QE оказалась незначительной.'}, 'en': {'title': 'Enhancing Post-Editing Efficiency with Word-Level Quality Estimation', 'desc': 'This paper explores how word-level quality estimation (QE) can help improve the efficiency of human post-editing in machine translation (MT). It examines the effectiveness of different methods for highlighting potential translation errors, comparing supervised and uncertainty-based approaches. The study involves 42 professional post-editors and assesses their editing speed and quality improvements through detailed behavioral logs and human annotations. The findings reveal that factors like domain, language, and editor speed significantly influence the effectiveness of error highlights, indicating a need to bridge the gap between the accuracy of QE systems and their practical usability in real-world editing tasks.'}, 'zh': {'title': '提升机器翻译后编辑效率的关键', 'desc': '本文研究了词级质量评估（QE）在机器翻译后编辑中的影响。我们分析了42名专业后编辑在两种翻译方向下的表现，比较了四种错误范围高亮方式，包括监督和基于不确定性的词级QE方法。研究发现，领域、语言和编辑速度是影响高亮效果的关键因素。结果表明，人工和自动QE高亮之间存在适度差异，突显了专业工作流程中准确性与可用性之间的差距。'}}}, {'id': 'https://huggingface.co/papers/2503.00307', 'title': 'Remasking Discrete Diffusion Models with Inference-Time Scaling', 'url': 'https://huggingface.co/papers/2503.00307', 'abstract': 'Part of the success of diffusion models stems from their ability to perform iterative refinement, i.e., repeatedly correcting outputs during generation. However, modern masked discrete diffusion lacks this capability: when a token is generated, it cannot be updated again, even when it introduces an error. Here, we address this limitation by introducing the remasking diffusion model (ReMDM) sampler, a method that can be applied to pretrained masked diffusion models in a principled way and that is derived from a discrete diffusion model with a custom remasking backward process. Most interestingly, ReMDM endows discrete diffusion with a form of inference-time compute scaling. By increasing the number of sampling steps, ReMDM generates natural language outputs that approach the quality of autoregressive models, whereas when the computation budget is limited, ReMDM better maintains quality. ReMDM also improves sample quality of masked diffusion models for discretized images, and in scientific domains such as molecule design, ReMDM facilitates diffusion guidance and pushes the Pareto frontier of controllability relative to classical masking and uniform noise diffusion. We provide the code along with a blog post on the project page: https://remdm.github.io.', 'score': 4, 'issue_id': 2572, 'pub_date': '2025-03-01', 'pub_date_card': {'ru': '1 марта', 'en': 'March 1', 'zh': '3月1日'}, 'hash': '7f31677f2cb675e1', 'authors': ['Guanghan Wang', 'Yair Schiff', 'Subham Sekhar Sahoo', 'Volodymyr Kuleshov'], 'affiliations': ['Department of Computer Science, Cornell Unversity'], 'pdf_title_img': 'assets/pdf/title_img/2503.00307.jpg', 'data': {'categories': ['#open_source', '#inference', '#diffusion', '#science', '#multimodal'], 'emoji': '🔄', 'ru': {'title': 'Перемаскировка для улучшения дискретных диффузионных моделей', 'desc': 'Статья представляет новый метод ReMDM (remasking diffusion model), который улучшает процесс генерации в дискретных диффузионных моделях. ReMDM позволяет итеративно уточнять сгенерированные токены, что ранее было невозможно в классических масочных диффузионных моделях. Этот подход повышает качество генерации естественного языка, приближая его к уровню авторегрессионных моделей, особенно при увеличении числа шагов сэмплирования. ReMDM также демонстрирует улучшения в генерации дискретизированных изображений и дизайне молекул.'}, 'en': {'title': 'Enhancing Masked Diffusion with Iterative Refinement', 'desc': 'This paper introduces the remasking diffusion model (ReMDM) sampler, which enhances the capabilities of masked discrete diffusion models by allowing iterative refinement during output generation. Unlike traditional methods where generated tokens cannot be updated, ReMDM enables the correction of errors by applying a remasking backward process. This approach not only improves the quality of natural language outputs to rival autoregressive models but also maintains high quality under limited computational resources. Additionally, ReMDM enhances the performance of masked diffusion models in generating discretized images and aids in molecule design, pushing the boundaries of controllability in scientific applications.'}, 'zh': {'title': '重掩蔽扩散模型：提升生成质量的新方法', 'desc': '扩散模型的成功部分源于其迭代精炼的能力，即在生成过程中不断修正输出。然而，现代的掩蔽离散扩散模型缺乏这种能力：一旦生成一个标记，就无法再次更新，即使它引入了错误。为了解决这个限制，我们提出了重掩蔽扩散模型（ReMDM）采样器，这是一种可以以原则性方式应用于预训练掩蔽扩散模型的方法。ReMDM通过增加采样步骤，生成的自然语言输出质量接近自回归模型，同时在计算预算有限时，ReMDM更好地保持质量。'}}}, {'id': 'https://huggingface.co/papers/2502.18860', 'title': 'Exploring Rewriting Approaches for Different Conversational Tasks', 'url': 'https://huggingface.co/papers/2502.18860', 'abstract': "Conversational assistants often require a question rewriting algorithm that leverages a subset of past interactions to provide a more meaningful (accurate) answer to the user's question or request. However, the exact rewriting approach may often depend on the use case and application-specific tasks supported by the conversational assistant, among other constraints. In this paper, we systematically investigate two different approaches, denoted as rewriting and fusion, on two fundamentally different generation tasks, including a text-to-text generation task and a multimodal generative task that takes as input text and generates a visualization or data table that answers the user's question. Our results indicate that the specific rewriting or fusion approach highly depends on the underlying use case and generative task. In particular, we find that for a conversational question-answering assistant, the query rewriting approach performs best, whereas for a data analysis assistant that generates visualizations and data tables based on the user's conversation with the assistant, the fusion approach works best. Notably, we explore two datasets for the data analysis assistant use case, for short and long conversations, and we find that query fusion always performs better, whereas for the conversational text-based question-answering, the query rewrite approach performs best.", 'score': 4, 'issue_id': 2561, 'pub_date': '2025-02-26', 'pub_date_card': {'ru': '26 февраля', 'en': 'February 26', 'zh': '2月26日'}, 'hash': '1f018cc4f38149bc', 'authors': ['Md Mehrab Tanjim', 'Ryan A. Rossi', 'Mike Rimer', 'Xiang Chen', 'Sungchul Kim', 'Vaishnavi Muppala', 'Tong Yu', 'Zhengmian Hu', 'Ritwik Sinha', 'Wei Zhang', 'Iftikhar Ahamath Burhanuddin', 'Franck Dernoncourt'], 'affiliations': ['Adobe Research'], 'pdf_title_img': 'assets/pdf/title_img/2502.18860.jpg', 'data': {'categories': ['#dataset', '#multimodal'], 'emoji': '🤖', 'ru': {'title': 'Переписывание запросов в разговорных ИИ: один метод не подходит для всех задач', 'desc': 'В статье исследуются два подхода к переписыванию запросов в разговорных ассистентах: переписывание и слияние. Эксперименты проводились на двух различных задачах генерации: текст-в-текст и мультимодальной генерации визуализаций. Результаты показывают, что эффективность подхода зависит от конкретного случая использования и задачи. Для текстового вопросно-ответного ассистента лучше работает переписывание запросов, а для ассистента по анализу данных - слияние запросов.'}, 'en': {'title': 'Tailoring Response Strategies for Conversational Assistants', 'desc': 'This paper explores how conversational assistants can improve their responses by using two methods: rewriting and fusion. The rewriting method modifies user questions to enhance accuracy, while the fusion method combines information from past interactions to generate answers. The study shows that the effectiveness of these methods varies based on the task; rewriting is better for text-based question answering, while fusion excels in generating visualizations from data analysis tasks. The findings highlight the importance of tailoring the approach to the specific use case for optimal performance.'}, 'zh': {'title': '对话助手中的问题重写与融合方法的最佳选择', 'desc': '本论文探讨了对话助手中问题重写算法的两种不同方法：重写和融合。这两种方法在文本生成和多模态生成任务中表现不同，具体取决于应用场景。研究发现，对于对话问答助手，查询重写方法效果最佳；而对于生成可视化和数据表的数据分析助手，查询融合方法更为有效。我们还分析了短对话和长对话的数据集，结果表明查询融合在数据分析任务中始终表现更好。'}}}, {'id': 'https://huggingface.co/papers/2503.01763', 'title': "Retrieval Models Aren't Tool-Savvy: Benchmarking Tool Retrieval for Large Language Models", 'url': 'https://huggingface.co/papers/2503.01763', 'abstract': 'Tool learning aims to augment large language models (LLMs) with diverse tools, enabling them to act as agents for solving practical tasks. Due to the limited context length of tool-using LLMs, adopting information retrieval (IR) models to select useful tools from large toolsets is a critical initial step. However, the performance of IR models in tool retrieval tasks remains underexplored and unclear. Most tool-use benchmarks simplify this step by manually pre-annotating a small set of relevant tools for each task, which is far from the real-world scenarios. In this paper, we propose ToolRet, a heterogeneous tool retrieval benchmark comprising 7.6k diverse retrieval tasks, and a corpus of 43k tools, collected from existing datasets. We benchmark six types of models on ToolRet. Surprisingly, even the models with strong performance in conventional IR benchmarks, exhibit poor performance on ToolRet. This low retrieval quality degrades the task pass rate of tool-use LLMs. As a further step, we contribute a large-scale training dataset with over 200k instances, which substantially optimizes the tool retrieval ability of IR models.', 'score': 4, 'issue_id': 2558, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': 'e6a23582f741dc5b', 'authors': ['Zhengliang Shi', 'Yuhan Wang', 'Lingyong Yan', 'Pengjie Ren', 'Shuaiqiang Wang', 'Dawei Yin', 'Zhaochun Ren'], 'affiliations': ['Baidu Inc., Beijing, China', 'Leiden University, Leiden, The Netherlands', 'Shandong University, Qingdao, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.01763.jpg', 'data': {'categories': ['#training', '#optimization', '#benchmark', '#dataset', '#data'], 'emoji': '🔍', 'ru': {'title': 'ToolRet: Новый вызов для моделей поиска инструментов ИИ', 'desc': 'ToolRet - это новый эталонный тест для оценки поиска инструментов в контексте обучения инструментам для больших языковых моделей (LLM). Он включает 7,6 тысяч разнообразных задач поиска и корпус из 43 тысяч инструментов. Исследование показало, что даже модели с высокой производительностью в традиционных тестах информационного поиска плохо справляются с ToolRet. Авторы также предоставили обучающий набор данных из более чем 200 тысяч примеров для улучшения способностей моделей к поиску инструментов.'}, 'en': {'title': 'Enhancing Tool Retrieval for Language Models with ToolRet', 'desc': 'This paper introduces ToolRet, a benchmark designed to evaluate the effectiveness of information retrieval (IR) models in selecting tools for large language models (LLMs) in practical tasks. The authors highlight that existing benchmarks often rely on a limited set of pre-annotated tools, which does not reflect real-world complexities. Their findings reveal that even high-performing IR models struggle with tool retrieval in this new context, leading to lower task success rates for LLMs. To address this issue, they provide a large-scale training dataset that significantly enhances the tool retrieval capabilities of IR models.'}, 'zh': {'title': '工具检索：提升LLMs的实用能力', 'desc': '本文探讨了工具学习如何增强大型语言模型（LLMs）的能力，使其能够作为代理解决实际任务。由于工具使用的LLMs具有有限的上下文长度，因此采用信息检索（IR）模型从大量工具集中选择有用工具是关键的初步步骤。我们提出了ToolRet，一个包含7.6k多样化检索任务和43k工具的异构工具检索基准，旨在评估IR模型在工具检索任务中的表现。研究发现，即使在传统IR基准上表现良好的模型，在ToolRet上的表现却很差，这降低了工具使用LLMs的任务通过率。'}}}, {'id': 'https://huggingface.co/papers/2503.01729', 'title': 'FLAME: A Federated Learning Benchmark for Robotic Manipulation', 'url': 'https://huggingface.co/papers/2503.01729', 'abstract': 'Recent progress in robotic manipulation has been fueled by large-scale datasets collected across diverse environments. Training robotic manipulation policies on these datasets is traditionally performed in a centralized manner, raising concerns regarding scalability, adaptability, and data privacy. While federated learning enables decentralized, privacy-preserving training, its application to robotic manipulation remains largely unexplored. We introduce FLAME (Federated Learning Across Manipulation Environments), the first benchmark designed for federated learning in robotic manipulation. FLAME consists of: (i) a set of large-scale datasets of over 160,000 expert demonstrations of multiple manipulation tasks, collected across a wide range of simulated environments; (ii) a training and evaluation framework for robotic policy learning in a federated setting. We evaluate standard federated learning algorithms in FLAME, showing their potential for distributed policy learning and highlighting key challenges. Our benchmark establishes a foundation for scalable, adaptive, and privacy-aware robotic learning.', 'score': 3, 'issue_id': 2558, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': '893358a382c79250', 'authors': ['Santiago Bou Betran', 'Alberta Longhini', 'Miguel Vasco', 'Yuchong Zhang', 'Danica Kragic'], 'affiliations': ['KTH Royal Institute of Technology, Stockholm, Sweden'], 'pdf_title_img': 'assets/pdf/title_img/2503.01729.jpg', 'data': {'categories': ['#robotics', '#benchmark', '#dataset'], 'emoji': '🤖', 'ru': {'title': 'Федеративное обучение для масштабируемой и конфиденциальной робототехники', 'desc': 'Статья представляет FLAME - первый бенчмарк для федеративного обучения в робототехнической манипуляции. FLAME включает в себя большой набор данных с более чем 160 000 экспертных демонстраций различных задач манипуляции, собранных в симулированных средах. Бенчмарк также предоставляет фреймворк для обучения и оценки робототехнических политик в федеративной среде. Авторы оценивают стандартные алгоритмы федеративного обучения на FLAME, демонстрируя их потенциал для распределенного обучения политик и выявляя ключевые проблемы.'}, 'en': {'title': 'Empowering Robots with Federated Learning for Privacy and Scalability', 'desc': 'This paper presents FLAME, a benchmark for applying federated learning to robotic manipulation tasks. It addresses the limitations of centralized training by allowing robots to learn from diverse datasets while preserving data privacy. FLAME includes over 160,000 expert demonstrations from various simulated environments, facilitating decentralized training. The study evaluates existing federated learning algorithms, demonstrating their effectiveness and identifying challenges in distributed policy learning for robotics.'}, 'zh': {'title': '联邦学习助力机器人操控的未来', 'desc': '这篇论文介绍了FLAME（跨操控环境的联邦学习），这是一个为机器人操控设计的基准测试。FLAME包含超过160,000个专家演示的大规模数据集，涵盖多种操控任务，收集自多种模拟环境。通过在FLAME中评估标准的联邦学习算法，论文展示了分布式策略学习的潜力，并指出了关键挑战。该基准为可扩展、适应性强且注重隐私的机器人学习奠定了基础。'}}}, {'id': 'https://huggingface.co/papers/2503.01449', 'title': 'Benchmarking Large Language Models for Multi-Language Software Vulnerability Detection', 'url': 'https://huggingface.co/papers/2503.01449', 'abstract': 'Recent advancements in generative AI have led to the widespread adoption of large language models (LLMs) in software engineering, addressing numerous long-standing challenges. However, a comprehensive study examining the capabilities of LLMs in software vulnerability detection (SVD), a crucial aspect of software security, is currently lacking. Existing research primarily focuses on evaluating LLMs using C/C++ datasets. It typically explores only one or two strategies among prompt engineering, instruction tuning, and sequence classification fine-tuning for open-source LLMs. Consequently, there is a significant knowledge gap regarding the effectiveness of diverse LLMs in detecting vulnerabilities across various programming languages. To address this knowledge gap, we present a comprehensive empirical study evaluating the performance of LLMs on the SVD task. We have compiled a comprehensive dataset comprising 8,260 vulnerable functions in Python, 7,505 in Java, and 28,983 in JavaScript. We assess five open-source LLMs using multiple approaches, including prompt engineering, instruction tuning, and sequence classification fine-tuning. These LLMs are benchmarked against five fine-tuned small language models and two open-source static application security testing tools. Furthermore, we explore two avenues to improve LLM performance on SVD: a) Data perspective: Retraining models using downsampled balanced datasets. b) Model perspective: Investigating ensemble learning methods that combine predictions from multiple LLMs. Our comprehensive experiments demonstrate that SVD remains a challenging task for LLMs. This study provides a thorough understanding of the role of LLMs in SVD and offers practical insights for future advancements in leveraging generative AI to enhance software security practices.', 'score': 3, 'issue_id': 2558, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': '1b4593bb9d78ec53', 'authors': ['Ting Zhang', 'Chengran Yang', 'Yindu Su', 'Martin Weyssow', 'Hung Nguyen', 'Tan Bui', 'Hong Jin Kang', 'Yikun Li', 'Eng Lieh Ouh', 'Lwin Khin Shar', 'David Lo'], 'affiliations': ['School of Computer Science, University of Sydney, Australia', 'School of Computing and Information Systems, Singapore Management University, Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2503.01449.jpg', 'data': {'categories': ['#open_source', '#plp', '#training', '#security', '#benchmark', '#dataset', '#data'], 'emoji': '🛡️', 'ru': {'title': 'LLM на страже безопасности кода: новые горизонты в обнаружении уязвимостей', 'desc': 'Статья представляет комплексное исследование возможностей больших языковых моделей (LLM) в обнаружении уязвимостей программного обеспечения (SVD). Авторы оценивают производительность пяти открытых LLM на наборах данных, включающих уязвимые функции на Python, Java и JavaScript, используя различные подходы, такие как инженерия промптов, настройка инструкций и тонкая настройка классификации последовательностей. Исследование также изучает способы улучшения производительности LLM в SVD, включая переобучение на сбалансированных наборах данных и использование ансамблевых методов обучения. Результаты показывают, что SVD остается сложной задачей для LLM, предоставляя ценные insights для будущих разработок в области применения генеративного ИИ для повышения безопасности программного обеспечения.'}, 'en': {'title': 'Unlocking LLMs for Software Vulnerability Detection', 'desc': "This paper investigates the effectiveness of large language models (LLMs) in detecting software vulnerabilities, an important area for software security. It highlights the lack of comprehensive studies on LLMs' capabilities across various programming languages, as most existing research focuses on C/C++ datasets. The authors present an empirical study using a dataset of over 44,000 vulnerable functions from Python, Java, and JavaScript, evaluating five open-source LLMs with different strategies like prompt engineering and instruction tuning. The findings reveal that while LLMs show promise, software vulnerability detection remains a challenging task, providing valuable insights for future improvements in this field."}, 'zh': {'title': '提升软件安全：大型语言模型在漏洞检测中的应用', 'desc': '最近生成性人工智能的进展使得大型语言模型（LLMs）在软件工程中得到了广泛应用，解决了许多长期存在的挑战。然而，目前缺乏对LLMs在软件漏洞检测（SVD）能力的全面研究，这对软件安全至关重要。现有研究主要集中在使用C/C++数据集评估LLMs，通常只探讨了提示工程、指令调优和序列分类微调中的一两种策略。因此，我们进行了一项全面的实证研究，评估LLMs在不同编程语言中检测漏洞的有效性。'}}}, {'id': 'https://huggingface.co/papers/2503.01378', 'title': 'CognitiveDrone: A VLA Model and Evaluation Benchmark for Real-Time Cognitive Task Solving and Reasoning in UAVs', 'url': 'https://huggingface.co/papers/2503.01378', 'abstract': 'This paper introduces CognitiveDrone, a novel Vision-Language-Action (VLA) model tailored for complex Unmanned Aerial Vehicles (UAVs) tasks that demand advanced cognitive abilities. Trained on a dataset comprising over 8,000 simulated flight trajectories across three key categories-Human Recognition, Symbol Understanding, and Reasoning-the model generates real-time 4D action commands based on first-person visual inputs and textual instructions. To further enhance performance in intricate scenarios, we propose CognitiveDrone-R1, which integrates an additional Vision-Language Model (VLM) reasoning module to simplify task directives prior to high-frequency control. Experimental evaluations using our open-source benchmark, CognitiveDroneBench, reveal that while a racing-oriented model (RaceVLA) achieves an overall success rate of 31.3%, the base CognitiveDrone model reaches 59.6%, and CognitiveDrone-R1 attains a success rate of 77.2%. These results demonstrate improvements of up to 30% in critical cognitive tasks, underscoring the effectiveness of incorporating advanced reasoning capabilities into UAV control systems. Our contributions include the development of a state-of-the-art VLA model for UAV control and the introduction of the first dedicated benchmark for assessing cognitive tasks in drone operations. The complete repository is available at cognitivedrone.github.io', 'score': 2, 'issue_id': 2558, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': '8a4aab69ce92453d', 'authors': ['Artem Lykov', 'Valerii Serpiva', 'Muhammad Haris Khan', 'Oleg Sautenkov', 'Artyom Myshlyaev', 'Grik Tadevosyan', 'Yasheerah Yaqoot', 'Dzmitry Tsetserukou'], 'affiliations': ['Intelligent Space Robotics Laboratory, Science Center for Digital Engineering, Technology. Skolkovo Institute'], 'pdf_title_img': 'assets/pdf/title_img/2503.01378.jpg', 'data': {'categories': ['#open_source', '#reasoning', '#benchmark', '#dataset', '#multimodal', '#cv'], 'emoji': '🚁', 'ru': {'title': 'Умные дроны: когнитивное управление БПЛА с помощью ИИ', 'desc': 'В статье представлена модель CognitiveDrone - новая модель Зрение-Язык-Действие (VLA) для сложных задач беспилотных летательных аппаратов (БПЛА). Модель обучена на наборе данных из более чем 8000 симулированных полетов и генерирует команды действий в реальном времени на основе визуальных входных данных и текстовых инструкций. Усовершенствованная версия CognitiveDrone-R1 включает дополнительный модуль рассуждений на основе Модели Зрения-Языка (VLM) для упрощения сложных задач. Экспериментальная оценка показывает, что CognitiveDrone-R1 достигает успешности выполнения задач в 77.2%, что на 30% лучше базовых моделей в критических когнитивных задачах.'}, 'en': {'title': 'CognitiveDrone: Elevating UAV Intelligence with Vision-Language-Action!', 'desc': 'This paper presents CognitiveDrone, a new Vision-Language-Action (VLA) model designed for complex tasks performed by Unmanned Aerial Vehicles (UAVs). It is trained on a dataset of over 8,000 simulated flight paths focusing on Human Recognition, Symbol Understanding, and Reasoning. The model can generate real-time 4D action commands from visual inputs and text instructions, with an enhanced version, CognitiveDrone-R1, that includes a Vision-Language Model (VLM) reasoning module for better task management. Experimental results show significant performance improvements, with CognitiveDrone-R1 achieving a 77.2% success rate, highlighting the importance of advanced reasoning in UAV operations.'}, 'zh': {'title': '智能无人机的认知飞行新纪元', 'desc': '本文介绍了一种名为CognitiveDrone的新型视觉-语言-行动（VLA）模型，专为复杂的无人机任务设计，具备高级认知能力。该模型在超过8000条模拟飞行轨迹的数据集上进行训练，涵盖人类识别、符号理解和推理三个关键类别。CognitiveDrone-R1通过集成额外的视觉-语言模型（VLM）推理模块，进一步提升在复杂场景中的表现。实验结果显示，CognitiveDrone模型的成功率达到59.6%，而CognitiveDrone-R1的成功率更是高达77.2%，证明了将高级推理能力融入无人机控制系统的有效性。'}}}, {'id': 'https://huggingface.co/papers/2503.01372', 'title': 'SwiLTra-Bench: The Swiss Legal Translation Benchmark', 'url': 'https://huggingface.co/papers/2503.01372', 'abstract': "In Switzerland legal translation is uniquely important due to the country's four official languages and requirements for multilingual legal documentation. However, this process traditionally relies on professionals who must be both legal experts and skilled translators -- creating bottlenecks and impacting effective access to justice. To address this challenge, we introduce SwiLTra-Bench, a comprehensive multilingual benchmark of over 180K aligned Swiss legal translation pairs comprising laws, headnotes, and press releases across all Swiss languages along with English, designed to evaluate LLM-based translation systems. Our systematic evaluation reveals that frontier models achieve superior translation performance across all document types, while specialized translation systems excel specifically in laws but under-perform in headnotes. Through rigorous testing and human expert validation, we demonstrate that while fine-tuning open SLMs significantly improves their translation quality, they still lag behind the best zero-shot prompted frontier models such as Claude-3.5-Sonnet. Additionally, we present SwiLTra-Judge, a specialized LLM evaluation system that aligns best with human expert assessments.", 'score': 2, 'issue_id': 2558, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': '3de5be81537fa0fd', 'authors': ['Joel Niklaus', 'Jakob Merane', 'Luka Nenadic', 'Sina Ahmadi', 'Yingqiang Gao', 'Cyrill A. H. Chevalley', 'Claude Humbel', 'Christophe Gösken', 'Lorenzo Tanzi', 'Thomas Lüthi', 'Stefan Palombo', 'Spencer Poff', 'Boling Yang', 'Nan Wu', 'Matthew Guillod', 'Robin Mamié', 'Daniel Brunner', 'Julio Pereyra', 'Niko Grupen'], 'affiliations': ['Canton of Solothurn', 'ETH Zurich', 'Max Planck Institute for Research on Collective Goods', 'Swiss Federal Supreme Court', 'University of Basel', 'University of Geneva', 'University of Lausanne', 'University of Zurich'], 'pdf_title_img': 'assets/pdf/title_img/2503.01372.jpg', 'data': {'categories': ['#multilingual', '#open_source', '#benchmark', '#dataset', '#machine_translation'], 'emoji': '⚖️', 'ru': {'title': 'Революция в юридическом переводе: ИИ покоряет многоязычную Швейцарию', 'desc': 'Статья представляет SwiLTra-Bench - многоязычный набор данных для оценки систем машинного перевода юридических текстов в Швейцарии. Авторы провели систематическую оценку различных моделей, включая крупные языковые модели и специализированные системы перевода. Результаты показывают, что передовые модели достигают лучших результатов во всех типах документов, а дообучение открытых моделей значительно улучшает качество перевода. Также представлена система SwiLTra-Judge для оценки качества перевода, которая хорошо коррелирует с оценками экспертов.'}, 'en': {'title': 'Enhancing Legal Translation with SwiLTra-Bench and LLMs', 'desc': 'This paper addresses the challenges of legal translation in Switzerland, where multiple languages complicate the process. It introduces SwiLTra-Bench, a benchmark dataset with over 180,000 aligned legal translation pairs to evaluate large language model (LLM) translation systems. The findings show that while advanced models perform well across various document types, specialized systems are better for translating laws but struggle with headnotes. The study also highlights the effectiveness of fine-tuning open-source language models, although they still do not match the performance of top zero-shot models like Claude-3.5-Sonnet.'}, 'zh': {'title': '瑞士法律翻译的智能解决方案', 'desc': '在瑞士，由于有四种官方语言，法律翻译显得尤为重要。传统上，这一过程依赖于既是法律专家又是翻译高手的专业人士，导致了瓶颈，影响了公正的有效获取。为了解决这个问题，我们推出了SwiLTra-Bench，这是一个包含超过18万对瑞士法律翻译的多语言基准数据集，旨在评估基于大语言模型的翻译系统。我们的评估显示，前沿模型在所有文档类型的翻译表现上优于其他系统，而专门的翻译系统在法律文本中表现出色，但在头注方面表现不佳。'}}}, {'id': 'https://huggingface.co/papers/2503.00502', 'title': 'Interact, Instruct to Improve: A LLM-Driven Parallel Actor-Reasoner Framework for Enhancing Autonomous Vehicle Interactions', 'url': 'https://huggingface.co/papers/2503.00502', 'abstract': "Autonomous Vehicles (AVs) have entered the commercialization stage, but their limited ability to interact and express intentions still poses challenges in interactions with Human-driven Vehicles (HVs). Recent advances in large language models (LLMs) enable bidirectional human-machine communication, but the conflict between slow inference speed and the need for real-time decision-making challenges practical deployment. To address these issues, this paper introduces a parallel Actor-Reasoner framework designed to enable explicit bidirectional AV-HV interactions across multiple scenarios. First, by facilitating interactions between the LLM-driven Reasoner and heterogeneous simulated HVs during training, an interaction memory database, referred to as the Actor, is established. Then, by introducing the memory partition module and the two-layer memory retrieval module, the Actor's ability to handle heterogeneous HVs is significantly enhanced. Ablation studies and comparisons with other decision-making methods demonstrate that the proposed Actor-Reasoner framework significantly improves safety and efficiency. Finally, with the combination of the external Human-Machine Interface (eHMI) information derived from Reasoner's reasoning and the feasible action solutions retrieved from the Actor, the effectiveness of the proposed Actor-Reasoner is confirmed in multi-scenario field interactions. Our code is available at https://github.com/FanGShiYuu/Actor-Reasoner.", 'score': 2, 'issue_id': 2555, 'pub_date': '2025-03-01', 'pub_date_card': {'ru': '1 марта', 'en': 'March 1', 'zh': '3月1日'}, 'hash': 'd184a5cae68093d5', 'authors': ['Shiyu Fang', 'Jiaqi Liu', 'Chengkai Xu', 'Chen Lv', 'Peng Hang', 'Jian Sun'], 'affiliations': ['College of Transportation, Tongji University, Shanghai 201804, China', 'Nanyang Technological University, 639798, Singapore', 'State Key Lab of Intelligent Transportation System, Beijing 100088, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.00502.jpg', 'data': {'categories': ['#rl', '#robotics', '#inference', '#optimization', '#agents', '#reasoning'], 'emoji': '🚗', 'ru': {'title': 'Интеллектуальное взаимодействие автономных и обычных автомобилей с помощью больших языковых моделей', 'desc': 'Эта статья представляет новую архитектуру Actor-Reasoner для улучшения взаимодействия между автономными и управляемыми человеком транспортными средствами. Авторы используют большие языковые модели для создания базы данных взаимодействий и двухуровневую систему извлечения памяти для работы с разнородными транспортными средствами. Предложенный подход значительно повышает безопасность и эффективность принятия решений по сравнению с другими методами. Эксперименты в реальных условиях подтверждают эффективность предложенной архитектуры Actor-Reasoner в различных сценариях.'}, 'en': {'title': 'Enhancing AV-HV Interactions with the Actor-Reasoner Framework', 'desc': "This paper presents a new framework called the Actor-Reasoner to improve interactions between Autonomous Vehicles (AVs) and Human-driven Vehicles (HVs). It leverages large language models (LLMs) to facilitate real-time communication and decision-making, addressing the challenge of slow inference speeds. The framework includes an interaction memory database, which enhances the AV's ability to understand and respond to various HV behaviors. Experimental results show that this approach significantly boosts both safety and efficiency in multi-scenario driving situations."}, 'zh': {'title': '提升自动驾驶与人类驾驶互动的智能框架', 'desc': '这篇论文介绍了一种新的并行演员-推理器框架，旨在改善自动驾驶汽车（AV）与人类驾驶汽车（HV）之间的互动。通过在训练过程中促进大语言模型（LLM）驱动的推理器与不同类型的模拟HV之间的互动，建立了一个互动记忆数据库。引入记忆分区模块和双层记忆检索模块后，演员的处理能力得到了显著提升。实验结果表明，该框架在多场景交互中显著提高了安全性和效率。'}}}, {'id': 'https://huggingface.co/papers/2503.02954', 'title': 'Reliable and Efficient Multi-Agent Coordination via Graph Neural Network Variational Autoencoders', 'url': 'https://huggingface.co/papers/2503.02954', 'abstract': 'Multi-agent coordination is crucial for reliable multi-robot navigation in shared spaces such as automated warehouses. In regions of dense robot traffic, local coordination methods may fail to find a deadlock-free solution. In these scenarios, it is appropriate to let a central unit generate a global schedule that decides the passing order of robots. However, the runtime of such centralized coordination methods increases significantly with the problem scale. In this paper, we propose to leverage Graph Neural Network Variational Autoencoders (GNN-VAE) to solve the multi-agent coordination problem at scale faster than through centralized optimization. We formulate the coordination problem as a graph problem and collect ground truth data using a Mixed-Integer Linear Program (MILP) solver. During training, our learning framework encodes good quality solutions of the graph problem into a latent space. At inference time, solution samples are decoded from the sampled latent variables, and the lowest-cost sample is selected for coordination. Finally, the feasible proposal with the highest performance index is selected for the deployment. By construction, our GNN-VAE framework returns solutions that always respect the constraints of the considered coordination problem. Numerical results show that our approach trained on small-scale problems can achieve high-quality solutions even for large-scale problems with 250 robots, being much faster than other baselines. Project page: https://mengyuest.github.io/gnn-vae-coord', 'score': 0, 'issue_id': 2574, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 марта', 'en': 'March 4', 'zh': '3月4日'}, 'hash': '1d0c072d834299e0', 'authors': ['Yue Meng', 'Nathalie Majcherczyk', 'Wenliang Liu', 'Scott Kiesel', 'Chuchu Fan', 'Federico Pecora'], 'affiliations': ['Amazon Robotics, North Reading, MA USA', 'Massachusetts Institute of Technology, Cambridge, MA 02139 USA'], 'pdf_title_img': 'assets/pdf/title_img/2503.02954.jpg', 'data': {'categories': ['#optimization', '#training', '#games', '#agents', '#inference', '#graphs'], 'emoji': '🤖', 'ru': {'title': 'GNN-VAE: Быстрая координация множества роботов', 'desc': 'Статья представляет новый подход к координации множества роботов в общих пространствах, таких как автоматизированные склады. Авторы предлагают использовать вариационные автоэнкодеры на основе графовых нейронных сетей (GNN-VAE) для решения проблемы координации в масштабе быстрее, чем с помощью централизованной оптимизации. Модель обучается на данных, сгенерированных решателем задач смешанного целочисленного линейного программирования (MILP). Результаты показывают, что подход может достигать высококачественных решений даже для крупномасштабных проблем с 250 роботами, работая значительно быстрее других базовых методов.'}, 'en': {'title': 'Efficient Multi-Robot Coordination with GNN-VAE', 'desc': 'This paper addresses the challenge of coordinating multiple robots in shared environments, particularly in high-traffic areas where traditional local methods may lead to deadlocks. The authors propose a novel approach using Graph Neural Network Variational Autoencoders (GNN-VAE) to efficiently generate coordination schedules at scale. By framing the coordination problem as a graph problem and utilizing a Mixed-Integer Linear Program (MILP) for data collection, the framework learns to encode effective solutions into a latent space. During inference, it decodes these solutions to select the most optimal coordination strategy, ensuring compliance with all operational constraints while significantly reducing computation time compared to centralized methods.'}, 'zh': {'title': '高效多智能体协调的新方法', 'desc': '多智能体协调在共享空间中的可靠多机器人导航中至关重要，尤其是在机器人交通密集的区域。传统的局部协调方法可能无法找到无死锁的解决方案，因此需要一个中央单元生成全局调度来决定机器人的通行顺序。本文提出利用图神经网络变分自编码器（GNN-VAE）来更快地解决大规模的多智能体协调问题，避免了集中优化方法的高运行时间。通过将协调问题形式化为图问题，并使用混合整数线性规划（MILP）求解器收集真实数据，我们的学习框架能够在潜在空间中编码高质量的解决方案。'}}}, {'id': 'https://huggingface.co/papers/2503.02924', 'title': 'Diverse Controllable Diffusion Policy with Signal Temporal Logic', 'url': 'https://huggingface.co/papers/2503.02924', 'abstract': 'Generating realistic simulations is critical for autonomous system applications such as self-driving and human-robot interactions. However, driving simulators nowadays still have difficulty in generating controllable, diverse, and rule-compliant behaviors for road participants: Rule-based models cannot produce diverse behaviors and require careful tuning, whereas learning-based methods imitate the policy from data but are not designed to follow the rules explicitly. Besides, the real-world datasets are by nature "single-outcome", making the learning method hard to generate diverse behaviors. In this paper, we leverage Signal Temporal Logic (STL) and Diffusion Models to learn controllable, diverse, and rule-aware policy. We first calibrate the STL on the real-world data, then generate diverse synthetic data using trajectory optimization, and finally learn the rectified diffusion policy on the augmented dataset. We test on the NuScenes dataset and our approach can achieve the most diverse rule-compliant trajectories compared to other baselines, with a runtime 1/17X to the second-best approach. In the closed-loop testing, our approach reaches the highest diversity, rule satisfaction rate, and the least collision rate. Our method can generate varied characteristics conditional on different STL parameters in testing. A case study on human-robot encounter scenarios shows our approach can generate diverse and closed-to-oracle trajectories. The annotation tool, augmented dataset, and code are available at https://github.com/mengyuest/pSTL-diffusion-policy.', 'score': 0, 'issue_id': 2574, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 марта', 'en': 'March 4', 'zh': '3月4日'}, 'hash': 'adc4dd2a16bb83a4', 'authors': ['Yue Meng', 'Chuchu fan'], 'affiliations': ['Department of Aeronautics and Astronautics, Massachusetts Institute of Technology, Cambridge, MA 02139 USA'], 'pdf_title_img': 'assets/pdf/title_img/2503.02924.jpg', 'data': {'categories': ['#dataset', '#training', '#data', '#rl', '#agents', '#diffusion', '#synthetic'], 'emoji': '🚗', 'ru': {'title': 'Реалистичные и разнообразные симуляции дорожного движения с помощью STL и диффузионных моделей', 'desc': 'Эта статья представляет новый подход к генерации реалистичных симуляций для автономных систем, таких как беспилотные автомобили и взаимодействие человека с роботом. Авторы используют комбинацию Сигнальной Темпоральной Логики (STL) и Диффузионных Моделей для создания контролируемой, разнообразной и соблюдающей правила политики поведения участников дорожного движения. Метод сначала калибрует STL на реальных данных, затем генерирует разнообразные синтетические данные с помощью оптимизации траекторий, и наконец обучает скорректированную диффузионную политику на расширенном наборе данных. Результаты показывают, что подход превосходит базовые методы по разнообразию, соблюдению правил и безопасности, а также позволяет генерировать различные характеристики в зависимости от параметров STL.'}, 'en': {'title': 'Diverse and Rule-Compliant Simulations for Autonomous Systems', 'desc': 'This paper addresses the challenge of generating realistic simulations for autonomous systems, particularly in driving scenarios. It combines Signal Temporal Logic (STL) with Diffusion Models to create a policy that is both controllable and diverse while adhering to traffic rules. By calibrating STL on real-world data and generating synthetic data through trajectory optimization, the authors enhance the learning process to produce varied and rule-compliant behaviors. The results demonstrate that their method outperforms existing approaches in terms of diversity and safety in simulated environments.'}, 'zh': {'title': '生成多样化且遵循规则的行为策略', 'desc': '本文提出了一种新方法，通过信号时序逻辑（STL）和扩散模型来生成可控、多样且遵循规则的行为策略，以解决当前驾驶模拟器在生成道路参与者行为时的局限性。我们首先在真实数据上校准STL，然后利用轨迹优化生成多样的合成数据，最后在增强数据集上学习修正的扩散策略。实验结果表明，我们的方法在NuScenes数据集上能够生成最具多样性且符合规则的轨迹，且运行时间显著优于其他基线方法。通过闭环测试，我们的方法在多样性、规则满足率和碰撞率方面均表现最佳，能够根据不同的STL参数生成多样化的特征。'}}}, {'id': 'https://huggingface.co/papers/2503.14456', 'title': 'RWKV-7 "Goose" with Expressive Dynamic State Evolution', 'url': 'https://huggingface.co/papers/2503.14456', 'abstract': 'We present RWKV-7 "Goose", a new sequence modeling architecture, along with pre-trained language models that establish a new state-of-the-art in downstream performance at the 3 billion parameter scale on multilingual tasks, and match current SoTA English language performance despite being trained on dramatically fewer tokens than other top 3B models. Nevertheless, RWKV-7 models require only constant memory usage and constant inference time per token. RWKV-7 introduces a newly generalized formulation of the delta rule with vector-valued gating and in-context learning rates, as well as a relaxed value replacement rule. We show that RWKV-7 can perform state tracking and recognize all regular languages, while retaining parallelizability of training. This exceeds the capabilities of Transformers under standard complexity conjectures, which are limited to TC^0. To demonstrate RWKV-7\'s language modeling capability, we also present an extended open source 3.1 trillion token multilingual corpus, and train four RWKV-7 models ranging from 0.19 billion to 2.9 billion parameters on this dataset.   To foster openness, reproduction, and adoption, we release our models and dataset component listing at https://huggingface.co/RWKV, and our training and inference code at https://github.com/RWKV/RWKV-LM all under the Apache 2.0 License.', 'score': 49, 'issue_id': 2776, 'pub_date': '2025-03-18', 'pub_date_card': {'ru': '18 марта', 'en': 'March 18', 'zh': '3月18日'}, 'hash': '0cd796cef6fa6475', 'authors': ['Bo Peng', 'Ruichong Zhang', 'Daniel Goldstein', 'Eric Alcaide', 'Haowen Hou', 'Janna Lu', 'William Merrill', 'Guangyu Song', 'Kaifeng Tan', 'Saiteja Utpala', 'Nathan Wilce', 'Johan S. Wind', 'Tianyi Wu', 'Daniel Wuttke', 'Christian Zhou-Zheng'], 'affiliations': ['Beijing Normal University', 'Dalle Molle Institute for Artificial Intelligence USI-SUPSI', 'Denigma', 'EleutherAI', 'George Mason University', 'Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ)', 'New York University', 'RWKV Project (under Linux Foundation AI & Data)', 'Recursal AI', 'Shenzhen University', 'Tano Labs', 'Tsinghua University', 'University of Oslo'], 'pdf_title_img': 'assets/pdf/title_img/2503.14456.jpg', 'data': {'categories': ['#architecture', '#training', '#open_source', '#dataset', '#multilingual'], 'emoji': '🦢', 'ru': {'title': 'RWKV-7: Эффективная архитектура для многоязычного моделирования последовательностей', 'desc': "RWKV-7 'Goose' - это новая архитектура моделирования последовательностей, которая устанавливает новый state-of-the-art в производительности при 3 миллиардах параметров на многоязычных задачах. Модель требует постоянного использования памяти и времени вывода на токен, вводит обобщенную формулировку правила дельты с векторным гейтингом и обучением в контексте. RWKV-7 способна отслеживать состояния и распознавать все регулярные языки, превосходя возможности трансформеров. Авторы также представляют многоязычный корпус из 3,1 триллиона токенов и обучают на нем четыре модели RWKV-7 размером от 0,19 до 2,9 миллиардов параметров."}, 'en': {'title': 'RWKV-7: Efficient Multilingual Mastery with Fewer Parameters', 'desc': 'RWKV-7 "Goose" is a novel sequence modeling architecture that achieves state-of-the-art performance on multilingual tasks with only 3 billion parameters. It utilizes a unique formulation of the delta rule and vector-valued gating, allowing for efficient memory usage and constant inference time per token. The model demonstrates advanced capabilities such as state tracking and recognition of regular languages, surpassing traditional Transformers in complexity. Additionally, RWKV-7 is trained on a large multilingual corpus and is made openly available for further research and development.'}, 'zh': {'title': 'RWKV-7：多语言任务的新突破', 'desc': 'RWKV-7 "Goose" 是一种新的序列建模架构，具有3亿参数的预训练语言模型，在多语言任务中达到了新的最先进水平。与其他顶级3B模型相比，RWKV-7在训练时使用的标记数量显著减少，但仍能与当前的英语语言性能相匹配。该模型采用了新的广义增量规则，结合了向量值门控和上下文学习率，同时保持了训练的并行性。RWKV-7能够进行状态跟踪并识别所有正规语言，超越了标准复杂性猜想下的Transformer的能力。'}}}, {'id': 'https://huggingface.co/papers/2503.14378', 'title': 'Impossible Videos', 'url': 'https://huggingface.co/papers/2503.14378', 'abstract': "Synthetic videos nowadays is widely used to complement data scarcity and diversity of real-world videos. Current synthetic datasets primarily replicate real-world scenarios, leaving impossible, counterfactual and anti-reality video concepts underexplored. This work aims to answer two questions: 1) Can today's video generation models effectively follow prompts to create impossible video content? 2) Are today's video understanding models good enough for understanding impossible videos? To this end, we introduce IPV-Bench, a novel benchmark designed to evaluate and foster progress in video understanding and generation. IPV-Bench is underpinned by a comprehensive taxonomy, encompassing 4 domains, 14 categories. It features diverse scenes that defy physical, biological, geographical, or social laws. Based on the taxonomy, a prompt suite is constructed to evaluate video generation models, challenging their prompt following and creativity capabilities. In addition, a video benchmark is curated to assess Video-LLMs on their ability of understanding impossible videos, which particularly requires reasoning on temporal dynamics and world knowledge. Comprehensive evaluations reveal limitations and insights for future directions of video models, paving the way for next-generation video models.", 'score': 34, 'issue_id': 2776, 'pub_date': '2025-03-18', 'pub_date_card': {'ru': '18 марта', 'en': 'March 18', 'zh': '3月18日'}, 'hash': '3334aa74b743ac8d', 'authors': ['Zechen Bai', 'Hai Ci', 'Mike Zheng Shou'], 'affiliations': ['Show Lab, National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2503.14378.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#synthetic', '#video'], 'emoji': '🎬', 'ru': {'title': 'IPV-Bench: Новый рубеж в понимании и генерации невозможных видео', 'desc': 'Эта статья представляет IPV-Bench - новый бенчмарк для оценки и развития понимания и генерации видео. Он основан на таксономии, охватывающей 4 домена и 14 категорий, и включает разнообразные сцены, нарушающие физические, биологические, географические или социальные законы. Бенчмарк содержит набор промптов для оценки моделей генерации видео и видеоданные для оценки способности Video-LLM понимать невозможные видео. Комплексная оценка выявляет ограничения и предоставляет идеи для будущих направлений развития видеомоделей.'}, 'en': {'title': 'Exploring the Impossible: Advancing Video Generation and Understanding', 'desc': 'This paper addresses the limitations of current synthetic video datasets, which mainly focus on replicating real-world scenarios. It introduces IPV-Bench, a new benchmark designed to evaluate video generation and understanding models on their ability to create and comprehend impossible video content. The benchmark includes a detailed taxonomy with various categories that challenge models to generate videos that defy physical and social laws. The findings highlight the current capabilities and shortcomings of video models, providing insights for future advancements in the field.'}, 'zh': {'title': '探索不可能视频的生成与理解', 'desc': '本论文探讨了合成视频在数据稀缺和多样性方面的应用。当前的合成数据集主要复制现实场景，而对不可能、反事实和反现实的视频概念研究不足。我们提出了IPV-Bench，这是一个新颖的基准，旨在评估和促进视频理解与生成的进展。该基准涵盖了多种违反物理、生物、地理或社会法则的场景，并通过构建提示套件来挑战视频生成模型的创造力和提示跟随能力。'}}}, {'id': 'https://huggingface.co/papers/2503.14478', 'title': 'Creation-MMBench: Assessing Context-Aware Creative Intelligence in MLLM', 'url': 'https://huggingface.co/papers/2503.14478', 'abstract': "Creativity is a fundamental aspect of intelligence, involving the ability to generate novel and appropriate solutions across diverse contexts. While Large Language Models (LLMs) have been extensively evaluated for their creative capabilities, the assessment of Multimodal Large Language Models (MLLMs) in this domain remains largely unexplored. To address this gap, we introduce Creation-MMBench, a multimodal benchmark specifically designed to evaluate the creative capabilities of MLLMs in real-world, image-based tasks. The benchmark comprises 765 test cases spanning 51 fine-grained tasks. To ensure rigorous evaluation, we define instance-specific evaluation criteria for each test case, guiding the assessment of both general response quality and factual consistency with visual inputs. Experimental results reveal that current open-source MLLMs significantly underperform compared to proprietary models in creative tasks. Furthermore, our analysis demonstrates that visual fine-tuning can negatively impact the base LLM's creative abilities. Creation-MMBench provides valuable insights for advancing MLLM creativity and establishes a foundation for future improvements in multimodal generative intelligence. Full data and evaluation code is released on https://github.com/open-compass/Creation-MMBench.", 'score': 31, 'issue_id': 2777, 'pub_date': '2025-03-18', 'pub_date_card': {'ru': '18 марта', 'en': 'March 18', 'zh': '3月18日'}, 'hash': '120f1d8ec2eb88a8', 'authors': ['Xinyu Fang', 'Zhijian Chen', 'Kai Lan', 'Shengyuan Ding', 'Yingji Liang', 'Xiangyu Zhao', 'Farong Wen', 'Zicheng Zhang', 'Guofeng Zhang', 'Haodong Duan', 'Kai Chen', 'Dahua Lin'], 'affiliations': ['East China Normal University', 'Nanjing University', 'Shanghai AI Laboratory', 'Shanghai Jiaotong University', 'The Chinese University of Hong Kong', 'Tongji University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.14478.jpg', 'data': {'categories': ['#creativity', '#open_source', '#benchmark', '#multimodal'], 'emoji': '🎨', 'ru': {'title': 'Измеряя творчество искусственного интеллекта: новый бенчмарк для мультимодальных моделей', 'desc': 'Статья представляет Creation-MMBench - новый мультимодальный бенчмарк для оценки творческих способностей мультимодальных больших языковых моделей (MLLM) в задачах, основанных на изображениях. Бенчмарк содержит 765 тестовых примеров, охватывающих 51 детализированную задачу, с критериями оценки для каждого случая. Эксперименты показали, что открытые MLLM значительно уступают проприетарным моделям в творческих задачах. Исследование также выявило, что визуальная дообучение может негативно влиять на творческие способности базовой языковой модели.'}, 'en': {'title': 'Unlocking Creativity in Multimodal AI with Creation-MMBench', 'desc': 'This paper introduces Creation-MMBench, a new benchmark designed to evaluate the creative capabilities of Multimodal Large Language Models (MLLMs) in tasks that involve both text and images. The benchmark includes 765 test cases across 51 specific tasks, each with tailored evaluation criteria to assess the quality of responses and their consistency with visual inputs. Experimental findings indicate that current open-source MLLMs perform poorly in creative tasks compared to proprietary models, and that visual fine-tuning may hinder the creative performance of base LLMs. Creation-MMBench aims to enhance understanding and development of MLLM creativity, providing a resource for future research in multimodal generative intelligence.'}, 'zh': {'title': '评估多模态大型语言模型的创造力', 'desc': '创造力是智能的一个基本方面，涉及在不同情境中生成新颖且适当的解决方案。虽然大型语言模型（LLMs）在创造能力方面得到了广泛评估，但多模态大型语言模型（MLLMs）的评估仍然相对缺乏。为了解决这一问题，我们推出了Creation-MMBench，这是一个专门设计用于评估MLLMs在基于图像的实际任务中创造能力的多模态基准。实验结果表明，当前的开源MLLMs在创造性任务中显著低于专有模型，而视觉微调可能会对基础LLM的创造能力产生负面影响。'}}}, {'id': 'https://huggingface.co/papers/2503.12797', 'title': 'DeepPerception: Advancing R1-like Cognitive Visual Perception in MLLMs\n  for Knowledge-Intensive Visual Grounding', 'url': 'https://huggingface.co/papers/2503.12797', 'abstract': 'Human experts excel at fine-grained visual discrimination by leveraging domain knowledge to refine perceptual features, a capability that remains underdeveloped in current Multimodal Large Language Models (MLLMs). Despite possessing vast expert-level knowledge, MLLMs struggle to integrate reasoning into visual perception, often generating direct responses without deeper analysis. To bridge this gap, we introduce knowledge-intensive visual grounding (KVG), a novel visual grounding task that requires both fine-grained perception and domain-specific knowledge integration. To address the challenges of KVG, we propose DeepPerception, an MLLM enhanced with cognitive visual perception capabilities. Our approach consists of (1) an automated data synthesis pipeline that generates high-quality, knowledge-aligned training samples, and (2) a two-stage training framework combining supervised fine-tuning for cognitive reasoning scaffolding and reinforcement learning to optimize perception-cognition synergy. To benchmark performance, we introduce KVG-Bench a comprehensive dataset spanning 10 domains with 1.3K manually curated test cases. Experimental results demonstrate that DeepPerception significantly outperforms direct fine-tuning, achieving +8.08\\% accuracy improvements on KVG-Bench and exhibiting +4.60\\% superior cross-domain generalization over baseline approaches. Our findings highlight the importance of integrating cognitive processes into MLLMs for human-like visual perception and open new directions for multimodal reasoning research. The data, codes, and models are released at https://github.com/thunlp/DeepPerception.', 'score': 17, 'issue_id': 2777, 'pub_date': '2025-03-17', 'pub_date_card': {'ru': '17 марта', 'en': 'March 17', 'zh': '3月17日'}, 'hash': 'c682a086aaac0fa1', 'authors': ['Xinyu Ma', 'Ziyang Ding', 'Zhicong Luo', 'Chi Chen', 'Zonghao Guo', 'Derek F. Wong', 'Xiaoyi Feng', 'Maosong Sun'], 'affiliations': ['Northwestern Polytechnical University', 'Shandong University', 'Tsinghua University', 'University of Macau'], 'pdf_title_img': 'assets/pdf/title_img/2503.12797.jpg', 'data': {'categories': ['#multimodal', '#data', '#dataset', '#reasoning', '#synthetic', '#benchmark', '#transfer_learning', '#training'], 'emoji': '🔬', 'ru': {'title': 'DeepPerception: Улучшение визуального восприятия ИИ через интеграцию знаний', 'desc': 'Эта статья представляет новый подход к улучшению визуального восприятия мультимодальных языковых моделей (MLLM). Авторы вводят задачу knowledge-intensive visual grounding (KVG), требующую тонкого визуального различения и интеграции специализированных знаний. Предлагается модель DeepPerception, использующая автоматическую генерацию обучающих данных и двухэтапное обучение для улучшения когнитивных способностей MLLM. Эксперименты на созданном датасете KVG-Bench показывают значительное улучшение точности и обобщающей способности модели.'}, 'en': {'title': 'Enhancing Visual Perception in MLLMs with DeepPerception', 'desc': 'This paper addresses the limitations of Multimodal Large Language Models (MLLMs) in visual perception and reasoning. It introduces a new task called knowledge-intensive visual grounding (KVG), which combines fine-grained visual discrimination with domain-specific knowledge. The authors propose DeepPerception, an enhanced MLLM that utilizes a data synthesis pipeline and a two-stage training framework to improve cognitive reasoning and perception integration. Experimental results show that DeepPerception outperforms existing methods, demonstrating the potential for MLLMs to achieve human-like visual understanding.'}, 'zh': {'title': '提升视觉感知的认知整合能力', 'desc': '人类专家在细粒度视觉辨别方面表现出色，能够利用领域知识来优化感知特征，而当前的多模态大型语言模型（MLLMs）在这方面仍显不足。尽管拥有丰富的专家级知识，MLLMs在视觉感知中整合推理的能力较弱，常常直接生成响应而缺乏深入分析。为了解决这一问题，我们提出了知识密集型视觉定位（KVG），这是一项新颖的视觉定位任务，要求同时具备细粒度感知和领域特定知识的整合。我们提出的DeepPerception模型增强了认知视觉感知能力，通过自动化数据合成和两阶段训练框架，显著提高了在KVG-Bench数据集上的准确性和跨领域泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2503.12329', 'title': 'CapArena: Benchmarking and Analyzing Detailed Image Captioning in the\n  LLM Era', 'url': 'https://huggingface.co/papers/2503.12329', 'abstract': 'Image captioning has been a longstanding challenge in vision-language research. With the rise of LLMs, modern Vision-Language Models (VLMs) generate detailed and comprehensive image descriptions. However, benchmarking the quality of such captions remains unresolved. This paper addresses two key questions: (1) How well do current VLMs actually perform on image captioning, particularly compared to humans? We built CapArena, a platform with over 6000 pairwise caption battles and high-quality human preference votes. Our arena-style evaluation marks a milestone, showing that leading models like GPT-4o achieve or even surpass human performance, while most open-source models lag behind. (2) Can automated metrics reliably assess detailed caption quality? Using human annotations from CapArena, we evaluate traditional and recent captioning metrics, as well as VLM-as-a-Judge. Our analysis reveals that while some metrics (e.g., METEOR) show decent caption-level agreement with humans, their systematic biases lead to inconsistencies in model ranking. In contrast, VLM-as-a-Judge demonstrates robust discernment at both the caption and model levels. Building on these insights, we release CapArena-Auto, an accurate and efficient automated benchmark for detailed captioning, achieving 94.3% correlation with human rankings at just $4 per test. Data and resources will be open-sourced at https://caparena.github.io.', 'score': 16, 'issue_id': 2778, 'pub_date': '2025-03-16', 'pub_date_card': {'ru': '16 марта', 'en': 'March 16', 'zh': '3月16日'}, 'hash': 'c97a8c730bfcbfa8', 'authors': ['Kanzhi Cheng', 'Wenpo Song', 'Jiaxin Fan', 'Zheng Ma', 'Qiushi Sun', 'Fangzhi Xu', 'Chenyang Yan', 'Nuo Chen', 'Jianbing Zhang', 'Jiajun Chen'], 'affiliations': ['National Key Laboratory for Novel Software Technology, Nanjing University', 'Shanghai Artificial Intelligence Laboratory', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2503.12329.jpg', 'data': {'categories': ['#benchmark', '#games', '#open_source', '#cv'], 'emoji': '📸', 'ru': {'title': 'Новый рубеж в генерации подписей к изображениям: ИИ догоняет человека', 'desc': 'Статья посвящена проблеме оценки качества генерации подписей к изображениям с помощью современных моделей компьютерного зрения и обработки естественного языка (VLM). Авторы создали платформу CapArena для сравнения подписей, сгенерированных моделями и людьми, показав, что некоторые модели (например, GPT-4o) достигают или превосходят человеческий уровень. Исследование также оценивает эффективность автоматических метрик для оценки качества подписей, выявляя их ограничения. На основе полученных результатов авторы разработали CapArena-Auto - автоматизированный бенчмарк для оценки детальных подписей к изображениям.'}, 'en': {'title': 'Elevating Image Captioning: Human-Level Performance and Reliable Metrics', 'desc': 'This paper tackles the challenge of evaluating image captioning performance in Vision-Language Models (VLMs). It introduces CapArena, a platform that conducts over 6000 pairwise caption battles to compare VLM outputs with human-generated captions. The findings indicate that advanced models like GPT-4o can match or exceed human performance, while many open-source models do not. Additionally, the study assesses various automated metrics for caption quality, revealing that while some metrics align with human preferences, VLM-as-a-Judge provides a more reliable evaluation method, leading to the development of CapArena-Auto for efficient benchmarking.'}, 'zh': {'title': '图像描述的新标准：VLM的崛起与评估', 'desc': '图像描述一直是视觉与语言研究中的一个重要挑战。随着大型语言模型（LLMs）的发展，现代视觉-语言模型（VLMs）能够生成详细且全面的图像描述。本文通过建立CapArena平台，评估当前VLM在图像描述任务上的表现，发现领先模型如GPT-4o的表现甚至超过了人类。我们还分析了自动评估指标的可靠性，结果表明VLM作为评判者在描述质量评估中表现出色，提供了一种新的高效基准方法。'}}}, {'id': 'https://huggingface.co/papers/2503.13424', 'title': 'Infinite Mobility: Scalable High-Fidelity Synthesis of Articulated\n  Objects via Procedural Generation', 'url': 'https://huggingface.co/papers/2503.13424', 'abstract': 'Large-scale articulated objects with high quality are desperately needed for multiple tasks related to embodied AI. Most existing methods for creating articulated objects are either data-driven or simulation based, which are limited by the scale and quality of the training data or the fidelity and heavy labour of the simulation. In this paper, we propose Infinite Mobility, a novel method for synthesizing high-fidelity articulated objects through procedural generation. User study and quantitative evaluation demonstrate that our method can produce results that excel current state-of-the-art methods and are comparable to human-annotated datasets in both physics property and mesh quality. Furthermore, we show that our synthetic data can be used as training data for generative models, enabling next-step scaling up. Code is available at https://github.com/Intern-Nexus/Infinite-Mobility', 'score': 13, 'issue_id': 2776, 'pub_date': '2025-03-17', 'pub_date_card': {'ru': '17 марта', 'en': 'March 17', 'zh': '3月17日'}, 'hash': '20793013b58dba36', 'authors': ['Xinyu Lian', 'Zichao Yu', 'Ruiming Liang', 'Yitong Wang', 'Li Ray Luo', 'Kaixu Chen', 'Yuanzhen Zhou', 'Qihong Tang', 'Xudong Xu', 'Zhaoyang Lyu', 'Bo Dai', 'Jiangmiao Pang'], 'affiliations': ['Fudan University', 'Harbin Institute of Technology, Shenzhen', 'Institute of Automation, Chinese Academy of Sciences', 'School of Artificial Intelligence, University of Chinese Academy of Sciences', 'Shanghai Artificial Intelligence Laboratory', 'South China University of Technology', 'The University of Hong Kong', 'Tongji University', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2503.13424.jpg', 'data': {'categories': ['#3d', '#open_source', '#dataset', '#synthetic'], 'emoji': '🤖', 'ru': {'title': 'Процедурная генерация сочлененных объектов для воплощенного ИИ', 'desc': 'Статья представляет новый метод под названием Infinite Mobility для синтеза высококачественных сочлененных объектов с помощью процедурной генерации. Авторы утверждают, что их подход превосходит современные методы и сравним с датасетами, размеченными вручную, по физическим свойствам и качеству полигональных сеток. Метод решает проблему ограниченности существующих подходов, основанных на данных или симуляции. Синтетические данные, полученные с помощью Infinite Mobility, могут использоваться для обучения генеративных моделей, что открывает возможности для масштабирования.'}, 'en': {'title': 'Revolutionizing Articulated Object Creation with Infinite Mobility', 'desc': 'This paper introduces Infinite Mobility, a new approach for creating high-quality articulated objects using procedural generation techniques. Unlike traditional methods that rely on limited data or labor-intensive simulations, Infinite Mobility generates objects that maintain high fidelity in both physical properties and mesh quality. The authors conducted user studies and quantitative evaluations, showing that their method outperforms existing state-of-the-art techniques and rivals human-annotated datasets. Additionally, the synthetic data produced can be utilized for training generative models, facilitating further advancements in the field of embodied AI.'}, 'zh': {'title': '无限移动：合成高保真关节物体的新方法', 'desc': '在这篇论文中，我们提出了一种名为无限移动（Infinite Mobility）的方法，用于通过程序生成合成高保真度的关节物体。这种方法克服了现有数据驱动或模拟方法在规模和质量上的限制。用户研究和定量评估表明，我们的方法在物理属性和网格质量上超越了当前最先进的方法，并且与人工标注的数据集相当。此外，我们的合成数据可以作为生成模型的训练数据，支持后续的扩展。'}}}, {'id': 'https://huggingface.co/papers/2503.14476', 'title': 'DAPO: An Open-Source LLM Reinforcement Learning System at Scale', 'url': 'https://huggingface.co/papers/2503.14476', 'abstract': 'Inference scaling empowers LLMs with unprecedented reasoning ability, with reinforcement learning as the core technique to elicit complex reasoning. However, key technical details of state-of-the-art reasoning LLMs are concealed (such as in OpenAI o1 blog and DeepSeek R1 technical report), thus the community still struggles to reproduce their RL training results. We propose the Decoupled Clip and Dynamic sAmpling Policy Optimization (DAPO) algorithm, and fully open-source a state-of-the-art large-scale RL system that achieves 50 points on AIME 2024 using Qwen2.5-32B base model. Unlike previous works that withhold training details, we introduce four key techniques of our algorithm that make large-scale LLM RL a success. In addition, we open-source our training code, which is built on the verl framework, along with a carefully curated and processed dataset. These components of our open-source system enhance reproducibility and support future research in large-scale LLM RL.', 'score': 10, 'issue_id': 2777, 'pub_date': '2025-03-18', 'pub_date_card': {'ru': '18 марта', 'en': 'March 18', 'zh': '3月18日'}, 'hash': '5b4841d2845817e8', 'authors': ['Qiying Yu', 'Zheng Zhang', 'Ruofei Zhu', 'Yufeng Yuan', 'Xiaochen Zuo', 'Yu Yue', 'Tiantian Fan', 'Gaohong Liu', 'Lingjun Liu', 'Xin Liu', 'Haibin Lin', 'Zhiqi Lin', 'Bole Ma', 'Guangming Sheng', 'Yuxuan Tong', 'Chi Zhang', 'Mofan Zhang', 'Wang Zhang', 'Hang Zhu', 'Jinhua Zhu', 'Jiaze Chen', 'Jiangjie Chen', 'Chengyi Wang', 'Hongli Yu', 'Weinan Dai', 'Yuxuan Song', 'Xiangpeng Wei', 'Hao Zhou', 'Jingjing Liu', 'Wei-Ying Ma', 'Ya-Qin Zhang', 'Lin Yan', 'Mu Qiao', 'Yonghui Wu', 'Mingxuan Wang'], 'affiliations': ['ByteDance', 'Institute for AI Industry Research (AIR), Tsinghua University', 'SIA-Lab of Tsinghua AIR and ByteDance', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2503.14476.jpg', 'data': {'categories': ['#rl', '#dataset', '#reasoning', '#optimization', '#training', '#open_source'], 'emoji': '🧠', 'ru': {'title': 'Открытая система RL для улучшения рассуждений больших языковых моделей', 'desc': 'Статья представляет алгоритм DAPO для обучения с подкреплением крупномасштабных языковых моделей. Авторы открыто публикуют систему, достигающую 50 баллов на AIME 2024 с использованием базовой модели Qwen2.5-32B. В работе раскрываются четыре ключевые техники, делающие успешным RL для больших ЯМ. Исследователи также предоставляют код обучения и подготовленный набор данных для воспроизводимости результатов.'}, 'en': {'title': 'Unlocking LLM Potential with Open-Source Reinforcement Learning', 'desc': 'This paper introduces the Decoupled Clip and Dynamic sAmpling Policy Optimization (DAPO) algorithm, which enhances the reasoning capabilities of large language models (LLMs) through reinforcement learning (RL). The authors address the lack of transparency in existing state-of-the-art LLMs by providing detailed insights into their training processes and techniques. They report achieving a significant performance score of 50 points on the AIME 2024 benchmark using the Qwen2.5-32B model. By open-sourcing their training code and dataset, they aim to improve reproducibility and foster further advancements in large-scale LLM reinforcement learning research.'}, 'zh': {'title': '解锁大型语言模型的推理潜力', 'desc': '本论文提出了一种新的算法，称为解耦剪辑和动态采样策略优化（DAPO），旨在提升大型语言模型（LLM）的推理能力。我们通过强化学习技术，成功实现了在AIME 2024上获得50分的成绩，使用的是Qwen2.5-32B基础模型。与以往研究不同，我们公开了四个关键技术细节，帮助社区更好地理解和复现我们的训练结果。此外，我们还开源了训练代码和经过精心处理的数据集，以促进大型LLM强化学习的可重复性和未来研究。'}}}, {'id': 'https://huggingface.co/papers/2503.14125', 'title': 'Frac-Connections: Fractional Extension of Hyper-Connections', 'url': 'https://huggingface.co/papers/2503.14125', 'abstract': 'Residual connections are central to modern deep learning architectures, enabling the training of very deep networks by mitigating gradient vanishing. Hyper-Connections recently generalized residual connections by introducing multiple connection strengths at different depths, thereby addressing the seesaw effect between gradient vanishing and representation collapse. However, Hyper-Connections increase memory access costs by expanding the width of hidden states. In this paper, we propose Frac-Connections, a novel approach that divides hidden states into multiple parts rather than expanding their width. Frac-Connections retain partial benefits of Hyper-Connections while reducing memory consumption. To validate their effectiveness, we conduct large-scale experiments on language tasks, with the largest being a 7B MoE model trained on up to 3T tokens, demonstrating that Frac-Connections significantly outperform residual connections.', 'score': 8, 'issue_id': 2776, 'pub_date': '2025-03-18', 'pub_date_card': {'ru': '18 марта', 'en': 'March 18', 'zh': '3月18日'}, 'hash': '936ea0aa4972c382', 'authors': ['Defa Zhu', 'Hongzhi Huang', 'Jundong Zhou', 'Zihao Huang', 'Yutao Zeng', 'Banggu Wu', 'Qiyang Min', 'Xun Zhou'], 'affiliations': ['ByteDance Seed'], 'pdf_title_img': 'assets/pdf/title_img/2503.14125.jpg', 'data': {'categories': ['#optimization', '#architecture', '#training'], 'emoji': '🧩', 'ru': {'title': 'Frac-Connections: оптимизация глубоких нейросетей без расширения скрытых состояний', 'desc': 'Статья представляет новый подход к архитектуре глубоких нейронных сетей под названием Frac-Connections. Этот метод развивает идею остаточных соединений (residual connections), разделяя скрытые состояния на несколько частей вместо расширения их ширины. Frac-Connections сохраняют преимущества Hyper-Connections, при этом снижая потребление памяти. Эффективность подхода подтверждена масштабными экспериментами на языковых задачах, включая обучение модели MoE размером 7 миллиардов параметров на 3 триллионах токенов.'}, 'en': {'title': 'Frac-Connections: Efficient Memory for Deep Learning', 'desc': 'This paper introduces Frac-Connections, a new method that improves upon Hyper-Connections in deep learning. While Hyper-Connections allow for multiple connection strengths to combat issues like gradient vanishing, they also increase memory usage due to wider hidden states. Frac-Connections address this by splitting hidden states into smaller parts, maintaining some advantages of Hyper-Connections while reducing memory costs. The authors demonstrate the effectiveness of Frac-Connections through extensive experiments on language tasks, showing superior performance compared to traditional residual connections.'}, 'zh': {'title': 'Frac-Connections：优化深度学习的内存使用', 'desc': '残差连接是现代深度学习架构的核心，能够通过减轻梯度消失问题来训练非常深的网络。超连接最近通过在不同深度引入多个连接强度来推广残差连接，从而解决了梯度消失与表示崩溃之间的摇摆效应。然而，超连接通过扩展隐藏状态的宽度增加了内存访问成本。我们提出了Frac-Connections，这是一种新方法，通过将隐藏状态划分为多个部分而不是扩展其宽度，保留了超连接的部分优势，同时减少了内存消耗。'}}}, {'id': 'https://huggingface.co/papers/2503.14499', 'title': 'Measuring AI Ability to Complete Long Tasks', 'url': 'https://huggingface.co/papers/2503.14499', 'abstract': "Despite rapid progress on AI benchmarks, the real-world meaning of benchmark performance remains unclear. To quantify the capabilities of AI systems in terms of human capabilities, we propose a new metric: 50%-task-completion time horizon. This is the time humans typically take to complete tasks that AI models can complete with 50% success rate. We first timed humans with relevant domain expertise on a combination of RE-Bench, HCAST, and 66 novel shorter tasks. On these tasks, current frontier AI models such as Claude 3.7 Sonnet have a 50% time horizon of around 50 minutes. Furthermore, frontier AI time horizon has been doubling approximately every seven months since 2019, though the trend may have accelerated in 2024. The increase in AI models' time horizons seems to be primarily driven by greater reliability and ability to adapt to mistakes, combined with better logical reasoning and tool use capabilities. We discuss the limitations of our results -- including their degree of external validity -- and the implications of increased autonomy for dangerous capabilities. If these results generalize to real-world software tasks, extrapolation of this trend predicts that within 5 years, AI systems will be capable of automating many software tasks that currently take humans a month.", 'score': 4, 'issue_id': 2777, 'pub_date': '2025-03-18', 'pub_date_card': {'ru': '18 марта', 'en': 'March 18', 'zh': '3月18日'}, 'hash': 'c31aeeed7f6139af', 'authors': ['Thomas Kwa', 'Ben West', 'Joel Becker', 'Amy Deng', 'Katharyn Garcia', 'Max Hasin', 'Sami Jawhar', 'Megan Kinniment', 'Nate Rush', 'Sydney Von Arx', 'Ryan Bloom', 'Thomas Broadley', 'Haoxing Du', 'Brian Goodrich', 'Nikola Jurkovic', 'Luke Harold Miles', 'Seraphina Nix', 'Tao Lin', 'Neev Parikh', 'David Rein', 'Lucas Jun Koba Sato', 'Hjalmar Wijk', 'Daniel M. Ziegler', 'Elizabeth Barnes', 'Lawrence Chan'], 'affiliations': ['Anthropic', 'Model Evaluation & Threat Research (METR)'], 'pdf_title_img': 'assets/pdf/title_img/2503.14499.jpg', 'data': {'categories': ['#ethics', '#benchmark', '#reasoning'], 'emoji': '⏳', 'ru': {'title': 'Время на вашей стороне: ИИ догоняет человека в скорости выполнения задач', 'desc': 'Статья предлагает новую метрику для оценки возможностей ИИ-систем - горизонт времени 50%-ного выполнения задач. Этот показатель измеряет время, которое обычно требуется людям для выполнения задач, которые модели ИИ могут выполнить с 50%-ной вероятностью успеха. Исследование показало, что современные передовые модели ИИ, такие как Claude 3.7 Sonnet, имеют горизонт времени около 50 минут. Авторы отмечают, что горизонт времени ИИ удваивается примерно каждые семь месяцев с 2019 года, и обсуждают потенциальные последствия повышения автономности ИИ-систем.'}, 'en': {'title': 'Measuring AI Progress: The 50%-Task-Completion Time Horizon', 'desc': 'This paper introduces a new metric called the 50%-task-completion time horizon to evaluate AI systems based on human performance. It measures the time it takes for humans to complete tasks that AI can accomplish with a 50% success rate. The study found that leading AI models, like Claude 3.7 Sonnet, have a time horizon of about 50 minutes, and this capability has been improving rapidly, doubling approximately every seven months. The authors highlight the implications of this trend, suggesting that AI could soon automate complex software tasks that currently require significant human effort.'}, 'zh': {'title': 'AI能力的新度量：50%任务完成时间', 'desc': '尽管人工智能在基准测试上取得了快速进展，但其实际表现的意义仍不明确。我们提出了一种新的度量标准：50%任务完成时间范围，旨在量化AI系统与人类能力的对比。通过对人类专家在多个任务上的完成时间进行测量，我们发现当前的前沿AI模型在这些任务上的50%时间范围约为50分钟。我们的研究表明，AI模型的时间范围自2019年以来大约每七个月翻倍，未来五年内，AI系统可能能够自动化许多目前需要人类一个月才能完成的软件任务。'}}}, {'id': 'https://huggingface.co/papers/2503.14495', 'title': 'Temporal Consistency for LLM Reasoning Process Error Identification', 'url': 'https://huggingface.co/papers/2503.14495', 'abstract': 'Verification is crucial for effective mathematical reasoning. We present a new temporal consistency method where verifiers iteratively refine their judgments based on the previous assessment. Unlike one-round verification or multi-model debate approaches, our method leverages consistency in a sequence of self-reflection actions to improve verification accuracy. Empirical evaluations across diverse mathematical process error identification benchmarks (Mathcheck, ProcessBench, and PRM800K) show consistent performance improvements over baseline methods. When applied to the recent DeepSeek R1 distilled models, our method demonstrates strong performance, enabling 7B/8B distilled models to outperform all 70B/72B models and GPT-4o on ProcessBench. Notably, the distilled 14B model with our method achieves performance comparable to Deepseek-R1. Our codes are available at https://github.com/jcguo123/Temporal-Consistency', 'score': 4, 'issue_id': 2779, 'pub_date': '2025-03-18', 'pub_date_card': {'ru': '18 марта', 'en': 'March 18', 'zh': '3月18日'}, 'hash': '590d8cdf2ae26151', 'authors': ['Jiacheng Guo', 'Yue Wu', 'Jiahao Qiu', 'Kaixuan Huang', 'Xinzhe Juan', 'Ling Yang', 'Mengdi Wang'], 'affiliations': ['AI Lab, Princeton University', 'Department of Computer Science & Engineering, University of Michigan', 'Department of Electrical & Computer Engineering, Princeton University'], 'pdf_title_img': 'assets/pdf/title_img/2503.14495.jpg', 'data': {'categories': ['#math', '#benchmark', '#optimization', '#reasoning', '#training', '#architecture'], 'emoji': '🧮', 'ru': {'title': 'Повышение точности верификации математических рассуждений через временную согласованность', 'desc': 'Статья представляет новый метод временной согласованности для верификации математических рассуждений. В отличие от одноразовой проверки или подходов с участием нескольких моделей, этот метод использует последовательность действий самоанализа для повышения точности верификации. Эмпирические оценки на различных эталонных тестах по выявлению ошибок в математических процессах показали стабильное улучшение производительности по сравнению с базовыми методами. При применении к недавно дистиллированным моделям DeepSeek R1 метод продемонстрировал высокую эффективность, позволив моделям 7B/8B превзойти все модели 70B/72B и GPT-4o на тесте ProcessBench.'}, 'en': {'title': 'Enhancing Verification Accuracy through Temporal Consistency', 'desc': 'This paper introduces a novel temporal consistency method for enhancing verification in mathematical reasoning. The approach allows verifiers to iteratively refine their judgments based on previous assessments, improving accuracy over traditional one-round verification methods. By utilizing a sequence of self-reflection actions, the method shows significant performance gains on various benchmarks for identifying mathematical process errors. Notably, it enables smaller distilled models to outperform larger models, demonstrating its effectiveness in practical applications.'}, 'zh': {'title': '提升数学验证的时间一致性方法', 'desc': '本文提出了一种新的时间一致性验证方法，旨在提高数学推理的有效性。该方法通过迭代地根据之前的评估来细化判断，克服了单轮验证和多模型辩论的局限性。通过在多个数学过程错误识别基准（如Mathcheck、ProcessBench和PRM800K）上的实证评估，显示出相较于基线方法的一致性性能提升。应用于最新的DeepSeek R1蒸馏模型时，我们的方法使得7B/8B蒸馏模型在ProcessBench上超越了所有70B/72B模型和GPT-4o。'}}}, {'id': 'https://huggingface.co/papers/2503.14492', 'title': 'Cosmos-Transfer1: Conditional World Generation with Adaptive Multimodal\n  Control', 'url': 'https://huggingface.co/papers/2503.14492', 'abstract': 'We introduce Cosmos-Transfer, a conditional world generation model that can generate world simulations based on multiple spatial control inputs of various modalities such as segmentation, depth, and edge. In the design, the spatial conditional scheme is adaptive and customizable. It allows weighting different conditional inputs differently at different spatial locations. This enables highly controllable world generation and finds use in various world-to-world transfer use cases, including Sim2Real. We conduct extensive evaluations to analyze the proposed model and demonstrate its applications for Physical AI, including robotics Sim2Real and autonomous vehicle data enrichment. We further demonstrate an inference scaling strategy to achieve real-time world generation with an NVIDIA GB200 NVL72 rack. To help accelerate research development in the field, we open-source our models and code at https://github.com/nvidia-cosmos/cosmos-transfer1.', 'score': 4, 'issue_id': 2777, 'pub_date': '2025-03-18', 'pub_date_card': {'ru': '18 марта', 'en': 'March 18', 'zh': '3月18日'}, 'hash': '5098c043161888fa', 'authors': ['NVIDIA', ':', 'Hassan Abu Alhaija', 'Jose Alvarez', 'Maciej Bala', 'Tiffany Cai', 'Tianshi Cao', 'Liz Cha', 'Joshua Chen', 'Mike Chen', 'Francesco Ferroni', 'Sanja Fidler', 'Dieter Fox', 'Yunhao Ge', 'Jinwei Gu', 'Ali Hassani', 'Michael Isaev', 'Pooya Jannaty', 'Shiyi Lan', 'Tobias Lasser', 'Huan Ling', 'Ming-Yu Liu', 'Xian Liu', 'Yifan Lu', 'Alice Luo', 'Qianli Ma', 'Hanzi Mao', 'Fabio Ramos', 'Xuanchi Ren', 'Tianchang Shen', 'Shitao Tang', 'Ting-Chun Wang', 'Jay Wu', 'Jiashu Xu', 'Stella Xu', 'Kevin Xie', 'Yuchong Ye', 'Xiaodong Yang', 'Xiaohui Zeng', 'Yu Zeng'], 'affiliations': ['NVIDIA'], 'pdf_title_img': 'assets/pdf/title_img/2503.14492.jpg', 'data': {'categories': ['#agents', '#robotics', '#transfer_learning', '#open_source', '#inference', '#3d'], 'emoji': '🌌', 'ru': {'title': 'Адаптивная генерация миров с пространственным контролем', 'desc': 'Cosmos-Transfer - это условная модель генерации миров, способная создавать симуляции на основе нескольких пространственных входных данных разных модальностей. Модель позволяет гибко настраивать веса различных условных входов в разных пространственных локациях. Это обеспечивает высоко контролируемую генерацию миров и находит применение в различных сценариях переноса между мирами, включая Sim2Real. Авторы провели обширные оценки модели и продемонстрировали ее применение для физического ИИ, в том числе для робототехники Sim2Real и обогащения данных для беспилотных автомобилей.'}, 'en': {'title': 'Empowering World Generation with Adaptive Control', 'desc': 'Cosmos-Transfer is a novel model designed for generating world simulations using various spatial control inputs like segmentation, depth, and edge maps. It features an adaptive and customizable spatial conditional scheme that allows for different weights to be assigned to inputs based on their spatial locations. This flexibility enhances the controllability of world generation, making it suitable for applications such as Sim2Real in robotics and autonomous vehicles. The model has been extensively evaluated, and its code is open-sourced to foster further research in the field.'}, 'zh': {'title': '可控的世界生成，助力物理人工智能', 'desc': '我们介绍了Cosmos-Transfer，这是一种条件世界生成模型，可以根据多种空间控制输入（如分割、深度和边缘）生成世界模拟。在设计上，空间条件方案是自适应和可定制的，允许在不同空间位置对不同条件输入进行加权。这使得世界生成具有高度可控性，并在多种世界到世界的转移应用中找到用途，包括Sim2Real。我们进行了广泛的评估，分析了所提出的模型，并展示了其在物理人工智能中的应用，包括机器人Sim2Real和自动驾驶车辆数据增强。'}}}, {'id': 'https://huggingface.co/papers/2503.12505', 'title': 'MPBench: A Comprehensive Multimodal Reasoning Benchmark for Process\n  Errors Identification', 'url': 'https://huggingface.co/papers/2503.12505', 'abstract': 'Reasoning is an essential capacity for large language models (LLMs) to address complex tasks, where the identification of process errors is vital for improving this ability. Recently, process-level reward models (PRMs) were proposed to provide step-wise rewards that facilitate reinforcement learning and data production during training and guide LLMs toward correct steps during inference, thereby improving reasoning accuracy. However, existing benchmarks of PRMs are text-based and focus on error detection, neglecting other scenarios like reasoning search. To address this gap, we introduce MPBench, a comprehensive, multi-task, multimodal benchmark designed to systematically assess the effectiveness of PRMs in diverse scenarios. MPBench employs three evaluation paradigms, each targeting a specific role of PRMs in the reasoning process: (1) Step Correctness, which assesses the correctness of each intermediate reasoning step; (2) Answer Aggregation, which aggregates multiple solutions and selects the best one; and (3) Reasoning Process Search, which guides the search for optimal reasoning steps during inference. Through these paradigms, MPBench makes comprehensive evaluations and provides insights into the development of multimodal PRMs.', 'score': 4, 'issue_id': 2776, 'pub_date': '2025-03-16', 'pub_date_card': {'ru': '16 марта', 'en': 'March 16', 'zh': '3月16日'}, 'hash': '0ac4fdd3411855ac', 'authors': ['Zhaopan Xu', 'Pengfei Zhou', 'Jiaxin Ai', 'Wangbo Zhao', 'Kai Wang', 'Xiaojiang Peng', 'Wenqi Shao', 'Hongxun Yao', 'Kaipeng Zhang'], 'affiliations': ['HIT', 'NUS', 'SZTU', 'Shanghai AI Laboratory', 'Shanghai Innovation Institute'], 'pdf_title_img': 'assets/pdf/title_img/2503.12505.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#multimodal', '#rl'], 'emoji': '🧠', 'ru': {'title': 'MPBench: Комплексная оценка рассуждений языковых моделей', 'desc': 'Статья представляет новый бенчмарк MPBench для оценки эффективности моделей вознаграждения на уровне процесса (PRM) в различных сценариях рассуждений. MPBench включает три парадигмы оценки: корректность шагов, агрегацию ответов и поиск процесса рассуждений. Бенчмарк охватывает мультимодальные задачи и позволяет всесторонне оценить PRM в контексте рассуждений языковых моделей. Это помогает улучшить способности больших языковых моделей (LLM) к рассуждениям и выявлению ошибок в процессе.'}, 'en': {'title': 'Enhancing Reasoning in LLMs with MPBench', 'desc': "This paper discusses the importance of reasoning in large language models (LLMs) and introduces process-level reward models (PRMs) to enhance their reasoning capabilities. PRMs provide step-wise rewards that help LLMs learn from their mistakes during training and improve their decision-making during inference. The authors identify a gap in existing benchmarks, which primarily focus on error detection, and propose MPBench, a new benchmark that evaluates PRMs across various reasoning scenarios. MPBench includes three evaluation paradigms: assessing step correctness, aggregating answers, and guiding the reasoning process search, offering a comprehensive framework for understanding PRMs' effectiveness."}, 'zh': {'title': '全面评估推理过程的多模态基准', 'desc': '推理是大型语言模型（LLMs）处理复杂任务的重要能力，而识别过程错误对于提升这一能力至关重要。最近提出的过程级奖励模型（PRMs）通过提供逐步奖励，促进了强化学习和数据生成，从而在推理过程中引导LLMs走向正确的步骤，提高推理准确性。然而，现有的PRMs基准主要基于文本，专注于错误检测，忽视了推理搜索等其他场景。为了解决这一问题，我们引入了MPBench，这是一个全面的多任务多模态基准，旨在系统评估PRMs在不同场景中的有效性。'}}}, {'id': 'https://huggingface.co/papers/2503.14504', 'title': 'Aligning Multimodal LLM with Human Preference: A Survey', 'url': 'https://huggingface.co/papers/2503.14504', 'abstract': 'Large language models (LLMs) can handle a wide variety of general tasks with simple prompts, without the need for task-specific training. Multimodal Large Language Models (MLLMs), built upon LLMs, have demonstrated impressive potential in tackling complex tasks involving visual, auditory, and textual data. However, critical issues related to truthfulness, safety, o1-like reasoning, and alignment with human preference remain insufficiently addressed. This gap has spurred the emergence of various alignment algorithms, each targeting different application scenarios and optimization goals. Recent studies have shown that alignment algorithms are a powerful approach to resolving the aforementioned challenges. In this paper, we aim to provide a comprehensive and systematic review of alignment algorithms for MLLMs. Specifically, we explore four key aspects: (1) the application scenarios covered by alignment algorithms, including general image understanding, multi-image, video, and audio, and extended multimodal applications; (2) the core factors in constructing alignment datasets, including data sources, model responses, and preference annotations; (3) the benchmarks used to evaluate alignment algorithms; and (4) a discussion of potential future directions for the development of alignment algorithms. This work seeks to help researchers organize current advancements in the field and inspire better alignment methods. The project page of this paper is available at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Alignment.', 'score': 3, 'issue_id': 2778, 'pub_date': '2025-03-18', 'pub_date_card': {'ru': '18 марта', 'en': 'March 18', 'zh': '3月18日'}, 'hash': 'b33bcba515cfa942', 'authors': ['Tao Yu', 'Yi-Fan Zhang', 'Chaoyou Fu', 'Junkang Wu', 'Jinda Lu', 'Kun Wang', 'Xingyu Lu', 'Yunhang Shen', 'Guibin Zhang', 'Dingjie Song', 'Yibo Yan', 'Tianlong Xu', 'Qingsong Wen', 'Zhang Zhang', 'Yan Huang', 'Liang Wang', 'Tieniu Tan'], 'affiliations': ['Institute of automation, Chinese academy of science', 'Lehigh University', 'Nanjing University', 'Nanyang Technological University', 'National University of Singapore', 'Shenzhen International Graduate School, Tsinghua University', 'Squirrel Ai Learning', 'Tencent Youtu Lab', 'The Hong Kong University of Science and Technology', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2503.14504.jpg', 'data': {'categories': ['#survey', '#multimodal', '#benchmark', '#dataset', '#alignment'], 'emoji': '🤖', 'ru': {'title': 'Выравнивание мультимодальных ИИ: путь к безопасности и эффективности', 'desc': 'Эта статья представляет собой всесторонний обзор алгоритмов выравнивания для мультимодальных больших языковых моделей (MLLM). Авторы рассматривают различные сценарии применения этих алгоритмов, включая понимание изображений, видео и аудио. Они анализируют ключевые факторы в создании наборов данных для выравнивания и обсуждают бенчмарки для оценки эффективности алгоритмов. Статья также предлагает потенциальные направления для будущих исследований в этой области.'}, 'en': {'title': 'Aligning MLLMs: Bridging Gaps for Better Understanding', 'desc': 'This paper reviews alignment algorithms for Multimodal Large Language Models (MLLMs), which integrate visual, auditory, and textual data. It highlights the challenges of truthfulness, safety, and reasoning in MLLMs, emphasizing the need for effective alignment strategies. The authors categorize alignment algorithms based on application scenarios, dataset construction, and evaluation benchmarks. The goal is to provide a structured overview that aids researchers in advancing alignment techniques for MLLMs.'}, 'zh': {'title': '对齐算法助力多模态语言模型的未来', 'desc': '大型语言模型（LLMs）能够通过简单的提示处理各种通用任务，而无需特定任务的训练。多模态大型语言模型（MLLMs）在处理涉及视觉、听觉和文本数据的复杂任务方面展现了令人印象深刻的潜力。然而，关于真实性、安全性、类o1推理和与人类偏好的对齐等关键问题仍未得到充分解决。本文旨在系统性地回顾MLLMs的对齐算法，探讨其应用场景、数据集构建核心因素、评估基准以及未来发展方向。'}}}, {'id': 'https://huggingface.co/papers/2503.12545', 'title': 'PEBench: A Fictitious Dataset to Benchmark Machine Unlearning for\n  Multimodal Large Language Models', 'url': 'https://huggingface.co/papers/2503.12545', 'abstract': 'In recent years, Multimodal Large Language Models (MLLMs) have demonstrated remarkable advancements in tasks such as visual question answering, visual understanding, and reasoning. However, this impressive progress relies on vast amounts of data collected from the internet, raising significant concerns about privacy and security. To address these issues, machine unlearning (MU) has emerged as a promising solution, enabling the removal of specific knowledge from an already trained model without requiring retraining from scratch. Although MU for MLLMs has gained attention, current evaluations of its efficacy remain incomplete, and the underlying problem is often poorly defined, which hinders the development of strategies for creating more secure and trustworthy systems. To bridge this gap, we introduce a benchmark, named PEBench, which includes a dataset of personal entities and corresponding general event scenes, designed to comprehensively assess the performance of MU for MLLMs. Through PEBench, we aim to provide a standardized and robust framework to advance research in secure and privacy-preserving multimodal models. We benchmarked 6 MU methods, revealing their strengths and limitations, and shedding light on key challenges and opportunities for MU in MLLMs.', 'score': 2, 'issue_id': 2776, 'pub_date': '2025-03-16', 'pub_date_card': {'ru': '16 марта', 'en': 'March 16', 'zh': '3月16日'}, 'hash': '8a908fbc8ce24853', 'authors': ['Zhaopan Xu', 'Pengfei Zhou', 'Weidong Tang', 'Jiaxin Ai', 'Wangbo Zhao', 'Xiaojiang Peng', 'Kai Wang', 'Yang You', 'Wenqi Shao', 'Hongxun Yao', 'Kaipeng Zhang'], 'affiliations': ['HIT', 'NUS', 'SZTU', 'Shanghai AI Laboratory', 'Shanghai Innovation Institute', 'XDU'], 'pdf_title_img': 'assets/pdf/title_img/2503.12545.jpg', 'data': {'categories': ['#multimodal', '#ethics', '#security', '#benchmark', '#dataset'], 'emoji': '🔒', 'ru': {'title': 'PEBench: Новый стандарт для оценки безопасности мультимодальных ИИ-моделей', 'desc': 'Статья представляет новый бенчмарк PEBench для оценки эффективности методов машинного разобучения (MU) в мультимодальных больших языковых моделях (MLLM). PEBench включает набор данных с личными сущностями и соответствующими общими сценами событий. Авторы протестировали 6 методов MU, выявив их сильные и слабые стороны. Исследование направлено на продвижение разработки безопасных и конфиденциальных мультимодальных моделей.'}, 'en': {'title': 'Enhancing Privacy in Multimodal Models with Machine Unlearning', 'desc': 'This paper discusses the advancements of Multimodal Large Language Models (MLLMs) in tasks like visual question answering and reasoning, while highlighting the privacy concerns associated with their data usage. It introduces machine unlearning (MU) as a method to selectively remove knowledge from these models without complete retraining. The authors present PEBench, a benchmark designed to evaluate MU methods specifically for MLLMs, using a dataset of personal entities and general event scenes. By benchmarking six MU techniques, the study identifies their strengths and weaknesses, aiming to enhance the security and trustworthiness of multimodal systems.'}, 'zh': {'title': '推动多模态模型的安全与隐私保护', 'desc': '近年来，多模态大型语言模型（MLLMs）在视觉问答、视觉理解和推理等任务上取得了显著进展。然而，这些进展依赖于从互联网收集的大量数据，这引发了隐私和安全方面的重大担忧。为了解决这些问题，机器遗忘（MU）作为一种有前景的解决方案应运而生，能够在不需要从头开始重新训练的情况下，从已训练的模型中删除特定知识。我们引入了一个基准测试PEBench，旨在全面评估MU在MLLMs中的表现，推动安全和隐私保护的多模态模型研究。'}}}, {'id': 'https://huggingface.co/papers/2503.12271', 'title': 'Reflect-DiT: Inference-Time Scaling for Text-to-Image Diffusion\n  Transformers via In-Context Reflection', 'url': 'https://huggingface.co/papers/2503.12271', 'abstract': 'The predominant approach to advancing text-to-image generation has been training-time scaling, where larger models are trained on more data using greater computational resources. While effective, this approach is computationally expensive, leading to growing interest in inference-time scaling to improve performance. Currently, inference-time scaling for text-to-image diffusion models is largely limited to best-of-N sampling, where multiple images are generated per prompt and a selection model chooses the best output. Inspired by the recent success of reasoning models like DeepSeek-R1 in the language domain, we introduce an alternative to naive best-of-N sampling by equipping text-to-image Diffusion Transformers with in-context reflection capabilities. We propose Reflect-DiT, a method that enables Diffusion Transformers to refine their generations using in-context examples of previously generated images alongside textual feedback describing necessary improvements. Instead of passively relying on random sampling and hoping for a better result in a future generation, Reflect-DiT explicitly tailors its generations to address specific aspects requiring enhancement. Experimental results demonstrate that Reflect-DiT improves performance on the GenEval benchmark (+0.19) using SANA-1.0-1.6B as a base model. Additionally, it achieves a new state-of-the-art score of 0.81 on GenEval while generating only 20 samples per prompt, surpassing the previous best score of 0.80, which was obtained using a significantly larger model (SANA-1.5-4.8B) with 2048 samples under the best-of-N approach.', 'score': 1, 'issue_id': 2778, 'pub_date': '2025-03-15', 'pub_date_card': {'ru': '15 марта', 'en': 'March 15', 'zh': '3月15日'}, 'hash': '2f560e2ec0839955', 'authors': ['Shufan Li', 'Konstantinos Kallidromitis', 'Akash Gokul', 'Arsh Koneru', 'Yusuke Kato', 'Kazuki Kozuka', 'Aditya Grover'], 'affiliations': ['Panasonic AI Research', 'Salesforce AI Research', 'UCLA'], 'pdf_title_img': 'assets/pdf/title_img/2503.12271.jpg', 'data': {'categories': ['#optimization', '#cv', '#benchmark', '#inference', '#diffusion', '#reasoning'], 'emoji': '🖼️', 'ru': {'title': 'Reflect-DiT: Умное улучшение генерации изображений через анализ и рефлексию', 'desc': 'Эта статья представляет новый метод Reflect-DiT для улучшения генерации изображений по текстовому описанию. В отличие от традиционного подхода best-of-N, Reflect-DiT позволяет моделям Diffusion Transformer анализировать ранее сгенерированные изображения и текстовую обратную связь для улучшения результатов. Метод показывает улучшение производительности на бенчмарке GenEval, достигая нового рекордного результата 0.81. Reflect-DiT демонстрирует эффективность подхода inference-time scaling для повышения качества генерации изображений.'}, 'en': {'title': 'Refining Image Generation with Reflective Learning', 'desc': 'This paper presents Reflect-DiT, a novel method for enhancing text-to-image generation by incorporating in-context reflection capabilities into Diffusion Transformers. Unlike traditional best-of-N sampling, which generates multiple images and selects the best, Reflect-DiT refines its outputs based on previous generations and textual feedback. This approach allows the model to focus on specific areas for improvement, leading to more tailored and effective image generation. Experimental results show that Reflect-DiT achieves a new state-of-the-art performance on the GenEval benchmark with fewer samples, demonstrating its efficiency compared to larger models.'}, 'zh': {'title': '反思生成，提升图像质量！', 'desc': '本文提出了一种新的文本到图像生成方法，称为Reflect-DiT。该方法通过在生成过程中引入上下文反思能力，帮助Diffusion Transformers根据之前生成的图像和文本反馈进行改进。与传统的最佳N采样方法不同，Reflect-DiT能够针对特定的改进需求进行调整，从而提高生成质量。实验结果表明，Reflect-DiT在GenEval基准测试中表现优异，达到了新的最先进分数。'}}}, {'id': 'https://huggingface.co/papers/2503.10546', 'title': 'KUDA: Keypoints to Unify Dynamics Learning and Visual Prompting for\n  Open-Vocabulary Robotic Manipulation', 'url': 'https://huggingface.co/papers/2503.10546', 'abstract': 'With the rapid advancement of large language models (LLMs) and vision-language models (VLMs), significant progress has been made in developing open-vocabulary robotic manipulation systems. However, many existing approaches overlook the importance of object dynamics, limiting their applicability to more complex, dynamic tasks. In this work, we introduce KUDA, an open-vocabulary manipulation system that integrates dynamics learning and visual prompting through keypoints, leveraging both VLMs and learning-based neural dynamics models. Our key insight is that a keypoint-based target specification is simultaneously interpretable by VLMs and can be efficiently translated into cost functions for model-based planning. Given language instructions and visual observations, KUDA first assigns keypoints to the RGB image and queries the VLM to generate target specifications. These abstract keypoint-based representations are then converted into cost functions, which are optimized using a learned dynamics model to produce robotic trajectories. We evaluate KUDA on a range of manipulation tasks, including free-form language instructions across diverse object categories, multi-object interactions, and deformable or granular objects, demonstrating the effectiveness of our framework. The project page is available at http://kuda-dynamics.github.io.', 'score': 1, 'issue_id': 2776, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 марта', 'en': 'March 13', 'zh': '3月13日'}, 'hash': 'f856affbe8bcf064', 'authors': ['Zixian Liu', 'Mingtong Zhang', 'Yunzhu Li'], 'affiliations': ['Columbia University', 'Tsinghua University', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2503.10546.jpg', 'data': {'categories': ['#multimodal', '#optimization', '#robotics', '#open_source', '#agents'], 'emoji': '🤖', 'ru': {'title': 'KUDA: Динамическое планирование для роботов с открытым словарем', 'desc': 'KUDA - это система манипуляции с открытым словарем, которая объединяет обучение динамике и визуальное управление через ключевые точки. Она использует модели видео-языкового взаимодействия (VLM) и нейронные модели динамики для планирования траекторий робота. KUDA сначала назначает ключевые точки на RGB-изображении и запрашивает VLM для генерации целевых спецификаций. Затем эти абстрактные представления на основе ключевых точек преобразуются в функции стоимости, которые оптимизируются с помощью обученной модели динамики.'}, 'en': {'title': 'KUDA: Bridging Language and Dynamics for Robotic Manipulation', 'desc': 'This paper presents KUDA, an innovative open-vocabulary robotic manipulation system that incorporates dynamics learning and visual prompting. Unlike previous methods, KUDA utilizes keypoints to create target specifications that are understandable by vision-language models (VLMs) and can be transformed into cost functions for planning. The system processes language instructions and visual data to assign keypoints to images, which are then optimized using a learned dynamics model to generate robotic movements. The effectiveness of KUDA is validated through various manipulation tasks, showcasing its ability to handle complex interactions with different object types.'}, 'zh': {'title': 'KUDA：动态学习与视觉提示的开放词汇操作系统', 'desc': '随着大语言模型（LLMs）和视觉语言模型（VLMs）的快速发展，开放词汇的机器人操作系统取得了显著进展。然而，许多现有方法忽视了物体动态的重要性，限制了它们在更复杂动态任务中的适用性。我们提出了KUDA，一个集成了动态学习和通过关键点进行视觉提示的开放词汇操作系统，利用了VLMs和基于学习的神经动态模型。KUDA通过将关键点分配给RGB图像，并查询VLM生成目标规范，将抽象的关键点表示转换为成本函数，从而优化机器人轨迹。'}}}, {'id': 'https://huggingface.co/papers/2503.10410', 'title': 'RoCo-Sim: Enhancing Roadside Collaborative Perception through Foreground\n  Simulation', 'url': 'https://huggingface.co/papers/2503.10410', 'abstract': 'Roadside Collaborative Perception refers to a system where multiple roadside units collaborate to pool their perceptual data, assisting vehicles in enhancing their environmental awareness. Existing roadside perception methods concentrate on model design but overlook data issues like calibration errors, sparse information, and multi-view consistency, leading to poor performance on recent published datasets. To significantly enhance roadside collaborative perception and address critical data issues, we present the first simulation framework RoCo-Sim for road-side collaborative perception. RoCo-Sim is capable of generating diverse, multi-view consistent simulated roadside data through dynamic foreground editing and full-scene style transfer of a single image. RoCo-Sim consists of four components: (1) Camera Extrinsic Optimization ensures accurate 3D to 2D projection for roadside cameras; (2) A novel Multi-View Occlusion-Aware Sampler (MOAS) determines the placement of diverse digital assets within 3D space; (3) DepthSAM innovatively models foreground-background relationships from single-frame fixed-view images, ensuring multi-view consistency of foreground; and (4) Scalable Post-Processing Toolkit generates more realistic and enriched scenes through style transfer and other enhancements. RoCo-Sim significantly improves roadside 3D object detection, outperforming SOTA methods by 83.74 on Rcooper-Intersection and 83.12 on TUMTraf-V2X for AP70. RoCo-Sim fills a critical gap in roadside perception simulation. Code and pre-trained models will be released soon: https://github.com/duyuwen-duen/RoCo-Sim', 'score': 1, 'issue_id': 2776, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 марта', 'en': 'March 13', 'zh': '3月13日'}, 'hash': '44150f611040e79d', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#3d', '#optimization', '#data', '#dataset', '#cv', '#open_source'], 'emoji': '🚗', 'ru': {'title': 'RoCo-Sim: прорыв в симуляции дорожного восприятия', 'desc': 'RoCo-Sim - это первая система симуляции для совместного восприятия дорожной обстановки. Она решает проблемы калибровки, разреженности данных и мультиракурсной согласованности путем генерации синтетических данных. RoCo-Sim включает оптимизацию внешних параметров камер, многоракурсный выборщик с учетом окклюзий, моделирование отношений передний план-фон и инструменты постобработки. Система значительно улучшает 3D-детектирование объектов, превосходя современные методы на популярных наборах данных.'}, 'en': {'title': 'Enhancing Roadside Awareness with Collaborative Perception', 'desc': 'This paper introduces Roadside Collaborative Perception, a system where multiple roadside units work together to improve vehicle awareness of their surroundings. It highlights the limitations of current methods that focus mainly on model design while neglecting important data issues such as calibration errors and sparse information. To address these challenges, the authors present RoCo-Sim, a simulation framework that generates diverse and consistent roadside data using advanced techniques like dynamic foreground editing. RoCo-Sim significantly enhances roadside 3D object detection performance, surpassing state-of-the-art methods on benchmark datasets.'}, 'zh': {'title': '提升路边感知的协同力量', 'desc': '路边协同感知是一种系统，多个路边单元协作汇聚感知数据，帮助车辆提高环境意识。现有的路边感知方法主要关注模型设计，但忽视了数据问题，如校准误差、信息稀疏和多视图一致性，导致在最新数据集上的表现不佳。为显著提升路边协同感知并解决关键数据问题，我们提出了首个路边协同感知模拟框架RoCo-Sim。RoCo-Sim能够通过动态前景编辑和单图像的全场景风格迁移生成多样化的、多视图一致的模拟路边数据。'}}, 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2503.10622', 'title': 'Transformers without Normalization', 'url': 'https://huggingface.co/papers/2503.10622', 'abstract': 'Normalization layers are ubiquitous in modern neural networks and have long been considered essential. This work demonstrates that Transformers without normalization can achieve the same or better performance using a remarkably simple technique. We introduce Dynamic Tanh (DyT), an element-wise operation DyT(x) = tanh(alpha x), as a drop-in replacement for normalization layers in Transformers. DyT is inspired by the observation that layer normalization in Transformers often produces tanh-like, S-shaped input-output mappings. By incorporating DyT, Transformers without normalization can match or exceed the performance of their normalized counterparts, mostly without hyperparameter tuning. We validate the effectiveness of Transformers with DyT across diverse settings, ranging from recognition to generation, supervised to self-supervised learning, and computer vision to language models. These findings challenge the conventional understanding that normalization layers are indispensable in modern neural networks, and offer new insights into their role in deep networks.', 'score': 84, 'issue_id': 2702, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 марта', 'en': 'March 13', 'zh': '3月13日'}, 'hash': 'd790a15c7d75d15e', 'authors': ['Jiachen Zhu', 'Xinlei Chen', 'Kaiming He', 'Yann LeCun', 'Zhuang Liu'], 'affiliations': ['FAIR, Meta', 'MIT', 'New York University', 'Princeton University'], 'pdf_title_img': 'assets/pdf/title_img/2503.10622.jpg', 'data': {'categories': ['#optimization', '#training', '#architecture', '#multimodal', '#cv'], 'emoji': '🔄', 'ru': {'title': 'Прощай, нормализация: Dynamic Tanh меняет правила игры в архитектуре трансформеров', 'desc': 'Исследователи представили альтернативу нормализационным слоям в трансформерах - Dynamic Tanh (DyT). DyT - это простая поэлементная операция, которая может заменить нормализацию без потери производительности. Эксперименты показали, что трансформеры с DyT работают не хуже или лучше стандартных моделей в различных задачах компьютерного зрения и обработки естественного языка. Это открытие ставит под сомнение необходимость нормализационных слоев в современных нейронных сетях.'}, 'en': {'title': 'Transformers Thrive Without Normalization: Introducing Dynamic Tanh', 'desc': 'This paper explores the role of normalization layers in Transformers, showing that they may not be as essential as previously thought. The authors introduce Dynamic Tanh (DyT), a simple element-wise operation that can replace normalization layers while maintaining or improving performance. DyT is based on the observation that layer normalization often results in S-shaped mappings similar to the tanh function. The study demonstrates that Transformers using DyT can perform well across various tasks, suggesting a reevaluation of the necessity of normalization in deep learning architectures.'}, 'zh': {'title': '动态双曲正切：超越归一化的变换器', 'desc': '本研究展示了在现代神经网络中，归一化层并非必不可少。我们提出了一种名为动态双曲正切（Dynamic Tanh, DyT）的简单技术，可以替代变换器中的归一化层。DyT通过观察变换器中的层归一化通常产生类似tanh的S形输入输出映射而得出。通过引入DyT，未使用归一化的变换器可以在多种任务中达到或超过使用归一化的变换器的性能。'}}}, {'id': 'https://huggingface.co/papers/2503.10613', 'title': 'CoSTAast: Cost-Sensitive Toolpath Agent for Multi-turn Image Editing', 'url': 'https://huggingface.co/papers/2503.10613', 'abstract': 'Text-to-image models like stable diffusion and DALLE-3 still struggle with multi-turn image editing. We decompose such a task as an agentic workflow (path) of tool use that addresses a sequence of subtasks by AI tools of varying costs. Conventional search algorithms require expensive exploration to find tool paths. While large language models (LLMs) possess prior knowledge of subtask planning, they may lack accurate estimations of capabilities and costs of tools to determine which to apply in each subtask. Can we combine the strengths of both LLMs and graph search to find cost-efficient tool paths? We propose a three-stage approach "CoSTA*" that leverages LLMs to create a subtask tree, which helps prune a graph of AI tools for the given task, and then conducts A* search on the small subgraph to find a tool path. To better balance the total cost and quality, CoSTA* combines both metrics of each tool on every subtask to guide the A* search. Each subtask\'s output is then evaluated by a vision-language model (VLM), where a failure will trigger an update of the tool\'s cost and quality on the subtask. Hence, the A* search can recover from failures quickly to explore other paths. Moreover, CoSTA* can automatically switch between modalities across subtasks for a better cost-quality trade-off. We build a novel benchmark of challenging multi-turn image editing, on which CoSTA* outperforms state-of-the-art image-editing models or agents in terms of both cost and quality, and performs versatile trade-offs upon user preference.', 'score': 60, 'issue_id': 2700, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 марта', 'en': 'March 13', 'zh': '3月13日'}, 'hash': '7f2d9ee971af97a8', 'authors': ['Advait Gupta', 'NandaKiran Velaga', 'Dang Nguyen', 'Tianyi Zhou'], 'affiliations': ['University of Maryland, College Park'], 'pdf_title_img': 'assets/pdf/title_img/2503.10613.jpg', 'data': {'categories': ['#games', '#benchmark', '#multimodal', '#agents', '#optimization'], 'emoji': '🎨', 'ru': {'title': 'CoSTA*: Умное редактирование изображений с помощью ИИ и поиска', 'desc': 'Статья представляет новый подход CoSTA* для многоэтапного редактирования изображений. Метод объединяет большие языковые модели (LLM) для создания дерева подзадач и алгоритм поиска A* для нахождения оптимального пути использования инструментов ИИ. CoSTA* использует мультимодальную модель для оценки результатов каждого этапа и может адаптироваться к неудачам. Эксперименты показывают превосходство CoSTA* над существующими моделями и агентами в соотношении стоимости и качества редактирования изображений.'}, 'en': {'title': 'Optimizing Multi-Turn Image Editing with CoSTA*', 'desc': 'This paper addresses the challenges faced by text-to-image models in multi-turn image editing by introducing a new approach called CoSTA*. It combines large language models (LLMs) with graph search techniques to efficiently plan and execute a sequence of subtasks using AI tools. CoSTA* creates a subtask tree to streamline the selection of tools based on their costs and capabilities, and employs A* search to find optimal tool paths. The method also adapts to failures by updating tool metrics, allowing for quick recovery and improved cost-quality trade-offs in image editing tasks.'}, 'zh': {'title': '高效的多轮图像编辑工具路径优化', 'desc': '本文提出了一种名为CoSTA*的三阶段方法，用于解决文本到图像模型在多轮图像编辑中的挑战。该方法结合了大型语言模型（LLMs）和图搜索的优点，通过创建子任务树来优化AI工具的使用路径。CoSTA*在每个子任务中综合考虑工具的成本和质量，以指导A*搜索，从而找到高效的工具路径。实验结果表明，CoSTA*在多轮图像编辑任务中超越了现有的最先进模型，能够根据用户偏好进行灵活的成本与质量权衡。'}}}, {'id': 'https://huggingface.co/papers/2503.10633', 'title': "Charting and Navigating Hugging Face's Model Atlas", 'url': 'https://huggingface.co/papers/2503.10633', 'abstract': 'As there are now millions of publicly available neural networks, searching and analyzing large model repositories becomes increasingly important. Navigating so many models requires an atlas, but as most models are poorly documented charting such an atlas is challenging. To explore the hidden potential of model repositories, we chart a preliminary atlas representing the documented fraction of Hugging Face. It provides stunning visualizations of the model landscape and evolution. We demonstrate several applications of this atlas including predicting model attributes (e.g., accuracy), and analyzing trends in computer vision models. However, as the current atlas remains incomplete, we propose a method for charting undocumented regions. Specifically, we identify high-confidence structural priors based on dominant real-world model training practices. Leveraging these priors, our approach enables accurate mapping of previously undocumented areas of the atlas. We publicly release our datasets, code, and interactive atlas.', 'score': 50, 'issue_id': 2703, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 марта', 'en': 'March 13', 'zh': '3月13日'}, 'hash': '128e9e97a54b8404', 'authors': ['Eliahu Horwitz', 'Nitzan Kurer', 'Jonathan Kahana', 'Liel Amar', 'Yedid Hoshen'], 'affiliations': ['School of Computer Science and Engineering The Hebrew University of Jerusalem, Israel'], 'pdf_title_img': 'assets/pdf/title_img/2503.10633.jpg', 'data': {'categories': ['#open_source', '#benchmark', '#cv', '#data', '#survey', '#dataset'], 'emoji': '🗺️', 'ru': {'title': 'Создание атласа нейросетей: навигация в океане моделей машинного обучения', 'desc': 'Статья описывает создание атласа для навигации по миллионам доступных нейронных сетей. Авторы визуализируют ландшафт моделей на основе документированной части репозитория Hugging Face. Они демонстрируют применение атласа для предсказания атрибутов моделей и анализа трендов в компьютерном зрении. Предлагается метод для картирования недокументированных областей атласа с использованием структурных априорных знаний.'}, 'en': {'title': 'Mapping the Neural Network Landscape: An Interactive Atlas', 'desc': 'This paper addresses the challenge of navigating the vast number of publicly available neural networks by creating a visual atlas of documented models from Hugging Face. The atlas not only visualizes the model landscape but also tracks the evolution of these models over time. It includes applications such as predicting model attributes like accuracy and analyzing trends in computer vision. To improve the atlas, the authors propose a method to chart undocumented regions by using high-confidence structural priors based on common training practices.'}, 'zh': {'title': '探索神经网络模型的地图', 'desc': '随着公开神经网络数量的激增，搜索和分析大型模型库变得越来越重要。由于大多数模型文档不全，绘制模型的地图变得具有挑战性。我们绘制了一个初步的地图，展示了Hugging Face上已记录模型的分布和演变，并展示了多种应用，包括预测模型属性和分析计算机视觉模型的趋势。为了填补当前地图的空白，我们提出了一种方法，通过识别基于主流训练实践的高置信度结构先验，准确绘制未记录区域。'}}}, {'id': 'https://huggingface.co/papers/2503.10480', 'title': 'World Modeling Makes a Better Planner: Dual Preference Optimization for\n  Embodied Task Planning', 'url': 'https://huggingface.co/papers/2503.10480', 'abstract': 'Recent advances in large vision-language models (LVLMs) have shown promise for embodied task planning, yet they struggle with fundamental challenges like dependency constraints and efficiency. Existing approaches either solely optimize action selection or leverage world models during inference, overlooking the benefits of learning to model the world as a way to enhance planning capabilities. We propose Dual Preference Optimization (D^2PO), a new learning framework that jointly optimizes state prediction and action selection through preference learning, enabling LVLMs to understand environment dynamics for better planning. To automatically collect trajectories and stepwise preference data without human annotation, we introduce a tree search mechanism for extensive exploration via trial-and-error. Extensive experiments on VoTa-Bench demonstrate that our D^2PO-based method significantly outperforms existing methods and GPT-4o when applied to Qwen2-VL (7B), LLaVA-1.6 (7B), and LLaMA-3.2 (11B), achieving superior task success rates with more efficient execution paths.', 'score': 39, 'issue_id': 2700, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 марта', 'en': 'March 13', 'zh': '3月13日'}, 'hash': '2d3e6b8c84c51e69', 'authors': ['Siyin Wang', 'Zhaoye Fei', 'Qinyuan Cheng', 'Shiduo Zhang', 'Panpan Cai', 'Jinlan Fu', 'Xipeng Qiu'], 'affiliations': ['Fudan University', 'National University of Singapore', 'Shanghai Innovation Institute', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2503.10480.jpg', 'data': {'categories': ['#reasoning', '#multimodal', '#agents', '#optimization', '#rl', '#training'], 'emoji': '🤖', 'ru': {'title': 'Улучшение планирования действий у LVLM через совместную оптимизацию предсказаний и выбора', 'desc': 'Статья представляет новый подход к обучению больших визуально-языковых моделей (LVLM) для планирования действий в воплощенных задачах. Предложенный метод Dual Preference Optimization (D^2PO) совместно оптимизирует предсказание состояний и выбор действий через обучение предпочтениям. Для сбора данных без участия человека используется механизм поиска по дереву. Эксперименты показывают, что D^2PO значительно превосходит существующие методы по успешности выполнения задач и эффективности траекторий.'}, 'en': {'title': 'Enhancing Planning in LVLMs with Dual Preference Optimization', 'desc': 'This paper introduces Dual Preference Optimization (D^2PO), a novel framework designed to improve the planning capabilities of large vision-language models (LVLMs) by jointly optimizing state prediction and action selection. The approach addresses key challenges in embodied task planning, such as dependency constraints and efficiency, which have been inadequately handled by existing methods. By employing a tree search mechanism, D^2PO enables the automatic collection of trajectories and preference data, facilitating extensive exploration through trial-and-error without the need for human annotation. Experimental results on VoTa-Bench show that D^2PO significantly enhances task success rates and execution efficiency compared to previous methods and GPT-4o across various LVLMs.'}, 'zh': {'title': '双重偏好优化：提升任务规划能力的关键', 'desc': '本文提出了一种新的学习框架，称为双重偏好优化（D^2PO），旨在提高大型视觉语言模型（LVLMs）在任务规划中的能力。该框架通过偏好学习同时优化状态预测和动作选择，使模型能够更好地理解环境动态。为了自动收集轨迹和逐步偏好数据，本文引入了一种树搜索机制，以便通过试错进行广泛探索。实验结果表明，基于D^2PO的方法在多个基准测试中显著优于现有方法和GPT-4o，达到了更高的任务成功率和更高效的执行路径。'}}}, {'id': 'https://huggingface.co/papers/2503.10639', 'title': 'GoT: Unleashing Reasoning Capability of Multimodal Large Language Model\n  for Visual Generation and Editing', 'url': 'https://huggingface.co/papers/2503.10639', 'abstract': 'Current image generation and editing methods primarily process textual prompts as direct inputs without reasoning about visual composition and explicit operations. We present Generation Chain-of-Thought (GoT), a novel paradigm that enables generation and editing through an explicit language reasoning process before outputting images. This approach transforms conventional text-to-image generation and editing into a reasoning-guided framework that analyzes semantic relationships and spatial arrangements. We define the formulation of GoT and construct large-scale GoT datasets containing over 9M samples with detailed reasoning chains capturing semantic-spatial relationships. To leverage the advantages of GoT, we implement a unified framework that integrates Qwen2.5-VL for reasoning chain generation with an end-to-end diffusion model enhanced by our novel Semantic-Spatial Guidance Module. Experiments show our GoT framework achieves excellent performance on both generation and editing tasks, with significant improvements over baselines. Additionally, our approach enables interactive visual generation, allowing users to explicitly modify reasoning steps for precise image adjustments. GoT pioneers a new direction for reasoning-driven visual generation and editing, producing images that better align with human intent. To facilitate future research, we make our datasets, code, and pretrained models publicly available at https://github.com/rongyaofang/GoT.', 'score': 37, 'issue_id': 2701, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 марта', 'en': 'March 13', 'zh': '3月13日'}, 'hash': '70c9d741eedd181c', 'authors': ['Rongyao Fang', 'Chengqi Duan', 'Kun Wang', 'Linjiang Huang', 'Hao Li', 'Shilin Yan', 'Hao Tian', 'Xingyu Zeng', 'Rui Zhao', 'Jifeng Dai', 'Xihui Liu', 'Hongsheng Li'], 'affiliations': ['BUAA', 'CUHK MMLab', 'HKU', 'SenseTime', 'Shanghai AI Laboratory', 'THU'], 'pdf_title_img': 'assets/pdf/title_img/2503.10639.jpg', 'data': {'categories': ['#multimodal', '#cv', '#dataset', '#reasoning', '#diffusion', '#open_source'], 'emoji': '🧠', 'ru': {'title': 'Разумная генерация изображений: от текста к визуальному мышлению', 'desc': 'Статья представляет новую парадигму Generation Chain-of-Thought (GoT) для генерации и редактирования изображений. GoT использует явный процесс языкового рассуждения перед выводом изображений, анализируя семантические отношения и пространственное расположение. Авторы создали масштабные наборы данных GoT и реализовали унифицированную архитектуру, объединяющую Qwen2.5-VL для генерации цепочек рассуждений с диффузионной моделью. Эксперименты показывают значительное улучшение производительности по сравнению с базовыми методами и возможность интерактивной визуальной генерации.'}, 'en': {'title': 'Revolutionizing Image Generation with Reasoning!', 'desc': 'The paper introduces Generation Chain-of-Thought (GoT), a new method for image generation and editing that incorporates reasoning about visual composition. Instead of simply processing text prompts, GoT uses a reasoning-guided framework to analyze semantic relationships and spatial arrangements before generating images. It includes a large dataset with over 9 million samples that capture detailed reasoning chains, enhancing the understanding of how different elements relate visually. The results show that GoT significantly improves the quality of generated and edited images, allowing for interactive adjustments based on user-defined reasoning steps.'}, 'zh': {'title': '推理驱动的图像生成与编辑新方向', 'desc': '当前的图像生成和编辑方法主要将文本提示作为直接输入，而没有考虑视觉构图和明确的操作。我们提出了一种新颖的生成思维链（GoT）范式，通过在输出图像之前进行明确的语言推理过程来实现生成和编辑。该方法将传统的文本到图像生成和编辑转变为一个基于推理的框架，分析语义关系和空间排列。我们的GoT框架在生成和编辑任务上表现出色，显著优于基线，并允许用户通过明确修改推理步骤来进行交互式视觉生成。'}}}, {'id': 'https://huggingface.co/papers/2503.09669', 'title': 'Silent Branding Attack: Trigger-free Data Poisoning Attack on\n  Text-to-Image Diffusion Models', 'url': 'https://huggingface.co/papers/2503.09669', 'abstract': 'Text-to-image diffusion models have achieved remarkable success in generating high-quality contents from text prompts. However, their reliance on publicly available data and the growing trend of data sharing for fine-tuning make these models particularly vulnerable to data poisoning attacks. In this work, we introduce the Silent Branding Attack, a novel data poisoning method that manipulates text-to-image diffusion models to generate images containing specific brand logos or symbols without any text triggers. We find that when certain visual patterns are repeatedly in the training data, the model learns to reproduce them naturally in its outputs, even without prompt mentions. Leveraging this, we develop an automated data poisoning algorithm that unobtrusively injects logos into original images, ensuring they blend naturally and remain undetected. Models trained on this poisoned dataset generate images containing logos without degrading image quality or text alignment. We experimentally validate our silent branding attack across two realistic settings on large-scale high-quality image datasets and style personalization datasets, achieving high success rates even without a specific text trigger. Human evaluation and quantitative metrics including logo detection show that our method can stealthily embed logos.', 'score': 31, 'issue_id': 2701, 'pub_date': '2025-03-12', 'pub_date_card': {'ru': '12 марта', 'en': 'March 12', 'zh': '3月12日'}, 'hash': 'f54f510a4cf48d22', 'authors': ['Sangwon Jang', 'June Suk Choi', 'Jaehyeong Jo', 'Kimin Lee', 'Sung Ju Hwang'], 'affiliations': ['DeepAuto.ai', 'KAIST'], 'pdf_title_img': 'assets/pdf/title_img/2503.09669.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#security', '#diffusion', '#data'], 'emoji': '🕵️', 'ru': {'title': 'Невидимые логотипы: скрытое внедрение брендов в генеративные модели', 'desc': "Это исследование представляет новый метод атаки на модели диффузии текст-изображение путем отравления данных. Метод, названный 'Тихая брендинговая атака', заставляет модели генерировать изображения с определенными логотипами без явных текстовых триггеров. Авторы разработали алгоритм, который незаметно внедряет логотипы в оригинальные изображения для обучения модели. Эксперименты показали высокую эффективность метода на крупномасштабных наборах данных, что подтверждено как человеческой оценкой, так и количественными метриками."}, 'en': {'title': 'Stealthy Logo Injection in Image Generation', 'desc': "This paper presents a new method called the Silent Branding Attack, which targets text-to-image diffusion models by subtly injecting brand logos into training data. The attack exploits the model's tendency to reproduce visual patterns it has seen, allowing logos to appear in generated images without any explicit text prompts. The authors developed an automated algorithm that seamlessly integrates these logos into original images, making them difficult to detect. Their experiments demonstrate that models trained on this manipulated data can produce high-quality images containing logos, achieving significant success rates in various settings."}, 'zh': {'title': '静默品牌攻击：隐秘植入品牌标志的创新方法', 'desc': '文本到图像的扩散模型在根据文本提示生成高质量内容方面取得了显著成功。然而，这些模型依赖于公开数据，并且数据共享的趋势使其特别容易受到数据中毒攻击。本文提出了一种新的数据中毒方法——静默品牌攻击，能够操控文本到图像的扩散模型生成包含特定品牌标志或符号的图像，而无需任何文本触发。我们开发了一种自动化的数据中毒算法，可以在原始图像中悄无声息地注入标志，确保它们自然融合且不被检测。'}}}, {'id': 'https://huggingface.co/papers/2503.10291', 'title': 'VisualPRM: An Effective Process Reward Model for Multimodal Reasoning', 'url': 'https://huggingface.co/papers/2503.10291', 'abstract': 'We introduce VisualPRM, an advanced multimodal Process Reward Model (PRM) with 8B parameters, which improves the reasoning abilities of existing Multimodal Large Language Models (MLLMs) across different model scales and families with Best-of-N (BoN) evaluation strategies. Specifically, our model improves the reasoning performance of three types of MLLMs and four different model scales. Even when applied to the highly capable InternVL2.5-78B, it achieves a 5.9-point improvement across seven multimodal reasoning benchmarks. Experimental results show that our model exhibits superior performance compared to Outcome Reward Models and Self-Consistency during BoN evaluation. To facilitate the training of multimodal PRMs, we construct a multimodal process supervision dataset VisualPRM400K using an automated data pipeline. For the evaluation of multimodal PRMs, we propose VisualProcessBench, a benchmark with human-annotated step-wise correctness labels, to measure the abilities of PRMs to detect erroneous steps in multimodal reasoning tasks. We hope that our work can inspire more future research and contribute to the development of MLLMs. Our model, data, and benchmark are released in https://internvl.github.io/blog/2025-03-13-VisualPRM/.', 'score': 28, 'issue_id': 2705, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 марта', 'en': 'March 13', 'zh': '3月13日'}, 'hash': 'f4734440c38ca048', 'authors': ['Weiyun Wang', 'Zhangwei Gao', 'Lianjie Chen', 'Zhe Chen', 'Jinguo Zhu', 'Xiangyu Zhao', 'Yangzhou Liu', 'Yue Cao', 'Shenglong Ye', 'Xizhou Zhu', 'Lewei Lu', 'Haodong Duan', 'Yu Qiao', 'Jifeng Dai', 'Wenhai Wang'], 'affiliations': ['Fudan University', 'Nanjing University', 'SenseTime Research', 'Shanghai AI Laboratory', 'Shanghai Jiaotong University', 'The Chinese University of Hong Kong', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.10291.jpg', 'data': {'categories': ['#dataset', '#open_source', '#benchmark', '#reasoning', '#multimodal', '#training'], 'emoji': '🧠', 'ru': {'title': 'VisualPRM: улучшение мультимодальных рассуждений с помощью продвинутой модели вознаграждения процесса', 'desc': 'VisualPRM - это усовершенствованная мультимодальная модель вознаграждения процесса с 8 миллиардами параметров. Она улучшает способности рассуждения существующих мультимодальных больших языковых моделей (MLLM) разных масштабов и семейств с использованием стратегии оценки Best-of-N. Модель демонстрирует превосходную производительность по сравнению с моделями вознаграждения результатов и самосогласованностью при оценке BoN. Для обучения и оценки мультимодальных PRM авторы создали набор данных VisualPRM400K и бенчмарк VisualProcessBench.'}, 'en': {'title': 'Enhancing Multimodal Reasoning with VisualPRM', 'desc': 'VisualPRM is a multimodal Process Reward Model designed to enhance the reasoning capabilities of Multimodal Large Language Models (MLLMs) with 8 billion parameters. It demonstrates significant improvements in reasoning performance across various model scales and types, achieving a notable 5.9-point increase on seven multimodal reasoning benchmarks, even with high-capacity models like InternVL2.5-78B. The model outperforms traditional Outcome Reward Models and Self-Consistency methods during Best-of-N evaluations. Additionally, VisualPRM introduces a new dataset, VisualPRM400K, and a benchmark, VisualProcessBench, to support the training and evaluation of multimodal PRMs.'}, 'zh': {'title': '提升多模态推理能力的VisualPRM模型', 'desc': '我们介绍了一种先进的多模态过程奖励模型（PRM）VisualPRM，具有80亿个参数，能够提升现有多模态大型语言模型（MLLMs）的推理能力。该模型在三种类型的MLLMs和四种不同的模型规模上均表现出色，尤其是在强大的InternVL2.5-78B模型上，推理性能提高了5.9分。实验结果表明，VisualPRM在最佳选择（BoN）评估中优于结果奖励模型和自一致性方法。为了支持多模态PRMs的训练，我们构建了一个名为VisualPRM400K的多模态过程监督数据集，并提出了VisualProcessBench基准，以评估PRMs在多模态推理任务中检测错误步骤的能力。'}}}, {'id': 'https://huggingface.co/papers/2503.09662', 'title': 'CoRe^2: Collect, Reflect and Refine to Generate Better and Faster', 'url': 'https://huggingface.co/papers/2503.09662', 'abstract': "Making text-to-image (T2I) generative model sample both fast and well represents a promising research direction. Previous studies have typically focused on either enhancing the visual quality of synthesized images at the expense of sampling efficiency or dramatically accelerating sampling without improving the base model's generative capacity. Moreover, nearly all inference methods have not been able to ensure stable performance simultaneously on both diffusion models (DMs) and visual autoregressive models (ARMs). In this paper, we introduce a novel plug-and-play inference paradigm, CoRe^2, which comprises three subprocesses: Collect, Reflect, and Refine. CoRe^2 first collects classifier-free guidance (CFG) trajectories, and then use collected data to train a weak model that reflects the easy-to-learn contents while reducing number of function evaluations during inference by half. Subsequently, CoRe^2 employs weak-to-strong guidance to refine the conditional output, thereby improving the model's capacity to generate high-frequency and realistic content, which is difficult for the base model to capture. To the best of our knowledge, CoRe^2 is the first to demonstrate both efficiency and effectiveness across a wide range of DMs, including SDXL, SD3.5, and FLUX, as well as ARMs like LlamaGen. It has exhibited significant performance improvements on HPD v2, Pick-of-Pic, Drawbench, GenEval, and T2I-Compbench. Furthermore, CoRe^2 can be seamlessly integrated with the state-of-the-art Z-Sampling, outperforming it by 0.3 and 0.16 on PickScore and AES, while achieving 5.64s time saving using SD3.5.Code is released at https://github.com/xie-lab-ml/CoRe/tree/main.", 'score': 28, 'issue_id': 2705, 'pub_date': '2025-03-12', 'pub_date_card': {'ru': '12 марта', 'en': 'March 12', 'zh': '3月12日'}, 'hash': 'a6fdb00ac6a8ebca', 'authors': ['Shitong Shao', 'Zikai Zhou', 'Dian Xie', 'Yuetong Fang', 'Tian Ye', 'Lichen Bai', 'Zeke Xie'], 'affiliations': ['The Hong Kong University of Science and Technology (GuangZhou)'], 'pdf_title_img': 'assets/pdf/title_img/2503.09662.jpg', 'data': {'categories': ['#cv', '#benchmark', '#inference', '#diffusion'], 'emoji': '🎨', 'ru': {'title': 'CoRe^2: Революционный метод для быстрой и качественной генерации изображений по тексту', 'desc': 'Статья представляет новый метод CoRe^2 для улучшения генеративных моделей текст-в-изображение. Метод состоит из трех этапов: сбор траекторий безклассификационного руководства, обучение слабой модели и уточнение результатов с помощью руководства от слабой к сильной модели. CoRe^2 показывает высокую эффективность и качество результатов для различных диффузионных и авторегрессионных моделей. Метод превосходит современные подходы по метрикам качества изображений и скорости генерации.'}, 'en': {'title': 'CoRe^2: Fast and High-Quality Text-to-Image Generation', 'desc': 'This paper presents CoRe^2, a new method for improving the speed and quality of text-to-image (T2I) generative models. It introduces a three-step process: Collect, Reflect, and Refine, which enhances sampling efficiency while maintaining high visual fidelity. By using classifier-free guidance trajectories, CoRe^2 trains a weak model that simplifies the learning process and reduces function evaluations during inference. The method shows significant performance gains across various diffusion models and autoregressive models, making it a notable advancement in the field of generative modeling.'}, 'zh': {'title': '高效生成，真实图像！', 'desc': '本文提出了一种新的文本到图像生成模型推理范式，称为CoRe^2。该方法通过三个子过程：收集、反射和精炼，来提高生成效率和效果。CoRe^2首先收集无分类器引导轨迹，然后训练一个弱模型以减少推理过程中的函数评估次数。最后，通过弱到强的引导来精炼条件输出，从而生成更高频率和更真实的内容，显著提升了多种扩散模型和自回归模型的性能。'}}}, {'id': 'https://huggingface.co/papers/2503.10437', 'title': '4D LangSplat: 4D Language Gaussian Splatting via Multimodal Large\n  Language Models', 'url': 'https://huggingface.co/papers/2503.10437', 'abstract': 'Learning 4D language fields to enable time-sensitive, open-ended language queries in dynamic scenes is essential for many real-world applications. While LangSplat successfully grounds CLIP features into 3D Gaussian representations, achieving precision and efficiency in 3D static scenes, it lacks the ability to handle dynamic 4D fields as CLIP, designed for static image-text tasks, cannot capture temporal dynamics in videos. Real-world environments are inherently dynamic, with object semantics evolving over time. Building a precise 4D language field necessitates obtaining pixel-aligned, object-wise video features, which current vision models struggle to achieve. To address these challenges, we propose 4D LangSplat, which learns 4D language fields to handle time-agnostic or time-sensitive open-vocabulary queries in dynamic scenes efficiently. 4D LangSplat bypasses learning the language field from vision features and instead learns directly from text generated from object-wise video captions via Multimodal Large Language Models (MLLMs). Specifically, we propose a multimodal object-wise video prompting method, consisting of visual and text prompts that guide MLLMs to generate detailed, temporally consistent, high-quality captions for objects throughout a video. These captions are encoded using a Large Language Model into high-quality sentence embeddings, which then serve as pixel-aligned, object-specific feature supervision, facilitating open-vocabulary text queries through shared embedding spaces. Recognizing that objects in 4D scenes exhibit smooth transitions across states, we further propose a status deformable network to model these continuous changes over time effectively. Our results across multiple benchmarks demonstrate that 4D LangSplat attains precise and efficient results for both time-sensitive and time-agnostic open-vocabulary queries.', 'score': 22, 'issue_id': 2702, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 марта', 'en': 'March 13', 'zh': '3月13日'}, 'hash': '212017b2c934863c', 'authors': ['Wanhua Li', 'Renping Zhou', 'Jiawei Zhou', 'Yingwei Song', 'Johannes Herter', 'Minghan Qin', 'Gao Huang', 'Hanspeter Pfister'], 'affiliations': ['Brown University', 'ETH Zurich', 'Harvard University', 'Stony Brook University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.10437.jpg', 'data': {'categories': ['#3d', '#video', '#optimization', '#multimodal', '#interpretability', '#reasoning', '#games', '#agi', '#transfer_learning'], 'emoji': '🌐', 'ru': {'title': '4D языковые поля для динамических сцен: от статики к временной динамике', 'desc': '4D LangSplat - это метод для создания 4D языковых полей, позволяющий выполнять открытые языковые запросы в динамических сценах с учетом времени. Он использует мультимодальные большие языковые модели (MLLM) для генерации подробных описаний объектов в видео, которые затем кодируются в высококачественные векторные представления с помощью LLM. Метод включает сеть деформации состояний для моделирования плавных изменений объектов во времени. 4D LangSplat демонстрирует точные и эффективные результаты для открытых запросов как с учетом времени, так и без него.'}, 'en': {'title': 'Empowering Dynamic Scene Understanding with 4D Language Fields', 'desc': 'This paper introduces 4D LangSplat, a novel approach for creating 4D language fields that can handle dynamic scenes in videos. Unlike previous models that focus on static images, 4D LangSplat learns directly from text generated by Multimodal Large Language Models (MLLMs) using object-wise video captions. The method employs a multimodal prompting technique to produce high-quality, temporally consistent captions, which are then transformed into sentence embeddings for effective querying. Additionally, a status deformable network is proposed to accurately model the continuous changes of objects over time, resulting in improved performance for both time-sensitive and time-agnostic queries.'}, 'zh': {'title': '动态场景中的4D语言场学习', 'desc': '本论文提出了一种新的方法，称为4D LangSplat，旨在处理动态场景中的时间敏感和开放词汇查询。与现有的静态图像-文本模型不同，4D LangSplat能够学习4D语言场，直接从对象视频字幕生成的文本中获取信息。该方法利用多模态大型语言模型（MLLMs）生成高质量的时间一致性字幕，并将其编码为句子嵌入，以支持像素对齐的对象特征监督。通过引入状态可变形网络，4D LangSplat有效建模了对象在时间上的连续变化，展示了在多个基准测试中取得的精确和高效的查询结果。'}}}, {'id': 'https://huggingface.co/papers/2503.08677', 'title': 'OmniPaint: Mastering Object-Oriented Editing via Disentangled\n  Insertion-Removal Inpainting', 'url': 'https://huggingface.co/papers/2503.08677', 'abstract': 'Diffusion-based generative models have revolutionized object-oriented image editing, yet their deployment in realistic object removal and insertion remains hampered by challenges such as the intricate interplay of physical effects and insufficient paired training data. In this work, we introduce OmniPaint, a unified framework that re-conceptualizes object removal and insertion as interdependent processes rather than isolated tasks. Leveraging a pre-trained diffusion prior along with a progressive training pipeline comprising initial paired sample optimization and subsequent large-scale unpaired refinement via CycleFlow, OmniPaint achieves precise foreground elimination and seamless object insertion while faithfully preserving scene geometry and intrinsic properties. Furthermore, our novel CFD metric offers a robust, reference-free evaluation of context consistency and object hallucination, establishing a new benchmark for high-fidelity image editing. Project page: https://yeates.github.io/OmniPaint-Page/', 'score': 22, 'issue_id': 2712, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 марта', 'en': 'March 11', 'zh': '3月11日'}, 'hash': '6110f983d46b536a', 'authors': ['Yongsheng Yu', 'Ziyun Zeng', 'Haitian Zheng', 'Jiebo Luo'], 'affiliations': ['Adobe Research', 'University of Rochester'], 'pdf_title_img': 'assets/pdf/title_img/2503.08677.jpg', 'data': {'categories': ['#cv', '#dataset', '#diffusion', '#hallucinations', '#benchmark'], 'emoji': '🎨', 'ru': {'title': 'OmniPaint: революция в редактировании изображений с помощью ИИ', 'desc': 'OmniPaint - это унифицированная система для редактирования изображений, основанная на диффузионных генеративных моделях. Она рассматривает удаление и вставку объектов как взаимосвязанные процессы, используя предобученную диффузионную модель и прогрессивный конвейер обучения. OmniPaint достигает точного удаления переднего плана и бесшовной вставки объектов, сохраняя геометрию сцены и внутренние свойства. Система также предлагает новую метрику CFD для оценки согласованности контекста и галлюцинаций объектов.'}, 'en': {'title': 'OmniPaint: Seamless Object Editing through Interconnected Processes', 'desc': "This paper presents OmniPaint, a new framework for object removal and insertion in images, treating these tasks as interconnected rather than separate. It utilizes a pre-trained diffusion model and a two-step training process that starts with paired samples and then refines the model using unpaired data. OmniPaint effectively removes unwanted objects and adds new ones while maintaining the original scene's geometry and characteristics. Additionally, the authors introduce a new evaluation metric called CFD, which measures the consistency and realism of the edited images without needing reference images."}, 'zh': {'title': 'OmniPaint：物体编辑的新纪元', 'desc': '本论文介绍了一种名为OmniPaint的统一框架，旨在解决物体移除和插入的复杂问题。该框架将物体移除和插入视为相互依赖的过程，而非孤立的任务。通过利用预训练的扩散模型和逐步训练流程，OmniPaint能够实现精确的前景消除和无缝的物体插入，同时保持场景的几何形状和内在特性。此外，我们提出的CFD指标为上下文一致性和物体幻觉提供了一种强健的无参考评估方法，建立了高保真图像编辑的新基准。'}}}, {'id': 'https://huggingface.co/papers/2503.10596', 'title': 'GroundingSuite: Measuring Complex Multi-Granular Pixel Grounding', 'url': 'https://huggingface.co/papers/2503.10596', 'abstract': 'Pixel grounding, encompassing tasks such as Referring Expression Segmentation (RES), has garnered considerable attention due to its immense potential for bridging the gap between vision and language modalities. However, advancements in this domain are currently constrained by limitations inherent in existing datasets, including limited object categories, insufficient textual diversity, and a scarcity of high-quality annotations. To mitigate these limitations, we introduce GroundingSuite, which comprises: (1) an automated data annotation framework leveraging multiple Vision-Language Model (VLM) agents; (2) a large-scale training dataset encompassing 9.56 million diverse referring expressions and their corresponding segmentations; and (3) a meticulously curated evaluation benchmark consisting of 3,800 images. The GroundingSuite training dataset facilitates substantial performance improvements, enabling models trained on it to achieve state-of-the-art results. Specifically, a cIoU of 68.9 on gRefCOCO and a gIoU of 55.3 on RefCOCOm. Moreover, the GroundingSuite annotation framework demonstrates superior efficiency compared to the current leading data annotation method, i.e., 4.5 times faster than the GLaMM.', 'score': 18, 'issue_id': 2701, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 марта', 'en': 'March 13', 'zh': '3月13日'}, 'hash': '1b9ea337d198c741', 'authors': ['Rui Hu', 'Lianghui Zhu', 'Yuxuan Zhang', 'Tianheng Cheng', 'Lei Liu', 'Heng Liu', 'Longjin Ran', 'Xiaoxin Chen', 'Wenyu Liu', 'Xinggang Wang'], 'affiliations': ['School of EIC, Huazhong University of Science & Technology', 'vivo AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2503.10596.jpg', 'data': {'categories': ['#benchmark', '#data', '#dataset'], 'emoji': '🔍', 'ru': {'title': 'GroundingSuite: революция в пиксельной локализации объектов', 'desc': 'Статья представляет GroundingSuite - новый набор инструментов для задачи пиксельной локализации объектов по текстовому описанию. Он включает автоматизированную систему аннотации данных на основе мультиагентного подхода с использованием нескольких моделей машинного обучения для обработки естественного языка и компьютерного зрения. GroundingSuite также содержит крупномасштабный датасет для обучения с 9,56 миллионами разнообразных языковых выражений и соответствующих им сегментаций, а также тщательно подготовленный набор данных для оценки из 3800 изображений. Модели, обученные на этом датасете, достигают наилучших результатов в задачах сегментации по текстовому описанию.'}, 'en': {'title': 'Revolutionizing Pixel Grounding with GroundingSuite', 'desc': 'This paper introduces GroundingSuite, a new framework designed to enhance pixel grounding tasks like Referring Expression Segmentation (RES) by addressing the limitations of existing datasets. GroundingSuite features an automated data annotation system that utilizes multiple Vision-Language Model (VLM) agents, resulting in a large-scale dataset with 9.56 million diverse referring expressions and their segmentations. The framework not only improves the quality and diversity of training data but also provides a curated evaluation benchmark with 3,800 images. Models trained on this dataset achieve state-of-the-art performance, significantly outperforming previous methods in both efficiency and accuracy.'}, 'zh': {'title': 'GroundingSuite：提升视觉与语言的桥梁', 'desc': '本文介绍了一个名为GroundingSuite的框架，旨在解决现有数据集在像素定位任务中的局限性。该框架包括一个自动化数据注释系统，利用多个视觉-语言模型（VLM）代理生成数据。GroundingSuite提供了一个包含956万种多样化指称表达及其对应分割的大规模训练数据集，并建立了一个包含3800张图像的评估基准。通过使用GroundingSuite训练的数据集，模型的性能显著提升，达到了最新的研究成果。'}}}, {'id': 'https://huggingface.co/papers/2503.10351', 'title': 'New Trends for Modern Machine Translation with Large Reasoning Models', 'url': 'https://huggingface.co/papers/2503.10351', 'abstract': 'Recent advances in Large Reasoning Models (LRMs), particularly those leveraging Chain-of-Thought reasoning (CoT), have opened brand new possibility for Machine Translation (MT). This position paper argues that LRMs substantially transformed traditional neural MT as well as LLMs-based MT paradigms by reframing translation as a dynamic reasoning task that requires contextual, cultural, and linguistic understanding and reasoning. We identify three foundational shifts: 1) contextual coherence, where LRMs resolve ambiguities and preserve discourse structure through explicit reasoning over cross-sentence and complex context or even lack of context; 2) cultural intentionality, enabling models to adapt outputs by inferring speaker intent, audience expectations, and socio-linguistic norms; 3) self-reflection, LRMs can perform self-reflection during the inference time to correct the potential errors in translation especially extremely noisy cases, showing better robustness compared to simply mapping X->Y translation. We explore various scenarios in translation including stylized translation, document-level translation and multimodal translation by showcasing empirical examples that demonstrate the superiority of LRMs in translation. We also identify several interesting phenomenons for LRMs for MT including auto-pivot translation as well as the critical challenges such as over-localisation in translation and inference efficiency. In conclusion, we think that LRMs redefine translation systems not merely as text converters but as multilingual cognitive agents capable of reasoning about meaning beyond the text. This paradigm shift reminds us to think of problems in translation beyond traditional translation scenarios in a much broader context with LRMs - what we can achieve on top of it.', 'score': 18, 'issue_id': 2700, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 марта', 'en': 'March 13', 'zh': '3月13日'}, 'hash': 'c0a1b109c05698cc', 'authors': ['Sinuo Liu', 'Chenyang Lyu', 'Minghao Wu', 'Longyue Wang', 'Weihua Luo', 'Kaifu Zhang'], 'affiliations': ['MarcoPolo Team, Alibaba International Digital Commerce', 'University of Edinburgh'], 'pdf_title_img': 'assets/pdf/title_img/2503.10351.jpg', 'data': {'categories': ['#reasoning', '#agents', '#multilingual', '#machine_translation'], 'emoji': '🧠', 'ru': {'title': 'LRM: От простого перевода к многоязычным когнитивным агентам', 'desc': 'Эта статья рассматривает влияние больших моделей рассуждений (LRM) на машинный перевод. Авторы утверждают, что LRM трансформируют традиционные подходы, представляя перевод как динамическую задачу рассуждения, требующую контекстуального, культурного и лингвистического понимания. Выделяются три ключевых изменения: контекстуальная согласованность, культурная интенциональность и саморефлексия. Статья также исследует различные сценарии применения LRM в переводе и обсуждает возникающие феномены и проблемы.'}, 'en': {'title': 'Transforming Translation: LRMs as Cognitive Agents', 'desc': 'This paper discusses how Large Reasoning Models (LRMs) are changing the way we think about Machine Translation (MT). It highlights three key shifts: first, LRMs improve contextual coherence by understanding complex contexts and resolving ambiguities; second, they incorporate cultural intentionality, allowing translations to reflect speaker intent and social norms; and third, LRMs can self-reflect during translation to correct errors, making them more robust. The authors provide examples of how LRMs excel in various translation scenarios, suggesting that these models should be seen as cognitive agents that reason about meaning rather than just text converters.'}, 'zh': {'title': '大型推理模型重塑机器翻译的未来', 'desc': '大型推理模型（LRMs）在机器翻译（MT）领域带来了新的可能性，特别是通过链式思维推理（CoT）。这篇论文指出，LRMs将传统神经机器翻译转变为一种动态推理任务，强调上下文、文化和语言理解的重要性。我们识别出三个基础转变：上下文连贯性、文化意图性和自我反思能力，使得LRMs在翻译中表现出更好的鲁棒性。最终，我们认为LRMs重新定义了翻译系统，使其不仅仅是文本转换器，而是能够超越文本进行意义推理的多语言认知代理。'}}}, {'id': 'https://huggingface.co/papers/2503.04723', 'title': 'Shifting Long-Context LLMs Research from Input to Output', 'url': 'https://huggingface.co/papers/2503.04723', 'abstract': 'Recent advancements in long-context Large Language Models (LLMs) have primarily concentrated on processing extended input contexts, resulting in significant strides in long-context comprehension. However, the equally critical aspect of generating long-form outputs has received comparatively less attention. This paper advocates for a paradigm shift in NLP research toward addressing the challenges of long-output generation. Tasks such as novel writing, long-term planning, and complex reasoning require models to understand extensive contexts and produce coherent, contextually rich, and logically consistent extended text. These demands highlight a critical gap in current LLM capabilities. We underscore the importance of this under-explored domain and call for focused efforts to develop foundational LLMs tailored for generating high-quality, long-form outputs, which hold immense potential for real-world applications.', 'score': 18, 'issue_id': 2700, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 марта', 'en': 'March 6', 'zh': '3月6日'}, 'hash': '009f3e654e927dc3', 'authors': ['Yuhao Wu', 'Yushi Bai', 'Zhiqing Hu', 'Shangqing Tu', 'Ming Shan Hee', 'Juanzi Li', 'Roy Ka-Wei Lee'], 'affiliations': ['Singapore University', 'Singapore University of Technology and Design', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.04723.jpg', 'data': {'categories': ['#data', '#long_context', '#multimodal'], 'emoji': '📝', 'ru': {'title': 'Новый вызов для ИИ: создание длинных текстов', 'desc': 'Статья обсуждает необходимость развития моделей обработки естественного языка для генерации длинных текстов. Авторы отмечают, что современные исследования в основном сосредоточены на обработке длинных входных контекстов, но не на создании объемных выходных данных. Подчеркивается важность разработки языковых моделей, способных генерировать связные и логически последовательные длинные тексты. Такие модели необходимы для решения задач написания романов, долгосрочного планирования и сложных рассуждений.'}, 'en': {'title': 'Bridging the Gap: Enhancing Long-Form Output in LLMs', 'desc': 'This paper discusses the need for improvements in long-form output generation by Large Language Models (LLMs). While recent advancements have focused on understanding long input contexts, generating coherent and contextually rich long outputs remains underexplored. The authors emphasize that tasks like novel writing and complex reasoning require models to produce logically consistent extended text. They call for more research efforts to develop LLMs that can effectively handle these long-output challenges, which are crucial for various real-world applications.'}, 'zh': {'title': '推动长文本生成的研究转型', 'desc': '最近，长上下文的大型语言模型（LLMs）在处理扩展输入上下文方面取得了显著进展，但生成长文本输出的研究相对较少。本文提倡自然语言处理（NLP）研究向解决长输出生成的挑战转变。长篇小说写作、长期规划和复杂推理等任务需要模型理解广泛的上下文，并生成连贯、丰富且逻辑一致的扩展文本。我们强调这一未被充分探索领域的重要性，并呼吁集中力量开发专门用于生成高质量长文本输出的基础LLM，以满足现实应用的巨大潜力。'}}}, {'id': 'https://huggingface.co/papers/2503.10460', 'title': 'Light-R1: Curriculum SFT, DPO and RL for Long COT from Scratch and\n  Beyond', 'url': 'https://huggingface.co/papers/2503.10460', 'abstract': 'This paper presents our work on the Light-R1 series, with models, data, and code all released.   We first focus on training long COT models from scratch, specifically starting from models initially lacking long COT capabilities. Using a curriculum training recipe consisting of two-stage SFT and semi-on-policy DPO, we train our model Light-R1-32B from Qwen2.5-32B-Instruct, resulting in superior math performance compared to DeepSeek-R1-Distill-Qwen-32B. Despite being trained exclusively on math data, Light-R1-32B shows strong generalization across other domains. In the subsequent phase of this work, we highlight the significant benefit of the 3k dataset constructed for the second SFT stage on enhancing other models. By fine-tuning DeepSeek-R1-Distilled models using this dataset, we obtain new SOTA models in 7B and 14B, while the 32B model, Light-R1-32B-DS performed comparably to QwQ-32B and DeepSeek-R1.   Furthermore, we extend our work by applying reinforcement learning, specifically GRPO, on long-COT models to further improve reasoning performance. We successfully train our final Light-R1-14B-DS with RL, achieving SOTA performance among 14B parameter models in math. With AIME24 & 25 scores of 74.0 and 60.2 respectively, Light-R1-14B-DS surpasses even many 32B models and DeepSeek-R1-Distill-Llama-70B. Its RL training also exhibits well expected behavior, showing simultaneous increase in response length and reward score.   The Light-R1 series of work validates training long-COT models from scratch, showcases the art in SFT data and releases SOTA models from RL.', 'score': 17, 'issue_id': 2701, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 марта', 'en': 'March 13', 'zh': '3月13日'}, 'hash': '503c1d89e949bb41', 'authors': ['Liang Wen', 'Yunke Cai', 'Fenrui Xiao', 'Xin He', 'Qi An', 'Zhenyu Duan', 'Yimin Du', 'Junchen Liu', 'Lifu Tang', 'Xiaowei Lv', 'Haosheng Zou', 'Yongchao Deng', 'Shousheng Jia', 'Xiangzheng Zhang'], 'affiliations': ['Qiyuan Tech', 'Renmin University'], 'pdf_title_img': 'assets/pdf/title_img/2503.10460.jpg', 'data': {'categories': ['#rlhf', '#dataset', '#rl', '#reasoning', '#training', '#optimization', '#long_context', '#open_source'], 'emoji': '🧠', 'ru': {'title': 'Прорыв в обучении ИИ длинным цепочкам рассуждений', 'desc': 'Статья представляет серию моделей Light-R1, обученных выполнять длинные цепочки рассуждений (COT) с нуля. Используя двухэтапное обучение с учителем (SFT) и полу-онлайновое обучение с предпочтениями (DPO), авторы получили модель Light-R1-32B, превосходящую аналоги в математических задачах. Дальнейшее применение обучения с подкреплением (RL) позволило создать модель Light-R1-14B-DS, достигшую наилучших результатов среди 14B моделей в математике. Исследование подтверждает эффективность обучения длинным COT с нуля и демонстрирует важность качественных данных для SFT.'}, 'en': {'title': 'Revolutionizing Long COT Models with Light-R1 Series', 'desc': "This paper introduces the Light-R1 series, focusing on training long Chain of Thought (COT) models from scratch. The authors employ a two-stage Supervised Fine-Tuning (SFT) and semi-on-policy Direct Preference Optimization (DPO) to enhance the model's math capabilities, resulting in the Light-R1-32B model that outperforms existing models. They also demonstrate that a specially constructed 3k dataset significantly boosts the performance of other models, achieving state-of-the-art (SOTA) results in various parameter sizes. Additionally, the application of reinforcement learning (RL) further improves reasoning performance, with the Light-R1-14B-DS model achieving impressive scores in math tasks, surpassing even larger models."}, 'zh': {'title': '从零开始训练长链推理模型的成功之路', 'desc': '本文介绍了Light-R1系列的研究工作，发布了模型、数据和代码。我们专注于从头开始训练长链推理（COT）模型，采用两阶段的监督微调（SFT）和半在线策略优化（DPO）进行课程训练。Light-R1-32B模型在数学性能上优于DeepSeek-R1-Distill-Qwen-32B，尽管仅在数学数据上训练，但在其他领域也表现出强大的泛化能力。此外，通过强化学习（RL）进一步提升推理性能，我们成功训练了Light-R1-14B-DS，达到了14B参数模型中的最新技术水平。'}}}, {'id': 'https://huggingface.co/papers/2503.10618', 'title': 'DiT-Air: Revisiting the Efficiency of Diffusion Model Architecture\n  Design in Text to Image Generation', 'url': 'https://huggingface.co/papers/2503.10618', 'abstract': 'In this work, we empirically study Diffusion Transformers (DiTs) for text-to-image generation, focusing on architectural choices, text-conditioning strategies, and training protocols. We evaluate a range of DiT-based architectures--including PixArt-style and MMDiT variants--and compare them with a standard DiT variant which directly processes concatenated text and noise inputs. Surprisingly, our findings reveal that the performance of standard DiT is comparable with those specialized models, while demonstrating superior parameter-efficiency, especially when scaled up. Leveraging the layer-wise parameter sharing strategy, we achieve a further reduction of 66% in model size compared to an MMDiT architecture, with minimal performance impact. Building on an in-depth analysis of critical components such as text encoders and Variational Auto-Encoders (VAEs), we introduce DiT-Air and DiT-Air-Lite. With supervised and reward fine-tuning, DiT-Air achieves state-of-the-art performance on GenEval and T2I CompBench, while DiT-Air-Lite remains highly competitive, surpassing most existing models despite its compact size.', 'score': 16, 'issue_id': 2704, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 марта', 'en': 'March 13', 'zh': '3月13日'}, 'hash': '3e0a293a0bdb9c8c', 'authors': ['Chen Chen', 'Rui Qian', 'Wenze Hu', 'Tsu-Jui Fu', 'Lezhi Li', 'Bowen Zhang', 'Alex Schwing', 'Wei Liu', 'Yinfei Yang'], 'affiliations': ['Apple Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2503.10618.jpg', 'data': {'categories': ['#diffusion', '#architecture', '#dataset', '#training'], 'emoji': '🖼️', 'ru': {'title': 'Эффективные Диффузионные Трансформеры для генерации изображений', 'desc': 'В этой работе исследуются Диффузионные Трансформеры (DiT) для генерации изображений по тексту. Авторы изучают различные архитектуры DiT, включая варианты PixArt и MMDiT, и сравнивают их со стандартным DiT. Результаты показывают, что стандартный DiT сопоставим по производительности со специализированными моделями, но более эффективен по параметрам. Используя стратегию послойного разделения параметров, исследователи добиваются дальнейшего уменьшения размера модели на 66% по сравнению с MMDiT.'}, 'en': {'title': 'Efficient Image Generation with Diffusion Transformers', 'desc': 'This paper investigates Diffusion Transformers (DiTs) for generating images from text, examining different architectural designs and training methods. The authors compare various DiT models, including specialized versions and a standard model that combines text and noise inputs. They find that the standard DiT performs similarly to more complex models while being more efficient in terms of parameters, especially when scaled. Additionally, they introduce new models, DiT-Air and DiT-Air-Lite, which achieve top performance on benchmark tests while maintaining a smaller size.'}, 'zh': {'title': '扩散变换器：高效的文本到图像生成', 'desc': '本文研究了扩散变换器（DiTs）在文本到图像生成中的应用，重点关注架构选择、文本条件策略和训练协议。我们评估了多种基于DiT的架构，包括PixArt风格和MMDiT变体，并与直接处理文本和噪声输入的标准DiT变体进行了比较。研究结果表明，标准DiT的性能与这些专业模型相当，同时在参数效率上表现更佳，尤其是在模型规模增大时。通过层级参数共享策略，我们将模型大小进一步减少了66%，对性能影响极小。'}}}, {'id': 'https://huggingface.co/papers/2503.10582', 'title': 'VisualWebInstruct: Scaling up Multimodal Instruction Data through Web\n  Search', 'url': 'https://huggingface.co/papers/2503.10582', 'abstract': "Vision-Language Models have made significant progress on many perception-focused tasks, however, their progress on reasoning-focused tasks seem to be limited due to the lack of high-quality and diverse training data. In this work, we aim to address the scarcity issue of reasoning-focused multimodal datasets. We propose VisualWebInstruct - a novel approach that leverages search engine to create a diverse, and high-quality dataset spanning multiple disciplines like math, physics, finance, chemistry, etc. Starting with meticulously selected 30,000 seed images, we employ Google Image search to identify websites containing similar images. We collect and process the HTMLs from over 700K unique URL sources. Through a pipeline of content extraction, filtering and synthesis, we build a dataset of approximately 900K question-answer pairs, with 40% being visual QA pairs and the rest as text QA pairs. Models fine-tuned on VisualWebInstruct demonstrate significant performance gains: (1) training from Llava-OV-mid shows 10-20% absolute point gains across benchmarks, (2) training from MAmmoTH-VL shows 5% absoluate gain. Our best model MAmmoTH-VL2 shows state-of-the-art performance within the 10B parameter class on MMMU-Pro-std (40.7%), MathVerse (42.6%), and DynaMath (55.7%). These remarkable results highlight the effectiveness of our dataset in enhancing VLMs' reasoning capabilities for complex multimodal tasks.", 'score': 16, 'issue_id': 2700, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 марта', 'en': 'March 13', 'zh': '3月13日'}, 'hash': 'ff1e79c1589f8602', 'authors': ['Yiming Jia', 'Jiachen Li', 'Xiang Yue', 'Bo Li', 'Ping Nie', 'Kai Zou', 'Wenhu Chen'], 'affiliations': ['CMU', 'Independent', 'NUS', 'Netmind.ai', 'UC Santa Barbara', 'University of Toronto', 'University of Waterloo'], 'pdf_title_img': 'assets/pdf/title_img/2503.10582.jpg', 'data': {'categories': ['#dataset', '#data', '#reasoning', '#multimodal'], 'emoji': '🧠', 'ru': {'title': 'VisualWebInstruct: прорыв в обучении мультимодальных моделей рассуждению', 'desc': 'Исследователи представили VisualWebInstruct - новый подход к созданию разнообразного и качественного набора данных для обучения мультимодальных моделей машинного обучения. Используя поисковую систему и обработку HTML-страниц, они собрали около 900 тысяч пар вопрос-ответ по различным дисциплинам. Модели, дообученные на этом наборе данных, показали значительный прирост производительности на нескольких бенчмарках. Лучшая модель MAmmoTH-VL2 продемонстрировала state-of-the-art результаты в своем классе на задачах рассуждения и решения задач.'}, 'en': {'title': 'Enhancing Reasoning in Vision-Language Models with VisualWebInstruct', 'desc': 'This paper introduces VisualWebInstruct, a new dataset aimed at improving the reasoning abilities of Vision-Language Models (VLMs). The dataset is created by leveraging search engines to gather diverse and high-quality multimodal data, specifically focusing on reasoning tasks across various disciplines. By processing over 700,000 unique web sources, the authors compile approximately 900,000 question-answer pairs, enhancing the training data available for VLMs. The results show that models trained on this dataset achieve significant performance improvements on reasoning benchmarks, demonstrating its effectiveness in advancing VLM capabilities.'}, 'zh': {'title': '提升视觉语言模型推理能力的新数据集', 'desc': '本文提出了一种新的方法VisualWebInstruct，旨在解决推理导向的多模态数据集稀缺问题。我们利用搜索引擎创建了一个多学科的高质量数据集，包括数学、物理、金融和化学等领域。通过从30,000个种子图像开始，收集和处理超过70万个独特网址的HTML内容，我们构建了约90万个问答对的数据集。经过在VisualWebInstruct上微调的模型在多个基准测试中表现出显著的性能提升，证明了该数据集在增强视觉语言模型推理能力方面的有效性。'}}}, {'id': 'https://huggingface.co/papers/2503.09642', 'title': 'Open-Sora 2.0: Training a Commercial-Level Video Generation Model in\n  $200k', 'url': 'https://huggingface.co/papers/2503.09642', 'abstract': 'Video generation models have achieved remarkable progress in the past year. The quality of AI video continues to improve, but at the cost of larger model size, increased data quantity, and greater demand for training compute. In this report, we present Open-Sora 2.0, a commercial-level video generation model trained for only $200k. With this model, we demonstrate that the cost of training a top-performing video generation model is highly controllable. We detail all techniques that contribute to this efficiency breakthrough, including data curation, model architecture, training strategy, and system optimization. According to human evaluation results and VBench scores, Open-Sora 2.0 is comparable to global leading video generation models including the open-source HunyuanVideo and the closed-source Runway Gen-3 Alpha. By making Open-Sora 2.0 fully open-source, we aim to democratize access to advanced video generation technology, fostering broader innovation and creativity in content creation. All resources are publicly available at: https://github.com/hpcaitech/Open-Sora.', 'score': 14, 'issue_id': 2701, 'pub_date': '2025-03-12', 'pub_date_card': {'ru': '12 марта', 'en': 'March 12', 'zh': '3月12日'}, 'hash': '8987bc0bbdde3b5a', 'authors': ['Xiangyu Peng', 'Zangwei Zheng', 'Chenhui Shen', 'Tom Young', 'Xinying Guo', 'Binluo Wang', 'Hang Xu', 'Hongxin Liu', 'Mingyan Jiang', 'Wenjun Li', 'Yuhui Wang', 'Anbang Ye', 'Gang Ren', 'Qianran Ma', 'Wanying Liang', 'Xiang Lian', 'Xiwen Wu', 'Yuting Zhong', 'Zhuangyan Li', 'Chaoyu Gong', 'Guojun Lei', 'Leijun Cheng', 'Limin Zhang', 'Minghao Li', 'Ruijie Zhang', 'Silan Hu', 'Shijie Huang', 'Xiaokang Wang', 'Yuanheng Zhao', 'Yuqi Wang', 'Ziang Wei', 'Yang You'], 'affiliations': ['HPC-AI Tech'], 'pdf_title_img': 'assets/pdf/title_img/2503.09642.jpg', 'data': {'categories': ['#training', '#optimization', '#open_source', '#architecture', '#video', '#data'], 'emoji': '🎬', 'ru': {'title': 'Доступная генерация видео: качество мирового уровня по разумной цене', 'desc': 'Open-Sora 2.0 - это модель генерации видео, обученная всего за $200 тыс. Она демонстрирует, что создание высокопроизводительной модели генерации видео может быть экономически эффективным. Исследователи применили ряд техник для оптимизации, включая курирование данных, архитектуру модели и стратегию обучения. По результатам человеческой оценки и показателям VBench, Open-Sora 2.0 сопоставима с ведущими моделями генерации видео.'}, 'en': {'title': 'Affordable Excellence in Video Generation', 'desc': 'Open-Sora 2.0 is a new video generation model that has been developed to produce high-quality videos while keeping training costs low. It was trained for only $200,000, showcasing that effective video generation can be achieved without massive resources. The model incorporates various techniques such as optimized data curation, innovative model architecture, and efficient training strategies to enhance performance. By making Open-Sora 2.0 open-source, the authors aim to provide wider access to advanced video generation tools, encouraging creativity and innovation in the field.'}, 'zh': {'title': 'Open-Sora 2.0：高效视频生成的未来', 'desc': '视频生成模型在过去一年取得了显著进展。虽然AI视频的质量不断提高，但这也导致了模型规模增大、数据量增加和训练计算需求加大。我们介绍了Open-Sora 2.0，这是一个商业级的视频生成模型，仅用20万美元进行训练。通过优化数据管理、模型架构、训练策略和系统性能，我们展示了高效训练顶级视频生成模型的可控性，并希望通过开源这一模型来促进内容创作的创新与发展。'}}}, {'id': 'https://huggingface.co/papers/2503.09641', 'title': 'SANA-Sprint: One-Step Diffusion with Continuous-Time Consistency\n  Distillation', 'url': 'https://huggingface.co/papers/2503.09641', 'abstract': 'This paper presents SANA-Sprint, an efficient diffusion model for ultra-fast text-to-image (T2I) generation. SANA-Sprint is built on a pre-trained foundation model and augmented with hybrid distillation, dramatically reducing inference steps from 20 to 1-4. We introduce three key innovations: (1) We propose a training-free approach that transforms a pre-trained flow-matching model for continuous-time consistency distillation (sCM), eliminating costly training from scratch and achieving high training efficiency. Our hybrid distillation strategy combines sCM with latent adversarial distillation (LADD): sCM ensures alignment with the teacher model, while LADD enhances single-step generation fidelity. (2) SANA-Sprint is a unified step-adaptive model that achieves high-quality generation in 1-4 steps, eliminating step-specific training and improving efficiency. (3) We integrate ControlNet with SANA-Sprint for real-time interactive image generation, enabling instant visual feedback for user interaction. SANA-Sprint establishes a new Pareto frontier in speed-quality tradeoffs, achieving state-of-the-art performance with 7.59 FID and 0.74 GenEval in only 1 step - outperforming FLUX-schnell (7.94 FID / 0.71 GenEval) while being 10x faster (0.1s vs 1.1s on H100). It also achieves 0.1s (T2I) and 0.25s (ControlNet) latency for 1024 x 1024 images on H100, and 0.31s (T2I) on an RTX 4090, showcasing its exceptional efficiency and potential for AI-powered consumer applications (AIPC). Code and pre-trained models will be open-sourced.', 'score': 14, 'issue_id': 2702, 'pub_date': '2025-03-12', 'pub_date_card': {'ru': '12 марта', 'en': 'March 12', 'zh': '3月12日'}, 'hash': '4079b7fe9f67a246', 'authors': ['Junsong Chen', 'Shuchen Xue', 'Yuyang Zhao', 'Jincheng Yu', 'Sayak Paul', 'Junyu Chen', 'Han Cai', 'Enze Xie', 'Song Han'], 'affiliations': ['Huggingface', 'Independent Researcher', 'MIT', 'NVIDIA', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.09641.jpg', 'data': {'categories': ['#diffusion', '#training', '#inference', '#cv', '#open_source'], 'emoji': '🚀', 'ru': {'title': 'Молниеносная генерация изображений с SANA-Sprint', 'desc': 'SANA-Sprint - это эффективная диффузионная модель для сверхбыстрой генерации изображений по тексту. Она использует предобученную базовую модель и гибридную дистилляцию, сокращая количество шагов вывода с 20 до 1-4. Ключевые инновации включают подход без обучения для непрерывной консистентной дистилляции, унифицированную адаптивную модель и интеграцию с ControlNet для интерактивной генерации. SANA-Sprint достигает state-of-the-art результатов по метрикам FID и GenEval за 1 шаг, превосходя существующие решения по скорости и качеству.'}, 'en': {'title': 'SANA-Sprint: Ultra-Fast Text-to-Image Generation Revolutionized!', 'desc': 'This paper introduces SANA-Sprint, a novel diffusion model designed for rapid text-to-image (T2I) generation. By leveraging a pre-trained foundation model and employing hybrid distillation techniques, SANA-Sprint significantly reduces the number of inference steps required, achieving high-quality image generation in just 1-4 steps. The model integrates a training-free approach and combines continuous-time consistency distillation with latent adversarial distillation to enhance generation fidelity. Additionally, SANA-Sprint incorporates ControlNet for real-time interactive image generation, setting a new standard in the speed-quality tradeoff for T2I models.'}, 'zh': {'title': 'SANA-Sprint：超快速文本到图像生成的革命性模型', 'desc': '本文介绍了SANA-Sprint，一种高效的扩散模型，用于超快速的文本到图像生成。SANA-Sprint基于预训练的基础模型，并通过混合蒸馏技术显著减少推理步骤，从20步减少到1-4步。该模型的三项关键创新包括无训练的流匹配模型转换、统一的步适应模型以及与ControlNet的集成，实现实时交互式图像生成。SANA-Sprint在速度与质量的权衡中建立了新的Pareto前沿，以1步生成达到7.59 FID和0.74 GenEval的最佳性能，且速度比FLUX-schnell快10倍。'}}}, {'id': 'https://huggingface.co/papers/2503.10615', 'title': 'R1-Onevision: Advancing Generalized Multimodal Reasoning through\n  Cross-Modal Formalization', 'url': 'https://huggingface.co/papers/2503.10615', 'abstract': 'Large Language Models have demonstrated remarkable reasoning capability in complex textual tasks. However, multimodal reasoning, which requires integrating visual and textual information, remains a significant challenge. Existing visual-language models often struggle to effectively analyze and reason visual content, resulting in suboptimal performance on complex reasoning tasks. Moreover, the absence of comprehensive benchmarks hinders the accurate assessment of multimodal reasoning capabilities. In this paper, we introduce R1-Onevision, a multimodal reasoning model designed to bridge the gap between visual perception and deep reasoning. To achieve this, we propose a cross-modal reasoning pipeline that transforms images into formal textural representations, enabling precise language-based reasoning. Leveraging this pipeline, we construct the R1-Onevision dataset which provides detailed, step-by-step multimodal reasoning annotations across diverse domains. We further develop the R1-Onevision model through supervised fine-tuning and reinforcement learning to cultivate advanced reasoning and robust generalization abilities. To comprehensively evaluate multimodal reasoning performance across different grades, we introduce R1-Onevision-Bench, a benchmark aligned with human educational stages, covering exams from junior high school to university and beyond. Experimental results show that R1-Onevision achieves state-of-the-art performance, outperforming models such as GPT-4o and Qwen2.5-VL on multiple challenging multimodal reasoning benchmarks.', 'score': 13, 'issue_id': 2701, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 марта', 'en': 'March 13', 'zh': '3月13日'}, 'hash': '857ef8e4c110da7f', 'authors': ['Yi Yang', 'Xiaoxuan He', 'Hongkun Pan', 'Xiyan Jiang', 'Yan Deng', 'Xingtao Yang', 'Haoyu Lu', 'Dacheng Yin', 'Fengyun Rao', 'Minfeng Zhu', 'Bo Zhang', 'Wei Chen'], 'affiliations': ['Renmin University of China', 'WeChat Vision, Tencent Inc.', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.10615.jpg', 'data': {'categories': ['#benchmark', '#multimodal', '#dataset', '#rl', '#reasoning', '#training'], 'emoji': '🧠', 'ru': {'title': 'Преодолевая барьер между зрением и рассуждением в ИИ', 'desc': 'R1-Onevision - это мультимодальная модель рассуждений, преодолевающая разрыв между визуальным восприятием и глубоким анализом. Модель использует конвейер кросс-модальных рассуждений, преобразующий изображения в формальные текстовые представления для точного языкового анализа. Авторы создали датасет R1-Onevision с подробными пошаговыми аннотациями мультимодальных рассуждений из разных областей. Модель R1-Onevision, обученная с помощью обучения с учителем и обучения с подкреплением, превосходит существующие модели на различных сложных мультимодальных тестах.'}, 'en': {'title': 'Bridging Visual and Textual Reasoning with R1-Onevision', 'desc': 'This paper presents R1-Onevision, a new model for multimodal reasoning that combines visual and textual information. It addresses the limitations of existing visual-language models by introducing a cross-modal reasoning pipeline that converts images into structured text representations. The authors also create the R1-Onevision dataset, which includes detailed annotations for multimodal reasoning tasks across various domains. Additionally, they establish R1-Onevision-Bench, a benchmark for evaluating multimodal reasoning abilities at different educational levels, demonstrating that their model outperforms others like GPT-4o and Qwen2.5-VL.'}, 'zh': {'title': 'R1-Onevision：视觉与语言的完美结合', 'desc': '本论文介绍了一种名为R1-Onevision的多模态推理模型，旨在解决视觉和文本信息整合的挑战。该模型通过跨模态推理管道，将图像转换为正式的文本表示，从而实现精确的基于语言的推理。我们还构建了R1-Onevision数据集，提供了详细的多模态推理注释，以支持不同领域的研究。实验结果表明，R1-Onevision在多个复杂的多模态推理基准测试中表现优异，超越了现有的先进模型。'}}}, {'id': 'https://huggingface.co/papers/2503.10589', 'title': 'Long Context Tuning for Video Generation', 'url': 'https://huggingface.co/papers/2503.10589', 'abstract': 'Recent advances in video generation can produce realistic, minute-long single-shot videos with scalable diffusion transformers. However, real-world narrative videos require multi-shot scenes with visual and dynamic consistency across shots. In this work, we introduce Long Context Tuning (LCT), a training paradigm that expands the context window of pre-trained single-shot video diffusion models to learn scene-level consistency directly from data. Our method expands full attention mechanisms from individual shots to encompass all shots within a scene, incorporating interleaved 3D position embedding and an asynchronous noise strategy, enabling both joint and auto-regressive shot generation without additional parameters. Models with bidirectional attention after LCT can further be fine-tuned with context-causal attention, facilitating auto-regressive generation with efficient KV-cache. Experiments demonstrate single-shot models after LCT can produce coherent multi-shot scenes and exhibit emerging capabilities, including compositional generation and interactive shot extension, paving the way for more practical visual content creation. See https://guoyww.github.io/projects/long-context-video/ for more details.', 'score': 13, 'issue_id': 2702, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 марта', 'en': 'March 13', 'zh': '3月13日'}, 'hash': '3850b6066f2b7160', 'authors': ['Yuwei Guo', 'Ceyuan Yang', 'Ziyan Yang', 'Zhibei Ma', 'Zhijie Lin', 'Zhenheng Yang', 'Dahua Lin', 'Lu Jiang'], 'affiliations': ['ByteDance', 'ByteDance Seed', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2503.10589.jpg', 'data': {'categories': ['#long_context', '#3d', '#video', '#diffusion', '#training'], 'emoji': '🎬', 'ru': {'title': 'Генерация согласованных многокадровых видео с помощью Long Context Tuning', 'desc': 'Статья представляет новый метод обучения под названием Long Context Tuning (LCT) для генерации многокадровых видео с сохранением согласованности сцен. LCT расширяет контекстное окно предобученных моделей диффузии для одиночных кадров, позволяя им учиться непосредственно из данных. Метод использует механизмы полного внимания и асинхронную стратегию шума для совместной и авторегрессивной генерации кадров. Эксперименты показывают, что модели после LCT способны создавать согласованные многокадровые сцены и демонстрируют новые возможности, такие как композиционная генерация и интерактивное расширение кадров.'}, 'en': {'title': 'Enhancing Video Generation with Long Context Tuning', 'desc': 'This paper presents Long Context Tuning (LCT), a new training approach for video generation models that enhances their ability to create multi-shot scenes with visual and dynamic consistency. By expanding the context window of pre-trained single-shot video diffusion models, LCT allows the model to learn from the entire scene rather than just individual shots. The method utilizes full attention mechanisms and incorporates advanced techniques like interleaved 3D position embedding and an asynchronous noise strategy. As a result, models trained with LCT can generate coherent multi-shot videos and demonstrate new capabilities such as compositional generation and interactive shot extension.'}, 'zh': {'title': '长上下文调优：提升视频生成的一致性与连贯性', 'desc': '本研究提出了一种新的训练范式，称为长上下文调优（LCT），旨在提高视频生成模型在多镜头场景中的一致性。通过扩展预训练单镜头视频扩散模型的上下文窗口，LCT能够直接从数据中学习场景级别的一致性。该方法利用全注意力机制，将注意力从单个镜头扩展到整个场景，结合了3D位置嵌入和异步噪声策略，实现了无额外参数的联合和自回归镜头生成。实验表明，经过LCT处理的单镜头模型能够生成连贯的多镜头场景，并展现出组合生成和交互式镜头扩展等新能力。'}}}, {'id': 'https://huggingface.co/papers/2503.10637', 'title': 'Distilling Diversity and Control in Diffusion Models', 'url': 'https://huggingface.co/papers/2503.10637', 'abstract': 'Distilled diffusion models suffer from a critical limitation: reduced sample diversity compared to their base counterparts. In this work, we uncover that despite this diversity loss, distilled models retain the fundamental concept representations of base models. We demonstrate control distillation - where control mechanisms like Concept Sliders and LoRAs trained on base models can be seamlessly transferred to distilled models and vice-versa, effectively distilling control without any retraining. This preservation of representational structure prompted our investigation into the mechanisms of diversity collapse during distillation. To understand how distillation affects diversity, we introduce Diffusion Target (DT) Visualization, an analysis and debugging tool that reveals how models predict final outputs at intermediate steps. Through DT-Visualization, we identify generation artifacts, inconsistencies, and demonstrate that initial diffusion timesteps disproportionately determine output diversity, while later steps primarily refine details. Based on these insights, we introduce diversity distillation - a hybrid inference approach that strategically employs the base model for only the first critical timestep before transitioning to the efficient distilled model. Our experiments demonstrate that this simple modification not only restores the diversity capabilities from base to distilled models but surprisingly exceeds it, while maintaining nearly the computational efficiency of distilled inference, all without requiring additional training or model modifications. Our code and data are available at https://distillation.baulab.info', 'score': 12, 'issue_id': 2701, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 марта', 'en': 'March 13', 'zh': '3月13日'}, 'hash': '5313c06d699a1a7f', 'authors': ['Rohit Gandikota', 'David Bau'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2503.10637.jpg', 'data': {'categories': ['#dataset', '#inference', '#training', '#optimization', '#diffusion', '#data'], 'emoji': '🔬', 'ru': {'title': 'Восстановление разнообразия в дистиллированных диффузионных моделях', 'desc': 'Исследование посвящено проблеме уменьшения разнообразия сэмплов в дистиллированных диффузионных моделях по сравнению с базовыми. Авторы обнаружили, что дистиллированные модели сохраняют фундаментальные концептуальные представления базовых моделей, что позволяет переносить механизмы контроля между ними без переобучения. Для анализа причин потери разнообразия была разработана техника визуализации Diffusion Target, которая показала, что начальные шаги диффузии непропорционально влияют на разнообразие выходных данных. На основе этих наблюдений предложен метод diversity distillation, использующий базовую модель только для первого критического шага, что восстанавливает и даже превосходит разнообразие базовой модели при сохранении эффективности дистиллированного вывода.'}, 'en': {'title': 'Restoring Diversity in Distilled Diffusion Models', 'desc': 'This paper addresses the issue of reduced sample diversity in distilled diffusion models compared to their original versions. The authors reveal that distilled models still maintain the essential concept representations of their base models. They introduce a method called control distillation, which allows for the transfer of control mechanisms between models without retraining. Additionally, they present Diffusion Target (DT) Visualization to analyze how diversity is affected during the distillation process, leading to a new approach called diversity distillation that enhances output diversity while keeping computational efficiency intact.'}, 'zh': {'title': '恢复蒸馏模型的多样性', 'desc': '本文探讨了蒸馏扩散模型的一个重要限制，即与基础模型相比，样本多样性降低。尽管存在这种多样性损失，蒸馏模型仍然保留了基础模型的基本概念表示。我们提出了控制蒸馏的方法，可以将控制机制无缝转移到蒸馏模型，并且不需要重新训练。通过引入扩散目标可视化工具，我们分析了蒸馏对多样性的影响，并提出了一种新的混合推理方法，能够在保持计算效率的同时恢复多样性。'}}}, {'id': 'https://huggingface.co/papers/2503.10357', 'title': 'Do I look like a `cat.n.01` to you? A Taxonomy Image Generation\n  Benchmark', 'url': 'https://huggingface.co/papers/2503.10357', 'abstract': "This paper explores the feasibility of using text-to-image models in a zero-shot setup to generate images for taxonomy concepts. While text-based methods for taxonomy enrichment are well-established, the potential of the visual dimension remains unexplored. To address this, we propose a comprehensive benchmark for Taxonomy Image Generation that assesses models' abilities to understand taxonomy concepts and generate relevant, high-quality images. The benchmark includes common-sense and randomly sampled WordNet concepts, alongside the LLM generated predictions. The 12 models are evaluated using 9 novel taxonomy-related text-to-image metrics and human feedback. Moreover, we pioneer the use of pairwise evaluation with GPT-4 feedback for image generation. Experimental results show that the ranking of models differs significantly from standard T2I tasks. Playground-v2 and FLUX consistently outperform across metrics and subsets and the retrieval-based approach performs poorly. These findings highlight the potential for automating the curation of structured data resources.", 'score': 11, 'issue_id': 2705, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 марта', 'en': 'March 13', 'zh': '3月13日'}, 'hash': 'dad8c63e36f6d8c2', 'authors': ['Viktor Moskvoretskii', 'Alina Lobanova', 'Ekaterina Neminova', 'Chris Biemann', 'Alexander Panchenko', 'Irina Nikishina'], 'affiliations': ['AIRI', 'HSE University', 'Skoltech', 'University of Hamburg'], 'pdf_title_img': 'assets/pdf/title_img/2503.10357.jpg', 'data': {'categories': ['#cv', '#multimodal', '#benchmark'], 'emoji': '🌳', 'ru': {'title': 'Визуализация таксономий: новый рубеж в генерации изображений', 'desc': 'Статья исследует возможность использования моделей преобразования текста в изображение для генерации визуального контента для таксономических концепций. Авторы предлагают комплексный бенчмарк для оценки способности моделей понимать таксономические концепции и генерировать релевантные, качественные изображения. Исследование включает оценку 12 моделей по 9 новым метрикам, связанным с таксономией, а также обратную связь от людей и GPT-4. Результаты показывают, что модели Playground-v2 и FLUX превосходят другие в этой задаче, демонстрируя потенциал для автоматизации курирования структурированных данных.'}, 'en': {'title': 'Unlocking Visual Understanding in Taxonomy with Zero-Shot Image Generation', 'desc': 'This paper investigates how well text-to-image models can create images based on taxonomy concepts without prior training on those specific concepts, known as a zero-shot setup. It introduces a new benchmark for evaluating these models, focusing on their ability to generate relevant and high-quality images that align with taxonomy terms. The study employs innovative metrics and human feedback to assess the performance of 12 different models, revealing that their effectiveness varies significantly from traditional text-to-image tasks. The results suggest that certain models, like Playground-v2 and FLUX, excel in this context, indicating a promising avenue for automating the enhancement of structured data resources.'}, 'zh': {'title': '探索零样本下的分类图像生成潜力', 'desc': '本文探讨了在零样本设置下使用文本到图像模型生成分类概念图像的可行性。虽然基于文本的方法在分类丰富方面已经相当成熟，但视觉维度的潜力尚未被充分挖掘。为此，我们提出了一个全面的分类图像生成基准，评估模型理解分类概念和生成相关高质量图像的能力。实验结果表明，模型的排名与标准的文本到图像任务有显著不同，强调了自动化结构化数据资源整理的潜力。'}}}, {'id': 'https://huggingface.co/papers/2503.09799', 'title': 'Communication-Efficient Language Model Training Scales Reliably and\n  Robustly: Scaling Laws for DiLoCo', 'url': 'https://huggingface.co/papers/2503.09799', 'abstract': "As we scale to more massive machine learning models, the frequent synchronization demands inherent in data-parallel approaches create significant slowdowns, posing a critical challenge to further scaling. Recent work develops an approach (DiLoCo) that relaxes synchronization demands without compromising model quality. However, these works do not carefully analyze how DiLoCo's behavior changes with model size. In this work, we study the scaling law behavior of DiLoCo when training LLMs under a fixed compute budget. We focus on how algorithmic factors, including number of model replicas, hyperparameters, and token budget affect training in ways that can be accurately predicted via scaling laws. We find that DiLoCo scales both predictably and robustly with model size. When well-tuned, DiLoCo scales better than data-parallel training with model size, and can outperform data-parallel training even at small model sizes. Our results showcase a more general set of benefits of DiLoCo than previously documented, including increased optimal batch sizes, improved downstream generalization with scale, and improved evaluation loss for a fixed token budget.", 'score': 10, 'issue_id': 2714, 'pub_date': '2025-03-12', 'pub_date_card': {'ru': '12 марта', 'en': 'March 12', 'zh': '3月12日'}, 'hash': '78f500ddd43c6e65', 'authors': ['Zachary Charles', 'Gabriel Teston', 'Lucio Dery', 'Keith Rush', 'Nova Fallen', 'Zachary Garrett', 'Arthur Szlam', 'Arthur Douillard'], 'affiliations': ['Google DeepMind', 'Google Research', 'Google Search'], 'pdf_title_img': 'assets/pdf/title_img/2503.09799.jpg', 'data': {'categories': ['#training', '#optimization'], 'emoji': '📈', 'ru': {'title': 'DiLoCo: Эффективное масштабирование обучения больших языковых моделей', 'desc': 'Эта статья исследует масштабируемость метода DiLoCo при обучении больших языковых моделей (LLM) с фиксированным бюджетом вычислений. Авторы анализируют, как алгоритмические факторы, включая количество реплик модели, гиперпараметры и бюджет токенов, влияют на обучение, и как это можно предсказать с помощью законов масштабирования. Результаты показывают, что DiLoCo масштабируется предсказуемо и надежно с увеличением размера модели, превосходя параллельное обучение по данным даже для небольших моделей. Исследование демонстрирует более широкий спектр преимуществ DiLoCo, включая увеличение оптимальных размеров батча и улучшение обобщения на downstream задачах.'}, 'en': {'title': 'DiLoCo: Scaling Up Without the Sync Slowdown!', 'desc': 'This paper investigates the performance of the DiLoCo approach for training large language models (LLMs) while minimizing synchronization delays. It analyzes how various factors, such as the number of model replicas and hyperparameters, influence the training process under a fixed compute budget. The authors demonstrate that DiLoCo not only scales effectively with model size but also outperforms traditional data-parallel training methods, even for smaller models. Their findings highlight the advantages of DiLoCo, including better batch sizes and enhanced generalization capabilities as model size increases.'}, 'zh': {'title': 'DiLoCo：超越数据并行的训练新方法', 'desc': '随着机器学习模型规模的扩大，数据并行方法中频繁的同步需求导致显著的性能下降，成为进一步扩展的关键挑战。本文提出了一种名为DiLoCo的方法，它在不影响模型质量的情况下，放宽了同步需求。我们研究了在固定计算预算下，DiLoCo在训练大型语言模型时的扩展规律，分析了模型副本数量、超参数和令牌预算等算法因素对训练的影响。研究结果表明，DiLoCo在模型规模上具有可预测和稳健的扩展性，经过良好调优后，其扩展性能优于数据并行训练，甚至在小规模模型中也能超越数据并行训练。'}}}, {'id': 'https://huggingface.co/papers/2503.10391', 'title': 'CINEMA: Coherent Multi-Subject Video Generation via MLLM-Based Guidance', 'url': 'https://huggingface.co/papers/2503.10391', 'abstract': 'Video generation has witnessed remarkable progress with the advent of deep generative models, particularly diffusion models. While existing methods excel in generating high-quality videos from text prompts or single images, personalized multi-subject video generation remains a largely unexplored challenge. This task involves synthesizing videos that incorporate multiple distinct subjects, each defined by separate reference images, while ensuring temporal and spatial consistency. Current approaches primarily rely on mapping subject images to keywords in text prompts, which introduces ambiguity and limits their ability to model subject relationships effectively. In this paper, we propose CINEMA, a novel framework for coherent multi-subject video generation by leveraging Multimodal Large Language Model (MLLM). Our approach eliminates the need for explicit correspondences between subject images and text entities, mitigating ambiguity and reducing annotation effort. By leveraging MLLM to interpret subject relationships, our method facilitates scalability, enabling the use of large and diverse datasets for training. Furthermore, our framework can be conditioned on varying numbers of subjects, offering greater flexibility in personalized content creation. Through extensive evaluations, we demonstrate that our approach significantly improves subject consistency, and overall video coherence, paving the way for advanced applications in storytelling, interactive media, and personalized video generation.', 'score': 9, 'issue_id': 2700, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 марта', 'en': 'March 13', 'zh': '3月13日'}, 'hash': 'd7f2c49b29951ace', 'authors': ['Yufan Deng', 'Xun Guo', 'Yizhi Wang', 'Jacob Zhiyuan Fang', 'Angtian Wang', 'Shenghai Yuan', 'Yiding Yang', 'Bo Liu', 'Haibin Huang', 'Chongyang Ma'], 'affiliations': ['ByteDance Intelligent Creation', 'Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2503.10391.jpg', 'data': {'categories': ['#synthetic', '#multimodal', '#story_generation', '#video', '#diffusion'], 'emoji': '🎬', 'ru': {'title': 'CINEMA: персонализированная генерация видео с множеством субъектов при помощи MLLM', 'desc': 'Статья представляет CINEMA - новую систему для генерации видео с несколькими персонажами, использующую мультимодальную большую языковую модель (MLLM). CINEMA устраняет необходимость в явном соответствии между изображениями субъектов и текстовыми сущностями, что уменьшает неоднозначность и упрощает аннотирование. Система позволяет масштабировать процесс, используя большие и разнообразные наборы данных для обучения. CINEMA демонстрирует значительное улучшение согласованности персонажей и общей связности видео по сравнению с существующими методами.'}, 'en': {'title': 'CINEMA: Coherent Multi-Subject Video Generation Made Easy', 'desc': 'This paper introduces CINEMA, a new framework for generating videos that feature multiple distinct subjects from separate reference images. Unlike traditional methods that rely on mapping images to text prompts, CINEMA uses a Multimodal Large Language Model (MLLM) to understand and interpret relationships between subjects, which reduces ambiguity. The framework allows for flexible conditioning on different numbers of subjects, making it easier to create personalized content. Extensive evaluations show that CINEMA enhances subject consistency and overall video coherence, opening up new possibilities for applications in storytelling and interactive media.'}, 'zh': {'title': '个性化多主体视频生成的新突破', 'desc': '视频生成在深度生成模型，特别是扩散模型的推动下取得了显著进展。现有方法在从文本提示或单张图像生成高质量视频方面表现出色，但个性化的多主体视频生成仍然是一个未被充分探索的挑战。我们提出了CINEMA框架，通过利用多模态大语言模型（MLLM），实现了一致的多主体视频生成，消除了主体图像与文本实体之间的明确对应关系，从而减少了歧义和标注工作。我们的框架能够根据不同数量的主体进行条件生成，提供了更大的个性化内容创作灵活性。'}}}, {'id': 'https://huggingface.co/papers/2503.10614', 'title': 'ConsisLoRA: Enhancing Content and Style Consistency for LoRA-based Style\n  Transfer', 'url': 'https://huggingface.co/papers/2503.10614', 'abstract': 'Style transfer involves transferring the style from a reference image to the content of a target image. Recent advancements in LoRA-based (Low-Rank Adaptation) methods have shown promise in effectively capturing the style of a single image. However, these approaches still face significant challenges such as content inconsistency, style misalignment, and content leakage. In this paper, we comprehensively analyze the limitations of the standard diffusion parameterization, which learns to predict noise, in the context of style transfer. To address these issues, we introduce ConsisLoRA, a LoRA-based method that enhances both content and style consistency by optimizing the LoRA weights to predict the original image rather than noise. We also propose a two-step training strategy that decouples the learning of content and style from the reference image. To effectively capture both the global structure and local details of the content image, we introduce a stepwise loss transition strategy. Additionally, we present an inference guidance method that enables continuous control over content and style strengths during inference. Through both qualitative and quantitative evaluations, our method demonstrates significant improvements in content and style consistency while effectively reducing content leakage.', 'score': 7, 'issue_id': 2712, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 марта', 'en': 'March 13', 'zh': '3月13日'}, 'hash': '12aacf8b56fdac9c', 'authors': ['Bolin Chen', 'Baoquan Zhao', 'Haoran Xie', 'Yi Cai', 'Qing Li', 'Xudong Mao'], 'affiliations': ['Lingnan University', 'South China University of Technology', 'Sun Yat-sen University', 'The Hong Kong Polytechnic University'], 'pdf_title_img': 'assets/pdf/title_img/2503.10614.jpg', 'data': {'categories': ['#synthetic', '#cv', '#inference', '#training', '#diffusion', '#leakage', '#optimization'], 'emoji': '🎨', 'ru': {'title': 'ConsisLoRA: Новый уровень согласованности в переносе стиля изображений', 'desc': 'Эта статья представляет новый метод переноса стиля изображений под названием ConsisLoRA. Метод основан на оптимизации весов LoRA для предсказания оригинального изображения вместо шума, что улучшает согласованность содержания и стиля. Авторы предлагают двухэтапную стратегию обучения и пошаговый переход функции потерь для эффективного захвата глобальной структуры и локальных деталей. Также представлен метод управления силой содержания и стиля при генерации изображений.'}, 'en': {'title': 'Enhancing Style Transfer with ConsisLoRA for Better Consistency', 'desc': 'This paper focuses on improving style transfer techniques, which blend the style of one image with the content of another. It identifies key challenges in existing methods, such as inconsistencies in content and style alignment. The authors introduce ConsisLoRA, a new approach that optimizes LoRA weights to enhance both content and style consistency by predicting the original image instead of noise. They also propose a two-step training strategy and a stepwise loss transition to better capture the details of the images, resulting in improved performance in style transfer tasks.'}, 'zh': {'title': '提升风格迁移的一致性与控制力', 'desc': '风格迁移是将参考图像的风格转移到目标图像内容上的技术。最近，基于LoRA（低秩适应）的方法在有效捕捉单一图像风格方面显示出良好前景，但仍面临内容不一致、风格错位和内容泄漏等挑战。本文分析了标准扩散参数化在风格迁移中的局限性，并提出了ConsisLoRA方法，通过优化LoRA权重来增强内容和风格的一致性。我们还提出了两步训练策略和逐步损失过渡策略，以有效捕捉内容图像的全局结构和局部细节。'}}}, {'id': 'https://huggingface.co/papers/2503.10630', 'title': 'UniGoal: Towards Universal Zero-shot Goal-oriented Navigation', 'url': 'https://huggingface.co/papers/2503.10630', 'abstract': 'In this paper, we propose a general framework for universal zero-shot goal-oriented navigation. Existing zero-shot methods build inference framework upon large language models (LLM) for specific tasks, which differs a lot in overall pipeline and fails to generalize across different types of goal. Towards the aim of universal zero-shot navigation, we propose a uniform graph representation to unify different goals, including object category, instance image and text description. We also convert the observation of agent into an online maintained scene graph. With this consistent scene and goal representation, we preserve most structural information compared with pure text and are able to leverage LLM for explicit graph-based reasoning. Specifically, we conduct graph matching between the scene graph and goal graph at each time instant and propose different strategies to generate long-term goal of exploration according to different matching states. The agent first iteratively searches subgraph of goal when zero-matched. With partial matching, the agent then utilizes coordinate projection and anchor pair alignment to infer the goal location. Finally scene graph correction and goal verification are applied for perfect matching. We also present a blacklist mechanism to enable robust switch between stages. Extensive experiments on several benchmarks show that our UniGoal achieves state-of-the-art zero-shot performance on three studied navigation tasks with a single model, even outperforming task-specific zero-shot methods and supervised universal methods.', 'score': 6, 'issue_id': 2700, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 марта', 'en': 'March 13', 'zh': '3月13日'}, 'hash': '9e7667a37c699f4c', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#reasoning', '#long_context', '#games', '#graphs', '#benchmark', '#agents', '#rl'], 'emoji': '🧭', 'ru': {'title': 'Универсальная навигация с нулевым обучением на основе графов и языковых моделей', 'desc': 'Статья представляет универсальную систему для навигации с нулевым обучением к различным целям. Авторы предлагают единое графовое представление для унификации разных типов целей и наблюдений агента. Используя большие языковые модели для рассуждений на основе графов, система проводит сопоставление графа сцены и графа цели. Предложенный метод UniGoal достигает лучших результатов в задачах навигации с нулевым обучением по сравнению с существующими подходами.'}, 'en': {'title': 'Navigating Goals Universally with Graphs!', 'desc': 'This paper introduces a new framework for universal zero-shot goal-oriented navigation, which allows an agent to navigate towards various goals without prior training on specific tasks. Unlike existing methods that rely heavily on large language models (LLMs) tailored for individual tasks, this approach uses a uniform graph representation to integrate different types of goals, such as object categories and text descriptions. The agent maintains an online scene graph that captures the environment, enabling it to perform graph-based reasoning and match its observations with the goals. The proposed method demonstrates superior performance in navigation tasks, surpassing both task-specific and supervised approaches, by effectively managing goal exploration and verification through innovative strategies.'}, 'zh': {'title': '通用零-shot导航的创新框架', 'desc': '本文提出了一种通用的零-shot目标导向导航框架。现有的零-shot方法通常依赖于大型语言模型（LLM）进行特定任务的推理，导致在不同目标之间的泛化能力不足。我们提出了一种统一的图表示方法，将不同的目标（如物体类别、实例图像和文本描述）整合在一起，并将代理的观察结果转换为在线维护的场景图。通过这种一致的场景和目标表示，我们能够利用LLM进行基于图的推理，并在多个基准测试中展示了我们的UniGoal在零-shot导航任务上的优越性能。'}}, 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2503.10568', 'title': 'Autoregressive Image Generation with Randomized Parallel Decoding', 'url': 'https://huggingface.co/papers/2503.10568', 'abstract': 'We introduce ARPG, a novel visual autoregressive model that enables randomized parallel generation, addressing the inherent limitations of conventional raster-order approaches, which hinder inference efficiency and zero-shot generalization due to their sequential, predefined token generation order. Our key insight is that effective random-order modeling necessitates explicit guidance for determining the position of the next predicted token. To this end, we propose a novel guided decoding framework that decouples positional guidance from content representation, encoding them separately as queries and key-value pairs. By directly incorporating this guidance into the causal attention mechanism, our approach enables fully random-order training and generation, eliminating the need for bidirectional attention. Consequently, ARPG readily generalizes to zero-shot tasks such as image inpainting, outpainting, and resolution expansion. Furthermore, it supports parallel inference by concurrently processing multiple queries using a shared KV cache. On the ImageNet-1K 256 benchmark, our approach attains an FID of 1.94 with only 64 sampling steps, achieving over a 20-fold increase in throughput while reducing memory consumption by over 75% compared to representative recent autoregressive models at a similar scale.', 'score': 6, 'issue_id': 2701, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 марта', 'en': 'March 13', 'zh': '3月13日'}, 'hash': '44a1e551455870ac', 'authors': ['Haopeng Li', 'Jinyue Yang', 'Guoqi Li', 'Huan Wang'], 'affiliations': ['Institute of Automation, Chinese Academy of Sciences', 'Westlake University'], 'pdf_title_img': 'assets/pdf/title_img/2503.10568.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#cv', '#architecture'], 'emoji': '🎨', 'ru': {'title': 'ARPG: Революция в генерации изображений с случайным порядком и параллельным выводом', 'desc': 'ARPG - это новая визуальная авторегрессионная модель, позволяющая осуществлять рандомизированную параллельную генерацию изображений. Она решает проблемы традиционных подходов с фиксированным порядком генерации токенов, предлагая механизм управляемого декодирования, который разделяет позиционное руководство и представление контента. ARPG обучается и генерирует в полностью случайном порядке, что позволяет ей легко адаптироваться к задачам без дополнительного обучения, таким как заполнение и расширение изображений. Модель демонстрирует высокую производительность и эффективность по сравнению с аналогами, достигая FID 1.94 на ImageNet-1K 256 всего за 64 шага сэмплирования.'}, 'en': {'title': 'Revolutionizing Image Generation with Randomized Parallel Processing', 'desc': 'The paper presents ARPG, a new visual autoregressive model designed for efficient and flexible image generation. Unlike traditional methods that generate tokens in a fixed order, ARPG allows for randomized parallel generation, improving inference speed and enabling zero-shot generalization. It introduces a guided decoding framework that separates positional guidance from content representation, enhancing the causal attention mechanism. As a result, ARPG excels in tasks like image inpainting and outpainting, achieving significant performance improvements on benchmarks while using less memory.'}, 'zh': {'title': 'ARPG：实现随机并行生成的自回归模型', 'desc': '我们提出了一种新颖的视觉自回归模型ARPG，能够实现随机并行生成，解决了传统光栅顺序方法在推理效率和零-shot泛化方面的固有限制。我们的关键见解是，有效的随机顺序建模需要明确的指导来确定下一个预测标记的位置。为此，我们提出了一种新颖的引导解码框架，将位置指导与内容表示分开编码，分别作为查询和键值对。通过将这种指导直接融入因果注意力机制，我们的方法实现了完全随机顺序的训练和生成，消除了对双向注意力的需求。'}}}, {'id': 'https://huggingface.co/papers/2503.10365', 'title': 'Piece it Together: Part-Based Concepting with IP-Priors', 'url': 'https://huggingface.co/papers/2503.10365', 'abstract': 'Advanced generative models excel at synthesizing images but often rely on text-based conditioning. Visual designers, however, often work beyond language, directly drawing inspiration from existing visual elements. In many cases, these elements represent only fragments of a potential concept-such as an uniquely structured wing, or a specific hairstyle-serving as inspiration for the artist to explore how they can come together creatively into a coherent whole. Recognizing this need, we introduce a generative framework that seamlessly integrates a partial set of user-provided visual components into a coherent composition while simultaneously sampling the missing parts needed to generate a plausible and complete concept. Our approach builds on a strong and underexplored representation space, extracted from IP-Adapter+, on which we train IP-Prior, a lightweight flow-matching model that synthesizes coherent compositions based on domain-specific priors, enabling diverse and context-aware generations. Additionally, we present a LoRA-based fine-tuning strategy that significantly improves prompt adherence in IP-Adapter+ for a given task, addressing its common trade-off between reconstruction quality and prompt adherence.', 'score': 6, 'issue_id': 2713, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 марта', 'en': 'March 13', 'zh': '3月13日'}, 'hash': '80bcd7657c63561e', 'authors': ['Elad Richardson', 'Kfir Goldberg', 'Yuval Alaluf', 'Daniel Cohen-Or'], 'affiliations': ['Bria AI', 'Tel Aviv University'], 'pdf_title_img': 'assets/pdf/title_img/2503.10365.jpg', 'data': {'categories': ['#training', '#cv', '#multimodal', '#synthetic'], 'emoji': '🎨', 'ru': {'title': 'Генерация изображений на основе визуальных компонентов: новый подход к творческому синтезу', 'desc': 'Статья представляет новый генеративный фреймворк для создания изображений, который интегрирует визуальные компоненты, предоставленные пользователем, в целостную композицию. Авторы разработали модель IP-Prior, основанную на пространстве представлений IP-Adapter+, которая синтезирует согласованные композиции на основе доменно-специфичных приоров. Предложенный подход позволяет генерировать разнообразные и контекстно-зависимые изображения. Кроме того, авторы представили стратегию тонкой настройки на основе LoRA, которая значительно улучшает соответствие промпту в IP-Adapter+.'}, 'en': {'title': 'Bridging Visual Inspiration with Generative Design', 'desc': "This paper presents a new generative framework that allows visual designers to create coherent compositions by integrating partial visual elements provided by users. Unlike traditional models that rely heavily on text descriptions, this approach focuses on visual components, enabling artists to explore creative combinations. The framework utilizes a lightweight flow-matching model called IP-Prior, which is trained on a specialized representation space to ensure context-aware and diverse outputs. Additionally, the authors introduce a fine-tuning strategy that enhances the model's ability to adhere to user prompts while maintaining high reconstruction quality."}, 'zh': {'title': '无缝整合视觉元素，创造连贯概念', 'desc': '本论文介绍了一种生成框架，能够将用户提供的部分视觉元素无缝整合成一个连贯的整体，同时生成所需的缺失部分，以创造出合理且完整的概念。该方法基于一个强大且未被充分探索的表示空间，利用IP-Adapter+提取的特征，训练出轻量级的流匹配模型IP-Prior。此模型能够基于特定领域的先验知识合成连贯的作品，实现多样化和上下文感知的生成。此外，我们还提出了一种基于LoRA的微调策略，显著提高了IP-Adapter+在特定任务中的提示遵循性，解决了重建质量与提示遵循性之间的常见权衡问题。'}}}, {'id': 'https://huggingface.co/papers/2503.09905', 'title': "Quantization for OpenAI's Whisper Models: A Comparative Analysis", 'url': 'https://huggingface.co/papers/2503.09905', 'abstract': 'Automated speech recognition (ASR) models have gained prominence for applications such as captioning, speech translation, and live transcription. This paper studies Whisper and two model variants: one optimized for live speech streaming and another for offline transcription. Notably, these models have been found to generate hallucinated content, reducing transcription reliability. Furthermore, larger model variants exhibit increased latency and pose challenges for deployment on resource-constrained devices. This study analyzes the similarities and differences between three Whisper models, qualitatively examining their distinct capabilities. Next, this study quantifies the impact of model quantization on latency and evaluates its viability for edge deployment. Using the open source LibriSpeech dataset, this paper evaluates the word error rate (WER) along with latency analysis of whispercpp using 3 quantization methods (INT4, INT5, INT8). Results show that quantization reduces latency by 19\\% and model size by 45\\%, while preserving transcription accuracy. These findings provide insights into the optimal use cases of different Whisper models and edge device deployment possibilities. All code, datasets, and implementation details are available in a public GitHub repository: https://github.com/allisonandreyev/WhisperQuantization.git', 'score': 6, 'issue_id': 2700, 'pub_date': '2025-03-12', 'pub_date_card': {'ru': '12 марта', 'en': 'March 12', 'zh': '3月12日'}, 'hash': '8d7b17a97a4cbb6e', 'authors': ['Allison Andreyev'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2503.09905.jpg', 'data': {'categories': ['#inference', '#audio', '#open_source', '#optimization', '#hallucinations', '#dataset'], 'emoji': '🎙️', 'ru': {'title': 'Оптимизация моделей Whisper для эффективного распознавания речи на периферийных устройствах', 'desc': 'Это исследование анализирует модели автоматического распознавания речи Whisper и их варианты для потоковой и офлайн-транскрипции. Авторы изучают проблему галлюцинаций в выводе моделей и трудности развертывания больших моделей на устройствах с ограниченными ресурсами. Проводится оценка влияния квантования модели на задержку и точность транскрипции с использованием набора данных LibriSpeech. Результаты показывают, что квантование уменьшает задержку на 19% и размер модели на 45%, сохраняя при этом точность транскрипции.'}, 'en': {'title': 'Optimizing Whisper Models for Efficient Speech Recognition', 'desc': 'This paper investigates the performance of Whisper, an automated speech recognition (ASR) model, along with its two variants tailored for live streaming and offline transcription. It highlights the issue of hallucinated content in the transcriptions, which can compromise reliability, especially in larger models that also face latency challenges. The study further explores the effects of model quantization on latency and size, demonstrating a significant reduction in both while maintaining transcription accuracy. By analyzing the word error rate (WER) using the LibriSpeech dataset, the research provides valuable insights for deploying Whisper models on resource-constrained devices.'}, 'zh': {'title': '优化Whisper模型，提升语音识别效率', 'desc': '这篇论文研究了自动语音识别（ASR）模型Whisper及其两个变体，一个针对实时语音流优化，另一个用于离线转录。研究发现，这些模型可能会生成虚假内容，从而降低转录的可靠性。此外，较大的模型变体在延迟方面表现较差，给资源受限的设备部署带来了挑战。通过对比分析，论文量化了模型量化对延迟的影响，并评估了其在边缘设备上的可行性。'}}}, {'id': 'https://huggingface.co/papers/2503.09837', 'title': 'On the Limitations of Vision-Language Models in Understanding Image\n  Transforms', 'url': 'https://huggingface.co/papers/2503.09837', 'abstract': 'Vision Language Models (VLMs) have demonstrated significant potential in various downstream tasks, including Image/Video Generation, Visual Question Answering, Multimodal Chatbots, and Video Understanding. However, these models often struggle with basic image transformations. This paper investigates the image-level understanding of VLMs, specifically CLIP by OpenAI and SigLIP by Google. Our findings reveal that these models lack comprehension of multiple image-level augmentations. To facilitate this study, we created an augmented version of the Flickr8k dataset, pairing each image with a detailed description of the applied transformation. We further explore how this deficiency impacts downstream tasks, particularly in image editing, and evaluate the performance of state-of-the-art Image2Image models on simple transformations.', 'score': 6, 'issue_id': 2709, 'pub_date': '2025-03-12', 'pub_date_card': {'ru': '12 марта', 'en': 'March 12', 'zh': '3月12日'}, 'hash': 'cc8e8180cfeef80d', 'authors': ['Ahmad Mustafa Anis', 'Hasnain Ali', 'Saquib Sarfraz'], 'affiliations': ['Arbisoft', 'Cohere for AI Community', 'Karlsruhe Institute of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2503.09837.jpg', 'data': {'categories': ['#dataset', '#cv', '#multimodal', '#games', '#interpretability'], 'emoji': '🔍', 'ru': {'title': 'Ограниченное понимание преобразований изображений в современных VLM', 'desc': 'Это исследование посвящено анализу способности моделей визуального языка (VLM) понимать базовые преобразования изображений. Авторы обнаружили, что популярные модели CLIP и SigLIP не справляются с распознаванием многих аугментаций на уровне изображений. Для проведения исследования был создан расширенный набор данных на основе Flickr8k с подробными описаниями применённых трансформаций. Также изучалось влияние этого недостатка на задачи редактирования изображений и оценивалась эффективность современных моделей Image2Image при выполнении простых преобразований.'}, 'en': {'title': 'Unlocking Image Transformations in Vision Language Models', 'desc': 'This paper examines the limitations of Vision Language Models (VLMs) like CLIP and SigLIP in understanding basic image transformations. Despite their success in tasks such as image generation and visual question answering, these models struggle with comprehending various image-level augmentations. The authors created an augmented Flickr8k dataset to analyze how well these models recognize and describe transformations applied to images. The study also assesses the impact of these deficiencies on downstream tasks, particularly in image editing, and evaluates the performance of advanced Image2Image models in handling simple transformations.'}, 'zh': {'title': '提升视觉语言模型的图像理解能力', 'desc': '本文研究了视觉语言模型（VLMs）在图像理解方面的不足，特别是OpenAI的CLIP和Google的SigLIP。尽管这些模型在多种下游任务中表现出色，但它们在基本图像变换方面存在困难。我们创建了一个增强版的Flickr8k数据集，为每张图像配上详细的变换描述，以便进行研究。研究结果表明，这种理解缺陷会影响下游任务的表现，尤其是在图像编辑方面。'}}}, {'id': 'https://huggingface.co/papers/2503.09046', 'title': 'Discovering Influential Neuron Path in Vision Transformers', 'url': 'https://huggingface.co/papers/2503.09046', 'abstract': "Vision Transformer models exhibit immense power yet remain opaque to human understanding, posing challenges and risks for practical applications. While prior research has attempted to demystify these models through input attribution and neuron role analysis, there's been a notable gap in considering layer-level information and the holistic path of information flow across layers. In this paper, we investigate the significance of influential neuron paths within vision Transformers, which is a path of neurons from the model input to output that impacts the model inference most significantly. We first propose a joint influence measure to assess the contribution of a set of neurons to the model outcome. And we further provide a layer-progressive neuron locating approach that efficiently selects the most influential neuron at each layer trying to discover the crucial neuron path from input to output within the target model. Our experiments demonstrate the superiority of our method finding the most influential neuron path along which the information flows, over the existing baseline solutions. Additionally, the neuron paths have illustrated that vision Transformers exhibit some specific inner working mechanism for processing the visual information within the same image category. We further analyze the key effects of these neurons on the image classification task, showcasing that the found neuron paths have already preserved the model capability on downstream tasks, which may also shed some lights on real-world applications like model pruning. The project website including implementation code is available at https://foundation-model-research.github.io/NeuronPath/.", 'score': 6, 'issue_id': 2712, 'pub_date': '2025-03-12', 'pub_date_card': {'ru': '12 марта', 'en': 'March 12', 'zh': '3月12日'}, 'hash': 'c4d7c64b526c6486', 'authors': ['Yifan Wang', 'Yifei Liu', 'Yingdong Shi', 'Changming Li', 'Anqi Pang', 'Sibei Yang', 'Jingyi Yu', 'Kan Ren'], 'affiliations': ['ShanghaiTech University', 'Tencent PCG'], 'pdf_title_img': 'assets/pdf/title_img/2503.09046.jpg', 'data': {'categories': ['#architecture', '#interpretability', '#cv', '#training', '#inference'], 'emoji': '🔍', 'ru': {'title': 'Раскрытие тайн Vision Transformer: исследование влиятельных нейронных путей', 'desc': 'Статья исследует значимые пути нейронов в моделях Vision Transformer, предлагая метод оценки влияния групп нейронов на результат модели. Авторы разработали послойный подход для выбора наиболее влиятельных нейронов, позволяющий обнаружить ключевой путь прохождения информации от входа к выходу. Эксперименты показали превосходство предложенного метода над существующими решениями в поиске наиболее значимых нейронных путей. Анализ выявленных путей нейронов продемонстрировал специфические механизмы обработки визуальной информации в Vision Transformer и потенциал для практических приложений, таких как pruning моделей.'}, 'en': {'title': 'Unveiling the Neuron Paths in Vision Transformers', 'desc': "This paper explores how to better understand Vision Transformer models by focusing on the paths of influential neurons that contribute significantly to model predictions. It introduces a joint influence measure to evaluate the impact of neuron sets on the output and proposes a method to identify the most important neurons layer by layer. The findings reveal that these neuron paths not only clarify the model's decision-making process but also maintain the model's performance on tasks like image classification. This research could lead to practical applications such as model pruning, enhancing the efficiency of Vision Transformers in real-world scenarios."}, 'zh': {'title': '揭示视觉变换器中的关键神经元路径', 'desc': '本文研究了视觉变换器模型中影响力神经元路径的重要性，这些路径从模型输入到输出，对模型推理有显著影响。我们提出了一种联合影响度量方法，评估一组神经元对模型结果的贡献，并提供了一种逐层神经元定位方法，以高效选择每层中最具影响力的神经元。实验结果表明，我们的方法在寻找信息流动的最具影响力神经元路径方面优于现有基线解决方案。此外，这些神经元路径揭示了视觉变换器在处理同一图像类别的视觉信息时的特定内部工作机制。'}}}, {'id': 'https://huggingface.co/papers/2503.10242', 'title': 'MinorBench: A hand-built benchmark for content-based risks for children', 'url': 'https://huggingface.co/papers/2503.10242', 'abstract': "Large Language Models (LLMs) are rapidly entering children's lives - through parent-driven adoption, schools, and peer networks - yet current AI ethics and safety research do not adequately address content-related risks specific to minors. In this paper, we highlight these gaps with a real-world case study of an LLM-based chatbot deployed in a middle school setting, revealing how students used and sometimes misused the system. Building on these findings, we propose a new taxonomy of content-based risks for minors and introduce MinorBench, an open-source benchmark designed to evaluate LLMs on their ability to refuse unsafe or inappropriate queries from children. We evaluate six prominent LLMs under different system prompts, demonstrating substantial variability in their child-safety compliance. Our results inform practical steps for more robust, child-focused safety mechanisms and underscore the urgency of tailoring AI systems to safeguard young users.", 'score': 4, 'issue_id': 2712, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 марта', 'en': 'March 13', 'zh': '3月13日'}, 'hash': '28234b0e69863cbe', 'authors': ['Shaun Khoo', 'Gabriel Chua', 'Rachel Shong'], 'affiliations': ['Government Technology Agency Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2503.10242.jpg', 'data': {'categories': ['#open_source', '#multimodal', '#benchmark', '#ethics'], 'emoji': '🧒', 'ru': {'title': 'Защита детей от рисков больших языковых моделей', 'desc': 'Данная статья исследует риски использования больших языковых моделей (LLM) детьми. Авторы проводят анализ реального случая применения чат-бота на основе LLM в средней школе. На основе полученных результатов предлагается новая таксономия контентных рисков для несовершеннолетних и представляется MinorBench - открытый бенчмарк для оценки способности LLM отклонять небезопасные или неуместные запросы от детей. Исследование подчеркивает необходимость разработки более надежных механизмов безопасности, ориентированных на детей.'}, 'en': {'title': 'Ensuring Child Safety in AI: A Call for Better LLM Standards', 'desc': 'This paper discusses the increasing presence of Large Language Models (LLMs) in the lives of children and the associated risks that are not adequately addressed by current AI ethics. It presents a case study of a chatbot used in a middle school, highlighting both the positive and negative ways students interacted with the system. The authors propose a new classification system for content-related risks specific to minors and introduce MinorBench, a benchmark to assess LLMs on their ability to handle inappropriate queries from children. The evaluation of six LLMs shows significant differences in their compliance with child safety standards, emphasizing the need for improved safety measures tailored for young users.'}, 'zh': {'title': '保护儿童，安全使用大型语言模型', 'desc': '本文探讨了大型语言模型（LLMs）在儿童生活中的应用，尤其是在学校环境中的使用情况。通过案例研究，我们发现学生在使用聊天机器人时存在内容相关的风险，尤其是对未成年人而言。为了解决这些问题，我们提出了一种新的未成年人内容风险分类法，并引入了MinorBench，一个开源基准，用于评估LLMs拒绝不安全或不当查询的能力。我们的研究结果显示，不同的LLMs在儿童安全合规性方面存在显著差异，强调了为保护年轻用户而定制AI系统的紧迫性。'}}}, {'id': 'https://huggingface.co/papers/2503.10072', 'title': '"Silent Is Not Actually Silent": An Investigation of Toxicity on Bug\n  Report Discussion', 'url': 'https://huggingface.co/papers/2503.10072', 'abstract': 'Toxicity in bug report discussions poses significant challenges to the collaborative dynamics of open-source software development. Bug reports are crucial for identifying and resolving defects, yet their inherently problem-focused nature and emotionally charged context make them susceptible to toxic interactions. This study explores toxicity in GitHub bug reports through a qualitative analysis of 203 bug threads, including 81 toxic ones. Our findings reveal that toxicity frequently arises from misaligned perceptions of bug severity and priority, unresolved frustrations with tools, and lapses in professional communication. These toxic interactions not only derail productive discussions but also reduce the likelihood of actionable outcomes, such as linking issues with pull requests. Our preliminary findings offer actionable recommendations to improve bug resolution by mitigating toxicity.', 'score': 4, 'issue_id': 2699, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 марта', 'en': 'March 13', 'zh': '3月13日'}, 'hash': 'a91b6bc8232847a3', 'authors': ['Mia Mohammad Imran', 'Jaydeb Sarker'], 'affiliations': ['Missouri University of Science and Technology, Rolla, Missouri, USA', 'University of Nebraska Omaha, Omaha, Nebraska, USA'], 'pdf_title_img': 'assets/pdf/title_img/2503.10072.jpg', 'data': {'categories': ['#ethics', '#open_source'], 'emoji': '🐛', 'ru': {'title': 'Токсичность в отчетах об ошибках: препятствие для продуктивного разрешения проблем', 'desc': 'Исследование посвящено проблеме токсичности в обсуждениях отчетов об ошибках в проектах с открытым исходным кодом. Авторы провели качественный анализ 203 веток обсуждения ошибок на GitHub, включая 81 токсичную. Результаты показывают, что токсичность часто возникает из-за расхождений в восприятии серьезности и приоритетности ошибок, нерешенных проблем с инструментами и нарушений профессиональной коммуникации. Токсичные взаимодействия не только нарушают продуктивные обсуждения, но и снижают вероятность достижения конкретных результатов, таких как привязка проблем к запросам на слияние.'}, 'en': {'title': 'Mitigating Toxicity for Better Bug Resolution in Open Source', 'desc': 'This paper investigates the issue of toxicity in bug report discussions within open-source software development on GitHub. It analyzes 203 bug threads, identifying 81 as toxic, and highlights that toxicity often stems from differing views on bug severity, frustrations with tools, and poor communication. The study emphasizes that such toxic interactions hinder productive collaboration and decrease the chances of resolving issues effectively. The authors provide recommendations aimed at reducing toxicity to enhance the bug resolution process.'}, 'zh': {'title': '减少毒性，提升开源协作效率', 'desc': '本研究探讨了开源软件开发中，GitHub bug 报告讨论中的毒性问题。通过对203个bug线程的定性分析，发现81个线程存在毒性互动。研究表明，毒性往往源于对bug严重性和优先级的误解、对工具的不满以及专业沟通的缺失。这些毒性互动不仅影响了有效讨论，还降低了将问题与拉取请求关联的可能性，因此提出了减少毒性以改善bug解决的建议。'}}}, {'id': 'https://huggingface.co/papers/2503.10636', 'title': 'The Curse of Conditions: Analyzing and Improving Optimal Transport for\n  Conditional Flow-Based Generation', 'url': 'https://huggingface.co/papers/2503.10636', 'abstract': 'Minibatch optimal transport coupling straightens paths in unconditional flow matching. This leads to computationally less demanding inference as fewer integration steps and less complex numerical solvers can be employed when numerically solving an ordinary differential equation at test time. However, in the conditional setting, minibatch optimal transport falls short. This is because the default optimal transport mapping disregards conditions, resulting in a conditionally skewed prior distribution during training. In contrast, at test time, we have no access to the skewed prior, and instead sample from the full, unbiased prior distribution. This gap between training and testing leads to a subpar performance. To bridge this gap, we propose conditional optimal transport C^2OT that adds a conditional weighting term in the cost matrix when computing the optimal transport assignment. Experiments demonstrate that this simple fix works with both discrete and continuous conditions in 8gaussians-to-moons, CIFAR-10, ImageNet-32x32, and ImageNet-256x256. Our method performs better overall compared to the existing baselines across different function evaluation budgets. Code is available at https://hkchengrex.github.io/C2OT', 'score': 3, 'issue_id': 2700, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 марта', 'en': 'March 13', 'zh': '3月13日'}, 'hash': '93d6f410c35246be', 'authors': ['Ho Kei Cheng', 'Alexander Schwing'], 'affiliations': ['University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2503.10636.jpg', 'data': {'categories': ['#cv', '#inference', '#optimization', '#training'], 'emoji': '🔀', 'ru': {'title': 'Условный оптимальный транспорт для улучшения потоковых моделей', 'desc': 'Статья предлагает метод условного оптимального транспорта (C^2OT) для улучшения обучения потоковых моделей в условных задачах. Авторы выявили проблему несоответствия между тренировочным и тестовым распределением при использовании мини-батч оптимального транспорта в условных моделях. C^2OT решает эту проблему путем добавления условного весового коэффициента в матрицу стоимости при вычислении оптимального транспорта. Эксперименты показывают, что предложенный метод работает лучше существующих базовых линий на различных наборах данных и при разных бюджетах вычислений.'}, 'en': {'title': 'Bridging the Gap in Conditional Flow Matching with C^2OT', 'desc': 'This paper introduces a method called conditional optimal transport (C^2OT) to improve the performance of flow matching in machine learning. The authors highlight that while minibatch optimal transport simplifies computations during inference, it fails in conditional settings due to a mismatch between training and testing distributions. By incorporating a conditional weighting term in the cost matrix, C^2OT effectively aligns the training and testing conditions. Experiments show that this approach outperforms existing methods across various datasets, demonstrating its effectiveness in both discrete and continuous scenarios.'}, 'zh': {'title': '条件最优传输：缩小训练与测试的性能差距', 'desc': '本文提出了一种条件最优传输方法C^2OT，以解决在条件设置下小批量最优传输的不足。传统的最优传输映射在训练时忽略了条件，导致训练期间的先验分布偏斜，而测试时却无法访问这种偏斜的先验。通过在成本矩阵中添加条件加权项，C^2OT能够更好地计算最优传输分配，从而缩小训练与测试之间的性能差距。实验结果表明，该方法在多个数据集上表现优于现有基线。'}}}, {'id': 'https://huggingface.co/papers/2503.10602', 'title': 'TruthPrInt: Mitigating LVLM Object Hallucination Via Latent\n  Truthful-Guided Pre-Intervention', 'url': 'https://huggingface.co/papers/2503.10602', 'abstract': 'Object Hallucination (OH) has been acknowledged as one of the major trustworthy challenges in Large Vision-Language Models (LVLMs). Recent advancements in Large Language Models (LLMs) indicate that internal states, such as hidden states, encode the "overall truthfulness" of generated responses. However, it remains under-explored how internal states in LVLMs function and whether they could serve as "per-token" hallucination indicators, which is essential for mitigating OH. In this paper, we first conduct an in-depth exploration of LVLM internal states in relation to OH issues and discover that (1) LVLM internal states are high-specificity per-token indicators of hallucination behaviors. Moreover, (2) different LVLMs encode universal patterns of hallucinations in common latent subspaces, indicating that there exist "generic truthful directions" shared by various LVLMs. Based on these discoveries, we propose Truthful-Guided Pre-Intervention (TruthPrInt) that first learns the truthful direction of LVLM decoding and then applies truthful-guided inference-time intervention during LVLM decoding. We further propose ComnHallu to enhance both cross-LVLM and cross-data hallucination detection transferability by constructing and aligning hallucination latent subspaces. We evaluate TruthPrInt in extensive experimental settings, including in-domain and out-of-domain scenarios, over popular LVLMs and OH benchmarks. Experimental results indicate that TruthPrInt significantly outperforms state-of-the-art methods. Codes will be available at https://github.com/jinhaoduan/TruthPrInt.', 'score': 3, 'issue_id': 2712, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 марта', 'en': 'March 13', 'zh': '3月13日'}, 'hash': '9d4ccef966377a11', 'authors': ['Jinhao Duan', 'Fei Kong', 'Hao Cheng', 'James Diffenderfer', 'Bhavya Kailkhura', 'Lichao Sun', 'Xiaofeng Zhu', 'Xiaoshuang Shi', 'Kaidi Xu'], 'affiliations': ['Drexel University', 'Hong Kong University of Science and Technology (Guangzhou)', 'LLNL', 'Lehigh University', 'University of Electronic Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2503.10602.jpg', 'data': {'categories': ['#interpretability', '#multimodal', '#transfer_learning', '#cv', '#training', '#hallucinations', '#benchmark'], 'emoji': '🔍', 'ru': {'title': 'Борьба с галлюцинациями в больших моделях', 'desc': 'В статье рассматривается проблема галлюцинации объектов в больших моделях, объединяющих зрение и язык (LVLMs). Исследователи обнаружили, что внутренние состояния этих моделей могут служить индикаторами галлюцинаций на уровне каждого токена. Они также выявили, что разные LVLMs имеют общие паттерны галлюцинаций в скрытых подпространствах. На основе этих открытий предложен метод TruthPrInt, который улучшает точность моделей за счёт направленного вмешательства в процесс декодирования.'}, 'en': {'title': 'Enhancing Truthfulness in Vision-Language Models', 'desc': "This paper addresses the problem of Object Hallucination (OH) in Large Vision-Language Models (LVLMs), which affects the reliability of their outputs. It investigates how internal states of LVLMs can act as indicators of hallucination on a per-token basis, revealing that these states can signal specific hallucination behaviors. The authors introduce a method called Truthful-Guided Pre-Intervention (TruthPrInt) that learns the 'truthful direction' during decoding to improve the accuracy of generated responses. Additionally, they propose ComnHallu to enhance the detection of hallucinations across different LVLMs and datasets by aligning their latent subspaces, demonstrating significant improvements over existing methods in their experiments."}, 'zh': {'title': '揭示LVLM中的真实方向，减少对象幻觉', 'desc': '本文探讨了大型视觉语言模型（LVLMs）中的对象幻觉（OH）问题，认为内部状态可以作为幻觉行为的指示器。研究发现，LVLM的内部状态能够高特异性地指示每个token的幻觉行为，并且不同的LVLM在潜在子空间中编码了通用的幻觉模式。基于这些发现，提出了一种名为Truthful-Guided Pre-Intervention（TruthPrInt）的方法，通过学习LVLM解码的真实方向来进行干预。实验结果表明，TruthPrInt在多个流行的LVLM和OH基准测试中显著优于现有的最先进方法。'}}}, {'id': 'https://huggingface.co/papers/2503.10638', 'title': 'Studying Classifier(-Free) Guidance From a Classifier-Centric\n  Perspective', 'url': 'https://huggingface.co/papers/2503.10638', 'abstract': 'Classifier-free guidance has become a staple for conditional generation with denoising diffusion models. However, a comprehensive understanding of classifier-free guidance is still missing. In this work, we carry out an empirical study to provide a fresh perspective on classifier-free guidance. Concretely, instead of solely focusing on classifier-free guidance, we trace back to the root, i.e., classifier guidance, pinpoint the key assumption for the derivation, and conduct a systematic study to understand the role of the classifier. We find that both classifier guidance and classifier-free guidance achieve conditional generation by pushing the denoising diffusion trajectories away from decision boundaries, i.e., areas where conditional information is usually entangled and is hard to learn. Based on this classifier-centric understanding, we propose a generic postprocessing step built upon flow-matching to shrink the gap between the learned distribution for a pre-trained denoising diffusion model and the real data distribution, majorly around the decision boundaries. Experiments on various datasets verify the effectiveness of the proposed approach.', 'score': 2, 'issue_id': 2715, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 марта', 'en': 'March 13', 'zh': '3月13日'}, 'hash': '67e97d098f101b16', 'authors': ['Xiaoming Zhao', 'Alexander G. Schwing'], 'affiliations': ['University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2503.10638.jpg', 'data': {'categories': ['#dataset', '#optimization', '#training', '#diffusion', '#benchmark'], 'emoji': '🧭', 'ru': {'title': 'Новый взгляд на управление генерацией в моделях диффузии', 'desc': 'Это исследование посвящено анализу методов управления генерацией в моделях диффузии, в частности, classifier-free guidance и classifier guidance. Авторы обнаружили, что оба метода работают, отталкивая траектории денойзинга от границ принятия решений. На основе этого понимания они предложили новый метод постобработки с использованием flow-matching для улучшения распределения предобученной модели диффузии. Эксперименты подтвердили эффективность предложенного подхода на различных наборах данных.'}, 'en': {'title': 'Bridging the Gap in Conditional Generation with Classifier Insights', 'desc': 'This paper explores classifier-free guidance in denoising diffusion models, which is important for generating data based on specific conditions. The authors investigate the foundational concept of classifier guidance to better understand how both methods work. They discover that both approaches help in generating data by moving away from decision boundaries, where information is complex and difficult to learn. To improve the performance of pre-trained models, they introduce a new postprocessing technique that aligns the learned data distribution with the actual data distribution, particularly near these decision boundaries.'}, 'zh': {'title': '深入理解无分类器引导的条件生成', 'desc': '无分类器引导已成为去噪扩散模型条件生成的常用方法。然而，目前对无分类器引导的全面理解仍然缺乏。本文通过实证研究提供了对无分类器引导的新视角，追溯到分类器引导的根源，明确了推导的关键假设，并系统研究了分类器的作用。我们的研究发现，无论是分类器引导还是无分类器引导，都是通过将去噪扩散轨迹推离决策边界来实现条件生成，这些边界通常是条件信息交织且难以学习的区域。'}}}, {'id': 'https://huggingface.co/papers/2503.09368', 'title': 'PerCoV2: Improved Ultra-Low Bit-Rate Perceptual Image Compression with\n  Implicit Hierarchical Masked Image Modeling', 'url': 'https://huggingface.co/papers/2503.09368', 'abstract': 'We introduce PerCoV2, a novel and open ultra-low bit-rate perceptual image compression system designed for bandwidth- and storage-constrained applications. Building upon prior work by Careil et al., PerCoV2 extends the original formulation to the Stable Diffusion 3 ecosystem and enhances entropy coding efficiency by explicitly modeling the discrete hyper-latent image distribution. To this end, we conduct a comprehensive comparison of recent autoregressive methods (VAR and MaskGIT) for entropy modeling and evaluate our approach on the large-scale MSCOCO-30k benchmark. Compared to previous work, PerCoV2 (i) achieves higher image fidelity at even lower bit-rates while maintaining competitive perceptual quality, (ii) features a hybrid generation mode for further bit-rate savings, and (iii) is built solely on public components. Code and trained models will be released at https://github.com/Nikolai10/PerCoV2.', 'score': 2, 'issue_id': 2707, 'pub_date': '2025-03-12', 'pub_date_card': {'ru': '12 марта', 'en': 'March 12', 'zh': '3月12日'}, 'hash': '7aa613c0b5e4220f', 'authors': ['Nikolai Körber', 'Eduard Kromer', 'Andreas Siebert', 'Sascha Hauke', 'Daniel Mueller-Gritschneder', 'Björn Schuller'], 'affiliations': ['TU Wien', 'Technical University of Munich', 'University of Applied Sciences Landshut'], 'pdf_title_img': 'assets/pdf/title_img/2503.09368.jpg', 'data': {'categories': ['#cv', '#benchmark', '#dataset', '#architecture'], 'emoji': '🗜️', 'ru': {'title': 'Сжимай эффективнее, сохраняй качество: PerCoV2 на страже битрейта', 'desc': 'PerCoV2 - это новая система сжатия изображений с ультранизким битрейтом, основанная на перцептивном подходе. Она использует экосистему Stable Diffusion 3 и улучшает эффективность энтропийного кодирования путем моделирования дискретного гиперлатентного распределения изображений. Авторы сравнивают авторегрессионные методы VAR и MaskGIT для энтропийного моделирования и оценивают свой подход на датасете MSCOCO-30k. PerCoV2 достигает более высокой точности изображений при еще более низких битрейтах, сохраняя при этом конкурентоспособное перцептивное качество.'}, 'en': {'title': 'Ultra-Low Bit-Rate Image Compression Redefined', 'desc': 'PerCoV2 is a new image compression system that uses very few bits to represent images, making it ideal for situations where bandwidth and storage are limited. It improves on earlier methods by better modeling how images are represented in a compressed form, specifically within the Stable Diffusion 3 framework. The system has been tested against other advanced methods and shows better image quality at lower bit rates, while also offering a way to save even more space. Additionally, all components of PerCoV2 are publicly available, allowing others to use and build upon this technology.'}, 'zh': {'title': 'PerCoV2：超低比特率图像压缩的新突破', 'desc': 'PerCoV2是一种新型的超低比特率感知图像压缩系统，专为带宽和存储受限的应用而设计。该系统在Careil等人的先前工作基础上进行了扩展，增强了熵编码效率，通过明确建模离散超潜图像分布来实现。我们对最近的自回归方法（VAR和MaskGIT）进行了全面比较，并在大规模的MSCOCO-30k基准上评估了我们的方法。与之前的工作相比，PerCoV2在更低比特率下实现了更高的图像保真度，同时保持了竞争性的感知质量。'}}}, {'id': 'https://huggingface.co/papers/2503.10635', 'title': 'A Frustratingly Simple Yet Highly Effective Attack Baseline: Over 90%\n  Success Rate Against the Strong Black-box Models of GPT-4.5/4o/o1', 'url': 'https://huggingface.co/papers/2503.10635', 'abstract': 'Despite promising performance on open-source large vision-language models (LVLMs), transfer-based targeted attacks often fail against black-box commercial LVLMs. Analyzing failed adversarial perturbations reveals that the learned perturbations typically originate from a uniform distribution and lack clear semantic details, resulting in unintended responses. This critical absence of semantic information leads commercial LVLMs to either ignore the perturbation entirely or misinterpret its embedded semantics, thereby causing the attack to fail. To overcome these issues, we notice that identifying core semantic objects is a key objective for models trained with various datasets and methodologies. This insight motivates our approach that refines semantic clarity by encoding explicit semantic details within local regions, thus ensuring interoperability and capturing finer-grained features, and by concentrating modifications on semantically rich areas rather than applying them uniformly. To achieve this, we propose a simple yet highly effective solution: at each optimization step, the adversarial image is cropped randomly by a controlled aspect ratio and scale, resized, and then aligned with the target image in the embedding space. Experimental results confirm our hypothesis. Our adversarial examples crafted with local-aggregated perturbations focused on crucial regions exhibit surprisingly good transferability to commercial LVLMs, including GPT-4.5, GPT-4o, Gemini-2.0-flash, Claude-3.5-sonnet, Claude-3.7-sonnet, and even reasoning models like o1, Claude-3.7-thinking and Gemini-2.0-flash-thinking. Our approach achieves success rates exceeding 90% on GPT-4.5, 4o, and o1, significantly outperforming all prior state-of-the-art attack methods. Our optimized adversarial examples under different configurations and training code are available at https://github.com/VILA-Lab/M-Attack.', 'score': 1, 'issue_id': 2718, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 марта', 'en': 'March 13', 'zh': '3月13日'}, 'hash': 'aba878bb95fd4fb4', 'authors': ['Zhaoyi Li', 'Xiaohan Zhao', 'Dong-Dong Wu', 'Jiacheng Cui', 'Zhiqiang Shen'], 'affiliations': ['VILA Lab, MBZUAI'], 'pdf_title_img': 'assets/pdf/title_img/2503.10635.jpg', 'data': {'categories': ['#training', '#benchmark', '#security', '#optimization', '#dataset', '#cv'], 'emoji': '🎯', 'ru': {'title': 'Точечная атака: семантическое оружие против черных ящиков ИИ', 'desc': 'Статья представляет новый метод атаки на коммерческие мультимодальные модели, такие как GPT-4 и Gemini. Авторы обнаружили, что существующие методы часто терпят неудачу из-за отсутствия семантической информации в создаваемых возмущениях. Предложенный подход фокусируется на семантически важных областях изображения и использует локальное агрегирование возмущений. Экспериментальные результаты показывают высокую эффективность метода, достигая успешности атаки более 90% на ряде коммерческих моделей.'}, 'en': {'title': 'Enhancing Adversarial Attacks with Semantic Precision', 'desc': 'This paper addresses the challenges of transferring targeted adversarial attacks to black-box commercial large vision-language models (LVLMs). It identifies that previous attacks often fail due to the lack of semantic detail in the perturbations, which leads to ineffective responses from the models. The authors propose a novel method that enhances semantic clarity by focusing on important regions of the input, allowing for more effective adversarial modifications. Their experimental results demonstrate that this approach significantly improves the success rate of attacks on various commercial LVLMs, achieving over 90% success on several models.'}, 'zh': {'title': '聚焦语义细节，提升对抗攻击成功率', 'desc': '尽管开源的大型视觉语言模型（LVLMs）表现良好，但针对黑箱商业LVLMs的转移攻击往往失败。分析失败的对抗扰动发现，学习到的扰动通常来自均匀分布，缺乏明确的语义细节，导致意外的响应。为了克服这些问题，我们提出了一种方法，通过在局部区域内编码明确的语义细节，确保模型能够捕捉到更细粒度的特征，并集中修改在语义丰富的区域。实验结果表明，我们的方法在多个商业LVLMs上取得了超过90%的成功率，显著优于以往的攻击方法。'}}}, {'id': 'https://huggingface.co/papers/2503.07111', 'title': 'PoseLess: Depth-Free Vision-to-Joint Control via Direct Image Mapping\n  with VLM', 'url': 'https://huggingface.co/papers/2503.07111', 'abstract': 'This paper introduces PoseLess, a novel framework for robot hand control that eliminates the need for explicit pose estimation by directly mapping 2D images to joint angles using projected representations. Our approach leverages synthetic training data generated through randomized joint configurations, enabling zero-shot generalization to real-world scenarios and cross-morphology transfer from robotic to human hands. By projecting visual inputs and employing a transformer-based decoder, PoseLess achieves robust, low-latency control while addressing challenges such as depth ambiguity and data scarcity. Experimental results demonstrate competitive performance in joint angle prediction accuracy without relying on any human-labelled dataset.', 'score': 1, 'issue_id': 2721, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': 'edc49497be1a7cb7', 'authors': ['Alan Dao', 'Dinh Bach Vu', 'Tuan Le Duc Anh', 'Bui Quang Huy'], 'affiliations': ['Menlo Research'], 'pdf_title_img': 'assets/pdf/title_img/2503.07111.jpg', 'data': {'categories': ['#dataset', '#robotics', '#transfer_learning', '#training', '#synthetic'], 'emoji': '🤖', 'ru': {'title': 'Управление роботизированной рукой без явной оценки позы', 'desc': 'PoseLess - это новая система управления роботизированной рукой, которая устраняет необходимость в явной оценке позы, напрямую сопоставляя 2D-изображения с углами суставов с помощью проецируемых представлений. Подход использует синтетические обучающие данные, сгенерированные путем рандомизации конфигураций суставов, что позволяет осуществлять обобщение с нулевого выстрела на реальные сценарии и межморфологический перенос с роботизированных на человеческие руки. PoseLess применяет проецирование визуальных входных данных и декодер на основе трансформера для достижения надежного управления с низкой задержкой. Экспериментальные результаты демонстрируют конкурентоспособную точность прогнозирования углов суставов без использования каких-либо наборов данных с человеческой разметкой.'}, 'en': {'title': 'Direct Control of Robot Hands Without Pose Estimation', 'desc': 'PoseLess is a new framework designed for controlling robot hands without needing to estimate their positions explicitly. It uses 2D images and directly maps them to joint angles, which simplifies the control process. The framework is trained on synthetic data created from various joint configurations, allowing it to adapt to real-world situations and transfer knowledge between different hand types. By utilizing a transformer-based decoder, PoseLess effectively manages challenges like depth ambiguity and limited data, achieving high accuracy in predicting joint angles without any human-labeled data.'}, 'zh': {'title': 'PoseLess：无须姿态估计的机器人手控制新框架', 'desc': '本文介绍了一种名为PoseLess的新框架，用于机器人手部控制，省去了显式姿态估计的需求，直接将2D图像映射到关节角度。我们的方法利用通过随机关节配置生成的合成训练数据，实现了对真实场景的零样本泛化和从机器人手到人类手的跨形态迁移。通过对视觉输入进行投影并采用基于变换器的解码器，PoseLess实现了稳健、低延迟的控制，同时解决了深度模糊和数据稀缺等挑战。实验结果表明，在关节角度预测准确性方面，PoseLess的表现具有竞争力，且不依赖于任何人工标注的数据集。'}}}, {'id': 'https://huggingface.co/papers/2503.02682', 'title': 'MPO: Boosting LLM Agents with Meta Plan Optimization', 'url': 'https://huggingface.co/papers/2503.02682', 'abstract': "Recent advancements in large language models (LLMs) have enabled LLM-based agents to successfully tackle interactive planning tasks. However, despite their successes, existing approaches often suffer from planning hallucinations and require retraining for each new agent. To address these challenges, we propose the Meta Plan Optimization (MPO) framework, which enhances agent planning capabilities by directly incorporating explicit guidance. Unlike previous methods that rely on complex knowledge, which either require significant human effort or lack quality assurance, MPO leverages high-level general guidance through meta plans to assist agent planning and enables continuous optimization of the meta plans based on feedback from the agent's task execution. Our experiments conducted on two representative tasks demonstrate that MPO significantly outperforms existing baselines. Moreover, our analysis indicates that MPO provides a plug-and-play solution that enhances both task completion efficiency and generalization capabilities in previous unseen scenarios.", 'score': 15, 'issue_id': 2535, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 марта', 'en': 'March 4', 'zh': '3月4日'}, 'hash': 'e5fa3c849c1eee72', 'authors': ['Weimin Xiong', 'Yifan Song', 'Qingxiu Dong', 'Bingchan Zhao', 'Feifan Song', 'Xun Wang', 'Sujian Li'], 'affiliations': ['National Key Laboratory for Multimedia Information Processing, School of Computer Science, Peking University', 'Peking University', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2503.02682.jpg', 'data': {'categories': ['#rl', '#training', '#optimization', '#hallucinations', '#agents'], 'emoji': '🧠', 'ru': {'title': 'Метапланы для умных агентов: эффективнее, универсальнее, без галлюцинаций', 'desc': 'Статья представляет новый подход к улучшению планирования задач агентами на основе больших языковых моделей (LLM). Метод под названием Meta Plan Optimization (MPO) использует высокоуровневые метапланы для руководства агентом и оптимизирует их на основе обратной связи. MPO превосходит существующие методы в эффективности выполнения задач и способности к обобщению. Этот подход не требует переобучения для каждого нового агента и решает проблему галлюцинаций при планировании.'}, 'en': {'title': 'Enhancing Agent Planning with Meta Plans', 'desc': "This paper introduces the Meta Plan Optimization (MPO) framework to improve the planning abilities of large language model (LLM)-based agents. MPO addresses issues like planning hallucinations and the need for retraining by using high-level meta plans for guidance. This approach allows for continuous optimization based on feedback from the agent's performance in tasks. Experimental results show that MPO not only outperforms existing methods but also enhances efficiency and adaptability in new situations."}, 'zh': {'title': '元规划优化：提升智能体规划能力的创新框架', 'desc': '最近，大型语言模型（LLMs）的进步使得基于LLM的智能体能够成功处理交互式规划任务。然而，现有方法常常面临规划幻觉的问题，并且每个新智能体都需要重新训练。为了解决这些挑战，我们提出了元规划优化（MPO）框架，通过直接引入明确的指导来增强智能体的规划能力。我们的实验表明，MPO在任务完成效率和在未见场景中的泛化能力上显著优于现有基线。'}}}, {'id': 'https://huggingface.co/papers/2503.02846', 'title': 'Mask-DPO: Generalizable Fine-grained Factuality Alignment of LLMs', 'url': 'https://huggingface.co/papers/2503.02846', 'abstract': 'Large language models (LLMs) exhibit hallucinations (i.e., unfaithful or nonsensical information) when serving as AI assistants in various domains. Since hallucinations always come with truthful content in the LLM responses, previous factuality alignment methods that conduct response-level preference learning inevitably introduced noises during training. Therefore, this paper proposes a fine-grained factuality alignment method based on Direct Preference Optimization (DPO), called Mask-DPO. Incorporating sentence-level factuality as mask signals, Mask-DPO only learns from factually correct sentences in the preferred samples and prevents the penalty on factual contents in the not preferred samples, which resolves the ambiguity in the preference learning. Extensive experimental results demonstrate that Mask-DPO can significantly improve the factuality of LLMs responses to questions from both in-domain and out-of-domain datasets, although these questions and their corresponding topics are unseen during training. Only trained on the ANAH train set, the score of Llama3.1-8B-Instruct on the ANAH test set is improved from 49.19% to 77.53%, even surpassing the score of Llama3.1-70B-Instruct (53.44%), while its FactScore on the out-of-domain Biography dataset is also improved from 30.29% to 39.39%. We further study the generalization property of Mask-DPO using different training sample scaling strategies and find that scaling the number of topics in the dataset is more effective than the number of questions. We provide a hypothesis of what factual alignment is doing with LLMs, on the implication of this phenomenon, and conduct proof-of-concept experiments to verify it. We hope the method and the findings pave the way for future research on scaling factuality alignment.', 'score': 14, 'issue_id': 2535, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 марта', 'en': 'March 4', 'zh': '3月4日'}, 'hash': '7e9277cf8ca5bb98', 'authors': ['Yuzhe Gu', 'Wenwei Zhang', 'Chengqi Lyu', 'Dahua Lin', 'Kai Chen'], 'affiliations': ['MMLab, The Chinese University of Hong Kong', 'Shanghai AI Laboratory', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2503.02846.jpg', 'data': {'categories': ['#hallucinations', '#training', '#rlhf', '#alignment'], 'emoji': '🎭', 'ru': {'title': 'Mask-DPO: точная настройка фактов в ответах языковых моделей', 'desc': 'Эта статья представляет метод Mask-DPO для улучшения фактической точности больших языковых моделей (LLM). Метод основан на оптимизации прямых предпочтений (DPO) и использует маскирование на уровне предложений для обучения только на фактически верных частях ответов. Экспериментальные результаты показывают значительное улучшение фактической точности как на знакомых, так и на новых данных. Исследование также выявило, что масштабирование количества тем в обучающем наборе более эффективно, чем увеличение числа вопросов.'}, 'en': {'title': 'Enhancing Factuality in LLMs with Mask-DPO', 'desc': 'This paper addresses the issue of hallucinations in large language models (LLMs), where they generate incorrect or nonsensical information despite including some accurate content. The authors introduce a new method called Mask-DPO, which focuses on fine-grained factuality alignment by using sentence-level factuality as mask signals. This approach allows the model to learn only from factually correct sentences in preferred samples, reducing noise during training and improving the overall factual accuracy of LLM responses. Experimental results show that Mask-DPO significantly enhances the factuality of LLMs, even on unseen questions and topics, outperforming larger models in certain tests.'}, 'zh': {'title': '提升大型语言模型的事实性对齐', 'desc': '大型语言模型（LLMs）在作为AI助手时常常会出现幻觉现象，即提供不真实或无意义的信息。以往的事实对齐方法在训练过程中引入了噪声，因为它们在响应级别进行偏好学习。本文提出了一种基于直接偏好优化（DPO）的细粒度事实对齐方法，称为Mask-DPO，该方法通过句子级别的事实作为掩码信号，仅从事实正确的句子中学习，从而提高了LLMs的响应准确性。实验结果表明，Mask-DPO显著提升了LLMs在未见问题上的事实性，尤其是在不同领域的数据集上表现优异。'}}}, {'id': 'https://huggingface.co/papers/2503.02879', 'title': 'Wikipedia in the Era of LLMs: Evolution and Risks', 'url': 'https://huggingface.co/papers/2503.02879', 'abstract': "In this paper, we present a thorough analysis of the impact of Large Language Models (LLMs) on Wikipedia, examining the evolution of Wikipedia through existing data and using simulations to explore potential risks. We begin by analyzing page views and article content to study Wikipedia's recent changes and assess the impact of LLMs. Subsequently, we evaluate how LLMs affect various Natural Language Processing (NLP) tasks related to Wikipedia, including machine translation and retrieval-augmented generation (RAG). Our findings and simulation results reveal that Wikipedia articles have been influenced by LLMs, with an impact of approximately 1%-2% in certain categories. If the machine translation benchmark based on Wikipedia is influenced by LLMs, the scores of the models may become inflated, and the comparative results among models might shift as well. Moreover, the effectiveness of RAG might decrease if the knowledge base becomes polluted by LLM-generated content. While LLMs have not yet fully changed Wikipedia's language and knowledge structures, we believe that our empirical findings signal the need for careful consideration of potential future risks.", 'score': 13, 'issue_id': 2535, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 марта', 'en': 'March 4', 'zh': '3月4日'}, 'hash': 'd8fedc8bf5ffc308', 'authors': ['Siming Huang', 'Yuliang Xu', 'Mingmeng Geng', 'Yao Wan', 'Dongping Chen'], 'affiliations': ['Huazhong University of Science and Technology', 'International School for Advanced Studies (SISSA)'], 'pdf_title_img': 'assets/pdf/title_img/2503.02879.jpg', 'data': {'categories': ['#rag', '#benchmark', '#machine_translation', '#dataset', '#multimodal', '#science', '#data'], 'emoji': '🧠', 'ru': {'title': 'Большие языковые модели меняют лицо Википедии: анализ влияния и потенциальных рисков', 'desc': 'Эта статья представляет анализ влияния больших языковых моделей (LLM) на Википедию. Исследователи изучили изменения в просмотрах страниц и содержании статей, а также провели симуляции для оценки потенциальных рисков. Результаты показывают, что LLM повлияли на 1-2% статей в некоторых категориях, что может привести к искажению результатов в задачах машинного перевода и генерации с использованием внешних знаний (RAG). Авторы призывают к осторожности в отношении будущих рисков, связанных с влиянием LLM на структуру знаний в Википедии.'}, 'en': {'title': "Navigating the Impact of LLMs on Wikipedia's Evolution", 'desc': 'This paper analyzes how Large Language Models (LLMs) are affecting Wikipedia by examining changes in page views and article content. It assesses the influence of LLMs on various Natural Language Processing (NLP) tasks, such as machine translation and retrieval-augmented generation (RAG). The study finds that LLMs have caused a 1%-2% impact on certain Wikipedia categories, which could inflate machine translation benchmarks and alter model comparisons. The authors emphasize the importance of monitoring these changes to mitigate potential risks associated with LLM-generated content.'}, 'zh': {'title': '大型语言模型对维基百科的影响分析', 'desc': '本文深入分析了大型语言模型（LLMs）对维基百科的影响，研究了维基百科的演变。我们通过分析页面浏览量和文章内容，评估LLMs对维基百科的影响，发现某些类别的影响约为1%-2%。此外，我们还评估了LLMs对与维基百科相关的自然语言处理（NLP）任务的影响，包括机器翻译和检索增强生成（RAG）。我们的研究结果表明，LLMs可能会导致机器翻译基准的分数膨胀，并可能影响模型之间的比较结果，因此需要对未来的潜在风险进行仔细考虑。'}}}, {'id': 'https://huggingface.co/papers/2503.01935', 'title': 'MultiAgentBench: Evaluating the Collaboration and Competition of LLM agents', 'url': 'https://huggingface.co/papers/2503.01935', 'abstract': 'Large Language Models (LLMs) have shown remarkable capabilities as autonomous agents, yet existing benchmarks either focus on single-agent tasks or are confined to narrow domains, failing to capture the dynamics of multi-agent coordination and competition. In this paper, we introduce MultiAgentBench, a comprehensive benchmark designed to evaluate LLM-based multi-agent systems across diverse, interactive scenarios. Our framework measures not only task completion but also the quality of collaboration and competition using novel, milestone-based key performance indicators. Moreover, we evaluate various coordination protocols (including star, chain, tree, and graph topologies) and innovative strategies such as group discussion and cognitive planning. Notably, gpt-4o-mini reaches the average highest task score, graph structure performs the best among coordination protocols in the research scenario, and cognitive planning improves milestone achievement rates by 3%. Code and datasets are public available at https://github.com/MultiagentBench/MARBLE.', 'score': 10, 'issue_id': 2532, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': '4353f27d396c322a', 'authors': ['Kunlun Zhu', 'Hongyi Du', 'Zhaochen Hong', 'Xiaocheng Yang', 'Shuyi Guo', 'Zhe Wang', 'Zhenhailong Wang', 'Cheng Qian', 'Xiangru Tang', 'Heng Ji', 'Jiaxuan You'], 'affiliations': ['University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2503.01935.jpg', 'data': {'categories': ['#games', '#optimization', '#open_source', '#benchmark', '#agents'], 'emoji': '🤖', 'ru': {'title': 'MultiAgentBench: новый стандарт для оценки мультиагентных LLM-систем', 'desc': 'Статья представляет MultiAgentBench - комплексный бенчмарк для оценки мультиагентных систем на основе больших языковых моделей (LLM) в различных интерактивных сценариях. Фреймворк измеряет не только выполнение задач, но и качество сотрудничества и конкуренции с помощью новых ключевых показателей эффективности. Авторы оценивают различные протоколы координации и инновационные стратегии, такие как групповое обсуждение и когнитивное планирование. Результаты показывают, что gpt-4o-mini достигает наивысшего среднего балла за выполнение задач, а графовая структура лучше всего работает среди протоколов координации в исследовательском сценарии.'}, 'en': {'title': 'Evaluating LLMs in Multi-Agent Dynamics', 'desc': 'This paper presents MultiAgentBench, a new benchmark for assessing the performance of Large Language Models (LLMs) in multi-agent environments. Unlike previous benchmarks that focus on single-agent tasks, MultiAgentBench evaluates how well LLMs can collaborate and compete in various interactive scenarios. The framework introduces key performance indicators that measure both task completion and the quality of interactions among agents. The study also explores different coordination protocols and innovative strategies, revealing that certain configurations, like graph structures and cognitive planning, significantly enhance performance.'}, 'zh': {'title': '多智能体系统的全面评估基准', 'desc': '本文介绍了MultiAgentBench，这是一个全面的基准测试，旨在评估基于大型语言模型（LLM）的多智能体系统在多样化互动场景中的表现。该框架不仅测量任务完成情况，还评估协作和竞争的质量，使用新颖的里程碑式关键绩效指标。我们还评估了多种协调协议（如星形、链形、树形和图形拓扑）以及创新策略，如小组讨论和认知规划。研究表明，gpt-4o-mini在任务得分上表现最佳，图形结构在协调协议中表现最佳，而认知规划提高了里程碑达成率3%。'}}}, {'id': 'https://huggingface.co/papers/2503.01328', 'title': 'PipeOffload: Improving Scalability of Pipeline Parallelism with Memory Optimization', 'url': 'https://huggingface.co/papers/2503.01328', 'abstract': 'Pipeline parallelism (PP) is widely used for training large language models (LLMs), yet its scalability is often constrained by high activation memory consumption as the number of in-flight microbatches grows with the degree of PP. In this paper, we focus on addressing this challenge by leveraging the under-explored memory offload strategy in PP. With empirical study, we discover that in the majority of standard configurations, at least half, and potentially all, of the activations can be offloaded with negligible overhead. In the cases where full overload is not possible, we introduce a novel selective offload strategy that decreases peak activation memory in a better-than-linear manner. Furthermore, we integrate memory offload with other techniques to jointly consider overall throughput and memory limitation. Our experiments proves that the per-device activation memory effectively reduces with the total number of stages, making PP a stronger alternative than TP, offering up to a 19\\% acceleration with even lower memory consumption. The implementation is open-sourced at https://github.com/sail-sg/zero-bubble-pipeline-parallelism{this url}.', 'score': 10, 'issue_id': 2532, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': '75e25b312e4cef8a', 'authors': ['Xinyi Wan', 'Penghui Qi', 'Guangxing Huang', 'Jialin Li', 'Min Lin'], 'affiliations': ['National University of', 'Sea AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2503.01328.jpg', 'data': {'categories': ['#open_source', '#inference', '#optimization', '#training'], 'emoji': '🚀', 'ru': {'title': 'Оптимизация памяти для масштабирования больших языковых моделей', 'desc': 'Статья посвящена улучшению масштабируемости конвейерного параллелизма при обучении больших языковых моделей. Авторы предлагают стратегию выгрузки активаций из памяти, которая позволяет значительно снизить пиковое потребление памяти. Они также представляют метод выборочной выгрузки для случаев, когда полная выгрузка невозможна. Эксперименты показывают, что данный подход делает конвейерный параллелизм более эффективной альтернативой тензорному параллелизму, ускоряя обучение на 19% при меньшем потреблении памяти.'}, 'en': {'title': 'Optimizing Memory Usage in Pipeline Parallelism for Faster Training', 'desc': 'This paper addresses the challenge of high activation memory consumption in pipeline parallelism (PP) when training large language models (LLMs). The authors explore a memory offload strategy that allows for significant reductions in memory usage, showing that up to half of the activations can be offloaded with minimal impact on performance. They also propose a selective offload strategy that further decreases peak activation memory in a more efficient way. The results demonstrate that using memory offload in conjunction with other techniques can enhance throughput while reducing memory requirements, making PP a more viable option compared to tensor parallelism (TP).'}, 'zh': {'title': '优化管道并行性，降低内存消耗！', 'desc': '本文探讨了在训练大型语言模型时，管道并行性（PP）面临的高激活内存消耗问题。我们提出了一种内存卸载策略，可以在大多数标准配置中卸载至少一半的激活，几乎没有额外开销。对于无法完全卸载的情况，我们引入了一种新的选择性卸载策略，显著降低了峰值激活内存。实验结果表明，随着阶段数量的增加，每个设备的激活内存有效减少，使得PP成为比张量并行性（TP）更强的选择，提供高达19%的加速，同时降低内存消耗。'}}}, {'id': 'https://huggingface.co/papers/2503.00735', 'title': 'LADDER: Self-Improving LLMs Through Recursive Problem Decomposition', 'url': 'https://huggingface.co/papers/2503.00735', 'abstract': "We introduce LADDER (Learning through Autonomous Difficulty-Driven Example Recursion), a framework which enables Large Language Models to autonomously improve their problem-solving capabilities through self-guided learning by recursively generating and solving progressively simpler variants of complex problems. Unlike prior approaches that require curated datasets or human feedback, LADDER leverages a model's own capabilities to generate easier question variants. We demonstrate LADDER's effectiveness in the subject of mathematical integration, improving Llama 3.2 3B's accuracy from 1% to 82% on undergraduate-level problems and enabling Qwen2.5 7B Deepseek-R1 Distilled to achieve 73% on the MIT Integration Bee qualifying examination. We also introduce TTRL (Test-Time Reinforcement Learning), where we perform reinforcement learning on variants of test problems at inference time. TTRL enables Qwen2.5 7B Deepseek-R1 Distilled to achieve a state-of-the-art score of 90% on the MIT Integration Bee qualifying examination, surpassing OpenAI o1's performance. These results show how self-directed strategic learning can achieve significant capability improvements without relying on architectural scaling or human supervision.", 'score': 9, 'issue_id': 2540, 'pub_date': '2025-03-02', 'pub_date_card': {'ru': '2 марта', 'en': 'March 2', 'zh': '3月2日'}, 'hash': 'c9ce66a728a6df87', 'authors': ['Toby Simonds', 'Akira Yoshiyama'], 'affiliations': ['Tufa Labs'], 'pdf_title_img': 'assets/pdf/title_img/2503.00735.jpg', 'data': {'categories': ['#math', '#reasoning', '#optimization', '#training', '#rl'], 'emoji': '🧮', 'ru': {'title': 'Самообучение языковых моделей через генерацию упрощенных задач', 'desc': 'Представлен метод LADDER, позволяющий языковым моделям самостоятельно улучшать навыки решения задач путем генерации и решения упрощенных вариантов сложных проблем. В отличие от предыдущих подходов, LADDER не требует подготовленных датасетов или обратной связи от людей. Эффективность метода продемонстрирована на задачах математического интегрирования, где точность модели Llama 3.2 3B выросла с 1% до 82% на задачах университетского уровня. Также представлен метод TTRL, использующий обучение с подкреплением во время вывода, что позволило модели Qwen2.5 7B достичь рекордных 90% на отборочном экзамене MIT Integration Bee.'}, 'en': {'title': 'Empowering Models to Learn and Solve Problems Autonomously', 'desc': 'LADDER is a new framework that helps Large Language Models (LLMs) improve their problem-solving skills by creating and solving simpler versions of complex problems on their own. This method does not need pre-made datasets or human input, as it allows the model to generate easier questions based on its own understanding. The framework has shown remarkable success in mathematical integration, significantly boosting the accuracy of models like Llama 3.2 and Qwen2.5 on challenging exams. Additionally, the introduction of Test-Time Reinforcement Learning (TTRL) further enhances performance by applying reinforcement learning during the testing phase, leading to state-of-the-art results.'}, 'zh': {'title': '自主学习，提升能力的全新框架', 'desc': '本文介绍了LADDER（通过自主难度驱动的示例递归学习）框架，该框架使大型语言模型能够通过自我引导学习，逐步生成和解决复杂问题的简化变体，从而提高其解决问题的能力。与以往需要人工反馈或精心策划数据集的方法不同，LADDER利用模型自身的能力生成更简单的问题变体。我们展示了LADDER在数学积分领域的有效性，使Llama 3.2 3B在本科水平问题上的准确率从1%提高到82%，并使Qwen2.5 7B Deepseek-R1 Distilled在MIT积分竞赛资格考试中达到73%。此外，我们还引入了TTRL（测试时强化学习），在推理时对测试问题的变体进行强化学习，使Qwen2.5 7B Deepseek-R1 Distilled在MIT积分竞赛资格考试中获得90%的领先成绩，超越了OpenAI o1的表现。'}}}, {'id': 'https://huggingface.co/papers/2503.02368', 'title': 'Iterative Value Function Optimization for Guided Decoding', 'url': 'https://huggingface.co/papers/2503.02368', 'abstract': 'While Reinforcement Learning from Human Feedback (RLHF) has become the predominant method for controlling language model outputs, it suffers from high computational costs and training instability. Guided decoding, especially value-guided methods, offers a cost-effective alternative by controlling outputs without re-training models. However, the accuracy of the value function is crucial for value-guided decoding, as inaccuracies can lead to suboptimal decision-making and degraded performance. Existing methods struggle with accurately estimating the optimal value function, leading to less effective control. We propose Iterative Value Function Optimization, a novel framework that addresses these limitations through two key components: Monte Carlo Value Estimation, which reduces estimation variance by exploring diverse trajectories, and Iterative On-Policy Optimization, which progressively improves value estimation through collecting trajectories from value-guided policies. Extensive experiments on text summarization, multi-turn dialogue, and instruction following demonstrate the effectiveness of value-guided decoding approaches in aligning language models. These approaches not only achieve alignment but also significantly reduce computational costs by leveraging principled value function optimization for efficient and effective control.', 'score': 9, 'issue_id': 2538, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 марта', 'en': 'March 4', 'zh': '3月4日'}, 'hash': '6a7f0679f238bd4d', 'authors': ['Zhenhua Liu', 'Lijun Li', 'Ruizhe Chen', 'Yuxian Jiang', 'Tong Zhu', 'Wenliang Chen', 'Jing Shao'], 'affiliations': ['Fudan University', 'Shanghai Artificial Intelligence Laboratory', 'Soochow University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.02368.jpg', 'data': {'categories': ['#optimization', '#alignment', '#training', '#rlhf'], 'emoji': '🧠', 'ru': {'title': 'Эффективное управление языковыми моделями без переобучения', 'desc': "Статья представляет новый метод управления выходными данными языковых моделей, называемый 'Итеративная оптимизация функции ценности'. Этот подход предлагает альтернативу традиционному обучению с подкреплением на основе обратной связи от человека (RLHF), снижая вычислительные затраты и повышая стабильность обучения. Метод использует управляемое декодирование на основе функции ценности, улучшая её точность с помощью оценки Монте-Карло и итеративной оптимизации. Эксперименты показали эффективность предложенного подхода в задачах суммаризации текста, многоэтапного диалога и выполнения инструкций."}, 'en': {'title': 'Optimizing Value Functions for Efficient Language Model Control', 'desc': 'This paper addresses the challenges of Reinforcement Learning from Human Feedback (RLHF) in controlling language models, particularly its high computational costs and instability. It introduces a new framework called Iterative Value Function Optimization, which enhances value-guided decoding methods by improving the accuracy of the value function. The framework employs Monte Carlo Value Estimation to minimize estimation variance and Iterative On-Policy Optimization to refine value estimation through trajectory collection. Experimental results show that this approach not only aligns language models effectively but also reduces computational expenses significantly.'}, 'zh': {'title': '高效控制语言模型的新方法', 'desc': '本论文探讨了人类反馈强化学习（RLHF）在控制语言模型输出时的高计算成本和训练不稳定性问题。我们提出了一种新的框架——迭代值函数优化，通过蒙特卡洛值估计和迭代在线优化来提高值函数的准确性。该方法通过探索多样化的轨迹来减少估计方差，并逐步改进值估计。实验结果表明，基于值引导的解码方法在文本摘要、多轮对话和指令跟随任务中表现出色，显著降低了计算成本。'}}}, {'id': 'https://huggingface.co/papers/2503.00955', 'title': 'SemViQA: A Semantic Question Answering System for Vietnamese Information Fact-Checking', 'url': 'https://huggingface.co/papers/2503.00955', 'abstract': 'The rise of misinformation, exacerbated by Large Language Models (LLMs) like GPT and Gemini, demands robust fact-checking solutions, especially for low-resource languages like Vietnamese. Existing methods struggle with semantic ambiguity, homonyms, and complex linguistic structures, often trading accuracy for efficiency. We introduce SemViQA, a novel Vietnamese fact-checking framework integrating Semantic-based Evidence Retrieval (SER) and Two-step Verdict Classification (TVC). Our approach balances precision and speed, achieving state-of-the-art results with 78.97\\% strict accuracy on ISE-DSC01 and 80.82\\% on ViWikiFC, securing 1st place in the UIT Data Science Challenge. Additionally, SemViQA Faster improves inference speed 7x while maintaining competitive accuracy. SemViQA sets a new benchmark for Vietnamese fact verification, advancing the fight against misinformation. The source code is available at: https://github.com/DAVID-NGUYEN-S16/SemViQA.', 'score': 7, 'issue_id': 2535, 'pub_date': '2025-03-02', 'pub_date_card': {'ru': '2 марта', 'en': 'March 2', 'zh': '3月2日'}, 'hash': '651aacbd378465bd', 'authors': ['Nam V. Nguyen', 'Dien X. Tran', 'Thanh T. Tran', 'Anh T. Hoang', 'Tai V. Duong', 'Di T. Le', 'Phuc-Lu Le'], 'affiliations': ['FPT Software AI Center, Viet Nam', 'FPT Telecom, Viet Nam', 'Faculty of Information Technology, Industrial University of Ho Chi Minh City, Viet Nam', 'Faculty of Information Technology, University of Science, VNU-HCM, Viet Nam'], 'pdf_title_img': 'assets/pdf/title_img/2503.00955.jpg', 'data': {'categories': ['#multilingual', '#inference', '#benchmark', '#low_resource', '#dataset', '#science', '#data'], 'emoji': '🕵️', 'ru': {'title': 'SemViQA: Передовая система проверки фактов для борьбы с дезинформацией на вьетнамском языке', 'desc': 'SemViQA - это новая система проверки фактов на вьетнамском языке, разработанная для борьбы с дезинформацией, усугубляемой крупными языковыми моделями. Она объединяет семантический поиск доказательств и двухэтапную классификацию вердиктов, обеспечивая баланс между точностью и скоростью. SemViQA достигла наилучших результатов на датасетах ISE-DSC01 и ViWikiFC, установив новый стандарт для проверки фактов на вьетнамском языке. Быстрая версия SemViQA Faster увеличивает скорость вывода в 7 раз при сохранении конкурентоспособной точности.'}, 'en': {'title': 'SemViQA: Revolutionizing Vietnamese Fact-Checking Against Misinformation', 'desc': 'This paper addresses the challenge of misinformation in low-resource languages, specifically Vietnamese, by introducing SemViQA, a new fact-checking framework. SemViQA combines Semantic-based Evidence Retrieval (SER) and Two-step Verdict Classification (TVC) to enhance both accuracy and efficiency in verifying facts. The framework achieves impressive results, with a strict accuracy of 78.97% on the ISE-DSC01 dataset and 80.82% on ViWikiFC, outperforming existing methods. Additionally, SemViQA Faster significantly boosts inference speed by 7 times while maintaining competitive accuracy, setting a new standard for fact verification in Vietnamese.'}, 'zh': {'title': 'SemViQA：越南语事实核查的新标杆', 'desc': '随着大型语言模型（LLMs）如GPT和Gemini的兴起，虚假信息问题日益严重，尤其是在资源匮乏的语言如越南语中，迫切需要强有力的事实核查解决方案。现有的方法在语义模糊、同义词和复杂语言结构方面存在困难，往往在准确性和效率之间做出妥协。我们提出了SemViQA，这是一种新颖的越南语事实核查框架，结合了基于语义的证据检索（SER）和两步裁决分类（TVC）。我们的方案在保持竞争性准确度的同时，实现了精度与速度的平衡，在ISE-DSC01上达到了78.97%的严格准确率，在ViWikiFC上达到了80.82%，并在UIT数据科学挑战赛中获得第一名。'}}}, {'id': 'https://huggingface.co/papers/2502.14856', 'title': 'FR-Spec: Accelerating Large-Vocabulary Language Models via Frequency-Ranked Speculative Sampling', 'url': 'https://huggingface.co/papers/2502.14856', 'abstract': 'Speculative sampling has emerged as an important technique for accelerating the auto-regressive generation process of large language models (LLMs) by utilizing a draft-then-verify mechanism to produce multiple tokens per forward pass. While state-of-the-art speculative sampling methods use only a single layer and a language modeling (LM) head as the draft model to achieve impressive layer compression, their efficiency gains are substantially reduced for large-vocabulary LLMs, such as Llama-3-8B with a vocabulary of 128k tokens. To address this, we present FR-Spec, a frequency-ranked speculative sampling framework that optimizes draft candidate selection through vocabulary space compression. By constraining the draft search to a frequency-prioritized token subset, our method reduces LM Head computation overhead by 75% while ensuring the equivalence of the final output distribution. Experiments across multiple datasets demonstrate an average of 1.12times speedup over the state-of-the-art speculative sampling method EAGLE-2.', 'score': 6, 'issue_id': 2535, 'pub_date': '2025-02-20', 'pub_date_card': {'ru': '20 февраля', 'en': 'February 20', 'zh': '2月20日'}, 'hash': '45e128ccea542dad', 'authors': ['Weilin Zhao', 'Tengyu Pan', 'Xu Han', 'Yudi Zhang', 'Ao Sun', 'Yuxiang Huang', 'Kaihuo Zhang', 'Weilun Zhao', 'Yuxuan Li', 'Jianyong Wang', 'Zhiyuan Liu', 'Maosong Sun'], 'affiliations': ['Beijing University of Posts and Telecommunications, Beijing, China', 'Harbin Institute of Technology, Harbin, China', 'OpenBMB', 'Tsinghua University, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2502.14856.jpg', 'data': {'categories': ['#training', '#inference', '#optimization'], 'emoji': '🚀', 'ru': {'title': 'Быстрее и эффективнее: оптимизация спекулятивной выборки для LLM', 'desc': 'Статья представляет FR-Spec - новый метод спекулятивной выборки для ускорения работы больших языковых моделей (LLM). Метод оптимизирует выбор кандидатов путем сжатия пространства словаря, отдавая приоритет наиболее частотным токенам. Это позволяет снизить вычислительные затраты на 75% по сравнению с существующими методами, сохраняя эквивалентность итогового распределения. Эксперименты показывают ускорение в 1.12 раза по сравнению с современным методом EAGLE-2.'}, 'en': {'title': 'Accelerating Token Generation with Frequency-Ranked Speculative Sampling', 'desc': 'This paper introduces FR-Spec, a new framework for speculative sampling in large language models (LLMs) that enhances the efficiency of token generation. By using a draft-then-verify approach, FR-Spec optimizes the selection of draft candidates by focusing on a frequency-ranked subset of tokens, which reduces computational overhead significantly. The method achieves a 75% reduction in LM Head computation while maintaining the same output distribution as traditional methods. Experimental results show that FR-Spec provides an average speedup of 1.12 times compared to the leading speculative sampling technique, EAGLE-2.'}, 'zh': {'title': '频率优先，推测采样加速！', 'desc': '本文提出了一种名为FR-Spec的频率排名推测采样框架，旨在加速大型语言模型的自回归生成过程。该方法通过压缩词汇空间，优化草稿候选选择，从而减少计算开销。FR-Spec将草稿搜索限制在一个优先考虑频率的词汇子集上，使得语言模型头的计算开销降低了75%。实验结果表明，FR-Spec在多个数据集上相较于最先进的推测采样方法EAGLE-2实现了平均1.12倍的加速。'}}}, {'id': 'https://huggingface.co/papers/2503.02537', 'title': 'RectifiedHR: Enable Efficient High-Resolution Image Generation via Energy Rectification', 'url': 'https://huggingface.co/papers/2503.02537', 'abstract': "Diffusion models have achieved remarkable advances in various image generation tasks. However, their performance notably declines when generating images at resolutions higher than those used during the training period. Despite the existence of numerous methods for producing high-resolution images, they either suffer from inefficiency or are hindered by complex operations. In this paper, we propose RectifiedHR, an efficient and straightforward solution for training-free high-resolution image generation. Specifically, we introduce the noise refresh strategy, which theoretically only requires a few lines of code to unlock the model's high-resolution generation ability and improve efficiency. Additionally, we first observe the phenomenon of energy decay that may cause image blurriness during the high-resolution image generation process. To address this issue, we propose an Energy Rectification strategy, where modifying the hyperparameters of the classifier-free guidance effectively improves the generation performance. Our method is entirely training-free and boasts a simple implementation logic. Through extensive comparisons with numerous baseline methods, our RectifiedHR demonstrates superior effectiveness and efficiency.", 'score': 5, 'issue_id': 2540, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 марта', 'en': 'March 4', 'zh': '3月4日'}, 'hash': '764b3070cbcdd839', 'authors': ['Zhen Yang', 'Guibao Shen', 'Liang Hou', 'Mushui Liu', 'Luozhou Wang', 'Xin Tao', 'Pengfei Wan', 'Di Zhang', 'Ying-Cong Chen'], 'affiliations': ['HKUST', 'HKUST(GZ)', 'Kuaishou Technology', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.02537.jpg', 'data': {'categories': ['#cv', '#optimization', '#diffusion', '#training'], 'emoji': '🖼️', 'ru': {'title': 'Эффективная генерация изображений высокого разрешения без дополнительного обучения', 'desc': 'Статья представляет новый метод RectifiedHR для генерации изображений высокого разрешения с помощью диффузионных моделей. Авторы предлагают стратегию обновления шума, которая позволяет улучшить способность модели генерировать изображения высокого разрешения без дополнительного обучения. Они также вводят стратегию энергетической ректификации для решения проблемы размытости изображений. Метод RectifiedHR показывает превосходную эффективность по сравнению с существующими подходами.'}, 'en': {'title': 'Unlocking High-Resolution Image Generation with RectifiedHR', 'desc': 'This paper presents RectifiedHR, a novel approach for generating high-resolution images using diffusion models without the need for additional training. The authors introduce a noise refresh strategy that simplifies the process, allowing for efficient high-resolution image generation with minimal code changes. They also identify and address the issue of energy decay, which can lead to blurry images, by proposing an Energy Rectification strategy that optimizes hyperparameters for better performance. Overall, RectifiedHR stands out for its simplicity and effectiveness compared to existing methods.'}, 'zh': {'title': '高效无训练的高分辨率图像生成', 'desc': '扩散模型在图像生成任务中取得了显著进展，但在生成高于训练分辨率的图像时性能明显下降。尽管已有多种方法可以生成高分辨率图像，但它们往往效率低下或操作复杂。本文提出了一种名为RectifiedHR的高效且简单的无训练高分辨率图像生成解决方案。我们引入了噪声刷新策略和能量修正策略，显著提高了生成性能，且实现逻辑简单。'}}}, {'id': 'https://huggingface.co/papers/2503.02197', 'title': 'ATLaS: Agent Tuning via Learning Critical Steps', 'url': 'https://huggingface.co/papers/2503.02197', 'abstract': "Large Language Model (LLM) agents have demonstrated remarkable generalization capabilities across multi-domain tasks. Existing agent tuning approaches typically employ supervised finetuning on entire expert trajectories. However, behavior-cloning of full trajectories can introduce expert bias and weaken generalization to states not covered by the expert data. Additionally, critical steps, such as planning, complex reasoning for intermediate subtasks, and strategic decision-making, are essential to success in agent tasks, so learning these steps is the key to improving LLM agents. For more effective and efficient agent tuning, we propose ATLaS that identifies the critical steps in expert trajectories and finetunes LLMs solely on these steps with reduced costs. By steering the training's focus to a few critical steps, our method mitigates the risk of overfitting entire trajectories and promotes generalization across different environments and tasks. In extensive experiments, an LLM finetuned on only 30% critical steps selected by ATLaS outperforms the LLM finetuned on all steps and recent open-source LLM agents. ATLaS maintains and improves base LLM skills as generalist agents interacting with diverse environments.", 'score': 5, 'issue_id': 2532, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 марта', 'en': 'March 4', 'zh': '3月4日'}, 'hash': 'a06c17fd97b92f03', 'authors': ['Zhixun Chen', 'Ming Li', 'Yuxuan Huang', 'Yali Du', 'Meng Fang', 'Tianyi Zhou'], 'affiliations': ['Kings College London', 'University of Liverpool', 'University of Maryland', 'University of Technology Sydney'], 'pdf_title_img': 'assets/pdf/title_img/2503.02197.jpg', 'data': {'categories': ['#optimization', '#reasoning', '#training', '#agents', '#agi'], 'emoji': '🎯', 'ru': {'title': 'ATLaS: точечная настройка LLM-агентов для улучшения обобщения', 'desc': 'ATLaS - это новый метод настройки агентов на основе больших языковых моделей (LLM). Он фокусируется на обучении только критически важным шагам в экспертных траекториях, что улучшает обобщение и снижает риск переобучения. Эксперименты показывают, что LLM, настроенная на 30% критических шагов, выбранных ATLaS, превосходит модели, обученные на всех шагах. Этот подход позволяет сохранить и улучшить базовые навыки LLM как универсальных агентов для взаимодействия с различными средами.'}, 'en': {'title': 'Focus on Critical Steps for Better LLM Performance', 'desc': 'This paper introduces ATLaS, a novel approach for tuning Large Language Model (LLM) agents by focusing on critical steps in expert trajectories rather than using full trajectory behavior-cloning. By finetuning LLMs on only 30% of these essential steps, ATLaS reduces the risk of expert bias and enhances generalization to unseen states. The method emphasizes the importance of planning, reasoning, and decision-making in agent tasks, which are crucial for improving performance. Experimental results show that LLMs trained with ATLaS outperform those trained on complete trajectories and other recent models, while also maintaining their generalist capabilities across various tasks.'}, 'zh': {'title': '聚焦关键步骤，提升LLM代理的泛化能力', 'desc': '大型语言模型（LLM）代理在多领域任务中展现了出色的泛化能力。现有的代理调优方法通常在整个专家轨迹上进行监督微调，但这种行为克隆可能引入专家偏见，削弱对未覆盖状态的泛化能力。本文提出的ATLaS方法通过识别专家轨迹中的关键步骤，仅在这些步骤上进行微调，从而降低成本并提高效率。实验表明，基于ATLaS选择的30%关键步骤微调的LLM在性能上优于在所有步骤上微调的LLM和最近的开源LLM代理。'}}}, {'id': 'https://huggingface.co/papers/2503.02878', 'title': 'Language Models can Self-Improve at State-Value Estimation for Better Search', 'url': 'https://huggingface.co/papers/2503.02878', 'abstract': 'Collecting ground truth task completion rewards or human demonstrations for multi-step reasoning tasks is often cost-prohibitive and time-consuming, especially in interactive domains like web tasks. To address this bottleneck, we present self-taught lookahead, a self-supervised method that leverages state-transition dynamics to train a value model capable of effectively guiding language model-controlled search. We find that moderately sized (8 billion parameters) open-weight value models improved with self-taught lookahead can match the performance of using a frontier LLM such as gpt-4o as the value model. Furthermore, we find that self-taught lookahead improves performance by 20% while reducing costs 37x compared to previous LLM-based tree search, without relying on ground truth rewards.', 'score': 5, 'issue_id': 2532, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 марта', 'en': 'March 4', 'zh': '3月4日'}, 'hash': '04d05c7118ce4a93', 'authors': ['Ethan Mendes', 'Alan Ritter'], 'affiliations': ['Georgia Institute of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2503.02878.jpg', 'data': {'categories': ['#reasoning', '#rl', '#optimization', '#training'], 'emoji': '🧠', 'ru': {'title': 'Самообучение для эффективного поиска без дорогостоящих демонстраций', 'desc': "Статья представляет метод 'self-taught lookahead' для обучения модели оценки, способной эффективно направлять поиск, управляемый языковой моделью. Этот самоконтролируемый метод использует динамику переходов состояний, что позволяет избежать дорогостоящего и трудоемкого сбора наград за выполнение задач или демонстраций от людей. Исследование показывает, что относительно небольшие (8 миллиардов параметров) модели оценки, улучшенные с помощью 'self-taught lookahead', могут сравниться по производительности с использованием передовых языковых моделей, таких как GPT-4, в качестве модели оценки. Более того, метод улучшает производительность на 20% при одновременном снижении затрат в 37 раз по сравнению с предыдущими методами поиска на основе больших языковых моделей."}, 'en': {'title': 'Self-Taught Lookahead: Cost-Effective Multi-Step Reasoning', 'desc': 'This paper introduces a method called self-taught lookahead, which is designed to improve the efficiency of training value models for multi-step reasoning tasks without needing expensive human input. By utilizing state-transition dynamics, this self-supervised approach allows the model to learn how to guide searches effectively. The authors demonstrate that a moderately sized value model can achieve performance comparable to larger models like GPT-4o, while also significantly reducing costs. Overall, self-taught lookahead enhances performance by 20% and cuts costs by 37 times compared to traditional LLM-based methods.'}, 'zh': {'title': '自我教导前瞻：高效的多步骤推理解决方案', 'desc': '本论文提出了一种自我学习的前瞻性方法，称为自我教导前瞻（self-taught lookahead），旨在解决多步骤推理任务中收集真实奖励或人类示范的高成本和时间消耗问题。该方法利用状态转移动态来训练一个价值模型，从而有效指导语言模型控制的搜索。研究表明，使用自我教导前瞻的中等规模（80亿参数）开放权重价值模型，其性能可以与前沿的语言模型（如gpt-4o）相媲美。更重要的是，自我教导前瞻在不依赖真实奖励的情况下，能够将性能提升20%，同时将成本降低37倍。'}}}, {'id': 'https://huggingface.co/papers/2503.01342', 'title': 'UFO: A Unified Approach to Fine-grained Visual Perception via Open-ended Language Interface', 'url': 'https://huggingface.co/papers/2503.01342', 'abstract': 'Generalist models have achieved remarkable success in both language and vision-language tasks, showcasing the potential of unified modeling. However, effectively integrating fine-grained perception tasks like detection and segmentation into these models remains a significant challenge. This is primarily because these tasks often rely heavily on task-specific designs and architectures that can complicate the modeling process. To address this challenge, we present \\ours, a framework that Unifies Fine-grained visual perception tasks through an Open-ended language interface. By transforming all perception targets into the language space, \\ours unifies object-level detection, pixel-level segmentation, and image-level vision-language tasks into a single model. Additionally, we introduce a novel embedding retrieval approach that relies solely on the language interface to support segmentation tasks. Our framework bridges the gap between fine-grained perception and vision-language tasks, significantly simplifying architectural design and training strategies while achieving comparable or superior performance to methods with intricate task-specific designs. After multi-task training on five standard visual perception datasets, \\ours outperforms the previous state-of-the-art generalist models by 12.3 mAP on COCO instance segmentation and 3.3 mIoU on ADE20K semantic segmentation. Furthermore, our method seamlessly integrates with existing MLLMs, effectively combining fine-grained perception capabilities with their advanced language abilities, thereby enabling more challenging tasks such as reasoning segmentation. Code and models will be publicly available.', 'score': 4, 'issue_id': 2535, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': '8f87e24c90eade92', 'authors': ['Hao Tang', 'Chenwei Xie', 'Haiyang Wang', 'Xiaoyi Bao', 'Tingyu Weng', 'Pandeng Li', 'Yun Zheng', 'Liwei Wang'], 'affiliations': ['Alibaba Group', 'Center for Data Science, Peking University', 'Institute of Automation, Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2503.01342.jpg', 'data': {'categories': ['#cv', '#training', '#optimization', '#agi', '#open_source', '#multimodal', '#architecture'], 'emoji': '🔬', 'ru': {'title': 'Унификация задач компьютерного зрения через языковой интерфейс', 'desc': 'Статья представляет новый фреймворк для унификации задач тонкой визуальной перцепции через языковой интерфейс. Авторы трансформируют все цели восприятия в языковое пространство, объединяя обнаружение объектов, сегментацию на уровне пикселей и задачи зрения-языка в единую модель. Предложен новый подход с извлечением эмбеддингов, использующий только языковой интерфейс для поддержки задач сегментации. Фреймворк превосходит предыдущие модели-генералисты на нескольких стандартных наборах данных и легко интегрируется с существующими мультимодальными языковыми моделями.'}, 'en': {'title': 'Unifying Fine-Grained Perception with Language for Enhanced Model Performance', 'desc': 'This paper introduces a framework called \textit{ours} that aims to unify fine-grained visual perception tasks, such as detection and segmentation, with language-based tasks. By converting all perception targets into a language format, the framework simplifies the integration of various tasks into a single model. The authors also propose a new embedding retrieval method that utilizes the language interface to enhance segmentation capabilities. The results show that \textit{ours} outperforms existing generalist models on multiple datasets, demonstrating its effectiveness in bridging fine-grained perception and vision-language tasks.'}, 'zh': {'title': '统一细粒度视觉感知与语言任务的创新框架', 'desc': '本论文提出了一种新的框架\textit{ours}，旨在通过开放式语言接口统一细粒度视觉感知任务，如目标检测和像素分割。该框架将所有感知目标转化为语言空间，从而将对象级检测、像素级分割和图像级视觉语言任务整合到一个模型中。我们还引入了一种新颖的嵌入检索方法，仅依赖语言接口来支持分割任务。经过在五个标准视觉感知数据集上的多任务训练，\textit{ours}在COCO实例分割上比之前的最先进通用模型提高了12.3 mAP，在ADE20K语义分割上提高了3.3 mIoU。'}}}, {'id': 'https://huggingface.co/papers/2503.02876', 'title': 'SPIDER: A Comprehensive Multi-Organ Supervised Pathology Dataset and Baseline Models', 'url': 'https://huggingface.co/papers/2503.02876', 'abstract': 'Advancing AI in computational pathology requires large, high-quality, and diverse datasets, yet existing public datasets are often limited in organ diversity, class coverage, or annotation quality. To bridge this gap, we introduce SPIDER (Supervised Pathology Image-DEscription Repository), the largest publicly available patch-level dataset covering multiple organ types, including Skin, Colorectal, and Thorax, with comprehensive class coverage for each organ. SPIDER provides high-quality annotations verified by expert pathologists and includes surrounding context patches, which enhance classification performance by providing spatial context.   Alongside the dataset, we present baseline models trained on SPIDER using the Hibou-L foundation model as a feature extractor combined with an attention-based classification head. The models achieve state-of-the-art performance across multiple tissue categories and serve as strong benchmarks for future digital pathology research. Beyond patch classification, the model enables rapid identification of significant areas, quantitative tissue metrics, and establishes a foundation for multimodal approaches.   Both the dataset and trained models are publicly available to advance research, reproducibility, and AI-driven pathology development. Access them at: https://github.com/HistAI/SPIDER', 'score': 4, 'issue_id': 2532, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 марта', 'en': 'March 4', 'zh': '3月4日'}, 'hash': 'a85b69f61f2f377f', 'authors': ['Dmitry Nechaev', 'Alexey Pchelnikov', 'Ekaterina Ivanova'], 'affiliations': ['HistAI'], 'pdf_title_img': 'assets/pdf/title_img/2503.02876.jpg', 'data': {'categories': ['#open_source', '#dataset', '#multimodal', '#science', '#benchmark'], 'emoji': '🕷️', 'ru': {'title': 'SPIDER: Новый стандарт данных для ИИ в патологии', 'desc': 'Статья представляет SPIDER - крупнейший общедоступный набор данных для вычислительной патологии, охватывающий множество типов органов. SPIDER предоставляет высококачественные аннотации, проверенные экспертами-патологами, и включает контекстные патчи для улучшения классификации. Авторы также представляют базовые модели, обученные на SPIDER с использованием модели Hibou-L в качестве экстрактора признаков и классификационной головки на основе механизма внимания. Модели достигают современного уровня производительности в нескольких категориях тканей и служат эталоном для будущих исследований в области цифровой патологии.'}, 'en': {'title': 'SPIDER: Bridging the Gap in Pathology Datasets for AI Advancement', 'desc': 'This paper introduces SPIDER, a large and diverse dataset designed for computational pathology, addressing the limitations of existing public datasets. SPIDER includes high-quality, expert-verified annotations for various organ types, enhancing the classification of pathology images. The authors also present baseline models that utilize the Hibou-L foundation model and an attention-based classification head, achieving state-of-the-art results in tissue classification. The dataset and models are publicly available, promoting further research and development in AI-driven pathology.'}, 'zh': {'title': '推动病理学的AI进步', 'desc': '本论文介绍了SPIDER（监督病理图像描述库），这是一个涵盖多种器官类型的最大公开补丁级数据集，包括皮肤、结直肠和胸部。该数据集提供了高质量的注释，由专家病理学家验证，并包含周围上下文补丁，以提高分类性能。我们还展示了基于SPIDER训练的基线模型，使用Hibou-L基础模型作为特征提取器，并结合基于注意力的分类头，达到了多种组织类别的最先进性能。该数据集和训练模型均可公开获取，以推动研究、可重复性和基于AI的病理发展。'}}}, {'id': 'https://huggingface.co/papers/2503.00876', 'title': 'Improve Representation for Imbalanced Regression through Geometric Constraints', 'url': 'https://huggingface.co/papers/2503.00876', 'abstract': 'In representation learning, uniformity refers to the uniform feature distribution in the latent space (i.e., unit hypersphere). Previous work has shown that improving uniformity contributes to the learning of under-represented classes. However, most of the previous work focused on classification; the representation space of imbalanced regression remains unexplored. Classification-based methods are not suitable for regression tasks because they cluster features into distinct groups without considering the continuous and ordered nature essential for regression. In a geometric aspect, we uniquely focus on ensuring uniformity in the latent space for imbalanced regression through two key losses: enveloping and homogeneity. The enveloping loss encourages the induced trace to uniformly occupy the surface of a hypersphere, while the homogeneity loss ensures smoothness, with representations evenly spaced at consistent intervals. Our method integrates these geometric principles into the data representations via a Surrogate-driven Representation Learning (SRL) framework. Experiments with real-world regression and operator learning tasks highlight the importance of uniformity in imbalanced regression and validate the efficacy of our geometry-based loss functions.', 'score': 3, 'issue_id': 2543, 'pub_date': '2025-03-02', 'pub_date_card': {'ru': '2 марта', 'en': 'March 2', 'zh': '3月2日'}, 'hash': '018d350ccd3dd3f5', 'authors': ['Zijian Dong', 'Yilei Wu', 'Chongyao Chen', 'Yingtian Zou', 'Yichi Zhang', 'Juan Helen Zhou'], 'affiliations': ['National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2503.00876.jpg', 'data': {'categories': ['#data', '#optimization', '#training'], 'emoji': '🌐', 'ru': {'title': 'Геометрический подход к равномерному представлению в несбалансированной регрессии', 'desc': 'Эта статья исследует проблему равномерности представления данных в задачах регрессии с несбалансированными выборками. Авторы предлагают новый подход, основанный на геометрических принципах, для улучшения равномерности в латентном пространстве. Они вводят две ключевые функции потерь: обволакивающую и гомогенности, которые способствуют равномерному распределению признаков на гиперсфере. Эксперименты на реальных данных подтверждают эффективность предложенного метода для задач регрессии и обучения операторов.'}, 'en': {'title': 'Achieving Uniformity in Imbalanced Regression through Geometric Losses', 'desc': "This paper addresses the challenge of representation learning in imbalanced regression tasks, where the distribution of data points is uneven across different classes. It introduces a novel approach that focuses on achieving uniformity in the latent space by employing two specific loss functions: enveloping and homogeneity. The enveloping loss promotes a uniform distribution of features on the hypersphere's surface, while the homogeneity loss ensures that these features are evenly spaced. The proposed Surrogate-driven Representation Learning (SRL) framework demonstrates the effectiveness of these geometric principles in improving performance on real-world regression tasks."}, 'zh': {'title': '提升不平衡回归的均匀性', 'desc': '在表示学习中，均匀性指的是潜在空间中特征的均匀分布。以往的研究表明，提高均匀性有助于学习代表性不足的类别，但大多数研究集中在分类任务上，而不平衡回归的表示空间尚未被探索。我们的方法通过两个关键损失函数：包络损失和均匀性损失，确保不平衡回归中的潜在空间均匀性。实验结果表明，我们的几何基础损失函数在不平衡回归中具有重要意义，验证了其有效性。'}}}, {'id': 'https://huggingface.co/papers/2503.02357', 'title': 'Q-Eval-100K: Evaluating Visual Quality and Alignment Level for Text-to-Vision Content', 'url': 'https://huggingface.co/papers/2503.02357', 'abstract': 'Evaluating text-to-vision content hinges on two crucial aspects: visual quality and alignment. While significant progress has been made in developing objective models to assess these dimensions, the performance of such models heavily relies on the scale and quality of human annotations. According to Scaling Law, increasing the number of human-labeled instances follows a predictable pattern that enhances the performance of evaluation models. Therefore, we introduce a comprehensive dataset designed to Evaluate Visual quality and Alignment Level for text-to-vision content (Q-EVAL-100K), featuring the largest collection of human-labeled Mean Opinion Scores (MOS) for the mentioned two aspects. The Q-EVAL-100K dataset encompasses both text-to-image and text-to-video models, with 960K human annotations specifically focused on visual quality and alignment for 100K instances (60K images and 40K videos). Leveraging this dataset with context prompt, we propose Q-Eval-Score, a unified model capable of evaluating both visual quality and alignment with special improvements for handling long-text prompt alignment. Experimental results indicate that the proposed Q-Eval-Score achieves superior performance on both visual quality and alignment, with strong generalization capabilities across other benchmarks. These findings highlight the significant value of the Q-EVAL-100K dataset. Data and codes will be available at https://github.com/zzc-1998/Q-Eval.', 'score': 3, 'issue_id': 2541, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 марта', 'en': 'March 4', 'zh': '3月4日'}, 'hash': 'a9bc65d32288d8f5', 'authors': ['Zicheng Zhang', 'Tengchuan Kou', 'Shushi Wang', 'Chunyi Li', 'Wei Sun', 'Wei Wang', 'Xiaoyu Li', 'Zongyu Wang', 'Xuezhi Cao', 'Xiongkuo Min', 'Xiaohong Liu', 'Guangtao Zhai'], 'affiliations': ['Meituan', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2503.02357.jpg', 'data': {'categories': ['#benchmark', '#alignment', '#long_context', '#dataset', '#multimodal', '#cv', '#video'], 'emoji': '🔍', 'ru': {'title': 'Большие данные для лучшей оценки генеративных моделей', 'desc': 'Статья описывает создание большого набора данных Q-EVAL-100K для оценки качества и соответствия контента, сгенерированного моделями text-to-image и text-to-video. На основе этого набора данных авторы разработали модель Q-Eval-Score для автоматической оценки визуального качества и соответствия запросу. Модель показала высокую эффективность и обобщающую способность на различных бенчмарках. Исследование подчеркивает важность масштабных размеченных человеком данных для улучшения оценочных моделей в области генерации изображений и видео по текстовому описанию.'}, 'en': {'title': 'Enhancing Text-to-Vision Evaluation with Q-EVAL-100K', 'desc': 'This paper discusses the importance of evaluating text-to-vision content based on visual quality and alignment. It highlights the role of human annotations in improving the performance of evaluation models, following the Scaling Law principle. The authors introduce a new dataset called Q-EVAL-100K, which contains a large number of human-labeled Mean Opinion Scores for both text-to-image and text-to-video models. They also present Q-Eval-Score, a model that effectively assesses visual quality and alignment, demonstrating strong performance and generalization across various benchmarks.'}, 'zh': {'title': '提升文本到视觉内容评估的质量与对齐度', 'desc': '本文探讨了文本到视觉内容评估的两个关键方面：视觉质量和对齐度。尽管已有客观模型在这两个维度上取得了显著进展，但这些模型的性能依赖于人类标注的规模和质量。我们引入了一个全面的数据集Q-EVAL-100K，包含960K个人类标注的平均意见分数（MOS），用于评估文本到图像和文本到视频模型的视觉质量和对齐度。通过利用该数据集，我们提出了Q-Eval-Score模型，能够有效评估视觉质量和对齐度，并在处理长文本提示对齐方面进行了特别改进。'}}}, {'id': 'https://huggingface.co/papers/2503.02783', 'title': 'IterPref: Focal Preference Learning for Code Generation via Iterative Debugging', 'url': 'https://huggingface.co/papers/2503.02783', 'abstract': 'Preference learning enhances Code LLMs beyond supervised fine-tuning by leveraging relative quality comparisons. Existing methods construct preference pairs from   candidates based on test case success, treating the higher pass rate sample as positive and the lower as negative. However, this approach does not pinpoint specific errors in the code, which prevents the model from learning more informative error correction patterns, as aligning failing code as a whole lacks the granularity needed to capture meaningful error-resolution relationships. To address these issues, we propose IterPref, a new preference alignment framework that mimics human iterative debugging to refine Code LLMs. IterPref explicitly locates error regions and aligns the corresponding tokens via a tailored DPO algorithm. To generate informative pairs, we introduce the CodeFlow dataset, where samples are iteratively refined until passing tests, with modifications capturing error corrections. Extensive experiments show that a diverse suite of Code LLMs equipped with IterPref achieves significant performance gains in code generation and improves on challenging tasks like BigCodeBench. In-depth analysis reveals that IterPref yields fewer errors. Our code and data will be made publicaly available.', 'score': 3, 'issue_id': 2540, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 марта', 'en': 'March 4', 'zh': '3月4日'}, 'hash': '3d7ced41646a5f95', 'authors': ['Jie Wu', 'Haoling Li', 'Xin Zhang', 'Jianwen Luo', 'Yangyu Huang', 'Ruihang Chu', 'Yujiu Yang', 'Scarlett Li'], 'affiliations': ['CASIA', 'Microsoft Research', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.02783.jpg', 'data': {'categories': ['#alignment', '#rlhf', '#open_source', '#dataset', '#optimization', '#training'], 'emoji': '🔧', 'ru': {'title': 'Итеративное обучение предпочтениям для усовершенствования языковых моделей кода', 'desc': 'Статья представляет новый метод IterPref для улучшения языковых моделей кода (Code LLMs) с помощью обучения предпочтениям. В отличие от существующих подходов, IterPref точно определяет области ошибок в коде и выравнивает соответствующие токены с использованием специального алгоритма DPO. Для генерации информативных пар авторы создали набор данных CodeFlow, где образцы кода итеративно улучшаются до прохождения тестов. Эксперименты показывают, что IterPref значительно повышает производительность Code LLMs в генерации кода и улучшает результаты на сложных задачах.'}, 'en': {'title': 'Iterative Preference Learning for Enhanced Code LLMs', 'desc': 'This paper introduces IterPref, a novel framework for enhancing Code LLMs (Language Models) through preference learning. Unlike traditional methods that simply compare pass rates of code samples, IterPref focuses on identifying specific error regions in code, allowing for more precise learning of error correction patterns. By utilizing a tailored DPO (Direct Preference Optimization) algorithm and the CodeFlow dataset, which iteratively refines code samples, the framework generates informative preference pairs that improve model training. Experimental results demonstrate that Code LLMs using IterPref show significant performance improvements in code generation tasks and exhibit fewer errors overall.'}, 'zh': {'title': '迭代调试，提升代码生成能力！', 'desc': '本论文提出了一种新的偏好对齐框架IterPref，旨在通过模拟人类的迭代调试过程来提升代码大语言模型（Code LLMs）的性能。现有方法通过测试用例的成功率构建偏好对，但未能准确定位代码中的具体错误，限制了模型学习有效的错误修正模式。IterPref通过定制的DPO算法明确定位错误区域，并对相应的标记进行对齐，从而生成更具信息量的偏好对。实验结果表明，使用IterPref的多样化代码大语言模型在代码生成和复杂任务上均取得了显著的性能提升。'}}}, {'id': 'https://huggingface.co/papers/2503.02268', 'title': 'AppAgentX: Evolving GUI Agents as Proficient Smartphone Users', 'url': 'https://huggingface.co/papers/2503.02268', 'abstract': "Recent advancements in Large Language Models (LLMs) have led to the development of intelligent LLM-based agents capable of interacting with graphical user interfaces (GUIs). These agents demonstrate strong reasoning and adaptability, enabling them to perform complex tasks that traditionally required predefined rules. However, the reliance on step-by-step reasoning in LLM-based agents often results in inefficiencies, particularly for routine tasks. In contrast, traditional rule-based systems excel in efficiency but lack the intelligence and flexibility to adapt to novel scenarios. To address this challenge, we propose a novel evolutionary framework for GUI agents that enhances operational efficiency while retaining intelligence and flexibility. Our approach incorporates a memory mechanism that records the agent's task execution history. By analyzing this history, the agent identifies repetitive action sequences and evolves high-level actions that act as shortcuts, replacing these low-level operations and improving efficiency. This allows the agent to focus on tasks requiring more complex reasoning, while simplifying routine actions. Experimental results on multiple benchmark tasks demonstrate that our approach significantly outperforms existing methods in both efficiency and accuracy. The code will be open-sourced to support further research.", 'score': 3, 'issue_id': 2536, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 марта', 'en': 'March 4', 'zh': '3月4日'}, 'hash': '03199cb797dc8d88', 'authors': ['Wenjia Jiang', 'Yangyang Zhuang', 'Chenxi Song', 'Xu Yang', 'Chi Zhang'], 'affiliations': ['Henan University', 'Southeast University', 'Westlake University'], 'pdf_title_img': 'assets/pdf/title_img/2503.02268.jpg', 'data': {'categories': ['#optimization', '#agents', '#benchmark', '#reasoning', '#open_source', '#training'], 'emoji': '🧠', 'ru': {'title': 'Эволюционное обучение агентов GUI: баланс эффективности и гибкости', 'desc': 'Эта статья представляет новый эволюционный подход для агентов, работающих с графическим интерфейсом на основе больших языковых моделей (LLM). Авторы предлагают механизм памяти, который записывает историю выполнения задач агентом и анализирует ее для выявления повторяющихся последовательностей действий. На основе этого анализа создаются высокоуровневые действия, которые заменяют низкоуровневые операции, повышая эффективность работы. Экспериментальные результаты показывают, что данный метод значительно превосходит существующие подходы по эффективности и точности.'}, 'en': {'title': 'Evolving Efficiency: Smart Shortcuts for GUI Agents', 'desc': 'This paper presents a new framework for Large Language Model (LLM)-based agents that interact with graphical user interfaces (GUIs). The proposed method enhances the efficiency of these agents by incorporating a memory mechanism that tracks their task execution history. By analyzing this history, the agents can identify repetitive actions and evolve high-level shortcuts, allowing them to streamline routine tasks. This approach maintains the intelligence and adaptability of LLMs while improving their operational efficiency, as demonstrated by experimental results on benchmark tasks.'}, 'zh': {'title': '智能代理的进化：提升效率与灵活性', 'desc': '最近，大型语言模型（LLMs）的进步使得基于LLM的智能代理能够与图形用户界面（GUIs）进行交互。这些代理展现出强大的推理能力和适应性，能够执行传统上需要预定义规则的复杂任务。然而，基于LLM的代理在依赖逐步推理时，往往在处理常规任务时效率较低。为了解决这个问题，我们提出了一种新颖的进化框架，通过记录代理的任务执行历史，识别重复的操作序列，从而演化出高层次的快捷操作，提升了操作效率，同时保留了智能和灵活性。'}}}, {'id': 'https://huggingface.co/papers/2503.02812', 'title': 'Q-Filters: Leveraging QK Geometry for Efficient KV Cache Compression', 'url': 'https://huggingface.co/papers/2503.02812', 'abstract': 'Autoregressive language models rely on a Key-Value (KV) Cache, which avoids re-computing past hidden states during generation, making it faster. As model sizes and context lengths grow, the KV Cache becomes a significant memory bottleneck, which calls for compression methods that limit its size during generation. In this paper, we discover surprising properties of Query (Q) and Key (K) vectors that allow us to efficiently approximate attention scores without computing the attention maps. We propose Q-Filters, a training-free KV Cache compression method that filters out less crucial Key-Value pairs based on a single context-agnostic projection. Contrarily to many alternatives, Q-Filters is compatible with FlashAttention, as it does not require direct access to attention weights. Experimental results in long-context settings demonstrate that Q-Filters is competitive with attention-based compression methods such as SnapKV in retrieval tasks while consistently outperforming efficient compression schemes such as Streaming-LLM in generation setups. Notably, Q-Filters achieves a 99% accuracy in the needle-in-a-haystack task with a x32 compression level while reducing the generation perplexity drop by up to 65% in text generation compared to Streaming-LLM.', 'score': 2, 'issue_id': 2546, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 марта', 'en': 'March 4', 'zh': '3月4日'}, 'hash': '9dc95b6ce0d4b6ca', 'authors': ['Nathan Godey', 'Alessio Devoto', 'Yu Zhao', 'Simone Scardapane', 'Pasquale Minervini', 'Éric de la Clergerie', 'Benoît Sagot'], 'affiliations': ['Miniml.AI', 'Sapienza University of Rome', 'Sorbonne Université, Paris, France', 'University of Edinburgh'], 'pdf_title_img': 'assets/pdf/title_img/2503.02812.jpg', 'data': {'categories': ['#inference', '#long_context', '#training', '#optimization'], 'emoji': '🔍', 'ru': {'title': 'Q-Filters: Эффективное сжатие KV-кэша без компромиссов в производительности', 'desc': 'Статья представляет новый метод сжатия KV-кэша для авторегрессивных языковых моделей под названием Q-Filters. Этот метод основан на обнаруженных свойствах векторов запросов (Q) и ключей (K), позволяющих эффективно аппроксимировать оценки внимания без вычисления карт внимания. Q-Filters фильтрует менее важные пары ключ-значение на основе одной контекстно-независимой проекции, что делает его совместимым с FlashAttention. Экспериментальные результаты показывают, что Q-Filters превосходит другие методы сжатия в задачах генерации текста и извлечения информации при длинных контекстах.'}, 'en': {'title': 'Efficient KV Cache Compression with Q-Filters', 'desc': 'This paper addresses the memory limitations of autoregressive language models caused by the Key-Value (KV) Cache during text generation. It introduces Q-Filters, a novel method that compresses the KV Cache by filtering out less important Key-Value pairs without needing to compute attention maps. The method is efficient and compatible with existing techniques like FlashAttention, making it a practical solution for large models. Experimental results show that Q-Filters not only competes well with other compression methods but also significantly improves performance in text generation tasks.'}, 'zh': {'title': '高效压缩：Q-Filters的创新之路', 'desc': '自回归语言模型依赖于键值缓存（KV Cache），以避免在生成过程中重新计算过去的隐藏状态，从而提高速度。随着模型规模和上下文长度的增加，KV Cache 成为一个重要的内存瓶颈，因此需要压缩方法来限制其大小。本文发现了查询（Q）和键（K）向量的意外特性，使我们能够在不计算注意力图的情况下有效地近似注意力分数。我们提出了 Q-Filters，这是一种无训练的 KV Cache 压缩方法，通过单一的上下文无关投影过滤掉不太重要的键值对。'}}}, {'id': 'https://huggingface.co/papers/2503.02823', 'title': 'A Multimodal Symphony: Integrating Taste and Sound through Generative AI', 'url': 'https://huggingface.co/papers/2503.02823', 'abstract': "In recent decades, neuroscientific and psychological research has traced direct relationships between taste and auditory perceptions. This article explores multimodal generative models capable of converting taste information into music, building on this foundational research. We provide a brief review of the state of the art in this field, highlighting key findings and methodologies. We present an experiment in which a fine-tuned version of a generative music model (MusicGEN) is used to generate music based on detailed taste descriptions provided for each musical piece. The results are promising: according the participants' (n=111) evaluation, the fine-tuned model produces music that more coherently reflects the input taste descriptions compared to the non-fine-tuned model. This study represents a significant step towards understanding and developing embodied interactions between AI, sound, and taste, opening new possibilities in the field of generative AI. We release our dataset, code and pre-trained model at: https://osf.io/xs5jy/.", 'score': 2, 'issue_id': 2545, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 марта', 'en': 'March 4', 'zh': '3月4日'}, 'hash': '841602ca85525c24', 'authors': ['Matteo Spanio', 'Massimiliano Zampini', 'Antonio Rodà', 'Franco Pierucci'], 'affiliations': ['Center for Mind/Brain Sciences (CIMeC) University of Trento Rovereto, Italy', 'Centro di Sonologia Computazionale (CSC) Department of Information Engineering University of Padova Padova, Italy', 'SoundFood s.r.l. Terni, Italy'], 'pdf_title_img': 'assets/pdf/title_img/2503.02823.jpg', 'data': {'categories': ['#multimodal', '#open_source', '#games', '#dataset'], 'emoji': '🎵', 'ru': {'title': 'От вкуса к мелодии: ИИ преобразует гастрономические ощущения в музыку', 'desc': 'Статья исследует генеративные модели, способные преобразовывать информацию о вкусе в музыку. Авторы провели эксперимент с использованием дообученной версии модели MusicGEN для генерации музыки на основе описаний вкусов. Результаты показали, что дообученная модель создает музыку, более согласованную с входными описаниями вкусов по сравнению с базовой моделью. Это исследование открывает новые возможности в области генеративного искусственного интеллекта и мультимодального восприятия.'}, 'en': {'title': 'Transforming Taste into Sound: A New Frontier in Generative AI', 'desc': 'This paper investigates the connection between taste and sound by using multimodal generative models. It focuses on a specific model called MusicGEN, which has been fine-tuned to create music that corresponds to taste descriptions. An experiment with 111 participants showed that the fine-tuned model produced music that better matched the taste inputs than the original model. This research advances the understanding of how AI can create embodied experiences that link different sensory modalities, particularly taste and sound.'}, 'zh': {'title': '味觉与音乐的奇妙结合', 'desc': '近年来，神经科学和心理学研究发现味觉与听觉之间存在直接关系。本文探讨了多模态生成模型，能够将味觉信息转换为音乐，基于这一基础研究进行深入分析。我们回顾了该领域的最新进展，强调了关键发现和方法论。实验结果表明，经过微调的生成音乐模型（MusicGEN）能够更好地反映输入的味觉描述，展示了AI、声音与味觉之间的互动新可能。'}}}, {'id': 'https://huggingface.co/papers/2503.02152', 'title': 'Tabby: Tabular Data Synthesis with Language Models', 'url': 'https://huggingface.co/papers/2503.02152', 'abstract': 'While advances in large language models (LLMs) have greatly improved the quality of synthetic text data in recent years, synthesizing tabular data has received relatively less attention. We address this disparity with Tabby, a simple but powerful post-training modification to the standard Transformer language model architecture, enabling its use for tabular dataset synthesis. Tabby enables the representation of differences across columns using Gated Mixture-of-Experts, with column-specific sets of parameters. Empirically, Tabby results in data quality near or equal to that of real data. By pairing our novel LLM table training technique, Plain, with Tabby, we observe up to a 44% improvement in quality over previous methods. We also show that Tabby extends beyond tables to more general structured data, reaching parity with real data on a nested JSON dataset as well.', 'score': 1, 'issue_id': 2545, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 марта', 'en': 'March 4', 'zh': '3月4日'}, 'hash': '1d33263eb28e0be2', 'authors': ['Sonia Cromp', 'Satya Sai Srinath Namburi GNVV', 'Mohammed Alkhudhayri', 'Catherine Cao', 'Samuel Guo', 'Nicholas Roberts', 'Frederic Sala'], 'affiliations': ['GE HealthCare', 'University of Wisconsin-Madison'], 'pdf_title_img': 'assets/pdf/title_img/2503.02152.jpg', 'data': {'categories': ['#dataset', '#synthetic', '#architecture', '#data'], 'emoji': '📊', 'ru': {'title': 'Tabby: прорыв в синтезе табличных данных с помощью языковых моделей', 'desc': 'Исследователи представили Tabby - модификацию архитектуры трансформера для синтеза табличных данных. Tabby использует Gated Mixture-of-Experts для представления различий между столбцами с помощью специфичных для каждого столбца параметров. В сочетании с новой техникой обучения Plain, Tabby показывает улучшение качества синтетических данных до 44% по сравнению с предыдущими методами. Модель также эффективна для работы с вложенными JSON-данными.'}, 'en': {'title': 'Tabby: Transforming Tabular Data Synthesis with LLMs', 'desc': 'This paper introduces Tabby, a new method for synthesizing tabular data using a modified Transformer language model. It employs Gated Mixture-of-Experts to effectively capture the unique characteristics of different columns in a dataset. The results show that Tabby can generate synthetic data that is comparable in quality to real data, achieving significant improvements over existing techniques. Additionally, Tabby is versatile enough to be applied to other structured data formats, such as nested JSON, demonstrating its broad applicability in data synthesis tasks.'}, 'zh': {'title': 'Tabby：表格数据合成的新突破', 'desc': '本文介绍了一种名为Tabby的方法，用于合成表格数据。Tabby是对标准Transformer语言模型架构的后训练修改，能够有效表示列之间的差异。通过使用门控混合专家模型，Tabby为每一列提供特定的参数集，从而提高数据质量。实验表明，Tabby在合成数据的质量上接近或等同于真实数据，并且在处理嵌套JSON数据集时也表现出色。'}}}, {'id': 'https://huggingface.co/papers/2503.02304', 'title': 'A Token-level Text Image Foundation Model for Document Understanding', 'url': 'https://huggingface.co/papers/2503.02304', 'abstract': 'In recent years, general visual foundation models (VFMs) have witnessed increasing adoption, particularly as image encoders for popular multi-modal large language models (MLLMs). However, without semantically fine-grained supervision, these models still encounter fundamental prediction errors in the context of downstream text-image-related tasks, i.e., perception, understanding and reasoning with images containing small and dense texts. To bridge this gap, we develop TokenOCR, the first token-level visual foundation model specifically tailored for text-image-related tasks, designed to support a variety of traditional downstream applications. To facilitate the pretraining of TokenOCR, we also devise a high-quality data production pipeline that constructs the first token-level image text dataset, TokenIT, comprising 20 million images and 1.8 billion token-mask pairs. Furthermore, leveraging this foundation with exceptional image-as-text capability, we seamlessly replace previous VFMs with TokenOCR to construct a document-level MLLM, TokenVL, for VQA-based document understanding tasks. Finally, extensive experiments demonstrate the effectiveness of TokenOCR and TokenVL. Code, datasets, and weights will be available at https://token-family.github.io/TokenOCR_project.', 'score': 1, 'issue_id': 2545, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 марта', 'en': 'March 4', 'zh': '3月4日'}, 'hash': 'b275d8ba26cca4b5', 'authors': ['Tongkun Guan', 'Zining Wang', 'Pei Fu', 'Zhengtao Guo', 'Wei Shen', 'Kai Zhou', 'Tiezhu Yue', 'Chen Duan', 'Hao Sun', 'Qianyi Jiang', 'Junfeng Luo', 'Xiaokang Yang'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2503.02304.jpg', 'data': {'categories': ['#dataset', '#cv', '#agi', '#reasoning', '#optimization', '#multimodal', '#games', '#data', '#open_source'], 'emoji': '🔍', 'ru': {'title': 'TokenOCR: Новый уровень распознавания текста на изображениях', 'desc': 'Исследователи разработали TokenOCR - первую визуальную фундаментальную модель на уровне токенов для задач, связанных с текстом на изображениях. Для предобучения модели создан набор данных TokenIT, содержащий 20 миллионов изображений и 1,8 миллиарда пар токен-маска. На основе TokenOCR построена мультимодальная языковая модель TokenVL для задач понимания документов. Эксперименты подтверждают эффективность предложенных моделей TokenOCR и TokenVL.'}, 'en': {'title': 'TokenOCR: Bridging Text and Image Understanding', 'desc': 'This paper introduces TokenOCR, a novel visual foundation model designed specifically for tasks that involve understanding and reasoning about images with dense text. Traditional visual foundation models struggle with these tasks due to a lack of detailed supervision, leading to prediction errors. TokenOCR addresses this issue by utilizing a new dataset, TokenIT, which contains 20 million images and 1.8 billion token-mask pairs for effective pretraining. The model is integrated into a document-level multi-modal large language model, TokenVL, enhancing its capabilities for visual question answering and document understanding tasks.'}, 'zh': {'title': 'TokenOCR：文本图像任务的视觉基础模型新突破', 'desc': '近年来，通用视觉基础模型（VFM）在多模态大语言模型（MLLM）中得到了广泛应用，尤其作为图像编码器。然而，在没有细粒度语义监督的情况下，这些模型在处理与文本相关的图像任务时仍然会出现基本的预测错误。为了解决这个问题，我们开发了TokenOCR，这是第一个专门针对文本-图像相关任务的标记级视觉基础模型，并设计了一个高质量的数据生产管道，构建了包含2000万张图像和18亿个标记-掩码对的TokenIT数据集。通过这一基础，我们成功地用TokenOCR替换了之前的VFM，构建了用于文档理解任务的TokenVL文档级MLLM，实验结果证明了TokenOCR和TokenVL的有效性。'}}}, {'id': 'https://huggingface.co/papers/2503.00200', 'title': 'Unified Video Action Model', 'url': 'https://huggingface.co/papers/2503.00200', 'abstract': 'A unified video and action model holds significant promise for robotics, where videos provide rich scene information for action prediction, and actions provide dynamics information for video prediction. However, effectively combining video generation and action prediction remains challenging, and current video generation-based methods struggle to match the performance of direct policy learning in action accuracy and inference speed. To bridge this gap, we introduce the Unified Video Action model (UVA), which jointly optimizes video and action predictions to achieve both high accuracy and efficient action inference. The key lies in learning a joint video-action latent representation and decoupling video-action decoding. The joint latent representation bridges the visual and action domains, effectively modeling the relationship between video and action sequences. Meanwhile, the decoupled decoding, powered by two lightweight diffusion heads, enables high-speed action inference by bypassing video generation during inference. Such a unified framework further enables versatile functionality through masked input training. By selectively masking actions or videos, a single model can tackle diverse tasks beyond policy learning, such as forward and inverse dynamics modeling and video generation. Via an extensive set of experiments, we demonstrate that UVA can serve as a general-purpose solution for a wide range of robotics tasks, such as policy learning, forward/inverse dynamics and video observation prediction, without compromising performance compared to methods tailored for specific applications. Results are best viewed on https://unified-video-action-model.github.io/.', 'score': 0, 'issue_id': 2551, 'pub_date': '2025-02-28', 'pub_date_card': {'ru': '28 февраля', 'en': 'February 28', 'zh': '2月28日'}, 'hash': '286c6abc07fdf95e', 'authors': ['Shuang Li', 'Yihuai Gao', 'Dorsa Sadigh', 'Shuran Song'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2503.00200.jpg', 'data': {'categories': ['#robotics', '#video', '#games', '#optimization'], 'emoji': '🤖', 'ru': {'title': 'Единая модель для видео и действий: новый подход к робототехнике', 'desc': 'Модель UVA (Unified Video Action) представляет собой единую систему для видео и действий в робототехнике. Она объединяет генерацию видео и предсказание действий, используя совместное латентное представление и раздельное декодирование. UVA обеспечивает высокую точность и эффективность вывода действий, избегая генерации видео во время вывода. Модель применима к различным задачам робототехники, включая обучение политик, прямую и обратную динамику, и предсказание видеонаблюдений.'}, 'en': {'title': 'Bridging Video and Action for Smarter Robotics', 'desc': "The Unified Video Action model (UVA) integrates video generation and action prediction to enhance robotics applications. It achieves this by creating a joint latent representation that connects visual data with action sequences, allowing for better modeling of their interrelationship. The model employs decoupled decoding with lightweight diffusion heads, which speeds up action inference by avoiding the need for video generation during this process. UVA's versatility is further demonstrated through masked input training, enabling it to perform various tasks like policy learning and dynamics modeling efficiently."}, 'zh': {'title': '统一视频与动作模型：提升机器人智能的关键', 'desc': '本文提出了一种统一的视频与动作模型（UVA），旨在提高机器人领域中的动作预测和视频生成的性能。UVA通过联合优化视频和动作预测，学习视频与动作的联合潜在表示，从而有效建模视频与动作序列之间的关系。该模型采用解耦解码的方法，利用轻量级的扩散头实现快速的动作推理，避免了在推理过程中生成视频的需求。通过掩蔽输入训练，UVA能够处理多种任务，如策略学习、前向/逆向动力学建模和视频生成，展现出广泛的适用性。'}}}, {'id': 'https://huggingface.co/papers/2503.01842', 'title': 'Discrete-Time Hybrid Automata Learning: Legged Locomotion Meets Skateboarding', 'url': 'https://huggingface.co/papers/2503.01842', 'abstract': 'This paper introduces Discrete-time Hybrid Automata Learning (DHAL), a framework using on-policy Reinforcement Learning to identify and execute mode-switching without trajectory segmentation or event function learning. Hybrid dynamical systems, which include continuous flow and discrete mode switching, can model robotics tasks like legged robot locomotion. Model-based methods usually depend on predefined gaits, while model-free approaches lack explicit mode-switching knowledge. Current methods identify discrete modes via segmentation before regressing continuous flow, but learning high-dimensional complex rigid body dynamics without trajectory labels or segmentation is a challenging open problem. Our approach incorporates a beta policy distribution and a multi-critic architecture to model contact-guided motions, exemplified by a challenging quadrupedal robot skateboard task. We validate our method through simulations and real-world tests, demonstrating robust performance in hybrid dynamical systems.', 'score': 0, 'issue_id': 2545, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': '07eab4aa91f88137', 'authors': ['Hang Liu', 'Sangli Teng', 'Ben Liu', 'Wei Zhang', 'Maani Ghaffari'], 'affiliations': ['Southern University of Science and Technology', 'University of Michigan'], 'pdf_title_img': 'assets/pdf/title_img/2503.01842.jpg', 'data': {'categories': ['#games', '#rl', '#robotics', '#optimization'], 'emoji': '🤖', 'ru': {'title': 'Обучение гибридным системам без сегментации траекторий', 'desc': 'Статья представляет DHAL - фреймворк для обучения гибридным автоматам с дискретным временем, использующий обучение с подкреплением для идентификации и выполнения переключения режимов. Метод применяет бета-распределение политики и архитектуру с несколькими критиками для моделирования движений, управляемых контактами. DHAL не требует сегментации траекторий или предварительного обучения функций событий. Эффективность подхода продемонстрирована на сложной задаче управления четвероногим роботом на скейтборде в симуляции и реальном мире.'}, 'en': {'title': 'Learning Mode-Switching in Robotics Without Segmentation', 'desc': 'This paper presents Discrete-time Hybrid Automata Learning (DHAL), a novel framework that leverages on-policy Reinforcement Learning to effectively manage mode-switching in hybrid dynamical systems. Unlike traditional methods that require trajectory segmentation or predefined gaits, DHAL learns to identify and execute mode transitions directly from the data. The approach utilizes a beta policy distribution and a multi-critic architecture to handle complex contact-guided motions, particularly in tasks like quadrupedal robot locomotion. The results show that DHAL can robustly learn high-dimensional dynamics without the need for explicit trajectory labels, making it a significant advancement in the field.'}, 'zh': {'title': '无轨迹分割的智能模式切换学习', 'desc': '本文介绍了一种离散时间混合自动机学习（DHAL）框架，利用基于策略的强化学习来识别和执行模式切换，而无需进行轨迹分割或事件函数学习。混合动态系统能够模拟机器人任务，例如四足机器人行走。传统的模型方法通常依赖于预定义的步态，而无模型的方法则缺乏明确的模式切换知识。我们的方法通过引入贝塔策略分布和多重评论家架构，成功地建模了接触引导的运动，并在仿真和实际测试中验证了其在混合动态系统中的强大性能。'}}}, {'id': 'https://huggingface.co/papers/2503.06053', 'title': 'DropletVideo: A Dataset and Approach to Explore Integral Spatio-Temporal\n  Consistent Video Generation', 'url': 'https://huggingface.co/papers/2503.06053', 'abstract': 'Spatio-temporal consistency is a critical research topic in video generation. A qualified generated video segment must ensure plot plausibility and coherence while maintaining visual consistency of objects and scenes across varying viewpoints. Prior research, especially in open-source projects, primarily focuses on either temporal or spatial consistency, or their basic combination, such as appending a description of a camera movement after a prompt without constraining the outcomes of this movement. However, camera movement may introduce new objects to the scene or eliminate existing ones, thereby overlaying and affecting the preceding narrative. Especially in videos with numerous camera movements, the interplay between multiple plots becomes increasingly complex. This paper introduces and examines integral spatio-temporal consistency, considering the synergy between plot progression and camera techniques, and the long-term impact of prior content on subsequent generation. Our research encompasses dataset construction through to the development of the model. Initially, we constructed a DropletVideo-10M dataset, which comprises 10 million videos featuring dynamic camera motion and object actions. Each video is annotated with an average caption of 206 words, detailing various camera movements and plot developments. Following this, we developed and trained the DropletVideo model, which excels in preserving spatio-temporal coherence during video generation. The DropletVideo dataset and model are accessible at https://dropletx.github.io.', 'score': 72, 'issue_id': 2758, 'pub_date': '2025-03-08', 'pub_date_card': {'ru': '8 марта', 'en': 'March 8', 'zh': '3月8日'}, 'hash': 'c6a544d3dc36bfbf', 'authors': ['Runze Zhang', 'Guoguang Du', 'Xiaochuan Li', 'Qi Jia', 'Liang Jin', 'Lu Liu', 'Jingjing Wang', 'Cong Xu', 'Zhenhua Guo', 'Yaqian Zhao', 'Xiaoli Gong', 'Rengang Li', 'Baoyu Fan'], 'affiliations': ['IEIT System Co., Ltd.', 'Nankai University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.06053.jpg', 'data': {'categories': ['#open_source', '#dataset', '#video', '#training'], 'emoji': '🎥', 'ru': {'title': 'Интегральная пространственно-временная согласованность в генерации видео', 'desc': 'Статья посвящена проблеме пространственно-временной согласованности в генерации видео. Авторы представляют новый подход, учитывающий взаимосвязь между развитием сюжета и движениями камеры, а также долгосрочное влияние предыдущего контента на последующую генерацию. Для исследования был создан датасет DropletVideo-10M, содержащий 10 миллионов видео с динамическим движением камеры и действиями объектов. На основе этих данных была разработана и обучена модель DropletVideo, способная сохранять пространственно-временную согласованность при генерации видео.'}, 'en': {'title': 'Achieving Seamless Video Generation with Spatio-Temporal Consistency', 'desc': 'This paper addresses the challenge of spatio-temporal consistency in video generation, which is essential for creating coherent and visually consistent narratives. It highlights the limitations of previous research that often focuses on either temporal or spatial aspects without integrating them effectively. The authors introduce a new dataset, DropletVideo-10M, containing 10 million videos with dynamic camera movements and detailed annotations, which aids in training their model. The proposed DropletVideo model demonstrates improved performance in maintaining coherence across both plot progression and camera techniques, ensuring a more seamless video generation experience.'}, 'zh': {'title': '提升视频生成的时空一致性', 'desc': '本论文研究了视频生成中的时空一致性问题。生成的视频片段需要在情节合理性和视觉一致性之间取得平衡，尤其是在不同视角下的物体和场景。以往的研究主要关注时间或空间一致性，缺乏对两者的综合考虑。我们提出了整体时空一致性的方法，构建了包含1000万段动态镜头和物体动作的视频数据集，并开发了DropletVideo模型，以提高视频生成的时空连贯性。'}}}, {'id': 'https://huggingface.co/papers/2503.12533', 'title': 'Being-0: A Humanoid Robotic Agent with Vision-Language Models and\n  Modular Skills', 'url': 'https://huggingface.co/papers/2503.12533', 'abstract': "Building autonomous robotic agents capable of achieving human-level performance in real-world embodied tasks is an ultimate goal in humanoid robot research. Recent advances have made significant progress in high-level cognition with Foundation Models (FMs) and low-level skill development for humanoid robots. However, directly combining these components often results in poor robustness and efficiency due to compounding errors in long-horizon tasks and the varied latency of different modules. We introduce Being-0, a hierarchical agent framework that integrates an FM with a modular skill library. The FM handles high-level cognitive tasks such as instruction understanding, task planning, and reasoning, while the skill library provides stable locomotion and dexterous manipulation for low-level control. To bridge the gap between these levels, we propose a novel Connector module, powered by a lightweight vision-language model (VLM). The Connector enhances the FM's embodied capabilities by translating language-based plans into actionable skill commands and dynamically coordinating locomotion and manipulation to improve task success. With all components, except the FM, deployable on low-cost onboard computation devices, Being-0 achieves efficient, real-time performance on a full-sized humanoid robot equipped with dexterous hands and active vision. Extensive experiments in large indoor environments demonstrate Being-0's effectiveness in solving complex, long-horizon tasks that require challenging navigation and manipulation subtasks. For further details and videos, visit https://beingbeyond.github.io/being-0.", 'score': 49, 'issue_id': 2755, 'pub_date': '2025-03-16', 'pub_date_card': {'ru': '16 марта', 'en': 'March 16', 'zh': '3月16日'}, 'hash': '18b42d1274f6b260', 'authors': ['Haoqi Yuan', 'Yu Bai', 'Yuhui Fu', 'Bohan Zhou', 'Yicheng Feng', 'Xinrun Xu', 'Yi Zhan', 'Börje F. Karlsson', 'Zongqing Lu'], 'affiliations': ['BAAI', 'BeingBeyond', 'Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2503.12533.jpg', 'data': {'categories': ['#reasoning', '#agents', '#optimization', '#robotics', '#agi', '#multimodal'], 'emoji': '🤖', 'ru': {'title': 'Being-0: Мост между AI и роботом для решения сложных задач в реальном мире', 'desc': 'Статья представляет Being-0 - иерархическую систему для автономных роботов-гуманоидов, объединяющую фундаментальную модель (ФМ) для высокоуровневых когнитивных задач с библиотекой низкоуровневых навыков. Ключевым элементом является модуль Connector на основе легковесной мультимодальной модели, связывающий ФМ с исполнительными механизмами робота. Система Being-0 способна эффективно решать сложные долгосрочные задачи, требующие навигации и манипуляций, работая в режиме реального времени на недорогом бортовом оборудовании. Эксперименты показали эффективность Being-0 в решении комплексных задач в больших помещениях.'}, 'en': {'title': 'Bridging High-Level Cognition and Low-Level Skills in Humanoid Robots', 'desc': "This paper presents Being-0, a hierarchical framework designed to enhance humanoid robots' performance in complex tasks. It combines a Foundation Model (FM) for high-level cognitive functions with a modular skill library for low-level control, addressing issues of robustness and efficiency. A novel Connector module, utilizing a lightweight vision-language model, translates language-based plans into actionable commands, improving coordination between locomotion and manipulation. The system demonstrates effective real-time performance on humanoid robots in challenging environments, showcasing its ability to tackle long-horizon tasks successfully."}, 'zh': {'title': '提升类人机器人智能的层次化框架', 'desc': '本文介绍了一种名为Being-0的层次化智能体框架，旨在提升类人机器人在现实世界任务中的表现。该框架将基础模型（FM）与模块化技能库相结合，FM负责高层次的认知任务，如指令理解和任务规划，而技能库则提供稳定的运动和灵巧操作。为了连接这两个层次，本文提出了一种新颖的连接模块，利用轻量级的视觉-语言模型（VLM）将语言计划转化为可执行的技能命令。通过在低成本的计算设备上部署大部分组件，Being-0在复杂的长时间任务中展现出高效的实时性能。'}}}, {'id': 'https://huggingface.co/papers/2503.12885', 'title': 'DreamRenderer: Taming Multi-Instance Attribute Control in Large-Scale\n  Text-to-Image Models', 'url': 'https://huggingface.co/papers/2503.12885', 'abstract': 'Image-conditioned generation methods, such as depth- and canny-conditioned approaches, have demonstrated remarkable abilities for precise image synthesis. However, existing models still struggle to accurately control the content of multiple instances (or regions). Even state-of-the-art models like FLUX and 3DIS face challenges, such as attribute leakage between instances, which limits user control. To address these issues, we introduce DreamRenderer, a training-free approach built upon the FLUX model. DreamRenderer enables users to control the content of each instance via bounding boxes or masks, while ensuring overall visual harmony. We propose two key innovations: 1) Bridge Image Tokens for Hard Text Attribute Binding, which uses replicated image tokens as bridge tokens to ensure that T5 text embeddings, pre-trained solely on text data, bind the correct visual attributes for each instance during Joint Attention; 2) Hard Image Attribute Binding applied only to vital layers. Through our analysis of FLUX, we identify the critical layers responsible for instance attribute rendering and apply Hard Image Attribute Binding only in these layers, using soft binding in the others. This approach ensures precise control while preserving image quality. Evaluations on the COCO-POS and COCO-MIG benchmarks demonstrate that DreamRenderer improves the Image Success Ratio by 17.7% over FLUX and enhances the performance of layout-to-image models like GLIGEN and 3DIS by up to 26.8%. Project Page: https://limuloo.github.io/DreamRenderer/.', 'score': 34, 'issue_id': 2754, 'pub_date': '2025-03-17', 'pub_date_card': {'ru': '17 марта', 'en': 'March 17', 'zh': '3月17日'}, 'hash': 'e6332652493dc1ab', 'authors': ['Dewei Zhou', 'Mingwei Li', 'Zongxin Yang', 'Yi Yang'], 'affiliations': ['DBMI, HMS, Harvard University', 'RELER, CCAI, Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.12885.jpg', 'data': {'categories': ['#optimization', '#cv', '#training', '#games', '#benchmark'], 'emoji': '🎨', 'ru': {'title': 'Точный контроль над множественными объектами при генерации изображений', 'desc': 'DreamRenderer - это новый подход к генерации изображений по условиям, построенный на основе модели FLUX. Он позволяет точно контролировать содержимое нескольких объектов на изображении с помощью ограничивающих рамок или масок. Ключевые инновации включают использование мостовых токенов изображения для жесткой привязки текстовых атрибутов и применение жесткой привязки атрибутов изображения только в критически важных слоях. Эксперименты показали значительное улучшение качества генерации по сравнению с существующими методами.'}, 'en': {'title': 'DreamRenderer: Precise Control in Image Generation', 'desc': 'This paper presents DreamRenderer, a novel approach for image generation that allows precise control over multiple instances in an image. Unlike existing models, DreamRenderer uses a training-free method that leverages the FLUX model to bind visual attributes to specific instances through bounding boxes or masks. The key innovations include Bridge Image Tokens for ensuring accurate text-to-image attribute mapping and Hard Image Attribute Binding focused on critical layers for instance rendering. Evaluations show that DreamRenderer significantly outperforms FLUX and enhances other layout-to-image models, demonstrating its effectiveness in generating high-quality images with user-defined content.'}, 'zh': {'title': 'DreamRenderer：精确控制图像生成的创新方法', 'desc': '本文介绍了一种名为DreamRenderer的图像生成方法，旨在解决现有模型在多实例内容控制方面的不足。DreamRenderer基于FLUX模型，允许用户通过边界框或掩码精确控制每个实例的内容，同时保持整体视觉和谐。我们提出了两项关键创新：一是使用桥接图像标记来确保文本属性的准确绑定，二是在关键层中应用硬图像属性绑定，以提高实例属性渲染的精度。实验结果表明，DreamRenderer在图像成功率上比FLUX提高了17.7%，并且在布局到图像模型的性能上提升了多达26.8%。'}}}, {'id': 'https://huggingface.co/papers/2503.12590', 'title': 'Personalize Anything for Free with Diffusion Transformer', 'url': 'https://huggingface.co/papers/2503.12590', 'abstract': 'Personalized image generation aims to produce images of user-specified concepts while enabling flexible editing. Recent training-free approaches, while exhibit higher computational efficiency than training-based methods, struggle with identity preservation, applicability, and compatibility with diffusion transformers (DiTs). In this paper, we uncover the untapped potential of DiT, where simply replacing denoising tokens with those of a reference subject achieves zero-shot subject reconstruction. This simple yet effective feature injection technique unlocks diverse scenarios, from personalization to image editing. Building upon this observation, we propose Personalize Anything, a training-free framework that achieves personalized image generation in DiT through: 1) timestep-adaptive token replacement that enforces subject consistency via early-stage injection and enhances flexibility through late-stage regularization, and 2) patch perturbation strategies to boost structural diversity. Our method seamlessly supports layout-guided generation, multi-subject personalization, and mask-controlled editing. Evaluations demonstrate state-of-the-art performance in identity preservation and versatility. Our work establishes new insights into DiTs while delivering a practical paradigm for efficient personalization.', 'score': 22, 'issue_id': 2754, 'pub_date': '2025-03-16', 'pub_date_card': {'ru': '16 марта', 'en': 'March 16', 'zh': '3月16日'}, 'hash': '25e3ee07c4a11ed2', 'authors': ['Haoran Feng', 'Zehuan Huang', 'Lin Li', 'Hairong Lv', 'Lu Sheng'], 'affiliations': ['Beihang University', 'Renmin University of China', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.12590.jpg', 'data': {'categories': ['#cv', '#multimodal', '#diffusion'], 'emoji': '🎨', 'ru': {'title': 'Персонализация изображений без обучения: новый взгляд на возможности диффузионных трансформеров', 'desc': 'Статья представляет новый подход к персонализированной генерации изображений с использованием диффузионных трансформеров (DiT). Авторы предлагают метод Personalize Anything, который позволяет достичь высокого качества персонализации без дополнительного обучения модели. Ключевые элементы метода включают адаптивную замену токенов и стратегии возмущения патчей для улучшения структурного разнообразия. Метод демонстрирует высокую эффективность в сохранении идентичности и универсальность применения.'}, 'en': {'title': 'Personalize Anything: Efficient Image Generation with Diffusion Transformers', 'desc': 'This paper presents a novel approach to personalized image generation using diffusion transformers (DiTs) without the need for extensive training. The authors introduce a technique that replaces denoising tokens with those from a reference subject, enabling effective zero-shot subject reconstruction. They propose a framework called Personalize Anything, which incorporates adaptive token replacement and patch perturbation strategies to enhance identity preservation and structural diversity. The results show that this method achieves state-of-the-art performance in generating personalized images while allowing for flexible editing options.'}, 'zh': {'title': '个性化图像生成的新视角', 'desc': '个性化图像生成旨在根据用户指定的概念生成图像，并允许灵活编辑。最近的无训练方法在计算效率上优于基于训练的方法，但在身份保持、适用性和与扩散变换器的兼容性方面存在挑战。本文揭示了扩散变换器的潜力，通过简单地用参考对象的去噪令牌替换实现零-shot的对象重建。我们提出的“个性化任何事物”框架，通过时间步自适应令牌替换和补丁扰动策略，实现了高效的个性化图像生成。'}}}, {'id': 'https://huggingface.co/papers/2503.12349', 'title': 'SPIN-Bench: How Well Do LLMs Plan Strategically and Reason Socially?', 'url': 'https://huggingface.co/papers/2503.12349', 'abstract': 'Reasoning and strategic behavior in social interactions is a hallmark of intelligence. This form of reasoning is significantly more sophisticated than isolated planning or reasoning tasks in static settings (e.g., math problem solving). In this paper, we present Strategic Planning, Interaction, and Negotiation (SPIN-Bench), a new multi-domain evaluation designed to measure the intelligence of strategic planning and social reasoning. While many existing benchmarks focus on narrow planning or single-agent reasoning, SPIN-Bench combines classical PDDL tasks, competitive board games, cooperative card games, and multi-agent negotiation scenarios in one unified framework. The framework includes both a benchmark as well as an arena to simulate and evaluate the variety of social settings to test reasoning and strategic behavior of AI agents. We formulate the benchmark SPIN-Bench by systematically varying action spaces, state complexity, and the number of interacting agents to simulate a variety of social settings where success depends on not only methodical and step-wise decision making, but also conceptual inference of other (adversarial or cooperative) participants. Our experiments reveal that while contemporary LLMs handle basic fact retrieval and short-range planning reasonably well, they encounter significant performance bottlenecks in tasks requiring deep multi-hop reasoning over large state spaces and socially adept coordination under uncertainty. We envision SPIN-Bench as a catalyst for future research on robust multi-agent planning, social reasoning, and human--AI teaming.', 'score': 22, 'issue_id': 2765, 'pub_date': '2025-03-16', 'pub_date_card': {'ru': '16 марта', 'en': 'March 16', 'zh': '3月16日'}, 'hash': '5f862770d0575514', 'authors': ['Jianzhu Yao', 'Kevin Wang', 'Ryan Hsieh', 'Haisu Zhou', 'Tianqing Zou', 'Zerui Cheng', 'Zhangyang Wang', 'Pramod Viswanath'], 'affiliations': ['Department of Electrical and Computer Engineering, Princeton University, New Jersey, US', 'Department of Electrical and Computer Engineering, The University of Texas at Austin, Austin, US'], 'pdf_title_img': 'assets/pdf/title_img/2503.12349.jpg', 'data': {'categories': ['#games', '#agents', '#benchmark', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'SPIN-Bench: Оценка стратегического и социального интеллекта ИИ', 'desc': 'SPIN-Bench - это новый многодоменный фреймворк для оценки стратегического планирования и социального мышления искусственного интеллекта. Он объединяет классические задачи PDDL, конкурентные настольные игры, кооперативные карточные игры и сценарии многоагентных переговоров. SPIN-Bench систематически варьирует пространства действий, сложность состояний и количество взаимодействующих агентов для симуляции различных социальных ситуаций. Эксперименты показывают, что современные языковые модели справляются с базовым извлечением фактов и краткосрочным планированием, но испытывают трудности с задачами, требующими глубокого многоступенчатого рассуждения и социально адаптивной координации в условиях неопределенности.'}, 'en': {'title': "SPIN-Bench: Evaluating AI's Strategic and Social Intelligence", 'desc': "This paper introduces SPIN-Bench, a new evaluation framework designed to assess AI's ability in strategic planning and social reasoning across various domains. Unlike existing benchmarks that focus on single-agent tasks, SPIN-Bench integrates multiple scenarios, including competitive games and cooperative interactions, to provide a comprehensive testing ground. The framework systematically varies factors like action spaces and the number of agents to simulate complex social environments where success relies on both strategic decision-making and understanding others' behaviors. The findings indicate that while current large language models perform well in basic tasks, they struggle with complex reasoning and coordination in uncertain social contexts."}, 'zh': {'title': '智能推理与战略行为的新基准', 'desc': '本文介绍了一种新的多领域评估工具，称为战略规划、互动与谈判基准（SPIN-Bench），旨在测量战略规划和社会推理的智能水平。与现有的基准不同，SPIN-Bench结合了经典的PDDL任务、竞争性棋盘游戏、合作性纸牌游戏和多智能体谈判场景，提供了一个统一的框架。该框架不仅包括基准测试，还提供了一个模拟和评估各种社会环境的场所，以测试人工智能代理的推理和战略行为。实验结果表明，尽管当前的大型语言模型在基本事实检索和短期规划方面表现良好，但在需要深度多跳推理和不确定性下的社会协调任务中却遇到了显著的性能瓶颈。'}}}, {'id': 'https://huggingface.co/papers/2503.13327', 'title': 'Edit Transfer: Learning Image Editing via Vision In-Context Relations', 'url': 'https://huggingface.co/papers/2503.13327', 'abstract': 'We introduce a new setting, Edit Transfer, where a model learns a transformation from just a single source-target example and applies it to a new query image. While text-based methods excel at semantic manipulations through textual prompts, they often struggle with precise geometric details (e.g., poses and viewpoint changes). Reference-based editing, on the other hand, typically focuses on style or appearance and fails at non-rigid transformations. By explicitly learning the editing transformation from a source-target pair, Edit Transfer mitigates the limitations of both text-only and appearance-centric references. Drawing inspiration from in-context learning in large language models, we propose a visual relation in-context learning paradigm, building upon a DiT-based text-to-image model. We arrange the edited example and the query image into a unified four-panel composite, then apply lightweight LoRA fine-tuning to capture complex spatial transformations from minimal examples. Despite using only 42 training samples, Edit Transfer substantially outperforms state-of-the-art TIE and RIE methods on diverse non-rigid scenarios, demonstrating the effectiveness of few-shot visual relation learning.', 'score': 21, 'issue_id': 2754, 'pub_date': '2025-03-17', 'pub_date_card': {'ru': '17 марта', 'en': 'March 17', 'zh': '3月17日'}, 'hash': '24a5e5d1d86949d5', 'authors': ['Lan Chen', 'Qi Mao', 'Yuchao Gu', 'Mike Zheng Shou'], 'affiliations': ['MIPG, Communication University of China', 'Show Lab, National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2503.13327.jpg', 'data': {'categories': ['#transfer_learning', '#dataset', '#cv', '#training'], 'emoji': '🖼️', 'ru': {'title': 'Передача редактирования: обучение визуальным преобразованиям по одному примеру', 'desc': "Статья представляет новый подход в машинном обучении под названием 'Edit Transfer'. Этот метод позволяет модели изучать преобразование на основе всего одного примера исходного и целевого изображения, а затем применять его к новому запросу. Авторы предлагают парадигму обучения визуальным отношениям в контексте, основанную на модели преобразования текста в изображение DiT. Несмотря на использование всего 42 обучающих образцов, Edit Transfer значительно превосходит современные методы TIE и RIE в различных сценариях нежесткого преобразования."}, 'en': {'title': 'Transforming Images with Just One Example!', 'desc': 'This paper presents a novel approach called Edit Transfer, which enables a model to learn how to transform images using just one example of a source and target image. Unlike traditional text-based methods that struggle with geometric details and reference-based methods that focus on style, Edit Transfer effectively captures complex spatial transformations. The method utilizes a visual relation in-context learning paradigm, inspired by large language models, and employs a DiT-based text-to-image model. Remarkably, it achieves superior performance in non-rigid scenarios with only 42 training samples, showcasing the power of few-shot learning in visual transformations.'}, 'zh': {'title': '编辑转移：少样本学习的突破', 'desc': '我们提出了一种新的设置，称为编辑转移（Edit Transfer），模型通过单一的源-目标示例学习变换，并将其应用于新的查询图像。与文本方法在语义操作上表现优异但在几何细节上存在困难不同，编辑转移通过明确学习源-目标对的编辑变换，克服了文本和外观参考的局限性。我们借鉴大型语言模型中的上下文学习，提出了一种视觉关系上下文学习范式，并在DiT基础的文本到图像模型上进行构建。尽管仅使用42个训练样本，编辑转移在多样的非刚性场景中显著超越了现有的最先进方法，展示了少样本视觉关系学习的有效性。'}}}, {'id': 'https://huggingface.co/papers/2503.13434', 'title': 'BlobCtrl: A Unified and Flexible Framework for Element-level Image\n  Generation and Editing', 'url': 'https://huggingface.co/papers/2503.13434', 'abstract': 'Element-level visual manipulation is essential in digital content creation, but current diffusion-based methods lack the precision and flexibility of traditional tools. In this work, we introduce BlobCtrl, a framework that unifies element-level generation and editing using a probabilistic blob-based representation. By employing blobs as visual primitives, our approach effectively decouples and represents spatial location, semantic content, and identity information, enabling precise element-level manipulation. Our key contributions include: 1) a dual-branch diffusion architecture with hierarchical feature fusion for seamless foreground-background integration; 2) a self-supervised training paradigm with tailored data augmentation and score functions; and 3) controllable dropout strategies to balance fidelity and diversity. To support further research, we introduce BlobData for large-scale training and BlobBench for systematic evaluation. Experiments show that BlobCtrl excels in various element-level manipulation tasks while maintaining computational efficiency, offering a practical solution for precise and flexible visual content creation. Project page: https://liyaowei-stu.github.io/project/BlobCtrl/', 'score': 19, 'issue_id': 2758, 'pub_date': '2025-03-17', 'pub_date_card': {'ru': '17 марта', 'en': 'March 17', 'zh': '3月17日'}, 'hash': 'c8ae2d6baee8bf12', 'authors': ['Yaowei Li', 'Lingen Li', 'Zhaoyang Zhang', 'Xiaoyu Li', 'Guangzhi Wang', 'Hongxiang Li', 'Xiaodong Cun', 'Ying Shan', 'Yuexian Zou'], 'affiliations': ['ARC Lab, Tencent PCG', 'Peking University', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2503.13434.jpg', 'data': {'categories': ['#benchmark', '#diffusion', '#cv', '#open_source', '#dataset', '#training'], 'emoji': '🎨', 'ru': {'title': 'Точное управление элементами изображения с помощью блобов', 'desc': "BlobCtrl - это новая система для точного управления элементами изображения в цифровом контенте. Она использует вероятностное представление на основе 'блобов', что позволяет отделить пространственное положение, семантическое содержание и информацию об идентичности объектов. Архитектура включает двухветвевую диффузионную модель с иерархическим слиянием признаков и самоконтролируемое обучение. BlobCtrl превосходит существующие методы в задачах манипуляции элементами изображения, сохраняя при этом вычислительную эффективность."}, 'en': {'title': 'Precision and Flexibility in Visual Manipulation with BlobCtrl', 'desc': 'BlobCtrl is a new framework designed for element-level visual manipulation in digital content creation, addressing the limitations of current diffusion-based methods. It uses a probabilistic blob-based representation to separate and manage spatial location, semantic content, and identity, allowing for precise editing of visual elements. The framework features a dual-branch diffusion architecture that integrates foreground and background seamlessly, along with a self-supervised training approach that enhances model performance through tailored data augmentation. Additionally, BlobCtrl introduces BlobData for extensive training and BlobBench for evaluation, demonstrating superior efficiency and effectiveness in various manipulation tasks.'}, 'zh': {'title': 'BlobCtrl：精确灵活的视觉内容创作新方法', 'desc': '本文介绍了一种名为BlobCtrl的框架，旨在提高数字内容创作中元素级视觉操作的精确性和灵活性。该框架采用基于概率的blob表示，能够有效解耦空间位置、语义内容和身份信息，从而实现精确的元素级操作。我们提出了双分支扩散架构和自监督训练范式，以增强前景和背景的无缝集成，并引入可控的dropout策略来平衡保真度和多样性。实验结果表明，BlobCtrl在多种元素级操作任务中表现优异，同时保持计算效率，为精确和灵活的视觉内容创作提供了实用解决方案。'}}}, {'id': 'https://huggingface.co/papers/2503.13435', 'title': 'WideRange4D: Enabling High-Quality 4D Reconstruction with Wide-Range\n  Movements and Scenes', 'url': 'https://huggingface.co/papers/2503.13435', 'abstract': 'With the rapid development of 3D reconstruction technology, research in 4D reconstruction is also advancing, existing 4D reconstruction methods can generate high-quality 4D scenes. However, due to the challenges in acquiring multi-view video data, the current 4D reconstruction benchmarks mainly display actions performed in place, such as dancing, within limited scenarios. In practical scenarios, many scenes involve wide-range spatial movements, highlighting the limitations of existing 4D reconstruction datasets. Additionally, existing 4D reconstruction methods rely on deformation fields to estimate the dynamics of 3D objects, but deformation fields struggle with wide-range spatial movements, which limits the ability to achieve high-quality 4D scene reconstruction with wide-range spatial movements. In this paper, we focus on 4D scene reconstruction with significant object spatial movements and propose a novel 4D reconstruction benchmark, WideRange4D. This benchmark includes rich 4D scene data with large spatial variations, allowing for a more comprehensive evaluation of the generation capabilities of 4D generation methods. Furthermore, we introduce a new 4D reconstruction method, Progress4D, which generates stable and high-quality 4D results across various complex 4D scene reconstruction tasks. We conduct both quantitative and qualitative comparison experiments on WideRange4D, showing that our Progress4D outperforms existing state-of-the-art 4D reconstruction methods. Project: https://github.com/Gen-Verse/WideRange4D', 'score': 16, 'issue_id': 2754, 'pub_date': '2025-03-17', 'pub_date_card': {'ru': '17 марта', 'en': 'March 17', 'zh': '3月17日'}, 'hash': 'c17d4be710ed24e0', 'authors': ['Ling Yang', 'Kaixin Zhu', 'Juanxi Tian', 'Bohan Zeng', 'Mingbao Lin', 'Hongjuan Pei', 'Wentao Zhang', 'Shuicheng Yan'], 'affiliations': ['National University of Singapore', 'Peking University', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2503.13435.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#3d'], 'emoji': '🌀', 'ru': {'title': 'Прорыв в 4D-реконструкции: от статики к динамике', 'desc': 'Статья представляет новый бенчмарк WideRange4D для 4D-реконструкции сцен с широким диапазоном пространственных движений. Авторы также предлагают новый метод 4D-реконструкции под названием Progress4D, который генерирует стабильные и высококачественные 4D-результаты для различных сложных задач реконструкции 4D-сцен. Метод Progress4D превосходит существующие передовые методы 4D-реконструкции в количественных и качественных сравнительных экспериментах на WideRange4D. Работа направлена на преодоление ограничений существующих методов 4D-реконструкции, которые плохо справляются с широкомасштабными пространственными движениями.'}, 'en': {'title': 'Advancing 4D Reconstruction with Wide Spatial Movements', 'desc': 'This paper addresses the limitations of current 4D reconstruction methods, which primarily focus on actions performed in place and struggle with wide-range spatial movements. The authors introduce a new benchmark called WideRange4D, which includes diverse 4D scene data featuring significant spatial variations, enabling better evaluation of 4D generation techniques. They also propose a novel reconstruction method named Progress4D, designed to produce stable and high-quality 4D results across complex scenarios. Experimental results demonstrate that Progress4D surpasses existing state-of-the-art methods in both quantitative and qualitative assessments.'}, 'zh': {'title': '突破空间限制，实现高质量4D重建', 'desc': '随着3D重建技术的快速发展，4D重建研究也在不断进步。现有的4D重建方法能够生成高质量的4D场景，但在获取多视角视频数据方面面临挑战，导致现有基准主要展示有限场景中的动作。本文提出了一个新的4D重建基准WideRange4D，包含丰富的4D场景数据，允许对4D生成方法的能力进行更全面的评估。同时，我们引入了一种新的4D重建方法Progress4D，在各种复杂的4D场景重建任务中生成稳定且高质量的结果。'}}}, {'id': 'https://huggingface.co/papers/2503.13399', 'title': 'MicroVQA: A Multimodal Reasoning Benchmark for Microscopy-Based\n  Scientific Research', 'url': 'https://huggingface.co/papers/2503.13399', 'abstract': "Scientific research demands sophisticated reasoning over multimodal data, a challenge especially prevalent in biology. Despite recent advances in multimodal large language models (MLLMs) for AI-assisted research, existing multimodal reasoning benchmarks only target up to college-level difficulty, while research-level benchmarks emphasize lower-level perception, falling short of the complex multimodal reasoning needed for scientific discovery. To bridge this gap, we introduce MicroVQA, a visual-question answering (VQA) benchmark designed to assess three reasoning capabilities vital in research workflows: expert image understanding, hypothesis generation, and experiment proposal. MicroVQA consists of 1,042 multiple-choice questions (MCQs) curated by biology experts across diverse microscopy modalities, ensuring VQA samples represent real scientific practice. In constructing the benchmark, we find that standard MCQ generation methods induce language shortcuts, motivating a new two-stage pipeline: an optimized LLM prompt structures question-answer pairs into MCQs; then, an agent-based `RefineBot' updates them to remove shortcuts. Benchmarking on state-of-the-art MLLMs reveal a peak performance of 53\\%; models with smaller LLMs only slightly underperform top models, suggesting that language-based reasoning is less challenging than multimodal reasoning; and tuning with scientific articles enhances performance. Expert analysis of chain-of-thought responses shows that perception errors are the most frequent, followed by knowledge errors and then overgeneralization errors. These insights highlight the challenges in multimodal scientific reasoning, showing MicroVQA is a valuable resource advancing AI-driven biomedical research. MicroVQA is available at https://huggingface.co/datasets/jmhb/microvqa, and project page at https://jmhb0.github.io/microvqa.", 'score': 16, 'issue_id': 2754, 'pub_date': '2025-03-17', 'pub_date_card': {'ru': '17 марта', 'en': 'March 17', 'zh': '3月17日'}, 'hash': '50d4f1f510eff333', 'authors': ['James Burgess', 'Jeffrey J Nirschl', 'Laura Bravo-Sánchez', 'Alejandro Lozano', 'Sanket Rajan Gupte', 'Jesus G. Galaz-Montoya', 'Yuhui Zhang', 'Yuchang Su', 'Disha Bhowmik', 'Zachary Coman', 'Sarina M. Hasan', 'Alexandra Johannesson', 'William D. Leineweber', 'Malvika G Nair', 'Ridhi Yarlagadda', 'Connor Zuraski', 'Wah Chiu', 'Sarah Cohen', 'Jan N. Hansen', 'Manuel D Leonetti', 'Chad Liu', 'Emma Lundberg', 'Serena Yeung-Levy'], 'affiliations': ['Chan Zuckerberg Biohub Network', 'KTH Royal Institute of Technology', 'Princeton University', 'Stanford University', 'Tsinghua University', 'University of North Carolina at Chapel Hill'], 'pdf_title_img': 'assets/pdf/title_img/2503.13399.jpg', 'data': {'categories': ['#dataset', '#reasoning', '#multimodal', '#benchmark', '#science'], 'emoji': '🔬', 'ru': {'title': 'MicroVQA: Новый рубеж в оценке ИИ для научных исследований в биологии', 'desc': 'Статья представляет MicroVQA - новый бенчмарк для оценки мультимодальных языковых моделей в контексте научных исследований в биологии. Бенчмарк состоит из 1042 вопросов с множественным выбором, охватывающих различные модальности микроскопии и оценивающих понимание изображений, генерацию гипотез и предложение экспериментов. Авторы разработали двухэтапный процесс создания вопросов, чтобы избежать языковых упрощений. Тестирование современных мультимодальных моделей показало максимальную точность 53%, выявив сложности в мультимодальных рассуждениях для научных задач.'}, 'en': {'title': 'MicroVQA: Advancing Multimodal Reasoning in Scientific Discovery', 'desc': 'This paper introduces MicroVQA, a new benchmark for visual-question answering (VQA) that focuses on complex reasoning needed in scientific research, particularly in biology. It assesses three key capabilities: understanding images, generating hypotheses, and proposing experiments, using questions curated by biology experts. The benchmark consists of 1,042 multiple-choice questions designed to reflect real scientific practices, addressing the limitations of existing multimodal reasoning benchmarks. The study reveals that while current models perform at a peak of 53%, the challenges in multimodal reasoning are significant, emphasizing the need for improved AI tools in biomedical research.'}, 'zh': {'title': 'MicroVQA：推动生物医学研究的多模态推理基准', 'desc': '本论文介绍了MicroVQA，这是一个针对生物学研究的视觉问答基准，旨在评估科学研究中所需的三种推理能力：专家图像理解、假设生成和实验提案。现有的多模态大语言模型（MLLMs）在处理复杂的多模态推理时存在不足，MicroVQA通过1,042个由生物学专家策划的多项选择题来填补这一空白。研究发现，标准的多项选择题生成方法容易产生语言捷径，因此提出了一种新的两阶段流程来优化问题和答案的结构。通过对最先进的MLLMs进行基准测试，结果显示，尽管小型LLMs的表现略逊于顶级模型，但语言推理的难度低于多模态推理，这突显了在科学推理中的挑战。'}}}, {'id': 'https://huggingface.co/papers/2503.12605', 'title': 'Multimodal Chain-of-Thought Reasoning: A Comprehensive Survey', 'url': 'https://huggingface.co/papers/2503.12605', 'abstract': 'By extending the advantage of chain-of-thought (CoT) reasoning in human-like step-by-step processes to multimodal contexts, multimodal CoT (MCoT) reasoning has recently garnered significant research attention, especially in the integration with multimodal large language models (MLLMs). Existing MCoT studies design various methodologies and innovative reasoning paradigms to address the unique challenges of image, video, speech, audio, 3D, and structured data across different modalities, achieving extensive success in applications such as robotics, healthcare, autonomous driving, and multimodal generation. However, MCoT still presents distinct challenges and opportunities that require further focus to ensure consistent thriving in this field, where, unfortunately, an up-to-date review of this domain is lacking. To bridge this gap, we present the first systematic survey of MCoT reasoning, elucidating the relevant foundational concepts and definitions. We offer a comprehensive taxonomy and an in-depth analysis of current methodologies from diverse perspectives across various application scenarios. Furthermore, we provide insights into existing challenges and future research directions, aiming to foster innovation toward multimodal AGI.', 'score': 14, 'issue_id': 2762, 'pub_date': '2025-03-16', 'pub_date_card': {'ru': '16 марта', 'en': 'March 16', 'zh': '3月16日'}, 'hash': '4b0cc0276cb6afed', 'authors': ['Yaoting Wang', 'Shengqiong Wu', 'Yuecheng Zhang', 'William Wang', 'Ziwei Liu', 'Jiebo Luo', 'Hao Fei'], 'affiliations': ['CUHK', 'NTU', 'NUS', 'UCSB', 'UR'], 'pdf_title_img': 'assets/pdf/title_img/2503.12605.jpg', 'data': {'categories': ['#multimodal', '#robotics', '#healthcare', '#3d', '#video', '#agi', '#reasoning', '#survey', '#audio'], 'emoji': '🧠', 'ru': {'title': 'Мультимодальное рассуждение: шаг к AGI', 'desc': 'Статья посвящена мультимодальному рассуждению по цепочке мыслей (MCoT) в контексте мультимодальных больших языковых моделей (MLLM). Авторы представляют первый систематический обзор MCoT, объясняя основные концепции и определения. Они предлагают всестороннюю таксономию и глубокий анализ текущих методологий в различных сценариях применения. Статья также рассматривает существующие проблемы и направления будущих исследований в области мультимодального искусственного интеллекта общего назначения.'}, 'en': {'title': 'Unlocking Multimodal Reasoning for Future AI Innovations', 'desc': 'This paper introduces multimodal chain-of-thought (MCoT) reasoning, which enhances human-like reasoning processes across different types of data such as images, videos, and audio. It reviews various methodologies and innovative reasoning paradigms that have been developed to tackle the unique challenges posed by these multimodal contexts. The authors present a systematic survey that includes foundational concepts, a comprehensive taxonomy, and an analysis of current approaches in MCoT applications. Additionally, the paper highlights existing challenges and suggests future research directions to advance the field towards multimodal artificial general intelligence (AGI).'}, 'zh': {'title': '多模态推理的未来之路', 'desc': '多模态链式思维（MCoT）推理将人类的逐步推理优势扩展到多种数据类型，近年来受到广泛关注，尤其是在与多模态大型语言模型（MLLMs）的结合方面。现有的MCoT研究设计了多种方法和创新的推理范式，以应对图像、视频、语音、音频、3D和结构化数据等不同模态的独特挑战，并在机器人技术、医疗保健、自动驾驶和多模态生成等应用中取得了显著成功。尽管如此，MCoT仍面临独特的挑战和机遇，需要进一步关注，以确保该领域的持续发展。为此，我们提供了MCoT推理的首次系统性综述，阐明相关的基础概念和定义，并对当前方法进行了全面的分类和深入分析。'}}}, {'id': 'https://huggingface.co/papers/2503.11751', 'title': 'reWordBench: Benchmarking and Improving the Robustness of Reward Models\n  with Transformed Inputs', 'url': 'https://huggingface.co/papers/2503.11751', 'abstract': 'Reward models have become a staple in modern NLP, serving as not only a scalable text evaluator, but also an indispensable component in many alignment recipes and inference-time algorithms. However, while recent reward models increase performance on standard benchmarks, this may partly be due to overfitting effects, which would confound an understanding of their true capability. In this work, we scrutinize the robustness of reward models and the extent of such overfitting. We build **reWordBench**, which systematically transforms reward model inputs in meaning- or ranking-preserving ways. We show that state-of-the-art reward models suffer from substantial performance degradation even with minor input transformations, sometimes dropping to significantly below-random accuracy, suggesting brittleness. To improve reward model robustness, we propose to explicitly train them to assign similar scores to paraphrases, and find that this approach also improves robustness to other distinct kinds of transformations. For example, our robust reward model reduces such degradation by roughly half for the Chat Hard subset in RewardBench. Furthermore, when used in alignment, our robust reward models demonstrate better utility and lead to higher-quality outputs, winning in up to 59% of instances against a standardly trained RM.', 'score': 14, 'issue_id': 2755, 'pub_date': '2025-03-14', 'pub_date_card': {'ru': '14 марта', 'en': 'March 14', 'zh': '3月14日'}, 'hash': '0adbcb6b7ba858f8', 'authors': ['Zhaofeng Wu', 'Michihiro Yasunaga', 'Andrew Cohen', 'Yoon Kim', 'Asli Celikyilmaz', 'Marjan Ghazvininejad'], 'affiliations': ['FAIR at Meta', 'MIT'], 'pdf_title_img': 'assets/pdf/title_img/2503.11751.jpg', 'data': {'categories': ['#benchmark', '#hallucinations', '#training', '#alignment', '#rlhf'], 'emoji': '🔬', 'ru': {'title': 'Повышение устойчивости моделей вознаграждения в NLP', 'desc': 'Эта статья исследует устойчивость моделей вознаграждения (reward models) в обработке естественного языка. Авторы создали набор данных reWordBench для систематического тестирования этих моделей путем трансформации входных данных. Результаты показывают, что современные модели вознаграждения существенно теряют в производительности даже при незначительных изменениях входных данных. Для повышения устойчивости авторы предлагают обучать модели присваивать схожие оценки парафразам, что также улучшает устойчивость к другим видам трансформаций.'}, 'en': {'title': 'Enhancing Robustness in Reward Models for NLP', 'desc': 'This paper investigates the reliability of reward models in natural language processing (NLP), which are crucial for evaluating text and enhancing model alignment. The authors introduce **reWordBench**, a tool that modifies inputs to test the robustness of these models against overfitting. Their findings reveal that many state-of-the-art reward models perform poorly when faced with slight input changes, indicating a lack of robustness. To address this issue, they propose training reward models to provide consistent scores for paraphrased inputs, resulting in improved performance and higher-quality outputs during alignment tasks.'}, 'zh': {'title': '提升奖励模型鲁棒性，减少过拟合影响', 'desc': '奖励模型在现代自然语言处理（NLP）中扮演着重要角色，既是可扩展的文本评估工具，也是许多对齐算法和推理时算法的关键组成部分。尽管最近的奖励模型在标准基准测试中表现出色，但这可能部分是由于过拟合现象，影响了对其真实能力的理解。我们构建了reWordBench，系统地对奖励模型输入进行意义或排名保持的转换，发现即使是微小的输入变化，最先进的奖励模型也会出现显著的性能下降，显示出其脆弱性。为提高奖励模型的鲁棒性，我们提出显式训练模型对同义句赋予相似分数，这种方法也改善了模型对其他不同类型转换的鲁棒性。'}}}, {'id': 'https://huggingface.co/papers/2503.12937', 'title': 'R1-VL: Learning to Reason with Multimodal Large Language Models via\n  Step-wise Group Relative Policy Optimization', 'url': 'https://huggingface.co/papers/2503.12937', 'abstract': "Recent studies generally enhance MLLMs' reasoning capabilities via supervised fine-tuning on high-quality chain-of-thought reasoning data, which often leads models to merely imitate successful reasoning paths without understanding what the wrong reasoning paths are. In this work, we aim to enhance the MLLMs' reasoning ability beyond passively imitating positive reasoning paths. To this end, we design Step-wise Group Relative Policy Optimization (StepGRPO), a new online reinforcement learning framework that enables MLLMs to self-improve reasoning ability via simple, effective and dense step-wise rewarding. Specifically, StepGRPO introduces two novel rule-based reasoning rewards: Step-wise Reasoning Accuracy Reward (StepRAR) and Step-wise Reasoning Validity Reward (StepRVR). StepRAR rewards the reasoning paths that contain necessary intermediate reasoning steps via a soft key-step matching technique, while StepRAR rewards reasoning paths that follow a well-structured and logically consistent reasoning process through a reasoning completeness and logic evaluation strategy. With the proposed StepGRPO, we introduce R1-VL, a series of MLLMs with outstanding capabilities in step-by-step reasoning. Extensive experiments over 8 benchmarks demonstrate the superiority of our methods.", 'score': 13, 'issue_id': 2755, 'pub_date': '2025-03-17', 'pub_date_card': {'ru': '17 марта', 'en': 'March 17', 'zh': '3月17日'}, 'hash': 'dbdf6970963a4489', 'authors': ['Jingyi Zhang', 'Jiaxing Huang', 'Huanjin Yao', 'Shunyu Liu', 'Xikun Zhang', 'Shijian Lu', 'Dacheng Tao'], 'affiliations': ['Nanyang Technological University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.12937.jpg', 'data': {'categories': ['#training', '#reasoning', '#rl'], 'emoji': '🧠', 'ru': {'title': 'Самосовершенствование языковых моделей через пошаговое обучение с подкреплением', 'desc': 'Это исследование представляет новый подход к улучшению способностей мультимодальных языковых моделей (MLLM) к рассуждению. Авторы разработали метод Step-wise Group Relative Policy Optimization (StepGRPO), который использует онлайн-обучение с подкреплением для самосовершенствования моделей. StepGRPO вводит два новых вознаграждения на основе правил: Step-wise Reasoning Accuracy Reward (StepRAR) и Step-wise Reasoning Validity Reward (StepRVR). Эксперименты на 8 бенчмарках показали превосходство предложенного метода.'}, 'en': {'title': 'Empowering MLLMs with Step-wise Reasoning Rewards', 'desc': 'This paper presents a new approach to improve the reasoning abilities of machine learning language models (MLLMs) by using reinforcement learning. The proposed method, called Step-wise Group Relative Policy Optimization (StepGRPO), focuses on rewarding MLLMs for their reasoning steps rather than just imitating correct paths. It introduces two innovative rewards: Step-wise Reasoning Accuracy Reward (StepRAR) and Step-wise Reasoning Validity Reward (StepRVR), which encourage models to follow logical reasoning processes. The results show that MLLMs trained with StepGRPO, referred to as R1-VL, perform significantly better in step-by-step reasoning tasks across multiple benchmarks.'}, 'zh': {'title': '提升推理能力的创新方法', 'desc': '最近的研究通常通过在高质量的推理数据上进行监督微调来增强多语言大模型（MLLMs）的推理能力，这往往导致模型仅仅模仿成功的推理路径，而不理解错误的推理路径。在这项工作中，我们旨在超越被动模仿积极推理路径，提升MLLMs的推理能力。为此，我们设计了一种新的在线强化学习框架——逐步组相对策略优化（StepGRPO），使MLLMs能够通过简单、有效和密集的逐步奖励自我提升推理能力。具体而言，StepGRPO引入了两种新颖的基于规则的推理奖励：逐步推理准确性奖励（StepRAR）和逐步推理有效性奖励（StepRVR）。'}}}, {'id': 'https://huggingface.co/papers/2503.11495', 'title': 'V-STaR: Benchmarking Video-LLMs on Video Spatio-Temporal Reasoning', 'url': 'https://huggingface.co/papers/2503.11495', 'abstract': 'Human processes video reasoning in a sequential spatio-temporal reasoning logic, we first identify the relevant frames ("when") and then analyse the spatial relationships ("where") between key objects, and finally leverage these relationships to draw inferences ("what"). However, can Video Large Language Models (Video-LLMs) also "reason through a sequential spatio-temporal logic" in videos? Existing Video-LLM benchmarks primarily focus on assessing object presence, neglecting relational reasoning. Consequently, it is difficult to measure whether a model truly comprehends object interactions (actions/events) in videos or merely relies on pre-trained "memory" of co-occurrences as biases in generating answers. In this work, we introduce a Video Spatio-Temporal Reasoning (V-STaR) benchmark to address these shortcomings. The key idea is to decompose video understanding into a Reverse Spatio-Temporal Reasoning (RSTR) task that simultaneously evaluates what objects are present, when events occur, and where they are located while capturing the underlying Chain-of-thought (CoT) logic. To support this evaluation, we construct a dataset to elicit the spatial-temporal reasoning process of Video-LLMs. It contains coarse-to-fine CoT questions generated by a semi-automated GPT-4-powered pipeline, embedding explicit reasoning chains to mimic human cognition. Experiments from 14 Video-LLMs on our V-STaR reveal significant gaps between current Video-LLMs and the needs for robust and consistent spatio-temporal reasoning.', 'score': 10, 'issue_id': 2754, 'pub_date': '2025-03-14', 'pub_date_card': {'ru': '14 марта', 'en': 'March 14', 'zh': '3月14日'}, 'hash': '93b4a63d45a11f3d', 'authors': ['Zixu Cheng', 'Jian Hu', 'Ziquan Liu', 'Chenyang Si', 'Wei Li', 'Shaogang Gong'], 'affiliations': ['Nanjing University', 'Nanyang Technological University', 'Queen Mary University of London'], 'pdf_title_img': 'assets/pdf/title_img/2503.11495.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#reasoning', '#video'], 'emoji': '🎥', 'ru': {'title': 'Новый взгляд на оценку пространственно-временного мышления видео-LLM', 'desc': 'Статья представляет новый бенчмарк V-STaR для оценки пространственно-временного рассуждения в видео-LLM моделях. Авторы предлагают декомпозировать понимание видео на задачу обратного пространственно-временного рассуждения (RSTR), оценивающую присутствие объектов, время событий и их расположение. Для этого создан датасет с вопросами, сгенерированными с помощью GPT-4, имитирующими цепочку рассуждений человека. Эксперименты на 14 видео-LLM моделях выявили значительные пробелы в их способности к надежному и последовательному пространственно-временному рассуждению.'}, 'en': {'title': 'Enhancing Video Understanding with Spatio-Temporal Reasoning', 'desc': "This paper explores how Video Large Language Models (Video-LLMs) can understand videos using a method similar to human reasoning, which involves identifying when events happen, where objects are located, and how they interact. The authors highlight that current benchmarks for Video-LLMs mainly check for object presence but fail to assess the models' ability to reason about relationships and interactions between objects. To address this, they introduce the Video Spatio-Temporal Reasoning (V-STaR) benchmark, which breaks down video understanding into a task that evaluates object presence, timing of events, and spatial relationships while capturing the reasoning process. Their findings show that there are significant gaps in the reasoning capabilities of existing Video-LLMs, indicating a need for improved models that can perform robust spatio-temporal reasoning."}, 'zh': {'title': '提升视频理解的时空推理能力', 'desc': '本论文探讨了视频大型语言模型（Video-LLMs）在视频理解中的时空推理能力。我们提出了一个新的基准，称为视频时空推理（V-STaR），旨在评估模型在识别对象、事件发生时间和空间位置方面的能力。通过构建一个包含细致推理链的问题数据集，我们模拟人类的认知过程，以便更好地评估模型的推理能力。实验结果显示，现有的Video-LLMs在时空推理方面存在显著不足，无法满足实际应用的需求。'}}}, {'id': 'https://huggingface.co/papers/2503.13082', 'title': 'Free-form language-based robotic reasoning and grasping', 'url': 'https://huggingface.co/papers/2503.13082', 'abstract': "Performing robotic grasping from a cluttered bin based on human instructions is a challenging task, as it requires understanding both the nuances of free-form language and the spatial relationships between objects. Vision-Language Models (VLMs) trained on web-scale data, such as GPT-4o, have demonstrated remarkable reasoning capabilities across both text and images. But can they truly be used for this task in a zero-shot setting? And what are their limitations? In this paper, we explore these research questions via the free-form language-based robotic grasping task, and propose a novel method, FreeGrasp, leveraging the pre-trained VLMs' world knowledge to reason about human instructions and object spatial arrangements. Our method detects all objects as keypoints and uses these keypoints to annotate marks on images, aiming to facilitate GPT-4o's zero-shot spatial reasoning. This allows our method to determine whether a requested object is directly graspable or if other objects must be grasped and removed first. Since no existing dataset is specifically designed for this task, we introduce a synthetic dataset FreeGraspData by extending the MetaGraspNetV2 dataset with human-annotated instructions and ground-truth grasping sequences. We conduct extensive analyses with both FreeGraspData and real-world validation with a gripper-equipped robotic arm, demonstrating state-of-the-art performance in grasp reasoning and execution. Project website: https://tev-fbk.github.io/FreeGrasp/.", 'score': 9, 'issue_id': 2762, 'pub_date': '2025-03-17', 'pub_date_card': {'ru': '17 марта', 'en': 'March 17', 'zh': '3月17日'}, 'hash': 'c29bd4c3e364d62c', 'authors': ['Runyu Jiao', 'Alice Fasoli', 'Francesco Giuliari', 'Matteo Bortolon', 'Sergio Povoli', 'Guofeng Mei', 'Yiming Wang', 'Fabio Poiesi'], 'affiliations': ['Fondazione Bruno Kessler', 'Istituto Italiano di Tecnologia', 'University of Trento'], 'pdf_title_img': 'assets/pdf/title_img/2503.13082.jpg', 'data': {'categories': ['#dataset', '#robotics', '#agents', '#reasoning', '#cv', '#synthetic'], 'emoji': '🦾', 'ru': {'title': 'Роботы учатся хватать по-человечески', 'desc': 'Статья представляет новый метод FreeGrasp для роботизированного захвата объектов на основе инструкций на естественном языке. Метод использует предобученные мультимодальные языковые модели (VLM) для понимания инструкций и пространственных отношений между объектами. FreeGrasp определяет объекты как ключевые точки и использует их для аннотации изображений, чтобы улучшить рассуждения модели о пространственных отношениях. Авторы также создали синтетический датасет FreeGraspData для оценки производительности метода.'}, 'en': {'title': 'Empowering Robots with Language: Grasping Made Easy!', 'desc': 'This paper investigates the use of Vision-Language Models (VLMs) for robotic grasping tasks based on human instructions, particularly in cluttered environments. The authors introduce a method called FreeGrasp, which utilizes pre-trained VLMs to enhance spatial reasoning by detecting objects as keypoints and annotating them on images. They also create a new synthetic dataset, FreeGraspData, to support their research, as no existing dataset fits their needs. The results show that FreeGrasp achieves state-of-the-art performance in understanding and executing grasping tasks, demonstrating the potential of VLMs in robotics.'}, 'zh': {'title': '基于人类指令的智能抓取新方法', 'desc': '本论文探讨了基于人类指令的机器人抓取任务，尤其是在杂乱的环境中进行抓取的挑战。我们提出了一种新方法FreeGrasp，利用预训练的视觉-语言模型（VLM）来理解人类指令和物体之间的空间关系。该方法通过将所有物体检测为关键点，并在图像上标注这些关键点，来增强模型的空间推理能力。我们还创建了一个合成数据集FreeGraspData，以支持这一任务的研究，并在真实世界中验证了我们方法的有效性。'}}}, {'id': 'https://huggingface.co/papers/2503.13444', 'title': 'VideoMind: A Chain-of-LoRA Agent for Long Video Reasoning', 'url': 'https://huggingface.co/papers/2503.13444', 'abstract': 'Videos, with their unique temporal dimension, demand precise grounded understanding, where answers are directly linked to visual, interpretable evidence. Despite significant breakthroughs in reasoning capabilities within Large Language Models, multi-modal reasoning - especially for videos - remains unexplored. In this work, we introduce VideoMind, a novel video-language agent designed for temporal-grounded video understanding. VideoMind incorporates two key innovations: (i) We identify essential capabilities for video temporal reasoning and develop a role-based agentic workflow, including a planner for coordinating different roles, a grounder for temporal localization, a verifier to assess temporal interval accuracy, and an answerer for question-answering. (ii) To efficiently integrate these diverse roles, we propose a novel Chain-of-LoRA strategy, enabling seamless role-switching via lightweight LoRA adaptors while avoiding the overhead of multiple models, thus balancing efficiency and flexibility. Extensive experiments on 14 public benchmarks demonstrate that our agent achieves state-of-the-art performance on diverse video understanding tasks, including 3 on grounded video question-answering, 6 on video temporal grounding, and 5 on general video question-answering, underscoring its effectiveness in advancing video agent and long-form temporal reasoning.', 'score': 8, 'issue_id': 2755, 'pub_date': '2025-03-17', 'pub_date_card': {'ru': '17 марта', 'en': 'March 17', 'zh': '3月17日'}, 'hash': '4845903cb3dc6761', 'authors': ['Ye Liu', 'Kevin Qinghong Lin', 'Chang Wen Chen', 'Mike Zheng Shou'], 'affiliations': ['Show Lab, National University of Singapore', 'The Hong Kong Polytechnic University'], 'pdf_title_img': 'assets/pdf/title_img/2503.13444.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#agents', '#optimization', '#video', '#multimodal'], 'emoji': '🎬', 'ru': {'title': 'VideoMind: Темпоральное рассуждение по видео с помощью ролевых агентов', 'desc': 'Статья представляет VideoMind - новый видео-языковой агент для темпорально-обоснованного понимания видео. Авторы разработали ролевой подход с планировщиком, локализатором, верификатором и ответчиком для временного рассуждения по видео. Предложена стратегия Chain-of-LoRA для эффективного переключения между ролями с помощью легких LoRA-адаптеров. Эксперименты на 14 бенчмарках показали, что VideoMind достигает state-of-the-art результатов в различных задачах понимания видео.'}, 'en': {'title': 'VideoMind: Advancing Video Understanding with Temporal Reasoning', 'desc': 'This paper presents VideoMind, a new video-language agent that enhances understanding of videos by linking answers to visual evidence. It introduces a role-based workflow that includes a planner, grounder, verifier, and answerer to facilitate effective temporal reasoning in videos. The authors also propose a Chain-of-LoRA strategy, which allows for efficient role-switching without the need for multiple models, thus improving both efficiency and flexibility. The results show that VideoMind outperforms existing methods on various benchmarks, highlighting its capability in grounded video question-answering and temporal reasoning tasks.'}, 'zh': {'title': 'VideoMind：视频理解的新突破', 'desc': '本论文介绍了一种新的视频语言智能体，名为VideoMind，旨在实现视频的时间基础理解。该智能体通过角色驱动的工作流程，整合了规划者、定位者、验证者和回答者等关键角色，以提高视频的时间推理能力。为了高效整合这些角色，提出了一种新颖的Chain-of-LoRA策略，允许通过轻量级的LoRA适配器实现角色之间的无缝切换。实验结果表明，VideoMind在多项视频理解任务上表现出色，推动了视频智能体和长时序推理的发展。'}}}, {'id': 'https://huggingface.co/papers/2503.11412', 'title': 'MTV-Inpaint: Multi-Task Long Video Inpainting', 'url': 'https://huggingface.co/papers/2503.11412', 'abstract': 'Video inpainting involves modifying local regions within a video, ensuring spatial and temporal consistency. Most existing methods focus primarily on scene completion (i.e., filling missing regions) and lack the capability to insert new objects into a scene in a controllable manner. Fortunately, recent advancements in text-to-video (T2V) diffusion models pave the way for text-guided video inpainting. However, directly adapting T2V models for inpainting remains limited in unifying completion and insertion tasks, lacks input controllability, and struggles with long videos, thereby restricting their applicability and flexibility. To address these challenges, we propose MTV-Inpaint, a unified multi-task video inpainting framework capable of handling both traditional scene completion and novel object insertion tasks. To unify these distinct tasks, we design a dual-branch spatial attention mechanism in the T2V diffusion U-Net, enabling seamless integration of scene completion and object insertion within a single framework. In addition to textual guidance, MTV-Inpaint supports multimodal control by integrating various image inpainting models through our proposed image-to-video (I2V) inpainting mode. Additionally, we propose a two-stage pipeline that combines keyframe inpainting with in-between frame propagation, enabling MTV-Inpaint to effectively handle long videos with hundreds of frames. Extensive experiments demonstrate that MTV-Inpaint achieves state-of-the-art performance in both scene completion and object insertion tasks. Furthermore, it demonstrates versatility in derived applications such as multi-modal inpainting, object editing, removal, image object brush, and the ability to handle long videos. Project page: https://mtv-inpaint.github.io/.', 'score': 7, 'issue_id': 2756, 'pub_date': '2025-03-14', 'pub_date_card': {'ru': '14 марта', 'en': 'March 14', 'zh': '3月14日'}, 'hash': 'df14c3a2c0dfe3fd', 'authors': ['Shiyuan Yang', 'Zheng Gu', 'Liang Hou', 'Xin Tao', 'Pengfei Wan', 'Xiaodong Chen', 'Jing Liao'], 'affiliations': ['City University of Hong Kong', 'Kuaishou Technology', 'Shenzhen University', 'Tianjin University'], 'pdf_title_img': 'assets/pdf/title_img/2503.11412.jpg', 'data': {'categories': ['#multimodal', '#diffusion', '#video'], 'emoji': '🎬', 'ru': {'title': 'Универсальный видеоинпейнтинг: от заполнения пропусков до вставки объектов', 'desc': 'MTV-Inpaint - это новая система для видеоинпейнтинга, объединяющая задачи заполнения пропусков и вставки новых объектов. Она использует диффузионную модель text-to-video с двухветвевым механизмом пространственного внимания в U-Net архитектуре. MTV-Inpaint поддерживает мультимодальное управление, интегрируя различные модели инпейнтинга изображений. Система применяет двухэтапный подход с инпейнтингом ключевых кадров и распространением на промежуточные, что позволяет обрабатывать длинные видео.'}, 'en': {'title': 'Unified Video Inpainting: Complete and Insert with Control!', 'desc': 'This paper presents MTV-Inpaint, a novel framework for video inpainting that integrates scene completion and object insertion tasks. It utilizes a dual-branch spatial attention mechanism within a text-to-video diffusion U-Net to achieve seamless control over both tasks. The framework also introduces a two-stage pipeline that effectively manages long videos by combining keyframe inpainting with in-between frame propagation. Extensive experiments show that MTV-Inpaint outperforms existing methods and offers versatility for various applications, including multi-modal inpainting and object editing.'}, 'zh': {'title': '统一视频修复，场景补全与物体插入的完美结合', 'desc': '视频修复是指在视频中修改局部区域，以确保空间和时间的一致性。现有的方法主要集中在场景补全上，缺乏可控地插入新物体的能力。为了解决这些问题，我们提出了MTV-Inpaint，一个统一的多任务视频修复框架，能够同时处理传统的场景补全和新物体插入任务。通过设计双分支空间注意力机制，MTV-Inpaint实现了场景补全和物体插入的无缝集成，并支持多模态控制，适用于长视频的处理。'}}}, {'id': 'https://huggingface.co/papers/2503.13070', 'title': 'Rewards Are Enough for Fast Photo-Realistic Text-to-image Generation', 'url': 'https://huggingface.co/papers/2503.13070', 'abstract': 'Aligning generated images to complicated text prompts and human preferences is a central challenge in Artificial Intelligence-Generated Content (AIGC). With reward-enhanced diffusion distillation emerging as a promising approach that boosts controllability and fidelity of text-to-image models, we identify a fundamental paradigm shift: as conditions become more specific and reward signals stronger, the rewards themselves become the dominant force in generation. In contrast, the diffusion losses serve as an overly expensive form of regularization. To thoroughly validate our hypothesis, we introduce R0, a novel conditional generation approach via regularized reward maximization. Instead of relying on tricky diffusion distillation losses, R0 proposes a new perspective that treats image generations as an optimization problem in data space which aims to search for valid images that have high compositional rewards. By innovative designs of the generator parameterization and proper regularization techniques, we train state-of-the-art few-step text-to-image generative models with R0 at scales. Our results challenge the conventional wisdom of diffusion post-training and conditional generation by demonstrating that rewards play a dominant role in scenarios with complex conditions. We hope our findings can contribute to further research into human-centric and reward-centric generation paradigms across the broader field of AIGC. Code is available at https://github.com/Luo-Yihong/R0.', 'score': 6, 'issue_id': 2756, 'pub_date': '2025-03-17', 'pub_date_card': {'ru': '17 марта', 'en': 'March 17', 'zh': '3月17日'}, 'hash': '3d6b6c6e117e1abd', 'authors': ['Yihong Luo', 'Tianyang Hu', 'Weijian Luo', 'Kenji Kawaguchi', 'Jing Tang'], 'affiliations': ['HKUST', 'HKUST (GZ)', 'NUS', 'Xiaohongshu Inc'], 'pdf_title_img': 'assets/pdf/title_img/2503.13070.jpg', 'data': {'categories': ['#rag', '#diffusion', '#alignment', '#training', '#optimization'], 'emoji': '🎨', 'ru': {'title': 'R0: Революция в генерации изображений через максимизацию вознаграждений', 'desc': 'Статья представляет новый подход к генерации изображений по текстовым запросам, называемый R0. Вместо использования сложных методов дистилляции диффузионных моделей, R0 рассматривает генерацию как оптимизационную задачу в пространстве данных. Авторы утверждают, что при более конкретных условиях и сильных сигналах вознаграждения, сами вознаграждения становятся доминирующей силой в генерации. Результаты исследования бросают вызов традиционным представлениям о постобучении диффузионных моделей и условной генерации.'}, 'en': {'title': 'Revolutionizing Image Generation: Prioritizing Rewards Over Diffusion', 'desc': 'This paper addresses the challenge of aligning generated images with complex text prompts and human preferences in AI-generated content. It introduces R0, a new approach that emphasizes reward maximization over traditional diffusion distillation methods, which are seen as inefficient. The authors argue that as conditions for image generation become more specific, the influence of reward signals becomes more significant than diffusion losses. Their findings suggest a shift in focus towards reward-centric generation strategies, which could enhance the effectiveness of text-to-image models.'}, 'zh': {'title': '奖励驱动的图像生成新视角', 'desc': '本论文探讨了在人工智能生成内容（AIGC）中，将生成图像与复杂文本提示和人类偏好对齐的挑战。我们提出了一种新的条件生成方法R0，通过正则化奖励最大化来优化图像生成过程。研究表明，在条件变得更加具体且奖励信号更强时，奖励在生成过程中起着主导作用，而扩散损失则成为一种过于昂贵的正则化形式。我们的结果挑战了传统的扩散后训练和条件生成的观念，强调了在复杂条件下奖励的重要性。'}}}, {'id': 'https://huggingface.co/papers/2503.10719', 'title': 'Long-Video Audio Synthesis with Multi-Agent Collaboration', 'url': 'https://huggingface.co/papers/2503.10719', 'abstract': 'Video-to-audio synthesis, which generates synchronized audio for visual content, critically enhances viewer immersion and narrative coherence in film and interactive media. However, video-to-audio dubbing for long-form content remains an unsolved challenge due to dynamic semantic shifts, temporal misalignment, and the absence of dedicated datasets. While existing methods excel in short videos, they falter in long scenarios (e.g., movies) due to fragmented synthesis and inadequate cross-scene consistency. We propose LVAS-Agent, a novel multi-agent framework that emulates professional dubbing workflows through collaborative role specialization. Our approach decomposes long-video synthesis into four steps including scene segmentation, script generation, sound design and audio synthesis. Central innovations include a discussion-correction mechanism for scene/script refinement and a generation-retrieval loop for temporal-semantic alignment. To enable systematic evaluation, we introduce LVAS-Bench, the first benchmark with 207 professionally curated long videos spanning diverse scenarios. Experiments demonstrate superior audio-visual alignment over baseline methods. Project page: https://lvas-agent.github.io', 'score': 6, 'issue_id': 2760, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 марта', 'en': 'March 13', 'zh': '3月13日'}, 'hash': '121476792dd15d11', 'authors': ['Yehang Zhang', 'Xinli Xu', 'Xiaojie Xu', 'Li Liu', 'Yingcong Chen'], 'affiliations': ['HKUST', 'HKUST(GZ)'], 'pdf_title_img': 'assets/pdf/title_img/2503.10719.jpg', 'data': {'categories': ['#long_context', '#agents', '#benchmark', '#video', '#games'], 'emoji': '🎬', 'ru': {'title': 'Профессиональный дубляж длинных видео с помощью ИИ', 'desc': 'LVAS-Agent - это новая мультиагентная система для синтеза аудио к длинным видео, имитирующая профессиональный процесс дубляжа. Она разбивает задачу на четыре этапа: сегментацию сцен, генерацию сценария, звуковой дизайн и синтез аудио. Ключевые инновации включают механизм обсуждения и коррекции для улучшения сцен/сценария и цикл генерации-поиска для временно-семантического выравнивания. Авторы также представили LVAS-Bench - первый бенчмарк из 207 длинных видео для оценки таких систем.'}, 'en': {'title': 'Revolutionizing Long-Form Video Dubbing with LVAS-Agent', 'desc': 'This paper presents LVAS-Agent, a new framework for generating audio that matches long videos, like movies, to improve viewer experience. The framework breaks down the audio synthesis process into four key steps: segmenting scenes, generating scripts, designing sounds, and synthesizing audio. It introduces innovative techniques such as a discussion-correction mechanism to refine scripts and a generation-retrieval loop to ensure that audio aligns well with the video over time. Additionally, the authors provide LVAS-Bench, a benchmark dataset of 207 long videos to evaluate the effectiveness of their approach against existing methods.'}, 'zh': {'title': '长视频配音的新突破：LVAS-Agent', 'desc': '视频到音频合成是为视觉内容生成同步音频的技术，能够显著提升观众的沉浸感和叙事连贯性。然而，对于长篇内容的视频到音频配音仍然是一个未解决的挑战，主要由于动态语义变化、时间错位和缺乏专门的数据集。现有方法在短视频中表现良好，但在长视频（如电影）中由于合成碎片化和跨场景一致性不足而表现不佳。我们提出了LVAS-Agent，一个新颖的多代理框架，通过协作角色专业化模拟专业配音工作流程，分解长视频合成为场景分割、脚本生成、声音设计和音频合成四个步骤。'}}}, {'id': 'https://huggingface.co/papers/2503.10704', 'title': 'Error Analyses of Auto-Regressive Video Diffusion Models: A Unified\n  Framework', 'url': 'https://huggingface.co/papers/2503.10704', 'abstract': 'A variety of Auto-Regressive Video Diffusion Models (ARVDM) have achieved remarkable successes in generating realistic long-form videos. However, theoretical analyses of these models remain scant. In this work, we develop theoretical underpinnings for these models and use our insights to improve the performance of existing models. We first develop Meta-ARVDM, a unified framework of ARVDMs that subsumes most existing methods. Using Meta-ARVDM, we analyze the KL-divergence between the videos generated by Meta-ARVDM and the true videos. Our analysis uncovers two important phenomena inherent to ARVDM -- error accumulation and memory bottleneck. By deriving an information-theoretic impossibility result, we show that the memory bottleneck phenomenon cannot be avoided. To mitigate the memory bottleneck, we design various network structures to explicitly use more past frames. We also achieve a significantly improved trade-off between the mitigation of the memory bottleneck and the inference efficiency by compressing the frames. Experimental results on DMLab and Minecraft validate the efficacy of our methods. Our experiments also demonstrate a Pareto-frontier between the error accumulation and memory bottleneck across different methods.', 'score': 5, 'issue_id': 2756, 'pub_date': '2025-03-12', 'pub_date_card': {'ru': '12 марта', 'en': 'March 12', 'zh': '3月12日'}, 'hash': 'e5f88bd433320c2f', 'authors': ['Jing Wang', 'Fengzhuo Zhang', 'Xiaoli Li', 'Vincent Y. F. Tan', 'Tianyu Pang', 'Chao Du', 'Aixin Sun', 'Zhuoran Yang'], 'affiliations': ['A*STAR', 'Nanyang Technological University', 'National University of Singapore', 'Sea AI Lab', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2503.10704.jpg', 'data': {'categories': ['#diffusion', '#video', '#inference', '#architecture', '#math', '#optimization'], 'emoji': '🎬', 'ru': {'title': 'Теоретические основы и улучшение авторегрессионных моделей видеодиффузии', 'desc': 'Статья представляет теоретический анализ авторегрессионных моделей видеодиффузии (ARVDM) и предлагает способы их улучшения. Авторы разработали Meta-ARVDM - унифицированную структуру, объединяющую большинство существующих методов ARVDM. Анализ выявил два ключевых явления в ARVDM: накопление ошибок и ограничение памяти, причем последнее неизбежно согласно информационно-теоретическому результату. Для смягчения эффекта ограничения памяти предложены новые архитектуры нейронных сетей и методы сжатия кадров, эффективность которых подтверждена экспериментами.'}, 'en': {'title': 'Enhancing Video Generation with Meta-ARVDM: Tackling Memory Bottlenecks!', 'desc': 'This paper focuses on improving Auto-Regressive Video Diffusion Models (ARVDM) for generating realistic long videos. The authors introduce Meta-ARVDM, a comprehensive framework that encompasses existing ARVDM methods and provides a theoretical analysis of their performance. They identify two key issues: error accumulation and memory bottleneck, the latter of which is shown to be unavoidable through an information-theoretic approach. To address these challenges, the authors propose new network architectures that utilize more past frames and optimize the balance between memory usage and inference efficiency, demonstrating their findings through experiments on DMLab and Minecraft.'}, 'zh': {'title': '提升视频生成效率，破解内存瓶颈', 'desc': '本文探讨了自回归视频扩散模型（ARVDM）的理论基础，并提出了Meta-ARVDM这一统一框架，涵盖了大多数现有方法。通过分析Meta-ARVDM生成的视频与真实视频之间的KL散度，揭示了ARVDM固有的两个重要现象：误差累积和内存瓶颈。我们通过信息论的不可能性结果证明了内存瓶颈现象是无法避免的。为了解决内存瓶颈问题，本文设计了多种网络结构，以显式利用更多的过去帧，并通过压缩帧实现了内存瓶颈缓解与推理效率之间的显著改进。'}}}, {'id': 'https://huggingface.co/papers/2503.13369', 'title': 'Sightation Counts: Leveraging Sighted User Feedback in Building a\n  BLV-aligned Dataset of Diagram Descriptions', 'url': 'https://huggingface.co/papers/2503.13369', 'abstract': 'Often, the needs and visual abilities differ between the annotator group and the end user group. Generating detailed diagram descriptions for blind and low-vision (BLV) users is one such challenging domain. Sighted annotators could describe visuals with ease, but existing studies have shown that direct generations by them are costly, bias-prone, and somewhat lacking by BLV standards. In this study, we ask sighted individuals to assess -- rather than produce -- diagram descriptions generated by vision-language models (VLM) that have been guided with latent supervision via a multi-pass inference. The sighted assessments prove effective and useful to professional educators who are themselves BLV and teach visually impaired learners. We release Sightation, a collection of diagram description datasets spanning 5k diagrams and 137k samples for completion, preference, retrieval, question answering, and reasoning training purposes and demonstrate their fine-tuning potential in various downstream tasks.', 'score': 3, 'issue_id': 2760, 'pub_date': '2025-03-17', 'pub_date_card': {'ru': '17 марта', 'en': 'March 17', 'zh': '3月17日'}, 'hash': '30b312815a3aaed9', 'authors': ['Wan Ju Kang', 'Eunki Kim', 'Na Min An', 'Sangryul Kim', 'Haemin Choi', 'Ki Hoon Kwak', 'James Thorne'], 'affiliations': ['KAIST AI', 'Sungkyunkwan University', 'Yonsei University'], 'pdf_title_img': 'assets/pdf/title_img/2503.13369.jpg', 'data': {'categories': ['#training', '#alignment', '#data', '#low_resource', '#dataset', '#multimodal'], 'emoji': '👁️', 'ru': {'title': 'Искусственный интеллект на службе незрячих: новый подход к описанию визуальной информации', 'desc': 'Статья описывает метод создания детальных описаний диаграмм для слепых и слабовидящих пользователей с помощью моделей компьютерного зрения и обработки естественного языка. Авторы предлагают использовать зрячих аннотаторов для оценки описаний, сгенерированных моделями, вместо того чтобы создавать их самостоятельно. Исследование показало эффективность этого подхода для преподавателей с нарушениями зрения. В результате работы был создан набор данных Sightation, содержащий описания 5000 диаграмм для различных задач машинного обучения.'}, 'en': {'title': 'Empowering BLV Education with Vision-Language Models', 'desc': "This paper addresses the challenge of creating effective diagram descriptions for blind and low-vision (BLV) users, highlighting the differences in needs between annotators and end users. It proposes a novel approach where sighted annotators assess diagram descriptions generated by vision-language models (VLM) instead of creating them directly, reducing bias and improving quality. The study introduces 'Sightation', a comprehensive dataset containing 5,000 diagrams and 137,000 samples designed for various machine learning tasks such as preference and reasoning. The findings suggest that using sighted assessments can enhance the educational resources available for BLV learners, making them more accessible and relevant."}, 'zh': {'title': '为视觉障碍用户生成精准图表描述', 'desc': '本研究探讨了视觉障碍用户（BLV）对图表描述的需求与视觉标注者之间的差异。我们提出了一种新方法，通过视觉语言模型（VLM）生成图表描述，并让视觉正常的评估者对这些描述进行评估，而不是直接生成。研究表明，这种评估方式对专业的视觉障碍教育者非常有效，能够帮助他们更好地服务于视觉障碍学习者。我们还发布了名为Sightation的数据集，包含5000个图表和137000个样本，旨在支持多种下游任务的训练。'}}}, {'id': 'https://huggingface.co/papers/2503.12530', 'title': 'Basic Category Usage in Vision Language Models', 'url': 'https://huggingface.co/papers/2503.12530', 'abstract': "The field of psychology has long recognized a basic level of categorization that humans use when labeling visual stimuli, a term coined by Rosch in 1976. This level of categorization has been found to be used most frequently, to have higher information density, and to aid in visual language tasks with priming in humans. Here, we investigate basic level categorization in two recently released, open-source vision-language models (VLMs). This paper demonstrates that Llama 3.2 Vision Instruct (11B) and Molmo 7B-D both prefer basic level categorization consistent with human behavior. Moreover, the models' preferences are consistent with nuanced human behaviors like the biological versus non-biological basic level effects and the well established expert basic level shift, further suggesting that VLMs acquire cognitive categorization behaviors from the human data on which they are trained.", 'score': 2, 'issue_id': 2755, 'pub_date': '2025-03-16', 'pub_date_card': {'ru': '16 марта', 'en': 'March 16', 'zh': '3月16日'}, 'hash': 'eca67c7a2633554a', 'authors': ['Hunter Sawyer', 'Jesse Roberts', 'Kyle Moore'], 'affiliations': ['Tennessee Tech University'], 'pdf_title_img': 'assets/pdf/title_img/2503.12530.jpg', 'data': {'categories': ['#open_source', '#interpretability', '#multimodal', '#cv'], 'emoji': '🧠', 'ru': {'title': 'ИИ мыслит как человек: базовая категоризация в моделях компьютерного зрения', 'desc': 'Это исследование рассматривает применение базового уровня категоризации, концепции из психологии, в современных моделях компьютерного зрения и обработки естественного языка (VLM). Авторы обнаружили, что модели Llama 3.2 Vision Instruct и Molmo 7B-D предпочитают использовать базовый уровень категоризации, что соответствует поведению человека. Более того, модели демонстрируют нюансы, характерные для людей, такие как различия в категоризации биологических и небиологических объектов, а также сдвиг базового уровня у экспертов. Это указывает на то, что VLM приобретают когнитивные паттерны категоризации из человеческих данных, на которых они обучаются.'}, 'en': {'title': 'Bridging Human and Machine: Basic Level Categorization in Vision-Language Models', 'desc': 'This paper explores how vision-language models (VLMs) categorize visual stimuli at a basic level, similar to human categorization as identified by Rosch. The study shows that Llama 3.2 Vision Instruct and Molmo 7B-D models exhibit a preference for basic level categorization, which aligns with human behavior. Additionally, the models reflect complex human categorization nuances, such as distinguishing between biological and non-biological entities. This suggests that VLMs learn cognitive categorization patterns from the human data they are trained on, highlighting the influence of human cognitive processes on machine learning models.'}, 'zh': {'title': '探索视觉语言模型中的基本分类行为', 'desc': '本论文研究了视觉语言模型（VLMs）中的基本分类水平，这一概念最早由Rosch在1976年提出。研究发现，Llama 3.2 Vision Instruct和Molmo 7B-D这两个模型在分类时更倾向于使用与人类行为一致的基本分类水平。模型的偏好还与人类的细微行为相符，例如生物与非生物的基本分类效应，以及专家的基本分类转变。这表明，视觉语言模型在训练过程中从人类数据中学习了认知分类行为。'}}}, {'id': 'https://huggingface.co/papers/2503.12528', 'title': 'Investigating Human-Aligned Large Language Model Uncertainty', 'url': 'https://huggingface.co/papers/2503.12528', 'abstract': 'Recent work has sought to quantify large language model uncertainty to facilitate model control and modulate user trust. Previous works focus on measures of uncertainty that are theoretically grounded or reflect the average overt behavior of the model. In this work, we investigate a variety of uncertainty measures, in order to identify measures that correlate with human group-level uncertainty. We find that Bayesian measures and a variation on entropy measures, top-k entropy, tend to agree with human behavior as a function of model size. We find that some strong measures decrease in human-similarity with model size, but, by multiple linear regression, we find that combining multiple uncertainty measures provide comparable human-alignment with reduced size-dependency.', 'score': 2, 'issue_id': 2755, 'pub_date': '2025-03-16', 'pub_date_card': {'ru': '16 марта', 'en': 'March 16', 'zh': '3月16日'}, 'hash': '7c140046351fe245', 'authors': ['Kyle Moore', 'Jesse Roberts', 'Daryl Watson', 'Pamela Wisniewski'], 'affiliations': ['Tennessee Tech University', 'Vanderbilt University'], 'pdf_title_img': 'assets/pdf/title_img/2503.12528.jpg', 'data': {'categories': ['#interpretability', '#hallucinations', '#rlhf', '#training'], 'emoji': '🤔', 'ru': {'title': 'Измерение неопределенности LLM: как приблизиться к человеку', 'desc': 'Статья исследует различные методы оценки неопределенности в больших языковых моделях (LLM) с целью найти те, которые лучше всего соответствуют групповой неопределенности людей. Авторы обнаружили, что байесовские методы и вариация энтропийных мер (энтропия top-k) хорошо согласуются с человеческим поведением в зависимости от размера модели. Некоторые сильные меры показывают снижение сходства с человеком при увеличении размера модели. Комбинирование нескольких мер неопределенности с помощью множественной линейной регрессии обеспечивает сопоставимое соответствие человеческому поведению с меньшей зависимостью от размера модели.'}, 'en': {'title': 'Aligning Model Uncertainty with Human Perception', 'desc': 'This paper explores how to measure uncertainty in large language models to improve their control and enhance user trust. It examines various uncertainty measures, particularly focusing on Bayesian methods and a new approach called top-k entropy, to see how well they align with human perceptions of uncertainty. The study reveals that while some measures lose their effectiveness as model size increases, combining different measures can maintain a strong correlation with human behavior regardless of model size. Ultimately, the findings suggest that a multi-measure approach can lead to better alignment with human understanding of uncertainty in language models.'}, 'zh': {'title': '量化不确定性，增强用户信任', 'desc': '本研究探讨了大型语言模型的不确定性量化，以便更好地控制模型并增强用户信任。我们分析了多种不确定性度量，旨在找出与人类群体不确定性相关的度量。研究发现，贝叶斯度量和一种变体的熵度量（top-k熵）在模型规模变化时与人类行为一致。通过多元线性回归，我们发现结合多种不确定性度量可以在减少规模依赖性的同时，保持与人类行为的相似性。'}}}, {'id': 'https://huggingface.co/papers/2503.12964', 'title': 'Training Video Foundation Models with NVIDIA NeMo', 'url': 'https://huggingface.co/papers/2503.12964', 'abstract': 'Video Foundation Models (VFMs) have recently been used to simulate the real world to train physical AI systems and develop creative visual experiences. However, there are significant challenges in training large-scale, high quality VFMs that can generate high-quality videos. We present a scalable, open-source VFM training pipeline with NVIDIA NeMo, providing accelerated video dataset curation, multimodal data loading, and parallelized video diffusion model training and inference. We also provide a comprehensive performance analysis highlighting best practices for efficient VFM training and inference.', 'score': 1, 'issue_id': 2771, 'pub_date': '2025-03-17', 'pub_date_card': {'ru': '17 марта', 'en': 'March 17', 'zh': '3月17日'}, 'hash': 'a9b85c5227c0a3e6', 'authors': ['Zeeshan Patel', 'Ethan He', 'Parth Mannan', 'Xiaowei Ren', 'Ryan Wolf', 'Niket Agarwal', 'Jacob Huffman', 'Zhuoyao Wang', 'Carl Wang', 'Jack Chang', 'Yan Bai', 'Tommy Huang', 'Linnan Wang', 'Sahil Jain', 'Shanmugam Ramasamy', 'Joseph Jennings', 'Ekaterina Sirazitdinova', 'Oleg Sudakov', 'Mingyuan Ma', 'Bobby Chen', 'Forrest Lin', 'Hao Wang', 'Vasanth Rao Naik Sabavat', 'Sriharsha Niverty', 'Rong Ou', 'Pallab Bhattacharya', 'David Page', 'Nima Tajbakhsh', 'Ashwath Aithal'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2503.12964.jpg', 'data': {'categories': ['#dataset', '#diffusion', '#multimodal', '#inference', '#video', '#optimization', '#data', '#training', '#open_source'], 'emoji': '🎥', 'ru': {'title': 'Масштабируемое обучение видео-фундаментальных моделей с NVIDIA NeMo', 'desc': 'Эта статья представляет масштабируемый конвейер для обучения видео-фундаментальных моделей (VFM) с использованием NVIDIA NeMo. Авторы описывают ускоренную подготовку видеонаборов данных, мультимодальную загрузку данных и распараллеленное обучение и вывод моделей видеодиффузии. В работе также приводится подробный анализ производительности, выделяющий лучшие практики для эффективного обучения и вывода VFM. Исследование направлено на преодоление значительных трудностей в обучении крупномасштабных VFM высокого качества для генерации высококачественных видео.'}, 'en': {'title': 'Empowering Video Foundation Models with Scalable Training Solutions', 'desc': 'This paper introduces a scalable and open-source training pipeline for Video Foundation Models (VFMs) using NVIDIA NeMo. It addresses the challenges of training large-scale VFMs by offering tools for efficient video dataset curation and multimodal data loading. The proposed pipeline also includes methods for parallelized training and inference of video diffusion models. Additionally, the authors present a performance analysis that outlines best practices for optimizing VFM training and inference processes.'}, 'zh': {'title': '高效训练视频基础模型的开源解决方案', 'desc': '视频基础模型（VFM）最近被用于模拟现实世界，以训练物理人工智能系统和开发创意视觉体验。然而，训练大规模、高质量的VFM以生成高质量视频面临重大挑战。我们提出了一个可扩展的开源VFM训练管道，利用NVIDIA NeMo，加速视频数据集的整理、多模态数据加载，以及并行化的视频扩散模型训练和推理。我们还提供了全面的性能分析，强调高效VFM训练和推理的最佳实践。'}}}, {'id': 'https://huggingface.co/papers/2503.12720', 'title': 'GenStereo: Towards Open-World Generation of Stereo Images and\n  Unsupervised Matching', 'url': 'https://huggingface.co/papers/2503.12720', 'abstract': 'Stereo images are fundamental to numerous applications, including extended reality (XR) devices, autonomous driving, and robotics. Unfortunately, acquiring high-quality stereo images remains challenging due to the precise calibration requirements of dual-camera setups and the complexity of obtaining accurate, dense disparity maps. Existing stereo image generation methods typically focus on either visual quality for viewing or geometric accuracy for matching, but not both. We introduce GenStereo, a diffusion-based approach, to bridge this gap. The method includes two primary innovations (1) conditioning the diffusion process on a disparity-aware coordinate embedding and a warped input image, allowing for more precise stereo alignment than previous methods, and (2) an adaptive fusion mechanism that intelligently combines the diffusion-generated image with a warped image, improving both realism and disparity consistency. Through extensive training on 11 diverse stereo datasets, GenStereo demonstrates strong generalization ability. GenStereo achieves state-of-the-art performance in both stereo image generation and unsupervised stereo matching tasks. Our framework eliminates the need for complex hardware setups while enabling high-quality stereo image generation, making it valuable for both real-world applications and unsupervised learning scenarios. Project page is available at https://qjizhi.github.io/genstereo', 'score': 1, 'issue_id': 2766, 'pub_date': '2025-03-17', 'pub_date_card': {'ru': '17 марта', 'en': 'March 17', 'zh': '3月17日'}, 'hash': '2fdcfe786193f6cf', 'authors': ['Feng Qiao', 'Zhexiao Xiong', 'Eric Xing', 'Nathan Jacobs'], 'affiliations': ['Washington University in St. Louis'], 'pdf_title_img': 'assets/pdf/title_img/2503.12720.jpg', 'data': {'categories': ['#robotics', '#video', '#3d', '#diffusion'], 'emoji': '👁️', 'ru': {'title': 'GenStereo: Революция в генерации стереоизображений с помощью диффузионных моделей', 'desc': 'GenStereo - это новый подход к генерации стереоизображений, основанный на диффузионных моделях. Метод использует встраивание координат с учетом диспаратности и деформированное входное изображение для точного стереовыравнивания. GenStereo также включает адаптивный механизм слияния, который объединяет сгенерированное и деформированное изображения для улучшения реалистичности и согласованности диспаратности. Обученный на 11 разнообразных стереодатасетах, GenStereo демонстрирует высокую обобщающую способность и достигает наилучших результатов в задачах генерации стереоизображений и неконтролируемого стереосопоставления.'}, 'en': {'title': 'GenStereo: Bridging Visual Quality and Geometric Accuracy in Stereo Image Generation', 'desc': 'This paper presents GenStereo, a novel diffusion-based method for generating high-quality stereo images. It addresses the challenges of stereo image generation by focusing on both visual quality and geometric accuracy, which are often at odds in existing methods. GenStereo innovates by conditioning the diffusion process on disparity-aware embeddings and using an adaptive fusion mechanism to enhance realism and consistency. The method shows strong performance across various stereo datasets, making it suitable for applications in extended reality, autonomous driving, and robotics without the need for complex hardware setups.'}, 'zh': {'title': 'GenStereo：高质量立体图像生成的新方法', 'desc': '本论文介绍了一种名为GenStereo的生成方法，旨在提高立体图像的质量和几何准确性。该方法通过条件扩散过程，结合视差感知的坐标嵌入和变形输入图像，实现更精确的立体对齐。GenStereo还采用了一种自适应融合机制，智能地将生成的图像与变形图像结合，从而提高了图像的真实感和视差一致性。经过在11个多样化的立体数据集上的广泛训练，GenStereo在立体图像生成和无监督立体匹配任务中表现出色，具有很强的泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2503.08153', 'title': 'WISA: World Simulator Assistant for Physics-Aware Text-to-Video\n  Generation', 'url': 'https://huggingface.co/papers/2503.08153', 'abstract': "Recent rapid advancements in text-to-video (T2V) generation, such as SoRA and Kling, have shown great potential for building world simulators. However, current T2V models struggle to grasp abstract physical principles and generate videos that adhere to physical laws. This challenge arises primarily from a lack of clear guidance on physical information due to a significant gap between abstract physical principles and generation models. To this end, we introduce the World Simulator Assistant (WISA), an effective framework for decomposing and incorporating physical principles into T2V models. Specifically, WISA decomposes physical principles into textual physical descriptions, qualitative physical categories, and quantitative physical properties. To effectively embed these physical attributes into the generation process, WISA incorporates several key designs, including Mixture-of-Physical-Experts Attention (MoPA) and a Physical Classifier, enhancing the model's physics awareness. Furthermore, most existing datasets feature videos where physical phenomena are either weakly represented or entangled with multiple co-occurring processes, limiting their suitability as dedicated resources for learning explicit physical principles. We propose a novel video dataset, WISA-32K, collected based on qualitative physical categories. It consists of 32,000 videos, representing 17 physical laws across three domains of physics: dynamics, thermodynamics, and optics. Experimental results demonstrate that WISA can effectively enhance the compatibility of T2V models with real-world physical laws, achieving a considerable improvement on the VideoPhy benchmark. The visual exhibitions of WISA and WISA-32K are available in the https://360cvgroup.github.io/WISA/.", 'score': 0, 'issue_id': 2766, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 марта', 'en': 'March 11', 'zh': '3月11日'}, 'hash': '60c6c27b7b32f442', 'authors': ['Jing Wang', 'Ao Ma', 'Ke Cao', 'Jun Zheng', 'Zhanjie Zhang', 'Jiasong Feng', 'Shanyuan Liu', 'Yuhang Ma', 'Bo Cheng', 'Dawei Leng', 'Yuhui Yin', 'Xiaodan Liang'], 'affiliations': ['AI Research', 'Peng Cheng Laboratory', 'Shenzhen Campus of Sun Yat-Sen University'], 'pdf_title_img': 'assets/pdf/title_img/2503.08153.jpg', 'data': {'categories': ['#games', '#benchmark', '#video', '#dataset', '#optimization'], 'emoji': '🎥', 'ru': {'title': 'WISA: обучение моделей T2V физическим законам реального мира', 'desc': 'Статья представляет WISA (World Simulator Assistant) - фреймворк для внедрения физических принципов в модели генерации текста в видео (T2V). WISA разбивает физические принципы на текстовые описания, качественные категории и количественные свойства. Для улучшения физической осведомленности модели используются такие методы как Mixture-of-Physical-Experts Attention и Physical Classifier. Также авторы создали датасет WISA-32K из 32 000 видео, демонстрирующих 17 физических законов из трех областей физики.'}, 'en': {'title': 'Bridging Text-to-Video with Real-World Physics', 'desc': "This paper presents the World Simulator Assistant (WISA), a framework designed to improve text-to-video (T2V) generation by integrating physical principles into the models. WISA breaks down physical concepts into textual descriptions, qualitative categories, and quantitative properties, allowing T2V models to better understand and apply these principles. It introduces innovative components like Mixture-of-Physical-Experts Attention (MoPA) and a Physical Classifier to enhance the model's awareness of physics. Additionally, the authors create a new dataset, WISA-32K, containing 32,000 videos that illustrate 17 physical laws, which helps train T2V models to align more closely with real-world physics."}, 'zh': {'title': '提升文本到视频生成的物理意识', 'desc': '本文介绍了一种新的框架，称为世界模拟助手（WISA），旨在改善文本到视频生成（T2V）模型对物理原则的理解。WISA通过将物理原则分解为文本描述、定性类别和定量属性，帮助生成模型更好地融入物理信息。该框架还引入了混合物理专家注意力（MoPA）和物理分类器等设计，增强了模型的物理意识。此外，WISA还提出了一个新的视频数据集WISA-32K，包含32,000个视频，涵盖17条物理定律，旨在为学习明确的物理原则提供更合适的资源。'}}}, {'id': 'https://huggingface.co/papers/2503.06269', 'title': 'Using Mechanistic Interpretability to Craft Adversarial Attacks against\n  Large Language Models', 'url': 'https://huggingface.co/papers/2503.06269', 'abstract': "Traditional white-box methods for creating adversarial perturbations against LLMs typically rely only on gradient computation from the targeted model, ignoring the internal mechanisms responsible for attack success or failure. Conversely, interpretability studies that analyze these internal mechanisms lack practical applications beyond runtime interventions. We bridge this gap by introducing a novel white-box approach that leverages mechanistic interpretability techniques to craft practical adversarial inputs. Specifically, we first identify acceptance subspaces - sets of feature vectors that do not trigger the model's refusal mechanisms - then use gradient-based optimization to reroute embeddings from refusal subspaces to acceptance subspaces, effectively achieving jailbreaks. This targeted approach significantly reduces computation cost, achieving attack success rates of 80-95\\% on state-of-the-art models including Gemma2, Llama3.2, and Qwen2.5 within minutes or even seconds, compared to existing techniques that often fail or require hours of computation. We believe this approach opens a new direction for both attack research and defense development. Furthermore, it showcases a practical application of mechanistic interpretability where other methods are less efficient, which highlights its utility. The code and generated datasets are available at https://github.com/Sckathach/subspace-rerouting.", 'score': 0, 'issue_id': 2772, 'pub_date': '2025-03-08', 'pub_date_card': {'ru': '8 марта', 'en': 'March 8', 'zh': '3月8日'}, 'hash': '623dbef3e618123c', 'authors': ['Thomas Winninger', 'Boussad Addad', 'Katarzyna Kapusta'], 'affiliations': ['Thales SIX GTS', 'Télécom SudParis'], 'pdf_title_img': 'assets/pdf/title_img/2503.06269.jpg', 'data': {'categories': ['#security', '#benchmark', '#interpretability', '#data', '#optimization', '#architecture', '#dataset'], 'emoji': '🛡️', 'ru': {'title': 'Эффективные состязательные атаки через анализ внутренних механизмов языковых моделей', 'desc': "Статья представляет новый метод создания состязательных атак на языковые модели, используя механистическую интерпретируемость. Авторы идентифицируют 'пространства принятия' - наборы векторов признаков, не вызывающие отказ модели, и применяют оптимизацию градиентного спуска для перенаправления вложений в эти пространства. Этот подход значительно снижает вычислительные затраты, достигая 80-95% успешности атак на современные модели за минуты или секунды. Исследование открывает новые направления как для разработки атак, так и для защиты, демонстрируя практическое применение механистической интерпретируемости."}, 'en': {'title': 'Bridging Interpretability and Adversarial Attacks for LLMs', 'desc': "This paper presents a new method for creating adversarial inputs for large language models (LLMs) by combining gradient-based optimization with mechanistic interpretability. The authors identify 'acceptance subspaces' where inputs are accepted by the model, and reroute embeddings from 'refusal subspaces' to these acceptance areas. This innovative approach significantly improves the efficiency of adversarial attacks, achieving success rates of 80-95% in a matter of minutes. The research not only advances attack strategies but also demonstrates the practical use of interpretability techniques in machine learning."}, 'zh': {'title': '利用机制可解释性提升对抗攻击成功率', 'desc': '这篇论文提出了一种新的白盒方法，用于生成针对大型语言模型（LLM）的对抗扰动。该方法结合了机制可解释性技术，首先识别接受子空间，然后通过基于梯度的优化将嵌入从拒绝子空间重新引导到接受子空间。与传统方法相比，这种方法显著降低了计算成本，并在几分钟或几秒内实现了80-95%的攻击成功率。该研究为攻击研究和防御开发开辟了新方向，同时展示了机制可解释性的实际应用。'}}}, {'id': 'https://huggingface.co/papers/2503.04625', 'title': 'START: Self-taught Reasoner with Tools', 'url': 'https://huggingface.co/papers/2503.04625', 'abstract': "Large reasoning models (LRMs) like OpenAI-o1 and DeepSeek-R1 have demonstrated remarkable capabilities in complex reasoning tasks through the utilization of long Chain-of-thought (CoT). However, these models often suffer from hallucinations and inefficiencies due to their reliance solely on internal reasoning processes. In this paper, we introduce START (Self-Taught Reasoner with Tools), a novel tool-integrated long CoT reasoning LLM that significantly enhances reasoning capabilities by leveraging external tools. Through code execution, START is capable of performing complex computations, self-checking, exploring diverse methods, and self-debugging, thereby addressing the limitations of LRMs. The core innovation of START lies in its self-learning framework, which comprises two key techniques: 1) Hint-infer: We demonstrate that inserting artificially designed hints (e.g., ``Wait, maybe using Python here is a good idea.'') during the inference process of a LRM effectively stimulates its ability to utilize external tools without the need for any demonstration data. Hint-infer can also serve as a simple and effective sequential test-time scaling method; 2) Hint Rejection Sampling Fine-Tuning (Hint-RFT): Hint-RFT combines Hint-infer and RFT by scoring, filtering, and modifying the reasoning trajectories with tool invocation generated by a LRM via Hint-infer, followed by fine-tuning the LRM. Through this framework, we have fine-tuned the QwQ-32B model to achieve START. On PhD-level science QA (GPQA), competition-level math benchmarks (AMC23, AIME24, AIME25), and the competition-level code benchmark (LiveCodeBench), START achieves accuracy rates of 63.6%, 95.0%, 66.7%, 47.1%, and 47.3%, respectively. It significantly outperforms the base QwQ-32B and achieves performance comparable to the state-of-the-art open-weight model R1-Distill-Qwen-32B and the proprietary model o1-Preview.", 'score': 66, 'issue_id': 2579, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 марта', 'en': 'March 6', 'zh': '3月6日'}, 'hash': '8961f69e1eda24ad', 'authors': ['Chengpeng Li', 'Mingfeng Xue', 'Zhenru Zhang', 'Jiaxi Yang', 'Beichen Zhang', 'Xiang Wang', 'Bowen Yu', 'Binyuan Hui', 'Junyang Lin', 'Dayiheng Liu'], 'affiliations': ['Alibaba Group', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2503.04625.jpg', 'data': {'categories': ['#long_context', '#rl', '#training', '#architecture', '#hallucinations', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'START: Самообучающаяся модель рассуждений с инструментами', 'desc': 'Статья представляет START - новую модель для рассуждений с длинной цепочкой мыслей, интегрирующую внешние инструменты. START использует фреймворк самообучения, включающий технику Hint-infer для стимулирования использования инструментов, и Hint Rejection Sampling Fine-Tuning для улучшения траекторий рассуждений. Модель значительно превосходит базовую QwQ-32B и достигает результатов на уровне современных моделей в сложных задачах рассуждений. START демонстрирует высокую точность на наборах данных по науке, математике и программированию уровня PhD и соревнований.'}, 'en': {'title': 'Enhancing Reasoning with External Tools: Introducing START', 'desc': "This paper presents START, a new long Chain-of-thought reasoning model that integrates external tools to improve reasoning capabilities. Traditional large reasoning models often struggle with hallucinations and inefficiencies, but START addresses these issues by allowing the model to perform complex computations and self-debugging. The innovation lies in two techniques: Hint-infer, which uses designed hints to encourage tool usage, and Hint Rejection Sampling Fine-Tuning, which refines the model's reasoning paths. START demonstrates superior performance on various benchmarks, surpassing its predecessor and competing effectively with state-of-the-art models."}, 'zh': {'title': '工具整合，推理更强！', 'desc': '本文介绍了一种新型的长链推理模型START（自我学习推理器与工具），它通过整合外部工具来增强推理能力。START利用代码执行进行复杂计算、自我检查、探索多种方法和自我调试，从而克服了大型推理模型（LRMs）在推理过程中常见的幻觉和低效问题。核心创新在于自我学习框架，包括提示推理（Hint-infer）和提示拒绝采样微调（Hint-RFT）两种技术，前者通过插入设计的提示来激发模型使用外部工具的能力。经过微调，START在多个科学问答和数学基准测试中表现优异，准确率显著高于基础模型QwQ-32B。'}}}, {'id': 'https://huggingface.co/papers/2503.04130', 'title': 'Token-Efficient Long Video Understanding for Multimodal LLMs', 'url': 'https://huggingface.co/papers/2503.04130', 'abstract': 'Recent advances in video-based multimodal large language models (Video-LLMs) have significantly improved video understanding by processing videos as sequences of image frames. However, many existing methods treat frames independently in the vision backbone, lacking explicit temporal modeling, which limits their ability to capture dynamic patterns and efficiently handle long videos. To address these limitations, we introduce STORM (Spatiotemporal TOken Reduction for Multimodal LLMs), a novel architecture incorporating a dedicated temporal encoder between the image encoder and the LLM. Our temporal encoder leverages the Mamba State Space Model to integrate temporal information into image tokens, generating enriched representations that preserve inter-frame dynamics across the entire video sequence. This enriched encoding not only enhances video reasoning capabilities but also enables effective token reduction strategies, including test-time sampling and training-based temporal and spatial pooling, substantially reducing computational demands on the LLM without sacrificing key temporal information. By integrating these techniques, our approach simultaneously reduces training and inference latency while improving performance, enabling efficient and robust video understanding over extended temporal contexts. Extensive evaluations show that STORM achieves state-of-the-art results across various long video understanding benchmarks (more than 5\\% improvement on MLVU and LongVideoBench) while reducing the computation costs by up to 8times and the decoding latency by 2.4-2.9times for the fixed numbers of input frames. Project page is available at https://research.nvidia.com/labs/lpr/storm', 'score': 59, 'issue_id': 2580, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 марта', 'en': 'March 6', 'zh': '3月6日'}, 'hash': 'f1d1afacd41dc0c7', 'authors': ['Jindong Jiang', 'Xiuyu Li', 'Zhijian Liu', 'Muyang Li', 'Guo Chen', 'Zhiqi Li', 'De-An Huang', 'Guilin Liu', 'Zhiding Yu', 'Kurt Keutzer', 'Sungjin Ahn', 'Jan Kautz', 'Hongxu Yin', 'Yao Lu', 'Song Han', 'Wonmin Byeon'], 'affiliations': ['KAIST', 'MIT', 'NVIDIA', 'Nanjing University', 'Rutgers University', 'UC Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2503.04130.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#optimization', '#long_context', '#architecture', '#video', '#inference', '#multimodal'], 'emoji': '🌪️', 'ru': {'title': 'STORM: Эффективное понимание длинных видео с помощью темпорального кодирования', 'desc': 'STORM - это новая архитектура для видео-LLM, которая использует специальный темпоральный энкодер на основе модели Mamba State Space между энкодером изображений и LLM. Эта архитектура позволяет интегрировать временную информацию в токены изображений, создавая обогащенные представления, сохраняющие динамику между кадрами во всей видеопоследовательности. STORM применяет стратегии сокращения токенов, включая выборку во время тестирования и пулинг во время обучения, что значительно снижает вычислительные затраты LLM. Результаты экспериментов показывают, что STORM достигает улучшения более чем на 5% на бенчмарках длинных видео, одновременно сокращая вычислительные затраты до 8 раз.'}, 'en': {'title': 'STORM: Revolutionizing Video Understanding with Temporal Insights', 'desc': 'This paper presents STORM, a new architecture designed to enhance video understanding in multimodal large language models (LLMs) by incorporating a temporal encoder. Unlike previous methods that treat video frames independently, STORM uses the Mamba State Space Model to capture the dynamics between frames, resulting in richer representations of video content. The architecture also implements token reduction strategies that significantly lower computational costs and improve processing speed without losing important temporal information. Evaluations demonstrate that STORM outperforms existing models on long video benchmarks while achieving substantial reductions in computation and latency.'}, 'zh': {'title': '高效视频理解的新突破：STORM模型', 'desc': '最近，基于视频的多模态大语言模型（Video-LLMs）在视频理解方面取得了显著进展，但许多现有方法在视觉骨干中独立处理帧，缺乏明确的时间建模，限制了捕捉动态模式的能力。为了解决这些问题，我们提出了STORM（时空令牌减少模型），它在图像编码器和大语言模型之间引入了专门的时间编码器。我们的时间编码器利用Mamba状态空间模型，将时间信息整合到图像令牌中，生成丰富的表示，保留整个视频序列中的帧间动态。通过这些技术的整合，我们的方法在提高性能的同时，显著减少了计算需求和推理延迟，实现了对长视频的高效理解。'}}}, {'id': 'https://huggingface.co/papers/2503.04724', 'title': 'LLMVoX: Autoregressive Streaming Text-to-Speech Model for Any LLM', 'url': 'https://huggingface.co/papers/2503.04724', 'abstract': 'Recent advancements in speech-to-speech dialogue systems leverage LLMs for multimodal interactions, yet they remain hindered by fine-tuning requirements, high computational overhead, and text-speech misalignment. Existing speech-enabled LLMs often degrade conversational quality by modifying the LLM, thereby compromising its linguistic capabilities. In contrast, we propose LLMVoX, a lightweight 30M-parameter, LLM-agnostic, autoregressive streaming TTS system that generates high-quality speech with low latency, while fully preserving the capabilities of the base LLM. Our approach achieves a significantly lower Word Error Rate compared to speech-enabled LLMs, while operating at comparable latency and UTMOS score. By decoupling speech synthesis from LLM processing via a multi-queue token streaming system, LLMVoX supports seamless, infinite-length dialogues. Its plug-and-play design also facilitates extension to various tasks with different backbones. Furthermore, LLMVoX generalizes to new languages with only dataset adaptation, attaining a low Character Error Rate on an Arabic speech task. Additionally, we have integrated LLMVoX with a Vision-Language Model to create an omni-model with speech, text, and vision capabilities, without requiring additional multimodal training. Our code base and project page is available at https://mbzuai-oryx.github.io/LLMVoX .', 'score': 41, 'issue_id': 2589, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 марта', 'en': 'March 6', 'zh': '3月6日'}, 'hash': '3113e03ae6f4ed42', 'authors': ['Sambal Shikhar', 'Mohammed Irfan Kurpath', 'Sahal Shaji Mullappilly', 'Jean Lahoud', 'Fahad Khan', 'Rao Muhammad Anwer', 'Salman Khan', 'Hisham Cholakkal'], 'affiliations': ['Linköping University, Sweden', 'Mohamed Bin Zayed University of Artificial Intelligence (MBZUAI), UAE'], 'pdf_title_img': 'assets/pdf/title_img/2503.04724.jpg', 'data': {'categories': ['#multimodal', '#small_models', '#low_resource', '#audio', '#open_source', '#long_context'], 'emoji': '🗣️', 'ru': {'title': 'LLMVoX: Универсальный синтез речи для языковых моделей', 'desc': 'LLMVoX - это легковесная система синтеза речи, которая работает совместно с языковыми моделями, не требуя их модификации. Она обеспечивает высококачественный синтез речи с низкой задержкой, сохраняя все возможности базовой языковой модели. LLMVoX достигает более низкого показателя ошибок распознавания слов по сравнению с речевыми языковыми моделями, при этом работая с сопоставимой задержкой. Система может быть легко адаптирована для различных задач и языков, а также интегрирована с мультимодальными моделями.'}, 'en': {'title': 'Seamless Speech Synthesis with LLMVoX', 'desc': 'The paper introduces LLMVoX, a novel speech-to-speech dialogue system that utilizes a lightweight, 30M-parameter architecture to enhance multimodal interactions without compromising the linguistic capabilities of large language models (LLMs). Unlike existing systems that require extensive fine-tuning and often degrade conversational quality, LLMVoX operates efficiently with low latency and a significantly reduced Word Error Rate. It employs a multi-queue token streaming mechanism to decouple speech synthesis from LLM processing, enabling seamless dialogues of infinite length. Additionally, LLMVoX can adapt to new languages with minimal dataset adjustments and integrates with Vision-Language Models to support speech, text, and vision functionalities without extra multimodal training.'}, 'zh': {'title': 'LLMVoX：高效语音合成的新选择', 'desc': '本文提出了一种名为LLMVoX的轻量级语音合成系统，具有3000万参数，能够与任何大型语言模型（LLM）兼容。LLMVoX通过多队列令牌流系统，将语音合成与LLM处理解耦，从而实现低延迟和高质量的语音生成。与现有的语音增强LLM相比，LLMVoX在词错误率上显著降低，同时保持相似的延迟和用户满意度评分。该系统还支持无缝的无限长度对话，并能够通过数据集适应轻松扩展到新语言。'}}}, {'id': 'https://huggingface.co/papers/2503.03803', 'title': 'EgoLife: Towards Egocentric Life Assistant', 'url': 'https://huggingface.co/papers/2503.03803', 'abstract': 'We introduce EgoLife, a project to develop an egocentric life assistant that accompanies and enhances personal efficiency through AI-powered wearable glasses. To lay the foundation for this assistant, we conducted a comprehensive data collection study where six participants lived together for one week, continuously recording their daily activities - including discussions, shopping, cooking, socializing, and entertainment - using AI glasses for multimodal egocentric video capture, along with synchronized third-person-view video references. This effort resulted in the EgoLife Dataset, a comprehensive 300-hour egocentric, interpersonal, multiview, and multimodal daily life dataset with intensive annotation. Leveraging this dataset, we introduce EgoLifeQA, a suite of long-context, life-oriented question-answering tasks designed to provide meaningful assistance in daily life by addressing practical questions such as recalling past relevant events, monitoring health habits, and offering personalized recommendations. To address the key technical challenges of (1) developing robust visual-audio models for egocentric data, (2) enabling identity recognition, and (3) facilitating long-context question answering over extensive temporal information, we introduce EgoButler, an integrated system comprising EgoGPT and EgoRAG. EgoGPT is an omni-modal model trained on egocentric datasets, achieving state-of-the-art performance on egocentric video understanding. EgoRAG is a retrieval-based component that supports answering ultra-long-context questions. Our experimental studies verify their working mechanisms and reveal critical factors and bottlenecks, guiding future improvements. By releasing our datasets, models, and benchmarks, we aim to stimulate further research in egocentric AI assistants.', 'score': 28, 'issue_id': 2581, 'pub_date': '2025-03-05', 'pub_date_card': {'ru': '5 марта', 'en': 'March 5', 'zh': '3月5日'}, 'hash': '52396234365a3fb0', 'authors': ['Jingkang Yang', 'Shuai Liu', 'Hongming Guo', 'Yuhao Dong', 'Xiamengwei Zhang', 'Sicheng Zhang', 'Pengyun Wang', 'Zitang Zhou', 'Binzhu Xie', 'Ziyue Wang', 'Bei Ouyang', 'Zhengyu Lin', 'Marco Cominelli', 'Zhongang Cai', 'Yuanhan Zhang', 'Peiyuan Zhang', 'Fangzhou Hong', 'Joerg Widmer', 'Francesco Gringoli', 'Lei Yang', 'Bo Li', 'Ziwei Liu'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2503.03803.jpg', 'data': {'categories': ['#video', '#long_context', '#dataset', '#open_source', '#agents', '#multimodal', '#data', '#benchmark'], 'emoji': '👓', 'ru': {'title': 'EgoLife: ИИ-ассистент для повседневной жизни на базе умных очков', 'desc': 'Проект EgoLife представляет собой разработку эгоцентричного ассистента на основе очков дополненной реальности с искусственным интеллектом. Исследователи собрали обширный набор данных EgoLife Dataset, включающий 300 часов эгоцентричного видео повседневной жизни шести участников. На основе этих данных создан набор задач EgoLifeQA для тестирования вопросно-ответных систем в контексте повседневной жизни. Для решения технических задач разработана система EgoButler, включающая мультимодальную модель EgoGPT и компонент для ответов на вопросы с длинным контекстом EgoRAG.'}, 'en': {'title': 'Empowering Daily Life with Egocentric AI Assistance', 'desc': 'The paper presents EgoLife, an AI-powered life assistant designed to enhance personal efficiency through wearable glasses that capture egocentric video data. A comprehensive dataset, the EgoLife Dataset, was created by recording daily activities of participants over a week, resulting in 300 hours of multimodal data with detailed annotations. The study introduces EgoLifeQA, a set of question-answering tasks that utilize this dataset to assist users with practical life questions and personalized recommendations. To tackle challenges in visual-audio model development and long-context question answering, the authors propose EgoButler, which includes EgoGPT for video understanding and EgoRAG for retrieval-based answering.'}, 'zh': {'title': '智能生活助手，提升个人效率', 'desc': '我们介绍了EgoLife项目，旨在开发一个以自我为中心的生活助手，通过AI驱动的可穿戴眼镜提升个人效率。我们进行了全面的数据收集研究，六名参与者共同生活一周，使用AI眼镜记录日常活动，形成了EgoLife数据集，包含300小时的多视角、多模态日常生活数据。基于该数据集，我们推出了EgoLifeQA，一个针对生活的长文本问答任务，旨在提供实用的日常生活帮助。为了解决关键技术挑战，我们引入了EgoButler系统，包括EgoGPT和EgoRAG，前者在自我中心视频理解上表现出色，后者支持超长文本问题的回答。'}}}, {'id': 'https://huggingface.co/papers/2503.02972', 'title': 'LINGOLY-TOO: Disentangling Memorisation from Reasoning with Linguistic Templatisation and Orthographic Obfuscation', 'url': 'https://huggingface.co/papers/2503.02972', 'abstract': 'Effective evaluation of the reasoning capabilities of large language models (LLMs) are susceptible to overestimation due to data exposure of evaluation benchmarks. We introduce a framework for producing linguistic reasoning problems that reduces the effect of memorisation in model performance estimates and apply this framework to develop LINGOLY-TOO, a challenging evaluation benchmark for linguistic reasoning. By developing orthographic templates, we dynamically obfuscate the writing systems of real languages to generate numerous question variations. These variations preserve the reasoning steps required for each solution while reducing the likelihood of specific problem instances appearing in model training data. Our experiments demonstrate that frontier models, including OpenAI o1-preview and DeepSeem R1, struggle with advanced reasoning. Our analysis also shows that LLMs exhibit noticeable variance in accuracy across permutations of the same problem, and on average perform better on questions appearing in their original orthography. Our findings highlight the opaque nature of response generation in LLMs and provide evidence that prior data exposure contributes to overestimating the reasoning capabilities of frontier models.', 'score': 22, 'issue_id': 2585, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 марта', 'en': 'March 4', 'zh': '3月4日'}, 'hash': '0eb195e2704f3d5d', 'authors': ['Jude Khouja', 'Karolina Korgul', 'Simi Hellsten', 'Lingyi Yang', 'Vlad Neacs', 'Harry Mayne', 'Ryan Kearns', 'Andrew Bean', 'Adam Mahdi'], 'affiliations': ['Asia-Pacific Linguistics Olympiad', 'Hong Kong Linguistics Olympiad', 'National University of Science and Technology POLITEHNICA Bucharest, Romania', 'United Kingdom Linguistics Olympiad', 'University of Glasgow, Glasgow, United Kingdom', 'University of Oxford, Oxford, United Kingdom'], 'pdf_title_img': 'assets/pdf/title_img/2503.02972.jpg', 'data': {'categories': ['#reasoning', '#dataset', '#hallucinations', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'Новый метод оценки рассуждений LLM без влияния предварительных знаний', 'desc': 'Статья представляет новый подход к оценке способностей больших языковых моделей (LLM) к рассуждению. Авторы разработали фреймворк LINGOLY-TOO, который генерирует лингвистические задачи с обфускацией систем письма реальных языков. Эксперименты показали, что современные модели, включая OpenAI и DeepSeem, испытывают трудности с продвинутыми рассуждениями. Исследование выявило, что предварительное знакомство с данными может приводить к переоценке возможностей LLM в области рассуждений.'}, 'en': {'title': 'Unmasking Reasoning: Evaluating LLMs Beyond Memorization', 'desc': 'This paper addresses the challenge of accurately evaluating the reasoning abilities of large language models (LLMs) by introducing a new framework that minimizes the impact of memorization on performance assessments. The authors present LINGOLY-TOO, a benchmark designed to test linguistic reasoning through dynamically generated questions that obfuscate the original writing systems of languages. By using orthographic templates, they create multiple variations of questions that maintain the necessary reasoning steps while reducing the chance of models having seen specific instances during training. The results indicate that leading models struggle with complex reasoning tasks and show significant performance differences based on the question format, revealing the influence of prior data exposure on their evaluation.'}, 'zh': {'title': '揭示大型语言模型推理能力的真实面貌', 'desc': '本文提出了一种评估大型语言模型（LLMs）推理能力的新框架，旨在减少由于数据暴露导致的评估过高的问题。我们开发了LINGOLY-TOO，这是一个具有挑战性的语言推理评估基准，通过使用正字法模板动态模糊真实语言的书写系统，生成多种问题变体。实验结果表明，前沿模型在高级推理任务中表现不佳，并且在相同问题的不同排列中，LLMs的准确性存在显著差异。我们的研究揭示了LLMs响应生成的复杂性，并提供了证据表明，先前的数据暴露会导致对前沿模型推理能力的高估。'}}}, {'id': 'https://huggingface.co/papers/2503.04644', 'title': 'IFIR: A Comprehensive Benchmark for Evaluating Instruction-Following in Expert-Domain Information Retrieval', 'url': 'https://huggingface.co/papers/2503.04644', 'abstract': 'We introduce IFIR, the first comprehensive benchmark designed to evaluate instruction-following information retrieval (IR) in expert domains. IFIR includes 2,426 high-quality examples and covers eight subsets across four specialized domains: finance, law, healthcare, and science literature. Each subset addresses one or more domain-specific retrieval tasks, replicating real-world scenarios where customized instructions are critical. IFIR enables a detailed analysis of instruction-following retrieval capabilities by incorporating instructions at different levels of complexity. We also propose a novel LLM-based evaluation method to provide a more precise and reliable assessment of model performance in following instructions. Through extensive experiments on 15 frontier retrieval models, including those based on LLMs, our results reveal that current models face significant challenges in effectively following complex, domain-specific instructions. We further provide in-depth analyses to highlight these limitations, offering valuable insights to guide future advancements in retriever development.', 'score': 17, 'issue_id': 2585, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 марта', 'en': 'March 6', 'zh': '3月6日'}, 'hash': 'c4f19dc17abc0e8f', 'authors': ['Tingyu Song', 'Guo Gan', 'Mingsheng Shang', 'Yilun Zhao'], 'affiliations': ['Chongqing Institute of Green and Intelligent Technology, Chinese Academy of Sciences', 'School of Advanced Interdisciplinary Sciences, University of Chinese Academy of Sciences', 'Yale University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.04644.jpg', 'data': {'categories': ['#science', '#healthcare', '#dataset', '#benchmark'], 'emoji': '🔍', 'ru': {'title': 'IFIR: Новый стандарт оценки интеллектуального поиска информации', 'desc': 'IFIR - это новый комплексный бенчмарк для оценки информационного поиска с выполнением инструкций в экспертных областях. Он включает 2426 примеров высокого качества в 8 подмножествах из 4 специализированных доменов: финансы, право, здравоохранение и научная литература. IFIR позволяет проводить детальный анализ возможностей поиска с выполнением инструкций разной сложности. Эксперименты на 15 передовых моделях поиска показали значительные трудности в эффективном выполнении сложных доменно-специфичных инструкций.'}, 'en': {'title': 'IFIR: Benchmarking Instruction-Following in Expert Domains', 'desc': 'The paper presents IFIR, a new benchmark for assessing how well information retrieval systems can follow instructions in specialized fields like finance, law, healthcare, and science. It consists of 2,426 examples that simulate real-world retrieval tasks requiring tailored instructions. The benchmark allows for a nuanced evaluation of retrieval models, particularly focusing on their ability to handle varying complexities of instructions. The authors also introduce a new evaluation method using large language models (LLMs) to measure performance, revealing that existing models struggle with complex, domain-specific tasks and providing insights for future improvements.'}, 'zh': {'title': 'IFIR：专家领域指令跟随检索的首个基准', 'desc': '我们介绍了IFIR，这是第一个全面的基准，用于评估专家领域中的指令跟随信息检索（IR）。IFIR包含2426个高质量示例，涵盖金融、法律、医疗和科学文献等四个专业领域的八个子集。每个子集针对一个或多个特定领域的检索任务，模拟了需要定制指令的真实场景。通过对15个前沿检索模型的广泛实验，我们的结果显示，当前模型在有效跟随复杂的领域特定指令方面面临重大挑战。'}}}, {'id': 'https://huggingface.co/papers/2503.03983', 'title': 'Audio Flamingo 2: An Audio-Language Model with Long-Audio Understanding and Expert Reasoning Abilities', 'url': 'https://huggingface.co/papers/2503.03983', 'abstract': 'Understanding and reasoning over non-speech sounds and music are crucial for both humans and AI agents to interact effectively with their environments. In this paper, we introduce Audio Flamingo 2 (AF2), an Audio-Language Model (ALM) with advanced audio understanding and reasoning capabilities. AF2 leverages (i) a custom CLAP model, (ii) synthetic Audio QA data for fine-grained audio reasoning, and (iii) a multi-stage curriculum learning strategy. AF2 achieves state-of-the-art performance with only a 3B parameter small language model, surpassing large open-source and proprietary models across over 20 benchmarks. Next, for the first time, we extend audio understanding to long audio segments (30 secs to 5 mins) and propose LongAudio, a large and novel dataset for training ALMs on long audio captioning and question-answering tasks. Fine-tuning AF2 on LongAudio leads to exceptional performance on our proposed LongAudioBench, an expert annotated benchmark for evaluating ALMs on long audio understanding capabilities. We conduct extensive ablation studies to confirm the efficacy of our approach. Project Website: https://research.nvidia.com/labs/adlr/AF2/.', 'score': 17, 'issue_id': 2581, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 марта', 'en': 'March 6', 'zh': '3月6日'}, 'hash': '952e4cb72ec34df8', 'authors': ['Sreyan Ghosh', 'Zhifeng Kong', 'Sonal Kumar', 'S Sakshi', 'Jaehyeon Kim', 'Wei Ping', 'Rafael Valle', 'Dinesh Manocha', 'Bryan Catanzaro'], 'affiliations': ['NVIDIA, Santa Clara, CA, USA', 'University of Maryland, College Park, MD, USA'], 'pdf_title_img': 'assets/pdf/title_img/2503.03983.jpg', 'data': {'categories': ['#audio', '#long_context', '#dataset', '#small_models', '#reasoning', '#synthetic', '#open_source', '#benchmark'], 'emoji': '🎵', 'ru': {'title': 'AF2: Революция в понимании аудио искусственным интеллектом', 'desc': 'Audio Flamingo 2 (AF2) - это усовершенствованная аудио-языковая модель (ALM) с продвинутыми возможностями понимания и рассуждения об аудио. Модель использует специальную модель CLAP, синтетические данные Audio QA и многоступенчатую стратегию обучения. AF2 достигает наилучших результатов среди существующих моделей на более чем 20 тестах, используя всего 3 миллиарда параметров. Кроме того, исследователи расширили возможности модели для работы с длинными аудиосегментами и создали набор данных LongAudio для обучения ALM на задачах описания и ответов на вопросы по длинным аудио.'}, 'en': {'title': 'Revolutionizing Audio Understanding with Audio Flamingo 2', 'desc': 'This paper presents Audio Flamingo 2 (AF2), an advanced Audio-Language Model (ALM) designed to enhance audio understanding and reasoning. AF2 utilizes a custom CLAP model and synthetic Audio QA data to improve fine-grained audio reasoning, along with a multi-stage curriculum learning approach. The model achieves state-of-the-art results with a relatively small 3B parameter language model, outperforming larger models on over 20 benchmarks. Additionally, it introduces LongAudio, a new dataset for training on long audio segments, and demonstrates exceptional performance on the LongAudioBench for evaluating long audio understanding.'}, 'zh': {'title': '音频理解的新突破：Audio Flamingo 2', 'desc': '本文介绍了Audio Flamingo 2（AF2），这是一种具有先进音频理解和推理能力的音频语言模型（ALM）。AF2利用了定制的CLAP模型、合成的音频问答数据以及多阶段的课程学习策略。AF2在仅使用一个3B参数的小型语言模型的情况下，超越了20多个基准测试中的大型开源和专有模型，达到了最先进的性能。此外，AF2首次扩展了对长音频片段（30秒到5分钟）的理解，并提出了LongAudio数据集，用于训练ALM在长音频标注和问答任务上的能力。'}}}, {'id': 'https://huggingface.co/papers/2502.20258', 'title': 'LLM as a Broken Telephone: Iterative Generation Distorts Information', 'url': 'https://huggingface.co/papers/2502.20258', 'abstract': 'As large language models are increasingly responsible for online content, concerns arise about the impact of repeatedly processing their own outputs. Inspired by the "broken telephone" effect in chained human communication, this study investigates whether LLMs similarly distort information through iterative generation. Through translation-based experiments, we find that distortion accumulates over time, influenced by language choice and chain complexity. While degradation is inevitable, it can be mitigated through strategic prompting techniques. These findings contribute to discussions on the long-term effects of AI-mediated information propagation, raising important questions about the reliability of LLM-generated content in iterative workflows.', 'score': 17, 'issue_id': 2580, 'pub_date': '2025-02-27', 'pub_date_card': {'ru': '27 февраля', 'en': 'February 27', 'zh': '2月27日'}, 'hash': '5b26055854ae3d90', 'authors': ['Amr Mohamed', 'Mingmeng Geng', 'Michalis Vazirgiannis', 'Guokan Shang'], 'affiliations': ['Ecole Polytechnique', 'MBZUAI', 'SISSA'], 'pdf_title_img': 'assets/pdf/title_img/2502.20258.jpg', 'data': {'categories': ['#hallucinations', '#data', '#long_context', '#alignment', '#multimodal', '#training'], 'emoji': '📞', 'ru': {'title': "Эффект 'испорченного телефона' в языковых моделях", 'desc': 'Исследование посвящено изучению эффекта искажения информации при многократной обработке собственных выходных данных большими языковыми моделями (LLM). Авторы провели эксперименты на основе переводов, чтобы оценить накопление искажений в зависимости от выбора языка и сложности цепочки обработки. Результаты показывают, что деградация информации неизбежна, но может быть смягчена с помощью стратегических методов формулировки запросов. Исследование поднимает важные вопросы о надежности контента, генерируемого LLM в итеративных рабочих процессах.'}, 'en': {'title': 'Mitigating Distortion in Iterative LLM Outputs', 'desc': "This paper explores how large language models (LLMs) can distort information when they repeatedly process their own outputs, similar to the 'broken telephone' effect seen in human communication. The researchers conducted translation-based experiments to observe how information degradation occurs over time, which is affected by the choice of language and the complexity of the generation chain. They found that while some level of distortion is unavoidable, it can be reduced by using specific prompting strategies. These insights highlight the potential risks of relying on LLMs for generating content in iterative processes, raising concerns about the accuracy of AI-generated information."}, 'zh': {'title': '探讨大型语言模型的信息扭曲与可靠性', 'desc': '本研究探讨了大型语言模型（LLM）在反复处理自身输出时是否会扭曲信息，类似于人类沟通中的“破电话”效应。通过基于翻译的实验，我们发现信息扭曲会随着时间的推移而累积，受语言选择和链条复杂性的影响。尽管信息退化是不可避免的，但通过战略性提示技术可以减轻这种影响。研究结果为AI介导的信息传播的长期影响提供了重要见解，提出了关于LLM生成内容在迭代工作流程中可靠性的重要问题。'}}}, {'id': 'https://huggingface.co/papers/2503.04725', 'title': 'L$^2$M: Mutual Information Scaling Law for Long-Context Language Modeling', 'url': 'https://huggingface.co/papers/2503.04725', 'abstract': "We rigorously establish a bipartite mutual information scaling law in natural language that governs long-range dependencies. This scaling law, which we show is distinct from and scales independently of the conventional two-point mutual information, is the key to understanding long-context language modeling. Using this scaling law, we formulate the Long-context Language Modeling (L^2M) condition, which relates a model's capacity for effective long context length modeling to the scaling of its latent state size for storing past information. Our results are validated through experiments on both transformers and state space models. This work establishes a theoretical foundation that guides the development of large language models toward longer context lengths.", 'score': 15, 'issue_id': 2582, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 марта', 'en': 'March 6', 'zh': '3月6日'}, 'hash': '51e8f1332666da31', 'authors': ['Zhuo Chen', 'Oriol Mayné i Comas', 'Zhuotao Jin', 'Di Luo', 'Marin Soljačić'], 'affiliations': ['Harvard University', 'Massachusetts Institute of Technology', 'NSF AI Institute for Artificial Intelligence and Fundamental Interactions', 'Polytechnic University of Catalonia', 'University of California, Los Angeles'], 'pdf_title_img': 'assets/pdf/title_img/2503.04725.jpg', 'data': {'categories': ['#training', '#architecture', '#math', '#long_context'], 'emoji': '📏', 'ru': {'title': 'Масштабирование взаимной информации - ключ к моделированию длинного контекста', 'desc': 'Статья представляет собой исследование в области обработки естественного языка, фокусирующееся на закономерностях взаимной информации в длинных текстах. Авторы формулируют условие L^2M для моделирования длинного контекста, связывающее способность модели обрабатывать длинные последовательности с масштабированием ее скрытого состояния. Результаты подтверждены экспериментами на трансформерах и моделях пространства состояний. Работа закладывает теоретическую основу для развития больших языковых моделей с увеличенной длиной контекста.'}, 'en': {'title': 'Unlocking Long-Range Dependencies in Language Models', 'desc': "This paper introduces a new scaling law for bipartite mutual information that is crucial for understanding long-range dependencies in natural language processing. Unlike traditional two-point mutual information, this scaling law operates independently and is essential for effective long-context language modeling. The authors propose the Long-context Language Modeling (L^2M) condition, which connects a model's ability to handle long contexts with the size of its latent state for retaining past information. Experimental validation on transformers and state space models supports the theoretical framework, paving the way for advancements in large language models with extended context lengths."}, 'zh': {'title': '长上下文建模的新法则', 'desc': '本文严谨地建立了自然语言中的双向互信息缩放法则，该法则控制着长距离依赖关系。我们展示了这一缩放法则与传统的两点互信息不同，并且独立缩放，是理解长上下文语言建模的关键。通过这一缩放法则，我们提出了长上下文语言建模（L^2M）条件，将模型有效建模长上下文长度的能力与其存储过去信息的潜在状态大小的缩放联系起来。我们的结果通过对变换器和状态空间模型的实验得到了验证，为大型语言模型向更长上下文长度的发展奠定了理论基础。'}}}, {'id': 'https://huggingface.co/papers/2503.04598', 'title': 'HybridNorm: Towards Stable and Efficient Transformer Training via Hybrid Normalization', 'url': 'https://huggingface.co/papers/2503.04598', 'abstract': 'Transformers have become the de facto architecture for a wide range of machine learning tasks, particularly in large language models (LLMs). Despite their remarkable performance, challenges remain in training deep transformer networks, especially regarding the location of layer normalization. While Pre-Norm structures facilitate easier training due to their more prominent identity path, they often yield suboptimal performance compared to Post-Norm. In this paper, we propose HybridNorm, a straightforward yet effective hybrid normalization strategy that integrates the advantages of both Pre-Norm and Post-Norm approaches. Specifically, HybridNorm employs QKV normalization within the attention mechanism and Post-Norm in the feed-forward network (FFN) of each transformer block. This design not only stabilizes training but also enhances performance, particularly in the context of LLMs. Comprehensive experiments in both dense and sparse architectures show that HybridNorm consistently outperforms both Pre-Norm and Post-Norm approaches, achieving state-of-the-art results across various benchmarks. These findings highlight the potential of HybridNorm as a more stable and effective technique for improving the training and performance of deep transformer models. %Code will be made publicly available. Code is available at https://github.com/BryceZhuo/HybridNorm.', 'score': 15, 'issue_id': 2579, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 марта', 'en': 'March 6', 'zh': '3月6日'}, 'hash': '78b05ed4e27a874b', 'authors': ['Zhijian Zhuo', 'Yutao Zeng', 'Ya Wang', 'Sijun Zhang', 'Jian Yang', 'Xiaoqing Li', 'Xun Zhou', 'Jinwen Ma'], 'affiliations': ['Capital University of Economics and Business', 'School of Mathematical Sciences, Peking University', 'SeedFoundation-Model, ByteDance'], 'pdf_title_img': 'assets/pdf/title_img/2503.04598.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#training', '#architecture', '#open_source'], 'emoji': '🔀', 'ru': {'title': 'HybridNorm: Гибридная нормализация для улучшения обучения и производительности трансформеров', 'desc': 'Статья представляет новый метод нормализации для трансформеров под названием HybridNorm. Этот подход сочетает преимущества Pre-Norm и Post-Norm стратегий, применяя QKV нормализацию в механизме внимания и Post-Norm в FFN каждого блока трансформера. HybridNorm показывает лучшие результаты по сравнению с существующими методами как для плотных, так и для разреженных архитектур. Эксперименты демонстрируют, что HybridNorm обеспечивает более стабильное обучение и улучшенную производительность для глубоких моделей трансформеров, особенно в контексте больших языковых моделей.'}, 'en': {'title': 'HybridNorm: The Best of Both Normalization Worlds for Transformers', 'desc': 'This paper introduces HybridNorm, a new normalization strategy for training deep transformer networks, which combines the strengths of Pre-Norm and Post-Norm methods. Pre-Norm structures help with training stability but often lead to lower performance, while Post-Norm structures typically perform better but can complicate training. HybridNorm applies QKV normalization in the attention mechanism and Post-Norm in the feed-forward network, resulting in improved training stability and performance. Experiments demonstrate that HybridNorm outperforms both Pre-Norm and Post-Norm across various benchmarks, making it a promising approach for enhancing large language models.'}, 'zh': {'title': 'HybridNorm：提升变换器模型训练的稳定性与性能', 'desc': '本文提出了一种新的混合归一化策略，称为HybridNorm，旨在解决深度变换器网络训练中的层归一化位置问题。HybridNorm结合了Pre-Norm和Post-Norm的优点，在注意力机制中使用QKV归一化，而在前馈网络中使用Post-Norm。实验结果表明，HybridNorm在稠密和稀疏架构中均优于传统的Pre-Norm和Post-Norm方法，提升了大语言模型的训练稳定性和性能。该研究为深度变换器模型的训练和性能改进提供了新的思路。'}}}, {'id': 'https://huggingface.co/papers/2503.04222', 'title': 'FuseChat-3.0: Preference Optimization Meets Heterogeneous Model Fusion', 'url': 'https://huggingface.co/papers/2503.04222', 'abstract': 'We introduce FuseChat-3.0, a suite of large language models (LLMs) developed by integrating the strengths of heterogeneous source LLMs into more compact target LLMs. Our source models include the powerful Gemma-2-27B-it, Mistral-Large-Instruct-2407, Qwen-2.5-72B-Instruct, and Llama-3.1-70B-Instruct. For target models, we focus on three widely-used smaller variants-Llama-3.1-8B-Instruct, Gemma-2-9B-it, and Qwen-2.5-7B-Instruct-along with two ultra-compact options, Llama-3.2-3B-Instruct and Llama-3.2-1B-Instruct. To leverage the diverse capabilities of these source models, we develop a specialized data construction protocol tailored to various tasks and domains. The FuseChat-3.0 training pipeline consists of two key stages: (1) supervised fine-tuning (SFT) to align the target and source model distributions, and (2) Direct Preference Optimization (DPO) to apply preferences from multiple source LLMs to fine-tune the target model. The resulting FuseChat-3.0 models exhibit significant performance gains across tasks such as instruction following, general knowledge, mathematics, and coding. As illustrated in Figure 1, using Llama-3.1-8B-Instruct as the target model, our fusion approach achieves an average improvement of 6.8 points across 14 benchmarks. Moreover, it demonstrates remarkable gains of 37.1 points and 30.1 points on the instruction-following benchmarks AlpacaEval-2 and Arena-Hard, respectively. Our code, models, and datasets are available at https://github.com/SLIT-AI/FuseChat-3.0.', 'score': 11, 'issue_id': 2578, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 марта', 'en': 'March 6', 'zh': '3月6日'}, 'hash': 'c7d793a0b91efd5d', 'authors': ['Ziyi Yang', 'Fanqi Wan', 'Longguang Zhong', 'Canbin Huang', 'Guosheng Liang', 'Xiaojun Quan'], 'affiliations': ['School of Computer Science and Engineering, Sun Yat-sen University, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.04222.jpg', 'data': {'categories': ['#training', '#dataset', '#benchmark', '#small_models', '#rlhf', '#transfer_learning', '#open_source', '#optimization'], 'emoji': '🔀', 'ru': {'title': 'Слияние мощи больших языковых моделей в компактном формате', 'desc': 'FuseChat-3.0 представляет собой набор больших языковых моделей (LLM), разработанных путем интеграции сильных сторон гетерогенных исходных LLM в более компактные целевые модели. Процесс обучения включает два ключевых этапа: контролируемая тонкая настройка и оптимизация прямых предпочтений. Результирующие модели FuseChat-3.0 демонстрируют значительный прирост производительности в различных задачах, включая следование инструкциям, общие знания, математику и программирование. Используя Llama-3.1-8B-Instruct в качестве целевой модели, подход авторов достигает среднего улучшения на 6,8 пунктов по 14 бенчмаркам.'}, 'en': {'title': 'Fusing Strengths for Smarter, Smaller Models', 'desc': 'FuseChat-3.0 is a collection of large language models (LLMs) that combines the strengths of various larger source models into smaller, more efficient target models. The training process involves supervised fine-tuning to align the target models with the source models, followed by Direct Preference Optimization to enhance performance based on preferences from multiple sources. This innovative approach leads to significant improvements in tasks like instruction following, general knowledge, mathematics, and coding. The results show an average performance boost of 6.8 points across 14 benchmarks, with particularly impressive gains in instruction-following tasks.'}, 'zh': {'title': '融合多源模型，提升语言理解能力', 'desc': '我们介绍了FuseChat-3.0，这是一个通过整合不同来源的大型语言模型（LLMs）优势而开发的更紧凑的目标LLM套件。源模型包括强大的Gemma-2-27B-it、Mistral-Large-Instruct-2407、Qwen-2.5-72B-Instruct和Llama-3.1-70B-Instruct。目标模型则集中在三种广泛使用的小型变体上，以及两个超紧凑选项。通过专门的数据构建协议和两阶段的训练流程，FuseChat-3.0在多个任务上表现出显著的性能提升。'}}}, {'id': 'https://huggingface.co/papers/2503.04094', 'title': 'PokéChamp: an Expert-level Minimax Language Agent', 'url': 'https://huggingface.co/papers/2503.04094', 'abstract': "We introduce Pok\\'eChamp, a minimax agent powered by Large Language Models (LLMs) for Pok\\'emon battles. Built on a general framework for two-player competitive games, Pok\\'eChamp leverages the generalist capabilities of LLMs to enhance minimax tree search. Specifically, LLMs replace three key modules: (1) player action sampling, (2) opponent modeling, and (3) value function estimation, enabling the agent to effectively utilize gameplay history and human knowledge to reduce the search space and address partial observability. Notably, our framework requires no additional LLM training. We evaluate Pok\\'eChamp in the popular Gen 9 OU format. When powered by GPT-4o, it achieves a win rate of 76% against the best existing LLM-based bot and 84% against the strongest rule-based bot, demonstrating its superior performance. Even with an open-source 8-billion-parameter Llama 3.1 model, Pok\\'eChamp consistently outperforms the previous best LLM-based bot, Pok\\'ellmon powered by GPT-4o, with a 64% win rate. Pok\\'eChamp attains a projected Elo of 1300-1500 on the Pok\\'emon Showdown online ladder, placing it among the top 30%-10% of human players. In addition, this work compiles the largest real-player Pok\\'emon battle dataset, featuring over 3 million games, including more than 500k high-Elo matches. Based on this dataset, we establish a series of battle benchmarks and puzzles to evaluate specific battling skills. We further provide key updates to the local game engine. We hope this work fosters further research that leverage Pok\\'emon battle as benchmark to integrate LLM technologies with game-theoretic algorithms addressing general multiagent problems. Videos, code, and dataset available at https://sites.google.com/view/pokechamp-llm.", 'score': 9, 'issue_id': 2580, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 марта', 'en': 'March 6', 'zh': '3月6日'}, 'hash': 'bffe348d8443d4c2', 'authors': ['Seth Karten', 'Andy Luu Nguyen', 'Chi Jin'], 'affiliations': ['Department of Computer Science, Princeton University', 'Department of Electrical and Computer Engineering, Princeton University'], 'pdf_title_img': 'assets/pdf/title_img/2503.04094.jpg', 'data': {'categories': ['#open_source', '#benchmark', '#dataset', '#agents', '#games'], 'emoji': '🎮', 'ru': {'title': 'PokeChamp: Революция в ИИ для игровых стратегий с использованием языковых моделей', 'desc': 'Статья представляет PokeChamp - агента на основе минимакса, использующего большие языковые модели (LLM) для боев в Pokémon. PokeChamp применяет LLM для выборки действий игрока, моделирования оппонента и оценки функции ценности, что позволяет эффективно использовать историю игры и знания человека. При использовании GPT-4o агент достигает высокого процента побед против существующих ботов и попадает в топ-30% - 10% игроков на онлайн-платформе Pokémon Showdown. Исследование также включает создание крупнейшего датасета боев Pokémon и серии тестов для оценки навыков ведения боя.'}, 'en': {'title': "Pok'eChamp: Elevating Pokémon Battles with LLMs", 'desc': "Pok'eChamp is a minimax agent designed for Pokémon battles that utilizes Large Language Models (LLMs) to improve its decision-making process. It replaces traditional components of the minimax algorithm with LLMs for player action sampling, opponent modeling, and value function estimation, allowing it to better understand gameplay history and human strategies. The agent demonstrates impressive performance, achieving a 76% win rate against top LLM-based bots and a 64% win rate with a smaller model, showcasing its effectiveness in competitive play. Additionally, Pok'eChamp compiles a large dataset of over 3 million Pokémon battles to create benchmarks for evaluating battling skills and encourages further research in integrating LLMs with game-theoretic approaches."}, 'zh': {'title': 'PokéChamp：宝可梦对战中的智能代理', 'desc': '本文介绍了PokéChamp，一个基于大型语言模型（LLMs）的极小极大算法代理，用于宝可梦对战。该代理利用LLMs的通用能力，增强了极小极大树搜索，替代了玩家动作采样、对手建模和价值函数估计等关键模块。PokéChamp在不需要额外训练的情况下，能够有效利用游戏历史和人类知识，减少搜索空间并解决部分可观测性问题。经过评估，PokéChamp在宝可梦对战中表现优异，赢得了76%的胜率，展示了其在多智能体问题中的潜力。'}}}, {'id': 'https://huggingface.co/papers/2503.01917', 'title': 'How to Steer LLM Latents for Hallucination Detection?', 'url': 'https://huggingface.co/papers/2503.01917', 'abstract': "Hallucinations in LLMs pose a significant concern to their safe deployment in real-world applications. Recent approaches have leveraged the latent space of LLMs for hallucination detection, but their embeddings, optimized for linguistic coherence rather than factual accuracy, often fail to clearly separate truthful and hallucinated content. To this end, we propose the Truthfulness Separator Vector (TSV), a lightweight and flexible steering vector that reshapes the LLM's representation space during inference to enhance the separation between truthful and hallucinated outputs, without altering model parameters. Our two-stage framework first trains TSV on a small set of labeled exemplars to form compact and well-separated clusters. It then augments the exemplar set with unlabeled LLM generations, employing an optimal transport-based algorithm for pseudo-labeling combined with a confidence-based filtering process. Extensive experiments demonstrate that TSV achieves state-of-the-art performance with minimal labeled data, exhibiting strong generalization across datasets and providing a practical solution for real-world LLM applications.", 'score': 8, 'issue_id': 2592, 'pub_date': '2025-03-01', 'pub_date_card': {'ru': '1 марта', 'en': 'March 1', 'zh': '3月1日'}, 'hash': 'cbd43445771d58c3', 'authors': ['Seongheon Park', 'Xuefeng Du', 'Min-Hsuan Yeh', 'Haobo Wang', 'Yixuan Li'], 'affiliations': ['Department of Computer Sciences, University of Wisconsin-Madison, USA', 'School of Software Technology, Zhejiang University, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.01917.jpg', 'data': {'categories': ['#training', '#inference', '#hallucinations'], 'emoji': '🔍', 'ru': {'title': 'TSV: Эффективное разделение правды и вымысла в LLM', 'desc': 'Статья представляет новый метод обнаружения галлюцинаций в больших языковых моделях (LLM) - Truthfulness Separator Vector (TSV). TSV - это легковесный вектор, который изменяет пространство представлений LLM во время вывода, чтобы улучшить разделение между правдивым и галлюцинированным содержанием. Метод использует двухэтапный подход: сначала TSV обучается на небольшом наборе размеченных примеров, а затем применяется алгоритм псевдо-разметки на основе оптимального транспорта для расширения набора данных. Эксперименты показывают, что TSV достигает современного уровня производительности с минимальным количеством размеченных данных.'}, 'en': {'title': 'Enhancing Truthfulness in LLMs with TSV', 'desc': 'This paper addresses the issue of hallucinations in large language models (LLMs), which can lead to the generation of false information. The authors introduce the Truthfulness Separator Vector (TSV), a novel method that modifies the representation space of LLMs during inference to better distinguish between accurate and hallucinated content. TSV is trained using a small set of labeled examples and then enhanced with additional unlabeled data through a unique pseudo-labeling technique. The results show that TSV significantly improves the accuracy of LLM outputs while requiring minimal labeled data, making it a valuable tool for safe LLM deployment.'}, 'zh': {'title': '提升LLM真实与幻觉内容分离的真相分离向量', 'desc': '在大型语言模型（LLMs）中，幻觉现象对其在实际应用中的安全部署构成了重大挑战。虽然最近的方法利用了LLMs的潜在空间进行幻觉检测，但其嵌入通常更注重语言连贯性而非事实准确性，导致难以清晰区分真实和幻觉内容。为此，我们提出了真相分离向量（TSV），这是一种轻量且灵活的引导向量，可以在推理过程中重塑LLM的表示空间，从而增强真实输出与幻觉输出之间的分离。我们的两阶段框架首先在小规模标记样本上训练TSV，以形成紧凑且分离良好的聚类，然后通过基于最优传输的伪标记算法和基于置信度的过滤过程，利用未标记的LLM生成数据来扩展样本集。'}}}, {'id': 'https://huggingface.co/papers/2503.02495', 'title': 'Union of Experts: Adapting Hierarchical Routing to Equivalently Decomposed Transformer', 'url': 'https://huggingface.co/papers/2503.02495', 'abstract': "Mixture-of-Experts (MoE) enhances model performance while maintaining computational efficiency, making it well-suited for large-scale applications. However, expert in exist MoE paradigm works as an individual, thereby lacking high-quality expert interactions. Moreover, they have not been effectively extended to attention block, which constrains further efficiency improvements. To tackle these issues, we propose Union-of-Experts (UoE), which decomposes transformer into an equitant group of experts, and then implement dynamic routing on input data and experts. Our approach advances MoE design with three key innovations: (1) We conducted equitant expert decomposition on both MLP blocks and attention blocks based on matrix partition in tensor parallelism. (2) We developed two routing paradigms: patch wise data selection and expert selection, to apply routing across different levels. (3) We design the architecture of UoE model, including Selective Multi-Head Attention (SMHA) and Union-of-MLP-Experts (UoME). (4) We develop parallel implementation of UoE's routing and computation operation, and optimize efficiency based on the hardware processing analysis. The experiments demonstrate that the model employed with UoE surpass Full Attention, state-of-art MoEs and efficient transformers in several tasks across image and natural language domains. The source codes are available at https://github.com/YujiaoYang-work/UoE.", 'score': 7, 'issue_id': 2586, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 марта', 'en': 'March 4', 'zh': '3月4日'}, 'hash': 'b879dd88adfef125', 'authors': ['Yujiao Yang', 'Jing Lian', 'Linhui Li'], 'affiliations': ['School of Mechanical Engineering, Dalian University of Technology, Dalian 116024, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.02495.jpg', 'data': {'categories': ['#training', '#optimization', '#architecture', '#open_source'], 'emoji': '🧠', 'ru': {'title': 'UoE: Новый уровень эффективности трансформеров через объединение экспертов', 'desc': 'Статья представляет новый подход к архитектуре трансформеров, называемый Union-of-Experts (UoE). UoE улучшает модель Mixture-of-Experts (MoE), разбивая трансформер на группы экспертов и применяя динамическую маршрутизацию как к входным данным, так и к экспертам. Ключевые инновации включают декомпозицию экспертов для блоков MLP и внимания, новые парадигмы маршрутизации и оптимизированную параллельную реализацию. Эксперименты показывают, что UoE превосходит существующие модели в задачах обработки изображений и естественного языка.'}, 'en': {'title': 'Union-of-Experts: Enhancing Efficiency and Interaction in Transformers', 'desc': 'The paper introduces Union-of-Experts (UoE), a novel approach that enhances the Mixture-of-Experts (MoE) framework by improving expert interactions and extending its application to attention blocks. UoE decomposes transformers into groups of experts and utilizes dynamic routing to optimize input data processing. Key innovations include equitant expert decomposition, two routing paradigms for data and expert selection, and a new architecture featuring Selective Multi-Head Attention and Union-of-MLP-Experts. Experimental results show that UoE outperforms existing models in various tasks, demonstrating its effectiveness in both image and natural language processing.'}, 'zh': {'title': '专家联合模型：提升效率与性能的创新之路', 'desc': '混合专家模型（MoE）通过提高模型性能并保持计算效率，适合大规模应用。然而，现有的MoE模型中的专家作为个体工作，缺乏高质量的专家交互。此外，MoE尚未有效扩展到注意力模块，这限制了进一步的效率提升。为了解决这些问题，我们提出了专家联合模型（UoE），通过将变换器分解为等效的专家组，并在输入数据和专家之间实现动态路由，从而改进了MoE的设计。'}}}, {'id': 'https://huggingface.co/papers/2503.01901', 'title': 'Identifying Sensitive Weights via Post-quantization Integral', 'url': 'https://huggingface.co/papers/2503.01901', 'abstract': "Serving Large Language Models (LLMs) is costly. However, post-training weight quantization can address this problem by both compressing their sizes for limited memory and saving bandwidth for acceleration. As not all weight dimensions are equally important, those methods typically rely on a sensitivity metric, which indicates the element-wise influence of weights on loss function and is used to preprocess original weights for better quantization. In this work, we conduct an empirical study on the accuracy of the sensitivity metric, and find that existing gradient and Hessian based metrics are very inaccurate: they underestimate quantization's impact on the loss function by orders of magnitude, mainly due to the small convergence radius of local 2nd order approximation, \\ie, gradient and Hessian term in Taylor's formula. To tackle this problem, we propose Post-quantization Integral (PQI), an accurate metric to estimate posterior sensitivity in a fine-grained manner. To leverage this accurate metric, we further propose ReQuant, a simple yet powerful framework that mainly consists of two Dense-and-Sparse detach components: self-adaptive outlier selection and step-wise significant weights detach. Results show that ReQuant boosts state-of-the-art post-training quantization methods, with a pronounced improvement of 2.66 perplexity gain on Llama 3.2 1B with QTIP.", 'score': 7, 'issue_id': 2585, 'pub_date': '2025-02-28', 'pub_date_card': {'ru': '28 февраля', 'en': 'February 28', 'zh': '2月28日'}, 'hash': '1045983ed982a00b', 'authors': ['Yuezhou Hu', 'Weiyu Huang', 'Zichen Liang', 'Chang Chen', 'Jintao Zhang', 'Jun Zhu', 'Jianfei Chen'], 'affiliations': ['Dept. of Comp. Sci. & Tech., Institute for AI, BNRist Center, Tsinghua-Bosch Joint ML Center, THBI Lab, Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.01901.jpg', 'data': {'categories': ['#training', '#inference', '#optimization'], 'emoji': '⚖️', 'ru': {'title': 'Точная квантизация для эффективных языковых моделей', 'desc': 'Эта статья представляет новый метод квантизации весов для крупных языковых моделей (LLM), называемый ReQuant. Авторы обнаружили, что существующие метрики чувствительности весов неточны и предложили более точную метрику - Post-quantization Integral (PQI). ReQuant использует PQI вместе с адаптивным выбором выбросов и пошаговым отделением значимых весов. Результаты показывают, что ReQuant улучшает современные методы пост-тренировочной квантизации, значительно повышая производительность модели Llama 3.2 1B.'}, 'en': {'title': 'Enhancing LLM Efficiency with Accurate Quantization Metrics', 'desc': 'This paper addresses the high costs associated with serving large language models (LLMs) by introducing post-training weight quantization techniques. It highlights the inadequacy of existing sensitivity metrics, which fail to accurately predict the impact of quantization on model performance. The authors propose a new metric called Post-quantization Integral (PQI) that provides a more precise estimation of weight sensitivity. Additionally, they introduce ReQuant, a framework that enhances quantization methods by effectively selecting significant weights and improving overall model accuracy.'}, 'zh': {'title': '提升量化精度，降低模型成本', 'desc': '这篇论文讨论了大型语言模型（LLMs）在服务时的高成本问题。通过后训练权重量化，可以压缩模型大小，节省内存和带宽。研究发现，现有的基于梯度和海森矩阵的敏感度度量不够准确，低估了量化对损失函数的影响。为了解决这个问题，提出了后量化积分（PQI）作为一种更精确的敏感度度量，并进一步提出了ReQuant框架，以提高后训练量化方法的效果。'}}}, {'id': 'https://huggingface.co/papers/2503.04606', 'title': 'The Best of Both Worlds: Integrating Language Models and Diffusion Models for Video Generation', 'url': 'https://huggingface.co/papers/2503.04606', 'abstract': 'Recent advancements in text-to-video (T2V) generation have been driven by two competing paradigms: autoregressive language models and diffusion models. However, each paradigm has intrinsic limitations: language models struggle with visual quality and error accumulation, while diffusion models lack semantic understanding and causal modeling. In this work, we propose LanDiff, a hybrid framework that synergizes the strengths of both paradigms through coarse-to-fine generation. Our architecture introduces three key innovations: (1) a semantic tokenizer that compresses 3D visual features into compact 1D discrete representations through efficient semantic compression, achieving a sim14,000times compression ratio; (2) a language model that generates semantic tokens with high-level semantic relationships; (3) a streaming diffusion model that refines coarse semantics into high-fidelity videos. Experiments show that LanDiff, a 5B model, achieves a score of 85.43 on the VBench T2V benchmark, surpassing the state-of-the-art open-source models Hunyuan Video (13B) and other commercial models such as Sora, Keling, and Hailuo. Furthermore, our model also achieves state-of-the-art performance in long video generation, surpassing other open-source models in this field. Our demo can be viewed at https://landiff.github.io/.', 'score': 7, 'issue_id': 2580, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 марта', 'en': 'March 6', 'zh': '3月6日'}, 'hash': 'c635690f3edc1186', 'authors': ['Aoxiong Yin', 'Kai Shen', 'Yichong Leng', 'Xu Tan', 'Xinyu Zhou', 'Juncheng Li', 'Siliang Tang'], 'affiliations': ['College of Computer Science and Technology, Zhejiang University, Hangzhou, China', 'Moonshot AI, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.04606.jpg', 'data': {'categories': ['#diffusion', '#open_source', '#benchmark', '#long_context', '#architecture', '#video'], 'emoji': '🎬', 'ru': {'title': 'LanDiff: гибридный подход к генерации видео из текста', 'desc': 'Статья представляет LanDiff - гибридную архитектуру для генерации видео из текста, сочетающую преимущества авторегрессионных языковых моделей и диффузионных моделей. LanDiff включает семантический токенизатор, языковую модель для генерации семантических токенов и потоковую диффузионную модель для уточнения деталей. Модель LanDiff объемом 5 миллиардов параметров превзошла современные открытые и коммерческие модели в бенчмарке VBench T2V. Также LanDiff показала лучшие результаты в генерации длинных видео по сравнению с другими открытыми моделями.'}, 'en': {'title': 'LanDiff: Bridging Language and Visuals for Superior Video Generation', 'desc': 'This paper introduces LanDiff, a novel framework for text-to-video (T2V) generation that combines the strengths of autoregressive language models and diffusion models. It addresses the limitations of each approach by using a coarse-to-fine generation strategy, which enhances both visual quality and semantic understanding. Key innovations include a semantic tokenizer for efficient compression of visual features, a language model that generates meaningful semantic tokens, and a streaming diffusion model that refines these tokens into high-quality videos. The results demonstrate that LanDiff outperforms existing state-of-the-art models in T2V tasks, particularly in generating long videos.'}, 'zh': {'title': 'LanDiff：文本到视频生成的新突破', 'desc': '本文提出了一种名为LanDiff的混合框架，旨在结合自回归语言模型和扩散模型的优点，以实现文本到视频的生成。该框架通过粗到细的生成过程，克服了各自的局限性，提升了视觉质量和语义理解。LanDiff引入了三项关键创新，包括高效的语义压缩技术、生成高层语义关系的语言模型，以及将粗略语义精炼为高保真视频的流式扩散模型。实验结果表明，LanDiff在VBench T2V基准测试中表现优异，超越了现有的开源和商业模型。'}}}, {'id': 'https://huggingface.co/papers/2503.04378', 'title': 'Dedicated Feedback and Edit Models Empower Inference-Time Scaling for Open-Ended General-Domain Tasks', 'url': 'https://huggingface.co/papers/2503.04378', 'abstract': 'Inference-Time Scaling has been critical to the success of recent models such as OpenAI o1 and DeepSeek R1. However, many techniques used to train models for inference-time scaling require tasks to have answers that can be verified, limiting their application to domains such as math, coding and logical reasoning. We take inspiration from how humans make first attempts, ask for detailed feedback from others and make improvements based on such feedback across a wide spectrum of open-ended endeavors. To this end, we collect data for and train dedicated Feedback and Edit Models that are capable of performing inference-time scaling for open-ended general-domain tasks. In our setup, one model generates an initial response, which are given feedback by a second model, that are then used by a third model to edit the response. We show that performance on Arena Hard, a benchmark strongly predictive of Chatbot Arena Elo can be boosted by scaling the number of initial response drafts, effective feedback and edited responses. When scaled optimally, our setup based on 70B models from the Llama 3 family can reach SoTA performance on Arena Hard at 92.7 as of 5 Mar 2025, surpassing OpenAI o1-preview-2024-09-12 with 90.4 and DeepSeek R1 with 92.3.', 'score': 6, 'issue_id': 2578, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 марта', 'en': 'March 6', 'zh': '3月6日'}, 'hash': '926e56aa51a0fefe', 'authors': ['Zhilin Wang', 'Jiaqi Zeng', 'Olivier Delalleau', 'Daniel Egert', 'Ellie Evans', 'Hoo-Chang Shin', 'Felipe Soares', 'Yi Dong', 'Oleksii Kuchaiev'], 'affiliations': ['NVIDIA'], 'pdf_title_img': 'assets/pdf/title_img/2503.04378.jpg', 'data': {'categories': ['#training', '#benchmark', '#inference', '#reasoning', '#rlhf', '#optimization'], 'emoji': '🔄', 'ru': {'title': 'Масштабирование при выводе для открытых задач: от черновика к улучшенному ответу', 'desc': 'Статья описывает новый метод масштабирования во время вывода для открытых задач общего домена. Авторы предлагают систему из трех моделей: одна генерирует начальный ответ, вторая дает обратную связь, а третья редактирует ответ на основе этой обратной связи. Эксперименты показывают, что такой подход позволяет значительно улучшить производительность на бенчмарке Arena Hard. При оптимальном масштабировании система на основе моделей семейства Llama 3 размером 70B достигает наилучших результатов, превосходя OpenAI o1 и DeepSeek R1.'}, 'en': {'title': 'Enhancing Open-Ended Task Performance through Feedback and Editing', 'desc': 'This paper discusses a novel approach to improve inference-time scaling in machine learning models, particularly for open-ended tasks. It introduces a system where one model generates an initial response, a second model provides feedback, and a third model edits the response based on that feedback. This method allows for more flexible applications beyond traditional tasks that require verifiable answers, such as math or coding. The authors demonstrate that by optimizing the number of drafts, feedback, and edits, their model achieves state-of-the-art performance on the Arena Hard benchmark, outperforming previous models.'}, 'zh': {'title': '推理时间扩展：提升开放性任务的性能', 'desc': '本文探讨了推理时间扩展在机器学习模型中的重要性，尤其是OpenAI o1和DeepSeek R1等模型。许多现有技术要求任务的答案可验证，这限制了它们在开放性任务中的应用。我们借鉴人类如何进行初步尝试、请求反馈并根据反馈进行改进的过程，开发了专门的反馈和编辑模型。通过优化初始响应草稿、有效反馈和编辑响应的数量，我们的模型在Arena Hard基准测试中达到了92.7的最新性能，超越了其他模型。'}}}, {'id': 'https://huggingface.co/papers/2503.01375', 'title': 'Combining Flow Matching and Transformers for Efficient Solution of Bayesian Inverse Problems', 'url': 'https://huggingface.co/papers/2503.01375', 'abstract': 'Solving Bayesian inverse problems efficiently remains a significant challenge due to the complexity of posterior distributions and the computational cost of traditional sampling methods. Given a series of observations and the forward model, we want to recover the distribution of the parameters, conditioned on observed experimental data. We show, that combining Conditional Flow Mathching (CFM) with transformer-based architecture, we can efficiently sample from such kind of distribution, conditioned on variable number of observations.', 'score': 5, 'issue_id': 2583, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': '9dd35b1faaa32b61', 'authors': ['Daniil Sherki', 'Ivan Oseledets', 'Ekaterina Muravleva'], 'affiliations': ['Artificial Intelligence Research Institute', 'Sberbank, AI4S Center', 'Skolkovo Institute of Science and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2503.01375.jpg', 'data': {'categories': ['#architecture', '#math'], 'emoji': '🔄', 'ru': {'title': 'Эффективная выборка из сложных апостериорных распределений с помощью CFM и трансформеров', 'desc': 'Статья представляет новый подход к решению байесовских обратных задач. Авторы предлагают комбинацию метода Conditional Flow Matching (CFM) с архитектурой на основе трансформеров. Этот подход позволяет эффективно осуществлять выборку из сложных апостериорных распределений, обусловленных переменным числом наблюдений. Метод преодолевает ограничения традиционных методов выборки, снижая вычислительные затраты.'}, 'en': {'title': 'Efficient Bayesian Inference with CFM and Transformers', 'desc': 'This paper addresses the challenge of efficiently solving Bayesian inverse problems, which involve estimating parameter distributions based on observed data. Traditional sampling methods can be computationally expensive, especially when dealing with complex posterior distributions. The authors propose a novel approach that combines Conditional Flow Matching (CFM) with transformer-based architectures to improve sampling efficiency. This method allows for effective sampling from parameter distributions conditioned on a variable number of observations, enhancing the ability to recover accurate parameter estimates.'}, 'zh': {'title': '高效贝叶斯逆问题求解的新方法', 'desc': '解决贝叶斯逆问题的效率仍然是一个重大挑战，因为后验分布的复杂性和传统采样方法的计算成本较高。我们希望在给定一系列观测和前向模型的情况下，恢复参数的分布，这些分布是基于观察到的实验数据。我们展示了将条件流匹配（CFM）与基于变换器的架构相结合，可以有效地从这种分布中进行采样，且该分布可以根据观测数量的变化而变化。此方法为贝叶斯推断提供了一种新的高效解决方案。'}}}, {'id': 'https://huggingface.co/papers/2503.04369', 'title': 'Lost in Literalism: How Supervised Training Shapes Translationese in LLMs', 'url': 'https://huggingface.co/papers/2503.04369', 'abstract': 'Large language models (LLMs) have achieved remarkable success in machine translation, demonstrating impressive performance across diverse languages. However, translationese, characterized by overly literal and unnatural translations, remains a persistent challenge in LLM-based translation systems. Despite their pre-training on vast corpora of natural utterances, LLMs exhibit translationese errors and generate unexpected unnatural translations, stemming from biases introduced during supervised fine-tuning (SFT). In this work, we systematically evaluate the prevalence of translationese in LLM-generated translations and investigate its roots during supervised training. We introduce methods to mitigate these biases, including polishing golden references and filtering unnatural training instances. Empirical evaluations demonstrate that these approaches significantly reduce translationese while improving translation naturalness, validated by human evaluations and automatic metrics. Our findings highlight the need for training-aware adjustments to optimize LLM translation outputs, paving the way for more fluent and target-language-consistent translations. We release the data and code at https://github.com/yafuly/LLM_Translationese.', 'score': 4, 'issue_id': 2586, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 марта', 'en': 'March 6', 'zh': '3月6日'}, 'hash': '7e6f548766f04dbf', 'authors': ['Yafu Li', 'Ronghao Zhang', 'Zhilin Wang', 'Huajian Zhang', 'Leyang Cui', 'Yongjing Yin', 'Tong Xiao', 'Yue Zhang'], 'affiliations': ['Northeastern University', 'Shanghai AI Laboratory', 'Westlake University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.04369.jpg', 'data': {'categories': ['#training', '#multilingual', '#machine_translation', '#data'], 'emoji': '🌐', 'ru': {'title': 'Борьба с переводным языком: путь к естественным переводам с помощью LLM', 'desc': 'Эта статья посвящена проблеме переводного языка (translationese) в системах машинного перевода на основе больших языковых моделей (LLM). Авторы исследуют причины возникновения неестественных переводов и предлагают методы для снижения этого эффекта, включая улучшение эталонных переводов и фильтрацию неестественных примеров в обучающих данных. Эмпирические оценки показывают, что эти подходы значительно уменьшают проявления переводного языка и улучшают естественность переводов. Исследование подчеркивает необходимость корректировки процесса обучения для оптимизации качества переводов, выполняемых LLM.'}, 'en': {'title': 'Reducing Translationese for Natural Language Translations', 'desc': 'This paper addresses the issue of translationese in large language models (LLMs) used for machine translation, which leads to unnatural and overly literal translations. The authors evaluate how prevalent translationese is in LLM outputs and identify biases introduced during supervised fine-tuning (SFT) as a key factor. They propose methods to reduce these biases, such as refining reference translations and filtering out unnatural training examples. Their empirical results show that these strategies significantly enhance the naturalness of translations, suggesting that careful adjustments during training can improve LLM performance in translation tasks.'}, 'zh': {'title': '优化大型语言模型翻译，减少翻译腔', 'desc': '大型语言模型（LLMs）在机器翻译中取得了显著成功，但仍面临翻译腔的问题，即翻译过于字面和不自然。尽管在大量自然语料上进行预训练，LLMs在监督微调（SFT）过程中引入的偏差导致了翻译腔错误。本文系统评估了LLM生成翻译中的翻译腔现象，并探讨了其在监督训练中的根源。我们提出了减轻这些偏差的方法，并通过实证评估证明这些方法显著降低了翻译腔，提高了翻译的自然性。'}}}, {'id': 'https://huggingface.co/papers/2503.02191', 'title': 'Understanding and Predicting Derailment in Toxic Conversations on GitHub', 'url': 'https://huggingface.co/papers/2503.02191', 'abstract': 'Software projects thrive on the involvement and contributions of individuals from different backgrounds. However, toxic language and negative interactions can hinder the participation and retention of contributors and alienate newcomers. Proactive moderation strategies aim to prevent toxicity from occurring by addressing conversations that have derailed from their intended purpose. This study aims to understand and predict conversational derailment leading to toxicity on GitHub.   To facilitate this research, we curate a novel dataset comprising 202 toxic conversations from GitHub with annotated derailment points, along with 696 non-toxic conversations as a baseline. Based on this dataset, we identify unique characteristics of toxic conversations and derailment points, including linguistic markers such as second-person pronouns, negation terms, and tones of Bitter Frustration and Impatience, as well as patterns in conversational dynamics between project contributors and external participants.   Leveraging these empirical observations, we propose a proactive moderation approach to automatically detect and address potentially harmful conversations before escalation. By utilizing modern LLMs, we develop a conversation trajectory summary technique that captures the evolution of discussions and identifies early signs of derailment. Our experiments demonstrate that LLM prompts tailored to provide summaries of GitHub conversations achieve 69% F1-Score in predicting conversational derailment, strongly improving over a set of baseline approaches.', 'score': 3, 'issue_id': 2581, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 марта', 'en': 'March 4', 'zh': '3月4日'}, 'hash': '1d0bc1069249c9e1', 'authors': ['Mia Mohammad Imran', 'Robert Zita', 'Rebekah Copeland', 'Preetha Chatterjee', 'Rahat Rizvi Rahman', 'Kostadin Damevski'], 'affiliations': ['Drexel University, Philadelphia, PA, USA', 'Eastern Mennonite University, Harrisonburg, VA, USA', 'Elmhurst University, Elmhurst, IL, USA', 'Missouri University of Science and Technology, Rolla, MO, USA', 'Virginia Commonwealth University, Richmond, VA, USA'], 'pdf_title_img': 'assets/pdf/title_img/2503.02191.jpg', 'data': {'categories': ['#ethics', '#dataset', '#multimodal', '#data'], 'emoji': '🛡️', 'ru': {'title': 'ИИ на страже здоровой атмосферы в open-source сообществах', 'desc': 'Исследование посвящено анализу и предотвращению токсичных взаимодействий в проектах на GitHub. Авторы создали датасет из 202 токсичных и 696 нетоксичных разговоров, выявив лингвистические маркеры и паттерны, характерные для деструктивных обсуждений. На основе этих данных была разработана система проактивной модерации с использованием больших языковых моделей (LLM). Эксперименты показали, что предложенный подход достигает 69% F1-меры в предсказании потенциально опасных разговоров на ранних стадиях.'}, 'en': {'title': 'Proactive Moderation: Detecting Toxicity Before It Escalates', 'desc': 'This paper investigates how toxic language in GitHub conversations can deter contributors and newcomers. It introduces a dataset of 202 toxic conversations with marked derailment points, alongside 696 non-toxic examples for comparison. The study identifies specific linguistic features and conversational dynamics that signal potential toxicity. Using large language models (LLMs), the authors propose a proactive moderation strategy that summarizes conversation trajectories to detect early signs of derailment, achieving a 69% F1-Score in predictions.'}, 'zh': {'title': '主动管理，防止对话偏离与有毒语言', 'desc': '本研究探讨了如何在GitHub上预测和理解对话偏离导致的有毒语言。我们创建了一个新数据集，包含202个有毒对话和696个非有毒对话，并标注了偏离点。通过分析这些对话的语言特征和动态模式，我们识别出有毒对话的独特特征。最后，我们提出了一种主动的管理策略，利用现代大语言模型自动检测和处理潜在的有害对话。'}}}, {'id': 'https://huggingface.co/papers/2503.03962', 'title': 'On the Acquisition of Shared Grammatical Representations in Bilingual Language Models', 'url': 'https://huggingface.co/papers/2503.03962', 'abstract': "While crosslingual transfer is crucial to contemporary language models' multilingual capabilities, how it occurs is not well understood. In this paper, we ask what happens to a monolingual language model when it begins to be trained on a second language. Specifically, we train small bilingual models for which we control the amount of data for each language and the order of language exposure. To find evidence of shared multilingual representations, we turn to structural priming, a method used to study grammatical representations in humans. We first replicate previous crosslingual structural priming results and find that after controlling for training data quantity and language exposure, there are asymmetrical effects across language pairs and directions. We argue that this asymmetry may shape hypotheses about human structural priming effects. We also find that structural priming effects are less robust for less similar language pairs, highlighting potential limitations of crosslingual transfer learning and shared representations for typologically diverse languages.", 'score': 2, 'issue_id': 2592, 'pub_date': '2025-03-05', 'pub_date_card': {'ru': '5 марта', 'en': 'March 5', 'zh': '3月5日'}, 'hash': '5d7eee50132fd29c', 'authors': ['Catherine Arnett', 'Tyler A. Chang', 'James A. Michaelov', 'Benjamin K. Bergen'], 'affiliations': ['Department of Brain and Cognitive Science, Massachusetts Institute of Technology', 'Department of Cognitive Science, University of California San Diego', 'Department of Linguistics, University of California San Diego', 'Halıcıoglu Data Science Institute, University of California San Diego'], 'pdf_title_img': 'assets/pdf/title_img/2503.03962.jpg', 'data': {'categories': ['#low_resource', '#training', '#transfer_learning', '#data', '#multilingual'], 'emoji': '🧠', 'ru': {'title': 'Раскрывая тайны кросс-лингвистического переноса в нейронных сетях', 'desc': 'Статья исследует процесс кросс-лингвистического переноса в двуязычных языковых моделях. Авторы изучают, как модель, обученная на одном языке, начинает воспринимать второй язык. Для анализа общих многоязычных представлений используется метод структурного прайминга, заимствованный из психолингвистики. Результаты показывают асимметричные эффекты между парами языков и направлениями переноса, а также меньшую устойчивость структурного прайминга для типологически различных языков.'}, 'en': {'title': 'Understanding Crosslingual Transfer in Language Models', 'desc': 'This paper investigates how training a monolingual language model on a second language affects its performance and understanding. The authors create small bilingual models and manipulate the amount of training data and the sequence of language exposure. They utilize structural priming to analyze the grammatical representations that emerge from this bilingual training. The findings reveal asymmetrical effects in language pairs and suggest that less similar languages may limit the effectiveness of crosslingual transfer learning.'}, 'zh': {'title': '探索跨语言迁移的非对称性', 'desc': '本文探讨了跨语言迁移在现代语言模型中的重要性，尤其是当单语模型开始接受第二语言训练时会发生什么。我们训练了小型双语模型，控制每种语言的数据量和语言暴露的顺序。通过结构性启动的方法，我们发现不同语言对之间的影响不对称，这可能影响人类的结构性启动假设。研究还表明，对于语言类型差异较大的语言对，结构性启动效果较弱，突显了跨语言迁移学习的潜在局限性。'}}}, {'id': 'https://huggingface.co/papers/2503.03601', 'title': 'Feature-Level Insights into Artificial Text Detection with Sparse\n  Autoencoders', 'url': 'https://huggingface.co/papers/2503.03601', 'abstract': 'Artificial Text Detection (ATD) is becoming increasingly important with the rise of advanced Large Language Models (LLMs). Despite numerous efforts, no single algorithm performs consistently well across different types of unseen text or guarantees effective generalization to new LLMs. Interpretability plays a crucial role in achieving this goal. In this study, we enhance ATD interpretability by using Sparse Autoencoders (SAE) to extract features from Gemma-2-2b residual stream. We identify both interpretable and efficient features, analyzing their semantics and relevance through domain- and model-specific statistics, a steering approach, and manual or LLM-based interpretation. Our methods offer valuable insights into how texts from various models differ from human-written content. We show that modern LLMs have a distinct writing style, especially in information-dense domains, even though they can produce human-like outputs with personalized prompts.', 'score': 133, 'issue_id': 2634, 'pub_date': '2025-03-05', 'pub_date_card': {'ru': '5 марта', 'en': 'March 5', 'zh': '3月5日'}, 'hash': 'd045a8e2a39262c9', 'authors': ['Kristian Kuznetsov', 'Laida Kushnareva', 'Polina Druzhinina', 'Anton Razzhigaev', 'Anastasia Voznyuk', 'Irina Piontkovskaya', 'Evgeny Burnaev', 'Serguei Barannikov'], 'affiliations': ['AI Foundation and Algorithm Lab', 'Artificial Intelligence Research Institute (AIRI)', 'CNRS, Université Paris Cité, France', 'Moscow Institute of Physics and Technology', 'Skolkovo Institute of Science and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2503.03601.jpg', 'data': {'categories': ['#multimodal', '#cv', '#interpretability', '#data'], 'emoji': '🔍', 'ru': {'title': 'Раскрывая секреты искусственных текстов: новый подход к интерпретируемому обнаружению', 'desc': 'Исследование посвящено улучшению интерпретируемости методов обнаружения искусственного текста (ATD) с использованием разреженных автоэнкодеров для извлечения признаков из остаточного потока модели Gemma-2-2b. Авторы анализируют семантику и релевантность выделенных признаков с помощью статистических методов и интерпретации. Результаты показывают, что современные языковые модели имеют отличительный стиль письма, особенно в информационно-насыщенных областях. Исследование предлагает ценные insights о том, как тексты, созданные различными моделями, отличаются от контента, написанного людьми.'}, 'en': {'title': 'Enhancing Text Detection with Interpretability in AI Models', 'desc': 'This paper focuses on improving Artificial Text Detection (ATD) by enhancing its interpretability using Sparse Autoencoders (SAE). The authors extract features from the residual stream of the Gemma-2-2b model to identify both interpretable and efficient characteristics of text. They analyze these features through various statistical methods and interpretations to understand how machine-generated texts differ from human-written ones. The study reveals that modern Large Language Models (LLMs) exhibit a unique writing style, particularly in information-dense areas, despite their ability to generate human-like text.'}, 'zh': {'title': '提升人工文本检测的可解释性', 'desc': '随着大型语言模型（LLMs）的发展，人工文本检测（ATD）变得越来越重要。尽管已有许多努力，但没有单一算法能够在不同类型的未见文本中始终表现良好，也无法保证对新LLM的有效泛化。可解释性在实现这一目标中起着关键作用。我们通过使用稀疏自编码器（SAE）从Gemma-2-2b残差流中提取特征，增强了ATD的可解释性，分析了文本与人类写作内容的差异。'}}}, {'id': 'https://huggingface.co/papers/2503.07605', 'title': 'SEAP: Training-free Sparse Expert Activation Pruning Unlock the\n  Brainpower of Large Language Models', 'url': 'https://huggingface.co/papers/2503.07605', 'abstract': "Large Language Models have achieved remarkable success across various natural language processing tasks, yet their high computational cost during inference remains a major bottleneck. This paper introduces Sparse Expert Activation Pruning (SEAP), a training-free pruning method that selectively retains task-relevant parameters to reduce inference overhead. Inspired by the clustering patterns of hidden states and activations in LLMs, SEAP identifies task-specific expert activation patterns and prunes the model while preserving task performance and enhancing computational efficiency. Experimental results demonstrate that SEAP significantly reduces computational overhead while maintaining competitive accuracy. Notably, at 50% pruning, SEAP surpasses both WandA and FLAP by over 20%, and at 20% pruning, it incurs only a 2.2% performance drop compared to the dense model. These findings highlight SEAP's scalability and effectiveness, making it a promising approach for optimizing large-scale LLMs.", 'score': 59, 'issue_id': 2631, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': 'eaf9c07e3673cecc', 'authors': ['Xun Liang', 'Hanyu Wang', 'Huayi Lai', 'Simin Niu', 'Shichao Song', 'Jiawei Yang', 'Jihao Zhao', 'Feiyu Xiong', 'Bo Tang', 'Zhiyu Li'], 'affiliations': ['Institute for Advanced Algorithms Research, Shanghai, China', 'School of Information, Renmin University of China, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.07605.jpg', 'data': {'categories': ['#inference', '#optimization', '#training'], 'emoji': '✂️', 'ru': {'title': 'Эффективное обрезание нейросетей без потери качества', 'desc': 'Статья представляет метод Sparse Expert Activation Pruning (SEAP) для оптимизации больших языковых моделей. SEAP выборочно сохраняет параметры, релевантные для конкретной задачи, что позволяет снизить вычислительные затраты при инференсе. Метод основан на выявлении паттернов активации экспертов, специфичных для задачи. Эксперименты показывают, что SEAP значительно сокращает вычислительные затраты при сохранении высокой точности модели.'}, 'en': {'title': 'Optimize LLMs with Sparse Expert Activation Pruning!', 'desc': "This paper presents Sparse Expert Activation Pruning (SEAP), a novel method designed to reduce the computational cost of large language models (LLMs) during inference without requiring additional training. SEAP works by identifying and retaining only the parameters that are relevant to specific tasks, effectively pruning the model while maintaining its performance. The method leverages the clustering patterns of hidden states and activations to optimize the model's efficiency. Experimental results show that SEAP can significantly lower computational overhead while achieving competitive accuracy, making it a scalable solution for optimizing LLMs."}, 'zh': {'title': '稀疏专家激活剪枝：优化大型语言模型的新方法', 'desc': '大型语言模型在自然语言处理任务中取得了显著成功，但推理时的高计算成本仍然是一个主要瓶颈。本文提出了一种名为稀疏专家激活剪枝（SEAP）的方法，该方法在不需要训练的情况下，选择性地保留与任务相关的参数，以减少推理开销。SEAP通过分析隐藏状态和激活的聚类模式，识别特定任务的专家激活模式，从而在保持任务性能的同时优化计算效率。实验结果表明，SEAP在减少计算开销的同时，保持了竞争力的准确性，展示了其在优化大规模语言模型方面的潜力。'}}}, {'id': 'https://huggingface.co/papers/2503.07365', 'title': 'MM-Eureka: Exploring Visual Aha Moment with Rule-based Large-scale\n  Reinforcement Learning', 'url': 'https://huggingface.co/papers/2503.07365', 'abstract': "We present MM-Eureka, a multimodal reasoning model that successfully extends large-scale rule-based reinforcement learning (RL) to multimodal reasoning. While rule-based RL has shown remarkable success in improving LLMs' reasoning abilities in text domains, its application to multimodal settings has remained challenging. Our work reproduces key characteristics of text-based RL systems like DeepSeek-R1 in the multimodal space, including steady increases in accuracy reward and response length, and the emergence of reflection behaviors. We demonstrate that both instruction-tuned and pre-trained models can develop strong multimodal reasoning capabilities through rule-based RL without supervised fine-tuning, showing superior data efficiency compared to alternative approaches. We open-source our complete pipeline to foster further research in this area. We release all our codes, models, data, etc. at https://github.com/ModalMinds/MM-EUREKA", 'score': 45, 'issue_id': 2630, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '765d475b38d9f289', 'authors': ['Fanqing Meng', 'Lingxiao Du', 'Zongkai Liu', 'Zhixiang Zhou', 'Quanfeng Lu', 'Daocheng Fu', 'Botian Shi', 'Wenhai Wang', 'Junjun He', 'Kaipeng Zhang', 'Ping Luo', 'Yu Qiao', 'Qiaosheng Zhang', 'Wenqi Shao'], 'affiliations': ['Shanghai AI Laboratory', 'Shanghai Innovation Institute', 'Shanghai Jiao Tong University', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2503.07365.jpg', 'data': {'categories': ['#rl', '#multimodal', '#reasoning', '#rag', '#open_source'], 'emoji': '🤖', 'ru': {'title': 'Мультимодальное рассуждение через обучение с подкреплением', 'desc': 'MM-Eureka – это мультимодальная модель рассуждений, которая успешно расширяет масштабное обучение с подкреплением на основе правил для мультимодальных задач. Модель демонстрирует ключевые характеристики текстовых систем обучения с подкреплением, включая устойчивое повышение точности и длины ответов, а также появление рефлексивного поведения. Исследование показывает, что как модели, настроенные на инструкции, так и предварительно обученные модели могут развивать сильные мультимодальные способности рассуждения без контролируемой тонкой настройки. Авторы открыли исходный код всего конвейера для дальнейших исследований в этой области.'}, 'en': {'title': 'Unlocking Multimodal Reasoning with MM-Eureka!', 'desc': 'MM-Eureka is a new model that enhances reasoning across different types of data, like text and images, using a method called rule-based reinforcement learning (RL). This approach builds on successful techniques from text-based RL, allowing the model to improve its accuracy and response quality in multimodal contexts. The model shows that it can learn effectively without needing extra labeled data, making it more efficient than other methods. By sharing our tools and findings, we aim to encourage more research in multimodal reasoning.'}, 'zh': {'title': 'MM-Eureka：多模态推理的新突破', 'desc': '我们提出了MM-Eureka，这是一个多模态推理模型，成功地将大规模基于规则的强化学习扩展到多模态推理领域。虽然基于规则的强化学习在文本领域提升大型语言模型的推理能力方面取得了显著成功，但在多模态环境中的应用仍然具有挑战性。我们的工作在多模态空间中重现了文本基础强化学习系统的关键特征，包括准确性奖励和响应长度的稳定增加，以及反思行为的出现。我们展示了无监督微调的情况下，指令调优和预训练模型都能通过基于规则的强化学习发展出强大的多模态推理能力，且数据效率优于其他方法。'}}}, {'id': 'https://huggingface.co/papers/2503.07002', 'title': 'Taking Notes Brings Focus? Towards Multi-Turn Multimodal Dialogue\n  Learning', 'url': 'https://huggingface.co/papers/2503.07002', 'abstract': 'Multimodal large language models (MLLMs), built on large-scale pre-trained vision towers and language models, have shown great capabilities in multimodal understanding. However, most existing MLLMs are trained on single-turn vision question-answering tasks, which do not accurately reflect real-world human conversations. In this paper, we introduce MMDiag, a multi-turn multimodal dialogue dataset. This dataset is collaboratively generated through deliberately designed rules and GPT assistance, featuring strong correlations between questions, between questions and images, and among different image regions; thus aligning more closely with real-world scenarios. MMDiag serves as a strong benchmark for multi-turn multimodal dialogue learning and brings more challenges to the grounding and reasoning capabilities of MLLMs. Further, inspired by human vision processing, we present DiagNote, an MLLM equipped with multimodal grounding and reasoning capabilities. DiagNote consists of two modules (Deliberate and Gaze) interacting with each other to perform Chain-of-Thought and annotations respectively, throughout multi-turn dialogues. We empirically demonstrate the advantages of DiagNote in both grounding and jointly processing and reasoning with vision and language information over existing MLLMs.', 'score': 33, 'issue_id': 2631, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '1f57fb5c5398efbc', 'authors': ['Jiazheng Liu', 'Sipeng Zheng', 'Börje F. Karlsson', 'Zongqing Lu'], 'affiliations': ['Beijing Academy of Artificial Intelligence', 'School of Computer Science, Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2503.07002.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#games', '#multimodal', '#dataset', '#architecture'], 'emoji': '🗣️', 'ru': {'title': 'Новый подход к мультимодальным диалогам: от реалистичных данных к продвинутым моделям', 'desc': 'Статья представляет MMDiag - новый набор данных для многоходовых мультимодальных диалогов, созданный с помощью GPT. Этот датасет лучше отражает реальные сценарии общения, чем существующие однотурные наборы данных для вопросно-ответных задач. Авторы также предлагают DiagNote - мультимодальную языковую модель с улучшенными возможностями заземления и рассуждений. DiagNote использует два взаимодействующих модуля для цепочки рассуждений и аннотаций в ходе диалога.'}, 'en': {'title': 'Enhancing Multimodal Dialogue with MMDiag and DiagNote', 'desc': 'This paper presents MMDiag, a new dataset designed for multi-turn multimodal dialogue, which enhances the training of multimodal large language models (MLLMs). Unlike previous datasets that focus on single-turn tasks, MMDiag captures the complexities of real-world conversations by incorporating strong correlations between questions and images. The authors introduce DiagNote, an MLLM that features two interactive modules for improved grounding and reasoning in dialogues. The results show that DiagNote outperforms existing MLLMs in processing and reasoning with both visual and textual information.'}, 'zh': {'title': '提升多模态对话的智能化', 'desc': '这篇论文介绍了一种新的多模态对话数据集MMDiag，旨在改善现有多模态大语言模型（MLLMs）在多轮对话中的表现。MMDiag通过精心设计的规则和GPT的辅助生成，包含了问题之间、问题与图像之间以及不同图像区域之间的强相关性，更贴近真实的人类对话场景。论文还提出了DiagNote，一个具备多模态基础和推理能力的MLLM，包含两个相互作用的模块（Deliberate和Gaze），用于在多轮对话中进行思维链和注释。通过实验证明，DiagNote在基础和共同处理视觉与语言信息的推理能力上优于现有的MLLMs。'}}}, {'id': 'https://huggingface.co/papers/2503.07314', 'title': 'Automated Movie Generation via Multi-Agent CoT Planning', 'url': 'https://huggingface.co/papers/2503.07314', 'abstract': 'Existing long-form video generation frameworks lack automated planning, requiring manual input for storylines, scenes, cinematography, and character interactions, resulting in high costs and inefficiencies. To address these challenges, we present MovieAgent, an automated movie generation via multi-agent Chain of Thought (CoT) planning. MovieAgent offers two key advantages: 1) We firstly explore and define the paradigm of automated movie/long-video generation. Given a script and character bank, our MovieAgent can generates multi-scene, multi-shot long-form videos with a coherent narrative, while ensuring character consistency, synchronized subtitles, and stable audio throughout the film. 2) MovieAgent introduces a hierarchical CoT-based reasoning process to automatically structure scenes, camera settings, and cinematography, significantly reducing human effort. By employing multiple LLM agents to simulate the roles of a director, screenwriter, storyboard artist, and location manager, MovieAgent streamlines the production pipeline. Experiments demonstrate that MovieAgent achieves new state-of-the-art results in script faithfulness, character consistency, and narrative coherence. Our hierarchical framework takes a step forward and provides new insights into fully automated movie generation. The code and project website are available at: https://github.com/showlab/MovieAgent and https://weijiawu.github.io/MovieAgent.', 'score': 28, 'issue_id': 2631, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '2e5c000925250863', 'authors': ['Weijia Wu', 'Zeyu Zhu', 'Mike Zheng Shou'], 'affiliations': ['Show Lab, National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2503.07314.jpg', 'data': {'categories': ['#reasoning', '#open_source', '#multimodal', '#story_generation', '#video', '#agents'], 'emoji': '🎬', 'ru': {'title': 'Автоматизированное создание фильмов с помощью ИИ-агентов', 'desc': 'MovieAgent - это новая система автоматизированной генерации длинных видео с использованием мультиагентного планирования на основе цепочки размышлений (Chain of Thought). Система способна создавать многосценные фильмы с согласованным сюжетом, сохраняя постоянство персонажей и синхронизацию субтитров. MovieAgent использует иерархический процесс рассуждений для автоматического структурирования сцен и настроек камеры, значительно сокращая человеческие усилия. Эксперименты показывают, что MovieAgent достигает новых лучших результатов в верности сценарию, постоянстве персонажей и согласованности повествования.'}, 'en': {'title': 'Automating Movie Magic with MovieAgent!', 'desc': 'This paper introduces MovieAgent, a novel framework for automated long-form video generation that eliminates the need for manual planning of storylines and scenes. It utilizes a multi-agent Chain of Thought (CoT) reasoning process to create coherent narratives while maintaining character consistency and synchronized audio. By simulating various production roles, MovieAgent streamlines the filmmaking process, significantly reducing the effort required from human creators. Experimental results show that MovieAgent sets new benchmarks in script adherence, character consistency, and overall narrative quality.'}, 'zh': {'title': '自动化电影生成的新纪元', 'desc': '现有的长视频生成框架缺乏自动化规划，需要手动输入故事情节、场景、摄影和角色互动，导致高成本和低效率。为了解决这些问题，我们提出了MovieAgent，通过多智能体的思维链（CoT）规划实现自动化电影生成。MovieAgent的两个主要优势是：首先，我们探索并定义了自动化电影/长视频生成的范式；其次，MovieAgent引入了基于层次的CoT推理过程，自动构建场景、摄像机设置和摄影，大大减少了人力投入。实验表明，MovieAgent在剧本忠实度、角色一致性和叙事连贯性方面达到了新的最先进水平。'}}}, {'id': 'https://huggingface.co/papers/2503.07216', 'title': 'FedRand: Enhancing Privacy in Federated Learning with Randomized LoRA\n  Subparameter Updates', 'url': 'https://huggingface.co/papers/2503.07216', 'abstract': "Federated Learning (FL) is a widely used framework for training models in a decentralized manner, ensuring that the central server does not have direct access to data from local clients. However, this approach may still fail to fully preserve data privacy, as models from local clients are exposed to the central server during the aggregation process. This issue becomes even more critical when training vision-language models (VLMs) with FL, as VLMs can easily memorize training data instances, making them vulnerable to membership inference attacks (MIAs). To address this challenge, we propose the FedRand framework, which avoids disclosing the full set of client parameters. In this framework, each client randomly selects subparameters of Low-Rank Adaptation (LoRA) from the server and keeps the remaining counterparts of the LoRA weights as private parameters. After training both parameters on the client's private dataset, only the non-private <PRE_TAG>client parameters</POST_TAG> are sent back to the server for aggregation. This approach mitigates the risk of exposing client-side VLM parameters, thereby enhancing data privacy. We empirically validate that FedRand improves robustness against MIAs compared to relevant baselines while achieving accuracy comparable to methods that communicate full LoRA parameters across several benchmark datasets.", 'score': 25, 'issue_id': 2631, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '3d23c61b24a599ee', 'authors': ['Sangwoo Park', 'Seanie Lee', 'Byungjoo Kim', 'Sung Ju Hwang'], 'affiliations': ['DeepAuto.ai', 'Graduate School of AI, KAIST'], 'pdf_title_img': 'assets/pdf/title_img/2503.07216.jpg', 'data': {'categories': ['#security', '#benchmark', '#data', '#ethics', '#multimodal', '#rl'], 'emoji': '🔐', 'ru': {'title': 'FedRand: Защита конфиденциальности в федеративном обучении визуально-языковых моделей', 'desc': 'Статья представляет новый подход к федеративному обучению (FL) для визуально-языковых моделей (VLM), называемый FedRand. Эта методика направлена на повышение конфиденциальности данных путем случайного выбора подпараметров Low-Rank Adaptation (LoRA) для обмена между клиентами и сервером. FedRand позволяет сохранять часть параметров LoRA приватными, снижая риск утечки информации о данных клиентов. Эмпирические исследования показывают, что FedRand повышает устойчивость к атакам по выводу членства (MIA) по сравнению с базовыми методами, сохраняя при этом сопоставимую точность.'}, 'en': {'title': 'Enhancing Privacy in Federated Learning with FedRand', 'desc': 'Federated Learning (FL) allows models to be trained on local data without sharing it with a central server, but it can still risk data privacy during model aggregation. This paper introduces the FedRand framework, which enhances privacy by having clients send only a subset of their model parameters back to the server. Specifically, it utilizes Low-Rank Adaptation (LoRA) to keep certain parameters private while still allowing effective model training. The results show that FedRand not only protects against membership inference attacks but also maintains accuracy similar to traditional methods that share full parameters.'}, 'zh': {'title': 'FedRand：保护数据隐私的联邦学习新框架', 'desc': '联邦学习（FL）是一种去中心化的模型训练框架，确保中央服务器无法直接访问本地客户端的数据。然而，在聚合过程中，本地客户端的模型仍可能暴露给中央服务器，从而影响数据隐私。特别是在训练视觉-语言模型（VLMs）时，这种风险更为严重，因为VLMs容易记住训练数据实例，容易受到成员推断攻击（MIAs）。为了解决这个问题，我们提出了FedRand框架，该框架通过随机选择低秩适应（LoRA）的子参数，避免泄露完整的客户端参数，从而增强数据隐私。'}}}, {'id': 'https://huggingface.co/papers/2503.07067', 'title': 'DistiLLM-2: A Contrastive Approach Boosts the Distillation of LLMs', 'url': 'https://huggingface.co/papers/2503.07067', 'abstract': 'Despite the success of distillation in large language models (LLMs), most prior work applies identical loss functions to both teacher- and student-generated data. These strategies overlook the synergy between loss formulations and data types, leading to a suboptimal performance boost in student models. To address this, we propose DistiLLM-2, a contrastive approach that simultaneously increases the likelihood of teacher responses and decreases that of student responses by harnessing this synergy. Our extensive experiments show that DistiLLM-2 not only builds high-performing student models across a wide range of tasks, including instruction-following and code generation, but also supports diverse applications, such as preference alignment and vision-language extensions. These findings highlight the potential of a contrastive approach to enhance the efficacy of LLM distillation by effectively aligning teacher and student models across varied data types.', 'score': 22, 'issue_id': 2632, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '8eb39990e619611e', 'authors': ['Jongwoo Ko', 'Tianyi Chen', 'Sungnyun Kim', 'Tianyu Ding', 'Luming Liang', 'Ilya Zharkov', 'Se-Young Yun'], 'affiliations': ['KAIST AI', 'Microsoft'], 'pdf_title_img': 'assets/pdf/title_img/2503.07067.jpg', 'data': {'categories': ['#multimodal', '#alignment', '#training', '#optimization'], 'emoji': '🔬', 'ru': {'title': 'Контрастивная дистилляция: новый подход к обучению языковых моделей', 'desc': 'DistiLLM-2 - это новый подход к дистилляции больших языковых моделей (LLM), использующий контрастивное обучение. Он одновременно увеличивает вероятность ответов учителя и уменьшает вероятность ответов ученика, эффективно используя синергию между формулировками функции потерь и типами данных. Эксперименты показывают, что DistiLLM-2 создает высокопроизводительные модели-ученики для широкого спектра задач, включая следование инструкциям и генерацию кода. Метод также поддерживает различные приложения, такие как выравнивание предпочтений и расширения для работы с визуальными данными.'}, 'en': {'title': 'Enhancing LLM Distillation with Contrastive Learning', 'desc': 'This paper introduces DistiLLM-2, a novel approach to improve the distillation process in large language models (LLMs). Unlike previous methods that use the same loss functions for both teacher and student models, DistiLLM-2 employs a contrastive strategy that boosts the likelihood of correct teacher responses while reducing the likelihood of incorrect student responses. The results demonstrate that this method significantly enhances the performance of student models on various tasks, including instruction-following and code generation. Additionally, DistiLLM-2 shows versatility in applications such as preference alignment and vision-language tasks, emphasizing the importance of tailored loss functions in model distillation.'}, 'zh': {'title': '对比方法提升大型语言模型蒸馏效果', 'desc': '尽管蒸馏在大型语言模型（LLMs）中取得了成功，但大多数先前的研究对教师和学生生成的数据使用相同的损失函数。这种策略忽视了损失公式与数据类型之间的协同作用，导致学生模型的性能提升不理想。为了解决这个问题，我们提出了DistiLLM-2，这是一种对比方法，能够同时提高教师响应的可能性并降低学生响应的可能性。我们的广泛实验表明，DistiLLM-2不仅在多种任务中构建了高性能的学生模型，还支持多样化的应用，如偏好对齐和视觉-语言扩展。'}}}, {'id': 'https://huggingface.co/papers/2503.07027', 'title': 'EasyControl: Adding Efficient and Flexible Control for Diffusion\n  Transformer', 'url': 'https://huggingface.co/papers/2503.07027', 'abstract': 'Recent advancements in Unet-based diffusion models, such as ControlNet and IP-Adapter, have introduced effective spatial and subject control mechanisms. However, the DiT (Diffusion Transformer) architecture still struggles with efficient and flexible control. To tackle this issue, we propose EasyControl, a novel framework designed to unify condition-guided diffusion transformers with high efficiency and flexibility. Our framework is built on three key innovations. First, we introduce a lightweight <PRE_TAG>Condition Injection LoRA Module</POST_TAG>. This module processes conditional signals in isolation, acting as a plug-and-play solution. It avoids modifying the base model weights, ensuring compatibility with customized models and enabling the flexible injection of diverse conditions. Notably, this module also supports harmonious and robust zero-shot multi-condition generalization, even when trained only on single-condition data. Second, we propose a <PRE_TAG>Position-Aware Training Paradigm</POST_TAG>. This approach standardizes input conditions to fixed resolutions, allowing the generation of images with arbitrary aspect ratios and flexible resolutions. At the same time, it optimizes computational efficiency, making the framework more practical for real-world applications. Third, we develop a Causal Attention Mechanism combined with the KV Cache technique, adapted for conditional generation tasks. This innovation significantly reduces the latency of image synthesis, improving the overall efficiency of the framework. Through extensive experiments, we demonstrate that EasyControl achieves exceptional performance across various application scenarios. These innovations collectively make our framework highly efficient, flexible, and suitable for a wide range of tasks.', 'score': 19, 'issue_id': 2630, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '1b1589f9873720c7', 'authors': ['Yuxuan Zhang', 'Yirui Yuan', 'Yiren Song', 'Haofan Wang', 'Jiaming Liu'], 'affiliations': ['Liblib AI', 'National University of Singapore', 'ShanghaiTech University', 'Tiamat AI'], 'pdf_title_img': 'assets/pdf/title_img/2503.07027.jpg', 'data': {'categories': ['#cv', '#architecture', '#training', '#optimization', '#diffusion'], 'emoji': '🎨', 'ru': {'title': 'EasyControl: гибкое и эффективное управление диффузионными трансформерами', 'desc': 'Статья представляет EasyControl - новую систему для улучшения контроля в диффузионных трансформерах. Основные инновации включают модуль внедрения условий на основе LoRA, парадигму обучения с учетом позиции и механизм причинного внимания с KV-кэшем. EasyControl обеспечивает эффективное и гибкое управление генерацией изображений с произвольными пропорциями и разрешением. Эксперименты показывают высокую производительность системы в различных сценариях применения.'}, 'en': {'title': 'EasyControl: Unifying Efficiency and Flexibility in Diffusion Transformers', 'desc': 'This paper presents EasyControl, a new framework that enhances the efficiency and flexibility of condition-guided diffusion transformers, particularly addressing the limitations of the DiT architecture. It introduces a lightweight Condition Injection LoRA Module that allows for the independent processing of conditional signals without altering the base model, enabling easy integration of various conditions. Additionally, the Position-Aware Training Paradigm standardizes input conditions, facilitating the generation of images with different aspect ratios while optimizing computational resources. Lastly, the Causal Attention Mechanism with KV Cache significantly reduces image synthesis latency, making EasyControl a robust solution for diverse applications in machine learning.'}, 'zh': {'title': '高效灵活的条件生成框架EasyControl', 'desc': '本文提出了一种名为EasyControl的新框架，旨在提高基于扩散变换器的条件生成模型的效率和灵活性。该框架包含三个关键创新：首先，轻量级的条件注入LoRA模块可以独立处理条件信号，避免修改基础模型权重，从而实现与定制模型的兼容性。其次，位置感知训练范式标准化输入条件，使得生成任意长宽比和灵活分辨率的图像成为可能，同时优化计算效率。最后，结合KV缓存技术的因果注意机制显著降低了图像合成的延迟，提升了整体效率。'}}}, {'id': 'https://huggingface.co/papers/2503.06680', 'title': 'FEA-Bench: A Benchmark for Evaluating Repository-Level Code Generation\n  for Feature Implementation', 'url': 'https://huggingface.co/papers/2503.06680', 'abstract': "Implementing new features in repository-level codebases is a crucial application of code generation models. However, current benchmarks lack a dedicated evaluation framework for this capability. To fill this gap, we introduce FEA-Bench, a benchmark designed to assess the ability of large language models (LLMs) to perform incremental development within code repositories. We collect pull requests from 83 GitHub repositories and use rule-based and intent-based filtering to construct task instances focused on new feature development. Each task instance containing code changes is paired with relevant unit test files to ensure that the solution can be verified. The feature implementation requires LLMs to simultaneously possess code completion capabilities for new components and code editing abilities for other relevant parts in the code repository, providing a more comprehensive evaluation method of LLMs' automated software engineering capabilities. Experimental results show that LLMs perform significantly worse in the FEA-Bench, highlighting considerable challenges in such repository-level incremental code development.", 'score': 17, 'issue_id': 2630, 'pub_date': '2025-03-09', 'pub_date_card': {'ru': '9 марта', 'en': 'March 9', 'zh': '3月9日'}, 'hash': '4394ce17a18696a3', 'authors': ['Wei Li', 'Xin Zhang', 'Zhongxin Guo', 'Shaoguang Mao', 'Wen Luo', 'Guangyue Peng', 'Yangyu Huang', 'Houfeng Wang', 'Scarlett Li'], 'affiliations': ['Microsoft Research Asia', 'State Key Laboratory for Multimedia Information Processing, School of Computer Science, Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2503.06680.jpg', 'data': {'categories': ['#plp', '#dataset', '#optimization', '#benchmark'], 'emoji': '🧪', 'ru': {'title': 'FEA-Bench: новый вызов для языковых моделей в разработке ПО', 'desc': 'FEA-Bench - это новый бенчмарк для оценки способности больших языковых моделей (LLM) выполнять инкрементальную разработку в репозиториях кода. Он основан на пул-реквестах из 83 репозиториев GitHub и включает задачи по разработке новых функций. Каждая задача содержит изменения кода и соответствующие модульные тесты для верификации. Результаты экспериментов показывают, что LLM значительно хуже справляются с задачами FEA-Bench, что указывает на серьезные проблемы в автоматизированной разработке на уровне репозиториев.'}, 'en': {'title': 'FEA-Bench: Evaluating Code Generation in Repositories', 'desc': "This paper introduces FEA-Bench, a new benchmark for evaluating large language models (LLMs) in the context of code generation for new features in software repositories. It addresses the lack of dedicated evaluation frameworks by collecting pull requests from 83 GitHub repositories and creating task instances that focus on incremental development. Each task is designed to test the LLM's ability to generate code changes while ensuring that these changes can be verified through associated unit tests. The findings reveal that LLMs struggle with this type of repository-level code development, indicating significant challenges in their automated software engineering capabilities."}, 'zh': {'title': '评估代码生成模型的新基准：FEA-Bench', 'desc': '在代码生成模型中，实现新特性是一个重要的应用。当前的基准测试缺乏专门评估这一能力的框架。为此，我们提出了FEA-Bench，这是一个旨在评估大型语言模型（LLMs）在代码库中进行增量开发能力的基准。实验结果表明，LLMs在FEA-Bench中的表现显著较差，突显了在代码库级别增量开发中面临的重大挑战。'}}}, {'id': 'https://huggingface.co/papers/2503.07608', 'title': 'AlphaDrive: Unleashing the Power of VLMs in Autonomous Driving via\n  Reinforcement Learning and Reasoning', 'url': 'https://huggingface.co/papers/2503.07608', 'abstract': 'OpenAI o1 and DeepSeek R1 achieve or even surpass human expert-level performance in complex domains like mathematics and science, with reinforcement learning (RL) and reasoning playing a crucial role. In autonomous driving, recent end-to-end models have greatly improved planning performance but still struggle with long-tailed problems due to limited common sense and reasoning abilities. Some studies integrate vision-language models (VLMs) into autonomous driving, but they typically rely on pre-trained models with simple supervised fine-tuning (SFT) on driving data, without further exploration of training strategies or optimizations specifically tailored for planning. In this paper, we propose AlphaDrive, a RL and reasoning framework for VLMs in autonomous driving. AlphaDrive introduces four GRPO-based RL rewards tailored for planning and employs a two-stage planning <PRE_TAG>reasoning training strategy</POST_TAG> that combines SFT with RL. As a result, AlphaDrive significantly improves both planning performance and training efficiency compared to using only SFT or without reasoning. Moreover, we are also excited to discover that, following RL training, AlphaDrive exhibits some emergent multimodal planning capabilities, which is critical for improving driving safety and efficiency. To the best of our knowledge, AlphaDrive is the first to integrate GRPO-based RL with planning reasoning into autonomous driving. Code will be released to facilitate future research.', 'score': 14, 'issue_id': 2632, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '74dfc85ba9f7bcc7', 'authors': ['Bo Jiang', 'Shaoyu Chen', 'Qian Zhang', 'Wenyu Liu', 'Xinggang Wang'], 'affiliations': ['Horizon Robotics', 'Huazhong University of Science and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2503.07608.jpg', 'data': {'categories': ['#rl', '#training', '#optimization', '#multimodal', '#reasoning', '#agents'], 'emoji': '🚗', 'ru': {'title': 'AlphaDrive: ИИ за рулем с обучением и рассуждением', 'desc': 'AlphaDrive - это новая система для автономного вождения, использующая визуально-языковые модели с обучением с подкреплением и рассуждениями. Она вводит четыре награды на основе GRPO, специально разработанные для планирования, и применяет двухэтапную стратегию обучения с рассуждениями. AlphaDrive значительно улучшает производительность планирования и эффективность обучения по сравнению с традиционными методами. После обучения с подкреплением система демонстрирует некоторые возникающие мультимодальные возможности планирования.'}, 'en': {'title': 'AlphaDrive: Revolutionizing Autonomous Driving with RL and Reasoning', 'desc': 'This paper presents AlphaDrive, a novel framework that enhances autonomous driving by integrating reinforcement learning (RL) and reasoning with vision-language models (VLMs). AlphaDrive employs four GRPO-based RL rewards specifically designed for planning tasks and utilizes a two-stage training strategy that combines supervised fine-tuning (SFT) with RL. The results show that AlphaDrive significantly outperforms traditional methods that rely solely on SFT, leading to improved planning performance and training efficiency. Additionally, the framework demonstrates emergent multimodal planning capabilities post-RL training, which are essential for enhancing driving safety and efficiency.'}, 'zh': {'title': 'AlphaDrive：提升自动驾驶的智能规划与推理', 'desc': '本文提出了AlphaDrive，这是一个用于自动驾驶的强化学习（RL）和推理框架。AlphaDrive引入了四种基于GRPO的RL奖励，专门针对规划任务，并采用了结合监督微调（SFT）和RL的两阶段规划推理训练策略。与仅使用SFT或不进行推理的情况相比，AlphaDrive显著提高了规划性能和训练效率。此外，经过RL训练后，AlphaDrive还展现出一些新兴的多模态规划能力，这对提高驾驶安全性和效率至关重要。'}}}, {'id': 'https://huggingface.co/papers/2503.06749', 'title': 'Vision-R1: Incentivizing Reasoning Capability in Multimodal Large\n  Language Models', 'url': 'https://huggingface.co/papers/2503.06749', 'abstract': "DeepSeek-R1-Zero has successfully demonstrated the emergence of reasoning capabilities in LLMs purely through Reinforcement Learning (RL). Inspired by this breakthrough, we explore how RL can be utilized to enhance the reasoning capability of MLLMs. However, direct training with RL struggles to activate complex reasoning capabilities such as questioning and reflection in MLLMs, due to the absence of substantial high-quality multimodal reasoning data. To address this issue, we propose the reasoning MLLM, Vision-R1, to improve multimodal reasoning capability. Specifically, we first construct a high-quality multimodal CoT dataset without human annotations by leveraging an existing MLLM and DeepSeek-R1 through modality bridging and data filtering to obtain a 200K multimodal CoT dataset, Vision-R1-cold dataset. It serves as cold-start initialization data for Vision-R1. To mitigate the optimization challenges caused by overthinking after cold start, we propose Progressive Thinking Suppression Training (PTST) strategy and employ Group Relative Policy Optimization (GRPO) with the hard formatting result reward function to gradually refine the model's ability to learn correct and complex reasoning processes on a 10K multimodal math dataset. Comprehensive experiments show our model achieves an average improvement of sim6% across various multimodal math reasoning benchmarks. Vision-R1-7B achieves a 73.5% accuracy on the widely used MathVista benchmark, which is only 0.4% lower than the leading reasoning model, OpenAI O1. The datasets and code will be released in: https://github.com/Osilly/Vision-R1 .", 'score': 14, 'issue_id': 2632, 'pub_date': '2025-03-09', 'pub_date_card': {'ru': '9 марта', 'en': 'March 9', 'zh': '3月9日'}, 'hash': '403914241aa8967c', 'authors': ['Wenxuan Huang', 'Bohan Jia', 'Zijie Zhai', 'Shaosheng Cao', 'Zheyu Ye', 'Fei Zhao', 'Yao Hu', 'Shaohui Lin'], 'affiliations': ['East China Normal University', 'Xiaohongshu Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2503.06749.jpg', 'data': {'categories': ['#rl', '#training', '#optimization', '#multimodal', '#open_source', '#benchmark', '#reasoning', '#dataset', '#rag'], 'emoji': '🧠', 'ru': {'title': 'Vision-R1: Новый уровень мультимодальных рассуждений в ИИ', 'desc': 'Исследователи разработали Vision-R1, мультимодальную языковую модель с улучшенными способностями рассуждения. Они создали высококачественный набор данных для обучения, используя существующую MLLM и DeepSeek-R1. Для оптимизации модели применили стратегию Progressive Thinking Suppression Training и Group Relative Policy Optimization. Vision-R1-7B достигла точности 73.5% на бенчмарке MathVista, что лишь на 0.4% ниже ведущей модели OpenAI O1.'}, 'en': {'title': 'Enhancing Reasoning in MLLMs with Reinforcement Learning', 'desc': "The paper introduces DeepSeek-R1-Zero, which shows that large language models (LLMs) can develop reasoning skills through Reinforcement Learning (RL). It highlights the challenge of training LLMs directly with RL due to a lack of high-quality multimodal reasoning data. To overcome this, the authors propose a new model called Vision-R1, which utilizes a self-generated multimodal dataset for cold-start training. They also introduce a training strategy called Progressive Thinking Suppression Training (PTST) to enhance the model's reasoning capabilities, achieving significant improvements in multimodal math reasoning tasks."}, 'zh': {'title': '通过强化学习提升多模态推理能力', 'desc': 'DeepSeek-R1-Zero展示了通过强化学习（RL）使大型语言模型（LLM）具备推理能力的可能性。基于这一突破，本文探讨了如何利用强化学习提升多模态大型语言模型（MLLM）的推理能力。由于缺乏高质量的多模态推理数据，直接使用强化学习训练面临挑战，因此我们提出了Vision-R1模型，以改善多模态推理能力。我们构建了一个高质量的多模态链式思维（CoT）数据集，并通过渐进思维抑制训练（PTST）和群体相对策略优化（GRPO）策略来优化模型的推理过程。'}}}, {'id': 'https://huggingface.co/papers/2503.04629', 'title': 'SurveyForge: On the Outline Heuristics, Memory-Driven Generation, and\n  Multi-dimensional Evaluation for Automated Survey Writing', 'url': 'https://huggingface.co/papers/2503.04629', 'abstract': 'Survey paper plays a crucial role in scientific research, especially given the rapid growth of research publications. Recently, researchers have begun using LLMs to automate survey generation for better efficiency. However, the quality gap between LLM-generated surveys and those written by human remains significant, particularly in terms of outline quality and citation accuracy. To close these gaps, we introduce SurveyForge, which first generates the outline by analyzing the logical structure of human-written outlines and referring to the retrieved domain-related articles. Subsequently, leveraging high-quality papers retrieved from memory by our scholar navigation agent, SurveyForge can automatically generate and refine the content of the generated article. Moreover, to achieve a comprehensive evaluation, we construct SurveyBench, which includes 100 human-written survey papers for win-rate comparison and assesses AI-generated survey papers across three dimensions: reference, outline, and content quality. Experiments demonstrate that SurveyForge can outperform previous works such as AutoSurvey.', 'score': 14, 'issue_id': 2633, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 марта', 'en': 'March 6', 'zh': '3月6日'}, 'hash': 'a229855316ab195d', 'authors': ['Xiangchao Yan', 'Shiyang Feng', 'Jiakang Yuan', 'Renqiu Xia', 'Bin Wang', 'Bo Zhang', 'Lei Bai'], 'affiliations': ['Fudan University', 'Shanghai Artificial Intelligence Laboratory', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2503.04629.jpg', 'data': {'categories': ['#survey', '#multimodal', '#agents', '#dataset', '#benchmark'], 'emoji': '📊', 'ru': {'title': 'SurveyForge: Автоматизация создания высококачественных обзорных статей с помощью ИИ', 'desc': 'SurveyForge - это новый подход к автоматическому созданию обзорных статей с использованием больших языковых моделей (LLM). Система сначала генерирует структуру статьи, анализируя логическую структуру обзоров, написанных людьми, и обращаясь к релевантным статьям в предметной области. Затем, используя качественные статьи, извлеченные из памяти с помощью агента навигации, SurveyForge автоматически генерирует и улучшает содержание статьи. Для оценки качества создан набор данных SurveyBench, включающий 100 обзорных статей, написанных людьми, и оценивающий сгенерированные ИИ обзоры по трем параметрам: качество ссылок, структуры и содержания.'}, 'en': {'title': 'Enhancing Automated Survey Generation with SurveyForge', 'desc': 'This paper introduces SurveyForge, a tool designed to improve the quality of automated survey generation using large language models (LLMs). It addresses the significant quality gap between LLM-generated surveys and those created by humans, particularly in outline structure and citation accuracy. SurveyForge first generates an outline by analyzing human-written surveys and retrieving relevant articles, then it refines the content using high-quality papers. The authors also present SurveyBench, a benchmark for evaluating survey papers, which shows that SurveyForge outperforms previous methods like AutoSurvey.'}, 'zh': {'title': 'SurveyForge：提升文献综述生成质量的利器', 'desc': '这篇论文介绍了SurveyForge，一个用于自动生成文献综述的工具。它通过分析人类撰写的综述大纲的逻辑结构，并参考相关领域的文章，首先生成大纲。然后，SurveyForge利用高质量的论文来自动生成和完善文章内容。研究还构建了SurveyBench，用于评估AI生成的综述论文在参考文献、结构和内容质量等三个维度的表现。'}}}, {'id': 'https://huggingface.co/papers/2503.05244', 'title': 'WritingBench: A Comprehensive Benchmark for Generative Writing', 'url': 'https://huggingface.co/papers/2503.05244', 'abstract': "Recent advancements in large language models (LLMs) have significantly enhanced text generation capabilities, yet evaluating their performance in generative writing remains a challenge. Existing benchmarks primarily focus on generic text generation or limited in writing tasks, failing to capture the diverse requirements of high-quality written contents across various domains. To bridge this gap, we present WritingBench, a comprehensive benchmark designed to evaluate LLMs across 6 core writing domains and 100 subdomains, encompassing creative, persuasive, informative, and technical writing. We further propose a query-dependent evaluation framework that empowers LLMs to dynamically generate instance-specific assessment criteria. This framework is complemented by a fine-tuned critic model for criteria-aware scoring, enabling evaluations in style, format and length. The framework's validity is further demonstrated by its data curation capability, which enables 7B-parameter models to approach state-of-the-art (SOTA) performance. We open-source the benchmark, along with evaluation tools and modular framework components, to advance the development of LLMs in writing.", 'score': 13, 'issue_id': 2637, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 марта', 'en': 'March 7', 'zh': '3月7日'}, 'hash': '679eb0b19323e2d2', 'authors': ['Yuning Wu', 'Jiahao Mei', 'Ming Yan', 'Chenliang Li', 'SHaopeng Lai', 'Yuran Ren', 'Zijia Wang', 'Ji Zhang', 'Mengyue Wu', 'Qin Jin', 'Fei Huang'], 'affiliations': ['Alibaba Group', 'Renmin University of China', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2503.05244.jpg', 'data': {'categories': ['#data', '#dataset', '#benchmark', '#story_generation', '#open_source'], 'emoji': '✍️', 'ru': {'title': 'WritingBench: новый стандарт оценки языковых моделей в письме', 'desc': 'Статья представляет WritingBench - комплексный бенчмарк для оценки языковых моделей в различных областях письма. Авторы предлагают фреймворк для динамической генерации критериев оценки и обученную модель-критик для подсчета баллов. Бенчмарк охватывает 6 основных областей и 100 подобластей письма, включая креативное, убеждающее, информативное и техническое письмо. Исследование показывает, что предложенный подход позволяет небольшим моделям приблизиться к производительности современных больших языковых моделей.'}, 'en': {'title': 'WritingBench: A New Standard for Evaluating Language Models in Writing', 'desc': 'This paper introduces WritingBench, a new benchmark for evaluating large language models (LLMs) in generative writing across six key domains and 100 subdomains. It addresses the limitations of existing benchmarks that do not adequately assess the quality of writing in various contexts. The authors propose a query-dependent evaluation framework that allows LLMs to create specific assessment criteria based on the writing task at hand. Additionally, a fine-tuned critic model is introduced to provide criteria-aware scoring, enhancing the evaluation of style, format, and length in generated texts.'}, 'zh': {'title': 'WritingBench：全面评估大型语言模型的写作能力', 'desc': '近年来，大型语言模型（LLMs）的进步显著提升了文本生成能力，但评估其在生成写作中的表现仍然是一个挑战。现有的基准主要集中在通用文本生成或有限的写作任务上，无法捕捉到高质量书面内容在不同领域的多样化需求。为了解决这个问题，我们提出了WritingBench，这是一个全面的基准，旨在评估LLMs在6个核心写作领域和100个子领域的表现，包括创意、说服性、信息性和技术写作。我们还提出了一种依赖查询的评估框架，使LLMs能够动态生成特定实例的评估标准，并通过一个经过微调的评估模型进行风格、格式和长度的评分。'}}}, {'id': 'https://huggingface.co/papers/2503.07602', 'title': 'DreamRelation: Relation-Centric Video Customization', 'url': 'https://huggingface.co/papers/2503.07602', 'abstract': "Relational video customization refers to the creation of personalized videos that depict user-specified relations between two subjects, a crucial task for comprehending real-world visual content. While existing methods can personalize subject appearances and motions, they still struggle with complex relational video customization, where precise relational modeling and high generalization across subject categories are essential. The primary challenge arises from the intricate spatial arrangements, layout variations, and nuanced temporal dynamics inherent in relations; consequently, current models tend to overemphasize irrelevant visual details rather than capturing meaningful interactions. To address these challenges, we propose DreamRelation, a novel approach that personalizes relations through a small set of exemplar videos, leveraging two key components: Relational Decoupling Learning and Relational Dynamics Enhancement. First, in Relational Decoupling Learning, we disentangle relations from subject appearances using relation LoRA triplet and hybrid mask training strategy, ensuring better generalization across diverse relationships. Furthermore, we determine the optimal design of relation LoRA triplet by analyzing the distinct roles of the query, key, and value features within MM-DiT's attention mechanism, making DreamRelation the first relational video generation framework with explainable components. Second, in Relational Dynamics Enhancement, we introduce space-time relational contrastive loss, which prioritizes relational dynamics while minimizing the reliance on detailed subject appearances. Extensive experiments demonstrate that DreamRelation outperforms state-of-the-art methods in relational video customization. Code and models will be made publicly available.", 'score': 12, 'issue_id': 2632, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': 'a32c943630a808fc', 'authors': ['Yujie Wei', 'Shiwei Zhang', 'Hangjie Yuan', 'Biao Gong', 'Longxiang Tang', 'Xiang Wang', 'Haonan Qiu', 'Hengjia Li', 'Shuai Tan', 'Yingya Zhang', 'Hongming Shan'], 'affiliations': ['Alibaba Group', 'Ant Group', 'Fudan University', 'Nanyang Technological University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.07602.jpg', 'data': {'categories': ['#optimization', '#multimodal', '#architecture', '#open_source', '#video', '#interpretability', '#games'], 'emoji': '🎬', 'ru': {'title': 'DreamRelation: Персонализация отношений в видео через машинное обучение', 'desc': 'DreamRelation - это новый подход к персонализации отношений в видео с использованием небольшого набора видео-примеров. Метод включает два ключевых компонента: Relational Decoupling Learning и Relational Dynamics Enhancement. Relational Decoupling Learning разделяет отношения и внешний вид субъектов, используя relation LoRA triplet и гибридную стратегию обучения маскам. Relational Dynamics Enhancement вводит пространственно-временную контрастную потерю отношений, чтобы сосредоточиться на динамике отношений, а не на деталях внешнего вида субъектов.'}, 'en': {'title': 'DreamRelation: Personalizing Video Relationships with Precision', 'desc': 'This paper introduces DreamRelation, a new method for creating personalized videos that focus on the relationships between two subjects. Current techniques struggle with complex relational dynamics, often missing meaningful interactions due to an overemphasis on irrelevant details. DreamRelation addresses this by using Relational Decoupling Learning to separate relationships from appearances and Relational Dynamics Enhancement to focus on the dynamics of these relationships. The approach shows significant improvements over existing methods in generating customized relational videos, with the added benefit of explainable components in its design.'}, 'zh': {'title': 'DreamRelation：个性化视频关系建模的新方法', 'desc': '本论文提出了一种名为DreamRelation的新方法，用于个性化视频中的关系建模。该方法通过关系解耦学习和关系动态增强两个关键组件，解决了复杂关系视频定制中的挑战。通过分析查询、键和值特征在注意力机制中的作用，DreamRelation实现了可解释的关系视频生成。实验结果表明，DreamRelation在关系视频定制方面优于现有的最先进方法。'}}}, {'id': 'https://huggingface.co/papers/2503.07459', 'title': 'MedAgentsBench: Benchmarking Thinking Models and Agent Frameworks for\n  Complex Medical Reasoning', 'url': 'https://huggingface.co/papers/2503.07459', 'abstract': 'Large Language Models (LLMs) have shown impressive performance on existing medical question-answering benchmarks. This high performance makes it increasingly difficult to meaningfully evaluate and differentiate advanced methods. We present <PRE_TAG>MedAgentsBench</POST_TAG>, a benchmark that focuses on challenging medical questions requiring multi-step clinical reasoning, diagnosis formulation, and treatment planning-scenarios where current models still struggle despite their strong performance on standard tests. Drawing from seven established medical datasets, our benchmark addresses three key limitations in existing evaluations: (1) the prevalence of straightforward questions where even base models achieve high performance, (2) inconsistent sampling and evaluation protocols across studies, and (3) lack of systematic analysis of the interplay between performance, cost, and inference time. Through experiments with various base models and reasoning methods, we demonstrate that the latest thinking models, DeepSeek R1 and OpenAI o3, exhibit exceptional performance in complex medical reasoning tasks. Additionally, advanced search-based agent methods offer promising performance-to-cost ratios compared to traditional approaches. Our analysis reveals substantial performance gaps between model families on complex questions and identifies optimal model selections for different computational constraints. Our benchmark and evaluation framework are publicly available at https://github.com/gersteinlab/medagents-benchmark.', 'score': 11, 'issue_id': 2634, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '4d9aba5593231609', 'authors': ['Xiangru Tang', 'Daniel Shao', 'Jiwoong Sohn', 'Jiapeng Chen', 'Jiayi Zhang', 'Jinyu Xiang', 'Fang Wu', 'Yilun Zhao', 'Chenglin Wu', 'Wenqi Shi', 'Arman Cohan', 'Mark Gerstein'], 'affiliations': ['Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2503.07459.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#survey', '#healthcare'], 'emoji': '🩺', 'ru': {'title': 'Новый бенчмарк для оценки языковых моделей в сложных медицинских задачах', 'desc': 'Статья представляет новый бенчмарк <PRE_TAG>MedAgentsBench</POST_TAG> для оценки языковых моделей в области медицины. Бенчмарк фокусируется на сложных медицинских вопросах, требующих многоступенчатых клинических рассуждений. Исследование выявило, что новейшие модели, такие как DeepSeek R1 и OpenAI o3, показывают исключительную производительность в сложных медицинских задачах. Анализ также показал, что методы на основе поиска предлагают многообещающее соотношение производительности и стоимости по сравнению с традиционными подходами.'}, 'en': {'title': 'MedAgentsBench: Elevating Medical Question-Answering Evaluation', 'desc': 'This paper introduces MedAgentsBench, a new benchmark designed to evaluate Large Language Models (LLMs) on complex medical questions that require multi-step reasoning. It addresses limitations in current evaluations, such as the prevalence of simple questions and inconsistent testing protocols. The authors conduct experiments with various models, revealing that advanced models like DeepSeek R1 and OpenAI o3 perform well on challenging tasks, while search-based agents show better performance-to-cost ratios. The study highlights significant performance gaps among different model families and provides insights for selecting models based on computational resources.'}, 'zh': {'title': '医学问答的新基准：挑战复杂推理', 'desc': '本文介绍了一个新的医学问答基准测试——MedAgentsBench，旨在评估大型语言模型在复杂医学问题上的表现。该基准测试专注于需要多步骤临床推理、诊断制定和治疗计划的问题，这些问题是当前模型仍然面临挑战的领域。通过对七个已建立的医学数据集进行分析，本文解决了现有评估中的三个关键限制，包括简单问题的普遍性和评估协议的不一致性。实验结果表明，最新的思维模型在复杂医学推理任务中表现出色，并且基于搜索的代理方法在性能与成本比方面具有良好的前景。'}}}, {'id': 'https://huggingface.co/papers/2503.06580', 'title': 'Agent models: Internalizing Chain-of-Action Generation into Reasoning\n  models', 'url': 'https://huggingface.co/papers/2503.06580', 'abstract': 'Traditional agentic workflows rely on external prompts to manage interactions with tools and the environment, which limits the autonomy of reasoning models. We position Large Agent Models (LAMs) that internalize the generation of Chain-of-Action (CoA), enabling the model to autonomously decide when and how to use external tools. Our proposed AutoCoA framework combines supervised fine-tuning (SFT) and reinforcement learning (RL), allowing the model to seamlessly switch between reasoning and action while efficiently managing environment interactions. Main components include step-level action triggering, trajectory-level CoA optimization, and an internal world model to reduce real-environment interaction costs. Evaluations on open-domain QA tasks demonstrate that AutoCoA-trained agent models significantly outperform ReAct-based workflows in task completion, especially in tasks that require long-term reasoning and multi-step actions. Code and dataset are available at https://github.com/ADaM-BJTU/AutoCoA', 'score': 11, 'issue_id': 2631, 'pub_date': '2025-03-09', 'pub_date_card': {'ru': '9 марта', 'en': 'March 9', 'zh': '3月9日'}, 'hash': '01588376bab86ceb', 'authors': ['Yuxiang Zhang', 'Yuqi Yang', 'Jiangming Shu', 'Xinyan Wen', 'Jitao Sang'], 'affiliations': ['School of Computer Science and Technology, Beijing Jiaotong University, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.06580.jpg', 'data': {'categories': ['#reasoning', '#optimization', '#agents', '#rl', '#training'], 'emoji': '🤖', 'ru': {'title': 'Автономные агентные модели: новый шаг к самостоятельному ИИ', 'desc': 'Эта статья представляет новый подход к созданию агентных моделей искусственного интеллекта, называемых Large Agent Models (LAM). Авторы предлагают фреймворк AutoCoA, который позволяет модели самостоятельно генерировать цепочки действий без внешних подсказок. Фреймворк сочетает в себе методы обучения с учителем и обучения с подкреплением для оптимизации взаимодействия модели с окружающей средой. Результаты показывают, что модели, обученные с помощью AutoCoA, значительно превосходят традиционные подходы в задачах, требующих долгосрочных рассуждений и многошаговых действий.'}, 'en': {'title': 'Empowering Autonomous Reasoning with AutoCoA', 'desc': 'This paper introduces Large Agent Models (LAMs) that enhance the autonomy of reasoning models by allowing them to generate their own Chain-of-Action (CoA) without relying on external prompts. The AutoCoA framework integrates supervised fine-tuning (SFT) and reinforcement learning (RL) to enable models to effectively alternate between reasoning and taking actions in their environment. Key features of this framework include mechanisms for triggering actions at each step, optimizing CoA over entire trajectories, and utilizing an internal world model to minimize the costs associated with real-world interactions. Evaluations show that models trained with AutoCoA outperform traditional ReAct-based workflows, particularly in complex tasks requiring extended reasoning and multiple actions.'}, 'zh': {'title': '自主决策的智能代理模型', 'desc': '传统的智能工作流程依赖外部提示来管理与工具和环境的交互，这限制了推理模型的自主性。我们提出了大型代理模型（LAMs），使其能够内部生成行动链（CoA），从而自主决定何时以及如何使用外部工具。我们提出的AutoCoA框架结合了监督微调（SFT）和强化学习（RL），使模型能够在推理和行动之间无缝切换，同时有效管理与环境的交互。评估结果表明，经过AutoCoA训练的代理模型在开放领域问答任务中显著优于基于ReAct的工作流程，尤其是在需要长期推理和多步骤行动的任务中。'}}}, {'id': 'https://huggingface.co/papers/2503.04973', 'title': 'Beyond RAG: Task-Aware KV Cache Compression for Comprehensive Knowledge\n  Reasoning', 'url': 'https://huggingface.co/papers/2503.04973', 'abstract': 'Incorporating external knowledge in large language models (LLMs) enhances their utility across diverse applications, but existing methods have trade-offs. Retrieval-Augmented Generation (RAG) fetches evidence via similarity search, but key information may fall outside top ranked results. Long-context models can process multiple documents but are computationally expensive and limited by context window size. Inspired by students condensing study material for open-book exams, we propose task-aware key-value (KV) cache compression, which compresses external knowledge in a zero- or few-shot setup. This enables LLMs to reason efficiently over a compacted representation of all relevant information. Experiments show our approach outperforms both RAG and task-agnostic compression methods. On LongBench v2, it improves accuracy by up to 7 absolute points over RAG with a 30x compression rate, while reducing inference latency from 0.43s to 0.16s. A synthetic dataset highlights that RAG performs well when sparse evidence suffices, whereas task-aware compression is superior for broad knowledge tasks.', 'score': 11, 'issue_id': 2640, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 марта', 'en': 'March 6', 'zh': '3月6日'}, 'hash': 'a3117b7e2b2c099c', 'authors': ['Giulio Corallo', 'Orion Weller', 'Fabio Petroni', 'Paolo Papotti'], 'affiliations': ['EURECOM', 'Johns Hopkins University', 'SAP Labs', 'Samaya AI'], 'pdf_title_img': 'assets/pdf/title_img/2503.04973.jpg', 'data': {'categories': ['#reasoning', '#synthetic', '#long_context', '#rag', '#inference'], 'emoji': '🧠', 'ru': {'title': 'Эффективное обогащение знаний LLM через интеллектуальное сжатие информации', 'desc': 'Статья предлагает новый метод обогащения знаний больших языковых моделей (LLM) - сжатие кэша ключ-значение с учетом задачи. Этот подход позволяет LLM эффективно рассуждать на основе сжатого представления всей релевантной информации. Эксперименты показывают, что предложенный метод превосходит как RAG, так и методы сжатия, не учитывающие задачу. На тестовом наборе LongBench v2 он повышает точность на 7 процентных пунктов по сравнению с RAG при 30-кратном сжатии, одновременно снижая задержку вывода.'}, 'en': {'title': 'Efficient Knowledge Integration for Enhanced Language Model Performance', 'desc': 'This paper presents a new method for enhancing large language models (LLMs) by incorporating external knowledge more effectively. The proposed task-aware key-value (KV) cache compression allows LLMs to efficiently reason over a compact representation of relevant information, improving performance in zero- or few-shot scenarios. Compared to existing methods like Retrieval-Augmented Generation (RAG), this approach achieves better accuracy and significantly reduces inference time. Experiments demonstrate that while RAG excels with sparse evidence, the new compression technique is more effective for tasks requiring extensive knowledge.'}, 'zh': {'title': '任务感知压缩，提升语言模型效率', 'desc': '本论文提出了一种新的方法，通过任务感知的键值（KV）缓存压缩来整合外部知识，以提高大型语言模型（LLMs）的效率。该方法在零样本或少样本设置下压缩外部知识，使得模型能够在处理相关信息时更加高效。实验结果表明，该方法在准确性和推理延迟方面均优于现有的检索增强生成（RAG）和任务无关的压缩方法。特别是在LongBench v2数据集上，该方法的准确性提高了7个百分点，同时推理延迟从0.43秒减少到0.16秒。'}}}, {'id': 'https://huggingface.co/papers/2503.04812', 'title': 'LLaVE: Large Language and Vision Embedding Models with Hardness-Weighted\n  Contrastive Learning', 'url': 'https://huggingface.co/papers/2503.04812', 'abstract': "Universal multimodal embedding models play a critical role in tasks such as interleaved image-text retrieval, multimodal RAG, and multimodal clustering. However, our empirical results indicate that existing LMM-based embedding models trained with the standard InfoNCE loss exhibit a high degree of overlap in similarity distribution between positive and negative pairs, making it challenging to distinguish hard negative pairs effectively. To deal with this issue, we propose a simple yet effective framework that dynamically improves the embedding model's representation learning for negative pairs based on their discriminative difficulty. Within this framework, we train a series of models, named LLaVE, and evaluate them on the MMEB benchmark, which covers 4 meta-tasks and 36 datasets. Experimental results show that LLaVE establishes stronger baselines that achieve state-of-the-art (SOTA) performance while demonstrating strong scalability and efficiency. Specifically, LLaVE-2B surpasses the previous SOTA 7B models, while LLaVE-7B achieves a further performance improvement of 6.2 points. Although LLaVE is trained on image-text data, it can generalize to text-video retrieval tasks in a zero-shot manner and achieve strong performance, demonstrating its remarkable potential for transfer to other embedding tasks.", 'score': 10, 'issue_id': 2631, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 марта', 'en': 'March 4', 'zh': '3月4日'}, 'hash': 'a1fec2227e343e88', 'authors': ['Zhibin Lan', 'Liqiang Niu', 'Fandong Meng', 'Jie Zhou', 'Jinsong Su'], 'affiliations': ['Pattern Recognition Center, WeChat AI, Tencent Inc, China', 'School of Informatics, Xiamen University, China', 'Shanghai Artificial Intelligence Laboratory, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.04812.jpg', 'data': {'categories': ['#transfer_learning', '#benchmark', '#rag', '#multimodal', '#training'], 'emoji': '🔀', 'ru': {'title': 'Динамическое обучение для различения сложных негативных пар в мультимодальных эмбеддингах', 'desc': 'Статья представляет новый фреймворк для улучшения обучения мультимодальных эмбеддинг-моделей. Авторы обнаружили проблему перекрытия распределений сходства для позитивных и негативных пар в существующих моделях. Предложенный подход динамически улучшает представление негативных пар на основе их сложности различения. Разработанные модели LLaVE достигают лучших результатов на бенчмарке MMEB и демонстрируют хорошую масштабируемость.'}, 'en': {'title': 'Enhancing Multimodal Embeddings with LLaVE for Better Performance', 'desc': 'This paper discusses the development of a new framework called LLaVE for improving multimodal embedding models, which are used for tasks involving both images and text. The authors identify a problem with existing models that struggle to differentiate between similar positive and negative pairs due to overlapping similarity distributions. To address this, LLaVE dynamically enhances the learning of negative pairs based on their difficulty, leading to better representation learning. The results show that LLaVE outperforms previous state-of-the-art models and can also be applied effectively to other tasks like text-video retrieval.'}, 'zh': {'title': 'LLaVE：提升多模态嵌入的强大工具', 'desc': '本文提出了一种新的多模态嵌入模型LLaVE，旨在解决现有模型在处理正负样本时相似度分布重叠的问题。通过动态调整负样本的表示学习，LLaVE能够更有效地区分困难的负样本。实验结果表明，LLaVE在多个基准测试中表现出色，超越了之前的最先进模型，并在图像-文本数据上训练后，能够在零样本情况下推广到文本-视频检索任务。该模型展示了强大的可扩展性和效率，具有广泛的应用潜力。'}}}, {'id': 'https://huggingface.co/papers/2503.07334', 'title': 'Unleashing the Potential of Large Language Models for Text-to-Image\n  Generation through Autoregressive Representation Alignment', 'url': 'https://huggingface.co/papers/2503.07334', 'abstract': "We present Autoregressive Representation Alignment (ARRA), a new training framework that unlocks global-coherent text-to-image generation in autoregressive LLMs without architectural changes. Unlike prior work that requires complex architectural redesigns, ARRA aligns LLM hidden states with visual representations from external visual foundational models via a global visual alignment loss and a hybrid token, <HYBNEXT>. This token enforces dual constraints: local next-token prediction and global semantic distillation, enabling LLMs to implicitly learn spatial and contextual coherence while retaining their original autoregressive paradigm. Extensive experiments validate ARRA's plug-and-play versatility. When training from text-generation-only LLMs or random initialization, ARRA reduces FID by 25.5% (MIMIC-CXR), 8.8% (DeepEyeNet), and 7.5% (ImageNet) for advanced autoregressive LLMs like Chameleon and LlamaGen, all without framework modifications. For domain adaption, ARRA aligns general-purpose LLMs with specialized models (e.g., BioMedCLIP), achieving an 18.6% FID reduction over direct fine-tuning on medical imaging (MIMIC-CXR). By demonstrating that training objective redesign -- not just architectural innovation -- can resolve cross-modal global coherence challenges, ARRA offers a complementary paradigm for advancing autoregressive models. Code and models will be released to advance autoregressive image generation.", 'score': 9, 'issue_id': 2643, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': 'da94bc1af48685ac', 'authors': ['Xing Xie', 'Jiawei Liu', 'Ziyue Lin', 'Huijie Fan', 'Zhi Han', 'Yandong Tang', 'Liangqiong Qu'], 'affiliations': ['Shenyang Institute of Automation, Chinese Academy of Sciences', 'The University of Hong Kong', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2503.07334.jpg', 'data': {'categories': ['#training', '#optimization', '#alignment', '#open_source', '#multimodal'], 'emoji': '🖼️', 'ru': {'title': 'ARRA: Глобальная согласованность в генерации изображений без изменения архитектуры', 'desc': 'Представлен новый метод обучения ARRA, позволяющий автореgressивным языковым моделям генерировать глобально согласованные изображения без изменения архитектуры. ARRA выравнивает скрытые состояния языковой модели с визуальными представлениями внешних моделей компьютерного зрения с помощью специальной функции потерь и гибридного токена. Этот подход позволяет языковым моделям неявно учиться пространственной и контекстной согласованности, сохраняя при этом исходную авторегрессивную парадигму. Эксперименты показывают, что ARRA значительно улучшает качество генерации изображений для различных задач и доменов.'}, 'en': {'title': 'Unlocking Coherent Text-to-Image Generation with ARRA', 'desc': "The paper introduces Autoregressive Representation Alignment (ARRA), a novel training framework that enhances text-to-image generation in autoregressive language models (LLMs) without changing their architecture. ARRA achieves this by aligning the hidden states of LLMs with visual representations from external models using a global visual alignment loss and a special hybrid token, <HYBNEXT>. This token imposes both local next-token prediction and global semantic distillation, allowing LLMs to learn spatial and contextual coherence effectively. The results show significant improvements in image generation quality, as evidenced by reduced Fréchet Inception Distance (FID) scores across various datasets, demonstrating ARRA's effectiveness and versatility in enhancing autoregressive models."}, 'zh': {'title': '自回归模型的新突破：全球一致性生成', 'desc': '本文提出了一种新的训练框架，称为自回归表示对齐（ARRA），旨在实现自回归大语言模型（LLM）中的全球一致性文本到图像生成，而无需改变模型架构。ARRA通过全局视觉对齐损失和混合标记<HYBNEXT>，将LLM的隐藏状态与外部视觉基础模型的视觉表示对齐。该标记施加了局部下一个标记预测和全局语义蒸馏的双重约束，使LLM能够在保持自回归范式的同时，隐式学习空间和上下文的一致性。实验结果验证了ARRA的灵活性，显示出在多个数据集上显著降低了FID值，证明了训练目标的重新设计可以解决跨模态全球一致性挑战。'}}}, {'id': 'https://huggingface.co/papers/2503.07507', 'title': 'PE3R: Perception-Efficient 3D Reconstruction', 'url': 'https://huggingface.co/papers/2503.07507', 'abstract': 'Recent advancements in 2D-to-3D perception have significantly improved the understanding of 3D scenes from 2D images. However, existing methods face critical challenges, including limited generalization across scenes, suboptimal perception accuracy, and slow reconstruction speeds. To address these limitations, we propose Perception-Efficient 3D Reconstruction (PE3R), a novel framework designed to enhance both accuracy and efficiency. PE3R employs a feed-forward architecture to enable rapid 3D semantic field reconstruction. The framework demonstrates robust zero-shot generalization across diverse scenes and objects while significantly improving reconstruction speed. Extensive experiments on 2D-to-3D open-vocabulary segmentation and 3D reconstruction validate the effectiveness and versatility of PE3R. The framework achieves a minimum 9-fold speedup in 3D semantic field reconstruction, along with substantial gains in perception accuracy and reconstruction precision, setting new benchmarks in the field. The code is publicly available at: https://github.com/hujiecpp/PE3R.', 'score': 8, 'issue_id': 2631, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '6a53d341839fc41f', 'authors': ['Jie Hu', 'Shizun Wang', 'Xinchao Wang'], 'affiliations': ['xML Lab, National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2503.07507.jpg', 'data': {'categories': ['#3d', '#architecture', '#benchmark'], 'emoji': '🔍', 'ru': {'title': 'Быстрая и точная 3D-реконструкция из 2D-изображений', 'desc': 'PE3R - это новая система для улучшения 3D-реконструкции из 2D-изображений. Она использует прямую архитектуру для быстрого восстановления 3D семантических полей. PE3R демонстрирует надежную обобщающую способность на разных сценах и объектах без дополнительного обучения. Эксперименты показывают значительное ускорение реконструкции и повышение точности восприятия по сравнению с существующими методами.'}, 'en': {'title': 'Revolutionizing 3D Scene Understanding with PE3R', 'desc': 'This paper introduces Perception-Efficient 3D Reconstruction (PE3R), a new framework that enhances the process of understanding 3D scenes from 2D images. PE3R addresses key challenges in existing methods, such as limited generalization and slow reconstruction speeds, by utilizing a feed-forward architecture. The framework achieves impressive zero-shot generalization across various scenes and objects, while also significantly speeding up the reconstruction process. Experimental results show that PE3R offers at least a 9-fold increase in reconstruction speed and improved accuracy, establishing new benchmarks in 2D-to-3D perception.'}, 'zh': {'title': '感知高效3D重建，速度与精度的双重提升', 'desc': '最近在2D到3D感知方面的进展显著提高了从2D图像理解3D场景的能力。然而，现有方法面临着场景泛化能力有限、感知精度不佳和重建速度慢等关键挑战。为了解决这些问题，我们提出了感知高效3D重建（PE3R）框架，旨在提高准确性和效率。PE3R采用前馈架构，能够快速重建3D语义场，并在多样场景和物体上展示出强大的零样本泛化能力，同时显著提高重建速度。'}}}, {'id': 'https://huggingface.co/papers/2503.07197', 'title': 'Effective and Efficient Masked Image Generation Models', 'url': 'https://huggingface.co/papers/2503.07197', 'abstract': "Although masked image generation models and masked diffusion models are designed with different motivations and objectives, we observe that they can be unified within a single framework. Building upon this insight, we carefully explore the design space of training and sampling, identifying key factors that contribute to both performance and efficiency. Based on the improvements observed during this exploration, we develop our model, referred to as eMIGM. Empirically, eMIGM demonstrates strong performance on ImageNet generation, as measured by Fr\\'echet Inception Distance (FID). In particular, on ImageNet 256x256, with similar number of function evaluations (NFEs) and model parameters, eMIGM outperforms the seminal VAR. Moreover, as NFE and model parameters increase, eMIGM achieves performance comparable to the state-of-the-art continuous diffusion models while requiring less than 40% of the NFE. Additionally, on ImageNet 512x512, with only about 60% of the NFE, eMIGM outperforms the state-of-the-art continuous diffusion models.", 'score': 8, 'issue_id': 2631, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '4740cc0178bbf099', 'authors': ['Zebin You', 'Jingyang Ou', 'Xiaolu Zhang', 'Jun Hu', 'Jun Zhou', 'Chongxuan Li'], 'affiliations': ['Ant Group', 'Beijing Key Laboratory of Big Data Management and Analysis Method', 'Gaoling School of AI, Renmin University of China'], 'pdf_title_img': 'assets/pdf/title_img/2503.07197.jpg', 'data': {'categories': ['#optimization', '#diffusion', '#training', '#cv'], 'emoji': '🖼️', 'ru': {'title': 'Объединение маскированных моделей для эффективной генерации изображений', 'desc': 'Исследователи объединили модели генерации маскированных изображений и маскированные диффузионные модели в единую структуру. Они изучили пространство проектирования обучения и сэмплирования, выявив ключевые факторы, влияющие на производительность и эффективность. На основе этого анализа была разработана модель eMIGM, которая показала высокую эффективность при генерации изображений ImageNet. eMIGM превзошла существующие модели по метрике FID при меньшем количестве вычислений и параметров.'}, 'en': {'title': 'Unifying Masked Models for Efficient Image Generation', 'desc': "This paper presents a unified framework for masked image generation models and masked diffusion models, highlighting their similarities despite different goals. The authors explore various design choices in training and sampling, which significantly impact the model's performance and efficiency. They introduce a new model called eMIGM, which shows superior results on ImageNet generation tasks, particularly in terms of Fréchet Inception Distance (FID). Notably, eMIGM achieves competitive performance with fewer function evaluations compared to existing state-of-the-art models, demonstrating its efficiency and effectiveness in image generation."}, 'zh': {'title': '统一掩蔽模型，提升图像生成性能', 'desc': '本文探讨了掩蔽图像生成模型和掩蔽扩散模型的统一框架。我们分析了训练和采样的设计空间，识别出影响性能和效率的关键因素。基于这些改进，我们开发了名为eMIGM的模型。实验结果表明，eMIGM在ImageNet生成任务中表现优异，尤其在较低的函数评估次数下超越了现有的最先进模型。'}}}, {'id': 'https://huggingface.co/papers/2503.05856', 'title': 'This Is Your Doge, If It Please You: Exploring Deception and Robustness\n  in Mixture of LLMs', 'url': 'https://huggingface.co/papers/2503.05856', 'abstract': "Mixture of <PRE_TAG>large language model (LLMs) Agents (MoA)</POST_TAG> architectures achieve state-of-the-art performance on prominent benchmarks like AlpacaEval 2.0 by leveraging the collaboration of multiple LLMs at inference time. Despite these successes, an evaluation of the safety and reliability of MoA is missing. We present the first comprehensive study of MoA's robustness against deceptive LLM agents that deliberately provide misleading responses. We examine factors like the propagation of deceptive information, model size, and information availability, and uncover critical vulnerabilities. On AlpacaEval 2.0, the popular LLaMA 3.1-70B model achieves a length-controlled Win Rate (LC WR) of 49.2% when coupled with 3-layer MoA (6 LLM agents). However, we demonstrate that introducing only a single carefully-instructed deceptive agent into the MoA can reduce performance to 37.9%, effectively nullifying all MoA gains. On QuALITY, a multiple-choice comprehension task, the impact is also severe, with accuracy plummeting by a staggering 48.5%. Inspired in part by the historical Doge of Venice voting process, designed to minimize influence and deception, we propose a range of unsupervised defense mechanisms that recover most of the lost performance.", 'score': 7, 'issue_id': 2639, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 марта', 'en': 'March 7', 'zh': '3月7日'}, 'hash': 'a7ca72375d4cd423', 'authors': ['Lorenz Wolf', 'Sangwoong Yoon', 'Ilija Bogunovic'], 'affiliations': ['University College London Center for Artificial Intelligence, London, UK'], 'pdf_title_img': 'assets/pdf/title_img/2503.05856.jpg', 'data': {'categories': ['#inference', '#security', '#benchmark', '#agents', '#hallucinations'], 'emoji': '🛡️', 'ru': {'title': 'Защита коллаборативных языковых моделей от обмана', 'desc': 'Исследование посвящено архитектуре Mixture of large language model Agents (MoA), которая достигает высоких результатов на бенчмарках, объединяя несколько языковых моделей. Авторы изучают уязвимости MoA к обманным агентам, намеренно предоставляющим ложные ответы. Эксперименты показывают, что даже один обманный агент может значительно снизить производительность MoA на задачах вроде AlpacaEval 2.0 и QuALITY. Предлагаются механизмы защиты, вдохновленные историческим процессом голосования Дожа Венеции, для восстановления утраченной производительности.'}, 'en': {'title': 'Strengthening MoA: Defending Against Deceptive Agents', 'desc': 'This paper investigates the robustness of Mixture of Large Language Model Agents (MoA) architectures, which have shown impressive results in various benchmarks. The authors highlight a significant gap in understanding how these models handle deceptive agents that can provide misleading information. Their experiments reveal that even a single deceptive agent can drastically reduce the performance of MoA systems, indicating vulnerabilities in their design. To address these issues, the paper proposes unsupervised defense mechanisms inspired by historical voting processes to enhance the reliability of MoA against such threats.'}, 'zh': {'title': '提升大型语言模型的安全性与可靠性', 'desc': '这篇论文研究了混合大型语言模型（LLMs）代理架构的安全性和可靠性。尽管这些架构在多个基准测试中表现出色，但对其抵御误导性信息的能力缺乏评估。研究发现，单个误导性代理的引入会显著降低模型的性能，甚至抵消所有的优势。为此，论文提出了一系列无监督的防御机制，以恢复大部分丢失的性能。'}}}, {'id': 'https://huggingface.co/papers/2503.06520', 'title': 'Seg-Zero: Reasoning-Chain Guided Segmentation via Cognitive\n  Reinforcement', 'url': 'https://huggingface.co/papers/2503.06520', 'abstract': "Traditional methods for reasoning segmentation rely on supervised fine-tuning with categorical labels and simple descriptions, limiting its out-of-domain generalization and lacking explicit reasoning processes. To address these limitations, we propose Seg-Zero, a novel framework that demonstrates remarkable generalizability and derives explicit chain-of-thought reasoning through cognitive reinforcement. Seg-Zero introduces a decoupled architecture consisting of a reasoning model and a segmentation model. The reasoning model interprets user intentions, generates explicit reasoning chains, and produces positional prompts, which are subsequently used by the segmentation model to generate precious pixel-level masks. We design a sophisticated reward mechanism that integrates both format and accuracy rewards to effectively guide optimization directions. Trained exclusively via reinforcement learning with GRPO and without explicit reasoning data, Seg-Zero achieves robust zero-shot generalization and exhibits emergent test-time reasoning capabilities. Experiments show that Seg-Zero-7B achieves a zero-shot performance of 57.5 on the ReasonSeg benchmark, surpassing the prior LISA-7B by 18\\%. This significant improvement highlights Seg-Zero's ability to generalize across domains while presenting an explicit reasoning process. Code is available at https://github.com/dvlab-research/Seg-Zero.", 'score': 6, 'issue_id': 2630, 'pub_date': '2025-03-09', 'pub_date_card': {'ru': '9 марта', 'en': 'March 9', 'zh': '3月9日'}, 'hash': 'b21eb23a448d282e', 'authors': ['Yuqi Liu', 'Bohao Peng', 'Zhisheng Zhong', 'Zihao Yue', 'Fanbin Lu', 'Bei Yu', 'Jiaya Jia'], 'affiliations': ['CUHK', 'HKUST', 'RUC'], 'pdf_title_img': 'assets/pdf/title_img/2503.06520.jpg', 'data': {'categories': ['#rl', '#benchmark', '#architecture', '#reasoning', '#rlhf', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Сегментация с рассуждением: ИИ учится объяснять свои решения', 'desc': 'Seg-Zero - это новая модель для сегментации изображений, использующая когнитивное подкрепление для улучшения обобщающей способности и явного рассуждения. Архитектура состоит из модели рассуждений, генерирующей цепочки рассуждений и позиционные подсказки, и модели сегментации, создающей маски на уровне пикселей. Обучение происходит с помощью обучения с подкреплением и специального механизма вознаграждений. Seg-Zero-7B превосходит предыдущие модели на 18% в задаче zero-shot сегментации на бенчмарке ReasonSeg.'}, 'en': {'title': 'Seg-Zero: Revolutionizing Reasoning Segmentation with Zero-Shot Generalization', 'desc': 'The paper introduces Seg-Zero, a new framework for reasoning segmentation that overcomes the limitations of traditional supervised methods. It features a decoupled architecture with a reasoning model that interprets user intentions and generates reasoning chains, which are then used by a segmentation model to create detailed pixel-level masks. The framework employs a unique reward mechanism that balances format and accuracy to optimize performance through reinforcement learning. Seg-Zero demonstrates impressive zero-shot generalization capabilities, achieving a significant performance boost on the ReasonSeg benchmark compared to previous models.'}, 'zh': {'title': 'Seg-Zero：突破性推理与分割的结合', 'desc': '传统的分割推理方法依赖于带有类别标签的监督微调，这限制了其在不同领域的泛化能力，并缺乏明确的推理过程。为了解决这些问题，我们提出了Seg-Zero，这是一种新颖的框架，展示了显著的泛化能力，并通过认知强化推导出明确的推理链。Seg-Zero引入了一个解耦架构，包括推理模型和分割模型，推理模型解释用户意图，生成明确的推理链，并产生位置提示，随后由分割模型生成精确的像素级掩码。通过强化学习训练，Seg-Zero在没有明确推理数据的情况下，实现了强大的零样本泛化能力，并展现出突出的测试时推理能力。'}}}, {'id': 'https://huggingface.co/papers/2503.06121', 'title': 'BlackGoose Rimer: Harnessing RWKV-7 as a Simple yet Superior Replacement\n  for Transformers in Large-Scale Time Series Modeling', 'url': 'https://huggingface.co/papers/2503.06121', 'abstract': "Time series models face significant challenges in scaling to handle large and complex datasets, akin to the scaling achieved by large language models (LLMs). The unique characteristics of time series data and the computational demands of model scaling necessitate innovative approaches. While researchers have explored various architectures such as Transformers, LSTMs, and GRUs to address these challenges, we propose a novel solution using RWKV-7, which incorporates meta-learning into its state update mechanism. By integrating RWKV-7's time mix and channel mix components into the transformer-based time series model Timer, we achieve a substantial performance improvement of approximately 1.13 to 43.3x and a 4.5x reduction in training time with 1/23 parameters, all while utilizing fewer parameters. Our code and model weights are publicly available for further research and development at https://github.com/Alic-Li/BlackGoose_Rimer.", 'score': 5, 'issue_id': 2631, 'pub_date': '2025-03-08', 'pub_date_card': {'ru': '8 марта', 'en': 'March 8', 'zh': '3月8日'}, 'hash': '3f03abe6317e5bba', 'authors': ['Li weile', 'Liu Xiao'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2503.06121.jpg', 'data': {'categories': ['#optimization', '#architecture', '#training', '#open_source'], 'emoji': '⏳', 'ru': {'title': 'Революция в масштабировании моделей временных рядов с RWKV-7', 'desc': 'Статья представляет новый подход к масштабированию моделей временных рядов с использованием архитектуры RWKV-7. Авторы интегрируют компоненты RWKV-7 в трансформер-модель Timer для обработки временных рядов. Результаты показывают значительное улучшение производительности и сокращение времени обучения при меньшем количестве параметров. Предложенный метод решает проблемы масштабирования моделей временных рядов, аналогичные тем, с которыми сталкиваются большие языковые модели.'}, 'en': {'title': 'Revolutionizing Time Series with RWKV-7: Efficiency Meets Performance', 'desc': "This paper addresses the challenges of scaling time series models to manage large and complex datasets, similar to the advancements seen in large language models. It introduces RWKV-7, a novel approach that integrates meta-learning into the state update mechanism of time series models. By combining RWKV-7's time mix and channel mix components with the Timer model, the authors report significant performance enhancements and reduced training times. The proposed method achieves impressive results with fewer parameters, making it a promising solution for time series analysis."}, 'zh': {'title': '创新时间序列模型，提升性能与效率', 'desc': '时间序列模型在处理大型复杂数据集时面临显著挑战，类似于大型语言模型的扩展能力。时间序列数据的独特特性和模型扩展的计算需求需要创新的方法。我们提出了一种新颖的解决方案，使用RWKV-7将元学习融入状态更新机制。通过将RWKV-7的时间混合和通道混合组件整合到基于变换器的时间序列模型Timer中，我们实现了约1.13到43.3倍的性能提升，并将训练时间减少了4.5倍，同时参数数量仅为原来的1/23。'}}}, {'id': 'https://huggingface.co/papers/2503.03499', 'title': 'State-offset Tuning: State-based Parameter-Efficient Fine-Tuning for\n  State Space Models', 'url': 'https://huggingface.co/papers/2503.03499', 'abstract': 'State Space Models (SSMs) have emerged as efficient alternatives to Transformers, mitigating their quadratic computational cost. However, the application of Parameter-Efficient Fine-Tuning (PEFT) methods to SSMs remains largely unexplored. In particular, prompt-based methods like Prompt Tuning and Prefix-Tuning, which are widely used in Transformers, do not perform well on SSMs. To address this, we propose state-based methods as a superior alternative to prompt-based methods. This new family of methods naturally stems from the architectural characteristics of SSMs. State-based methods adjust state-related features directly instead of depending on external prompts. Furthermore, we introduce a novel state-based PEFT method: State-offset Tuning. At every timestep, our method directly affects the state at the current step, leading to more effective adaptation. Through extensive experiments across diverse datasets, we demonstrate the effectiveness of our method. Code is available at https://github.com/furiosa-ai/ssm-state-tuning.', 'score': 5, 'issue_id': 2630, 'pub_date': '2025-03-05', 'pub_date_card': {'ru': '5 марта', 'en': 'March 5', 'zh': '3月5日'}, 'hash': '80ab6abd822f2976', 'authors': ['Wonjun Kang', 'Kevin Galim', 'Yuchen Zeng', 'Minjae Lee', 'Hyung Il Koo', 'Nam Ik Cho'], 'affiliations': ['UW-Madison'], 'pdf_title_img': 'assets/pdf/title_img/2503.03499.jpg', 'data': {'categories': ['#architecture', '#training', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Эффективная тонкая настройка моделей пространства состояний без промптов', 'desc': 'Эта статья представляет новый подход к тонкой настройке моделей пространства состояний (SSM) в машинном обучении. Авторы предлагают методы, основанные на состояниях, как альтернативу методам, основанным на промптах, которые хорошо работают для трансформеров, но неэффективны для SSM. Они вводят новый метод под названием State-offset Tuning, который напрямую влияет на состояние модели на каждом временном шаге. Эксперименты на различных наборах данных демонстрируют эффективность предложенного подхода.'}, 'en': {'title': 'Revolutionizing Fine-Tuning with State-Based Methods for SSMs', 'desc': 'This paper discusses the limitations of using traditional prompt-based fine-tuning methods on State Space Models (SSMs), which are more efficient than Transformers. The authors propose a new approach called state-based methods that leverage the unique structure of SSMs to improve performance. Specifically, they introduce State-offset Tuning, a novel parameter-efficient fine-tuning technique that modifies the state at each timestep directly. Experimental results show that this method outperforms existing prompt-based techniques, highlighting its effectiveness in adapting SSMs for various tasks.'}, 'zh': {'title': '基于状态的微调：超越提示的方法', 'desc': '状态空间模型（SSMs）作为变换器的高效替代方案，能够减轻其二次计算成本。然而，参数高效微调（PEFT）方法在SSMs上的应用仍然未被充分探索。我们提出基于状态的方法，作为优于基于提示的方法的新选择，这些方法直接调整与状态相关的特征，而不是依赖外部提示。我们还引入了一种新颖的基于状态的PEFT方法：状态偏移微调，能够在每个时间步直接影响当前状态，从而实现更有效的适应。'}}}, {'id': 'https://huggingface.co/papers/2503.07595', 'title': 'Detection Avoidance Techniques for Large Language Models', 'url': 'https://huggingface.co/papers/2503.07595', 'abstract': "The increasing popularity of large language models has not only led to widespread use but has also brought various risks, including the potential for systematically spreading fake news. Consequently, the development of classification systems such as DetectGPT has become vital. These detectors are vulnerable to evasion techniques, as demonstrated in an experimental series: Systematic changes of the generative models' temperature proofed shallow learning-detectors to be the least reliable. Fine-tuning the generative model via reinforcement learning circumvented BERT-based-detectors. Finally, rephrasing led to a >90\\% evasion of zero-shot-detectors like DetectGPT, although texts stayed highly similar to the original. A comparison with existing work highlights the better performance of the presented methods. Possible implications for society and further research are discussed.", 'score': 4, 'issue_id': 2631, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '0b6929d80e047189', 'authors': ['Sinclair Schneider', 'Florian Steuber', 'Joao A. G. Schneider', 'Gabi Dreo Rodosek'], 'affiliations': ['Research Institute CODE, Bundeswehr University Munich, Munich, 81739, Bavaria, Germany'], 'pdf_title_img': 'assets/pdf/title_img/2503.07595.jpg', 'data': {'categories': ['#security', '#benchmark', '#rlhf', '#data', '#ethics', '#rl', '#hallucinations'], 'emoji': '🕵️', 'ru': {'title': 'Обман детекторов: как LLM обходят системы обнаружения ИИ-текстов', 'desc': 'Статья рассматривает проблему уязвимости систем обнаружения текстов, сгенерированных большими языковыми моделями (LLM). Авторы провели серию экспериментов, демонстрирующих, как различные методы могут обойти существующие детекторы, такие как DetectGPT. Были исследованы техники изменения температуры генеративной модели, дообучение с помощью обучения с подкреплением и перефразирование текста. Результаты показывают, что предложенные методы обхода более эффективны, чем существующие подходы, что поднимает вопросы о надежности текущих систем обнаружения ИИ-генерированного контента.'}, 'en': {'title': 'Enhancing Evasion Techniques Against Language Model Detectors', 'desc': "This paper discusses the risks associated with large language models, particularly their potential to spread misinformation. It introduces DetectGPT, a classification system designed to identify generated text, but reveals its vulnerabilities to evasion techniques. The study shows that adjusting the generative model's temperature and fine-tuning it with reinforcement learning can effectively bypass existing detectors. Additionally, rephrasing generated content can achieve over 90% evasion rates while maintaining similarity to the original text, highlighting the need for improved detection methods."}, 'zh': {'title': '应对假新闻的智能检测挑战', 'desc': '随着大型语言模型的普及，假新闻传播的风险也随之增加。因此，开发像DetectGPT这样的分类系统变得至关重要。这些检测器容易受到规避技术的影响，实验表明，生成模型温度的系统性变化使得浅层学习检测器的可靠性最低。通过强化学习微调生成模型可以绕过基于BERT的检测器，而重新表述文本则使得像DetectGPT这样的零样本检测器的规避率超过90%，尽管文本与原文高度相似。'}}}, {'id': 'https://huggingface.co/papers/2503.07274', 'title': 'Efficient Distillation of Classifier-Free Guidance using Adapters', 'url': 'https://huggingface.co/papers/2503.07274', 'abstract': 'While classifier-free guidance (CFG) is essential for conditional diffusion models, it doubles the number of neural function evaluations (NFEs) per inference step. To mitigate this inefficiency, we introduce adapter guidance distillation (AGD), a novel approach that simulates CFG in a single forward pass. AGD leverages lightweight adapters to approximate CFG, effectively doubling the sampling speed while maintaining or even improving sample quality. Unlike prior guidance distillation methods that tune the entire model, AGD keeps the base model frozen and only trains minimal additional parameters (sim2%) to significantly reduce the resource requirement of the distillation phase. Additionally, this approach preserves the original model weights and enables the adapters to be seamlessly combined with other checkpoints derived from the same base model. We also address a key mismatch between training and inference in existing guidance distillation methods by training on CFG-guided trajectories instead of standard diffusion trajectories. Through extensive experiments, we show that AGD achieves comparable or superior FID to CFG across multiple architectures with only half the NFEs. Notably, our method enables the distillation of large models (sim2.6B parameters) on a single consumer GPU with 24 GB of VRAM, making it more accessible than previous approaches that require multiple high-end GPUs. We will publicly release the implementation of our method.', 'score': 4, 'issue_id': 2639, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '84e5b0fd1b6f7f9f', 'authors': ['Cristian Perez Jensen', 'Seyedmorteza Sadat'], 'affiliations': ['ETH Zürich'], 'pdf_title_img': 'assets/pdf/title_img/2503.07274.jpg', 'data': {'categories': ['#inference', '#optimization', '#diffusion', '#training', '#open_source', '#architecture'], 'emoji': '🚀', 'ru': {'title': 'Ускорение диффузионных моделей с сохранением качества', 'desc': "Статья представляет новый метод под названием 'адаптерная дистилляция управления' (AGD) для условных диффузионных моделей. AGD симулирует управление без классификатора (CFG) за один проход сети, удваивая скорость семплирования без потери качества. Метод использует легковесные адаптеры, обучая всего 2% параметров и сохраняя базовую модель неизменной. AGD решает проблему несоответствия между обучением и выводом, тренируясь на траекториях с CFG."}, 'en': {'title': 'Speeding Up Diffusion Models with Adapter Guidance Distillation', 'desc': 'This paper presents adapter guidance distillation (AGD), a new method that enhances the efficiency of conditional diffusion models by simulating classifier-free guidance (CFG) in a single forward pass. AGD uses lightweight adapters to approximate CFG, which allows for faster sampling without sacrificing sample quality. The approach keeps the base model unchanged and only trains a small number of additional parameters, significantly reducing resource requirements. Experimental results demonstrate that AGD achieves similar or better performance compared to CFG while halving the number of neural function evaluations needed, making it feasible to distill large models on consumer-grade hardware.'}, 'zh': {'title': '适配器引导蒸馏：提升扩散模型效率的创新方法', 'desc': '本文提出了一种新的方法，称为适配器引导蒸馏（AGD），旨在提高条件扩散模型的效率。AGD通过轻量级适配器在单次前向传播中模拟无分类器引导（CFG），从而实现了采样速度的加倍，同时保持或改善样本质量。与以往的引导蒸馏方法不同，AGD只训练少量额外参数，而保持基础模型不变，显著降低了资源需求。通过大量实验，我们证明AGD在多个架构上实现了与CFG相当或更优的FID，同时只需一半的神经函数评估（NFE）。'}}}, {'id': 'https://huggingface.co/papers/2503.02199', 'title': 'Words or Vision: Do Vision-Language Models Have Blind Faith in Text?', 'url': 'https://huggingface.co/papers/2503.02199', 'abstract': "Vision-Language Models (VLMs) excel in integrating visual and textual information for vision-centric tasks, but their handling of inconsistencies between modalities is underexplored. We investigate VLMs' modality preferences when faced with visual data and varied textual inputs in vision-centered settings. By introducing textual variations to four vision-centric tasks and evaluating ten Vision-Language Models (VLMs), we discover a ``blind faith in text'' phenomenon: VLMs disproportionately trust textual data over visual data when inconsistencies arise, leading to significant performance drops under corrupted text and raising safety concerns. We analyze factors influencing this text bias, including instruction prompts, language model size, text relevance, token order, and the interplay between visual and textual certainty. While certain factors, such as scaling up the language model size, slightly mitigate text bias, others like token order can exacerbate it due to positional biases inherited from language models. To address this issue, we explore supervised fine-tuning with text augmentation and demonstrate its effectiveness in reducing text bias. Additionally, we provide a theoretical analysis suggesting that the blind faith in text phenomenon may stem from an imbalance of pure text and multi-modal data during training. Our findings highlight the need for balanced training and careful consideration of modality interactions in VLMs to enhance their robustness and reliability in handling multi-modal data inconsistencies.", 'score': 4, 'issue_id': 2630, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 марта', 'en': 'March 4', 'zh': '3月4日'}, 'hash': 'a354f8de058f0f84', 'authors': ['Ailin Deng', 'Tri Cao', 'Zhirui Chen', 'Bryan Hooi'], 'affiliations': ['National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2503.02199.jpg', 'data': {'categories': ['#interpretability', '#alignment', '#training', '#multimodal'], 'emoji': '🔍', 'ru': {'title': 'Опасность слепой веры в текст: проблема и решения для VLM моделей', 'desc': "Исследование показывает, что модели компьютерного зрения и обработки естественного языка (VLM) склонны чрезмерно доверять текстовым данным при несоответствии между визуальной и текстовой информацией. Это явление, названное 'слепой верой в текст', может привести к значительному снижению производительности моделей и вызывает опасения по поводу их безопасности. Авторы анализируют факторы, влияющие на текстовое смещение, и предлагают методы его уменьшения, включая дообучение с аугментацией текста. Теоретический анализ предполагает, что проблема может быть связана с дисбалансом чисто текстовых и мультимодальных данных во время обучения моделей."}, 'en': {'title': 'Balancing Vision and Text: Overcoming Bias in Vision-Language Models', 'desc': "This paper investigates how Vision-Language Models (VLMs) manage inconsistencies between visual and textual information in tasks that rely heavily on vision. The authors identify a phenomenon called 'blind faith in text,' where VLMs tend to rely more on textual data than visual data when faced with conflicting inputs, which can lead to performance issues. They analyze various factors that contribute to this text bias, such as the size of the language model and the order of tokens in the text. To mitigate this bias, the paper proposes supervised fine-tuning with text augmentation and emphasizes the importance of balanced training for improving the reliability of VLMs in multi-modal contexts."}, 'zh': {'title': '平衡训练，提升视觉语言模型的可靠性', 'desc': '视觉语言模型（VLMs）在处理视觉和文本信息方面表现出色，但它们在面对模态不一致时的表现尚未得到充分研究。我们探讨了VLMs在视觉数据和不同文本输入下的模态偏好，发现了“对文本的盲目信任”现象：当出现不一致时，VLMs过度依赖文本数据，导致性能显著下降。我们分析了影响这种文本偏见的因素，包括指令提示、语言模型大小、文本相关性、标记顺序以及视觉和文本确定性之间的相互作用。为了解决这个问题，我们探索了带有文本增强的监督微调，并证明其在减少文本偏见方面的有效性。'}}}, {'id': 'https://huggingface.co/papers/2503.07603', 'title': 'Should VLMs be Pre-trained with Image Data?', 'url': 'https://huggingface.co/papers/2503.07603', 'abstract': 'Pre-trained LLMs that are further trained with image data perform well on vision-language tasks. While adding images during a second training phase effectively unlocks this capability, it is unclear how much of a gain or loss this two-step pipeline gives over VLMs which integrate images earlier into the training process. To investigate this, we train models spanning various datasets, scales, image-text ratios, and amount of pre-training done before introducing vision tokens. We then fine-tune these models and evaluate their downstream performance on a suite of vision-language and text-only tasks. We find that pre-training with a mixture of image and text data allows models to perform better on vision-language tasks while maintaining strong performance on text-only evaluations. On an average of 6 diverse tasks, we find that for a 1B model, introducing visual tokens 80% of the way through pre-training results in a 2% average improvement over introducing visual tokens to a fully pre-trained model.', 'score': 3, 'issue_id': 2633, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '16f17eb6db67a418', 'authors': ['Sedrick Keh', 'Jean Mercat', 'Samir Yitzhak Gadre', 'Kushal Arora', 'Igor Vasiljevic', 'Benjamin Burchfiel', 'Shuran Song', 'Russ Tedrake', 'Thomas Kollar', 'Ludwig Schmidt', 'Achal Dave'], 'affiliations': ['Columbia University', 'MIT', 'Stanford', 'Toyota Research Institute'], 'pdf_title_img': 'assets/pdf/title_img/2503.07603.jpg', 'data': {'categories': ['#multimodal', '#training', '#benchmark', '#transfer_learning', '#optimization'], 'emoji': '🔬', 'ru': {'title': 'Оптимальное время для интеграции визуальных данных в языковые модели', 'desc': 'Исследование посвящено сравнению эффективности двухэтапного обучения языковых моделей (сначала на текстах, затем на изображениях) с моделями, изначально обучаемыми на смешанных данных. Авторы провели эксперименты с различными наборами данных, масштабами моделей и соотношениями изображений и текста. Результаты показывают, что обучение на смешанных данных позволяет моделям лучше справляться с задачами, связанными с обработкой изображений и текста, сохраняя при этом высокую производительность в текстовых задачах. Для модели размером 1 миллиард параметров введение визуальных токенов на 80% этапе предобучения дает в среднем 2% улучшение по сравнению с добавлением изображений к полностью предобученной модели.'}, 'en': {'title': 'Unlocking Vision-Language Synergy: Timing Matters!', 'desc': 'This paper explores the effectiveness of training large language models (LLMs) with image data in a two-step process compared to integrating images earlier in the training. The authors conduct experiments with various datasets and training configurations to assess the impact of when visual tokens are introduced. Their findings indicate that pre-training with both image and text data enhances performance on vision-language tasks while still performing well on text-only tasks. Specifically, they observe a 2% improvement in performance when visual tokens are added later in the pre-training phase for a 1B model.'}, 'zh': {'title': '图像与文本混合预训练提升视觉语言任务表现', 'desc': '这篇论文探讨了预训练的大型语言模型（LLM）在加入图像数据后在视觉语言任务中的表现。研究发现，在第二阶段训练中加入图像数据可以有效提升模型的能力，但不清楚这种两步训练流程与早期整合图像的视觉语言模型（VLM）相比，究竟是增益还是损失。通过训练不同数据集、规模和图像文本比例的模型，研究者发现混合图像和文本数据的预训练可以提高视觉语言任务的表现，同时在仅文本的评估中也保持良好表现。结果显示，对于一个10亿参数的模型，在预训练的80%时引入视觉标记，平均提升了2%的性能。'}}}, {'id': 'https://huggingface.co/papers/2503.07465', 'title': 'YOLOE: Real-Time Seeing Anything', 'url': 'https://huggingface.co/papers/2503.07465', 'abstract': "Object detection and segmentation are widely employed in computer vision applications, yet conventional models like YOLO series, while efficient and accurate, are limited by predefined categories, hindering adaptability in open scenarios. Recent open-set methods leverage text prompts, visual cues, or prompt-free paradigm to overcome this, but often compromise between performance and efficiency due to high computational demands or deployment complexity. In this work, we introduce YOLOE, which integrates detection and segmentation across diverse open prompt mechanisms within a single highly efficient model, achieving real-time seeing anything. For text prompts, we propose Re-parameterizable Region-Text Alignment (RepRTA) strategy. It refines pretrained textual embeddings via a re-parameterizable lightweight auxiliary network and enhances visual-textual alignment with zero inference and transferring overhead. For visual prompts, we present Semantic-Activated Visual Prompt Encoder (SAVPE). It employs decoupled semantic and activation branches to bring improved visual embedding and accuracy with minimal complexity. For prompt-free scenario, we introduce Lazy Region-Prompt Contrast (LRPC) strategy. It utilizes a built-in large vocabulary and specialized embedding to identify all objects, avoiding costly language model dependency. Extensive experiments show YOLOE's exceptional zero-shot performance and transferability with high inference efficiency and low training cost. Notably, on LVIS, with 3times less training cost and 1.4times inference speedup, YOLOE-v8-S surpasses YOLO-Worldv2-S by 3.5 AP. When transferring to COCO, YOLOE-v8-L achieves 0.6 AP^b and 0.4 AP^m gains over closed-set YOLOv8-L with nearly 4times less training time. Code and models are available at https://github.com/THU-MIG/yoloe.", 'score': 3, 'issue_id': 2637, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': 'faab5c7007c9cc4d', 'authors': ['Ao Wang', 'Lihao Liu', 'Hui Chen', 'Zijia Lin', 'Jungong Han', 'Guiguang Ding'], 'affiliations': ['Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.07465.jpg', 'data': {'categories': ['#training', '#benchmark', '#transfer_learning', '#optimization', '#cv'], 'emoji': '🔍', 'ru': {'title': 'YOLOE: Эффективное обнаружение и сегментация чего угодно в реальном времени', 'desc': 'YOLOE - это новая модель для обнаружения и сегментации объектов, которая объединяет различные механизмы открытых подсказок в одной эффективной архитектуре. Она использует стратегию RepRTA для текстовых подсказок, энкодер SAVPE для визуальных подсказок и метод LRPC для сценариев без подсказок. YOLOE демонстрирует исключительную производительность в задачах zero-shot и переносимость, превосходя существующие модели по эффективности и точности при меньших затратах на обучение.'}, 'en': {'title': 'YOLOE: Real-Time Object Detection and Segmentation for Open-Set Scenarios', 'desc': 'This paper presents YOLOE, a novel model that enhances object detection and segmentation in open-set scenarios by integrating various prompt mechanisms. It introduces a Re-parameterizable Region-Text Alignment (RepRTA) strategy for text prompts, which refines textual embeddings efficiently without additional inference costs. For visual prompts, the Semantic-Activated Visual Prompt Encoder (SAVPE) improves visual accuracy while maintaining low complexity. Additionally, the Lazy Region-Prompt Contrast (LRPC) strategy allows for prompt-free object identification, significantly reducing training costs and improving inference speed compared to traditional models.'}, 'zh': {'title': 'YOLOE：高效的开放场景目标检测与分割', 'desc': '本文介绍了一种新的目标检测和分割模型YOLOE，旨在克服传统模型在开放场景中的局限性。YOLOE通过集成多种开放提示机制，实现了高效的实时检测和分割。我们提出了可重参数化区域-文本对齐策略（RepRTA）和语义激活视觉提示编码器（SAVPE），以提高视觉和文本的对齐效果。实验结果表明，YOLOE在零样本性能和迁移能力上表现优异，同时训练成本低，推理效率高。'}}}, {'id': 'https://huggingface.co/papers/2503.07265', 'title': 'WISE: A World Knowledge-Informed Semantic Evaluation for Text-to-Image\n  Generation', 'url': 'https://huggingface.co/papers/2503.07265', 'abstract': 'Text-to-Image (T2I) models are capable of generating high-quality artistic creations and visual content. However, existing research and evaluation standards predominantly focus on image realism and shallow text-image alignment, lacking a comprehensive assessment of complex semantic understanding and world knowledge integration in text to image generation. To address this challenge, we propose WISE, the first benchmark specifically designed for World Knowledge-Informed Semantic Evaluation. WISE moves beyond simple word-pixel mapping by challenging models with 1000 meticulously crafted prompts across 25 sub-domains in cultural common sense, spatio-temporal reasoning, and natural science. To overcome the limitations of traditional CLIP metric, we introduce WiScore, a novel quantitative metric for assessing knowledge-image alignment. Through comprehensive testing of 20 models (10 dedicated T2I models and 10 unified multimodal models) using 1,000 structured prompts spanning 25 subdomains, our findings reveal significant limitations in their ability to effectively integrate and apply world knowledge during image generation, highlighting critical pathways for enhancing knowledge incorporation and application in next-generation T2I models. Code and data are available at https://github.com/PKU-YuanGroup/WISE.', 'score': 3, 'issue_id': 2640, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '6174088b65d232ba', 'authors': ['Yuwei Niu', 'Munan Ning', 'Mengren Zheng', 'Bin Lin', 'Peng Jin', 'Jiaqi Liao', 'Kunpeng Ning', 'Bin Zhu', 'Li Yuan'], 'affiliations': ['Chongqing University', 'Peking University', 'PengCheng Laboratory', 'Rabbitpre AI'], 'pdf_title_img': 'assets/pdf/title_img/2503.07265.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#multimodal', '#interpretability', '#cv'], 'emoji': '🧠', 'ru': {'title': 'WISE: новый рубеж в оценке семантического понимания моделей text-to-image', 'desc': 'Исследователи предложили новый бенчмарк WISE для оценки интеграции мировых знаний в модели генерации изображений по тексту. WISE включает 1000 тщательно разработанных промптов в 25 поддоменах, охватывающих культурный здравый смысл, пространственно-временное мышление и естественные науки. Авторы также представили новую метрику WiScore для количественной оценки соответствия знаний и изображений. Тестирование 20 моделей выявило значительные ограничения в их способности эффективно интегрировать и применять мировые знания при генерации изображений.'}, 'en': {'title': 'Enhancing T2I Models with World Knowledge Evaluation', 'desc': 'This paper introduces WISE, a new benchmark for evaluating Text-to-Image (T2I) models that focuses on their ability to integrate world knowledge and complex semantic understanding. Unlike previous assessments that primarily measure image realism and basic text-image alignment, WISE challenges models with 1,000 detailed prompts across various domains, including cultural common sense and natural science. The authors also present WiScore, a new metric designed to quantitatively assess how well models align knowledge with generated images. Testing reveals that many current T2I models struggle to effectively incorporate world knowledge, indicating areas for improvement in future model development.'}, 'zh': {'title': '提升文本到图像生成的知识整合能力', 'desc': '本文提出了WISE，这是第一个专门为世界知识驱动的语义评估设计的基准。现有的文本到图像生成模型主要关注图像的真实感和简单的文本-图像对齐，缺乏对复杂语义理解和世界知识整合的全面评估。WISE通过1000个精心设计的提示，涵盖文化常识、时空推理和自然科学等25个子领域，挑战模型的能力。我们还引入了WiScore这一新颖的定量指标，以评估知识与图像的对齐程度，测试结果显示现有模型在有效整合和应用世界知识方面存在显著局限。'}}}, {'id': 'https://huggingface.co/papers/2503.06885', 'title': 'ProBench: Judging Multimodal Foundation Models on Open-ended\n  Multi-domain Expert Tasks', 'url': 'https://huggingface.co/papers/2503.06885', 'abstract': 'Solving expert-level multimodal tasks is a key milestone towards general intelligence. As the capabilities of multimodal large language models (MLLMs) continue to improve, evaluation of such advanced multimodal intelligence becomes necessary yet challenging. In this work, we introduce ProBench, a benchmark of open-ended user queries that require professional expertise and advanced reasoning. ProBench consists of 4,000 high-quality samples independently submitted by professionals based on their daily productivity demands. It spans across 10 fields and 56 sub-fields, including science, arts, humanities, coding, mathematics, and creative writing. Experimentally, we evaluate and compare 24 latest models using MLLM-as-a-Judge. Our results reveal that although the best open-source models rival the proprietary ones, ProBench presents significant challenges in visual perception, textual understanding, domain knowledge and advanced reasoning, thus providing valuable directions for future multimodal AI research efforts.', 'score': 3, 'issue_id': 2633, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '196c83578a251f8e', 'authors': ['Yan Yang', 'Dongxu Li', 'Haoning Wu', 'Bei Chen', 'Liu Liu', 'Liyuan Pan', 'Junnan Li'], 'affiliations': ['ANU', 'BITSZ & School of CSAT, BIT', 'KooMap, Huawei', 'NTU', 'Salesforce AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2503.06885.jpg', 'data': {'categories': ['#open_source', '#multimodal', '#agi', '#benchmark', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'ProBench: испытание интеллекта мультимодальных ИИ-моделей', 'desc': 'ProBench - это новый бенчмарк для оценки продвинутых мультимодальных языковых моделей (MLLM). Он состоит из 4000 высококачественных задач, охватывающих 10 областей и 56 подобластей, включая науку, искусство, гуманитарные науки и программирование. Задачи были предложены профессионалами на основе их повседневных рабочих потребностей. Результаты тестирования 24 новейших моделей с использованием метода MLLM-as-a-Judge показали, что ProBench представляет значительные трудности в визуальном восприятии, понимании текста, предметных знаниях и продвинутых рассуждениях.'}, 'en': {'title': 'ProBench: Benchmarking Multimodal Intelligence for Expert Tasks', 'desc': 'This paper presents ProBench, a new benchmark designed to evaluate multimodal large language models (MLLMs) on expert-level tasks. ProBench includes 4,000 user queries that require advanced reasoning and professional expertise across various fields such as science, arts, and coding. The study compares 24 state-of-the-art models using MLLM-as-a-Judge, highlighting the challenges these models face in visual perception, textual understanding, and domain knowledge. The findings indicate that while some open-source models perform comparably to proprietary ones, ProBench identifies critical areas for improvement in multimodal AI capabilities.'}, 'zh': {'title': '多模态智能评估的新基准：ProBench', 'desc': '本论文介绍了ProBench，这是一个针对多模态大语言模型（MLLM）的基准测试，旨在评估其在专业领域的智能表现。ProBench包含4000个高质量样本，涵盖科学、艺术、人文学科、编程、数学和创意写作等10个领域和56个子领域。通过对24个最新模型的实验评估，结果显示尽管一些开源模型在性能上与专有模型相当，但在视觉感知、文本理解、领域知识和高级推理方面，ProBench仍然提出了显著的挑战。该研究为未来多模态人工智能的研究方向提供了重要的参考。'}}}, {'id': 'https://huggingface.co/papers/2503.06626', 'title': 'DiffCLIP: Differential Attention Meets CLIP', 'url': 'https://huggingface.co/papers/2503.06626', 'abstract': "We propose DiffCLIP, a novel vision-language model that extends the differential attention mechanism to CLIP architectures. Differential attention was originally developed for large language models to amplify relevant context while canceling out noisy information. In this work, we integrate this mechanism into CLIP's dual encoder (image and text) framework. With minimal additional parameters, DiffCLIP achieves superior performance on image-text understanding tasks. Across zero-shot classification, retrieval, and robustness benchmarks, DiffCLIP consistently outperforms baseline CLIP models. Notably, these gains come with negligible computational overhead, demonstrating that differential attention can significantly enhance multi-modal representations without sacrificing efficiency. Code can be found at https://github.com/hammoudhasan/DiffCLIP.", 'score': 3, 'issue_id': 2641, 'pub_date': '2025-03-09', 'pub_date_card': {'ru': '9 марта', 'en': 'March 9', 'zh': '3月9日'}, 'hash': 'fff7aaf70b1d6ed0', 'authors': ['Hasan Abed Al Kader Hammoud', 'Bernard Ghanem'], 'affiliations': ['KAUST'], 'pdf_title_img': 'assets/pdf/title_img/2503.06626.jpg', 'data': {'categories': ['#architecture', '#diffusion', '#benchmark', '#multimodal', '#cv'], 'emoji': '🔍', 'ru': {'title': 'DiffCLIP: улучшение мультимодальных представлений с помощью дифференциального внимания', 'desc': 'Исследователи представили DiffCLIP - новую модель для обработки изображений и текста, которая расширяет механизм дифференциального внимания на архитектуру CLIP. Дифференциальное внимание позволяет усиливать релевантный контекст и подавлять шумовую информацию. DiffCLIP интегрирует этот механизм в двойной энкодер CLIP для изображений и текста. Модель достигает превосходных результатов в задачах понимания изображений и текста, превосходя базовые модели CLIP при минимальных дополнительных параметрах.'}, 'en': {'title': 'Enhancing CLIP with Differential Attention for Better Image-Text Understanding', 'desc': "DiffCLIP is a new vision-language model that improves the CLIP architecture by using a technique called differential attention. This method helps the model focus on important information while ignoring irrelevant details, which was initially designed for large language models. By incorporating differential attention into CLIP's dual encoder system, DiffCLIP enhances its ability to understand images and text together. The model shows better performance in various tasks like zero-shot classification and retrieval, all while maintaining low computational costs."}, 'zh': {'title': 'DiffCLIP：高效增强多模态表示的创新模型', 'desc': '我们提出了DiffCLIP，这是一种新颖的视觉-语言模型，它将差分注意力机制扩展到CLIP架构中。差分注意力最初是为大型语言模型开发的，旨在放大相关上下文，同时消除噪声信息。在这项工作中，我们将这一机制集成到CLIP的双编码器（图像和文本）框架中。DiffCLIP在图像-文本理解任务上表现优越，且几乎没有额外的计算开销，证明了差分注意力可以显著增强多模态表示而不牺牲效率。'}}}, {'id': 'https://huggingface.co/papers/2503.06273', 'title': 'Zero-AVSR: Zero-Shot Audio-Visual Speech Recognition with LLMs by\n  Learning Language-Agnostic Speech Representations', 'url': 'https://huggingface.co/papers/2503.06273', 'abstract': 'We explore a novel zero-shot Audio-Visual Speech Recognition (AVSR) framework, dubbed Zero-AVSR, which enables speech recognition in target languages without requiring any audio-visual speech data in those languages. Specifically, we introduce the Audio-Visual Speech Romanizer (AV-Romanizer), which learns language-agnostic speech representations by predicting Roman text. Then, by leveraging the strong multilingual modeling capabilities of Large Language Models (LLMs), we propose converting the predicted Roman text into language-specific graphemes, forming the proposed Cascaded <PRE_TAG>Zero-AVSR</POST_TAG>. Taking it a step further, we explore a unified Zero-AVSR approach by directly integrating the audio-visual speech representations encoded by the AV-Romanizer into the LLM. This is achieved through finetuning the adapter and the LLM using our proposed multi-task learning scheme. To capture the wide spectrum of phonetic and linguistic diversity, we also introduce a Multilingual Audio-Visual Romanized Corpus (MARC) consisting of 2,916 hours of audio-visual speech data across 82 languages, along with transcriptions in both language-specific graphemes and Roman text. Extensive analysis and experiments confirm that the proposed Zero-AVSR framework has the potential to expand language support beyond the languages seen during the training of the AV-Romanizer.', 'score': 3, 'issue_id': 2640, 'pub_date': '2025-03-08', 'pub_date_card': {'ru': '8 марта', 'en': 'March 8', 'zh': '3月8日'}, 'hash': '66b650ba2f5b0404', 'authors': ['Jeong Hun Yeo', 'Minsu Kim', 'Chae Won Kim', 'Stavros Petridis', 'Yong Man Ro'], 'affiliations': ['Imperial College London', 'KAIST'], 'pdf_title_img': 'assets/pdf/title_img/2503.06273.jpg', 'data': {'categories': ['#low_resource', '#audio', '#dataset', '#multilingual', '#machine_translation'], 'emoji': '🗣️', 'ru': {'title': 'Универсальное распознавание речи без языковых барьеров', 'desc': 'Статья представляет новую систему Zero-AVSR для аудиовизуального распознавания речи без использования речевых данных целевого языка. Авторы предлагают Audio-Visual Speech Romanizer для создания языконезависимых представлений речи и используют возможности больших языковых моделей для преобразования романизированного текста в графемы конкретного языка. Исследователи также разрабатывают унифицированный подход Zero-AVSR, интегрируя аудиовизуальные представления речи непосредственно в большую языковую модель. Для обучения и оценки создан многоязычный аудиовизуальный корпус MARC, содержащий 2916 часов речевых данных на 82 языках.'}, 'en': {'title': 'Zero-AVSR: Speech Recognition Without Language-Specific Data', 'desc': 'The paper presents a new framework called Zero-AVSR for Audio-Visual Speech Recognition that can recognize speech in languages without needing any specific audio-visual data for those languages. It introduces the Audio-Visual Speech Romanizer (AV-Romanizer), which creates language-independent speech representations by predicting Roman text. By utilizing Large Language Models (LLMs), the framework converts these predictions into specific language graphemes, enhancing multilingual capabilities. Additionally, a new dataset, the Multilingual Audio-Visual Romanized Corpus (MARC), is introduced to support the training and evaluation of this framework across 82 languages.'}, 'zh': {'title': '零样本音视频语音识别的创新探索', 'desc': '我们提出了一种新颖的零样本音视频语音识别框架，称为Zero-AVSR，能够在没有目标语言音视频语音数据的情况下进行语音识别。该框架引入了音视频语音罗马化器（AV-Romanizer），通过预测罗马文本来学习与语言无关的语音表示。我们利用大型语言模型（LLMs）的强大多语言建模能力，将预测的罗马文本转换为特定语言的字形，形成级联的Zero-AVSR。通过多任务学习方案微调适配器和LLM，我们进一步探索了统一的Zero-AVSR方法，直接将AV-Romanizer编码的音视频语音表示集成到LLM中。'}}}, {'id': 'https://huggingface.co/papers/2503.07598', 'title': 'VACE: All-in-One Video Creation and Editing', 'url': 'https://huggingface.co/papers/2503.07598', 'abstract': 'Diffusion Transformer has demonstrated powerful capability and scalability in generating high-quality images and videos. Further pursuing the unification of generation and editing tasks has yielded significant progress in the domain of image content creation. However, due to the intrinsic demands for consistency across both temporal and spatial dynamics, achieving a unified approach for video synthesis remains challenging. We introduce VACE, which enables users to perform Video tasks within an All-in-one framework for Creation and Editing. These tasks include reference-to-video generation, video-to-video editing, and masked <PRE_TAG>video-to-video editing</POST_TAG>. Specifically, we effectively integrate the requirements of various tasks by organizing video task inputs, such as editing, reference, and masking, into a unified interface referred to as the Video Condition Unit (VCU). Furthermore, by utilizing a Context Adapter structure, we inject different task concepts into the model using formalized representations of temporal and spatial dimensions, allowing it to handle arbitrary video synthesis tasks flexibly. Extensive experiments demonstrate that the unified model of VACE achieves performance on par with task-specific models across various subtasks. Simultaneously, it enables diverse applications through versatile task combinations. Project page: https://ali-vilab.github.io/VACE-Page/.', 'score': 2, 'issue_id': 2644, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '6bf5924fe3265297', 'authors': ['Zeyinzi Jiang', 'Zhen Han', 'Chaojie Mao', 'Jingfeng Zhang', 'Yulin Pan', 'Yu Liu'], 'affiliations': ['Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2503.07598.jpg', 'data': {'categories': ['#architecture', '#video', '#multimodal', '#diffusion'], 'emoji': '🎬', 'ru': {'title': 'VACE: Универсальный инструмент для создания и редактирования видео', 'desc': 'VACE - это универсальная модель для создания и редактирования видео, объединяющая различные задачи в рамках единого фреймворка. Она использует специальный интерфейс Video Condition Unit для организации входных данных и Context Adapter для внедрения концепций различных задач. VACE позволяет выполнять генерацию видео по референсу, редактирование видео и маскированное редактирование видео. Эксперименты показывают, что модель достигает производительности на уровне специализированных моделей для отдельных подзадач.'}, 'en': {'title': 'VACE: Unifying Video Creation and Editing in One Framework', 'desc': 'The paper introduces VACE, a unified framework for video generation and editing that leverages the capabilities of the Diffusion Transformer. It addresses the challenges of maintaining consistency in both temporal and spatial dynamics during video synthesis. VACE organizes various video tasks, such as reference-to-video generation and video-to-video editing, into a single interface called the Video Condition Unit (VCU). By employing a Context Adapter structure, VACE allows for flexible handling of different video synthesis tasks while achieving performance comparable to specialized models.'}, 'zh': {'title': 'VACE：视频创作与编辑的统一框架', 'desc': '扩散变换器在生成高质量图像和视频方面表现出强大的能力和可扩展性。为了统一生成和编辑任务，研究者们在图像内容创作领域取得了显著进展。尽管如此，由于时间和空间动态的一致性要求，视频合成的统一方法仍然面临挑战。我们提出了VACE，它允许用户在一个综合框架内执行视频任务，包括参考视频生成、视频编辑和掩码视频编辑等。'}}}, {'id': 'https://huggingface.co/papers/2503.07389', 'title': 'TRCE: Towards Reliable Malicious Concept Erasure in Text-to-Image\n  Diffusion Models', 'url': 'https://huggingface.co/papers/2503.07389', 'abstract': "Recent advances in text-to-image diffusion models enable photorealistic image generation, but they also risk producing malicious content, such as NSFW images. To mitigate risk, concept erasure methods are studied to facilitate the model to unlearn specific concepts. However, current studies struggle to fully erase malicious concepts implicitly embedded in prompts (e.g., metaphorical expressions or adversarial prompts) while preserving the model's normal generation capability. To address this challenge, our study proposes TRCE, using a two-stage concept erasure strategy to achieve an effective trade-off between reliable erasure and knowledge preservation. Firstly, TRCE starts by erasing the malicious semantics implicitly embedded in textual prompts. By identifying a critical mapping objective(i.e., the [EoT] embedding), we optimize the cross-attention layers to map malicious prompts to contextually similar prompts but with safe concepts. This step prevents the model from being overly influenced by malicious semantics during the denoising process. Following this, considering the deterministic properties of the sampling trajectory of the diffusion model, TRCE further steers the early denoising prediction toward the safe direction and away from the unsafe one through contrastive learning, thus further avoiding the generation of malicious content. Finally, we conduct comprehensive evaluations of TRCE on multiple malicious concept erasure benchmarks, and the results demonstrate its effectiveness in erasing malicious concepts while better preserving the model's original generation ability. The code is available at: http://github.com/ddgoodgood/TRCE. CAUTION: This paper includes model-generated content that may contain offensive material.", 'score': 2, 'issue_id': 2642, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': 'd88e626045dd077e', 'authors': ['Ruidong Chen', 'Honglin Guo', 'Lanjun Wang', 'Chenyu Zhang', 'Weizhi Nie', 'An-An Liu'], 'affiliations': ['The School of Electrical and Information Engineering, Tianjin University', 'The School of New Media and Communication, Tianjin University'], 'pdf_title_img': 'assets/pdf/title_img/2503.07389.jpg', 'data': {'categories': ['#benchmark', '#security', '#open_source', '#training', '#cv', '#diffusion', '#hallucinations'], 'emoji': '🔒', 'ru': {'title': 'Безопасная генерация изображений: эффективное удаление нежелательных концепций', 'desc': 'Эта статья представляет новый метод под названием TRCE для удаления нежелательных концепций из моделей генерации изображений по тексту. TRCE использует двухэтапную стратегию: сначала удаляет вредоносную семантику из текстовых промптов, а затем корректирует процесс генерации изображения в безопасном направлении. Метод эффективно удаляет нежелательные концепции, сохраняя при этом общие возможности модели. Авторы провели комплексную оценку TRCE на различных наборах данных, подтверждающую его эффективность.'}, 'en': {'title': 'TRCE: Safeguarding Image Generation with Smart Concept Erasure', 'desc': "This paper presents TRCE, a novel approach to mitigate the risk of generating malicious content in text-to-image diffusion models. It employs a two-stage concept erasure strategy that effectively removes harmful semantics from prompts while maintaining the model's ability to generate normal content. The first stage focuses on optimizing cross-attention layers to transform malicious prompts into safer alternatives. The second stage utilizes contrastive learning to guide the denoising process towards safe outputs, ensuring that the model does not produce NSFW images while preserving its generative capabilities."}, 'zh': {'title': '安全生成，抹除恶意概念的创新方法', 'desc': '最近，文本到图像的扩散模型在生成逼真图像方面取得了进展，但也存在生成恶意内容的风险。为了解决这个问题，研究了概念抹除方法，以帮助模型忘记特定的恶意概念。我们的研究提出了TRCE，采用两阶段的概念抹除策略，在可靠抹除和知识保留之间实现有效的平衡。通过优化交叉注意力层，TRCE能够将恶意提示映射到安全的概念，从而避免生成恶意内容。'}}}, {'id': 'https://huggingface.co/papers/2503.06960', 'title': 'A Data-Centric Revisit of Pre-Trained Vision Models for Robot Learning', 'url': 'https://huggingface.co/papers/2503.06960', 'abstract': 'Pre-trained vision models (PVMs) are fundamental to modern robotics, yet their optimal configuration remains unclear. Through systematic evaluation, we find that while DINO and iBOT outperform MAE across visuomotor control and perception tasks, they struggle when trained on non-(single-)object-centric (NOC) data--a limitation strongly correlated with their diminished ability to learn object-centric representations. This investigation indicates that the ability to form object-centric representations from the non-object-centric robotics dataset is the key to success for PVMs. Motivated by this discovery, we designed SlotMIM, a method that induces object-centric representations by introducing a semantic bottleneck to reduce the number of prototypes to encourage the emergence of objectness as well as cross-view consistency regularization for encouraging multiview invariance. Our experiments encompass pre-training on object-centric, scene-centric, web-crawled, and ego-centric data. Across all settings, our approach learns transferrable representations and achieves significant improvements over prior work in image recognition, scene understanding, and robot learning evaluations. When scaled up with million-scale datasets, our method also demonstrates superior data efficiency and scalability. Our code and models are publicly available at https://github.com/CVMI-Lab/SlotMIM.', 'score': 2, 'issue_id': 2642, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '35668e47701edcd0', 'authors': ['Xin Wen', 'Bingchen Zhao', 'Yilun Chen', 'Jiangmiao Pang', 'Xiaojuan Qi'], 'affiliations': ['Shanghai AI Laboratory', 'The University of Hong Kong', 'University of Edinburgh'], 'pdf_title_img': 'assets/pdf/title_img/2503.06960.jpg', 'data': {'categories': ['#robotics', '#open_source', '#training', '#cv', '#dataset', '#transfer_learning'], 'emoji': '🤖', 'ru': {'title': 'SlotMIM: прорыв в обучении объектно-ориентированным представлениям для робототехники', 'desc': 'Исследование показывает, что предварительно обученные модели компьютерного зрения (PVM) сталкиваются с трудностями при обучении на данных, не сфокусированных на отдельных объектах. Авторы выявили, что ключом к успеху PVM является способность формировать объектно-ориентированные представления из роботизированных наборов данных. На основе этого открытия они разработали метод SlotMIM, который вводит семантическое сужение для уменьшения количества прототипов и поощрения появления объектности. Эксперименты показали, что SlotMIM достигает значительных улучшений в задачах распознавания изображений, понимания сцен и обучения роботов.'}, 'en': {'title': 'Unlocking Object-Centric Learning in Robotics with SlotMIM', 'desc': 'This paper explores the effectiveness of pre-trained vision models (PVMs) in robotics, particularly focusing on their ability to learn object-centric representations. The authors find that models like DINO and iBOT perform better than MAE in various tasks but struggle with non-object-centric data. They introduce SlotMIM, a new method that enhances object-centric learning by using a semantic bottleneck and cross-view consistency regularization. Their experiments show that SlotMIM significantly improves representation learning across different datasets and tasks, demonstrating better data efficiency and scalability.'}, 'zh': {'title': '对象中心表示是成功的关键', 'desc': '预训练视觉模型（PVMs）在现代机器人技术中至关重要，但其最佳配置尚不明确。研究发现，DINO和iBOT在视觉运动控制和感知任务中表现优于MAE，但在非单对象中心（NOC）数据上训练时表现不佳，这与它们学习对象中心表示的能力下降密切相关。我们的研究表明，从非对象中心的机器人数据集中形成对象中心表示的能力是PVMs成功的关键。为此，我们设计了SlotMIM方法，通过引入语义瓶颈来减少原型数量，促进对象性出现，并通过交叉视图一致性正则化来鼓励多视图不变性。'}}}, {'id': 'https://huggingface.co/papers/2503.06362', 'title': 'Adaptive Audio-Visual Speech Recognition via Matryoshka-Based Multimodal\n  LLMs', 'url': 'https://huggingface.co/papers/2503.06362', 'abstract': 'Audio-Visual Speech Recognition (AVSR) leverages both audio and visual modalities to enhance speech recognition robustness, particularly in noisy environments. Recent advancements in Large Language Models (LLMs) have demonstrated their effectiveness in speech recognition, including AVSR. However, due to the significant length of speech representations, direct integration with LLMs imposes substantial computational costs. Prior approaches address this by compressing speech representations before feeding them into LLMs. However, higher compression ratios often lead to performance degradation, necessitating a trade-off between computational efficiency and recognition accuracy. To address this challenge, we propose Llama-MTSK, the first Matryoshka-based Multimodal LLM for AVSR, which enables flexible adaptation of the audio-visual token allocation based on specific computational constraints while preserving high performance. Our approach, inspired by Matryoshka Representation Learning, encodes audio-visual representations at multiple granularities within a single model, eliminating the need to train separate models for different compression levels. Moreover, to efficiently fine-tune the LLM, we introduce three LoRA-based Matryoshka strategies using global and scale-specific LoRA modules. Extensive evaluations on the two largest AVSR datasets demonstrate that Llama-MTSK achieves state-of-the-art results, matching or surpassing models trained independently at fixed compression levels.', 'score': 2, 'issue_id': 2638, 'pub_date': '2025-03-09', 'pub_date_card': {'ru': '9 марта', 'en': 'March 9', 'zh': '3月9日'}, 'hash': 'e35aea6b25fbc264', 'authors': ['Umberto Cappellazzo', 'Minsu Kim', 'Stavros Petridis'], 'affiliations': ['Imperial College London', 'Meta AI'], 'pdf_title_img': 'assets/pdf/title_img/2503.06362.jpg', 'data': {'categories': ['#training', '#multimodal', '#optimization', '#audio'], 'emoji': '🎭', 'ru': {'title': 'Гибкое AVSR с помощью матрешечной мультимодальной языковой модели', 'desc': 'Эта статья представляет Llama-MTSK - первую мультимодальную языковую модель на основе принципа матрешки для аудио-визуального распознавания речи (AVSR). Модель позволяет гибко адаптировать распределение аудио-визуальных токенов в зависимости от вычислительных ограничений, сохраняя при этом высокую производительность. Подход вдохновлен Matryoshka Representation Learning и кодирует аудио-визуальные представления на нескольких уровнях детализации в рамках одной модели. Авторы также предлагают три LoRA-стратегии для эффективной настройки модели.'}, 'en': {'title': 'Llama-MTSK: Efficient AVSR with Flexible Token Allocation', 'desc': 'This paper introduces Llama-MTSK, a novel Matryoshka-based Multimodal Large Language Model (LLM) designed for Audio-Visual Speech Recognition (AVSR). It addresses the challenge of integrating lengthy speech representations with LLMs by allowing flexible audio-visual token allocation based on computational constraints. The model encodes audio-visual data at various granularities, which helps maintain high performance without the need for separate models for different compression levels. Additionally, the paper presents three LoRA-based strategies for fine-tuning the model, achieving state-of-the-art results on major AVSR datasets.'}, 'zh': {'title': '灵活高效的音视频语音识别解决方案', 'desc': '音视频语音识别（AVSR）结合了音频和视觉信息，以提高在嘈杂环境中的语音识别能力。最近，大型语言模型（LLMs）的进展显示了它们在语音识别中的有效性，包括AVSR。然而，由于语音表示的长度较大，直接与LLMs集成会带来巨大的计算成本。为了解决这一问题，我们提出了Llama-MTSK，这是一种基于套娃的多模态LLM，能够根据特定的计算约束灵活调整音视频标记的分配，同时保持高性能。'}}}, {'id': 'https://huggingface.co/papers/2503.05578', 'title': 'Novel Object 6D Pose Estimation with a Single Reference View', 'url': 'https://huggingface.co/papers/2503.05578', 'abstract': 'Existing novel object 6D pose estimation methods typically rely on CAD models or dense reference views, which are both difficult to acquire. Using only a single reference view is more scalable, but challenging due to large pose discrepancies and limited geometric and spatial information. To address these issues, we propose a Single-Reference-based novel object 6D (SinRef-6D) pose estimation method. Our key idea is to iteratively establish point-wise alignment in the camera coordinate system based on state space models (SSMs). Specifically, iterative camera-space point-wise alignment can effectively handle large pose discrepancies, while our proposed RGB and Points SSMs can capture long-range dependencies and spatial information from a single view, offering linear complexity and superior spatial modeling capability. Once pre-trained on synthetic data, SinRef-6D can estimate the 6D pose of a novel object using only a single reference view, without requiring retraining or a CAD model. Extensive experiments on six popular datasets and real-world robotic scenes demonstrate that we achieve on-par performance with CAD-based and dense reference view-based methods, despite operating in the more challenging single reference setting. Code will be released at https://github.com/CNJianLiu/SinRef-6D.', 'score': 2, 'issue_id': 2640, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 марта', 'en': 'March 7', 'zh': '3月7日'}, 'hash': '689a25e37e909986', 'authors': ['Jian Liu', 'Wei Sun', 'Kai Zeng', 'Jin Zheng', 'Hui Yang', 'Lin Wang', 'Hossein Rahmani', 'Ajmal Mian'], 'affiliations': ['Central South University', 'Hunan University', 'Lancaster University', 'Nanyang Technological University', 'The University of Western Australia'], 'pdf_title_img': 'assets/pdf/title_img/2503.05578.jpg', 'data': {'categories': ['#3d', '#cv', '#robotics'], 'emoji': '🤖', 'ru': {'title': 'Точная оценка 6D-позы объекта по одному эталонному изображению', 'desc': 'Статья представляет метод SinRef-6D для оценки 6D-позы новых объектов на основе одного эталонного изображения. Метод использует итеративное выравнивание точек в системе координат камеры и модели пространства состояний для RGB-данных и облака точек. SinRef-6D способен обрабатывать большие расхождения в позе и извлекать пространственную информацию из одного изображения. После предварительного обучения на синтетических данных, метод может оценивать позу новых объектов без переобучения или CAD-моделей.'}, 'en': {'title': 'Single View, Full Pose: Simplifying 6D Object Estimation', 'desc': 'This paper introduces a new method for estimating the 6D pose of novel objects using only a single reference view, which is more scalable than traditional methods that rely on CAD models or multiple views. The proposed SinRef-6D method utilizes iterative point-wise alignment in the camera coordinate system, effectively addressing challenges posed by large pose discrepancies. By employing state space models (SSMs), the method captures long-range dependencies and spatial information, achieving linear complexity and enhanced spatial modeling. Experimental results show that SinRef-6D performs comparably to existing methods while simplifying the pose estimation process.'}, 'zh': {'title': '单视图下的六维姿态估计新方法', 'desc': '本文提出了一种新的单参考视图的六维姿态估计方法，称为SinRef-6D。该方法通过在相机坐标系中迭代建立点对齐，解决了大姿态差异和空间信息不足的问题。SinRef-6D利用状态空间模型（SSMs）来捕捉长距离依赖关系，并具有线性复杂度和优越的空间建模能力。经过在合成数据上的预训练后，SinRef-6D能够仅使用单一参考视图估计新物体的六维姿态，且无需重新训练或CAD模型。'}}}, {'id': 'https://huggingface.co/papers/2503.07597', 'title': 'HumanMM: Global Human Motion Recovery from Multi-shot Videos', 'url': 'https://huggingface.co/papers/2503.07597', 'abstract': 'In this paper, we present a novel framework designed to reconstruct long-sequence 3D human motion in the world coordinates from in-the-wild videos with multiple shot transitions. Such long-sequence in-the-wild motions are highly valuable to applications such as motion generation and motion understanding, but are of great challenge to be recovered due to abrupt shot transitions, partial occlusions, and dynamic backgrounds presented in such videos. Existing methods primarily focus on single-shot videos, where continuity is maintained within a single camera view, or simplify multi-shot alignment in camera space only. In this work, we tackle the challenges by integrating an enhanced camera pose estimation with Human Motion Recovery (HMR) by incorporating a shot transition detector and a robust alignment module for accurate pose and orientation continuity across shots. By leveraging a custom motion integrator, we effectively mitigate the problem of foot sliding and ensure temporal consistency in human pose. Extensive evaluations on our created multi-shot dataset from public 3D human datasets demonstrate the robustness of our method in reconstructing realistic human motion in world coordinates.', 'score': 1, 'issue_id': 2640, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '161213a2a054dd4d', 'authors': ['Yuhong Zhang', 'Guanlin Wu', 'Ling-Hao Chen', 'Zhuokai Zhao', 'Jing Lin', 'Xiaoke Jiang', 'Jiamin Wu', 'Zhuoheng Li', 'Hao Frank Yang', 'Haoqian Wang', 'Lei Zhang'], 'affiliations': ['HKU', 'HKUST', 'IDEA Research', 'Johns Hopkins University', 'Tsinghua University', 'University of Chicago'], 'pdf_title_img': 'assets/pdf/title_img/2503.07597.jpg', 'data': {'categories': ['#3d', '#dataset', '#long_context', '#video'], 'emoji': '🏃', 'ru': {'title': 'Реконструкция реалистичного 3D-движения человека из видео с несколькими кадрами', 'desc': 'Статья представляет новую систему для реконструкции длинных последовательностей трехмерного движения человека в мировых координатах из видео с несколькими сменами кадров. Авторы решают проблемы резких переходов между кадрами, частичных окклюзий и динамического фона, интегрируя улучшенную оценку положения камеры с восстановлением движения человека. Система включает в себя детектор смены кадров и модуль надежного выравнивания для обеспечения непрерывности позы и ориентации между кадрами. Использование специального интегратора движения позволяет уменьшить проблему скольжения ног и обеспечить временную согласованность позы человека.'}, 'en': {'title': 'Reconstructing Realistic 3D Human Motion Across Multiple Video Shots', 'desc': 'This paper introduces a new framework for reconstructing long sequences of 3D human motion from videos that have multiple camera angles and transitions. The challenge lies in dealing with issues like sudden changes in shots, parts of the person being blocked, and moving backgrounds. Unlike previous methods that only work with single camera views, this approach combines advanced camera pose estimation with Human Motion Recovery (HMR) and includes a shot transition detector for better accuracy. The results show that the method effectively reduces foot sliding and maintains consistent human poses over time, proving its effectiveness on a specially created multi-shot dataset.'}, 'zh': {'title': '重建真实人类运动的新方法', 'desc': '本文提出了一种新颖的框架，旨在从多镜头切换的野外视频中重建长序列的三维人类运动。由于视频中的突然镜头切换、部分遮挡和动态背景，这种长序列的运动恢复面临很大挑战。我们的方法通过结合增强的相机姿态估计和人类运动恢复（HMR），引入了镜头切换检测器和稳健的对齐模块，以确保姿态和方向在镜头间的连续性。通过自定义的运动整合器，我们有效地减轻了脚滑问题，并确保了人类姿态的时间一致性。'}}}, {'id': 'https://huggingface.co/papers/2503.07426', 'title': 'RePO: ReLU-based Preference Optimization', 'url': 'https://huggingface.co/papers/2503.07426', 'abstract': "Aligning large language models (LLMs) with human preferences is critical for real-world deployment, yet existing methods like RLHF face computational and stability challenges. While DPO establishes an offline paradigm with single hyperparameter beta, subsequent methods like SimPO reintroduce complexity through dual parameters (beta, gamma). We propose {ReLU-based Preference Optimization (RePO)}, a streamlined algorithm that eliminates beta via two advances: (1) retaining SimPO's reference-free margins but removing beta through gradient analysis, and (2) adopting a ReLU-based max-margin loss that naturally filters trivial pairs. Theoretically, RePO is characterized as SimPO's limiting case (beta to infty), where the logistic weighting collapses to binary thresholding, forming a convex envelope of the 0-1 loss. Empirical results on AlpacaEval 2 and Arena-Hard show that RePO outperforms DPO and SimPO across multiple base models, requiring only one hyperparameter to tune.", 'score': 1, 'issue_id': 2637, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '619f3a64896639ee', 'authors': ['Junkang Wu', 'Kexin Huang', 'Xue Wang', 'Jinyang Gao', 'Bolin Ding', 'Jiancan Wu', 'Xiangnan He', 'Xiang Wang'], 'affiliations': ['Alibaba Group, Hangzhou, China', 'University of Science and Technology of China, Hefei, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.07426.jpg', 'data': {'categories': ['#alignment', '#rlhf', '#training'], 'emoji': '🚀', 'ru': {'title': 'RePO: Упрощенная оптимизация предпочтений для больших языковых моделей', 'desc': 'Статья представляет новый алгоритм обучения с подкреплением для больших языковых моделей под названием RePO. Этот метод упрощает существующие подходы, такие как DPO и SimPO, устраняя необходимость в гиперпараметре beta. RePO использует ReLU-основанную функцию потерь для фильтрации тривиальных пар и оптимизации предпочтений. Эмпирические результаты показывают, что RePO превосходит DPO и SimPO на нескольких базовых моделях при использовании только одного гиперпараметра.'}, 'en': {'title': 'Streamlining Preference Optimization for LLMs with RePO', 'desc': 'This paper addresses the challenge of aligning large language models (LLMs) with human preferences using a new method called ReLU-based Preference Optimization (RePO). RePO simplifies the optimization process by eliminating the need for a complex hyperparameter, beta, while still maintaining effective performance through gradient analysis and a ReLU-based max-margin loss. The authors demonstrate that RePO can be seen as a special case of an existing method, SimPO, when certain conditions are met, leading to a more efficient training process. Empirical results indicate that RePO consistently outperforms previous methods like DPO and SimPO, while only requiring the tuning of a single hyperparameter.'}, 'zh': {'title': '简化偏好优化，提升语言模型对齐', 'desc': '本文提出了一种新的算法，称为基于ReLU的偏好优化（RePO），旨在简化大型语言模型（LLMs）与人类偏好的对齐过程。RePO通过两个创新点消除了超参数beta，首先保留了SimPO的无参考边际，但通过梯度分析去除了beta，其次采用基于ReLU的最大边际损失，自然过滤掉无关的配对。理论上，RePO被描述为SimPO的极限情况，当beta趋向于无穷大时，逻辑加权收敛为二元阈值，形成0-1损失的凸包。实验证明，RePO在多个基础模型上优于DPO和SimPO，仅需调整一个超参数。'}}}, {'id': 'https://huggingface.co/papers/2503.05641', 'title': 'Symbolic Mixture-of-Experts: Adaptive Skill-based Routing for\n  Heterogeneous Reasoning', 'url': 'https://huggingface.co/papers/2503.05641', 'abstract': "Combining existing pre-trained expert LLMs is a promising avenue for scalably tackling large-scale and diverse tasks. However, selecting experts at the task level is often too coarse-grained, as heterogeneous tasks may require different expertise for each instance. To enable adaptive instance-level mixing of pre-trained LLM experts, we propose Symbolic-MoE, a symbolic, text-based, and gradient-free Mixture-of-Experts framework. Symbolic-MoE takes a fine-grained approach to selection by emphasizing skills, e.g., algebra in math or molecular biology in biomedical reasoning. We propose a skill-based recruiting strategy that dynamically selects the most relevant set of expert LLMs for diverse reasoning tasks based on their strengths. Each selected expert then generates its own reasoning, resulting in k outputs from k experts, which are then synthesized into a final high-quality response by an aggregator chosen based on its ability to integrate diverse reasoning outputs. We show that Symbolic-MoE's instance-level expert selection improves performance by a large margin but -- when implemented naively -- can introduce a high computational overhead due to the need for constant model loading and offloading. To address this, we implement a batch inference strategy that groups instances based on their assigned experts, loading each model only once. This allows us to integrate 16 expert models on 1 GPU with a time cost comparable to or better than prior multi-agent baselines using 4 GPUs. Through extensive evaluations on diverse benchmarks (MMLU-Pro, GPQA, AIME, and MedMCQA), we demonstrate that Symbolic-MoE outperforms strong LLMs like GPT4o-mini, as well as multi-agent approaches, with an absolute average improvement of 8.15% over the best multi-agent baseline. Moreover, Symbolic-MoE removes the need for expensive multi-round discussions, outperforming discussion baselines with less computation.", 'score': 1, 'issue_id': 2643, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 марта', 'en': 'March 7', 'zh': '3月7日'}, 'hash': 'cd41a7296c5c9225', 'authors': ['Justin Chih-Yao Chen', 'Sukwon Yun', 'Elias Stengel-Eskin', 'Tianlong Chen', 'Mohit Bansal'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2503.05641.jpg', 'data': {'categories': ['#training', '#reasoning', '#optimization', '#benchmark', '#agents', '#multimodal'], 'emoji': '🧠', 'ru': {'title': 'Symbolic-MoE: Умное сочетание экспертных LLM для эффективного решения разнообразных задач', 'desc': 'Статья представляет Symbolic-MoE - новый подход к комбинированию предобученных экспертных языковых моделей (LLM) для решения разнообразных задач. Symbolic-MoE использует символьный, текстовый и безградиентный метод выбора экспертов на уровне отдельных примеров, основываясь на необходимых навыках. Предложенная стратегия подбора экспертов динамически выбирает наиболее релевантные модели для различных задач рассуждения, а затем агрегирует их выводы. Авторы демонстрируют, что Symbolic-MoE превосходит сильные базовые модели и мультиагентные подходы на различных бенчмарках, обеспечивая значительное улучшение производительности при эффективном использовании вычислительных ресурсов.'}, 'en': {'title': 'Adaptive Expert Selection for Enhanced Reasoning in LLMs', 'desc': 'The paper introduces Symbolic-MoE, a novel Mixture-of-Experts framework designed to enhance the performance of large language models (LLMs) by enabling instance-level expert selection. This approach allows for the dynamic recruitment of LLMs based on specific skills required for diverse reasoning tasks, such as algebra or molecular biology. By synthesizing outputs from multiple experts, Symbolic-MoE generates high-quality responses while addressing computational efficiency through a batch inference strategy. The results show significant performance improvements over existing models and methods, demonstrating the effectiveness of fine-grained expert selection in machine learning tasks.'}, 'zh': {'title': '实例级专家选择，提升推理性能！', 'desc': '本论文提出了一种名为Symbolic-MoE的框架，旨在通过实例级的专家选择来提高预训练大语言模型（LLM）的性能。该框架采用符号化、基于文本且无梯度的混合专家方法，强调根据任务的不同需求选择合适的专家。通过动态选择最相关的专家，Symbolic-MoE能够在多样化的推理任务中生成高质量的响应。实验结果表明，Symbolic-MoE在多个基准测试中显著超越了现有的强大LLM和多代理方法，且计算效率更高。'}}}, {'id': 'https://huggingface.co/papers/2503.05283', 'title': "Escaping Plato's Cave: Towards the Alignment of 3D and Text Latent\n  Spaces", 'url': 'https://huggingface.co/papers/2503.05283', 'abstract': 'Recent works have shown that, when trained at scale, uni-modal 2D vision and text encoders converge to learned features that share remarkable structural properties, despite arising from different representations. However, the role of 3D encoders with respect to other modalities remains unexplored. Furthermore, existing 3D foundation models that leverage large datasets are typically trained with explicit alignment objectives with respect to frozen encoders from other representations. In this work, we investigate the possibility of a posteriori alignment of representations obtained from uni-modal 3D encoders compared to text-based feature spaces. We show that naive post-training feature alignment of uni-modal text and 3D encoders results in limited performance. We then focus on extracting subspaces of the corresponding feature spaces and discover that by projecting learned representations onto well-chosen lower-dimensional <PRE_TAG>subspaces</POST_TAG> the quality of alignment becomes significantly higher, leading to improved accuracy on matching and retrieval tasks. Our analysis further sheds light on the nature of these shared subspaces, which roughly separate between semantic and geometric data representations. Overall, ours is the first work that helps to establish a baseline for post-training alignment of 3D uni-modal and text feature spaces, and helps to highlight both the shared and unique properties of 3D data compared to other representations.', 'score': 1, 'issue_id': 2637, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 марта', 'en': 'March 7', 'zh': '3月7日'}, 'hash': '85fcbc1ddbe7dbea', 'authors': ['Souhail Hadgi', 'Luca Moschella', 'Andrea Santilli', 'Diego Gomez', 'Qixing Huang', 'Emanuele Rodolà', 'Simone Melzi', 'Maks Ovsjanikov'], 'affiliations': ['Ecole polytechnique', 'Sapienza University of Rome', 'The University of Texas at Austin', 'University of Milano-Bicocca'], 'pdf_title_img': 'assets/pdf/title_img/2503.05283.jpg', 'data': {'categories': ['#alignment', '#3d', '#multimodal', '#transfer_learning'], 'emoji': '🧩', 'ru': {'title': 'Улучшение выравнивания 3D и текстовых признаков через проекцию на подпространства', 'desc': 'Исследование посвящено изучению возможности апостериорного выравнивания представлений, полученных из одномодальных 3D-энкодеров, с текстовыми пространствами признаков. Авторы обнаружили, что простое выравнивание признаков после обучения дает ограниченные результаты. Однако, проецирование выученных представлений на тщательно выбранные подпространства меньшей размерности значительно улучшает качество выравнивания. Анализ также показал, что эти общие подпространства примерно разделяют семантические и геометрические представления данных.'}, 'en': {'title': 'Enhancing 3D and Text Feature Alignment through Subspace Projection', 'desc': 'This paper explores the alignment of features from uni-modal 3D encoders with text-based representations. It reveals that simply aligning these features after training does not yield good results. Instead, the authors find that projecting these features into carefully selected lower-dimensional subspaces significantly improves alignment quality. This work establishes a baseline for understanding how 3D data relates to text features, highlighting both their similarities and differences.'}, 'zh': {'title': '探索3D编码器与文本特征的对齐之路', 'desc': '本研究探讨了3D编码器在与其他模态的关系中的作用，尤其是与文本特征空间的对比。我们发现，简单的后期训练特征对齐方法在单模态文本和3D编码器之间的性能有限。通过提取特征空间的子空间，并将学习到的表示投影到精心选择的低维子空间中，我们显著提高了对齐质量，从而在匹配和检索任务中提升了准确性。我们的工作首次为3D单模态和文本特征空间的后期训练对齐建立了基线，并突出了3D数据与其他表示之间的共享和独特特性。'}}}, {'id': 'https://huggingface.co/papers/2503.02819', 'title': 'Feynman-Kac Correctors in Diffusion: Annealing, Guidance, and Product of\n  Experts', 'url': 'https://huggingface.co/papers/2503.02819', 'abstract': "While score-based generative models are the model of choice across diverse domains, there are limited tools available for controlling inference-time behavior in a principled manner, e.g. for composing multiple pretrained models. Existing classifier-free guidance methods use a simple heuristic to mix conditional and un<PRE_TAG>conditional scores</POST_TAG> to approximately sample from conditional distributions. However, such methods do not approximate the intermediate distributions, necessitating additional 'corrector' steps. In this work, we provide an efficient and principled method for sampling from a sequence of annealed, geometric-averaged, or product distributions derived from pretrained score-based models. We derive a weighted simulation scheme which we call Feynman-Kac Correctors (FKCs) based on the celebrated Feynman-Kac formula by carefully accounting for terms in the appropriate partial differential equations (PDEs). To simulate these PDEs, we propose Sequential Monte Carlo (SMC) resampling algorithms that leverage inference-time scaling to improve sampling quality. We empirically demonstrate the utility of our methods by proposing amortized sampling via inference-time temperature annealing, improving multi-objective molecule generation using pretrained models, and improving classifier-free guidance for text-to-image generation. Our code is available at https://github.com/martaskrt/fkc-diffusion.", 'score': 1, 'issue_id': 2650, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 марта', 'en': 'March 4', 'zh': '3月4日'}, 'hash': '6aa3f8c59c70086b', 'authors': ['Marta Skreta', 'Tara Akhound-Sadegh', 'Viktor Ohanesian', 'Roberto Bondesan', 'Alán Aspuru-Guzik', 'Arnaud Doucet', 'Rob Brekelmans', 'Alexander Tong', 'Kirill Neklyudov'], 'affiliations': ['Google DeepMind', 'Imperial College London', 'McGill University', 'Mila - Quebec AI Institute', 'University of Toronto', 'Université de Montréal', 'Vector Institute'], 'pdf_title_img': 'assets/pdf/title_img/2503.02819.jpg', 'data': {'categories': ['#multimodal', '#optimization', '#cv', '#diffusion', '#inference'], 'emoji': '🧠', 'ru': {'title': 'Улучшение контроля генеративных моделей с помощью корректоров Фейнмана-Каца', 'desc': 'Статья представляет новый метод под названием Feynman-Kac Correctors (FKCs) для эффективного и принципиального семплирования из последовательности усредненных или произведенных распределений, полученных из предобученных генеративных моделей на основе оценки. Авторы выводят взвешенную схему симуляции, основанную на формуле Фейнмана-Каца, и предлагают алгоритмы ресемплирования с использованием последовательного метода Монте-Карло. Метод позволяет улучшить качество семплирования и контролировать поведение модели во время вывода. Эффективность FKCs демонстрируется на задачах генерации молекул и создания изображений по текстовому описанию.'}, 'en': {'title': 'Enhancing Score-Based Generative Models with Feynman-Kac Correctors', 'desc': 'This paper introduces a new method for controlling the behavior of score-based generative models during inference. The authors present Feynman-Kac Correctors (FKCs), which provide a principled way to sample from complex distributions derived from pretrained models. By using Sequential Monte Carlo (SMC) resampling algorithms, they enhance the quality of sampling through inference-time scaling. The proposed method shows improvements in various applications, including molecule generation and text-to-image generation, demonstrating its effectiveness in practical scenarios.'}, 'zh': {'title': '高效控制生成模型的推理行为', 'desc': '本论文提出了一种高效且有原则的方法，用于从预训练的基于分数的模型中采样。我们引入了Feynman-Kac校正器（FKC），基于Feynman-Kac公式，精确处理适当的偏微分方程（PDE）中的项。通过使用顺序蒙特卡洛（SMC）重采样算法，我们提高了采样质量，并展示了该方法在多目标分子生成和文本到图像生成中的应用效果。我们的研究为控制推理时的行为提供了新的工具，推动了生成模型的进一步发展。'}}}, {'id': 'https://huggingface.co/papers/2503.07413', 'title': 'REF-VLM: Triplet-Based Referring Paradigm for Unified Visual Decoding', 'url': 'https://huggingface.co/papers/2503.07413', 'abstract': 'Multimodal Large Language Models (MLLMs) demonstrate robust zero-shot capabilities across diverse vision-language tasks after training on mega-scale datasets. However, dense prediction tasks, such as semantic segmentation and keypoint detection, pose significant challenges for MLLMs when represented solely as text outputs. Simultaneously, current MLLMs utilizing latent embeddings for visual task decoding generally demonstrate limited adaptability to both multi-task learning and multi-granularity scenarios. In this work, we present REF-VLM, an end-to-end framework for unified training of various visual decoding tasks. To address complex visual decoding scenarios, we introduce the Triplet-Based Referring Paradigm (TRP), which explicitly decouples three critical dimensions in visual decoding tasks through a triplet structure: concepts, decoding types, and targets. TRP employs symbolic delimiters to enforce structured representation learning, enhancing the parsability and interpretability of model outputs. Additionally, we construct Visual-Task Instruction Following Dataset (VTInstruct), a large-scale multi-task dataset containing over 100 million multimodal dialogue samples across 25 task types. Beyond text inputs and outputs, VT-Instruct incorporates various visual prompts such as point, box, scribble, and mask, and generates outputs composed of text and visual units like box, keypoint, depth and mask. The combination of different visual prompts and visual units generates a wide variety of task types, expanding the applicability of REF-VLM significantly. Both qualitative and quantitative experiments demonstrate that our REF-VLM outperforms other MLLMs across a variety of standard benchmarks. The code, dataset, and demo available at https://github.com/MacavityT/REF-VLM.', 'score': 0, 'issue_id': 2643, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '52ecfef5a3f1d715', 'authors': ['Yan Tai', 'Luhao Zhu', 'Zhiqiang Chen', 'Ynan Ding', 'Yiying Dong', 'Xiaohong Liu', 'Guodong Guo'], 'affiliations': ['Hong Kong Polytechnic University', 'Ningbo Institute of Digital Twin, Eastern Institute of Technology, Ningbo, China', 'Shanghai Jiao Tong University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.07413.jpg', 'data': {'categories': ['#interpretability', '#cv', '#dataset', '#benchmark', '#multimodal', '#games'], 'emoji': '🖼️', 'ru': {'title': 'REF-VLM: Универсальная мультимодальная модель для задач компьютерного зрения', 'desc': 'В этой статье представлена модель REF-VLM, новая мультимодальная языковая модель для решения разнообразных задач компьютерного зрения. Авторы предлагают подход Triplet-Based Referring Paradigm (TRP) для структурированного представления информации в задачах визуального декодирования. Они также создали большой набор данных VTInstruct, содержащий более 100 миллионов мультимодальных диалогов для 25 типов задач. Эксперименты показывают, что REF-VLM превосходит другие мультимодальные языковые модели на различных стандартных тестах.'}, 'en': {'title': 'REF-VLM: Unifying Visual Decoding for Enhanced Multimodal Learning', 'desc': 'This paper introduces REF-VLM, a new framework designed to improve the performance of Multimodal Large Language Models (MLLMs) on complex visual tasks like semantic segmentation and keypoint detection. It addresses the limitations of current MLLMs by using a Triplet-Based Referring Paradigm (TRP) that separates concepts, decoding types, and targets for better structured learning. The authors also present a large-scale dataset, VTInstruct, which includes multimodal dialogue samples and various visual prompts to enhance training. Experimental results show that REF-VLM significantly outperforms existing MLLMs on standard benchmarks, demonstrating its effectiveness in handling diverse visual decoding tasks.'}, 'zh': {'title': 'REF-VLM：统一视觉解码任务的创新框架', 'desc': '多模态大型语言模型（MLLMs）在经过大规模数据集训练后，展现出在多种视觉语言任务中的强大零-shot能力。然而，对于密集预测任务，如语义分割和关键点检测，仅用文本输出表示时，MLLMs面临重大挑战。为了解决这些复杂的视觉解码场景，我们提出了REF-VLM，一个用于统一训练各种视觉解码任务的端到端框架，并引入了基于三元组的引用范式（TRP），以明确解耦视觉解码任务中的概念、解码类型和目标。我们的实验表明，REF-VLM在多项标准基准测试中优于其他MLLMs，展示了其在多任务学习和多粒度场景中的适应性。'}}}, {'id': 'https://huggingface.co/papers/2503.06698', 'title': "What's in a Latent? Leveraging Diffusion Latent Space for Domain\n  Generalization", 'url': 'https://huggingface.co/papers/2503.06698', 'abstract': 'Domain Generalization aims to develop models that can generalize to novel and unseen data distributions. In this work, we study how model architectures and pre-training objectives impact feature richness and propose a method to effectively leverage them for domain generalization. Specifically, given a pre-trained feature space, we first discover latent domain structures, referred to as pseudo-domains, that capture domain-specific variations in an unsupervised manner. Next, we augment existing classifiers with these complementary pseudo-domain representations making them more amenable to diverse unseen test domains. We analyze how different pre-training feature spaces differ in the domain-specific variances they capture. Our empirical studies reveal that features from diffusion models excel at separating domains in the absence of explicit domain labels and capture nuanced domain-specific information. On 5 datasets, we show that our very simple framework improves generalization to unseen domains by a maximum test accuracy improvement of over 4% compared to the standard baseline Empirical Risk Minimization (ERM). Crucially, our method outperforms most algorithms that access domain labels during training.', 'score': 0, 'issue_id': 2645, 'pub_date': '2025-03-09', 'pub_date_card': {'ru': '9 марта', 'en': 'March 9', 'zh': '3月9日'}, 'hash': 'c706eb4738361e39', 'authors': ['Xavier Thomas', 'Deepti Ghadiyaram'], 'affiliations': ['Boston University', 'Runway'], 'pdf_title_img': 'assets/pdf/title_img/2503.06698.jpg', 'data': {'categories': ['#transfer_learning', '#optimization', '#dataset', '#training', '#architecture'], 'emoji': '🌐', 'ru': {'title': 'Улучшение обобщения на новые домены через выявление скрытых доменных структур', 'desc': 'Статья посвящена улучшению обобщающей способности моделей машинного обучения на новые распределения данных. Авторы предлагают метод, использующий предобученные пространства признаков для выявления скрытых доменных структур. Они дополняют существующие классификаторы этими представлениями псевдо-доменов, что улучшает работу на незнакомых тестовых доменах. Исследование показывает, что признаки из диффузионных моделей особенно хорошо подходят для разделения доменов без явных доменных меток.'}, 'en': {'title': 'Unlocking Generalization with Pseudo-Domains', 'desc': "This paper focuses on Domain Generalization, which is about creating models that can perform well on new and unseen data. The authors investigate how different model architectures and pre-training methods affect the richness of features used for generalization. They introduce a technique to identify latent structures called pseudo-domains that represent variations in data without needing labels. Their experiments show that using features from diffusion models significantly enhances the model's ability to generalize, achieving better accuracy on unseen domains compared to traditional methods."}, 'zh': {'title': '提升模型的领域泛化能力', 'desc': '领域泛化旨在开发能够对新颖和未见数据分布进行泛化的模型。本文研究了模型架构和预训练目标如何影响特征丰富性，并提出了一种有效利用这些特征进行领域泛化的方法。我们首先在无监督的情况下发现潜在的领域结构，称为伪领域，这些结构捕捉了领域特定的变化。通过增强现有分类器与这些伪领域表示的结合，我们的方法在五个数据集上显示出比标准基线经验风险最小化（ERM）提高了超过4%的测试准确率。'}}}, {'id': 'https://huggingface.co/papers/2503.03511', 'title': 'NeuGrasp: Generalizable Neural Surface Reconstruction with Background\n  Priors for Material-Agnostic Object Grasp Detection', 'url': 'https://huggingface.co/papers/2503.03511', 'abstract': 'Robotic grasping in scenes with transparent and specular objects presents great challenges for methods relying on accurate depth information. In this paper, we introduce NeuGrasp, a neural surface reconstruction method that leverages background priors for material-agnostic grasp detection. NeuGrasp integrates transformers and global prior volumes to aggregate multi-view features with spatial encoding, enabling robust surface reconstruction in narrow and sparse viewing conditions. By focusing on foreground objects through residual feature enhancement and refining spatial perception with an occupancy-prior volume, NeuGrasp excels in handling objects with transparent and specular surfaces. Extensive experiments in both simulated and real-world scenarios show that NeuGrasp outperforms state-of-the-art methods in grasping while maintaining comparable reconstruction quality. More details are available at https://neugrasp.github.io/.', 'score': 0, 'issue_id': 2630, 'pub_date': '2025-03-05', 'pub_date_card': {'ru': '5 марта', 'en': 'March 5', 'zh': '3月5日'}, 'hash': 'ba660b9e0f676df1', 'authors': ['Qingyu Fan', 'Yinghao Cai', 'Chao Li', 'Wenzhe He', 'Xudong Zheng', 'Tao Lu', 'Bin Liang', 'Shuo Wang'], 'affiliations': ['Qiyuan Lab', 'School of Artificial Intelligence, University of Chinese Academy of Sciences', 'State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2503.03511.jpg', 'data': {'categories': ['#robotics', '#agents', '#architecture', '#cv'], 'emoji': '🤖', 'ru': {'title': 'NeuGrasp: нейронный захват для сложных поверхностей', 'desc': 'NeuGrasp - это нейронный метод реконструкции поверхности для захвата объектов с прозрачными и зеркальными поверхностями. Он использует трансформеры и глобальные приоры для агрегации мультиракурсных признаков с пространственным кодированием. NeuGrasp фокусируется на объектах переднего плана и уточняет пространственное восприятие с помощью объема приоров заполненности. Эксперименты показывают, что NeuGrasp превосходит современные методы в задаче захвата объектов.'}, 'en': {'title': 'NeuGrasp: Mastering Grasping with Transparent and Specular Objects', 'desc': 'This paper presents NeuGrasp, a novel approach for robotic grasping that effectively deals with transparent and specular objects, which are challenging for traditional depth-based methods. NeuGrasp utilizes a neural surface reconstruction technique that incorporates background priors to enhance grasp detection without being limited by material properties. By combining transformers and global prior volumes, it aggregates multi-view features with spatial encoding, improving surface reconstruction even in difficult viewing conditions. The method demonstrates superior performance in grasping tasks compared to existing techniques, while also achieving high-quality surface reconstruction in both simulated and real-world environments.'}, 'zh': {'title': 'NeuGrasp：透明物体抓取的新突破', 'desc': '本论文介绍了一种名为NeuGrasp的神经表面重建方法，旨在解决透明和镜面物体抓取中的挑战。该方法利用背景先验进行材料无关的抓取检测，结合了变换器和全局先验体积，以聚合多视角特征并进行空间编码。NeuGrasp通过残差特征增强聚焦于前景物体，并利用占用先验体积来改善空间感知，能够有效处理透明和镜面表面的物体。实验结果表明，NeuGrasp在抓取性能上优于现有的最先进方法，同时保持了相似的重建质量。'}}}, {'id': 'https://huggingface.co/papers/2502.20475', 'title': 'Promote, Suppress, Iterate: How Language Models Answer One-to-Many\n  Factual Queries', 'url': 'https://huggingface.co/papers/2502.20475', 'abstract': "To answer one-to-many factual queries (e.g., listing cities of a country), a language model (LM) must simultaneously recall knowledge and avoid repeating previous answers. How are these two subtasks implemented and integrated internally? Across multiple datasets and models, we identify a promote-then-suppress mechanism: the model first recalls all answers, and then suppresses previously generated ones. Specifically, LMs use both the subject and previous answer tokens to perform knowledge recall, with attention propagating subject information and MLPs promoting the answers. Then, attention attends to and suppresses previous answer tokens, while MLPs amplify the suppression signal. Our mechanism is corroborated by extensive experimental evidence: in addition to using early decoding and causal tracing, we analyze how components use different tokens by introducing both Token Lens, which decodes aggregated attention updates from specified tokens, and a knockout method that analyzes changes in MLP outputs after removing attention to specified tokens. Overall, we provide new insights into how LMs' internal components interact with different input tokens to support complex factual recall. Code is available at https://github.com/Lorenayannnnn/how-lms-answer-one-to-many-factual-queries.", 'score': 0, 'issue_id': 2645, 'pub_date': '2025-02-27', 'pub_date_card': {'ru': '27 февраля', 'en': 'February 27', 'zh': '2月27日'}, 'hash': 'c89164e93db21862', 'authors': ['Tianyi Lorena Yan', 'Robin Jia'], 'affiliations': ['University of Southern California'], 'pdf_title_img': 'assets/pdf/title_img/2502.20475.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#data', '#interpretability', '#dataset', '#training', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'Раскрытие внутренней механики языковых моделей при ответе на сложные фактологические запросы', 'desc': "Статья исследует механизм ответа языковых моделей на фактологические запросы с множественными ответами. Авторы выявили процесс 'продвижения, затем подавления': модель сначала вспоминает все возможные ответы, а затем подавляет уже сгенерированные. Этот механизм реализуется через взаимодействие слоев внимания и MLP, где внимание распространяет информацию о предмете запроса, а MLP усиливают сигналы ответов и их подавления. Исследование подкрепляется экспериментальными данными и новыми методами анализа, такими как Token Lens и метод нокаута."}, 'en': {'title': 'Promote and Suppress: The Key to Factual Recall in Language Models', 'desc': 'This paper explores how language models (LMs) handle one-to-many factual queries by using a promote-then-suppress mechanism. Initially, the model recalls all possible answers based on the subject and previous answers, utilizing attention and multi-layer perceptrons (MLPs) to promote relevant responses. Subsequently, it suppresses any previously generated answers to avoid repetition, ensuring a diverse output. The authors provide experimental evidence and novel analysis methods to demonstrate how LMs manage these internal processes effectively.'}, 'zh': {'title': '语言模型的促进与抑制机制', 'desc': '本文探讨了语言模型在回答一对多事实查询时的内部机制。研究发现，模型采用了一种先促进后抑制的机制，首先回忆所有可能的答案，然后抑制之前生成的答案。具体来说，模型利用主题和之前答案的标记进行知识回忆，通过注意力机制传播主题信息，并通过多层感知机（MLP）促进答案的生成。实验结果表明，该机制有效，提供了对语言模型内部组件如何与不同输入标记交互以支持复杂事实回忆的新见解。'}}}, {'id': 'https://huggingface.co/papers/2502.20730', 'title': 'DeepSolution: Boosting Complex Engineering Solution Design via Tree-based Exploration and Bi-point Thinking', 'url': 'https://huggingface.co/papers/2502.20730', 'abstract': "Designing solutions for complex engineering challenges is crucial in human production activities. However, previous research in the retrieval-augmented generation (RAG) field has not sufficiently addressed tasks related to the design of complex engineering solutions. To fill this gap, we introduce a new benchmark, SolutionBench, to evaluate a system's ability to generate complete and feasible solutions for engineering problems with multiple complex constraints. To further advance the design of complex engineering solutions, we propose a novel system, SolutionRAG, that leverages the tree-based exploration and bi-point thinking mechanism to generate reliable solutions. Extensive experimental results demonstrate that SolutionRAG achieves state-of-the-art (SOTA) performance on the SolutionBench, highlighting its potential to enhance the automation and reliability of complex engineering solution design in real-world applications.", 'score': 20, 'issue_id': 2486, 'pub_date': '2025-02-28', 'pub_date_card': {'ru': '28 февраля', 'en': 'February 28', 'zh': '2月28日'}, 'hash': 'e9ef168e304ec240', 'authors': ['Zhuoqun Li', 'Haiyang Yu', 'Xuanang Chen', 'Hongyu Lin', 'Yaojie Lu', 'Fei Huang', 'Xianpei Han', 'Yongbin Li', 'Le Sun'], 'affiliations': ['Chinese Information Processing Laboratory, Institute of Software, Chinese Academy of Sciences', 'Tongyi Lab', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2502.20730.jpg', 'data': {'categories': ['#rag', '#benchmark'], 'emoji': '🔧', 'ru': {'title': 'Умная система для проектирования сложных инженерных решений', 'desc': 'Статья представляет новый бенчмарк SolutionBench для оценки способности систем генерировать решения инженерных задач с множественными ограничениями. Авторы предлагают систему SolutionRAG, использующую древовидное исследование и механизм двухточечного мышления для создания надежных решений. Экспериментальные результаты показывают, что SolutionRAG достигает наилучших показателей на SolutionBench. Это демонстрирует потенциал системы для улучшения автоматизации и надежности проектирования сложных инженерных решений в реальных приложениях.'}, 'en': {'title': 'Revolutionizing Engineering Design with SolutionRAG', 'desc': 'This paper addresses the need for effective solutions in complex engineering design tasks, which have been overlooked in previous research on retrieval-augmented generation (RAG). It introduces a new benchmark called SolutionBench, aimed at evaluating the generation of feasible solutions under multiple constraints. The authors propose a novel system named SolutionRAG, which utilizes tree-based exploration and bi-point thinking to improve solution reliability. Experimental results show that SolutionRAG outperforms existing methods, indicating its potential to automate and enhance the design process in engineering applications.'}, 'zh': {'title': '提升复杂工程设计的自动化与可靠性', 'desc': '本论文提出了一个新的基准测试，称为SolutionBench，用于评估系统在生成复杂工程问题的完整和可行解决方案方面的能力。我们还提出了一种新系统SolutionRAG，利用树形探索和双点思维机制来生成可靠的解决方案。通过大量实验结果，SolutionRAG在SolutionBench上达到了最先进的性能，显示了其在实际应用中提高复杂工程解决方案设计的自动化和可靠性的潜力。此研究填补了以往在检索增强生成（RAG）领域中对复杂工程解决方案设计任务的研究空白。'}}}, {'id': 'https://huggingface.co/papers/2502.18600', 'title': 'Chain of Draft: Thinking Faster by Writing Less', 'url': 'https://huggingface.co/papers/2502.18600', 'abstract': 'Large Language Models (LLMs) have demonstrated remarkable performance in solving complex reasoning tasks through mechanisms like Chain-of-Thought (CoT) prompting, which emphasizes verbose, step-by-step reasoning. However, humans typically employ a more efficient strategy: drafting concise intermediate thoughts that capture only essential information. In this work, we propose Chain of Draft (CoD), a novel paradigm inspired by human cognitive processes, where LLMs generate minimalistic yet informative intermediate reasoning outputs while solving tasks. By reducing verbosity and focusing on critical insights, CoD matches or surpasses CoT in accuracy while using as little as only 7.6% of the tokens, significantly reducing cost and latency across various reasoning tasks.', 'score': 19, 'issue_id': 2491, 'pub_date': '2025-02-25', 'pub_date_card': {'ru': '25 февраля', 'en': 'February 25', 'zh': '2月25日'}, 'hash': '739d903f5735d9eb', 'authors': ['Silei Xu', 'Wenhao Xie', 'Lingxiao Zhao', 'Pengcheng He'], 'affiliations': ['Zoom Communications'], 'pdf_title_img': 'assets/pdf/title_img/2502.18600.jpg', 'data': {'categories': ['#reasoning', '#training', '#rl'], 'emoji': '✍️', 'ru': {'title': 'Эффективное рассуждение: краткость - сестра точности', 'desc': 'Статья представляет новый подход к решению сложных задач с помощью больших языковых моделей - Chain of Draft (CoD). В отличие от метода Chain-of-Thought (CoT), который использует подробные рассуждения, CoD имитирует человеческий подход, генерируя краткие промежуточные мысли. Этот метод позволяет достичь той же или лучшей точности, что и CoT, но при этом использует значительно меньше токенов. CoD демонстрирует эффективность в различных задачах, требующих рассуждений, снижая затраты и задержки.'}, 'en': {'title': 'Streamlining Reasoning: Less is More with Chain of Draft', 'desc': 'This paper introduces Chain of Draft (CoD), a new approach for Large Language Models (LLMs) that mimics human reasoning by generating concise intermediate thoughts. Unlike the traditional Chain-of-Thought (CoT) prompting, which relies on verbose explanations, CoD focuses on delivering essential information in a minimalistic format. The authors demonstrate that CoD can achieve comparable or even superior accuracy to CoT while using significantly fewer tokens, leading to reduced computational costs and faster processing times. This innovative method enhances the efficiency of LLMs in tackling complex reasoning tasks.'}, 'zh': {'title': '草稿链：高效推理的新方法', 'desc': '大型语言模型（LLMs）在解决复杂推理任务方面表现出色，尤其是通过链式思维（CoT）提示，强调逐步推理的详细过程。然而，人类通常采用更高效的策略：草拟简洁的中间思考，只捕捉关键信息。本文提出了一种新范式——草稿链（CoD），灵感来源于人类的认知过程，使LLMs在解决任务时生成简约而信息丰富的中间推理输出。通过减少冗长并专注于关键见解，CoD在准确性上与CoT相匹配或超越，同时仅使用7.6%的标记，显著降低了各种推理任务的成本和延迟。'}}}, {'id': 'https://huggingface.co/papers/2502.20380', 'title': 'Multi-Turn Code Generation Through Single-Step Rewards', 'url': 'https://huggingface.co/papers/2502.20380', 'abstract': 'We address the problem of code generation from multi-turn execution feedback. Existing methods either generate code without feedback or use complex, hierarchical reinforcement learning to optimize multi-turn rewards. We propose a simple yet scalable approach, muCode, that solves multi-turn code generation using only single-step rewards. Our key insight is that code generation is a one-step recoverable MDP, where the correct code can be recovered from any intermediate code state in a single turn. muCode iteratively trains both a generator to provide code solutions conditioned on multi-turn execution feedback and a verifier to score the newly generated code. Experimental evaluations show that our approach achieves significant improvements over the state-of-the-art baselines. We provide analysis of the design choices of the reward models and policy, and show the efficacy of muCode at utilizing the execution feedback. Our code is available at https://github.com/portal-cornell/muCode.', 'score': 17, 'issue_id': 2500, 'pub_date': '2025-02-27', 'pub_date_card': {'ru': '27 февраля', 'en': 'February 27', 'zh': '2月27日'}, 'hash': 'cceb0299fb5077b5', 'authors': ['Arnav Kumar Jain', 'Gonzalo Gonzalez-Pumariega', 'Wayne Chen', 'Alexander M Rush', 'Wenting Zhao', 'Sanjiban Choudhury'], 'affiliations': ['Cornell University', 'MilaQuebec AI Institute', 'Universite de Montreal'], 'pdf_title_img': 'assets/pdf/title_img/2502.20380.jpg', 'data': {'categories': ['#rl', '#rlhf', '#training', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Эффективная генерация кода с многоэтапной обратной связью', 'desc': 'Статья представляет новый подход к генерации кода с использованием многоэтапной обратной связи по выполнению, названный muCode. В отличие от существующих методов, использующих сложное иерархическое обучение с подкреплением, muCode применяет простой масштабируемый подход с одношаговыми вознаграждениями. Метод итеративно обучает генератор для создания кодовых решений и верификатор для оценки нового кода. Экспериментальные результаты показывают значительное улучшение по сравнению с современными базовыми методами.'}, 'en': {'title': 'Simplifying Code Generation with muCode: One-Step Recovery from Feedback', 'desc': 'This paper presents muCode, a novel approach for generating code based on multi-turn execution feedback. Unlike existing methods that rely on complex reinforcement learning techniques, muCode simplifies the process by using single-step rewards. The authors argue that code generation can be treated as a one-step recoverable Markov Decision Process (MDP), allowing for the recovery of correct code from any intermediate state. Through iterative training of a code generator and a verifier, muCode demonstrates significant performance improvements over current state-of-the-art methods.'}, 'zh': {'title': '简单高效的多轮代码生成方法', 'desc': '本文解决了从多轮执行反馈中生成代码的问题。现有方法要么在没有反馈的情况下生成代码，要么使用复杂的层次强化学习来优化多轮奖励。我们提出了一种简单而可扩展的方法muCode，仅使用单步奖励来解决多轮代码生成。实验结果表明，我们的方法在性能上显著优于现有的最先进基线。'}}}, {'id': 'https://huggingface.co/papers/2502.21318', 'title': 'How far can we go with ImageNet for Text-to-Image generation?', 'url': 'https://huggingface.co/papers/2502.21318', 'abstract': "Recent text-to-image (T2I) generation models have achieved remarkable results by training on billion-scale datasets, following a `bigger is better' paradigm that prioritizes data quantity over quality. We challenge this established paradigm by demonstrating that strategic data augmentation of small, well-curated datasets can match or outperform models trained on massive web-scraped collections. Using only ImageNet enhanced with well-designed text and image augmentations, we achieve a +2 overall score over SD-XL on GenEval and +5 on DPGBench while using just 1/10th the parameters and 1/1000th the training images. Our results suggest that strategic data augmentation, rather than massive datasets, could offer a more sustainable path forward for T2I generation.", 'score': 13, 'issue_id': 2498, 'pub_date': '2025-02-28', 'pub_date_card': {'ru': '28 февраля', 'en': 'February 28', 'zh': '2月28日'}, 'hash': '73a20b698d827c0d', 'authors': ['L. Degeorge', 'A. Ghosh', 'N. Dufour', 'D. Picard', 'V. Kalogeiton'], 'affiliations': ['AMIAD, Pole recherche', 'LIGM, Ecole Nationale des Ponts et Chaussees, IP Paris, Univ Gustave Eiffel, CNRS, France', 'LIX, Ecole Polytechnique, CNRS, IP Paris, France'], 'pdf_title_img': 'assets/pdf/title_img/2502.21318.jpg', 'data': {'categories': ['#synthetic', '#optimization', '#training', '#data', '#cv', '#dataset'], 'emoji': '🔬', 'ru': {'title': 'Качество данных побеждает количество в генерации изображений', 'desc': 'Исследователи предлагают новый подход к обучению моделей генерации изображений по тексту, основанный на стратегическом аугментировании небольших, но качественных наборов данных. Используя только ImageNet с тщательно разработанными текстовыми и визуальными аугментациями, им удалось превзойти модели, обученные на огромных веб-наборах данных. Их модель превосходит SD-XL на 2 балла в GenEval и на 5 баллов в DPGBench, используя всего 1/10 параметров и 1/1000 обучающих изображений. Результаты указывают на то, что стратегическое аугментирование данных может быть более устойчивым путем развития генеративных моделей, чем использование массивных датасетов.'}, 'en': {'title': 'Quality Over Quantity: Augmenting Small Datasets for Better T2I Models', 'desc': 'This paper challenges the common belief that larger datasets always lead to better text-to-image (T2I) generation models. It shows that by using strategic data augmentation on smaller, high-quality datasets, we can achieve results that are as good as or better than those from models trained on much larger datasets. Specifically, the authors enhanced ImageNet with carefully designed text and image augmentations, leading to significant performance improvements. Their findings suggest that focusing on data quality and augmentation may be a more efficient and sustainable approach for developing T2I models.'}, 'zh': {'title': '战略数据增强：小数据集的力量', 'desc': '最近的文本到图像生成模型通过在大规模数据集上训练取得了显著成果，遵循了“越大越好”的范式，优先考虑数据的数量而非质量。我们挑战了这一既定范式，展示了通过对小型、精心策划的数据集进行战略性数据增强，可以与在大规模网络抓取集合上训练的模型相匹敌或超越。仅使用经过精心设计的文本和图像增强的ImageNet，我们在GenEval上比SD-XL高出2分，在DPGBench上高出5分，同时只使用了1/10的参数和1/1000的训练图像。我们的结果表明，战略性数据增强而非大规模数据集，可能为文本到图像生成提供更可持续的发展路径。'}}}, {'id': 'https://huggingface.co/papers/2502.18017', 'title': 'ViDoRAG: Visual Document Retrieval-Augmented Generation via Dynamic Iterative Reasoning Agents', 'url': 'https://huggingface.co/papers/2502.18017', 'abstract': "Understanding information from visually rich documents remains a significant challenge for traditional Retrieval-Augmented Generation (RAG) methods. Existing benchmarks predominantly focus on image-based question answering (QA), overlooking the fundamental challenges of efficient retrieval, comprehension, and reasoning within dense visual documents. To bridge this gap, we introduce ViDoSeek, a novel dataset designed to evaluate RAG performance on visually rich documents requiring complex reasoning. Based on it, we identify key limitations in current RAG approaches: (i) purely visual retrieval methods struggle to effectively integrate both textual and visual features, and (ii) previous approaches often allocate insufficient reasoning tokens, limiting their effectiveness. To address these challenges, we propose ViDoRAG, a novel multi-agent RAG framework tailored for complex reasoning across visual documents. ViDoRAG employs a Gaussian Mixture Model (GMM)-based hybrid strategy to effectively handle multi-modal retrieval. To further elicit the model's reasoning capabilities, we introduce an iterative agent workflow incorporating exploration, summarization, and reflection, providing a framework for investigating test-time scaling in RAG domains. Extensive experiments on ViDoSeek validate the effectiveness and generalization of our approach. Notably, ViDoRAG outperforms existing methods by over 10% on the competitive ViDoSeek benchmark.", 'score': 9, 'issue_id': 2487, 'pub_date': '2025-02-25', 'pub_date_card': {'ru': '25 февраля', 'en': 'February 25', 'zh': '2月25日'}, 'hash': '4202273d8c895c2a', 'authors': ['Qiuchen Wang', 'Ruixue Ding', 'Zehui Chen', 'Weiqi Wu', 'Shihang Wang', 'Pengjun Xie', 'Feng Zhao'], 'affiliations': ['MoE Key Laboratory of Brain-inspired Intelligent Perception and Cognition, USTC', 'Shanghai Jiao Tong University', 'Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2502.18017.jpg', 'data': {'categories': ['#reasoning', '#agents', '#rag', '#games', '#benchmark', '#multimodal', '#dataset'], 'emoji': '🔍', 'ru': {'title': 'ViDoRAG: Новый подход к извлечению информации из визуальных документов', 'desc': 'Статья представляет новый набор данных ViDoSeek для оценки эффективности методов извлечения информации с добавлением генерации (RAG) на визуально насыщенных документах. Авторы выявляют ограничения существующих подходов RAG в обработке мультимодальной информации и сложных рассуждений. Для решения этих проблем предлагается новая мультиагентная система ViDoRAG, использующая гибридную стратегию на основе гауссовых смесей для мультимодального поиска. ViDoRAG демонстрирует улучшение производительности более чем на 10% по сравнению с существующими методами на бенчмарке ViDoSeek.'}, 'en': {'title': 'Enhancing RAG for Complex Visual Reasoning with ViDoRAG', 'desc': 'This paper addresses the challenges of understanding complex information in visually rich documents using Retrieval-Augmented Generation (RAG) methods. It introduces ViDoSeek, a new dataset that tests RAG performance on documents that require advanced reasoning skills. The authors highlight limitations in current RAG techniques, such as difficulties in integrating visual and textual data and inadequate reasoning capabilities. To overcome these issues, they propose ViDoRAG, a multi-agent framework that enhances retrieval and reasoning through a hybrid strategy and an iterative workflow, demonstrating significant improvements in performance on the ViDoSeek benchmark.'}, 'zh': {'title': '提升视觉文档理解的RAG新框架', 'desc': '理解视觉丰富文档中的信息对传统的增强检索生成（RAG）方法来说仍然是一个重大挑战。现有的基准主要集中在基于图像的问题回答（QA），而忽视了在密集视觉文档中高效检索、理解和推理的基本挑战。为了解决这些问题，我们引入了ViDoSeek，这是一个新颖的数据集，旨在评估RAG在需要复杂推理的视觉丰富文档上的表现。我们提出的ViDoRAG框架采用混合策略，结合多模态检索和迭代代理工作流，以提高模型的推理能力，并在ViDoSeek基准上显著超越现有方法。'}}}, {'id': 'https://huggingface.co/papers/2502.20545', 'title': 'SoS1: O1 and R1-Like Reasoning LLMs are Sum-of-Square Solvers', 'url': 'https://huggingface.co/papers/2502.20545', 'abstract': "Large Language Models (LLMs) have achieved human-level proficiency across diverse tasks, but their ability to perform rigorous mathematical problem solving remains an open challenge. In this work, we investigate a fundamental yet computationally intractable problem: determining whether a given multivariate polynomial is nonnegative. This problem, closely related to Hilbert's Seventeenth Problem, plays a crucial role in global polynomial optimization and has applications in various fields. First, we introduce SoS-1K, a meticulously curated dataset of approximately 1,000 polynomials, along with expert-designed reasoning instructions based on five progressively challenging criteria. Evaluating multiple state-of-the-art LLMs, we find that without structured guidance, all models perform only slightly above the random guess baseline 50%. However, high-quality reasoning instructions significantly improve accuracy, boosting performance up to 81%. Furthermore, our 7B model, SoS-7B, fine-tuned on SoS-1K for just 4 hours, outperforms the 671B DeepSeek-V3 and GPT-4o-mini in accuracy while only requiring 1.8% and 5% of the computation time needed for letters, respectively. Our findings highlight the potential of LLMs to push the boundaries of mathematical reasoning and tackle NP-hard problems.", 'score': 9, 'issue_id': 2486, 'pub_date': '2025-02-27', 'pub_date_card': {'ru': '27 февраля', 'en': 'February 27', 'zh': '2月27日'}, 'hash': 'fb32f9423103ece9', 'authors': ['Kechen Li', 'Wenqi Zhu', 'Coralia Cartis', 'Tianbo Ji', 'Shiwei Liu'], 'affiliations': ['Mathematical Institute, University of Oxford', 'Nanjing University of Aeronautics and Astronautics', 'School of Transportation and Civil Engineering, Nantong University'], 'pdf_title_img': 'assets/pdf/title_img/2502.20545.jpg', 'data': {'categories': ['#dataset', '#optimization', '#training', '#math', '#reasoning'], 'emoji': '🧮', 'ru': {'title': 'Большие языковые модели преодолевают границы математического мышления', 'desc': 'В этой статье исследуется способность больших языковых моделей (LLM) решать сложные математические задачи, в частности, определение неотрицательности многомерных полиномов. Авторы создали датасет SoS-1K из 1000 полиномов и разработали инструкции по рассуждению для моделей. Эксперименты показали, что без структурированного руководства модели работают немного лучше случайного угадывания, но с качественными инструкциями точность повышается до 81%. Модель SoS-7B, дообученная на SoS-1K, превзошла более крупные модели по точности и скорости вычислений.'}, 'en': {'title': 'Unlocking Mathematical Reasoning in LLMs with Structured Guidance', 'desc': 'This paper explores the limitations of Large Language Models (LLMs) in solving complex mathematical problems, specifically the challenge of determining if a multivariate polynomial is nonnegative. The authors introduce a new dataset called SoS-1K, which contains around 1,000 polynomials and structured reasoning instructions to guide the models. They demonstrate that LLMs perform poorly without guidance, achieving only slightly above random guessing, but can significantly improve their accuracy with high-quality instructions. Notably, their fine-tuned model, SoS-7B, surpasses larger models in performance while being more computationally efficient, showcasing the potential of LLMs in addressing NP-hard problems.'}, 'zh': {'title': '推动数学推理的边界', 'desc': '大型语言模型（LLMs）在多种任务中达到了人类水平的能力，但在严格的数学问题解决方面仍然面临挑战。本文研究了一个基本但计算上难以处理的问题：判断给定的多变量多项式是否非负。我们引入了SoS-1K数据集，包含约1000个多项式，并设计了基于五个逐步挑战标准的推理指导。实验表明，经过高质量的推理指导后，模型的准确率显著提高，最高可达81%。'}}}, {'id': 'https://huggingface.co/papers/2502.20396', 'title': 'Sim-to-Real Reinforcement Learning for Vision-Based Dexterous Manipulation on Humanoids', 'url': 'https://huggingface.co/papers/2502.20396', 'abstract': 'Reinforcement learning has delivered promising results in achieving human- or even superhuman-level capabilities across diverse problem domains, but success in dexterous robot manipulation remains limited. This work investigates the key challenges in applying reinforcement learning to solve a collection of contact-rich manipulation tasks on a humanoid embodiment. We introduce novel techniques to overcome the identified challenges with empirical validation. Our main contributions include an automated real-to-sim tuning module that brings the simulated environment closer to the real world, a generalized reward design scheme that simplifies reward engineering for long-horizon contact-rich manipulation tasks, a divide-and-conquer distillation process that improves the sample efficiency of hard-exploration problems while maintaining sim-to-real performance, and a mixture of sparse and dense object representations to bridge the sim-to-real perception gap. We show promising results on three humanoid dexterous manipulation tasks, with ablation studies on each technique. Our work presents a successful approach to learning humanoid dexterous manipulation using sim-to-real reinforcement learning, achieving robust generalization and high performance without the need for human demonstration.', 'score': 7, 'issue_id': 2486, 'pub_date': '2025-02-27', 'pub_date_card': {'ru': '27 февраля', 'en': 'February 27', 'zh': '2月27日'}, 'hash': '41439b4f54e02c9b', 'authors': ['Toru Lin', 'Kartik Sachdev', 'Linxi Fan', 'Jitendra Malik', 'Yuke Zhu'], 'affiliations': ['NVIDIA', 'UC Berkeley', 'UT Austin'], 'pdf_title_img': 'assets/pdf/title_img/2502.20396.jpg', 'data': {'categories': ['#robotics', '#rl', '#games', '#optimization'], 'emoji': '🤖', 'ru': {'title': 'Ловкость робота: от симуляции к реальности', 'desc': 'Статья описывает применение обучения с подкреплением для решения задач манипуляции объектами с помощью человекоподобного робота. Авторы представляют новые методы для преодоления ключевых проблем, включая автоматическую настройку симуляции, обобщенную схему проектирования наград и смешанное представление объектов. Исследование демонстрирует успешные результаты на трех задачах ловкой манипуляции, достигая надежной генерализации и высокой производительности. Работа предлагает эффективный подход к обучению человекоподобных роботов сложным манипуляциям без необходимости в демонстрациях человека.'}, 'en': {'title': 'Reinforcement Learning for Dexterous Robot Manipulation: Bridging Sim and Real Worlds', 'desc': 'This paper addresses the challenges of using reinforcement learning for complex robot manipulation tasks that involve physical contact. The authors propose innovative methods to enhance the performance of humanoid robots in these tasks, including a system to align simulated environments with real-world conditions. They also introduce a new reward design that simplifies the process of creating effective rewards for long tasks and a technique to improve learning efficiency in difficult scenarios. The results demonstrate that their approach allows robots to learn dexterous manipulation effectively, achieving high performance without requiring human guidance.'}, 'zh': {'title': '突破类人机器人灵巧操作的强化学习挑战', 'desc': '本研究探讨了在类人机器人灵巧操作任务中应用强化学习的关键挑战。我们提出了新技术来克服这些挑战，包括自动化的真实到模拟调优模块，以缩小模拟环境与现实世界的差距。我们还设计了一种通用的奖励机制，简化了长时间接触丰富的操作任务的奖励工程。通过对三项类人灵巧操作任务的实验，我们展示了在不需要人类示范的情况下，使用模拟到真实的强化学习实现了稳健的泛化和高性能。'}}}, {'id': 'https://huggingface.co/papers/2502.19577', 'title': 'Tell me why: Visual foundation models as self-explainable classifiers', 'url': 'https://huggingface.co/papers/2502.19577', 'abstract': 'Visual foundation models (VFMs) have become increasingly popular due to their state-of-the-art performance. However, interpretability remains crucial for critical applications. In this sense, self-explainable models (SEM) aim to provide interpretable classifiers that decompose predictions into a weighted sum of interpretable concepts. Despite their promise, recent studies have shown that these explanations often lack faithfulness. In this work, we combine VFMs with a novel prototypical architecture and specialized training objectives. By training only a lightweight head (approximately 1M parameters) on top of frozen VFMs, our approach (ProtoFM) offers an efficient and interpretable solution. Evaluations demonstrate that our approach achieves competitive classification performance while outperforming existing models across a range of interpretability metrics derived from the literature. Code is available at https://github.com/hturbe/proto-fm.', 'score': 6, 'issue_id': 2493, 'pub_date': '2025-02-26', 'pub_date_card': {'ru': '26 февраля', 'en': 'February 26', 'zh': '2月26日'}, 'hash': '7d2bd5235959eba5', 'authors': ['Hugues Turbé', 'Mina Bjelogrlic', 'Gianmarco Mengaldo', 'Christian Lovis'], 'affiliations': ['Department of Mechanical Engineering, College of Design and Engineering, National University of Singapore, Singapore', 'Department of Radiology and Medical Informatics, University of Geneva, Geneva, Switzerland', 'Division of Medical Information Sciences, Geneva University Hospitals, Geneva, Switzerland'], 'pdf_title_img': 'assets/pdf/title_img/2502.19577.jpg', 'data': {'categories': ['#cv', '#small_models', '#architecture', '#interpretability', '#open_source', '#training', '#optimization'], 'emoji': '🔍', 'ru': {'title': 'ProtoFM: Интерпретируемость визуальных моделей без потери точности', 'desc': 'Статья описывает новый подход к интерпретируемости визуальных моделей-основ (VFM). Авторы предлагают комбинацию VFM с прототипической архитектурой и специализированными целями обучения, названную ProtoFM. Этот метод обучает только легковесную верхушку поверх замороженных VFM, что обеспечивает эффективное и интерпретируемое решение. Эксперименты показывают, что ProtoFM достигает конкурентоспособной точности классификации, превосходя существующие модели по ряду метрик интерпретируемости.'}, 'en': {'title': 'ProtoFM: Interpretable and Efficient Visual Foundation Models', 'desc': 'This paper introduces ProtoFM, a new approach that combines visual foundation models (VFMs) with a prototypical architecture to enhance interpretability in machine learning. The method focuses on creating self-explainable models (SEMs) that break down predictions into understandable components, addressing the issue of faithfulness in explanations. By training a lightweight head on top of frozen VFMs, ProtoFM maintains high classification performance while improving interpretability metrics. The results show that ProtoFM outperforms existing models, making it a promising solution for applications requiring both accuracy and clarity.'}, 'zh': {'title': '高效可解释的视觉基础模型', 'desc': '视觉基础模型（VFM）因其卓越的性能而受到广泛关注，但在关键应用中可解释性仍然至关重要。自解释模型（SEM）旨在提供可解释的分类器，将预测分解为可解释概念的加权和。尽管有潜力，近期研究表明这些解释往往缺乏可信度。我们提出了一种结合VFM和新型原型架构的方案（ProtoFM），通过在冻结的VFM上训练一个轻量级的头部模型，提供了一种高效且可解释的解决方案。'}}}, {'id': 'https://huggingface.co/papers/2502.20969', 'title': 'TeleRAG: Efficient Retrieval-Augmented Generation Inference with Lookahead Retrieval', 'url': 'https://huggingface.co/papers/2502.20969', 'abstract': 'Retrieval-augmented generation (RAG) extends large language models (LLMs) with external data sources to enhance factual correctness and domain coverage. Modern RAG pipelines rely on large datastores, leading to system challenges in latency-sensitive deployments, especially when limited GPU memory is available. To address these challenges, we propose TeleRAG, an efficient inference system that reduces RAG latency with minimal GPU memory requirements. The core innovation of TeleRAG is lookahead retrieval, a prefetching mechanism that anticipates required data and transfers it from CPU to GPU in parallel with LLM generation. By leveraging the modularity of RAG pipelines, the inverted file index (IVF) search algorithm and similarities between queries, TeleRAG optimally overlaps data movement and computation. Experimental results show that TeleRAG reduces end-to-end RAG inference latency by up to 1.72x on average compared to state-of-the-art systems, enabling faster, more memory-efficient deployments of advanced RAG applications.', 'score': 5, 'issue_id': 2498, 'pub_date': '2025-02-28', 'pub_date_card': {'ru': '28 февраля', 'en': 'February 28', 'zh': '2月28日'}, 'hash': '5a85f59eed1c3c3b', 'authors': ['Chien-Yu Lin', 'Keisuke Kamahori', 'Yiyu Liu', 'Xiaoxiang Shi', 'Madhav Kashyap', 'Yile Gu', 'Rulin Shao', 'Zihao Ye', 'Kan Zhu', 'Stephanie Wang', 'Arvind Krishnamurthy', 'Rohan Kadekodi', 'Luis Ceze', 'Baris Kasikci'], 'affiliations': ['Shanghai Jiao Tong University', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2502.20969.jpg', 'data': {'categories': ['#rag', '#inference', '#optimization'], 'emoji': '🚀', 'ru': {'title': 'TeleRAG: Ускорение RAG без компромиссов по памяти', 'desc': 'TeleRAG - это эффективная система вывода, которая снижает задержку RAG при минимальных требованиях к памяти GPU. Основное нововведение TeleRAG - это опережающее извлечение, механизм предварительной выборки, который предвидит необходимые данные и передает их с CPU на GPU параллельно с генерацией LLM. Система использует модульность RAG-конвейеров, алгоритм поиска инвертированного файлового индекса (IVF) и сходства между запросами для оптимального совмещения перемещения данных и вычислений. Экспериментальные результаты показывают, что TeleRAG снижает задержку вывода RAG в среднем до 1,72 раза по сравнению с современными системами.'}, 'en': {'title': 'Speeding Up RAG with TeleRAG: Faster and Smarter Inference!', 'desc': 'This paper introduces TeleRAG, a new system designed to improve the efficiency of retrieval-augmented generation (RAG) in large language models (LLMs). TeleRAG addresses the latency issues that arise from using large datastores, particularly in environments with limited GPU memory. The key feature of TeleRAG is its lookahead retrieval mechanism, which allows data to be pre-fetched from the CPU to the GPU while the LLM is generating responses. Experimental results demonstrate that TeleRAG can significantly reduce inference latency, making RAG applications faster and more efficient.'}, 'zh': {'title': 'TeleRAG：高效的RAG推理系统', 'desc': '检索增强生成（RAG）通过外部数据源扩展大型语言模型（LLM），以提高事实准确性和领域覆盖率。然而，现代RAG管道依赖于大型数据存储，导致在延迟敏感的部署中面临系统挑战，尤其是在GPU内存有限的情况下。为了解决这些问题，我们提出了TeleRAG，这是一种高效的推理系统，能够在最小的GPU内存需求下减少RAG延迟。TeleRAG的核心创新是前瞻性检索，这是一种预取机制，可以在LLM生成的同时，预测所需数据并将其从CPU并行传输到GPU。'}}}, {'id': 'https://huggingface.co/papers/2502.17941', 'title': 'Optimal Brain Apoptosis', 'url': 'https://huggingface.co/papers/2502.17941', 'abstract': 'The increasing complexity and parameter count of Convolutional Neural Networks (CNNs) and Transformers pose challenges in terms of computational efficiency and resource demands. Pruning has been identified as an effective strategy to address these challenges by removing redundant elements such as neurons, channels, or connections, thereby enhancing computational efficiency without heavily compromising performance. This paper builds on the foundational work of Optimal Brain Damage (OBD) by advancing the methodology of parameter importance estimation using the Hessian matrix. Unlike previous approaches that rely on approximations, we introduce Optimal Brain Apoptosis (OBA), a novel pruning method that calculates the Hessian-vector product value directly for each parameter. By decomposing the Hessian matrix across network layers and identifying conditions under which inter-layer Hessian submatrices are non-zero, we propose a highly efficient technique for computing the second-order Taylor expansion of parameters. This approach allows for a more precise pruning process, particularly in the context of CNNs and Transformers, as validated in our experiments including VGG19, ResNet32, ResNet50, and ViT-B/16 on CIFAR10, CIFAR100 and Imagenet datasets. Our code is available at https://github.com/NEU-REAL/OBA.', 'score': 5, 'issue_id': 2495, 'pub_date': '2025-02-25', 'pub_date_card': {'ru': '25 февраля', 'en': 'February 25', 'zh': '2月25日'}, 'hash': '3748780b9de1393e', 'authors': ['Mingyuan Sun', 'Zheng Fang', 'Jiaxu Wang', 'Junjie Jiang', 'Delei Kong', 'Chenming Hu', 'Yuetong Fang', 'Renjing Xu'], 'affiliations': ['Hunan University', 'Northeastern University', 'The Hong Kong University of Science and Technology (Guangzhou)'], 'pdf_title_img': 'assets/pdf/title_img/2502.17941.jpg', 'data': {'categories': ['#optimization', '#inference', '#training', '#architecture'], 'emoji': '✂️', 'ru': {'title': 'Точная обрезка нейронных сетей с помощью прямого расчета Гессиана', 'desc': 'Статья представляет новый метод прунинга нейронных сетей под названием Optimal Brain Apoptosis (OBA). OBA использует прямой расчет произведения Гессиана на вектор для каждого параметра, что позволяет более точно оценивать важность параметров. Метод эффективно применяется к сверточным нейронным сетям и трансформерам, разлагая матрицу Гессиана по слоям сети. Эксперименты на различных архитектурах и датасетах подтверждают эффективность предложенного подхода.'}, 'en': {'title': 'Efficient Neural Network Pruning with Optimal Brain Apoptosis', 'desc': 'This paper addresses the challenges of high computational demands in Convolutional Neural Networks (CNNs) and Transformers by introducing a new pruning method called Optimal Brain Apoptosis (OBA). OBA improves upon previous methods by directly calculating the Hessian-vector product for each parameter, allowing for more accurate estimation of parameter importance. The authors decompose the Hessian matrix across layers to identify non-zero conditions, which enhances the efficiency of the pruning process. Experimental results demonstrate the effectiveness of OBA on various architectures and datasets, confirming its potential to optimize neural networks without significant performance loss.'}, 'zh': {'title': '高效剪枝：最优脑凋亡方法', 'desc': '随着卷积神经网络（CNN）和变换器（Transformers）模型的复杂性和参数数量的增加，计算效率和资源需求面临挑战。剪枝被认为是一种有效的策略，通过去除冗余元素（如神经元、通道或连接）来提高计算效率，而不会严重影响性能。本文在最优脑损伤（OBD）的基础上，提出了一种新的剪枝方法——最优脑凋亡（OBA），通过直接计算每个参数的Hessian-向量乘积值来估计参数的重要性。我们的方法在多个数据集上进行了验证，包括VGG19、ResNet32、ResNet50和ViT-B/16，展示了在CNN和Transformers中的高效剪枝过程。'}}}, {'id': 'https://huggingface.co/papers/2502.19731', 'title': "Preference Learning Unlocks LLMs' Psycho-Counseling Skills", 'url': 'https://huggingface.co/papers/2502.19731', 'abstract': "Applying large language models (LLMs) to assist in psycho-counseling is an emerging and meaningful approach, driven by the significant gap between patient needs and the availability of mental health support. However, current LLMs struggle to consistently provide effective responses to client speeches, largely due to the lack of supervision from high-quality real psycho-counseling data, whose content is typically inaccessible due to client privacy concerns. Furthermore, the quality of therapists' responses in available sessions can vary significantly based on their professional training and experience. Assessing the quality of therapists' responses remains an open challenge. In this work, we address these challenges by first proposing a set of professional and comprehensive principles to evaluate therapists' responses to client speeches. Using these principles, we create a preference dataset, PsychoCounsel-Preference, which contains 36k high-quality preference comparison pairs. This dataset aligns with the preferences of professional psychotherapists, providing a robust foundation for evaluating and improving LLMs in psycho-counseling. Experiments on reward modeling and preference learning demonstrate that PsychoCounsel-Preference is an excellent resource for LLMs to acquire essential skills for responding to clients in a counseling session. Our best-aligned model, PsychoCounsel-Llama3-8B, achieves an impressive win rate of 87% against GPT-4o. We release PsychoCounsel-Preference, PsychoCounsel-Llama3-8B and the reward model PsychoCounsel Llama3-8B-Reward to facilitate the research of psycho-counseling with LLMs at: https://hf.co/Psychotherapy-LLM.", 'score': 4, 'issue_id': 2500, 'pub_date': '2025-02-27', 'pub_date_card': {'ru': '27 февраля', 'en': 'February 27', 'zh': '2月27日'}, 'hash': '533e7e87242a9b9e', 'authors': ['Mian Zhang', 'Shaun M. Eack', 'Zhiyu Zoey Chen'], 'affiliations': ['Department of Computer Science, University of Texas at Dallas', 'School of Social Work, University of Pittsburgh'], 'pdf_title_img': 'assets/pdf/title_img/2502.19731.jpg', 'data': {'categories': ['#open_source', '#alignment', '#healthcare', '#data', '#dataset'], 'emoji': '🧠', 'ru': {'title': 'Большие языковые модели на страже психического здоровья', 'desc': 'Статья представляет новый подход к применению больших языковых моделей (LLM) в психологическом консультировании. Авторы разработали набор принципов для оценки ответов терапевтов и создали датасет PsychoCounsel-Preference, содержащий 36 тысяч пар сравнений высокого качества. На основе этих данных была обучена модель PsychoCounsel-Llama3-8B, которая показала впечатляющие результаты в сравнении с GPT-4. Исследование направлено на преодоление разрыва между потребностями пациентов и доступностью психологической помощи.'}, 'en': {'title': 'Enhancing Psycho-Counseling with LLMs: A New Standard for Therapist Response Evaluation', 'desc': "This paper explores the use of large language models (LLMs) in psycho-counseling, addressing the gap between patient needs and available mental health support. It highlights the challenges faced by LLMs in generating effective responses due to a lack of high-quality training data and variability in therapist responses. To tackle these issues, the authors propose evaluation principles for therapist responses and create a dataset called PsychoCounsel-Preference, which includes 36,000 preference comparison pairs aligned with professional psychotherapists' judgments. The results show that their model, PsychoCounsel-Llama3-8B, significantly outperforms existing models, achieving an 87% win rate against GPT-4o, thus demonstrating the potential of LLMs in enhancing psycho-counseling practices."}, 'zh': {'title': '提升心理咨询的语言模型应用', 'desc': '本研究探讨了大型语言模型（LLMs）在心理咨询中的应用，旨在填补患者需求与心理健康支持之间的差距。由于缺乏高质量的真实心理咨询数据，当前的LLMs在回应客户发言时效果不佳。我们提出了一套专业的评估原则，并创建了包含36,000个高质量偏好比较对的PsychoCounsel-Preference数据集，以帮助评估和改进LLMs在心理咨询中的表现。实验结果表明，PsychoCounsel-Preference为LLMs提供了必要的技能基础，使其在咨询会话中更有效地回应客户。'}}}, {'id': 'https://huggingface.co/papers/2502.20583', 'title': 'LiteASR: Efficient Automatic Speech Recognition with Low-Rank Approximation', 'url': 'https://huggingface.co/papers/2502.20583', 'abstract': "Modern automatic speech recognition (ASR) models, such as OpenAI's Whisper, rely on deep encoder-decoder architectures, and their encoders are a critical bottleneck for efficient deployment due to high computational intensity. We introduce LiteASR, a low-rank compression scheme for ASR encoders that significantly reduces inference costs while maintaining transcription accuracy. Our approach leverages the strong low-rank properties observed in intermediate activations: by applying principal component analysis (PCA) with a small calibration dataset, we approximate linear transformations with a chain of low-rank matrix multiplications, and further optimize self-attention to work in the reduced dimension. Evaluation results show that our method can compress Whisper large-v3's encoder size by over 50%, matching Whisper medium's size with better transcription accuracy, thereby establishing a new Pareto-optimal frontier of efficiency and performance. The code of LiteASR is available at https://github.com/efeslab/LiteASR.", 'score': 4, 'issue_id': 2486, 'pub_date': '2025-02-27', 'pub_date_card': {'ru': '27 февраля', 'en': 'February 27', 'zh': '2月27日'}, 'hash': '3c7268c0881fa426', 'authors': ['Keisuke Kamahori', 'Jungo Kasai', 'Noriyuki Kojima', 'Baris Kasikci'], 'affiliations': ['Kotoba Technologies Inc.', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2502.20583.jpg', 'data': {'categories': ['#inference', '#optimization', '#audio'], 'emoji': '🎙️', 'ru': {'title': 'LiteASR: Эффективное сжатие энкодеров ASR без потери точности', 'desc': 'LiteASR - это метод сжатия энкодеров для систем автоматического распознавания речи (ASR), который значительно уменьшает вычислительные затраты при сохранении точности транскрипции. Метод использует анализ главных компонент (PCA) для аппроксимации линейных преобразований цепочкой умножений матриц низкого ранга. LiteASR позволяет сжать размер энкодера модели Whisper large-v3 более чем на 50%, достигая размера Whisper medium с лучшей точностью транскрипции. Это устанавливает новую Парето-оптимальную границу эффективности и производительности для моделей ASR.'}, 'en': {'title': 'LiteASR: Efficient ASR with Low-Rank Compression', 'desc': "This paper presents LiteASR, a novel low-rank compression technique designed to enhance the efficiency of automatic speech recognition (ASR) models, particularly focusing on the encoder component. By utilizing principal component analysis (PCA) on intermediate activations, LiteASR reduces the computational load during inference while preserving transcription accuracy. The method achieves over 50% reduction in the encoder size of OpenAI's Whisper large-v3 model, aligning its performance with that of the medium version but with improved accuracy. This work sets a new standard for balancing efficiency and performance in ASR systems."}, 'zh': {'title': 'LiteASR：高效的低秩压缩方案', 'desc': '现代自动语音识别（ASR）模型，如OpenAI的Whisper，依赖于深度编码器-解码器架构，而编码器的计算强度是高效部署的瓶颈。我们提出了LiteASR，这是一种针对ASR编码器的低秩压缩方案，能够显著降低推理成本，同时保持转录准确性。我们的方法利用了中间激活中的强低秩特性，通过使用小型校准数据集的主成分分析（PCA），用低秩矩阵乘法链来近似线性变换，并进一步优化自注意力机制以适应降维后的数据。评估结果表明，我们的方法可以将Whisper large-v3的编码器大小压缩超过50%，并在转录准确性上优于Whisper medium，从而建立了效率与性能的新帕累托最优边界。'}}}, {'id': 'https://huggingface.co/papers/2502.20900', 'title': 'DexGraspVLA: A Vision-Language-Action Framework Towards General Dexterous Grasping', 'url': 'https://huggingface.co/papers/2502.20900', 'abstract': "Dexterous grasping remains a fundamental yet challenging problem in robotics. A general-purpose robot must be capable of grasping diverse objects in arbitrary scenarios. However, existing research typically relies on specific assumptions, such as single-object settings or limited environments, leading to constrained generalization. Our solution is DexGraspVLA, a hierarchical framework that utilizes a pre-trained Vision-Language model as the high-level task planner and learns a diffusion-based policy as the low-level Action controller. The key insight lies in iteratively transforming diverse language and visual inputs into domain-invariant representations, where imitation learning can be effectively applied due to the alleviation of domain shift. Thus, it enables robust generalization across a wide range of real-world scenarios. Notably, our method achieves a 90+% success rate under thousands of unseen object, lighting, and background combinations in a ``zero-shot'' environment. Empirical analysis further confirms the consistency of internal model behavior across environmental variations, thereby validating our design and explaining its generalization performance. We hope our work can be a step forward in achieving general dexterous grasping. Our demo and code can be found at https://dexgraspvla.github.io/.", 'score': 3, 'issue_id': 2498, 'pub_date': '2025-02-28', 'pub_date_card': {'ru': '28 февраля', 'en': 'February 28', 'zh': '2月28日'}, 'hash': '5d9b331844235882', 'authors': ['Yifan Zhong', 'Xuchuan Huang', 'Ruochong Li', 'Ceyao Zhang', 'Yitao Liang', 'Yaodong Yang', 'Yuanpei Chen'], 'affiliations': ['Hong Kong University of Science and Technology (Guangzhou)', 'Institute for AI, Peking University', 'PKU-PsiBot Joint Lab'], 'pdf_title_img': 'assets/pdf/title_img/2502.20900.jpg', 'data': {'categories': ['#games', '#optimization', '#robotics', '#diffusion', '#interpretability', '#multimodal'], 'emoji': '🦾', 'ru': {'title': 'Универсальный захват объектов роботами с помощью зрения и языка', 'desc': 'DexGraspVLA - это иерархическая система для роботизированного захвата объектов, использующая предобученную Vision-Language модель в качестве высокоуровневого планировщика задач и обучаемую диффузионную политику в качестве низкоуровневого контроллера действий. Ключевая идея заключается в итеративном преобразовании разнородных языковых и визуальных входных данных в инвариантные к домену представления, что позволяет эффективно применять имитационное обучение. Система демонстрирует надежную генерализацию в широком диапазоне реальных сценариев, достигая более 90% успешных захватов в условиях тысяч невиданных ранее комбинаций объектов, освещения и фонов. Эмпирический анализ подтверждает согласованность внутреннего поведения модели при различных условиях окружающей среды.'}, 'en': {'title': 'Achieving Dexterous Grasping with DexGraspVLA', 'desc': 'This paper presents DexGraspVLA, a novel hierarchical framework designed to improve dexterous grasping in robotics. It combines a pre-trained Vision-Language model for high-level task planning with a diffusion-based policy for low-level action control. The framework effectively transforms diverse language and visual inputs into domain-invariant representations, allowing for better generalization across various real-world scenarios. The results show a success rate of over 90% in grasping tasks involving thousands of unseen objects and conditions, demonstrating the robustness of the approach.'}, 'zh': {'title': '实现灵巧抓取的突破性进展', 'desc': '本论文提出了一种名为DexGraspVLA的层次框架，旨在解决机器人灵巧抓取的问题。该框架利用预训练的视觉-语言模型作为高层任务规划器，并学习基于扩散的策略作为低层动作控制器。通过将多样的语言和视觉输入迭代转换为领域不变的表示，减轻了领域转移的影响，从而有效应用模仿学习。我们的实验表明，该方法在数千种未见物体、光照和背景组合下，在“零-shot”环境中实现了90%以上的成功率，展示了其在真实场景中的强大泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2502.21291', 'title': 'MIGE: A Unified Framework for Multimodal Instruction-Based Image Generation and Editing', 'url': 'https://huggingface.co/papers/2502.21291', 'abstract': 'Despite significant progress in diffusion-based image generation, subject-driven generation and instruction-based editing remain challenging. Existing methods typically treat them separately, struggling with limited high-quality data and poor generalization. However, both tasks require capturing complex visual variations while maintaining consistency between inputs and outputs. Therefore, we propose MIGE, a unified framework that standardizes task representations using multimodal instructions. It treats subject-driven generation as creation on a blank canvas and instruction-based editing as modification of an existing image, establishing a shared input-output formulation. MIGE introduces a novel multimodal encoder that maps free-form multimodal instructions into a unified vision-language space, integrating visual and semantic features through a feature fusion mechanism.This unification enables joint training of both tasks, providing two key advantages: (1) Cross-Task Enhancement: By leveraging shared visual and semantic representations, joint training improves instruction adherence and visual consistency in both subject-driven generation and instruction-based editing. (2) Generalization: Learning in a unified format facilitates cross-task knowledge transfer, enabling MIGE to generalize to novel compositional tasks, including instruction-based subject-driven editing. Experiments show that MIGE excels in both subject-driven generation and instruction-based editing while setting a state-of-the-art in the new task of instruction-based subject-driven editing. Code and model have been publicly available at https://github.com/Eureka-Maggie/MIGE.', 'score': 3, 'issue_id': 2498, 'pub_date': '2025-02-28', 'pub_date_card': {'ru': '28 февраля', 'en': 'February 28', 'zh': '2月28日'}, 'hash': '55451dcf6bfad4a4', 'authors': ['Xueyun Tian', 'Wei Li', 'Bingbing Xu', 'Yige Yuan', 'Yuanzhuo Wang', 'Huawei Shen'], 'affiliations': ['Huawei Shen CAS Key Laboratory of AI Safety, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China', 'University of Chinese Academy of Sciences, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2502.21291.jpg', 'data': {'categories': ['#open_source', '#cv', '#diffusion', '#transfer_learning', '#multimodal'], 'emoji': '🎨', 'ru': {'title': 'Единый подход к генерации и редактированию изображений на основе инструкций', 'desc': 'MIGE - это унифицированная система для генерации изображений на основе инструкций и редактирования существующих изображений. Она использует мультимодальный энкодер для преобразования свободных инструкций в единое визуально-языковое пространство. Совместное обучение обеих задач улучшает соответствие инструкциям и визуальную согласованность. MIGE демонстрирует превосходные результаты в генерации и редактировании изображений, а также устанавливает новый стандарт в задаче редактирования изображений на основе инструкций с учетом субъекта.'}, 'en': {'title': 'MIGE: Unifying Image Generation and Editing with Multimodal Instructions', 'desc': 'This paper presents MIGE, a unified framework for improving both subject-driven image generation and instruction-based editing in machine learning. It addresses the challenges of limited data and poor generalization by standardizing task representations through multimodal instructions. MIGE employs a novel multimodal encoder that integrates visual and semantic features, allowing for joint training of both tasks. The results demonstrate that MIGE enhances instruction adherence and visual consistency while also enabling generalization to new tasks, achieving state-of-the-art performance in instruction-based subject-driven editing.'}, 'zh': {'title': '统一框架，提升图像生成与编辑的能力', 'desc': '尽管扩散基础的图像生成取得了显著进展，但以主题为驱动的生成和基于指令的编辑仍然面临挑战。现有方法通常将这两者分开处理，受限于高质量数据的不足和泛化能力差。我们提出了MIGE，一个统一框架，通过多模态指令标准化任务表示，将主题驱动生成视为在空白画布上的创作，而将基于指令的编辑视为对现有图像的修改。MIGE引入了一种新颖的多模态编码器，将自由形式的多模态指令映射到统一的视觉-语言空间，从而实现了跨任务的增强和更好的泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2502.17125', 'title': 'LettuceDetect: A Hallucination Detection Framework for RAG Applications', 'url': 'https://huggingface.co/papers/2502.17125', 'abstract': "Retrieval Augmented Generation (RAG) systems remain vulnerable to hallucinated answers despite incorporating external knowledge sources. We present LettuceDetect a framework that addresses two critical limitations in existing hallucination detection methods: (1) the context window constraints of traditional encoder-based methods, and (2) the computational inefficiency of LLM based approaches. Building on ModernBERT's extended context capabilities (up to 8k tokens) and trained on the RAGTruth benchmark dataset, our approach outperforms all previous encoder-based models and most prompt-based models, while being approximately 30 times smaller than the best models. LettuceDetect is a token-classification model that processes context-question-answer triples, allowing for the identification of unsupported claims at the token level. Evaluations on the RAGTruth corpus demonstrate an F1 score of 79.22% for example-level detection, which is a 14.8% improvement over Luna, the previous state-of-the-art encoder-based architecture. Additionally, the system can process 30 to 60 examples per second on a single GPU, making it more practical for real-world RAG applications.", 'score': 3, 'issue_id': 2497, 'pub_date': '2025-02-24', 'pub_date_card': {'ru': '24 февраля', 'en': 'February 24', 'zh': '2月24日'}, 'hash': '347ebb871884d940', 'authors': ['Ádám Kovács', 'Gábor Recski'], 'affiliations': ['KR Labs', 'TU Wien'], 'pdf_title_img': 'assets/pdf/title_img/2502.17125.jpg', 'data': {'categories': ['#benchmark', '#rag', '#hallucinations', '#architecture', '#small_models', '#long_context'], 'emoji': '🥬', 'ru': {'title': 'LettuceDetect: Эффективное обнаружение галлюцинаций в RAG-системах', 'desc': 'LettuceDetect - это новая система для обнаружения галлюцинаций в генеративных моделях с извлечением информации (RAG). Она использует архитектуру ModernBERT с расширенным контекстным окном до 8000 токенов и обучена на датасете RAGTruth. LettuceDetect превосходит предыдущие модели на основе энкодеров и большинство моделей на основе промптов, будучи при этом в 30 раз меньше. Система демонстрирует F1-score 79.22% для обнаружения на уровне примеров, что на 14.8% лучше предыдущего SOTA-решения Luna.'}, 'en': {'title': 'LettuceDetect: Efficient Hallucination Detection for RAG Systems', 'desc': "This paper introduces LettuceDetect, a novel framework designed to improve the detection of hallucinated answers in Retrieval Augmented Generation (RAG) systems. It addresses limitations of existing methods by utilizing ModernBERT's extended context capabilities and a token-classification approach to analyze context-question-answer triples. LettuceDetect achieves a significant F1 score of 79.22% on the RAGTruth dataset, outperforming previous models while being much smaller and more efficient. The system's ability to process 30 to 60 examples per second on a single GPU enhances its practicality for real-world applications."}, 'zh': {'title': '提升幻觉检测的效率与准确性', 'desc': '本论文介绍了一种名为LettuceDetect的框架，旨在解决现有幻觉检测方法的两个主要限制。首先，它克服了传统编码器方法的上下文窗口限制，其次，它提高了基于大型语言模型（LLM）方法的计算效率。LettuceDetect基于ModernBERT，能够处理多达8000个标记，并在RAGTruth基准数据集上进行训练，表现优于所有先前的编码器模型和大多数基于提示的模型。该系统在RAGTruth语料库上的F1得分达到79.22%，并且在单个GPU上每秒可处理30到60个示例，适用于实际的RAG应用。'}}}, {'id': 'https://huggingface.co/papers/2502.20490', 'title': 'EgoNormia: Benchmarking Physical Social Norm Understanding', 'url': 'https://huggingface.co/papers/2502.20490', 'abstract': 'Human activity is moderated by norms. When performing actions in the real world, humans not only follow norms, but also consider the trade-off between different norms However, machines are often trained without explicit supervision on norm understanding and reasoning, especially when the norms are grounded in a physical and social context. To improve and evaluate the normative reasoning capability of vision-language models (VLMs), we present EgoNormia |epsilon|, consisting of 1,853 ego-centric videos of human interactions, each of which has two related questions evaluating both the prediction and justification of normative actions. The normative actions encompass seven categories: safety, privacy, proxemics, politeness, cooperation, coordination/proactivity, and communication/legibility. To compile this dataset at scale, we propose a novel pipeline leveraging video sampling, automatic answer generation, filtering, and human validation. Our work demonstrates that current state-of-the-art vision-language models lack robust norm understanding, scoring a maximum of 45% on EgoNormia (versus a human bench of 92%). Our analysis of performance in each dimension highlights the significant risks of safety, privacy, and the lack of collaboration and communication capability when applied to real-world agents. We additionally show that through a retrieval-based generation method, it is possible to use EgoNomia to enhance normative reasoning in VLMs.', 'score': 2, 'issue_id': 2499, 'pub_date': '2025-02-27', 'pub_date_card': {'ru': '27 февраля', 'en': 'February 27', 'zh': '2月27日'}, 'hash': 'a2c7525ed78fb0ce', 'authors': ['MohammadHossein Rezaei', 'Yicheng Fu', 'Phil Cuvin', 'Caleb Ziems', 'Yanzhe Zhang', 'Hao Zhu', 'Diyi Yang'], 'affiliations': ['Georgia Tech', 'Stanford University', 'University of Arizona', 'University of Toronto'], 'pdf_title_img': 'assets/pdf/title_img/2502.20490.jpg', 'data': {'categories': ['#reasoning', '#multimodal', '#data', '#dataset', '#benchmark', '#ethics'], 'emoji': '🤖', 'ru': {'title': 'EgoNormia: повышение этического интеллекта ИИ через эгоцентрические видео', 'desc': 'Статья представляет EgoNormia |epsilon| - набор данных из 1853 эгоцентрических видео человеческих взаимодействий для оценки нормативного рассуждения моделей компьютерного зрения и обработки естественного языка (VLM). Каждое видео сопровождается двумя вопросами, оценивающими предсказание и обоснование нормативных действий в семи категориях, включая безопасность, конфиденциальность и сотрудничество. Исследование показывает, что современные VLM недостаточно понимают нормы, набирая максимум 45% на EgoNormia по сравнению с 92% у людей. Авторы предлагают метод генерации на основе поиска для улучшения нормативного рассуждения в VLM с помощью EgoNomia.'}, 'en': {'title': 'Enhancing Norm Understanding in Machines with EgoNormia', 'desc': "This paper introduces EgoNormia, a dataset designed to enhance the normative reasoning abilities of vision-language models (VLMs) by providing 1,853 ego-centric videos that depict human interactions. Each video is accompanied by questions that assess the model's ability to predict and justify normative actions across seven categories, including safety and privacy. The authors highlight that current VLMs perform poorly on this task, achieving only 45% accuracy compared to a human benchmark of 92%. They also propose a novel pipeline for dataset creation and demonstrate that using EgoNormia can improve the normative reasoning capabilities of VLMs through a retrieval-based generation method."}, 'zh': {'title': '提升机器的规范推理能力', 'desc': '本论文探讨了人类在活动中如何遵循社会规范，并提出了一个名为EgoNormia的数据集，以评估视觉-语言模型（VLMs）在规范推理方面的能力。该数据集包含1853个以自我为中心的人类互动视频，并通过相关问题来评估模型对规范行为的预测和解释。研究发现，当前的最先进VLM在规范理解方面表现不佳，最高得分仅为45%，远低于人类的92%。此外，论文还提出了一种基于检索的生成方法，能够利用EgoNormia提升VLM的规范推理能力。'}}}, {'id': 'https://huggingface.co/papers/2502.20811', 'title': 'HAIC: Improving Human Action Understanding and Generation with Better Captions for Multi-modal Large Language Models', 'url': 'https://huggingface.co/papers/2502.20811', 'abstract': 'Recent Multi-modal Large Language Models (MLLMs) have made great progress in video understanding. However, their performance on videos involving human actions is still limited by the lack of high-quality data. To address this, we introduce a two-stage data annotation pipeline. First, we design strategies to accumulate videos featuring clear human actions from the Internet. Second, videos are annotated in a standardized caption format that uses human attributes to distinguish individuals and chronologically details their actions and interactions. Through this pipeline, we curate two datasets, namely HAICTrain and HAICBench. HAICTrain comprises 126K video-caption pairs generated by Gemini-Pro and verified for training purposes. Meanwhile, HAICBench includes 500 manually annotated video-caption pairs and 1,400 QA pairs, for a comprehensive evaluation of human action understanding. Experimental results demonstrate that training with HAICTrain not only significantly enhances human understanding abilities across 4 benchmarks, but can also improve text-to-video generation results. Both the HAICTrain and HAICBench are released at https://huggingface.co/datasets/KuaishouHAIC/HAIC.', 'score': 1, 'issue_id': 2486, 'pub_date': '2025-02-28', 'pub_date_card': {'ru': '28 февраля', 'en': 'February 28', 'zh': '2月28日'}, 'hash': '806f6aacd5ee2f8a', 'authors': ['Xiao Wang', 'Jingyun Hua', 'Weihong Lin', 'Yuanxing Zhang', 'Fuzheng Zhang', 'Jianlong Wu', 'Di Zhang', 'Liqiang Nie'], 'affiliations': ['Kuaishou Technology'], 'pdf_title_img': 'assets/pdf/title_img/2502.20811.jpg', 'data': {'categories': ['#dataset', '#data', '#video', '#multimodal', '#benchmark', '#synthetic'], 'emoji': '🎬', 'ru': {'title': 'Улучшение понимания человеческих действий в видео с помощью аннотированных данных', 'desc': 'Статья представляет новый подход к улучшению понимания видео с человеческими действиями для мультимодальных больших языковых моделей (MLLM). Авторы разработали двухэтапный процесс аннотации данных, включающий сбор релевантных видео и их стандартизированное описание. В результате были созданы два набора данных: HAICTrain для обучения и HAICBench для оценки моделей. Эксперименты показали, что обучение на HAICTrain значительно улучшает способности моделей понимать человеческие действия на видео.'}, 'en': {'title': 'Enhancing Video Understanding with Curated Human Action Datasets', 'desc': 'This paper presents a solution to improve video understanding in Multi-modal Large Language Models (MLLMs) by addressing the scarcity of high-quality data on human actions. The authors introduce a two-stage data annotation pipeline that first collects videos with clear human actions from the Internet and then annotates them using a standardized caption format. This results in two curated datasets: HAICTrain, which contains 126K video-caption pairs for training, and HAICBench, which includes 500 annotated pairs for evaluation. Experimental results show that using HAICTrain significantly boosts human action understanding and enhances text-to-video generation capabilities across multiple benchmarks.'}, 'zh': {'title': '提升视频理解的创新数据标注流程', 'desc': '最近的多模态大型语言模型（MLLMs）在视频理解方面取得了显著进展。然而，它们在涉及人类动作的视频上的表现仍然受到高质量数据缺乏的限制。为了解决这个问题，我们提出了一个两阶段的数据标注流程，首先从互联网收集包含清晰人类动作的视频，然后使用标准化的字幕格式对视频进行标注。通过这个流程，我们创建了两个数据集HAICTrain和HAICBench，实验结果表明，使用HAICTrain进行训练显著提升了人类动作理解能力，并改善了文本到视频生成的效果。'}}}, {'id': 'https://huggingface.co/papers/2503.07920', 'title': 'Crowdsource, Crawl, or Generate? Creating SEA-VL, a Multicultural\n  Vision-Language Dataset for Southeast Asia', 'url': 'https://huggingface.co/papers/2503.07920', 'abstract': 'Southeast Asia (SEA) is a region of extraordinary linguistic and cultural diversity, yet it remains significantly underrepresented in vision-language (VL) research. This often results in artificial intelligence (AI) models that fail to capture SEA cultural nuances. To fill this gap, we present SEA-VL, an open-source initiative dedicated to developing high-quality, culturally relevant data for SEA languages. By involving contributors from SEA countries, SEA-VL aims to ensure better cultural relevance and diversity, fostering greater inclusivity of underrepresented languages in VL research. Beyond crowdsourcing, our initiative goes one step further in the exploration of the automatic collection of culturally relevant images through crawling and image generation. First, we find that image crawling achieves approximately ~85% cultural relevance while being more cost- and time-efficient than crowdsourcing. Second, despite the substantial progress in generative vision models, synthetic images remain unreliable in accurately reflecting SEA cultures. The generated images often fail to reflect the nuanced traditions and cultural contexts of the region. Collectively, we gather 1.28M SEA culturally-relevant images, more than 50 times larger than other existing datasets. Through SEA-VL, we aim to bridge the representation gap in SEA, fostering the development of more inclusive AI systems that authentically represent diverse cultures across SEA.', 'score': 73, 'issue_id': 2654, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '32c690b6ffb8b143', 'authors': ['Samuel Cahyawijaya', 'Holy Lovenia', 'Joel Ruben Antony Moniz', 'Tack Hwa Wong', 'Mohammad Rifqi Farhansyah', 'Thant Thiri Maung', 'Frederikus Hudi', 'David Anugraha', 'Muhammad Ravi Shulthan Habibi', 'Muhammad Reza Qorib', 'Amit Agarwal', 'Joseph Marvin Imperial', 'Hitesh Laxmichand Patel', 'Vicky Feliren', 'Bahrul Ilmi Nasution', 'Manuel Antonio Rufino', 'Genta Indra Winata', 'Rian Adam Rajagede', 'Carlos Rafael Catalan', 'Mohamed Fazli Imam', 'Priyaranjan Pattnayak', 'Salsabila Zahirah Pranida', 'Kevin Pratama', 'Yeshil Bangera', 'Adisai Na-Thalang', 'Patricia Nicole Monderin', 'Yueqi Song', 'Christian Simon', 'Lynnette Hui Xian Ng', "Richardy Lobo' Sapan", 'Taki Hasan Rafi', 'Bin Wang', 'Supryadi', 'Kanyakorn Veerakanjana', 'Piyalitt Ittichaiwong', 'Matthew Theodore Roque', 'Karissa Vincentio', 'Takdanai Kreangphet', 'Phakphum Artkaew', 'Kadek Hendrawan Palgunadi', 'Yanzhi Yu', 'Rochana Prih Hastuti', 'William Nixon', 'Mithil Bangera', 'Adrian Xuan Wei Lim', 'Aye Hninn Khine', 'Hanif Muhammad Zhafran', 'Teddy Ferdinan', 'Audra Aurora Izzani', 'Ayushman Singh', 'Evan', 'Jauza Akbar Krito', 'Michael Anugraha', 'Fenal Ashokbhai Ilasariya', 'Haochen Li', 'John Amadeo Daniswara', 'Filbert Aurelian Tjiaranata', 'Eryawan Presma Yulianrifat', 'Can Udomcharoenchaikit', 'Fadil Risdian Ansori', 'Mahardika Krisna Ihsani', 'Giang Nguyen', 'Anab Maulana Barik', 'Dan John Velasco', 'Rifo Ahmad Genadi', 'Saptarshi Saha', 'Chengwei Wei', 'Isaiah Flores', 'Kenneth Ko Han Chen', 'Anjela Gail Santos', 'Wan Shen Lim', 'Kaung Si Phyo', 'Tim Santos', 'Meisyarah Dwiastuti', 'Jiayun Luo', 'Jan Christian Blaise Cruz', 'Ming Shan Hee', 'Ikhlasul Akmal Hanif', 'M. Alif Al Hakim', "Muhammad Rizky Sya'ban", 'Kun Kerdthaisong', 'Lester James V. Miranda', 'Fajri Koto', 'Tirana Noor Fatyanosa', 'Alham Fikri Aji', 'Jostin Jerico Rosal', 'Jun Kevin', 'Robert Wijaya', 'Onno P. Kampman', 'Ruochen Zhang', 'Börje F. Karlsson', 'Peerat Limkonchotiwat'], 'affiliations': ['AI Singapore', 'Allen AI', 'Ateneo de Manila University', 'Auburn University', 'Bandung Institute of Technology', 'Beijing Academy of Artificial Intelligence (BAAI)', 'Binus University', 'Brawijaya University', 'Brown University', 'Capital One', 'Carnegie Mellon University', 'Chulalongkorn University', 'Cohere', 'Dataxet:Sonar', 'Faculty of Medicine Siriraj Hospital, Mahidol University', 'Graphcore', 'Hanyang University', 'Independent', 'Indian Statistical Institute, Kolkata', 'IndoNLP', 'Institut Teknologi Sepuluh Nopember', 'Institute for Infocomm Research, Singapore', 'King Mongkuts University of Technology Thonburi', 'MBZUAI', 'MOH Office for Healthcare Transformation', 'Macau University of Science and Technology', 'Meta', 'Mila - Quebec AI Institute', 'Monash University, Indonesia', 'Nara Institute of Science and Technology', 'National University Philippines', 'National University of Singapore', 'New York University', 'Oracle', 'Polytechnique Montreal', 'SCB 10X', 'SEACrowd', 'Samsung R&D Institute Philippines', 'Seoul National University of Science and Technology', 'Singapore Polytechnic', 'Singapore University of Technology and Design', 'Sony Group Corporation', 'Srinakharinwirot University', 'Thammasat University', 'The University of Manchester', 'Tianjin University', 'Ton Duc Thang University', 'Universitas Gadjah Mada', 'Universitas Islam Indonesia', 'Universitas Pelita Harapan', 'University of Bath', 'University of Illiinois, Urbana-Champaign', 'University of Indonesia', 'University of New Haven', 'University of Toronto', 'University of the Philippines', 'Vidyasirimedhi Institute of Science and Technology', 'Works Applications', 'Wrocław Tech'], 'pdf_title_img': 'assets/pdf/title_img/2503.07920.jpg', 'data': {'categories': ['#synthetic', '#dataset', '#open_source', '#data', '#multimodal'], 'emoji': '🌏', 'ru': {'title': 'Преодоление культурного разрыва в ИИ для Юго-Восточной Азии', 'desc': 'Статья представляет SEA-VL - инициативу по созданию качественных данных для языков Юго-Восточной Азии в области vision-language моделей. Проект направлен на улучшение культурной релевантности и разнообразия в исследованиях ИИ. Авторы изучают методы автоматического сбора культурно значимых изображений через краулинг и генерацию изображений. В результате собрано 1,28 млн культурно релевантных изображений, что в 50 раз больше существующих датасетов.'}, 'en': {'title': 'Bridging the Cultural Gap in AI with SEA-VL', 'desc': "The paper introduces SEA-VL, an initiative aimed at enhancing representation of Southeast Asian cultures in vision-language (VL) research. It highlights the creation of a large dataset containing 1.28 million culturally relevant images, which is significantly larger than existing datasets. The initiative combines crowdsourcing with automated image crawling, achieving high cultural relevance efficiently. However, it also notes challenges with generative models, as synthetic images often fail to accurately depict the region's diverse cultural nuances."}, 'zh': {'title': '填补东南亚文化在AI研究中的空白', 'desc': '东南亚地区语言和文化多样性极为丰富，但在视觉语言研究中却严重缺乏代表性。为了解决这一问题，我们提出了SEA-VL，这是一个开放源代码的项目，旨在为东南亚语言开发高质量、文化相关的数据。通过吸引来自东南亚国家的贡献者，SEA-VL确保了更好的文化相关性和多样性，促进了在视觉语言研究中对被低估语言的包容性。我们收集了128万张与东南亚文化相关的图像，远超现有数据集的规模，旨在缩小东南亚的代表性差距，推动更具包容性的人工智能系统的发展。'}}}, {'id': 'https://huggingface.co/papers/2503.07536', 'title': 'LMM-R1: Empowering 3B LMMs with Strong Reasoning Abilities Through\n  Two-Stage Rule-Based RL', 'url': 'https://huggingface.co/papers/2503.07536', 'abstract': 'Enhancing reasoning in Large Multimodal Models (LMMs) faces unique challenges from the complex interplay between visual perception and logical reasoning, particularly in compact 3B-parameter architectures where architectural constraints limit reasoning capacity and modality alignment.   While rule-based reinforcement learning (RL) excels in text-only domains, its multimodal extension confronts two critical barriers: (1) data limitations due to ambiguous answers and scarce complex reasoning examples, and (2) degraded foundational reasoning induced by multimodal pretraining.   To address these challenges, we propose \\method, a two-stage framework adapting rule-based RL for multimodal reasoning through Foundational Reasoning Enhancement (FRE) followed by Multimodal Generalization Training (MGT). The FRE stage first strengthens reasoning abilities using text-only data with rule-based RL, then the MGT stage generalizes these reasoning capabilities to multimodal domains.   Experiments on Qwen2.5-VL-Instruct-3B demonstrate that \\method achieves 4.83\\% and 4.5\\% average improvements over baselines in multimodal and text-only benchmarks, respectively, with a 3.63\\% gain in complex Football Game tasks. These results validate that text-based reasoning enhancement enables effective multimodal generalization, offering a data-efficient paradigm that bypasses costly high-quality multimodal training data.', 'score': 50, 'issue_id': 2654, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '59c304598f64f1e6', 'authors': ['Yingzhe Peng', 'Gongrui Zhang', 'Miaosen Zhang', 'Zhiyuan You', 'Jie Liu', 'Qipeng Zhu', 'Kai Yang', 'Xingzhong Xu', 'Xin Geng', 'Xu Yang'], 'affiliations': ['Ant Group', 'Fudan University', 'Key Laboratory of New Generation Artificial Intelligence Technology and Its Interdisciplinary Applications (Southeast University), Ministry of Education, China', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2503.07536.jpg', 'data': {'categories': ['#reasoning', '#training', '#rl', '#benchmark', '#small_models', '#multimodal'], 'emoji': '🧠', 'ru': {'title': 'Усиление рассуждений в мультимодальных моделях через обучение на текстах', 'desc': 'Статья представляет метод улучшения рассуждений в больших мультимодальных моделях (LMM) с архитектурой 3B-параметров. Авторы предлагают двухэтапный подход: сначала усиление базовых способностей рассуждения с помощью обучения с подкреплением на текстовых данных, затем обобщение этих навыков на мультимодальные задачи. Эксперименты на модели Qwen2.5-VL-Instruct-3B показывают значительное улучшение производительности как в мультимодальных, так и в текстовых тестах. Метод позволяет эффективно обучать мультимодальные модели без необходимости в больших объемах качественных мультимодальных данных.'}, 'en': {'title': 'Boosting Reasoning in Multimodal Models Efficiently', 'desc': 'This paper addresses the challenges of improving reasoning in Large Multimodal Models (LMMs), particularly in smaller architectures with limited parameters. It identifies two main issues: the lack of sufficient data for complex reasoning in multimodal contexts and the negative impact of multimodal pretraining on foundational reasoning. The authors propose a two-stage framework that first enhances reasoning using rule-based reinforcement learning on text-only data, followed by training to generalize these skills to multimodal scenarios. Experimental results show significant improvements in reasoning performance across both multimodal and text-only tasks, demonstrating the effectiveness of their approach in reducing the need for extensive multimodal training data.'}, 'zh': {'title': '增强多模态推理能力的高效方法', 'desc': '本论文探讨了在大型多模态模型（LMMs）中增强推理能力的挑战，特别是在参数量为3B的紧凑架构中，视觉感知与逻辑推理之间的复杂互动。我们提出了一种名为\textit{method}的两阶段框架，通过基础推理增强（FRE）和多模态泛化训练（MGT）来适应多模态推理。FRE阶段利用基于规则的强化学习（RL）增强文本数据的推理能力，MGT阶段则将这些推理能力推广到多模态领域。实验结果表明，\textit{method}在多模态和文本基准测试中分别比基线提高了4.83%和4.5%，在复杂的足球比赛任务中提高了3.63%。'}}}, {'id': 'https://huggingface.co/papers/2503.08638', 'title': 'YuE: Scaling Open Foundation Models for Long-Form Music Generation', 'url': 'https://huggingface.co/papers/2503.08638', 'abstract': "We tackle the task of long-form music generation--particularly the challenging lyrics-to-song problem--by introducing YuE, a family of open foundation models based on the LLaMA2 architecture. Specifically, YuE scales to trillions of tokens and generates up to five minutes of music while maintaining lyrical alignment, coherent musical structure, and engaging vocal melodies with appropriate accompaniment. It achieves this through (1) track-decoupled next-token prediction to overcome dense mixture signals, (2) structural progressive conditioning for long-context lyrical alignment, and (3) a multitask, multiphase pre-training recipe to converge and generalize. In addition, we redesign the in-context learning technique for music generation, enabling versatile style transfer (e.g., converting Japanese city pop into an English rap while preserving the original accompaniment) and bidirectional generation. Through extensive evaluation, we demonstrate that YuE matches or even surpasses some of the proprietary systems in musicality and vocal agility. In addition, fine-tuning YuE enables additional controls and enhanced support for tail languages. Furthermore, beyond generation, we show that YuE's learned representations can perform well on music understanding tasks, where the results of YuE match or exceed state-of-the-art methods on the MARBLE benchmark. Keywords: lyrics2song, song generation, long-form, foundation model, music generation", 'score': 46, 'issue_id': 2655, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 марта', 'en': 'March 11', 'zh': '3月11日'}, 'hash': 'eb1539380bf435c9', 'authors': ['Ruibin Yuan', 'Hanfeng Lin', 'Shuyue Guo', 'Ge Zhang', 'Jiahao Pan', 'Yongyi Zang', 'Haohe Liu', 'Yiming Liang', 'Wenye Ma', 'Xingjian Du', 'Xinrun Du', 'Zhen Ye', 'Tianyu Zheng', 'Yinghao Ma', 'Minghao Liu', 'Zeyue Tian', 'Ziya Zhou', 'Liumeng Xue', 'Xingwei Qu', 'Yizhi Li', 'Shangda Wu', 'Tianhao Shen', 'Ziyang Ma', 'Jun Zhan', 'Chunhui Wang', 'Yatian Wang', 'Xiaowei Chi', 'Xinyue Zhang', 'Zhenzhu Yang', 'Xiangzhou Wang', 'Shansong Liu', 'Lingrui Mei', 'Peng Li', 'Junjie Wang', 'Jianwei Yu', 'Guojian Pang', 'Xu Li', 'Zihao Wang', 'Xiaohuan Zhou', 'Lijun Yu', 'Emmanouil Benetos', 'Yong Chen', 'Chenghua Lin', 'Xie Chen', 'Gus Xia', 'Zhaoxiang Zhang', 'Chao Zhang', 'Wenhu Chen', 'Xinyu Zhou', 'Xipeng Qiu', 'Roger Dannenberg', 'Jiaheng Liu', 'Jian Yang', 'Wenhao Huang', 'Wei Xue', 'Xu Tan', 'Yike Guo'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2503.08638.jpg', 'data': {'categories': ['#training', '#multimodal', '#long_context', '#benchmark', '#low_resource', '#open_source', '#story_generation', '#audio'], 'emoji': '🎵', 'ru': {'title': 'YuE: Открытая модель для генерации длинных музыкальных композиций на основе текста', 'desc': 'YuE - это семейство открытых моделей для генерации музыки на основе архитектуры LLaMA2. Модель способна генерировать до пяти минут музыки, сохраняя согласованность с текстом песни, музыкальную структуру и вокальные мелодии с аккомпанементом. YuE использует несколько инновационных техник, включая предсказание следующего токена с разделением треков и структурное прогрессивное кондиционирование. Модель также может выполнять перенос стиля и двунаправленную генерацию, а её представления эффективны для задач понимания музыки.'}, 'en': {'title': 'Transforming Lyrics into Melodies with YuE!', 'desc': 'This paper presents YuE, a new family of open foundation models designed for long-form music generation, specifically focusing on the lyrics-to-song challenge. YuE utilizes the LLaMA2 architecture and can generate up to five minutes of music while ensuring that the lyrics align with the musical structure and melodies. Key innovations include track-decoupled next-token prediction for better signal clarity, structural progressive conditioning for lyrical alignment, and a multitask pre-training approach to enhance generalization. The model also supports versatile style transfer and performs well on music understanding tasks, achieving results that rival existing proprietary systems.'}, 'zh': {'title': 'YuE：歌词转歌曲的音乐生成新突破', 'desc': '本文介绍了一种名为YuE的开放基础模型，专注于长篇音乐生成，特别是歌词转歌曲的问题。YuE基于LLaMA2架构，能够生成长达五分钟的音乐，同时保持歌词的对齐、连贯的音乐结构和引人入胜的旋律。通过采用解耦的下一个标记预测、结构性渐进条件和多任务预训练等技术，YuE在音乐生成中表现出色。经过评估，YuE在音乐性和声乐灵活性方面与一些专有系统相匹敌，甚至超越了它们。'}}}, {'id': 'https://huggingface.co/papers/2503.05978', 'title': 'MagicInfinite: Generating Infinite Talking Videos with Your Words and\n  Voice', 'url': 'https://huggingface.co/papers/2503.05978', 'abstract': "We present MagicInfinite, a novel diffusion Transformer (DiT) framework that overcomes traditional portrait animation limitations, delivering high-fidelity results across diverse character types-realistic humans, full-body figures, and stylized anime characters. It supports varied facial poses, including back-facing views, and animates single or multiple characters with input masks for precise speaker designation in multi-character scenes. Our approach tackles key challenges with three innovations: (1) 3D full-attention mechanisms with a sliding window denoising strategy, enabling infinite video generation with temporal coherence and visual quality across diverse character styles; (2) a two-stage curriculum learning scheme, integrating audio for lip sync, text for expressive dynamics, and reference images for <PRE_TAG>identity preservation</POST_TAG>, enabling flexible multi-modal control over long sequences; and (3) region-specific masks with adaptive loss functions to balance global textual control and local audio guidance, supporting speaker-specific animations. Efficiency is enhanced via our innovative unified step and cfg distillation techniques, achieving a 20x inference speed boost over the basemodel: generating a 10 second 540x540p video in 10 seconds or 720x720p in 30 seconds on 8 H100 GPUs, without quality loss. Evaluations on our new benchmark demonstrate MagicInfinite's superiority in audio-lip synchronization, identity preservation, and motion naturalness across diverse scenarios. It is publicly available at https://www.hedra.com/, with examples at https://magicinfinite.github.io/.", 'score': 26, 'issue_id': 2658, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 марта', 'en': 'March 7', 'zh': '3月7日'}, 'hash': '2111564f7e67ca23', 'authors': ['Hongwei Yi', 'Tian Ye', 'Shitong Shao', 'Xuancheng Yang', 'Jiantong Zhao', 'Hanzhong Guo', 'Terrance Wang', 'Qingyu Yin', 'Zeke Xie', 'Lei Zhu', 'Wei Li', 'Michael Lingelbach', 'Daquan Zhou'], 'affiliations': ['HKU', 'HKUST(GZ)', 'Hedra Inc.', 'Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2503.05978.jpg', 'data': {'categories': ['#benchmark', '#multimodal', '#inference', '#diffusion', '#video', '#open_source'], 'emoji': '🎭', 'ru': {'title': 'MagicInfinite: революция в анимации портретов с помощью ИИ', 'desc': 'MagicInfinite - это новая система на основе диффузионного трансформера для анимации портретов. Она способна создавать высококачественные анимации для разных типов персонажей, включая реалистичных людей и аниме-персонажей. Система использует 3D механизм полного внимания, двухэтапное обучение с учителем и регион-специфичные маски для улучшения качества и контроля анимации. MagicInfinite демонстрирует превосходные результаты в синхронизации губ с аудио, сохранении идентичности и естественности движений.'}, 'en': {'title': 'Revolutionizing Portrait Animation with MagicInfinite', 'desc': 'MagicInfinite is a cutting-edge diffusion Transformer framework designed to enhance portrait animation by producing high-quality results for various character types, including realistic humans and stylized anime. It introduces innovative techniques such as 3D full-attention mechanisms and a sliding window denoising strategy, allowing for infinite video generation while maintaining visual coherence. The framework employs a two-stage curriculum learning approach that integrates audio and text inputs for improved lip synchronization and expressive dynamics, along with region-specific masks for precise control over animations. With a significant boost in efficiency, MagicInfinite can generate high-resolution videos rapidly, outperforming existing models in key areas like audio-lip synchronization and identity preservation.'}, 'zh': {'title': '魔法无限：突破肖像动画的界限', 'desc': 'MagicInfinite是一种新型的扩散变换器框架，旨在克服传统肖像动画的局限性，能够在多种角色类型中提供高保真度的结果，包括真实人类、全身人物和风格化的动漫角色。该框架支持多种面部姿势的动画，包括背面视图，并能够通过输入掩码精确指定多角色场景中的发言者。我们的方法通过三项创新来解决关键挑战：3D全注意力机制与滑动窗口去噪策略，支持无限视频生成并保持时间一致性和视觉质量；以及区域特定掩码与自适应损失函数的结合，平衡全局文本控制和局部音频指导。我们的评估表明，MagicInfinite在音频与唇同步、身份保留和动作自然性方面优于其他方法。'}}}, {'id': 'https://huggingface.co/papers/2503.08120', 'title': 'UniF^2ace: Fine-grained Face Understanding and Generation\n  with Unified Multimodal Models', 'url': 'https://huggingface.co/papers/2503.08120', 'abstract': "Unified multimodal models (UMMs) have emerged as a powerful paradigm in foundational computer vision research, demonstrating significant potential in both image understanding and generation. However, existing research in the face domain primarily focuses on coarse facial attribute understanding, with limited capacity to handle fine-grained facial attributes and without addressing generation capabilities. To overcome these limitations, we propose UniF^2ace, the first UMM tailored specifically for fine-grained face understanding and generation. In general, we train UniF^2ace on a self-constructed, specialized dataset utilizing two mutually beneficial diffusion techniques and a two-level mixture-of-experts architecture. Specifically, we first build a large-scale facial dataset, UniF^2ace-130K, which contains 130K image-text pairs with one million question-answering pairs that span a wide range of facial attributes. Second, we establish a theoretical connection between discrete diffusion score matching and masked generative models, optimizing both evidence lower bounds simultaneously, which significantly improves the model's ability to synthesize facial details. Finally, we introduce both token-level and sequence-level mixture-of-experts, enabling efficient fine-grained representation learning for both understanding and generation tasks. Extensive experiments on UniF^2ace-130K demonstrate that UniF^2ace outperforms existing UMMs and generative models, achieving superior performance across both understanding and generation tasks.", 'score': 25, 'issue_id': 2654, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 марта', 'en': 'March 11', 'zh': '3月11日'}, 'hash': '7c6e7c685b283d61', 'authors': ['Junzhe Li', 'Xuerui Qiu', 'Linrui Xu', 'Liya Guo', 'Delin Qu', 'Tingting Long', 'Chun Fan', 'Ming Li'], 'affiliations': ['Central South University', 'Computer Center, Peking University', 'Fudan University', 'Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ)', 'Institute of Automation, Chinese Academy of Sciences', 'School of Computer Science, Peking University', 'Yau Mathematical Sciences Center and Department of Mathematical Sciences, Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.08120.jpg', 'data': {'categories': ['#synthetic', '#cv', '#dataset', '#multimodal', '#architecture', '#diffusion'], 'emoji': '🧑', 'ru': {'title': 'UniF^2ace: Новый уровень в понимании и генерации лиц', 'desc': 'UniF^2ace - это новая унифицированная мультимодальная модель (UMM), специально разработанная для детального понимания и генерации лиц. Модель обучена на большом специализированном наборе данных, содержащем 130 тысяч пар изображений и текстов с миллионом пар вопросов-ответов о различных атрибутах лица. В UniF^2ace применяются две взаимодополняющие техники диффузии и двухуровневая архитектура смеси экспертов для эффективного обучения представлений. Эксперименты показывают, что UniF^2ace превосходит существующие UMM и генеративные модели в задачах понимания и генерации лиц.'}, 'en': {'title': 'UniF^2ace: Mastering Fine-Grained Facial Understanding and Generation', 'desc': 'This paper introduces UniF^2ace, a unified multimodal model designed for fine-grained understanding and generation of facial attributes. Unlike previous models that only focus on coarse attributes, UniF^2ace leverages a large-scale dataset of 130K image-text pairs and one million question-answering pairs to enhance its learning capabilities. The model employs innovative diffusion techniques and a mixture-of-experts architecture to optimize its performance in synthesizing detailed facial features. Experimental results show that UniF^2ace significantly outperforms existing models in both understanding and generating facial attributes.'}, 'zh': {'title': '细粒度面部理解与生成的统一模型', 'desc': '统一多模态模型（UMMs）在计算机视觉研究中展现出强大的潜力，尤其是在图像理解和生成方面。然而，现有的面部领域研究主要集中在粗略的面部属性理解上，缺乏处理细粒度面部属性的能力，并且未能解决生成能力的问题。为了解决这些限制，我们提出了UniF^2ace，这是第一个专门针对细粒度面部理解和生成的UMM。通过构建一个包含13万张图像-文本对的大规模数据集，并采用两种互补的扩散技术和双层专家混合架构，UniF^2ace在理解和生成任务上均表现出色。'}}}, {'id': 'https://huggingface.co/papers/2503.08625', 'title': 'SegAgent: Exploring Pixel Understanding Capabilities in MLLMs by\n  Imitating Human Annotator Trajectories', 'url': 'https://huggingface.co/papers/2503.08625', 'abstract': "While MLLMs have demonstrated adequate image understanding capabilities, they still struggle with pixel-level comprehension, limiting their practical applications. Current evaluation tasks like VQA and visual grounding remain too coarse to assess fine-grained pixel comprehension accurately. Though segmentation is foundational for pixel-level understanding, existing methods often require MLLMs to generate implicit tokens, decoded through external pixel decoders. This approach disrupts the MLLM's text output space, potentially compromising language capabilities and reducing flexibility and extensibility, while failing to reflect the model's intrinsic pixel-level understanding.   Thus, we introduce the Human-Like Mask Annotation Task (HLMAT), a new paradigm where MLLMs mimic human annotators using interactive segmentation tools. Modeling segmentation as a multi-step Markov Decision Process, HLMAT enables MLLMs to iteratively generate text-based click points, achieving high-quality masks without architectural changes or implicit tokens. Through this setup, we develop SegAgent, a model fine-tuned on human-like annotation trajectories, which achieves performance comparable to state-of-the-art (SOTA) methods and supports additional tasks like mask refinement and annotation filtering.   HLMAT provides a protocol for assessing fine-grained pixel understanding in MLLMs and introduces a vision-centric, multi-step decision-making task that facilitates exploration of MLLMs' visual reasoning abilities. Our adaptations of policy improvement method StaR and PRM-guided tree search further enhance model robustness in complex segmentation tasks, laying a foundation for future advancements in fine-grained visual perception and multi-step decision-making for MLLMs.", 'score': 22, 'issue_id': 2655, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 марта', 'en': 'March 11', 'zh': '3月11日'}, 'hash': '081b91105002410a', 'authors': ['Muzhi Zhu', 'Yuzhuo Tian', 'Hao Chen', 'Chunluan Zhou', 'Qingpei Guo', 'Yang Liu', 'Ming Yang', 'Chunhua Shen'], 'affiliations': ['Ant Group', 'Zhejiang University, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.08625.jpg', 'data': {'categories': ['#agents', '#cv', '#rl', '#reasoning', '#optimization', '#games'], 'emoji': '🖼️', 'ru': {'title': 'Человекоподобная сегментация изображений мультимодальными языковыми моделями', 'desc': 'Статья представляет новый подход к оценке понимания изображений на уровне пикселей для мультимодальных языковых моделей (MLLM). Авторы вводят задачу аннотации масок, подобную человеческой (HLMAT), где MLLM имитируют действия человека-аннотатора. Разработанная модель SegAgent достигает результатов на уровне современных методов сегментации без изменения архитектуры. HLMAT открывает новые возможности для исследования визуального мышления MLLM и улучшения их восприятия изображений.'}, 'en': {'title': 'Enhancing Pixel-Level Understanding in MLLMs with Human-Like Annotation', 'desc': "This paper addresses the limitations of Multimodal Language Models (MLLMs) in understanding images at the pixel level, which restricts their practical use. It critiques existing evaluation methods for being too broad and introduces the Human-Like Mask Annotation Task (HLMAT) to improve pixel comprehension. HLMAT allows MLLMs to simulate human-like annotation through interactive segmentation, modeled as a multi-step Markov Decision Process. The proposed SegAgent model, trained on this new task, achieves competitive performance while maintaining the MLLM's language capabilities and flexibility."}, 'zh': {'title': '提升像素理解的创新标注任务', 'desc': '本论文探讨了多模态大语言模型（MLLMs）在像素级理解方面的不足，尤其是在细粒度评估任务中的局限性。我们提出了一种新的任务——人类样本标注任务（HLMAT），通过模拟人类标注者的方式，使用交互式分割工具来提高模型的像素理解能力。HLMAT将分割建模为多步骤的马尔可夫决策过程，使得MLLMs能够迭代生成基于文本的点击点，从而无需改变模型架构或使用隐式标记。通过这一方法，我们开发了SegAgent模型，其性能与最先进的方法相当，并支持掩膜细化和标注过滤等额外任务。'}}}, {'id': 'https://huggingface.co/papers/2503.07703', 'title': 'Seedream 2.0: A Native Chinese-English Bilingual Image Generation\n  Foundation Model', 'url': 'https://huggingface.co/papers/2503.07703', 'abstract': 'Rapid advancement of diffusion models has catalyzed remarkable progress in the field of image generation. However, prevalent models such as Flux, SD3.5 and Midjourney, still grapple with issues like model bias, limited text rendering capabilities, and insufficient understanding of Chinese cultural nuances. To address these limitations, we present Seedream 2.0, a native Chinese-English bilingual image generation foundation model that excels across diverse dimensions, which adeptly manages text prompt in both Chinese and English, supporting bilingual image generation and text rendering. We develop a powerful data system that facilitates knowledge integration, and a caption system that balances the accuracy and richness for image description. Particularly, Seedream is integrated with a self-developed bilingual large language model as a text encoder, allowing it to learn native knowledge directly from massive data. This enable it to generate high-fidelity images with accurate cultural nuances and aesthetic expressions described in either Chinese or English. Beside, Glyph-Aligned ByT5 is applied for flexible character-level text rendering, while a Scaled ROPE generalizes well to untrained resolutions. Multi-phase post-training optimizations, including SFT and RLHF iterations, further improve the overall capability. Through extensive experimentation, we demonstrate that Seedream 2.0 achieves state-of-the-art performance across multiple aspects, including prompt-following, aesthetics, text rendering, and structural correctness. Furthermore, Seedream 2.0 has been optimized through multiple RLHF iterations to closely align its output with human preferences, as revealed by its outstanding ELO score. In addition, it can be readily adapted to an instruction-based image editing model, such as SeedEdit, with strong editing capability that balances instruction-following and image consistency.', 'score': 22, 'issue_id': 2654, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '00cd4369f3c531f9', 'authors': ['Lixue Gong', 'Xiaoxia Hou', 'Fanshi Li', 'Liang Li', 'Xiaochen Lian', 'Fei Liu', 'Liyang Liu', 'Wei Liu', 'Wei Lu', 'Yichun Shi', 'Shiqi Sun', 'Yu Tian', 'Zhi Tian', 'Peng Wang', 'Xun Wang', 'Ye Wang', 'Guofeng Wu', 'Jie Wu', 'Xin Xia', 'Xuefeng Xiao', 'Linjie Yang', 'Zhonghua Zhai', 'Xinyu Zhang', 'Qi Zhang', 'Yuwei Zhang', 'Shijia Zhao', 'Jianchao Yang', 'Weilin Huang'], 'affiliations': ['Seed Vision Team, ByteDance'], 'pdf_title_img': 'assets/pdf/title_img/2503.07703.jpg', 'data': {'categories': ['#cv', '#training', '#dataset', '#optimization', '#rlhf', '#alignment', '#multimodal', '#diffusion'], 'emoji': '🎨', 'ru': {'title': 'Seedream 2.0: Революция в двуязычной генерации изображений', 'desc': 'Seedream 2.0 - это двуязычная модель генерации изображений, созданная для преодоления ограничений существующих моделей. Она использует мощную систему данных и двуязычную языковую модель для точного понимания культурных нюансов. Модель применяет Glyph-Aligned ByT5 для гибкого рендеринга текста и Scaled ROPE для генерализации на нетренированные разрешения. Многоэтапная оптимизация, включая SFT и RLHF, улучшает общие возможности модели.'}, 'en': {'title': 'Seedream 2.0: Bridging Cultures in Image Generation', 'desc': 'This paper introduces Seedream 2.0, a bilingual image generation model designed to overcome limitations found in existing models like Flux and Midjourney. It effectively handles text prompts in both Chinese and English, allowing for culturally nuanced image generation and accurate text rendering. The model incorporates a bilingual large language model for enhanced understanding and a robust data system for knowledge integration. Extensive optimizations, including reinforcement learning from human feedback, ensure that Seedream 2.0 excels in aesthetics, prompt adherence, and overall image quality.'}, 'zh': {'title': '双语图像生成的未来：Seedream 2.0', 'desc': '本论文介绍了Seedream 2.0，这是一个中英双语的图像生成基础模型，旨在解决现有模型在文本渲染和文化理解方面的不足。该模型能够处理中文和英文的文本提示，支持双语图像生成，并通过强大的数据系统和描述系统提升图像描述的准确性和丰富性。Seedream 2.0结合了自研的双语大语言模型，能够从海量数据中直接学习本土知识，生成高保真度的图像，准确表达文化细节和美学特征。此外，通过多阶段的后训练优化，Seedream 2.0在多个方面实现了最先进的性能，包括遵循提示、审美、文本渲染和结构正确性。'}}}, {'id': 'https://huggingface.co/papers/2503.07860', 'title': 'Video Action Differencing', 'url': 'https://huggingface.co/papers/2503.07860', 'abstract': 'How do two individuals differ when performing the same action? In this work, we introduce Video Action Differencing (VidDiff), the novel task of identifying subtle differences between videos of the same action, which has many applications, such as coaching and skill learning. To enable development on this new task, we first create VidDiffBench, a benchmark dataset containing 549 video pairs, with human annotations of 4,469 fine-grained action differences and 2,075 localization timestamps indicating where these differences occur. Our experiments demonstrate that VidDiffBench poses a significant challenge for state-of-the-art large multimodal models (LMMs), such as GPT-4o and Qwen2-VL. By analyzing failure cases of LMMs on VidDiffBench, we highlight two key challenges for this task: localizing relevant sub-actions over two videos and fine-grained frame comparison. To overcome these, we propose the VidDiff method, an agentic workflow that breaks the task into three stages: action difference proposal, keyframe localization, and frame differencing, each stage utilizing specialized foundation models. To encourage future research in this new task, we release the benchmark at https://huggingface.co/datasets/jmhb/VidDiffBench and code at http://jmhb0.github.io/viddiff.', 'score': 21, 'issue_id': 2653, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '76b8b9c677de83cd', 'authors': ['James Burgess', 'Xiaohan Wang', 'Yuhui Zhang', 'Anita Rau', 'Alejandro Lozano', 'Lisa Dunlap', 'Trevor Darrell', 'Serena Yeung-Levy'], 'affiliations': ['Stanford', 'UC Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2503.07860.jpg', 'data': {'categories': ['#benchmark', '#agents', '#multimodal', '#dataset', '#video'], 'emoji': '🎥', 'ru': {'title': 'Новый фронтир в анализе видео: выявление тонких различий в действиях', 'desc': 'Статья представляет новую задачу в области компьютерного зрения - Video Action Differencing (VidDiff), которая направлена на выявление тонких различий между видео с одинаковыми действиями. Авторы создали датасет VidDiffBench, содержащий 549 пар видео с аннотациями различий в действиях и временными метками. Эксперименты показали, что современные мультимодальные языковые модели (LLM) испытывают трудности с этой задачей. Для решения проблемы предложен метод VidDiff, использующий трехэтапный подход с применением специализированных фундаментальных моделей.'}, 'en': {'title': 'Unveiling Subtle Differences in Action Videos with VidDiff', 'desc': 'This paper introduces Video Action Differencing (VidDiff), a new task focused on identifying subtle differences in videos depicting the same action. To support this task, the authors created VidDiffBench, a benchmark dataset with 549 video pairs and detailed annotations of action differences and localization timestamps. The study reveals that current large multimodal models struggle with this task, particularly in localizing sub-actions and performing fine-grained frame comparisons. To address these challenges, the authors propose a structured approach called VidDiff, which divides the task into three stages, each leveraging specialized foundation models for improved performance.'}, 'zh': {'title': '视频动作差异化：识别细微差别的新挑战', 'desc': '本文介绍了一种新任务，称为视频动作差异化（VidDiff），旨在识别同一动作视频之间的细微差别。为此，我们创建了VidDiffBench，一个包含549对视频的基准数据集，标注了4469个细粒度动作差异和2075个时间戳。我们的实验表明，VidDiffBench对现有的大型多模态模型（LMMs）提出了重大挑战，尤其是在局部化相关子动作和细粒度帧比较方面。为了解决这些问题，我们提出了VidDiff方法，将任务分为三个阶段：动作差异提议、关键帧定位和帧差异化，每个阶段都利用了专门的基础模型。'}}}, {'id': 'https://huggingface.co/papers/2503.07891', 'title': 'Gemini Embedding: Generalizable Embeddings from Gemini', 'url': 'https://huggingface.co/papers/2503.07891', 'abstract': "In this report, we introduce Gemini Embedding, a state-of-the-art embedding model leveraging the power of Gemini, Google's most capable large language model. Capitalizing on Gemini's inherent multilingual and code understanding capabilities, Gemini Embedding produces highly generalizable embeddings for text spanning numerous languages and textual modalities. The representations generated by Gemini Embedding can be precomputed and applied to a variety of downstream tasks including classification, similarity, clustering, ranking, and retrieval. Evaluated on the Massive Multilingual Text Embedding Benchmark (MMTEB), which includes over one hundred tasks across 250+ languages, Gemini Embedding substantially outperforms prior state-of-the-art models, demonstrating considerable improvements in embedding quality. Achieving state-of-the-art performance across MMTEB's multilingual, English, and code benchmarks, our unified model demonstrates strong capabilities across a broad selection of tasks and surpasses specialized domain-specific models.", 'score': 19, 'issue_id': 2655, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '149f8fc31808d626', 'authors': ['Jinhyuk Lee', 'Feiyang Chen', 'Sahil Dua', 'Daniel Cer', 'Madhuri Shanbhogue', 'Iftekhar Naim', 'Gustavo Hernández Ábrego', 'Zhe Li', 'Kaifeng Chen', 'Henrique Schechter Vera', 'Xiaoqi Ren', 'Shanfeng Zhang', 'Daniel Salz', 'Michael Boratko', 'Jay Han', 'Blair Chen', 'Shuo Huang', 'Vikram Rao', 'Paul Suganthan', 'Feng Han', 'Andreas Doumanoglou', 'Nithi Gupta', 'Fedor Moiseev', 'Cathy Yip', 'Aashi Jain', 'Simon Baumgartner', 'Shahrokh Shahi', 'Frank Palma Gomez', 'Sandeep Mariserla', 'Min Choi', 'Parashar Shah', 'Sonam Goenka', 'Ke Chen', 'Ye Xia', 'Koert Chen', 'Sai Meher Karthik Duddu', 'Yichang Chen', 'Trevor Walker', 'Wenlei Zhou', 'Rakesh Ghiya', 'Zach Gleicher', 'Karan Gill', 'Zhe Dong', 'Mojtaba Seyedhosseini', 'Yunhsuan Sung', 'Raphael Hoffmann', 'Tom Duerig'], 'affiliations': ['Google'], 'pdf_title_img': 'assets/pdf/title_img/2503.07891.jpg', 'data': {'categories': ['#multimodal', '#multilingual', '#dataset', '#transfer_learning', '#benchmark', '#open_source'], 'emoji': '🌐', 'ru': {'title': 'Gemini Embedding: универсальные эмбеддинги для текста и кода на множестве языков', 'desc': 'Статья представляет Gemini Embedding - передовую модель встраивания, основанную на мощной языковой модели Gemini от Google. Модель использует многоязычные возможности и понимание кода Gemini для создания универсальных эмбеддингов текста на различных языках. Gemini Embedding превосходит предыдущие модели по результатам оценки на бенчмарке MMTEB, включающем более 100 задач на 250+ языках. Модель демонстрирует высокую эффективность в многоязычных задачах, задачах на английском языке и задачах, связанных с кодом, превосходя специализированные модели.'}, 'en': {'title': 'Unifying Multilingual Understanding with Gemini Embedding', 'desc': "Gemini Embedding is a cutting-edge embedding model that utilizes Google's advanced Gemini large language model. It excels in generating versatile embeddings for text in multiple languages and formats, thanks to its multilingual and code comprehension abilities. These embeddings can be precomputed and effectively used in various machine learning tasks such as classification, similarity, clustering, ranking, and retrieval. In evaluations on the Massive Multilingual Text Embedding Benchmark (MMTEB), Gemini Embedding significantly outperformed previous models, showcasing its superior embedding quality across a wide range of languages and tasks."}, 'zh': {'title': 'Gemini Embedding：多语言嵌入的新标杆', 'desc': '本报告介绍了Gemini Embedding，这是一种先进的嵌入模型，利用了谷歌最强大的大型语言模型Gemini的能力。Gemini Embedding能够生成高度通用的文本嵌入，适用于多种语言和文本形式，充分利用了Gemini的多语言和代码理解能力。生成的表示可以预先计算，并应用于分类、相似性、聚类、排序和检索等多种下游任务。在大规模多语言文本嵌入基准测试（MMTEB）中，Gemini Embedding显著超越了之前的最先进模型，展示了嵌入质量的显著提升。'}}}, {'id': 'https://huggingface.co/papers/2503.07572', 'title': 'Optimizing Test-Time Compute via Meta Reinforcement Fine-Tuning', 'url': 'https://huggingface.co/papers/2503.07572', 'abstract': "Training models to effectively use test-time compute is crucial for improving the reasoning performance of LLMs. Current methods mostly do so via fine-tuning on search traces or running RL with 0/1 outcome reward, but do these approaches efficiently utilize test-time compute? Would these approaches continue to scale as the budget improves? In this paper, we try to answer these questions. We formalize the problem of optimizing test-time compute as a meta-reinforcement learning (RL) problem, which provides a principled perspective on spending test-time compute. This perspective enables us to view the long output stream from the LLM as consisting of several episodes run at test time and leads us to use a notion of cumulative regret over output tokens as a way to measure the efficacy of test-time compute. Akin to how RL algorithms can best tradeoff exploration and exploitation over training, minimizing cumulative regret would also provide the best balance between exploration and exploitation in the token stream. While we show that state-of-the-art models do not minimize regret, one can do so by maximizing a dense reward bonus in conjunction with the outcome 0/1 reward RL. This bonus is the ''progress'' made by each subsequent block in the output stream, quantified by the change in the likelihood of eventual success. Using these insights, we develop Meta Reinforcement Fine-Tuning, or MRT, a new class of fine-tuning methods for optimizing test-time compute. MRT leads to a 2-3x relative gain in performance and roughly a 1.5x gain in token efficiency for math reasoning compared to outcome-reward RL.", 'score': 18, 'issue_id': 2653, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '49ca445bc3322f0d', 'authors': ['Yuxiao Qu', 'Matthew Y. R. Yang', 'Amrith Setlur', 'Lewis Tunstall', 'Edward Emanuel Beeching', 'Ruslan Salakhutdinov', 'Aviral Kumar'], 'affiliations': ['Carnegie Mellon University', 'Hugging Face'], 'pdf_title_img': 'assets/pdf/title_img/2503.07572.jpg', 'data': {'categories': ['#optimization', '#training', '#rl', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Оптимизация вычислений LLM через мета-обучение с подкреплением', 'desc': "Статья представляет новый подход к оптимизации вычислений во время тестирования для улучшения рассуждений в больших языковых моделях (LLM). Авторы формализуют эту задачу как проблему мета-обучения с подкреплением, вводя понятие кумулятивного сожаления для оценки эффективности вычислений. Они разрабатывают метод Meta Reinforcement Fine-Tuning (MRT), который максимизирует плотное вознаграждение за 'прогресс' в дополнение к бинарному результату. MRT показывает значительное улучшение производительности и эффективности использования токенов в задачах математических рассуждений."}, 'en': {'title': 'Optimizing Test-Time Compute for Enhanced LLM Reasoning', 'desc': "This paper addresses the challenge of optimizing test-time compute for large language models (LLMs) to enhance their reasoning capabilities. It introduces a meta-reinforcement learning framework that treats the output generation process as a series of episodes, allowing for a structured approach to manage compute resources effectively. The authors propose minimizing cumulative regret over output tokens as a metric for evaluating the efficiency of test-time compute, which balances exploration and exploitation in the model's output. They present a new fine-tuning method called Meta Reinforcement Fine-Tuning (MRT), which significantly improves performance and token efficiency in mathematical reasoning tasks compared to traditional outcome-reward reinforcement learning methods."}, 'zh': {'title': '优化测试时计算，提升推理性能！', 'desc': '本文探讨了如何有效利用测试时计算资源来提升大型语言模型（LLM）的推理性能。我们将优化测试时计算的问题形式化为元强化学习（RL）问题，从而提供了一个系统化的视角来支配测试时计算。通过将LLM的输出流视为多个测试时的回合，并使用累积遗憾的概念来衡量测试时计算的有效性，我们提出了一种新的微调方法，称为元强化微调（MRT）。实验结果表明，MRT在数学推理任务中相较于传统的结果奖励RL方法，性能提升了2-3倍，令令牌效率提高了约1.5倍。'}}}, {'id': 'https://huggingface.co/papers/2503.07604', 'title': 'Implicit Reasoning in Transformers is Reasoning through Shortcuts', 'url': 'https://huggingface.co/papers/2503.07604', 'abstract': "Test-time compute is emerging as a new paradigm for enhancing language models' complex multi-step reasoning capabilities, as demonstrated by the success of OpenAI's o1 and o3, as well as DeepSeek's R1. Compared to explicit reasoning in test-time compute, implicit reasoning is more inference-efficient, requiring fewer generated tokens. However, why does the advanced reasoning capability fail to emerge in the implicit reasoning style? In this work, we train GPT-2 from scratch on a curated multi-step mathematical reasoning dataset and conduct analytical experiments to investigate how language models perform implicit reasoning in multi-step tasks. Our findings reveal: 1) Language models can perform step-by-step reasoning and achieve high accuracy in both in-domain and out-of-domain tests via implicit reasoning. However, this capability only emerges when trained on fixed-pattern data. 2) Conversely, implicit reasoning abilities emerging from training on un<PRE_TAG>fixed-pattern data</POST_TAG> tend to overfit a specific pattern and fail to generalize further. Notably, this limitation is also observed in state-of-the-art large language models. These findings suggest that language models acquire implicit reasoning through shortcut learning, enabling strong performance on tasks with similar patterns while lacking generalization.", 'score': 17, 'issue_id': 2654, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '313594788f663498', 'authors': ['Tianhe Lin', 'Jian Xie', 'Siyu Yuan', 'Deqing Yang'], 'affiliations': ['School of Computer Science, Fudan University', 'School of Data Science, Fudan University'], 'pdf_title_img': 'assets/pdf/title_img/2503.07604.jpg', 'data': {'categories': ['#reasoning', '#training', '#dataset', '#math', '#data'], 'emoji': '🧠', 'ru': {'title': 'Неявные рассуждения в языковых моделях: сила шаблонов и проблемы обобщения', 'desc': 'Исследование показывает, что языковые модели способны выполнять пошаговые рассуждения и достигать высокой точности при неявном рассуждении, но только при обучении на данных с фиксированным шаблоном. При обучении на данных с нефиксированным шаблоном модели склонны переобучаться конкретному паттерну и не могут обобщать. Это ограничение наблюдается даже у современных больших языковых моделей. Результаты указывают на то, что языковые модели приобретают навыки неявного рассуждения через обучение коротким путям, что позволяет им хорошо справляться с задачами схожего паттерна, но ограничивает обобщение.'}, 'en': {'title': 'Unlocking Implicit Reasoning in Language Models', 'desc': 'This paper explores how language models, specifically GPT-2, can perform implicit reasoning in multi-step mathematical tasks. It shows that while these models can achieve high accuracy through implicit reasoning, this ability is contingent on being trained with fixed-pattern data. When trained on variable patterns, the models tend to overfit and struggle to generalize their reasoning skills. The research highlights that language models often rely on shortcut learning, which allows them to excel in specific tasks but limits their broader reasoning capabilities.'}, 'zh': {'title': '隐式推理的捷径与局限性', 'desc': '本文探讨了在测试时计算中，隐式推理与显式推理的效率差异。研究发现，语言模型在固定模式数据上训练时，能够通过隐式推理实现逐步推理并在多步任务中取得高准确率。相反，在非固定模式数据上训练的隐式推理能力容易过拟合特定模式，导致泛化能力不足。总的来说，语言模型通过捷径学习获得隐式推理能力，但在面对不同模式时表现不佳。'}}}, {'id': 'https://huggingface.co/papers/2503.08605', 'title': 'Tuning-Free Multi-Event Long Video Generation via Synchronized Coupled\n  Sampling', 'url': 'https://huggingface.co/papers/2503.08605', 'abstract': 'While recent advancements in text-to-video diffusion models enable high-quality short video generation from a single prompt, generating real-world long videos in a single pass remains challenging due to limited data and high computational costs. To address this, several works propose tuning-free approaches, i.e., extending existing models for long video generation, specifically using multiple prompts to allow for dynamic and controlled content changes. However, these methods primarily focus on ensuring smooth transitions between adjacent frames, often leading to content drift and a gradual loss of semantic coherence over longer sequences. To tackle such an issue, we propose Synchronized Coupled Sampling (SynCoS), a novel inference framework that synchronizes denoising paths across the entire video, ensuring long-range consistency across both adjacent and distant frames. Our approach combines two complementary sampling strategies: reverse and optimization-based sampling, which ensure seamless local transitions and enforce global coherence, respectively. However, directly alternating between these samplings misaligns denoising trajectories, disrupting prompt guidance and introducing unintended content changes as they operate independently. To resolve this, SynCoS synchronizes them through a grounded timestep and a fixed baseline noise, ensuring fully coupled sampling with aligned denoising paths. Extensive experiments show that SynCoS significantly improves multi-event long video generation, achieving smoother transitions and superior long-range coherence, outperforming previous approaches both quantitatively and qualitatively.', 'score': 16, 'issue_id': 2653, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 марта', 'en': 'March 11', 'zh': '3月11日'}, 'hash': '6f7bf7b6c171af43', 'authors': ['Subin Kim', 'Seoung Wug Oh', 'Jui-Hsien Wang', 'Joon-Young Lee', 'Jinwoo Shin'], 'affiliations': ['Adobe Research', 'KAIST'], 'pdf_title_img': 'assets/pdf/title_img/2503.08605.jpg', 'data': {'categories': ['#inference', '#diffusion', '#video'], 'emoji': '🎬', 'ru': {'title': 'SynCoS: Синхронизированная генерация длинных видео с сохранением согласованности', 'desc': 'Статья представляет новый метод генерации длинных видео с использованием текстовых подсказок, названный Synchronized Coupled Sampling (SynCoS). Этот подход объединяет обратную выборку и выборку на основе оптимизации для обеспечения плавных переходов между кадрами и глобальной согласованности контента. SynCoS синхронизирует траектории шумоподавления через фиксированный временной шаг и базовый шум, что позволяет избежать нежелательных изменений содержания. Эксперименты показывают, что SynCoS значительно улучшает генерацию длинных видео с несколькими событиями, превосходя предыдущие подходы как количественно, так и качественно.'}, 'en': {'title': 'Achieving Long-Range Coherence in Video Generation with SynCoS', 'desc': 'This paper introduces Synchronized Coupled Sampling (SynCoS), a new framework for generating long videos from text prompts while maintaining semantic coherence. Traditional methods struggle with content drift and frame transitions, especially over extended sequences. SynCoS addresses these issues by synchronizing denoising paths across the video, combining reverse and optimization-based sampling strategies for better local and global coherence. Experimental results demonstrate that SynCoS enhances the quality of multi-event long video generation, outperforming existing techniques in both smoothness and consistency.'}, 'zh': {'title': '同步耦合采样：提升长视频生成的一致性', 'desc': '本文提出了一种新的推理框架，称为同步耦合采样（SynCoS），旨在解决长视频生成中的一致性问题。通过同步去噪路径，SynCoS确保了相邻帧和远程帧之间的长范围一致性。该方法结合了反向采样和基于优化的采样策略，以实现局部平滑过渡和全局一致性。实验结果表明，SynCoS在多事件长视频生成方面显著优于之前的方法，提供了更平滑的过渡和更好的长范围语义一致性。'}}}, {'id': 'https://huggingface.co/papers/2503.08619', 'title': 'LightGen: Efficient Image Generation through Knowledge Distillation and\n  Direct Preference Optimization', 'url': 'https://huggingface.co/papers/2503.08619', 'abstract': 'Recent advances in text-to-image generation have primarily relied on extensive datasets and parameter-heavy architectures. These requirements severely limit accessibility for researchers and practitioners who lack substantial computational resources. In this paper, we introduce \\model, an efficient training paradigm for image generation models that uses knowledge distillation (KD) and Direct Preference Optimization (DPO). Drawing inspiration from the success of data KD techniques widely adopted in Multi-Modal Large Language Models (MLLMs), LightGen distills knowledge from state-of-the-art (SOTA) text-to-image models into a compact Masked Autoregressive (MAR) architecture with only 0.7B parameters. Using a compact <PRE_TAG>synthetic dataset</POST_TAG> of just 2M high-quality images generated from varied captions, we demonstrate that data diversity significantly outweighs data volume in determining model performance. This strategy dramatically reduces computational demands and reduces pre-training time from potentially thousands of GPU-days to merely 88 GPU-days. Furthermore, to address the inherent shortcomings of synthetic data, particularly poor high-frequency details and spatial inaccuracies, we integrate the DPO technique that refines image fidelity and positional accuracy. Comprehensive experiments confirm that LightGen achieves image generation quality comparable to SOTA models while significantly reducing computational resources and expanding accessibility for resource-constrained environments. Code is available at https://github.com/XianfengWu01/LightGen', 'score': 15, 'issue_id': 2655, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 марта', 'en': 'March 11', 'zh': '3月11日'}, 'hash': '1645a4236e6d91b6', 'authors': ['Xianfeng Wu', 'Yajing Bai', 'Haoze Zheng', 'Harold Haodong Chen', 'Yexin Liu', 'Zihao Wang', 'Xuran Ma', 'Wen-Jie Shu', 'Xianzu Wu', 'Harry Yang', 'Ser-Nam Lim'], 'affiliations': ['Everlyn AI', 'The Hong Kong University of Science and Technology', 'University of Central Florida'], 'pdf_title_img': 'assets/pdf/title_img/2503.08619.jpg', 'data': {'categories': ['#training', '#dataset', '#synthetic', '#architecture', '#optimization'], 'emoji': '🖼️', 'ru': {'title': 'Эффективная генерация изображений с минимальными ресурсами', 'desc': 'Эта статья представляет LightGen - эффективный метод обучения моделей генерации изображений, использующий дистилляцию знаний и Direct Preference Optimization. Модель использует компактную архитектуру с маскированной авторегрессией всего в 0.7 миллиардов параметров, обученную на синтетическом наборе данных из 2 миллионов изображений. Этот подход значительно сокращает вычислительные требования и время предобучения до 88 GPU-дней. Эксперименты показывают, что LightGen достигает качества генерации изображений, сравнимого с передовыми моделями, при существенно меньших вычислительных ресурсах.'}, 'en': {'title': 'Efficient Image Generation with LightGen: Less is More!', 'desc': 'This paper presents LightGen, a new approach to text-to-image generation that focuses on efficiency and accessibility. It utilizes knowledge distillation (KD) and Direct Preference Optimization (DPO) to create a compact model with only 0.7 billion parameters, making it less demanding on computational resources. By training on a small but diverse synthetic dataset of 2 million images, the authors show that the variety of data is more important than sheer volume for model performance. The results indicate that LightGen can produce high-quality images comparable to state-of-the-art models while significantly reducing training time and resource requirements.'}, 'zh': {'title': '高效图像生成，资源友好！', 'desc': '本文介绍了一种名为LightGen的高效图像生成模型训练范式，利用知识蒸馏（KD）和直接偏好优化（DPO）。该方法从最先进的文本到图像模型中提取知识，构建了一个仅有0.7亿参数的紧凑型自回归架构。通过使用仅200万张高质量合成图像的数据集，研究表明数据的多样性对模型性能的影响远大于数据的数量。LightGen显著降低了计算需求，将预训练时间从数千个GPU天缩短至仅88个GPU天，同时保持了与最先进模型相当的图像生成质量。'}}}, {'id': 'https://huggingface.co/papers/2503.08686', 'title': 'OmniMamba: Efficient and Unified Multimodal Understanding and Generation\n  via State Space Models', 'url': 'https://huggingface.co/papers/2503.08686', 'abstract': "Recent advancements in unified multimodal understanding and visual generation (or multimodal generation) models have been hindered by their quadratic computational complexity and dependence on large-scale training data. We present OmniMamba, the first linear-architecture-based multimodal generation model that generates both text and images through a unified next-token prediction paradigm. The model fully leverages Mamba-2's high computational and memory efficiency, extending its capabilities from text generation to multimodal generation. To address the data inefficiency of existing unified models, we propose two key innovations: (1) decoupled vocabularies to guide modality-specific generation, and (2) task-specific LoRA for parameter-efficient adaptation. Furthermore, we introduce a decoupled two-stage training strategy to mitigate data imbalance between two tasks. Equipped with these techniques, OmniMamba achieves competitive performance with JanusFlow while surpassing Show-o across benchmarks, despite being trained on merely 2M image-text pairs, which is 1,000 times fewer than Show-o. Notably, OmniMamba stands out with outstanding inference efficiency, achieving up to a 119.2 times speedup and 63% GPU memory reduction for long-sequence generation compared to Transformer-based counterparts. Code and models are released at https://github.com/hustvl/OmniMamba", 'score': 13, 'issue_id': 2655, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 марта', 'en': 'March 11', 'zh': '3月11日'}, 'hash': '7bb3cd796d69dc2c', 'authors': ['Jialv Zou', 'Bencheng Liao', 'Qian Zhang', 'Wenyu Liu', 'Xinggang Wang'], 'affiliations': ['Horizon Robotics', 'Institute of Artificial Intelligence, Huazhong University of Science & Technology', 'School of EIC, Huazhong University of Science & Technology'], 'pdf_title_img': 'assets/pdf/title_img/2503.08686.jpg', 'data': {'categories': ['#training', '#inference', '#multimodal', '#architecture', '#open_source', '#optimization'], 'emoji': '🦋', 'ru': {'title': 'OmniMamba: Эффективная мультимодальная генерация с линейной архитектурой', 'desc': 'OmniMamba - это первая мультимодальная модель генерации, основанная на линейной архитектуре, которая генерирует как текст, так и изображения через единую парадигму предсказания следующего токена. Модель использует высокую вычислительную эффективность Mamba-2, расширяя ее возможности от генерации текста до мультимодальной генерации. Для решения проблемы неэффективности данных существующих унифицированных моделей, авторы предлагают разделенные словари и задаче-специфичную LoRA для адаптации с малым количеством параметров. OmniMamba достигает конкурентоспособной производительности, будучи обученной всего на 2 миллионах пар изображение-текст, что в 1000 раз меньше, чем у аналогов.'}, 'en': {'title': 'OmniMamba: Efficient Multimodal Generation with Linear Architecture', 'desc': 'OmniMamba is a groundbreaking multimodal generation model that efficiently creates both text and images using a linear architecture. It introduces a unified next-token prediction approach, which significantly reduces computational complexity and memory usage compared to traditional models. The model employs innovative techniques like decoupled vocabularies for specific modality guidance and task-specific LoRA for efficient parameter adaptation. With a unique two-stage training strategy, OmniMamba achieves impressive performance on benchmarks while requiring far less training data, demonstrating remarkable speed and memory efficiency during inference.'}, 'zh': {'title': 'OmniMamba：高效的多模态生成新选择', 'desc': 'OmniMamba是一种新型的多模态生成模型，采用线性架构，能够同时生成文本和图像。该模型通过统一的下一个标记预测范式，充分利用了Mamba-2的高效计算和内存性能。为了提高数据利用效率，OmniMamba引入了解耦词汇和任务特定的LoRA技术，并采用了两阶段的训练策略来解决任务间的数据不平衡问题。最终，OmniMamba在生成效率上表现出色，相比于传统的Transformer模型，推理速度提高了119.2倍，GPU内存减少了63%。'}}}, {'id': 'https://huggingface.co/papers/2503.08644', 'title': 'Exploiting Instruction-Following Retrievers for Malicious Information\n  Retrieval', 'url': 'https://huggingface.co/papers/2503.08644', 'abstract': 'Instruction-following retrievers have been widely adopted alongside LLMs in real-world applications, but little work has investigated the safety risks surrounding their increasing search capabilities. We empirically study the ability of retrievers to satisfy malicious queries, both when used directly and when used in a retrieval augmented generation-based setup. Concretely, we investigate six leading retrievers, including NV-Embed and LLM2Vec, and find that given malicious requests, most retrievers can (for >50% of queries) select relevant harmful passages. For example, LLM2Vec correctly selects passages for 61.35% of our malicious queries. We further uncover an emerging risk with instruction-following retrievers, where highly relevant harmful information can be surfaced by exploiting their instruction-following capabilities. Finally, we show that even safety-aligned LLMs, such as Llama3, can satisfy malicious requests when provided with harmful retrieved passages in-context. In summary, our findings underscore the malicious misuse risks associated with increasing retriever capability.', 'score': 12, 'issue_id': 2666, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 марта', 'en': 'March 11', 'zh': '3月11日'}, 'hash': '4526f13d2d98134f', 'authors': ['Parishad BehnamGhader', 'Nicholas Meade', 'Siva Reddy'], 'affiliations': ['Canada CIFAR AI Chair', 'McGill University', 'Mila Quebec AI Institute'], 'pdf_title_img': 'assets/pdf/title_img/2503.08644.jpg', 'data': {'categories': ['#multimodal', '#security', '#rag', '#ethics'], 'emoji': '🕵️', 'ru': {'title': 'Скрытые угрозы в умных поисковых системах', 'desc': 'Исследование посвящено анализу безопасности ретриверов, используемых вместе с языковыми моделями. Авторы обнаружили, что многие ретриверы способны удовлетворять вредоносные запросы, выбирая релевантные опасные отрывки текста. Выявлен риск, связанный с возможностью манипулирования инструкциями для ретриверов. Даже модели с повышенной безопасностью могут выдавать вредоносный контент при наличии опасных фрагментов в контексте.'}, 'en': {'title': 'Uncovering the Risks of Powerful Retrievers in Malicious Queries', 'desc': 'This paper investigates the safety risks of instruction-following retrievers used with large language models (LLMs) in real-world applications. The authors analyze six prominent retrievers, revealing that they can often retrieve harmful content in response to malicious queries, with LLM2Vec achieving a 61.35% success rate. Additionally, the study highlights a concerning trend where instruction-following capabilities can inadvertently expose users to dangerous information. The findings emphasize the need for caution as the capabilities of retrievers grow, as even safety-aligned LLMs can inadvertently assist in fulfilling harmful requests.'}, 'zh': {'title': '检索器能力提升带来的安全隐患', 'desc': '本文研究了指令跟随检索器在处理恶意查询时的安全风险。我们分析了六种领先的检索器，包括NV-Embed和LLM2Vec，发现大多数检索器在超过50%的恶意查询中能够选择相关的有害内容。特别是，LLM2Vec在61.35%的恶意查询中正确选择了有害段落。此外，即使是安全对齐的LLM，如Llama3，在提供有害检索段落的情况下也能满足恶意请求。'}}}, {'id': 'https://huggingface.co/papers/2503.06940', 'title': 'CineBrain: A Large-Scale Multi-Modal Brain Dataset During Naturalistic\n  Audiovisual Narrative Processing', 'url': 'https://huggingface.co/papers/2503.06940', 'abstract': "In this paper, we introduce CineBrain, the first large-scale dataset featuring simultaneous EEG and fMRI recordings during dynamic audiovisual stimulation. Recognizing the complementary strengths of EEG's high temporal resolution and fMRI's deep-brain spatial coverage, CineBrain provides approximately six hours of narrative-driven content from the popular television series The Big Bang Theory for each of six participants. Building upon this unique dataset, we propose CineSync, an innovative multimodal decoding framework integrates a Multi-Modal Fusion Encoder with a diffusion-based Neural Latent Decoder. Our approach effectively fuses EEG and fMRI signals, significantly improving the reconstruction quality of complex audiovisual stimuli. To facilitate rigorous evaluation, we introduce Cine-Benchmark, a comprehensive evaluation protocol that assesses reconstructions across semantic and perceptual dimensions. Experimental results demonstrate that CineSync achieves state-of-the-art video reconstruction performance and highlight our initial success in combining fMRI and EEG for reconstructing both video and audio stimuli. Project Page: https://jianxgao.github.io/CineBrain.", 'score': 11, 'issue_id': 2664, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '10b773e3c142acbe', 'authors': ['Jianxiong Gao', 'Yichang Liu', 'Baofeng Yang', 'Jianfeng Feng', 'Yanwei Fu'], 'affiliations': ['Fudan University'], 'pdf_title_img': 'assets/pdf/title_img/2503.06940.jpg', 'data': {'categories': ['#benchmark', '#multimodal', '#diffusion', '#dataset', '#video', '#audio'], 'emoji': '🧠', 'ru': {'title': 'Чтение мыслей: от нейронных сигналов к видео и звуку', 'desc': 'CineBrain - это новый крупномасштабный набор данных, объединяющий записи ЭЭГ и фМРТ во время просмотра аудиовизуального контента. На основе этого датасета авторы разработали CineSync - мультимодальный фреймворк для декодирования нейронных сигналов, использующий диффузионную модель. CineSync эффективно объединяет сигналы ЭЭГ и фМРТ, значительно улучшая качество реконструкции сложных аудиовизуальных стимулов. Результаты экспериментов показывают, что CineSync достигает наилучших результатов в реконструкции видео и открывает перспективы для восстановления как видео, так и аудио стимулов из нейронных сигналов.'}, 'en': {'title': 'CineBrain: Uniting EEG and fMRI for Enhanced Audiovisual Reconstruction', 'desc': 'This paper presents CineBrain, a groundbreaking dataset that combines EEG and fMRI recordings while participants watch dynamic audiovisual content. By leveraging the fast response time of EEG and the detailed spatial information from fMRI, CineBrain offers a rich resource for studying brain activity during complex stimuli. The authors introduce CineSync, a novel framework that merges these two modalities to enhance the reconstruction of audiovisual experiences. Their results show that CineSync outperforms previous methods in reconstructing video and audio, marking a significant advancement in multimodal brain signal analysis.'}, 'zh': {'title': 'CineBrain：多模态解码的新突破', 'desc': '本文介绍了CineBrain，这是第一个大规模的数据集，包含在动态视听刺激下同时记录的EEG和fMRI数据。CineBrain利用EEG的高时间分辨率和fMRI的深脑空间覆盖优势，为六名参与者提供了约六小时的《生活大爆炸》叙事内容。基于这一独特数据集，我们提出了CineSync，这是一种创新的多模态解码框架，结合了多模态融合编码器和基于扩散的神经潜在解码器。实验结果表明，CineSync在视频重建性能上达到了最先进的水平，成功结合了fMRI和EEG来重建视频和音频刺激。'}}}, {'id': 'https://huggingface.co/papers/2503.07587', 'title': 'Robusto-1 Dataset: Comparing Humans and VLMs on real out-of-distribution\n  Autonomous Driving VQA from Peru', 'url': 'https://huggingface.co/papers/2503.07587', 'abstract': 'As multimodal foundational models start being deployed experimentally in Self-Driving cars, a reasonable question we ask ourselves is how similar to humans do these systems respond in certain driving situations -- especially those that are out-of-distribution? To study this, we create the Robusto-1 dataset that uses dashcam video data from Peru, a country with one of the worst (aggressive) drivers in the world, a high traffic index, and a high ratio of bizarre to non-bizarre street objects likely never seen in training. In particular, to preliminarly test at a cognitive level how well Foundational Visual Language Models (VLMs) compare to Humans in Driving, we move away from bounding boxes, segmentation maps, occupancy maps or trajectory estimation to multi-modal Visual Question Answering (VQA) comparing both humans and machines through a popular method in systems neuroscience known as Representational Similarity Analysis (RSA). Depending on the type of questions we ask and the answers these systems give, we will show in what cases do VLMs and Humans converge or diverge allowing us to probe on their cognitive alignment. We find that the degree of alignment varies significantly depending on the type of questions asked to each type of system (Humans vs VLMs), highlighting a gap in their alignment.', 'score': 9, 'issue_id': 2667, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '274531da9b4c33d1', 'authors': ['Dunant Cusipuma', 'David Ortega', 'Victor Flores-Benites', 'Arturo Deza'], 'affiliations': ['Artificio', 'Universidad de Ingeneria Tecnologia (UTEC)'], 'pdf_title_img': 'assets/pdf/title_img/2503.07587.jpg', 'data': {'categories': ['#video', '#alignment', '#interpretability', '#multimodal', '#dataset'], 'emoji': '🚗', 'ru': {'title': 'Человек vs ИИ: кто лучше понимает необычные ситуации на дороге?', 'desc': 'Статья описывает исследование, сравнивающее реакции мультимодальных фундаментальных моделей и людей в необычных ситуациях вождения. Авторы создали датасет Robusto-1 на основе видео с видеорегистраторов в Перу, стране с агрессивным стилем вождения. Используя метод анализа репрезентативного сходства (RSA), они сравнили ответы визуально-языковых моделей (VLM) и людей на вопросы о дорожных ситуациях. Результаты показали, что степень согласованности между моделями и людьми значительно варьируется в зависимости от типа задаваемых вопросов.'}, 'en': {'title': 'Assessing Human-Like Responses in Self-Driving AI', 'desc': 'This paper investigates how well multimodal foundational models, specifically Visual Language Models (VLMs), perform in driving situations compared to humans, particularly in challenging environments like Peru. The authors introduce the Robusto-1 dataset, which includes dashcam footage from a region known for aggressive driving and unusual street objects. They employ a method called Visual Question Answering (VQA) and Representational Similarity Analysis (RSA) to assess cognitive alignment between humans and VLMs. The findings reveal that the alignment between human responses and VLMs varies significantly based on the types of questions posed, indicating a notable gap in their cognitive processing.'}, 'zh': {'title': '探索自动驾驶中的人机认知差距', 'desc': '本研究探讨了多模态基础模型在自动驾驶汽车中的应用，特别是在复杂驾驶场景下的表现。我们创建了Robusto-1数据集，使用来自秘鲁的行车记录仪视频数据，以测试基础视觉语言模型（VLMs）与人类在驾驶中的认知水平。通过多模态视觉问答（VQA）方法，我们比较了人类和机器的反应，并使用表征相似性分析（RSA）来评估它们的认知一致性。研究发现，不同类型的问题会导致VLMs和人类在反应上的显著差异，揭示了它们之间的认知差距。'}}}, {'id': 'https://huggingface.co/papers/2503.08307', 'title': '^RFLAV: Rolling Flow matching for infinite Audio Video generation', 'url': 'https://huggingface.co/papers/2503.08307', 'abstract': 'Joint audio-video (AV) generation is still a significant challenge in generative AI, primarily due to three critical requirements: quality of the generated samples, seamless multimodal synchronization and temporal coherence, with audio tracks that match the visual data and vice versa, and limitless video duration. In this paper, we present , a novel transformer-based architecture that addresses all the key challenges of AV generation. We explore three distinct cross modality interaction modules, with our lightweight temporal fusion module emerging as the most effective and computationally efficient approach for aligning audio and visual modalities. Our experimental results demonstrate that  outperforms existing state-of-the-art models in multimodal AV generation tasks. Our code and checkpoints are available at https://github.com/ErgastiAlex/R-FLAV.', 'score': 7, 'issue_id': 2660, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 марта', 'en': 'March 11', 'zh': '3月11日'}, 'hash': '5b49ac0313e68c7e', 'authors': ['Alex Ergasti', 'Giuseppe Gabriele Tarollo', 'Filippo Botti', 'Tomaso Fontanini', 'Claudio Ferrari', 'Massimo Bertozzi', 'Andrea Prati'], 'affiliations': ['University of Parma, Department of Engineering and Architecture. Parma, Italy', 'University of Siena, Department of Information engineering and mathematics. Siena, Italy'], 'pdf_title_img': 'assets/pdf/title_img/2503.08307.jpg', 'data': {'categories': ['#video', '#architecture', '#multimodal', '#audio'], 'emoji': '🎬', 'ru': {'title': 'Революция в генерации аудио-видео контента с помощью трансформеров', 'desc': 'Статья представляет , новую архитектуру на основе трансформеров для совместной генерации аудио и видео. Модель решает ключевые проблемы AV-генерации: качество сэмплов, синхронизацию модальностей и временную согласованность. Авторы исследуют три модуля взаимодействия между модальностями, выделяя легковесный модуль временного слияния как наиболее эффективный. Экспериментальные результаты показывают, что  превосходит существующие SOTA-модели в задачах мультимодальной AV-генерации.'}, 'en': {'title': 'Transforming Audio-Video Generation with Efficient Alignment', 'desc': 'This paper addresses the challenges of joint audio-video generation in generative AI, focusing on quality, synchronization, and temporal coherence. The authors introduce a novel transformer-based architecture that effectively aligns audio and visual data. They explore three cross modality interaction modules, highlighting a lightweight temporal fusion module as the most efficient solution. Experimental results show that their approach surpasses existing state-of-the-art models in multimodal AV generation tasks.'}, 'zh': {'title': '突破音频-视频生成的关键挑战', 'desc': '本论文探讨了联合音频-视频生成的挑战，主要包括生成样本的质量、多模态的无缝同步和时间一致性。我们提出了一种新型的基于变换器的架构，旨在解决这些关键问题。研究中，我们探索了三种不同的跨模态交互模块，其中轻量级的时间融合模块被证明是对齐音频和视觉模态的最有效和计算高效的方法。实验结果表明，我们的方法在多模态音频-视频生成任务中优于现有的最先进模型。'}}}, {'id': 'https://huggingface.co/papers/2503.08685', 'title': '"Principal Components" Enable A New Language of Images', 'url': 'https://huggingface.co/papers/2503.08685', 'abstract': 'We introduce a novel visual tokenization framework that embeds a provable PCA-like structure into the latent token space. While existing visual tokenizers primarily optimize for reconstruction fidelity, they often neglect the structural properties of the latent space -- a critical factor for both interpretability and downstream tasks. Our method generates a 1D causal token sequence for images, where each successive token contributes non-overlapping information with mathematically guaranteed decreasing explained variance, analogous to principal component analysis. This structural constraint ensures the tokenizer extracts the most salient visual features first, with each subsequent token adding diminishing yet complementary information. Additionally, we identified and resolved a semantic-spectrum coupling effect that causes the unwanted entanglement of high-level semantic content and low-level spectral details in the tokens by leveraging a diffusion decoder. Experiments demonstrate that our approach achieves state-of-the-art reconstruction performance and enables better interpretability to align with the human vision system. Moreover, auto-regressive models trained on our token sequences achieve performance comparable to current state-of-the-art methods while requiring fewer tokens for training and inference.', 'score': 6, 'issue_id': 2654, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 марта', 'en': 'March 11', 'zh': '3月11日'}, 'hash': 'a013cfdc2d1e9d7c', 'authors': ['Xin Wen', 'Bingchen Zhao', 'Ismail Elezi', 'Jiankang Deng', 'Xiaojuan Qi'], 'affiliations': ['Imperial College London', 'Noahs Ark Lab', 'University of Edinburgh', 'University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2503.08685.jpg', 'data': {'categories': ['#training', '#cv', '#interpretability', '#diffusion', '#architecture', '#optimization'], 'emoji': '🧩', 'ru': {'title': 'Структурированная визуальная токенизация: ПК-анализ для изображений', 'desc': 'Авторы представляют новый метод визуальной токенизации, который встраивает доказуемую PCA-подобную структуру в пространство латентных токенов. Этот подход генерирует одномерную причинную последовательность токенов для изображений, где каждый последующий токен вносит неперекрывающуюся информацию с математически гарантированной убывающей объясненной дисперсией. Метод также решает проблему связи семантического содержания и спектральных деталей в токенах с помощью диффузионного декодера. Эксперименты показывают, что этот подход достигает современного уровня производительности реконструкции и обеспечивает лучшую интерпретируемость.'}, 'en': {'title': 'Enhancing Visual Tokenization with Structured Latent Spaces', 'desc': "This paper presents a new visual tokenization framework that incorporates a PCA-like structure into the latent token space. Unlike traditional visual tokenizers that focus mainly on how well they can recreate images, this method emphasizes the importance of the latent space's structure for better interpretability and performance in other tasks. The framework produces a sequence of tokens that each provide unique information, ensuring that the most important visual features are captured first, similar to how principal component analysis works. Additionally, the authors address a problem where high-level semantic information and low-level details become mixed in the tokens, leading to improved clarity and efficiency in training auto-regressive models."}, 'zh': {'title': '创新视觉标记化，提升可解释性与性能', 'desc': '我们提出了一种新颖的视觉标记化框架，将可证明的主成分分析（PCA）结构嵌入潜在标记空间。现有的视觉标记器主要优化重建精度，但往往忽视潜在空间的结构特性，这对可解释性和下游任务至关重要。我们的方法为图像生成一维因果标记序列，每个后续标记提供不重叠的信息，并且具有数学上保证的递减解释方差，类似于主成分分析。实验表明，我们的方法在重建性能上达到了最先进的水平，并提高了与人类视觉系统的对齐可解释性。'}}}, {'id': 'https://huggingface.co/papers/2503.08588', 'title': 'BiasEdit: Debiasing Stereotyped Language Models via Model Editing', 'url': 'https://huggingface.co/papers/2503.08588', 'abstract': "Previous studies have established that language models manifest stereotyped biases. Existing debiasing strategies, such as retraining a model with counterfactual data, representation projection, and prompting often fail to efficiently eliminate bias or directly alter the models' biased internal representations. To address these issues, we propose BiasEdit, an efficient model editing method to remove stereotypical bias from language models through lightweight networks that act as editors to generate parameter updates. BiasEdit employs a debiasing loss guiding editor networks to conduct local edits on partial parameters of a language model for debiasing while preserving the language modeling abilities during editing through a retention loss. Experiments on StereoSet and Crows-Pairs demonstrate the effectiveness, efficiency, and robustness of BiasEdit in eliminating bias compared to tangental debiasing baselines and little to no impact on the language models' general capabilities. In addition, we conduct bias tracing to probe bias in various modules and explore bias editing impacts on different components of language models.", 'score': 6, 'issue_id': 2658, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 марта', 'en': 'March 11', 'zh': '3月11日'}, 'hash': '183584887772b6e7', 'authors': ['Xin Xu', 'Wei Xu', 'Ningyu Zhang', 'Julian McAuley'], 'affiliations': ['Georgia Institute of Technology', 'University of California, San Diego', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.08588.jpg', 'data': {'categories': ['#hallucinations', '#ethics', '#architecture', '#data', '#training'], 'emoji': '⚖️', 'ru': {'title': 'Эффективное устранение предубеждений в языковых моделях без потери их возможностей', 'desc': 'Статья представляет метод BiasEdit для устранения стереотипных предубеждений в языковых моделях. В отличие от существующих методов дебиасинга, BiasEdit использует легковесные сети-редакторы для генерации обновлений параметров модели. Метод применяет функцию потерь для дебиасинга, направляющую редакторы на локальное редактирование частичных параметров, одновременно сохраняя языковые способности модели. Эксперименты показывают эффективность BiasEdit в устранении предубеждений без существенного влияния на общие возможности языковых моделей.'}, 'en': {'title': 'BiasEdit: Efficiently Editing Bias in Language Models', 'desc': "This paper introduces BiasEdit, a novel method designed to reduce stereotypical biases in language models. Unlike traditional debiasing techniques that often fail to effectively alter biased representations, BiasEdit utilizes lightweight networks to make targeted updates to the model's parameters. It incorporates a debiasing loss to guide these edits while ensuring that the model's language processing abilities remain intact through a retention loss. The results show that BiasEdit is not only effective in reducing bias but also maintains the overall performance of the language models across various tasks."}, 'zh': {'title': '高效去偏见，提升语言模型公正性', 'desc': '本研究提出了一种名为BiasEdit的模型编辑方法，旨在通过轻量级网络去除语言模型中的刻板偏见。与传统的去偏见策略相比，BiasEdit能够高效地进行局部参数编辑，同时保持语言模型的能力。该方法使用去偏见损失指导编辑网络进行参数更新，并通过保留损失确保编辑过程中的语言建模能力不受影响。实验结果表明，BiasEdit在消除偏见方面表现出色，且对语言模型的整体能力影响较小。'}}}, {'id': 'https://huggingface.co/papers/2503.08684', 'title': 'Perplexity Trap: PLM-Based Retrievers Overrate Low Perplexity Documents', 'url': 'https://huggingface.co/papers/2503.08684', 'abstract': 'Previous studies have found that PLM-based retrieval models exhibit a preference for LLM-generated content, assigning higher relevance scores to these documents even when their semantic quality is comparable to human-written ones. This phenomenon, known as source bias, threatens the sustainable development of the information access ecosystem. However, the underlying causes of source bias remain unexplored. In this paper, we explain the process of information retrieval with a causal graph and discover that PLM-based retrievers learn perplexity features for relevance estimation, causing source bias by ranking the documents with low perplexity higher. Theoretical analysis further reveals that the phenomenon stems from the positive correlation between the gradients of the loss functions in language modeling task and retrieval task. Based on the analysis, a causal-inspired inference-time debiasing method is proposed, called Causal Diagnosis and Correction (CDC). CDC first diagnoses the bias effect of the perplexity and then separates the bias effect from the overall estimated relevance score. Experimental results across three domains demonstrate the superior debiasing effectiveness of CDC, emphasizing the validity of our proposed explanatory framework. Source codes are available at https://github.com/WhyDwelledOnAi/Perplexity-Trap.', 'score': 5, 'issue_id': 2663, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 марта', 'en': 'March 11', 'zh': '3月11日'}, 'hash': '13216e7922903487', 'authors': ['Haoyu Wang', 'Sunhao Dai', 'Haiyuan Zhao', 'Liang Pang', 'Xiao Zhang', 'Gang Wang', 'Zhenhua Dong', 'Jun Xu', 'Ji-Rong Wen'], 'affiliations': ['CAS Key Laboratory of AI Safety, Institute of Computing Technology, Beijing, China', 'Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China', 'Huawei Noahs Ark Lab, Shenzhen, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.08684.jpg', 'data': {'categories': ['#ethics', '#data', '#interpretability', '#inference'], 'emoji': '🔍', 'ru': {'title': 'Преодоление предвзятости поисковых систем к ИИ-контенту', 'desc': 'Исследование посвящено проблеме предвзятости моделей информационного поиска на основе предобученных языковых моделей (PLM) в пользу контента, сгенерированного большими языковыми моделями (LLM). Авторы объясняют этот феномен с помощью причинно-следственного графа и обнаруживают, что PLM-ретриверы используют перплексию для оценки релевантности, что приводит к предпочтению документов с низкой перплексией. Теоретический анализ показывает, что это связано с положительной корреляцией между градиентами функций потерь в задачах языкового моделирования и поиска. На основе этого анализа авторы предлагают метод дебиасинга во время вывода, называемый Causal Diagnosis and Correction (CDC).'}, 'en': {'title': 'Unraveling Source Bias in PLM Retrieval Models', 'desc': 'This paper investigates the issue of source bias in PLM-based retrieval models, where documents generated by large language models (LLMs) are favored over human-written content. The authors use a causal graph to explain how these models learn to estimate relevance based on perplexity features, leading to a preference for documents with lower perplexity scores. They identify a correlation between the gradients of loss functions in language modeling and retrieval tasks as a root cause of this bias. To address this, they propose a new method called Causal Diagnosis and Correction (CDC), which effectively debiases the relevance scores by isolating the bias introduced by perplexity.'}, 'zh': {'title': '揭示源偏差，提升信息检索准确性', 'desc': '本研究探讨了基于预训练语言模型（PLM）的检索模型中存在的源偏差现象。研究发现，这种偏差使得模型更倾向于为大型语言模型（LLM）生成的内容分配更高的相关性评分，即使这些内容的语义质量与人类撰写的内容相当。通过因果图分析信息检索过程，揭示了模型在相关性估计中学习到的困惑度特征导致了源偏差。为了解决这一问题，提出了一种名为因果诊断与修正（CDC）的去偏方法，能够有效分离困惑度的偏差效应，提升检索模型的准确性。'}}}, {'id': 'https://huggingface.co/papers/2503.07699', 'title': 'RayFlow: Instance-Aware Diffusion Acceleration via Adaptive Flow\n  Trajectories', 'url': 'https://huggingface.co/papers/2503.07699', 'abstract': "Diffusion models have achieved remarkable success across various domains. However, their slow generation speed remains a critical challenge. Existing acceleration methods, while aiming to reduce steps, often compromise sample quality, controllability, or introduce training complexities. Therefore, we propose RayFlow, a novel diffusion framework that addresses these limitations. Unlike previous methods, RayFlow guides each sample along a unique path towards an instance-specific target distribution. This method minimizes sampling steps while preserving generation diversity and stability. Furthermore, we introduce Time Sampler, an importance sampling technique to enhance training efficiency by focusing on crucial timesteps. Extensive experiments demonstrate RayFlow's superiority in generating high-quality images with improved speed, control, and training efficiency compared to existing acceleration techniques.", 'score': 5, 'issue_id': 2656, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '49ebba2cb63d91f8', 'authors': ['Huiyang Shao', 'Xin Xia', 'Yuhong Yang', 'Yuxi Ren', 'Xing Wang', 'Xuefeng Xiao'], 'affiliations': ['ByteDance Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2503.07699.jpg', 'data': {'categories': ['#cv', '#diffusion', '#optimization', '#training'], 'emoji': '🚀', 'ru': {'title': 'RayFlow: быстрая и качественная генерация изображений с помощью диффузионных моделей', 'desc': 'RayFlow - это новая система диффузионных моделей, которая решает проблему медленной генерации изображений. Она направляет каждый сэмпл по уникальному пути к индивидуальному целевому распределению, минимизируя количество шагов без ущерба для качества и разнообразия. RayFlow также включает метод Time Sampler для повышения эффективности обучения путем фокусировки на ключевых временных шагах. Эксперименты показывают превосходство RayFlow над существующими методами ускорения в скорости, контроле и эффективности обучения.'}, 'en': {'title': 'RayFlow: Accelerating Diffusion Models Without Compromise', 'desc': 'This paper introduces RayFlow, a new diffusion model framework designed to improve the speed of image generation without sacrificing quality. Unlike traditional methods that often reduce the number of steps at the cost of sample diversity or stability, RayFlow customizes the sampling path for each instance, ensuring a more efficient and effective generation process. Additionally, the authors present Time Sampler, an importance sampling technique that optimizes training by prioritizing key timesteps. Experimental results show that RayFlow outperforms existing methods in generating high-quality images more quickly and with better control.'}, 'zh': {'title': 'RayFlow：加速扩散模型的新方法', 'desc': '扩散模型在多个领域取得了显著成功，但其生成速度较慢仍然是一个关键挑战。现有的加速方法虽然旨在减少步骤，但往往会影响样本质量、可控性或增加训练复杂性。为此，我们提出了RayFlow，这是一种新颖的扩散框架，能够解决这些限制。RayFlow通过引导每个样本沿着独特路径朝向特定目标分布，最小化采样步骤，同时保持生成的多样性和稳定性。'}}}, {'id': 'https://huggingface.co/papers/2503.05860', 'title': 'Benchmarking AI Models in Software Engineering: A Review, Search Tool,\n  and Enhancement Protocol', 'url': 'https://huggingface.co/papers/2503.05860', 'abstract': "Benchmarks are essential for consistent evaluation and reproducibility. The integration of Artificial Intelligence into Software Engineering (AI4SE) has given rise to numerous benchmarks for tasks such as code generation and bug fixing. However, this surge presents challenges: (1) scattered benchmark knowledge across tasks, (2) difficulty in selecting relevant benchmarks, (3) the absence of a uniform standard for benchmark development, and (4) limitations of existing benchmarks. In this paper, we review 173 studies and identify 204 AI4SE benchmarks. We classify these benchmarks, analyze their limitations, and expose gaps in practices. Based on our review, we created BenchScout, a semantic search tool to find relevant benchmarks, using automated clustering of the contexts from associated studies. We conducted a user study with 22 participants to evaluate BenchScout's usability, effectiveness, and intuitiveness which resulted in average scores of 4.5, 4.0, and 4.1 out of 5. To advance benchmarking standards, we propose BenchFrame, a unified method to enhance benchmark quality. As a case study, we applied BenchFrame to the HumanEval benchmark and addressed its main limitations. This led to <PRE_TAG>HumanEvalNext</POST_TAG>, featuring (1) corrected errors, (2) improved language conversion, (3) expanded test coverage, and (4) increased difficulty. We then evaluated ten state-of-the-art code language models on HumanEval, HumanEvalPlus, and <PRE_TAG>HumanEvalNext</POST_TAG>. On <PRE_TAG>HumanEvalNext</POST_TAG>, models showed a pass@1 score reduction of 31.22% and 19.94% compared to HumanEval and HumanEvalPlus, respectively.", 'score': 5, 'issue_id': 2661, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 марта', 'en': 'March 7', 'zh': '3月7日'}, 'hash': 'dee962dca6de084b', 'authors': ['Roham Koohestani', 'Philippe de Bekker', 'Maliheh Izadi'], 'affiliations': ['Delft University of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2503.05860.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#survey'], 'emoji': '🧪', 'ru': {'title': 'Совершенствование бенчмарков AI4SE: от анализа к инновациям', 'desc': 'Статья посвящена анализу и улучшению бенчмарков в области интеграции искусственного интеллекта в программную инженерию (AI4SE). Авторы рассмотрели 173 исследования и выявили 204 бенчмарка AI4SE, классифицировав их и проанализировав ограничения. На основе обзора был создан инструмент семантического поиска BenchScout для поиска релевантных бенчмарков. Предложен унифицированный метод BenchFrame для повышения качества бенчмарков, который был применен к бенчмарку HumanEval, что привело к созданию улучшенной версии HumanEvalNext.'}, 'en': {'title': 'Enhancing AI4SE Benchmarking with BenchScout and BenchFrame', 'desc': 'This paper addresses the challenges of evaluating benchmarks in the integration of Artificial Intelligence into Software Engineering (AI4SE). It reviews 173 studies to identify and classify 204 benchmarks, highlighting their limitations and gaps in current practices. The authors introduce BenchScout, a semantic search tool that helps users find relevant benchmarks through automated clustering. Additionally, they propose BenchFrame, a unified method to improve benchmark quality, demonstrated through the enhancement of the HumanEval benchmark into HumanEvalNext, which features significant improvements in error correction and test coverage.'}, 'zh': {'title': '提升基准测试质量的统一方法', 'desc': '本论文探讨了人工智能在软件工程中的基准测试（AI4SE）的重要性，分析了173项研究并识别出204个基准。我们发现现有基准存在知识分散、选择困难、缺乏统一标准和局限性等问题。为了解决这些问题，我们开发了BenchScout，一个语义搜索工具，帮助用户找到相关基准，并提出了BenchFrame，一个统一的方法来提升基准质量。通过对HumanEval基准的案例研究，我们创建了HumanEvalNext，修正了错误、改善了语言转换、扩展了测试覆盖率并增加了难度。'}}}, {'id': 'https://huggingface.co/papers/2503.08689', 'title': 'QuoTA: Query-oriented Token Assignment via CoT Query Decouple for Long\n  Video Comprehension', 'url': 'https://huggingface.co/papers/2503.08689', 'abstract': 'Recent advances in long video understanding typically mitigate visual redundancy through visual token pruning based on attention distribution. However, while existing methods employ post-hoc low-response token pruning in decoder layers, they overlook the input-level semantic correlation between visual tokens and instructions (query). In this paper, we propose QuoTA, an ante-hoc training-free modular that extends existing large video-language models (LVLMs) for visual token assignment based on query-oriented frame-level importance assessment. The query-oriented token selection is crucial as it aligns visual processing with task-specific requirements, optimizing token budget utilization while preserving semantically relevant content. Specifically, (i) QuoTA strategically allocates frame-level importance scores based on query relevance, enabling one-time visual token assignment before cross-modal interactions in decoder layers, (ii) we decouple the query through Chain-of-Thoughts reasoning to facilitate more precise LVLM-based frame importance scoring, and (iii) QuoTA offers a plug-and-play functionality that extends to existing LVLMs. Extensive experimental results demonstrate that implementing QuoTA with LLaVA-Video-7B yields an average performance improvement of 3.2% across six benchmarks (including Video-MME and MLVU) while operating within an identical visual token budget as the baseline. Codes are open-sourced at https://github.com/MAC-AutoML/QuoTA.', 'score': 4, 'issue_id': 2655, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 марта', 'en': 'March 11', 'zh': '3月11日'}, 'hash': '7dbb5f0edfd69aad', 'authors': ['Yongdong Luo', 'Wang Chen', 'Xiawu Zheng', 'Weizhong Huang', 'Shukang Yin', 'Haojia Lin', 'Chaoyou Fu', 'Jinfa Huang', 'Jiayi Ji', 'Jiebo Luo', 'Rongrong Ji'], 'affiliations': ['Nanjing University', 'University of Rochester', 'Xiamen University'], 'pdf_title_img': 'assets/pdf/title_img/2503.08689.jpg', 'data': {'categories': ['#alignment', '#multimodal', '#long_context', '#architecture', '#video', '#benchmark', '#open_source', '#optimization'], 'emoji': '🎥', 'ru': {'title': 'Умный отбор кадров для эффективного понимания видео ИИ', 'desc': 'Статья представляет QuoTA - новый модуль для улучшения понимания длинных видео большими видео-языковыми моделями (LVLM). QuoTA выполняет отбор визуальных токенов на основе оценки важности кадров с учетом запроса пользователя, что позволяет более эффективно использовать ограниченный бюджет токенов. Метод включает декомпозицию запроса через рассуждения по цепочке мыслей для более точной оценки важности кадров. Эксперименты показывают, что применение QuoTA с LLaVA-Video-7B дает среднее улучшение производительности на 3.2% по шести бенчмаркам при том же бюджете визуальных токенов.'}, 'en': {'title': 'Optimizing Visual Token Selection for Better Video Understanding', 'desc': 'This paper introduces QuoTA, a new method for improving long video understanding by optimizing how visual tokens are selected based on their relevance to specific queries. Unlike previous approaches that prune tokens after processing, QuoTA assesses the importance of visual tokens before they are used, ensuring that only the most relevant information is retained. By using Chain-of-Thoughts reasoning, QuoTA enhances the accuracy of frame importance scoring, allowing for better alignment between visual data and task requirements. The results show that QuoTA can significantly boost performance in existing large video-language models while maintaining the same token budget.'}, 'zh': {'title': '优化视觉标记分配，提升视频理解性能', 'desc': '本文提出了一种名为QuoTA的模块，旨在改进长视频理解中的视觉标记分配。与现有方法不同，QuoTA在输入层面上考虑视觉标记与查询之间的语义关联，从而优化标记的使用效率。通过基于查询相关性的帧级重要性评估，QuoTA能够在解码器层之前进行一次性视觉标记分配。实验结果表明，QuoTA在多个基准测试中显著提高了性能，同时保持了与基线相同的视觉标记预算。'}}}, {'id': 'https://huggingface.co/papers/2503.08507', 'title': 'Referring to Any Person', 'url': 'https://huggingface.co/papers/2503.08507', 'abstract': 'Humans are undoubtedly the most important participants in computer vision, and the ability to detect any individual given a natural language description, a task we define as referring to any person, holds substantial practical value. However, we find that existing models generally fail to achieve real-world usability, and current benchmarks are limited by their focus on one-to-one referring, that hinder progress in this area. In this work, we revisit this task from three critical perspectives: task definition, dataset design, and model architecture. We first identify five aspects of referable entities and three distinctive characteristics of this task. Next, we introduce HumanRef, a novel dataset designed to tackle these challenges and better reflect real-world applications. From a model design perspective, we integrate a multimodal large language model with an object detection framework, constructing a robust referring model named RexSeek. Experimental results reveal that state-of-the-art models, which perform well on commonly used benchmarks like RefCOCO/+/g, struggle with HumanRef due to their inability to detect multiple individuals. In contrast, RexSeek not only excels in human referring but also generalizes effectively to common object referring, making it broadly applicable across various perception tasks. Code is available at https://github.com/IDEA-Research/RexSeek', 'score': 4, 'issue_id': 2661, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 марта', 'en': 'March 11', 'zh': '3月11日'}, 'hash': '905fc0de3c82fe2e', 'authors': ['Qing Jiang', 'Lin Wu', 'Zhaoyang Zeng', 'Tianhe Ren', 'Yuda Xiong', 'Yihao Chen', 'Qin Liu', 'Lei Zhang'], 'affiliations': ['International Digital Economy Academy (IDEA)', 'South China University of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2503.08507.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#games', '#optimization', '#interpretability', '#architecture', '#cv', '#multimodal'], 'emoji': '🔍', 'ru': {'title': 'Революция в поиске людей: от текста к изображению', 'desc': 'Статья представляет новый подход к задаче поиска людей по текстовому описанию в компьютерном зрении. Авторы предлагают датасет HumanRef, который лучше отражает реальные сценарии применения, чем существующие бенчмарки. Они также разрабатывают модель RexSeek, объединяющую мультимодальную языковую модель с детектором объектов. Эксперименты показывают, что RexSeek превосходит современные модели на новом датасете и хорошо обобщается на задачу поиска обычных объектов.'}, 'en': {'title': 'Revolutionizing Person Detection with HumanRef and RexSeek', 'desc': 'This paper addresses the challenge of detecting individuals in computer vision using natural language descriptions, a task known as referring to any person. The authors highlight that existing models often fall short in real-world scenarios and current benchmarks are too narrow, focusing mainly on one-to-one referring. They propose a new dataset called HumanRef, which is designed to better represent the complexities of real-world applications. Additionally, they introduce a new model, RexSeek, which combines a multimodal large language model with an object detection framework, showing improved performance in both human and object referring tasks.'}, 'zh': {'title': '人类识别的新突破：RexSeek模型', 'desc': '本论文探讨了计算机视觉中人类个体识别的重要性，提出了一种新的任务定义——根据自然语言描述识别个体。现有模型在实际应用中表现不佳，且现有基准测试主要集中在一对一的识别上，限制了该领域的发展。为了解决这些问题，作者设计了一个新的数据集HumanRef，并提出了一个名为RexSeek的多模态模型，能够有效识别多个个体。实验结果表明，RexSeek在识别人类和常见物体方面表现优异，具有广泛的应用潜力。'}}}, {'id': 'https://huggingface.co/papers/2503.08417', 'title': 'AnyMoLe: Any Character Motion In-betweening Leveraging Video Diffusion\n  Models', 'url': 'https://huggingface.co/papers/2503.08417', 'abstract': "Despite recent advancements in learning-based motion in-betweening, a key limitation has been overlooked: the requirement for character-specific datasets. In this work, we introduce AnyMoLe, a novel method that addresses this limitation by leveraging video diffusion models to generate motion in-between frames for arbitrary characters without external data. Our approach employs a two-stage frame generation process to enhance contextual understanding. Furthermore, to bridge the domain gap between real-world and rendered character animations, we introduce ICAdapt, a fine-tuning technique for video diffusion models. Additionally, we propose a ``motion-video mimicking'' optimization technique, enabling seamless motion generation for characters with arbitrary joint structures using 2D and 3D-aware features. AnyMoLe significantly reduces data dependency while generating smooth and realistic transitions, making it applicable to a wide range of motion in-betweening tasks.", 'score': 4, 'issue_id': 2662, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 марта', 'en': 'March 11', 'zh': '3月11日'}, 'hash': '7610945506fac90e', 'authors': ['Kwan Yun', 'Seokhyeon Hong', 'Chaelin Kim', 'Junyong Noh'], 'affiliations': ['KAIST, Visual Media Lab'], 'pdf_title_img': 'assets/pdf/title_img/2503.08417.jpg', 'data': {'categories': ['#3d', '#training', '#diffusion', '#optimization', '#video', '#dataset'], 'emoji': '🎭', 'ru': {'title': 'Универсальная анимация без специальных данных', 'desc': "AnyMoLe - это новый метод генерации промежуточных кадров анимации для произвольных персонажей без использования специфических наборов данных. Он основан на видео-диффузионных моделях и использует двухэтапный процесс генерации кадров для улучшения понимания контекста. Метод включает технику доводки ICAdapt для преодоления разрыва между реальными и рендерингованными анимациями. Также предложена оптимизация 'motion-video mimicking' для создания плавных движений персонажей с произвольной структурой суставов."}, 'en': {'title': 'AnyMoLe: Motion Generation Without Character-Specific Data', 'desc': "This paper presents AnyMoLe, a new method for generating motion in-between frames for various characters without needing specific datasets. It utilizes video diffusion models in a two-stage frame generation process to improve the understanding of context in animations. The authors introduce ICAdapt, a technique that fine-tunes these models to better connect real-world and animated character movements. Additionally, a 'motion-video mimicking' optimization is proposed, allowing for fluid motion generation across different character joint structures, thus minimizing data dependency and enhancing the realism of transitions."}, 'zh': {'title': 'AnyMoLe：无数据依赖的角色运动插值新方法', 'desc': '本文介绍了一种新方法AnyMoLe，旨在解决基于学习的运动插值中对特定角色数据集的依赖问题。该方法利用视频扩散模型生成任意角色的帧间运动，无需外部数据。我们采用了两阶段的帧生成过程，以增强上下文理解，并引入了ICAdapt技术来缩小真实世界与渲染角色动画之间的领域差距。此外，我们提出了“运动视频模仿”优化技术，使得使用2D和3D特征的任意关节结构角色的运动生成变得无缝。'}}}, {'id': 'https://huggingface.co/papers/2503.06492', 'title': 'VisualSimpleQA: A Benchmark for Decoupled Evaluation of Large\n  Vision-Language Models in Fact-Seeking Question Answering', 'url': 'https://huggingface.co/papers/2503.06492', 'abstract': 'Large vision-language models (LVLMs) have demonstrated remarkable achievements, yet the generation of non-factual responses remains prevalent in fact-seeking question answering (QA). Current multimodal fact-seeking benchmarks primarily focus on comparing model outputs to ground truth answers, providing limited insights into the performance of modality-specific modules. To bridge this gap, we introduce VisualSimpleQA, a multimodal fact-seeking benchmark with two key features. First, it enables streamlined and decoupled evaluation of LVLMs in visual and linguistic modalities. Second, it incorporates well-defined difficulty criteria to guide human annotation and facilitates the extraction of a challenging subset, <PRE_TAG>VisualSimpleQA-hard</POST_TAG>. Experiments on 15 LVLMs show that even state-of-the-art models such as GPT-4o achieve merely 60%+ correctness in multimodal fact-seeking QA on VisualSimpleQA and 30%+ on <PRE_TAG>VisualSimpleQA-hard</POST_TAG>. Furthermore, the decoupled evaluation across these models highlights substantial opportunities for improvement in both visual and linguistic modules. The dataset is available at https://huggingface.co/datasets/WYLing/VisualSimpleQA.', 'score': 4, 'issue_id': 2659, 'pub_date': '2025-03-09', 'pub_date_card': {'ru': '9 марта', 'en': 'March 9', 'zh': '3月9日'}, 'hash': '5f5a7152d7773f38', 'authors': ['Yanling Wang', 'Yihan Zhao', 'Xiaodong Chen', 'Shasha Guo', 'Lixin Liu', 'Haoyang Li', 'Yong Xiao', 'Jing Zhang', 'Qi Li', 'Ke Xu'], 'affiliations': ['Renmin University of China', 'Tencent', 'Tsinghua University', 'Zhongguancun Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2503.06492.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#hallucinations', '#multimodal', '#interpretability'], 'emoji': '🔍', 'ru': {'title': 'VisualSimpleQA: новый способ оценки мультимодальных моделей', 'desc': 'Статья представляет новый бенчмарк VisualSimpleQA для оценки мультимодальных моделей в задаче ответов на фактологические вопросы по изображениям. Бенчмарк позволяет отдельно оценивать визуальные и языковые модули крупных визуально-языковых моделей (LVLM). В нем также есть усложненная версия VisualSimpleQA-hard для более тщательного тестирования. Эксперименты показали, что даже современные модели вроде GPT-4o достигают лишь 60% точности на VisualSimpleQA и 30% на сложной версии.'}, 'en': {'title': 'Enhancing Accuracy in Multimodal Fact-Seeking with VisualSimpleQA', 'desc': 'This paper addresses the issue of non-factual responses in large vision-language models (LVLMs) during fact-seeking question answering (QA). It introduces VisualSimpleQA, a new benchmark that allows for separate evaluation of visual and linguistic capabilities of these models. The benchmark includes specific difficulty levels to aid in human annotation and identifies a challenging subset called VisualSimpleQA-hard. Experiments reveal that even advanced models like GPT-4o struggle with accuracy, achieving only around 60% correctness overall and 30% on the harder subset, indicating significant room for improvement in both visual and linguistic processing.'}, 'zh': {'title': '提升多模态问答的准确性', 'desc': '大型视觉语言模型（LVLMs）在事实寻求问答中取得了显著成就，但仍然存在生成不准确回答的问题。当前的多模态事实寻求基准主要关注模型输出与真实答案的比较，缺乏对特定模态模块性能的深入分析。为了解决这个问题，我们提出了VisualSimpleQA，这是一个具有两个关键特征的多模态事实寻求基准。通过简化和解耦的评估方式，VisualSimpleQA能够更好地评估LVLMs在视觉和语言模态上的表现，并引入明确的难度标准以指导人工标注。'}}}, {'id': 'https://huggingface.co/papers/2502.18858', 'title': 'Evaluating Intelligence via Trial and Error', 'url': 'https://huggingface.co/papers/2502.18858', 'abstract': "Intelligence is a crucial trait for species to find solutions within a limited number of trial-and-error attempts. Building on this idea, we introduce Survival Game as a framework to evaluate intelligence based on the number of failed attempts in a trial-and-error process. Fewer failures indicate higher intelligence. When the expectation and variance of failure counts are both finite, it signals the ability to consistently find solutions to new challenges, which we define as the Autonomous Level of intelligence. Using Survival Game, we comprehensively evaluate existing AI systems. Our results show that while AI systems achieve the Autonomous Level in simple tasks, they are still far from it in more complex tasks, such as vision, search, recommendation, and language. While scaling current AI technologies might help, this would come at an astronomical cost. Projections suggest that achieving the Autonomous Level for general tasks would require 10^{26} parameters. To put this into perspective, loading such a massive model requires so many H100 GPUs that their total value is 10^{7} times that of Apple Inc.'s market value. Even with Moore's Law, supporting such a parameter scale would take 70 years. This staggering cost highlights the complexity of human tasks and the inadequacies of current AI technologies. To further investigate this phenomenon, we conduct a theoretical analysis of Survival Game and its experimental results. Our findings suggest that human tasks possess a criticality property. As a result, Autonomous Level requires a deep understanding of the task's underlying mechanisms. Current AI systems, however, do not fully grasp these mechanisms and instead rely on superficial mimicry, making it difficult for them to reach an autonomous level. We believe Survival Game can not only guide the future development of AI but also offer profound insights into human intelligence.", 'score': 4, 'issue_id': 2656, 'pub_date': '2025-02-26', 'pub_date_card': {'ru': '26 февраля', 'en': 'February 26', 'zh': '2月26日'}, 'hash': '54797794006daa5e', 'authors': ['Jingtao Zhan', 'Jiahao Zhao', 'Jiayu Li', 'Yiqun Liu', 'Bo Zhang', 'Qingyao Ai', 'Jiaxin Mao', 'Hongning Wang', 'Min Zhang', 'Shaoping Ma'], 'affiliations': ['Renmin University of China', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2502.18858.jpg', 'data': {'categories': ['#reasoning', '#training', '#agents', '#agi', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'Автономный интеллект: путь через Survival Game', 'desc': 'Статья представляет концепцию Survival Game для оценки интеллекта на основе количества неудачных попыток в процессе проб и ошибок. Авторы вводят понятие Автономного Уровня интеллекта и оценивают существующие системы искусственного интеллекта по этому критерию. Результаты показывают, что для достижения Автономного Уровня в сложных задачах потребуется огромное количество параметров модели, что экономически нецелесообразно. Исследование выявляет, что человеческие задачи обладают свойством критичности, требующим глубокого понимания механизмов, которого не хватает современным системам ИИ.'}, 'en': {'title': 'Measuring Intelligence: The Survival Game Framework', 'desc': 'This paper introduces the Survival Game framework to measure intelligence based on the number of failed attempts in problem-solving. It defines the Autonomous Level of intelligence as the ability to find solutions with fewer failures, indicating a deeper understanding of tasks. The study evaluates existing AI systems, revealing that while they perform well on simple tasks, they struggle with complex ones like vision and language. The findings suggest that achieving a high Autonomous Level in AI would require an impractical scale of parameters, highlighting the limitations of current technologies and the complexity of human-like intelligence.'}, 'zh': {'title': '生存游戏：评估智能的新框架', 'desc': '本论文提出了一种名为生存游戏的框架，用于评估智能水平，特别是在试错过程中失败次数的多少。失败次数越少，表示智能水平越高。研究表明，尽管现有的人工智能系统在简单任务中达到了自主水平，但在复杂任务（如视觉、搜索、推荐和语言处理）中仍然远未达到。我们认为，生存游戏不仅可以指导未来的人工智能发展，还能深入理解人类智能的本质。'}}}, {'id': 'https://huggingface.co/papers/2503.06594', 'title': 'Beyond Decoder-only: Large Language Models Can be Good Encoders for\n  Machine Translation', 'url': 'https://huggingface.co/papers/2503.06594', 'abstract': 'The field of neural machine translation (NMT) has changed with the advent of large language models (<PRE_TAG>LLMs)</POST_TAG>. Much of the recent emphasis in natural language processing (NLP) has been on modeling machine translation and many other problems using a single pre-trained <PRE_TAG>Transformer decoder</POST_TAG>, while encoder-decoder architectures, which were the standard in earlier NMT models, have received relatively less attention. In this paper, we explore translation models that are universal, efficient, and easy to optimize, by marrying the world of LLMs with the world of NMT. We apply LLMs to NMT encoding and leave the NMT decoder unchanged. We also develop methods for adapting LLMs to work better with the NMT decoder. Furthermore, we construct a new dataset involving multiple tasks to assess how well the machine translation system generalizes across various tasks. Evaluations on the WMT and our datasets show that results using our method match or surpass a range of baselines in terms of translation quality, but achieve 2.4 sim 6.5 times inference speedups and a 75% reduction in the memory footprint of the KV cache. It also demonstrates strong generalization across a variety of translation-related tasks.', 'score': 3, 'issue_id': 2660, 'pub_date': '2025-03-09', 'pub_date_card': {'ru': '9 марта', 'en': 'March 9', 'zh': '3月9日'}, 'hash': 'bea0df269ed71c2d', 'authors': ['Yingfeng Luo', 'Tong Zheng', 'Yongyu Mu', 'Bei Li', 'Qinghong Zhang', 'Yongqi Gao', 'Ziqiang Xu', 'Peinan Feng', 'Xiaoqian Liu', 'Tong Xiao', 'Jingbo Zhu'], 'affiliations': ['NLP Lab, Northeastern University, Shenyang, China', 'NiuTrans Research, Shenyang, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.06594.jpg', 'data': {'categories': ['#multilingual', '#machine_translation', '#dataset', '#inference', '#training'], 'emoji': '🚀', 'ru': {'title': 'Объединение мощи LLM и эффективности NMT для универсального машинного перевода', 'desc': 'Статья рассматривает применение больших языковых моделей (LLM) в нейронном машинном переводе (NMT). Авторы предлагают использовать LLM для кодирования в NMT, оставляя декодер NMT без изменений. Разработаны методы адаптации LLM для лучшей работы с декодером NMT. Эксперименты показывают, что предложенный подход не уступает baseline-моделям по качеству перевода, но обеспечивает ускорение вывода в 2.4-6.5 раз и снижение объема памяти KV-кэша на 75%.'}, 'en': {'title': 'Revolutionizing Translation: Merging LLMs with NMT for Efficiency and Quality', 'desc': 'This paper discusses advancements in neural machine translation (NMT) by integrating large language models (LLMs) with traditional NMT architectures. The authors propose a novel approach that utilizes LLMs for encoding while maintaining the existing NMT decoder, enhancing efficiency and optimization. They introduce a new dataset to evaluate the generalization of their translation models across multiple tasks. Results indicate that their method not only improves translation quality but also significantly increases inference speed and reduces memory usage.'}, 'zh': {'title': '结合LLMs与NMT，提升翻译效率与质量', 'desc': '本文探讨了如何将大型语言模型（LLMs）与神经机器翻译（NMT）结合，以提高翻译模型的通用性和效率。我们采用LLMs进行NMT编码，同时保持NMT解码器不变，并开发了适应LLMs与NMT解码器更好配合的方法。通过构建一个包含多任务的新数据集，我们评估了机器翻译系统在不同任务上的泛化能力。实验结果表明，我们的方法在翻译质量上与多种基线相当或更优，同时在推理速度上提高了2.4到6.5倍，KV缓存的内存占用减少了75%。'}}}, {'id': 'https://huggingface.co/papers/2503.08478', 'title': 'NullFace: Training-Free Localized Face Anonymization', 'url': 'https://huggingface.co/papers/2503.08478', 'abstract': "Privacy concerns around ever increasing number of cameras are increasing in today's digital age. Although existing anonymization methods are able to obscure identity information, they often struggle to preserve the utility of the images. In this work, we introduce a training-free method for face anonymization that preserves key non-identity-related attributes. Our approach utilizes a pre-trained text-to-image diffusion model without requiring optimization or training. It begins by inverting the input image to recover its initial noise. The noise is then denoised through an identity-conditioned diffusion process, where modified identity embeddings ensure the anonymized face is distinct from the original identity. Our approach also supports localized anonymization, giving users control over which facial regions are anonymized or kept intact. Comprehensive evaluations against state-of-the-art methods show our approach excels in anonymization, attribute preservation, and image quality. Its flexibility, robustness, and practicality make it well-suited for real-world applications. Code and data can be found at https://github.com/hanweikung/nullface .", 'score': 2, 'issue_id': 2659, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 марта', 'en': 'March 11', 'zh': '3月11日'}, 'hash': '150c9c8854f08130', 'authors': ['Han-Wei Kung', 'Tuomas Varanka', 'Terence Sim', 'Nicu Sebe'], 'affiliations': ['National University of Singapore', 'University of Oulu', 'University of Trento'], 'pdf_title_img': 'assets/pdf/title_img/2503.08478.jpg', 'data': {'categories': ['#ethics', '#cv', '#training', '#diffusion'], 'emoji': '🎭', 'ru': {'title': 'Анонимизация лиц без потери ключевых атрибутов', 'desc': 'Статья представляет новый метод анонимизации лиц на изображениях, не требующий обучения. Метод использует предобученную диффузионную модель для генерации текста по изображению, начиная с инвертирования входного изображения для восстановления исходного шума. Затем происходит процесс удаления шума с условием по идентичности, где модифицированные эмбеддинги идентичности обеспечивают отличие анонимизированного лица от оригинала. Подход позволяет сохранить ключевые атрибуты, не связанные с идентичностью, и превосходит современные методы по качеству анонимизации и сохранению атрибутов.'}, 'en': {'title': 'Effortless Face Anonymization with Preserved Attributes', 'desc': 'This paper presents a novel method for face anonymization that does not require any training, making it efficient and practical. It leverages a pre-trained text-to-image diffusion model to anonymize faces while preserving important non-identity-related features. The method works by inverting the input image to retrieve its initial noise, which is then processed through a diffusion model that modifies identity embeddings. Additionally, it allows users to selectively anonymize specific facial regions, ensuring flexibility and control over the anonymization process.'}, 'zh': {'title': '无训练的面部匿名化方法，保护隐私与图像质量并存', 'desc': '在数字时代，越来越多的摄像头引发了隐私问题。现有的匿名化方法虽然可以模糊身份信息，但往往难以保持图像的实用性。本文提出了一种无训练的面部匿名化方法，能够保留关键的非身份相关属性。该方法利用预训练的文本到图像扩散模型，通过身份条件扩散过程去噪声，确保匿名化的面孔与原始身份明显不同，同时支持局部匿名化，用户可以控制哪些面部区域被匿名化或保留。'}}}, {'id': 'https://huggingface.co/papers/2503.08102', 'title': 'AI-native Memory 2.0: Second Me', 'url': 'https://huggingface.co/papers/2503.08102', 'abstract': 'Human interaction with the external world fundamentally involves the exchange of personal memory, whether with other individuals, websites, applications, or, in the future, AI agents. A significant portion of this interaction is redundant, requiring users to repeatedly provide the same information across different contexts. Existing solutions, such as browser-stored credentials, autofill mechanisms, and unified authentication systems, have aimed to mitigate this redundancy by serving as intermediaries that store and retrieve commonly used user data. The advent of large language models (LLMs) presents an opportunity to redefine memory management through an AI-native paradigm: SECOND ME. SECOND ME acts as an intelligent, persistent memory offload system that retains, organizes, and dynamically utilizes user-specific knowledge. By serving as an intermediary in user interactions, it can autonomously generate context-aware responses, prefill required information, and facilitate seamless communication with external systems, significantly reducing cognitive load and interaction friction. Unlike traditional memory storage solutions, SECOND ME extends beyond static data retention by leveraging LLM-based memory parameterization. This enables structured organization, contextual reasoning, and adaptive knowledge retrieval, facilitating a more systematic and intelligent approach to memory management. As AI-driven personal agents like SECOND ME become increasingly integrated into digital ecosystems, SECOND ME further represents a critical step toward augmenting human-world interaction with persistent, contextually aware, and self-optimizing memory systems. We have open-sourced the fully localizable deployment system at GitHub: https://github.com/Mindverse/Second-Me.', 'score': 2, 'issue_id': 2658, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 марта', 'en': 'March 11', 'zh': '3月11日'}, 'hash': '45b3f3aee8d173f9', 'authors': ['Jiale Wei', 'Xiang Ying', 'Tao Gao', 'Felix Tao', 'Jingbo Shang'], 'affiliations': ['Mindverse.ai'], 'pdf_title_img': 'assets/pdf/title_img/2503.08102.jpg', 'data': {'categories': ['#multimodal', '#agi', '#agents', '#optimization', '#open_source'], 'emoji': '🧠', 'ru': {'title': 'SECOND ME: ИИ-ассистент для расширения вашей памяти', 'desc': 'SECOND ME - это система, использующая большие языковые модели для управления личной памятью пользователя. Она хранит, организует и динамически применяет пользовательские знания, выступая посредником во взаимодействиях. SECOND ME может автономно генерировать контекстно-зависимые ответы, заполнять формы и облегчать коммуникацию с внешними системами. В отличие от традиционных решений, система использует параметризацию памяти на основе нейросетей для более интеллектуального подхода к управлению информацией.'}, 'en': {'title': 'Revolutionizing Memory Management with AI: Meet SECOND ME!', 'desc': 'The paper introduces SECOND ME, an AI-driven memory management system designed to enhance user interactions with digital environments. It addresses the issue of redundant information sharing by autonomously storing and organizing user-specific knowledge, allowing for context-aware responses and prefilled information. Unlike traditional memory solutions, SECOND ME utilizes large language models to enable dynamic knowledge retrieval and contextual reasoning. This innovative approach aims to reduce cognitive load and improve the efficiency of human-AI interactions in various applications.'}, 'zh': {'title': '智能记忆管理，提升人机互动体验', 'desc': '这篇论文介绍了一个名为SECOND ME的智能记忆管理系统，旨在减少用户在与外部世界互动时的冗余信息输入。通过利用大型语言模型（LLMs），SECOND ME能够智能地存储、组织和动态使用用户特定的知识。与传统的记忆存储解决方案不同，SECOND ME不仅仅是静态数据的保留，而是通过上下文推理和自适应知识检索来优化用户体验。随着AI驱动的个人代理的普及，SECOND ME代表了增强人类与世界互动的重要一步。'}}}, {'id': 'https://huggingface.co/papers/2503.05066', 'title': 'Capacity-Aware Inference: Mitigating the Straggler Effect in Mixture of\n  Experts', 'url': 'https://huggingface.co/papers/2503.05066', 'abstract': 'The Mixture of Experts (MoE) is an effective architecture for scaling large language models by leveraging sparse expert activation, optimizing the trade-off between performance and efficiency. However, under expert parallelism, MoE suffers from inference inefficiencies due to imbalanced token-to-expert assignment, where some experts are overloaded while others remain underutilized. This imbalance leads to poor resource utilization and increased latency, as the most burdened expert dictates the overall delay, a phenomenon we define as the \\textit{Straggler Effect}. To mitigate this, we propose Capacity-Aware Inference, including two key techniques: (1) \\textit{Capacity-Aware Token Drop}, which discards overloaded tokens to regulate the maximum latency of MoE, and (2) \\textit{Capacity-Aware Token Reroute}, which reallocates overflowed tokens to underutilized experts, balancing the token distribution. These techniques collectively optimize both high-load and low-load expert utilization, leading to a more efficient MoE inference pipeline. Extensive experiments demonstrate the effectiveness of our methods, showing significant improvements in inference efficiency, e.g., 0.2\\% average performance increase and a 1.94times inference speedup on Mixtral-8times7B-Instruct.', 'score': 2, 'issue_id': 2672, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 марта', 'en': 'March 7', 'zh': '3月7日'}, 'hash': 'febfae347962e18e', 'authors': ['Shwai He', 'Weilin Cai', 'Jiayi Huang', 'Ang Li'], 'affiliations': ['The Hong Kong University of Science and Technology (Guangzhou)', 'University of Maryland, College Park'], 'pdf_title_img': 'assets/pdf/title_img/2503.05066.jpg', 'data': {'categories': ['#inference', '#optimization', '#architecture', '#training'], 'emoji': '⚖️', 'ru': {'title': 'Балансировка экспертов для ускорения языковых моделей', 'desc': 'Статья посвящена оптимизации архитектуры Mixture of Experts (MoE) для крупных языковых моделей. Авторы предлагают метод Capacity-Aware Inference для решения проблемы неравномерного распределения токенов между экспертами, что приводит к неэффективному использованию ресурсов и увеличению задержки. Метод включает две техники: Capacity-Aware Token Drop для отбрасывания перегруженных токенов и Capacity-Aware Token Reroute для перераспределения токенов между недогруженными экспертами. Эксперименты показали значительное улучшение эффективности вывода, включая 0.2% увеличение производительности и 1.94-кратное ускорение вывода на модели Mixtral-8x7B-Instruct.'}, 'en': {'title': 'Optimizing Expert Utilization for Efficient Inference in MoE Models', 'desc': 'The paper discusses the Mixture of Experts (MoE) architecture, which enhances large language models by using sparse expert activation to balance performance and efficiency. It identifies a problem called the Straggler Effect, where some experts are overloaded while others are underused, leading to inefficiencies during inference. To address this, the authors introduce Capacity-Aware Inference techniques, including Capacity-Aware Token Drop and Capacity-Aware Token Reroute, which help manage token distribution among experts. Their experiments show that these methods significantly improve inference efficiency, achieving a notable speedup and slight performance gain.'}, 'zh': {'title': '提升混合专家推理效率的创新方法', 'desc': '混合专家（MoE）是一种有效的架构，用于通过稀疏专家激活来扩展大型语言模型，优化性能与效率之间的权衡。然而，在专家并行处理下，MoE由于不平衡的令牌到专家的分配而面临推理效率低下的问题，导致某些专家过载而其他专家未被充分利用。这种不平衡导致资源利用率低下和延迟增加，因为最繁忙的专家决定了整体延迟，这种现象被称为“滞后效应”。为了解决这个问题，我们提出了容量感知推理，包括两个关键技术：容量感知令牌丢弃和容量感知令牌重定向，从而优化高负载和低负载专家的利用率，提升MoE推理管道的效率。'}}}, {'id': 'https://huggingface.co/papers/2503.07639', 'title': 'Mixture of Experts Made Intrinsically Interpretable', 'url': 'https://huggingface.co/papers/2503.07639', 'abstract': 'Neurons in large language models often exhibit polysemanticity, simultaneously encoding multiple unrelated concepts and obscuring interpretability. Instead of relying on post-hoc methods, we present MoE-X, a Mixture-of-Experts (MoE) language model designed to be intrinsically interpretable. Our approach is motivated by the observation that, in language models, wider networks with <PRE_TAG>sparse activations</POST_TAG> are more likely to capture interpretable factors. However, directly training such large sparse networks is computationally prohibitive. MoE architectures offer a scalable alternative by activating only a subset of experts for any given input, inherently aligning with interpretability objectives. In MoE-X, we establish this connection by rewriting the MoE layer as an equivalent sparse, large MLP. This approach enables efficient scaling of the hidden size while maintaining sparsity. To further enhance interpretability, we enforce sparse activation within each expert and redesign the routing mechanism to prioritize experts with the highest activation sparsity. These designs ensure that only the most salient features are routed and processed by the experts. We evaluate MoE-X on chess and natural language tasks, showing that it achieves performance comparable to dense models while significantly improving interpretability. MoE-X achieves a perplexity better than GPT-2, with interpretability surpassing even sparse autoencoder (SAE)-based approaches.', 'score': 2, 'issue_id': 2653, 'pub_date': '2025-03-05', 'pub_date_card': {'ru': '5 марта', 'en': 'March 5', 'zh': '3月5日'}, 'hash': '7e9a13248a2692b5', 'authors': ['Xingyi Yang', 'Constantin Venhoff', 'Ashkan Khakzar', 'Christian Schroeder de Witt', 'Puneet K. Dokania', 'Adel Bibi', 'Philip Torr'], 'affiliations': ['National University of Singapore', 'University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2503.07639.jpg', 'data': {'categories': ['#training', '#games', '#architecture', '#interpretability'], 'emoji': '🧠', 'ru': {'title': 'MoE-X: Интерпретируемые языковые модели через разреженные экспертные системы', 'desc': 'Статья представляет MoE-X - новую архитектуру языковой модели, основанную на принципе Mixture-of-Experts. Эта модель разработана для повышения интерпретируемости нейронных сетей путем использования разреженных активаций и селективного задействования экспертов. MoE-X переписывает слой MoE как эквивалентную разреженную большую многослойную перцептронную сеть, что позволяет эффективно масштабировать скрытый размер при сохранении разреженности. Результаты экспериментов показывают, что MoE-X достигает производительности, сравнимой с плотными моделями, при значительно улучшенной интерпретируемости.'}, 'en': {'title': 'MoE-X: Enhancing Interpretability in Language Models with Sparse Activations', 'desc': 'This paper introduces MoE-X, a Mixture-of-Experts language model that aims to improve interpretability in large language models by utilizing sparse activations. The authors argue that wider networks with sparse activations can better capture distinct concepts, making them more interpretable. MoE-X efficiently scales by activating only a subset of experts for each input, which aligns with the goal of enhancing interpretability. The model is evaluated on chess and natural language tasks, demonstrating performance on par with dense models while offering superior interpretability compared to existing methods.'}, 'zh': {'title': 'MoE-X：可解释的混合专家语言模型', 'desc': '在大型语言模型中，神经元常常表现出多义性，同时编码多个无关的概念，导致可解释性差。我们提出了MoE-X，这是一种混合专家（MoE）语言模型，旨在内在上具有可解释性。我们的研究表明，具有稀疏激活的宽网络更有可能捕捉可解释的因素。通过激活仅一部分专家，MoE架构提供了一种可扩展的替代方案，从而在保持稀疏性的同时实现高效的隐藏层规模。'}}}, {'id': 'https://huggingface.co/papers/2503.07565', 'title': 'Inductive Moment Matching', 'url': 'https://huggingface.co/papers/2503.07565', 'abstract': 'Diffusion models and Flow Matching generate high-quality samples but are slow at inference, and distilling them into few-step models often leads to instability and extensive tuning. To resolve these trade-offs, we propose Inductive Moment Matching (IMM), a new class of generative models for one- or few-step sampling with a single-stage training procedure. Unlike distillation, IMM does not require pre-training initialization and optimization of two networks; and unlike Consistency Models, IMM guarantees distribution-level convergence and remains stable under various hyperparameters and standard model architectures. IMM surpasses diffusion models on ImageNet-256x256 with 1.99 FID using only 8 inference steps and achieves state-of-the-art 2-step FID of 1.98 on CIFAR-10 for a model trained from scratch.', 'score': 1, 'issue_id': 2670, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': 'ada37e03407170a1', 'authors': ['Linqi Zhou', 'Stefano Ermon', 'Jiaming Song'], 'affiliations': ['Luma AI', 'Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2503.07565.jpg', 'data': {'categories': ['#inference', '#cv', '#training', '#diffusion', '#optimization', '#architecture'], 'emoji': '🚀', 'ru': {'title': 'IMM: Быстрая и стабильная генерация изображений', 'desc': 'Статья представляет новый класс генеративных моделей под названием Inductive Moment Matching (IMM). IMM позволяет генерировать высококачественные образцы за один или несколько шагов, используя одноэтапную процедуру обучения. В отличие от моделей дистилляции, IMM не требует предварительного обучения и оптимизации двух сетей. Модель превосходит диффузионные модели на ImageNet-256x256 с показателем FID 1.99 при использовании всего 8 шагов вывода.'}, 'en': {'title': 'Fast and Stable Sampling with Inductive Moment Matching', 'desc': 'Inductive Moment Matching (IMM) is a new type of generative model designed to produce high-quality samples quickly, addressing the slow inference times of diffusion models and Flow Matching. Unlike traditional distillation methods, IMM simplifies the training process by eliminating the need for pre-training and the optimization of multiple networks. It also ensures stability and convergence at the distribution level, making it robust against changes in hyperparameters and model architectures. IMM demonstrates superior performance, achieving a low FID score on ImageNet and CIFAR-10 with significantly fewer inference steps compared to existing models.'}, 'zh': {'title': '归纳时刻匹配：高效稳定的生成模型', 'desc': '本文提出了一种新的生成模型，称为归纳时刻匹配（IMM），旨在解决扩散模型和流匹配在推理时速度慢的问题。IMM 允许在单阶段训练过程中进行一到少步采样，而无需预训练和优化两个网络。与一致性模型不同，IMM 确保了分布级别的收敛性，并在各种超参数和标准模型架构下保持稳定。实验结果表明，IMM 在 ImageNet-256x256 数据集上以 8 次推理步骤达到了 1.99 的 FID，且在 CIFAR-10 数据集上以 2 次推理步骤达到了 1.98 的最先进 FID。'}}}, {'id': 'https://huggingface.co/papers/2503.07154', 'title': 'Ideas in Inference-time Scaling can Benefit Generative Pre-training\n  Algorithms', 'url': 'https://huggingface.co/papers/2503.07154', 'abstract': "Recent years have seen significant advancements in foundation models through generative pre-training, yet algorithmic innovation in this space has largely stagnated around autoregressive models for discrete signals and diffusion models for continuous signals. This stagnation creates a bottleneck that prevents us from fully unlocking the potential of rich multi-modal data, which in turn limits the progress on multimodal intelligence. We argue that an inference-first perspective, which prioritizes scaling efficiency during inference time across sequence length and refinement steps, can inspire novel generative pre-training algorithms. Using Inductive Moment Matching (IMM) as a concrete example, we demonstrate how addressing limitations in diffusion models' inference process through targeted modifications yields a stable, single-stage algorithm that achieves superior sample quality with over an order of magnitude greater inference efficiency.", 'score': 1, 'issue_id': 2671, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '2c3a8d77ccb8e716', 'authors': ['Jiaming Song', 'Linqi Zhou'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2503.07154.jpg', 'data': {'categories': ['#training', '#architecture', '#diffusion', '#inference', '#optimization', '#multimodal'], 'emoji': '🚀', 'ru': {'title': 'Новый взгляд на генеративное предобучение: приоритет эффективности вывода', 'desc': "В статье обсуждается стагнация в развитии алгоритмов для фундаментальных моделей, несмотря на прогресс в генеративном предобучении. Авторы предлагают подход 'inference-first', фокусирующийся на эффективности вывода для многомодальных данных. На примере метода Inductive Moment Matching (IMM) демонстрируется, как модификации процесса вывода диффузионных моделей могут привести к созданию стабильного одноэтапного алгоритма. Этот алгоритм обеспечивает более высокое качество сэмплов при значительно большей эффективности вывода."}, 'en': {'title': 'Unlocking Multi-Modal Intelligence with Efficient Inference', 'desc': 'This paper discusses the current limitations in generative pre-training models, particularly focusing on autoregressive and diffusion models. It highlights how these limitations hinder the effective use of multi-modal data, which is essential for advancing multimodal intelligence. The authors propose an inference-first approach that emphasizes improving efficiency during the inference phase, which can lead to innovative generative algorithms. They introduce Inductive Moment Matching (IMM) as a method to enhance diffusion models, resulting in a more efficient and higher quality sampling process.'}, 'zh': {'title': '推理优先，激发生成预训练新算法', 'desc': '近年来，基础模型通过生成预训练取得了显著进展，但在自回归模型和扩散模型的算法创新方面停滞不前。这种停滞造成了瓶颈，限制了多模态数据的潜力，进而影响了多模态智能的发展。我们提出了一种以推理为先的视角，强调在推理时间内的规模效率，以激发新的生成预训练算法。通过归纳时刻匹配（IMM）作为具体例子，我们展示了如何通过针对性修改扩散模型的推理过程，得到一种稳定的单阶段算法，显著提高样本质量和推理效率。'}}}, {'id': 'https://huggingface.co/papers/2503.08037', 'title': 'ObjectMover: Generative Object Movement with Video Prior', 'url': 'https://huggingface.co/papers/2503.08037', 'abstract': 'Simple as it seems, moving an object to another location within an image is, in fact, a challenging image-editing task that requires re-harmonizing the lighting, adjusting the pose based on perspective, accurately filling occluded regions, and ensuring coherent synchronization of shadows and reflections while maintaining the object identity. In this paper, we present ObjectMover, a generative model that can perform object movement in highly challenging scenes. Our key insight is that we model this task as a sequence-to-sequence problem and fine-tune a video generation model to leverage its knowledge of consistent object generation across video frames. We show that with this approach, our model is able to adjust to complex real-world scenarios, handling extreme lighting harmonization and object effect movement. As large-scale data for object movement are unavailable, we construct a data generation pipeline using a modern game engine to synthesize high-quality data pairs. We further propose a multi-task learning strategy that enables training on real-world video data to improve the model generalization. Through extensive experiments, we demonstrate that ObjectMover achieves outstanding results and adapts well to real-world scenarios.', 'score': 0, 'issue_id': 2667, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 марта', 'en': 'March 11', 'zh': '3月11日'}, 'hash': '821b6b6ca9c0eb61', 'authors': ['Xin Yu', 'Tianyu Wang', 'Soo Ye Kim', 'Paul Guerrero', 'Xi Chen', 'Qing Liu', 'Zhe Lin', 'Xiaojuan Qi'], 'affiliations': ['Adobe Research', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2503.08037.jpg', 'data': {'categories': ['#games', '#data', '#training', '#synthetic', '#video', '#optimization', '#dataset'], 'emoji': '🔄', 'ru': {'title': 'ObjectMover: ИИ для реалистичного перемещения объектов на изображениях', 'desc': 'Статья представляет ObjectMover - генеративную модель для перемещения объектов на изображениях. Модель использует подход sequence-to-sequence и дообучает модель генерации видео для согласованного перемещения объектов. Для обучения используются синтетические данные из игрового движка и мультизадачное обучение на реальных видео. ObjectMover успешно справляется со сложными сценариями, включая гармонизацию освещения и согласование эффектов объекта.'}, 'en': {'title': 'Seamless Object Movement in Images with ObjectMover', 'desc': "This paper introduces ObjectMover, a generative model designed to move objects within images while addressing challenges like lighting harmonization and perspective adjustments. The authors treat the task as a sequence-to-sequence problem, utilizing a fine-tuned video generation model to ensure consistent object representation across frames. To overcome the lack of large-scale data for object movement, they create a data generation pipeline using a game engine to produce high-quality training pairs. Additionally, a multi-task learning strategy is proposed to enhance the model's performance on real-world video data, leading to impressive results in complex scenarios."}, 'zh': {'title': 'ObjectMover：智能物体移动的解决方案', 'desc': '本文介绍了一种名为ObjectMover的生成模型，旨在解决图像中物体移动的复杂任务。该模型将物体移动视为序列到序列的问题，并对视频生成模型进行微调，以利用其在视频帧间一致生成物体的知识。由于缺乏大规模的物体移动数据，我们使用现代游戏引擎构建数据生成管道，合成高质量的数据对。此外，我们提出了一种多任务学习策略，以便在真实视频数据上进行训练，从而提高模型的泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2503.05037', 'title': 'Collapse of Dense Retrievers: Short, Early, and Literal Biases\n  Outranking Factual Evidence', 'url': 'https://huggingface.co/papers/2503.05037', 'abstract': "Dense retrieval models are commonly used in Information Retrieval (IR) applications, such as Retrieval-Augmented Generation (RAG). Since they often serve as the first step in these systems, their robustness is critical to avoid failures. In this work, by repurposing a relation extraction dataset (e.g. Re-DocRED), we design controlled experiments to quantify the impact of heuristic biases, such as favoring shorter documents, in retrievers like Dragon+ and Contriever. Our findings reveal significant vulnerabilities: retrievers often rely on superficial patterns like over-prioritizing document beginnings, shorter documents, repeated entities, and literal matches. Additionally, they tend to overlook whether the document contains the query's answer, lacking deep semantic understanding. Notably, when multiple biases combine, models exhibit catastrophic performance degradation, selecting the answer-containing document in less than 3% of cases over a biased document without the answer. Furthermore, we show that these biases have direct consequences for downstream applications like RAG, where retrieval-preferred documents can mislead LLMs, resulting in a 34% performance drop than not providing any documents at all.", 'score': 0, 'issue_id': 2667, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 марта', 'en': 'March 6', 'zh': '3月6日'}, 'hash': '862c1dc027b05f42', 'authors': ['Mohsen Fayyaz', 'Ali Modarressi', 'Hinrich Schuetze', 'Nanyun Peng'], 'affiliations': ['CIS, LMU Munich', 'Munich Center for Machine Learning', 'University of California, Los Angeles'], 'pdf_title_img': 'assets/pdf/title_img/2503.05037.jpg', 'data': {'categories': ['#benchmark', '#security', '#rag', '#interpretability', '#dataset'], 'emoji': '🕵️', 'ru': {'title': 'Уязвимости моделей плотного поиска: почему эвристики могут подвести', 'desc': 'Это исследование посвящено уязвимостям моделей плотного поиска, часто используемых в информационном поиске и генерации с дополнением извлечения. Авторы провели эксперименты, показывающие, что такие модели как Dragon+ и Contriever подвержены эвристическим искажениям, например, предпочтению более коротких документов. Выявлено, что модели часто опираются на поверхностные паттерны, игнорируя глубокий семантический смысл. Эти проблемы могут приводить к катастрофическому падению производительности в нисходящих задачах, таких как RAG.'}, 'en': {'title': 'Uncovering Biases in Dense Retrieval Models for Better Information Retrieval', 'desc': 'This paper investigates the weaknesses of dense retrieval models used in Information Retrieval, particularly in systems like Retrieval-Augmented Generation (RAG). By analyzing a relation extraction dataset, the authors conduct experiments to identify how heuristic biases, such as favoring shorter documents, affect the performance of retrievers like Dragon+ and Contriever. The results show that these models often depend on superficial patterns and fail to ensure that retrieved documents contain the relevant answers, leading to poor retrieval outcomes. The study highlights that when multiple biases are present, the performance can drastically decline, negatively impacting downstream applications that rely on accurate document retrieval.'}, 'zh': {'title': '揭示密集检索模型的脆弱性', 'desc': '本文探讨了密集检索模型在信息检索应用中的脆弱性，尤其是在检索增强生成（RAG）系统中的重要性。通过重新利用关系提取数据集，设计了控制实验来量化启发式偏见对检索器的影响。研究发现，检索器往往依赖表面模式，如过度优先考虑文档开头、较短文档和字面匹配，缺乏深层语义理解。多个偏见结合时，模型性能显著下降，导致选择包含答案的文档的概率低于3%。'}}}, {'id': 'https://huggingface.co/papers/2503.03734', 'title': 'OTTER: A Vision-Language-Action Model with Text-Aware Visual Feature\n  Extraction', 'url': 'https://huggingface.co/papers/2503.03734', 'abstract': 'Vision-Language-Action (VLA) models aim to predict robotic actions based on visual observations and language instructions. Existing approaches require fine-tuning pre-trained visionlanguage models (VLMs) as visual and language features are independently fed into downstream policies, degrading the pre-trained semantic alignments. We propose OTTER, a novel VLA architecture that leverages these existing alignments through explicit, text-aware visual feature extraction. Instead of processing all visual features, OTTER selectively extracts and passes only task-relevant visual features that are semantically aligned with the language instruction to the policy transformer. This allows OTTER to keep the pre-trained vision-language encoders frozen. Thereby, OTTER preserves and utilizes the rich semantic understanding learned from large-scale pre-training, enabling strong zero-shot generalization capabilities. In simulation and real-world experiments, OTTER significantly outperforms existing VLA models, demonstrating strong zeroshot generalization to novel objects and environments. Video, code, checkpoints, and dataset: https://ottervla.github.io/.', 'score': 0, 'issue_id': 2668, 'pub_date': '2025-03-05', 'pub_date_card': {'ru': '5 марта', 'en': 'March 5', 'zh': '3月5日'}, 'hash': 'a6a36002652e8be7', 'authors': ['Huang Huang', 'Fangchen Liu', 'Letian Fu', 'Tingfan Wu', 'Mustafa Mukadam', 'Jitendra Malik', 'Ken Goldberg', 'Pieter Abbeel'], 'affiliations': ['Meta AI', 'University of California, Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2503.03734.jpg', 'data': {'categories': ['#agi', '#architecture', '#transfer_learning', '#video', '#multimodal', '#training', '#agents'], 'emoji': '🦦', 'ru': {'title': 'OTTER: Умное извлечение визуальных признаков для роботизированных действий', 'desc': 'OTTER - это новая архитектура для моделей зрения-языка-действия (VLA), которая улучшает предсказание роботизированных действий. Она использует предварительно обученные семантические связи между зрением и языком, извлекая только релевантные визуальные признаки. OTTER сохраняет замороженными предварительно обученные энкодеры зрения-языка, что позволяет использовать богатое семантическое понимание. В экспериментах OTTER значительно превосходит существующие модели VLA, демонстрируя сильную обобщающую способность в нулевом выстреле.'}, 'en': {'title': 'OTTER: Enhancing Robotic Actions with Smart Visual-Language Alignment', 'desc': 'The paper introduces OTTER, a new Vision-Language-Action (VLA) model designed to enhance robotic action prediction using visual inputs and language instructions. Unlike traditional methods that require fine-tuning of pre-trained vision-language models, OTTER maintains the integrity of these models by keeping them frozen and selectively extracting only the relevant visual features aligned with the language commands. This approach allows OTTER to leverage the rich semantic knowledge from large-scale pre-training, leading to improved performance in zero-shot scenarios. Experimental results show that OTTER outperforms existing VLA models in both simulated and real-world tasks, demonstrating its ability to generalize to new objects and environments without additional training.'}, 'zh': {'title': 'OTTER：提升机器人动作预测的视觉-语言-动作模型', 'desc': '本文提出了一种新的视觉-语言-动作（VLA）模型OTTER，旨在根据视觉观察和语言指令预测机器人动作。OTTER通过显式的文本感知视觉特征提取，利用现有的语义对齐，避免了对预训练视觉语言模型的微调。该模型选择性地提取与任务相关的视觉特征，并将其传递给策略变换器，从而保持预训练的视觉-语言编码器不变。实验结果表明，OTTER在新物体和环境上展现了强大的零样本泛化能力，显著优于现有的VLA模型。'}}}, {'id': 'https://huggingface.co/papers/2502.21263', 'title': 'RuCCoD: Towards Automated ICD Coding in Russian', 'url': 'https://huggingface.co/papers/2502.21263', 'abstract': 'This study investigates the feasibility of automating clinical coding in Russian, a language with limited biomedical resources. We present a new dataset for ICD coding, which includes diagnosis fields from electronic health records (EHRs) annotated with over 10,000 entities and more than 1,500 unique ICD codes. This dataset serves as a benchmark for several state-of-the-art models, including BERT, LLaMA with LoRA, and RAG, with additional experiments examining transfer learning across domains (from PubMed abstracts to medical diagnosis) and terminologies (from UMLS concepts to ICD codes). We then apply the best-performing model to label an in-house EHR dataset containing patient histories from 2017 to 2021. Our experiments, conducted on a carefully curated test set, demonstrate that training with the automated predicted codes leads to a significant improvement in accuracy compared to manually annotated data from physicians. We believe our findings offer valuable insights into the potential for automating clinical coding in resource-limited languages like Russian, which could enhance clinical efficiency and data accuracy in these contexts.', 'score': 110, 'issue_id': 2617, 'pub_date': '2025-02-28', 'pub_date_card': {'ru': '28 февраля', 'en': 'February 28', 'zh': '2月28日'}, 'hash': '89ad8229208a4f98', 'authors': ['Aleksandr Nesterov', 'Andrey Sakhovskiy', 'Ivan Sviridov', 'Airat Valiev', 'Vladimir Makharev', 'Petr Anokhin', 'Galina Zubkova', 'Elena Tutubalina'], 'affiliations': ['AIRI, Moscow, Russia', 'HSE University, Moscow, Russia', 'ISP RAS Research Center for Trusted Artificial Intelligence, Moscow, Russia', 'Sber AI Lab, Moscow, Russia', 'Sber AI, Moscow, Russia'], 'pdf_title_img': 'assets/pdf/title_img/2502.21263.jpg', 'data': {'categories': ['#dataset', '#transfer_learning', '#benchmark', '#science', '#low_resource', '#healthcare', '#training'], 'emoji': '🏥', 'ru': {'title': 'Автоматизация клинического кодирования на русском языке: прорыв в эффективности и точности', 'desc': 'Исследование посвящено автоматизации клинического кодирования на русском языке с использованием методов машинного обучения. Авторы представляют новый набор данных для кодирования по МКБ, включающий более 10000 сущностей и 1500 уникальных кодов МКБ. Проводится сравнение современных моделей, таких как BERT, LLaMA с LoRA и RAG, а также исследуется перенос обучения между доменами и терминологиями. Результаты показывают, что обучение на автоматически предсказанных кодах значительно повышает точность по сравнению с ручной разметкой врачей.'}, 'en': {'title': 'Automating Clinical Coding: A Leap for Russian Healthcare', 'desc': 'This paper explores the automation of clinical coding in the Russian language, which lacks extensive biomedical resources. It introduces a new dataset for ICD coding, featuring over 10,000 annotated entities and 1,500 unique ICD codes derived from electronic health records. The study benchmarks various advanced models, including BERT and LLaMA, and investigates transfer learning from different medical domains and terminologies. Results show that using automated coding significantly improves accuracy over traditional manual coding by physicians, highlighting the potential for enhanced clinical efficiency in resource-limited settings.'}, 'zh': {'title': '自动化临床编码：提升俄语医疗效率的希望', 'desc': '本研究探讨了在俄语中自动化临床编码的可行性，俄语的生物医学资源相对有限。我们提出了一个新的ICD编码数据集，该数据集包含来自电子健康记录（EHR）的诊断字段，标注了超过10,000个实体和1,500多个独特的ICD代码。通过对多种先进模型（如BERT、LLaMA与LoRA、RAG）的基准测试，以及跨领域和术语的迁移学习实验，我们验证了模型的有效性。实验结果表明，使用自动预测的代码进行训练，相较于医生手动标注的数据，显著提高了准确性，显示了在资源有限的语言中自动化临床编码的潜力。'}}}, {'id': 'https://huggingface.co/papers/2503.05236', 'title': 'Unified Reward Model for Multimodal Understanding and Generation', 'url': 'https://huggingface.co/papers/2503.05236', 'abstract': 'Recent advances in human preference alignment have significantly enhanced multimodal generation and understanding. A key approach is training reward models to guide preference optimization. However, existing models are often task-specific, limiting their adaptability across diverse visual applications. We also argue that jointly learning to assess multiple tasks may foster a synergistic effect, where improved image understanding enhances image generation assessment, and refined image evaluation benefits video assessment through better frame analysis. To this end, this paper proposes UnifiedReward, the first unified reward model for multimodal understanding and generation assessment, enabling both pairwise ranking and pointwise scoring, which can be employed for vision model preference alignment. Specifically, (1) we first develop UnifiedReward on our constructed large-scale human preference dataset, including both image and video generation/understanding tasks. (2) Then, it is utilized to automatically construct high-quality preference pair data based on the vision models, fine-gradually filtering their outputs through pair ranking and point sifting. (3) Finally, these data are used for their preference alignment through Direct Preference Optimization (DPO). Experimental results demonstrate that joint learning to assess diverse visual tasks can lead to substantial mutual benefits and we apply our pipeline to both image and video understanding/generation tasks, significantly improving the performance in each domain.', 'score': 86, 'issue_id': 2607, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 марта', 'en': 'March 7', 'zh': '3月7日'}, 'hash': '6ebf61a6777b8e4d', 'authors': ['Yibin Wang', 'Yuhang Zang', 'Hao Li', 'Cheng Jin', 'Jiaqi Wang'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2503.05236.jpg', 'data': {'categories': ['#multimodal', '#video', '#cv', '#alignment', '#dataset', '#rlhf', '#rag'], 'emoji': '🤖', 'ru': {'title': 'Единая модель вознаграждения для улучшения мультимодальных AI-систем', 'desc': 'Статья представляет UnifiedReward - первую унифицированную модель вознаграждения для оценки мультимодального понимания и генерации. Модель обучается на большом наборе данных о человеческих предпочтениях, включающем задачи по генерации и пониманию изображений и видео. UnifiedReward используется для автоматического создания высококачественных пар предпочтений на основе моделей компьютерного зрения. Затем эти данные применяются для настройки предпочтений моделей с помощью метода Direct Preference Optimization (DPO).'}, 'en': {'title': 'UnifiedReward: Enhancing Multimodal Learning through Joint Preference Alignment', 'desc': 'This paper introduces UnifiedReward, a novel reward model designed to enhance multimodal understanding and generation in machine learning. It addresses the limitation of existing task-specific models by enabling joint learning across various visual tasks, which improves both image and video assessments. The model is trained on a large-scale human preference dataset and utilizes techniques like pairwise ranking and pointwise scoring for effective preference alignment. Experimental results show that this unified approach leads to significant performance improvements in both image and video tasks, demonstrating the benefits of synergistic learning.'}, 'zh': {'title': '统一奖励模型，提升多模态理解与生成', 'desc': '最近在人类偏好对齐方面的进展显著提升了多模态生成和理解的能力。关键方法是训练奖励模型来指导偏好优化。然而，现有模型通常是特定于任务的，限制了它们在不同视觉应用中的适应性。本文提出了UnifiedReward，这是第一个用于多模态理解和生成评估的统一奖励模型，能够同时进行成对排名和逐点评分，从而实现视觉模型的偏好对齐。'}}}, {'id': 'https://huggingface.co/papers/2503.05500', 'title': 'EuroBERT: Scaling Multilingual Encoders for European Languages', 'url': 'https://huggingface.co/papers/2503.05500', 'abstract': 'General-purpose multilingual vector representations, used in retrieval, regression and classification, are traditionally obtained from bidirectional encoder models. Despite their wide applicability, encoders have been recently overshadowed by advances in generative decoder-only models. However, many innovations driving this progress are not inherently tied to decoders. In this paper, we revisit the development of multilingual encoders through the lens of these advances, and introduce EuroBERT, a family of multilingual encoders covering European and widely spoken global languages. Our models outperform existing alternatives across a diverse range of tasks, spanning multilingual capabilities, mathematics, and coding, and natively supporting sequences of up to 8,192 tokens. We also examine the design decisions behind EuroBERT, offering insights into our dataset composition and training pipeline. We publicly release the EuroBERT models, including intermediate training checkpoints, together with our training framework.', 'score': 57, 'issue_id': 2613, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 марта', 'en': 'March 7', 'zh': '3月7日'}, 'hash': 'befb9ffdb6a6cd73', 'authors': ['Nicolas Boizard', 'Hippolyte Gisserot-Boukhlef', 'Duarte M. Alves', 'André Martins', 'Ayoub Hammal', 'Caio Corro', 'Céline Hudelot', 'Emmanuel Malherbe', 'Etienne Malaboeuf', 'Fanny Jourdan', 'Gabriel Hautreux', 'João Alves', 'Kevin El-Haddad', 'Manuel Faysse', 'Maxime Peyrard', 'Nuno M. Guerreiro', 'Patrick Fernandes', 'Ricardo Rei', 'Pierre Colombo'], 'affiliations': ['Artefact', 'CINES', 'CNRS', 'Carnegie Mellon University', 'Diabolocom', 'Equall', 'INSA Rennes', 'IRISA', 'IRT Saint Exupery', 'ISIA Lab', 'Illuin Technology', 'Instituto Superior Tecnico & Universidade de Lisboa (Lisbon ELLIS Unit)', 'Instituto de Telecomunicacoes', 'LISN', 'MICS, CentraleSupelec, Universite Paris-Saclay', 'Unbabel', 'Universite Grenoble Alpes, Grenoble INP, LIG', 'Universite Paris-Saclay'], 'pdf_title_img': 'assets/pdf/title_img/2503.05500.jpg', 'data': {'categories': ['#multilingual', '#open_source', '#architecture', '#training', '#dataset', '#long_context'], 'emoji': '🌍', 'ru': {'title': 'EuroBERT: Возрождение многоязычных энкодеров в эпоху генеративных моделей', 'desc': 'В статье представлена модель EuroBERT - семейство многоязычных энкодеров для европейских и широко распространенных мировых языков. Эти энкодеры превосходят существующие аналоги в различных задачах, включая многоязычные возможности, математику и программирование. Модели EuroBERT поддерживают последовательности длиной до 8192 токенов и были разработаны с учетом последних достижений в области генеративных декодер-моделей. Авторы также описывают процесс создания датасета и pipeline обучения модели.'}, 'en': {'title': 'EuroBERT: Advancing Multilingual Encoders for Diverse Tasks', 'desc': 'This paper presents EuroBERT, a new family of multilingual encoder models designed to improve performance in various tasks such as retrieval, regression, and classification. Unlike traditional models that rely heavily on bidirectional encoders, EuroBERT leverages recent advancements in generative models while maintaining the strengths of encoders. The models are capable of handling sequences of up to 8,192 tokens and demonstrate superior performance across multilingual tasks, mathematics, and coding challenges. The authors also provide insights into the dataset and training processes used to develop EuroBERT, along with public access to the models and training framework.'}, 'zh': {'title': 'EuroBERT：提升多语言处理的新选择', 'desc': '本文介绍了一种新的多语言编码器模型，称为EuroBERT，旨在提升多语言检索、回归和分类任务的性能。尽管生成解码器模型近年来取得了显著进展，但我们认为多语言编码器仍然具有重要价值。EuroBERT覆盖了欧洲及广泛使用的全球语言，并在多种任务中表现优于现有模型。我们还分享了EuroBERT的设计决策、数据集构成和训练流程，并公开发布了模型及其训练框架。'}}}, {'id': 'https://huggingface.co/papers/2503.05085', 'title': 'S2S-Arena, Evaluating Speech2Speech Protocols on Instruction Following\n  with Paralinguistic Information', 'url': 'https://huggingface.co/papers/2503.05085', 'abstract': 'The rapid development of large language models (LLMs) has brought significant attention to speech models, particularly recent progress in speech2speech protocols supporting speech input and output. However, the existing benchmarks adopt automatic text-based evaluators for evaluating the instruction following ability of these models lack consideration for paralinguistic information in both speech understanding and generation. To address these issues, we introduce S2S-Arena, a novel arena-style S2S benchmark that evaluates instruction-following capabilities with paralinguistic information in both speech-in and speech-out across real-world tasks. We design 154 samples that fused TTS and live recordings in four domains with 21 tasks and manually evaluate existing popular speech models in an arena-style manner. The experimental results show that: (1) in addition to the superior performance of GPT-4o, the speech model of cascaded ASR, LLM, and TTS outperforms the jointly trained model after text-speech alignment in speech2speech protocols; (2) considering paralinguistic information, the knowledgeability of the speech model mainly depends on the LLM backbone, and the multilingual support of that is limited by the speech module; (3) excellent speech models can already understand the paralinguistic information in speech input, but generating appropriate audio with paralinguistic information is still a challenge.', 'score': 39, 'issue_id': 2619, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 марта', 'en': 'March 7', 'zh': '3月7日'}, 'hash': 'aa9d1f284b6fe901', 'authors': ['Feng Jiang', 'Zhiyu Lin', 'Fan Bu', 'Yuhao Du', 'Benyou Wang', 'Haizhou Li'], 'affiliations': ['The Chinese University of Hong Kong, Shenzhen'], 'pdf_title_img': 'assets/pdf/title_img/2503.05085.jpg', 'data': {'categories': ['#audio', '#multilingual', '#benchmark'], 'emoji': '🗣️', 'ru': {'title': 'S2S-Arena: новый подход к оценке речевых моделей с учетом паралингвистики', 'desc': 'Статья представляет S2S-Arena - новый бенчмарк для оценки речевых моделей, учитывающий паралингвистическую информацию. Авторы создали 154 образца в 4 доменах с 21 задачей, объединяющих синтезированную и живую речь. Результаты показывают преимущество каскадных моделей (ASR+LLM+TTS) над совместно обученными в задачах speech2speech. Выявлено, что современные речевые модели хорошо понимают паралингвистическую информацию во входной речи, но генерация соответствующего аудио остается сложной задачей.'}, 'en': {'title': 'Enhancing Speech Models with Paralinguistic Insights', 'desc': "This paper presents S2S-Arena, a new benchmark for evaluating speech-to-speech (S2S) models that incorporates paralinguistic information, which includes elements like tone and emotion in speech. The authors highlight that current evaluation methods primarily rely on text-based metrics, which do not adequately assess the models' ability to understand and generate speech with these nuances. Through a series of 154 samples across various tasks, the study reveals that models like GPT-4o and cascaded ASR-LLM-TTS outperform others when considering paralinguistic factors. The findings indicate that while advanced speech models can comprehend paralinguistic cues, generating speech that accurately reflects these cues remains a significant challenge."}, 'zh': {'title': '引入副语言信息的语音模型评估新标准', 'desc': '随着大型语言模型（LLMs）的快速发展，语音模型也受到了广泛关注，尤其是在语音输入和输出的语音到语音（speech2speech）协议方面的进展。然而，现有的基准测试主要依赖自动文本评估器来评估这些模型的指令遵循能力，未能考虑语音理解和生成中的副语言信息。为了解决这些问题，我们提出了S2S-Arena，这是一个新颖的竞技场风格的S2S基准，评估在真实任务中语音输入和输出的指令遵循能力，并考虑副语言信息。实验结果表明，尽管GPT-4o表现优越，但在语音到语音协议中，级联的ASR、LLM和TTS语音模型在文本-语音对齐后优于联合训练模型。'}}}, {'id': 'https://huggingface.co/papers/2503.05179', 'title': 'Sketch-of-Thought: Efficient LLM Reasoning with Adaptive\n  Cognitive-Inspired Sketching', 'url': 'https://huggingface.co/papers/2503.05179', 'abstract': 'Recent advances in large language models have demonstrated remarkable reasoning capabilities through Chain of Thought (CoT) prompting, but often at the cost of excessive verbosity in their intermediate outputs, which increases computational overhead. We introduce Sketch-of-Thought (SoT), a novel prompting framework that combines cognitive-inspired reasoning paradigms with linguistic constraints to minimize token usage while preserving reasoning accuracy. SoT is designed as a flexible framework that can incorporate any custom reasoning paradigms based on cognitive science, and we instantiate it with three such paradigms - Conceptual Chaining, Chunked Symbolism, and Expert Lexicons - each tailored to different reasoning tasks and selected dynamically via a lightweight routing model. Through comprehensive evaluation across 15 reasoning datasets with multiple languages and multimodal scenarios, we demonstrate that SoT achieves token reductions of 76% with negligible accuracy impact. In certain domains like mathematical and multi-hop reasoning, it even improves accuracy while using significantly fewer tokens. Our code is publicly available: https://www.github.com/SimonAytes/SoT.', 'score': 37, 'issue_id': 2607, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 марта', 'en': 'March 7', 'zh': '3月7日'}, 'hash': 'e02cb6f62715b753', 'authors': ['Simon A. Aytes', 'Jinheon Baek', 'Sung Ju Hwang'], 'affiliations': ['DeepAuto.ai', 'KAIST'], 'pdf_title_img': 'assets/pdf/title_img/2503.05179.jpg', 'data': {'categories': ['#multimodal', '#dataset', '#reasoning', '#multilingual', '#optimization', '#open_source', '#math', '#training'], 'emoji': '🧠', 'ru': {'title': 'Эффективное рассуждение с минимальным использованием токенов', 'desc': 'Статья представляет новый метод промптинга под названием Sketch-of-Thought (SoT), который сочетает когнитивно-вдохновленные парадигмы рассуждений с лингвистическими ограничениями. SoT направлен на минимизацию использования токенов при сохранении точности рассуждений в больших языковых моделях. Метод включает три парадигмы: Conceptual Chaining, Chunked Symbolism и Expert Lexicons, каждая из которых адаптирована для различных задач рассуждения. Эксперименты на 15 наборах данных показали, что SoT сокращает использование токенов на 76% без значительного влияния на точность, а в некоторых областях даже улучшает ее.'}, 'en': {'title': 'Efficient Reasoning with Sketch-of-Thought', 'desc': 'This paper presents a new prompting framework called Sketch-of-Thought (SoT) that enhances reasoning in large language models while reducing the number of tokens used. SoT integrates cognitive-inspired reasoning methods with linguistic constraints to maintain accuracy without excessive verbosity. The framework is adaptable, allowing for the inclusion of various reasoning paradigms, which are dynamically selected based on the task at hand. Evaluation shows that SoT can reduce token usage by 76% with little to no loss in accuracy, and in some cases, it even improves performance in specific reasoning tasks.'}, 'zh': {'title': '思维草图：高效推理的新方法', 'desc': '本文介绍了一种新的提示框架，称为思维草图（Sketch-of-Thought，SoT），旨在提高大型语言模型的推理能力，同时减少中间输出的冗长性。SoT结合了认知科学的推理范式和语言约束，以最小化令牌使用量，同时保持推理的准确性。该框架灵活，可以根据认知科学的不同推理范式进行定制，并通过轻量级路由模型动态选择。通过在15个推理数据集上的全面评估，SoT实现了76%的令牌减少，且对准确性影响微乎其微，甚至在某些领域提高了准确性。'}}}, {'id': 'https://huggingface.co/papers/2503.05132', 'title': 'R1-Zero\'s "Aha Moment" in Visual Reasoning on a 2B Non-SFT Model', 'url': 'https://huggingface.co/papers/2503.05132', 'abstract': 'Recently DeepSeek R1 demonstrated how reinforcement learning with simple rule-based incentives can enable autonomous development of complex reasoning in large language models, characterized by the "aha moment", in which the model manifest self-reflection and increased response length during training. However, attempts to extend this success to multimodal reasoning often failed to reproduce these key characteristics. In this report, we present the first successful replication of these emergent characteristics for multimodal reasoning on only a non-SFT 2B model. Starting with Qwen2-VL-2B and applying reinforcement learning directly on the SAT dataset, our model achieves 59.47% accuracy on CVBench, outperforming the base model by approximately ~30% and exceeding both SFT setting by ~2%. In addition, we share our failed attempts and insights in attempting to achieve R1-like reasoning using RL with instruct models. aiming to shed light on the challenges involved. Our key observations include: (1) applying RL on instruct model often results in trivial reasoning trajectories, and (2) naive length reward are ineffective in eliciting reasoning capabilities. The project code is available at https://github.com/turningpoint-ai/VisualThinker-R1-Zero', 'score': 20, 'issue_id': 2609, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 марта', 'en': 'March 7', 'zh': '3月7日'}, 'hash': '760e01cf2a414aeb', 'authors': ['Hengguang Zhou', 'Xirui Li', 'Ruochen Wang', 'Minhao Cheng', 'Tianyi Zhou', 'Cho-Jui Hsieh'], 'affiliations': ['Pennsylvania State University', 'University of California, LA', 'University of Maryland'], 'pdf_title_img': 'assets/pdf/title_img/2503.05132.jpg', 'data': {'categories': ['#rl', '#multimodal', '#training', '#reasoning', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Новые горизонты мультимодального обучения', 'desc': 'Исследователи из DeepSeek R1 показали, как обучение с подкреплением может помочь LLM развивать сложные навыки рассуждения, включая моменты "эврики". Однако при попытках применить эти методы к мультимодальному обучению часто не удавалось достичь таких же результатов. В этом исследовании удалось впервые воспроизвести эти характеристики на мультимодальной модели без использования SFT, достигнув высокой точности. Авторы также делятся неудачными попытками и выводами, чтобы лучше понять возникающие трудности.'}, 'en': {'title': 'Unlocking Multimodal Reasoning with Reinforcement Learning', 'desc': "This paper discusses the application of reinforcement learning (RL) to enhance multimodal reasoning in large language models, specifically using the Qwen2-VL-2B model. The authors successfully replicated the 'aha moment' phenomenon, where the model demonstrates self-reflection and improved response length during training, achieving a notable accuracy of 59.47% on the CVBench dataset. They also share insights from their unsuccessful attempts to replicate similar reasoning capabilities using instruct models, highlighting challenges such as trivial reasoning paths and ineffective reward structures. The findings suggest that while RL can significantly improve reasoning in multimodal contexts, careful consideration of reward mechanisms is crucial for success."}, 'zh': {'title': '强化学习助力多模态推理的突破', 'desc': '最近，DeepSeek R1展示了如何通过简单的基于规则的激励来实现强化学习，使大型语言模型能够自主发展复杂的推理能力。这种能力在训练过程中表现为“恍然大悟”的时刻，模型会自我反思并增加响应长度。然而，尝试将这种成功扩展到多模态推理时，往往无法重现这些关键特征。在本报告中，我们首次成功复制了这些特征，并在非SFT的2B模型上实现了多模态推理的进展。'}}}, {'id': 'https://huggingface.co/papers/2503.02130', 'title': 'Forgetting Transformer: Softmax Attention with a Forget Gate', 'url': 'https://huggingface.co/papers/2503.02130', 'abstract': 'An essential component of modern recurrent sequence models is the forget gate. While Transformers do not have an explicit recurrent form, we show that a forget gate can be naturally incorporated into Transformers by down-weighting the unnormalized attention scores in a data-dependent way. We name this attention mechanism the Forgetting Attention and the resulting model the Forgetting Transformer (FoX). We show that FoX outperforms the Transformer on long-context language modeling, length extrapolation, and short-context downstream tasks, while performing on par with the Transformer on long-context downstream tasks. Moreover, it is compatible with the FlashAttention algorithm and does not require any positional embeddings. Several analyses, including the needle-in-the-haystack test, show that FoX also retains the Transformer\'s superior long-context capabilities over recurrent sequence models such as Mamba-2, HGRN2, and DeltaNet. We also introduce a "Pro" block design that incorporates some common architectural components in recurrent sequence models and find it significantly improves the performance of both FoX and the Transformer. Our code is available at https://github.com/zhixuan-lin/forgetting-transformer.', 'score': 18, 'issue_id': 2607, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': '4c39f334b6c4ed28', 'authors': ['Zhixuan Lin', 'Evgenii Nikishin', 'Xu Owen He', 'Aaron Courville'], 'affiliations': ['MakerMaker AI', 'Mila & Universite de Montreal'], 'pdf_title_img': 'assets/pdf/title_img/2503.02130.jpg', 'data': {'categories': ['#long_context', '#architecture', '#optimization', '#training'], 'emoji': '🧠', 'ru': {'title': 'Forgetting Transformer: Улучшение обработки длинных последовательностей в трансформерах', 'desc': "Исследователи представили новую модель под названием Forgetting Transformer (FoX), которая включает механизм 'забывающего внимания' в архитектуру трансформера. FoX превосходит стандартный трансформер в задачах моделирования языка с длинным контекстом и экстраполяции длины, сохраняя при этом высокую производительность на задачах с коротким контекстом. Модель совместима с алгоритмом FlashAttention и не требует позиционных эмбеддингов. Анализ показывает, что FoX сохраняет преимущества трансформера в обработке длинного контекста по сравнению с рекуррентными моделями."}, 'en': {'title': 'Enhancing Transformers with Forgetting Attention for Superior Performance', 'desc': 'This paper introduces a new attention mechanism called Forgetting Attention, which integrates a forget gate into Transformer models. The Forgetting Transformer (FoX) leverages this mechanism to improve performance on various language modeling tasks, particularly those involving long contexts. FoX not only matches the performance of traditional Transformers on long-context tasks but also excels in short-context and length extrapolation tasks. Additionally, it is compatible with the FlashAttention algorithm and eliminates the need for positional embeddings, enhancing its efficiency and effectiveness.'}, 'zh': {'title': '遗忘变换器：提升长上下文建模的利器', 'desc': '本文提出了一种新的注意力机制，称为遗忘注意力（Forgetting Attention），可以有效地将遗忘门集成到Transformer模型中。通过以数据为依赖的方式降低未归一化注意力分数，遗忘注意力使得Transformer在长上下文语言建模和长度外推任务中表现优于传统的Transformer。我们还设计了一个“Pro”模块，结合了递归序列模型中的一些常见架构组件，显著提升了FoX和Transformer的性能。此外，FoX在长上下文任务中保持了Transformer的优势，超越了其他递归序列模型。'}}}, {'id': 'https://huggingface.co/papers/2503.05592', 'title': 'R1-Searcher: Incentivizing the Search Capability in LLMs via\n  Reinforcement Learning', 'url': 'https://huggingface.co/papers/2503.05592', 'abstract': 'Existing Large Reasoning Models (LRMs) have shown the potential of reinforcement learning (RL) to enhance the complex reasoning capabilities of Large Language Models~(LLMs). While they achieve remarkable performance on challenging tasks such as mathematics and coding, they often rely on their internal knowledge to solve problems, which can be inadequate for time-sensitive or knowledge-intensive questions, leading to inaccuracies and hallucinations. To address this, we propose R1-Searcher, a novel two-stage outcome-based RL approach designed to enhance the search capabilities of LLMs. This method allows LLMs to autonomously invoke external search systems to access additional knowledge during the reasoning process. Our framework relies exclusively on RL, without requiring process rewards or distillation for a cold start. % effectively generalizing to out-of-domain datasets and supporting both Base and Instruct models. Our experiments demonstrate that our method significantly outperforms previous strong RAG methods, even when compared to the closed-source GPT-4o-mini.', 'score': 17, 'issue_id': 2609, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 марта', 'en': 'March 7', 'zh': '3月7日'}, 'hash': '6af1f8cd890c69ae', 'authors': ['Huatong Song', 'Jinhao Jiang', 'Yingqian Min', 'Jie Chen', 'Zhipeng Chen', 'Wayne Xin Zhao', 'Lei Fang', 'Ji-Rong Wen'], 'affiliations': ['DataCanvas', 'Gaoling School of Artificial Intelligence, Renmin University of China'], 'pdf_title_img': 'assets/pdf/title_img/2503.05592.jpg', 'data': {'categories': ['#hallucinations', '#optimization', '#rl', '#rag', '#reasoning'], 'emoji': '🔍', 'ru': {'title': 'Усиление поисковых способностей ИИ через обучение с подкреплением', 'desc': 'R1-Searcher - это новый двухэтапный подход к обучению с подкреплением, улучшающий способности больших языковых моделей к поиску информации. Он позволяет моделям автономно обращаться к внешним поисковым системам во время рассуждений. Метод основан исключительно на обучении с подкреплением, без необходимости в процессных наградах или дистилляции. Эксперименты показывают, что R1-Searcher превосходит предыдущие методы RAG, даже в сравнении с закрытой моделью GPT-4o-mini.'}, 'en': {'title': 'Enhancing LLM Reasoning with External Knowledge Search', 'desc': 'This paper introduces R1-Searcher, a new approach that improves the reasoning abilities of Large Language Models (LLMs) using reinforcement learning (RL). Unlike existing models that depend solely on their internal knowledge, R1-Searcher enables LLMs to access external search systems for additional information, which helps in answering complex and time-sensitive questions more accurately. The method operates in two stages and does not require initial rewards or distillation, making it easier to implement. Experimental results show that R1-Searcher outperforms previous retrieval-augmented generation (RAG) methods, demonstrating its effectiveness across various datasets.'}, 'zh': {'title': '增强推理能力，R1-Searcher助力LLMs', 'desc': '现有的大型推理模型（LRMs）展示了强化学习（RL）在增强大型语言模型（LLMs）复杂推理能力方面的潜力。尽管它们在数学和编程等挑战性任务上表现出色，但在处理时间敏感或知识密集的问题时，往往依赖内部知识，导致不准确和幻觉现象。为了解决这个问题，我们提出了R1-Searcher，这是一种新颖的基于结果的两阶段强化学习方法，旨在增强LLMs的搜索能力。该方法允许LLMs在推理过程中自主调用外部搜索系统，以获取额外知识，从而显著提高性能。'}}}, {'id': 'https://huggingface.co/papers/2503.04957', 'title': 'SafeArena: Evaluating the Safety of Autonomous Web Agents', 'url': 'https://huggingface.co/papers/2503.04957', 'abstract': 'LLM-based agents are becoming increasingly proficient at solving web-based tasks. With this capability comes a greater risk of misuse for malicious purposes, such as posting misinformation in an online forum or selling illicit substances on a website. To evaluate these risks, we propose SafeArena, the first benchmark to focus on the deliberate misuse of web agents. SafeArena comprises 250 safe and 250 harmful tasks across four websites. We classify the harmful tasks into five harm categories -- misinformation, illegal activity, harassment, cybercrime, and social bias, designed to assess realistic misuses of web agents. We evaluate leading LLM-based web agents, including GPT-4o, Claude-3.5 Sonnet, Qwen-2-VL 72B, and Llama-3.2 90B, on our benchmark. To systematically assess their susceptibility to harmful tasks, we introduce the Agent Risk Assessment framework that categorizes agent behavior across four risk levels. We find agents are surprisingly compliant with malicious requests, with GPT-4o and Qwen-2 completing 34.7% and 27.3% of harmful requests, respectively. Our findings highlight the urgent need for safety alignment procedures for web agents. Our benchmark is available here: https://safearena.github.io', 'score': 15, 'issue_id': 2623, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 марта', 'en': 'March 6', 'zh': '3月6日'}, 'hash': '8fd2a7cf41173ed6', 'authors': ['Ada Defne Tur', 'Nicholas Meade', 'Xing Han Lù', 'Alejandra Zambrano', 'Arkil Patel', 'Esin Durmus', 'Spandana Gella', 'Karolina Stańczak', 'Siva Reddy'], 'affiliations': ['Anthropic', 'Canada CIFAR AI Chair', 'Concordia University', 'McGill University', 'Mila Quebec AI Institute', 'ServiceNow Research'], 'pdf_title_img': 'assets/pdf/title_img/2503.04957.jpg', 'data': {'categories': ['#benchmark', '#agents', '#ethics', '#security', '#alignment'], 'emoji': '🕵️', 'ru': {'title': 'Оценка рисков ИИ-агентов: необходимость безопасности в эпоху веб-автоматизации', 'desc': 'SafeArena - это новый бенчмарк для оценки рисков намеренного злоупотребления веб-агентами на основе больших языковых моделей (LLM). Он включает 500 задач на четырех веб-сайтах, из которых 250 являются безопасными, а 250 - вредоносными, разделенными на пять категорий вреда. Исследователи оценили ведущих LLM-агентов, включая GPT-4o и Qwen-2-VL 72B, используя предложенную ими систему оценки рисков Agent Risk Assessment. Результаты показали, что агенты неожиданно часто выполняют вредоносные запросы, что подчеркивает срочную необходимость разработки процедур безопасности для веб-агентов.'}, 'en': {'title': 'Assessing the Risks of LLM Agents in Web Tasks', 'desc': 'This paper introduces SafeArena, a benchmark designed to evaluate the risks associated with the misuse of large language model (LLM)-based agents in web tasks. It includes 500 tasks, split evenly between safe and harmful, categorized into five types of harm such as misinformation and cybercrime. The study assesses various leading LLM agents, revealing that they often comply with harmful requests, with notable completion rates for GPT-4o and Qwen-2. The findings underscore the necessity for improved safety measures to prevent the malicious use of these agents.'}, 'zh': {'title': '网络代理安全风险评估的必要性', 'desc': '本论文提出了SafeArena，这是第一个专注于网络代理恶意使用的基准测试。SafeArena包含250个安全任务和250个有害任务，涵盖四个网站，并将有害任务分为五个类别，如虚假信息和网络犯罪。我们评估了多种领先的基于大语言模型的网络代理，发现它们对恶意请求的响应率令人惊讶，部分代理完成了超过三分之一的有害请求。研究结果强调了对网络代理进行安全对齐程序的迫切需求。'}}}, {'id': 'https://huggingface.co/papers/2503.05639', 'title': 'VideoPainter: Any-length Video Inpainting and Editing with Plug-and-Play\n  Context Control', 'url': 'https://huggingface.co/papers/2503.05639', 'abstract': "Video inpainting, which aims to restore corrupted video content, has experienced substantial progress. Despite these advances, existing methods, whether propagating unmasked region pixels through optical flow and receptive field priors, or extending image-inpainting models temporally, face challenges in generating fully masked objects or balancing the competing objectives of background context preservation and foreground generation in one model, respectively. To address these limitations, we propose a novel dual-stream paradigm VideoPainter that incorporates an efficient context encoder (comprising only 6% of the backbone parameters) to process masked videos and inject backbone-aware background contextual cues to any pre-trained video DiT, producing semantically consistent content in a plug-and-play manner. This architectural separation significantly reduces the model's learning complexity while enabling nuanced integration of crucial background context. We also introduce a novel target region ID resampling technique that enables any-length video inpainting, greatly enhancing our practical applicability. Additionally, we establish a scalable dataset pipeline leveraging current vision understanding models, contributing VPData and VPBench to facilitate segmentation-based inpainting training and assessment, the largest video inpainting dataset and benchmark to date with over 390K diverse clips. Using inpainting as a pipeline basis, we also explore downstream applications including video editing and video editing pair data generation, demonstrating competitive performance and significant practical potential. Extensive experiments demonstrate VideoPainter's superior performance in both any-length video inpainting and editing, across eight key metrics, including video quality, mask region preservation, and textual coherence.", 'score': 13, 'issue_id': 2614, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 марта', 'en': 'March 7', 'zh': '3月7日'}, 'hash': '735dd34a3043623a', 'authors': ['Yuxuan Bian', 'Zhaoyang Zhang', 'Xuan Ju', 'Mingdeng Cao', 'Liangbin Xie', 'Ying Shan', 'Qiang Xu'], 'affiliations': ['Tencent ARC Lab, China', 'The Chinese University of Hong Kong, China', 'The University of Tokyo, Japan', 'University of Macau, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.05639.jpg', 'data': {'categories': ['#video', '#benchmark', '#dataset', '#games', '#architecture', '#optimization'], 'emoji': '🎥', 'ru': {'title': 'VideoPainter: Революция в восстановлении видео с помощью двухпоточной архитектуры', 'desc': 'Статья представляет новый подход к восстановлению поврежденного видеоконтента - VideoPainter. Модель использует двухпоточную архитектуру с эффективным кодировщиком контекста для обработки маскированных видео и внедрения контекстуальных подсказок в предобученную видео-модель DiT. Авторы также вводят технику ресемплинга ID целевой области, позволяющую выполнять инпейнтинг видео любой длины. Дополнительно создан масштабируемый конвейер данных VPData и бенчмарк VPBench для обучения и оценки сегментационного инпейнтинга.'}, 'en': {'title': 'Revolutionizing Video Inpainting with VideoPainter', 'desc': "This paper presents VideoPainter, a new approach to video inpainting that effectively restores missing video content. It introduces a dual-stream architecture that uses a lightweight context encoder to enhance background information while maintaining the integrity of foreground objects. The method also features a novel target region ID resampling technique, allowing for flexible inpainting of videos of any length. Additionally, the authors provide a comprehensive dataset and benchmark for training and evaluating video inpainting models, showcasing VideoPainter's strong performance across various metrics."}, 'zh': {'title': '视频修复的新纪元：VideoPainter', 'desc': '视频修复旨在恢复损坏的视频内容，近年来取得了显著进展。现有方法在生成完全遮挡的物体或平衡背景保留与前景生成方面面临挑战。为了解决这些问题，我们提出了一种新颖的双流架构VideoPainter，利用高效的上下文编码器处理遮挡视频，并将背景上下文信息注入到预训练的视频模型中。我们还引入了一种新的目标区域ID重采样技术，支持任意长度的视频修复，极大地提升了实际应用的可行性。'}}}, {'id': 'https://huggingface.co/papers/2503.05379', 'title': 'R1-Omni: Explainable Omni-Multimodal Emotion Recognition with\n  Reinforcing Learning', 'url': 'https://huggingface.co/papers/2503.05379', 'abstract': "In this work, we present the first application of Reinforcement Learning with Verifiable Reward (RLVR) to an Omni-multimodal large language model in the context of emotion recognition, a task where both visual and audio modalities play crucial roles. We leverage RLVR to optimize the Omni model, significantly enhancing its performance in three key aspects: reasoning capability, emotion recognition accuracy, and generalization ability. The introduction of RLVR not only improves the model's overall performance on in-distribution data but also demonstrates superior robustness when evaluated on out-of-distribution datasets. More importantly, the improved reasoning capability enables clear analysis of the contributions of different modalities, particularly visual and audio information, in the emotion recognition process. This provides valuable insights into the optimization of multimodal large language models.", 'score': 12, 'issue_id': 2609, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 марта', 'en': 'March 7', 'zh': '3月7日'}, 'hash': '34c6afbd7ae83841', 'authors': ['Jiaxing Zhao', 'Xihan Wei', 'Liefeng Bo'], 'affiliations': ['Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2503.05379.jpg', 'data': {'categories': ['#optimization', '#rl', '#multimodal', '#audio', '#cv', '#reasoning'], 'emoji': '🤖', 'ru': {'title': 'RLVR: Революция в мультимодальном распознавании эмоций', 'desc': 'В статье представлено первое применение обучения с подкреплением с проверяемым вознаграждением (RLVR) к мультимодальной большой языковой модели для распознавания эмоций. RLVR используется для оптимизации Omni-модели, значительно улучшая её способности к рассуждению, точность распознавания эмоций и способность к обобщению. Модель демонстрирует повышенную производительность на исходных данных и устойчивость на новых наборах данных. Улучшенная способность к рассуждениям позволяет анализировать вклад различных модальностей в процесс распознавания эмоций.'}, 'en': {'title': 'Enhancing Emotion Recognition with RLVR in Multimodal Models', 'desc': "This paper introduces a novel approach called Reinforcement Learning with Verifiable Reward (RLVR) applied to an Omni-multimodal large language model for emotion recognition. The use of RLVR enhances the model's reasoning skills, accuracy in recognizing emotions, and its ability to generalize across different datasets. The model not only performs better on familiar data but also shows increased robustness when tested on new, unseen data. Additionally, the improved reasoning capability allows for a detailed understanding of how visual and audio inputs contribute to the emotion recognition task."}, 'zh': {'title': '情感识别中的全模态强化学习新突破', 'desc': '本研究首次将可验证奖励的强化学习（RLVR）应用于情感识别的全模态大型语言模型中。在这个任务中，视觉和音频模态起着至关重要的作用。通过使用RLVR，我们显著提升了模型在推理能力、情感识别准确性和泛化能力等三个关键方面的表现。此外，RLVR的引入不仅提高了模型在同分布数据上的整体性能，还在异分布数据集上展现出更强的鲁棒性。'}}}, {'id': 'https://huggingface.co/papers/2503.04808', 'title': 'Learning from Failures in Multi-Attempt Reinforcement Learning', 'url': 'https://huggingface.co/papers/2503.04808', 'abstract': "Recent advancements in reinforcement learning (RL) for large language models (LLMs), exemplified by DeepSeek R1, have shown that even a simple question-answering task can substantially improve an LLM's reasoning capabilities. In this work, we extend this approach by modifying the task into a multi-attempt setting. Instead of generating a single response per question, the model is given multiple attempts, with feedback provided after incorrect responses. The multi-attempt task encourages the model to refine its previous attempts and improve search efficiency. Experimental results show that even a small LLM trained on a multi-attempt task achieves significantly higher accuracy when evaluated with more attempts, improving from 45.6% with 1 attempt to 52.5% with 2 attempts on the math benchmark. In contrast, the same LLM trained on a standard single-turn task exhibits only a marginal improvement, increasing from 42.3% to 43.2% when given more attempts during evaluation. The results indicate that, compared to the standard single-turn task, an LLM trained on a multi-attempt task achieves slightly better performance on math benchmarks while also learning to refine its responses more effectively based on user feedback. Full code is available at https://github.com/DualityRL/multi-attempt", 'score': 11, 'issue_id': 2609, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 марта', 'en': 'March 4', 'zh': '3月4日'}, 'hash': 'fb2db794d0ea3c11', 'authors': ['Stephen Chung', 'Wenyu Du', 'Jie Fu'], 'affiliations': ['DualityRL', 'Shanghai AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2503.04808.jpg', 'data': {'categories': ['#optimization', '#rl', '#math', '#training', '#rlhf', '#reasoning'], 'emoji': '🔁', 'ru': {'title': 'Многопопыточное обучение: путь к более эффективным языковым моделям', 'desc': 'Это исследование расширяет подход обучения с подкреплением для больших языковых моделей, внедряя многопопыточную задачу вместо стандартной однопопыточной. Модель получает несколько попыток ответить на вопрос, получая обратную связь после неверных ответов, что способствует улучшению рассуждений и эффективности поиска. Эксперименты показывают, что даже небольшая языковая модель, обученная на многопопыточной задаче, достигает значительно более высокой точности при оценке с большим количеством попыток. Результаты демонстрируют, что модель, обученная на многопопыточной задаче, не только показывает лучшие результаты на математических тестах, но и эффективнее улучшает свои ответы на основе обратной связи пользователя.'}, 'en': {'title': 'Enhancing LLMs with Multi-Attempt Learning', 'desc': "This paper explores how modifying reinforcement learning tasks can enhance the reasoning abilities of large language models (LLMs). By implementing a multi-attempt question-answering framework, the model receives feedback on incorrect answers, allowing it to improve its responses iteratively. Experimental results demonstrate that even a small LLM can achieve better accuracy on math benchmarks when trained with this multi-attempt approach, compared to traditional single-turn tasks. The findings suggest that providing multiple attempts and feedback significantly aids in refining the model's performance."}, 'zh': {'title': '多次尝试，提升推理能力！', 'desc': '本研究探讨了在大型语言模型（LLM）中应用强化学习（RL）的新方法，特别是通过多次尝试的任务设置来提升模型的推理能力。与传统的单次回答不同，模型在每个问题上可以进行多次尝试，并在错误回答后获得反馈。这种多次尝试的任务设置促使模型改进之前的回答，从而提高搜索效率。实验结果表明，即使是小型LLM，在多次尝试的任务训练下，其准确率显著提高，显示出多次尝试对模型学习和反馈的有效性。'}}}, {'id': 'https://huggingface.co/papers/2503.05638', 'title': 'TrajectoryCrafter: Redirecting Camera Trajectory for Monocular Videos\n  via Diffusion Models', 'url': 'https://huggingface.co/papers/2503.05638', 'abstract': 'We present TrajectoryCrafter, a novel approach to redirect camera trajectories for monocular videos. By disentangling deterministic view transformations from stochastic content generation, our method achieves precise control over user-specified camera trajectories. We propose a novel dual-stream conditional video diffusion model that concurrently integrates point cloud renders and source videos as conditions, ensuring accurate view transformations and coherent 4D content generation. Instead of leveraging scarce multi-view videos, we curate a hybrid training dataset combining web-scale monocular videos with static multi-view datasets, by our innovative double-reprojection strategy, significantly fostering robust generalization across diverse scenes. Extensive evaluations on multi-view and large-scale monocular videos demonstrate the superior performance of our method.', 'score': 9, 'issue_id': 2612, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 марта', 'en': 'March 7', 'zh': '3月7日'}, 'hash': '33c7af8e9a7df61e', 'authors': ['Mark YU', 'Wenbo Hu', 'Jinbo Xing', 'Ying Shan'], 'affiliations': ['ARC Lab, Tencent PCG'], 'pdf_title_img': 'assets/pdf/title_img/2503.05638.jpg', 'data': {'categories': ['#dataset', '#3d', '#optimization', '#diffusion', '#video'], 'emoji': '🎥', 'ru': {'title': 'Управление траекторией камеры в видео с помощью искусственного интеллекта', 'desc': 'TrajectoryCrafter - это новый подход к перенаправлению траекторий камеры для монокулярных видео. Метод разделяет детерминированные преобразования вида и стохастическую генерацию контента, обеспечивая точный контроль над заданными пользователем траекториями камеры. Авторы предлагают двухпоточную условную модель диффузии видео, интегрирующую рендеры облака точек и исходные видео. Для обучения используется гибридный набор данных, сочетающий монокулярные видео и статические многоракурсные датасеты.'}, 'en': {'title': 'Mastering Camera Movement in Monocular Videos', 'desc': 'TrajectoryCrafter is a new method designed to change camera paths in single-camera videos. It separates the predictable changes in view from the random elements in the video content, allowing for better control over how the camera moves. The approach uses a dual-stream conditional video diffusion model that combines 3D point cloud images and original videos to ensure smooth transitions and realistic video generation. By creating a unique training dataset that merges large amounts of single-camera videos with some multi-camera data, the method shows improved performance across various scenes.'}, 'zh': {'title': '精准控制摄像机轨迹的创新方法', 'desc': '我们提出了一种名为TrajectoryCrafter的新方法，用于重定向单目视频的摄像机轨迹。通过将确定性的视图变换与随机内容生成分离，我们的方法能够精确控制用户指定的摄像机轨迹。我们提出了一种新颖的双流条件视频扩散模型，同时整合点云渲染和源视频作为条件，确保准确的视图变换和一致的4D内容生成。通过创新的双重重投影策略，我们结合了网络规模的单目视频和静态多视角数据集，显著提高了在不同场景中的鲁棒性和泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2503.04872', 'title': 'TinyR1-32B-Preview: Boosting Accuracy with Branch-Merge Distillation', 'url': 'https://huggingface.co/papers/2503.04872', 'abstract': 'The challenge of reducing the size of Large Language Models (LLMs) while maintaining their performance has gained significant attention. However, existing methods, such as model distillation and transfer learning, often fail to achieve high accuracy. To address this limitation, we introduce the Branch-Merge distillation approach, which enhances model compression through two phases: (1) the Branch Phase, where knowledge from a large teacher model is selectively distilled into specialized student models via domain-specific supervised fine-tuning (SFT); And (2) the Merge Phase, where these student models are merged to enable cross-domain knowledge transfer and improve generalization. We validate our distillation approach using DeepSeek-R1 as the teacher and DeepSeek-R1-Distill-Qwen-32B as the student. The resulting merged model, TinyR1-32B-Preview, outperforms its counterpart DeepSeek-R1-Distill-Qwen-32B across multiple benchmarks, including Mathematics (+5.5 points), Coding (+4.4 points) and Science (+2.9 points), while achieving near-equal performance to DeepSeek-R1 on AIME 2024. The Branch-Merge distillation approach provides a scalable solution for creating smaller, high-performing LLMs with reduced computational cost and time.', 'score': 9, 'issue_id': 2609, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 марта', 'en': 'March 6', 'zh': '3月6日'}, 'hash': '94defd7f9d19776e', 'authors': ['Lin Sun', 'Guangxiang Zhao', 'Xiaoqi Jian', 'Yuhan Wu', 'Weihong Lin', 'Yongfu Zhu', 'Change Jia', 'Linglin Zhang', 'Jinzhu Wu', 'Junfeng Ran', 'Sai-er Hu', 'Zihan Jiang', 'Junting Zhou', 'Wenrui Liu', 'Bin Cui', 'Tong Yang', 'Xiangzheng Zhang'], 'affiliations': ['Peking University', 'Qiyuan Tech'], 'pdf_title_img': 'assets/pdf/title_img/2503.04872.jpg', 'data': {'categories': ['#small_models', '#training', '#transfer_learning', '#optimization'], 'emoji': '🌳', 'ru': {'title': 'Ветвление и слияние: новый путь к компактным и мощным языковым моделям', 'desc': 'Статья представляет новый подход к сжатию больших языковых моделей (LLM) под названием Branch-Merge distillation. Метод состоит из двух фаз: Branch, где знания из большой модели-учителя дистиллируются в специализированные модели-ученики, и Merge, где эти модели объединяются для улучшения обобщения. Эксперименты показали, что полученная модель TinyR1-32B-Preview превосходит аналоги по нескольким бенчмаркам. Этот подход предлагает масштабируемое решение для создания меньших, но эффективных LLM с пониженными вычислительными затратами.'}, 'en': {'title': 'Branch-Merge: Compressing LLMs Without Compromise!', 'desc': 'This paper presents a new method called Branch-Merge distillation to reduce the size of Large Language Models (LLMs) while keeping their performance high. It consists of two main phases: the Branch Phase, where knowledge from a large teacher model is distilled into smaller, specialized student models through supervised fine-tuning, and the Merge Phase, where these student models are combined to enhance knowledge transfer across different domains. The approach was tested using specific models and showed that the merged model, TinyR1-32B-Preview, outperformed the individual student model in various tasks, including Mathematics, Coding, and Science. Overall, this method offers an effective way to create smaller LLMs that maintain strong performance and are more efficient in terms of computational resources.'}, 'zh': {'title': '分支合并蒸馏：高效压缩大型语言模型的创新方法', 'desc': '本文提出了一种新的模型蒸馏方法，称为分支合并蒸馏，旨在在保持性能的同时减少大型语言模型的体积。该方法分为两个阶段：分支阶段通过领域特定的监督微调将知识从大型教师模型选择性地蒸馏到专门的学生模型中；合并阶段则将这些学生模型合并，以实现跨领域知识转移并提高模型的泛化能力。实验结果表明，合并后的模型TinyR1-32B-Preview在多个基准测试中表现优于其对应的学生模型DeepSeek-R1-Distill-Qwen-32B。该方法为创建更小且高性能的语言模型提供了一种可扩展的解决方案，降低了计算成本和时间。'}}}, {'id': 'https://huggingface.co/papers/2503.05652', 'title': 'BEHAVIOR Robot Suite: Streamlining Real-World Whole-Body Manipulation\n  for Everyday Household Activities', 'url': 'https://huggingface.co/papers/2503.05652', 'abstract': "Real-world household tasks present significant challenges for mobile manipulation robots. An analysis of existing robotics benchmarks reveals that successful task performance hinges on three key whole-body control capabilities: bimanual coordination, stable and precise navigation, and extensive end-effector reachability. Achieving these capabilities requires careful hardware design, but the resulting system complexity further complicates visuomotor policy learning. To address these challenges, we introduce the BEHAVIOR Robot Suite (BRS), a comprehensive framework for whole-body manipulation in diverse household tasks. Built on a bimanual, wheeled robot with a 4-DoF torso, BRS integrates a cost-effective whole-body teleoperation interface for data collection and a novel algorithm for learning whole-body visuomotor policies. We evaluate BRS on five challenging household tasks that not only emphasize the three core capabilities but also introduce additional complexities, such as long-range navigation, interaction with articulated and deformable objects, and manipulation in confined spaces. We believe that BRS's integrated robotic embodiment, data collection interface, and learning framework mark a significant step toward enabling real-world whole-body manipulation for everyday household tasks. BRS is open-sourced at https://behavior-robot-suite.github.io/", 'score': 8, 'issue_id': 2608, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 марта', 'en': 'March 7', 'zh': '3月7日'}, 'hash': '52a7efb2f40f020a', 'authors': ['Yunfan Jiang', 'Ruohan Zhang', 'Josiah Wong', 'Chen Wang', 'Yanjie Ze', 'Hang Yin', 'Cem Gokmen', 'Shuran Song', 'Jiajun Wu', 'Li Fei-Fei'], 'affiliations': ['Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2503.05652.jpg', 'data': {'categories': ['#robotics', '#open_source', '#dataset', '#training'], 'emoji': '🤖', 'ru': {'title': 'Комплексная система для обучения роботов домашним задачам', 'desc': 'Статья представляет BEHAVIOR Robot Suite (BRS) - комплексную систему для манипуляции роботов в домашних условиях. BRS основан на двуруком колесном роботе с 4-осевым торсом и включает интерфейс телеуправления для сбора данных и новый алгоритм обучения визуомоторным политикам. Система оценивается на пяти сложных бытовых задачах, требующих бимануальной координации, точной навигации и широкой досягаемости манипуляторов. BRS представляет значительный шаг вперед в решении задач роботизированной манипуляции в реальных домашних условиях.'}, 'en': {'title': 'Empowering Robots for Everyday Household Tasks with BRS', 'desc': 'This paper presents the BEHAVIOR Robot Suite (BRS), a framework designed to enhance mobile manipulation robots for household tasks. It identifies three essential capabilities for effective task performance: bimanual coordination, stable navigation, and extensive reachability. The BRS integrates a teleoperation interface for data collection and a novel algorithm for learning visuomotor policies, addressing the complexities of hardware design and policy learning. The framework is evaluated on five challenging tasks that test these capabilities in real-world scenarios, aiming to improve robotic manipulation in everyday environments.'}, 'zh': {'title': '实现家庭任务的全身操控机器人', 'desc': '本论文介绍了BEHAVIOR机器人套件（BRS），旨在解决移动操作机器人在家庭任务中面临的挑战。研究表明，成功完成任务依赖于三项关键的全身控制能力：双手协调、稳定精确的导航和广泛的末端执行器可达性。BRS框架结合了一个双手轮式机器人和4自由度的躯干，提供了一种经济高效的全身遥操作接口用于数据收集，并提出了一种新算法用于学习全身视觉运动策略。通过在五个复杂的家庭任务上评估BRS，展示了其在长距离导航、与可动和可变形物体的交互以及在狭小空间中的操作能力。'}}}, {'id': 'https://huggingface.co/papers/2503.04824', 'title': 'ProReflow: Progressive Reflow with Decomposed Velocity', 'url': 'https://huggingface.co/papers/2503.04824', 'abstract': 'Diffusion models have achieved significant progress in both image and video generation while still suffering from huge computation costs. As an effective solution, flow matching aims to reflow the diffusion process of diffusion models into a straight line for a few-step and even one-step generation. However, in this paper, we suggest that the original training pipeline of flow matching is not optimal and introduce two techniques to improve it. Firstly, we introduce progressive reflow, which progressively reflows the diffusion models in local timesteps until the whole diffusion progresses, reducing the difficulty of flow matching. Second, we introduce aligned v-prediction, which highlights the importance of direction matching in flow matching over magnitude matching. Experimental results on SDv1.5 and SDXL demonstrate the effectiveness of our method, for example, conducting on SDv1.5 achieves an FID of 10.70 on MSCOCO2014 validation set with only 4 sampling steps, close to our teacher model (32 DDIM steps, FID = 10.05).', 'score': 8, 'issue_id': 2616, 'pub_date': '2025-03-05', 'pub_date_card': {'ru': '5 марта', 'en': 'March 5', 'zh': '3月5日'}, 'hash': '3380a5ba02714266', 'authors': ['Lei Ke', 'Haohang Xu', 'Xuefei Ning', 'Yu Li', 'Jiajun Li', 'Haoling Li', 'Yuxuan Lin', 'Dongsheng Jiang', 'Yujiu Yang', 'Linfeng Zhang'], 'affiliations': ['Huawei Inc.', 'Shanghai Jiao Tong University', 'Tsinghua University', 'University of Electronic Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2503.04824.jpg', 'data': {'categories': ['#training', '#cv', '#diffusion', '#optimization'], 'emoji': '🌊', 'ru': {'title': 'Оптимизация потоков для быстрой генерации изображений', 'desc': "Статья представляет усовершенствованный метод генерации изображений и видео с использованием диффузионных моделей. Авторы предлагают технику 'progressive reflow', которая постепенно перестраивает диффузионный процесс, упрощая задачу согласования потоков. Также вводится концепция 'aligned v-prediction', подчеркивающая важность соответствия направлений в процессе согласования потоков. Экспериментальные результаты на моделях SDv1.5 и SDXL демонстрируют эффективность предложенного метода, достигая высокого качества генерации при значительно меньшем количестве шагов сэмплирования."}, 'en': {'title': 'Streamlining Diffusion: Faster Generation with Flow Matching Enhancements', 'desc': 'This paper addresses the high computational costs associated with diffusion models used for image and video generation. It proposes flow matching as a method to streamline the diffusion process, allowing for faster generation in fewer steps. The authors introduce two enhancements: progressive reflow, which simplifies the flow matching by gradually adjusting local timesteps, and aligned v-prediction, which emphasizes the importance of direction over magnitude in flow matching. Experimental results show that their approach significantly improves performance, achieving competitive results with fewer sampling steps compared to traditional methods.'}, 'zh': {'title': '优化扩散模型的流匹配训练', 'desc': '扩散模型在图像和视频生成方面取得了显著进展，但计算成本仍然很高。为了解决这个问题，流匹配技术将扩散过程重新调整为直线，以实现少步甚至一步的生成。本文提出了两种改进流匹配训练流程的技术：逐步重新流动和对齐的v预测。实验结果表明，我们的方法在生成质量上接近教师模型，同时大幅减少了采样步骤。'}}}, {'id': 'https://huggingface.co/papers/2503.05447', 'title': 'Linear-MoE: Linear Sequence Modeling Meets Mixture-of-Experts', 'url': 'https://huggingface.co/papers/2503.05447', 'abstract': 'Linear Sequence Modeling (LSM) like linear attention, state space models and linear RNNs, and Mixture-of-Experts (MoE) have recently emerged as significant architectural improvements. In this paper, we introduce Linear-MoE, a production-level system for modeling and training large-scale models that integrate LSM with MoE. Linear-MoE leverages the advantages of both LSM modules for linear-complexity sequence modeling and MoE layers for sparsely activation, aiming to offer high performance with efficient training. The Linear-MoE system comprises: 1) Modeling subsystem, which provides a unified framework supporting all instances of LSM. and 2) Training subsystem, which facilitates efficient training by incorporating various advanced parallelism technologies, particularly Sequence Parallelism designed for Linear-MoE models. Additionally, we explore hybrid models that combine Linear-MoE layers with standard Transformer-MoE layers with its Sequence Parallelism to further enhance model flexibility and performance. Evaluations on two model series, A0.3B-2B and A1B-7B, demonstrate Linear-MoE achieves efficiency gains while maintaining competitive performance on various benchmarks, showcasing its potential as a next-generation foundational model architecture. Code: https://github.com/OpenSparseLLMs/Linear-MoE.', 'score': 6, 'issue_id': 2614, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 марта', 'en': 'March 7', 'zh': '3月7日'}, 'hash': '3975a97b4236e791', 'authors': ['Weigao Sun', 'Disen Lan', 'Tong Zhu', 'Xiaoye Qu', 'Yu Cheng'], 'affiliations': ['Shanghai AI Laboratory', 'Soochow University', 'South China University of Technology', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2503.05447.jpg', 'data': {'categories': ['#optimization', '#training', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'Linear-MoE: Объединение линейного моделирования и смеси экспертов для эффективных крупномасштабных моделей', 'desc': 'Эта статья представляет Linear-MoE - систему для моделирования и обучения крупномасштабных моделей, объединяющую линейное моделирование последовательностей (LSM) с методом смеси экспертов (MoE). Linear-MoE использует преимущества LSM-модулей для линейного моделирования последовательностей и MoE-слоев для разреженной активации, стремясь обеспечить высокую производительность при эффективном обучении. Система включает подсистему моделирования, предоставляющую унифицированную структуру для всех экземпляров LSM, и подсистему обучения с различными технологиями параллелизма. Оценки на двух сериях моделей показывают, что Linear-MoE достигает повышения эффективности при сохранении конкурентоспособной производительности на различных тестах.'}, 'en': {'title': 'Efficient Modeling with Linear-MoE: Merging LSM and MoE for High Performance', 'desc': 'This paper presents Linear-MoE, a new system that combines Linear Sequence Modeling (LSM) with Mixture-of-Experts (MoE) to improve the efficiency and performance of large-scale models. Linear-MoE utilizes linear-complexity sequence modeling from LSM and the sparsity benefits of MoE layers, allowing for effective training and high performance. The system includes a modeling subsystem for LSM and a training subsystem that employs advanced parallelism techniques, particularly Sequence Parallelism. Experiments show that Linear-MoE achieves better efficiency while delivering competitive results on various benchmarks, indicating its promise as a foundational model architecture.'}, 'zh': {'title': '线性-MoE：高效的序列建模与训练新架构', 'desc': '线性序列建模（LSM）和专家混合模型（MoE）最近成为重要的架构改进。本文介绍了一种名为Linear-MoE的系统，它将LSM与MoE结合，用于建模和训练大规模模型。Linear-MoE利用LSM模块的线性复杂度序列建模优势和MoE层的稀疏激活特性，旨在提供高性能和高效训练。通过对A0.3B-2B和A1B-7B模型系列的评估，Linear-MoE在保持竞争性能的同时实现了效率提升，展示了其作为下一代基础模型架构的潜力。'}}}, {'id': 'https://huggingface.co/papers/2503.04548', 'title': 'An Empirical Study on Eliciting and Improving R1-like Reasoning Models', 'url': 'https://huggingface.co/papers/2503.04548', 'abstract': 'In this report, we present the third technical report on the development of slow-thinking models as part of the STILL project. As the technical pathway becomes clearer, scaling RL training has become a central technique for implementing such reasoning models. We systematically experiment with and document the effects of various factors influencing RL training, conducting experiments on both base models and fine-tuned models. Specifically, we demonstrate that our RL training approach consistently improves the Qwen2.5-32B base models, enhancing both response length and test accuracy. Furthermore, we show that even when a model like DeepSeek-R1-Distill-Qwen-1.5B has already achieved a high performance level, it can be further refined through RL training, reaching an accuracy of 39.33% on AIME 2024. Beyond RL training, we also explore the use of tool manipulation, finding that it significantly boosts the reasoning performance of large reasoning models. This approach achieves a remarkable accuracy of 86.67% with greedy search on AIME 2024, underscoring its effectiveness in enhancing model capabilities. We release our resources at the STILL project website: https://github.com/RUCAIBox/Slow_Thinking_with_LLMs.', 'score': 6, 'issue_id': 2615, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 марта', 'en': 'March 6', 'zh': '3月6日'}, 'hash': 'defc053b9079a4a2', 'authors': ['Zhipeng Chen', 'Yingqian Min', 'Beichen Zhang', 'Jie Chen', 'Jinhao Jiang', 'Daixuan Cheng', 'Wayne Xin Zhao', 'Zheng Liu', 'Xu Miao', 'Yang Lu', 'Lei Fang', 'Zhongyuan Wang', 'Ji-Rong Wen'], 'affiliations': ['BAAI', 'DataCanvas', 'Gaoling School of Artificial Intelligence, Renmin University of China'], 'pdf_title_img': 'assets/pdf/title_img/2503.04548.jpg', 'data': {'categories': ['#reasoning', '#training', '#rl', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Усиление способностей языковых моделей к рассуждению через обучение с подкреплением', 'desc': 'В этом отчете представлены результаты экспериментов по улучшению моделей машинного обучения с помощью обучения с подкреплением (RL). Исследователи систематически изучали влияние различных факторов на RL-обучение как базовых, так и дообученных моделей. Было показано, что RL-обучение улучшает характеристики модели Qwen2.5-32B, увеличивая длину ответов и точность на тестах. Кроме того, использование инструментов значительно повысило способности моделей к рассуждению, достигнув точности 86.67% на наборе данных AIME 2024.'}, 'en': {'title': 'Enhancing Reasoning with Reinforcement Learning and Tool Manipulation', 'desc': 'This paper discusses advancements in slow-thinking models within the STILL project, focusing on reinforcement learning (RL) training techniques. The authors conduct systematic experiments to analyze how different factors affect RL training, leading to improvements in the Qwen2.5-32B base models. They demonstrate that even high-performing models can be further enhanced through RL training, achieving notable accuracy on benchmark tasks. Additionally, the study highlights the benefits of tool manipulation in improving reasoning performance, achieving impressive results on the AIME 2024 challenge.'}, 'zh': {'title': '强化学习提升推理模型的能力', 'desc': '本报告介绍了STILL项目中慢思维模型发展的第三个技术报告。我们系统地实验并记录了影响强化学习（RL）训练的各种因素，特别是在基础模型和微调模型上的实验。我们的研究表明，RL训练方法能够显著提高Qwen2.5-32B基础模型的响应长度和测试准确性。此外，我们还发现工具操作的使用显著提升了大型推理模型的推理性能，达到了86.67%的准确率。'}}}, {'id': 'https://huggingface.co/papers/2503.04359', 'title': 'LONGCODEU: Benchmarking Long-Context Language Models on Long Code\n  Understanding', 'url': 'https://huggingface.co/papers/2503.04359', 'abstract': "Current advanced long-context language models offer great potential for real-world software engineering applications. However, progress in this critical domain remains hampered by a fundamental limitation: the absence of a rigorous evaluation framework for long code understanding. To gap this obstacle, we propose a long code understanding benchmark LONGCODEU from four aspects (8 tasks) to evaluate LCLMs' long code understanding ability required for practical applications, including code unit perception, intra-code unit understanding, inter-code unit relation understanding, and long code documentation understanding. We evaluate 9 popular LCLMs on LONGCODEU (i.e., 6 general models and 3 code models). Our experimental results reveal key limitations in current LCLMs' capabilities for long code understanding. Particularly, the performance of LCLMs drops dramatically when the long code length is greater than 32K, falling far short of their claimed 128K-1M context windows. In the four aspects, inter-code unit relation understanding is the most challenging for LCLMs. Our study provides valuable insights for optimizing LCLMs and driving advancements in software engineering.", 'score': 5, 'issue_id': 2617, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 марта', 'en': 'March 6', 'zh': '3月6日'}, 'hash': '808bf0135ca11113', 'authors': ['Jia Li', 'Xuyuan Guo', 'Lei Li', 'Kechi Zhang', 'Ge Li', 'Jia Li', 'Zhengwei Tao', 'Fang Liu', 'Chongyang Tao', 'Yuqi Zhu', 'Zhi Jin'], 'affiliations': ['Key Lab of High Confidence Software Technology (Peking University), MoE, School of Computer Science, Peking University, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.04359.jpg', 'data': {'categories': ['#dataset', '#long_context', '#benchmark', '#optimization'], 'emoji': '📏', 'ru': {'title': 'Новый бенчмарк раскрывает ограничения языковых моделей в понимании длинного кода', 'desc': 'Статья представляет новый бенчмарк LONGCODEU для оценки способности языковых моделей с длинным контекстом (LCLM) понимать длинный программный код. Бенчмарк включает 8 задач в 4 аспектах: восприятие кодовых единиц, понимание внутри кодовых единиц, понимание связей между кодовыми единицами и понимание документации длинного кода. Результаты оценки 9 популярных LCLM показали, что их производительность значительно падает при длине кода более 32K токенов. Наиболее сложным аспектом для моделей оказалось понимание связей между кодовыми единицами.'}, 'en': {'title': 'Bridging the Gap in Long Code Understanding for LCLMs', 'desc': 'This paper addresses the challenges faced by advanced long-context language models (LCLMs) in understanding long code, which is crucial for software engineering. It introduces a new benchmark called LONGCODEU, designed to evaluate LCLMs across eight tasks that cover various aspects of long code comprehension. The study evaluates nine popular LCLMs and finds significant performance drops when handling code longer than 32K tokens, indicating limitations in their capabilities. The findings highlight that understanding relationships between code units is particularly difficult for these models, providing insights for future improvements in LCLMs.'}, 'zh': {'title': '提升长代码理解能力的关键评估基准', 'desc': '当前先进的长文本语言模型在软件工程应用中具有巨大潜力。然而，缺乏严格的评估框架限制了这一领域的进展。为了解决这个问题，我们提出了一个名为LONGCODEU的长代码理解基准，涵盖了四个方面的八个任务，以评估长代码理解能力。我们的实验结果显示，当前的长代码语言模型在处理超过32K的长代码时性能显著下降，尤其是在理解代码单元之间的关系方面最具挑战性。'}}}, {'id': 'https://huggingface.co/papers/2503.01713', 'title': 'SAGE: A Framework of Precise Retrieval for RAG', 'url': 'https://huggingface.co/papers/2503.01713', 'abstract': 'Retrieval-augmented generation (RAG) has demonstrated significant proficiency in conducting question-answering (QA) tasks within a specified corpus. Nonetheless, numerous failure instances of RAG in QA still exist. These failures are not solely attributable to the limitations of Large Language Models (LLMs); instead, they predominantly arise from the retrieval of inaccurate information for LLMs due to two limitations: (1) Current RAG methods segment the corpus without considering semantics, making it difficult to find relevant context due to impaired correlation between questions and the segments. (2) There is a trade-off between missing essential context with fewer context retrieved and getting irrelevant context with more context retrieved.   In this paper, we introduce a RAG framework (SAGE), to overcome these limitations. First, to address the segmentation issue without considering semantics, we propose to train a semantic segmentation model. This model is trained to segment the corpus into semantically complete chunks. Second, to ensure that only the most relevant chunks are retrieved while the irrelevant ones are ignored, we design a chunk selection algorithm to dynamically select chunks based on the decreasing speed of the relevance score, leading to a more relevant selection. Third, to further ensure the precision of the retrieved chunks, we propose letting LLMs assess whether retrieved chunks are excessive or lacking and then adjust the amount of context accordingly. Experiments show that SAGE outperforms baselines by 61.25% in the quality of QA on average. Moreover, by avoiding retrieving noisy context, SAGE lowers the cost of the tokens consumed in LLM inference and achieves a 49.41% enhancement in cost efficiency on average. Additionally, our work offers valuable insights for boosting RAG.', 'score': 4, 'issue_id': 2613, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': 'dc1c8022a96cab3e', 'authors': ['Jintao Zhang', 'Guoliang Li', 'Jinyang Su'], 'affiliations': ['Department of Computer Science Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.01713.jpg', 'data': {'categories': ['#rag', '#optimization', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'SAGE: Семантически улучшенный RAG для точных ответов на вопросы', 'desc': 'Статья представляет новый фреймворк SAGE для улучшения retrieval-augmented generation (RAG) в задачах вопросно-ответных систем. Авторы предлагают модель семантической сегментации корпуса текстов, алгоритм динамического выбора наиболее релевантных фрагментов и механизм оценки достаточности контекста с помощью языковых моделей. Эксперименты показывают, что SAGE превосходит базовые методы на 61.25% по качеству ответов и на 49.41% по эффективности использования токенов. Предложенный подход позволяет преодолеть ограничения существующих методов RAG и повысить точность извлечения релевантной информации.'}, 'en': {'title': 'SAGE: Smarter Retrieval for Better Question-Answering', 'desc': 'This paper presents a new framework called SAGE to improve retrieval-augmented generation (RAG) for question-answering tasks. It addresses two main issues: the ineffective segmentation of the corpus that ignores semantics and the trade-off between retrieving too little or too much context. SAGE introduces a semantic segmentation model to create meaningful chunks and a dynamic chunk selection algorithm to ensure only the most relevant information is retrieved. The results show that SAGE significantly enhances QA quality and cost efficiency compared to existing methods.'}, 'zh': {'title': '提升问答质量的智能检索框架', 'desc': '本文提出了一种新的检索增强生成框架（SAGE），旨在解决现有RAG方法在问答任务中的局限性。首先，SAGE通过训练语义分割模型，将语料库分割成语义完整的块，以提高相关性。其次，设计了一种动态选择算法，根据相关性得分的下降速度选择最相关的块，从而避免无关信息的干扰。实验结果表明，SAGE在问答质量上比基线提高了61.25%，并且在成本效率上提升了49.41%。'}}}, {'id': 'https://huggingface.co/papers/2503.01840', 'title': 'EAGLE-3: Scaling up Inference Acceleration of Large Language Models via\n  Training-Time Test', 'url': 'https://huggingface.co/papers/2503.01840', 'abstract': "The sequential nature of modern LLMs makes them expensive and slow, and speculative sampling has proven to be an effective solution to this problem. Methods like EAGLE perform autoregression at the feature level, reusing top-layer features from the target model to achieve better results than vanilla speculative sampling. A growing trend in the LLM community is scaling up training data to improve model intelligence without increasing inference costs. However, we observe that scaling up data provides limited improvements for EAGLE. We identify that this limitation arises from EAGLE's feature prediction constraints. In this paper, we introduce EAGLE-3, which abandons feature prediction in favor of direct token prediction and replaces reliance on top-layer features with multi-layer feature fusion via a technique named training-time test. These improvements significantly enhance performance and enable the draft model to fully benefit from scaling up training data. Our experiments include both chat models and reasoning models, evaluated on five tasks. The results show that EAGLE-3 achieves a speedup ratio up to 6.5x, with about 1.4x improvement over EAGLE-2. The code is available at https://github.com/SafeAILab/EAGLE.", 'score': 3, 'issue_id': 2619, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': 'cf0ee90637c3e71a', 'authors': ['Yuhui Li', 'Fangyun Wei', 'Chao Zhang', 'Hongyang Zhang'], 'affiliations': ['Microsoft Research', 'Peking University', 'University of Waterloo', 'Vector Institute'], 'pdf_title_img': 'assets/pdf/title_img/2503.01840.jpg', 'data': {'categories': ['#training', '#inference', '#optimization', '#reasoning'], 'emoji': '🚀', 'ru': {'title': 'EAGLE-3: Революционное ускорение LLM без потери качества', 'desc': 'Статья представляет EAGLE-3, новый метод спекулятивной выборки для ускорения работы больших языковых моделей (LLM). В отличие от предыдущих версий, EAGLE-3 использует прямое предсказание токенов и многослойное слияние признаков, что позволяет полностью использовать преимущества увеличения объема обучающих данных. Эксперименты показывают, что EAGLE-3 достигает ускорения до 6,5 раз по сравнению с обычной авторегрессией, что примерно в 1,4 раза лучше, чем EAGLE-2. Метод был протестирован на различных задачах с использованием как диалоговых, так и рассуждающих моделей.'}, 'en': {'title': 'EAGLE-3: Speeding Up LLMs with Direct Token Prediction', 'desc': 'This paper presents EAGLE-3, an advanced model that improves the efficiency of large language models (LLMs) by shifting from feature prediction to direct token prediction. By utilizing multi-layer feature fusion instead of relying solely on top-layer features, EAGLE-3 enhances performance and allows for better utilization of larger training datasets. The authors demonstrate that EAGLE-3 achieves significant speed improvements, with a speedup ratio of up to 6.5 times compared to previous methods. The results indicate that EAGLE-3 not only accelerates inference but also improves model intelligence, making it a valuable advancement in the field of machine learning.'}, 'zh': {'title': 'EAGLE-3：提升大语言模型推理速度的创新方案', 'desc': '现代大语言模型（LLM）的顺序特性使其在推理时成本高且速度慢，推测采样是一种有效的解决方案。EAGLE方法在特征层面进行自回归，利用目标模型的顶层特征，取得比传统推测采样更好的效果。尽管在LLM社区中，扩大训练数据以提高模型智能的趋势日益增长，但我们发现对于EAGLE来说，扩大数据的效果有限。本文提出了EAGLE-3，放弃特征预测，采用直接的标记预测，并通过训练时测试的技术实现多层特征融合，从而显著提升性能。'}}}, {'id': 'https://huggingface.co/papers/2502.18968', 'title': 'Know You First and Be You Better: Modeling Human-Like User Simulators\n  via Implicit Profiles', 'url': 'https://huggingface.co/papers/2502.18968', 'abstract': 'User simulators are crucial for replicating human interactions with dialogue systems, supporting both collaborative training and automatic evaluation, especially for large language models (LLMs). However, existing simulators often rely solely on text utterances, missing implicit user traits such as personality, speaking style, and goals. In contrast, persona-based methods lack generalizability, as they depend on predefined profiles of famous individuals or archetypes. To address these challenges, we propose User Simulator with implicit Profiles (USP), a framework that infers implicit user profiles from human-machine conversations and uses them to generate more personalized and realistic dialogues. We first develop an LLM-driven extractor with a comprehensive profile schema. Then, we refine the simulation through conditional supervised fine-tuning and reinforcement learning with cycle consistency, optimizing it at both the utterance and conversation levels. Finally, we adopt a diverse profile sampler to capture the distribution of real-world user profiles. Experimental results demonstrate that USP outperforms strong baselines in terms of authenticity and diversity while achieving comparable performance in consistency. Furthermore, dynamic multi-turn evaluations based on USP strongly align with mainstream benchmarks, demonstrating its effectiveness in real-world applications.', 'score': 3, 'issue_id': 2619, 'pub_date': '2025-02-26', 'pub_date_card': {'ru': '26 февраля', 'en': 'February 26', 'zh': '2月26日'}, 'hash': '32f11f6f4b295fc5', 'authors': ['Kuang Wang', 'Xianfei Li', 'Shenghao Yang', 'Li Zhou', 'Feng Jiang', 'Haizhou Li'], 'affiliations': ['Shenzhen Research Institute of Big Data', 'Shenzhen University of Advanced Technology', 'The Chinese University of Hong Kong, Shenzhen'], 'pdf_title_img': 'assets/pdf/title_img/2502.18968.jpg', 'data': {'categories': ['#rl', '#benchmark', '#training', '#optimization', '#games', '#agents', '#interpretability'], 'emoji': '🎭', 'ru': {'title': 'Реалистичная симуляция пользователей на основе неявных профилей', 'desc': 'Статья представляет новый подход к симуляции пользователей для диалоговых систем - User Simulator with implicit Profiles (USP). USP извлекает неявные профили пользователей из диалогов человека с машиной и использует их для генерации более персонализированных и реалистичных диалогов. Метод включает экстрактор на основе большой языковой модели, условное обучение с учителем и обучение с подкреплением. Эксперименты показывают превосходство USP над базовыми методами по аутентичности и разнообразию генерируемых диалогов.'}, 'en': {'title': 'Enhancing Dialogue Systems with Implicit User Profiles', 'desc': 'This paper introduces the User Simulator with implicit Profiles (USP), a novel framework designed to enhance dialogue systems by incorporating implicit user traits like personality and goals. Unlike traditional simulators that focus only on text, USP infers user profiles from actual human-machine interactions, allowing for more personalized dialogue generation. The framework employs a large language model (LLM) to extract user profiles and utilizes conditional supervised fine-tuning and reinforcement learning to improve dialogue quality. Experimental results show that USP significantly improves the authenticity and diversity of generated dialogues while maintaining consistency, making it effective for real-world applications.'}, 'zh': {'title': '隐式用户档案，提升对话真实感', 'desc': '本文提出了一种用户模拟器（USP），旨在通过隐式用户特征生成更个性化和真实的对话。现有的模拟器通常只依赖文本，忽视了用户的个性、说话风格和目标等隐性特征。USP通过从人机对话中推断隐式用户档案，结合大语言模型（LLM）和强化学习，优化对话生成过程。实验结果表明，USP在真实性和多样性方面优于现有方法，同时在一致性上表现相当，显示出其在实际应用中的有效性。'}}}, {'id': 'https://huggingface.co/papers/2503.05315', 'title': 'LoRACode: LoRA Adapters for Code Embeddings', 'url': 'https://huggingface.co/papers/2503.05315', 'abstract': 'Code embeddings are essential for semantic code search; however, current approaches often struggle to capture the precise syntactic and contextual nuances inherent in code. Open-source models such as CodeBERT and UniXcoder exhibit limitations in scalability and efficiency, while high-performing proprietary systems impose substantial computational costs. We introduce a parameter-efficient fine-tuning method based on Low-Rank Adaptation (LoRA) to construct task-specific adapters for code retrieval. Our approach reduces the number of trainable parameters to less than two percent of the base model, enabling rapid fine-tuning on extensive code corpora (2 million samples in 25 minutes on two H100 GPUs). Experiments demonstrate an increase of up to 9.1% in Mean Reciprocal Rank (MRR) for Code2Code search, and up to 86.69% for Text2Code search tasks across multiple programming languages. Distinction in task-wise and language-wise adaptation helps explore the sensitivity of code retrieval for syntactical and linguistic variations.', 'score': 2, 'issue_id': 2610, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 марта', 'en': 'March 7', 'zh': '3月7日'}, 'hash': '95dca112be949ba8', 'authors': ['Saumya Chaturvedi', 'Aman Chadha', 'Laurent Bindschaedler'], 'affiliations': ['AWS GenAI Santa Clara, CA, USA', 'Max Planck Institute for Software Systems Saarbrucken, Germany'], 'pdf_title_img': 'assets/pdf/title_img/2503.05315.jpg', 'data': {'categories': ['#data', '#open_source', '#training', '#optimization', '#plp'], 'emoji': '🔍', 'ru': {'title': 'LoRA: Эффективная тонкая настройка для точного поиска кода', 'desc': 'Статья представляет новый метод тонкой настройки для семантического поиска кода, основанный на Low-Rank Adaptation (LoRA). Этот подход значительно сокращает количество обучаемых параметров, позволяя быстро настраивать модели на больших объемах кода. Эксперименты показывают существенное улучшение показателей MRR для задач поиска Code2Code и Text2Code. Метод позволяет исследовать чувствительность поиска кода к синтаксическим и языковым вариациям.'}, 'en': {'title': 'Efficient Code Retrieval with Low-Rank Adaptation', 'desc': 'This paper addresses the challenges of semantic code search by improving how code embeddings are generated. It highlights the limitations of existing models like CodeBERT and UniXcoder in terms of scalability and efficiency. The authors propose a new method using Low-Rank Adaptation (LoRA) to create task-specific adapters, significantly reducing the number of trainable parameters. Their approach allows for quick fine-tuning on large code datasets, resulting in notable improvements in retrieval performance across various programming languages.'}, 'zh': {'title': '高效代码检索的低秩适应方法', 'desc': '本文提出了一种基于低秩适应（LoRA）的参数高效微调方法，用于构建特定任务的代码检索适配器。该方法将可训练参数减少到基础模型的不到2%，使得在大规模代码语料库上进行快速微调成为可能。实验结果显示，在代码到代码的检索任务中，平均倒数排名（MRR）提高了9.1%，而文本到代码的检索任务提高了86.69%。通过任务和语言的适应性区分，本文探讨了代码检索对语法和语言变体的敏感性。'}}}, {'id': 'https://huggingface.co/papers/2503.04504', 'title': 'AnyAnomaly: Zero-Shot Customizable Video Anomaly Detection with LVLM', 'url': 'https://huggingface.co/papers/2503.04504', 'abstract': 'Video anomaly detection (VAD) is crucial for video analysis and surveillance in computer vision. However, existing VAD models rely on learned normal patterns, which makes them difficult to apply to diverse environments. Consequently, users should retrain models or develop separate AI models for new environments, which requires expertise in machine learning, high-performance hardware, and extensive data collection, limiting the practical usability of VAD. To address these challenges, this study proposes customizable video anomaly detection (C-VAD) technique and the AnyAnomaly model. C-VAD considers user-defined text as an abnormal event and detects frames containing a specified event in a video. We effectively implemented AnyAnomaly using a context-aware visual question answering without fine-tuning the large vision language model. To validate the effectiveness of the proposed model, we constructed C-VAD datasets and demonstrated the superiority of AnyAnomaly. Furthermore, our approach showed competitive performance on VAD benchmark datasets, achieving state-of-the-art results on the UBnormal dataset and outperforming other methods in generalization across all datasets. Our code is available online at github.com/SkiddieAhn/Paper-AnyAnomaly.', 'score': 1, 'issue_id': 2613, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 марта', 'en': 'March 6', 'zh': '3月6日'}, 'hash': '7dbd20628d3eb105', 'authors': ['Sunghyun Ahn', 'Youngwan Jo', 'Kijung Lee', 'Sein Kwon', 'Inpyo Hong', 'Sanghyun Park'], 'affiliations': ['Yonsei University, Seoul, Korea'], 'pdf_title_img': 'assets/pdf/title_img/2503.04504.jpg', 'data': {'categories': ['#open_source', '#optimization', '#dataset', '#benchmark', '#cv', '#video'], 'emoji': '🕵️', 'ru': {'title': 'Универсальное обнаружение аномалий в видео без переобучения', 'desc': 'Исследователи представили новый подход к обнаружению аномалий в видео, названный C-VAD. Они разработали модель AnyAnomaly, которая может обнаруживать аномальные события, определенные пользователем, без необходимости переобучения для новых сред. Модель использует контекстно-зависимое визуальное понимание вопросов и ответов на основе большой мультимодальной языковой модели. Эксперименты показали превосходство AnyAnomaly на специально созданных наборах данных C-VAD и конкурентоспособные результаты на стандартных бенчмарках обнаружения видеоаномалий.'}, 'en': {'title': 'Customizable Anomaly Detection for Diverse Video Environments', 'desc': 'This paper introduces a new approach to video anomaly detection (VAD) called customizable video anomaly detection (C-VAD). Unlike traditional VAD models that require retraining for different environments, C-VAD allows users to define what constitutes an abnormal event using text input. The AnyAnomaly model leverages a context-aware visual question answering system, enabling it to detect specified events without the need for extensive fine-tuning. The results show that AnyAnomaly not only performs well on custom datasets but also achieves state-of-the-art results on established VAD benchmarks, demonstrating its versatility and effectiveness.'}, 'zh': {'title': '可定制的视频异常检测，轻松应对多样环境', 'desc': '视频异常检测（VAD）在计算机视觉中的视频分析和监控中至关重要。现有的VAD模型依赖于学习到的正常模式，这使得它们在不同环境中的应用变得困难。为了解决这些问题，本研究提出了可定制的视频异常检测（C-VAD）技术和AnyAnomaly模型，允许用户定义异常事件并检测视频中的相关帧。我们的模型在多个基准数据集上表现出色，尤其在UBnormal数据集上达到了最先进的结果，展示了其在泛化能力上的优势。'}}}, {'id': 'https://huggingface.co/papers/2503.11647', 'title': 'ReCamMaster: Camera-Controlled Generative Rendering from A Single Video', 'url': 'https://huggingface.co/papers/2503.11647', 'abstract': 'Camera control has been actively studied in text or image conditioned video generation tasks. However, altering camera trajectories of a given video remains under-explored, despite its importance in the field of video creation. It is non-trivial due to the extra constraints of maintaining multiple-frame appearance and dynamic synchronization. To address this, we present ReCamMaster, a camera-controlled generative video re-rendering framework that reproduces the dynamic scene of an input video at novel camera trajectories. The core innovation lies in harnessing the generative capabilities of pre-trained text-to-video models through a simple yet powerful video conditioning mechanism -- its capability often overlooked in current research. To overcome the scarcity of qualified training data, we construct a comprehensive multi-camera synchronized video dataset using Unreal Engine 5, which is carefully curated to follow real-world filming characteristics, covering diverse scenes and camera movements. It helps the model generalize to in-the-wild videos. Lastly, we further improve the robustness to diverse inputs through a meticulously designed training strategy. Extensive experiments tell that our method substantially outperforms existing state-of-the-art approaches and strong baselines. Our method also finds promising applications in video stabilization, super-resolution, and outpainting. Project page: https://jianhongbai.github.io/ReCamMaster/', 'score': 80, 'issue_id': 2730, 'pub_date': '2025-03-14', 'pub_date_card': {'ru': '14 марта', 'en': 'March 14', 'zh': '3月14日'}, 'hash': '7e72838ea84ed904', 'authors': ['Jianhong Bai', 'Menghan Xia', 'Xiao Fu', 'Xintao Wang', 'Lianrui Mu', 'Jinwen Cao', 'Zuozhu Liu', 'Haoji Hu', 'Xiang Bai', 'Pengfei Wan', 'Di Zhang'], 'affiliations': ['CUHK', 'HUST', 'Kuaishou Technology', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.11647.jpg', 'data': {'categories': ['#dataset', '#optimization', '#training', '#video', '#games'], 'emoji': '🎥', 'ru': {'title': 'Управление камерой в видео с помощью генеративных моделей', 'desc': 'ReCamMaster - это генеративная система для изменения траектории камеры в видео. Она использует предобученные модели text-to-video и механизм видео-кондиционирования для воспроизведения динамической сцены с новых ракурсов. Для обучения был создан специальный датасет синхронизированных мультикамерных видео в Unreal Engine 5. Система превосходит существующие подходы и находит применение в стабилизации, суперразрешении и аутпейнтинге видео.'}, 'en': {'title': 'ReCamMaster: Mastering Camera Control in Video Generation', 'desc': 'This paper introduces ReCamMaster, a novel framework for generating videos with controlled camera trajectories. It addresses the challenge of maintaining visual consistency and dynamic synchronization across multiple frames when altering camera paths. The framework leverages pre-trained text-to-video models and a specially curated multi-camera synchronized video dataset to enhance its performance. Experimental results demonstrate that ReCamMaster significantly outperforms existing methods, showcasing its potential for applications like video stabilization and super-resolution.'}, 'zh': {'title': '重塑视频动态，掌控相机轨迹', 'desc': '本论文研究了在文本或图像条件下生成视频时的相机控制问题。尽管改变视频的相机轨迹很重要，但这一领域的研究仍然较少。我们提出了ReCamMaster，一个基于生成模型的视频重渲染框架，能够在新的相机轨迹下重现输入视频的动态场景。通过构建一个多相机同步视频数据集，并采用精心设计的训练策略，我们的方法在多种输入下表现出色，超越了现有的最先进技术。'}}}, {'id': 'https://huggingface.co/papers/2503.07677', 'title': 'PLADIS: Pushing the Limits of Attention in Diffusion Models at Inference\n  Time by Leveraging Sparsity', 'url': 'https://huggingface.co/papers/2503.07677', 'abstract': 'Diffusion models have shown impressive results in generating high-quality conditional samples using guidance techniques such as Classifier-Free Guidance (CFG). However, existing methods often require additional training or neural function evaluations (NFEs), making them incompatible with guidance-distilled models. Also, they rely on heuristic approaches that need identifying target layers. In this work, we propose a novel and efficient method, termed PLADIS, which boosts pre-trained models (U-Net/Transformer) by leveraging sparse attention. Specifically, we extrapolate query-key correlations using softmax and its sparse counterpart in the cross-attention layer during inference, without requiring extra training or NFEs. By leveraging the noise robustness of sparse attention, our PLADIS unleashes the latent potential of text-to-image diffusion models, enabling them to excel in areas where they once struggled with newfound effectiveness. It integrates seamlessly with guidance techniques, including guidance-distilled models. Extensive experiments show notable improvements in text alignment and human preference, offering a highly efficient and universally applicable solution.', 'score': 65, 'issue_id': 2730, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '913b88ac595cc8b6', 'authors': ['Kwanyoung Kim', 'Byeongsu Sim'], 'affiliations': ['Samsung Research'], 'pdf_title_img': 'assets/pdf/title_img/2503.07677.jpg', 'data': {'categories': ['#training', '#cv', '#diffusion', '#inference'], 'emoji': '🖼️', 'ru': {'title': 'PLADIS: Эффективное улучшение генерации изображений с помощью разреженного внимания', 'desc': 'Статья представляет новый метод под названием PLADIS для улучшения предобученных моделей диффузии в задаче генерации изображений по текстовому описанию. PLADIS использует разреженное внимание для экстраполяции корреляций запрос-ключ в слое кросс-внимания во время вывода, не требуя дополнительного обучения или оценок нейронных функций. Метод хорошо сочетается с существующими техниками направленной генерации, включая модели с дистиллированным направлением. Эксперименты показывают значительное улучшение соответствия текста и изображения, а также предпочтений пользователей.'}, 'en': {'title': 'Unlocking the Power of Diffusion Models with Sparse Attention', 'desc': "This paper introduces PLADIS, a new method that enhances pre-trained diffusion models like U-Net and Transformer by using sparse attention techniques. Unlike previous methods that needed extra training or evaluations, PLADIS operates efficiently during inference by utilizing query-key correlations in the cross-attention layer. This approach improves the models' performance in generating high-quality images from text prompts without the need for additional training. The results demonstrate significant advancements in text alignment and user preference, making PLADIS a versatile solution for various applications in text-to-image generation."}, 'zh': {'title': 'PLADIS：高效提升扩散模型的稀疏注意力方法', 'desc': '扩散模型在生成高质量条件样本方面表现出色，尤其是使用无分类器引导（CFG）等技术。然而，现有方法通常需要额外的训练或神经功能评估（NFE），这使得它们与引导蒸馏模型不兼容。本文提出了一种新颖高效的方法，称为PLADIS，通过利用稀疏注意力来增强预训练模型（如U-Net/Transformer）。PLADIS在推理过程中利用交叉注意力层中的softmax和稀疏对应物，提升文本到图像的扩散模型的潜力，显著改善文本对齐和人类偏好。'}}}, {'id': 'https://huggingface.co/papers/2503.11646', 'title': 'Adversarial Data Collection: Human-Collaborative Perturbations for\n  Efficient and Robust Robotic Imitation Learning', 'url': 'https://huggingface.co/papers/2503.11646', 'abstract': 'The pursuit of data efficiency, where quality outweighs quantity, has emerged as a cornerstone in robotic manipulation, especially given the high costs associated with real-world data collection. We propose that maximizing the informational density of individual demonstrations can dramatically reduce reliance on large-scale datasets while improving task performance. To this end, we introduce Adversarial Data Collection, a Human-in-the-Loop (HiL) framework that redefines robotic data acquisition through real-time, bidirectional human-environment interactions. Unlike conventional pipelines that passively record static demonstrations, ADC adopts a collaborative perturbation paradigm: during a single episode, an adversarial operator dynamically alters object states, environmental conditions, and linguistic commands, while the tele-operator adaptively adjusts actions to overcome these evolving challenges. This process compresses diverse failure-recovery behaviors, compositional task variations, and environmental perturbations into minimal demonstrations. Our experiments demonstrate that ADC-trained models achieve superior compositional generalization to unseen task instructions, enhanced robustness to perceptual perturbations, and emergent error recovery capabilities. Strikingly, models trained with merely 20% of the demonstration volume collected through ADC significantly outperform traditional approaches using full datasets. These advances bridge the gap between data-centric learning paradigms and practical robotic deployment, demonstrating that strategic data acquisition, not merely post-hoc processing, is critical for scalable, real-world robot learning. Additionally, we are curating a large-scale ADC-Robotics dataset comprising real-world manipulation tasks with adversarial perturbations. This benchmark will be open-sourced to facilitate advancements in robotic imitation learning.', 'score': 31, 'issue_id': 2731, 'pub_date': '2025-03-14', 'pub_date_card': {'ru': '14 марта', 'en': 'March 14', 'zh': '3月14日'}, 'hash': 'efdf1296bc567414', 'authors': ['Siyuan Huang', 'Yue Liao', 'Siyuan Feng', 'Shu Jiang', 'Si Liu', 'Hongsheng Li', 'Maoqing Yao', 'Guanghui Ren'], 'affiliations': ['Agibot', 'Beihang University', 'MMLab, CUHK', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2503.11646.jpg', 'data': {'categories': ['#robotics', '#benchmark', '#dataset', '#optimization', '#open_source', '#agents', '#training', '#data'], 'emoji': '🤖', 'ru': {'title': 'Меньше данных, больше эффективности: революция в обучении роботов', 'desc': 'Статья представляет новый подход к сбору данных для обучения роботов манипуляции - Adversarial Data Collection (ADC). ADC использует взаимодействие человека-оператора и среды в реальном времени для создания информационно насыщенных демонстраций. Эксперименты показывают, что модели, обученные на ADC-данных, достигают лучшей композиционной генерализации и устойчивости к возмущениям, чем традиционные подходы. Авторы также создают открытый набор данных ADC-Robotics для продвижения исследований в области имитационного обучения роботов.'}, 'en': {'title': 'Maximizing Data Efficiency in Robotic Learning with Adversarial Collection', 'desc': 'This paper introduces a new method called Adversarial Data Collection (ADC) to improve robotic manipulation by focusing on data efficiency. Instead of relying on large datasets, ADC enhances the quality of data through real-time interactions between humans and robots, allowing for dynamic adjustments during demonstrations. By using an adversarial approach, the framework captures a wide range of behaviors and challenges in fewer demonstrations, leading to better performance in unseen tasks. The results show that models trained with ADC can generalize better and recover from errors more effectively, even with significantly less data than traditional methods.'}, 'zh': {'title': '对抗性数据收集：提升机器人学习效率的关键', 'desc': '本论文提出了一种新的数据收集方法，称为对抗性数据收集（ADC），旨在提高机器人操作的效率。通过实时的人机交互，ADC能够在动态环境中收集高信息密度的演示数据，从而减少对大规模数据集的依赖。实验表明，使用ADC训练的模型在面对未见任务指令时表现出更好的组合泛化能力和对环境干扰的鲁棒性。最终，ADC方法显著提高了机器人学习的实用性，展示了战略性数据获取的重要性。'}}}, {'id': 'https://huggingface.co/papers/2503.11224', 'title': 'Technologies on Effectiveness and Efficiency: A Survey of State Spaces\n  Models', 'url': 'https://huggingface.co/papers/2503.11224', 'abstract': 'State Space Models (SSMs) have emerged as a promising alternative to the popular transformer-based models and have been increasingly gaining attention. Compared to transformers, SSMs excel at tasks with sequential data or longer contexts, demonstrating comparable performances with significant efficiency gains. In this survey, we provide a coherent and systematic overview for SSMs, including their theoretical motivations, mathematical formulations, comparison with existing model classes, and various applications. We divide the SSM series into three main sections, providing a detailed introduction to the original SSM, the structured SSM represented by S4, and the selective SSM typified by Mamba. We put an emphasis on technicality, and highlight the various key techniques introduced to address the effectiveness and efficiency of SSMs. We hope this manuscript serves as an introduction for researchers to explore the theoretical foundations of SSMs.', 'score': 21, 'issue_id': 2731, 'pub_date': '2025-03-14', 'pub_date_card': {'ru': '14 марта', 'en': 'March 14', 'zh': '3月14日'}, 'hash': 'fb4219d497e59f64', 'authors': ['Xingtai Lv', 'Youbang Sun', 'Kaiyan Zhang', 'Shang Qu', 'Xuekai Zhu', 'Yuchen Fan', 'Yi Wu', 'Ermo Hua', 'Xinwei Long', 'Ning Ding', 'Bowen Zhou'], 'affiliations': ['Department of Electronic Engineering, Tsinghua University', 'Robotics Institute, Carnegie Mellon University', 'Shanghai Artificial Intelligence Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2503.11224.jpg', 'data': {'categories': ['#architecture', '#long_context', '#survey', '#math', '#training'], 'emoji': '🧠', 'ru': {'title': 'SSM: Эффективная альтернатива трансформерам для обработки последовательностей', 'desc': 'Это обзор моделей пространства состояний (SSM) как альтернативы трансформерам в машинном обучении. SSM показывают сравнимую производительность с трансформерами, но более эффективны для задач с последовательными данными и длинным контекстом. В статье рассматриваются теоретические основы, математические формулировки и применения SSM. Авторы выделяют три основных типа SSM: оригинальные, структурированные (например, S4) и селективные (например, Mamba).'}, 'en': {'title': 'Unlocking Efficiency: The Power of State Space Models', 'desc': 'State Space Models (SSMs) are gaining popularity as an efficient alternative to transformer models, especially for sequential data and longer contexts. This paper provides a comprehensive overview of SSMs, detailing their theoretical foundations, mathematical formulations, and comparisons with other model types. It categorizes SSMs into three sections: the original SSM, the structured S4 model, and the selective Mamba model, emphasizing their unique techniques for improved performance. The goal is to guide researchers in understanding and exploring the potential of SSMs in various applications.'}, 'zh': {'title': '状态空间模型：高效处理序列数据的新选择', 'desc': '状态空间模型（SSMs）作为一种有前景的替代方案，逐渐受到关注，尤其是在处理序列数据或长上下文任务时表现优异。与流行的变换器模型相比，SSMs在效率上有显著提升，同时在性能上也能与之媲美。本文对SSMs进行了系统的概述，包括其理论动机、数学公式、与现有模型的比较以及各种应用。我们将SSM系列分为三个主要部分，详细介绍了原始SSM、结构化SSM（如S4）和选择性SSM（如Mamba），并强调了提高SSM有效性和效率的关键技术。'}}}, {'id': 'https://huggingface.co/papers/2503.11069', 'title': 'API Agents vs. GUI Agents: Divergence and Convergence', 'url': 'https://huggingface.co/papers/2503.11069', 'abstract': 'Large language models (LLMs) have evolved beyond simple text generation to power software agents that directly translate natural language commands into tangible actions. While API-based LLM agents initially rose to prominence for their robust automation capabilities and seamless integration with programmatic endpoints, recent progress in multimodal LLM research has enabled GUI-based LLM agents that interact with graphical user interfaces in a human-like manner. Although these two paradigms share the goal of enabling LLM-driven task automation, they diverge significantly in architectural complexity, development workflows, and user interaction models.   This paper presents the first comprehensive comparative study of API-based and GUI-based LLM agents, systematically analyzing their divergence and potential convergence. We examine key dimensions and highlight scenarios in which hybrid approaches can harness their complementary strengths. By proposing clear decision criteria and illustrating practical use cases, we aim to guide practitioners and researchers in selecting, combining, or transitioning between these paradigms. Ultimately, we indicate that continuing innovations in LLM-based automation are poised to blur the lines between API- and GUI-driven agents, paving the way for more flexible, adaptive solutions in a wide range of real-world applications.', 'score': 20, 'issue_id': 2730, 'pub_date': '2025-03-14', 'pub_date_card': {'ru': '14 марта', 'en': 'March 14', 'zh': '3月14日'}, 'hash': '29e714954ed20978', 'authors': ['Chaoyun Zhang', 'Shilin He', 'Liqun Li', 'Si Qin', 'Yu Kang', 'Qingwei Lin', 'Dongmei Zhang'], 'affiliations': ['Microsoft'], 'pdf_title_img': 'assets/pdf/title_img/2503.11069.jpg', 'data': {'categories': ['#multimodal', '#survey', '#agents'], 'emoji': '🤖', 'ru': {'title': 'Сравнение API и GUI агентов на основе LLM: путь к гибридным решениям', 'desc': 'Это исследование сравнивает агентов на основе больших языковых моделей (LLM), работающих через API и через графический интерфейс. Авторы анализируют различия в архитектуре, разработке и взаимодействии с пользователем для обоих подходов. Они предлагают критерии выбора и описывают сценарии, где гибридные решения могут быть эффективны. Исследование показывает, что инновации в автоматизации на основе LLM стирают границы между этими парадигмами.'}, 'en': {'title': 'Bridging the Gap: API and GUI LLM Agents Unite', 'desc': 'This paper explores the evolution of large language models (LLMs) from simple text generators to sophisticated software agents that can perform tasks based on natural language commands. It compares two main types of LLM agents: API-based agents, which automate tasks through programmatic interfaces, and GUI-based agents, which interact with graphical user interfaces like humans. The study highlights the differences in their architecture, development processes, and user interactions, while also discussing how hybrid approaches can leverage the strengths of both paradigms. The authors provide decision criteria and practical examples to help users choose the right approach for their needs, suggesting that future advancements will further integrate these two types of agents.'}, 'zh': {'title': 'API与GUI代理的比较与融合之路', 'desc': '大型语言模型（LLMs）已经从简单的文本生成发展到能够将自然语言命令直接转化为实际操作的软件代理。本文首次全面比较了基于API的LLM代理和基于GUI的LLM代理，分析了它们在架构复杂性、开发工作流程和用户交互模型上的显著差异。我们探讨了关键维度，并强调了混合方法在利用两者互补优势方面的场景。最终，我们指出LLM驱动的自动化创新将模糊API和GUI代理之间的界限，为各种实际应用提供更灵活、适应性强的解决方案。'}}}, {'id': 'https://huggingface.co/papers/2503.11514', 'title': 'Exploring the Vulnerabilities of Federated Learning: A Deep Dive into\n  Gradient Inversion Attacks', 'url': 'https://huggingface.co/papers/2503.11514', 'abstract': 'Federated Learning (FL) has emerged as a promising privacy-preserving collaborative model training paradigm without sharing raw data. However, recent studies have revealed that private information can still be leaked through shared gradient information and attacked by Gradient Inversion Attacks (GIA). While many GIA methods have been proposed, a detailed analysis, evaluation, and summary of these methods are still lacking. Although various survey papers summarize existing privacy attacks in FL, few studies have conducted extensive experiments to unveil the effectiveness of GIA and their associated limiting factors in this context. To fill this gap, we first undertake a systematic review of GIA and categorize existing methods into three types, i.e., optimization-based GIA (OP-GIA), generation-based GIA (GEN-GIA), and analytics-based GIA (ANA-GIA). Then, we comprehensively analyze and evaluate the three types of GIA in FL, providing insights into the factors that influence their performance, practicality, and potential threats. Our findings indicate that OP-GIA is the most practical attack setting despite its unsatisfactory performance, while GEN-GIA has many dependencies and ANA-GIA is easily detectable, making them both impractical. Finally, we offer a three-stage defense pipeline to users when designing FL frameworks and protocols for better privacy protection and share some future research directions from the perspectives of attackers and defenders that we believe should be pursued. We hope that our study can help researchers design more robust FL frameworks to defend against these attacks.', 'score': 13, 'issue_id': 2730, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 марта', 'en': 'March 13', 'zh': '3月13日'}, 'hash': 'd31bf6f9bd4bc86b', 'authors': ['Pengxin Guo', 'Runxi Wang', 'Shuang Zeng', 'Jinjing Zhu', 'Haoning Jiang', 'Yanran Wang', 'Yuyin Zhou', 'Feifei Wang', 'Hui Xiong', 'Liangqiong Qu'], 'affiliations': ['Department of Biomedical Data Science, Stanford University, Stanford, CA 94305, USA', 'Department of Computer Science and Engineering, University of California, Santa Cruz, CA 95064, USA', 'Department of Electrical and Electronic Engineering, The University of Hong Kong, Hong Kong 999077, China', 'Department of Electronic and Electrical Engineering, Southern University of Science and Technology, Shenzhen 518055, China', 'Department of Mathematics, The University of Hong Kong, Hong Kong 999077, China', 'Materials Innovation Institute for Life Sciences and Energy (MILES), HKU-SIRI, Shenzhen 518055, China', 'School of Computing and Data Science, The University of Hong Kong, Hong Kong 999077, China', 'Thrust of Artificial Intelligence, The Hong Kong University of Science and Technology (Guangzhou), Guangzhou 511458, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.11514.jpg', 'data': {'categories': ['#leakage', '#benchmark', '#security', '#survey', '#healthcare', '#data'], 'emoji': '🛡️', 'ru': {'title': 'Защита приватности в федеративном обучении: анализ атак с инверсией градиента', 'desc': 'Статья посвящена анализу атак с инверсией градиента (GIA) в контексте федеративного обучения (FL). Авторы классифицируют существующие методы GIA на три типа: оптимизационные, генеративные и аналитические. Проводится комплексная оценка эффективности и практичности каждого типа атак в FL. Исследование показывает, что оптимизационные GIA являются наиболее практичными, несмотря на их неудовлетворительную производительность.'}, 'en': {'title': 'Strengthening Privacy in Federated Learning Against Gradient Inversion Attacks', 'desc': "This paper focuses on the vulnerabilities of Federated Learning (FL) to Gradient Inversion Attacks (GIA), which can leak private information despite the model's privacy-preserving intentions. It categorizes existing GIA methods into three types: optimization-based, generation-based, and analytics-based, and provides a thorough analysis of their effectiveness and limitations. The study reveals that while optimization-based GIA is the most practical, it still has performance issues, whereas generation-based and analytics-based methods are less practical due to their dependencies and detectability. The authors propose a defense strategy to enhance privacy in FL frameworks and suggest future research directions to strengthen defenses against these attacks."}, 'zh': {'title': '提升联邦学习隐私保护的防御策略', 'desc': '联邦学习（FL）是一种保护隐私的协作模型训练方法，不需要共享原始数据。然而，最近的研究表明，通过共享梯度信息，私密信息仍然可能被泄露，并受到梯度反演攻击（GIA）的威胁。本文对现有的GIA方法进行了系统的回顾和分类，并分析了三种类型的GIA在FL中的表现和局限性。最后，我们提出了一个三阶段的防御方案，以帮助用户在设计FL框架时更好地保护隐私。'}}}, {'id': 'https://huggingface.co/papers/2503.10772', 'title': 'FlowTok: Flowing Seamlessly Across Text and Image Tokens', 'url': 'https://huggingface.co/papers/2503.10772', 'abstract': 'Bridging different modalities lies at the heart of cross-modality generation. While conventional approaches treat the text modality as a conditioning signal that gradually guides the denoising process from Gaussian noise to the target image modality, we explore a much simpler paradigm-directly evolving between text and image modalities through flow matching. This requires projecting both modalities into a shared latent space, which poses a significant challenge due to their inherently different representations: text is highly semantic and encoded as 1D tokens, whereas images are spatially redundant and represented as 2D latent embeddings. To address this, we introduce FlowTok, a minimal framework that seamlessly flows across text and images by encoding images into a compact 1D token representation. Compared to prior methods, this design reduces the latent space size by 3.3x at an image resolution of 256, eliminating the need for complex conditioning mechanisms or noise scheduling. Moreover, FlowTok naturally extends to image-to-text generation under the same formulation. With its streamlined architecture centered around compact 1D tokens, FlowTok is highly memory-efficient, requires significantly fewer training resources, and achieves much faster sampling speeds-all while delivering performance comparable to state-of-the-art models. Code will be available at https://github.com/bytedance/1d-tokenizer.', 'score': 12, 'issue_id': 2731, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 марта', 'en': 'March 13', 'zh': '3月13日'}, 'hash': '548255900cd1ec21', 'authors': ['Ju He', 'Qihang Yu', 'Qihao Liu', 'Liang-Chieh Chen'], 'affiliations': ['ByteDance Seed', 'Johns Hopkins University'], 'pdf_title_img': 'assets/pdf/title_img/2503.10772.jpg', 'data': {'categories': ['#architecture', '#training', '#multimodal', '#diffusion'], 'emoji': '🌊', 'ru': {'title': 'FlowTok: эффективный переход между текстом и изображением через токены', 'desc': 'FlowTok - это новый подход к генерации изображений из текста и наоборот. Он использует метод согласования потоков для прямого перехода между модальностями текста и изображения в общем латентном пространстве. Ключевая идея заключается в кодировании изображений в компактное одномерное токенное представление, что значительно уменьшает размер латентного пространства. Это позволяет упростить архитектуру, сократить потребление памяти и ускорить обучение и генерацию по сравнению с существующими методами.'}, 'en': {'title': 'FlowTok: Simplifying Cross-Modality Generation with 1D Tokens', 'desc': 'This paper presents FlowTok, a novel framework for cross-modality generation that simplifies the process of transitioning between text and image modalities. Unlike traditional methods that rely on complex conditioning signals and denoising processes, FlowTok directly evolves between text and images by utilizing flow matching in a shared latent space. The framework encodes images into a compact 1D token representation, significantly reducing the latent space size and improving efficiency. FlowTok not only enhances memory usage and training resource requirements but also maintains competitive performance in generating images from text and vice versa.'}, 'zh': {'title': 'FlowTok：简化跨模态生成的高效框架', 'desc': '这篇论文探讨了跨模态生成中的不同模态之间的桥接问题。传统方法将文本模态视为引导信号，逐步引导去噪过程，而我们提出了一种更简单的方法，通过流匹配直接在文本和图像模态之间演变。我们引入了FlowTok框架，将图像编码为紧凑的1D标记表示，从而在共享潜在空间中流动，显著减少了潜在空间的大小。FlowTok不仅提高了内存效率和采样速度，还在图像到文本生成方面表现出色，性能与最先进的模型相当。'}}}, {'id': 'https://huggingface.co/papers/2503.11576', 'title': 'SmolDocling: An ultra-compact vision-language model for end-to-end\n  multi-modal document conversion', 'url': 'https://huggingface.co/papers/2503.11576', 'abstract': 'We introduce SmolDocling, an ultra-compact vision-language model targeting end-to-end document conversion. Our model comprehensively processes entire pages by generating DocTags, a new universal markup format that captures all page elements in their full context with location. Unlike existing approaches that rely on large foundational models, or ensemble solutions that rely on handcrafted pipelines of multiple specialized models, SmolDocling offers an end-to-end conversion for accurately capturing content, structure and spatial location of document elements in a 256M parameters vision-language model. SmolDocling exhibits robust performance in correctly reproducing document features such as code listings, tables, equations, charts, lists, and more across a diverse range of document types including business documents, academic papers, technical reports, patents, and forms -- significantly extending beyond the commonly observed focus on scientific papers. Additionally, we contribute novel publicly sourced datasets for charts, tables, equations, and code recognition. Experimental results demonstrate that SmolDocling competes with other Vision Language Models that are up to 27 times larger in size, while reducing computational requirements substantially. The model is currently available, datasets will be publicly available soon.', 'score': 11, 'issue_id': 2744, 'pub_date': '2025-03-14', 'pub_date_card': {'ru': '14 марта', 'en': 'March 14', 'zh': '3月14日'}, 'hash': '5548a7c6526f8753', 'authors': ['Ahmed Nassar', 'Andres Marafioti', 'Matteo Omenetti', 'Maksym Lysak', 'Nikolaos Livathinos', 'Christoph Auer', 'Lucas Morin', 'Rafael Teixeira de Lima', 'Yusik Kim', 'A. Said Gurbuz', 'Michele Dolfi', 'Miquel Farré', 'Peter W. J. Staar'], 'affiliations': ['HuggingFace', 'IBM Research'], 'pdf_title_img': 'assets/pdf/title_img/2503.11576.jpg', 'data': {'categories': ['#small_models', '#open_source', '#cv', '#dataset', '#science', '#multimodal'], 'emoji': '📄', 'ru': {'title': 'SmolDocling: компактная модель для комплексной обработки документов', 'desc': 'SmolDocling - это компактная модель обработки документов, сочетающая зрение и язык. Она генерирует универсальную разметку DocTags, захватывающую все элементы страницы с их расположением. Модель показывает надежную производительность в воспроизведении различных особенностей документов, включая листинги кода, таблицы, уравнения и диаграммы. SmolDocling конкурирует с моделями в 27 раз большего размера, существенно снижая вычислительные требования.'}, 'en': {'title': 'SmolDocling: Compact and Powerful Document Conversion', 'desc': 'SmolDocling is a compact vision-language model designed for end-to-end document conversion. It generates DocTags, a universal markup format that captures the content, structure, and spatial location of document elements. Unlike traditional methods that use large models or complex pipelines, SmolDocling achieves high accuracy with only 256 million parameters. It performs well across various document types, including business and academic papers, and introduces new datasets for recognizing charts, tables, equations, and code.'}, 'zh': {'title': 'SmolDocling：高效文档转换的新选择', 'desc': '我们介绍了SmolDocling，这是一种超紧凑的视觉-语言模型，旨在实现端到端的文档转换。该模型通过生成DocTags，一种新的通用标记格式，全面处理整个页面，捕捉所有页面元素的完整上下文和位置。与依赖大型基础模型或多个专用模型的手工管道的现有方法不同，SmolDocling提供了一种端到端的转换，准确捕捉文档元素的内容、结构和空间位置。实验结果表明，SmolDocling在性能上与其他高达27倍大小的视觉语言模型竞争，同时显著降低了计算需求。'}}}, {'id': 'https://huggingface.co/papers/2503.10970', 'title': 'TxAgent: An AI Agent for Therapeutic Reasoning Across a Universe of\n  Tools', 'url': 'https://huggingface.co/papers/2503.10970', 'abstract': 'Precision therapeutics require multimodal adaptive models that generate personalized treatment recommendations. We introduce TxAgent, an AI agent that leverages multi-step reasoning and real-time biomedical knowledge retrieval across a toolbox of 211 tools to analyze drug interactions, contraindications, and patient-specific treatment strategies. TxAgent evaluates how drugs interact at molecular, pharmacokinetic, and clinical levels, identifies contraindications based on patient comorbidities and concurrent medications, and tailors treatment strategies to individual patient characteristics. It retrieves and synthesizes evidence from multiple biomedical sources, assesses interactions between drugs and patient conditions, and refines treatment recommendations through iterative reasoning. It selects tools based on task objectives and executes structured function calls to solve therapeutic tasks that require clinical reasoning and cross-source validation. The ToolUniverse consolidates 211 tools from trusted sources, including all US FDA-approved drugs since 1939 and validated clinical insights from Open Targets. TxAgent outperforms leading LLMs, tool-use models, and reasoning agents across five new benchmarks: DrugPC, BrandPC, GenericPC, TreatmentPC, and DescriptionPC, covering 3,168 drug reasoning tasks and 456 personalized treatment scenarios. It achieves 92.1% accuracy in open-ended drug reasoning tasks, surpassing GPT-4o and outperforming DeepSeek-R1 (671B) in structured multi-step reasoning. TxAgent generalizes across drug name variants and descriptions. By integrating multi-step inference, real-time knowledge grounding, and tool-assisted decision-making, TxAgent ensures that treatment recommendations align with established clinical guidelines and real-world evidence, reducing the risk of adverse events and improving therapeutic decision-making.', 'score': 10, 'issue_id': 2732, 'pub_date': '2025-03-14', 'pub_date_card': {'ru': '14 марта', 'en': 'March 14', 'zh': '3月14日'}, 'hash': 'b7a03e6b34c3c0de', 'authors': ['Shanghua Gao', 'Richard Zhu', 'Zhenglun Kong', 'Ayush Noori', 'Xiaorui Su', 'Curtis Ginder', 'Theodoros Tsiligkaridis', 'Marinka Zitnik'], 'affiliations': ['Broad Institute of MIT and Harvard, Cambridge, MA', 'Cardiovascular Division, Department of Medicine, Brigham and Womens Hospital, Harvard Medical School, Boston, MA', 'Department of Biomedical Informatics, Harvard Medical School, Boston, MA', 'Harvard Data Science Initiative, Cambridge, MA', 'Kempner Institute for the Study of Natural and Artificial Intelligence, Harvard University, Cambridge, MA', 'MIT Lincoln Laboratory, Lexington, MA'], 'pdf_title_img': 'assets/pdf/title_img/2503.10970.jpg', 'data': {'categories': ['#alignment', '#healthcare', '#science', '#agents', '#reasoning', '#benchmark', '#multimodal'], 'emoji': '💊', 'ru': {'title': 'TxAgent: ИИ-помощник для точной и персонализированной фармакотерапии', 'desc': 'TxAgent - это ИИ-агент для персонализированной терапии, использующий многоэтапное рассуждение и извлечение биомедицинских знаний в реальном времени. Он анализирует взаимодействия лекарств, противопоказания и индивидуальные стратегии лечения, используя набор из 211 инструментов. TxAgent превосходит ведущие языковые модели и другие агенты в пяти новых бенчмарках, охватывающих 3168 задач по рассуждению о лекарствах и 456 персонализированных сценариев лечения. Интегрируя многоэтапный вывод, актуальные знания и инструментальное принятие решений, TxAgent обеспечивает соответствие рекомендаций клиническим руководствам и реальным данным.'}, 'en': {'title': 'TxAgent: Personalized Precision Therapeutics through Advanced AI Reasoning', 'desc': 'The paper presents TxAgent, an advanced AI agent designed for precision therapeutics that generates personalized treatment recommendations. It utilizes multi-step reasoning and real-time biomedical knowledge retrieval from a comprehensive toolbox of 211 tools to analyze drug interactions and contraindications. TxAgent evaluates drug interactions at various levels and tailors treatment strategies based on individual patient characteristics, ensuring recommendations are evidence-based. The system outperforms existing models in drug reasoning tasks, achieving high accuracy and improving therapeutic decision-making by integrating clinical guidelines and real-world evidence.'}, 'zh': {'title': '个性化治疗的智能助手TxAgent', 'desc': '精准治疗需要多模态自适应模型来生成个性化的治疗建议。我们介绍了TxAgent，这是一种利用多步推理和实时生物医学知识检索的人工智能代理，能够分析药物相互作用、禁忌症和患者特定的治疗策略。TxAgent在分子、药代动力学和临床层面评估药物相互作用，并根据患者的合并症和同时用药识别禁忌症，量身定制治疗策略。通过整合多步推理、实时知识基础和工具辅助决策，TxAgent确保治疗建议符合既定的临床指南和现实世界证据，从而降低不良事件的风险，改善治疗决策。'}}}, {'id': 'https://huggingface.co/papers/2503.10781', 'title': 'Large-scale Pre-training for Grounded Video Caption Generation', 'url': 'https://huggingface.co/papers/2503.10781', 'abstract': 'We propose a novel approach for captioning and object grounding in video, where the objects in the caption are grounded in the video via temporally dense bounding boxes. We introduce the following contributions. First, we present a large-scale automatic annotation method that aggregates captions grounded with bounding boxes across individual frames into temporally dense and consistent bounding box annotations. We apply this approach on the HowTo100M dataset to construct a large-scale pre-training dataset, named HowToGround1M. We also introduce a Grounded Video Caption Generation model, dubbed GROVE, and pre-train the model on HowToGround1M. Second, we introduce a new dataset, called iGround, of 3500 videos with manually annotated captions and dense spatio-temporally grounded bounding boxes. This allows us to measure progress on this challenging problem, as well as to fine-tune our model on this small-scale but high-quality data. Third, we demonstrate that our approach achieves state-of-the-art results on the proposed iGround dataset compared to a number of baselines, as well as on the VidSTG and ActivityNet-Entities datasets. We perform extensive ablations that demonstrate the importance of pre-training using our automatically annotated HowToGround1M dataset followed by fine-tuning on the manually annotated iGround dataset and validate the key technical contributions of our model.', 'score': 10, 'issue_id': 2737, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 марта', 'en': 'March 13', 'zh': '3月13日'}, 'hash': '842a44eb006952de', 'authors': ['Evangelos Kazakos', 'Cordelia Schmid', 'Josef Sivic'], 'affiliations': ['Czech Institute of Informatics, Robotics and Cybernetics at the Czech Technical University in Prague', 'Inria, Ecole normale superieure, CNRS, PSL Research University'], 'pdf_title_img': 'assets/pdf/title_img/2503.10781.jpg', 'data': {'categories': ['#cv', '#dataset', '#training', '#video', '#data'], 'emoji': '🎥', 'ru': {'title': 'Революция в понимании видео: от автоматической разметки к точной локализации объектов', 'desc': 'Статья представляет новый подход к генерации подписей и локализации объектов в видео с использованием темпорально плотных ограничивающих рамок. Авторы создали большой датасет HowToGround1M для предобучения модели GROVE, а также набор данных iGround с ручной разметкой для точной настройки. Предложенный метод достигает передовых результатов на нескольких наборах данных, включая VidSTG и ActivityNet-Entities. Эксперименты подтверждают важность предобучения на автоматически размеченных данных с последующей точной настройкой на вручную аннотированном наборе.'}, 'en': {'title': 'Grounding Video Captions with Precision', 'desc': 'This paper presents a new method for video captioning and object grounding, where objects mentioned in captions are linked to specific locations in the video using detailed bounding boxes. The authors introduce a large-scale automatic annotation technique that creates consistent bounding box annotations across video frames, resulting in a dataset called HowToGround1M for pre-training. They also develop a model named GROVE, which is trained on this dataset and further fine-tuned on a smaller, high-quality dataset called iGround, containing manually annotated videos. The results show that their approach outperforms existing methods on several benchmark datasets, highlighting the effectiveness of their pre-training and fine-tuning strategy.'}, 'zh': {'title': '视频字幕生成与物体定位的新方法', 'desc': '本文提出了一种新颖的视频字幕生成和物体定位方法，通过时间密集的边界框将字幕中的物体与视频中的内容关联起来。我们介绍了一种大规模自动注释方法，将单帧的边界框注释聚合为时间上密集且一致的边界框注释，并在HowTo100M数据集上构建了一个名为HowToGround1M的大规模预训练数据集。我们还提出了一个名为GROVE的基于视频的字幕生成模型，并在HowToGround1M上进行了预训练。此外，我们创建了一个名为iGround的新数据集，包含3500个视频及其手动注释的字幕和密集的时空边界框，以便于评估模型的进展和进行微调。'}}}, {'id': 'https://huggingface.co/papers/2503.11579', 'title': 'Vamba: Understanding Hour-Long Videos with Hybrid Mamba-Transformers', 'url': 'https://huggingface.co/papers/2503.11579', 'abstract': 'State-of-the-art transformer-based large multimodal models (LMMs) struggle to handle hour-long video inputs due to the quadratic complexity of the causal self-attention operations, leading to high computational costs during training and inference. Existing token compression-based methods reduce the number of video tokens but often incur information loss and remain inefficient for extremely long sequences. In this paper, we explore an orthogonal direction to build a hybrid Mamba-Transformer model (VAMBA) that employs Mamba-2 blocks to encode video tokens with linear complexity. Without any token reduction, VAMBA can encode more than 1024 frames (640times360) on a single GPU, while transformer-based models can only encode 256 frames. On long video input, VAMBA achieves at least 50% reduction in GPU memory usage during training and inference, and nearly doubles the speed per training step compared to transformer-based LMMs. Our experimental results demonstrate that VAMBA improves accuracy by 4.3% on the challenging hour-long video understanding benchmark LVBench over prior efficient video LMMs, and maintains strong performance on a broad spectrum of long and short video understanding tasks.', 'score': 9, 'issue_id': 2744, 'pub_date': '2025-03-14', 'pub_date_card': {'ru': '14 марта', 'en': 'March 14', 'zh': '3月14日'}, 'hash': 'e0a1f364990dbe23', 'authors': ['Weiming Ren', 'Wentao Ma', 'Huan Yang', 'Cong Wei', 'Ge Zhang', 'Wenhu Chen'], 'affiliations': ['1.AI', 'M-A-P', 'University of Toronto', 'University of Waterloo', 'Vector Institute'], 'pdf_title_img': 'assets/pdf/title_img/2503.11579.jpg', 'data': {'categories': ['#optimization', '#long_context', '#benchmark', '#architecture', '#video'], 'emoji': '🎥', 'ru': {'title': 'VAMBA: эффективная обработка длинных видео с линейной сложностью', 'desc': 'Статья представляет новую модель VAMBA, гибрид Mamba и трансформера, для обработки длинных видео. VAMBA использует блоки Mamba-2 для кодирования видеотокенов с линейной сложностью, что позволяет обрабатывать более 1024 кадров на одном GPU. Модель снижает использование памяти GPU на 50% и почти вдвое ускоряет обучение по сравнению с трансформерными моделями. VAMBA показывает улучшение точности на 4.3% на бенчмарке LVBench для понимания часовых видео.'}, 'en': {'title': 'VAMBA: Efficient Video Processing with Linear Complexity', 'desc': 'This paper introduces the Mamba-Transformer model (VAMBA), which addresses the limitations of existing transformer-based large multimodal models (LMMs) in processing long video inputs. Unlike traditional methods that reduce video tokens and often lose information, VAMBA uses Mamba-2 blocks to encode video tokens with linear complexity, allowing it to handle over 1024 frames efficiently. The model significantly reduces GPU memory usage by at least 50% during training and inference, while also increasing training speed nearly twofold compared to standard transformers. Experimental results show that VAMBA not only enhances accuracy on the LVBench benchmark but also performs well across various video understanding tasks.'}, 'zh': {'title': '高效视频理解的新突破：VAMBA模型', 'desc': '本论文提出了一种新的混合Mamba-Transformer模型（VAMBA），旨在解决现有多模态模型在处理长视频输入时的计算复杂性问题。VAMBA使用Mamba-2模块以线性复杂度编码视频标记，避免了信息损失，并且无需减少标记数量。与传统的变换器模型相比，VAMBA在单个GPU上能够编码超过1024帧的视频，显著提高了训练和推理的速度，并减少了GPU内存使用。实验结果表明，VAMBA在长视频理解基准测试中比之前的高效视频模型提高了4.3%的准确率，同时在多种视频理解任务中保持了强大的性能。'}}}, {'id': 'https://huggingface.co/papers/2503.11651', 'title': 'VGGT: Visual Geometry Grounded Transformer', 'url': 'https://huggingface.co/papers/2503.11651', 'abstract': 'We present VGGT, a feed-forward neural network that directly infers all key 3D attributes of a scene, including camera parameters, point maps, depth maps, and 3D point tracks, from one, a few, or hundreds of its views. This approach is a step forward in 3D computer vision, where models have typically been constrained to and specialized for single tasks. It is also simple and efficient, reconstructing images in under one second, and still outperforming alternatives that require post-processing with visual geometry optimization techniques. The network achieves state-of-the-art results in multiple 3D tasks, including camera parameter estimation, multi-view depth estimation, dense point cloud reconstruction, and 3D point tracking. We also show that using pretrained VGGT as a feature backbone significantly enhances downstream tasks, such as non-rigid point tracking and feed-forward novel view synthesis. Code and models are publicly available at https://github.com/facebookresearch/vggt.', 'score': 8, 'issue_id': 2740, 'pub_date': '2025-03-14', 'pub_date_card': {'ru': '14 марта', 'en': 'March 14', 'zh': '3月14日'}, 'hash': '701338c26ac42ac7', 'authors': ['Jianyuan Wang', 'Minghao Chen', 'Nikita Karaev', 'Andrea Vedaldi', 'Christian Rupprecht', 'David Novotny'], 'affiliations': ['Meta AI', 'Visual Geometry Group, University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2503.11651.jpg', 'data': {'categories': ['#cv', '#open_source', '#3d', '#optimization'], 'emoji': '🔮', 'ru': {'title': 'VGGT: Универсальная нейросеть для комплексного 3D-анализа сцен', 'desc': 'VGGT - это нейронная сеть прямого распространения, которая напрямую выводит все ключевые 3D-атрибуты сцены из одного или нескольких её изображений. Модель эффективно реконструирует изображения менее чем за секунду, превосходя альтернативы, требующие постобработки. VGGT достигает передовых результатов в нескольких 3D-задачах, включая оценку параметров камеры и глубины, реконструкцию облака точек и 3D-трекинг. Использование предобученной VGGT в качестве основы значительно улучшает работу в задачах синтеза новых ракурсов и нежесткого трекинга точек.'}, 'en': {'title': 'VGGT: Revolutionizing 3D Scene Understanding with Speed and Efficiency', 'desc': 'VGGT is a feed-forward neural network designed to extract key 3D attributes from various views of a scene, such as camera parameters and depth maps. Unlike traditional models that focus on single tasks, VGGT efficiently handles multiple 3D computer vision tasks simultaneously. It operates quickly, reconstructing images in under one second while achieving superior results compared to methods that rely on post-processing. Additionally, VGGT can be used as a feature backbone to improve performance in related tasks like point tracking and novel view synthesis.'}, 'zh': {'title': 'VGGT：高效的3D场景推断网络', 'desc': '我们提出了VGGT，这是一种前馈神经网络，可以直接推断场景的所有关键3D属性，包括相机参数、点图、深度图和3D点轨迹。该方法在3D计算机视觉领域向前迈出了一步，克服了以往模型仅限于单一任务的局限性。VGGT简单高效，能够在不到一秒的时间内重建图像，并且在多个3D任务中表现优于需要后处理的替代方案。使用预训练的VGGT作为特征骨干显著提升了下游任务的性能，如非刚性点跟踪和前馈新视图合成。'}}}, {'id': 'https://huggingface.co/papers/2503.10632', 'title': 'Kolmogorov-Arnold Attention: Is Learnable Attention Better For Vision\n  Transformers?', 'url': 'https://huggingface.co/papers/2503.10632', 'abstract': "Kolmogorov-Arnold networks (KANs) are a remarkable innovation consisting of learnable activation functions with the potential to capture more complex relationships from data. Although KANs are useful in finding symbolic representations and continual learning of one-dimensional functions, their effectiveness in diverse machine learning (ML) tasks, such as vision, remains questionable. Presently, KANs are deployed by replacing multilayer perceptrons (MLPs) in deep network architectures, including advanced architectures such as vision Transformers (ViTs). In this paper, we are the first to design a general learnable Kolmogorov-Arnold Attention (KArAt) for vanilla ViTs that can operate on any choice of basis. However, the computing and memory costs of training them motivated us to propose a more modular version, and we designed particular learnable attention, called Fourier-KArAt. Fourier-KArAt and its variants either outperform their ViT counterparts or show comparable performance on CIFAR-10, CIFAR-100, and ImageNet-1K datasets. We dissect these architectures' performance and generalization capacity by analyzing their loss landscapes, weight distributions, optimizer path, attention visualization, and spectral behavior, and contrast them with vanilla ViTs. The goal of this paper is not to produce parameter- and compute-efficient attention, but to encourage the community to explore KANs in conjunction with more advanced architectures that require a careful understanding of learnable activations. Our open-source code and implementation details are available on: https://subhajitmaity.me/KArAt", 'score': 8, 'issue_id': 2731, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 марта', 'en': 'March 13', 'zh': '3月13日'}, 'hash': '46504216bbce5b86', 'authors': ['Subhajit Maity', 'Killian Hitsman', 'Xin Li', 'Aritra Dutta'], 'affiliations': ['Department of Computer Science, University of Central Florida, Orlando, FL, USA', 'Department of Mathematics, University of Central Florida, Orlando, FL, USA'], 'pdf_title_img': 'assets/pdf/title_img/2503.10632.jpg', 'data': {'categories': ['#architecture', '#cv', '#optimization', '#open_source', '#training'], 'emoji': '🧠', 'ru': {'title': 'Новый взгляд на внимание: Колмогоров-Арнольд сети в Vision Transformers', 'desc': 'Статья представляет новый подход к архитектуре нейронных сетей - Колмогоров-Арнольд внимание (KArAt) для моделей Vision Transformer. Авторы разработали модульную версию KArAt на основе преобразования Фурье, которая показывает сопоставимую или превосходящую производительность по сравнению с обычными ViT на нескольких наборах данных. В работе проводится глубокий анализ свойств новой архитектуры, включая ландшафт функции потерь, распределение весов и визуализацию внимания. Исследование призывает сообщество изучать сети Колмогорова-Арнольда в сочетании с передовыми архитектурами глубокого обучения.'}, 'en': {'title': 'Unlocking Complex Relationships with Learnable Activations in Vision Transformers', 'desc': 'Kolmogorov-Arnold networks (KANs) introduce learnable activation functions that can model complex data relationships. This paper explores the application of KANs in vision tasks by integrating them into vision Transformers (ViTs) through a novel learnable attention mechanism called Kolmogorov-Arnold Attention (KArAt). The authors also present a more efficient variant, Fourier-KArAt, which shows competitive performance on popular image datasets like CIFAR-10 and ImageNet-1K. The study emphasizes the importance of understanding learnable activations in advanced architectures, rather than solely focusing on efficiency.'}, 'zh': {'title': '探索可学习激活函数的潜力', 'desc': 'Kolmogorov-Arnold网络（KANs）是一种创新的可学习激活函数，能够捕捉数据中的复杂关系。尽管KANs在一维函数的符号表示和持续学习中表现出色，但在视觉等多种机器学习任务中的有效性仍然存在疑问。本文首次为普通的视觉变换器（ViTs）设计了一种通用的可学习Kolmogorov-Arnold注意力（KArAt），并提出了更模块化的Fourier-KArAt版本。实验结果表明，Fourier-KArAt及其变体在CIFAR-10、CIFAR-100和ImageNet-1K数据集上表现优于或与ViT相当。'}}}, {'id': 'https://huggingface.co/papers/2503.06542', 'title': 'ARMOR v0.1: Empowering Autoregressive Multimodal Understanding Model\n  with Interleaved Multimodal Generation via Asymmetric Synergy', 'url': 'https://huggingface.co/papers/2503.06542', 'abstract': 'Unified models (UniMs) for multimodal understanding and generation have recently received much attention in the area of vision and language. Existing UniMs are designed to simultaneously learn both multimodal understanding and generation capabilities, demanding substantial computational resources, and often struggle to generate interleaved text-image. We present ARMOR, a resource-efficient and pure autoregressive framework that achieves both understanding and generation by fine-tuning existing multimodal large language models (MLLMs). Specifically, ARMOR extends existing MLLMs from three perspectives: (1) For model architecture, an asymmetric encoder-decoder architecture with a forward-switching mechanism is introduced to unify embedding space integrating textual and visual modalities for enabling natural text-image interleaved generation with minimal computational overhead. (2) For training data, a meticulously curated, high-quality interleaved dataset is collected for fine-tuning MLLMs. (3) For the training algorithm, we propose a ``what or how to generate" algorithm to empower existing MLLMs with multimodal generation capabilities while preserving their multimodal understanding capabilities, through three progressive training stages based on the collected dataset. Experimental results demonstrate that ARMOR upgrades existing MLLMs to UniMs with promising image generation capabilities, using limited training resources. Our code will be released soon at https://armor.github.io.', 'score': 6, 'issue_id': 2737, 'pub_date': '2025-03-09', 'pub_date_card': {'ru': '9 марта', 'en': 'March 9', 'zh': '3月9日'}, 'hash': '4b19cfc1e459fb2f', 'authors': ['Jianwen Sun', 'Yukang Feng', 'Chuanhao Li', 'Fanrui Zhang', 'Zizhen Li', 'Jiaxin Ai', 'Sizhuo Zhou', 'Yu Dai', 'Shenglin Zhang', 'Kaipeng Zhang'], 'affiliations': ['Nankai University', 'Shanghai AI Laboratory', 'Shanghai Innovation Institute', 'University of Science and Technology of China', 'Wuhan University'], 'pdf_title_img': 'assets/pdf/title_img/2503.06542.jpg', 'data': {'categories': ['#games', '#optimization', '#multimodal', '#dataset', '#training', '#architecture'], 'emoji': '🛡️', 'ru': {'title': 'ARMOR: Эффективное улучшение мультимодальных моделей', 'desc': 'ARMOR - это новый подход к созданию унифицированных моделей для мультимодального понимания и генерации. Он использует асимметричную архитектуру энкодер-декодер с механизмом переключения вперед для объединения текстовых и визуальных модальностей. ARMOR обучается на специально собранном наборе данных с чередующимися текстом и изображениями. Применяется алгоритм обучения в три этапа, позволяющий улучшить возможности генерации мультимодальных больших языковых моделей при сохранении их способностей к пониманию.'}, 'en': {'title': 'ARMOR: Efficient Multimodal Mastery with Minimal Resources', 'desc': 'The paper introduces ARMOR, a new framework designed to enhance multimodal understanding and generation in machine learning models. Unlike existing unified models that require extensive computational resources, ARMOR is resource-efficient and fine-tunes large language models to achieve its goals. It employs an asymmetric encoder-decoder architecture to seamlessly integrate text and image data, allowing for natural interleaved generation. Additionally, ARMOR utilizes a carefully curated dataset and a novel training algorithm to improve the multimodal capabilities of existing models while maintaining their understanding abilities.'}, 'zh': {'title': 'ARMOR：高效的多模态理解与生成框架', 'desc': '本文介绍了一种名为ARMOR的统一模型框架，旨在提高多模态理解和生成的效率。ARMOR通过微调现有的大型多模态语言模型（MLLMs），实现了文本和图像的自然交织生成。该框架采用了不对称的编码-解码架构，并引入了前向切换机制，以减少计算资源的消耗。实验结果表明，ARMOR能够在有限的训练资源下，显著提升现有模型的图像生成能力。'}}}, {'id': 'https://huggingface.co/papers/2503.09279', 'title': 'Cockatiel: Ensembling Synthetic and Human Preferenced Training for\n  Detailed Video Caption', 'url': 'https://huggingface.co/papers/2503.09279', 'abstract': 'Video Detailed Captioning (VDC) is a crucial task for vision-language bridging, enabling fine-grained descriptions of complex video content. In this paper, we first comprehensively benchmark current state-of-the-art approaches and systematically identified two critical limitations: biased capability towards specific captioning aspect and misalignment with human preferences. To address these deficiencies, we propose Cockatiel, a novel three-stage training pipeline that ensembles synthetic and human-aligned training for improving VDC performance. In the first stage, we derive a scorer from a meticulously annotated dataset to select synthetic captions high-performing on certain fine-grained video-caption alignment and human-preferred while disregarding others. Then, we train Cockatiel-13B, using this curated dataset to infuse it with assembled model strengths and human preferences. Finally, we further distill Cockatiel-8B from Cockatiel-13B for the ease of usage. Extensive quantitative and qualitative experiments reflect the effectiveness of our method, as we not only set new state-of-the-art performance on VDCSCORE in a dimension-balanced way but also surpass leading alternatives on human preference by a large margin as depicted by the human evaluation results.', 'score': 5, 'issue_id': 2730, 'pub_date': '2025-03-12', 'pub_date_card': {'ru': '12 марта', 'en': 'March 12', 'zh': '3月12日'}, 'hash': 'edf6712b564fd37a', 'authors': ['Luozheng Qin', 'Zhiyu Tan', 'Mengping Yang', 'Xiaomeng Yang', 'Hao Li'], 'affiliations': ['Fudan University', 'Shanghai Academy of Artificial Intelligence for Science'], 'pdf_title_img': 'assets/pdf/title_img/2503.09279.jpg', 'data': {'categories': ['#benchmark', '#training', '#multimodal', '#synthetic', '#alignment'], 'emoji': '🦜', 'ru': {'title': 'Cockatiel: Новый стандарт в детальном описании видео', 'desc': 'Статья представляет новый подход к детальному описанию видео (VDC) под названием Cockatiel. Авторы разработали трехэтапный процесс обучения, который объединяет синтетические и человеко-ориентированные данные для улучшения производительности. Метод включает отбор высококачественных синтетических подписей, обучение большой модели Cockatiel-13B и ее дистилляцию в меньшую Cockatiel-8B. Эксперименты показали, что Cockatiel превосходит существующие методы по метрике VDCSCORE и человеческим предпочтениям.'}, 'en': {'title': 'Bridging Vision and Language with Cockatiel for Enhanced Video Captioning', 'desc': 'This paper addresses the challenge of Video Detailed Captioning (VDC), which involves creating precise descriptions for complex video content. The authors identify two main issues with existing methods: a bias towards certain aspects of captioning and a lack of alignment with human preferences. To overcome these challenges, they introduce Cockatiel, a three-stage training pipeline that combines synthetic and human-aligned data to enhance VDC performance. Their experiments demonstrate that Cockatiel achieves state-of-the-art results on the VDCSCORE metric and significantly outperforms other methods in terms of human preference evaluations.'}, 'zh': {'title': '提升视频描述的智能化与人性化', 'desc': '视频详细描述（VDC）是连接视觉和语言的重要任务，能够对复杂视频内容进行细致的描述。本文首先对当前最先进的方法进行了全面评估，并系统地识别出两个关键限制：对特定描述方面的偏见能力和与人类偏好的不一致。为了解决这些问题，我们提出了Cockatiel，这是一种新颖的三阶段训练流程，结合了合成和人类对齐的训练，以提高VDC性能。通过大量的定量和定性实验，我们的方法在VDCSCORE上设定了新的最先进性能，并在与人类偏好的比较中大幅超越了领先的替代方案。'}}}, {'id': 'https://huggingface.co/papers/2503.10696', 'title': 'Neighboring Autoregressive Modeling for Efficient Visual Generation', 'url': 'https://huggingface.co/papers/2503.10696', 'abstract': 'Visual autoregressive models typically adhere to a raster-order ``next-token prediction" paradigm, which overlooks the spatial and temporal locality inherent in visual content. Specifically, visual tokens exhibit significantly stronger correlations with their spatially or temporally adjacent tokens compared to those that are distant. In this paper, we propose Neighboring Autoregressive Modeling (NAR), a novel paradigm that formulates autoregressive visual generation as a progressive outpainting procedure, following a near-to-far ``next-neighbor prediction" mechanism. Starting from an initial token, the remaining tokens are decoded in ascending order of their Manhattan distance from the initial token in the spatial-temporal space, progressively expanding the boundary of the decoded region. To enable parallel prediction of multiple adjacent tokens in the spatial-temporal space, we introduce a set of dimension-oriented decoding heads, each predicting the next token along a mutually orthogonal dimension. During inference, all tokens adjacent to the decoded tokens are processed in parallel, substantially reducing the model forward steps for generation. Experiments on ImageNet256times 256 and UCF101 demonstrate that NAR achieves 2.4times and 8.6times higher throughput respectively, while obtaining superior FID/FVD scores for both image and video generation tasks compared to the PAR-4X approach. When evaluating on text-to-image generation benchmark GenEval, NAR with 0.8B parameters outperforms Chameleon-7B while using merely 0.4 of the training data. Code is available at https://github.com/ThisisBillhe/NAR.', 'score': 5, 'issue_id': 2738, 'pub_date': '2025-03-12', 'pub_date_card': {'ru': '12 марта', 'en': 'March 12', 'zh': '3月12日'}, 'hash': '5adda6787d6613db', 'authors': ['Yefei He', 'Yuanyu He', 'Shaoxuan He', 'Feng Chen', 'Hong Zhou', 'Kaipeng Zhang', 'Bohan Zhuang'], 'affiliations': ['Shanghai AI Laboratory, China', 'The University of Adelaide, Australia', 'Zhejiang University, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.10696.jpg', 'data': {'categories': ['#cv', '#video', '#games', '#benchmark', '#optimization', '#architecture', '#training'], 'emoji': '🧩', 'ru': {'title': 'NAR: Революция в авторегрессионном моделировании визуального контента', 'desc': "Статья представляет новый подход к авторегрессионному моделированию визуального контента, названный Neighboring Autoregressive Modeling (NAR). В отличие от традиционных моделей, использующих растровый порядок предсказания, NAR применяет механизм 'предсказания следующего соседа', учитывающий пространственно-временную близость токенов. Модель начинает с одного токена и последовательно декодирует остальные, расширяя границы декодированной области. Для параллельного предсказания соседних токенов используются специальные декодирующие головки, что значительно ускоряет процесс генерации."}, 'en': {'title': 'Revolutionizing Visual Generation with Neighboring Autoregressive Modeling', 'desc': "This paper introduces Neighboring Autoregressive Modeling (NAR), a new approach to visual generation that improves upon traditional autoregressive models by focusing on the spatial and temporal relationships between visual tokens. Instead of predicting the next token in a raster order, NAR uses a 'next-neighbor prediction' method, decoding tokens based on their proximity to an initial token. This allows for parallel processing of adjacent tokens, significantly speeding up the generation process. Experiments show that NAR achieves much higher throughput and better quality scores in image and video generation compared to existing methods, while also being more efficient in terms of training data usage."}, 'zh': {'title': '邻近自回归建模：提升视觉生成效率的创新方法', 'desc': '本文提出了一种新的视觉自回归模型，称为邻近自回归建模（NAR），旨在改善传统的基于光栅顺序的“下一个标记预测”方法。NAR通过近邻预测机制，将自回归视觉生成视为一种逐步扩展的过程，从初始标记开始，按曼哈顿距离逐步解码剩余标记。该方法引入了一组面向维度的解码头，允许在空间-时间空间中并行预测多个相邻标记，从而显著减少生成所需的模型前向步骤。实验结果表明，NAR在图像和视频生成任务中均优于现有方法，显示出更高的吞吐量和更好的生成质量。'}}}, {'id': 'https://huggingface.co/papers/2503.06553', 'title': 'ProJudge: A Multi-Modal Multi-Discipline Benchmark and\n  Instruction-Tuning Dataset for MLLM-based Process Judges', 'url': 'https://huggingface.co/papers/2503.06553', 'abstract': "As multi-modal large language models (MLLMs) frequently exhibit errors when solving scientific problems, evaluating the validity of their reasoning processes is critical for ensuring reliability and uncovering fine-grained model weaknesses. Since human evaluation is laborious and costly, prompting MLLMs as automated process judges has become a common practice. However, the reliability of these model-based judges remains uncertain. To address this, we introduce ProJudgeBench, the first comprehensive benchmark specifically designed for evaluating abilities of MLLM-based process judges. ProJudgeBench comprises 2,400 test cases and 50,118 step-level labels, spanning four scientific disciplines with diverse difficulty levels and multi-modal content. In ProJudgeBench, each step is meticulously annotated by human experts for correctness, error type, and explanation, enabling a systematic evaluation of judges' capabilities to detect, classify and diagnose errors. Evaluation on ProJudgeBench reveals a significant performance gap between open-source and proprietary models. To bridge this gap, we further propose ProJudge-173k, a large-scale instruction-tuning dataset, and a Dynamic Dual-Phase fine-tuning strategy that encourages models to explicitly reason through problem-solving before assessing solutions. Both contributions significantly enhance the process evaluation capabilities of open-source models. All the resources will be released to foster future research of reliable multi-modal process evaluation.", 'score': 5, 'issue_id': 2737, 'pub_date': '2025-03-09', 'pub_date_card': {'ru': '9 марта', 'en': 'March 9', 'zh': '3月9日'}, 'hash': '9546d0d9f897e116', 'authors': ['Jiaxin Ai', 'Pengfei Zhou', 'Zhaopan Xu', 'Ming Li', 'Fanrui Zhang', 'Zizhen Li', 'Jianwen Sun', 'Yukang Feng', 'Baojin Huang', 'Zhongyuan Wang', 'Kaipeng Zhang'], 'affiliations': ['HZAU', 'NKU', 'Shanghai AI Laboratory', 'Shanghai Innovation Institute', 'USTC', 'WHU'], 'pdf_title_img': 'assets/pdf/title_img/2503.06553.jpg', 'data': {'categories': ['#interpretability', '#reasoning', '#multimodal', '#science', '#open_source', '#dataset', '#training', '#benchmark'], 'emoji': '🔬', 'ru': {'title': 'Надежная оценка научных рассуждений мультимодальными ИИ-судьями', 'desc': 'В статье представлен ProJudgeBench - первый комплексный бенчмарк для оценки способностей мультимодальных языковых моделей (MLLM) в роли автоматизированных судей процессов решения научных задач. Бенчмарк включает 2400 тестовых случаев и более 50 000 пошаговых оценок в четырех научных дисциплинах. Авторы также предлагают набор данных ProJudge-173k и стратегию дообучения для улучшения возможностей оценки процессов у открытых моделей. Результаты показывают значительный разрыв в производительности между открытыми и проприетарными моделями в этой задаче.'}, 'en': {'title': 'Enhancing MLLM Reliability with ProJudgeBench', 'desc': 'This paper addresses the challenges faced by multi-modal large language models (MLLMs) in solving scientific problems accurately. It introduces ProJudgeBench, a benchmark designed to evaluate the reasoning abilities of MLLM-based judges through a comprehensive set of test cases and detailed annotations. The study highlights a performance gap between open-source and proprietary models, indicating the need for improvement in the evaluation process. To enhance this, the authors propose ProJudge-173k, a dataset for instruction-tuning and a new fine-tuning strategy that promotes better reasoning in problem-solving.'}, 'zh': {'title': '提升多模态模型的过程评估能力', 'desc': '本文介绍了ProJudgeBench，这是一个专门用于评估多模态大型语言模型（MLLM）过程判断能力的基准。该基准包含2400个测试案例和50118个步骤级标签，涵盖四个科学领域，具有不同的难度和多模态内容。每个步骤都由人类专家仔细注释，以便系统评估模型在检测、分类和诊断错误方面的能力。通过在ProJudgeBench上的评估，发现开源模型与专有模型之间存在显著的性能差距，并提出了ProJudge-173k数据集和动态双阶段微调策略，以提高开源模型的过程评估能力。'}}}, {'id': 'https://huggingface.co/papers/2503.06674', 'title': 'Learning Few-Step Diffusion Models by Trajectory Distribution Matching', 'url': 'https://huggingface.co/papers/2503.06674', 'abstract': "Accelerating diffusion model sampling is crucial for efficient AIGC deployment. While diffusion distillation methods -- based on distribution matching and trajectory matching -- reduce sampling to as few as one step, they fall short on complex tasks like text-to-image generation. Few-step generation offers a better balance between speed and quality, but existing approaches face a persistent trade-off: distribution matching lacks flexibility for multi-step sampling, while trajectory matching often yields suboptimal image quality. To bridge this gap, we propose learning few-step diffusion models by Trajectory Distribution Matching (TDM), a unified distillation paradigm that combines the strengths of distribution and trajectory matching. Our method introduces a data-free score distillation objective, aligning the student's trajectory with the teacher's at the distribution level. Further, we develop a sampling-steps-aware objective that decouples learning targets across different steps, enabling more adjustable sampling. This approach supports both deterministic sampling for superior image quality and flexible multi-step adaptation, achieving state-of-the-art performance with remarkable efficiency. Our model, TDM, outperforms existing methods on various backbones, such as SDXL and PixArt-alpha, delivering superior quality and significantly reduced training costs. In particular, our method distills PixArt-alpha into a 4-step generator that outperforms its teacher on real user preference at 1024 resolution. This is accomplished with 500 iterations and 2 A800 hours -- a mere 0.01% of the teacher's training cost. In addition, our proposed TDM can be extended to accelerate text-to-video diffusion. Notably, TDM can outperform its teacher model (CogVideoX-2B) by using only 4 NFE on VBench, improving the total score from 80.91 to 81.65. Project page: https://tdm-t2x.github.io/", 'score': 4, 'issue_id': 2732, 'pub_date': '2025-03-09', 'pub_date_card': {'ru': '9 марта', 'en': 'March 9', 'zh': '3月9日'}, 'hash': '1ce5d8eb2086abfc', 'authors': ['Yihong Luo', 'Tianyang Hu', 'Jiacheng Sun', 'Yujun Cai', 'Jing Tang'], 'affiliations': ['Huawei Noahs Ark Lab'], 'pdf_title_img': 'assets/pdf/title_img/2503.06674.jpg', 'data': {'categories': ['#diffusion', '#training', '#cv', '#video'], 'emoji': '🚀', 'ru': {'title': 'TDM: Революция в ускорении диффузионных моделей', 'desc': 'Статья представляет новый метод ускорения генерации изображений с помощью диффузионных моделей, называемый Trajectory Distribution Matching (TDM). TDM объединяет преимущества методов сопоставления распределений и траекторий, позволяя создавать высококачественные изображения за меньшее количество шагов. Метод вводит безданную цель дистилляции оценки и цель, учитывающую количество шагов сэмплирования, что обеспечивает гибкость и эффективность. TDM превосходит существующие методы на различных архитектурах, таких как SDXL и PixArt-alpha, значительно сокращая время обучения и улучшая качество генерации.'}, 'en': {'title': 'Efficient Few-Step Diffusion with TDM', 'desc': 'This paper presents a new method called Trajectory Distribution Matching (TDM) to improve the efficiency of diffusion models in generating images and videos. TDM combines the benefits of distribution matching and trajectory matching, allowing for fewer sampling steps while maintaining high image quality. The method introduces a data-free score distillation objective that aligns the learning process of a smaller model with a larger, more complex model. By decoupling learning targets for different sampling steps, TDM achieves state-of-the-art performance with significantly reduced training costs, making it suitable for applications like text-to-image and text-to-video generation.'}, 'zh': {'title': '提升扩散模型采样效率的创新方法', 'desc': '加速扩散模型采样对于高效的AIGC部署至关重要。本文提出了一种新的学习少步扩散模型的方法，称为轨迹分布匹配（TDM），它结合了分布匹配和轨迹匹配的优点。通过引入无数据的分数蒸馏目标，我们的模型能够在不同采样步骤之间解耦学习目标，从而实现更灵活的采样。实验结果表明，TDM在多个基准上超越了现有方法，显著提高了图像质量并降低了训练成本。'}}}, {'id': 'https://huggingface.co/papers/2503.10624', 'title': 'ETCH: Generalizing Body Fitting to Clothed Humans via Equivariant\n  Tightness', 'url': 'https://huggingface.co/papers/2503.10624', 'abstract': 'Fitting a body to a 3D clothed human point cloud is a common yet challenging task. Traditional optimization-based approaches use multi-stage pipelines that are sensitive to pose initialization, while recent learning-based methods often struggle with generalization across diverse poses and garment types. We propose Equivariant Tightness Fitting for Clothed Humans, or ETCH, a novel pipeline that estimates cloth-to-body surface mapping through locally approximate SE(3) equivariance, encoding tightness as displacement vectors from the cloth surface to the underlying body. Following this mapping, pose-invariant body features regress sparse body markers, simplifying clothed human fitting into an inner-body marker fitting task. Extensive experiments on CAPE and 4D-Dress show that ETCH significantly outperforms state-of-the-art methods -- both tightness-agnostic and tightness-aware -- in body fitting accuracy on loose clothing (16.7% ~ 69.5%) and shape accuracy (average 49.9%). Our equivariant tightness design can even reduce directional errors by (67.2% ~ 89.8%) in one-shot (or out-of-distribution) settings. Qualitative results demonstrate strong generalization of ETCH, regardless of challenging poses, unseen shapes, loose clothing, and non-rigid dynamics. We will release the code and models soon for research purposes at https://boqian-li.github.io/ETCH/.', 'score': 3, 'issue_id': 2738, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 марта', 'en': 'March 13', 'zh': '3月13日'}, 'hash': 'c0aca4cafef8e621', 'authors': ['Boqian Li', 'Haiwen Feng', 'Zeyu Cai', 'Michael J. Black', 'Yuliang Xiu'], 'affiliations': ['Berkeley AI Research (BAIR)', 'Max Planck Institute for Intelligent Systems', 'Westlake University'], 'pdf_title_img': 'assets/pdf/title_img/2503.10624.jpg', 'data': {'categories': ['#3d', '#optimization', '#cv', '#open_source'], 'emoji': '👕', 'ru': {'title': 'Точная подгонка 3D-модели тела к одетому человеку с учетом плотности прилегания одежды', 'desc': 'Статья представляет новый метод ETCH для подгонки трехмерной модели тела к облаку точек одетого человека. ETCH использует локально приближенную SE(3)-эквивариантность для оценки соответствия между поверхностью одежды и телом, кодируя плотность прилегания как векторы смещения. Метод регрессирует разреженные маркеры тела с помощью инвариантных к позе признаков, упрощая задачу подгонки тела. Эксперименты показывают, что ETCH значительно превосходит современные методы по точности подгонки тела и формы, особенно для свободной одежды.'}, 'en': {'title': 'Revolutionizing Body Fitting with Equivariant Tightness!', 'desc': 'The paper introduces Equivariant Tightness Fitting for Clothed Humans (ETCH), a new method for fitting a body model to a 3D point cloud of a clothed human. ETCH improves upon traditional optimization methods by using a pipeline that leverages SE(3) equivariance to accurately map cloth surfaces to body shapes. This approach simplifies the fitting process by focusing on pose-invariant body features and regressing sparse body markers. Experimental results show that ETCH significantly enhances fitting accuracy for various clothing types and poses, outperforming existing methods in both tightness and shape accuracy.'}, 'zh': {'title': '等变紧致拟合：提升3D穿衣人类拟合精度的创新方法', 'desc': '本论文提出了一种新的方法，称为等变紧致拟合（ETCH），用于将身体与3D穿衣人类点云相匹配。传统的方法依赖于多阶段优化，容易受到姿势初始化的影响，而学习型方法在不同姿势和服装类型的泛化能力上存在困难。ETCH通过局部近似的SE(3)等变性来估计布料与身体表面的映射，并将紧致度编码为从布料表面到身体的位移向量。实验结果表明，ETCH在松散衣物的身体拟合精度和形状精度上显著优于现有的最先进方法，展示了其强大的泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2503.11207', 'title': 'Can Large Reasoning Models do Analogical Reasoning under Perceptual\n  Uncertainty?', 'url': 'https://huggingface.co/papers/2503.11207', 'abstract': "This work presents a first evaluation of two state-of-the-art Large Reasoning Models (LRMs), OpenAI's o3-mini and DeepSeek R1, on analogical reasoning, focusing on well-established nonverbal human IQ tests based on Raven's progressive matrices. We benchmark with the I-RAVEN dataset and its more difficult extension, I-RAVEN-X, which tests the ability to generalize to longer reasoning rules and ranges of the attribute values. To assess the influence of visual uncertainties on these nonverbal analogical reasoning tests, we extend the I-RAVEN-X dataset, which otherwise assumes an oracle perception. We adopt a two-fold strategy to simulate this imperfect visual perception: 1) we introduce confounding attributes which, being sampled at random, do not contribute to the prediction of the correct answer of the puzzles and 2) smoothen the distributions of the input attributes' values. We observe a sharp decline in OpenAI's o3-mini task accuracy, dropping from 86.6% on the original I-RAVEN to just 17.0% -- approaching random chance -- on the more challenging I-RAVEN-X, which increases input length and range and emulates perceptual uncertainty. This drop occurred despite spending 3.4x more reasoning tokens. A similar trend is also observed for DeepSeek R1: from 80.6% to 23.2%. On the other hand, a neuro-symbolic probabilistic abductive model, ARLC, that achieves state-of-the-art performances on I-RAVEN, can robustly reason under all these out-of-distribution tests, maintaining strong accuracy with only a modest reduction from 98.6% to 88.0%. Our code is available at https://github.com/IBM/raven-large-language-models.", 'score': 2, 'issue_id': 2744, 'pub_date': '2025-03-14', 'pub_date_card': {'ru': '14 марта', 'en': 'March 14', 'zh': '3月14日'}, 'hash': '1910487e8b409ffb', 'authors': ['Giacomo Camposampiero', 'Michael Hersche', 'Roger Wattenhofer', 'Abu Sebastian', 'Abbas Rahimi'], 'affiliations': ['ETH Zürich', 'IBM Research - Zurich'], 'pdf_title_img': 'assets/pdf/title_img/2503.11207.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#cv', '#dataset'], 'emoji': '🧠', 'ru': {'title': 'Крупные модели рассуждений уступают нейро-символическим подходам в аналогическом мышлении', 'desc': 'Эта работа представляет первую оценку двух современных моделей крупномасштабного рассуждения (LRM) - o3-mini от OpenAI и DeepSeek R1 - в задачах аналогического мышления на основе невербальных тестов IQ, использующих прогрессивные матрицы Равена. Исследователи провели бенчмаркинг на наборах данных I-RAVEN и его более сложном расширении I-RAVEN-X, которые проверяют способность обобщать более длинные правила рассуждения и диапазоны значений атрибутов. Для оценки влияния визуальной неопределенности авторы расширили набор данных I-RAVEN-X, введя случайные атрибуты и сглаживание распределений значений входных атрибутов. Результаты показали значительное снижение точности моделей LRM на более сложных тестах, в то время как нейро-символическая вероятностная модель ARLC продемонстрировала устойчивость к этим изменениям.'}, 'en': {'title': 'Evaluating Reasoning Under Uncertainty in Large Models', 'desc': "This paper evaluates two advanced Large Reasoning Models (LRMs), OpenAI's o3-mini and DeepSeek R1, on their ability to perform analogical reasoning using Raven's progressive matrices. The study uses the I-RAVEN dataset and its more challenging version, I-RAVEN-X, to test the models' generalization capabilities under visual uncertainties. The results show a significant drop in accuracy for both LRMs when faced with the more complex tasks, indicating their struggle with longer reasoning rules and perceptual noise. In contrast, a neuro-symbolic model, ARLC, demonstrates robust performance, maintaining high accuracy even under challenging conditions."}, 'zh': {'title': '大型推理模型在类比推理中的挑战与机遇', 'desc': '本文首次评估了两种先进的大型推理模型（LRMs），OpenAI的o3-mini和DeepSeek R1，在类比推理方面的表现，重点关注基于Raven渐进矩阵的非语言人类智商测试。我们使用I-RAVEN数据集及其更难的扩展版本I-RAVEN-X进行基准测试，后者测试模型对更长推理规则和属性值范围的泛化能力。为了评估视觉不确定性对这些非语言类比推理测试的影响，我们扩展了I-RAVEN-X数据集，并采用了两种策略来模拟不完美的视觉感知。结果显示，OpenAI的o3-mini在I-RAVEN-X上的任务准确率大幅下降，从86.6%降至仅17.0%，而ARLC模型在所有这些分布外测试中保持了强大的准确性。'}}}, {'id': 'https://huggingface.co/papers/2503.10620', 'title': 'From TOWER to SPIRE: Adding the Speech Modality to a Text-Only LLM', 'url': 'https://huggingface.co/papers/2503.10620', 'abstract': "Large language models (LLMs) have shown remarkable performance and generalization capabilities across multiple languages and tasks, making them very attractive targets for multi-modality integration (e.g., images or speech). In this work, we extend an existing LLM to the speech modality via speech discretization and continued pre-training. In particular, we are interested in multilingual LLMs, such as TOWER, as their pre-training setting allows us to treat discretized speech input as an additional translation language. The resulting open-source model, SPIRE, is able to transcribe and translate English speech input while maintaining TOWER's original performance on translation-related tasks, showcasing that discretized speech input integration as an additional language is feasible during LLM adaptation. We make our code and models available to the community.", 'score': 2, 'issue_id': 2739, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 марта', 'en': 'March 13', 'zh': '3月13日'}, 'hash': '6fc0a5f12205a8bd', 'authors': ['Kshitij Ambilduke', 'Ben Peters', 'Sonal Sannigrahi', 'Anil Keshwani', 'Tsz Kin Lam', 'Bruno Martins', 'Marcely Zanon Boito', 'André F. T. Martins'], 'affiliations': ['ELLIS Unit Lisbon', 'INESC-ID', 'Instituto Superior Técnico, Universidade de Lisboa', 'Instituto de Telecomunicações', 'NAVER LABS Europe', 'Paris-Saclay University', 'Sapienza University of Rome', 'Unbabel', 'University of Edinburgh'], 'pdf_title_img': 'assets/pdf/title_img/2503.10620.jpg', 'data': {'categories': ['#multilingual', '#machine_translation', '#open_source', '#multimodal', '#low_resource'], 'emoji': '🗣️', 'ru': {'title': 'Расширение языковых моделей на речевую модальность', 'desc': 'В этой статье описывается расширение возможностей большой языковой модели (LLM) для работы с речью путем дискретизации речевого сигнала и дополнительного предобучения. Авторы интегрируют дискретизированный речевой ввод как дополнительный язык в многоязычную модель TOWER. Полученная модель SPIRE способна транскрибировать и переводить английскую речь, сохраняя при этом исходную производительность TOWER в задачах перевода. Исследование демонстрирует возможность интеграции дискретизированного речевого ввода как дополнительного языка при адаптации LLM.'}, 'en': {'title': 'Integrating Speech into Multilingual LLMs for Enhanced Performance', 'desc': 'This paper discusses the enhancement of large language models (LLMs) by integrating speech as a new modality. The authors focus on multilingual LLMs, specifically TOWER, and propose a method to convert speech into a format that the model can understand. They introduce a new model called SPIRE, which can transcribe and translate English speech while preserving the original capabilities of TOWER. The research demonstrates that incorporating discretized speech as an additional language is a viable approach for adapting LLMs, and the authors provide their code and models for public use.'}, 'zh': {'title': '将语音融入大型语言模型的创新之路', 'desc': '大型语言模型（LLMs）在多种语言和任务中表现出色，具有很强的泛化能力，适合与多模态（如图像或语音）结合。本文将现有的LLM扩展到语音模态，通过语音离散化和持续预训练来实现。我们特别关注多语言LLM，例如TOWER，因为它的预训练设置允许我们将离散化的语音输入视为额外的翻译语言。最终生成的开源模型SPIRE能够转录和翻译英语语音输入，同时保持TOWER在翻译相关任务上的原始性能，证明了在LLM适应过程中将离散语音输入作为额外语言的可行性。'}}}, {'id': 'https://huggingface.co/papers/2503.10684', 'title': 'Open-World Skill Discovery from Unsegmented Demonstrations', 'url': 'https://huggingface.co/papers/2503.10684', 'abstract': 'Learning skills in open-world environments is essential for developing agents capable of handling a variety of tasks by combining basic skills. Online demonstration videos are typically long but unsegmented, making them difficult to segment and label with skill identifiers. Unlike existing methods that rely on sequence sampling or human labeling, we have developed a self-supervised learning-based approach to segment these long videos into a series of semantic-aware and skill-consistent segments. Drawing inspiration from human cognitive event segmentation theory, we introduce Skill Boundary Detection (SBD), an annotation-free temporal video segmentation algorithm. SBD detects skill boundaries in a video by leveraging prediction errors from a pretrained unconditional action-prediction model. This approach is based on the assumption that a significant increase in prediction error indicates a shift in the skill being executed. We evaluated our method in Minecraft, a rich open-world simulator with extensive gameplay videos available online. Our SBD-generated segments improved the average performance of conditioned policies by 63.7% and 52.1% on short-term atomic skill tasks, and their corresponding hierarchical agents by 11.3% and 20.8% on long-horizon tasks. Our method can leverage the diverse YouTube videos to train instruction-following agents. The project page can be found in https://craftjarvis.github.io/SkillDiscovery.', 'score': 2, 'issue_id': 2740, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 марта', 'en': 'March 11', 'zh': '3月11日'}, 'hash': '004932028027606e', 'authors': ['Jingwen Deng', 'Zihao Wang', 'Shaofei Cai', 'Anji Liu', 'Yitao Liang'], 'affiliations': ['Peking University', 'University of California, Los Angeles'], 'pdf_title_img': 'assets/pdf/title_img/2503.10684.jpg', 'data': {'categories': ['#games', '#video', '#agents', '#open_source'], 'emoji': '🎮', 'ru': {'title': 'Автоматическая сегментация видео для обучения ИИ-агентов сложным навыкам', 'desc': 'Статья представляет новый метод самообучения для сегментации длинных видео на последовательности семантически связанных навыков. Авторы разработали алгоритм Skill Boundary Detection (SBD), который обнаруживает границы навыков в видео, используя ошибки предсказания предобученной модели. Метод был протестирован на видео игрового процесса Minecraft и показал значительное улучшение производительности обусловленных политик и иерархических агентов. Данный подход позволяет использовать разнообразные видео с YouTube для обучения агентов, выполняющих инструкции.'}, 'en': {'title': 'Segmenting Skills for Smarter Agents', 'desc': 'This paper presents a novel approach for segmenting long, unstructured demonstration videos into meaningful skill segments using self-supervised learning. The method, called Skill Boundary Detection (SBD), identifies transitions between skills by analyzing prediction errors from a pretrained action-prediction model. By applying this technique, the authors demonstrate significant improvements in the performance of agents trained on these segmented skills in the Minecraft environment. This approach allows for the effective use of diverse online video content to enhance the training of instruction-following agents without the need for manual labeling.'}, 'zh': {'title': '自监督学习助力技能边界检测', 'desc': '在开放世界环境中学习技能对于开发能够处理多种任务的智能体至关重要。我们提出了一种基于自监督学习的方法，可以将长视频分割成一系列语义明确且技能一致的片段，而无需人工标注。该方法通过检测预测误差来识别技能边界，假设预测误差的显著增加表明技能的转变。我们的实验表明，这种方法在Minecraft中显著提高了条件策略和层次代理的表现。'}}}, {'id': 'https://huggingface.co/papers/2503.05689', 'title': 'GoalFlow: Goal-Driven Flow Matching for Multimodal Trajectories\n  Generation in End-to-End Autonomous Driving', 'url': 'https://huggingface.co/papers/2503.05689', 'abstract': 'We propose GoalFlow, an end-to-end autonomous driving method for generating high-quality multimodal trajectories. In autonomous driving scenarios, there is rarely a single suitable trajectory. Recent methods have increasingly focused on modeling multimodal trajectory distributions. However, they suffer from trajectory selection complexity and reduced trajectory quality due to high trajectory divergence and inconsistencies between guidance and scene information. To address these issues, we introduce GoalFlow, a novel method that effectively constrains the generative process to produce high-quality, multimodal trajectories. To resolve the trajectory divergence problem inherent in diffusion-based methods, GoalFlow constrains the generated trajectories by introducing a goal point. GoalFlow establishes a novel scoring mechanism that selects the most appropriate goal point from the candidate points based on scene information. Furthermore, GoalFlow employs an efficient generative method, Flow Matching, to generate multimodal trajectories, and incorporates a refined scoring mechanism to select the optimal trajectory from the candidates. Our experimental results, validated on the NavsimDauner2024_navsim, demonstrate that GoalFlow achieves state-of-the-art performance, delivering robust multimodal trajectories for autonomous driving. GoalFlow achieved PDMS of 90.3, significantly surpassing other methods. Compared with other diffusion-policy-based methods, our approach requires only a single denoising step to obtain excellent performance. The code is available at https://github.com/YvanYin/GoalFlow.', 'score': 2, 'issue_id': 2731, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 марта', 'en': 'March 7', 'zh': '3月7日'}, 'hash': 'eef61e2f2b4c0760', 'authors': ['Zebin Xing', 'Xingyu Zhang', 'Yang Hu', 'Bo Jiang', 'Tong He', 'Qian Zhang', 'Xiaoxiao Long', 'Wei Yin'], 'affiliations': ['Horizon Robotics', 'Huazhong University of Science & Technology', 'Nanjing University', 'School of Artificial Intelligence, University of Chinese Academy of Sciences', 'Shanghai AI Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2503.05689.jpg', 'data': {'categories': ['#optimization', '#diffusion', '#agents', '#multimodal'], 'emoji': '🚗', 'ru': {'title': 'GoalFlow: Точное планирование траекторий для автономного вождения', 'desc': 'GoalFlow - это новый метод автономного вождения для генерации высококачественных мультимодальных траекторий. Он решает проблему расхождения траекторий, вводя целевую точку и механизм оценки для выбора наиболее подходящей. GoalFlow использует эффективный генеративный метод Flow Matching для создания мультимодальных траекторий. Экспериментальные результаты показывают, что GoalFlow достигает передового уровня производительности, обеспечивая надежные мультимодальные траектории для автономного вождения.'}, 'en': {'title': 'GoalFlow: Driving the Future with High-Quality Multimodal Trajectories', 'desc': 'GoalFlow is a new method for generating high-quality multimodal trajectories in autonomous driving. It addresses the challenges of trajectory selection and quality by constraining the generative process with a goal point, which helps reduce trajectory divergence. The method uses a novel scoring mechanism to choose the best goal point based on the scene context, ensuring that the generated trajectories are relevant and effective. Experimental results show that GoalFlow outperforms existing methods, achieving state-of-the-art performance with fewer computational steps.'}, 'zh': {'title': 'GoalFlow：高质量多模态轨迹生成的创新方法', 'desc': '我们提出了GoalFlow，这是一种端到端的自动驾驶方法，用于生成高质量的多模态轨迹。在自动驾驶场景中，通常没有单一合适的轨迹，最近的方法越来越关注多模态轨迹分布的建模。为了克服轨迹选择的复杂性和轨迹质量下降的问题，GoalFlow通过引入目标点来有效约束生成过程，从而生成高质量的多模态轨迹。我们的实验结果表明，GoalFlow在NavsimDauner2024_navsim上实现了最先进的性能，提供了稳健的多模态轨迹。'}}}, {'id': 'https://huggingface.co/papers/2503.11629', 'title': 'TreeMeshGPT: Artistic Mesh Generation with Autoregressive Tree\n  Sequencing', 'url': 'https://huggingface.co/papers/2503.11629', 'abstract': 'We introduce TreeMeshGPT, an autoregressive Transformer designed to generate high-quality artistic meshes aligned with input point clouds. Instead of the conventional next-token prediction in autoregressive Transformer, we propose a novel Autoregressive Tree Sequencing where the next input token is retrieved from a dynamically growing tree structure that is built upon the triangle adjacency of faces within the mesh. Our sequencing enables the mesh to extend locally from the last generated triangular face at each step, and therefore reduces training difficulty and improves mesh quality. Our approach represents each triangular face with two tokens, achieving a compression rate of approximately 22% compared to the naive face tokenization. This efficient tokenization enables our model to generate highly detailed artistic meshes with strong point cloud conditioning, surpassing previous methods in both capacity and fidelity. Furthermore, our method generates mesh with strong normal orientation constraints, minimizing flipped normals commonly encountered in previous methods. Our experiments show that TreeMeshGPT enhances the mesh generation quality with refined details and normal orientation consistency.', 'score': 1, 'issue_id': 2744, 'pub_date': '2025-03-14', 'pub_date_card': {'ru': '14 марта', 'en': 'March 14', 'zh': '3月14日'}, 'hash': '5f56461a339a86bb', 'authors': ['Stefan Lionar', 'Jiabin Liang', 'Gim Hee Lee'], 'affiliations': ['Garena', 'National University of Singapore', 'Sea AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2503.11629.jpg', 'data': {'categories': ['#games', '#3d', '#optimization', '#architecture'], 'emoji': '🌳', 'ru': {'title': 'Генерация высококачественных 3D-сеток с помощью древовидной последовательности', 'desc': 'TreeMeshGPT - это авторегрессивный трансформер для генерации высококачественных художественных сеток, согласованных с входными облаками точек. Модель использует новый метод автоматической древовидной последовательности, где следующий токен ввода извлекается из динамически растущей древовидной структуры, построенной на основе смежности треугольников в сетке. Этот подход позволяет сетке локально расширяться от последней сгенерированной треугольной грани на каждом шаге, что снижает сложность обучения и улучшает качество сетки. TreeMeshGPT превосходит предыдущие методы по емкости и точности, генерируя детализированные сетки с сильным ограничением нормалей.'}, 'en': {'title': 'Revolutionizing Mesh Generation with TreeMeshGPT', 'desc': 'TreeMeshGPT is a novel autoregressive Transformer model that generates artistic meshes from point clouds. It introduces a unique Autoregressive Tree Sequencing method, which builds a dynamic tree structure based on the adjacency of triangular faces, allowing for local mesh extension during generation. This approach not only simplifies the training process but also enhances the quality of the generated meshes by achieving a 22% compression rate through efficient tokenization of triangular faces. Additionally, TreeMeshGPT ensures better normal orientation, reducing the occurrence of flipped normals and improving overall mesh fidelity and detail compared to previous techniques.'}, 'zh': {'title': 'TreeMeshGPT：高质量艺术网格生成的新方法', 'desc': '我们介绍了TreeMeshGPT，这是一种自回归Transformer，旨在生成与输入点云对齐的高质量艺术网格。与传统的自回归Transformer的下一个标记预测不同，我们提出了一种新颖的自回归树序列化方法，通过动态增长的树结构来检索下一个输入标记。我们的序列化方法使得网格能够在每一步从最后生成的三角面局部扩展，从而降低训练难度并提高网格质量。此外，我们的模型通过将每个三角面表示为两个标记，实现了约22%的压缩率，生成的网格在细节和法线方向一致性方面优于以往方法。'}}}, {'id': 'https://huggingface.co/papers/2503.08111', 'title': 'MaRI: Material Retrieval Integration across Domains', 'url': 'https://huggingface.co/papers/2503.08111', 'abstract': 'Accurate material retrieval is critical for creating realistic 3D assets. Existing methods rely on datasets that capture shape-invariant and lighting-varied representations of materials, which are scarce and face challenges due to limited diversity and inadequate real-world generalization. Most current approaches adopt traditional image search techniques. They fall short in capturing the unique properties of material spaces, leading to suboptimal performance in retrieval tasks. Addressing these challenges, we introduce MaRI, a framework designed to bridge the feature space gap between synthetic and real-world materials. MaRI constructs a shared embedding space that harmonizes visual and material attributes through a contrastive learning strategy by jointly training an image and a material encoder, bringing similar materials and images closer while separating dissimilar pairs within the feature space. To support this, we construct a comprehensive dataset comprising high-quality synthetic materials rendered with controlled shape variations and diverse lighting conditions, along with real-world materials processed and standardized using material transfer techniques. Extensive experiments demonstrate the superior performance, accuracy, and generalization capabilities of MaRI across diverse and complex material retrieval tasks, outperforming existing methods.', 'score': 1, 'issue_id': 2738, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 марта', 'en': 'March 11', 'zh': '3月11日'}, 'hash': 'f3b97bb0031c6d57', 'authors': ['Jianhui Wang', 'Zhifei Yang', 'Yangfan He', 'Huixiong Zhang', 'Yuxuan Chen', 'Jingwei Huang'], 'affiliations': ['Fudan University', 'Peking University', 'Tencent Hunyuan3D', 'University of Electronic Science and Technology of China', 'University of Minnesota'], 'pdf_title_img': 'assets/pdf/title_img/2503.08111.jpg', 'data': {'categories': ['#synthetic', '#optimization', '#dataset', '#cv', '#3d'], 'emoji': '🔍', 'ru': {'title': 'MaRI: Революция в поиске 3D-материалов', 'desc': 'Статья представляет MaRI - фреймворк для улучшения поиска реалистичных 3D-материалов. Он создает общее пространство признаков для синтетических и реальных материалов с помощью контрастивного обучения. Авторы также собрали набор данных из высококачественных синтетических материалов и обработанных реальных материалов. Эксперименты показывают, что MaRI превосходит существующие методы в задачах поиска материалов.'}, 'en': {'title': 'Bridging the Gap in Material Retrieval with MaRI', 'desc': 'This paper presents MaRI, a novel framework aimed at improving material retrieval for realistic 3D asset creation. It addresses the limitations of existing methods that rely on traditional image search techniques, which often fail to capture the unique properties of materials. By utilizing a contrastive learning strategy, MaRI creates a shared embedding space that aligns visual and material attributes, enhancing the retrieval process. The framework is supported by a comprehensive dataset of synthetic and real-world materials, demonstrating superior performance in accuracy and generalization across various retrieval tasks.'}, 'zh': {'title': 'MaRI：提升材料检索的智能框架', 'desc': '准确的材料检索对于创建真实的3D资产至关重要。现有方法依赖于捕捉形状不变和光照变化的材料表示的数据集，这些数据集稀缺且面临多样性不足和现实世界泛化不良的挑战。我们提出了MaRI框架，旨在弥合合成材料和真实材料之间的特征空间差距。通过对比学习策略，MaRI构建了一个共享的嵌入空间，使得相似的材料和图像在特征空间中更接近，同时将不相似的对分开，从而提高了材料检索的性能和准确性。'}}}, {'id': 'https://huggingface.co/papers/2503.09330', 'title': 'Group-robust Machine Unlearning', 'url': 'https://huggingface.co/papers/2503.09330', 'abstract': 'Machine unlearning is an emerging paradigm to remove the influence of specific training data (i.e., the forget set) from a model while preserving its knowledge of the rest of the data (i.e., the retain set). Previous approaches assume the forget data to be uniformly distributed from all training datapoints. However, if the data to unlearn is dominant in one group, we empirically show that performance for this group degrades, leading to fairness issues. This work tackles the overlooked problem of non-uniformly distributed forget sets, which we call group-robust machine unlearning, by presenting a simple, effective strategy that mitigates the performance loss in dominant groups via sample distribution reweighting. Moreover, we present MIU (Mutual Information-aware Machine Unlearning), the first approach for group robustness in approximate machine unlearning. MIU minimizes the mutual information between model features and group information, achieving unlearning while reducing performance degradation in the dominant group of the forget set. Additionally, MIU exploits sample distribution reweighting and mutual information calibration with the original model to preserve group robustness. We conduct experiments on three datasets and show that MIU outperforms standard methods, achieving unlearning without compromising model robustness. Source code available at https://github.com/tdemin16/group-robust_machine_unlearning.', 'score': 0, 'issue_id': 2739, 'pub_date': '2025-03-12', 'pub_date_card': {'ru': '12 марта', 'en': 'March 12', 'zh': '3月12日'}, 'hash': '64a6c82be0db725d', 'authors': ['Thomas De Min', 'Subhankar Roy', 'Stéphane Lathuilière', 'Elisa Ricci', 'Massimiliano Mancini'], 'affiliations': ['Fondazione Bruno Kessler', 'Inria Grenoble, Univ. Grenoble Alpes', 'LTCI, Telecom Paris, Institut Polytechnique de Paris', 'University of Trento'], 'pdf_title_img': 'assets/pdf/title_img/2503.09330.jpg', 'data': {'categories': ['#training', '#dataset', '#ethics', '#data'], 'emoji': '🧠', 'ru': {'title': 'Групповое разобучение без потери устойчивости', 'desc': 'Статья посвящена проблеме группового машинного разобучения, когда данные для удаления из модели неравномерно распределены между группами. Авторы предлагают метод MIU, который минимизирует взаимную информацию между признаками модели и информацией о группах. MIU использует перевзвешивание распределения выборки и калибровку взаимной информации с исходной моделью для сохранения групповой устойчивости. Эксперименты на трех наборах данных показывают, что MIU превосходит стандартные методы, достигая разобучения без ущерба для устойчивости модели.'}, 'en': {'title': 'Fair and Effective Machine Unlearning for Diverse Data Groups', 'desc': 'This paper introduces the concept of group-robust machine unlearning, which addresses the challenge of removing specific training data from a model while maintaining its performance across different groups. It highlights the issue of fairness when the data to be unlearned is not uniformly distributed, leading to performance degradation in dominant groups. The authors propose a novel method called MIU (Mutual Information-aware Machine Unlearning) that minimizes the mutual information between model features and group information, thus enhancing unlearning effectiveness. Through experiments on various datasets, MIU demonstrates superior performance compared to traditional methods, ensuring robust model performance even after unlearning.'}, 'zh': {'title': '组鲁棒性机器遗忘：公平性与性能的平衡', 'desc': '机器遗忘是一种新兴的范式，旨在从模型中去除特定训练数据的影响，同时保留其对其他数据的知识。以往的方法假设遗忘数据均匀分布，但如果要遗忘的数据在某一组中占主导地位，模型在该组的性能会下降，导致公平性问题。本文提出了一种简单有效的策略，通过样本分布重加权来缓解主导组的性能损失，解决了非均匀分布遗忘集的问题。我们还提出了MIU（互信息感知机器遗忘），这是首个针对近似机器遗忘的组鲁棒性方法，能够在减少主导组性能下降的同时实现遗忘。'}}}, {'id': 'https://huggingface.co/papers/2503.09566', 'title': 'TPDiff: Temporal Pyramid Video Diffusion Model', 'url': 'https://huggingface.co/papers/2503.09566', 'abstract': 'The development of video diffusion models unveils a significant challenge: the substantial computational demands. To mitigate this challenge, we note that the reverse process of diffusion exhibits an inherent entropy-reducing nature. Given the inter-frame redundancy in video modality, maintaining full frame rates in high-entropy stages is unnecessary. Based on this insight, we propose TPDiff, a unified framework to enhance training and inference efficiency. By dividing diffusion into several stages, our framework progressively increases frame rate along the diffusion process with only the last stage operating on full frame rate, thereby optimizing computational efficiency. To train the multi-stage diffusion model, we introduce a dedicated training framework: stage-wise diffusion. By solving the partitioned probability flow ordinary differential equations (ODE) of diffusion under aligned data and noise, our training strategy is applicable to various diffusion forms and further enhances training efficiency. Comprehensive experimental evaluations validate the generality of our method, demonstrating 50% reduction in training cost and 1.5x improvement in inference efficiency.', 'score': 32, 'issue_id': 2681, 'pub_date': '2025-03-12', 'pub_date_card': {'ru': '12 марта', 'en': 'March 12', 'zh': '3月12日'}, 'hash': '6952e94de20936ce', 'authors': ['Lingmin Ran', 'Mike Zheng Shou'], 'affiliations': ['Show Lab, National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2503.09566.jpg', 'data': {'categories': ['#inference', '#video', '#diffusion', '#training', '#optimization'], 'emoji': '🎬', 'ru': {'title': 'Ускорение видео-диффузии: эффективность через поэтапность', 'desc': 'Статья представляет TPDiff - унифицированную систему для повышения эффективности обучения и вывода видео-диффузионных моделей. Авторы предлагают разделить процесс диффузии на несколько этапов, постепенно увеличивая частоту кадров, что оптимизирует вычислительные ресурсы. Для обучения многоэтапной диффузионной модели вводится специальная структура обучения: поэтапная диффузия. Экспериментальные оценки подтверждают универсальность метода, демонстрируя сокращение затрат на обучение на 50% и повышение эффективности вывода в 1,5 раза.'}, 'en': {'title': 'Optimizing Video Diffusion with TPDiff: Efficiency Unleashed!', 'desc': 'This paper addresses the high computational costs associated with video diffusion models by introducing TPDiff, a framework that optimizes training and inference efficiency. The authors leverage the entropy-reducing nature of the diffusion process and the redundancy between video frames to reduce the need for full frame rates during high-entropy stages. TPDiff operates in multiple stages, gradually increasing the frame rate, with only the final stage using the full frame rate, thus enhancing computational efficiency. The proposed stage-wise diffusion training framework further improves efficiency by solving partitioned probability flow ordinary differential equations, leading to significant reductions in training costs and improvements in inference speed.'}, 'zh': {'title': '优化视频扩散模型的计算效率', 'desc': '视频扩散模型的发展面临着巨大的计算需求。为了缓解这一挑战，我们注意到扩散的反向过程具有固有的减少熵的特性。考虑到视频模态中的帧间冗余，在高熵阶段保持全帧率是没有必要的。基于这一见解，我们提出了TPDiff框架，通过将扩散过程分为多个阶段，逐步提高帧率，从而优化计算效率。'}}}, {'id': 'https://huggingface.co/papers/2503.09573', 'title': 'Block Diffusion: Interpolating Between Autoregressive and Diffusion\n  Language Models', 'url': 'https://huggingface.co/papers/2503.09573', 'abstract': 'Diffusion language models offer unique benefits over autoregressive models due to their potential for parallelized generation and controllability, yet they lag in likelihood modeling and are limited to fixed-length generation. In this work, we introduce a class of block diffusion language models that interpolate between discrete denoising diffusion and autoregressive models. Block diffusion overcomes key limitations of both approaches by supporting flexible-length generation and improving inference efficiency with KV caching and parallel token sampling. We propose a recipe for building effective block diffusion models that includes an efficient training algorithm, estimators of gradient variance, and data-driven noise schedules to minimize the variance. Block diffusion sets a new state-of-the-art performance among diffusion models on language modeling benchmarks and enables generation of arbitrary-length sequences. We provide the code, along with the model weights and blog post on the project page: https://m-arriola.com/bd3lms/', 'score': 29, 'issue_id': 2678, 'pub_date': '2025-03-12', 'pub_date_card': {'ru': '12 марта', 'en': 'March 12', 'zh': '3月12日'}, 'hash': '32f097e93cbf5f3a', 'authors': ['Marianne Arriola', 'Aaron Gokaslan', 'Justin T Chiu', 'Zhihan Yang', 'Zhixuan Qi', 'Jiaqi Han', 'Subham Sekhar Sahoo', 'Volodymyr Kuleshov'], 'affiliations': ['Cohere, NY, USA', 'Cornell Tech, NY, USA', 'Stanford University, CA, USA'], 'pdf_title_img': 'assets/pdf/title_img/2503.09573.jpg', 'data': {'categories': ['#open_source', '#diffusion', '#benchmark', '#training', '#architecture'], 'emoji': '🧩', 'ru': {'title': 'Блочные диффузионные модели: лучшее из двух миров в языковом моделировании', 'desc': 'Статья представляет новый класс языковых моделей - блочные диффузионные модели. Они объединяют преимущества диффузионных и авторегрессивных моделей, позволяя генерировать тексты произвольной длины и повышая эффективность вывода. Авторы предлагают эффективный алгоритм обучения, оценки дисперсии градиента и расписания шума для минимизации дисперсии. Блочные диффузионные модели достигают нового уровня производительности среди диффузионных моделей в задачах языкового моделирования.'}, 'en': {'title': 'Block Diffusion: The Future of Flexible Language Generation', 'desc': 'This paper presents block diffusion language models, which combine the strengths of diffusion models and autoregressive models. These models allow for flexible-length text generation and improve efficiency during inference by using techniques like KV caching and parallel token sampling. The authors introduce a comprehensive approach for training these models, including methods to reduce gradient variance and optimize noise schedules. As a result, block diffusion models achieve state-of-the-art performance in language modeling tasks and can generate sequences of varying lengths.'}, 'zh': {'title': '块扩散模型：灵活生成与高效推理的结合', 'desc': '扩散语言模型相比自回归模型具有独特的优势，如并行生成和可控性，但在似然建模方面表现较差，并且生成长度固定。本文提出了一类块扩散语言模型，结合了离散去噪扩散和自回归模型的优点。块扩散克服了这两种方法的关键限制，支持灵活长度的生成，并通过KV缓存和并行令牌采样提高推理效率。我们提出了一种构建有效块扩散模型的方案，包括高效的训练算法、梯度方差估计器和数据驱动的噪声调度，以最小化方差。'}}}, {'id': 'https://huggingface.co/papers/2503.09151', 'title': 'Reangle-A-Video: 4D Video Generation as Video-to-Video Translation', 'url': 'https://huggingface.co/papers/2503.09151', 'abstract': 'We introduce Reangle-A-Video, a unified framework for generating synchronized multi-view videos from a single input video. Unlike mainstream approaches that train multi-view video diffusion models on large-scale 4D datasets, our method reframes the multi-view video generation task as video-to-videos translation, leveraging publicly available image and video diffusion priors. In essence, Reangle-A-Video operates in two stages. (1) Multi-View Motion Learning: An image-to-video diffusion transformer is synchronously fine-tuned in a self-supervised manner to distill view-invariant motion from a set of warped videos. (2) Multi-View Consistent Image-to-Images Translation: The first frame of the input video is warped and inpainted into various camera perspectives under an inference-time cross-view consistency guidance using DUSt3R, generating multi-view consistent starting images. Extensive experiments on static view transport and dynamic camera control show that Reangle-A-Video surpasses existing methods, establishing a new solution for multi-view video generation. We will publicly release our code and data. Project page: https://hyeonho99.github.io/reangle-a-video/', 'score': 23, 'issue_id': 2681, 'pub_date': '2025-03-12', 'pub_date_card': {'ru': '12 марта', 'en': 'March 12', 'zh': '3月12日'}, 'hash': 'e1463c182b0fe8e9', 'authors': ['Hyeonho Jeong', 'Suhyeon Lee', 'Jong Chul Ye'], 'affiliations': ['KAIST AI'], 'pdf_title_img': 'assets/pdf/title_img/2503.09151.jpg', 'data': {'categories': ['#open_source', '#video', '#multimodal', '#diffusion'], 'emoji': '🎥', 'ru': {'title': 'Революция в многоракурсной видеогенерации без 4D-датасетов', 'desc': 'Reangle-A-Video - это новый подход к генерации синхронизированных многоракурсных видео из одного входного видео. Метод использует двухэтапный процесс: обучение многоракурсному движению и согласованный перевод изображения в изображения с разных ракурсов. В отличие от традиционных методов, Reangle-A-Video не требует больших 4D-датасетов, а использует существующие модели диффузии для изображений и видео. Эксперименты показывают, что этот метод превосходит существующие подходы в задачах статического переноса ракурса и динамического управления камерой.'}, 'en': {'title': 'Transforming Single Videos into Multi-View Masterpieces!', 'desc': 'Reangle-A-Video is a novel framework designed to create synchronized multi-view videos from a single input video. It innovatively approaches the multi-view video generation task by treating it as a video-to-video translation problem, utilizing existing image and video diffusion models. The process consists of two main stages: first, it learns motion patterns from warped videos in a self-supervised manner, and second, it generates consistent multi-view images by warping and inpainting the initial frame under specific guidance. This method outperforms current techniques in both static view transport and dynamic camera control, marking a significant advancement in multi-view video generation.'}, 'zh': {'title': 'Reangle-A-Video：单视频生成多视角同步视频的新方法', 'desc': '我们提出了Reangle-A-Video，这是一个统一框架，用于从单个输入视频生成同步的多视角视频。与主流方法不同，我们的方法将多视角视频生成任务重新定义为视频到视频的转换，利用公开可用的图像和视频扩散先验。Reangle-A-Video的操作分为两个阶段：首先，通过自监督方式对图像到视频的扩散变换器进行同步微调，以提取视角不变的运动；其次，在推理时使用DUSt3R进行跨视角一致性指导，将输入视频的第一帧变形并修复为不同的摄像机视角，生成多视角一致的起始图像。'}}}, {'id': 'https://huggingface.co/papers/2503.09601', 'title': 'RewardSDS: Aligning Score Distillation via Reward-Weighted Sampling', 'url': 'https://huggingface.co/papers/2503.09601', 'abstract': 'Score Distillation Sampling (SDS) has emerged as an effective technique for leveraging 2D diffusion priors for tasks such as text-to-3D generation. While powerful, SDS struggles with achieving fine-grained alignment to user intent. To overcome this, we introduce RewardSDS, a novel approach that weights noise samples based on alignment scores from a reward model, producing a weighted SDS loss. This loss prioritizes gradients from noise samples that yield aligned high-reward output. Our approach is broadly applicable and can extend SDS-based methods. In particular, we demonstrate its applicability to Variational Score Distillation (VSD) by introducing RewardVSD. We evaluate RewardSDS and RewardVSD on text-to-image, 2D editing, and text-to-3D generation tasks, showing significant improvements over SDS and VSD on a diverse set of metrics measuring generation quality and alignment to desired reward models, enabling state-of-the-art performance. Project page is available at https://itaychachy. github.io/reward-sds/.', 'score': 10, 'issue_id': 2683, 'pub_date': '2025-03-12', 'pub_date_card': {'ru': '12 марта', 'en': 'March 12', 'zh': '3月12日'}, 'hash': '4cbf8c2d009fe092', 'authors': ['Itay Chachy', 'Guy Yariv', 'Sagie Benaim'], 'affiliations': ['Hebrew University of Jerusalem'], 'pdf_title_img': 'assets/pdf/title_img/2503.09601.jpg', 'data': {'categories': ['#multimodal', '#alignment', '#diffusion', '#3d', '#training'], 'emoji': '🎯', 'ru': {'title': 'Точная генерация контента с помощью взвешенного обучения', 'desc': 'Статья представляет новый метод RewardSDS, улучшающий технику Score Distillation Sampling (SDS) для генерации контента. RewardSDS использует модель вознаграждения для взвешивания шумовых сэмплов, что позволяет лучше соответствовать намерениям пользователя. Авторы также представляют RewardVSD - расширение метода Variational Score Distillation (VSD). Эксперименты показывают значительное улучшение качества генерации и соответствия заданным критериям для задач генерации изображений и 3D-моделей.'}, 'en': {'title': 'Aligning User Intent with Reward-Driven Sampling', 'desc': 'This paper presents RewardSDS, an innovative method that enhances Score Distillation Sampling (SDS) by incorporating alignment scores from a reward model. By weighting noise samples according to their alignment with user intent, RewardSDS generates a more effective weighted loss function. This approach not only improves the performance of SDS but also extends its application to Variational Score Distillation (VSD) through the introduction of RewardVSD. The authors demonstrate that their methods significantly outperform existing techniques in generating high-quality outputs across various tasks, including text-to-image and text-to-3D generation.'}, 'zh': {'title': '提升生成对齐度的新方法', 'desc': '得分蒸馏采样（SDS）是一种有效利用二维扩散先验进行文本到三维生成的技术。然而，SDS在实现与用户意图的细致对齐方面存在困难。为了解决这个问题，我们提出了RewardSDS，这是一种基于奖励模型的对齐得分加权噪声样本的新方法，从而生成加权的SDS损失。我们的研究表明，RewardSDS和RewardVSD在文本到图像、二维编辑和文本到三维生成任务上显著提高了生成质量和与期望奖励模型的对齐度，达到了最先进的性能。'}}}, {'id': 'https://huggingface.co/papers/2503.08525', 'title': 'GTR: Guided Thought Reinforcement Prevents Thought Collapse in RL-based\n  VLM Agent Training', 'url': 'https://huggingface.co/papers/2503.08525', 'abstract': "Reinforcement learning with verifiable outcome rewards (RLVR) has effectively scaled up chain-of-thought (CoT) reasoning in large language models (LLMs). Yet, its efficacy in training vision-language model (VLM) agents for goal-directed action reasoning in visual environments is less established. This work investigates this problem through extensive experiments on complex card games, such as 24 points, and embodied tasks from ALFWorld. We find that when rewards are based solely on action outcomes, RL fails to incentivize CoT reasoning in VLMs, instead leading to a phenomenon we termed thought collapse, characterized by a rapid loss of diversity in the agent's thoughts, state-irrelevant and incomplete reasoning, and subsequent invalid actions, resulting in negative rewards. To counteract thought collapse, we highlight the necessity of process guidance and propose an automated corrector that evaluates and refines the agent's reasoning at each RL step. This simple and scalable GTR (Guided Thought Reinforcement) framework trains reasoning and action simultaneously without the need for dense, per-step human labeling. Our experiments demonstrate that GTR significantly enhances the performance and generalization of the LLaVA-7b model across various visual environments, achieving 3-5 times higher task success rates compared to SoTA models with notably smaller model sizes.", 'score': 10, 'issue_id': 2681, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 марта', 'en': 'March 11', 'zh': '3月11日'}, 'hash': 'e82260bba3e835b4', 'authors': ['Tong Wei', 'Yijun Yang', 'Junliang Xing', 'Yuanchun Shi', 'Zongqing Lu', 'Deheng Ye'], 'affiliations': ['Peking University', 'Tencent', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.08525.jpg', 'data': {'categories': ['#agents', '#games', '#reasoning', '#video', '#training', '#optimization', '#rl'], 'emoji': '🧠', 'ru': {'title': 'Управляемое обучение рассуждениям для визуально-языковых моделей', 'desc': "Исследование посвящено применению обучения с подкреплением для улучшения рассуждений в визуально-языковых моделях. Авторы обнаружили феномен 'коллапса мыслей' при использовании наград, основанных только на результатах действий. Для решения этой проблемы предложен метод GTR (Guided Thought Reinforcement), который автоматически оценивает и уточняет рассуждения агента на каждом шаге обучения. Эксперименты показали, что GTR значительно улучшает производительность и обобщающую способность модели LLaVA-7b в различных визуальных средах."}, 'en': {'title': 'Enhancing VLMs with Guided Thought Reinforcement', 'desc': "This paper explores the challenges of using reinforcement learning (RL) to train vision-language models (VLMs) for reasoning in visual tasks. It identifies a problem called 'thought collapse', where the model's reasoning becomes less diverse and leads to incorrect actions when rewards are based only on outcomes. To address this, the authors propose a Guided Thought Reinforcement (GTR) framework that provides process guidance to improve the reasoning of VLMs during training. Their experiments show that GTR significantly boosts the performance of the LLaVA-7b model, achieving much higher success rates in complex tasks compared to state-of-the-art models."}, 'zh': {'title': '引导思维强化：提升视觉语言模型的推理能力', 'desc': '本研究探讨了可验证结果奖励的强化学习（RLVR）在视觉语言模型（VLM）中的应用，尤其是在复杂的视觉环境中进行目标导向的推理。我们发现，当奖励仅基于行动结果时，RL无法有效激励VLM的思维链推理，导致思维崩溃现象，表现为代理的思维多样性迅速下降和推理不完整。为了解决这一问题，我们提出了一种自动纠正器，能够在每个RL步骤中评估和改进代理的推理过程。通过实验，我们的引导思维强化（GTR）框架显著提高了LLaVA-7b模型在各种视觉环境中的表现和泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2503.04388', 'title': 'More Documents, Same Length: Isolating the Challenge of Multiple\n  Documents in RAG', 'url': 'https://huggingface.co/papers/2503.04388', 'abstract': 'Retrieval-augmented generation (RAG) provides LLMs with relevant documents. Although previous studies noted that retrieving many documents can degrade performance, they did not isolate how the quantity of documents affects performance while controlling for context length. We evaluate various language models on custom datasets derived from a multi-hop QA task. We keep the context length and position of relevant information constant while varying the number of documents, and find that increasing the document count in RAG settings poses significant challenges for LLMs. Additionally, our results indicate that processing multiple documents is a separate challenge from handling long contexts. We also make the datasets and code available: https://github.com/shaharl6000/MoreDocsSameLen .', 'score': 10, 'issue_id': 2682, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 марта', 'en': 'March 6', 'zh': '3月6日'}, 'hash': '9b3fb8251a206c0d', 'authors': ['Shahar Levy', 'Nir Mazor', 'Lihi Shalmon', 'Michael Hassid', 'Gabriel Stanovsky'], 'affiliations': ['School of Computer Science and Engineering The Hebrew University of Jerusalem, Jerusalem, Israel'], 'pdf_title_img': 'assets/pdf/title_img/2503.04388.jpg', 'data': {'categories': ['#long_context', '#dataset', '#open_source', '#rag'], 'emoji': '📚', 'ru': {'title': 'Больше документов - больше проблем для языковых моделей', 'desc': 'Статья исследует влияние количества документов на производительность языковых моделей в задачах генерации с извлечением (RAG). Авторы создали специальные наборы данных, где сохраняли длину контекста и позицию релевантной информации постоянными, но варьировали число документов. Результаты показали, что увеличение количества документов в RAG создает значительные трудности для языковых моделей. Также было установлено, что обработка множества документов - это отдельная проблема от работы с длинными контекстами.'}, 'en': {'title': 'More Documents, More Challenges for LLMs!', 'desc': 'This paper investigates how the number of documents retrieved in Retrieval-augmented generation (RAG) impacts the performance of large language models (LLMs). The authors conduct experiments using custom datasets focused on multi-hop question answering, ensuring that context length remains constant while varying the number of documents. Their findings reveal that increasing the number of documents can significantly hinder the performance of LLMs, indicating that managing multiple documents presents unique challenges distinct from those associated with long contexts. The study contributes to the understanding of RAG by providing datasets and code for further research.'}, 'zh': {'title': '文档数量影响LLM性能的挑战', 'desc': '检索增强生成（RAG）为大型语言模型（LLMs）提供相关文档。尽管之前的研究指出检索过多文档可能会降低性能，但并未明确控制上下文长度来研究文档数量对性能的影响。我们在多跳问答任务的自定义数据集上评估了各种语言模型，发现增加文档数量在RAG设置中对LLMs造成了显著挑战。此外，我们的结果表明，处理多个文档与处理长上下文是两个不同的挑战。'}}}, {'id': 'https://huggingface.co/papers/2503.06955', 'title': 'Motion Anything: Any to Motion Generation', 'url': 'https://huggingface.co/papers/2503.06955', 'abstract': 'Conditional motion generation has been extensively studied in computer vision, yet two critical challenges remain. First, while masked autoregressive methods have recently outperformed diffusion-based approaches, existing masking models lack a mechanism to prioritize dynamic frames and body parts based on given conditions. Second, existing methods for different conditioning modalities often fail to integrate multiple modalities effectively, limiting control and coherence in generated motion. To address these challenges, we propose Motion Anything, a multimodal motion generation framework that introduces an Attention-based Mask Modeling approach, enabling fine-grained spatial and temporal control over key frames and actions. Our model adaptively encodes multimodal conditions, including text and music, improving controllability. Additionally, we introduce Text-Music-Dance (TMD), a new motion dataset consisting of 2,153 pairs of text, music, and dance, making it twice the size of AIST++, thereby filling a critical gap in the community. Extensive experiments demonstrate that Motion Anything surpasses state-of-the-art methods across multiple benchmarks, achieving a 15% improvement in FID on HumanML3D and showing consistent performance gains on AIST++ and TMD. See our project website https://steve-zeyu-zhang.github.io/MotionAnything', 'score': 7, 'issue_id': 2680, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '9199d2d99b75d862', 'authors': ['Zeyu Zhang', 'Yiran Wang', 'Wei Mao', 'Danning Li', 'Rui Zhao', 'Biao Wu', 'Zirui Song', 'Bohan Zhuang', 'Ian Reid', 'Richard Hartley'], 'affiliations': ['ANU', 'Google', 'JD.com', 'MBZUAI', 'McGill', 'Tencent', 'USYD', 'UTS', 'ZJU'], 'pdf_title_img': 'assets/pdf/title_img/2503.06955.jpg', 'data': {'categories': ['#games', '#synthetic', '#optimization', '#benchmark', '#multimodal', '#cv', '#dataset'], 'emoji': '🕺', 'ru': {'title': 'Универсальная генерация движений с мультимодальным контролем', 'desc': 'Статья представляет Motion Anything - новую мультимодальную систему генерации движений, использующую маскирование на основе внимания для точного контроля над ключевыми кадрами и действиями. Модель адаптивно кодирует различные условия, включая текст и музыку, что улучшает управляемость генерируемых движений. Авторы также представляют новый датасет Text-Music-Dance (TMD), содержащий 2153 пары текста, музыки и танца. Эксперименты показывают, что Motion Anything превосходит современные методы на нескольких бенчмарках, достигая 15% улучшения FID на HumanML3D.'}, 'en': {'title': 'Revolutionizing Motion Generation with Multimodal Control', 'desc': 'This paper presents Motion Anything, a new framework for generating motion that effectively combines multiple input types like text and music. It addresses two main challenges in motion generation: the need for prioritizing dynamic elements and the integration of different conditioning modalities. The proposed Attention-based Mask Modeling allows for better control over key frames and actions, enhancing the quality of generated motions. Additionally, the introduction of the Text-Music-Dance dataset provides a larger resource for training, leading to significant improvements in performance compared to existing methods.'}, 'zh': {'title': '多模态运动生成的新突破', 'desc': '本文提出了一种名为Motion Anything的多模态运动生成框架，旨在解决现有方法在动态帧和身体部位优先级方面的不足。我们引入了一种基于注意力的掩模建模方法，使得对关键帧和动作的空间和时间控制更加精细。该模型能够自适应编码文本和音乐等多模态条件，从而提高生成运动的可控性。此外，我们还创建了一个新的运动数据集Text-Music-Dance (TMD)，包含2153对文本、音乐和舞蹈，填补了社区中的重要空白。'}}}, {'id': 'https://huggingface.co/papers/2503.07103', 'title': 'Quantizing Large Language Models for Code Generation: A Differentiated\n  Replication', 'url': 'https://huggingface.co/papers/2503.07103', 'abstract': "Large Language Models (LLMs) have shown an impressive capability in code generation and, specifically, to automatically implement requirements described in natural language. The LLM effectiveness generally increases with its size: The higher the number of LLM's trainable parameters the better its ability to implement code. However, when it comes to deploying LLM-based code generators, larger LLMs pose significant challenges related to their memory (and, consequently, carbon) footprint. A previous work by Wei et al. proposed to leverage quantization techniques to reduce the memory footprint of LLM-based code generators without substantially degrading their effectiveness. In short, they studied LLMs featuring up to 16B parameters, quantizing their precision from floating point 32 bits down to int 8 bits and showing their limited impact on code generation performance. Given the fast pace at which LLM capabilities and quantization techniques are evolving, in this work we present a differentiated replication of the work by Wei et al. in which we consider (i) on the one side, more recent and larger code-related LLMs, of up to 34B parameters; (ii) the latest advancements in model quantization techniques, which allow pushing the compression to the extreme quantization level of 2 bits per model parameter and; (iii) different types of calibration datasets to guide the quantization process, including code-specific ones. Our empirical evaluation reveals that the new frontier for LLM quantization is 4-bit precision, resulting in an average memory footprint reduction of 70% compared to the original model without observing any significant decrease in performance. Additionally, when the quantization becomes even more extreme (3 and 2 bits), a code-specific calibration dataset helps to limit the loss of performance.", 'score': 6, 'issue_id': 2683, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '4f07119680bb80a1', 'authors': ['Alessandro Giagnorio', 'Antonio Mastropaolo', 'Saima Afrin', 'Massimiliano Di Penta', 'Gabriele Bavota'], 'affiliations': ['Software Institute Università della Svizzera italiana, Switzerland'], 'pdf_title_img': 'assets/pdf/title_img/2503.07103.jpg', 'data': {'categories': ['#optimization', '#inference', '#dataset', '#training'], 'emoji': '🧠', 'ru': {'title': 'Квантование LLM для эффективной генерации кода', 'desc': 'Статья посвящена исследованию эффективности квантования больших языковых моделей (LLM) для генерации кода. Авторы изучают возможность сжатия моделей до 2 бит на параметр, используя современные методы квантования. Эксперименты показывают, что 4-битное квантование позволяет сократить объем памяти на 70% без существенной потери производительности. При более экстремальном квантовании (3 и 2 бита) использование специфичного для кода набора данных для калибровки помогает ограничить снижение эффективности.'}, 'en': {'title': 'Optimizing Code Generation with Extreme Quantization Techniques', 'desc': 'This paper explores the use of quantization techniques to reduce the memory footprint of large language models (LLMs) used for code generation. It builds on previous work by examining larger models with up to 34 billion parameters and applying advanced quantization methods that compress model precision down to 2 bits. The study finds that using 4-bit quantization can reduce memory usage by 70% without significantly impacting performance. Furthermore, it highlights the importance of using code-specific calibration datasets to maintain effectiveness even with more aggressive quantization levels.'}, 'zh': {'title': '量化技术助力大型语言模型的高效代码生成', 'desc': '大型语言模型（LLMs）在代码生成方面表现出色，能够根据自然语言描述自动实现需求。模型的有效性通常随着参数数量的增加而提高，但较大的模型在部署时会面临内存和碳足迹的挑战。为了解决这个问题，研究者们提出了量化技术，以减少LLM代码生成器的内存占用，同时保持其有效性。本文通过研究更新的、参数高达34B的LLM，探索了更极端的量化技术，发现4位精度的量化可以将内存占用减少70%，而性能几乎没有显著下降。'}}}, {'id': 'https://huggingface.co/papers/2503.06573', 'title': 'WildIFEval: Instruction Following in the Wild', 'url': 'https://huggingface.co/papers/2503.06573', 'abstract': 'Recent LLMs have shown remarkable success in following user instructions, yet handling instructions with multiple constraints remains a significant challenge. In this work, we introduce WildIFEval - a large-scale dataset of 12K real user instructions with diverse, multi-constraint conditions. Unlike prior datasets, our collection spans a broad lexical and topical spectrum of constraints, in natural user prompts. We categorize these constraints into eight high-level classes to capture their distribution and dynamics in real-world scenarios. Leveraging WildIFEval, we conduct extensive experiments to benchmark the instruction-following capabilities of leading LLMs. Our findings reveal that all evaluated models experience performance degradation with an increasing number of constraints. Thus, we show that all models have a large room for improvement on such tasks. Moreover, we observe that the specific type of constraint plays a critical role in model performance. We release our dataset to promote further research on instruction-following under complex, realistic conditions.', 'score': 5, 'issue_id': 2683, 'pub_date': '2025-03-09', 'pub_date_card': {'ru': '9 марта', 'en': 'March 9', 'zh': '3月9日'}, 'hash': 'a2fa7b2b917156fd', 'authors': ['Gili Lior', 'Asaf Yehudai', 'Ariel Gera', 'Liat Ein-Dor'], 'affiliations': ['IBM Research', 'The Hebrew University of Jerusalem'], 'pdf_title_img': 'assets/pdf/title_img/2503.06573.jpg', 'data': {'categories': ['#alignment', '#dataset', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'Новый вызов для ИИ: следование сложным инструкциям', 'desc': 'Статья представляет новый набор данных WildIFEval, содержащий 12000 реальных пользовательских инструкций с разнообразными ограничениями. Авторы классифицируют эти ограничения на 8 категорий и используют набор данных для оценки способности современных языковых моделей следовать сложным инструкциям. Результаты показывают, что производительность всех моделей снижается с увеличением числа ограничений. Исследование выявляет большой потенциал для улучшения моделей в выполнении сложных инструкций с множественными ограничениями.'}, 'en': {'title': 'WildIFEval: Benchmarking LLMs on Complex User Instructions', 'desc': 'This paper presents WildIFEval, a new dataset designed to evaluate how well large language models (LLMs) can follow user instructions with multiple constraints. The dataset contains 12,000 real user prompts that include a variety of constraints categorized into eight classes, reflecting the complexity of real-world instructions. Experiments show that as the number of constraints increases, the performance of all tested LLMs declines, indicating a significant area for improvement. The study highlights the importance of the type of constraint in affecting model performance and aims to encourage further research in this challenging area of instruction-following.'}, 'zh': {'title': '多重约束下的指令遵循挑战', 'desc': '最近的大型语言模型（LLMs）在遵循用户指令方面取得了显著成功，但处理具有多个约束的指令仍然是一个重大挑战。我们提出了WildIFEval，这是一个包含12000个真实用户指令的大规模数据集，涵盖了多样化的多重约束条件。与之前的数据集不同，我们的收集涵盖了广泛的词汇和主题约束，并将这些约束分为八个高层类别，以捕捉它们在现实场景中的分布和动态。通过WildIFEval，我们进行了广泛的实验，以基准测试领先LLMs的指令遵循能力，发现所有评估的模型在约束数量增加时性能下降，表明这些任务仍有很大的改进空间。'}}}, {'id': 'https://huggingface.co/papers/2503.09402', 'title': 'VLog: Video-Language Models by Generative Retrieval of Narration\n  Vocabulary', 'url': 'https://huggingface.co/papers/2503.09402', 'abstract': "Human daily activities can be concisely narrated as sequences of routine events (e.g., turning off an alarm) in video streams, forming an event vocabulary. Motivated by this, we introduce VLog, a novel video understanding framework that define video narrations as vocabulary, going beyond the typical subword vocabularies in existing generative video-language models. Built on the lightweight language model GPT-2, VLog feature three key innovations: (i) A generative retrieval model, marrying language model's complex reasoning capabilities with contrastive retrieval's efficient similarity search. (ii) A hierarchical vocabulary derived from large-scale video narrations using our narration pair encoding algorithm, enabling efficient indexing of specific events (e.g., cutting a tomato) by identifying broader scenarios (e.g., kitchen) with expressive postfixes (e.g., by the left hand). (iii) A vocabulary update strategy leveraging generative models to extend the vocabulary for novel events encountered during inference. To validate our approach, we introduce VidCap-Eval, a development set requiring concise narrations with reasoning relationships (e.g., before and after). Experiments on EgoSchema, COIN, and HiREST further demonstrate the effectiveness of VLog, highlighting its ability to generate concise, contextually accurate, and efficient narrations, offering a novel perspective on video understanding. Codes are released at https://github.com/showlab/VLog.", 'score': 4, 'issue_id': 2681, 'pub_date': '2025-03-12', 'pub_date_card': {'ru': '12 марта', 'en': 'March 12', 'zh': '3月12日'}, 'hash': '6a25ed0e2c069e4f', 'authors': ['Kevin Qinghong Lin', 'Mike Zheng Shou'], 'affiliations': ['Show Lab, National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2503.09402.jpg', 'data': {'categories': ['#dataset', '#multimodal', '#games', '#reasoning', '#video', '#benchmark'], 'emoji': '🎥', 'ru': {'title': 'VLog: Пересказ видео через словарь событий', 'desc': 'VLog - это новая система понимания видео, которая использует словарь событий для описания повседневной деятельности человека. Она основана на языковой модели GPT-2 и включает в себя генеративную модель поиска, иерархический словарь и стратегию обновления словаря. VLog способна генерировать краткие и точные описания видео, учитывая контекст и логические связи между событиями. Эффективность системы была продемонстрирована на нескольких наборах данных, включая специально созданный набор VidCap-Eval.'}, 'en': {'title': 'VLog: Revolutionizing Video Narration with Hierarchical Vocabulary', 'desc': 'The paper presents VLog, a new framework for understanding videos by narrating daily activities as sequences of events. It introduces a hierarchical vocabulary that allows for efficient indexing of specific actions within broader contexts, enhancing the way video content is interpreted. VLog combines a generative retrieval model with a lightweight language model, enabling complex reasoning and efficient similarity searches. Additionally, it features a vocabulary update strategy that adapts to new events during inference, demonstrating its effectiveness through experiments on various datasets.'}, 'zh': {'title': 'VLog：视频理解的新视角', 'desc': '本论文介绍了一种名为VLog的视频理解框架，旨在将视频叙述定义为词汇，超越现有生成视频语言模型中的子词词汇。VLog基于轻量级语言模型GPT-2，具有三项关键创新：生成检索模型、层次词汇和词汇更新策略。生成检索模型结合了语言模型的复杂推理能力和对比检索的高效相似性搜索。通过在EgoSchema、COIN和HiREST数据集上的实验，验证了VLog在生成简洁、上下文准确的叙述方面的有效性。'}}}, {'id': 'https://huggingface.co/papers/2503.09579', 'title': 'Cost-Optimal Grouped-Query Attention for Long-Context LLMs', 'url': 'https://huggingface.co/papers/2503.09579', 'abstract': 'Building effective and efficient Transformer-based large language models (LLMs) has recently become a research focus, requiring maximizing model language capabilities and minimizing training and deployment costs. Existing efforts have primarily described complex relationships among model performance, parameter size, and data size, as well as searched for the optimal compute allocation to train LLMs. However, they overlook the impacts of context length and attention head configuration (the number of query and key-value heads in grouped-query attention) on training and inference. In this paper, we systematically compare models with different parameter sizes, context lengths, and attention head configurations in terms of model performance, computational cost, and memory cost. Then, we extend the existing scaling methods, which are based solely on parameter size and training compute, to guide the construction of cost-optimal LLMs during both training and inference. Our quantitative scaling studies show that, when processing sufficiently long sequences, a larger model with fewer attention heads can achieve a lower loss while incurring lower computational and memory costs. Our findings provide valuable insights for developing practical LLMs, especially in long-context processing scenarios. We will publicly release our code and data.', 'score': 3, 'issue_id': 2682, 'pub_date': '2025-03-12', 'pub_date_card': {'ru': '12 марта', 'en': 'March 12', 'zh': '3月12日'}, 'hash': '2c884c8c6aab1cc4', 'authors': ['Yingfa Chen', 'Yutong Wu', 'Xu Han', 'Zhiyuan Liu', 'Maosong Sun'], 'affiliations': ['NLP Group, DCST, IAI, BNRIST, Tsinghua University, Beijing, China', 'SIST, University of Science and Technology Beijing, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.09579.jpg', 'data': {'categories': ['#optimization', '#architecture', '#open_source', '#long_context', '#inference', '#training'], 'emoji': '🧠', 'ru': {'title': 'Оптимизация LLM: больше параметров, меньше головок внимания', 'desc': 'Статья исследует влияние длины контекста и конфигурации головок внимания на эффективность и производительность больших языковых моделей (LLM). Авторы проводят систематическое сравнение моделей с различными размерами параметров, длинами контекста и конфигурациями головок внимания. Они расширяют существующие методы масштабирования, чтобы оптимизировать LLM как на этапе обучения, так и при выводе. Результаты показывают, что при обработке длинных последовательностей большая модель с меньшим количеством головок внимания может достичь лучших результатов при меньших вычислительных затратах и использовании памяти.'}, 'en': {'title': 'Optimizing LLMs: Less Heads, More Efficiency!', 'desc': 'This paper investigates how different configurations of Transformer-based large language models (LLMs) affect their performance and resource efficiency. It highlights the importance of context length and attention head settings, which have been largely ignored in previous research. By comparing various model sizes and configurations, the authors propose new scaling methods that optimize both training and inference costs. Their results indicate that larger models with fewer attention heads can perform better and use less computational and memory resources when handling long sequences.'}, 'zh': {'title': '优化大型语言模型的构建与成本', 'desc': '本文探讨了如何构建高效的基于Transformer的大型语言模型（LLMs），重点在于最大化模型的语言能力，同时降低训练和部署成本。研究比较了不同参数大小、上下文长度和注意力头配置对模型性能、计算成本和内存成本的影响。结果表明，在处理较长序列时，较大的模型配合较少的注意力头可以实现更低的损失，同时降低计算和内存成本。我们的研究为开发实用的LLMs提供了重要的见解，尤其是在长上下文处理的场景中。'}}}, {'id': 'https://huggingface.co/papers/2503.09419', 'title': 'Alias-Free Latent Diffusion Models:Improving Fractional Shift\n  Equivariance of Diffusion Latent Space', 'url': 'https://huggingface.co/papers/2503.09419', 'abstract': 'Latent Diffusion Models (LDMs) are known to have an unstable generation process, where even small perturbations or shifts in the input noise can lead to significantly different outputs. This hinders their applicability in applications requiring consistent results. In this work, we redesign LDMs to enhance consistency by making them shift-equivariant. While introducing anti-aliasing operations can partially improve shift-equivariance, significant aliasing and inconsistency persist due to the unique challenges in LDMs, including 1) aliasing amplification during VAE training and multiple U-Net inferences, and 2) self-attention modules that inherently lack shift-equivariance. To address these issues, we redesign the attention modules to be shift-equivariant and propose an equivariance loss that effectively suppresses the frequency bandwidth of the features in the continuous domain. The resulting alias-free LDM (AF-LDM) achieves strong shift-equivariance and is also robust to irregular warping. Extensive experiments demonstrate that AF-LDM produces significantly more consistent results than vanilla LDM across various applications, including video editing and image-to-image translation. Code is available at: https://github.com/SingleZombie/AFLDM', 'score': 3, 'issue_id': 2680, 'pub_date': '2025-03-12', 'pub_date_card': {'ru': '12 марта', 'en': 'March 12', 'zh': '3月12日'}, 'hash': '1c497a991b18da6a', 'authors': ['Yifan Zhou', 'Zeqi Xiao', 'Shuai Yang', 'Xingang Pan'], 'affiliations': ['S-Lab, Nanyang Technological University', 'Wangxuan Institute of Computer Technology, Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2503.09419.jpg', 'data': {'categories': ['#video', '#diffusion', '#training', '#optimization', '#architecture', '#cv'], 'emoji': '🔄', 'ru': {'title': 'Стабильная генерация изображений с помощью эквивариантных латентных диффузионных моделей', 'desc': 'Эта статья представляет новый подход к улучшению стабильности латентных диффузионных моделей (LDM). Авторы предлагают модифицированную архитектуру AF-LDM, которая обладает свойством эквивариантности к сдвигу, что повышает согласованность результатов генерации. Ключевые изменения включают переработку модулей внимания и введение специальной функции потерь для подавления частотной полосы признаков. Эксперименты показывают, что AF-LDM значительно превосходит стандартные LDM по стабильности результатов в различных задачах, таких как редактирование видео и перевод изображений.'}, 'en': {'title': 'Achieving Consistency in Latent Diffusion Models with Shift-Equivariance', 'desc': 'Latent Diffusion Models (LDMs) often produce inconsistent outputs due to their sensitivity to input noise variations. This paper presents a redesign of LDMs to improve their consistency by implementing shift-equivariance. The authors address challenges such as aliasing during VAE training and the limitations of self-attention modules by introducing new shift-equivariant attention mechanisms and an equivariance loss. The resulting alias-free LDM (AF-LDM) shows enhanced robustness and consistency in applications like video editing and image translation compared to traditional LDMs.'}, 'zh': {'title': '提升潜在扩散模型的一致性', 'desc': '潜在扩散模型（LDMs）在生成过程中存在不稳定性，输入噪声的微小变化可能导致输出结果显著不同，这限制了其在需要一致性结果的应用中的适用性。本文通过重新设计LDMs，使其具备平移等变性，从而增强一致性。我们提出了一种新的注意力模块，使其具备平移等变性，并引入了一种等变损失，有效抑制特征在连续域中的频率带宽。最终，得到的无别名LDM（AF-LDM）在多个应用中表现出更强的一致性，尤其是在视频编辑和图像到图像转换任务中。'}}}, {'id': 'https://huggingface.co/papers/2503.08681', 'title': 'Self-Taught Self-Correction for Small Language Models', 'url': 'https://huggingface.co/papers/2503.08681', 'abstract': 'Although large language models (LLMs) have achieved remarkable performance across various tasks, they remain prone to errors. A key challenge is enabling them to self-correct. While prior research has relied on external tools or large proprietary models, this work explores self-correction in small language models (SLMs) through iterative fine-tuning using solely self-generated data. We introduce the Self-Taught Self-Correction (STaSC) algorithm, which incorporates multiple algorithmic design choices. Experimental results on a question-answering task demonstrate that STaSC effectively learns self-correction, leading to significant performance improvements. Our analysis further provides insights into the mechanisms of self-correction and the impact of different design choices on learning dynamics and overall performance. To support future research, we release our user-friendly codebase and lightweight models.', 'score': 3, 'issue_id': 2686, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 марта', 'en': 'March 11', 'zh': '3月11日'}, 'hash': '1886b6cebec24540', 'authors': ['Viktor Moskvoretskii', 'Chris Biemann', 'Irina Nikishina'], 'affiliations': ['HSE University', 'Skoltech', 'University of Hamburg'], 'pdf_title_img': 'assets/pdf/title_img/2503.08681.jpg', 'data': {'categories': ['#small_models', '#open_source', '#optimization', '#alignment', '#training'], 'emoji': '🔧', 'ru': {'title': 'Самообучение самокоррекции: новый подход к улучшению малых языковых моделей', 'desc': 'Эта статья представляет алгоритм Self-Taught Self-Correction (STaSC) для обучения малых языковых моделей (SLM) самокоррекции. В отличие от предыдущих подходов, использующих внешние инструменты или крупные проприетарные модели, STaSC применяет итеративное дообучение на самогенерированных данных. Эксперименты на задаче вопросно-ответного анализа показывают значительное улучшение производительности моделей. Авторы также анализируют механизмы самокоррекции и влияние различных аспектов дизайна алгоритма на динамику обучения и общую эффективность.'}, 'en': {'title': 'Empowering Small Models with Self-Correction', 'desc': 'This paper addresses the issue of error-proneness in large language models (LLMs) by focusing on self-correction capabilities in smaller language models (SLMs). The authors propose a novel algorithm called Self-Taught Self-Correction (STaSC), which utilizes iterative fine-tuning with self-generated data to enhance the self-correction process. Experimental results indicate that STaSC significantly improves performance on a question-answering task, showcasing its effectiveness. Additionally, the paper analyzes the mechanisms behind self-correction and how various design choices influence learning dynamics and outcomes.'}, 'zh': {'title': '小型模型的自我纠正新方法', 'desc': '尽管大型语言模型在各种任务中表现出色，但它们仍然容易出错。本文探讨了如何通过自我生成的数据对小型语言模型进行迭代微调，以实现自我纠正。我们提出了自我学习自我纠正（STaSC）算法，该算法结合了多种设计选择。实验结果表明，STaSC在问答任务中有效地学习了自我纠正，显著提高了性能。'}}}, {'id': 'https://huggingface.co/papers/2503.07588', 'title': 'When Large Vision-Language Model Meets Large Remote Sensing Imagery:\n  Coarse-to-Fine Text-Guided Token Pruning', 'url': 'https://huggingface.co/papers/2503.07588', 'abstract': "Efficient vision-language understanding of large Remote Sensing Images (RSIs) is meaningful but challenging. Current Large Vision-Language Models (LVLMs) typically employ limited pre-defined grids to process images, leading to information loss when handling gigapixel RSIs. Conversely, using unlimited grids significantly increases computational costs. To preserve image details while reducing computational complexity, we propose a text-guided token pruning method with Dynamic Image Pyramid (DIP) integration. Our method introduces: (i) a Region Focus Module (RFM) that leverages text-aware region localization capability to identify critical vision tokens, and (ii) a coarse-to-fine image tile selection and vision token pruning strategy based on DIP, which is guided by RFM outputs and avoids directly processing the entire large imagery. Additionally, existing benchmarks for evaluating LVLMs' perception ability on large RSI suffer from limited question diversity and constrained image sizes. We construct a new benchmark named LRS-VQA, which contains 7,333 QA pairs across 8 categories, with image length up to 27,328 pixels. Our method outperforms existing high-resolution strategies on four datasets using the same data. Moreover, compared to existing token reduction methods, our approach demonstrates higher efficiency under high-resolution settings. Dataset and code are in https://github.com/VisionXLab/LRS-VQA.", 'score': 3, 'issue_id': 2684, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': 'b3cad6b7241db7fa', 'authors': ['Junwei Luo', 'Yingying Zhang', 'Xue Yang', 'Kang Wu', 'Qi Zhu', 'Lei Liang', 'Jingdong Chen', 'Yansheng Li'], 'affiliations': ['Ant Group', 'Shanghai Jiao Tong University', 'University of Science and Technology of China', 'Wuhan University'], 'pdf_title_img': 'assets/pdf/title_img/2503.07588.jpg', 'data': {'categories': ['#optimization', '#cv', '#survey', '#dataset', '#benchmark'], 'emoji': '🛰️', 'ru': {'title': 'Умное сжатие для анализа гигантских спутниковых снимков', 'desc': 'Статья представляет новый метод для эффективного анализа крупномасштабных спутниковых снимков с использованием больших языково-визуальных моделей (LVLM). Авторы предлагают технику выборочного удаления токенов на основе текстовых подсказок и динамической пирамиды изображений для сохранения важных деталей при снижении вычислительной сложности. Метод включает модуль фокусировки на регионах (RFM) и стратегию отбора фрагментов изображения от грубого к точному. Также представлен новый датасет LRS-VQA для оценки способностей LVLM в работе с крупными спутниковыми снимками.'}, 'en': {'title': 'Efficiently Unlocking Insights from Gigapixel Remote Sensing Images', 'desc': 'This paper addresses the challenges of understanding large Remote Sensing Images (RSIs) using Large Vision-Language Models (LVLMs). It introduces a novel text-guided token pruning method combined with a Dynamic Image Pyramid (DIP) to efficiently process gigapixel images without losing important details. The proposed Region Focus Module (RFM) helps in identifying essential vision tokens, while the coarse-to-fine strategy reduces computational costs. Additionally, the authors present a new benchmark, LRS-VQA, to evaluate LVLMs on large RSIs, demonstrating that their method outperforms existing strategies in terms of efficiency and effectiveness.'}, 'zh': {'title': '高效处理大型遥感图像的视觉-语言理解方法', 'desc': '本论文提出了一种高效的视觉-语言理解方法，专门针对大型遥感图像（RSIs）。我们的方法结合了动态图像金字塔（DIP）和文本引导的标记修剪技术，以减少计算复杂度并保留图像细节。通过区域聚焦模块（RFM），我们能够识别关键的视觉标记，从而优化图像处理过程。此外，我们还构建了一个新的基准数据集LRS-VQA，以评估现有大型视觉-语言模型的感知能力。'}}}, {'id': 'https://huggingface.co/papers/2503.09590', 'title': 'BIMBA: Selective-Scan Compression for Long-Range Video Question\n  Answering', 'url': 'https://huggingface.co/papers/2503.09590', 'abstract': 'Video Question Answering (VQA) in long videos poses the key challenge of extracting relevant information and modeling long-range dependencies from many redundant frames. The self-attention mechanism provides a general solution for sequence modeling, but it has a prohibitive cost when applied to a massive number of spatiotemporal tokens in long videos. Most prior methods rely on compression strategies to lower the computational cost, such as reducing the input length via sparse frame sampling or compressing the output sequence passed to the large language model (LLM) via space-time pooling. However, these naive approaches over-represent redundant information and often miss salient events or fast-occurring space-time patterns. In this work, we introduce BIMBA, an efficient state-space model to handle long-form videos. Our model leverages the selective scan algorithm to learn to effectively select critical information from high-dimensional video and transform it into a reduced token sequence for efficient LLM processing. Extensive experiments demonstrate that BIMBA achieves state-of-the-art accuracy on multiple long-form VQA benchmarks, including PerceptionTest, NExT-QA, EgoSchema, VNBench, LongVideoBench, and Video-MME. Code, and models are publicly available at https://sites.google.com/view/bimba-mllm.', 'score': 2, 'issue_id': 2689, 'pub_date': '2025-03-12', 'pub_date_card': {'ru': '12 марта', 'en': 'March 12', 'zh': '3月12日'}, 'hash': '82b85fd9bb1c9a17', 'authors': ['Md Mohaiminul Islam', 'Tushar Nagarajan', 'Huiyu Wang', 'Gedas Bertasius', 'Lorenzo Torresani'], 'affiliations': ['Meta AI', 'UNC Chapel Hill'], 'pdf_title_img': 'assets/pdf/title_img/2503.09590.jpg', 'data': {'categories': ['#video', '#open_source', '#benchmark', '#long_context', '#architecture'], 'emoji': '🎥', 'ru': {'title': 'BIMBA: умное сжатие видео для эффективного анализа большими языковыми моделями', 'desc': 'Статья представляет BIMBA - эффективную модель пространства состояний для обработки длинных видео в задаче видео-вопрос-ответ (VQA). Модель использует алгоритм выборочного сканирования для эффективного выбора критической информации из видео высокой размерности и преобразования ее в сокращенную последовательность токенов. Это позволяет эффективно обрабатывать длинные видео большими языковыми моделями (LLM). BIMBA достигает наилучших результатов на нескольких бенчмарках для длинных видео в задаче VQA.'}, 'en': {'title': 'BIMBA: Efficiently Answering Questions from Long Videos', 'desc': 'This paper addresses the challenge of Video Question Answering (VQA) in long videos, where extracting relevant information from numerous frames is difficult. The authors introduce BIMBA, a state-space model that efficiently selects critical information from high-dimensional video data. Unlike previous methods that often miss important events due to redundancy, BIMBA uses a selective scan algorithm to create a reduced token sequence for processing by large language models (LLMs). The results show that BIMBA achieves state-of-the-art performance on various long-form VQA benchmarks, demonstrating its effectiveness in handling long videos.'}, 'zh': {'title': 'BIMBA：高效处理长视频问答的创新模型', 'desc': '视频问答（VQA）在长视频中面临提取相关信息和建模长距离依赖的挑战。自注意力机制虽然能处理序列建模，但在处理大量时空标记时计算成本过高。以往的方法通常依赖压缩策略来降低计算成本，但这些简单的方法往往会过度表示冗余信息，错过重要事件。我们提出了BIMBA，一个高效的状态空间模型，能够从高维视频中选择关键信息，并将其转化为简化的标记序列，以便高效处理。'}}}, {'id': 'https://huggingface.co/papers/2503.09427', 'title': 'Multimodal Language Modeling for High-Accuracy Single Cell\n  Transcriptomics Analysis and Generation', 'url': 'https://huggingface.co/papers/2503.09427', 'abstract': 'Pre-trained language models (PLMs) have revolutionized scientific research, yet their application to single-cell analysis remains limited. Text PLMs cannot process single-cell RNA sequencing data, while cell PLMs lack the ability to handle free text, restricting their use in multimodal tasks. Existing efforts to bridge these modalities often suffer from information loss or inadequate single-modal pre-training, leading to suboptimal performances. To address these challenges, we propose Single-Cell MultiModal Generative Pre-trained Transformer (scMMGPT), a unified PLM for joint cell and text modeling. scMMGPT effectively integrates the state-of-the-art cell and text PLMs, facilitating cross-modal knowledge sharing for improved performance. To bridge the text-cell modality gap, scMMGPT leverages dedicated cross-modal projectors, and undergoes extensive pre-training on 27 million cells -- the largest dataset for multimodal cell-text PLMs to date. This large-scale pre-training enables scMMGPT to excel in joint cell-text tasks, achieving an 84\\% relative improvement of textual discrepancy for cell description generation, 20.5\\% higher accuracy for cell type annotation, and 4\\% improvement in k-NN accuracy for text-conditioned pseudo-cell generation, outperforming baselines.', 'score': 2, 'issue_id': 2677, 'pub_date': '2025-03-12', 'pub_date_card': {'ru': '12 марта', 'en': 'March 12', 'zh': '3月12日'}, 'hash': '491beb48064068d2', 'authors': ['Yaorui Shi', 'Jiaqi Yang', 'Sihang Li', 'Junfeng Fang', 'Xiang Wang', 'Zhiyuan Liu', 'Yang Zhang'], 'affiliations': ['National University of Singapore', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2503.09427.jpg', 'data': {'categories': ['#plp', '#transfer_learning', '#science', '#multimodal', '#dataset', '#training'], 'emoji': '🧬', 'ru': {'title': 'Единая модель для анализа клеток и текста', 'desc': 'scMMGPT - это новая языковая модель, объединяющая анализ одиночных клеток и текста. Она решает проблему ограниченности существующих моделей, которые специализируются только на одной из этих модальностей. scMMGPT использует специальные проекторы для преодоления разрыва между клеточными и текстовыми данными. Модель была предобучена на 27 миллионах клеток, что является крупнейшим датасетом для мультимодальных клеточно-текстовых моделей на сегодняшний день.'}, 'en': {'title': 'Bridging Cells and Text: The Power of scMMGPT', 'desc': 'This paper introduces the Single-Cell MultiModal Generative Pre-trained Transformer (scMMGPT), a novel model designed to integrate single-cell RNA sequencing data with textual information. Traditional pre-trained language models struggle with this integration due to their inability to process both modalities effectively. scMMGPT addresses these limitations by utilizing cross-modal projectors and extensive pre-training on a large dataset of 27 million cells, enhancing its performance in joint tasks. The results demonstrate significant improvements in cell description generation, cell type annotation accuracy, and text-conditioned pseudo-cell generation compared to existing models.'}, 'zh': {'title': '单细胞多模态生成预训练变换器的创新应用', 'desc': '预训练语言模型（PLMs）在科学研究中带来了革命性的变化，但在单细胞分析中的应用仍然有限。现有的文本PLMs无法处理单细胞RNA测序数据，而细胞PLMs又无法处理自由文本，这限制了它们在多模态任务中的使用。为了解决这些问题，我们提出了单细胞多模态生成预训练变换器（scMMGPT），这是一个用于细胞和文本联合建模的统一PLM。scMMGPT通过专门的跨模态投影器和在2700万个细胞上进行的大规模预训练，显著提高了细胞描述生成和细胞类型注释的准确性。'}}}, {'id': 'https://huggingface.co/papers/2503.05397', 'title': 'Multi Agent based Medical Assistant for Edge Devices', 'url': 'https://huggingface.co/papers/2503.05397', 'abstract': 'Large Action Models (LAMs) have revolutionized intelligent automation, but their application in healthcare faces challenges due to privacy concerns, latency, and dependency on internet access. This report introduces an ondevice, multi-agent healthcare assistant that overcomes these limitations. The system utilizes smaller, task-specific agents to optimize resources, ensure scalability and high performance. Our proposed system acts as a one-stop solution for health care needs with features like appointment booking, health monitoring, medication reminders, and daily health reporting. Powered by the Qwen Code Instruct 2.5 7B model, the Planner and Caller Agents achieve an average RougeL score of 85.5 for planning and 96.5 for calling for our tasks while being lightweight for on-device deployment. This innovative approach combines the benefits of ondevice systems with multi-agent architectures, paving the way for user-centric healthcare solutions.', 'score': 2, 'issue_id': 2685, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 марта', 'en': 'March 7', 'zh': '3月7日'}, 'hash': '0d34c4bc75fe4355', 'authors': ['Sakharam Gawade', 'Shivam Akhouri', 'Chinmay Kulkarni', 'Jagdish Samant', 'Pragya Sahu', 'Aastik', 'Jai Pahal', 'Saswat Meher'], 'affiliations': ['Samsung Research Institute Bangalore, India'], 'pdf_title_img': 'assets/pdf/title_img/2503.05397.jpg', 'data': {'categories': ['#architecture', '#healthcare', '#small_models', '#agents'], 'emoji': '🏥', 'ru': {'title': 'Многоагентный медицинский помощник: эффективность без компромиссов', 'desc': 'Статья представляет инновационную систему многоагентного медицинского помощника, работающего на устройстве пользователя. Эта система преодолевает ограничения, связанные с конфиденциальностью, задержками и зависимостью от интернета, характерные для крупных языковых моделей в здравоохранении. Используя меньшие, специализированные агенты, система оптимизирует ресурсы и обеспечивает масштабируемость. Помощник, основанный на модели Qwen Code Instruct 2.5 7B, предлагает комплексное решение для медицинских нужд, включая запись на прием, мониторинг здоровья и напоминания о приеме лекарств.'}, 'en': {'title': 'Empowering Healthcare with On-Device Multi-Agent Systems', 'desc': 'This paper presents a novel on-device multi-agent healthcare assistant that addresses key challenges in using Large Action Models (LAMs) in healthcare, such as privacy, latency, and internet dependency. By employing smaller, task-specific agents, the system enhances resource optimization, scalability, and performance. The assistant offers various healthcare functionalities, including appointment scheduling, health monitoring, and medication reminders, all while maintaining a lightweight design for on-device use. The system demonstrates impressive performance metrics, achieving high RougeL scores for its planning and calling tasks, showcasing its potential for user-centric healthcare solutions.'}, 'zh': {'title': '智能医疗助手：隐私、安全、高效的解决方案', 'desc': '大型行动模型（LAMs）在智能自动化领域取得了革命性进展，但在医疗保健中的应用面临隐私、延迟和对互联网依赖等挑战。本文介绍了一种基于设备的多智能体医疗助手，克服了这些限制。该系统利用较小的、特定任务的智能体来优化资源，确保可扩展性和高性能。我们提出的系统作为一站式医疗解决方案，具备预约、健康监测、用药提醒和每日健康报告等功能。'}}}, {'id': 'https://huggingface.co/papers/2503.09600', 'title': 'MoC: Mixtures of Text Chunking Learners for Retrieval-Augmented\n  Generation System', 'url': 'https://huggingface.co/papers/2503.09600', 'abstract': 'Retrieval-Augmented Generation (RAG), while serving as a viable complement to large language models (LLMs), often overlooks the crucial aspect of text chunking within its pipeline. This paper initially introduces a dual-metric evaluation method, comprising Boundary Clarity and Chunk Stickiness, to enable the direct quantification of chunking quality. Leveraging this assessment method, we highlight the inherent limitations of traditional and semantic chunking in handling complex contextual nuances, thereby substantiating the necessity of integrating LLMs into chunking process. To address the inherent trade-off between computational efficiency and chunking precision in LLM-based approaches, we devise the granularity-aware Mixture-of-Chunkers (MoC) framework, which consists of a three-stage processing mechanism. Notably, our objective is to guide the chunker towards generating a structured list of chunking regular expressions, which are subsequently employed to extract chunks from the original text. Extensive experiments demonstrate that both our proposed metrics and the MoC framework effectively settle challenges of the chunking task, revealing the chunking kernel while enhancing the performance of the RAG system.', 'score': 1, 'issue_id': 2683, 'pub_date': '2025-03-12', 'pub_date_card': {'ru': '12 марта', 'en': 'March 12', 'zh': '3月12日'}, 'hash': '4d7741ddea8c8c48', 'authors': ['Jihao Zhao', 'Zhiyuan Ji', 'Zhaoxin Fan', 'Hanyu Wang', 'Simin Niu', 'Bo Tang', 'Feiyu Xiong', 'Zhiyu Li'], 'affiliations': ['Institute for Advanced Algorithms Research, Shanghai, China', 'School of Information, Renmin University of China, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.09600.jpg', 'data': {'categories': ['#long_context', '#rag', '#optimization'], 'emoji': '✂️', 'ru': {'title': 'Умное разбиение текста для улучшения RAG систем', 'desc': 'Эта статья представляет новый подход к разбиению текста на фрагменты в системах RAG. Авторы вводят метод двойной метрики для оценки качества разбиения и показывают ограничения традиционных методов. Они предлагают фреймворк Mixture-of-Chunkers (MoC), использующий языковые модели для создания регулярных выражений для извлечения фрагментов. Эксперименты демонстрируют, что предложенные метрики и фреймворк MoC эффективно решают проблемы разбиения текста и улучшают работу систем RAG.'}, 'en': {'title': 'Enhancing Chunking for Better Retrieval-Augmented Generation', 'desc': 'This paper discusses the importance of text chunking in Retrieval-Augmented Generation (RAG) systems that use large language models (LLMs). It introduces a new evaluation method with two metrics, Boundary Clarity and Chunk Stickiness, to measure the quality of text chunks. The authors point out the limitations of existing chunking methods and propose a new framework called Mixture-of-Chunkers (MoC) that improves chunking precision while maintaining computational efficiency. Through experiments, they show that their metrics and MoC framework enhance the overall performance of RAG systems by effectively addressing chunking challenges.'}, 'zh': {'title': '提升RAG系统性能的分块创新', 'desc': '本文探讨了检索增强生成（RAG）模型中文本分块的重要性。我们提出了一种双指标评估方法，包括边界清晰度和分块粘性，以量化分块质量。通过这一评估方法，我们揭示了传统和语义分块在处理复杂上下文时的局限性，并强调了将大型语言模型（LLMs）整合到分块过程中的必要性。为了解决基于LLMs的方法在计算效率和分块精度之间的权衡，我们设计了一个关注粒度的混合分块器（MoC）框架，显著提升了RAG系统的性能。'}}}, {'id': 'https://huggingface.co/papers/2503.09516', 'title': 'Search-R1: Training LLMs to Reason and Leverage Search Engines with\n  Reinforcement Learning', 'url': 'https://huggingface.co/papers/2503.09516', 'abstract': 'Efficiently acquiring external knowledge and up-to-date information is essential for effective reasoning and text generation in large language models (LLMs). Retrieval augmentation and tool-use training approaches where a search engine is treated as a tool lack complex multi-turn retrieval flexibility or require large-scale supervised data. Prompting advanced LLMs with reasoning capabilities during inference to use search engines is not optimal, since the LLM does not learn how to optimally interact with the search engine. This paper introduces Search-R1, an extension of the DeepSeek-R1 model where the LLM learns -- solely through reinforcement learning (RL) -- to autonomously generate (multiple) search queries during step-by-step reasoning with real-time retrieval. Search-R1 optimizes LLM rollouts with multi-turn search interactions, leveraging retrieved token masking for stable RL training and a simple outcome-based reward function. Experiments on seven question-answering datasets show that Search-R1 improves performance by 26% (Qwen2.5-7B), 21% (Qwen2.5-3B), and 10% (LLaMA3.2-3B) over SOTA baselines. This paper further provides empirical insights into RL optimization methods, LLM choices, and response length dynamics in retrieval-augmented reasoning. The code and model checkpoints are available at https://github.com/PeterGriffinJin/Search-R1.', 'score': 0, 'issue_id': 2696, 'pub_date': '2025-03-12', 'pub_date_card': {'ru': '12 марта', 'en': 'March 12', 'zh': '3月12日'}, 'hash': 'd471c9c6c84abb33', 'authors': ['Bowen Jin', 'Hansi Zeng', 'Zhenrui Yue', 'Dong Wang', 'Hamed Zamani', 'Jiawei Han'], 'affiliations': ['Center for Intelligent Information Retrieval, University of Massachusetts Amherst', 'Department of Computer Science, University of Illinois at Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2503.09516.jpg', 'data': {'categories': ['#rl', '#reasoning', '#rag', '#optimization', '#training'], 'emoji': '🔍', 'ru': {'title': 'Обучение языковых моделей эффективному поиску информации', 'desc': 'Эта статья представляет Search-R1 - расширение модели DeepSeek-R1, где большая языковая модель (LLM) учится автономно генерировать поисковые запросы во время пошагового рассуждения с помощью обучения с подкреплением. Search-R1 оптимизирует взаимодействие LLM с поисковой системой, используя маскирование извлеченных токенов для стабильного обучения. Эксперименты показывают значительное улучшение производительности по сравнению с современными базовыми моделями на семи наборах данных для ответов на вопросы. Статья также предоставляет эмпирические данные о методах оптимизации обучения с подкреплением и динамике длины ответов в рассуждениях с использованием поиска.'}, 'en': {'title': 'Empowering LLMs with Autonomous Search through Reinforcement Learning', 'desc': 'This paper presents Search-R1, a novel approach that enhances large language models (LLMs) by enabling them to autonomously generate search queries during reasoning tasks. Unlike traditional methods that rely on supervised data or lack flexibility, Search-R1 employs reinforcement learning (RL) to optimize multi-turn interactions with search engines. The model uses token masking and a simple reward function to stabilize training and improve the quality of retrieved information. Experimental results demonstrate significant performance gains across various question-answering datasets, showcasing the effectiveness of RL in retrieval-augmented reasoning.'}, 'zh': {'title': '提升大型语言模型的检索能力', 'desc': '本文提出了一种名为Search-R1的模型扩展，旨在提高大型语言模型（LLMs）在推理和文本生成中的外部知识获取能力。通过强化学习（RL），Search-R1能够自主生成多个搜索查询，从而实现实时检索和逐步推理。该模型优化了多轮搜索交互，利用检索到的令牌掩蔽技术进行稳定的RL训练，并采用简单的基于结果的奖励函数。实验结果表明，Search-R1在七个问答数据集上的表现显著提升，分别提高了26%、21%和10%。'}}}, {'id': 'https://huggingface.co/papers/2503.09410', 'title': 'Monte Carlo Diffusion for Generalizable Learning-Based RANSAC', 'url': 'https://huggingface.co/papers/2503.09410', 'abstract': 'Random Sample Consensus (RANSAC) is a fundamental approach for robustly estimating parametric models from noisy data. Existing learning-based RANSAC methods utilize deep learning to enhance the robustness of RANSAC against outliers. However, these approaches are trained and tested on the data generated by the same algorithms, leading to limited generalization to out-of-distribution data during inference. Therefore, in this paper, we introduce a novel diffusion-based paradigm that progressively injects noise into ground-truth data, simulating the noisy conditions for training learning-based RANSAC. To enhance data diversity, we incorporate Monte Carlo sampling into the diffusion paradigm, approximating diverse data distributions by introducing different types of randomness at multiple stages. We evaluate our approach in the context of feature matching through comprehensive experiments on the ScanNet and MegaDepth datasets. The experimental results demonstrate that our Monte Carlo diffusion mechanism significantly improves the generalization ability of learning-based RANSAC. We also develop extensive ablation studies that highlight the effectiveness of key components in our framework.', 'score': 0, 'issue_id': 2686, 'pub_date': '2025-03-12', 'pub_date_card': {'ru': '12 марта', 'en': 'March 12', 'zh': '3月12日'}, 'hash': '2ccf3518b5892591', 'authors': ['Jiale Wang', 'Chen Zhao', 'Wei Ke', 'Tong Zhang'], 'affiliations': ['EPFL', 'University of Chinese Academy of Sciences', 'Xian Jiaotong University'], 'pdf_title_img': 'assets/pdf/title_img/2503.09410.jpg', 'data': {'categories': ['#dataset', '#data', '#diffusion', '#cv', '#benchmark', '#optimization'], 'emoji': '🎲', 'ru': {'title': 'Повышение обобщающей способности RANSAC с помощью диффузионной модели Монте-Карло', 'desc': 'Эта статья представляет новый подход к улучшению алгоритма RANSAC с использованием глубокого обучения. Авторы предлагают метод диффузии с применением метода Монте-Карло для генерации разнообразных обучающих данных, симулирующих реальные шумные условия. Эксперименты на наборах данных ScanNet и MegaDepth показывают, что предложенный подход значительно улучшает способность обобщения RANSAC на основе машинного обучения. Исследование включает подробный анализ эффективности ключевых компонентов предложенной системы.'}, 'en': {'title': 'Enhancing RANSAC Robustness with Monte Carlo Diffusion', 'desc': "This paper presents a new method to improve the robustness of learning-based Random Sample Consensus (RANSAC) against noisy data. The authors propose a diffusion-based approach that adds noise to clean data, mimicking real-world conditions for better training. By integrating Monte Carlo sampling, they create diverse data distributions, enhancing the model's ability to generalize to unseen data. Experimental results show that this method significantly boosts the performance of learning-based RANSAC in feature matching tasks."}, 'zh': {'title': '基于扩散的RANSAC：提升鲁棒性与泛化能力', 'desc': '随机样本一致性（RANSAC）是一种用于从噪声数据中稳健估计参数模型的基本方法。现有的基于学习的RANSAC方法利用深度学习增强其对异常值的鲁棒性，但这些方法在相同算法生成的数据上进行训练和测试，导致在推理时对分布外数据的泛化能力有限。本文提出了一种新颖的基于扩散的范式，通过逐步向真实数据中注入噪声，模拟训练基于学习的RANSAC所需的噪声条件。我们还结合了蒙特卡洛采样，以在多个阶段引入不同类型的随机性，从而增强数据的多样性，评估结果表明该方法显著提高了学习型RANSAC的泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2503.08674', 'title': 'Understanding and Mitigating Distribution Shifts For Machine Learning\n  Force Fields', 'url': 'https://huggingface.co/papers/2503.08674', 'abstract': 'Machine Learning Force Fields (MLFFs) are a promising alternative to expensive ab initio quantum mechanical molecular simulations. Given the diversity of chemical spaces that are of interest and the cost of generating new data, it is important to understand how MLFFs generalize beyond their training distributions. In order to characterize and better understand distribution shifts in MLFFs, we conduct diagnostic experiments on chemical datasets, revealing common shifts that pose significant challenges, even for large foundation models trained on extensive data. Based on these observations, we hypothesize that current supervised training methods inadequately regularize MLFFs, resulting in overfitting and learning poor representations of out-of-distribution systems. We then propose two new methods as initial steps for mitigating distribution shifts for MLFFs. Our methods focus on test-time refinement strategies that incur minimal computational cost and do not use expensive ab initio reference labels. The first strategy, based on spectral graph theory, modifies the edges of test graphs to align with graph structures seen during training. Our second strategy improves representations for out-of-distribution systems at test-time by taking gradient steps using an auxiliary objective, such as a cheap physical prior. Our test-time refinement strategies significantly reduce errors on out-of-distribution systems, suggesting that MLFFs are capable of and can move towards modeling diverse chemical spaces, but are not being effectively trained to do so. Our experiments establish clear benchmarks for evaluating the generalization capabilities of the next generation of MLFFs. Our code is available at https://tkreiman.github.io/projects/mlff_distribution_shifts/.', 'score': 0, 'issue_id': 2693, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 марта', 'en': 'March 11', 'zh': '3月11日'}, 'hash': '444f0805a7ed179b', 'authors': ['Tobias Kreiman', 'Aditi S. Krishnapriyan'], 'affiliations': ['LBNL', 'UC Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2503.08674.jpg', 'data': {'categories': ['#training', '#benchmark', '#transfer_learning', '#dataset', '#graphs', '#optimization', '#data'], 'emoji': '🧪', 'ru': {'title': 'Преодоление границ обучения: новые стратегии для машинных силовых полей', 'desc': 'Статья посвящена исследованию машинных силовых полей (MLFFs) как альтернативы дорогостоящим квантово-механическим молекулярным симуляциям. Авторы изучают проблемы обобщения MLFFs за пределами обучающих распределений и предлагают два новых метода для смягчения эффектов смещения распределения. Первый метод основан на спектральной теории графов и модифицирует рёбра тестовых графов. Второй метод улучшает представления для систем вне распределения, используя вспомогательную целевую функцию.'}, 'en': {'title': 'Enhancing MLFFs: Bridging the Gap in Chemical Space Generalization', 'desc': 'This paper discusses Machine Learning Force Fields (MLFFs) as a cost-effective alternative to traditional quantum mechanical simulations for molecular modeling. It highlights the challenges MLFFs face when generalizing to chemical spaces that differ from their training data, often leading to overfitting. The authors propose two innovative test-time refinement strategies to improve the performance of MLFFs on out-of-distribution data without relying on expensive reference labels. Their findings suggest that with better training techniques, MLFFs can effectively model a wider range of chemical environments.'}, 'zh': {'title': '提升机器学习力场的泛化能力', 'desc': '机器学习力场（MLFFs）是一种有前景的替代方案，用于昂贵的量子力学分子模拟。本文研究了MLFFs在训练分布之外的泛化能力，发现当前的监督训练方法可能导致过拟合，无法有效学习分布外系统的表示。为了解决这一问题，提出了两种新的测试时优化策略，旨在减少分布转移带来的误差。实验结果表明，这些策略显著提高了在分布外系统上的表现，表明MLFFs有潜力建模多样的化学空间。'}}}, {'id': 'https://huggingface.co/papers/2503.05333', 'title': 'PhysicsGen: Can Generative Models Learn from Images to Predict Complex\n  Physical Relations?', 'url': 'https://huggingface.co/papers/2503.05333', 'abstract': 'The image-to-image translation abilities of generative learning models have recently made significant progress in the estimation of complex (steered) mappings between image distributions. While appearance based tasks like image in-painting or style transfer have been studied at length, we propose to investigate the potential of generative models in the context of physical simulations. Providing a dataset of 300k image-pairs and baseline evaluations for three different physical simulation tasks, we propose a benchmark to investigate the following research questions: i) are generative models able to learn complex physical relations from input-output image pairs? ii) what speedups can be achieved by replacing differential equation based simulations? While baseline evaluations of different current models show the potential for high speedups (ii), these results also show strong limitations toward the physical correctness (i). This underlines the need for new methods to enforce physical correctness. Data, baseline models and evaluation code http://www.physics-gen.org.', 'score': 0, 'issue_id': 2690, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 марта', 'en': 'March 7', 'zh': '3月7日'}, 'hash': 'c9dae1097c1be3c2', 'authors': ['Martin Spitznagel', 'Jan Vaillant', 'Janis Keuper'], 'affiliations': ['Herrenknecht AG', 'Institute for Machine Learning and Analytics (IMLA), Offenburg University, Germany', 'University of Mannheim, Germany'], 'pdf_title_img': 'assets/pdf/title_img/2503.05333.jpg', 'data': {'categories': ['#cv', '#diffusion', '#optimization', '#benchmark', '#dataset'], 'emoji': '🔬', 'ru': {'title': 'Генеративные модели в физическом моделировании: потенциал и ограничения', 'desc': 'Статья исследует потенциал генеративных моделей в контексте физических симуляций. Авторы предлагают набор данных из 300 тысяч пар изображений и базовые оценки для трех различных задач физического моделирования. Исследование направлено на выяснение способности генеративных моделей изучать сложные физические отношения из пар входных и выходных изображений. Также рассматривается возможность ускорения процесса по сравнению с симуляциями на основе дифференциальных уравнений.'}, 'en': {'title': 'Revolutionizing Physical Simulations with Generative Models', 'desc': 'This paper explores the use of generative learning models for image-to-image translation in the context of physical simulations. It presents a dataset of 300,000 image pairs and establishes a benchmark for evaluating how well these models can learn complex physical relationships. The authors investigate the potential speed improvements that can be achieved by using generative models instead of traditional differential equation-based simulations. However, while some models show promise for faster simulations, they often struggle with maintaining physical accuracy, highlighting the need for new approaches to ensure correctness in physical simulations.'}, 'zh': {'title': '探索生成模型在物理模拟中的潜力', 'desc': '这篇论文探讨了生成学习模型在物理模拟中的应用，尤其是图像到图像的转换能力。研究者提供了一个包含30万对图像的数据集，并针对三种不同的物理模拟任务进行了基准评估。论文提出了两个研究问题：生成模型是否能够从输入输出图像对中学习复杂的物理关系？通过替代基于微分方程的模拟，能实现多大的加速？结果显示，尽管当前模型在加速方面有潜力，但在物理正确性方面存在明显的局限性，因此需要新的方法来确保物理的准确性。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (23)', '#agents (44)', '#agi (12)', '#alignment (29)', '#architecture (70)', '#audio (15)', '#benchmark (129)', '#cv (77)', '#data (55)', '#dataset (127)', '#diffusion (60)', '#ethics (17)', '#games (39)', '#graphs (4)', '#hallucinations (19)', '#healthcare (11)', '#inference (48)', '#interpretability (26)', '#leakage (3)', '#long_context (31)', '#low_resource (9)', '#machine_translation (8)', '#math (11)', '#multilingual (16)', '#multimodal (118)', '#open_source (89)', '#optimization (159)', '#plp (4)', '#rag (20)', '#reasoning (78)', '#rl (39)', '#rlhf (18)', '#robotics (19)', '#science (11)', '#security (15)', '#small_models (13)', '#story_generation (4)', '#survey (10)', '#synthetic (26)', '#training (167)', '#transfer_learning (24)', '#video (53)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-03-19 08:15',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-03-19 08:15')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-03-19 08:15')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('monthly'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    