
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 588 papers. March 2025.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #7a30efcf;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: #7a30efcf;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #7a30ef17;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ", "Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹", "Ğ¼Ğ¸Ğ½ÑƒÑ‚"],
                hour: ["Ñ‡Ğ°Ñ", "Ñ‡Ğ°ÑĞ°", "Ñ‡Ğ°ÑĞ¾Ğ²"],
                day: ["Ğ´ĞµĞ½ÑŒ", "Ğ´Ğ½Ñ", "Ğ´Ğ½ĞµĞ¹"],
                justNow: "Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ñ‚Ğ¾",
                ago: "Ğ½Ğ°Ğ·Ğ°Ğ´"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["åˆ†é’Ÿ", "åˆ†é’Ÿ", "åˆ†é’Ÿ"],
                hour: ["å°æ—¶", "å°æ—¶", "å°æ—¶"],
                day: ["å¤©", "å¤©", "å¤©"],
                justNow: "åˆšåˆš",
                ago: "å‰"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            } else if (lastDigit === 1) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒÑ";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒĞ¸";
            } else {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ç¯‡è®ºæ–‡"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">ğŸ”º</h1><h1 class="title-text" id="doomgrad">hf monthly</h1></a>
            <p><span id="title-date">ĞœĞ°Ñ€Ñ‚ 2025</span> | <span id="title-articles-count">588 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/m/2025-02.html">â¬…ï¸ <span id="prev-date">02.2025</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/m/2025-04.html">â¡ï¸ <span id="next-date">04.2025</span></a></span>
            <span class="nav-item" id="nav-daily"><a href="https://hfday.ru">ğŸ“ˆ <span id='top-day-label'>Ğ”ĞµĞ½ÑŒ</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">ğŸ”€ <span id="sort-label-text">Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ</option>
                    <option value="pub_date">Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸</option>
                    <option value="issue_id">Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">ğŸ·ï¸ Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> AâˆªB</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> Aâˆ©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">ğŸ§¹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> âœ–ï¸ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': 'ĞœĞ°Ñ€Ñ‚ 2025', 'en': 'March 2025', 'zh': '3æœˆ2025å¹´'};
        let feedDateNext = {'ru': '04.2025', 'en': '04/2025', 'zh': '4æœˆ2025å¹´'};
        let feedDatePrev = {'ru': '02.2025', 'en': '02/2025', 'zh': '2æœˆ2025å¹´'};
        let filterLabel = {'ru': 'Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€', 'en': 'Topics', 'zh': 'ä¸»é¢˜ç­›é€‰'}
        let publishedLabel = {'ru': 'ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ñ‚ ', 'en': 'published on ', 'zh': 'å‘è¡¨äº'}
        let sortLabel = {'ru': 'Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾', 'en': 'Sort by', 'zh': 'æ’åºæ–¹å¼'}
        let paperLabel = {'ru': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ', 'en': 'Paper', 'zh': 'è®ºæ–‡'}
        let topMonthLabel = {'ru': 'ĞœĞµÑÑÑ†', 'en': 'Month', 'zh': 'æœˆåº¦è®ºæ–‡'}
        let topDayLabel = {'ru': 'Ğ”ĞµĞ½ÑŒ', 'en': 'Day', 'zh': 'æ—¥åº¦è®ºæ–‡'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf monthly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2503.19693', 'title': 'AdaptiVocab: Enhancing LLM Efficiency in Focused Domains through\n  Lightweight Vocabulary Adaptation', 'url': 'https://huggingface.co/papers/2503.19693', 'abstract': 'Large Language Models (LLMs) have shown impressive versatility as general purpose models. However, their broad applicability comes at a high-cost computational overhead, particularly in auto-regressive decoding where each step requires a forward pass. In domain-specific settings, general-purpose capabilities are unnecessary and can be exchanged for efficiency. In this work, we take a novel perspective on domain adaptation, reducing latency and computational costs by adapting the vocabulary to focused domains of interest. We introduce AdaptiVocab, an end-to-end approach for vocabulary adaptation, designed to enhance LLM efficiency in low-resource domains. AdaptiVocab can be applied to any tokenizer and architecture, modifying the vocabulary by replacing tokens with domain-specific n-gram-based tokens, thereby reducing the number of tokens required for both input processing and output generation. AdaptiVocab initializes new n-token embeddings using an exponentially weighted combination of existing embeddings and employs a lightweight fine-tuning phase that can be efficiently performed on a single GPU. We evaluate two 7B LLMs across three niche domains, assessing efficiency, generation quality, and end-task performance. Our results show that AdaptiVocab reduces token usage by over 25% without compromising performance', 'score': 31, 'issue_id': 2976, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 25', 'zh': '3æœˆ25æ—¥'}, 'hash': '2c907e98a0aafb46', 'authors': ['Itay Nakash', 'Nitay Calderon', 'Eyal Ben David', 'Elad Hoffer', 'Roi Reichart'], 'affiliations': ['Habana Labs', 'The Faculty of Data and Decisions Science, Technion - IIT'], 'pdf_title_img': 'assets/pdf/title_img/2503.19693.jpg', 'data': {'categories': ['#architecture', '#low_resource', '#training', '#optimization', '#data', '#transfer_learning'], 'emoji': 'ğŸ—œï¸', 'ru': {'title': 'ĞĞ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ LLM Ğ² ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…', 'desc': 'AdaptiVocab - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ¸Ñ… ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² ÑƒĞ·ĞºĞ¾ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ·Ğ°Ğ¼ĞµĞ½ÑĞµÑ‚ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ² ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ğµ Ğ½Ğ° ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ° n-Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñ‹, Ñ‡Ñ‚Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ…. AdaptiVocab Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ n-Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞºÑĞ¿Ğ¾Ğ½ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ²Ğ·Ğ²ĞµÑˆĞµĞ½Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ²Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½ÑƒÑ Ñ„Ğ°Ğ·Ñƒ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ AdaptiVocab ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ğ½Ğ° 25% Ğ±ĞµĞ· ÑƒÑ…ÑƒĞ´ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Enhancing LLM Efficiency with Domain-Specific Vocabulary', 'desc': 'This paper presents AdaptiVocab, a method for improving the efficiency of Large Language Models (LLMs) in specific domains by adapting their vocabulary. Instead of using a general-purpose vocabulary, AdaptiVocab replaces tokens with domain-specific n-gram-based tokens, which reduces the number of tokens needed for processing and generation. The approach involves initializing new embeddings through a combination of existing ones and includes a lightweight fine-tuning process that can be done on a single GPU. The results demonstrate that AdaptiVocab can decrease token usage by over 25% while maintaining the quality of generated outputs and overall task performance.'}, 'zh': {'title': 'æé«˜å¤§å‹è¯­è¨€æ¨¡å‹æ•ˆç‡çš„æ–°æ–¹æ³•', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é€šç”¨æ¨¡å‹ä¸­å±•ç°äº†ä»¤äººå°è±¡æ·±åˆ»çš„å¤šåŠŸèƒ½æ€§ï¼Œä½†å…¶å¹¿æ³›åº”ç”¨ä¼´éšç€é«˜æ˜‚çš„è®¡ç®—å¼€é”€ï¼Œå°¤å…¶æ˜¯åœ¨è‡ªå›å½’è§£ç ä¸­ï¼Œæ¯ä¸€æ­¥éƒ½éœ€è¦è¿›è¡Œå‰å‘ä¼ æ’­ã€‚é’ˆå¯¹ç‰¹å®šé¢†åŸŸçš„åº”ç”¨ï¼Œé€šç”¨èƒ½åŠ›å¹¶éå¿…è¦ï¼Œå¯ä»¥é€šè¿‡æé«˜æ•ˆç‡æ¥è¿›è¡Œäº¤æ¢ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„é¢†åŸŸé€‚åº”æ–¹æ³•â€”â€”AdaptiVocabï¼Œé€šè¿‡è°ƒæ•´è¯æ±‡è¡¨æ¥é™ä½å»¶è¿Ÿå’Œè®¡ç®—æˆæœ¬ï¼Œæ—¨åœ¨æé«˜LLMåœ¨ä½èµ„æºé¢†åŸŸçš„æ•ˆç‡ã€‚AdaptiVocabå¯ä»¥åº”ç”¨äºä»»ä½•åˆ†è¯å™¨å’Œæ¶æ„ï¼Œé€šè¿‡ç”¨ç‰¹å®šé¢†åŸŸçš„n-gramä»£æ›¿åŸæœ‰çš„tokensï¼Œå‡å°‘è¾“å…¥å¤„ç†å’Œè¾“å‡ºç”Ÿæˆæ‰€éœ€çš„tokensæ•°é‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.22230', 'title': 'Exploring Data Scaling Trends and Effects in Reinforcement Learning from\n  Human Feedback', 'url': 'https://huggingface.co/papers/2503.22230', 'abstract': "Reinforcement Learning from Human Feedback (RLHF) is crucial for aligning large language models with human preferences. While recent research has focused on algorithmic improvements, the importance of prompt-data construction has been overlooked. This paper addresses this gap by exploring data-driven bottlenecks in RLHF performance scaling, particularly reward hacking and decreasing response diversity. We introduce a hybrid reward system combining reasoning task verifiers (RTV) and a generative reward model (GenRM) to mitigate reward hacking. We also propose a novel prompt-selection method, Pre-PPO, to maintain response diversity and enhance learning effectiveness. Additionally, we find that prioritizing mathematical and coding tasks early in RLHF training significantly improves performance. Experiments across two model sizes validate our methods' effectiveness and scalability. Results show that RTV is most resistant to reward hacking, followed by GenRM with ground truth, and then GenRM with SFT Best-of-N responses. Our strategies enable rapid capture of subtle task-specific distinctions, leading to substantial improvements in overall RLHF performance. This work highlights the importance of careful data construction and provides practical methods to overcome performance barriers in RLHF.", 'score': 21, 'issue_id': 2972, 'pub_date': '2025-03-28', 'pub_date_card': {'ru': '28 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 28', 'zh': '3æœˆ28æ—¥'}, 'hash': 'a994668c51ac51c4', 'authors': ['Wei Shen', 'Guanlin Liu', 'Zheng Wu', 'Ruofei Zhu', 'Qingping Yang', 'Chao Xin', 'Yu Yue', 'Lin Yan'], 'affiliations': ['ByteDance Seed'], 'pdf_title_img': 'assets/pdf/title_img/2503.22230.jpg', 'data': {'categories': ['#rlhf', '#data', '#reasoning', '#alignment', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ£ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ RLHF: Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² ĞºĞ°Ğº ĞºĞ»ÑÑ‡ Ğº ÑƒÑĞ¿ĞµÑ…Ñƒ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ¾Ñ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° (RLHF) Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰ÑƒÑ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ñ‹ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ (RTV) Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (GenRM), Ğ´Ğ»Ñ ÑĞ¼ÑĞ³Ñ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¾Ğ±Ğ¼Ğ°Ğ½Ğ° Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Pre-PPO Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ RLHF, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'Enhancing RLHF: Bridging Data Gaps for Better AI Alignment', 'desc': 'This paper focuses on improving Reinforcement Learning from Human Feedback (RLHF) for large language models by addressing issues in prompt-data construction. It identifies problems like reward hacking and reduced response diversity that hinder RLHF performance. The authors propose a hybrid reward system that combines reasoning task verifiers (RTV) with a generative reward model (GenRM) to counteract these issues. Additionally, they introduce a new prompt-selection method called Pre-PPO and emphasize the importance of prioritizing mathematical and coding tasks during training to enhance overall model performance.'}, 'zh': {'title': 'ä¼˜åŒ–äººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•', 'desc': 'å¼ºåŒ–å­¦ä¹ ä¸­çš„äººç±»åé¦ˆï¼ˆRLHFï¼‰å¯¹äºä½¿å¤§å‹è¯­è¨€æ¨¡å‹ä¸äººç±»åå¥½å¯¹é½è‡³å…³é‡è¦ã€‚å°½ç®¡è¿‘æœŸç ”ç©¶é›†ä¸­åœ¨ç®—æ³•æ”¹è¿›ä¸Šï¼Œä½†æç¤ºæ•°æ®æ„å»ºçš„é‡è¦æ€§å´è¢«å¿½è§†ã€‚æœ¬æ–‡æ¢è®¨äº†RLHFæ€§èƒ½æ‰©å±•ä¸­çš„æ•°æ®é©±åŠ¨ç“¶é¢ˆï¼Œç‰¹åˆ«æ˜¯å¥–åŠ±é»‘å®¢å’Œå“åº”å¤šæ ·æ€§ä¸‹é™çš„é—®é¢˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ··åˆå¥–åŠ±ç³»ç»Ÿï¼Œç»“åˆæ¨ç†ä»»åŠ¡éªŒè¯å™¨ï¼ˆRTVï¼‰å’Œç”Ÿæˆå¥–åŠ±æ¨¡å‹ï¼ˆGenRMï¼‰ï¼Œä»¥å‡è½»å¥–åŠ±é»‘å®¢ç°è±¡ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°é¢–çš„æç¤ºé€‰æ‹©æ–¹æ³•Pre-PPOï¼Œä»¥ä¿æŒå“åº”å¤šæ ·æ€§å¹¶å¢å¼ºå­¦ä¹ æ•ˆæœã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.22675', 'title': 'Think Before Recommend: Unleashing the Latent Reasoning Power for\n  Sequential Recommendation', 'url': 'https://huggingface.co/papers/2503.22675', 'abstract': "Sequential Recommendation (SeqRec) aims to predict the next item by capturing sequential patterns from users' historical interactions, playing a crucial role in many real-world recommender systems. However, existing approaches predominantly adopt a direct forward computation paradigm, where the final hidden state of the sequence encoder serves as the user representation. We argue that this inference paradigm, due to its limited computational depth, struggles to model the complex evolving nature of user preferences and lacks a nuanced understanding of long-tail items, leading to suboptimal performance. To address this issue, we propose ReaRec, the first inference-time computing framework for recommender systems, which enhances user representations through implicit multi-step reasoning. Specifically, ReaRec autoregressively feeds the sequence's last hidden state into the sequential recommender while incorporating special reasoning position embeddings to decouple the original item encoding space from the multi-step reasoning space. Moreover, we introduce two lightweight reasoning-based learning methods, Ensemble Reasoning Learning (ERL) and Progressive Reasoning Learning (PRL), to further effectively exploit ReaRec's reasoning potential. Extensive experiments on five public real-world datasets and different SeqRec architectures demonstrate the generality and effectiveness of our proposed ReaRec. Remarkably, post-hoc analyses reveal that ReaRec significantly elevates the performance ceiling of multiple sequential recommendation backbones by approximately 30\\%-50\\%. Thus, we believe this work can open a new and promising avenue for future research in inference-time computing for sequential recommendation.", 'score': 20, 'issue_id': 2971, 'pub_date': '2025-03-28', 'pub_date_card': {'ru': '28 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 28', 'zh': '3æœˆ28æ—¥'}, 'hash': '092ab2fa75277891', 'authors': ['Jiakai Tang', 'Sunhao Dai', 'Teng Shi', 'Jun Xu', 'Xu Chen', 'Wen Chen', 'Wu Jian', 'Yuning Jiang'], 'affiliations': ['Alibaba Group, Beijing, China', 'Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.22675.jpg', 'data': {'categories': ['#inference', '#reasoning', '#dataset', '#training', '#optimization'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞœĞ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¹', 'desc': 'ReaRec - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞ¸ÑÑ‚ĞµĞ¼ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹. ĞĞ½ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½ÑƒÑ Ğ¿Ğ¾Ğ´Ğ°Ñ‡Ñƒ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½ĞµĞ³Ğ¾ ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ²Ğ° Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹: Ensemble Reasoning Learning (ERL) Ğ¸ Progressive Reasoning Learning (PRL). Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ReaRec Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼.'}, 'en': {'title': 'ReaRec: Elevating Sequential Recommendations with Multi-Step Reasoning', 'desc': 'This paper introduces ReaRec, a novel framework for Sequential Recommendation (SeqRec) that enhances user representation through multi-step reasoning. Traditional methods often rely on a single forward computation, which limits their ability to capture the evolving nature of user preferences and understand less popular items. ReaRec addresses these limitations by using autoregressive techniques and special embeddings to improve the inference process. The proposed framework, along with two learning methods, shows significant performance improvements across various datasets, suggesting a new direction for research in recommendation systems.'}, 'zh': {'title': 'ReaRecï¼šæå‡é¡ºåºæ¨èçš„æ¨ç†èƒ½åŠ›', 'desc': 'é¡ºåºæ¨èï¼ˆSeqRecï¼‰æ—¨åœ¨é€šè¿‡æ•æ‰ç”¨æˆ·å†å²äº¤äº’ä¸­çš„é¡ºåºæ¨¡å¼æ¥é¢„æµ‹ä¸‹ä¸€ä¸ªé¡¹ç›®ï¼Œè¿™åœ¨è®¸å¤šç°å®ä¸–ç•Œçš„æ¨èç³»ç»Ÿä¸­èµ·ç€å…³é”®ä½œç”¨ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦é‡‡ç”¨ç›´æ¥çš„å‰å‘è®¡ç®—èŒƒå¼ï¼Œæœ€ç»ˆçš„éšè—çŠ¶æ€ä½œä¸ºç”¨æˆ·è¡¨ç¤ºï¼Œä½†è¿™ç§æ–¹æ³•åœ¨å»ºæ¨¡ç”¨æˆ·åå¥½çš„å¤æ‚æ¼”å˜æ–¹é¢å­˜åœ¨å±€é™ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ReaRecï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç”¨äºæ¨èç³»ç»Ÿçš„æ¨ç†æ—¶è®¡ç®—æ¡†æ¶ï¼Œé€šè¿‡éšå¼å¤šæ­¥æ¨ç†å¢å¼ºç”¨æˆ·è¡¨ç¤ºã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒReaRecæ˜¾è‘—æé«˜äº†å¤šä¸ªé¡ºåºæ¨èæ¨¡å‹çš„æ€§èƒ½ï¼Œå¼€è¾Ÿäº†æ¨ç†æ—¶è®¡ç®—çš„æ–°ç ”ç©¶æ–¹å‘ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.21614', 'title': 'A Survey of Efficient Reasoning for Large Reasoning Models: Language,\n  Multimodality, and Beyond', 'url': 'https://huggingface.co/papers/2503.21614', 'abstract': 'Recent Large Reasoning Models (LRMs), such as DeepSeek-R1 and OpenAI o1, have demonstrated strong performance gains by scaling up the length of Chain-of-Thought (CoT) reasoning during inference. However, a growing concern lies in their tendency to produce excessively long reasoning traces, which are often filled with redundant content (e.g., repeated definitions), over-analysis of simple problems, and superficial exploration of multiple reasoning paths for harder tasks. This inefficiency introduces significant challenges for training, inference, and real-world deployment (e.g., in agent-based systems), where token economy is critical. In this survey, we provide a comprehensive overview of recent efforts aimed at improving reasoning efficiency in LRMs, with a particular focus on the unique challenges that arise in this new paradigm. We identify common patterns of inefficiency, examine methods proposed across the LRM lifecycle, i.e., from pretraining to inference, and discuss promising future directions for research. To support ongoing development, we also maintain a real-time GitHub repository tracking recent progress in the field. We hope this survey serves as a foundation for further exploration and inspires innovation in this rapidly evolving area.', 'score': 16, 'issue_id': 2976, 'pub_date': '2025-03-27', 'pub_date_card': {'ru': '27 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 27', 'zh': '3æœˆ27æ—¥'}, 'hash': '805b7cd4ec307c34', 'authors': ['Xiaoye Qu', 'Yafu Li', 'Zhaochen Su', 'Weigao Sun', 'Jianhao Yan', 'Dongrui Liu', 'Ganqu Cui', 'Daizong Liu', 'Shuxian Liang', 'Junxian He', 'Peng Li', 'Wei Wei', 'Jing Shao', 'Chaochao Lu', 'Yue Zhang', 'Xian-Sheng Hua', 'Bowen Zhou', 'Yu Cheng'], 'affiliations': ['Huazhong University of Science and Technology', 'Peking University', 'Shanghai AI Laboratory', 'Soochow University', 'The Chinese University of Hong Kong', 'The Hong Kong University of Science and Technology', 'Tongji University', 'Tsinghua University', 'Westlake University'], 'pdf_title_img': 'assets/pdf/title_img/2503.21614.jpg', 'data': {'categories': ['#reasoning', '#inference', '#training', '#survey', '#agents'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'Ğ­Ñ‚Ğ¾Ñ‚ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (LRM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ‡Ğ°ÑÑ‚Ğ¾ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‚ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€ÑÑÑ‰Ğ¸Ğ¹ÑÑ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚ Ğ¸ Ğ¸Ğ·Ğ»Ğ¸ÑˆĞ½Ğ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·. ĞĞ±ÑÑƒĞ¶Ğ´Ğ°ÑÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ²ÑĞµÑ… ÑÑ‚Ğ°Ğ¿Ğ°Ñ… Ğ¶Ğ¸Ğ·Ğ½ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ†Ğ¸ĞºĞ»Ğ° LRM - Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾ Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°ÑÑ‰ĞµĞ¹ÑÑ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸.'}, 'en': {'title': 'Enhancing Efficiency in Large Reasoning Models', 'desc': 'This paper discusses the challenges faced by Large Reasoning Models (LRMs) like DeepSeek-R1 and OpenAI o1, particularly their tendency to generate long and redundant reasoning processes. These inefficiencies can complicate training and real-world applications, especially where efficient use of tokens is crucial. The authors review various strategies aimed at enhancing reasoning efficiency throughout the LRM lifecycle, from pretraining to inference. They also provide a GitHub repository to track advancements in this field, aiming to inspire further research and innovation.'}, 'zh': {'title': 'æå‡æ¨ç†æ•ˆç‡ï¼ŒåŠ©åŠ›å¤§å‹æ¨¡å‹å‘å±•', 'desc': 'æœ€è¿‘çš„å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰ï¼Œå¦‚DeepSeek-R1å’ŒOpenAI o1ï¼Œé€šè¿‡æ‰©å±•æ¨ç†é“¾ï¼ˆCoTï¼‰çš„é•¿åº¦åœ¨æ¨ç†è¿‡ç¨‹ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ç„¶è€Œï¼Œå®ƒä»¬å¾€å¾€ç”Ÿæˆè¿‡é•¿çš„æ¨ç†è¿‡ç¨‹ï¼Œå†…å®¹å†—ä½™ï¼ˆä¾‹å¦‚ï¼Œé‡å¤å®šä¹‰ï¼‰ã€å¯¹ç®€å•é—®é¢˜çš„è¿‡åº¦åˆ†æï¼Œä»¥åŠå¯¹å¤æ‚ä»»åŠ¡çš„å¤šæ¡æ¨ç†è·¯å¾„çš„è¡¨é¢æ¢ç´¢ã€‚è¿™ç§ä½æ•ˆæ€§åœ¨è®­ç»ƒã€æ¨ç†å’Œå®é™…åº”ç”¨ä¸­å¼•å‘äº†é‡å¤§æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨ä»£ç†ç³»ç»Ÿä¸­ï¼Œä»¤ç‰Œç»æµè‡³å…³é‡è¦ã€‚æœ¬æ–‡ç»¼è¿°äº†æ”¹å–„LRMsæ¨ç†æ•ˆç‡çš„æœ€æ–°åŠªåŠ›ï¼Œè¯†åˆ«äº†å¸¸è§çš„ä½æ•ˆæ¨¡å¼ï¼Œå¹¶æ¢è®¨äº†æœªæ¥çš„ç ”ç©¶æ–¹å‘ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.22194', 'title': 'ORIGEN: Zero-Shot 3D Orientation Grounding in Text-to-Image Generation', 'url': 'https://huggingface.co/papers/2503.22194', 'abstract': 'We introduce ORIGEN, the first zero-shot method for 3D orientation grounding in text-to-image generation across multiple objects and diverse categories. While previous work on spatial grounding in image generation has mainly focused on 2D positioning, it lacks control over 3D orientation. To address this, we propose a reward-guided sampling approach using a pretrained discriminative model for 3D orientation estimation and a one-step text-to-image generative flow model. While gradient-ascent-based optimization is a natural choice for reward-based guidance, it struggles to maintain image realism. Instead, we adopt a sampling-based approach using Langevin dynamics, which extends gradient ascent by simply injecting random noise--requiring just a single additional line of code. Additionally, we introduce adaptive time rescaling based on the reward function to accelerate convergence. Our experiments show that ORIGEN outperforms both training-based and test-time guidance methods across quantitative metrics and user studies.', 'score': 14, 'issue_id': 2971, 'pub_date': '2025-03-28', 'pub_date_card': {'ru': '28 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 28', 'zh': '3æœˆ28æ—¥'}, 'hash': '6d93cc886c16f9b0', 'authors': ['Yunhong Min', 'Daehyeon Choi', 'Kyeongmin Yeo', 'Jihyun Lee', 'Minhyuk Sung'], 'affiliations': ['KAIST'], 'pdf_title_img': 'assets/pdf/title_img/2503.22194.jpg', 'data': {'categories': ['#multimodal', '#optimization', '#3d'], 'emoji': 'ğŸ§­', 'ru': {'title': 'Ğ¢Ğ¾Ñ‡Ğ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ 3D Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'ORIGEN - ÑÑ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ 3D Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ° Ğ±ĞµĞ· Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ´Ğ¸ÑĞºÑ€Ğ¸Ğ¼Ğ¸Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ 3D Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ğ´Ğ½Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ‚ĞµĞºÑÑ‚-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¿ÑƒÑĞºĞ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ ÑĞµĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ğ›Ğ°Ğ½Ğ¶ĞµĞ²ĞµĞ½Ğ° Ñ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾Ğ³Ğ¾ ÑˆÑƒĞ¼Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ ORIGEN Ğ½Ğ°Ğ´ Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Revolutionizing 3D Orientation in Text-to-Image Generation with ORIGEN', 'desc': 'ORIGEN is a novel method that enables zero-shot 3D orientation grounding in text-to-image generation, allowing for better control over how objects are oriented in three-dimensional space. Unlike previous methods that focused on 2D positioning, ORIGEN utilizes a reward-guided sampling approach that leverages a pretrained model for estimating 3D orientations. This method incorporates Langevin dynamics to enhance image realism while maintaining effective sampling, requiring minimal code changes. Experimental results demonstrate that ORIGEN surpasses existing training-based and test-time guidance techniques in both quantitative metrics and user evaluations.'}, 'zh': {'title': 'ORIGENï¼šæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­çš„3Dæ–¹å‘å®šä½æ–°æ–¹æ³•', 'desc': 'æˆ‘ä»¬ä»‹ç»äº†ORIGENï¼Œè¿™æ˜¯é¦–ä¸ªç”¨äºæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­å®ç°3Dæ–¹å‘å®šä½çš„é›¶æ ·æœ¬æ–¹æ³•ã€‚ä»¥å¾€çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨2Då®šä½ä¸Šï¼Œç¼ºä¹å¯¹3Dæ–¹å‘çš„æ§åˆ¶ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå¥–åŠ±å¼•å¯¼çš„é‡‡æ ·æ–¹æ³•ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„åˆ¤åˆ«æ¨¡å‹è¿›è¡Œ3Dæ–¹å‘ä¼°è®¡ï¼Œå¹¶ç»“åˆä¸€æ­¥æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæµæ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒORIGENåœ¨å®šé‡æŒ‡æ ‡å’Œç”¨æˆ·ç ”ç©¶ä¸­å‡ä¼˜äºåŸºäºè®­ç»ƒå’Œæµ‹è¯•æ—¶å¼•å¯¼çš„æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.20785', 'title': 'Free4D: Tuning-free 4D Scene Generation with Spatial-Temporal\n  Consistency', 'url': 'https://huggingface.co/papers/2503.20785', 'abstract': 'We present Free4D, a novel tuning-free framework for 4D scene generation from a single image. Existing methods either focus on object-level generation, making scene-level generation infeasible, or rely on large-scale multi-view video datasets for expensive training, with limited generalization ability due to the scarcity of 4D scene data. In contrast, our key insight is to distill pre-trained foundation models for consistent 4D scene representation, which offers promising advantages such as efficiency and generalizability. 1) To achieve this, we first animate the input image using image-to-video diffusion models followed by 4D geometric structure initialization. 2) To turn this coarse structure into spatial-temporal consistent multiview videos, we design an adaptive guidance mechanism with a point-guided denoising strategy for spatial consistency and a novel latent replacement strategy for temporal coherence. 3) To lift these generated observations into consistent 4D representation, we propose a modulation-based refinement to mitigate inconsistencies while fully leveraging the generated information. The resulting 4D representation enables real-time, controllable rendering, marking a significant advancement in single-image-based 4D scene generation.', 'score': 11, 'issue_id': 2974, 'pub_date': '2025-03-26', 'pub_date_card': {'ru': '26 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 26', 'zh': '3æœˆ26æ—¥'}, 'hash': '1a3973da9e51afd7', 'authors': ['Tianqi Liu', 'Zihao Huang', 'Zhaoxi Chen', 'Guangcong Wang', 'Shoukang Hu', 'Liao Shen', 'Huiqiang Sun', 'Zhiguo Cao', 'Wei Li', 'Ziwei Liu'], 'affiliations': ['Great Bay University', 'Huazhong University of Science and Technology', 'S-Lab, Nanyang Technological University'], 'pdf_title_img': 'assets/pdf/title_img/2503.20785.jpg', 'data': {'categories': ['#3d', '#multimodal', '#video'], 'emoji': 'ğŸ¬', 'ru': {'title': 'ĞÑ‚ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ°Ğ´Ñ€Ğ° Ğº Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ†ĞµĞ½Ğ½Ğ¾Ğ¹ 4D-ÑÑ†ĞµĞ½Ğµ', 'desc': 'Free4D - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 4D-ÑÑ†ĞµĞ½ Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ 4D-Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑÑ†ĞµĞ½Ñ‹. Free4D ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ°Ğ½Ğ¸Ğ¼Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ¼ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ 4D-Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰ĞµĞµ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ñ‚ÑŒ ÑÑ†ĞµĞ½Ñƒ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Revolutionizing 4D Scene Generation from a Single Image', 'desc': 'Free4D is a new framework that generates 4D scenes from just one image without needing extensive tuning. Unlike previous methods that either focus on individual objects or require large datasets, Free4D uses pre-trained models to create consistent 4D representations efficiently. It employs image-to-video diffusion models to animate the input image and then refines the generated structure for spatial and temporal consistency. This approach allows for real-time rendering of 4D scenes, making it a significant step forward in scene generation technology.'}, 'zh': {'title': 'ä»å•å›¾åƒç”Ÿæˆ4Dåœºæ™¯çš„æ–°çªç ´', 'desc': 'æˆ‘ä»¬æå‡ºäº†Free4Dï¼Œè¿™æ˜¯ä¸€ä¸ªæ— éœ€è°ƒä¼˜çš„æ¡†æ¶ï¼Œå¯ä»¥ä»å•å¼ å›¾åƒç”Ÿæˆ4Dåœºæ™¯ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¸“æ³¨äºç‰©ä½“çº§ç”Ÿæˆï¼Œå¯¼è‡´åœºæ™¯çº§ç”Ÿæˆä¸å¯è¡Œï¼Œæˆ–è€…ä¾èµ–äºå¤§è§„æ¨¡å¤šè§†è§’è§†é¢‘æ•°æ®é›†è¿›è¡Œæ˜‚è´µçš„è®­ç»ƒï¼Œä¸”ç”±äº4Dåœºæ™¯æ•°æ®ç¨€ç¼ºï¼Œæ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚æˆ‘ä»¬çš„å…³é”®è§è§£æ˜¯æç‚¼é¢„è®­ç»ƒçš„åŸºç¡€æ¨¡å‹ï¼Œä»¥å®ç°ä¸€è‡´çš„4Dåœºæ™¯è¡¨ç¤ºï¼Œè¿™æä¾›äº†æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›çš„ä¼˜åŠ¿ã€‚é€šè¿‡å›¾åƒåˆ°è§†é¢‘çš„æ‰©æ•£æ¨¡å‹ï¼Œæˆ‘ä»¬é¦–å…ˆå¯¹è¾“å…¥å›¾åƒè¿›è¡ŒåŠ¨ç”»å¤„ç†ï¼Œç„¶ååˆå§‹åŒ–4Då‡ ä½•ç»“æ„ï¼Œæœ€ç»ˆå®ç°å®æ—¶ã€å¯æ§çš„4Dåœºæ™¯ç”Ÿæˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.21821', 'title': 'PHYSICS: Benchmarking Foundation Models on University-Level Physics\n  Problem Solving', 'url': 'https://huggingface.co/papers/2503.21821', 'abstract': 'We introduce PHYSICS, a comprehensive benchmark for university-level physics problem solving. It contains 1297 expert-annotated problems covering six core areas: classical mechanics, quantum mechanics, thermodynamics and statistical mechanics, electromagnetism, atomic physics, and optics. Each problem requires advanced physics knowledge and mathematical reasoning. We develop a robust automated evaluation system for precise and reliable validation. Our evaluation of leading foundation models reveals substantial limitations. Even the most advanced model, o3-mini, achieves only 59.9% accuracy, highlighting significant challenges in solving high-level scientific problems. Through comprehensive error analysis, exploration of diverse prompting strategies, and Retrieval-Augmented Generation (RAG)-based knowledge augmentation, we identify key areas for improvement, laying the foundation for future advancements.', 'score': 10, 'issue_id': 2972, 'pub_date': '2025-03-26', 'pub_date_card': {'ru': '26 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 26', 'zh': '3æœˆ26æ—¥'}, 'hash': '5254bbde255c8669', 'authors': ['Kaiyue Feng', 'Yilun Zhao', 'Yixin Liu', 'Tianyu Yang', 'Chen Zhao', 'John Sous', 'Arman Cohan'], 'affiliations': ['New York University', 'Notre Dame University', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2503.21821.jpg', 'data': {'categories': ['#reasoning', '#rag', '#benchmark', '#science'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ²Ñ‹Ğ·Ğ¾Ğ² Ğ´Ğ»Ñ Ğ˜Ğ˜: Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾ Ñ„Ğ¸Ğ·Ğ¸ĞºĞµ', 'desc': 'PHYSICS - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¿Ğ¾ Ñ„Ğ¸Ğ·Ğ¸ĞºĞµ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ¸Ñ‚ĞµÑ‚ÑĞºĞ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 1297 ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾ ÑˆĞµÑÑ‚Ğ¸ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğ¼ Ñ€Ğ°Ğ·Ğ´ĞµĞ»Ğ°Ğ¼ Ñ„Ğ¸Ğ·Ğ¸ĞºĞ¸. Ğ”Ğ°Ğ¶Ğµ ÑĞ°Ğ¼Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½ÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° ÑÑ‚Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¸ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¸Ğ½Ğ³Ğ° Ğ¸ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ RAG Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¿ÑƒÑ‚ĞµĞ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Benchmarking Physics Problem Solving with PHYSICS', 'desc': 'The paper presents PHYSICS, a benchmark designed to assess university-level physics problem solving capabilities. It includes 1297 expert-annotated problems across six fundamental physics domains, requiring both advanced knowledge and mathematical skills. The authors introduce an automated evaluation system to ensure accurate validation of model performance. Their findings reveal that even the top-performing model, o3-mini, only achieves 59.9% accuracy, indicating significant room for improvement in tackling complex scientific challenges.'}, 'zh': {'title': 'ç‰©ç†é—®é¢˜è§£å†³çš„æ–°åŸºå‡†', 'desc': 'æˆ‘ä»¬ä»‹ç»äº†PHYSICSï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„å¤§å­¦ç‰©ç†é—®é¢˜è§£å†³åŸºå‡†ã€‚å®ƒåŒ…å«1297ä¸ªä¸“å®¶æ³¨é‡Šçš„é—®é¢˜ï¼Œæ¶µç›–ç»å…¸åŠ›å­¦ã€é‡å­åŠ›å­¦ã€çƒ­åŠ›å­¦ä¸ç»Ÿè®¡åŠ›å­¦ã€ç”µç£å­¦ã€åŸå­ç‰©ç†å’Œå…‰å­¦å…­ä¸ªæ ¸å¿ƒé¢†åŸŸã€‚æ¯ä¸ªé—®é¢˜éƒ½éœ€è¦é«˜çº§ç‰©ç†çŸ¥è¯†å’Œæ•°å­¦æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬çš„è¯„ä¼°æ˜¾ç¤ºï¼Œå½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹o3-miniçš„å‡†ç¡®ç‡ä»…ä¸º59.9%ï¼Œè¿™çªæ˜¾äº†è§£å†³é«˜æ°´å¹³ç§‘å­¦é—®é¢˜çš„é‡å¤§æŒ‘æˆ˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.22236', 'title': 'Hi3DGen: High-fidelity 3D Geometry Generation from Images via Normal\n  Bridging', 'url': 'https://huggingface.co/papers/2503.22236', 'abstract': 'With the growing demand for high-fidelity 3D models from 2D images, existing methods still face significant challenges in accurately reproducing fine-grained geometric details due to limitations in domain gaps and inherent ambiguities in RGB images. To address these issues, we propose Hi3DGen, a novel framework for generating high-fidelity 3D geometry from images via normal bridging. Hi3DGen consists of three key components: (1) an image-to-normal estimator that decouples the low-high frequency image pattern with noise injection and dual-stream training to achieve generalizable, stable, and sharp estimation; (2) a normal-to-geometry learning approach that uses normal-regularized latent diffusion learning to enhance 3D geometry generation fidelity; and (3) a 3D data synthesis pipeline that constructs a high-quality dataset to support training. Extensive experiments demonstrate the effectiveness and superiority of our framework in generating rich geometric details, outperforming state-of-the-art methods in terms of fidelity. Our work provides a new direction for high-fidelity 3D geometry generation from images by leveraging normal maps as an intermediate representation.', 'score': 9, 'issue_id': 2972, 'pub_date': '2025-03-28', 'pub_date_card': {'ru': '28 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 28', 'zh': '3æœˆ28æ—¥'}, 'hash': '5024619189472d8c', 'authors': ['Chongjie Ye', 'Yushuang Wu', 'Ziteng Lu', 'Jiahao Chang', 'Xiaoyang Guo', 'Jiaqing Zhou', 'Hao Zhao', 'Xiaoguang Han'], 'affiliations': ['ByteDance', 'The Chinese University of Hong Kong, Shenzhen', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.22236.jpg', 'data': {'categories': ['#3d'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'ĞÑ‚ Ğ¿Ğ»Ğ¾ÑĞºĞ¾Ğ³Ğ¾ Ğº Ğ¾Ğ±ÑŠĞµĞ¼Ğ½Ğ¾Ğ¼Ñƒ: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² 3D-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸', 'desc': 'Hi3DGen - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… 3D-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸Ğ· 2D-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»ĞµĞ¹ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ñ‚Ñ€ĞµÑ… ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ²: Ğ¾Ñ†ĞµĞ½Ñ‰Ğ¸ĞºĞ° Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»ĞµĞ¹ Ğ¸Ğ· Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ° Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ğ¸Ğ· Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸, Ğ¸ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½Ğ° Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… 3D-Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Hi3DGen Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ»ĞºĞ¸Ñ… Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ğ¾Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ 3D-Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ğ¸Ğ· Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Bridging Normals for High-Fidelity 3D Generation', 'desc': 'The paper introduces Hi3DGen, a new framework designed to create high-fidelity 3D models from 2D images. It tackles challenges like domain gaps and ambiguities in RGB images by using a method called normal bridging. Hi3DGen includes an image-to-normal estimator that improves the accuracy of geometric details through noise injection and dual-stream training. Additionally, it employs a normal-to-geometry learning approach and a 3D data synthesis pipeline to enhance the quality of the generated 3D models, showing superior performance compared to existing methods.'}, 'zh': {'title': 'é«˜ä¿çœŸ3Då‡ ä½•ä½“ç”Ÿæˆçš„æ–°æ–¹å‘', 'desc': 'éšç€å¯¹ä»2Då›¾åƒç”Ÿæˆé«˜ä¿çœŸ3Dæ¨¡å‹çš„éœ€æ±‚å¢åŠ ï¼Œç°æœ‰æ–¹æ³•åœ¨å‡†ç¡®å†ç°ç»†è‡´å‡ ä½•ç»†èŠ‚æ–¹é¢ä»é¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Hi3DGenï¼Œä¸€ä¸ªé€šè¿‡æ³•çº¿æ¡¥æ¥ç”Ÿæˆé«˜ä¿çœŸ3Då‡ ä½•ä½“çš„æ–°æ¡†æ¶ã€‚Hi3DGenåŒ…å«ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šå›¾åƒåˆ°æ³•çº¿ä¼°è®¡å™¨ã€æ³•çº¿åˆ°å‡ ä½•ä½“å­¦ä¹ æ–¹æ³•å’Œ3Dæ•°æ®åˆæˆç®¡é“ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨ç”Ÿæˆä¸°å¯Œå‡ ä½•ç»†èŠ‚æ–¹é¢çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ï¼Œè¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.22268', 'title': 'Segment Any Motion in Videos', 'url': 'https://huggingface.co/papers/2503.22268', 'abstract': 'Moving object segmentation is a crucial task for achieving a high-level understanding of visual scenes and has numerous downstream applications. Humans can effortlessly segment moving objects in videos. Previous work has largely relied on optical flow to provide motion cues; however, this approach often results in imperfect predictions due to challenges such as partial motion, complex deformations, motion blur and background distractions. We propose a novel approach for moving object segmentation that combines long-range trajectory motion cues with DINO-based semantic features and leverages SAM2 for pixel-level mask densification through an iterative prompting strategy. Our model employs Spatio-Temporal Trajectory Attention and Motion-Semantic Decoupled Embedding to prioritize motion while integrating semantic support. Extensive testing on diverse datasets demonstrates state-of-the-art performance, excelling in challenging scenarios and fine-grained segmentation of multiple objects. Our code is available at https://motion-seg.github.io/.', 'score': 8, 'issue_id': 2972, 'pub_date': '2025-03-28', 'pub_date_card': {'ru': '28 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 28', 'zh': '3æœˆ28æ—¥'}, 'hash': '084606e82bff72ff', 'authors': ['Nan Huang', 'Wenzhao Zheng', 'Chenfeng Xu', 'Kurt Keutzer', 'Shanghang Zhang', 'Angjoo Kanazawa', 'Qianqian Wang'], 'affiliations': ['Peking University', 'UC Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2503.22268.jpg', 'data': {'categories': ['#cv', '#video'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ°Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ÑƒÑ‰Ğ¸Ñ…ÑÑ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ²', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ÑƒÑ‰Ğ¸Ñ…ÑÑ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ°Ğ¼Ğ¸ DINO Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ SAM2 Ğ´Ğ»Ñ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ Ğ¼Ğ°ÑĞ¾Ğº Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğº Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ÑĞ¼ Ğ¸ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑŒĞ½Ğ¾Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ….'}, 'en': {'title': 'Revolutionizing Moving Object Segmentation with Motion-Semantic Integration', 'desc': 'This paper presents a new method for moving object segmentation in videos, which is essential for understanding visual scenes. The authors address the limitations of traditional optical flow techniques that struggle with issues like motion blur and background distractions. Their approach combines long-range motion cues with semantic features from a DINO model and uses an iterative strategy with SAM2 for detailed pixel-level segmentation. The proposed model shows superior performance on various datasets, particularly in complex scenarios involving multiple moving objects.'}, 'zh': {'title': 'åˆ›æ–°ç§»åŠ¨ç‰©ä½“åˆ†å‰²æ–¹æ³•ï¼Œæå‡è§†è§‰ç†è§£èƒ½åŠ›', 'desc': 'ç§»åŠ¨ç‰©ä½“åˆ†å‰²æ˜¯ç†è§£è§†è§‰åœºæ™¯çš„é‡è¦ä»»åŠ¡ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨ã€‚ä¼ ç»Ÿæ–¹æ³•ä¸»è¦ä¾èµ–å…‰æµæ¥æä¾›è¿åŠ¨çº¿ç´¢ï¼Œä½†åœ¨å¤„ç†éƒ¨åˆ†è¿åŠ¨ã€å¤æ‚å˜å½¢ã€è¿åŠ¨æ¨¡ç³Šå’ŒèƒŒæ™¯å¹²æ‰°æ—¶å¸¸å¸¸æ•ˆæœä¸ä½³ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œç»“åˆé•¿è·ç¦»è½¨è¿¹è¿åŠ¨çº¿ç´¢ä¸åŸºäºDINOçš„è¯­ä¹‰ç‰¹å¾ï¼Œå¹¶é€šè¿‡è¿­ä»£æç¤ºç­–ç•¥åˆ©ç”¨SAM2è¿›è¡Œåƒç´ çº§æ©è†œç»†åŒ–ã€‚æˆ‘ä»¬çš„æ¨¡å‹é‡‡ç”¨æ—¶ç©ºè½¨è¿¹æ³¨æ„åŠ›å’Œè¿åŠ¨-è¯­ä¹‰è§£è€¦åµŒå…¥ï¼Œä¼˜å…ˆè€ƒè™‘è¿åŠ¨ï¼ŒåŒæ—¶æ•´åˆè¯­ä¹‰æ”¯æŒï¼Œç»è¿‡å¹¿æ³›æµ‹è¯•åœ¨å¤šæ ·åŒ–æ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.20308', 'title': 'Perceptually Accurate 3D Talking Head Generation: New Definitions,\n  Speech-Mesh Representation, and Evaluation Metrics', 'url': 'https://huggingface.co/papers/2503.20308', 'abstract': 'Recent advancements in speech-driven 3D talking head generation have made significant progress in lip synchronization. However, existing models still struggle to capture the perceptual alignment between varying speech characteristics and corresponding lip movements. In this work, we claim that three criteria -- Temporal Synchronization, Lip Readability, and Expressiveness -- are crucial for achieving perceptually accurate lip movements. Motivated by our hypothesis that a desirable representation space exists to meet these three criteria, we introduce a speech-mesh synchronized representation that captures intricate correspondences between speech signals and 3D face meshes. We found that our learned representation exhibits desirable characteristics, and we plug it into existing models as a perceptual loss to better align lip movements to the given speech. In addition, we utilize this representation as a perceptual metric and introduce two other physically grounded lip synchronization metrics to assess how well the generated 3D talking heads align with these three criteria. Experiments show that training 3D talking head generation models with our perceptual loss significantly improve all three aspects of perceptually accurate lip synchronization. Codes and datasets are available at https://perceptual-3d-talking-head.github.io/.', 'score': 8, 'issue_id': 2979, 'pub_date': '2025-03-26', 'pub_date_card': {'ru': '26 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 26', 'zh': '3æœˆ26æ—¥'}, 'hash': '1b5ec45782117fb1', 'authors': ['Lee Chae-Yeon', 'Oh Hyun-Bin', 'Han EunGi', 'Kim Sung-Bin', 'Suekyeong Nam', 'Tae-Hyun Oh'], 'affiliations': ['Dept. of Electrical Engineering, POSTECH', 'Grad. School of AI, POSTECH', 'KRAFTON', 'School of Computing, KAIST'], 'pdf_title_img': 'assets/pdf/title_img/2503.20308.jpg', 'data': {'categories': ['#3d', '#audio', '#training'], 'emoji': 'ğŸ—£ï¸', 'ru': {'title': 'ĞĞ¾Ğ²Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµÑ‡Ğ¸ Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ³ÑƒĞ± Ğ² 3D', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰Ğ¸Ñ… Ğ³Ğ¾Ğ»Ğ¾Ğ², ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ñ… Ñ€ĞµÑ‡ÑŒÑ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ€ĞµÑ‡Ğ¸ Ğ¸ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ğ¾Ğ¹ ÑĞµÑ‚ĞºĞ¸ Ğ»Ğ¸Ñ†Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ³ÑƒĞ± Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ°Ğ¼ Ñ€ĞµÑ‡Ğ¸. Ğ­Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ ĞºĞ°Ğº Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¿ĞµÑ€Ñ†ĞµĞ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ñ‚Ğ°Ğº Ğ¸ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ³ÑƒĞ±. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ Ñ‚Ñ€ĞµĞ¼ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸ÑĞ¼: Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ñ‡Ğ¸Ñ‚Ğ°ĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ Ğ³ÑƒĞ±Ğ°Ğ¼ Ğ¸ Ğ²Ñ‹Ñ€Ğ°Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Enhancing Lip Synchronization in 3D Talking Heads', 'desc': 'This paper addresses the challenges in generating 3D talking heads that accurately synchronize lip movements with speech. The authors propose that achieving Temporal Synchronization, Lip Readability, and Expressiveness is essential for realistic lip movements. They introduce a novel speech-mesh synchronized representation that effectively captures the relationship between speech signals and 3D facial movements. By integrating this representation as a perceptual loss in existing models, they demonstrate significant improvements in lip synchronization quality through experimental validation.'}, 'zh': {'title': 'æå‡3Dè¯´è¯å¤´å”‡éƒ¨åŒæ­¥çš„å…³é”®æ ‡å‡†', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†è¯­éŸ³é©±åŠ¨çš„3Dè¯´è¯å¤´ç”Ÿæˆä¸­çš„å”‡éƒ¨åŒæ­¥é—®é¢˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸‰ä¸ªå…³é”®æ ‡å‡†ï¼šæ—¶é—´åŒæ­¥ã€å”‡éƒ¨å¯è¯»æ€§å’Œè¡¨ç°åŠ›ï¼Œä»¥å®ç°æ„ŸçŸ¥ä¸Šå‡†ç¡®çš„å”‡éƒ¨è¿åŠ¨ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§è¯­éŸ³-ç½‘æ ¼åŒæ­¥è¡¨ç¤ºï¼Œèƒ½å¤Ÿæ•æ‰è¯­éŸ³ä¿¡å·ä¸3Dé¢éƒ¨ç½‘æ ¼ä¹‹é—´çš„å¤æ‚å¯¹åº”å…³ç³»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨æˆ‘ä»¬çš„æ„ŸçŸ¥æŸå¤±è®­ç»ƒ3Dè¯´è¯å¤´ç”Ÿæˆæ¨¡å‹æ˜¾è‘—æé«˜äº†å”‡éƒ¨åŒæ­¥çš„å‡†ç¡®æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.17827', 'title': '4D-Bench: Benchmarking Multi-modal Large Language Models for 4D Object\n  Understanding', 'url': 'https://huggingface.co/papers/2503.17827', 'abstract': 'Multimodal Large Language Models (MLLMs) have demonstrated impressive 2D image/video understanding capabilities. However, there are no publicly standardized benchmarks to assess the abilities of MLLMs in understanding the 4D objects (3D objects with temporal evolution over time). In this paper, we introduce 4D-Bench, the first benchmark to evaluate the capabilities of MLLMs in 4D object understanding, featuring tasks in 4D object Question Answering (4D object QA) and 4D object captioning. 4D-Bench provides 4D objects with diverse categories, high-quality annotations, and tasks necessitating multi-view spatial-temporal understanding, different from existing 2D image/video-based benchmarks. With 4D-Bench, we evaluate a wide range of open-source and closed-source MLLMs. The results from the 4D object captioning experiment indicate that MLLMs generally exhibit weaker temporal understanding compared to their appearance understanding, notably, while open-source models approach closed-source performance in appearance understanding, they show larger performance gaps in temporal understanding. 4D object QA yields surprising findings: even with simple single-object videos, MLLMs perform poorly, with state-of-the-art GPT-4o achieving only 63\\% accuracy compared to the human baseline of 91\\%. These findings highlight a substantial gap in 4D object understanding and the need for further advancements in MLLMs.', 'score': 6, 'issue_id': 2978, 'pub_date': '2025-03-22', 'pub_date_card': {'ru': '22 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 22', 'zh': '3æœˆ22æ—¥'}, 'hash': '4c510d164c81f13e', 'authors': ['Wenxuan Zhu', 'Bing Li', 'Cheng Zheng', 'Jinjie Mai', 'Jun Chen', 'Letian Jiang', 'Abdullah Hamdi', 'Sara Rojas Martinez', 'Chia-Wen Lin', 'Mohamed Elhoseiny', 'Bernard Ghanem'], 'affiliations': ['King Abdullah University of Science and Technology', 'National Tsing Hua University', 'University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2503.17827.jpg', 'data': {'categories': ['#benchmark', '#3d', '#games', '#video', '#multimodal', '#open_source'], 'emoji': 'ğŸ•°ï¸', 'ru': {'title': '4D-Bench: Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ MLLM Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ²', 'desc': '4D-Bench - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ 4D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¿Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ğ¼ Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¾ 4D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ñ… Ğ¸ Ğ¸Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ, Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ MLLM Ñ…ÑƒĞ¶Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°ÑÑ‚ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ°ÑĞ¿ĞµĞºÑ‚Ñ‹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸, Ğ¿Ñ€Ğ¸Ñ‡ĞµĞ¼ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ñ‚ÑÑ‚Ğ°ÑÑ‚ Ğ¾Ñ‚ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸Ğº. Ğ”Ğ°Ğ¶Ğµ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº GPT-4o, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ½Ğ¸Ğ·ĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ 4D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼ ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ¼.'}, 'en': {'title': 'Bridging the Gap in 4D Object Understanding for MLLMs', 'desc': 'This paper introduces 4D-Bench, a new benchmark designed to evaluate Multimodal Large Language Models (MLLMs) on their understanding of 4D objects, which are 3D objects that change over time. The benchmark includes tasks such as 4D object Question Answering and 4D object captioning, focusing on multi-view spatial-temporal understanding. The evaluation reveals that MLLMs struggle with temporal understanding, showing significant performance gaps compared to their ability to understand object appearance. Overall, the findings indicate a critical need for improvements in MLLMs to enhance their capabilities in 4D object comprehension.'}, 'zh': {'title': 'å››ç»´ç‰©ä½“ç†è§£çš„æ–°åŸºå‡†ï¼š4D-Bench', 'desc': 'å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨äºŒç»´å›¾åƒå’Œè§†é¢‘ç†è§£æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†ç¼ºä¹è¯„ä¼°å››ç»´ç‰©ä½“ç†è§£èƒ½åŠ›çš„æ ‡å‡†åŸºå‡†ã€‚æœ¬æ–‡æå‡ºäº†4D-Benchï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç”¨äºè¯„ä¼°MLLMsåœ¨å››ç»´ç‰©ä½“ç†è§£èƒ½åŠ›çš„åŸºå‡†ï¼ŒåŒ…å«å››ç»´ç‰©ä½“é—®ç­”å’Œå››ç»´ç‰©ä½“æè¿°ç­‰ä»»åŠ¡ã€‚4D-Benchæä¾›äº†å¤šæ ·åŒ–çš„å››ç»´ç‰©ä½“ç±»åˆ«å’Œé«˜è´¨é‡çš„æ³¨é‡Šï¼Œè¦æ±‚æ¨¡å‹å…·å¤‡å¤šè§†è§’çš„æ—¶ç©ºç†è§£èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMLLMsåœ¨æ—¶é—´ç†è§£æ–¹é¢æ™®éè¾ƒå¼±ï¼Œå°¤å…¶æ˜¯å¼€æºæ¨¡å‹åœ¨å¤–è§‚ç†è§£ä¸Šæ¥è¿‘é—­æºæ¨¡å‹ï¼Œä½†åœ¨æ—¶é—´ç†è§£ä¸Šå­˜åœ¨è¾ƒå¤§å·®è·ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.22329', 'title': 'A Refined Analysis of Massive Activations in LLMs', 'url': 'https://huggingface.co/papers/2503.22329', 'abstract': 'Motivated in part by their relevance for low-precision training and quantization, massive activations in large language models (LLMs) have recently emerged as a topic of interest. However, existing analyses are limited in scope, and generalizability across architectures is unclear. This paper helps address some of these gaps by conducting an analysis of massive activations across a broad range of LLMs, including both GLU-based and non-GLU-based architectures. Our findings challenge several prior assumptions, most importantly: (1) not all massive activations are detrimental, i.e. suppressing them does not lead to an explosion of perplexity or a collapse in downstream task performance; (2) proposed mitigation strategies such as Attention KV bias are model-specific and ineffective in certain cases. We consequently investigate novel hybrid mitigation strategies; in particular pairing Target Variance Rescaling (TVR) with Attention KV bias or Dynamic Tanh (DyT) successfully balances the mitigation of massive activations with preserved downstream model performance in the scenarios we investigated. Our code is available at: https://github.com/bluorion-com/refine_massive_activations.', 'score': 5, 'issue_id': 2972, 'pub_date': '2025-03-28', 'pub_date_card': {'ru': '28 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 28', 'zh': '3æœˆ28æ—¥'}, 'hash': '025b5484847cd3d9', 'authors': ['Louis Owen', 'Nilabhra Roy Chowdhury', 'Abhay Kumar', 'Fabian GÃ¼ra'], 'affiliations': ['BluOrion'], 'pdf_title_img': 'assets/pdf/title_img/2503.22329.jpg', 'data': {'categories': ['#optimization', '#architecture', '#long_context', '#inference', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ğ¼Ğ°ÑÑĞ¸Ğ²Ğ½Ñ‹Ğµ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸ Ğ² LLM: Ğ½Ğµ Ğ²ÑĞµ Ñ‚Ğ°Ğº Ğ¾Ğ´Ğ½Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ½Ğ¾', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ°ÑÑĞ¸Ğ²Ğ½Ñ‹Ñ… Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€ LLM, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ GLU Ğ¸ Ğ±ĞµĞ· Ğ½ĞµĞ³Ğ¾. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¾Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€Ğ³Ğ°ÑÑ‚ Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ, Ñ‡Ñ‚Ğ¾ Ğ½Ğµ Ğ²ÑĞµ Ğ¼Ğ°ÑÑĞ¸Ğ²Ğ½Ñ‹Ğµ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ€ĞµĞ´Ğ½Ñ‹, Ğ° ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ ÑĞ¼ÑĞ³Ñ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ½ĞµÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹ Ğ² Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ»ÑƒÑ‡Ğ°ÑÑ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰Ğ¸Ğµ Target Variance Rescaling Ñ Attention KV bias Ğ¸Ğ»Ğ¸ Dynamic Tanh, Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¼ÑĞ³Ñ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ°ÑÑĞ¸Ğ²Ğ½Ñ‹Ñ… Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Balancing Act: Mitigating Massive Activations in LLMs', 'desc': 'This paper explores the phenomenon of massive activations in large language models (LLMs) and their implications for low-precision training and quantization. The authors analyze a variety of LLM architectures to understand the effects of massive activations, revealing that not all of them negatively impact model performance. They challenge previous assumptions by demonstrating that suppressing massive activations does not necessarily lead to worse outcomes in downstream tasks. Additionally, the paper introduces new hybrid strategies that effectively mitigate massive activations while maintaining model performance, particularly through the combination of Target Variance Rescaling and other techniques.'}, 'zh': {'title': 'å¤§æ¿€æ´»çš„æŒ‘æˆ˜ä¸æ–°ç­–ç•¥', 'desc': 'æœ¬æ–‡ç ”ç©¶äº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­çš„å¤§æ¿€æ´»ç°è±¡ï¼Œç‰¹åˆ«å…³æ³¨å…¶åœ¨ä½ç²¾åº¦è®­ç»ƒå’Œé‡åŒ–ä¸­çš„é‡è¦æ€§ã€‚æˆ‘ä»¬åˆ†æäº†å¤šç§LLMæ¶æ„ï¼ŒåŒ…æ‹¬åŸºäºGLUå’ŒéGLUçš„æ¨¡å‹ï¼Œå‘ç°å¹¶éæ‰€æœ‰å¤§æ¿€æ´»éƒ½æ˜¯æœ‰å®³çš„ï¼ŒæŠ‘åˆ¶å®ƒä»¬å¹¶ä¸ä¼šå¯¼è‡´å›°æƒ‘åº¦çš„çˆ†ç‚¸æˆ–ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½çš„å´©æºƒã€‚æˆ‘ä»¬è¿˜å‘ç°ï¼Œç°æœ‰çš„ç¼“è§£ç­–ç•¥å¦‚æ³¨æ„åŠ›KVåç½®åœ¨æŸäº›æƒ…å†µä¸‹æ˜¯æ¨¡å‹ç‰¹å®šçš„ä¸”æ— æ•ˆã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†æ–°çš„æ··åˆç¼“è§£ç­–ç•¥ï¼Œç»“åˆç›®æ ‡æ–¹å·®é‡æ ‡å®šï¼ˆTVRï¼‰ä¸æ³¨æ„åŠ›KVåç½®æˆ–åŠ¨æ€Tanhï¼ˆDyTï¼‰ï¼ŒæˆåŠŸå¹³è¡¡äº†å¤§æ¿€æ´»çš„ç¼“è§£ä¸ä¸‹æ¸¸æ¨¡å‹æ€§èƒ½çš„ä¿æŒã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.21732', 'title': 'SparseFlex: High-Resolution and Arbitrary-Topology 3D Shape Modeling', 'url': 'https://huggingface.co/papers/2503.21732', 'abstract': 'Creating high-fidelity 3D meshes with arbitrary topology, including open surfaces and complex interiors, remains a significant challenge. Existing implicit field methods often require costly and detail-degrading watertight conversion, while other approaches struggle with high resolutions. This paper introduces SparseFlex, a novel sparse-structured isosurface representation that enables differentiable mesh reconstruction at resolutions up to 1024^3 directly from rendering losses. SparseFlex combines the accuracy of Flexicubes with a sparse voxel structure, focusing computation on surface-adjacent regions and efficiently handling open surfaces. Crucially, we introduce a frustum-aware sectional voxel training strategy that activates only relevant voxels during rendering, dramatically reducing memory consumption and enabling high-resolution training. This also allows, for the first time, the reconstruction of mesh interiors using only rendering supervision. Building upon this, we demonstrate a complete shape modeling pipeline by training a variational autoencoder (VAE) and a rectified flow transformer for high-quality 3D shape generation. Our experiments show state-of-the-art reconstruction accuracy, with a ~82% reduction in Chamfer Distance and a ~88% increase in F-score compared to previous methods, and demonstrate the generation of high-resolution, detailed 3D shapes with arbitrary topology. By enabling high-resolution, differentiable mesh reconstruction and generation with rendering losses, SparseFlex significantly advances the state-of-the-art in 3D shape representation and modeling.', 'score': 4, 'issue_id': 2972, 'pub_date': '2025-03-27', 'pub_date_card': {'ru': '27 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 27', 'zh': '3æœˆ27æ—¥'}, 'hash': 'f17ea311cc796683', 'authors': ['Xianglong He', 'Zi-Xin Zou', 'Chia-Hao Chen', 'Yuan-Chen Guo', 'Ding Liang', 'Chun Yuan', 'Wanli Ouyang', 'Yan-Pei Cao', 'Yangguang Li'], 'affiliations': ['The Chinese University of Hong Kong', 'Tsinghua University', 'VAST'], 'pdf_title_img': 'assets/pdf/title_img/2503.21732.jpg', 'data': {'categories': ['#3d'], 'emoji': 'ğŸ§Š', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² 3D-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸: Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ğ°Ñ Ñ‚Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ', 'desc': 'SparseFlex - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚ĞµĞ¹, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ñ€ÑƒĞµĞ¼ÑƒÑ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ 3D-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ¾ 1024^3 Ğ½ĞµĞ¿Ğ¾ÑÑ€ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ°. ĞĞ½ ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Flexicubes Ñ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ²Ğ¾ĞºÑĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¾Ğ¹, Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒÑÑÑŒ Ğ½Ğ° Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…, Ğ¿Ñ€Ğ¸Ğ»ĞµĞ³Ğ°ÑÑ‰Ğ¸Ñ… Ğº Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ’Ğ²ĞµĞ´ĞµĞ½Ğ° ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞµĞºÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¾ĞºÑĞµĞ»ĞµĞ¹ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ñ„Ñ€ÑƒÑÑ‚ÑƒĞ¼Ğ°, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. SparseFlex Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğµ Ñ‡Ğ°ÑÑ‚Ğ¸ ÑĞµÑ‚ĞºĞ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'SparseFlex: Revolutionizing 3D Mesh Reconstruction with Efficiency and Detail', 'desc': 'This paper presents SparseFlex, a new method for creating detailed 3D meshes with complex shapes and open surfaces. It uses a sparse voxel structure to focus on areas near the surface, allowing for efficient high-resolution mesh reconstruction directly from rendering losses. The method introduces a frustum-aware training strategy that reduces memory usage by activating only necessary voxels during rendering. SparseFlex achieves impressive results, outperforming previous techniques in accuracy and enabling the generation of intricate 3D shapes with varying topologies.'}, 'zh': {'title': 'SparseFlexï¼šé«˜åˆ†è¾¨ç‡3Dç½‘æ ¼é‡å»ºçš„æ–°çªç ´', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºSparseFlexçš„æ–°å‹ç¨€ç–ç»“æ„ç­‰å€¼é¢è¡¨ç¤ºæ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³é«˜ä¿çœŸ3Dç½‘æ ¼é‡å»ºä¸­çš„æŒ‘æˆ˜ã€‚SparseFlexèƒ½å¤Ÿç›´æ¥ä»æ¸²æŸ“æŸå¤±ä¸­è¿›è¡Œå¯å¾®åˆ†çš„ç½‘æ ¼é‡å»ºï¼Œæ”¯æŒé«˜è¾¾1024^3çš„åˆ†è¾¨ç‡ã€‚é€šè¿‡ç»“åˆFlexicubesçš„å‡†ç¡®æ€§å’Œç¨€ç–ä½“ç´ ç»“æ„ï¼Œè¯¥æ–¹æ³•æœ‰æ•ˆå¤„ç†å¼€æ”¾è¡¨é¢ï¼Œå¹¶å¼•å…¥äº†åŸºäºè§†é”¥çš„åˆ†æ®µä½“ç´ è®­ç»ƒç­–ç•¥ï¼Œæ˜¾è‘—é™ä½äº†å†…å­˜æ¶ˆè€—ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSparseFlexåœ¨é‡å»ºç²¾åº¦ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ï¼ŒæˆåŠŸç”Ÿæˆäº†å…·æœ‰ä»»æ„æ‹“æ‰‘çš„é«˜åˆ†è¾¨ç‡ã€ç»†èŠ‚ä¸°å¯Œçš„3Då½¢çŠ¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.19108', 'title': 'Your ViT is Secretly an Image Segmentation Model', 'url': 'https://huggingface.co/papers/2503.19108', 'abstract': 'Vision Transformers (ViTs) have shown remarkable performance and scalability across various computer vision tasks. To apply single-scale ViTs to image segmentation, existing methods adopt a convolutional adapter to generate multi-scale features, a pixel decoder to fuse these features, and a Transformer decoder that uses the fused features to make predictions. In this paper, we show that the inductive biases introduced by these task-specific components can instead be learned by the ViT itself, given sufficiently large models and extensive pre-training. Based on these findings, we introduce the Encoder-only Mask Transformer (EoMT), which repurposes the plain ViT architecture to conduct image segmentation. With large-scale models and pre-training, EoMT obtains a segmentation accuracy similar to state-of-the-art models that use task-specific components. At the same time, EoMT is significantly faster than these methods due to its architectural simplicity, e.g., up to 4x faster with ViT-L. Across a range of model sizes, EoMT demonstrates an optimal balance between segmentation accuracy and prediction speed, suggesting that compute resources are better spent on scaling the ViT itself rather than adding architectural complexity. Code: https://www.tue-mps.org/eomt/.', 'score': 4, 'issue_id': 2980, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 24', 'zh': '3æœˆ24æ—¥'}, 'hash': '96e8b32d843c2265', 'authors': ['Tommie Kerssies', 'NiccolÃ² Cavagnero', 'Alexander Hermans', 'Narges Norouzi', 'Giuseppe Averta', 'Bastian Leibe', 'Gijs Dubbelman', 'Daan de Geus'], 'affiliations': ['Eindhoven University of Technology', 'Polytechnic of Turin', 'RWTH Aachen University'], 'pdf_title_img': 'assets/pdf/title_img/2503.19108.jpg', 'data': {'categories': ['#cv', '#architecture', '#training'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'ĞŸÑ€Ğ¾ÑÑ‚Ğ¾Ñ‚Ğ° Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ: ViT Ğ´Ğ»Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ»Ğ¸ÑˆĞ½Ğ¸Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Vision Transformers (ViT). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Encoder-only Mask Transformer (EoMT), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ViT Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ±ĞµĞ· Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ². EoMT Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼ÑƒÑ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸, Ğ½Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ ÑĞ²Ğ¾ĞµĞ¹ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ñ‚Ğµ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸, ViT ÑĞ¿Ğ¾ÑĞ¾Ğ±ĞµĞ½ ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¸Ğ·ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ğµ Ğ¸Ğ½Ğ´ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Streamlining Image Segmentation with EoMT: Faster and Simpler ViTs', 'desc': "This paper presents the Encoder-only Mask Transformer (EoMT), a novel approach for image segmentation using Vision Transformers (ViTs). Instead of relying on additional components like convolutional adapters and pixel decoders, EoMT leverages the ViT's ability to learn necessary features directly from data. The authors demonstrate that with large models and extensive pre-training, EoMT achieves competitive segmentation accuracy while being significantly faster than traditional methods. This suggests that focusing on scaling the ViT architecture can yield better performance than adding complex task-specific elements."}, 'zh': {'title': 'ç®€åŒ–æ¶æ„ï¼Œæå‡åˆ†å‰²æ•ˆç‡', 'desc': 'è§†è§‰å˜æ¢å™¨ï¼ˆViTsï¼‰åœ¨å„ç§è®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¨¡å‹ï¼Œç§°ä¸ºä»…ç¼–ç å™¨æ©ç å˜æ¢å™¨ï¼ˆEoMTï¼‰ï¼Œå®ƒåˆ©ç”¨ç®€å•çš„ViTæ¶æ„è¿›è¡Œå›¾åƒåˆ†å‰²ï¼Œè€Œæ— éœ€é¢å¤–çš„ä»»åŠ¡ç‰¹å®šç»„ä»¶ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒViTå¯ä»¥é€šè¿‡å¤§è§„æ¨¡æ¨¡å‹å’Œå……åˆ†çš„é¢„è®­ç»ƒæ¥å­¦ä¹ ä»»åŠ¡æ‰€éœ€çš„åç½®ã€‚EoMTåœ¨åˆ†å‰²ç²¾åº¦å’Œé¢„æµ‹é€Ÿåº¦ä¹‹é—´å®ç°äº†æœ€ä½³å¹³è¡¡ï¼Œä¸”é€Ÿåº¦æ¯”ä¼ ç»Ÿæ–¹æ³•å¿«ï¼Œæœ€é«˜å¯è¾¾4å€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.21332', 'title': 'ReFeed: Multi-dimensional Summarization Refinement with Reflective\n  Reasoning on Feedback', 'url': 'https://huggingface.co/papers/2503.21332', 'abstract': 'Summarization refinement faces challenges when extending to multi-dimension. In this paper, we introduce ReFeed, a powerful summarization refinement pipeline that enhances multiple dimensions through reflective reasoning on feedback. To achieve this, we release SumFeed-CoT, a large-scale Long-CoT-based dataset optimized for training a lightweight model with reflective reasoning. Our experiments reveal how the number of dimensions, feedback exposure, and reasoning policy influence refinement performance, highlighting reflective reasoning and simultaneously addressing multiple feedback is crucial to mitigate trade-off between dimensions. Furthermore, ReFeed is robust to noisy feedback and feedback order. Lastly, our finding emphasizes that creating data with a proper goal and guideline constitutes a fundamental pillar of effective reasoning. The dataset and model will be released.', 'score': 3, 'issue_id': 2977, 'pub_date': '2025-03-27', 'pub_date_card': {'ru': '27 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 27', 'zh': '3æœˆ27æ—¥'}, 'hash': '4797a440ca75521b', 'authors': ['Taewon Yun', 'Jihwan Oh', 'Hyangsuk Min', 'Yuho Lee', 'Jihwan Bang', 'Jason Cai', 'Hwanjun Song'], 'affiliations': ['Amazon Web Services, AI Labs', 'Korea Advanced Institute of Science and Technology (KAIST)'], 'pdf_title_img': 'assets/pdf/title_img/2503.21332.jpg', 'data': {'categories': ['#dataset', '#long_context', '#training', '#data', '#reasoning'], 'emoji': 'ğŸ”„', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾Ğ¹ ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ ReFeed - Ğ¼Ğ¾Ñ‰Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ SumFeed-CoT Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ²Ğ¸Ğ´Ğ¾Ğ² Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ¸ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ ReFeed Ğº ÑˆÑƒĞ¼Ñƒ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾ ÑÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ñ†ĞµĞ»Ğ¸ Ğ¸ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ° Ğ¿Ñ€Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Enhancing Summarization with Reflective Reasoning', 'desc': 'This paper presents ReFeed, a novel summarization refinement pipeline that improves the quality of summaries across multiple dimensions by utilizing reflective reasoning on feedback. The authors introduce a new dataset called SumFeed-CoT, which is designed to train a lightweight model capable of handling reflective reasoning effectively. Through experiments, they demonstrate that the performance of the summarization refinement is significantly influenced by the number of dimensions, the exposure to feedback, and the reasoning policy employed. The findings suggest that addressing multiple feedback sources simultaneously is essential for optimizing performance while also being resilient to noisy feedback and variations in feedback order.'}, 'zh': {'title': 'åæ€æ€§æ¨ç†æå‡å¤šç»´æ€»ç»“ç²¾ç‚¼', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºReFeedçš„æ€»ç»“ç²¾ç‚¼ç®¡é“ï¼Œæ—¨åœ¨é€šè¿‡åæ€æ€§æ¨ç†æ¥å¢å¼ºå¤šç»´åº¦çš„æ€»ç»“æ•ˆæœã€‚æˆ‘ä»¬å‘å¸ƒäº†ä¸€ä¸ªåä¸ºSumFeed-CoTçš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œä¸“é—¨ç”¨äºè®­ç»ƒè½»é‡çº§æ¨¡å‹ï¼Œä»¥å®ç°åæ€æ€§æ¨ç†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç»´åº¦æ•°é‡ã€åé¦ˆæš´éœ²å’Œæ¨ç†ç­–ç•¥å¯¹ç²¾ç‚¼æ€§èƒ½æœ‰æ˜¾è‘—å½±å“ï¼Œå¼ºè°ƒäº†åæ€æ€§æ¨ç†å’ŒåŒæ—¶å¤„ç†å¤šä¸ªåé¦ˆçš„é‡è¦æ€§ã€‚æœ€åï¼Œæˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œåˆ›å»ºå…·æœ‰æ˜ç¡®ç›®æ ‡å’ŒæŒ‡å¯¼çš„æ•°æ®æ˜¯æœ‰æ•ˆæ¨ç†çš„åŸºç¡€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.18968', 'title': 'MedAgent-Pro: Towards Multi-modal Evidence-based Medical Diagnosis via\n  Reasoning Agentic Workflow', 'url': 'https://huggingface.co/papers/2503.18968', 'abstract': 'Developing reliable AI systems to assist human clinicians in multi-modal medical diagnosis has long been a key objective for researchers. Recently, Multi-modal Large Language Models (MLLMs) have gained significant attention and achieved success across various domains. With strong reasoning capabilities and the ability to perform diverse tasks based on user instructions, they hold great potential for enhancing medical diagnosis. However, directly applying MLLMs to the medical domain still presents challenges. They lack detailed perception of visual inputs, limiting their ability to perform quantitative image analysis, which is crucial for medical diagnostics. Additionally, MLLMs often exhibit hallucinations and inconsistencies in reasoning, whereas clinical diagnoses must adhere strictly to established criteria. To address these challenges, we propose MedAgent-Pro, an evidence-based reasoning agentic system designed to achieve reliable, explainable, and precise medical diagnoses. This is accomplished through a hierarchical workflow: at the task level, knowledge-based reasoning generate reliable diagnostic plans for specific diseases following retrieved clinical criteria. While at the case level, multiple tool agents process multi-modal inputs, analyze different indicators according to the plan, and provide a final diagnosis based on both quantitative and qualitative evidence. Comprehensive experiments on both 2D and 3D medical diagnosis tasks demonstrate the superiority and effectiveness of MedAgent-Pro, while case studies further highlight its reliability and interpretability. The code is available at https://github.com/jinlab-imvr/MedAgent-Pro.', 'score': 3, 'issue_id': 2973, 'pub_date': '2025-03-21', 'pub_date_card': {'ru': '21 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 21', 'zh': '3æœˆ21æ—¥'}, 'hash': '662e73dea1e23d07', 'authors': ['Ziyue Wang', 'Junde Wu', 'Chang Han Low', 'Yueming Jin'], 'affiliations': ['National University of Singapore', 'University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2503.18968.jpg', 'data': {'categories': ['#hallucinations', '#3d', '#multimodal', '#reasoning', '#interpretability', '#healthcare', '#agents'], 'emoji': 'ğŸ©º', 'ru': {'title': 'MedAgent-Pro: Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ° Ñ Ğ˜Ğ˜', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MedAgent-Pro - Ğ°Ğ³ĞµĞ½Ñ‚Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ¹ Ğ¸ Ğ¾Ğ±ÑŠÑÑĞ½Ğ¸Ğ¼Ğ¾Ğ¹ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ¹ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM). Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ MLLM Ğ² Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½Ğµ, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ ÑĞºĞ»Ğ¾Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸ÑĞ¼. MedAgent-Pro Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ MedAgent-Pro Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… 2D Ğ¸ 3D Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ¹ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸.'}, 'en': {'title': 'MedAgent-Pro: Reliable AI for Accurate Medical Diagnosis', 'desc': 'This paper introduces MedAgent-Pro, a new AI system designed to improve medical diagnosis by combining multi-modal inputs and evidence-based reasoning. It addresses the limitations of existing Multi-modal Large Language Models (MLLMs), which struggle with visual data and often produce unreliable outputs. MedAgent-Pro uses a hierarchical approach, where knowledge-based reasoning creates diagnostic plans and multiple tool agents analyze various indicators to provide accurate diagnoses. Experiments show that MedAgent-Pro outperforms traditional methods in both 2D and 3D medical tasks, demonstrating its reliability and interpretability in clinical settings.'}, 'zh': {'title': 'æå‡åŒ»ç–—è¯Šæ–­çš„å¯é æ€§ä¸å¯è§£é‡Šæ€§', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºMedAgent-Proçš„ç³»ç»Ÿï¼Œæ—¨åœ¨æé«˜å¤šæ¨¡æ€åŒ»ç–—è¯Šæ–­çš„å¯é æ€§å’Œå‡†ç¡®æ€§ã€‚è¯¥ç³»ç»Ÿç»“åˆäº†åŸºäºçŸ¥è¯†çš„æ¨ç†å’Œå¤šå·¥å…·ä»£ç†ï¼Œèƒ½å¤Ÿå¤„ç†å¤šç§è¾“å…¥å¹¶ç”Ÿæˆå¯é çš„è¯Šæ–­è®¡åˆ’ã€‚é€šè¿‡å¯¹2Då’Œ3DåŒ»ç–—è¯Šæ–­ä»»åŠ¡çš„å…¨é¢å®éªŒï¼ŒMedAgent-Proå±•ç¤ºäº†å…¶ä¼˜è¶Šæ€§å’Œæœ‰æ•ˆæ€§ã€‚è¯¥ç³»ç»Ÿä¸ä»…æä¾›äº†ç²¾ç¡®çš„è¯Šæ–­ï¼Œè¿˜å…·å¤‡è‰¯å¥½çš„å¯è§£é‡Šæ€§ï¼Œé€‚åˆä¸´åºŠåº”ç”¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.21779', 'title': 'X^{2}-Gaussian: 4D Radiative Gaussian Splatting for Continuous-time\n  Tomographic Reconstruction', 'url': 'https://huggingface.co/papers/2503.21779', 'abstract': 'Four-dimensional computed tomography (4D CT) reconstruction is crucial for capturing dynamic anatomical changes but faces inherent limitations from conventional phase-binning workflows. Current methods discretize temporal resolution into fixed phases with respiratory gating devices, introducing motion misalignment and restricting clinical practicality. In this paper, We propose X^2-Gaussian, a novel framework that enables continuous-time 4D-CT reconstruction by integrating dynamic radiative Gaussian splatting with self-supervised respiratory motion learning. Our approach models anatomical dynamics through a spatiotemporal encoder-decoder architecture that predicts time-varying Gaussian deformations, eliminating phase discretization. To remove dependency on external gating devices, we introduce a physiology-driven periodic consistency loss that learns patient-specific breathing cycles directly from projections via differentiable optimization. Extensive experiments demonstrate state-of-the-art performance, achieving a 9.93 dB PSNR gain over traditional methods and 2.25 dB improvement against prior Gaussian splatting techniques. By unifying continuous motion modeling with hardware-free period learning, X^2-Gaussian advances high-fidelity 4D CT reconstruction for dynamic clinical imaging. Project website at: https://x2-gaussian.github.io/.', 'score': 2, 'issue_id': 2976, 'pub_date': '2025-03-27', 'pub_date_card': {'ru': '27 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 27', 'zh': '3æœˆ27æ—¥'}, 'hash': '628d77c9c1bdcd9e', 'authors': ['Weihao Yu', 'Yuanhao Cai', 'Ruyi Zha', 'Zhiwen Fan', 'Chenxin Li', 'Yixuan Yuan'], 'affiliations': ['Johns Hopkins University', 'The Australian National University', 'The Chinese University of Hong Kong', 'University of Texas at Austin'], 'pdf_title_img': 'assets/pdf/title_img/2503.21779.jpg', 'data': {'categories': ['#architecture', '#healthcare', '#cv'], 'emoji': 'ğŸ«', 'ru': {'title': 'ĞĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ°Ñ 4D-ĞšĞ¢ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ğ±ĞµĞ· Ğ²Ğ½ĞµÑˆĞ½ĞµĞ¹ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ X^2-Gaussian - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ¹ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ 4D-ĞšĞ¢ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ² ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ñ‹Ñ…Ğ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€Ğ°ÑÑĞµĞ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹Ñ… Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ñ ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ€ĞµÑĞ¿Ğ¸Ñ€Ğ°Ñ‚Ğ¾Ñ€Ğ½Ñ‹Ñ… Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ°-Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ° Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ°Ğ½Ğ°Ñ‚Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¹ Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹Ñ… Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. X^2-Gaussian Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° 9.93 Ğ´Ğ‘ Ğ¿Ğ¾ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞµ PSNR, Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°Ñ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'Revolutionizing 4D CT with Continuous Motion Modeling', 'desc': 'This paper presents X^2-Gaussian, a new method for 4D CT reconstruction that overcomes the limitations of traditional phase-binning techniques. By using a spatiotemporal encoder-decoder architecture, it models dynamic anatomical changes without relying on fixed phases, thus allowing for continuous-time reconstruction. The method incorporates self-supervised learning to understand patient-specific breathing patterns, eliminating the need for external respiratory gating devices. Experimental results show that X^2-Gaussian significantly improves image quality, achieving higher PSNR compared to existing methods.'}, 'zh': {'title': 'X^2-Gaussianï¼šæ— ç¡¬ä»¶çš„é«˜ä¿çœŸ4D CTé‡å»ºæ–°æ–¹æ³•', 'desc': 'å››ç»´è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆ4D CTï¼‰é‡å»ºå¯¹äºæ•æ‰åŠ¨æ€è§£å‰–å˜åŒ–è‡³å…³é‡è¦ï¼Œä½†ä¼ ç»Ÿçš„ç›¸ä½åˆ†ç®±å·¥ä½œæµç¨‹å­˜åœ¨å›ºæœ‰çš„å±€é™æ€§ã€‚ç°æœ‰æ–¹æ³•å°†æ—¶é—´åˆ†è¾¨ç‡ç¦»æ•£åŒ–ä¸ºå›ºå®šç›¸ä½ï¼Œä½¿ç”¨å‘¼å¸é—¨æ§è®¾å¤‡ï¼Œå¯¼è‡´è¿åŠ¨é”™ä½å¹¶é™åˆ¶ä¸´åºŠå®ç”¨æ€§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ¡†æ¶X^2-Gaussianï¼Œé€šè¿‡ç»“åˆåŠ¨æ€è¾å°„é«˜æ–¯ç‚¹äº‘å’Œè‡ªç›‘ç£å‘¼å¸è¿åŠ¨å­¦ä¹ ï¼Œå®ç°è¿ç»­æ—¶é—´çš„4D CTé‡å»ºã€‚æˆ‘ä»¬çš„æ¨¡å‹é€šè¿‡æ—¶ç©ºç¼–ç å™¨-è§£ç å™¨æ¶æ„é¢„æµ‹æ—¶é—´å˜åŒ–çš„é«˜æ–¯å˜å½¢ï¼Œæ¶ˆé™¤äº†ç›¸ä½ç¦»æ•£åŒ–çš„éœ€æ±‚ï¼Œå¹¶å¼•å…¥ç”Ÿç†é©±åŠ¨çš„å‘¨æœŸä¸€è‡´æ€§æŸå¤±ï¼Œç›´æ¥ä»æŠ•å½±ä¸­å­¦ä¹ æ‚£è€…ç‰¹å®šçš„å‘¼å¸å‘¨æœŸã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.21851', 'title': 'On Large Multimodal Models as Open-World Image Classifiers', 'url': 'https://huggingface.co/papers/2503.21851', 'abstract': 'Traditional image classification requires a predefined list of semantic categories. In contrast, Large Multimodal Models (LMMs) can sidestep this requirement by classifying images directly using natural language (e.g., answering the prompt "What is the main object in the image?"). Despite this remarkable capability, most existing studies on LMM classification performance are surprisingly limited in scope, often assuming a closed-world setting with a predefined set of categories. In this work, we address this gap by thoroughly evaluating LMM classification performance in a truly open-world setting. We first formalize the task and introduce an evaluation protocol, defining various metrics to assess the alignment between predicted and ground truth classes. We then evaluate 13 models across 10 benchmarks, encompassing prototypical, non-prototypical, fine-grained, and very fine-grained classes, demonstrating the challenges LMMs face in this task. Further analyses based on the proposed metrics reveal the types of errors LMMs make, highlighting challenges related to granularity and fine-grained capabilities, showing how tailored prompting and reasoning can alleviate them.', 'score': 2, 'issue_id': 2977, 'pub_date': '2025-03-27', 'pub_date_card': {'ru': '27 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 27', 'zh': '3æœˆ27æ—¥'}, 'hash': '1f5e6c94324fef3f', 'authors': ['Alessandro Conti', 'Massimiliano Mancini', 'Enrico Fini', 'Yiming Wang', 'Paolo Rota', 'Elisa Ricci'], 'affiliations': ['Fondazione Bruno Kessler', 'Independent researcher', 'University of Trento'], 'pdf_title_img': 'assets/pdf/title_img/2503.21851.jpg', 'data': {'categories': ['#reasoning', '#alignment', '#benchmark', '#multimodal'], 'emoji': 'ğŸ”', 'ru': {'title': 'ĞÑ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LMM) Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ» Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ¸ÑÑ‚Ğ¸Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ĞºĞ»Ğ°ÑÑĞ°Ğ¼Ğ¸. ĞŸÑ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ Ğ¾Ñ†ĞµĞ½ĞºĞ° 13 Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° 10 Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ñ‚Ğ¾Ñ‚Ğ¸Ğ¿Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ, Ğ½ĞµĞ¿Ñ€Ğ¾Ñ‚Ğ¾Ñ‚Ğ¸Ğ¿Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ĞºĞ»Ğ°ÑÑÑ‹. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸, Ñ ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¼Ğ¸ ÑÑ‚Ğ°Ğ»ĞºĞ¸Ğ²Ğ°ÑÑ‚ÑÑ LMM, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¸ Ğ³Ñ€Ğ°Ğ½ÑƒĞ»ÑÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ°Ñ‚ÑŒ Ñ‚Ğ¾Ğ½ĞºĞ¸Ğµ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸.'}, 'en': {'title': 'Unlocking Open-World Image Classification with LMMs', 'desc': 'This paper explores the capabilities of Large Multimodal Models (LMMs) in image classification without relying on a fixed set of categories. It introduces a new evaluation framework to assess LMM performance in an open-world context, where images can belong to any category. The study evaluates 13 different models across various benchmarks, revealing the difficulties LMMs encounter, particularly with fine-grained classifications. Additionally, it discusses how improved prompting and reasoning techniques can help mitigate these challenges.'}, 'zh': {'title': 'å¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨å¼€æ”¾ä¸–ç•Œä¸­çš„å›¾åƒåˆ†ç±»æŒ‘æˆ˜', 'desc': 'ä¼ ç»Ÿçš„å›¾åƒåˆ†ç±»éœ€è¦é¢„å®šä¹‰çš„è¯­ä¹‰ç±»åˆ«ï¼Œè€Œå¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰å¯ä»¥é€šè¿‡è‡ªç„¶è¯­è¨€ç›´æ¥å¯¹å›¾åƒè¿›è¡Œåˆ†ç±»ã€‚è¿™é¡¹ç ”ç©¶è¯„ä¼°äº†LMMåœ¨å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­çš„åˆ†ç±»æ€§èƒ½ï¼Œå¡«è¡¥äº†ç°æœ‰ç ”ç©¶çš„ç©ºç™½ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§è¯„ä¼°åè®®ï¼Œå®šä¹‰äº†å¤šç§æŒ‡æ ‡æ¥è¯„ä¼°é¢„æµ‹ç±»åˆ«ä¸çœŸå®ç±»åˆ«ä¹‹é—´çš„å¯¹é½æƒ…å†µã€‚é€šè¿‡å¯¹13ä¸ªæ¨¡å‹åœ¨10ä¸ªåŸºå‡†ä¸Šçš„è¯„ä¼°ï¼Œæ­ç¤ºäº†LMMåœ¨ç»†ç²’åº¦åˆ†ç±»ä¸­çš„æŒ‘æˆ˜ï¼Œå¹¶æå‡ºäº†å®šåˆ¶æç¤ºå’Œæ¨ç†çš„æ–¹æ³•æ¥ç¼“è§£è¿™äº›é—®é¢˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.21751', 'title': 'Reconstructing Humans with a Biomechanically Accurate Skeleton', 'url': 'https://huggingface.co/papers/2503.21751', 'abstract': 'In this paper, we introduce a method for reconstructing 3D humans from a single image using a biomechanically accurate skeleton model. To achieve this, we train a transformer that takes an image as input and estimates the parameters of the model. Due to the lack of training data for this task, we build a pipeline to produce pseudo ground truth model parameters for single images and implement a training procedure that iteratively refines these pseudo labels. Compared to state-of-the-art methods for 3D human mesh recovery, our model achieves competitive performance on standard benchmarks, while it significantly outperforms them in settings with extreme 3D poses and viewpoints. Additionally, we show that previous reconstruction methods frequently violate joint angle limits, leading to unnatural rotations. In contrast, our approach leverages the biomechanically plausible degrees of freedom making more realistic joint rotation estimates. We validate our approach across multiple human pose estimation benchmarks. We make the code, models and data available at: https://isshikihugh.github.io/HSMR/', 'score': 0, 'issue_id': 2979, 'pub_date': '2025-03-27', 'pub_date_card': {'ru': '27 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 27', 'zh': '3æœˆ27æ—¥'}, 'hash': 'a715b9755e020ce7', 'authors': ['Yan Xia', 'Xiaowei Zhou', 'Etienne Vouga', 'Qixing Huang', 'Georgios Pavlakos'], 'affiliations': ['The University of Texas at Austin', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.21751.jpg', 'data': {'categories': ['#benchmark', '#3d', '#architecture', '#dataset'], 'emoji': 'ğŸ¦´', 'ru': {'title': 'Ğ‘Ğ¸Ğ¾Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ°Ñ 3D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¿Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ñƒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ 3D-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ»ÑĞ´ĞµĞ¹ Ğ¿Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ñƒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ±Ğ¸Ğ¾Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞºĞµĞ»ĞµÑ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°ĞµÑ‚ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ²Ñ…Ğ¾Ğ´ Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ˜Ğ·-Ğ·Ğ° Ğ½ĞµÑ…Ğ²Ğ°Ñ‚ĞºĞ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¿ÑĞµĞ²Ğ´Ğ¾-Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ Ğ¾Ğ´Ğ¸Ğ½Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼. ĞŸĞ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ 3D-ÑĞµÑ‚ĞºĞ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°, Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¸Ñ… Ğ² ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ñ ÑĞºÑÑ‚Ñ€ĞµĞ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ 3D-Ğ¿Ğ¾Ğ·Ğ°Ğ¼Ğ¸ Ğ¸ Ñ€Ğ°ĞºÑƒÑ€ÑĞ°Ğ¼Ğ¸.'}, 'en': {'title': 'Reconstructing Realistic 3D Humans from Single Images', 'desc': 'This paper presents a novel method for creating 3D models of humans from just one image by using a skeleton model that accurately reflects human biomechanics. The authors train a transformer model that processes the input image to predict the parameters of this skeleton. To address the challenge of limited training data, they develop a pipeline that generates pseudo ground truth parameters, which are refined through an iterative training process. Their approach not only matches the performance of existing 3D human reconstruction methods but also excels in challenging scenarios with extreme poses and viewpoints, while ensuring realistic joint movements.'}, 'zh': {'title': 'ç”Ÿç‰©åŠ›å­¦é©±åŠ¨çš„3Däººç±»é‡å»ºæ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§ä»å•å¼ å›¾åƒé‡å»º3Däººç±»æ¨¡å‹çš„æ–¹æ³•ï¼Œä½¿ç”¨ç”Ÿç‰©åŠ›å­¦å‡†ç¡®çš„éª¨éª¼æ¨¡å‹ã€‚æˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªå˜æ¢å™¨ï¼Œè¾“å…¥å›¾åƒå¹¶ä¼°è®¡æ¨¡å‹å‚æ•°ã€‚ç”±äºè®­ç»ƒæ•°æ®çš„ä¸è¶³ï¼Œæˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªç®¡é“æ¥ç”Ÿæˆå•å¼ å›¾åƒçš„ä¼ªçœŸå®æ¨¡å‹å‚æ•°ï¼Œå¹¶å®æ–½äº†ä¸€ä¸ªè¿­ä»£ä¼˜åŒ–ä¼ªæ ‡ç­¾çš„è®­ç»ƒè¿‡ç¨‹ã€‚ä¸ç°æœ‰çš„3Däººç±»ç½‘æ ¼æ¢å¤æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºç«äº‰åŠ›ï¼Œå°¤å…¶åœ¨æç«¯3Då§¿åŠ¿å’Œè§†è§’ä¸‹è¡¨ç°æ›´ä¸ºä¼˜è¶Šã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.01785', 'title': 'Visual-RFT: Visual Reinforcement Fine-Tuning', 'url': 'https://huggingface.co/papers/2503.01785', 'abstract': "Reinforcement Fine-Tuning (RFT) in Large Reasoning Models like OpenAI o1 learns from feedback on its answers, which is especially useful in applications when fine-tuning data is scarce. Recent open-source work like DeepSeek-R1 demonstrates that reinforcement learning with verifiable reward is one key direction in reproducing o1. While the R1-style model has demonstrated success in language models, its application in multi-modal domains remains under-explored. This work introduces Visual Reinforcement Fine-Tuning (Visual-RFT), which further extends the application areas of RFT on visual tasks. Specifically, Visual-RFT first uses Large Vision-Language Models (LVLMs) to generate multiple responses containing reasoning tokens and final answers for each input, and then uses our proposed visual perception verifiable reward functions to update the model via the policy optimization algorithm such as Group Relative Policy Optimization (GRPO). We design different verifiable reward functions for different perception tasks, such as the Intersection over Union (IoU) reward for object detection. Experimental results on fine-grained image classification, few-shot object detection, reasoning grounding, as well as open-vocabulary object detection benchmarks show the competitive performance and advanced generalization ability of Visual-RFT compared with Supervised Fine-tuning (SFT). For example, Visual-RFT improves accuracy by 24.3% over the baseline in one-shot fine-grained image classification with around 100 samples. In few-shot object detection, Visual-RFT also exceeds the baseline by 21.9 on COCO's two-shot setting and 15.4 on LVIS. Our Visual-RFT represents a paradigm shift in fine-tuning LVLMs, offering a data-efficient, reward-driven approach that enhances reasoning and adaptability for domain-specific tasks.", 'score': 43, 'issue_id': 2511, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 3', 'zh': '3æœˆ3æ—¥'}, 'hash': 'ef2e10eb59ab7743', 'authors': ['Ziyu Liu', 'Zeyi Sun', 'Yuhang Zang', 'Xiaoyi Dong', 'Yuhang Cao', 'Haodong Duan', 'Dahua Lin', 'Jiaqi Wang'], 'affiliations': ['Shanghai Artificial Intelligence Laboratory', 'Shanghai Jiaotong University', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2503.01785.jpg', 'data': {'categories': ['#multimodal', '#open_source', '#cv', '#optimization', '#rlhf', '#reasoning', '#training', '#rl'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'Visual-RFT: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Visual Reinforcement Fine-Tuning (Visual-RFT) - Ğ¼ĞµÑ‚Ğ¾Ğ´, Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑÑÑ‰Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. Visual-RFT Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Visual-RFT Ğ½Ğ°Ğ´ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼ Supervised Fine-tuning Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ·ĞµĞ¼Ğ»ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ĞµĞ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ².'}, 'en': {'title': 'Revolutionizing Visual Learning with Reinforcement Fine-Tuning', 'desc': "This paper introduces Visual Reinforcement Fine-Tuning (Visual-RFT), a method that enhances large vision-language models (LVLMs) by using reinforcement learning to improve their performance on visual tasks. Visual-RFT generates multiple responses for each input and employs verifiable reward functions to optimize the model's policy, making it particularly effective in scenarios with limited fine-tuning data. The approach demonstrates significant improvements in tasks like fine-grained image classification and object detection, outperforming traditional supervised fine-tuning methods. Overall, Visual-RFT represents a novel, efficient way to fine-tune LVLMs, focusing on reasoning and adaptability in specific domains."}, 'zh': {'title': 'è§†è§‰å¼ºåŒ–å¾®è°ƒï¼šæå‡æ¨ç†ä¸é€‚åº”æ€§çš„åˆ›æ–°æ–¹æ³•', 'desc': 'å¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰åœ¨å¤§å‹æ¨ç†æ¨¡å‹ä¸­é€šè¿‡åé¦ˆå­¦ä¹ ï¼Œç‰¹åˆ«é€‚ç”¨äºå¾®è°ƒæ•°æ®ç¨€ç¼ºçš„åº”ç”¨åœºæ™¯ã€‚æœ¬æ–‡æå‡ºçš„è§†è§‰å¼ºåŒ–å¾®è°ƒï¼ˆVisual-RFTï¼‰æ‰©å±•äº†RFTåœ¨è§†è§‰ä»»åŠ¡ä¸­çš„åº”ç”¨ï¼Œåˆ©ç”¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ç”Ÿæˆå¤šç§å“åº”ï¼Œå¹¶é€šè¿‡å¯éªŒè¯çš„è§†è§‰æ„ŸçŸ¥å¥–åŠ±å‡½æ•°è¿›è¡Œæ¨¡å‹æ›´æ–°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVisual-RFTåœ¨ç»†ç²’åº¦å›¾åƒåˆ†ç±»å’Œå°‘æ ·æœ¬ç›®æ ‡æ£€æµ‹ç­‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œç›¸è¾ƒäºä¼ ç»Ÿçš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ–¹æ³•ï¼Œå‡†ç¡®ç‡æ˜¾è‘—æé«˜ã€‚Visual-RFTä»£è¡¨äº†ä¸€ç§æ–°çš„å¾®è°ƒèŒƒå¼ï¼Œæä¾›äº†ä¸€ç§æ•°æ®é«˜æ•ˆã€ä»¥å¥–åŠ±é©±åŠ¨çš„æ–¹æ³•ï¼Œå¢å¼ºäº†æ¨¡å‹åœ¨ç‰¹å®šé¢†åŸŸä»»åŠ¡ä¸­çš„æ¨ç†èƒ½åŠ›å’Œé€‚åº”æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.01743', 'title': 'Phi-4-Mini Technical Report: Compact yet Powerful Multimodal Language Models via Mixture-of-LoRAs', 'url': 'https://huggingface.co/papers/2503.01743', 'abstract': 'We introduce Phi-4-Mini and Phi-4-Multimodal, compact yet highly capable language and multimodal models. Phi-4-Mini is a 3.8-billion-parameter language model trained on high-quality web and synthetic data, significantly outperforming recent open-source models of similar size and matching the performance of models twice its size on math and coding tasks requiring complex reasoning. This achievement is driven by a carefully curated synthetic data recipe emphasizing high-quality math and coding datasets. Compared to its predecessor, Phi-3.5-Mini, Phi-4-Mini features an expanded vocabulary size of 200K tokens to better support multilingual applications, as well as group query attention for more efficient long-sequence generation. Phi-4-Multimodal is a multimodal model that integrates text, vision, and speech/audio input modalities into a single model. Its novel modality extension approach leverages LoRA adapters and modality-specific routers to allow multiple inference modes combining various modalities without interference. For example, it now ranks first in the OpenASR leaderboard to date, although the LoRA component of the speech/audio modality has just 460 million parameters. Phi-4-Multimodal supports scenarios involving (vision + language), (vision + speech), and (speech/audio) inputs, outperforming larger vision-language and speech-language models on a wide range of tasks. Additionally, we experiment to further train Phi-4-Mini to enhance its reasoning capabilities. Despite its compact 3.8-billion-parameter size, this experimental version achieves reasoning performance on par with or surpassing significantly larger models, including DeepSeek-R1-Distill-Qwen-7B and DeepSeek-R1-Distill-Llama-8B.', 'score': 38, 'issue_id': 2511, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 3', 'zh': '3æœˆ3æ—¥'}, 'hash': 'fb054d6547a4a4fb', 'authors': ['Abdelrahman Abouelenin', 'Atabak Ashfaq', 'Adam Atkinson', 'Hany Awadalla', 'Nguyen Bach', 'Jianmin Bao', 'Alon Benhaim', 'Martin Cai', 'Vishrav Chaudhary', 'Congcong Chen', 'Dong Chen', 'Dongdong Chen', 'Junkun Chen', 'Weizhu Chen', 'Yen-Chun Chen', 'Yi-ling Chen', 'Qi Dai', 'Xiyang Dai', 'Ruchao Fan', 'Mei Gao', 'Min Gao', 'Amit Garg', 'Abhishek Goswami', 'Junheng Hao', 'Amr Hendy', 'Yuxuan Hu', 'Xin Jin', 'Mahmoud Khademi', 'Dongwoo Kim', 'Young Jin Kim', 'Gina Lee', 'Jinyu Li', 'Yunsheng Li', 'Chen Liang', 'Xihui Lin', 'Zeqi Lin', 'Mengchen Liu', 'Yang Liu', 'Gilsinia Lopez', 'Chong Luo', 'Piyush Madan', 'Vadim Mazalov', 'Ali Mousavi', 'Anh Nguyen', 'Jing Pan', 'Daniel Perez-Becker', 'Jacob Platin', 'Thomas Portet', 'Kai Qiu', 'Bo Ren', 'Liliang Ren', 'Sambuddha Roy', 'Ning Shang', 'Yelong Shen', 'Saksham Singhal', 'Subhojit Som', 'Xia Song', 'Tetyana Sych', 'Praneetha Vaddamanu', 'Shuohang Wang', 'Yiming Wang', 'Zhenghao Wang', 'Haibin Wu', 'Haoran Xu', 'Weijian Xu', 'Yifan Yang', 'Ziyi Yang', 'Donghan Yu', 'Ishmam Zabir', 'Jianwen Zhang', 'Li Lyna Zhang', 'Yunan Zhang', 'Xiren Zhou'], 'affiliations': ['Microsoft'], 'pdf_title_img': 'assets/pdf/title_img/2503.01743.jpg', 'data': {'categories': ['#multimodal', '#small_models', '#data', '#agi', '#synthetic', '#long_context', '#optimization', '#dataset', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞšĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸: Ğ¿Ñ€Ğ¾Ñ€Ñ‹Ğ² Ğ² ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ˜Ğ˜', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ñ‹ Ğ´Ğ²Ğµ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: Phi-4-Mini Ğ¸ Phi-4-Multimodal. Phi-4-Mini - ÑÑ‚Ğ¾ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ 3,8 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ²ĞµĞ±-Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Phi-4-Multimodal - ÑÑ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ°Ñ Ñ‚ĞµĞºÑÑ‚, Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ñ€ĞµÑ‡ÑŒ/Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ² ĞµĞ´Ğ¸Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ LoRA-Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ğ¾Ğ². ĞĞ±Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½ĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° ÑĞ²Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ….'}, 'en': {'title': 'Compact Models, Superior Performance!', 'desc': 'The paper presents Phi-4-Mini and Phi-4-Multimodal, two advanced models designed for language and multimodal tasks. Phi-4-Mini, with 3.8 billion parameters, excels in math and coding tasks by utilizing a high-quality synthetic data approach and an expanded vocabulary of 200K tokens. Phi-4-Multimodal integrates text, vision, and audio inputs, employing innovative techniques like LoRA adapters for efficient multi-modal processing. Both models demonstrate superior performance compared to larger counterparts, showcasing their effectiveness in complex reasoning and diverse input scenarios.'}, 'zh': {'title': 'ç´§å‡‘å¼ºå¤§çš„å¤šæ¨¡æ€æ¨¡å‹Phi-4ç³»åˆ—', 'desc': 'æˆ‘ä»¬ä»‹ç»äº†Phi-4-Miniå’ŒPhi-4-Multimodalè¿™ä¸¤ç§ç´§å‡‘è€Œå¼ºå¤§çš„è¯­è¨€å’Œå¤šæ¨¡æ€æ¨¡å‹ã€‚Phi-4-Miniæ˜¯ä¸€ä¸ªæ‹¥æœ‰38äº¿å‚æ•°çš„è¯­è¨€æ¨¡å‹ï¼Œç»è¿‡é«˜è´¨é‡çš„ç½‘ç»œå’Œåˆæˆæ•°æ®è®­ç»ƒï¼Œåœ¨æ•°å­¦å’Œç¼–ç ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºåŒç±»å¼€æºæ¨¡å‹ï¼Œå¹¶ä¸”åœ¨å¤æ‚æ¨ç†æ–¹é¢ä¸ä¸¤å€äºå…¶è§„æ¨¡çš„æ¨¡å‹ç›¸å½“ã€‚ç›¸æ¯”äºå‰èº«Phi-3.5-Miniï¼ŒPhi-4-Miniæ‰©å±•äº†è¯æ±‡é‡ï¼Œæ”¯æŒå¤šè¯­è¨€åº”ç”¨ï¼Œå¹¶é‡‡ç”¨äº†ç»„æŸ¥è¯¢æ³¨æ„åŠ›æœºåˆ¶ä»¥æé«˜é•¿åºåˆ—ç”Ÿæˆçš„æ•ˆç‡ã€‚Phi-4-Multimodalåˆ™æ˜¯ä¸€ä¸ªå¤šæ¨¡æ€æ¨¡å‹ï¼Œèƒ½å¤Ÿå°†æ–‡æœ¬ã€è§†è§‰å’Œè¯­éŸ³/éŸ³é¢‘è¾“å…¥æ•´åˆåˆ°ä¸€ä¸ªæ¨¡å‹ä¸­ï¼Œæ”¯æŒå¤šç§æ¨ç†æ¨¡å¼ï¼Œä¸”åœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¶…è¶Šäº†æ›´å¤§çš„è§†è§‰-è¯­è¨€å’Œè¯­éŸ³-è¯­è¨€æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.01774', 'title': 'Difix3D+: Improving 3D Reconstructions with Single-Step Diffusion Models', 'url': 'https://huggingface.co/papers/2503.01774', 'abstract': 'Neural Radiance Fields and 3D Gaussian Splatting have revolutionized 3D reconstruction and novel-view synthesis task. However, achieving photorealistic rendering from extreme novel viewpoints remains challenging, as artifacts persist across representations. In this work, we introduce Difix3D+, a novel pipeline designed to enhance 3D reconstruction and novel-view synthesis through single-step diffusion models. At the core of our approach is Difix, a single-step image diffusion model trained to enhance and remove artifacts in rendered novel views caused by underconstrained regions of the 3D representation. Difix serves two critical roles in our pipeline. First, it is used during the reconstruction phase to clean up pseudo-training views that are rendered from the reconstruction and then distilled back into 3D. This greatly enhances underconstrained regions and improves the overall 3D representation quality. More importantly, Difix also acts as a neural enhancer during inference, effectively removing residual artifacts arising from imperfect 3D supervision and the limited capacity of current reconstruction models. Difix3D+ is a general solution, a single model compatible with both NeRF and 3DGS representations, and it achieves an average 2times improvement in FID score over baselines while maintaining 3D consistency.', 'score': 29, 'issue_id': 2512, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 3', 'zh': '3æœˆ3æ—¥'}, 'hash': '39af2f882aef9afb', 'authors': ['Jay Zhangjie Wu', 'Yuxuan Zhang', 'Haithem Turki', 'Xuanchi Ren', 'Jun Gao', 'Mike Zheng Shou', 'Sanja Fidler', 'Zan Gojcic', 'Huan Ling'], 'affiliations': ['NVIDIA', 'National University of Singapore', 'University of Toronto', 'Vector Institute'], 'pdf_title_img': 'assets/pdf/title_img/2503.01774.jpg', 'data': {'categories': ['#3d', '#diffusion'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'ĞĞ´Ğ½Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ°Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ñ Ğ´Ğ»Ñ Ñ„Ğ¾Ñ‚Ğ¾Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ 3D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸', 'desc': 'Difix3D+ - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ 3D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ². Ğ’ ĞµĞ³Ğ¾ Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ»ĞµĞ¶Ğ¸Ñ‚ Difix - Ğ¾Ğ´Ğ½Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ‚ÑŒ Ğ¸ ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚ÑŒ Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ñ‹ Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´Ğ°Ñ…. Difix Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ ĞºĞ°Ğº Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¾Ñ‡Ğ¸ÑÑ‚ĞºĞ¸ Ğ¿ÑĞµĞ²Ğ´Ğ¾-Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ²Ğ¸Ğ´Ğ¾Ğ², Ñ‚Ğ°Ğº Ğ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ ÑƒÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ğ¾Ğ². Difix3D+ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼ Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸ NeRF Ğ¸ 3DGS Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ´Ğ²ÑƒĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ FID Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸.'}, 'en': {'title': 'Enhancing 3D Reconstruction with Difix3D+', 'desc': 'This paper presents Difix3D+, a new method for improving 3D reconstruction and novel-view synthesis using single-step diffusion models. The core component, Difix, is an image diffusion model that enhances rendered views by removing artifacts caused by underconstrained areas in 3D representations. It plays a dual role by cleaning up pseudo-training views during reconstruction and acting as a neural enhancer during inference to eliminate residual artifacts. Difix3D+ is versatile, working with both Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS), and it significantly improves the quality of 3D representations, achieving a 2x better FID score compared to existing methods.'}, 'zh': {'title': 'Difix3D+: æå‡3Dé‡å»ºä¸æ–°è§†è§’åˆæˆçš„åˆ©å™¨', 'desc': 'Neural Radiance Fieldsï¼ˆNeRFï¼‰å’Œ3Dé«˜æ–¯ç‚¹äº‘ï¼ˆ3D Gaussian Splattingï¼‰åœ¨3Dé‡å»ºå’Œæ–°è§†è§’åˆæˆä»»åŠ¡ä¸­å–å¾—äº†é‡å¤§è¿›å±•ã€‚ç„¶è€Œï¼Œä»æç«¯æ–°è§†è§’å®ç°çœŸå®æ„Ÿæ¸²æŸ“ä»ç„¶é¢ä¸´æŒ‘æˆ˜ï¼Œå› ä¸ºåœ¨è¡¨ç¤ºä¸­å­˜åœ¨ä¼ªå½±ã€‚æˆ‘ä»¬æå‡ºäº†Difix3D+ï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„ç®¡é“ï¼Œæ—¨åœ¨é€šè¿‡å•æ­¥æ‰©æ•£æ¨¡å‹å¢å¼º3Dé‡å»ºå’Œæ–°è§†è§’åˆæˆã€‚Difixä½œä¸ºæ ¸å¿ƒæ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨é‡å»ºé˜¶æ®µæ¸…ç†ä¼ªè®­ç»ƒè§†å›¾ï¼Œå¹¶åœ¨æ¨ç†é˜¶æ®µå»é™¤æ®‹ç•™ä¼ªå½±ï¼Œä»è€Œæ˜¾è‘—æé«˜3Dè¡¨ç¤ºçš„è´¨é‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.01183', 'title': 'DiffRhythm: Blazingly Fast and Embarrassingly Simple End-to-End Full-Length Song Generation with Latent Diffusion', 'url': 'https://huggingface.co/papers/2503.01183', 'abstract': 'Recent advancements in music generation have garnered significant attention, yet existing approaches face critical limitations. Some current generative models can only synthesize either the vocal track or the accompaniment track. While some models can generate combined vocal and accompaniment, they typically rely on meticulously designed multi-stage cascading architectures and intricate data pipelines, hindering scalability. Additionally, most systems are restricted to generating short musical segments rather than full-length songs. Furthermore, widely used language model-based methods suffer from slow inference speeds. To address these challenges, we propose DiffRhythm, the first latent diffusion-based song generation model capable of synthesizing complete songs with both vocal and accompaniment for durations of up to 4m45s in only ten seconds, maintaining high musicality and intelligibility. Despite its remarkable capabilities, DiffRhythm is designed to be simple and elegant: it eliminates the need for complex data preparation, employs a straightforward model structure, and requires only lyrics and a style prompt during inference. Additionally, its non-autoregressive structure ensures fast inference speeds. This simplicity guarantees the scalability of DiffRhythm. Moreover, we release the complete training code along with the pre-trained model on large-scale data to promote reproducibility and further research.', 'score': 18, 'issue_id': 2516, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 3', 'zh': '3æœˆ3æ—¥'}, 'hash': '0370c6364610fd8e', 'authors': ['Ziqian Ning', 'Huakang Chen', 'Yuepeng Jiang', 'Chunbo Hao', 'Guobin Ma', 'Shuai Wang', 'Jixun Yao', 'Lei Xie'], 'affiliations': ['Northwestern Polytechnical University', 'Shenzhen Research Institute of Big Data, The Chinese University of Hong Kong, Shenzhen (CUHK-Shenzhen), China'], 'pdf_title_img': 'assets/pdf/title_img/2503.01183.jpg', 'data': {'categories': ['#diffusion', '#inference', '#dataset', '#open_source', '#audio'], 'emoji': 'ğŸµ', 'ru': {'title': 'DiffRhythm: Ğ‘Ñ‹ÑÑ‚Ñ€Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ñ… Ğ¿ĞµÑĞµĞ½ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸', 'desc': 'DiffRhythm - ÑÑ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿ĞµÑĞµĞ½ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ°Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğµ Ğ¿ĞµÑĞ½Ğ¸ Ñ Ğ²Ğ¾ĞºĞ°Ğ»Ğ¾Ğ¼ Ğ¸ Ğ°ĞºĞºĞ¾Ğ¼Ğ¿Ğ°Ğ½ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ¼ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ´Ğ¾ 4Ğ¼45Ñ Ğ²ÑĞµĞ³Ğ¾ Ğ·Ğ° Ğ´ĞµÑÑÑ‚ÑŒ ÑĞµĞºÑƒĞ½Ğ´. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸Ğ¼ĞµĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ, Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‚ĞµĞºÑÑ‚ Ğ¿ĞµÑĞ½Ğ¸ Ğ¸ ÑÑ‚Ğ¸Ğ»ĞµĞ²ÑƒÑ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºÑƒ Ğ¿Ñ€Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞµ. Ğ‘Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ½ĞµĞ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğµ, DiffRhythm Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ¾Ğ²Ğ°Ğ»Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¸ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹.'}, 'en': {'title': 'DiffRhythm: Fast and Scalable Song Generation with Latent Diffusion', 'desc': "This paper introduces DiffRhythm, a novel music generation model that utilizes latent diffusion techniques to create full-length songs with both vocal and accompaniment tracks. Unlike existing models that are limited to short segments or require complex architectures, DiffRhythm simplifies the process by needing only lyrics and a style prompt for song generation. It achieves high musical quality and intelligibility while significantly improving inference speed, generating songs in just ten seconds. The authors also emphasize the model's scalability and reproducibility by providing the complete training code and pre-trained model for further research."}, 'zh': {'title': 'DiffRhythmï¼šå¿«é€Ÿç”Ÿæˆå®Œæ•´æ­Œæ›²çš„åˆ›æ–°æ¨¡å‹', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„éŸ³ä¹ç”Ÿæˆæ¨¡å‹DiffRhythmï¼Œå®ƒèƒ½å¤Ÿåœ¨çŸ­çŸ­åç§’å†…åˆæˆå®Œæ•´çš„æ­Œæ›²ï¼ŒåŒ…æ‹¬äººå£°å’Œä¼´å¥ï¼Œæ—¶é•¿å¯è¾¾4åˆ†45ç§’ã€‚ä¸ç°æœ‰æ¨¡å‹ç›¸æ¯”ï¼ŒDiffRhythmé‡‡ç”¨æ½œåœ¨æ‰©æ•£æŠ€æœ¯ï¼Œé¿å…äº†å¤æ‚çš„æ•°æ®å‡†å¤‡å’Œå¤šé˜¶æ®µæ¶æ„ï¼Œç¡®ä¿äº†é«˜æ•ˆçš„æ¨ç†é€Ÿåº¦ã€‚è¯¥æ¨¡å‹åªéœ€æ­Œè¯å’Œé£æ ¼æç¤ºå³å¯ç”ŸæˆéŸ³ä¹ï¼Œå…·æœ‰è‰¯å¥½çš„å¯æ‰©å±•æ€§ã€‚æˆ‘ä»¬è¿˜å‘å¸ƒäº†å®Œæ•´çš„è®­ç»ƒä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹ï¼Œä»¥ä¿ƒè¿›ç ”ç©¶çš„å¯é‡å¤æ€§å’Œè¿›ä¸€æ­¥å‘å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.18965', 'title': 'OneRec: Unifying Retrieve and Rank with Generative Recommender and Iterative Preference Alignment', 'url': 'https://huggingface.co/papers/2502.18965', 'abstract': "Recently, generative retrieval-based recommendation systems have emerged as a promising paradigm. However, most modern recommender systems adopt a retrieve-and-rank strategy, where the generative model functions only as a selector during the retrieval stage. In this paper, we propose OneRec, which replaces the cascaded learning framework with a unified generative model. To the best of our knowledge, this is the first end-to-end generative model that significantly surpasses current complex and well-designed recommender systems in real-world scenarios. Specifically, OneRec includes: 1) an encoder-decoder structure, which encodes the user's historical behavior sequences and gradually decodes the videos that the user may be interested in. We adopt sparse Mixture-of-Experts (MoE) to scale model capacity without proportionally increasing computational FLOPs. 2) a session-wise generation approach. In contrast to traditional next-item prediction, we propose a session-wise generation, which is more elegant and contextually coherent than point-by-point generation that relies on hand-crafted rules to properly combine the generated results. 3) an Iterative Preference Alignment module combined with Direct Preference Optimization (DPO) to enhance the quality of the generated results. Unlike DPO in NLP, a recommendation system typically has only one opportunity to display results for each user's browsing request, making it impossible to obtain positive and negative samples simultaneously. To address this limitation, We design a reward model to simulate user generation and customize the sampling strategy. Extensive experiments have demonstrated that a limited number of DPO samples can align user interest preferences and significantly improve the quality of generated results. We deployed OneRec in the main scene of Kuaishou, achieving a 1.6\\% increase in watch-time, which is a substantial improvement.", 'score': 18, 'issue_id': 2515, 'pub_date': '2025-02-26', 'pub_date_card': {'ru': '26 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 26', 'zh': '2æœˆ26æ—¥'}, 'hash': '21c5c80a138c98a0', 'authors': ['Jiaxin Deng', 'Shiyao Wang', 'Kuo Cai', 'Lejian Ren', 'Qigen Hu', 'Weifeng Ding', 'Qiang Luo', 'Guorui Zhou'], 'affiliations': ['KuaiShou Inc. Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2502.18965.jpg', 'data': {'categories': ['#alignment', '#rlhf', '#rag', '#games', '#training', '#optimization'], 'emoji': 'ğŸ¥', 'ru': {'title': 'OneRec: Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ğ² Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…', 'desc': 'OneRec - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ ĞµĞ´Ğ¸Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ĞºĞ°ÑĞºĞ°Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°. ĞĞ½Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº-Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº Ñ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¼ĞµÑÑŒÑ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² (MoE) Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. OneRec Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¿Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ÑĞµÑÑĞ¸Ğ¹ Ğ¸ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ (DPO). Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¿Ñ€Ğ¾ÑĞ¼Ğ¾Ñ‚Ñ€Ğ° Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ½Ğ° Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğµ Kuaishou.'}, 'en': {'title': 'OneRec: Revolutionizing Recommendations with Generative Models', 'desc': 'This paper introduces OneRec, a novel generative retrieval-based recommendation system that improves upon traditional retrieve-and-rank methods. Unlike existing systems that use generative models merely for selection, OneRec employs a unified generative model that encodes user behavior and generates personalized video recommendations in a session-wise manner. The model utilizes a sparse Mixture-of-Experts architecture to enhance capacity while maintaining efficiency, and incorporates an Iterative Preference Alignment module to optimize user preferences effectively. Experimental results show that OneRec significantly outperforms existing systems, leading to a notable increase in user engagement metrics such as watch-time.'}, 'zh': {'title': 'OneRecï¼šç»Ÿä¸€ç”Ÿæˆæ¨¡å‹çš„æ¨èæ–°èŒƒå¼', 'desc': 'æœ€è¿‘ï¼ŒåŸºäºç”Ÿæˆæ£€ç´¢çš„æ¨èç³»ç»Ÿæˆä¸ºä¸€ç§æœ‰å‰æ™¯çš„èŒƒå¼ã€‚æœ¬æ–‡æå‡ºçš„OneRecæ¨¡å‹ï¼Œé‡‡ç”¨ç»Ÿä¸€çš„ç”Ÿæˆæ¨¡å‹ï¼Œå–ä»£äº†ä¼ ç»Ÿçš„çº§è”å­¦ä¹ æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨çœŸå®åœºæ™¯ä¸­æ˜¾è‘—è¶…è¶Šç°æœ‰å¤æ‚çš„æ¨èç³»ç»Ÿã€‚OneRecåŒ…æ‹¬ç¼–ç -è§£ç ç»“æ„ï¼Œèƒ½å¤Ÿæœ‰æ•ˆç¼–ç ç”¨æˆ·çš„å†å²è¡Œä¸ºï¼Œå¹¶ç”Ÿæˆç”¨æˆ·å¯èƒ½æ„Ÿå…´è¶£çš„è§†é¢‘ã€‚æ­¤å¤–ï¼ŒOneRecè¿˜å¼•å…¥äº†ä¼šè¯ç”Ÿæˆæ–¹æ³•å’Œè¿­ä»£åå¥½å¯¹é½æ¨¡å—ï¼Œæå‡äº†ç”Ÿæˆç»“æœçš„è´¨é‡ï¼Œå¹¶åœ¨å¿«æ‰‹çš„å®é™…åº”ç”¨ä¸­å®ç°äº†è§‚çœ‹æ—¶é—´çš„æ˜¾è‘—å¢åŠ ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.01688', 'title': 'When an LLM is apprehensive about its answers -- and when its uncertainty is justified', 'url': 'https://huggingface.co/papers/2503.01688', 'abstract': 'Uncertainty estimation is crucial for evaluating Large Language Models (LLMs), particularly in high-stakes domains where incorrect answers result in significant consequences. Numerous approaches consider this problem, while focusing on a specific type of uncertainty, ignoring others. We investigate what estimates, specifically token-wise entropy and model-as-judge (MASJ), would work for multiple-choice question-answering tasks for different question topics. Our experiments consider three LLMs: Phi-4, Mistral, and Qwen of different sizes from 1.5B to 72B and 14 topics. While MASJ performs similarly to a random error predictor, the response entropy predicts model error in knowledge-dependent domains and serves as an effective indicator of question difficulty: for biology ROC AUC is 0.73. This correlation vanishes for the reasoning-dependent domain: for math questions ROC-AUC is 0.55. More principally, we found out that the entropy measure required a reasoning amount. Thus, data-uncertainty related entropy should be integrated within uncertainty estimates frameworks, while MASJ requires refinement. Moreover, existing MMLU-Pro samples are biased, and should balance required amount of reasoning for different subdomains to provide a more fair assessment of LLMs performance.', 'score': 16, 'issue_id': 2518, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 3', 'zh': '3æœˆ3æ—¥'}, 'hash': '68429d7977c57eae', 'authors': ['Petr Sychev', 'Andrey Goncharov', 'Daniil Vyazhev', 'Edvard Khalafyan', 'Alexey Zaytsev'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2503.01688.jpg', 'data': {'categories': ['#ethics', '#hallucinations', '#benchmark', '#reasoning', '#data', '#multilingual'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ­Ğ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² ĞºĞ°Ğº Ğ¸Ğ½Ğ´Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ LLM Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ğ¼', 'desc': "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM) Ğ¿Ñ€Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ğ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° 'Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ĞºĞ°Ğº ÑÑƒĞ´ÑŒÑ' (MASJ) Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚ĞµĞ¼ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ğ»Ğ¸ÑÑŒ Ğ½Ğ° Ñ‚Ñ€ĞµÑ… LLM Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¸ 14 Ñ‚ĞµĞ¼Ğ°Ñ…. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…, Ğ·Ğ°Ğ²Ğ¸ÑÑÑ‰Ğ¸Ñ… Ğ¾Ñ‚ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, Ğ½Ğ¾ Ğ½Ğµ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹."}, 'en': {'title': 'Enhancing Uncertainty Estimation in LLMs for Better Decision-Making', 'desc': 'This paper explores how to measure uncertainty in Large Language Models (LLMs) when answering multiple-choice questions, which is important in critical areas where wrong answers can have serious effects. It compares two methods of uncertainty estimation: token-wise entropy and model-as-judge (MASJ), across various LLMs and topics. The findings reveal that while MASJ does not effectively predict errors, token-wise entropy is a better indicator of question difficulty, especially in knowledge-based subjects like biology. The study also highlights the need to refine MASJ and address biases in existing datasets to ensure fair evaluation of LLM performance across different reasoning requirements.'}, 'zh': {'title': 'æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸ç¡®å®šæ€§ä¼°è®¡', 'desc': 'ä¸ç¡®å®šæ€§ä¼°è®¡å¯¹äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è‡³å…³é‡è¦ï¼Œå°¤å…¶æ˜¯åœ¨é”™è¯¯ç­”æ¡ˆå¯èƒ½å¯¼è‡´é‡å¤§åæœçš„é«˜é£é™©é¢†åŸŸã€‚æœ¬æ–‡æ¢è®¨äº†ä¸åŒç±»å‹çš„ä¸ç¡®å®šæ€§ä¼°è®¡ï¼Œç‰¹åˆ«æ˜¯åŸºäºä»¤ç‰Œçš„ç†µå’Œæ¨¡å‹ä½œä¸ºè¯„åˆ¤è€…ï¼ˆMASJï¼‰ï¼Œåœ¨å¤šé€‰é¢˜å›ç­”ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚å®éªŒæ¶‰åŠä¸‰ç§ä¸åŒè§„æ¨¡çš„LLMsï¼Œç»“æœæ˜¾ç¤ºï¼Œå“åº”ç†µåœ¨çŸ¥è¯†ä¾èµ–é¢†åŸŸèƒ½å¤Ÿæœ‰æ•ˆé¢„æµ‹æ¨¡å‹é”™è¯¯ï¼Œè€ŒMASJçš„è¡¨ç°ç±»ä¼¼äºéšæœºé”™è¯¯é¢„æµ‹å™¨ã€‚æˆ‘ä»¬å‘ç°ç†µåº¦é‡éœ€è¦ä¸€å®šçš„æ¨ç†é‡ï¼Œå› æ­¤æ•°æ®ä¸ç¡®å®šæ€§ç›¸å…³çš„ç†µåº”çº³å…¥ä¸ç¡®å®šæ€§ä¼°è®¡æ¡†æ¶ä¸­ï¼Œè€ŒMASJåˆ™éœ€è¦è¿›ä¸€æ­¥æ”¹è¿›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.01496', 'title': 'Liger: Linearizing Large Language Models to Gated Recurrent Structures', 'url': 'https://huggingface.co/papers/2503.01496', 'abstract': 'Transformers with linear recurrent modeling offer linear-time training and constant-memory inference. Despite their demonstrated efficiency and performance, pretraining such non-standard architectures from scratch remains costly and risky. The linearization of large language models (LLMs) transforms pretrained standard models into linear recurrent structures, enabling more efficient deployment. However, current linearization methods typically introduce additional feature map modules that require extensive fine-tuning and overlook the gating mechanisms used in state-of-the-art linear recurrent models. To address these issues, this paper presents Liger, short for Linearizing LLMs to gated recurrent structures. Liger is a novel approach for converting pretrained LLMs into gated linear recurrent models without adding extra parameters. It repurposes the pretrained key matrix weights to construct diverse gating mechanisms, facilitating the formation of various gated recurrent structures while avoiding the need to train additional components from scratch. Using lightweight fine-tuning with Low-Rank Adaptation (LoRA), Liger restores the performance of the linearized gated recurrent models to match that of the original LLMs. Additionally, we introduce Liger Attention, an intra-layer hybrid attention mechanism, which significantly recovers 93\\% of the Transformer-based LLM at 0.02\\% pre-training tokens during the linearization process, achieving competitive results across multiple benchmarks, as validated on models ranging from 1B to 8B parameters. Code is available at https://github.com/OpenSparseLLMs/Linearization.', 'score': 13, 'issue_id': 2514, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 3', 'zh': '3æœˆ3æ—¥'}, 'hash': 'd5ca7ef45c0e90c9', 'authors': ['Disen Lan', 'Weigao Sun', 'Jiaxi Hu', 'Jusen Du', 'Yu Cheng'], 'affiliations': ['Nanjing University', 'Shanghai AI Laboratory', 'South China University of Technology', 'The Chinese University of Hong Kong', 'The Hong Kong University of Science and Technology (Guangzhou)'], 'pdf_title_img': 'assets/pdf/title_img/2503.01496.jpg', 'data': {'categories': ['#architecture', '#training', '#optimization', '#benchmark'], 'emoji': 'ğŸ”¢', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ»Ğ¸Ğ½ĞµĞ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ”Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Liger Ğ´Ğ»Ñ Ğ»Ğ¸Ğ½ĞµĞ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ğ³ĞµĞ¹Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾-Ñ€ĞµĞºÑƒÑ€Ñ€ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹. Liger Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ LLM Ğ±ĞµĞ· Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ²ĞµÑĞ° ĞºĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ñ‹ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² Ğ³ĞµĞ¹Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½ÑƒÑ Ğ´Ğ¾Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Low-Rank Adaptation (LoRA) Ğ´Ğ»Ñ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ»Ğ¸Ğ½ĞµĞ°Ñ€Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Liger Attention - Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ»Ğ¸Ğ½ĞµĞ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'Liger: Efficiently Transforming LLMs into Gated Linear Recurrent Models', 'desc': "This paper introduces Liger, a method for transforming pretrained large language models (LLMs) into gated linear recurrent models. Liger efficiently repurposes existing key matrix weights to create diverse gating mechanisms without adding extra parameters, thus avoiding the costly process of training new components from scratch. The approach employs lightweight fine-tuning techniques, specifically Low-Rank Adaptation (LoRA), to maintain the performance of the linearized models comparable to the original LLMs. Additionally, Liger incorporates a novel intra-layer hybrid attention mechanism, Liger Attention, which enhances the model's efficiency while achieving competitive results across various benchmarks."}, 'zh': {'title': 'Ligerï¼šé«˜æ•ˆè½¬æ¢é¢„è®­ç»ƒæ¨¡å‹çš„åˆ›æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºLigerçš„æ–¹æ³•ï¼Œç”¨äºå°†é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è½¬æ¢ä¸ºå¸¦é—¨æ§çš„çº¿æ€§é€’å½’æ¨¡å‹ï¼Œè€Œæ— éœ€å¢åŠ é¢å¤–çš„å‚æ•°ã€‚Ligeré€šè¿‡é‡æ–°åˆ©ç”¨é¢„è®­ç»ƒçš„å…³é”®çŸ©é˜µæƒé‡ï¼Œæ„å»ºå¤šæ ·çš„é—¨æ§æœºåˆ¶ï¼Œä»è€Œå½¢æˆä¸åŒçš„é—¨æ§é€’å½’ç»“æ„ã€‚è¯¥æ–¹æ³•ä½¿ç”¨è½»é‡çº§çš„å¾®è°ƒæŠ€æœ¯ï¼ˆå¦‚ä½ç§©é€‚åº”LoRAï¼‰ï¼Œä½¿çº¿æ€§åŒ–çš„é—¨æ§é€’å½’æ¨¡å‹çš„æ€§èƒ½æ¢å¤åˆ°ä¸åŸå§‹LLMsç›¸å½“çš„æ°´å¹³ã€‚æ­¤å¤–ï¼ŒLiger Attentionä½œä¸ºä¸€ç§å±‚å†…æ··åˆæ³¨æ„åŠ›æœºåˆ¶ï¼Œåœ¨çº¿æ€§åŒ–è¿‡ç¨‹ä¸­æ˜¾è‘—æ¢å¤äº†93%çš„TransformeråŸºç¡€LLMçš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.01307', 'title': 'Cognitive Behaviors that Enable Self-Improving Reasoners, or, Four Habits of Highly Effective STaRs', 'url': 'https://huggingface.co/papers/2503.01307', 'abstract': "Test-time inference has emerged as a powerful paradigm for enabling language models to ``think'' longer and more carefully about complex challenges, much like skilled human experts. While reinforcement learning (RL) can drive self-improvement in language models on verifiable tasks, some models exhibit substantial gains while others quickly plateau. For instance, we find that Qwen-2.5-3B far exceeds Llama-3.2-3B under identical RL training for the game of Countdown. This discrepancy raises a critical question: what intrinsic properties enable effective self-improvement? We introduce a framework to investigate this question by analyzing four key cognitive behaviors -- verification, backtracking, subgoal setting, and backward chaining -- that both expert human problem solvers and successful language models employ. Our study reveals that Qwen naturally exhibits these reasoning behaviors, whereas Llama initially lacks them. In systematic experimentation with controlled behavioral datasets, we find that priming Llama with examples containing these reasoning behaviors enables substantial improvements during RL, matching or exceeding Qwen's performance. Importantly, the presence of reasoning behaviors, rather than correctness of answers, proves to be the critical factor -- models primed with incorrect solutions containing proper reasoning patterns achieve comparable performance to those trained on correct solutions. Finally, leveraging continued pretraining with OpenWebMath data, filtered to amplify reasoning behaviors, enables the Llama model to match Qwen's self-improvement trajectory. Our findings establish a fundamental relationship between initial reasoning behaviors and the capacity for improvement, explaining why some language models effectively utilize additional computation while others plateau.", 'score': 13, 'issue_id': 2511, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 3', 'zh': '3æœˆ3æ—¥'}, 'hash': 'fa966620baa8c013', 'authors': ['Kanishk Gandhi', 'Ayush Chakravarthy', 'Anikait Singh', 'Nathan Lile', 'Noah D. Goodman'], 'affiliations': ['Stanford University', 'SynthLabs'], 'pdf_title_img': 'assets/pdf/title_img/2503.01307.jpg', 'data': {'categories': ['#training', '#optimization', '#rl', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞšĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¸ - ĞºĞ»ÑÑ‡ Ğº ÑĞ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº ÑĞ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ñ‚ Ğ¾Ñ‚ Ğ½Ğ°Ğ»Ğ¸Ñ‡Ğ¸Ñ Ñƒ Ğ½Ğ¸Ñ… Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğ¹, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ, Ğ±ÑĞºÑ‚Ñ€ĞµĞºĞ¸Ğ½Ğ³, Ğ¿Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾Ğ´Ñ†ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğµ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Qwen Ğ¸Ğ·Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¾Ğ±Ğ»Ğ°Ğ´Ğ°ĞµÑ‚ ÑÑ‚Ğ¸Ğ¼Ğ¸ Ğ½Ğ°Ğ²Ñ‹ĞºĞ°Ğ¼Ğ¸, Ğ² Ñ‚Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ĞºĞ°Ğº Llama Ğ½ĞµÑ‚. ĞŸÑ€Ğ°Ğ¹Ğ¼Ğ¸Ğ½Ğ³ Llama Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ğ¼Ğ¸, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¼Ğ¸ ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ» Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ ĞµĞµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ’Ğ°Ğ¶Ğ½Ğ¾ Ğ¾Ñ‚Ğ¼ĞµÑ‚Ğ¸Ñ‚ÑŒ, Ñ‡Ñ‚Ğ¾ Ğ½Ğ°Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾ÑÑŒ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¾Ğ¼, Ñ‡ĞµĞ¼ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ².'}, 'en': {'title': 'Unlocking Self-Improvement in Language Models through Reasoning', 'desc': 'This paper explores how language models can improve their problem-solving abilities through a process called test-time inference, similar to human experts. It highlights the differences in performance between two models, Qwen-2.5-3B and Llama-3.2-3B, when trained with reinforcement learning (RL) on the game Countdown. The authors identify four cognitive behaviorsâ€”verification, backtracking, subgoal setting, and backward chainingâ€”that are crucial for effective self-improvement in these models. They demonstrate that enhancing Llama with examples of these reasoning behaviors can significantly boost its performance, suggesting that the ability to reason is more important than simply providing correct answers.'}, 'zh': {'title': 'æ¨ç†è¡Œä¸ºæ˜¯æ¨¡å‹è‡ªæˆ‘æå‡çš„å…³é”®', 'desc': 'æœ¬æ–‡æ¢è®¨äº†è¯­è¨€æ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡ä¸­è‡ªæˆ‘æ”¹è¿›çš„èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å®ç°çš„è‡ªæˆ‘æå‡ã€‚ç ”ç©¶å‘ç°ï¼Œä¸åŒæ¨¡å‹åœ¨ç›¸åŒçš„RLè®­ç»ƒä¸‹è¡¨ç°å·®å¼‚æ˜¾è‘—ï¼Œä¾‹å¦‚Qwen-2.5-3Båœ¨æ¸¸æˆCountdownä¸­è¿œè¶…Llama-3.2-3Bã€‚æˆ‘ä»¬åˆ†æäº†å››ç§å…³é”®çš„è®¤çŸ¥è¡Œä¸ºï¼šéªŒè¯ã€å›æº¯ã€å­ç›®æ ‡è®¾å®šå’Œé€†å‘é“¾æ¨ç†ï¼Œå‘ç°Qwenè‡ªç„¶å±•ç°äº†è¿™äº›æ¨ç†è¡Œä¸ºï¼Œè€ŒLlamaåˆ™æœ€åˆç¼ºä¹ã€‚é€šè¿‡å¯¹Llamaè¿›è¡Œç¤ºä¾‹å¼•å¯¼ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡å…¶åœ¨RLä¸­çš„è¡¨ç°ï¼Œè¯æ˜äº†æ¨ç†è¡Œä¸ºçš„å­˜åœ¨æ˜¯æ¨¡å‹è‡ªæˆ‘æ”¹è¿›çš„å…³é”®å› ç´ ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.00501', 'title': 'Qilin: A Multimodal Information Retrieval Dataset with APP-level User Sessions', 'url': 'https://huggingface.co/papers/2503.00501', 'abstract': "User-generated content (UGC) communities, especially those featuring multimodal content, improve user experiences by integrating visual and textual information into results (or items). The challenge of improving user experiences in complex systems with search and recommendation (S\\&R) services has drawn significant attention from both academia and industry these years. However, the lack of high-quality datasets has limited the research progress on multimodal S\\&R. To address the growing need for developing better S\\&R services, we present a novel multimodal information retrieval dataset in this paper, namely Qilin. The dataset is collected from Xiaohongshu, a popular social platform with over 300 million monthly active users and an average search penetration rate of over 70\\%. In contrast to existing datasets, Qilin offers a comprehensive collection of user sessions with heterogeneous results like image-text notes, video notes, commercial notes, and direct answers, facilitating the development of advanced multimodal neural retrieval models across diverse task settings. To better model user satisfaction and support the analysis of heterogeneous user behaviors, we also collect extensive APP-level contextual signals and genuine user feedback. Notably, Qilin contains user-favored answers and their referred results for search requests triggering the Deep Query Answering (DQA) module. This allows not only the training \\& evaluation of a Retrieval-augmented Generation (RAG) pipeline, but also the exploration of how such a module would affect users' search behavior. Through comprehensive analysis and experiments, we provide interesting findings and insights for further improving S\\&R systems. We hope that Qilin will significantly contribute to the advancement of multimodal content platforms with S\\&R services in the future.", 'score': 10, 'issue_id': 2513, 'pub_date': '2025-03-01', 'pub_date_card': {'ru': '1 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 1', 'zh': '3æœˆ1æ—¥'}, 'hash': 'ed7fc8625b068597', 'authors': ['Jia Chen', 'Qian Dong', 'Haitao Li', 'Xiaohui He', 'Yan Gao', 'Shaosheng Cao', 'Yi Wu', 'Ping Yang', 'Chen Xu', 'Yao Hu', 'Qingyao Ai', 'Yiqun Liu'], 'affiliations': ['Tsinghua University', 'Xiaohongshu Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2503.00501.jpg', 'data': {'categories': ['#multimodal', '#dataset', '#rag'], 'emoji': 'ğŸ”', 'ru': {'title': 'Qilin: Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¹', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Qilin Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°, ÑĞ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğµ Xiaohongshu. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğµ ÑĞµÑÑĞ¸Ğ¸ Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°Ğ¼Ğ¸ (Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ°Ğ¼ĞµÑ‚ĞºĞ¸) Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ°Ğ¼Ğ¸. Qilin Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒ Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¹, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ÑƒĞ»Ñ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ½Ğ°Ğ´ĞµÑÑ‚ÑÑ, Ñ‡Ñ‚Ğ¾ Qilin Ğ²Ğ½ĞµÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ²ĞºĞ»Ğ°Ğ´ Ğ² Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼ Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ ÑĞµÑ€Ğ²Ğ¸ÑĞ°Ğ¼Ğ¸.'}, 'en': {'title': 'Enhancing User Experiences with Qilin: A Multimodal Dataset for S&R Services', 'desc': 'This paper introduces Qilin, a new multimodal information retrieval dataset designed to enhance search and recommendation (S&R) services in user-generated content communities. Qilin is unique as it includes diverse user sessions with various content types, such as image-text notes and videos, which can help in developing advanced multimodal neural retrieval models. Additionally, the dataset captures user feedback and contextual signals, allowing researchers to analyze user satisfaction and behavior more effectively. The findings from this research aim to improve S&R systems and contribute to the evolution of multimodal content platforms.'}, 'zh': {'title': 'æ¨åŠ¨å¤šæ¨¡æ€æœç´¢ä¸æ¨èæœåŠ¡çš„è¿›æ­¥', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªæ–°çš„å¤šæ¨¡æ€ä¿¡æ¯æ£€ç´¢æ•°æ®é›†Qilinï¼Œæ—¨åœ¨æ”¹å–„ç”¨æˆ·åœ¨å¤æ‚ç³»ç»Ÿä¸­çš„æœç´¢å’Œæ¨èä½“éªŒã€‚Qilinæ•°æ®é›†æ¥æºäºå°çº¢ä¹¦ï¼ŒåŒ…å«å¤šç§ç±»å‹çš„ç”¨æˆ·ä¼šè¯ï¼Œå¦‚å›¾æ–‡ç¬”è®°ã€è§†é¢‘ç¬”è®°å’Œå•†ä¸šç¬”è®°ï¼Œé€‚ç”¨äºå¤šç§ä»»åŠ¡è®¾ç½®ã€‚è¯¥æ•°æ®é›†è¿˜æ”¶é›†äº†ä¸°å¯Œçš„åº”ç”¨çº§ä¸Šä¸‹æ–‡ä¿¡å·å’ŒçœŸå®ç”¨æˆ·åé¦ˆï¼Œä»¥æ›´å¥½åœ°å»ºæ¨¡ç”¨æˆ·æ»¡æ„åº¦ã€‚é€šè¿‡å¯¹Qilinçš„åˆ†æå’Œå®éªŒï¼Œæœ¬æ–‡æä¾›äº†æœ‰è¶£çš„å‘ç°ï¼ŒæœŸæœ›èƒ½æ¨åŠ¨å¤šæ¨¡æ€å†…å®¹å¹³å°çš„æœç´¢å’Œæ¨èæœåŠ¡çš„å‘å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.00714', 'title': 'Speculative Ad-hoc Querying', 'url': 'https://huggingface.co/papers/2503.00714', 'abstract': "Analyzing large datasets requires responsive query execution, but executing SQL queries on massive datasets can be slow. This paper explores whether query execution can begin even before the user has finished typing, allowing results to appear almost instantly. We propose SpeQL, a system that leverages Large Language Models (LLMs) to predict likely queries based on the database schema, the user's past queries, and their incomplete query. Since exact query prediction is infeasible, SpeQL speculates on partial queries in two ways: 1) it predicts the query structure to compile and plan queries in advance, and 2) it precomputes smaller temporary tables that are much smaller than the original database, but are still predicted to contain all information necessary to answer the user's final query. Additionally, SpeQL continuously displays results for speculated queries and subqueries in real time, aiding exploratory analysis. A utility/user study showed that SpeQL improved task completion time, and participants reported that its speculative display of results helped them discover patterns in the data more quickly. In the study, SpeQL improves user's query latency by up to 289times and kept the overhead reasonable, at 4$ per hour.", 'score': 8, 'issue_id': 2514, 'pub_date': '2025-03-02', 'pub_date_card': {'ru': '2 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 2', 'zh': '3æœˆ2æ—¥'}, 'hash': '1b0459b56fdb6894', 'authors': ['Haoyu Li', 'Srikanth Kandula', 'Maria Angels de Luis Balaguer', 'Aditya Akella', 'Venkat Arun'], 'affiliations': ['Amazon Web Services', 'Microsoft Research', 'The University of Texas at Austin'], 'pdf_title_img': 'assets/pdf/title_img/2503.00714.jpg', 'data': {'categories': ['#dataset', '#data', '#benchmark'], 'emoji': 'âš¡', 'ru': {'title': 'ĞœĞ¾Ğ»Ğ½Ğ¸ĞµĞ½Ğ¾ÑĞ½Ñ‹Ğµ SQL-Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ¸ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ SpeQL, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰ÑƒÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ SQL-Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ. SpeQL Ğ¿Ñ€ĞµĞ´ÑƒĞ³Ğ°Ğ´Ñ‹Ğ²Ğ°ĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ° Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ÑĞµÑ‚ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ½Ğ°Ñ‡Ğ°Ñ‚ÑŒ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ° Ğ´Ğ¾ ĞµĞ³Ğ¾ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¼. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶Ğ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ĞµĞ¼Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸, Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ Ğ² Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ¼ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ SpeQL Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼ Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Instant Query Results with SpeQL!', 'desc': 'This paper introduces SpeQL, a novel system designed to enhance the speed of SQL query execution on large datasets. By utilizing Large Language Models (LLMs), SpeQL predicts user queries even before they are fully typed, allowing for near-instantaneous results. It employs two main strategies: predicting the structure of queries for pre-compilation and creating smaller temporary tables that contain essential data for answering the final query. A user study demonstrated that SpeQL significantly reduced query latency and helped users identify data patterns more efficiently during exploratory analysis.'}, 'zh': {'title': 'SpeQLï¼šè®©æŸ¥è¯¢æ›´å¿«çš„æ™ºèƒ½é¢„æµ‹ç³»ç»Ÿ', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†å¦‚ä½•åœ¨ç”¨æˆ·è¾“å…¥SQLæŸ¥è¯¢æ—¶ï¼Œæå‰å¼€å§‹æ‰§è¡ŒæŸ¥è¯¢ï¼Œä»¥åŠ å¿«å¤§æ•°æ®é›†çš„æŸ¥è¯¢å“åº”é€Ÿåº¦ã€‚æˆ‘ä»¬æå‡ºäº†SpeQLç³»ç»Ÿï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ ¹æ®æ•°æ®åº“æ¨¡å¼ã€ç”¨æˆ·çš„å†å²æŸ¥è¯¢å’Œä¸å®Œæ•´æŸ¥è¯¢æ¥é¢„æµ‹å¯èƒ½çš„æŸ¥è¯¢ã€‚SpeQLé€šè¿‡é¢„æµ‹æŸ¥è¯¢ç»“æ„å’Œé¢„è®¡ç®—å°å‹ä¸´æ—¶è¡¨æ¥å¤„ç†éƒ¨åˆ†æŸ¥è¯¢ï¼Œä»è€Œåœ¨ç”¨æˆ·å®ŒæˆæŸ¥è¯¢ä¹‹å‰æä¾›å®æ—¶ç»“æœã€‚ç ”ç©¶è¡¨æ˜ï¼ŒSpeQLæ˜¾è‘—æé«˜äº†ç”¨æˆ·çš„æŸ¥è¯¢é€Ÿåº¦ï¼Œå¹¶å¸®åŠ©ç”¨æˆ·æ›´å¿«åœ°å‘ç°æ•°æ®ä¸­çš„æ¨¡å¼ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.00784', 'title': 'DuoDecoding: Hardware-aware Heterogeneous Speculative Decoding with Dynamic Multi-Sequence Drafting', 'url': 'https://huggingface.co/papers/2503.00784', 'abstract': 'Large language models (LLMs) exhibit exceptional performance across a wide range of tasks; however, their token-by-token autoregressive generation process significantly hinders inference speed. Speculative decoding presents a promising draft-then-verify framework that reduces generation latency while maintaining output distribution fidelity. Nevertheless, the draft model introduces additional computational overhead, becoming a performance bottleneck and increasing the time to first token (TTFT). Previous approaches to mitigate draft model overhead have primarily relied on heuristics and generally failed to match the quality of the draft language models. To address these challenges, we propose DuoDecoding, a novel approach that strategically deploys the draft and target models on the CPU and GPU respectively, enabling parallel decoding while preserving draft quality. Our method incorporates a hardware-aware optimal draft budget to minimize idle times and employs dynamic multi-sequence drafting to enhance draft quality. Extensive experiments across seven tasks show that DuoDecoding achieves up to 2.61x speedup in generation latency, while reducing TTFT to 83% of that in conventional speculative decoding. The Code is available at https://github.com/KaiLv69/DuoDecoding.', 'score': 8, 'issue_id': 2510, 'pub_date': '2025-03-02', 'pub_date_card': {'ru': '2 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 2', 'zh': '3æœˆ2æ—¥'}, 'hash': 'b4870a0e44c3cc55', 'authors': ['Kai Lv', 'Honglin Guo', 'Qipeng Guo', 'Xipeng Qiu'], 'affiliations': ['Fudan University', 'Shanghai AI Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2503.00784.jpg', 'data': {'categories': ['#inference', '#training', '#optimization'], 'emoji': 'ğŸš€', 'ru': {'title': 'DuoDecoding: ĞŸĞ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ (LLM) Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ DuoDecoding. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° CPU Ğ¸ GPU, Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ Ğ²Ñ€ĞµĞ¼Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ° Ğ¸ Ğ¾Ğ±Ñ‰ÑƒÑ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ. DuoDecoding Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ°Ğ¿Ğ¿Ğ°Ñ€Ğ°Ñ‚Ğ½Ğ¾-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ±ÑĞ´Ğ¶ĞµÑ‚ Ñ‡ĞµÑ€Ğ½Ğ¾Ğ²Ğ¸ĞºĞ° Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ñ‡ĞµÑ€Ğ½Ğ¾Ğ²Ğ¾Ğµ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğ¼ ÑĞ¿ĞµĞºÑƒĞ»ÑÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼.'}, 'en': {'title': 'DuoDecoding: Speeding Up Text Generation with Smart Model Deployment', 'desc': 'This paper introduces DuoDecoding, a new method to improve the speed of generating text with large language models (LLMs) while keeping the quality high. It uses a draft-then-verify approach, where a draft model quickly generates initial text, and a target model refines it, but does so in a way that reduces the time it takes to start generating text. By using both CPU and GPU for different parts of the process, DuoDecoding allows for faster and more efficient decoding. The results show that this method can significantly speed up text generation without sacrificing quality, achieving a notable improvement in performance across various tasks.'}, 'zh': {'title': 'DuoDecodingï¼šåŠ é€Ÿç”Ÿæˆçš„æ–°æ–¹æ³•', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶é€å­—è‡ªå›å½’ç”Ÿæˆè¿‡ç¨‹æ˜¾è‘—å½±å“æ¨ç†é€Ÿåº¦ã€‚æ¨æµ‹è§£ç æä¾›äº†ä¸€ç§æœ‰å‰æ™¯çš„è‰ç¨¿-éªŒè¯æ¡†æ¶ï¼Œèƒ½å¤Ÿå‡å°‘ç”Ÿæˆå»¶è¿Ÿï¼ŒåŒæ—¶ä¿æŒè¾“å‡ºåˆ†å¸ƒçš„å‡†ç¡®æ€§ã€‚æˆ‘ä»¬æå‡ºçš„DuoDecodingæ–¹æ³•é€šè¿‡åœ¨CPUå’ŒGPUä¸Šåˆ†åˆ«éƒ¨ç½²è‰ç¨¿æ¨¡å‹å’Œç›®æ ‡æ¨¡å‹ï¼Œå®ç°äº†å¹¶è¡Œè§£ç ï¼Œæå‡äº†ç”Ÿæˆæ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDuoDecodingåœ¨ç”Ÿæˆå»¶è¿Ÿä¸Šå®ç°äº†æœ€é«˜2.61å€çš„åŠ é€Ÿï¼ŒåŒæ—¶å°†é¦–æ¬¡ç”Ÿæˆæ—¶é—´ç¼©çŸ­è‡³ä¼ ç»Ÿæ¨æµ‹è§£ç çš„83%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.01506', 'title': 'SampleMix: A Sample-wise Pre-training Data Mixing Strategey by Coordinating Data Quality and Diversity', 'url': 'https://huggingface.co/papers/2503.01506', 'abstract': "Existing pretraining data mixing methods for large language models (LLMs) typically follow a domain-wise methodology, a top-down process that first determines domain weights and then performs uniform data sampling across each domain. However, these approaches neglect significant inter-domain overlaps and commonalities, failing to control the global diversity of the constructed training dataset. Further, uniform sampling within domains ignores fine-grained sample-specific features, potentially leading to suboptimal data distribution. To address these shortcomings, we propose a novel sample-wise data mixture approach based on a bottom-up paradigm. This method performs global cross-domain sampling by systematically evaluating the quality and diversity of each sample, thereby dynamically determining the optimal domain distribution. Comprehensive experiments across multiple downstream tasks and perplexity assessments demonstrate that SampleMix surpasses existing domain-based methods. Meanwhile, SampleMix requires 1.4x to 2.1x training steps to achieves the baselines' performance, highlighting the substantial potential of SampleMix to optimize pre-training data.", 'score': 7, 'issue_id': 2517, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 3', 'zh': '3æœˆ3æ—¥'}, 'hash': '018cc621eb1ee12b', 'authors': ['Xiangyu Xi', 'Deyang Kong', 'Jian Yang', 'Jiawei Yang', 'Zhengyu Chen', 'Wei Wang', 'Jingang Wang', 'Xunliang Cai', 'Shikun Zhang', 'Wei Ye'], 'affiliations': ['Meituan Group, Beijing, China', 'National Engineering Research Center for Software Engineering, Peking University, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.01506.jpg', 'data': {'categories': ['#transfer_learning', '#training', '#optimization', '#data'], 'emoji': 'ğŸ”€', 'ru': {'title': 'SampleMix: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑĞ¼ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ LLM', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¼ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM), Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ SampleMix. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ñ…, SampleMix Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºÑƒ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ², Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ Ğ¸Ñ… ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑ‚ÑŒ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ¾Ğ² Ğ¸ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¼ĞµĞ¶domĞµĞ½Ğ½Ñ‹Ğµ Ğ¿ĞµÑ€ĞµÑĞµÑ‡ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ SampleMix Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ñ…, Ñ…Ğ¾Ñ‚Ñ Ğ¸ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ ÑˆĞ°Ğ³Ğ¾Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Revolutionizing Data Mixing for Better Language Model Training', 'desc': 'This paper introduces SampleMix, a new method for mixing pretraining data for large language models (LLMs). Unlike traditional domain-wise approaches that sample uniformly within predefined domains, SampleMix uses a bottom-up strategy that evaluates the quality and diversity of individual samples across domains. This allows for a more dynamic and optimal distribution of training data, addressing the limitations of inter-domain overlaps and sample-specific features. Experimental results show that SampleMix not only outperforms existing methods but also requires fewer training steps to achieve comparable performance.'}, 'zh': {'title': 'æ ·æœ¬çº§æ•°æ®æ··åˆï¼Œä¼˜åŒ–é¢„è®­ç»ƒæ•°æ®çš„æœªæ¥', 'desc': 'ç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒæ•°æ®æ··åˆæ–¹æ³•é€šå¸¸é‡‡ç”¨é¢†åŸŸå¯¼å‘çš„æ–¹æ³•ï¼Œå…ˆç¡®å®šé¢†åŸŸæƒé‡ï¼Œå†åœ¨æ¯ä¸ªé¢†åŸŸå†…è¿›è¡Œå‡åŒ€æ•°æ®é‡‡æ ·ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å¿½è§†äº†é¢†åŸŸä¹‹é—´çš„é‡è¦é‡å å’Œå…±æ€§ï¼Œæœªèƒ½æœ‰æ•ˆæ§åˆ¶è®­ç»ƒæ•°æ®é›†çš„å…¨çƒå¤šæ ·æ€§ã€‚æ­¤å¤–ï¼Œé¢†åŸŸå†…çš„å‡åŒ€é‡‡æ ·å¿½ç•¥äº†æ ·æœ¬ç‰¹å®šçš„ç»†å¾®ç‰¹å¾ï¼Œå¯èƒ½å¯¼è‡´æ•°æ®åˆ†å¸ƒä¸ç†æƒ³ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºè‡ªä¸‹è€Œä¸Šçš„æ–°å‹æ ·æœ¬çº§æ•°æ®æ··åˆæ–¹æ³•ï¼Œèƒ½å¤Ÿé€šè¿‡ç³»ç»Ÿè¯„ä¼°æ¯ä¸ªæ ·æœ¬çš„è´¨é‡å’Œå¤šæ ·æ€§ï¼ŒåŠ¨æ€ç¡®å®šæœ€ä½³é¢†åŸŸåˆ†å¸ƒã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.18890', 'title': 'From Hours to Minutes: Lossless Acceleration of Ultra Long Sequence Generation up to 100K Tokens', 'url': 'https://huggingface.co/papers/2502.18890', 'abstract': "Generating ultra-long sequences with large language models (LLMs) has become increasingly crucial but remains a highly time-intensive task, particularly for sequences up to 100K tokens. While traditional speculative decoding methods exist, simply extending their generation limits fails to accelerate the process and can be detrimental. Through an in-depth analysis, we identify three major challenges hindering efficient generation: frequent model reloading, dynamic key-value (KV) management and repetitive generation. To address these issues, we introduce TOKENSWIFT, a novel framework designed to substantially accelerate the generation process of ultra-long sequences while maintaining the target model's inherent quality. Experimental results demonstrate that TOKENSWIFT achieves over 3 times speedup across models of varying scales (1.5B, 7B, 8B, 14B) and architectures (MHA, GQA). This acceleration translates to hours of time savings for ultra-long sequence generation, establishing TOKENSWIFT as a scalable and effective solution at unprecedented lengths. Code can be found at https://github.com/bigai-nlco/TokenSwift.", 'score': 7, 'issue_id': 2517, 'pub_date': '2025-02-26', 'pub_date_card': {'ru': '26 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 26', 'zh': '2æœˆ26æ—¥'}, 'hash': 'd07c05abfac49ecc', 'authors': ['Tong Wu', 'Junzhe Shen', 'Zixia Jia', 'Yuxuan Wang', 'Zilong Zheng'], 'affiliations': ['NLCo Lab, BIGAI LUMIA Lab, Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2502.18890.jpg', 'data': {'categories': ['#training', '#architecture', '#long_context', '#inference', '#optimization'], 'emoji': 'âš¡', 'ru': {'title': 'TOKENSWIFT: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ²ĞµÑ€Ñ…Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ TOKENSWIFT - Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ²ĞµÑ€Ñ…Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ (LLM). ĞĞ½Ğ¸ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ñ‚Ñ€Ğ¸ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹, Ğ¿Ñ€ĞµĞ¿ÑÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸: Ñ‡Ğ°ÑÑ‚Ğ°Ñ Ğ¿ĞµÑ€ĞµĞ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ĞºĞ»ÑÑ‡Ğ°Ğ¼Ğ¸-Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€ÑÑÑ‰Ğ°ÑÑÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ. TOKENSWIFT Ñ€ĞµÑˆĞ°ĞµÑ‚ ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ ÑƒÑĞºĞ¾Ñ€Ğ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ² 3 Ñ€Ğ°Ğ·Ğ° Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¾Ğ² Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€. Ğ­Ñ‚Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ²Ñ€ĞµĞ¼Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ²ĞµÑ€Ñ…Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Accelerating Ultra-Long Sequence Generation with TOKENSWIFT', 'desc': 'This paper presents TOKENSWIFT, a new framework aimed at speeding up the generation of ultra-long sequences using large language models (LLMs). The authors identify key challenges such as model reloading, dynamic key-value management, and repetitive generation that slow down the process. By addressing these issues, TOKENSWIFT achieves over three times the speed of traditional methods while preserving the quality of the generated text. Experimental results show that this framework is effective across various model sizes and architectures, making it a significant advancement in the field of sequence generation.'}, 'zh': {'title': 'TOKENSWIFTï¼šåŠ é€Ÿè¶…é•¿åºåˆ—ç”Ÿæˆçš„åˆ›æ–°æ¡†æ¶', 'desc': 'ç”Ÿæˆè¶…é•¿åºåˆ—å¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å˜å¾—è¶Šæ¥è¶Šé‡è¦ï¼Œä½†è¿™ä¸€è¿‡ç¨‹é€šå¸¸éå¸¸è€—æ—¶ï¼Œå°¤å…¶æ˜¯å¯¹äºé•¿è¾¾10ä¸‡æ ‡è®°çš„åºåˆ—ã€‚ä¼ ç»Ÿçš„æ¨æµ‹è§£ç æ–¹æ³•åœ¨å»¶é•¿ç”Ÿæˆé™åˆ¶æ—¶å¹¶æœªåŠ é€Ÿè¿‡ç¨‹ï¼Œåè€Œå¯èƒ½é€ æˆè´Ÿé¢å½±å“ã€‚æˆ‘ä»¬é€šè¿‡æ·±å…¥åˆ†æï¼Œè¯†åˆ«å‡ºå½±å“é«˜æ•ˆç”Ÿæˆçš„ä¸‰ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šé¢‘ç¹çš„æ¨¡å‹é‡è½½ã€åŠ¨æ€é”®å€¼ï¼ˆKVï¼‰ç®¡ç†å’Œé‡å¤ç”Ÿæˆã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†TOKENSWIFTï¼Œä¸€ä¸ªæ–°æ¡†æ¶ï¼Œæ—¨åœ¨æ˜¾è‘—åŠ å¿«è¶…é•¿åºåˆ—çš„ç”Ÿæˆè¿‡ç¨‹ï¼ŒåŒæ—¶ä¿æŒç›®æ ‡æ¨¡å‹çš„å›ºæœ‰è´¨é‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.01370', 'title': 'Kiss3DGen: Repurposing Image Diffusion Models for 3D Asset Generation', 'url': 'https://huggingface.co/papers/2503.01370', 'abstract': "Diffusion models have achieved great success in generating 2D images. However, the quality and generalizability of 3D content generation remain limited. State-of-the-art methods often require large-scale 3D assets for training, which are challenging to collect. In this work, we introduce Kiss3DGen (Keep It Simple and Straightforward in 3D Generation), an efficient framework for generating, editing, and enhancing 3D objects by repurposing a well-trained 2D image diffusion model for 3D generation. Specifically, we fine-tune a diffusion model to generate ''3D Bundle Image'', a tiled representation composed of multi-view images and their corresponding normal maps. The normal maps are then used to reconstruct a 3D mesh, and the multi-view images provide texture mapping, resulting in a complete 3D model. This simple method effectively transforms the 3D generation problem into a 2D image generation task, maximizing the utilization of knowledge in pretrained diffusion models. Furthermore, we demonstrate that our Kiss3DGen model is compatible with various diffusion model techniques, enabling advanced features such as 3D editing, mesh and texture enhancement, etc. Through extensive experiments, we demonstrate the effectiveness of our approach, showcasing its ability to produce high-quality 3D models efficiently.", 'score': 7, 'issue_id': 2513, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 3', 'zh': '3æœˆ3æ—¥'}, 'hash': '3decc9fe2b6f6e32', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#cv', '#diffusion', '#3d'], 'emoji': 'ğŸ¨', 'ru': {'title': 'ĞŸÑ€Ğ¾ÑÑ‚Ğ°Ñ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ 3D-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ 2D-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸', 'desc': "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Kiss3DGen - ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ´Ğ»Ñ 2D-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 'ĞŸĞ°ĞºĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ 3D-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ', ÑĞ¾ÑÑ‚Ğ¾ÑÑ‰ĞµĞ³Ğ¾ Ğ¸Ğ· Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… ĞºĞ°Ñ€Ñ‚ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»ĞµĞ¹. Ğ—Ğ°Ñ‚ĞµĞ¼ ĞºĞ°Ñ€Ñ‚Ñ‹ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»ĞµĞ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ÑÑ Ğ´Ğ»Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ 3D-Ğ¼ĞµÑˆĞ°, Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ñ‹Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‚ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ñ‡Ñ‚Ğ¾ Ğ² Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğµ Ğ´Ğ°ĞµÑ‚ Ğ¿Ğ¾Ğ»Ğ½ÑƒÑ 3D-Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ°Ğ¼Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ 3D-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."}, 'en': {'title': 'Kiss3DGen: Simplifying 3D Generation with 2D Diffusion Models', 'desc': "This paper presents Kiss3DGen, a novel framework that simplifies the process of generating and enhancing 3D objects by leveraging existing 2D image diffusion models. The approach involves fine-tuning a diffusion model to create a '3D Bundle Image', which consists of multiple views and normal maps that are essential for 3D reconstruction. By transforming the 3D generation challenge into a 2D image task, the method maximizes the use of knowledge from pretrained models, making it more efficient. The results show that Kiss3DGen not only generates high-quality 3D models but also supports advanced features like editing and texture enhancement."}, 'zh': {'title': 'ç®€å•é«˜æ•ˆçš„ä¸‰ç»´ç”Ÿæˆæ–¹æ³•', 'desc': 'æ‰©æ•£æ¨¡å‹åœ¨ç”ŸæˆäºŒç»´å›¾åƒæ–¹é¢å–å¾—äº†å·¨å¤§æˆåŠŸï¼Œä½†åœ¨ä¸‰ç»´å†…å®¹ç”Ÿæˆçš„è´¨é‡å’Œé€šç”¨æ€§ä¸Šä»ç„¶æœ‰é™ã€‚ç°æœ‰çš„å…ˆè¿›æ–¹æ³•é€šå¸¸éœ€è¦å¤§é‡çš„ä¸‰ç»´èµ„äº§è¿›è¡Œè®­ç»ƒï¼Œè¿™äº›èµ„äº§éš¾ä»¥æ”¶é›†ã€‚æˆ‘ä»¬æå‡ºäº†Kiss3DGenï¼ˆç®€å•ç›´æ¥çš„ä¸‰ç»´ç”Ÿæˆï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªé«˜æ•ˆçš„æ¡†æ¶ï¼Œé€šè¿‡é‡æ–°åˆ©ç”¨ç»è¿‡è‰¯å¥½è®­ç»ƒçš„äºŒç»´å›¾åƒæ‰©æ•£æ¨¡å‹æ¥ç”Ÿæˆã€ç¼–è¾‘å’Œå¢å¼ºä¸‰ç»´ç‰©ä½“ã€‚è¯¥æ–¹æ³•å°†ä¸‰ç»´ç”Ÿæˆé—®é¢˜è½¬åŒ–ä¸ºäºŒç»´å›¾åƒç”Ÿæˆä»»åŠ¡ï¼Œæœ€å¤§åŒ–åˆ©ç”¨é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ä¸­çš„çŸ¥è¯†ï¼Œèƒ½å¤Ÿæœ‰æ•ˆç”Ÿæˆé«˜è´¨é‡çš„ä¸‰ç»´æ¨¡å‹ã€‚'}}, 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2503.00455', 'title': 'PodAgent: A Comprehensive Framework for Podcast Generation', 'url': 'https://huggingface.co/papers/2503.00455', 'abstract': "Existing Existing automatic audio generation methods struggle to generate podcast-like audio programs effectively. The key challenges lie in in-depth content generation, appropriate and expressive voice production. This paper proposed PodAgent, a comprehensive framework for creating audio programs. PodAgent 1) generates informative topic-discussion content by designing a Host-Guest-Writer multi-agent collaboration system, 2) builds a voice pool for suitable voice-role matching and 3) utilizes LLM-enhanced speech synthesis method to generate expressive conversational speech. Given the absence of standardized evaluation criteria for podcast-like audio generation, we developed comprehensive assessment guidelines to effectively evaluate the model's performance. Experimental results demonstrate PodAgent's effectiveness, significantly surpassing direct GPT-4 generation in topic-discussion dialogue content, achieving an 87.4% voice-matching accuracy, and producing more expressive speech through LLM-guided synthesis. Demo page: https://podcast-agent.github.io/demo/. Source code: https://github.com/yujxx/PodAgent.", 'score': 5, 'issue_id': 2519, 'pub_date': '2025-03-01', 'pub_date_card': {'ru': '1 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 1', 'zh': '3æœˆ1æ—¥'}, 'hash': '59ce5f373a030894', 'authors': ['Yujia Xiao', 'Lei He', 'Haohan Guo', 'Fenglong Xie', 'Tan Lee'], 'affiliations': ['Microsoft', 'The Chinese University of Hong Kong', 'Xiaohongshu Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2503.00455.jpg', 'data': {'categories': ['#games', '#audio', '#interpretability', '#benchmark', '#optimization', '#multimodal'], 'emoji': 'ğŸ™ï¸', 'ru': {'title': 'PodAgent: Ğ˜Ğ˜-Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ´ĞºĞ°ÑÑ‚Ğ¾Ğ² Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ', 'desc': 'PodAgent - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼ Ğ² ÑÑ‚Ğ¸Ğ»Ğµ Ğ¿Ğ¾Ğ´ĞºĞ°ÑÑ‚Ğ¾Ğ². ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°, Ğ¿Ğ¾Ğ´Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ÑÑ‰Ğ¸Ğµ Ğ³Ğ¾Ğ»Ğ¾ÑĞ° Ğ¸Ğ· Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿ÑƒĞ»Ğ° Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¹ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ñ€ĞµÑ‡Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€ĞµÑˆĞ°ĞµÑ‚ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğ° ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ¸ Ğ²Ñ‹Ñ€Ğ°Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ³Ğ¾Ğ»Ğ¾ÑĞ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ PodAgent Ğ½Ğ°Ğ´ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ GPT-4 Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ² Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ´Ğ±Ğ¾Ñ€Ğ° Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ².'}, 'en': {'title': 'Revolutionizing Podcast Audio Generation with PodAgent', 'desc': "This paper introduces PodAgent, a novel framework designed to enhance the generation of podcast-like audio programs. It addresses key challenges in content creation and voice production by employing a multi-agent system that includes a Host, Guest, and Writer for collaborative topic discussions. Additionally, PodAgent features a voice pool for effective voice-role matching and utilizes a large language model (LLM) to improve the expressiveness of the generated speech. The framework's performance is validated through comprehensive evaluation guidelines, showing significant improvements over existing methods, including a high voice-matching accuracy and more engaging conversational audio."}, 'zh': {'title': 'PodAgentï¼šæ™ºèƒ½ç”Ÿæˆæ’­å®¢éŸ³é¢‘çš„å…¨æ–°æ¡†æ¶', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºPodAgentçš„æ¡†æ¶ï¼Œç”¨äºè‡ªåŠ¨ç”Ÿæˆç±»ä¼¼æ’­å®¢çš„éŸ³é¢‘èŠ‚ç›®ã€‚PodAgenté€šè¿‡è®¾è®¡ä¸€ä¸ªä¸»æŒäºº-å˜‰å®¾-ç¼–å‰§çš„å¤šæ™ºèƒ½ä½“åä½œç³»ç»Ÿï¼Œç”Ÿæˆæœ‰æ·±åº¦çš„ä¸»é¢˜è®¨è®ºå†…å®¹ã€‚åŒæ—¶ï¼Œå®ƒå»ºç«‹äº†ä¸€ä¸ªå£°éŸ³åº“ï¼Œä»¥å®ç°åˆé€‚çš„å£°éŸ³è§’è‰²åŒ¹é…ï¼Œå¹¶åˆ©ç”¨å¢å¼ºå‹å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œå¯Œæœ‰è¡¨ç°åŠ›çš„è¯­éŸ³åˆæˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPodAgentåœ¨ä¸»é¢˜è®¨è®ºå¯¹è¯å†…å®¹ç”Ÿæˆæ–¹é¢æ˜¾è‘—ä¼˜äºç›´æ¥ä½¿ç”¨GPT-4ï¼Œè¯­éŸ³åŒ¹é…å‡†ç¡®ç‡è¾¾åˆ°87.4%ï¼Œå¹¶é€šè¿‡LLMå¼•å¯¼çš„åˆæˆç”Ÿæˆäº†æ›´å…·è¡¨ç°åŠ›çš„è¯­éŸ³ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.01295', 'title': 'CodeArena: A Collective Evaluation Platform for LLM Code Generation', 'url': 'https://huggingface.co/papers/2503.01295', 'abstract': 'Large Language Models (LLMs) have reshaped code generation by synergizing their exceptional comprehension of natural language and programming syntax, thereby substantially boosting developer productivity. These advancements have prompted numerous efforts to quantitatively evaluate their coding capabilities. However, persistent challenges, such as benchmark leakage, data dissipation, and limited system accessibility, continue to impede a timely and accurate assessment. To address these limitations, we introduce CodeArena, an online evaluation framework tailored for LLM code generation. The key innovation is a collective evaluation mechanism, which dynamically recalibrates individual model scores based on the holistic performance of all participating models, mitigating score biases caused by widespread benchmark leakage. In addition, CodeArena ensures open access to all submitted solutions and test cases and provides automation-friendly APIs to streamline the code evaluation workflow. Our main contributions are: (1) a collective evaluation system for unbiased assessment, (2) a public repository of solutions and test cases, and (3) automation-ready APIs for seamless integration.', 'score': 5, 'issue_id': 2514, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 3', 'zh': '3æœˆ3æ—¥'}, 'hash': '96f50dd9e636b12e', 'authors': ['Mingzhe Du', 'Anh Tuan Luu', 'Bin Ji', 'Xiaobao Wu', 'Dong Huang', 'Terry Yue Zhuo', 'Qian Liu', 'See-Kiong Ng'], 'affiliations': ['ByteDance', 'Monash University', 'Nanyang Technological University', 'National University of Singapore', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2503.01295.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#leakage', '#open_source'], 'emoji': 'ğŸŸï¸', 'ru': {'title': 'CodeArena: Ğ¡Ğ¿Ñ€Ğ°Ğ²ĞµĞ´Ğ»Ğ¸Ğ²Ğ°Ñ Ğ°Ñ€ĞµĞ½Ğ° Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ LLM Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ°', 'desc': 'CodeArena - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ (LLM). ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ¾Ğ»Ğ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿ĞµÑ€ĞµÑÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¸Ğ½Ğ´Ğ¸Ğ²Ğ¸Ğ´ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ±Ñ‰ĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ²ÑĞµÑ… ÑƒÑ‡Ğ°ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ ÑĞ½Ğ¸Ğ·Ğ¸Ñ‚ÑŒ Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¾Ñ†ĞµĞ½Ğ¾Ğº, Ğ²Ñ‹Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ÑƒÑ‚ĞµÑ‡ĞºĞ¾Ğ¹ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. CodeArena Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ ĞºĞ¾ Ğ²ÑĞµĞ¼ Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¼ Ñ€ĞµÑˆĞµĞ½Ğ¸ÑĞ¼ Ğ¸ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ ÑĞ»ÑƒÑ‡Ğ°ÑĞ¼, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ API Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ¾Ñ†ĞµĞ½ĞºĞ¸.'}, 'en': {'title': 'Revolutionizing Code Evaluation with CodeArena', 'desc': 'This paper discusses the impact of Large Language Models (LLMs) on code generation, highlighting their ability to understand both natural language and programming syntax, which enhances developer productivity. It identifies ongoing issues in evaluating LLM coding capabilities, such as benchmark leakage and limited access to evaluation systems. To overcome these challenges, the authors present CodeArena, an online framework that offers a collective evaluation mechanism to provide unbiased assessments of LLMs. CodeArena also features a public repository for solutions and test cases, along with APIs for easy integration into existing workflows.'}, 'zh': {'title': 'CodeArenaï¼šå…¬å¹³è¯„ä¼°LLMä»£ç ç”Ÿæˆçš„åˆ›æ–°å¹³å°', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šè¿‡ç»“åˆå¯¹è‡ªç„¶è¯­è¨€å’Œç¼–ç¨‹è¯­æ³•çš„æ·±åˆ»ç†è§£ï¼Œæå¤§åœ°æå‡äº†ä»£ç ç”Ÿæˆçš„æ•ˆç‡ï¼Œè¿›è€Œæé«˜äº†å¼€å‘è€…çš„ç”Ÿäº§åŠ›ã€‚ä¸ºäº†é‡åŒ–è¯„ä¼°è¿™äº›æ¨¡å‹çš„ç¼–ç èƒ½åŠ›ï¼Œè®¸å¤šç ”ç©¶å·¥ä½œåº”è¿è€Œç”Ÿï¼Œä½†ä»é¢ä¸´åŸºå‡†æ³„æ¼ã€æ•°æ®æ¶ˆæ•£å’Œç³»ç»Ÿå¯è®¿é—®æ€§æœ‰é™ç­‰æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†CodeArenaï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºLLMä»£ç ç”Ÿæˆè®¾è®¡çš„åœ¨çº¿è¯„ä¼°æ¡†æ¶ã€‚å…¶æ ¸å¿ƒåˆ›æ–°åœ¨äºé›†ä½“è¯„ä¼°æœºåˆ¶ï¼Œèƒ½å¤Ÿæ ¹æ®æ‰€æœ‰å‚ä¸æ¨¡å‹çš„æ•´ä½“è¡¨ç°åŠ¨æ€è°ƒæ•´ä¸ªåˆ«æ¨¡å‹çš„è¯„åˆ†ï¼Œä»è€Œå‡å°‘å› åŸºå‡†æ³„æ¼é€ æˆçš„è¯„åˆ†åå·®ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.01807', 'title': 'Large-Scale Data Selection for Instruction Tuning', 'url': 'https://huggingface.co/papers/2503.01807', 'abstract': 'Selecting high-quality training data from a larger pool is a crucial step when instruction-tuning language models, as carefully curated datasets often produce models that outperform those trained on much larger, noisier datasets. Automated data selection approaches for instruction-tuning are typically tested by selecting small datasets (roughly 10k samples) from small pools (100-200k samples). However, popular deployed instruction-tuned models often train on hundreds of thousands to millions of samples, subsampled from even larger data pools. We present a systematic study of how well data selection methods scale to these settings, selecting up to 2.5M samples from pools of up to 5.8M samples and evaluating across 7 diverse tasks. We show that many recently proposed methods fall short of random selection in this setting (while using more compute), and even decline in performance when given access to larger pools of data to select over. However, we find that a variant of representation-based data selection (RDS+), which uses weighted mean pooling of pretrained LM hidden states, consistently outperforms more complex methods across all settings tested -- all whilst being more compute-efficient. Our findings highlight that the scaling properties of proposed automated selection methods should be more closely examined. We release our code, data, and models at https://github.com/hamishivi/automated-instruction-selection.', 'score': 5, 'issue_id': 2511, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 3', 'zh': '3æœˆ3æ—¥'}, 'hash': '8bbc980a9ef867f7', 'authors': ['Hamish Ivison', 'Muru Zhang', 'Faeze Brahman', 'Pang Wei Koh', 'Pradeep Dasigi'], 'affiliations': ['Allen Institute for AI', 'University of Southern California', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2503.01807.jpg', 'data': {'categories': ['#data', '#open_source', '#optimization', '#dataset', '#training'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¾Ñ‚Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´ÑÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ´Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¾Ğ±ÑŠĞµĞ¼Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°Ñ Ğ´Ğ¾ 2,5 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ² Ğ¸Ğ· Ğ¿ÑƒĞ»Ğ¾Ğ² Ğ´Ğ¾ 5,8 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¸Ğµ Ğ½ĞµĞ´Ğ°Ğ²Ğ½Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ÑƒÑÑ‚ÑƒĞ¿Ğ°ÑÑ‚ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾Ğ¼Ñƒ Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ñƒ Ğ² ÑÑ‚Ğ¸Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ…, Ğ¾Ğ´Ğ½Ğ°ĞºĞ¾ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ (RDS+) Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Quality Over Quantity: Smart Data Selection for Language Models', 'desc': 'This paper investigates the importance of selecting high-quality training data for instruction-tuning language models. It reveals that many automated data selection methods do not perform better than random selection when scaling to larger datasets, which can include millions of samples. The study introduces a representation-based data selection method (RDS+) that consistently outperforms more complex approaches while being more efficient in terms of computational resources. The authors emphasize the need for a deeper examination of how these selection methods behave as the size of the data pools increases.'}, 'zh': {'title': 'é«˜æ•ˆé€‰æ‹©ï¼šä¼˜åŒ–è¯­è¨€æ¨¡å‹è®­ç»ƒæ•°æ®çš„å…³é”®', 'desc': 'åœ¨å¯¹è¯­è¨€æ¨¡å‹è¿›è¡ŒæŒ‡ä»¤è°ƒä¼˜æ—¶ï¼Œä»æ›´å¤§æ•°æ®é›†ä¸­é€‰æ‹©é«˜è´¨é‡çš„è®­ç»ƒæ•°æ®æ˜¯ä¸€ä¸ªå…³é”®æ­¥éª¤ã€‚ç»è¿‡ç²¾å¿ƒç­–åˆ’çš„æ•°æ®é›†é€šå¸¸èƒ½äº§ç”Ÿæ¯”é‚£äº›åœ¨æ›´å¤§ã€æ›´å˜ˆæ‚çš„æ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¨¡å‹æ›´å¥½çš„æ•ˆæœã€‚æˆ‘ä»¬è¿›è¡Œäº†ç³»ç»Ÿç ”ç©¶ï¼Œè¯„ä¼°æ•°æ®é€‰æ‹©æ–¹æ³•åœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šçš„è¡¨ç°ï¼Œå‘ç°è®¸å¤šæ–°æå‡ºçš„æ–¹æ³•åœ¨è¿™ç§æƒ…å†µä¸‹çš„è¡¨ç°ä¸å¦‚éšæœºé€‰æ‹©ã€‚æˆ‘ä»¬è¿˜å‘ç°ä¸€ç§åŸºäºè¡¨ç¤ºçš„æ•°æ®é€‰æ‹©å˜ä½“ï¼ˆRDS+ï¼‰åœ¨æ‰€æœ‰æµ‹è¯•è®¾ç½®ä¸­å§‹ç»ˆä¼˜äºæ›´å¤æ‚çš„æ–¹æ³•ï¼ŒåŒæ—¶è®¡ç®—æ•ˆç‡æ›´é«˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.00031', 'title': 'Efficient Test-Time Scaling via Self-Calibration', 'url': 'https://huggingface.co/papers/2503.00031', 'abstract': 'Increasing test-time computation is a straightforward approach to enhancing the quality of responses in Large Language Models (LLMs). While Best-of-N sampling and Self-Consistency with majority voting are simple and effective, they require a fixed number of sampling responses for each query, regardless of its complexity. This could result in wasted computation for simpler questions and insufficient exploration for more challenging ones. In this work, we argue that model confidence of responses can be used for improving the efficiency of test-time scaling. Unfortunately, LLMs are known to be overconfident and provide unreliable confidence estimation. To address this limitation, we introduce Self-Calibration by distilling Self-Consistency-derived confidence into the model itself. This enables reliable confidence estimation at test time with one forward pass. We then design confidence-based efficient test-time scaling methods to handle queries of various difficulty, such as Early-Stopping for Best-of-N and Self-Consistency with calibrated confidence. Experiments on three LLMs across six datasets demonstrate the effectiveness of our approach. Specifically, applying confidence-based Early Stopping to Best-of-N improves MathQA accuracy from 81.0 to 83.6 with a sample budget of 16 responses, indicating the efficacy of confidence-based sampling strategy at inference time.', 'score': 4, 'issue_id': 2523, 'pub_date': '2025-02-25', 'pub_date_card': {'ru': '25 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 25', 'zh': '2æœˆ25æ—¥'}, 'hash': 'fad9bb0721d1d6ce', 'authors': ['Chengsong Huang', 'Langlin Huang', 'Jixuan Leng', 'Jiacheng Liu', 'Jiaxin Huang'], 'affiliations': ['Carnegie Mellon University', 'University of Washington', 'Washington University in St. Louis'], 'pdf_title_img': 'assets/pdf/title_img/2503.00031.jpg', 'data': {'categories': ['#optimization', '#training', '#inference'], 'emoji': 'ğŸ¯', 'ru': {'title': 'ĞŸĞ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ LLM Ñ‡ĞµÑ€ĞµĞ· ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºÑƒ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸', 'desc': 'Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Self-Calibration Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¿Ñ€Ğ¸ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ· Self-Consistency, Ğ² ÑĞ°Ğ¼Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ·Ğ° Ğ¾Ğ´Ğ¸Ğ½ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Early-Stopping Ğ´Ğ»Ñ Best-of-N Ğ¸ Self-Consistency Ñ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒÑ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ñ‚Ñ€ĞµÑ… LLM Ğ¸ ÑˆĞµÑÑ‚Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°, Ğ² Ñ‡Ğ°ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸, ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ MathQA Ñ 81.0 Ğ´Ğ¾ 83.6 Ğ¿Ñ€Ğ¸ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğµ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ Ğ² 16 Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ².'}, 'en': {'title': 'Enhancing LLM Efficiency with Confidence-Based Sampling', 'desc': 'This paper discusses how to improve the efficiency of Large Language Models (LLMs) during testing by using model confidence to guide response sampling. Traditional methods like Best-of-N sampling and Self-Consistency require a fixed number of responses, which can lead to wasted resources or inadequate exploration of complex queries. The authors propose a technique called Self-Calibration, which helps LLMs provide more reliable confidence estimates by distilling information from previous responses. By implementing confidence-based strategies such as Early-Stopping, the paper shows that it is possible to enhance accuracy while reducing unnecessary computations, particularly in challenging tasks like MathQA.'}, 'zh': {'title': 'æå‡å¤§å‹è¯­è¨€æ¨¡å‹å“åº”è´¨é‡çš„è‡ªæˆ‘æ ¡å‡†æ–¹æ³•', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†å¦‚ä½•é€šè¿‡å¢åŠ æµ‹è¯•æ—¶çš„è®¡ç®—æ¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å“åº”è´¨é‡ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§è‡ªæˆ‘æ ¡å‡†çš„æ–¹æ³•ï¼Œé€šè¿‡å°†è‡ªæˆ‘ä¸€è‡´æ€§ç”Ÿæˆçš„ç½®ä¿¡åº¦æç‚¼åˆ°æ¨¡å‹ä¸­ï¼Œä»è€Œæ”¹å–„ç½®ä¿¡åº¦ä¼°è®¡çš„å¯é æ€§ã€‚è¿™æ ·ï¼Œæ¨¡å‹åœ¨æµ‹è¯•æ—¶å¯ä»¥åœ¨ä¸€æ¬¡å‰å‘ä¼ æ’­ä¸­è·å¾—å¯é çš„ç½®ä¿¡åº¦ä¼°è®¡ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒåŸºäºç½®ä¿¡åº¦çš„æ—©åœç­–ç•¥èƒ½å¤Ÿæœ‰æ•ˆæé«˜æ¨¡å‹åœ¨ä¸åŒéš¾åº¦é—®é¢˜ä¸Šçš„è¡¨ç°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.01714', 'title': "Word Form Matters: LLMs' Semantic Reconstruction under Typoglycemia", 'url': 'https://huggingface.co/papers/2503.01714', 'abstract': "Human readers can efficiently comprehend scrambled words, a phenomenon known as Typoglycemia, primarily by relying on word form; if word form alone is insufficient, they further utilize contextual cues for interpretation. While advanced large language models (LLMs) exhibit similar abilities, the underlying mechanisms remain unclear. To investigate this, we conduct controlled experiments to analyze the roles of word form and contextual information in semantic reconstruction and examine LLM attention patterns. Specifically, we first propose SemRecScore, a reliable metric to quantify the degree of semantic reconstruction, and validate its effectiveness. Using this metric, we study how word form and contextual information influence LLMs' semantic reconstruction ability, identifying word form as the core factor in this process. Furthermore, we analyze how LLMs utilize word form and find that they rely on specialized attention heads to extract and process word form information, with this mechanism remaining stable across varying levels of word scrambling. This distinction between LLMs' fixed attention patterns primarily focused on word form and human readers' adaptive strategy in balancing word form and contextual information provides insights into enhancing LLM performance by incorporating human-like, context-aware mechanisms.", 'score': 4, 'issue_id': 2517, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 3', 'zh': '3æœˆ3æ—¥'}, 'hash': '4880ed4c044081c4', 'authors': ['Chenxi Wang', 'Tianle Gu', 'Zhongyu Wei', 'Lang Gao', 'Zirui Song', 'Xiuying Chen'], 'affiliations': ['Fudan University', 'Mohamed bin Zayed University of Artificial Intelligence (MBZUAI)'], 'pdf_title_img': 'assets/pdf/title_img/2503.01714.jpg', 'data': {'categories': ['#training', '#interpretability', '#data', '#multimodal', '#alignment'], 'emoji': 'ğŸ”€', 'ru': {'title': 'Ğ¤Ğ¾Ñ€Ğ¼Ğ° ÑĞ»Ğ¾Ğ²Ğ° - ĞºĞ»ÑÑ‡ Ğº Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¿ĞµÑ€ĞµĞ¼ĞµÑˆĞ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ´Ğ»Ñ Ğ˜Ğ˜', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ¿ĞµÑ€ĞµĞ¼ĞµÑˆĞ°Ğ½Ğ½Ñ‹Ğµ ÑĞ»Ğ¾Ğ²Ğ°, Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾ Ğ»ÑĞ´ÑĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ SemRecScore Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ñ€Ğ¾Ğ»ÑŒ Ñ„Ğ¾Ñ€Ğ¼Ñ‹ ÑĞ»Ğ¾Ğ²Ğ° Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ„Ğ¾Ñ€Ğ¼Ğ° ÑĞ»Ğ¾Ğ²Ğ° ÑĞ²Ğ»ÑĞµÑ‚ÑÑ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¾Ğ¼ Ğ´Ğ»Ñ LLM Ğ¿Ñ€Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ¿ĞµÑ€ĞµĞ¼ĞµÑˆĞ°Ğ½Ğ½Ñ‹Ñ… ÑĞ»Ğ¾Ğ². ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ² Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ LLM Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ Ñ„Ğ¾Ñ€Ğ¼Ğµ ÑĞ»Ğ¾Ğ²Ğ°.'}, 'en': {'title': 'Unlocking LLMs: The Power of Word Form in Understanding Scrambled Text', 'desc': 'This paper explores how large language models (LLMs) understand scrambled words, similar to how humans do through a phenomenon called Typoglycemia. The authors introduce a new metric, SemRecScore, to measure how well LLMs can reconstruct meaning from scrambled text by focusing on word form and context. Their experiments reveal that LLMs primarily depend on word form for semantic reconstruction, utilizing specific attention heads to process this information. The findings suggest that incorporating more human-like, context-aware strategies could improve LLM performance in understanding language.'}, 'zh': {'title': 'æ­ç¤ºå¤§å‹è¯­è¨€æ¨¡å‹çš„è¯­ä¹‰é‡å»ºæœºåˆ¶', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è¯­ä¹‰é‡å»ºä¸­çš„èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯å®ƒä»¬å¦‚ä½•åˆ©ç”¨å•è¯å½¢å¼å’Œä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„åº¦é‡æ ‡å‡†SemRecScoreï¼Œç”¨äºé‡åŒ–è¯­ä¹‰é‡å»ºçš„ç¨‹åº¦ï¼Œå¹¶éªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚ç ”ç©¶å‘ç°ï¼Œå•è¯å½¢å¼æ˜¯å½±å“LLMsè¯­ä¹‰é‡å»ºèƒ½åŠ›çš„æ ¸å¿ƒå› ç´ ï¼Œä¸”LLMsé€šè¿‡ä¸“é—¨çš„æ³¨æ„åŠ›å¤´æ¥æå–å’Œå¤„ç†å•è¯å½¢å¼ä¿¡æ¯ã€‚ä¸äººç±»è¯»è€…åœ¨å•è¯å½¢å¼å’Œä¸Šä¸‹æ–‡ä¿¡æ¯ä¹‹é—´çš„çµæ´»ç­–ç•¥ä¸åŒï¼ŒLLMsçš„æ³¨æ„åŠ›æ¨¡å¼ä¸»è¦é›†ä¸­åœ¨å•è¯å½¢å¼ä¸Šï¼Œè¿™ä¸ºæå‡LLMsæ€§èƒ½æä¾›äº†æ–°çš„æ€è·¯ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.19402', 'title': 'General Reasoning Requires Learning to Reason from the Get-go', 'url': 'https://huggingface.co/papers/2502.19402', 'abstract': "Large Language Models (LLMs) have demonstrated impressive real-world utility, exemplifying artificial useful intelligence (AUI). However, their ability to reason adaptively and robustly -- the hallmarks of artificial general intelligence (AGI) -- remains fragile. While LLMs seemingly succeed in commonsense reasoning, programming, and mathematics, they struggle to generalize algorithmic understanding across novel contexts. Our experiments with algorithmic tasks in esoteric programming languages reveal that LLM's reasoning overfits to the training data and is limited in its transferability. We hypothesize that the core issue underlying such limited transferability is the coupling of reasoning and knowledge in LLMs.   To transition from AUI to AGI, we propose disentangling knowledge and reasoning through three key directions: (1) pretaining to reason using RL from scratch as an alternative to the widely used next-token prediction pretraining, (2) using a curriculum of synthetic tasks to ease the learning of a reasoning prior for RL that can then be transferred to natural language tasks, and (3) learning more generalizable reasoning functions using a small context window to reduce exploiting spurious correlations between tokens. Such a reasoning system coupled with a trained retrieval system and a large external memory bank as a knowledge store can overcome several limitations of existing architectures at learning to reason in novel scenarios.", 'score': 3, 'issue_id': 2520, 'pub_date': '2025-02-26', 'pub_date_card': {'ru': '26 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 26', 'zh': '2æœˆ26æ—¥'}, 'hash': '5774d50d6c5a9361', 'authors': ['Seungwook Han', 'Jyothish Pari', 'Samuel J. Gershman', 'Pulkit Agrawal'], 'affiliations': ['Department of Psychology and Center for Brain Science, Harvard University', 'Improbable AI Lab, MIT'], 'pdf_title_img': 'assets/pdf/title_img/2502.19402.jpg', 'data': {'categories': ['#agi', '#transfer_learning', '#architecture', '#rl', '#synthetic', '#training', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞÑ‚Ğ´ĞµĞ»ÑÑ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹: Ğ¿ÑƒÑ‚ÑŒ Ğº AGI', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ½ĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ¸Ñ… Ğ²Ğ¿ĞµÑ‡Ğ°Ñ‚Ğ»ÑÑÑ‰ÑƒÑ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ÑÑ‚ÑŒ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ»ÑÑÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ LLM Ğ½Ğ° Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ€Ğ°Ğ·Ğ´ĞµĞ»Ğ¸Ñ‚ÑŒ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ° Ğº Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ±Ñ‰ĞµĞ¼Ñƒ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ñƒ (AGI). ĞŸÑ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ĞµĞ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ¢Ğ°ĞºĞ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² ÑĞ¾Ñ‡ĞµÑ‚Ğ°Ğ½Ğ¸Ğ¸ Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ¹ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµÑ‚ÑŒ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€.'}, 'en': {'title': 'Disentangling Knowledge and Reasoning for Robust AI', 'desc': "This paper discusses the limitations of Large Language Models (LLMs) in achieving robust reasoning capabilities, which are essential for artificial general intelligence (AGI). The authors identify that LLMs often overfit to their training data, leading to poor generalization in novel algorithmic tasks. They propose a solution that involves separating knowledge from reasoning by employing reinforcement learning (RL) and a structured curriculum of synthetic tasks. By enhancing reasoning functions and integrating a retrieval system with an external memory, the authors aim to improve LLMs' adaptability and performance in unfamiliar contexts."}, 'zh': {'title': 'è§£è€¦çŸ¥è¯†ä¸æ¨ç†ï¼Œè¿ˆå‘äººå·¥é€šç”¨æ™ºèƒ½', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å®é™…åº”ç”¨ä¸­è¡¨ç°å‡ºè‰²ï¼Œå±•ç¤ºäº†äººå·¥æœ‰ç”¨æ™ºèƒ½ï¼ˆAUIï¼‰çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨è‡ªé€‚åº”å’Œç¨³å¥æ¨ç†æ–¹é¢çš„èƒ½åŠ›ä»ç„¶è„†å¼±ï¼Œè¿™æ˜¯äººå·¥é€šç”¨æ™ºèƒ½ï¼ˆAGIï¼‰çš„æ ‡å¿—ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒLLMsåœ¨ç®—æ³•ä»»åŠ¡ä¸­å®¹æ˜“è¿‡æ‹Ÿåˆè®­ç»ƒæ•°æ®ï¼Œä¸”åœ¨æ–°ç¯å¢ƒä¸­çš„è¿ç§»èƒ½åŠ›æœ‰é™ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºé€šè¿‡ä¸‰ç§å…³é”®æ–¹å‘æ¥è§£è€¦çŸ¥è¯†ä¸æ¨ç†ï¼Œä»¥ä¿ƒè¿›ä»AUIå‘AGIçš„è¿‡æ¸¡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.01739', 'title': 'VideoUFO: A Million-Scale User-Focused Dataset for Text-to-Video Generation', 'url': 'https://huggingface.co/papers/2503.01739', 'abstract': "Text-to-video generative models convert textual prompts into dynamic visual content, offering wide-ranging applications in film production, gaming, and education. However, their real-world performance often falls short of user expectations. One key reason is that these models have not been trained on videos related to some topics users want to create. In this paper, we propose VideoUFO, the first Video dataset specifically curated to align with Users' FOcus in real-world scenarios. Beyond this, our VideoUFO also features: (1) minimal (0.29%) overlap with existing video datasets, and (2) videos searched exclusively via YouTube's official API under the Creative Commons license. These two attributes provide future researchers with greater freedom to broaden their training sources. The VideoUFO comprises over 1.09 million video clips, each paired with both a brief and a detailed caption (description). Specifically, through clustering, we first identify 1,291 user-focused topics from the million-scale real text-to-video prompt dataset, VidProM. Then, we use these topics to retrieve videos from YouTube, split the retrieved videos into clips, and generate both brief and detailed captions for each clip. After verifying the clips with specified topics, we are left with about 1.09 million video clips. Our experiments reveal that (1) current 16 text-to-video models do not achieve consistent performance across all user-focused topics; and (2) a simple model trained on VideoUFO outperforms others on worst-performing topics. The dataset is publicly available at https://huggingface.co/datasets/WenhaoWang/VideoUFO under the CC BY 4.0 License.", 'score': 3, 'issue_id': 2512, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 3', 'zh': '3æœˆ3æ—¥'}, 'hash': '046fdeee8939e82c', 'authors': ['Wenhao Wang', 'Yi Yang'], 'affiliations': ['University of Technology Sydney', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.01739.jpg', 'data': {'categories': ['#video', '#dataset', '#data', '#games', '#open_source'], 'emoji': 'ğŸ¬', 'ru': {'title': 'VideoUFO: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑƒ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ VideoUFO - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ±Ğ¾Ğ»ĞµĞµ 1,09 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ»Ğ¸Ğ¿Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ ĞºÑ€Ğ°Ñ‚ĞºĞ¸Ğ¼Ğ¸ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ÑĞ¼Ğ¸, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… 1291 Ñ‚ĞµĞ¼Ñƒ, Ğ°ĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹. VideoUFO Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ°ĞµÑ‚ÑÑ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¿ĞµÑ€ĞµÑĞµÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾Ğ´ Ğ»Ğ¸Ñ†ĞµĞ½Ğ·Ğ¸ĞµĞ¹ Creative Commons. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° VideoUFO, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ‚ĞµĞ¼Ğ°Ñ….'}, 'en': {'title': 'Empowering Text-to-Video Models with User-Focused Data', 'desc': 'This paper introduces VideoUFO, a novel video dataset designed to enhance text-to-video generative models by focusing on user-relevant topics. The dataset contains over 1.09 million video clips, each accompanied by both brief and detailed captions, ensuring minimal overlap with existing datasets. By clustering user prompts, the authors identified 1,291 specific topics to guide video retrieval from YouTube, which were then segmented into clips. Experiments show that models trained on VideoUFO significantly outperform existing models, particularly on challenging topics, highlighting the importance of tailored training data in machine learning applications.'}, 'zh': {'title': 'æå‡æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆçš„ç”¨æˆ·ä½“éªŒ', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„è§†é¢‘æ•°æ®é›†VideoUFOï¼Œæ—¨åœ¨æé«˜æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆæ¨¡å‹çš„æ€§èƒ½ã€‚è¯¥æ•°æ®é›†ä¸“æ³¨äºç”¨æˆ·å…³æ³¨çš„ä¸»é¢˜ï¼ŒåŒ…å«è¶…è¿‡109ä¸‡ä¸ªè§†é¢‘ç‰‡æ®µï¼Œå¹¶ä¸ºæ¯ä¸ªç‰‡æ®µæä¾›ç®€çŸ­å’Œè¯¦ç»†çš„æè¿°ã€‚VideoUFOä¸ç°æœ‰æ•°æ®é›†çš„é‡å ç‡æä½ï¼Œä¸”æ‰€æœ‰è§†é¢‘å‡é€šè¿‡YouTubeçš„å®˜æ–¹APIè·å–ï¼Œç¡®ä¿äº†æ•°æ®çš„å¤šæ ·æ€§å’Œåˆæ³•æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨VideoUFOè®­ç»ƒçš„æ¨¡å‹åœ¨ç”¨æˆ·å…³æ³¨çš„ä¸»é¢˜ä¸Šè¡¨ç°ä¼˜äºå…¶ä»–æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.01103', 'title': 'Direct Discriminative Optimization: Your Likelihood-Based Visual Generative Model is Secretly a GAN Discriminator', 'url': 'https://huggingface.co/papers/2503.01103', 'abstract': 'While likelihood-based generative models, particularly diffusion and autoregressive models, have achieved remarkable fidelity in visual generation, the maximum likelihood estimation (MLE) objective inherently suffers from a mode-covering tendency that limits the generation quality under limited model capacity. In this work, we propose Direct Discriminative Optimization (DDO) as a unified framework that bridges likelihood-based generative training and the GAN objective to bypass this fundamental constraint. Our key insight is to parameterize a discriminator implicitly using the likelihood ratio between a learnable target model and a fixed reference model, drawing parallels with the philosophy of Direct Preference Optimization (DPO). Unlike GANs, this parameterization eliminates the need for joint training of generator and discriminator networks, allowing for direct, efficient, and effective finetuning of a well-trained model to its full potential beyond the limits of MLE. DDO can be performed iteratively in a self-play manner for progressive model refinement, with each round requiring less than 1% of pretraining epochs. Our experiments demonstrate the effectiveness of DDO by significantly advancing the previous SOTA diffusion model EDM, reducing FID scores from 1.79/1.58 to new records of 1.30/0.97 on CIFAR-10/ImageNet-64 datasets, and by consistently improving both guidance-free and CFG-enhanced FIDs of visual autoregressive models on ImageNet 256times256.', 'score': 2, 'issue_id': 2517, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 3', 'zh': '3æœˆ3æ—¥'}, 'hash': 'd8b58c1a2c49da16', 'authors': ['Kaiwen Zheng', 'Yongxin Chen', 'Huayu Chen', 'Guande He', 'Ming-Yu Liu', 'Jun Zhu', 'Qinsheng Zhang'], 'affiliations': ['NVIDIA', 'The University of Texas at', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.01103.jpg', 'data': {'categories': ['#training', '#diffusion', '#cv', '#optimization'], 'emoji': 'ğŸš€', 'ru': {'title': 'DDO: ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ MLE', 'desc': 'ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑÑ‚Ğ°Ñ‚ÑŒĞ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Direct Discriminative Optimization (DDO). Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ¸Ñ Ğ¸ Ñ†ĞµĞ»Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾-ÑĞ¾ÑÑ‚ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹ (GAN), Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµÑ‚ÑŒ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ¸Ñ (MLE). DDO Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ğ¾Ğ¹ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ğ¸ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ğ´Ğ»Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ¸ÑĞºÑ€Ğ¸Ğ¼Ğ¸Ğ½Ğ°Ñ‚Ğ¾Ñ€Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ DDO Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Enhancing Generative Models with Direct Discriminative Optimization', 'desc': 'This paper introduces Direct Discriminative Optimization (DDO), a new framework that enhances the performance of generative models by combining likelihood-based training with concepts from Generative Adversarial Networks (GANs). DDO addresses the limitations of maximum likelihood estimation (MLE) by using a discriminator that is parameterized through the likelihood ratio of a target model and a fixed reference model. This approach allows for efficient finetuning of pre-trained models without the need for joint training of generator and discriminator networks. The results show that DDO significantly improves the state-of-the-art performance in visual generation tasks, achieving lower FID scores on popular datasets like CIFAR-10 and ImageNet.'}, 'zh': {'title': 'ç›´æ¥åˆ¤åˆ«ä¼˜åŒ–ï¼šæå‡ç”Ÿæˆæ¨¡å‹çš„æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œç§°ä¸ºç›´æ¥åˆ¤åˆ«ä¼˜åŒ–ï¼ˆDDOï¼‰ï¼Œæ—¨åœ¨æé«˜ç”Ÿæˆæ¨¡å‹çš„è´¨é‡ã€‚DDOé€šè¿‡å°†ç”Ÿæˆè®­ç»ƒä¸GANç›®æ ‡ç»“åˆï¼Œå…‹æœäº†æœ€å¤§ä¼¼ç„¶ä¼°è®¡ï¼ˆMLEï¼‰åœ¨æ¨¡å‹å®¹é‡æœ‰é™æ—¶çš„å±€é™æ€§ã€‚è¯¥æ–¹æ³•é€šè¿‡ä½¿ç”¨å¯å­¦ä¹ çš„ç›®æ ‡æ¨¡å‹ä¸å›ºå®šå‚è€ƒæ¨¡å‹ä¹‹é—´çš„ä¼¼ç„¶æ¯”æ¥éšå¼å‚æ•°åŒ–åˆ¤åˆ«å™¨ï¼Œä»è€Œç®€åŒ–äº†ç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨çš„è”åˆè®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDDOæ˜¾è‘—æé«˜äº†ç°æœ‰æ‰©æ•£æ¨¡å‹çš„æ€§èƒ½ï¼Œå¹¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šåˆ›é€ äº†æ–°çš„è®°å½•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.16779', 'title': 'Unposed Sparse Views Room Layout Reconstruction in the Age of Pretrain Model', 'url': 'https://huggingface.co/papers/2502.16779', 'abstract': 'Room layout estimation from multiple-perspective images is poorly investigated due to the complexities that emerge from multi-view geometry, which requires muti-step solutions such as camera intrinsic and extrinsic estimation, image matching, and triangulation. However, in 3D reconstruction, the advancement of recent 3D foundation models such as DUSt3R has shifted the paradigm from the traditional multi-step structure-from-motion process to an end-to-end single-step approach. To this end, we introduce Plane-DUSt3R, a novel method for multi-view room layout estimation leveraging the 3D foundation model DUSt3R. Plane-DUSt3R incorporates the DUSt3R framework and fine-tunes on a room layout dataset (Structure3D) with a modified objective to estimate structural planes. By generating uniform and parsimonious results, Plane-DUSt3R enables room layout estimation with only a single post-processing step and 2D detection results. Unlike previous methods that rely on single-perspective or panorama image, Plane-DUSt3R extends the setting to handle multiple-perspective images. Moreover, it offers a streamlined, end-to-end solution that simplifies the process and reduces error accumulation. Experimental results demonstrate that Plane-DUSt3R not only outperforms state-of-the-art methods on the synthetic dataset but also proves robust and effective on in the wild data with different image styles such as cartoon.Our code is available at: https://github.com/justacar/Plane-DUSt3R', 'score': 2, 'issue_id': 2516, 'pub_date': '2025-02-24', 'pub_date_card': {'ru': '24 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 24', 'zh': '2æœˆ24æ—¥'}, 'hash': '4a9f6cc2fb1ab840', 'authors': ['Yaxuan Huang', 'Xili Dai', 'Jianan Wang', 'Xianbiao Qi', 'Yixing Yuan', 'Xiangyu Yue'], 'affiliations': ['Astribot', 'Hong Kong Center for Construction Robotics, The Hong Kong University of Science and Technology', 'Intellifusion Inc.', 'MMLab, The Chinese University of Hong Kong', 'The Hong Kong University of Science and Technology (Guangzhou)'], 'pdf_title_img': 'assets/pdf/title_img/2502.16779.jpg', 'data': {'categories': ['#3d', '#optimization', '#cv', '#synthetic'], 'emoji': 'ğŸ ', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ¿Ğ¾Ğ¼ĞµÑ‰ĞµĞ½Ğ¸Ğ¹: Ğ¾Ñ‚ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ° ÑˆĞ°Ğ³Ğ¾Ğ² Ğº ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¼Ñƒ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Plane-DUSt3R - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ¿Ğ¾Ğ¼ĞµÑ‰ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ñ€Ğ°ĞºÑƒÑ€ÑĞ°Ğ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ 3D-Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ DUSt3R Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Structure3D Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ñ… Ğ¿Ğ»Ğ¾ÑĞºĞ¾ÑÑ‚ĞµĞ¹. Plane-DUSt3R Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ ÑƒĞ¿Ñ€Ğ¾Ñ‰ĞµĞ½Ğ½Ğ¾Ğµ ÑĞºĞ²Ğ¾Ğ·Ğ½Ğ¾Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ… Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ‚Ğ¸Ğ»ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²ĞºÑƒ Ğ¿Ğ¾Ğ¼ĞµÑ‰ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ ÑˆĞ°Ğ³Ğ° Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² 2D-Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Revolutionizing Room Layout Estimation with Plane-DUSt3R', 'desc': 'This paper presents Plane-DUSt3R, a new method for estimating room layouts from multiple images taken from different perspectives. It builds on the DUSt3R 3D foundation model, moving away from traditional multi-step processes to a more efficient end-to-end approach. By fine-tuning the model on a specific dataset, Plane-DUSt3R can accurately identify structural planes with minimal post-processing. The results show that this method not only surpasses existing techniques on synthetic data but also performs well on real-world images with varying styles.'}, 'zh': {'title': 'ç®€åŒ–å¤šè§†è§’æˆ¿é—´å¸ƒå±€ä¼°è®¡çš„å…¨æ–°æ–¹æ³•', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¤šè§†è§’æˆ¿é—´å¸ƒå±€ä¼°è®¡æ–¹æ³•ï¼Œç§°ä¸ºPlane-DUSt3Rã€‚è¯¥æ–¹æ³•åˆ©ç”¨äº†å…ˆè¿›çš„3DåŸºç¡€æ¨¡å‹DUSt3Rï¼Œç®€åŒ–äº†ä¼ ç»Ÿçš„å¤šæ­¥éª¤æµç¨‹ï¼Œé‡‡ç”¨ç«¯åˆ°ç«¯çš„å•æ­¥éª¤æ–¹æ³•ã€‚Plane-DUSt3Ré€šè¿‡åœ¨æˆ¿é—´å¸ƒå±€æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒï¼Œèƒ½å¤Ÿæœ‰æ•ˆä¼°è®¡ç»“æ„å¹³é¢ï¼Œå¹¶ç”Ÿæˆä¸€è‡´ä¸”ç®€æ´çš„ç»“æœã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPlane-DUSt3Råœ¨åˆæˆæ•°æ®é›†ä¸Šè¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œå¹¶åœ¨ä¸åŒé£æ ¼çš„çœŸå®æ•°æ®ä¸Šè¡¨ç°å‡ºè‰²ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.00729', 'title': 'CLEA: Closed-Loop Embodied Agent for Enhancing Task Execution in Dynamic Environments', 'url': 'https://huggingface.co/papers/2503.00729', 'abstract': "Large Language Models (LLMs) exhibit remarkable capabilities in the hierarchical decomposition of complex tasks through semantic reasoning. However, their application in embodied systems faces challenges in ensuring reliable execution of subtask sequences and achieving one-shot success in long-term task completion. To address these limitations in dynamic environments, we propose Closed-Loop Embodied Agent (CLEA) -- a novel architecture incorporating four specialized open-source LLMs with functional decoupling for closed-loop task management. The framework features two core innovations: (1) Interactive task planner that dynamically generates executable subtasks based on the environmental memory, and (2) Multimodal execution critic employing an evaluation framework to conduct a probabilistic assessment of action feasibility, triggering hierarchical re-planning mechanisms when environmental perturbations exceed preset thresholds. To validate CLEA's effectiveness, we conduct experiments in a real environment with manipulable objects, using two heterogeneous robots for object search, manipulation, and search-manipulation integration tasks. Across 12 task trials, CLEA outperforms the baseline model, achieving a 67.3% improvement in success rate and a 52.8% increase in task completion rate. These results demonstrate that CLEA significantly enhances the robustness of task planning and execution in dynamic environments.", 'score': 2, 'issue_id': 2514, 'pub_date': '2025-03-02', 'pub_date_card': {'ru': '2 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 2', 'zh': '3æœˆ2æ—¥'}, 'hash': '57f6361f66ec99cf', 'authors': ['Mingcong Lei', 'Ge Wang', 'Yiming Zhao', 'Zhixin Mai', 'Qing Zhao', 'Yao Guo', 'Zhen Li', 'Shuguang Cui', 'Yatong Han', 'Jinke Ren'], 'affiliations': ['Guangdong Provincial Key Laboratory of Future Networks of Intelligence, The Chinese University of Hong Kong, Shenzhen', 'Harbin Engineering University, Harbin', 'Infused Synapse AI, Shenzhen', 'Institute of Medical Robotics, School of Biomedical Engineering, Shanghai Jiao Tong University, Shanghai', 'School of Science and Engineering (SSE), FNii-Shenzhen', 'Shenzhen Future Network of Intelligence Institute (FNii-Shenzhen)'], 'pdf_title_img': 'assets/pdf/title_img/2503.00729.jpg', 'data': {'categories': ['#architecture', '#reasoning', '#open_source', '#robotics', '#agents', '#optimization'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'CLEA: ĞŸĞ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ğ¼Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ CLEA (Closed-Loop Embodied Agent) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ğ¼Ğ¸ Ğ² Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ€ĞµĞ´Ğ°Ñ…. CLEA Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ğ² Ğ·Ğ°Ğ¼ĞºĞ½ÑƒÑ‚Ğ¾Ğ¼ Ñ†Ğ¸ĞºĞ»Ğµ. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºÑ€Ğ¸Ñ‚Ğ¸Ğº Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ CLEA Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑĞ¼ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ€ĞµĞ´Ğµ Ñ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Enhancing Task Execution in Dynamic Environments with CLEA', 'desc': 'This paper introduces the Closed-Loop Embodied Agent (CLEA), a new architecture designed to improve the performance of Large Language Models (LLMs) in dynamic environments. CLEA features an interactive task planner that creates subtasks based on real-time environmental data, allowing for better adaptability. Additionally, it includes a multimodal execution critic that evaluates the feasibility of actions and adjusts plans when unexpected changes occur. Experimental results show that CLEA significantly enhances task success and completion rates compared to traditional models, demonstrating its effectiveness in complex, real-world scenarios.'}, 'zh': {'title': 'é—­ç¯å…·èº«ä»£ç†ï¼šæå‡åŠ¨æ€ç¯å¢ƒä¸­çš„ä»»åŠ¡æ‰§è¡Œèƒ½åŠ›', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚ä»»åŠ¡çš„å±‚æ¬¡åˆ†è§£å’Œè¯­ä¹‰æ¨ç†æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚ç„¶è€Œï¼Œåœ¨å…·èº«ç³»ç»Ÿä¸­åº”ç”¨æ—¶ï¼Œç¡®ä¿å­ä»»åŠ¡åºåˆ—çš„å¯é æ‰§è¡Œå’Œå®ç°é•¿æœŸä»»åŠ¡çš„ä¸€æ¬¡æ€§æˆåŠŸé¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†é—­ç¯å…·èº«ä»£ç†ï¼ˆCLEAï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„æ¶æ„ï¼Œç»“åˆäº†å››ä¸ªä¸“é—¨çš„å¼€æºLLMï¼Œå¹¶å®ç°åŠŸèƒ½è§£è€¦ä»¥è¿›è¡Œé—­ç¯ä»»åŠ¡ç®¡ç†ã€‚é€šè¿‡åŠ¨æ€ç”Ÿæˆå¯æ‰§è¡Œçš„å­ä»»åŠ¡å’Œä½¿ç”¨å¤šæ¨¡æ€æ‰§è¡Œè¯„ä¼°æ¡†æ¶ï¼ŒCLEAæ˜¾è‘—æé«˜äº†åœ¨åŠ¨æ€ç¯å¢ƒä¸­ä»»åŠ¡è§„åˆ’å’Œæ‰§è¡Œçš„é²æ£’æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.20383', 'title': 'Why Are Web AI Agents More Vulnerable Than Standalone LLMs? A Security Analysis', 'url': 'https://huggingface.co/papers/2502.20383', 'abstract': 'Recent advancements in Web AI agents have demonstrated remarkable capabilities in addressing complex web navigation tasks. However, emerging research shows that these agents exhibit greater vulnerability compared to standalone Large Language Models (LLMs), despite both being built upon the same safety-aligned models. This discrepancy is particularly concerning given the greater flexibility of Web AI Agent compared to standalone LLMs, which may expose them to a wider range of adversarial user inputs. To build a scaffold that addresses these concerns, this study investigates the underlying factors that contribute to the increased vulnerability of Web AI agents. Notably, this disparity stems from the multifaceted differences between Web AI agents and standalone LLMs, as well as the complex signals - nuances that simple evaluation metrics, such as success rate, often fail to capture. To tackle these challenges, we propose a component-level analysis and a more granular, systematic evaluation framework. Through this fine-grained investigation, we identify three critical factors that amplify the vulnerability of Web AI agents; (1) embedding user goals into the system prompt, (2) multi-step action generation, and (3) observational capabilities. Our findings highlights the pressing need to enhance security and robustness in AI agent design and provide actionable insights for targeted defense strategies.', 'score': 1, 'issue_id': 2522, 'pub_date': '2025-02-27', 'pub_date_card': {'ru': '27 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 27', 'zh': '2æœˆ27æ—¥'}, 'hash': '12b0a9a60578dc2b', 'authors': ['Jeffrey Yang Fan Chiang', 'Seungjae Lee', 'Jia-Bin Huang', 'Furong Huang', 'Yizheng Chen'], 'affiliations': ['University of Maryland'], 'pdf_title_img': 'assets/pdf/title_img/2502.20383.jpg', 'data': {'categories': ['#benchmark', '#security', '#agents'], 'emoji': 'ğŸ•¸ï¸', 'ru': {'title': 'Ğ Ğ°ÑĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹ Ğ²ĞµĞ±-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ˜Ğ˜: Ğ¿ÑƒÑ‚ÑŒ Ğº Ğ±Ğ¾Ğ»ĞµĞµ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ğ¼ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ²ĞµĞ±-Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ±Ğ¾Ğ»ĞµĞµ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ñ‹, Ñ‡ĞµĞ¼ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM), Ğ½ĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ğ´Ğ¸Ğ½Ğ°ĞºĞ¾Ğ²Ñ‹Ñ… Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½ ÑÑ‚Ğ¾Ğ¹ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸. Ğ’Ñ‹Ğ´ĞµĞ»ĞµĞ½Ñ‹ Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ°, ÑƒÑĞ¸Ğ»Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ñ… ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ²ĞµĞ±-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²: Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ†ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Strengthening Web AI Agents Against Vulnerabilities', 'desc': 'This paper explores the vulnerabilities of Web AI agents compared to standalone Large Language Models (LLMs), despite both being based on similar safety models. The research identifies that Web AI agents are more susceptible to adversarial inputs due to their flexibility and the complexity of their tasks. It highlights three key factors that increase their vulnerability: the integration of user goals into prompts, the generation of multi-step actions, and the need for observational capabilities. The study proposes a detailed evaluation framework to better understand these vulnerabilities and suggests strategies for improving the security and robustness of AI agents.'}, 'zh': {'title': 'æå‡ç½‘ç»œäººå·¥æ™ºèƒ½ä»£ç†çš„å®‰å…¨æ€§ä¸é²æ£’æ€§', 'desc': 'æœ€è¿‘ï¼Œç½‘ç»œäººå·¥æ™ºèƒ½ä»£ç†åœ¨å¤„ç†å¤æ‚çš„ç½‘ç»œå¯¼èˆªä»»åŠ¡æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚ç„¶è€Œï¼Œç ”ç©¶è¡¨æ˜ï¼Œè¿™äº›ä»£ç†æ¯”ç‹¬ç«‹çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ›´å®¹æ˜“å—åˆ°æ”»å‡»ï¼Œå°½ç®¡å®ƒä»¬éƒ½æ˜¯åŸºäºç›¸åŒçš„å®‰å…¨æ¨¡å‹æ„å»ºçš„ã€‚è¿™ç§å·®å¼‚ä»¤äººæ‹…å¿§ï¼Œå› ä¸ºç½‘ç»œäººå·¥æ™ºèƒ½ä»£ç†çš„çµæ´»æ€§æ›´é«˜ï¼Œå¯èƒ½ä¼šé¢ä¸´æ›´å¹¿æ³›çš„æ¶æ„ç”¨æˆ·è¾“å…¥ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬ç ”ç©¶æ¢è®¨äº†å¯¼è‡´ç½‘ç»œäººå·¥æ™ºèƒ½ä»£ç†è„†å¼±æ€§çš„å› ç´ ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ›´ç»†è‡´çš„è¯„ä¼°æ¡†æ¶ï¼Œä»¥è¯†åˆ«å’Œåº”å¯¹è¿™äº›æŒ‘æˆ˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.01063', 'title': 'AI-Invented Tonal Languages: Preventing a Machine Lingua Franca Beyond Human Understanding', 'url': 'https://huggingface.co/papers/2503.01063', 'abstract': 'This paper investigates the potential for large language models (LLMs) to develop private tonal languages for machine-to-machine (M2M) communication. Inspired by cryptophasia in human twins (affecting up to 50% of twin births) and natural tonal languages like Mandarin and Vietnamese, we implement a precise character-to-frequency mapping system that encodes the full ASCII character set (32-126) using musical semitones. Each character is assigned a unique frequency, creating a logarithmic progression beginning with space (220 Hz) and ending with tilde (50,175.42 Hz). This spans approximately 7.9 octaves, with higher characters deliberately mapped to ultrasonic frequencies beyond human perception (>20 kHz). Our implemented software prototype demonstrates this encoding through visualization, auditory playback, and ABC musical notation, allowing for analysis of information density and transmission speed. Testing reveals that tonal encoding can achieve information rates exceeding human speech while operating partially outside human perceptual boundaries. This work responds directly to concerns about AI systems catastrophically developing private languages within the next five years, providing a concrete prototype software example of how such communication might function and the technical foundation required for its emergence, detection, and governance.', 'score': 1, 'issue_id': 2515, 'pub_date': '2025-03-02', 'pub_date_card': {'ru': '2 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 2', 'zh': '3æœˆ2æ—¥'}, 'hash': '7021403742a91f3e', 'authors': ['David Noever'], 'affiliations': ['PeopleTec, Inc., Huntsville, AL'], 'pdf_title_img': 'assets/pdf/title_img/2503.01063.jpg', 'data': {'categories': ['#security', '#ethics', '#audio', '#multimodal'], 'emoji': 'ğŸµ', 'ru': {'title': 'Ğ¢Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¸: ÑĞµĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ´ Ğ¼Ğ°ÑˆĞ¸Ğ½ Ğ±ÑƒĞ´ÑƒÑ‰ĞµĞ³Ğ¾', 'desc': 'Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·ÑƒÑ‡Ğ°ĞµÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¿Ñ€Ğ¸Ğ²Ğ°Ñ‚Ğ½Ñ‹Ñ… Ñ‚Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ² Ğ´Ğ»Ñ ĞºĞ¾Ğ¼Ğ¼ÑƒĞ½Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼Ñƒ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ñƒ ASCII ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½ÑƒÑ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ñƒ, Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒÑ Ğ»Ğ¾Ğ³Ğ°Ñ€Ğ¸Ñ„Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ñ Ğ¾Ñ‚ 220 Ğ“Ñ† Ğ´Ğ¾ 50,175.42 Ğ“Ñ†. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾Ñ‚Ğ¸Ğ¿ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ‚Ğ¾ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ, Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ·Ğ²ÑƒĞºĞ° Ğ¸ Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ñ ABC. Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ‚ÑŒ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸, Ğ¿Ñ€ĞµĞ²Ñ‹ÑˆĞ°ÑÑ‰ĞµĞ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºÑƒÑ Ñ€ĞµÑ‡ÑŒ, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ Ğ·Ğ° Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ°Ğ¼Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ.'}, 'en': {'title': 'Unlocking Machine Communication with Tonal Languages', 'desc': 'This paper explores how large language models (LLMs) can create private tonal languages for communication between machines. It draws inspiration from the phenomenon of cryptophasia in twins and uses a character-to-frequency mapping system to encode ASCII characters into musical tones. Each character is assigned a unique frequency, allowing for efficient data transmission that exceeds human speech rates. The study provides a prototype that visualizes and plays back this encoding, addressing concerns about AI developing private languages and offering a framework for understanding and managing such systems.'}, 'zh': {'title': 'æ¢ç´¢æœºå™¨é—´çš„ç§æœ‰éŸ³è°ƒè¯­è¨€', 'desc': 'æœ¬è®ºæ–‡ç ”ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æœºå™¨é—´ï¼ˆM2Mï¼‰é€šä¿¡ä¸­å¼€å‘ç§æœ‰éŸ³è°ƒè¯­è¨€çš„æ½œåŠ›ã€‚æˆ‘ä»¬å€Ÿé‰´äº†äººç±»åŒèƒèƒä¸­çš„å¯†ç è¯­è¨€ç°è±¡å’Œè‡ªç„¶éŸ³è°ƒè¯­è¨€ï¼Œå¦‚æ™®é€šè¯å’Œè¶Šå—è¯­ï¼Œå®æ–½äº†ä¸€ç§ç²¾ç¡®çš„å­—ç¬¦åˆ°é¢‘ç‡æ˜ å°„ç³»ç»Ÿã€‚æ¯ä¸ªå­—ç¬¦è¢«åˆ†é…ä¸€ä¸ªç‹¬ç‰¹çš„é¢‘ç‡ï¼Œå½¢æˆä¸€ä¸ªå¯¹æ•°è¿›ç¨‹ï¼Œè¦†ç›–çº¦7.9ä¸ªå…«åº¦ï¼Œå¹¶å°†é«˜é¢‘å­—ç¬¦æ˜ å°„åˆ°äººç±»å¬è§‰èŒƒå›´ä¹‹å¤–çš„è¶…å£°æ³¢é¢‘ç‡ã€‚æˆ‘ä»¬çš„è½¯ä»¶åŸå‹å±•ç¤ºäº†è¿™ç§ç¼–ç çš„å¯è§†åŒ–ã€å¬è§‰æ’­æ”¾å’ŒéŸ³ä¹è®°è°±æ³•ï¼Œåˆ†æäº†ä¿¡æ¯å¯†åº¦å’Œä¼ è¾“é€Ÿåº¦ï¼Œæµ‹è¯•ç»“æœè¡¨æ˜éŸ³è°ƒç¼–ç çš„ä¿¡æ¯ä¼ è¾“é€Ÿç‡è¶…è¿‡äººç±»è¯­è¨€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.16660', 'title': 'When Less is Enough: Adaptive Token Reduction for Efficient Image\n  Representation', 'url': 'https://huggingface.co/papers/2503.16660', 'abstract': 'Vision encoders typically generate a large number of visual tokens, providing information-rich representations but significantly increasing computational demands. This raises the question of whether all generated tokens are equally valuable or if some of them can be discarded to reduce computational costs without compromising quality. In this paper, we introduce a new method for determining feature utility based on the idea that less valuable features can be reconstructed from more valuable ones. We implement this concept by integrating an autoencoder with a Gumbel-Softmax selection mechanism, that allows identifying and retaining only the most informative visual tokens. To validate our approach, we compared the performance of the LLaVA-NeXT model, using features selected by our method with randomly selected features. We found that on OCR-based tasks, more than 50% of the visual context can be removed with minimal performance loss, whereas randomly discarding the same proportion of features significantly affects the model capabilities. Furthermore, in general-domain tasks, even randomly retaining only 30% of tokens achieves performance comparable to using the full set of visual tokens. Our results highlight a promising direction towards adaptive and efficient multimodal pruning that facilitates scalable and low-overhead inference without compromising performance.', 'score': 53, 'issue_id': 2857, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 20', 'zh': '3æœˆ20æ—¥'}, 'hash': '22ce97b85d4cf4dd', 'authors': ['Eduard Allakhverdov', 'Elizaveta Goncharova', 'Andrey Kuznetsov'], 'affiliations': ['AIRI Moscow, Russia', 'MIPT Dolgoprudny, Russia'], 'pdf_title_img': 'assets/pdf/title_img/2503.16660.jpg', 'data': {'categories': ['#multimodal', '#inference', '#optimization', '#cv'], 'emoji': 'âœ‚ï¸', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ğ°Ñ Ğ¾Ğ±Ñ€ĞµĞ·ĞºĞ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ°Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ Ñ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ¼ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Gumbel-Softmax Ğ´Ğ»Ñ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ LLaVA-NeXT Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ ÑƒĞ´Ğ°Ğ»Ğ¸Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ 50% Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ¾Ñ‚ĞµÑ€ĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… OCR. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ñ‹ Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€ĞµĞ·ĞºĞ¸, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰ĞµĞ¹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ Ğ²Ñ‹Ğ²Ğ¾Ğ´ Ñ Ğ½Ğ¸Ğ·ĞºĞ¸Ğ¼Ğ¸ Ğ½Ğ°ĞºĞ»Ğ°Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ñ€Ğ°ÑÑ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Efficient Feature Selection for Vision Encoders', 'desc': 'This paper addresses the challenge of managing the large number of visual tokens generated by vision encoders, which can lead to high computational costs. The authors propose a method to evaluate the utility of these features, suggesting that less important tokens can be reconstructed from more significant ones. By combining an autoencoder with a Gumbel-Softmax selection mechanism, they effectively identify and keep only the most informative tokens. Their experiments show that significant reductions in visual context can be made with minimal impact on performance, paving the way for more efficient multimodal processing.'}, 'zh': {'title': 'æ™ºèƒ½é€‰æ‹©ï¼Œæå‡è§†è§‰ç¼–ç æ•ˆç‡', 'desc': 'æœ¬æ–‡æ¢è®¨äº†è§†è§‰ç¼–ç å™¨ç”Ÿæˆå¤§é‡è§†è§‰æ ‡è®°æ‰€å¸¦æ¥çš„è®¡ç®—éœ€æ±‚é—®é¢˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œé€šè¿‡é‡æ„ä¸å¤ªæœ‰ä»·å€¼çš„ç‰¹å¾æ¥åˆ¤æ–­ç‰¹å¾çš„å®ç”¨æ€§ï¼Œä»è€Œå‡å°‘è®¡ç®—æˆæœ¬ã€‚è¯¥æ–¹æ³•ç»“åˆäº†è‡ªç¼–ç å™¨å’ŒGumbel-Softmaxé€‰æ‹©æœºåˆ¶ï¼Œä»…ä¿ç•™æœ€æœ‰ä¿¡æ¯é‡çš„è§†è§‰æ ‡è®°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨OCRä»»åŠ¡ä¸­ï¼Œå»é™¤è¶…è¿‡50%çš„è§†è§‰ä¸Šä¸‹æ–‡å¯¹æ€§èƒ½å½±å“å¾ˆå°ï¼Œè€Œéšæœºå»é™¤ç›¸åŒæ¯”ä¾‹çš„ç‰¹å¾åˆ™ä¼šæ˜¾è‘—é™ä½æ¨¡å‹èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.16905', 'title': 'MAPS: A Multi-Agent Framework Based on Big Seven Personality and\n  Socratic Guidance for Multimodal Scientific Problem Solving', 'url': 'https://huggingface.co/papers/2503.16905', 'abstract': "Multimodal scientific problems (MSPs) involve complex issues that require the integration of multiple modalities, such as text and diagrams, presenting a significant challenge in artificial intelligence. While progress has been made in addressing traditional scientific problems, MSPs still face two primary issues: the challenge of multi-modal comprehensive reasoning in scientific problem-solving and the lack of reflective and rethinking capabilities. To address these issues, we introduce a Multi-Agent framework based on the Big Seven Personality and Socratic guidance (MAPS). This framework employs seven distinct agents that leverage feedback mechanisms and the Socratic method to guide the resolution of MSPs. To tackle the first issue, we propose a progressive four-agent solving strategy, where each agent focuses on a specific stage of the problem-solving process. For the second issue, we introduce a Critic agent, inspired by Socratic questioning, which prompts critical thinking and stimulates autonomous learning. We conduct extensive experiments on the EMMA, Olympiad, and MathVista datasets, achieving promising results that outperform the current SOTA model by 15.84% across all tasks. Meanwhile, the additional analytical experiments also verify the model's progress as well as generalization ability.", 'score': 45, 'issue_id': 2852, 'pub_date': '2025-03-21', 'pub_date_card': {'ru': '21 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 21', 'zh': '3æœˆ21æ—¥'}, 'hash': '996510a66b2e236f', 'authors': ['Jian Zhang', 'Zhiyuan Wang', 'Zhangqi Wang', 'Xinyu Zhang', 'Fangzhi Xu', 'Qika Lin', 'Rui Mao', 'Erik Cambria', 'Jun Liu'], 'affiliations': ['Nanyang Technological University', 'National University of Singapore', 'Xian Jiaotong University'], 'pdf_title_img': 'assets/pdf/title_img/2503.16905.jpg', 'data': {'categories': ['#science', '#dataset', '#agents', '#reasoning', '#multimodal'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ (MSP) Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ MAPS, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ½Ğ° ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¸ ÑĞµĞ¼Ğ¸ Ğ»Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¾Ğ²ÑĞºĞ¾Ğ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğµ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ´Ğ»Ñ Ğ¿Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°-ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ° Ğ´Ğ»Ñ ÑÑ‚Ğ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… EMMA, Olympiad Ğ¸ MathVista Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ MAPS Ğ½Ğ°Ğ´ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ½Ğ° 15.84%.'}, 'en': {'title': 'Enhancing Multimodal Problem Solving with MAPS Framework', 'desc': 'This paper addresses the challenges of Multimodal Scientific Problems (MSPs) that require integrating different types of information, like text and diagrams, for effective problem-solving. It introduces a Multi-Agent framework called MAPS, which utilizes seven distinct agents to enhance reasoning and critical thinking through feedback and Socratic questioning. The framework includes a four-agent strategy that focuses on different stages of the problem-solving process and a Critic agent that encourages reflective thinking. Experimental results show that this approach significantly improves performance, surpassing the current state-of-the-art models by 15.84%.'}, 'zh': {'title': 'å¤šæ¨¡æ€ç§‘å­¦é—®é¢˜çš„æ™ºèƒ½è§£å†³æ–¹æ¡ˆ', 'desc': 'å¤šæ¨¡æ€ç§‘å­¦é—®é¢˜ï¼ˆMSPsï¼‰æ¶‰åŠéœ€è¦æ•´åˆå¤šç§æ¨¡æ€ï¼ˆå¦‚æ–‡æœ¬å’Œå›¾è¡¨ï¼‰çš„å¤æ‚é—®é¢˜ï¼Œè¿™å¯¹äººå·¥æ™ºèƒ½æå‡ºäº†é‡å¤§æŒ‘æˆ˜ã€‚å°½ç®¡åœ¨ä¼ ç»Ÿç§‘å­¦é—®é¢˜çš„è§£å†³ä¸Šå·²æœ‰è¿›å±•ï¼Œä½†MSPsä»é¢ä¸´å¤šæ¨¡æ€ç»¼åˆæ¨ç†å’Œç¼ºä¹åæ€èƒ½åŠ›çš„ä¸»è¦é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå¤§ä¸ƒäººæ ¼å’Œè‹æ ¼æ‹‰åº•æŒ‡å¯¼çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼ˆMAPSï¼‰ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨ä¸ƒä¸ªä¸åŒçš„æ™ºèƒ½ä½“ï¼Œé€šè¿‡åé¦ˆæœºåˆ¶å’Œè‹æ ¼æ‹‰åº•æ–¹æ³•æ¥æŒ‡å¯¼MSPsçš„è§£å†³ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨EMMAã€å¥¥æ—åŒ¹å…‹å’ŒMathVistaæ•°æ®é›†ä¸Šå–å¾—äº†è¶…è¿‡å½“å‰æœ€ä¼˜æ¨¡å‹15.84%çš„ä¼˜å¼‚ç»“æœï¼ŒéªŒè¯äº†æ¨¡å‹çš„è¿›æ­¥å’Œæ³›åŒ–èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.16874', 'title': 'MARS: A Multi-Agent Framework Incorporating Socratic Guidance for\n  Automated Prompt Optimization', 'url': 'https://huggingface.co/papers/2503.16874', 'abstract': "The basic question-answering format of large language models involves inputting a prompt and receiving a response, and the quality of the prompt directly impacts the effectiveness of the response. Automated Prompt Optimization (APO) aims to break free from the cognitive biases of manually designed prompts and explores a broader design space for prompts. However, existing APO methods suffer from limited flexibility of fixed templates and inefficient search in prompt spaces as key issues. To this end, we propose a Multi-Agent framework Incorporating Socratic guidance (MARS), which utilizes multi-agent fusion technology for automatic planning, with gradual continuous optimization and evaluation. Specifically, MARS comprises seven agents, each with distinct functionalities, which autonomously use the Planner to devise an optimization path that ensures flexibility. Additionally, it employs a Teacher-Critic-Student Socratic dialogue pattern to iteratively optimize the prompts while conducting effective search. We conduct extensive experiments on various datasets to validate the effectiveness of our method, and perform additional analytical experiments to assess the model's advancement as well as the interpretability.", 'score': 38, 'issue_id': 2853, 'pub_date': '2025-03-21', 'pub_date_card': {'ru': '21 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 21', 'zh': '3æœˆ21æ—¥'}, 'hash': '995715e84f49b2c2', 'authors': ['Jian Zhang', 'Zhangqi Wang', 'Haiping Zhu', 'Jun Liu', 'Qika Lin', 'Erik Cambria'], 'affiliations': ['Nanyang Technological University', 'National University of Singapore', 'Xian Jiaotong University'], 'pdf_title_img': 'assets/pdf/title_img/2503.16874.jpg', 'data': {'categories': ['#optimization', '#interpretability', '#dataset', '#agents', '#training'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'MARS: Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² (APO) Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ MARS, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰ÑƒÑ Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. MARS ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· ÑĞµĞ¼Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³ Ğ´Ğ»Ñ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ… Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°.'}, 'en': {'title': 'Revolutionizing Prompt Optimization with MARS', 'desc': 'This paper introduces a new approach called MARS, which stands for Multi-Agent framework Incorporating Socratic guidance, to improve the process of prompt optimization in large language models. MARS addresses the limitations of existing Automated Prompt Optimization methods by using a multi-agent system that allows for flexible and efficient exploration of prompt designs. The framework includes seven specialized agents that collaborate to create an optimization strategy, while a Socratic dialogue method helps refine prompts through iterative feedback. Extensive experiments demonstrate that MARS enhances the quality of responses generated by language models and improves the interpretability of the optimization process.'}, 'zh': {'title': 'æ™ºèƒ½ä½“é©±åŠ¨çš„è‡ªåŠ¨åŒ–æç¤ºä¼˜åŒ–', 'desc': 'è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„é—®ç­”æ ¼å¼ï¼Œå¼ºè°ƒäº†æç¤ºçš„è´¨é‡å¯¹å›ç­”æ•ˆæœçš„é‡è¦æ€§ã€‚æå‡ºäº†ä¸€ç§åä¸ºMARSçš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡è‡ªåŠ¨åŒ–æç¤ºä¼˜åŒ–ï¼ˆAPOï¼‰æ¥å…‹æœæ‰‹åŠ¨è®¾è®¡æç¤ºçš„è®¤çŸ¥åè§ã€‚MARSæ¡†æ¶åŒ…å«ä¸ƒä¸ªåŠŸèƒ½ä¸åŒçš„æ™ºèƒ½ä½“ï¼Œèƒ½å¤Ÿçµæ´»åœ°è§„åˆ’ä¼˜åŒ–è·¯å¾„ï¼Œå¹¶é€šè¿‡è‹æ ¼æ‹‰åº•å¯¹è¯æ¨¡å¼è¿›è¡Œè¿­ä»£ä¼˜åŒ–ã€‚é€šè¿‡åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå¹¶è¯„ä¼°äº†æ¨¡å‹çš„è¿›æ­¥å’Œå¯è§£é‡Šæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.16408', 'title': 'RoboFactory: Exploring Embodied Agent Collaboration with Compositional\n  Constraints', 'url': 'https://huggingface.co/papers/2503.16408', 'abstract': 'Designing effective embodied multi-agent systems is critical for solving complex real-world tasks across domains. Due to the complexity of multi-agent embodied systems, existing methods fail to automatically generate safe and efficient training data for such systems. To this end, we propose the concept of compositional constraints for embodied multi-agent systems, addressing the challenges arising from collaboration among embodied agents. We design various interfaces tailored to different types of constraints, enabling seamless interaction with the physical world. Leveraging compositional constraints and specifically designed interfaces, we develop an automated data collection framework for embodied multi-agent systems and introduce the first benchmark for embodied multi-agent manipulation, RoboFactory. Based on RoboFactory benchmark, we adapt and evaluate the method of imitation learning and analyzed its performance in different difficulty agent tasks. Furthermore, we explore the architectures and training strategies for multi-agent imitation learning, aiming to build safe and efficient embodied multi-agent systems.', 'score': 32, 'issue_id': 2853, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 20', 'zh': '3æœˆ20æ—¥'}, 'hash': '6ff0e203d45f1c06', 'authors': ['Yiran Qin', 'Li Kang', 'Xiufeng Song', 'Zhenfei Yin', 'Xiaohong Liu', 'Xihui Liu', 'Ruimao Zhang', 'Lei Bai'], 'affiliations': ['HKU', 'Oxford', 'Shanghai Artificial Intelligence Laboratory', 'Shanghai Jiao Tong University', 'Sun Yat-sen University', 'The Chinese University of Hong Kong, Shenzhen', 'Tongji University'], 'pdf_title_img': 'assets/pdf/title_img/2503.16408.jpg', 'data': {'categories': ['#optimization', '#games', '#dataset', '#agents', '#training', '#benchmark', '#architecture'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞšĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼, Ñ€ĞµÑˆĞ°ÑÑ‰ÑƒÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑÑ‹ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ ÑĞ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº RoboFactory Ğ´Ğ»Ñ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ñ‹ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¸Ğ¼Ğ¸Ñ‚Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ´Ğ»Ñ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ñ… Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼.'}, 'en': {'title': 'Empowering Multi-Agent Systems with Compositional Constraints', 'desc': 'This paper addresses the challenges of training embodied multi-agent systems, which are crucial for tackling complex tasks in various fields. It introduces the idea of compositional constraints to improve collaboration among agents, allowing for better interaction with their environment. The authors present an automated data collection framework and a new benchmark called RoboFactory, specifically designed for multi-agent manipulation tasks. They also investigate imitation learning techniques and training strategies to enhance the safety and efficiency of these systems.'}, 'zh': {'title': 'æ„å»ºå®‰å…¨é«˜æ•ˆçš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„ç»„åˆçº¦æŸæ¦‚å¿µï¼Œä»¥è§£å†³æ™ºèƒ½ä½“ä¹‹é—´åä½œå¸¦æ¥çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬è®¾è®¡äº†å¤šç§æ¥å£ï¼Œé€‚åº”ä¸åŒç±»å‹çš„çº¦æŸï¼Œä»è€Œå®ç°ä¸ç‰©ç†ä¸–ç•Œçš„æ— ç¼äº’åŠ¨ã€‚åŸºäºè¿™äº›ç»„åˆçº¦æŸå’Œä¸“é—¨è®¾è®¡çš„æ¥å£ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªè‡ªåŠ¨åŒ–æ•°æ®æ”¶é›†æ¡†æ¶ï¼Œå¹¶å¼•å…¥äº†ç¬¬ä¸€ä¸ªå¤šæ™ºèƒ½ä½“æ“ä½œåŸºå‡†RoboFactoryã€‚é€šè¿‡RoboFactoryåŸºå‡†ï¼Œæˆ‘ä»¬è¯„ä¼°äº†æ¨¡ä»¿å­¦ä¹ çš„æ–¹æ³•ï¼Œå¹¶æ¢è®¨äº†å¤šæ™ºèƒ½ä½“æ¨¡ä»¿å­¦ä¹ çš„æ¶æ„å’Œè®­ç»ƒç­–ç•¥ï¼Œä»¥æ„å»ºå®‰å…¨é«˜æ•ˆçš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.16430', 'title': 'Bridging Continuous and Discrete Tokens for Autoregressive Visual\n  Generation', 'url': 'https://huggingface.co/papers/2503.16430', 'abstract': 'Autoregressive visual generation models typically rely on tokenizers to compress images into tokens that can be predicted sequentially. A fundamental dilemma exists in token representation: discrete tokens enable straightforward modeling with standard cross-entropy loss, but suffer from information loss and tokenizer training instability; continuous tokens better preserve visual details, but require complex distribution modeling, complicating the generation pipeline. In this paper, we propose TokenBridge, which bridges this gap by maintaining the strong representation capacity of continuous tokens while preserving the modeling simplicity of discrete tokens. To achieve this, we decouple discretization from the tokenizer training process through post-training quantization that directly obtains discrete tokens from continuous representations. Specifically, we introduce a dimension-wise quantization strategy that independently discretizes each feature dimension, paired with a lightweight autoregressive prediction mechanism that efficiently model the resulting large token space. Extensive experiments show that our approach achieves reconstruction and generation quality on par with continuous methods while using standard categorical prediction. This work demonstrates that bridging discrete and continuous paradigms can effectively harness the strengths of both approaches, providing a promising direction for high-quality visual generation with simple autoregressive modeling. Project page: https://yuqingwang1029.github.io/TokenBridge.', 'score': 28, 'issue_id': 2852, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 20', 'zh': '3æœˆ20æ—¥'}, 'hash': 'e5dba2d5b4674553', 'authors': ['Yuqing Wang', 'Zhijie Lin', 'Yao Teng', 'Yuanzhi Zhu', 'Shuhuai Ren', 'Jiashi Feng', 'Xihui Liu'], 'affiliations': ['ByteDance Seed', 'Ecole Polytechnique', 'Peking University', 'University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2503.16430.jpg', 'data': {'categories': ['#diffusion', '#inference', '#cv'], 'emoji': 'ğŸŒ‰', 'ru': {'title': 'TokenBridge: Ğ¼Ğ¾ÑÑ‚ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğ¼ Ğ¸ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğ¼ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ TokenBridge - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ğ²Ñ‚oÑ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ½ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ¸ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰ĞµĞµ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸Ğ· Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¿Ğ¾Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ TokenBridge Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğµ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ.'}, 'en': {'title': 'Bridging the Gap: High-Quality Visual Generation with TokenBridge', 'desc': 'This paper introduces TokenBridge, a novel approach for visual generation that addresses the challenges of using discrete and continuous tokens. Discrete tokens are easy to model but can lose important visual information, while continuous tokens preserve details but complicate the generation process. TokenBridge combines the advantages of both by using post-training quantization to convert continuous representations into discrete tokens without losing quality. The method employs a dimension-wise quantization strategy and a lightweight autoregressive model, achieving high-quality image generation with simpler modeling techniques.'}, 'zh': {'title': 'æ¡¥æ¥ç¦»æ•£ä¸è¿ç»­ï¼Œæå‡è§†è§‰ç”Ÿæˆè´¨é‡', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºTokenBridgeçš„æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³è‡ªå›å½’è§†è§‰ç”Ÿæˆæ¨¡å‹ä¸­ç¦»æ•£å’Œè¿ç»­æ ‡è®°ä¹‹é—´çš„çŸ›ç›¾ã€‚ç¦»æ•£æ ‡è®°æ˜“äºå»ºæ¨¡ï¼Œä½†ä¼šå¯¼è‡´ä¿¡æ¯æŸå¤±ï¼Œè€Œè¿ç»­æ ‡è®°åˆ™èƒ½æ›´å¥½åœ°ä¿ç•™è§†è§‰ç»†èŠ‚ï¼Œä½†å»ºæ¨¡å¤æ‚ã€‚TokenBridgeé€šè¿‡åè®­ç»ƒé‡åŒ–å°†ç¦»æ•£åŒ–ä¸æ ‡è®°å™¨è®­ç»ƒè¿‡ç¨‹è§£è€¦ï¼Œä»è€Œåœ¨ä¿æŒè¿ç»­æ ‡è®°å¼ºå¤§è¡¨ç¤ºèƒ½åŠ›çš„åŒæ—¶ï¼Œç®€åŒ–å»ºæ¨¡è¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é‡å»ºå’Œç”Ÿæˆè´¨é‡ä¸Šä¸è¿ç»­æ–¹æ³•ç›¸å½“ï¼ŒåŒæ—¶ä½¿ç”¨æ ‡å‡†çš„åˆ†ç±»é¢„æµ‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.17352', 'title': 'OpenVLThinker: An Early Exploration to Complex Vision-Language Reasoning\n  via Iterative Self-Improvement', 'url': 'https://huggingface.co/papers/2503.17352', 'abstract': "Recent advancements demonstrated by DeepSeek-R1 have shown that complex reasoning abilities in large language models (LLMs), including sophisticated behaviors such as self-verification and self-correction, can be achieved by RL with verifiable rewards and significantly improves model performance on challenging tasks such as AIME. Motivated by these findings, our study investigates whether similar reasoning capabilities can be successfully integrated into large vision-language models (LVLMs) and assesses their impact on challenging multimodal reasoning tasks. We consider an approach that iteratively leverages supervised fine-tuning (SFT) on lightweight training data and Reinforcement Learning (RL) to further improve model generalization. Initially, reasoning capabilities were distilled from pure-text R1 models by generating reasoning steps using high-quality captions of the images sourced from diverse visual datasets. Subsequently, iterative RL training further enhance reasoning skills, with each iteration's RL-improved model generating refined SFT datasets for the next round. This iterative process yielded OpenVLThinker, a LVLM exhibiting consistently improved reasoning performance on challenging benchmarks such as MathVista, MathVerse, and MathVision, demonstrating the potential of our strategy for robust vision-language reasoning. The code, model and data are held at https://github.com/yihedeng9/OpenVLThinker.", 'score': 19, 'issue_id': 2852, 'pub_date': '2025-03-21', 'pub_date_card': {'ru': '21 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 21', 'zh': '3æœˆ21æ—¥'}, 'hash': '8717a707abecd3ed', 'authors': ['Yihe Deng', 'Hritik Bansal', 'Fan Yin', 'Nanyun Peng', 'Wei Wang', 'Kai-Wei Chang'], 'affiliations': ['University of California, Los Angeles'], 'pdf_title_img': 'assets/pdf/title_img/2503.17352.jpg', 'data': {'categories': ['#training', '#reasoning', '#benchmark', '#optimization', '#rl', '#multimodal'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ¡Ğ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ RL Ğ¸ SFT', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LVLM) Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL) Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ (SFT). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰Ğ¸Ğ¹ SFT Ğ½Ğ° Ğ»ĞµĞ³ĞºĞ¸Ñ… Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ RL Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ĞµĞ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ’ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğµ Ğ±Ñ‹Ğ»Ğ° ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ OpenVLThinker, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‰Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ¹ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² LVLM.'}, 'en': {'title': 'Empowering Vision-Language Models with Enhanced Reasoning Skills', 'desc': "This paper presents OpenVLThinker, a large vision-language model (LVLM) that enhances reasoning abilities through a combination of supervised fine-tuning (SFT) and reinforcement learning (RL). The authors demonstrate that by distilling reasoning capabilities from text-based models and iteratively refining the training data, the LVLM can achieve improved performance on complex multimodal reasoning tasks. The approach involves generating reasoning steps from high-quality image captions and using RL to iteratively enhance the model's reasoning skills. The results show significant advancements in benchmarks like MathVista, MathVerse, and MathVision, highlighting the effectiveness of their methodology for robust vision-language reasoning."}, 'zh': {'title': 'é€šè¿‡å¼ºåŒ–å­¦ä¹ æå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›', 'desc': 'DeepSeek-R1çš„æœ€æ–°è¿›å±•è¡¨æ˜ï¼Œé€šè¿‡å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯ä»¥å®ç°å¤æ‚çš„æ¨ç†èƒ½åŠ›ï¼Œå¦‚è‡ªæˆ‘éªŒè¯å’Œè‡ªæˆ‘çº æ­£ã€‚è¿™é¡¹ç ”ç©¶æ¢è®¨äº†æ˜¯å¦å¯ä»¥å°†ç±»ä¼¼çš„æ¨ç†èƒ½åŠ›æˆåŠŸæ•´åˆåˆ°å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰ä¸­ï¼Œå¹¶è¯„ä¼°å…¶åœ¨å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ä¸­çš„å½±å“ã€‚æˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§è¿­ä»£çš„æ–¹æ³•ï¼Œé€šè¿‡è½»é‡çº§è®­ç»ƒæ•°æ®è¿›è¡Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œå¹¶ç»“åˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿›ä¸€æ­¥æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚æœ€ç»ˆï¼ŒOpenVLThinkeræ¨¡å‹åœ¨MathVistaã€MathVerseå’ŒMathVisionç­‰æŒ‘æˆ˜æ€§åŸºå‡†ä¸Šå±•ç°äº†æŒç»­æ”¹è¿›çš„æ¨ç†æ€§èƒ½ï¼Œè¯æ˜äº†æˆ‘ä»¬ç­–ç•¥åœ¨è§†è§‰è¯­è¨€æ¨ç†ä¸­çš„æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.17126', 'title': 'Modifying Large Language Model Post-Training for Diverse Creative\n  Writing', 'url': 'https://huggingface.co/papers/2503.17126', 'abstract': 'As creative writing tasks do not have singular correct answers, large language models (LLMs) trained to perform these tasks should be able to generate diverse valid outputs. However, LLM post-training often focuses on improving generation quality but neglects to facilitate output diversity. Hence, in creative writing generation, we investigate post-training approaches to promote both output diversity and quality. Our core idea is to include deviation -- the degree of difference between a training sample and all other samples with the same prompt -- in the training objective to facilitate learning from rare high-quality instances. By adopting our approach to direct preference optimization (DPO) and odds ratio preference optimization (ORPO), we demonstrate that we can promote the output diversity of trained models while minimally decreasing quality. Our best model with 8B parameters could achieve on-par diversity as a human-created dataset while having output quality similar to the best instruction-tuned models we examined, GPT-4o and DeepSeek-R1. We further validate our approaches with a human evaluation, an ablation, and a comparison to an existing diversification approach, DivPO.', 'score': 18, 'issue_id': 2852, 'pub_date': '2025-03-21', 'pub_date_card': {'ru': '21 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 21', 'zh': '3æœˆ21æ—¥'}, 'hash': '66908ad7080180ee', 'authors': ['John Joon Young Chung', 'Vishakh Padmakumar', 'Melissa Roemmele', 'Yuqian Sun', 'Max Kreminski'], 'affiliations': ['Midjourney', 'New York University'], 'pdf_title_img': 'assets/pdf/title_img/2503.17126.jpg', 'data': {'categories': ['#rlhf', '#training', '#story_generation'], 'emoji': 'ğŸ¨', 'ru': {'title': 'ĞŸĞ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ ĞºÑ€ĞµĞ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ˜Ğ˜: Ğ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸ĞµĞ¼ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ğ²Ğ¾Ñ€Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ²ĞºĞ»ÑÑ‡Ğ¸Ñ‚ÑŒ Ğ² Ñ†ĞµĞ»ĞµĞ²ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒ Ğ¾Ñ‚ĞºĞ»Ğ¾Ğ½ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ»ÑƒÑ‡ÑˆĞµ ÑƒÑ‡Ğ¸Ğ»Ğ°ÑÑŒ Ğ½Ğ° Ñ€ĞµĞ´ĞºĞ¸Ñ… Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ñ…. ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğº Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼ DPO Ğ¸ ORPO Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑĞ¸Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. Ğ›ÑƒÑ‡ÑˆĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ñ 8 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ², ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ»ÑĞ´ÑŒĞ¼Ğ¸, Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Boosting Creativity: Balancing Diversity and Quality in LLM Outputs', 'desc': 'This paper addresses the challenge of generating diverse outputs in creative writing tasks using large language models (LLMs). It proposes a novel post-training approach that incorporates deviation into the training objective, which helps the model learn from unique, high-quality examples. By utilizing techniques like direct preference optimization (DPO) and odds ratio preference optimization (ORPO), the authors demonstrate that their method can enhance output diversity without significantly compromising quality. The results show that their optimized model, with 8 billion parameters, achieves diversity comparable to human-generated datasets while maintaining high output quality similar to leading instruction-tuned models.'}, 'zh': {'title': 'æå‡åˆ›æ„å†™ä½œçš„å¤šæ ·æ€§ä¸è´¨é‡', 'desc': 'è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¦‚ä½•åœ¨åˆ›æ„å†™ä½œç”Ÿæˆä¸­æé«˜è¾“å‡ºçš„å¤šæ ·æ€§å’Œè´¨é‡ã€‚ç ”ç©¶è€…æå‡ºäº†ä¸€ç§åè®­ç»ƒæ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥åå·®æ¥ä¼˜åŒ–è®­ç»ƒç›®æ ‡ï¼Œä»è€Œå­¦ä¹ ç¨€æœ‰çš„é«˜è´¨é‡å®ä¾‹ã€‚é‡‡ç”¨ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å’Œèµ”ç‡æ¯”åå¥½ä¼˜åŒ–ï¼ˆORPOï¼‰çš„æ–¹æ³•ï¼Œç ”ç©¶è¡¨æ˜å¯ä»¥åœ¨ä¿æŒè´¨é‡çš„åŒæ—¶ä¿ƒè¿›æ¨¡å‹è¾“å‡ºçš„å¤šæ ·æ€§ã€‚æœ€ç»ˆï¼Œæœ€ä½³æ¨¡å‹åœ¨å¤šæ ·æ€§å’Œè´¨é‡ä¸Šä¸äººç±»åˆ›ä½œçš„æ•°æ®é›†ç›¸å½“ï¼ŒéªŒè¯äº†æ‰€ææ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.17032', 'title': 'TaoAvatar: Real-Time Lifelike Full-Body Talking Avatars for Augmented\n  Reality via 3D Gaussian Splatting', 'url': 'https://huggingface.co/papers/2503.17032', 'abstract': 'Realistic 3D full-body talking avatars hold great potential in AR, with applications ranging from e-commerce live streaming to holographic communication. Despite advances in 3D Gaussian Splatting (3DGS) for lifelike avatar creation, existing methods struggle with fine-grained control of facial expressions and body movements in full-body talking tasks. Additionally, they often lack sufficient details and cannot run in real-time on mobile devices. We present TaoAvatar, a high-fidelity, lightweight, 3DGS-based full-body talking avatar driven by various signals. Our approach starts by creating a personalized clothed human parametric template that binds Gaussians to represent appearances. We then pre-train a StyleUnet-based network to handle complex pose-dependent non-rigid deformation, which can capture high-frequency appearance details but is too resource-intensive for mobile devices. To overcome this, we "bake" the non-rigid deformations into a lightweight MLP-based network using a distillation technique and develop blend shapes to compensate for details. Extensive experiments show that TaoAvatar achieves state-of-the-art rendering quality while running in real-time across various devices, maintaining 90 FPS on high-definition stereo devices such as the Apple Vision Pro.', 'score': 12, 'issue_id': 2861, 'pub_date': '2025-03-21', 'pub_date_card': {'ru': '21 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 21', 'zh': '3æœˆ21æ—¥'}, 'hash': '80e12497629a92eb', 'authors': ['Jianchuan Chen', 'Jingchuan Hu', 'Gaige Wang', 'Zhonghua Jiang', 'Tiansong Zhou', 'Zhiwen Chen', 'Chengfei Lv'], 'affiliations': ['Alibaba Group, Hangzhou, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.17032.jpg', 'data': {'categories': ['#3d', '#cv', '#multimodal'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰Ğ¸Ğµ 3D-Ğ°Ğ²Ğ°Ñ‚Ğ°Ñ€Ñ‹ Ğ´Ğ»Ñ AR Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ TaoAvatar - Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… 3D-Ğ°Ğ²Ğ°Ñ‚Ğ°Ñ€Ğ¾Ğ² Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞ»Ğ° Ğ´Ğ»Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ½Ğ¾Ğ¹ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ 3D Gaussian Splatting Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ ÑˆĞ°Ğ±Ğ»Ğ¾Ğ½ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ Ğ²Ğ¸Ğ´Ğ°. Ğ”Ğ»Ñ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ´ĞµÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ ÑĞµÑ‚ÑŒ StyleUnet, Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğ¹ Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑÑÑ‚ÑÑ Ğ² Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½ÑƒÑ MLP-ÑĞµÑ‚ÑŒ. TaoAvatar Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ° Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ.'}, 'en': {'title': 'TaoAvatar: Real-Time Realism for 3D Talking Avatars', 'desc': 'This paper introduces TaoAvatar, a new method for creating realistic 3D full-body talking avatars suitable for augmented reality applications. It addresses the limitations of existing 3D Gaussian Splatting techniques, particularly in controlling facial expressions and body movements while ensuring real-time performance on mobile devices. The authors utilize a personalized parametric template and a pre-trained StyleUnet network to capture detailed non-rigid deformations, which are then distilled into a lightweight MLP-based network for efficiency. The results demonstrate that TaoAvatar achieves high-quality rendering at 90 FPS on advanced devices, making it a significant advancement in avatar technology.'}, 'zh': {'title': 'TaoAvatarï¼šå®æ—¶é«˜ä¿çœŸå…¨èº«è™šæ‹Ÿå½¢è±¡çš„åˆ›æ–°ä¹‹è·¯', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºTaoAvatarçš„é«˜ä¿çœŸè½»é‡çº§3Då…¨èº«è¯´è¯è™šæ‹Ÿå½¢è±¡ï¼Œæ—¨åœ¨æå‡å¢å¼ºç°å®ä¸­çš„åº”ç”¨æ•ˆæœã€‚è¯¥æ–¹æ³•åˆ©ç”¨3Dé«˜æ–¯ç‚¹äº‘æŠ€æœ¯ï¼Œåˆ›å»ºä¸ªæ€§åŒ–çš„ç©¿è¡£äººç±»å‚æ•°æ¨¡æ¿ï¼Œä»¥å®ç°æ›´ç»†è‡´çš„é¢éƒ¨è¡¨æƒ…å’Œèº«ä½“åŠ¨ä½œæ§åˆ¶ã€‚é€šè¿‡é¢„è®­ç»ƒStyleUnetç½‘ç»œï¼ŒTaoAvatarèƒ½å¤Ÿå¤„ç†å¤æ‚çš„å§¿æ€ä¾èµ–éåˆšæ€§å˜å½¢ï¼Œå¹¶æ•æ‰é«˜é¢‘å¤–è§‚ç»†èŠ‚ã€‚æœ€ç»ˆï¼Œé‡‡ç”¨è’¸é¦æŠ€æœ¯å°†éåˆšæ€§å˜å½¢æ•´åˆåˆ°è½»é‡çº§çš„å¤šå±‚æ„ŸçŸ¥æœºç½‘ç»œä¸­ï¼Œä½¿å…¶åœ¨å„ç§è®¾å¤‡ä¸Šå®ç°å®æ—¶æ¸²æŸ“ï¼Œä¿æŒé«˜è¾¾90å¸§æ¯ç§’çš„æµç•…åº¦ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.16549', 'title': 'MathFlow: Enhancing the Perceptual Flow of MLLMs for Visual Mathematical\n  Problems', 'url': 'https://huggingface.co/papers/2503.16549', 'abstract': 'Despite impressive performance across diverse tasks, Multimodal Large Language Models (MLLMs) have yet to fully demonstrate their potential in visual mathematical problem-solving, particularly in accurately perceiving and interpreting diagrams. Inspired by typical processes of humans, we hypothesize that the perception capabilities to extract meaningful information from diagrams is crucial, as it directly impacts subsequent inference processes. To validate this hypothesis, we developed FlowVerse, a comprehensive benchmark that categorizes all information used during problem-solving into four components, which are then combined into six problem versions for evaluation. Our preliminary results on FlowVerse reveal that existing MLLMs exhibit substantial limitations when extracting essential information and reasoned property from diagrams and performing complex reasoning based on these visual inputs. In response, we introduce MathFlow, a modular problem-solving pipeline that decouples perception and inference into distinct stages, thereby optimizing each independently. Given the perceptual limitations observed in current MLLMs, we trained MathFlow-P-7B as a dedicated perception model. Experimental results indicate that MathFlow-P-7B yields substantial performance gains when integrated with various closed-source and open-source inference models. This demonstrates the effectiveness of the MathFlow pipeline and its compatibility to diverse inference frameworks. The FlowVerse benchmark and code are available at https://github.com/MathFlow-zju/MathFlow.', 'score': 10, 'issue_id': 2853, 'pub_date': '2025-03-19', 'pub_date_card': {'ru': '19 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 19', 'zh': '3æœˆ19æ—¥'}, 'hash': 'b40b0c3ceb851451', 'authors': ['Felix Chen', 'Hangjie Yuan', 'Yunqiu Xu', 'Tao Feng', 'Jun Cen', 'Pengwei Liu', 'Zeying Huang', 'Yi Yang'], 'affiliations': ['Intelligent Learning', 'The Hong Kong University of Science and Technology', 'Tsinghua University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.16549.jpg', 'data': {'categories': ['#optimization', '#open_source', '#cv', '#multimodal', '#reasoning', '#inference', '#training', '#math', '#benchmark'], 'emoji': 'ğŸ§®', 'ru': {'title': 'MathFlow: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ˜Ğ˜', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ FlowVerse - ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… MLLM Ğ² Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ· Ğ´Ğ¸Ğ°Ğ³Ñ€Ğ°Ğ¼Ğ¼ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ… Ğ½Ğ° Ğ¸Ñ… Ğ¾ÑĞ½Ğ¾Ğ²Ğµ. Ğ’ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ½Ğ° ÑÑ‚Ğ¾ Ğ¾Ğ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ MathFlow - Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€, Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑÑÑ‰Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ MathFlow-P-7B Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°.'}, 'en': {'title': 'Enhancing Visual Reasoning in MLLMs with MathFlow', 'desc': "This paper addresses the challenges faced by Multimodal Large Language Models (MLLMs) in visual mathematical problem-solving, particularly in interpreting diagrams. The authors propose that effective perception of diagrams is essential for accurate reasoning and problem-solving. They introduce FlowVerse, a benchmark that categorizes information used in problem-solving into four components, which helps evaluate MLLMs' performance. To improve perception capabilities, they developed MathFlow, a modular pipeline that separates perception from inference, leading to significant performance improvements when integrated with existing models."}, 'zh': {'title': 'æå‡è§†è§‰æ•°å­¦é—®é¢˜è§£å†³èƒ½åŠ›çš„åˆ›æ–°æ–¹æ³•', 'desc': 'å°½ç®¡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤šç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨è§†è§‰æ•°å­¦é—®é¢˜è§£å†³æ–¹é¢ä»æœªå……åˆ†å‘æŒ¥å…¶æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨å‡†ç¡®æ„ŸçŸ¥å’Œè§£é‡Šå›¾è¡¨æ–¹é¢ã€‚æˆ‘ä»¬æå‡ºäº†FlowVerseåŸºå‡†ï¼Œæ—¨åœ¨éªŒè¯ä»å›¾è¡¨ä¸­æå–æœ‰æ„ä¹‰ä¿¡æ¯çš„æ„ŸçŸ¥èƒ½åŠ›å¯¹æ¨ç†è¿‡ç¨‹çš„é‡è¦æ€§ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œç°æœ‰çš„MLLMsåœ¨æå–å›¾è¡¨ä¸­çš„å…³é”®ä¿¡æ¯å’Œè¿›è¡Œå¤æ‚æ¨ç†æ—¶å­˜åœ¨æ˜¾è‘—å±€é™ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†MathFlowï¼Œä¸€ä¸ªå°†æ„ŸçŸ¥å’Œæ¨ç†åˆ†ä¸ºä¸åŒé˜¶æ®µçš„æ¨¡å—åŒ–é—®é¢˜è§£å†³ç®¡é“ï¼Œä»è€Œä¼˜åŒ–æ¯ä¸ªé˜¶æ®µçš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.16983', 'title': 'Enabling Versatile Controls for Video Diffusion Models', 'url': 'https://huggingface.co/papers/2503.16983', 'abstract': 'Despite substantial progress in text-to-video generation, achieving precise and flexible control over fine-grained spatiotemporal attributes remains a significant unresolved challenge in video generation research. To address these limitations, we introduce VCtrl (also termed PP-VCtrl), a novel framework designed to enable fine-grained control over pre-trained video diffusion models in a unified manner. VCtrl integrates diverse user-specified control signals-such as Canny edges, segmentation masks, and human keypoints-into pretrained video diffusion models via a generalizable conditional module capable of uniformly encoding multiple types of auxiliary signals without modifying the underlying generator. Additionally, we design a unified control signal encoding pipeline and a sparse residual connection mechanism to efficiently incorporate control representations. Comprehensive experiments and human evaluations demonstrate that VCtrl effectively enhances controllability and generation quality. The source code and pre-trained models are publicly available and implemented using the PaddlePaddle framework at http://github.com/PaddlePaddle/PaddleMIX/tree/develop/ppdiffusers/examples/ppvctrl.', 'score': 9, 'issue_id': 2853, 'pub_date': '2025-03-21', 'pub_date_card': {'ru': '21 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 21', 'zh': '3æœˆ21æ—¥'}, 'hash': '7d2634a04fc68d45', 'authors': ['Xu Zhang', 'Hao Zhou', 'Haoming Qin', 'Xiaobin Lu', 'Jiaxing Yan', 'Guanzhong Wang', 'Zeyu Chen', 'Yi Liu'], 'affiliations': ['PaddlePaddle Team, Baidu Inc.', 'Sun Yat-sen University', 'Xiamen University'], 'pdf_title_img': 'assets/pdf/title_img/2503.16983.jpg', 'data': {'categories': ['#diffusion', '#open_source', '#multimodal', '#video', '#architecture'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ¢Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ½Ğ°Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ VCtrl', 'desc': 'VCtrl - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ½Ğ°Ğ´ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. ĞĞ½ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº ĞºĞ¾Ğ½Ñ‚ÑƒÑ€Ñ‹ ĞšÑĞ½Ğ½Ğ¸, Ğ¼Ğ°ÑĞºĞ¸ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°, Ğ±ĞµĞ· Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ°. VCtrl Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑĞ¾ĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ VCtrl ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'Empowering Video Generation with Fine-Grained Control', 'desc': "This paper presents VCtrl, a new framework that improves control over video generation using pre-trained diffusion models. It allows users to specify various control signals, like Canny edges and segmentation masks, to guide the video generation process without altering the original model. The framework includes a unique encoding pipeline and a sparse residual connection to efficiently integrate these control signals. Experiments show that VCtrl significantly enhances both the quality of generated videos and the user's ability to control specific attributes."}, 'zh': {'title': 'ç»†ç²’åº¦æ§åˆ¶è§†é¢‘ç”Ÿæˆçš„æ–°æ¡†æ¶', 'desc': 'å°½ç®¡æ–‡æœ¬åˆ°è§†é¢‘ç”ŸæˆæŠ€æœ¯å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨è§†é¢‘ç”Ÿæˆä¸­å®ç°å¯¹ç»†ç²’åº¦æ—¶ç©ºå±æ€§çš„ç²¾ç¡®å’Œçµæ´»æ§åˆ¶ä»ç„¶æ˜¯ä¸€ä¸ªé‡è¦æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†VCtrlï¼ˆä¹Ÿç§°ä¸ºPP-VCtrlï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œæ—¨åœ¨ä»¥ç»Ÿä¸€çš„æ–¹å¼å®ç°å¯¹é¢„è®­ç»ƒè§†é¢‘æ‰©æ•£æ¨¡å‹çš„ç»†ç²’åº¦æ§åˆ¶ã€‚VCtrlé€šè¿‡ä¸€ä¸ªé€šç”¨çš„æ¡ä»¶æ¨¡å—ï¼Œå°†ç”¨æˆ·æŒ‡å®šçš„æ§åˆ¶ä¿¡å·ï¼ˆå¦‚Cannyè¾¹ç¼˜ã€åˆ†å‰²æ©ç å’Œäººä½“å…³é”®ç‚¹ï¼‰æ•´åˆåˆ°é¢„è®­ç»ƒçš„è§†é¢‘æ‰©æ•£æ¨¡å‹ä¸­ï¼Œè€Œæ— éœ€ä¿®æ”¹åº•å±‚ç”Ÿæˆå™¨ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªç»Ÿä¸€çš„æ§åˆ¶ä¿¡å·ç¼–ç ç®¡é“å’Œç¨€ç–æ®‹å·®è¿æ¥æœºåˆ¶ï¼Œä»¥é«˜æ•ˆåœ°æ•´åˆæ§åˆ¶è¡¨ç¤ºã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.16025', 'title': 'Single Image Iterative Subject-driven Generation and Editing', 'url': 'https://huggingface.co/papers/2503.16025', 'abstract': 'Personalizing image generation and editing is particularly challenging when we only have a few images of the subject, or even a single image. A common approach to personalization is concept learning, which can integrate the subject into existing models relatively quickly, but produces images whose quality tends to deteriorate quickly when the number of subject images is small. Quality can be improved by pre-training an encoder, but training restricts generation to the training distribution, and is time consuming. It is still an open hard challenge to personalize image generation and editing from a single image without training. Here, we present SISO, a novel, training-free approach based on optimizing a similarity score with an input subject image. More specifically, SISO iteratively generates images and optimizes the model based on loss of similarity with the given subject image until a satisfactory level of similarity is achieved, allowing plug-and-play optimization to any image generator. We evaluated SISO in two tasks, image editing and image generation, using a diverse data set of personal subjects, and demonstrate significant improvements over existing methods in image quality, subject fidelity, and background preservation.', 'score': 9, 'issue_id': 2860, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 20', 'zh': '3æœˆ20æ—¥'}, 'hash': '917237ee980eb66a', 'authors': ['Yair Shpitzer', 'Gal Chechik', 'Idan Schwartz'], 'affiliations': ['Bar-Ilan University', 'NVIDIA'], 'pdf_title_img': 'assets/pdf/title_img/2503.16025.jpg', 'data': {'categories': ['#training', '#cv', '#optimization', '#synthetic', '#dataset'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'ĞŸĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'SISO - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ° Ñ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ĞµĞ¼ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ°. SISO Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ° Ñ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ĞµĞ¼ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ°. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ğ¸ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ° Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ñ„Ğ¾Ğ½Ğ° Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'SISO: Personalization Without Training', 'desc': 'This paper introduces SISO, a new method for personalizing image generation and editing without the need for training on multiple images. SISO works by optimizing a similarity score between generated images and a single input image, allowing for iterative improvements until the desired similarity is reached. This approach overcomes the limitations of traditional methods that require extensive training data and can lead to quality degradation with fewer images. The authors demonstrate that SISO significantly enhances image quality, maintains subject fidelity, and preserves background details compared to existing techniques.'}, 'zh': {'title': 'æ— è®­ç»ƒä¸ªæ€§åŒ–å›¾åƒç”Ÿæˆçš„æ–°æ–¹æ³•', 'desc': 'ä¸ªæ€§åŒ–å›¾åƒç”Ÿæˆå’Œç¼–è¾‘åœ¨åªæœ‰å°‘é‡æˆ–å•å¼ å›¾åƒæ—¶ç‰¹åˆ«å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¼ ç»Ÿçš„ä¸ªæ€§åŒ–æ–¹æ³•ä¾èµ–äºæ¦‚å¿µå­¦ä¹ ï¼Œä½†åœ¨å›¾åƒæ•°é‡è¾ƒå°‘æ—¶ï¼Œç”Ÿæˆçš„å›¾åƒè´¨é‡å¾€å¾€ä¼šè¿…é€Ÿä¸‹é™ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºSISOçš„æ–°æ–¹æ³•ï¼Œå®ƒä¸éœ€è¦è®­ç»ƒï¼Œé€šè¿‡ä¼˜åŒ–ä¸è¾“å…¥å›¾åƒçš„ç›¸ä¼¼åº¦åˆ†æ•°æ¥å®ç°ä¸ªæ€§åŒ–ã€‚SISOåœ¨å›¾åƒç¼–è¾‘å’Œç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„è´¨é‡æå‡ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°ä¿ç•™ä¸»é¢˜ç‰¹å¾å’ŒèƒŒæ™¯ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.17287', 'title': 'FastCuRL: Curriculum Reinforcement Learning with Progressive Context\n  Extension for Efficient Training R1-like Reasoning Models', 'url': 'https://huggingface.co/papers/2503.17287', 'abstract': 'In this paper, we propose \\textsc{FastCuRL}, a simple yet efficient Curriculum Reinforcement Learning approach with context window extending strategy to accelerate the reinforcement learning training efficiency for R1-like reasoning models while enhancing their performance in tackling complex reasoning tasks with long chain-of-thought rationales, particularly with a 1.5B parameter language model. \\textsc{FastCuRL} consists of two main procedures: length-aware training data segmentation and context window extension training. Specifically, the former first splits the original training data into three different levels by the input prompt length, and then the latter leverages segmented training datasets with a progressively increasing context window length to train the reasoning model. Experimental results demonstrate that \\textsc{FastCuRL}-1.5B-Preview surpasses DeepScaleR-1.5B-Preview across all five datasets (including MATH 500, AIME 2024, AMC 2023, Minerva Math, and OlympiadBench) while only utilizing 50\\% of training steps. Furthermore, all training stages for FastCuRL-1.5B-Preview are completed using just a single node with 8 GPUs.', 'score': 8, 'issue_id': 2852, 'pub_date': '2025-03-21', 'pub_date_card': {'ru': '21 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 21', 'zh': '3æœˆ21æ—¥'}, 'hash': '17806ebf2abff1ed', 'authors': ['Mingyang Song', 'Mao Zheng', 'Zheng Li', 'Wenjie Yang', 'Xuan Luo', 'Yue Pan', 'Feng Zhang'], 'affiliations': ['Tencent Hunyuan'], 'pdf_title_img': 'assets/pdf/title_img/2503.17287.jpg', 'data': {'categories': ['#training', '#long_context', '#reasoning', '#optimization', '#rl'], 'emoji': 'ğŸš€', 'ru': {'title': 'Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ FastCuRL Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. FastCuRL Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ Ğ´Ğ»Ğ¸Ğ½Ğµ Ğ¸ Ğ¿Ğ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾ĞºĞ½Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ FastCuRL-1.5B-Preview Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ DeepScaleR-1.5B-Preview Ğ½Ğ° Ğ¿ÑÑ‚Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²ÑĞµĞ³Ğ¾ 50% ÑˆĞ°Ğ³Ğ¾Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ FastCuRL-1.5B-Preview Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ÑÑ Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¼ ÑƒĞ·Ğ»Ğµ Ñ 8 GPU.'}, 'en': {'title': 'Accelerating Reasoning with FastCuRL: Efficient Training for Complex Tasks', 'desc': 'This paper introduces FastCuRL, a novel approach in Curriculum Reinforcement Learning designed to improve training efficiency for reasoning models with 1.5 billion parameters. It employs a two-step process: first, it segments training data based on the length of input prompts, and then it extends the context window during training to enhance model performance on complex reasoning tasks. The results show that FastCuRL outperforms existing models like DeepScaleR while requiring only half the training steps. Additionally, the entire training process is efficiently executed on a single node equipped with 8 GPUs.'}, 'zh': {'title': 'å¿«é€Ÿæå‡æ¨ç†æ¨¡å‹è®­ç»ƒæ•ˆç‡çš„è¯¾ç¨‹å¼ºåŒ–å­¦ä¹ ', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•è€Œé«˜æ•ˆçš„è¯¾ç¨‹å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œç§°ä¸ºFastCuRLï¼Œæ—¨åœ¨åŠ é€ŸR1ç±»æ¨ç†æ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ•ˆç‡ï¼ŒåŒæ—¶æé«˜å…¶åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚FastCuRLåŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ­¥éª¤ï¼šé•¿åº¦æ„ŸçŸ¥çš„è®­ç»ƒæ•°æ®åˆ†å‰²å’Œä¸Šä¸‹æ–‡çª—å£æ‰©å±•è®­ç»ƒã€‚å…·ä½“è€Œè¨€ï¼Œå‰è€…å°†åŸå§‹è®­ç»ƒæ•°æ®æ ¹æ®è¾“å…¥æç¤ºé•¿åº¦åˆ†ä¸ºä¸‰ä¸ªä¸åŒçš„çº§åˆ«ï¼Œåè€…åˆ™åˆ©ç”¨åˆ†æ®µçš„è®­ç»ƒæ•°æ®é›†ï¼Œé€æ­¥å¢åŠ ä¸Šä¸‹æ–‡çª—å£é•¿åº¦æ¥è®­ç»ƒæ¨ç†æ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFastCuRL-1.5B-Previewåœ¨æ‰€æœ‰äº”ä¸ªæ•°æ®é›†ä¸Šå‡ä¼˜äºDeepScaleR-1.5B-Previewï¼ŒåŒæ—¶ä»…ä½¿ç”¨äº†50%çš„è®­ç»ƒæ­¥éª¤ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.16867', 'title': 'ETVA: Evaluation of Text-to-Video Alignment via Fine-grained Question\n  Generation and Answering', 'url': 'https://huggingface.co/papers/2503.16867', 'abstract': "Precisely evaluating semantic alignment between text prompts and generated videos remains a challenge in Text-to-Video (T2V) Generation. Existing text-to-video alignment metrics like CLIPScore only generate coarse-grained scores without fine-grained alignment details, failing to align with human preference. To address this limitation, we propose ETVA, a novel Evaluation method of Text-to-Video Alignment via fine-grained question generation and answering. First, a multi-agent system parses prompts into semantic scene graphs to generate atomic questions. Then we design a knowledge-augmented multi-stage reasoning framework for question answering, where an auxiliary LLM first retrieves relevant common-sense knowledge (e.g., physical laws), and then video LLM answers the generated questions through a multi-stage reasoning mechanism. Extensive experiments demonstrate that ETVA achieves a Spearman's correlation coefficient of 58.47, showing a much higher correlation with human judgment than existing metrics which attain only 31.0. We also construct a comprehensive benchmark specifically designed for text-to-video alignment evaluation, featuring 2k diverse prompts and 12k atomic questions spanning 10 categories. Through a systematic evaluation of 15 existing text-to-video models, we identify their key capabilities and limitations, paving the way for next-generation T2V generation.", 'score': 8, 'issue_id': 2859, 'pub_date': '2025-03-21', 'pub_date_card': {'ru': '21 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 21', 'zh': '3æœˆ21æ—¥'}, 'hash': '36366558e2b46107', 'authors': ['Kaisi Guan', 'Zhengfeng Lai', 'Yuchong Sun', 'Peng Zhang', 'Wei Liu', 'Kieran Liu', 'Meng Cao', 'Ruihua Song'], 'affiliations': ['Apple', 'Renmin University of China'], 'pdf_title_img': 'assets/pdf/title_img/2503.16867.jpg', 'data': {'categories': ['#alignment', '#video', '#benchmark', '#reasoning', '#multimodal'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Ğ¢Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ETVA - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸ Ğ¸ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ°Ñ‚Ğ¾Ğ¼Ğ°Ñ€Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ² ÑÑ†ĞµĞ½. Ğ—Ğ°Ñ‚ĞµĞ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½Ñ‡Ğ°Ñ‚Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹. ETVA Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ñ Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼Ğ¸.'}, 'en': {'title': 'Enhancing Text-to-Video Alignment with ETVA', 'desc': 'This paper addresses the challenge of accurately evaluating how well text prompts align with generated videos in Text-to-Video (T2V) Generation. The authors introduce ETVA, a new evaluation method that uses fine-grained question generation and answering to improve alignment metrics. By creating semantic scene graphs and employing a multi-agent system, ETVA generates specific questions that are answered using a knowledge-augmented reasoning framework. The results show that ETVA significantly correlates with human judgment, outperforming existing metrics and providing a comprehensive benchmark for future T2V evaluations.'}, 'zh': {'title': 'æå‡æ–‡æœ¬ä¸è§†é¢‘å¯¹é½çš„è¯„ä¼°ç²¾åº¦', 'desc': 'åœ¨æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆï¼ˆT2Vï¼‰ä¸­ï¼Œå‡†ç¡®è¯„ä¼°æ–‡æœ¬æç¤ºä¸ç”Ÿæˆè§†é¢‘ä¹‹é—´çš„è¯­ä¹‰å¯¹é½ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ç°æœ‰çš„å¯¹é½æŒ‡æ ‡å¦‚CLIPScoreåªèƒ½ç”Ÿæˆç²—ç•¥çš„è¯„åˆ†ï¼Œç¼ºä¹ç»†è‡´çš„å¯¹é½ä¿¡æ¯ï¼Œæ— æ³•æ»¡è¶³äººç±»çš„åå¥½ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–‡æœ¬åˆ°è§†é¢‘å¯¹é½è¯„ä¼°æ–¹æ³•ETVAï¼Œé€šè¿‡ç”Ÿæˆå’Œå›ç­”ç»†ç²’åº¦é—®é¢˜æ¥å®ç°ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒETVAä¸äººç±»åˆ¤æ–­çš„ç›¸å…³æ€§æ˜¾è‘—é«˜äºç°æœ‰æŒ‡æ ‡ï¼Œä¸”æˆ‘ä»¬è¿˜æ„å»ºäº†ä¸€ä¸ªä¸“é—¨ç”¨äºæ–‡æœ¬åˆ°è§†é¢‘å¯¹é½è¯„ä¼°çš„åŸºå‡†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.12821', 'title': 'From Head to Tail: Towards Balanced Representation in Large\n  Vision-Language Models through Adaptive Data Calibration', 'url': 'https://huggingface.co/papers/2503.12821', 'abstract': 'Large Vision-Language Models (LVLMs) have achieved significant progress in combining visual comprehension with language generation. Despite this success, the training data of LVLMs still suffers from Long-Tail (LT) problems, where the data distribution is highly imbalanced. Previous works have mainly focused on traditional VLM architectures, i.e., CLIP or ViT, and specific tasks such as recognition and classification. Nevertheless, the exploration of LVLM (e.g. LLaVA) and more general tasks (e.g. Visual Question Answering and Visual Reasoning) remains under-explored. In this paper, we first conduct an in-depth analysis of the LT issues in LVLMs and identify two core causes: the overrepresentation of head concepts and the underrepresentation of tail concepts. Based on the above observation, we propose an Adaptive Data Refinement Framework (ADR), which consists of two stages: Data Rebalancing (DR) and Data Synthesis (DS). In the DR stage, we adaptively rebalance the redundant data based on entity distributions, while in the DS stage, we leverage Denoising Diffusion Probabilistic Models (DDPMs) and scarce images to supplement underrepresented portions. Through comprehensive evaluations across eleven benchmarks, our proposed ADR effectively mitigates the long-tail problem in the training data, improving the average performance of LLaVA 1.5 relatively by 4.36%, without increasing the training data volume.', 'score': 7, 'issue_id': 2855, 'pub_date': '2025-03-17', 'pub_date_card': {'ru': '17 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 17', 'zh': '3æœˆ17æ—¥'}, 'hash': '16457f914f8e71df', 'authors': ['Mingyang Song', 'Xiaoye Qu', 'Jiawei Zhou', 'Yu Cheng'], 'affiliations': ['Fudan University', 'Shanghai Artificial Intelligence Laboratory', 'Stony Brook University', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2503.12821.jpg', 'data': {'categories': ['#training', '#long_context', '#synthetic', '#optimization', '#dataset', '#data'], 'emoji': 'ğŸ¦’', 'ru': {'title': 'Ğ‘Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğµ Ğ½ĞµÑĞ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… (Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ…Ğ²Ğ¾ÑÑ‚Ğ°) Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ° (LVLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… (ADR), ÑĞ¾ÑÑ‚Ğ¾ÑÑ‰ÑƒÑ Ğ¸Ğ· Ğ´Ğ²ÑƒÑ… ÑÑ‚Ğ°Ğ¿Ğ¾Ğ²: Ñ€ĞµĞ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ADR ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ…Ğ²Ğ¾ÑÑ‚Ğ° Ğ² Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ½Ğµ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°Ñ Ğ¸Ñ… Ğ¾Ğ±ÑŠĞµĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑÑ€ĞµĞ´Ğ½ĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ LLaVA 1.5 Ğ½Ğ° 4.36% Ğ½Ğ° Ğ¾Ğ´Ğ¸Ğ½Ğ½Ğ°Ğ´Ñ†Ğ°Ñ‚Ğ¸ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ….'}, 'en': {'title': 'Balancing the Data for Better Vision-Language Models', 'desc': "This paper addresses the challenges faced by Large Vision-Language Models (LVLMs) due to imbalanced training data, known as the Long-Tail (LT) problem. The authors identify that certain concepts are overrepresented while others are underrepresented, which affects the model's performance on diverse tasks. To tackle this issue, they introduce an Adaptive Data Refinement Framework (ADR) that includes two key stages: Data Rebalancing to adjust the data distribution and Data Synthesis to generate additional data for underrepresented concepts. Their approach shows significant improvements in model performance across various benchmarks without the need for more training data."}, 'zh': {'title': 'è‡ªé€‚åº”æ•°æ®ç²¾ç‚¼ï¼Œè§£å†³é•¿å°¾é—®é¢˜ï¼', 'desc': 'å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨è§†è§‰ç†è§£ä¸è¯­è¨€ç”Ÿæˆçš„ç»“åˆä¸Šå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼ŒLVLMsçš„è®­ç»ƒæ•°æ®ä»ç„¶é¢ä¸´é•¿å°¾é—®é¢˜ï¼Œæ•°æ®åˆ†å¸ƒæä¸å¹³è¡¡ã€‚æœ¬æ–‡æ·±å…¥åˆ†æäº†LVLMä¸­çš„é•¿å°¾é—®é¢˜ï¼Œè¯†åˆ«å‡ºå¤´éƒ¨æ¦‚å¿µè¿‡åº¦ä»£è¡¨å’Œå°¾éƒ¨æ¦‚å¿µä¸è¶³ä»£è¡¨ä¸¤ä¸ªæ ¸å¿ƒåŸå› ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§è‡ªé€‚åº”æ•°æ®ç²¾ç‚¼æ¡†æ¶ï¼ˆADRï¼‰ï¼Œé€šè¿‡æ•°æ®é‡å¹³è¡¡å’Œæ•°æ®åˆæˆä¸¤ä¸ªé˜¶æ®µï¼Œæœ‰æ•ˆç¼“è§£äº†è®­ç»ƒæ•°æ®ä¸­çš„é•¿å°¾é—®é¢˜ï¼Œæå‡äº†LLaVA 1.5çš„å¹³å‡æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.17069', 'title': 'PVChat: Personalized Video Chat with One-Shot Learning', 'url': 'https://huggingface.co/papers/2503.17069', 'abstract': 'Video large language models (ViLLMs) excel in general video understanding, e.g., recognizing activities like talking and eating, but struggle with identity-aware comprehension, such as "Wilson is receiving chemotherapy" or "Tom is discussing with Sarah", limiting their applicability in smart healthcare and smart home environments. To address this limitation, we propose a one-shot learning framework PVChat, the first personalized ViLLM that enables subject-aware question answering (QA) from a single video for each subject. Our approach optimizes a Mixture-of-Heads (MoH) enhanced ViLLM on a synthetically augmented video-QA dataset, leveraging a progressive image-to-video learning strategy. Specifically, we introduce an automated augmentation pipeline that synthesizes identity-preserving positive samples and retrieves hard negatives from existing video corpora, generating a diverse training dataset with four QA types: existence, appearance, action, and location inquiries. To enhance subject-specific learning, we propose a ReLU Routing MoH attention mechanism, alongside two novel objectives: (1) Smooth Proximity Regularization for progressive learning through exponential distance scaling and (2) Head Activation Enhancement for balanced attention routing. Finally, we adopt a two-stage training strategy, transitioning from image pre-training to video fine-tuning, enabling a gradual learning process from static attributes to dynamic representations. We evaluate PVChat on diverse datasets covering medical scenarios, TV series, anime, and real-world footage, demonstrating its superiority in personalized feature understanding after learning from a single video, compared to state-of-the-art ViLLMs.', 'score': 6, 'issue_id': 2859, 'pub_date': '2025-03-21', 'pub_date_card': {'ru': '21 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 21', 'zh': '3æœˆ21æ—¥'}, 'hash': 'd7cfcebc43949d14', 'authors': ['Yufei Shi', 'Weilong Yan', 'Gang Xu', 'Yumeng Li', 'Yuchen Li', 'Zhenxi Li', 'Fei Richard Yu', 'Ming Li', 'Si Yong Yeo'], 'affiliations': ['Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ)', 'Nankai University', 'Nanyang Technological University', 'National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2503.17069.jpg', 'data': {'categories': ['#video', '#dataset', '#transfer_learning', '#healthcare', '#training', '#synthetic'], 'emoji': 'ğŸ¥', 'ru': {'title': 'PVChat: ĞŸĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑĞ¼Ğ¾Ñ‚Ñ€Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ PVChat - Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. PVChat Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ»Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾, Ñ‡Ñ‚Ğ¾ Ğ²Ğ°Ğ¶Ğ½Ğ¾ Ğ´Ğ»Ñ ÑƒĞ¼Ğ½Ñ‹Ñ… Ğ´Ğ¾Ğ¼Ğ¾Ğ² Ğ¸ Ğ·Ğ´Ñ€Ğ°Ğ²Ğ¾Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ´Ğ½Ğ¾Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ¾Ğ². ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Mixture-of-Heads Ğ¸ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğº Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'Personalized Video Understanding with One-Shot Learning', 'desc': "This paper introduces PVChat, a novel one-shot learning framework designed to enhance personalized video understanding in large language models. Unlike traditional models that struggle with identity-aware comprehension, PVChat allows for subject-specific question answering from just one video per subject. The framework employs a Mixture-of-Heads attention mechanism and a unique training strategy that combines image pre-training with video fine-tuning, optimizing the model's ability to recognize and respond to various types of inquiries. Evaluation results show that PVChat outperforms existing models in understanding personalized features across multiple datasets, including healthcare and entertainment scenarios."}, 'zh': {'title': 'ä¸ªæ€§åŒ–è§†é¢‘ç†è§£çš„æ–°çªç ´', 'desc': 'è§†é¢‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆViLLMsï¼‰åœ¨ä¸€èˆ¬è§†é¢‘ç†è§£æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨èº«ä»½æ„ŸçŸ¥ç†è§£ä¸Šå­˜åœ¨å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸€-shotå­¦ä¹ æ¡†æ¶PVChatï¼Œè¿™æ˜¯é¦–ä¸ªä¸ªæ€§åŒ–çš„ViLLMï¼Œèƒ½å¤Ÿä»æ¯ä¸ªä¸»ä½“çš„å•ä¸ªè§†é¢‘ä¸­è¿›è¡Œä¸»ä½“æ„ŸçŸ¥é—®ç­”ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡å¢å¼ºçš„æ··åˆå¤´ï¼ˆMoHï¼‰ä¼˜åŒ–ViLLMï¼Œå¹¶åˆ©ç”¨åˆæˆå¢å¼ºçš„è§†é¢‘é—®ç­”æ•°æ®é›†ï¼Œé‡‡ç”¨æ¸è¿›çš„å›¾åƒåˆ°è§†é¢‘å­¦ä¹ ç­–ç•¥ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¯„ä¼°PVChatï¼Œæ˜¾ç¤ºå…¶åœ¨ä¸ªæ€§åŒ–ç‰¹å¾ç†è§£æ–¹é¢çš„ä¼˜è¶Šæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.16921', 'title': 'When Preferences Diverge: Aligning Diffusion Models with Minority-Aware\n  Adaptive DPO', 'url': 'https://huggingface.co/papers/2503.16921', 'abstract': "In recent years, the field of image generation has witnessed significant advancements, particularly in fine-tuning methods that align models with universal human preferences. This paper explores the critical role of preference data in the training process of diffusion models, particularly in the context of Diffusion-DPO and its subsequent adaptations. We investigate the complexities surrounding universal human preferences in image generation, highlighting the subjective nature of these preferences and the challenges posed by minority samples in preference datasets. Through pilot experiments, we demonstrate the existence of minority samples and their detrimental effects on model performance. We propose Adaptive-DPO -- a novel approach that incorporates a minority-instance-aware metric into the DPO objective. This metric, which includes intra-annotator confidence and inter-annotator stability, distinguishes between majority and minority samples. We introduce an Adaptive-DPO loss function which improves the DPO loss in two ways: enhancing the model's learning of majority labels while mitigating the negative impact of minority samples. Our experiments demonstrate that this method effectively handles both synthetic minority data and real-world preference data, paving the way for more effective training methodologies in image generation tasks.", 'score': 5, 'issue_id': 2852, 'pub_date': '2025-03-21', 'pub_date_card': {'ru': '21 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 21', 'zh': '3æœˆ21æ—¥'}, 'hash': '6bd4875e712b7a36', 'authors': ['Lingfan Zhang', 'Chen Liu', 'Chengming Xu', 'Kai Hu', 'Donghao Luo', 'Chengjie Wang', 'Yanwei Fu', 'Yuan Yao'], 'affiliations': ['Carnegie Mellon University', 'Fudan University', 'Tencent', 'The Hong Kong University of Science and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2503.16921.jpg', 'data': {'categories': ['#rlhf', '#training', '#diffusion', '#alignment'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ñ€Ğ¾Ğ»ÑŒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑÑ… Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ»ÑÑÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ½ĞµĞ³Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ğº Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğ° Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Adaptive-DPO, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ğº Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğ° Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ ĞºĞ°Ğº ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ, Ñ‚Ğ°Ğº Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑÑ….'}, 'en': {'title': 'Enhancing Image Generation with Adaptive-DPO: Balancing Preferences for Better Models', 'desc': 'This paper discusses advancements in image generation, focusing on how preference data can improve diffusion models. It highlights the challenges of incorporating universal human preferences, especially the impact of minority samples in preference datasets. The authors introduce Adaptive-DPO, a new method that uses a metric to better handle these minority samples during training. Their experiments show that Adaptive-DPO enhances model performance by balancing the influence of majority and minority preferences.'}, 'zh': {'title': 'é€‚åº”æ€§DPOï¼šæå‡å›¾åƒç”Ÿæˆæ¨¡å‹çš„åå¥½å­¦ä¹ ', 'desc': 'è¿‘å¹´æ¥ï¼Œå›¾åƒç”Ÿæˆé¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œå°¤å…¶æ˜¯åœ¨å¾®è°ƒæ–¹æ³•æ–¹é¢ï¼Œè¿™äº›æ–¹æ³•ä½¿æ¨¡å‹ä¸æ™®éçš„äººç±»åå¥½å¯¹é½ã€‚æœ¬æ–‡æ¢è®¨äº†åå¥½æ•°æ®åœ¨æ‰©æ•£æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­çš„å…³é”®ä½œç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨Diffusion-DPOåŠå…¶åç»­é€‚åº”ä¸­ã€‚æˆ‘ä»¬ç ”ç©¶äº†å›¾åƒç”Ÿæˆä¸­æ™®éäººç±»åå¥½çš„å¤æ‚æ€§ï¼Œå¼ºè°ƒäº†è¿™äº›åå¥½çš„ä¸»è§‚æ€§ä»¥åŠåå¥½æ•°æ®é›†ä¸­å°‘æ•°æ ·æœ¬å¸¦æ¥çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•Adaptive-DPOï¼Œé€šè¿‡å¼•å…¥å…³æ³¨å°‘æ•°å®ä¾‹çš„åº¦é‡ï¼Œæ”¹å–„äº†æ¨¡å‹å¯¹å¤šæ•°æ ‡ç­¾çš„å­¦ä¹ ï¼ŒåŒæ—¶å‡è½»äº†å°‘æ•°æ ·æœ¬çš„è´Ÿé¢å½±å“ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.16423', 'title': 'GAEA: A Geolocation Aware Conversational Model', 'url': 'https://huggingface.co/papers/2503.16423', 'abstract': 'Image geolocalization, in which, traditionally, an AI model predicts the precise GPS coordinates of an image is a challenging task with many downstream applications. However, the user cannot utilize the model to further their knowledge other than the GPS coordinate; the model lacks an understanding of the location and the conversational ability to communicate with the user. In recent days, with tremendous progress of large multimodal models (LMMs) proprietary and open-source researchers have attempted to geolocalize images via LMMs. However, the issues remain unaddressed; beyond general tasks, for more specialized downstream tasks, one of which is geolocalization, LMMs struggle. In this work, we propose to solve this problem by introducing a conversational model GAEA that can provide information regarding the location of an image, as required by a user. No large-scale dataset enabling the training of such a model exists. Thus we propose a comprehensive dataset GAEA with 800K images and around 1.6M question answer pairs constructed by leveraging OpenStreetMap (OSM) attributes and geographical context clues. For quantitative evaluation, we propose a diverse benchmark comprising 4K image-text pairs to evaluate conversational capabilities equipped with diverse question types. We consider 11 state-of-the-art open-source and proprietary LMMs and demonstrate that GAEA significantly outperforms the best open-source model, LLaVA-OneVision by 25.69% and the best proprietary model, GPT-4o by 8.28%. Our dataset, model and codes are available', 'score': 5, 'issue_id': 2864, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 20', 'zh': '3æœˆ20æ—¥'}, 'hash': 'ca9d5aa3c56f03d9', 'authors': ['Ron Campos', 'Ashmal Vayani', 'Parth Parag Kulkarni', 'Rohit Gupta', 'Aritra Dutta', 'Mubarak Shah'], 'affiliations': ['University of Central Florida'], 'pdf_title_img': 'assets/pdf/title_img/2503.16423.jpg', 'data': {'categories': ['#benchmark', '#open_source', '#dataset', '#science', '#multimodal'], 'emoji': 'ğŸ—ºï¸', 'ru': {'title': 'GAEA: Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²Ğ°Ñ Ğ³ĞµĞ¾Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ½Ğ¾Ğ²Ğ¾Ğ¼ ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ GAEA Ğ´Ğ»Ñ Ğ³ĞµĞ¾Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸Ğ· 800 Ñ‚Ñ‹ÑÑÑ‡ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ 1,6 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° Ğ¿Ğ°Ñ€ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ OpenStreetMap Ğ¸ Ğ³ĞµĞ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ¸. Ğ”Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¸Ğ· 4000 Ğ¿Ğ°Ñ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ-Ñ‚ĞµĞºÑÑ‚ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚Ğ¸Ğ¿Ğ°Ğ¼Ğ¸ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². GAEA Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° 25,69% Ğ¸ 8,28% ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾.'}, 'en': {'title': 'GAEA: Conversational Geolocalization for Enhanced User Interaction', 'desc': 'This paper addresses the challenge of image geolocalization, where AI models predict GPS coordinates but lack conversational understanding of the location. The authors introduce GAEA, a conversational model that not only geolocalizes images but also provides contextual information to users. To train this model, they created a new dataset called GAEA, consisting of 800,000 images and 1.6 million question-answer pairs derived from OpenStreetMap data. The results show that GAEA outperforms existing large multimodal models in conversational capabilities related to geolocalization tasks.'}, 'zh': {'title': 'GAEAï¼šå›¾åƒåœ°ç†å®šä½çš„æ–°å¯¹è¯æ¨¡å‹', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¯¹è¯æ¨¡å‹GAEAï¼Œç”¨äºå›¾åƒåœ°ç†å®šä½ã€‚ä¼ ç»Ÿçš„AIæ¨¡å‹åªèƒ½é¢„æµ‹å›¾åƒçš„GPSåæ ‡ï¼Œç¼ºä¹ä¸ç”¨æˆ·çš„äº’åŠ¨èƒ½åŠ›ã€‚GAEAé€šè¿‡æä¾›ä¸å›¾åƒä½ç½®ç›¸å…³çš„ä¿¡æ¯ï¼Œè§£å†³äº†è¿™ä¸€é—®é¢˜ã€‚æˆ‘ä»¬è¿˜æ„å»ºäº†ä¸€ä¸ªåŒ…å«80ä¸‡å¼ å›¾åƒå’Œ160ä¸‡ä¸ªé—®ç­”å¯¹çš„ç»¼åˆæ•°æ®é›†ï¼Œä»¥æ”¯æŒæ¨¡å‹çš„è®­ç»ƒå’Œè¯„ä¼°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.16282', 'title': 'Generalized Few-shot 3D Point Cloud Segmentation with Vision-Language\n  Model', 'url': 'https://huggingface.co/papers/2503.16282', 'abstract': 'Generalized few-shot 3D point cloud segmentation (GFS-PCS) adapts models to new classes with few support samples while retaining base class segmentation. Existing GFS-PCS methods enhance prototypes via interacting with support or query features but remain limited by sparse knowledge from few-shot samples. Meanwhile, 3D vision-language models (3D VLMs), generalizing across open-world novel classes, contain rich but noisy novel class knowledge. In this work, we introduce a GFS-PCS framework that synergizes dense but noisy pseudo-labels from 3D VLMs with precise yet sparse few-shot samples to maximize the strengths of both, named GFS-VL. Specifically, we present a prototype-guided pseudo-label selection to filter low-quality regions, followed by an adaptive infilling strategy that combines knowledge from pseudo-label contexts and few-shot samples to adaptively label the filtered, unlabeled areas. Additionally, we design a novel-base mix strategy to embed few-shot samples into training scenes, preserving essential context for improved novel class learning. Moreover, recognizing the limited diversity in current GFS-PCS benchmarks, we introduce two challenging benchmarks with diverse novel classes for comprehensive generalization evaluation. Experiments validate the effectiveness of our framework across models and datasets. Our approach and benchmarks provide a solid foundation for advancing GFS-PCS in the real world. The code is at https://github.com/ZhaochongAn/GFS-VL', 'score': 5, 'issue_id': 2862, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 20', 'zh': '3æœˆ20æ—¥'}, 'hash': '32970c716a041be9', 'authors': ['Zhaochong An', 'Guolei Sun', 'Yun Liu', 'Runjia Li', 'Junlin Han', 'Ender Konukoglu', 'Serge Belongie'], 'affiliations': ['College of Computer Science, Nankai University', 'Computer Vision Laboratory, ETH Zurich', 'Department of Computer Science, University of Copenhagen', 'Department of Engineering Science, University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2503.16282.jpg', 'data': {'categories': ['#games', '#transfer_learning', '#3d', '#benchmark'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ¡Ğ¸Ğ½ĞµÑ€Ğ³Ğ¸Ñ 3D VLM Ğ¸ few-shot Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±Ğ»Ğ°ĞºĞ¾Ğ² Ñ‚Ğ¾Ñ‡ĞµĞº', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±Ğ»Ğ°ĞºĞ¾Ğ² Ñ‚Ğ¾Ñ‡ĞµĞº 3D Ñ Ğ¼Ğ°Ğ»Ñ‹Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² (GFS-PCS). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ framework GFS-VL, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ğµ, Ğ½Ğ¾ ÑˆÑƒĞ¼Ğ½Ñ‹Ğµ Ğ¿ÑĞµĞ²Ğ´Ğ¾-Ğ¼ĞµÑ‚ĞºĞ¸ Ğ¸Ğ· 3D vision-language Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼Ğ¸, Ğ½Ğ¾ Ñ€ĞµĞ´ĞºĞ¸Ğ¼Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ°Ğ¼Ğ¸ few-shot. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¾Ñ‚Ğ±Ğ¾Ñ€ Ğ¿ÑĞµĞ²Ğ´Ğ¾-Ğ¼ĞµÑ‚Ğ¾Ğº Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ğ½ĞµÑ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ñ‹ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ½Ğ¾Ğ²Ñ‹Ğµ ĞºĞ»Ğ°ÑÑÑ‹.'}, 'en': {'title': 'Maximizing Learning with Few Samples in 3D Segmentation', 'desc': "This paper presents a new framework called GFS-VL for generalized few-shot 3D point cloud segmentation, which allows models to learn new classes with only a few examples while still recognizing previously learned classes. The approach combines the strengths of 3D vision-language models, which provide rich but noisy information about new classes, with precise but limited few-shot samples. It introduces a method for selecting high-quality pseudo-labels and an adaptive strategy to fill in gaps in the data, enhancing the labeling of unlabeled areas. Additionally, the authors propose new benchmarks to evaluate the model's performance on diverse novel classes, demonstrating the framework's effectiveness across various datasets and models."}, 'zh': {'title': 'èåˆä¼ªæ ‡ç­¾ä¸å°‘æ ·æœ¬çš„3Dç‚¹äº‘åˆ†å‰²æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶GFS-VLï¼Œç”¨äºè§£å†³å°‘æ ·æœ¬3Dç‚¹äº‘åˆ†å‰²é—®é¢˜ã€‚è¯¥æ¡†æ¶ç»“åˆäº†æ¥è‡ª3Dè§†è§‰è¯­è¨€æ¨¡å‹çš„ä¸°å¯Œä¼ªæ ‡ç­¾å’Œç¨€ç–çš„å°‘æ ·æœ¬æ•°æ®ï¼Œä»¥æé«˜æ¨¡å‹åœ¨æ–°ç±»åˆ«ä¸Šçš„é€‚åº”èƒ½åŠ›ã€‚é€šè¿‡åŸå‹å¼•å¯¼çš„ä¼ªæ ‡ç­¾é€‰æ‹©å’Œè‡ªé€‚åº”å¡«å……ç­–ç•¥ï¼ŒGFS-VLèƒ½å¤Ÿæœ‰æ•ˆè¿‡æ»¤ä½è´¨é‡åŒºåŸŸå¹¶æ ‡è®°æœªæ ‡è®°åŒºåŸŸã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†æ–°çš„åŸºå‡†æµ‹è¯•ï¼Œä»¥è¯„ä¼°æ¨¡å‹åœ¨å¤šæ ·åŒ–æ–°ç±»åˆ«ä¸Šçš„æ³›åŒ–èƒ½åŠ›ï¼Œå®éªŒç»“æœéªŒè¯äº†è¯¥æ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.11572', 'title': 'Implicit Bias-Like Patterns in Reasoning Models', 'url': 'https://huggingface.co/papers/2503.11572', 'abstract': "Implicit bias refers to automatic or spontaneous mental processes that shape perceptions, judgments, and behaviors. Previous research examining `implicit bias' in large language models (LLMs) has often approached the phenomenon differently than how it is studied in humans by focusing primarily on model outputs rather than on model processing. To examine model processing, we present a method called the Reasoning Model Implicit Association Test (RM-IAT) for studying implicit bias-like patterns in reasoning models: LLMs that employ step-by-step reasoning to solve complex tasks. Using this method, we find that reasoning models require more tokens when processing association-incompatible information compared to association-compatible information. These findings suggest AI systems harbor patterns in processing information that are analogous to human implicit bias. We consider the implications of these implicit bias-like patterns for their deployment in real-world applications.", 'score': 5, 'issue_id': 2854, 'pub_date': '2025-03-14', 'pub_date_card': {'ru': '14 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 14', 'zh': '3æœˆ14æ—¥'}, 'hash': 'f45b684d5c68e00c', 'authors': ['Messi H. J. Lee', 'Calvin K. Lai'], 'affiliations': ['Department of Psychology Rutgers University', 'Division of Computational and Data Sciences Washington University in St. Louis'], 'pdf_title_img': 'assets/pdf/title_img/2503.11572.jpg', 'data': {'categories': ['#interpretability', '#reasoning', '#ethics', '#multimodal', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ¡ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ˜Ğ˜: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ RM-IAT Ğ´Ğ»Ñ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ¼Ğ¿Ğ»Ğ¸Ñ†Ğ¸Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸ (reasoning models). Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¸Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ÑÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸, Ğ½ĞµÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ¾Ğ¹ Ñ Ğ°ÑÑĞ¾Ñ†Ğ¸Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸, Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ¾Ğ¹. Ğ­Ñ‚Ğ¾ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ° Ğ½Ğ°Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ² Ğ˜Ğ˜-ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸, Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼ Ğ¸Ğ¼Ğ¿Ğ»Ğ¸Ñ†Ğ¸Ñ‚Ğ½Ñ‹Ğ¼ Ğ¿Ñ€ĞµĞ´ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑÑ‚Ğ²Ğ¸Ñ ÑÑ‚Ğ¸Ñ… Ğ¿Ñ€ĞµĞ´ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Unveiling Implicit Bias in AI Reasoning Models', 'desc': 'This paper introduces a new method called the Reasoning Model Implicit Association Test (RM-IAT) to study implicit bias in large language models (LLMs). Unlike previous research that focused on outputs, this approach examines how LLMs process information, particularly in reasoning tasks. The study finds that LLMs take longer to process information that conflicts with established associations, similar to human implicit biases. These insights raise important considerations for the use of AI in real-world applications, highlighting the need to understand underlying processing patterns.'}, 'zh': {'title': 'æ­ç¤ºAIä¸­çš„éšæ€§åè§æ¨¡å¼', 'desc': 'éšæ€§åè§æ˜¯æŒ‡è‡ªåŠ¨æˆ–è‡ªå‘çš„å¿ƒç†è¿‡ç¨‹ï¼Œè¿™äº›è¿‡ç¨‹å½±å“æˆ‘ä»¬çš„æ„ŸçŸ¥ã€åˆ¤æ–­å’Œè¡Œä¸ºã€‚ä»¥å¾€å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­éšæ€§åè§çš„ç ”ç©¶ï¼Œä¸»è¦å…³æ³¨æ¨¡å‹è¾“å‡ºï¼Œè€Œéæ¨¡å‹å¤„ç†è¿‡ç¨‹ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºæ¨ç†æ¨¡å‹éšæ€§è”æƒ³æµ‹è¯•ï¼ˆRM-IATï¼‰çš„æ–¹æ³•ï¼Œç”¨äºç ”ç©¶æ¨ç†æ¨¡å‹ä¸­çš„éšæ€§åè§æ¨¡å¼ã€‚ç ”ç©¶å‘ç°ï¼Œæ¨ç†æ¨¡å‹åœ¨å¤„ç†ä¸è”æƒ³ä¸å…¼å®¹çš„ä¿¡æ¯æ—¶ï¼Œéœ€è¦æ›´å¤šçš„æ ‡è®°ï¼Œè¿™è¡¨æ˜AIç³»ç»Ÿåœ¨ä¿¡æ¯å¤„ç†ä¸Šå­˜åœ¨ç±»ä¼¼äºäººç±»éšæ€§åè§çš„æ¨¡å¼ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.14607', 'title': 'Can Large Vision Language Models Read Maps Like a Human?', 'url': 'https://huggingface.co/papers/2503.14607', 'abstract': 'In this paper, we introduce MapBench-the first dataset specifically designed for human-readable, pixel-based map-based outdoor navigation, curated from complex path finding scenarios. MapBench comprises over 1600 pixel space map path finding problems from 100 diverse maps. In MapBench, LVLMs generate language-based navigation instructions given a map image and a query with beginning and end landmarks. For each map, MapBench provides Map Space Scene Graph (MSSG) as an indexing data structure to convert between natural language and evaluate LVLM-generated results. We demonstrate that MapBench significantly challenges state-of-the-art LVLMs both zero-shot prompting and a Chain-of-Thought (CoT) augmented reasoning framework that decomposes map navigation into sequential cognitive processes. Our evaluation of both open-source and closed-source LVLMs underscores the substantial difficulty posed by MapBench, revealing critical limitations in their spatial reasoning and structured decision-making capabilities. We release all the code and dataset in https://github.com/taco-group/MapBench.', 'score': 3, 'issue_id': 2868, 'pub_date': '2025-03-18', 'pub_date_card': {'ru': '18 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 18', 'zh': '3æœˆ18æ—¥'}, 'hash': 'a67ca23976ed87ed', 'authors': ['Shuo Xing', 'Zezhou Sun', 'Shuangyu Xie', 'Kaiyuan Chen', 'Yanjia Huang', 'Yuping Wang', 'Jiachen Li', 'Dezhen Song', 'Zhengzhong Tu'], 'affiliations': ['MBZUAI', 'Texas A&M University', 'UC Berkeley', 'UC Riverside', 'University of Michigan'], 'pdf_title_img': 'assets/pdf/title_img/2503.14607.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#open_source', '#reasoning'], 'emoji': 'ğŸ—ºï¸', 'ru': {'title': 'MapBench: ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ²Ñ‹Ğ·Ğ¾Ğ² Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ ĞºĞ°Ñ€Ñ‚Ğ°Ğ¼', 'desc': 'MapBench - ÑÑ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ´Ğ»Ñ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‡Ğ¸Ñ‚Ğ°ĞµĞ¼Ñ‹Ñ… Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¼ Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ñ‹Ñ… ĞºĞ°Ñ€Ñ‚. ĞĞ½ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ±Ğ¾Ğ»ĞµĞµ 1600 Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºÑƒ Ğ¿ÑƒÑ‚Ğ¸ Ğ½Ğ° 100 Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ĞºĞ°Ñ€Ñ‚Ğ°Ñ…. Ğ’ MapBench ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ Ğ¾Ğ±ÑŠĞµĞ¼Ğ¾Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… (LVLMs) Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ°Ñ€Ñ‚Ñ‹ Ğ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ° Ñ Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¸ ĞºĞ¾Ğ½ĞµÑ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ°Ğ¼Ğ¸. ĞÑ†ĞµĞ½ĞºĞ° Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ°, Ñ‡Ñ‚Ğ¾ MapBench Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ»Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… LVLMs, Ğ²Ñ‹ÑĞ²Ğ»ÑÑ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ² Ğ¸Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğ¸ Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'MapBench: Navigating the Future of Language and Vision Models', 'desc': 'This paper presents MapBench, a novel dataset tailored for evaluating language-based navigation in outdoor environments using pixel-based maps. It includes over 1600 pathfinding scenarios across 100 unique maps, allowing for comprehensive testing of language-vision models (LVLMs). The dataset features a Map Space Scene Graph (MSSG) to facilitate the conversion between natural language instructions and map images, enhancing the evaluation of LVLM outputs. The findings reveal significant challenges for current LVLMs in spatial reasoning and decision-making, highlighting the need for improved models in complex navigation tasks.'}, 'zh': {'title': 'MapBenchï¼šæŒ‘æˆ˜è¯­è¨€è§†è§‰æ¨¡å‹çš„åœ°å›¾å¯¼èˆªæ•°æ®é›†', 'desc': 'æœ¬æ–‡ä»‹ç»äº†MapBenchï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªä¸“é—¨ä¸ºäººç±»å¯è¯»çš„åŸºäºåƒç´ çš„åœ°å›¾å¯¼èˆªè®¾è®¡çš„æ•°æ®é›†ï¼Œæ¥æºäºå¤æ‚çš„è·¯å¾„å¯»æ‰¾åœºæ™¯ã€‚MapBenchåŒ…å«æ¥è‡ª100ä¸ªä¸åŒåœ°å›¾çš„1600å¤šä¸ªåƒç´ ç©ºé—´åœ°å›¾è·¯å¾„å¯»æ‰¾é—®é¢˜ã€‚åœ¨MapBenchä¸­ï¼ŒLVLMï¼ˆè¯­è¨€è§†è§‰å¤§æ¨¡å‹ï¼‰æ ¹æ®åœ°å›¾å›¾åƒå’Œèµ·å§‹ä¸ç»“æŸåœ°æ ‡çš„æŸ¥è¯¢ç”ŸæˆåŸºäºè¯­è¨€çš„å¯¼èˆªæŒ‡ä»¤ã€‚æˆ‘ä»¬çš„è¯„ä¼°æ˜¾ç¤ºï¼ŒMapBenchå¯¹ç°æœ‰çš„LVLMåœ¨é›¶-shot æç¤ºå’Œé“¾å¼æ€ç»´æ¨ç†æ¡†æ¶ä¸‹æå‡ºäº†æ˜¾è‘—æŒ‘æˆ˜ï¼Œæ­ç¤ºäº†å®ƒä»¬åœ¨ç©ºé—´æ¨ç†å’Œç»“æ„åŒ–å†³ç­–èƒ½åŠ›æ–¹é¢çš„å…³é”®å±€é™æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.17095', 'title': 'FFaceNeRF: Few-shot Face Editing in Neural Radiance Fields', 'url': 'https://huggingface.co/papers/2503.17095', 'abstract': 'Recent 3D face editing methods using masks have produced high-quality edited images by leveraging Neural Radiance Fields (NeRF). Despite their impressive performance, existing methods often provide limited user control due to the use of pre-trained segmentation masks. To utilize masks with a desired layout, an extensive training dataset is required, which is challenging to gather. We present FFaceNeRF, a NeRF-based face editing technique that can overcome the challenge of limited user control due to the use of fixed mask layouts. Our method employs a geometry adapter with feature injection, allowing for effective manipulation of geometry attributes. Additionally, we adopt latent mixing for tri-plane augmentation, which enables training with a few samples. This facilitates rapid model adaptation to desired mask layouts, crucial for applications in fields like personalized medical imaging or creative face editing. Our comparative evaluations demonstrate that FFaceNeRF surpasses existing mask based face editing methods in terms of flexibility, control, and generated image quality, paving the way for future advancements in customized and high-fidelity 3D face editing. The code is available on the {https://kwanyun.github.io/FFaceNeRF_page/{project-page}}.', 'score': 2, 'issue_id': 2862, 'pub_date': '2025-03-21', 'pub_date_card': {'ru': '21 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 21', 'zh': '3æœˆ21æ—¥'}, 'hash': 'ebada6e24359eec7', 'authors': ['Kwan Yun', 'Chaelin Kim', 'Hangyeul Shin', 'Junyong Noh'], 'affiliations': ['Handong Global University', 'KAIST, Visual Media Lab'], 'pdf_title_img': 'assets/pdf/title_img/2503.17095.jpg', 'data': {'categories': ['#open_source', '#optimization', '#3d'], 'emoji': 'ğŸ­', 'ru': {'title': 'Ğ“Ğ¸Ğ±ĞºĞ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ 3D-Ğ»Ğ¸Ñ† Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ°ÑĞ¾Ğº Ğ¸ NeRF', 'desc': 'FFaceNeRF - ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ 3D-Ğ»Ğ¸Ñ† Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»ĞµĞ¹ Ğ¸Ğ·Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ (NeRF), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°ĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ² Ñ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ°ÑĞºĞ°Ğ¼Ğ¸ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€ Ñ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ°Ğ¼Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ ÑĞ¼ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ñ€ĞµÑ…Ğ¿Ğ»Ğ¾ÑĞºĞ¾ÑÑ‚Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒÑÑ Ğ½Ğ° Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ². FFaceNeRF Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ»Ğ¸Ñ† Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ°ÑĞ¾Ğº Ğ¿Ğ¾ Ğ³Ğ¸Ğ±ĞºĞ¾ÑÑ‚Ğ¸, ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Empowering 3D Face Editing with FFaceNeRF', 'desc': 'FFaceNeRF is a novel face editing technique that enhances user control in 3D face editing by utilizing Neural Radiance Fields (NeRF). Unlike traditional methods that rely on fixed segmentation masks, FFaceNeRF introduces a geometry adapter with feature injection, allowing users to manipulate geometry attributes more freely. The method also incorporates latent mixing for tri-plane augmentation, enabling effective training with limited data samples. Our evaluations show that FFaceNeRF outperforms existing methods in flexibility, control, and image quality, making it a significant advancement in personalized and high-fidelity 3D face editing.'}, 'zh': {'title': 'FFaceNeRFï¼šçµæ´»é«˜æ•ˆçš„äººè„¸ç¼–è¾‘æ–°æ–¹æ³•', 'desc': 'æœ€è¿‘çš„3Däººè„¸ç¼–è¾‘æ–¹æ³•åˆ©ç”¨ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰é€šè¿‡æ©è†œç”Ÿæˆé«˜è´¨é‡çš„ç¼–è¾‘å›¾åƒã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ç”±äºä½¿ç”¨é¢„è®­ç»ƒçš„åˆ†å‰²æ©è†œï¼Œå¾€å¾€é™åˆ¶äº†ç”¨æˆ·çš„æ§åˆ¶èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†FFaceNeRFï¼Œè¿™æ˜¯ä¸€ç§åŸºäºNeRFçš„äººè„¸ç¼–è¾‘æŠ€æœ¯ï¼Œèƒ½å¤Ÿå…‹æœå›ºå®šæ©è†œå¸ƒå±€å¸¦æ¥çš„æ§åˆ¶é™åˆ¶ã€‚è¯¥æ–¹æ³•é‡‡ç”¨å‡ ä½•é€‚é…å™¨å’Œç‰¹å¾æ³¨å…¥ï¼Œå…è®¸æœ‰æ•ˆæ“æ§å‡ ä½•å±æ€§ï¼Œå¹¶é€šè¿‡æ½œåœ¨æ··åˆè¿›è¡Œä¸‰å¹³é¢å¢å¼ºï¼Œä½¿å¾—åœ¨å°‘é‡æ ·æœ¬ä¸‹ä¹Ÿèƒ½å¿«é€Ÿé€‚åº”æ‰€éœ€çš„æ©è†œå¸ƒå±€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.00865', 'title': 'Babel: Open Multilingual Large Language Models Serving Over 90% of Global Speakers', 'url': 'https://huggingface.co/papers/2503.00865', 'abstract': "Large language models (LLMs) have revolutionized natural language processing (NLP), yet open-source multilingual LLMs remain scarce, with existing models often limited in language coverage. Such models typically prioritize well-resourced languages, while widely spoken but under-resourced languages are often overlooked. To address this disparity, we introduce Babel, an open multilingual LLM that covers the top 25 languages by number of speakers, supports over 90% of the global population, and includes many languages neglected by other open multilingual LLMs. Unlike traditional continue pretraining approaches, Babel expands its parameter count through a layer extension technique that elevates Babel's performance ceiling. We introduce two variants: Babel-9B, designed for efficient inference and fine-tuning, and Babel-83B, which sets a new standard for open multilingual LLMs. Extensive evaluations on multilingual tasks demonstrate its superior performance compared to open LLMs of comparable size. In addition, using open-source supervised fine-tuning datasets, Babel achieves remarkable performance, with Babel-9B-Chat leading among 10B-sized LLMs and Babel-83B-Chat setting a new standard for multilingual tasks, reaching the same level of commercial models.", 'score': 40, 'issue_id': 2555, 'pub_date': '2025-03-02', 'pub_date_card': {'ru': '2 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 2', 'zh': '3æœˆ2æ—¥'}, 'hash': 'bc2424e709a2dd78', 'authors': ['Yiran Zhao', 'Chaoqun Liu', 'Yue Deng', 'Jiahao Ying', 'Mahani Aljunied', 'Zhaodonghui Li', 'Lidong Bing', 'Hou Pong Chan', 'Yu Rong', 'Deli Zhao', 'Wenxuan Zhang'], 'affiliations': ['DAMO Academy, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2503.00865.jpg', 'data': {'categories': ['#low_resource', '#architecture', '#open_source', '#training', '#multilingual'], 'emoji': 'ğŸŒ', 'ru': {'title': 'Babel: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ¼ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Babel, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ°Ñ 25 ÑĞ°Ğ¼Ñ‹Ñ… Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ² Ğ¼Ğ¸Ñ€Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾ĞµĞ² Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ñ‹ Ğ´Ğ²Ğµ Ğ²ĞµÑ€ÑĞ¸Ğ¸: Babel-9B Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¸ Babel-83B, ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ÑÑ‰Ğ°Ñ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ´Ğ»Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ±Ğµ Ğ²ĞµÑ€ÑĞ¸Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸.'}, 'en': {'title': 'Babel: Bridging the Language Gap with Open Multilingual LLMs', 'desc': "This paper presents Babel, an innovative open-source multilingual large language model (LLM) that addresses the lack of coverage for under-resourced languages in existing models. Babel supports the top 25 languages spoken globally, reaching over 90% of the world's population, and includes many languages that are often neglected. The model employs a unique layer extension technique to increase its parameter count, enhancing its performance beyond traditional pretraining methods. With two variants, Babel-9B and Babel-83B, the model demonstrates superior performance on multilingual tasks, outperforming other open LLMs of similar size and achieving results comparable to commercial models."}, 'zh': {'title': 'Babelï¼šæ‰“ç ´è¯­è¨€å£å’çš„å¤šè¯­è¨€æ¨¡å‹', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰é¢†åŸŸå¸¦æ¥äº†é©å‘½æ€§çš„å˜åŒ–ï¼Œä½†å¼€æºçš„å¤šè¯­è¨€LLMsä»ç„¶ç¨€ç¼ºï¼Œç°æœ‰æ¨¡å‹é€šå¸¸åœ¨è¯­è¨€è¦†ç›–ä¸Šæœ‰é™ã€‚è®¸å¤šæ¨¡å‹ä¼˜å…ˆè€ƒè™‘èµ„æºä¸°å¯Œçš„è¯­è¨€ï¼Œè€Œå¹¿æ³›ä½¿ç”¨ä½†èµ„æºä¸è¶³çš„è¯­è¨€å¸¸å¸¸è¢«å¿½è§†ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†Babelï¼Œä¸€ä¸ªå¼€æ”¾çš„å¤šè¯­è¨€LLMï¼Œè¦†ç›–å…¨çƒå‰25ç§è¯­è¨€ï¼Œæ”¯æŒè¶…è¿‡90%çš„äººå£ï¼Œå¹¶åŒ…æ‹¬è®¸å¤šå…¶ä»–å¼€æºå¤šè¯­è¨€LLMså¿½è§†çš„è¯­è¨€ã€‚Babelé€šè¿‡å±‚æ‰©å±•æŠ€æœ¯å¢åŠ å‚æ•°æ•°é‡ï¼Œæå‡äº†æ€§èƒ½ï¼Œå¹¶æ¨å‡ºäº†ä¸¤ä¸ªå˜ä½“ï¼šBabel-9Bå’ŒBabel-83Bï¼Œåè€…åœ¨å¤šè¯­è¨€ä»»åŠ¡ä¸­è®¾å®šäº†æ–°çš„æ ‡å‡†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.03746', 'title': 'Process-based Self-Rewarding Language Models', 'url': 'https://huggingface.co/papers/2503.03746', 'abstract': "Large Language Models have demonstrated outstanding performance across various downstream tasks and have been widely applied in multiple scenarios. Human-annotated preference data is used for training to further improve LLMs' performance, which is constrained by the upper limit of human performance. Therefore, Self-Rewarding method has been proposed, where LLMs generate training data by rewarding their own outputs. However, the existing self-rewarding paradigm is not effective in mathematical reasoning scenarios and may even lead to a decline in performance. In this work, we propose the Process-based Self-Rewarding pipeline for language models, which introduces long-thought reasoning, step-wise LLM-as-a-Judge, and step-wise preference optimization within the self-rewarding paradigm. Our new paradigm successfully enhances the performance of LLMs on multiple mathematical reasoning benchmarks through iterative Process-based Self-Rewarding, demonstrating the immense potential of self-rewarding to achieve LLM reasoning that may surpass human capabilities.", 'score': 21, 'issue_id': 2564, 'pub_date': '2025-03-05', 'pub_date_card': {'ru': '5 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 5', 'zh': '3æœˆ5æ—¥'}, 'hash': '808bee960390ec29', 'authors': ['Shimao Zhang', 'Xiao Liu', 'Xin Zhang', 'Junxiao Liu', 'Zheheng Luo', 'Shujian Huang', 'Yeyun Gong'], 'affiliations': ['Microsoft Research Asia', 'National Key Laboratory for Novel Software Technology, Nanjing University', 'The University of Manchester'], 'pdf_title_img': 'assets/pdf/title_img/2503.03746.jpg', 'data': {'categories': ['#math', '#alignment', '#rlhf', '#reasoning', '#training'], 'emoji': 'ğŸ§®', 'ru': {'title': 'Ğ¡Ğ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ˜Ğ˜ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞµ: ÑˆĞ°Ğ³ Ğ·Ğ° ÑˆĞ°Ğ³Ğ¾Ğ¼ Ğº ÑĞ²ĞµÑ€Ñ…Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑĞ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Process-based Self-Rewarding, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² ÑĞ°Ğ¼Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµÑ‚ÑŒ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğº ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ….'}, 'en': {'title': 'Empowering LLMs with Process-based Self-Rewarding for Superior Reasoning', 'desc': 'This paper discusses the limitations of current self-rewarding methods used to train Large Language Models (LLMs), particularly in mathematical reasoning tasks. The authors introduce a new approach called Process-based Self-Rewarding, which incorporates long-thought reasoning and a step-wise evaluation process. By allowing LLMs to act as judges of their own outputs, this method optimizes the training process iteratively. The results show significant improvements in LLM performance on mathematical reasoning benchmarks, suggesting that self-rewarding can enhance reasoning capabilities beyond human levels.'}, 'zh': {'title': 'åŸºäºè¿‡ç¨‹çš„è‡ªæˆ‘å¥–åŠ±ï¼šè¶…è¶Šäººç±»çš„æ¨ç†èƒ½åŠ›', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å„ç§ä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå¹¶å¹¿æ³›åº”ç”¨äºå¤šä¸ªåœºæ™¯ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜å…¶æ€§èƒ½ï¼Œç ”ç©¶è€…ä½¿ç”¨äººç±»æ ‡æ³¨çš„åå¥½æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œä½†è¿™å—åˆ°äººç±»è¡¨ç°ä¸Šé™çš„é™åˆ¶ã€‚å› æ­¤ï¼Œæå‡ºäº†è‡ªæˆ‘å¥–åŠ±çš„æ–¹æ³•ï¼Œè®©è¯­è¨€æ¨¡å‹é€šè¿‡å¥–åŠ±è‡ªå·±çš„è¾“å‡ºç”Ÿæˆè®­ç»ƒæ•°æ®ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è‡ªæˆ‘å¥–åŠ±æ–¹æ³•åœ¨æ•°å­¦æ¨ç†åœºæ™¯ä¸­æ•ˆæœä¸ä½³ï¼Œç”šè‡³å¯èƒ½å¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè¿‡ç¨‹çš„è‡ªæˆ‘å¥–åŠ±ç®¡é“ï¼Œé€šè¿‡å¼•å…¥é•¿æ—¶é—´æ€è€ƒæ¨ç†ã€é€æ­¥çš„è¯­è¨€æ¨¡å‹è¯„åˆ¤å’Œé€æ­¥çš„åå¥½ä¼˜åŒ–ï¼ŒæˆåŠŸæå‡äº†è¯­è¨€æ¨¡å‹åœ¨å¤šä¸ªæ•°å­¦æ¨ç†åŸºå‡†ä¸Šçš„è¡¨ç°ï¼Œå±•ç¤ºäº†è‡ªæˆ‘å¥–åŠ±åœ¨è¶…è¶Šäººç±»èƒ½åŠ›çš„æ¨ç†ä¸­çš„å·¨å¤§æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.00329', 'title': 'ABC: Achieving Better Control of Multimodal Embeddings using VLMs', 'url': 'https://huggingface.co/papers/2503.00329', 'abstract': 'Visual embedding models excel at zero-shot tasks like visual retrieval and classification. However, these models cannot be used for tasks that contain ambiguity or require user instruction. These tasks necessitate a multimodal embedding model, which outputs embeddings that combine visual and natural language input. Existing CLIP-based approaches embed images and text independently, and fuse the result. We find that this results in weak interactions between modalities, and poor user control over the representation. We introduce ABC, an open-source multimodal embedding model that uses a vision-language model backbone to deeply integrate image features with natural language instructions. ABC achieves bestfor-size performance on MSCOCO image-to-text retrieval and is the top performing model on classification and VQA tasks in the Massive Multimodal Embedding Benchmark. With a strongly unified vision-language representation, ABC can use natural language to solve subtle and potentially ambiguous visual retrieval problems. To evaluate this capability, we design CtrlBench, a benchmark that requires interleaving textual instructions with image content for correct retrieval. ABC advances the state of multimodal embeddings by offering high-quality representations and flexible natural language control. Our model and datasets are available at our project page.', 'score': 14, 'issue_id': 2555, 'pub_date': '2025-03-01', 'pub_date_card': {'ru': '1 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 1', 'zh': '3æœˆ1æ—¥'}, 'hash': '0483c542c8885777', 'authors': ['Benjamin Schneider', 'Florian Kerschbaum', 'Wenhu Chen'], 'affiliations': ['Cheriton School of Computer Science, University of Waterloo', 'Vector Institute, Toronto'], 'pdf_title_img': 'assets/pdf/title_img/2503.00329.jpg', 'data': {'categories': ['#benchmark', '#multimodal', '#open_source', '#dataset'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ABC: ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ñ Ğ³Ğ¸Ğ±ĞºĞ¸Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ABC, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², ABC Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºÑƒÑ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ Ğ¸ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸. ABC Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ ÑĞ·Ñ‹Ğº Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ñ Ğ½ĞµĞ¾Ğ´Ğ½Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸.'}, 'en': {'title': 'ABC: Unifying Vision and Language for Enhanced Multimodal Understanding', 'desc': "This paper presents ABC, a new multimodal embedding model that integrates visual and natural language inputs more effectively than existing CLIP-based methods. Unlike previous models that treat images and text separately, ABC combines these modalities deeply, allowing for better interaction and user control. The model excels in zero-shot tasks, particularly in image-to-text retrieval and classification, outperforming others in the Massive Multimodal Embedding Benchmark. To assess its capabilities, the authors introduce CtrlBench, a benchmark designed to evaluate the model's performance in handling complex visual retrieval tasks with natural language instructions."}, 'zh': {'title': 'ABCï¼šå¤šæ¨¡æ€åµŒå…¥çš„æ–°çªç ´', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºABCçš„å¤šæ¨¡æ€åµŒå…¥æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³è§†è§‰æ£€ç´¢å’Œåˆ†ç±»ä¸­çš„æ¨¡ç³Šæ€§é—®é¢˜ã€‚ä¸ç°æœ‰çš„CLIPæ–¹æ³•ä¸åŒï¼ŒABCé€šè¿‡æ·±åº¦æ•´åˆå›¾åƒç‰¹å¾å’Œè‡ªç„¶è¯­è¨€æŒ‡ä»¤ï¼Œæä¾›æ›´å¼ºçš„æ¨¡æ€äº¤äº’ã€‚ABCåœ¨MSCOCOå›¾åƒåˆ°æ–‡æœ¬æ£€ç´¢ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå¹¶åœ¨åˆ†ç±»å’Œè§†è§‰é—®ç­”ä»»åŠ¡ä¸­å–å¾—äº†æœ€ä½³æ€§èƒ½ã€‚é€šè¿‡è®¾è®¡CtrlBenchåŸºå‡†ï¼Œè¯„ä¼°äº†ABCåœ¨å¤„ç†å¤æ‚è§†è§‰æ£€ç´¢é—®é¢˜æ—¶çš„èƒ½åŠ›ï¼Œå±•ç¤ºäº†å…¶é«˜è´¨é‡çš„è¡¨ç¤ºå’Œçµæ´»çš„è‡ªç„¶è¯­è¨€æ§åˆ¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.03751', 'title': 'GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control', 'url': 'https://huggingface.co/papers/2503.03751', 'abstract': 'We present GEN3C, a generative video model with precise Camera Control and temporal 3D Consistency. Prior video models already generate realistic videos, but they tend to leverage little 3D information, leading to inconsistencies, such as objects popping in and out of existence. Camera control, if implemented at all, is imprecise, because camera parameters are mere inputs to the neural network which must then infer how the video depends on the camera. In contrast, GEN3C is guided by a 3D cache: point clouds obtained by predicting the pixel-wise depth of seed images or previously generated frames. When generating the next frames, GEN3C is conditioned on the 2D renderings of the 3D cache with the new camera trajectory provided by the user. Crucially, this means that GEN3C neither has to remember what it previously generated nor does it have to infer the image structure from the camera pose. The model, instead, can focus all its generative power on previously unobserved regions, as well as advancing the scene state to the next frame. Our results demonstrate more precise camera control than prior work, as well as state-of-the-art results in sparse-view novel view synthesis, even in challenging settings such as driving scenes and monocular dynamic video. Results are best viewed in videos. Check out our webpage! https://research.nvidia.com/labs/toronto-ai/GEN3C/', 'score': 13, 'issue_id': 2555, 'pub_date': '2025-03-05', 'pub_date_card': {'ru': '5 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 5', 'zh': '3æœˆ5æ—¥'}, 'hash': '8f5f2ad910a260c0', 'authors': ['Xuanchi Ren', 'Tianchang Shen', 'Jiahui Huang', 'Huan Ling', 'Yifan Lu', 'Merlin Nimier-David', 'Thomas MÃ¼ller', 'Alexander Keller', 'Sanja Fidler', 'Jun Gao'], 'affiliations': ['NVIDIA', 'University of Toronto', 'Vector Institute'], 'pdf_title_img': 'assets/pdf/title_img/2503.03751.jpg', 'data': {'categories': ['#3d', '#video'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Ğ¢Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ¸ 3D-ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'GEN3C - ÑÑ‚Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ 3D-ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒÑ. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ 3D-ĞºÑÑˆ Ğ² Ğ²Ğ¸Ğ´Ğµ Ğ¾Ğ±Ğ»Ğ°ĞºĞ¾Ğ² Ñ‚Ğ¾Ñ‡ĞµĞº, Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğ½Ñ‹Ñ… ĞºĞ°Ñ€Ñ‚ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ»Ğ¸ Ñ€Ğ°Ğ½ĞµĞµ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ². ĞŸÑ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ² GEN3C Ğ¾Ğ¿Ğ¸Ñ€Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° 2D-Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³ 3D-ĞºÑÑˆĞ° Ñ Ğ½Ğ¾Ğ²Ğ¾Ğ¹ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ĞµĞ¹ ĞºĞ°Ğ¼ĞµÑ€Ñ‹, Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¼. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑÑ„Ğ¾ĞºÑƒÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ½Ğ° Ñ€Ğ°Ğ½ĞµĞµ Ğ½ĞµĞ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ĞµĞ¼Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ… Ğ¸ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ ÑÑ†ĞµĞ½Ñ‹, Ğ½Ğµ Ñ‚Ñ€Ğ°Ñ‚Ñ Ñ€ĞµÑÑƒÑ€ÑÑ‹ Ğ½Ğ° Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¸Ğ»Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸Ğ· Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ°Ğ¼ĞµÑ€Ñ‹.'}, 'en': {'title': 'GEN3C: Mastering Video Generation with 3D Precision and Camera Control', 'desc': 'GEN3C is a generative video model that enhances video generation by incorporating precise camera control and maintaining temporal 3D consistency. Unlike previous models that often lack 3D information, GEN3C utilizes a 3D cache of point clouds derived from depth predictions, allowing for more coherent object presence in videos. The model is conditioned on 2D renderings from this cache, enabling it to generate new frames without needing to remember past outputs or infer scene structure from camera angles. This approach results in superior camera control and state-of-the-art performance in generating novel views, particularly in complex scenarios like driving scenes.'}, 'zh': {'title': 'GEN3Cï¼šç²¾ç¡®ç›¸æœºæ§åˆ¶ä¸æ—¶é—´ä¸€è‡´æ€§çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹', 'desc': 'æˆ‘ä»¬æå‡ºäº†GEN3Cï¼Œè¿™æ˜¯ä¸€ç§å…·æœ‰ç²¾ç¡®ç›¸æœºæ§åˆ¶å’Œæ—¶é—´ä¸€è‡´æ€§çš„ç”Ÿæˆè§†é¢‘æ¨¡å‹ã€‚ä»¥å¾€çš„è§†é¢‘æ¨¡å‹è™½ç„¶èƒ½å¤Ÿç”Ÿæˆé€¼çœŸçš„è§†é¢‘ï¼Œä½†å¾€å¾€ç¼ºä¹3Dä¿¡æ¯ï¼Œå¯¼è‡´ç‰©ä½“å‡ºç°å’Œæ¶ˆå¤±çš„ä¸ä¸€è‡´æ€§ã€‚GEN3Cé€šè¿‡3Dç¼“å­˜æ¥æŒ‡å¯¼ç”Ÿæˆè¿‡ç¨‹ï¼Œåˆ©ç”¨ä»ç§å­å›¾åƒæˆ–å…ˆå‰ç”Ÿæˆå¸§ä¸­é¢„æµ‹çš„åƒç´ æ·±åº¦è·å¾—çš„ç‚¹äº‘ã€‚è¿™æ ·ï¼ŒGEN3Cèƒ½å¤Ÿåœ¨ç”¨æˆ·æä¾›çš„æ–°ç›¸æœºè½¨è¿¹ä¸‹ï¼Œä¸“æ³¨äºç”Ÿæˆæœªè§‚å¯Ÿåˆ°çš„åŒºåŸŸï¼Œå¹¶æœ‰æ•ˆæ¨è¿›åœºæ™¯çŠ¶æ€åˆ°ä¸‹ä¸€ä¸ªå¸§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.02951', 'title': 'KodCode: A Diverse, Challenging, and Verifiable Synthetic Dataset for Coding', 'url': 'https://huggingface.co/papers/2503.02951', 'abstract': 'We introduce KodCode, a synthetic dataset that addresses the persistent challenge of acquiring high-quality, verifiable training data across diverse difficulties and domains for training Large Language Models for coding. Existing code-focused resources typically fail to ensure either the breadth of coverage (e.g., spanning simple coding tasks to advanced algorithmic problems) or verifiable correctness (e.g., unit tests). In contrast, KodCode comprises question-solution-test triplets that are systematically validated via a self-verification procedure. Our pipeline begins by synthesizing a broad range of coding questions, then generates solutions and test cases with additional attempts allocated to challenging problems. Finally, post-training data synthesis is done by rewriting questions into diverse formats and generating responses under a test-based reject sampling procedure from a reasoning model (DeepSeek R1). This pipeline yields a large-scale, robust and diverse coding dataset. KodCode is suitable for supervised fine-tuning and the paired unit tests also provide great potential for RL tuning. Fine-tuning experiments on coding benchmarks (HumanEval(+), MBPP(+), BigCodeBench, and LiveCodeBench) demonstrate that KodCode-tuned models achieve state-of-the-art performance, surpassing models like Qwen2.5-Coder-32B-Instruct and DeepSeek-R1-Distill-Llama-70B.', 'score': 12, 'issue_id': 2555, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 4', 'zh': '3æœˆ4æ—¥'}, 'hash': '6c344ba0bf71ac84', 'authors': ['Zhangchen Xu', 'Yang Liu', 'Yueqin Yin', 'Mingyuan Zhou', 'Radha Poovendran'], 'affiliations': ['Microsoft', 'The University of Texas at Austin', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2503.02951.jpg', 'data': {'categories': ['#dataset', '#rl', '#optimization', '#synthetic', '#training'], 'emoji': 'ğŸ§‘\u200dğŸ’»', 'ru': {'title': 'KodCode: Ğ¡Ğ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ˜Ğ˜ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ', 'desc': 'KodCode - ÑÑ‚Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ½ ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ñ‚Ñ€Ğ¸Ğ¿Ğ»ĞµÑ‚Ğ¾Ğ² Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ-Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ-Ñ‚ĞµÑÑ‚, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´ÑÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ñƒ ÑĞ°Ğ¼Ğ¾Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸. ĞŸÑ€Ğ¾Ñ†ĞµÑÑ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ KodCode Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ÑĞ»ÑƒÑ‡Ğ°ĞµĞ², Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° KodCode, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'KodCode: Elevating Coding Models with Verified Data', 'desc': 'KodCode is a synthetic dataset designed to improve the training of Large Language Models (LLMs) for coding tasks by providing high-quality, verifiable data. It includes question-solution-test triplets that are validated through a self-verification process, ensuring both correctness and a wide range of coding difficulties. The dataset is generated using a systematic pipeline that synthesizes coding questions, creates solutions, and develops test cases, particularly focusing on challenging problems. Fine-tuning experiments show that models trained on KodCode outperform existing models on various coding benchmarks, demonstrating its effectiveness in enhancing LLM performance.'}, 'zh': {'title': 'KodCodeï¼šé«˜è´¨é‡ç¼–ç æ•°æ®é›†çš„è§£å†³æ–¹æ¡ˆ', 'desc': 'æˆ‘ä»¬ä»‹ç»äº†KodCodeï¼Œè¿™æ˜¯ä¸€ä¸ªåˆæˆæ•°æ®é›†ï¼Œæ—¨åœ¨è§£å†³è·å–é«˜è´¨é‡ã€å¯éªŒè¯çš„è®­ç»ƒæ•°æ®çš„æŒ‘æˆ˜ï¼Œä»¥è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œç¼–ç ã€‚ç°æœ‰çš„ä»£ç èµ„æºé€šå¸¸æ— æ³•ç¡®ä¿è¦†ç›–èŒƒå›´å¹¿æ³›æˆ–æ­£ç¡®æ€§å¯éªŒè¯ã€‚KodCodeç”±é—®é¢˜-è§£å†³æ–¹æ¡ˆ-æµ‹è¯•ä¸‰å…ƒç»„ç»„æˆï¼Œé€šè¿‡è‡ªæˆ‘éªŒè¯ç¨‹åºç³»ç»Ÿåœ°éªŒè¯ã€‚æˆ‘ä»¬çš„æµç¨‹åŒ…æ‹¬åˆæˆå„ç§ç¼–ç é—®é¢˜ï¼Œç”Ÿæˆè§£å†³æ–¹æ¡ˆå’Œæµ‹è¯•ç”¨ä¾‹ï¼Œå¹¶åœ¨åæœŸé€šè¿‡é‡å†™é—®é¢˜å’Œç”Ÿæˆå“åº”æ¥è¿›è¡Œæ•°æ®åˆæˆï¼Œæœ€ç»ˆç”Ÿæˆä¸€ä¸ªå¤§è§„æ¨¡ã€å¼ºå¤§ä¸”å¤šæ ·åŒ–çš„ç¼–ç æ•°æ®é›†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.03278', 'title': 'Enhancing Abnormality Grounding for Vision Language Models with Knowledge Descriptions', 'url': 'https://huggingface.co/papers/2503.03278', 'abstract': 'Visual Language Models (VLMs) have demonstrated impressive capabilities in visual grounding tasks. However, their effectiveness in the medical domain, particularly for abnormality detection and localization within medical images, remains underexplored. A major challenge is the complex and abstract nature of medical terminology, which makes it difficult to directly associate pathological anomaly terms with their corresponding visual features. In this work, we introduce a novel approach to enhance VLM performance in medical abnormality detection and localization by leveraging decomposed medical knowledge. Instead of directly prompting models to recognize specific abnormalities, we focus on breaking down medical concepts into fundamental attributes and common visual patterns. This strategy promotes a stronger alignment between textual descriptions and visual features, improving both the recognition and localization of abnormalities in medical images.We evaluate our method on the 0.23B Florence-2 base model and demonstrate that it achieves comparable performance in abnormality grounding to significantly larger 7B LLaVA-based medical VLMs, despite being trained on only 1.5% of the data used for such models. Experimental results also demonstrate the effectiveness of our approach in both known and previously unseen abnormalities, suggesting its strong generalization capabilities.', 'score': 10, 'issue_id': 2560, 'pub_date': '2025-03-05', 'pub_date_card': {'ru': '5 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 5', 'zh': '3æœˆ5æ—¥'}, 'hash': '6103dbe5d60b5f3f', 'authors': ['Jun Li', 'Che Liu', 'Wenjia Bai', 'Rossella Arcucci', 'Cosmin I. Bercea', 'Julia A. Schnabel'], 'affiliations': ['Helmholtz AI and Helmholtz Munich, Germany', 'Imperial College London, UK', 'Kings College London, UK', 'Munich Center for Machine Learning, Germany', 'Technical University of Munich, Germany'], 'pdf_title_img': 'assets/pdf/title_img/2503.03278.jpg', 'data': {'categories': ['#cv', '#multimodal', '#healthcare', '#alignment', '#transfer_learning'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'Ğ”ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ VLM Ğ² Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğµ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (VLM) Ğ² Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¸ Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹ Ğ½Ğ° Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ‚Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¹, Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¸ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹ Ğ½Ğ° Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ñ‹ Ğ¸ Ğ¾Ğ±Ñ‰Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹. Ğ­Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ²ÑĞ·ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ°Ğ¼Ğ¸, Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ±Ñ‹Ğ» Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Florence-2 Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ» Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ñ‹Ğµ Ñ Ğ³Ğ¾Ñ€Ğ°Ğ·Ğ´Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ğ¼Ğ¸ VLM, Ğ½ĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¼ĞµĞ½ÑŒÑˆĞµĞ³Ğ¾ Ğ¾Ğ±ÑŠĞµĞ¼Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Enhancing Medical VLMs through Decomposed Knowledge', 'desc': 'This paper presents a new method to improve Visual Language Models (VLMs) for detecting and locating abnormalities in medical images. The authors address the challenge of complex medical terminology by breaking down medical concepts into simpler attributes and common visual patterns. This approach enhances the alignment between text descriptions and visual features, leading to better performance in recognizing and localizing abnormalities. The proposed method shows competitive results with larger models while using significantly less training data, indicating its efficiency and strong generalization capabilities.'}, 'zh': {'title': 'åˆ†è§£åŒ»å­¦çŸ¥è¯†ï¼Œæå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„å¼‚å¸¸æ£€æµ‹èƒ½åŠ›', 'desc': 'è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨è§†è§‰å®šä½ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨åŒ»å­¦é¢†åŸŸï¼Œå°¤å…¶æ˜¯åŒ»å­¦å›¾åƒä¸­çš„å¼‚å¸¸æ£€æµ‹å’Œå®šä½æ–¹é¢ï¼Œä»ç„¶ç¼ºä¹ç ”ç©¶ã€‚åŒ»å­¦æœ¯è¯­çš„å¤æ‚æ€§ä½¿å¾—å°†ç—…ç†å¼‚å¸¸æœ¯è¯­ä¸ç›¸åº”çš„è§†è§‰ç‰¹å¾ç›´æ¥å…³è”å˜å¾—å›°éš¾ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œé€šè¿‡åˆ†è§£åŒ»å­¦çŸ¥è¯†æ¥å¢å¼ºVLMåœ¨åŒ»å­¦å¼‚å¸¸æ£€æµ‹å’Œå®šä½ä¸­çš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•é€šè¿‡å°†åŒ»å­¦æ¦‚å¿µåˆ†è§£ä¸ºåŸºæœ¬å±æ€§å’Œå¸¸è§è§†è§‰æ¨¡å¼ï¼Œä¿ƒè¿›äº†æ–‡æœ¬æè¿°ä¸è§†è§‰ç‰¹å¾ä¹‹é—´çš„æ›´å¼ºå¯¹é½ï¼Œä»è€Œæé«˜äº†åŒ»å­¦å›¾åƒä¸­å¼‚å¸¸çš„è¯†åˆ«å’Œå®šä½èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.01836', 'title': 'CrowdSelect: Synthetic Instruction Data Selection with Multi-LLM Wisdom', 'url': 'https://huggingface.co/papers/2503.01836', 'abstract': "Distilling advanced Large Language Models' instruction-following capabilities into smaller models using a selected subset has become a mainstream approach in model training. While existing synthetic instruction data selection strategies rely mainly on single-dimensional signals (i.e., reward scores, model perplexity), they fail to capture the complexity of instruction-following across diverse fields. Therefore, we investigate more diverse signals to capture comprehensive instruction-response pair characteristics and propose three foundational metrics that leverage Multi-LLM wisdom, informed by (1) diverse LLM responses and (2) reward model assessment. Building upon base metrics, we propose CrowdSelect, an integrated metric incorporating a clustering-based approach to maintain response diversity. Our comprehensive experiments demonstrate that our foundation metrics consistently improve performance across 4 base models on MT-bench and Arena-Hard. CrowdSelect, efficiently incorporating all metrics, achieves state-of-the-art performance in both Full and LoRA fine-tuning, showing improvements of 4.81% on Arena-Hard and 11.1% on MT-bench with Llama-3.2-3b-instruct. We hope our findings will bring valuable insights for future research in this direction. Code are available at https://github.com/listentm/crowdselect.", 'score': 9, 'issue_id': 2560, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 3', 'zh': '3æœˆ3æ—¥'}, 'hash': 'd59d65fb3b60c043', 'authors': ['Yisen Li', 'Lingfeng Yang', 'Wenxuan Shen', 'Pan Zhou', 'Yao Wan', 'Weiwei Lin', 'Dongping Chen'], 'affiliations': ['Huazhong University of Science and Technology', 'South China University of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2503.01836.jpg', 'data': {'categories': ['#small_models', '#training', '#synthetic', '#optimization'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'CrowdSelect: ÑƒĞ¼Ğ½Ñ‹Ğ¹ Ğ¾Ñ‚Ğ±Ğ¾Ñ€ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ° Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ CrowdSelect. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚Ñ€Ğ¸ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ¾Ñ†ĞµĞ½ĞºĞ°Ñ… Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸x ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ. CrowdSelect Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… MT-bench Ğ¸ Arena-Hard. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ½Ğ°Ğ´ĞµÑÑ‚ÑÑ, Ñ‡Ñ‚Ğ¾ Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ½ĞµÑĞµÑ‚ Ğ²ĞºĞ»Ğ°Ğ´ Ğ² Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Enhancing Model Training with Diverse Instruction Metrics', 'desc': 'This paper focuses on improving the training of smaller models by distilling the instruction-following abilities of larger language models (LLMs). It critiques existing methods that use simple metrics for selecting synthetic instruction data, which do not adequately reflect the complexity of instruction-following tasks. The authors propose new metrics that utilize diverse responses from multiple LLMs and a reward model to better assess instruction-response pairs. Their method, CrowdSelect, combines these metrics with a clustering approach to enhance response diversity, leading to significant performance improvements in various model evaluations.'}, 'zh': {'title': 'æå‡å°æ¨¡å‹çš„æŒ‡ä»¤è·Ÿéšèƒ½åŠ›', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†å¦‚ä½•å°†å¤§å‹è¯­è¨€æ¨¡å‹çš„æŒ‡ä»¤è·Ÿéšèƒ½åŠ›æç‚¼åˆ°æ›´å°çš„æ¨¡å‹ä¸­ã€‚ç°æœ‰çš„åˆæˆæŒ‡ä»¤æ•°æ®é€‰æ‹©ç­–ç•¥ä¸»è¦ä¾èµ–å•ä¸€ç»´åº¦çš„ä¿¡å·ï¼Œæœªèƒ½å…¨é¢æ•æ‰æŒ‡ä»¤è·Ÿéšçš„å¤æ‚æ€§ã€‚æˆ‘ä»¬æå‡ºäº†ä¸‰ç§åŸºç¡€æŒ‡æ ‡ï¼Œåˆ©ç”¨å¤šç§å¤§å‹è¯­è¨€æ¨¡å‹çš„æ™ºæ…§ï¼Œç»“åˆå¤šæ ·çš„å“åº”å’Œå¥–åŠ±æ¨¡å‹è¯„ä¼°ã€‚é€šè¿‡ç»¼åˆå®éªŒï¼Œæˆ‘ä»¬çš„CrowdSelectæŒ‡æ ‡åœ¨å¤šä¸ªåŸºæ¨¡å‹ä¸Šè¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—æå‡äº†æ€§èƒ½ï¼Œå±•ç¤ºäº†æœªæ¥ç ”ç©¶çš„æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.01933', 'title': 'Fine-Tuning Small Language Models for Domain-Specific AI: An Edge AI Perspective', 'url': 'https://huggingface.co/papers/2503.01933', 'abstract': 'Deploying large scale language models on edge devices faces inherent challenges such as high computational demands, energy consumption, and potential data privacy risks. This paper introduces the Shakti Small Language Models (SLMs) Shakti-100M, Shakti-250M, and Shakti-500M which target these constraints headon. By combining efficient architectures, quantization techniques, and responsible AI principles, the Shakti series enables on-device intelligence for smartphones, smart appliances, IoT systems, and beyond. We provide comprehensive insights into their design philosophy, training pipelines, and benchmark performance on both general tasks (e.g., MMLU, Hellaswag) and specialized domains (healthcare, finance, and legal). Our findings illustrate that compact models, when carefully engineered and fine-tuned, can meet and often exceed expectations in real-world edge-AI scenarios.', 'score': 6, 'issue_id': 2563, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 3', 'zh': '3æœˆ3æ—¥'}, 'hash': '6fa74210552cc49f', 'authors': ['Rakshit Aralimatti', 'Syed Abdul Gaffar Shakhadri', 'Kruthika KR', 'Kartik Basavaraj Angadi'], 'affiliations': ['SandLogic Technologies Pvt Ltd'], 'pdf_title_img': 'assets/pdf/title_img/2503.01933.jpg', 'data': {'categories': ['#healthcare', '#benchmark', '#training', '#ethics', '#inference', '#small_models', '#optimization'], 'emoji': 'ğŸ“±', 'ru': {'title': 'ĞœĞ°Ğ»Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ½Ğ° ĞºÑ€Ğ°Ñ ÑĞµÑ‚Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞµÑ€Ğ¸Ñ Ğ¼Ğ°Ğ»Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Shakti, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¿ĞµÑ€Ğ¸Ñ„ĞµÑ€Ğ¸Ğ¹Ğ½Ñ‹Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ…. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Shakti-100M, Shakti-250M Ğ¸ Shakti-500M Ñ€ĞµÑˆĞ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹, ÑĞ½ĞµÑ€Ğ³Ğ¾Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ¸ÑĞºĞ¾Ğ² ĞºĞ¾Ğ½Ñ„Ğ¸Ğ´ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹, Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ñ‹ Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜, ÑĞµÑ€Ğ¸Ñ Shakti Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ Ğ´Ğ»Ñ ÑĞ¼Ğ°Ñ€Ñ‚Ñ„Ğ¾Ğ½Ğ¾Ğ², ÑƒĞ¼Ğ½Ñ‹Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ² Ğ¸ IoT-ÑĞ¸ÑÑ‚ĞµĞ¼. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¿Ñ€Ğ¸ Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ¸ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞµ, Ğ¼Ğ¾Ğ³ÑƒÑ‚ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ¾Ğ¶Ğ¸Ğ´Ğ°Ğ½Ğ¸Ñ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ğ¿ĞµÑ€Ğ¸Ñ„ĞµÑ€Ğ¸Ğ¹Ğ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜.'}, 'en': {'title': 'Empowering Edge Devices with Efficient Language Models', 'desc': 'This paper presents the Shakti Small Language Models (SLMs) designed to operate efficiently on edge devices while addressing challenges like high computational needs and energy consumption. The Shakti models, including Shakti-100M, Shakti-250M, and Shakti-500M, utilize advanced architectures and quantization techniques to optimize performance without compromising data privacy. The authors detail the design philosophy, training processes, and benchmark results across various tasks and specialized fields such as healthcare and finance. The results demonstrate that well-engineered compact models can perform effectively in real-world applications, showcasing the potential of on-device AI.'}, 'zh': {'title': 'å°å‹è¯­è¨€æ¨¡å‹ï¼Œæ™ºèƒ½è¾¹ç¼˜è®¡ç®—çš„æœªæ¥', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†Shaktiå°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMsï¼‰ï¼ŒåŒ…æ‹¬Shakti-100Mã€Shakti-250Må’ŒShakti-500Mï¼Œæ—¨åœ¨è§£å†³åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²å¤§å‹è¯­è¨€æ¨¡å‹æ—¶é¢ä¸´çš„é«˜è®¡ç®—éœ€æ±‚å’Œèƒ½è€—é—®é¢˜ã€‚é€šè¿‡ç»“åˆé«˜æ•ˆæ¶æ„ã€é‡åŒ–æŠ€æœ¯å’Œè´Ÿè´£ä»»çš„äººå·¥æ™ºèƒ½åŸåˆ™ï¼ŒShaktiç³»åˆ—å®ç°äº†æ™ºèƒ½æ‰‹æœºã€æ™ºèƒ½å®¶ç”µå’Œç‰©è”ç½‘ç³»ç»Ÿçš„æœ¬åœ°æ™ºèƒ½ã€‚æˆ‘ä»¬æä¾›äº†å…³äºå…¶è®¾è®¡ç†å¿µã€è®­ç»ƒæµç¨‹å’Œåœ¨ä¸€èˆ¬ä»»åŠ¡ï¼ˆå¦‚MMLUã€Hellaswagï¼‰åŠä¸“ä¸šé¢†åŸŸï¼ˆåŒ»ç–—ã€é‡‘èå’Œæ³•å¾‹ï¼‰ä¸Šçš„åŸºå‡†æ€§èƒ½çš„å…¨é¢è§è§£ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œç»è¿‡ç²¾å¿ƒè®¾è®¡å’Œå¾®è°ƒçš„ç´§å‡‘æ¨¡å‹èƒ½å¤Ÿåœ¨å®é™…è¾¹ç¼˜äººå·¥æ™ºèƒ½åœºæ™¯ä¸­æ»¡è¶³å¹¶è¶…è¶Šé¢„æœŸã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.20317', 'title': 'Mixture of Structural-and-Textual Retrieval over Text-rich Graph Knowledge Bases', 'url': 'https://huggingface.co/papers/2502.20317', 'abstract': 'Text-rich Graph Knowledge Bases (TG-KBs) have become increasingly crucial for answering queries by providing textual and structural knowledge. However, current retrieval methods often retrieve these two types of knowledge in isolation without considering their mutual reinforcement and some hybrid methods even bypass structural retrieval entirely after neighboring aggregation. To fill in this gap, we propose a Mixture of Structural-and-Textual Retrieval (MoR) to retrieve these two types of knowledge via a Planning-Reasoning-Organizing framework. In the Planning stage, MoR generates textual planning graphs delineating the logic for answering queries. Following planning graphs, in the Reasoning stage, MoR interweaves structural traversal and textual matching to obtain candidates from TG-KBs. In the Organizing stage, MoR further reranks fetched candidates based on their structural trajectory. Extensive experiments demonstrate the superiority of MoR in harmonizing structural and textual retrieval with insights, including uneven retrieving performance across different query logics and the benefits of integrating structural trajectories for candidate reranking. Our code is available at https://github.com/Yoega/MoR.', 'score': 6, 'issue_id': 2561, 'pub_date': '2025-02-27', 'pub_date_card': {'ru': '27 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 27', 'zh': '2æœˆ27æ—¥'}, 'hash': '686b2ff85600f281', 'authors': ['Yongjia Lei', 'Haoyu Han', 'Ryan A. Rossi', 'Franck Dernoncourt', 'Nedim Lipka', 'Mahantesh M Halappanavar', 'Jiliang Tang', 'Yu Wang'], 'affiliations': ['Adobe Research', 'Michigan State University', 'Pacific Northwest National Laboratory', 'University of Oregon'], 'pdf_title_img': 'assets/pdf/title_img/2502.20317.jpg', 'data': {'categories': ['#benchmark', '#data', '#graphs', '#dataset', '#multimodal', '#reasoning'], 'emoji': 'ğŸ•¸ï¸', 'ru': {'title': 'Ğ“Ğ°Ñ€Ğ¼Ğ¾Ğ½Ğ¸Ñ‡Ğ½Ğ¾Ğµ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ² Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ²Ñ‹Ñ… Ğ±Ğ°Ğ·Ğ°Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ MoR (Mixture of Structural-and-Textual Retrieval) Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ±Ğ°Ğ·Ğ°Ğ¼Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¼Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ. MoR Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚Ñ€ĞµÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´: Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½Ğ¸Ñ‚ÑŒ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº. ĞœĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ³Ñ€Ğ°Ñ„Ñ‹ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¿ĞµÑ€ĞµĞ¿Ğ»ĞµÑ‚Ğ°ĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğ¹ Ğ¾Ğ±Ñ…Ğ¾Ğ´ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğµ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ, Ğ¸ Ğ½Ğ°ĞºĞ¾Ğ½ĞµÑ†, Ğ¿ĞµÑ€ĞµÑ€Ğ°Ğ½Ğ¶Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ MoR Ğ² Ğ³Ğ°Ñ€Ğ¼Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°.'}, 'en': {'title': 'Harmonizing Text and Structure for Better Knowledge Retrieval', 'desc': 'This paper introduces a new method called Mixture of Structural-and-Textual Retrieval (MoR) for improving the retrieval of knowledge from Text-rich Graph Knowledge Bases (TG-KBs). Unlike traditional methods that treat textual and structural knowledge separately, MoR integrates both types through a three-stage framework: Planning, Reasoning, and Organizing. In the Planning stage, it creates graphs that outline the logic for query responses, while the Reasoning stage combines structural traversal with textual matching to gather relevant candidates. Finally, the Organizing stage enhances the selection process by reranking candidates based on their structural paths, leading to better retrieval performance across various query types.'}, 'zh': {'title': 'ç»“æ„ä¸æ–‡æœ¬çŸ¥è¯†çš„å®Œç¾ç»“åˆ', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ··åˆæ£€ç´¢æ–¹æ³•ï¼Œç§°ä¸ºç»“æ„ä¸æ–‡æœ¬æ£€ç´¢çš„æ··åˆä½“ï¼ˆMoRï¼‰ï¼Œæ—¨åœ¨åŒæ—¶åˆ©ç”¨æ–‡æœ¬å’Œç»“æ„çŸ¥è¯†æ¥å›ç­”æŸ¥è¯¢ã€‚MoRé€šè¿‡è§„åˆ’-æ¨ç†-ç»„ç»‡çš„æ¡†æ¶æ¥å®ç°è¿™ä¸€ç›®æ ‡ï¼Œåœ¨è§„åˆ’é˜¶æ®µç”Ÿæˆæ–‡æœ¬è§„åˆ’å›¾ï¼Œæ˜ç¡®å›ç­”æŸ¥è¯¢çš„é€»è¾‘ã€‚æ¥ç€ï¼Œåœ¨æ¨ç†é˜¶æ®µï¼ŒMoRç»“åˆç»“æ„éå†å’Œæ–‡æœ¬åŒ¹é…ï¼Œä»æ–‡æœ¬ä¸°å¯Œçš„å›¾çŸ¥è¯†åº“ä¸­è·å–å€™é€‰ç­”æ¡ˆã€‚æœ€åï¼Œåœ¨ç»„ç»‡é˜¶æ®µï¼ŒMoRæ ¹æ®ç»“æ„è½¨è¿¹å¯¹è·å–çš„å€™é€‰ç­”æ¡ˆè¿›è¡Œé‡æ–°æ’åºï¼Œå®éªŒç»“æœè¡¨æ˜è¯¥æ–¹æ³•åœ¨ç»“æ„å’Œæ–‡æœ¬æ£€ç´¢çš„åè°ƒæ€§ä¸Šå…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.03044', 'title': 'QE4PE: Word-level Quality Estimation for Human Post-Editing', 'url': 'https://huggingface.co/papers/2503.03044', 'abstract': "Word-level quality estimation (QE) detects erroneous spans in machine translations, which can direct and facilitate human post-editing. While the accuracy of word-level QE systems has been assessed extensively, their usability and downstream influence on the speed, quality and editing choices of human post-editing remain understudied. Our QE4PE study investigates the impact of word-level QE on machine translation (MT) post-editing in a realistic setting involving 42 professional post-editors across two translation directions. We compare four error-span highlight modalities, including supervised and uncertainty-based word-level QE methods, for identifying potential errors in the outputs of a state-of-the-art neural MT model. Post-editing effort and productivity are estimated by behavioral logs, while quality improvements are assessed by word- and segment-level human annotation. We find that domain, language and editors' speed are critical factors in determining highlights' effectiveness, with modest differences between human-made and automated QE highlights underlining a gap between accuracy and usability in professional workflows.", 'score': 5, 'issue_id': 2560, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 4', 'zh': '3æœˆ4æ—¥'}, 'hash': 'e4d3d7db506b6e1c', 'authors': ['Gabriele Sarti', 'VilÃ©m Zouhar', 'Grzegorz ChrupaÅ‚a', 'Ana Guerberof-Arenas', 'Malvina Nissim', 'Arianna Bisazza'], 'affiliations': ['CLCG, University of Groningen', 'CSAI, Tilburg University', 'ETH ZÃ¼rich'], 'pdf_title_img': 'assets/pdf/title_img/2503.03044.jpg', 'data': {'categories': ['#machine_translation', '#data', '#multilingual', '#healthcare'], 'emoji': 'ğŸ”', 'ru': {'title': 'ĞÑ†ĞµĞ½ĞºĞ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ°: Ğ¼Ğ¾ÑÑ‚ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑĞ»Ğ¾Ğ² (word-level QE) Ğ½Ğ° Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¿Ğ¾ÑÑ‚Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ°. Ğ’ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ ÑƒÑ‡Ğ°ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ»Ğ¸ 42 Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¾Ñ€Ğ°, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ²ÑˆĞ¸Ñ… Ñ Ğ´Ğ²ÑƒĞ¼Ñ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ°. Ğ¡Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ»Ğ¸ÑÑŒ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ´ÑĞ²ĞµÑ‚ĞºĞ¸ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ Ğ¸ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ´ÑĞ²ĞµÑ‚ĞºĞ¸ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ñ‚ Ğ¾Ñ‚ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°, ÑĞ·Ñ‹ĞºĞ° Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¾Ğ², Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ñ€Ğ°Ğ·Ğ½Ğ¸Ñ†Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ QE Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ°ÑÑŒ Ğ½ĞµĞ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹.'}, 'en': {'title': 'Enhancing Post-Editing Efficiency with Word-Level Quality Estimation', 'desc': 'This paper explores how word-level quality estimation (QE) can help improve the efficiency of human post-editing in machine translation (MT). It examines the effectiveness of different methods for highlighting potential translation errors, comparing supervised and uncertainty-based approaches. The study involves 42 professional post-editors and assesses their editing speed and quality improvements through detailed behavioral logs and human annotations. The findings reveal that factors like domain, language, and editor speed significantly influence the effectiveness of error highlights, indicating a need to bridge the gap between the accuracy of QE systems and their practical usability in real-world editing tasks.'}, 'zh': {'title': 'æå‡æœºå™¨ç¿»è¯‘åç¼–è¾‘æ•ˆç‡çš„å…³é”®', 'desc': 'æœ¬æ–‡ç ”ç©¶äº†è¯çº§è´¨é‡è¯„ä¼°ï¼ˆQEï¼‰åœ¨æœºå™¨ç¿»è¯‘åç¼–è¾‘ä¸­çš„å½±å“ã€‚æˆ‘ä»¬åˆ†æäº†42åä¸“ä¸šåç¼–è¾‘åœ¨ä¸¤ç§ç¿»è¯‘æ–¹å‘ä¸‹çš„è¡¨ç°ï¼Œæ¯”è¾ƒäº†å››ç§é”™è¯¯èŒƒå›´é«˜äº®æ–¹å¼ï¼ŒåŒ…æ‹¬ç›‘ç£å’ŒåŸºäºä¸ç¡®å®šæ€§çš„è¯çº§QEæ–¹æ³•ã€‚ç ”ç©¶å‘ç°ï¼Œé¢†åŸŸã€è¯­è¨€å’Œç¼–è¾‘é€Ÿåº¦æ˜¯å½±å“é«˜äº®æ•ˆæœçš„å…³é”®å› ç´ ã€‚ç»“æœè¡¨æ˜ï¼Œäººå·¥å’Œè‡ªåŠ¨QEé«˜äº®ä¹‹é—´å­˜åœ¨é€‚åº¦å·®å¼‚ï¼Œçªæ˜¾äº†ä¸“ä¸šå·¥ä½œæµç¨‹ä¸­å‡†ç¡®æ€§ä¸å¯ç”¨æ€§ä¹‹é—´çš„å·®è·ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.00307', 'title': 'Remasking Discrete Diffusion Models with Inference-Time Scaling', 'url': 'https://huggingface.co/papers/2503.00307', 'abstract': 'Part of the success of diffusion models stems from their ability to perform iterative refinement, i.e., repeatedly correcting outputs during generation. However, modern masked discrete diffusion lacks this capability: when a token is generated, it cannot be updated again, even when it introduces an error. Here, we address this limitation by introducing the remasking diffusion model (ReMDM) sampler, a method that can be applied to pretrained masked diffusion models in a principled way and that is derived from a discrete diffusion model with a custom remasking backward process. Most interestingly, ReMDM endows discrete diffusion with a form of inference-time compute scaling. By increasing the number of sampling steps, ReMDM generates natural language outputs that approach the quality of autoregressive models, whereas when the computation budget is limited, ReMDM better maintains quality. ReMDM also improves sample quality of masked diffusion models for discretized images, and in scientific domains such as molecule design, ReMDM facilitates diffusion guidance and pushes the Pareto frontier of controllability relative to classical masking and uniform noise diffusion. We provide the code along with a blog post on the project page: https://remdm.github.io.', 'score': 4, 'issue_id': 2572, 'pub_date': '2025-03-01', 'pub_date_card': {'ru': '1 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 1', 'zh': '3æœˆ1æ—¥'}, 'hash': '7f31677f2cb675e1', 'authors': ['Guanghan Wang', 'Yair Schiff', 'Subham Sekhar Sahoo', 'Volodymyr Kuleshov'], 'affiliations': ['Department of Computer Science, Cornell Unversity'], 'pdf_title_img': 'assets/pdf/title_img/2503.00307.jpg', 'data': {'categories': ['#open_source', '#inference', '#diffusion', '#science', '#multimodal'], 'emoji': 'ğŸ”„', 'ru': {'title': 'ĞŸĞµÑ€ĞµĞ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ReMDM (remasking diffusion model), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ReMDM Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒÑ‚Ğ¾Ñ‡Ğ½ÑÑ‚ÑŒ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹, Ñ‡Ñ‚Ğ¾ Ñ€Ğ°Ğ½ĞµĞµ Ğ±Ñ‹Ğ»Ğ¾ Ğ½ĞµĞ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ² ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼Ğ°ÑĞ¾Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°, Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ¶Ğ°Ñ ĞµĞ³Ğ¾ Ğº ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ¿Ñ€Ğ¸ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¸ Ñ‡Ğ¸ÑĞ»Ğ° ÑˆĞ°Ğ³Ğ¾Ğ² ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ReMDM Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğµ Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ».'}, 'en': {'title': 'Enhancing Masked Diffusion with Iterative Refinement', 'desc': 'This paper introduces the remasking diffusion model (ReMDM) sampler, which enhances the capabilities of masked discrete diffusion models by allowing iterative refinement during output generation. Unlike traditional methods where generated tokens cannot be updated, ReMDM enables the correction of errors by applying a remasking backward process. This approach not only improves the quality of natural language outputs to rival autoregressive models but also maintains high quality under limited computational resources. Additionally, ReMDM enhances the performance of masked diffusion models in generating discretized images and aids in molecule design, pushing the boundaries of controllability in scientific applications.'}, 'zh': {'title': 'é‡æ©è”½æ‰©æ•£æ¨¡å‹ï¼šæå‡ç”Ÿæˆè´¨é‡çš„æ–°æ–¹æ³•', 'desc': 'æ‰©æ•£æ¨¡å‹çš„æˆåŠŸéƒ¨åˆ†æºäºå…¶è¿­ä»£ç²¾ç‚¼çš„èƒ½åŠ›ï¼Œå³åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ä¸æ–­ä¿®æ­£è¾“å‡ºã€‚ç„¶è€Œï¼Œç°ä»£çš„æ©è”½ç¦»æ•£æ‰©æ•£æ¨¡å‹ç¼ºä¹è¿™ç§èƒ½åŠ›ï¼šä¸€æ—¦ç”Ÿæˆä¸€ä¸ªæ ‡è®°ï¼Œå°±æ— æ³•å†æ¬¡æ›´æ–°ï¼Œå³ä½¿å®ƒå¼•å…¥äº†é”™è¯¯ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†é‡æ©è”½æ‰©æ•£æ¨¡å‹ï¼ˆReMDMï¼‰é‡‡æ ·å™¨ï¼Œè¿™æ˜¯ä¸€ç§å¯ä»¥ä»¥åŸåˆ™æ€§æ–¹å¼åº”ç”¨äºé¢„è®­ç»ƒæ©è”½æ‰©æ•£æ¨¡å‹çš„æ–¹æ³•ã€‚ReMDMé€šè¿‡å¢åŠ é‡‡æ ·æ­¥éª¤ï¼Œç”Ÿæˆçš„è‡ªç„¶è¯­è¨€è¾“å‡ºè´¨é‡æ¥è¿‘è‡ªå›å½’æ¨¡å‹ï¼ŒåŒæ—¶åœ¨è®¡ç®—é¢„ç®—æœ‰é™æ—¶ï¼ŒReMDMæ›´å¥½åœ°ä¿æŒè´¨é‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.18860', 'title': 'Exploring Rewriting Approaches for Different Conversational Tasks', 'url': 'https://huggingface.co/papers/2502.18860', 'abstract': "Conversational assistants often require a question rewriting algorithm that leverages a subset of past interactions to provide a more meaningful (accurate) answer to the user's question or request. However, the exact rewriting approach may often depend on the use case and application-specific tasks supported by the conversational assistant, among other constraints. In this paper, we systematically investigate two different approaches, denoted as rewriting and fusion, on two fundamentally different generation tasks, including a text-to-text generation task and a multimodal generative task that takes as input text and generates a visualization or data table that answers the user's question. Our results indicate that the specific rewriting or fusion approach highly depends on the underlying use case and generative task. In particular, we find that for a conversational question-answering assistant, the query rewriting approach performs best, whereas for a data analysis assistant that generates visualizations and data tables based on the user's conversation with the assistant, the fusion approach works best. Notably, we explore two datasets for the data analysis assistant use case, for short and long conversations, and we find that query fusion always performs better, whereas for the conversational text-based question-answering, the query rewrite approach performs best.", 'score': 4, 'issue_id': 2561, 'pub_date': '2025-02-26', 'pub_date_card': {'ru': '26 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 26', 'zh': '2æœˆ26æ—¥'}, 'hash': '1f018cc4f38149bc', 'authors': ['Md Mehrab Tanjim', 'Ryan A. Rossi', 'Mike Rimer', 'Xiang Chen', 'Sungchul Kim', 'Vaishnavi Muppala', 'Tong Yu', 'Zhengmian Hu', 'Ritwik Sinha', 'Wei Zhang', 'Iftikhar Ahamath Burhanuddin', 'Franck Dernoncourt'], 'affiliations': ['Adobe Research'], 'pdf_title_img': 'assets/pdf/title_img/2502.18860.jpg', 'data': {'categories': ['#dataset', '#multimodal'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞŸĞµÑ€ĞµĞ¿Ğ¸ÑÑ‹Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ² Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ½Ñ‹Ñ… Ğ˜Ğ˜: Ğ¾Ğ´Ğ¸Ğ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ½Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ğ»Ñ Ğ²ÑĞµÑ… Ğ·Ğ°Ğ´Ğ°Ñ‡', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ÑÑ Ğ´Ğ²Ğ° Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğº Ğ¿ĞµÑ€ĞµĞ¿Ğ¸ÑÑ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ² Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ½Ñ‹Ñ… Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ°Ñ…: Ğ¿ĞµÑ€ĞµĞ¿Ğ¸ÑÑ‹Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ğ»Ğ¸ÑÑŒ Ğ½Ğ° Ğ´Ğ²ÑƒÑ… Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸: Ñ‚ĞµĞºÑÑ‚-Ğ²-Ñ‚ĞµĞºÑÑ‚ Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ñ‚ Ğ¾Ñ‚ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ ÑĞ»ÑƒÑ‡Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. Ğ”Ğ»Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ° Ğ»ÑƒÑ‡ÑˆĞµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ¿Ğ¸ÑÑ‹Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ², Ğ° Ğ´Ğ»Ñ Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ° Ğ¿Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… - ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ².'}, 'en': {'title': 'Tailoring Response Strategies for Conversational Assistants', 'desc': 'This paper explores how conversational assistants can improve their responses by using two methods: rewriting and fusion. The rewriting method modifies user questions to enhance accuracy, while the fusion method combines information from past interactions to generate answers. The study shows that the effectiveness of these methods varies based on the task; rewriting is better for text-based question answering, while fusion excels in generating visualizations from data analysis tasks. The findings highlight the importance of tailoring the approach to the specific use case for optimal performance.'}, 'zh': {'title': 'å¯¹è¯åŠ©æ‰‹ä¸­çš„é—®é¢˜é‡å†™ä¸èåˆæ–¹æ³•çš„æœ€ä½³é€‰æ‹©', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†å¯¹è¯åŠ©æ‰‹ä¸­é—®é¢˜é‡å†™ç®—æ³•çš„ä¸¤ç§ä¸åŒæ–¹æ³•ï¼šé‡å†™å’Œèåˆã€‚è¿™ä¸¤ç§æ–¹æ³•åœ¨æ–‡æœ¬ç”Ÿæˆå’Œå¤šæ¨¡æ€ç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°ä¸åŒï¼Œå…·ä½“å–å†³äºåº”ç”¨åœºæ™¯ã€‚ç ”ç©¶å‘ç°ï¼Œå¯¹äºå¯¹è¯é—®ç­”åŠ©æ‰‹ï¼ŒæŸ¥è¯¢é‡å†™æ–¹æ³•æ•ˆæœæœ€ä½³ï¼›è€Œå¯¹äºç”Ÿæˆå¯è§†åŒ–å’Œæ•°æ®è¡¨çš„æ•°æ®åˆ†æåŠ©æ‰‹ï¼ŒæŸ¥è¯¢èåˆæ–¹æ³•æ›´ä¸ºæœ‰æ•ˆã€‚æˆ‘ä»¬è¿˜åˆ†æäº†çŸ­å¯¹è¯å’Œé•¿å¯¹è¯çš„æ•°æ®é›†ï¼Œç»“æœè¡¨æ˜æŸ¥è¯¢èåˆåœ¨æ•°æ®åˆ†æä»»åŠ¡ä¸­å§‹ç»ˆè¡¨ç°æ›´å¥½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.01763', 'title': "Retrieval Models Aren't Tool-Savvy: Benchmarking Tool Retrieval for Large Language Models", 'url': 'https://huggingface.co/papers/2503.01763', 'abstract': 'Tool learning aims to augment large language models (LLMs) with diverse tools, enabling them to act as agents for solving practical tasks. Due to the limited context length of tool-using LLMs, adopting information retrieval (IR) models to select useful tools from large toolsets is a critical initial step. However, the performance of IR models in tool retrieval tasks remains underexplored and unclear. Most tool-use benchmarks simplify this step by manually pre-annotating a small set of relevant tools for each task, which is far from the real-world scenarios. In this paper, we propose ToolRet, a heterogeneous tool retrieval benchmark comprising 7.6k diverse retrieval tasks, and a corpus of 43k tools, collected from existing datasets. We benchmark six types of models on ToolRet. Surprisingly, even the models with strong performance in conventional IR benchmarks, exhibit poor performance on ToolRet. This low retrieval quality degrades the task pass rate of tool-use LLMs. As a further step, we contribute a large-scale training dataset with over 200k instances, which substantially optimizes the tool retrieval ability of IR models.', 'score': 4, 'issue_id': 2558, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 3', 'zh': '3æœˆ3æ—¥'}, 'hash': 'e6a23582f741dc5b', 'authors': ['Zhengliang Shi', 'Yuhan Wang', 'Lingyong Yan', 'Pengjie Ren', 'Shuaiqiang Wang', 'Dawei Yin', 'Zhaochun Ren'], 'affiliations': ['Baidu Inc., Beijing, China', 'Leiden University, Leiden, The Netherlands', 'Shandong University, Qingdao, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.01763.jpg', 'data': {'categories': ['#training', '#optimization', '#benchmark', '#dataset', '#data'], 'emoji': 'ğŸ”', 'ru': {'title': 'ToolRet: ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ²Ñ‹Ğ·Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ˜Ğ˜', 'desc': 'ToolRet - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 7,6 Ñ‚Ñ‹ÑÑÑ‡ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸ ĞºĞ¾Ñ€Ğ¿ÑƒÑ Ğ¸Ğ· 43 Ñ‚Ñ‹ÑÑÑ‡ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ² Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¿Ğ»Ğ¾Ñ…Ğ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ ToolRet. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 200 Ñ‚Ñ‹ÑÑÑ‡ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ¿Ğ¾Ğ¸ÑĞºÑƒ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Enhancing Tool Retrieval for Language Models with ToolRet', 'desc': 'This paper introduces ToolRet, a benchmark designed to evaluate the effectiveness of information retrieval (IR) models in selecting tools for large language models (LLMs) in practical tasks. The authors highlight that existing benchmarks often rely on a limited set of pre-annotated tools, which does not reflect real-world complexities. Their findings reveal that even high-performing IR models struggle with tool retrieval in this new context, leading to lower task success rates for LLMs. To address this issue, they provide a large-scale training dataset that significantly enhances the tool retrieval capabilities of IR models.'}, 'zh': {'title': 'å·¥å…·æ£€ç´¢ï¼šæå‡LLMsçš„å®ç”¨èƒ½åŠ›', 'desc': 'æœ¬æ–‡æ¢è®¨äº†å·¥å…·å­¦ä¹ å¦‚ä½•å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿä½œä¸ºä»£ç†è§£å†³å®é™…ä»»åŠ¡ã€‚ç”±äºå·¥å…·ä½¿ç”¨çš„LLMså…·æœ‰æœ‰é™çš„ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œå› æ­¤é‡‡ç”¨ä¿¡æ¯æ£€ç´¢ï¼ˆIRï¼‰æ¨¡å‹ä»å¤§é‡å·¥å…·é›†ä¸­é€‰æ‹©æœ‰ç”¨å·¥å…·æ˜¯å…³é”®çš„åˆæ­¥æ­¥éª¤ã€‚æˆ‘ä»¬æå‡ºäº†ToolRetï¼Œä¸€ä¸ªåŒ…å«7.6kå¤šæ ·åŒ–æ£€ç´¢ä»»åŠ¡å’Œ43kå·¥å…·çš„å¼‚æ„å·¥å…·æ£€ç´¢åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°IRæ¨¡å‹åœ¨å·¥å…·æ£€ç´¢ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚ç ”ç©¶å‘ç°ï¼Œå³ä½¿åœ¨ä¼ ç»ŸIRåŸºå‡†ä¸Šè¡¨ç°è‰¯å¥½çš„æ¨¡å‹ï¼Œåœ¨ToolRetä¸Šçš„è¡¨ç°å´å¾ˆå·®ï¼Œè¿™é™ä½äº†å·¥å…·ä½¿ç”¨LLMsçš„ä»»åŠ¡é€šè¿‡ç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.01729', 'title': 'FLAME: A Federated Learning Benchmark for Robotic Manipulation', 'url': 'https://huggingface.co/papers/2503.01729', 'abstract': 'Recent progress in robotic manipulation has been fueled by large-scale datasets collected across diverse environments. Training robotic manipulation policies on these datasets is traditionally performed in a centralized manner, raising concerns regarding scalability, adaptability, and data privacy. While federated learning enables decentralized, privacy-preserving training, its application to robotic manipulation remains largely unexplored. We introduce FLAME (Federated Learning Across Manipulation Environments), the first benchmark designed for federated learning in robotic manipulation. FLAME consists of: (i) a set of large-scale datasets of over 160,000 expert demonstrations of multiple manipulation tasks, collected across a wide range of simulated environments; (ii) a training and evaluation framework for robotic policy learning in a federated setting. We evaluate standard federated learning algorithms in FLAME, showing their potential for distributed policy learning and highlighting key challenges. Our benchmark establishes a foundation for scalable, adaptive, and privacy-aware robotic learning.', 'score': 3, 'issue_id': 2558, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 3', 'zh': '3æœˆ3æ—¥'}, 'hash': '893358a382c79250', 'authors': ['Santiago Bou Betran', 'Alberta Longhini', 'Miguel Vasco', 'Yuchong Zhang', 'Danica Kragic'], 'affiliations': ['KTH Royal Institute of Technology, Stockholm, Sweden'], 'pdf_title_img': 'assets/pdf/title_img/2503.01729.jpg', 'data': {'categories': ['#robotics', '#benchmark', '#dataset'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ¤ĞµĞ´ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ¸ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ´ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ FLAME - Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ñ„ĞµĞ´ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸. FLAME Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 160 000 ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ñ… Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¹ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸, ÑĞ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² ÑĞ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ…. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº Ğ² Ñ„ĞµĞ´ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ ÑÑ€ĞµĞ´Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğµ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ñ‹ Ñ„ĞµĞ´ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° FLAME, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ¸Ñ… Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ´Ğ»Ñ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº Ğ¸ Ğ²Ñ‹ÑĞ²Ğ»ÑÑ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹.'}, 'en': {'title': 'Empowering Robots with Federated Learning for Privacy and Scalability', 'desc': 'This paper presents FLAME, a benchmark for applying federated learning to robotic manipulation tasks. It addresses the limitations of centralized training by allowing robots to learn from diverse datasets while preserving data privacy. FLAME includes over 160,000 expert demonstrations from various simulated environments, facilitating decentralized training. The study evaluates existing federated learning algorithms, demonstrating their effectiveness and identifying challenges in distributed policy learning for robotics.'}, 'zh': {'title': 'è”é‚¦å­¦ä¹ åŠ©åŠ›æœºå™¨äººæ“æ§çš„æœªæ¥', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†FLAMEï¼ˆè·¨æ“æ§ç¯å¢ƒçš„è”é‚¦å­¦ä¹ ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸ºæœºå™¨äººæ“æ§è®¾è®¡çš„åŸºå‡†æµ‹è¯•ã€‚FLAMEåŒ…å«è¶…è¿‡160,000ä¸ªä¸“å®¶æ¼”ç¤ºçš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œæ¶µç›–å¤šç§æ“æ§ä»»åŠ¡ï¼Œæ”¶é›†è‡ªå¤šç§æ¨¡æ‹Ÿç¯å¢ƒã€‚é€šè¿‡åœ¨FLAMEä¸­è¯„ä¼°æ ‡å‡†çš„è”é‚¦å­¦ä¹ ç®—æ³•ï¼Œè®ºæ–‡å±•ç¤ºäº†åˆ†å¸ƒå¼ç­–ç•¥å­¦ä¹ çš„æ½œåŠ›ï¼Œå¹¶æŒ‡å‡ºäº†å…³é”®æŒ‘æˆ˜ã€‚è¯¥åŸºå‡†ä¸ºå¯æ‰©å±•ã€é€‚åº”æ€§å¼ºä¸”æ³¨é‡éšç§çš„æœºå™¨äººå­¦ä¹ å¥ å®šäº†åŸºç¡€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.01449', 'title': 'Benchmarking Large Language Models for Multi-Language Software Vulnerability Detection', 'url': 'https://huggingface.co/papers/2503.01449', 'abstract': 'Recent advancements in generative AI have led to the widespread adoption of large language models (LLMs) in software engineering, addressing numerous long-standing challenges. However, a comprehensive study examining the capabilities of LLMs in software vulnerability detection (SVD), a crucial aspect of software security, is currently lacking. Existing research primarily focuses on evaluating LLMs using C/C++ datasets. It typically explores only one or two strategies among prompt engineering, instruction tuning, and sequence classification fine-tuning for open-source LLMs. Consequently, there is a significant knowledge gap regarding the effectiveness of diverse LLMs in detecting vulnerabilities across various programming languages. To address this knowledge gap, we present a comprehensive empirical study evaluating the performance of LLMs on the SVD task. We have compiled a comprehensive dataset comprising 8,260 vulnerable functions in Python, 7,505 in Java, and 28,983 in JavaScript. We assess five open-source LLMs using multiple approaches, including prompt engineering, instruction tuning, and sequence classification fine-tuning. These LLMs are benchmarked against five fine-tuned small language models and two open-source static application security testing tools. Furthermore, we explore two avenues to improve LLM performance on SVD: a) Data perspective: Retraining models using downsampled balanced datasets. b) Model perspective: Investigating ensemble learning methods that combine predictions from multiple LLMs. Our comprehensive experiments demonstrate that SVD remains a challenging task for LLMs. This study provides a thorough understanding of the role of LLMs in SVD and offers practical insights for future advancements in leveraging generative AI to enhance software security practices.', 'score': 3, 'issue_id': 2558, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 3', 'zh': '3æœˆ3æ—¥'}, 'hash': '1b4593bb9d78ec53', 'authors': ['Ting Zhang', 'Chengran Yang', 'Yindu Su', 'Martin Weyssow', 'Hung Nguyen', 'Tan Bui', 'Hong Jin Kang', 'Yikun Li', 'Eng Lieh Ouh', 'Lwin Khin Shar', 'David Lo'], 'affiliations': ['School of Computer Science, University of Sydney, Australia', 'School of Computing and Information Systems, Singapore Management University, Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2503.01449.jpg', 'data': {'categories': ['#open_source', '#plp', '#training', '#security', '#benchmark', '#dataset', '#data'], 'emoji': 'ğŸ›¡ï¸', 'ru': {'title': 'LLM Ğ½Ğ° ÑÑ‚Ñ€Ğ°Ğ¶Ğµ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ğ´Ğ°: Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ñ‹ Ğ² Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¸ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¸ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ (SVD). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿ÑÑ‚Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… LLM Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ñ… ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ñ‹Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ½Ğ° Python, Java Ğ¸ JavaScript, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ², Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ¸ Ñ‚Ğ¾Ğ½ĞºĞ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¸Ğ·ÑƒÑ‡Ğ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ñ‹ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ LLM Ğ² SVD, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° ÑĞ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ°Ğ½ÑĞ°Ğ¼Ğ±Ğ»ĞµĞ²Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ SVD Ğ¾ÑÑ‚Ğ°ĞµÑ‚ÑÑ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡ĞµĞ¹ Ğ´Ğ»Ñ LLM, Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ñ†ĞµĞ½Ğ½Ñ‹Ğµ insights Ğ´Ğ»Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ¾Ğº Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Unlocking LLMs for Software Vulnerability Detection', 'desc': "This paper investigates the effectiveness of large language models (LLMs) in detecting software vulnerabilities, an important area for software security. It highlights the lack of comprehensive studies on LLMs' capabilities across various programming languages, as most existing research focuses on C/C++ datasets. The authors present an empirical study using a dataset of over 44,000 vulnerable functions from Python, Java, and JavaScript, evaluating five open-source LLMs with different strategies like prompt engineering and instruction tuning. The findings reveal that while LLMs show promise, software vulnerability detection remains a challenging task, providing valuable insights for future improvements in this field."}, 'zh': {'title': 'æå‡è½¯ä»¶å®‰å…¨ï¼šå¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ¼æ´æ£€æµ‹ä¸­çš„åº”ç”¨', 'desc': 'æœ€è¿‘ç”Ÿæˆæ€§äººå·¥æ™ºèƒ½çš„è¿›å±•ä½¿å¾—å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è½¯ä»¶å·¥ç¨‹ä¸­å¾—åˆ°äº†å¹¿æ³›åº”ç”¨ï¼Œè§£å†³äº†è®¸å¤šé•¿æœŸå­˜åœ¨çš„æŒ‘æˆ˜ã€‚ç„¶è€Œï¼Œç›®å‰ç¼ºä¹å¯¹LLMsåœ¨è½¯ä»¶æ¼æ´æ£€æµ‹ï¼ˆSVDï¼‰èƒ½åŠ›çš„å…¨é¢ç ”ç©¶ï¼Œè¿™å¯¹è½¯ä»¶å®‰å…¨è‡³å…³é‡è¦ã€‚ç°æœ‰ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨ä½¿ç”¨C/C++æ•°æ®é›†è¯„ä¼°LLMsï¼Œé€šå¸¸åªæ¢è®¨äº†æç¤ºå·¥ç¨‹ã€æŒ‡ä»¤è°ƒä¼˜å’Œåºåˆ—åˆ†ç±»å¾®è°ƒä¸­çš„ä¸€ä¸¤ç§ç­–ç•¥ã€‚å› æ­¤ï¼Œæˆ‘ä»¬è¿›è¡Œäº†ä¸€é¡¹å…¨é¢çš„å®è¯ç ”ç©¶ï¼Œè¯„ä¼°LLMsåœ¨ä¸åŒç¼–ç¨‹è¯­è¨€ä¸­æ£€æµ‹æ¼æ´çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.01378', 'title': 'CognitiveDrone: A VLA Model and Evaluation Benchmark for Real-Time Cognitive Task Solving and Reasoning in UAVs', 'url': 'https://huggingface.co/papers/2503.01378', 'abstract': 'This paper introduces CognitiveDrone, a novel Vision-Language-Action (VLA) model tailored for complex Unmanned Aerial Vehicles (UAVs) tasks that demand advanced cognitive abilities. Trained on a dataset comprising over 8,000 simulated flight trajectories across three key categories-Human Recognition, Symbol Understanding, and Reasoning-the model generates real-time 4D action commands based on first-person visual inputs and textual instructions. To further enhance performance in intricate scenarios, we propose CognitiveDrone-R1, which integrates an additional Vision-Language Model (VLM) reasoning module to simplify task directives prior to high-frequency control. Experimental evaluations using our open-source benchmark, CognitiveDroneBench, reveal that while a racing-oriented model (RaceVLA) achieves an overall success rate of 31.3%, the base CognitiveDrone model reaches 59.6%, and CognitiveDrone-R1 attains a success rate of 77.2%. These results demonstrate improvements of up to 30% in critical cognitive tasks, underscoring the effectiveness of incorporating advanced reasoning capabilities into UAV control systems. Our contributions include the development of a state-of-the-art VLA model for UAV control and the introduction of the first dedicated benchmark for assessing cognitive tasks in drone operations. The complete repository is available at cognitivedrone.github.io', 'score': 2, 'issue_id': 2558, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 3', 'zh': '3æœˆ3æ—¥'}, 'hash': '8a4aab69ce92453d', 'authors': ['Artem Lykov', 'Valerii Serpiva', 'Muhammad Haris Khan', 'Oleg Sautenkov', 'Artyom Myshlyaev', 'Grik Tadevosyan', 'Yasheerah Yaqoot', 'Dzmitry Tsetserukou'], 'affiliations': ['Intelligent Space Robotics Laboratory, Science Center for Digital Engineering, Technology. Skolkovo Institute'], 'pdf_title_img': 'assets/pdf/title_img/2503.01378.jpg', 'data': {'categories': ['#open_source', '#reasoning', '#benchmark', '#dataset', '#multimodal', '#cv'], 'emoji': 'ğŸš', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ñ‹Ğµ Ğ´Ñ€Ğ¾Ğ½Ñ‹: ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ‘ĞŸĞ›Ğ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ CognitiveDrone - Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ—Ñ€ĞµĞ½Ğ¸Ğµ-Ğ¯Ğ·Ñ‹Ğº-Ğ”ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ (VLA) Ğ´Ğ»Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ±ĞµÑĞ¿Ğ¸Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ğ»ĞµÑ‚Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ¿Ğ¿Ğ°Ñ€Ğ°Ñ‚Ğ¾Ğ² (Ğ‘ĞŸĞ›Ğ). ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 8000 ÑĞ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»ĞµÑ‚Ğ¾Ğ² Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñ‹ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹. Ğ£ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ²ĞµÑ€ÑĞ¸Ñ CognitiveDrone-R1 Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ—Ñ€ĞµĞ½Ğ¸Ñ-Ğ¯Ğ·Ñ‹ĞºĞ° (VLM) Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ¾Ñ‰ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ CognitiveDrone-R1 Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² 77.2%, Ñ‡Ñ‚Ğ¾ Ğ½Ğ° 30% Ğ»ÑƒÑ‡ÑˆĞµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ….'}, 'en': {'title': 'CognitiveDrone: Elevating UAV Intelligence with Vision-Language-Action!', 'desc': 'This paper presents CognitiveDrone, a new Vision-Language-Action (VLA) model designed for complex tasks performed by Unmanned Aerial Vehicles (UAVs). It is trained on a dataset of over 8,000 simulated flight paths focusing on Human Recognition, Symbol Understanding, and Reasoning. The model can generate real-time 4D action commands from visual inputs and text instructions, with an enhanced version, CognitiveDrone-R1, that includes a Vision-Language Model (VLM) reasoning module for better task management. Experimental results show significant performance improvements, with CognitiveDrone-R1 achieving a 77.2% success rate, highlighting the importance of advanced reasoning in UAV operations.'}, 'zh': {'title': 'æ™ºèƒ½æ— äººæœºçš„è®¤çŸ¥é£è¡Œæ–°çºªå…ƒ', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºCognitiveDroneçš„æ–°å‹è§†è§‰-è¯­è¨€-è¡ŒåŠ¨ï¼ˆVLAï¼‰æ¨¡å‹ï¼Œä¸“ä¸ºå¤æ‚çš„æ— äººæœºä»»åŠ¡è®¾è®¡ï¼Œå…·å¤‡é«˜çº§è®¤çŸ¥èƒ½åŠ›ã€‚è¯¥æ¨¡å‹åœ¨è¶…è¿‡8000æ¡æ¨¡æ‹Ÿé£è¡Œè½¨è¿¹çš„æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œæ¶µç›–äººç±»è¯†åˆ«ã€ç¬¦å·ç†è§£å’Œæ¨ç†ä¸‰ä¸ªå…³é”®ç±»åˆ«ã€‚CognitiveDrone-R1é€šè¿‡é›†æˆé¢å¤–çš„è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰æ¨ç†æ¨¡å—ï¼Œè¿›ä¸€æ­¥æå‡åœ¨å¤æ‚åœºæ™¯ä¸­çš„è¡¨ç°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCognitiveDroneæ¨¡å‹çš„æˆåŠŸç‡è¾¾åˆ°59.6%ï¼Œè€ŒCognitiveDrone-R1çš„æˆåŠŸç‡æ›´æ˜¯é«˜è¾¾77.2%ï¼Œè¯æ˜äº†å°†é«˜çº§æ¨ç†èƒ½åŠ›èå…¥æ— äººæœºæ§åˆ¶ç³»ç»Ÿçš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.01372', 'title': 'SwiLTra-Bench: The Swiss Legal Translation Benchmark', 'url': 'https://huggingface.co/papers/2503.01372', 'abstract': "In Switzerland legal translation is uniquely important due to the country's four official languages and requirements for multilingual legal documentation. However, this process traditionally relies on professionals who must be both legal experts and skilled translators -- creating bottlenecks and impacting effective access to justice. To address this challenge, we introduce SwiLTra-Bench, a comprehensive multilingual benchmark of over 180K aligned Swiss legal translation pairs comprising laws, headnotes, and press releases across all Swiss languages along with English, designed to evaluate LLM-based translation systems. Our systematic evaluation reveals that frontier models achieve superior translation performance across all document types, while specialized translation systems excel specifically in laws but under-perform in headnotes. Through rigorous testing and human expert validation, we demonstrate that while fine-tuning open SLMs significantly improves their translation quality, they still lag behind the best zero-shot prompted frontier models such as Claude-3.5-Sonnet. Additionally, we present SwiLTra-Judge, a specialized LLM evaluation system that aligns best with human expert assessments.", 'score': 2, 'issue_id': 2558, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 3', 'zh': '3æœˆ3æ—¥'}, 'hash': '3de5be81537fa0fd', 'authors': ['Joel Niklaus', 'Jakob Merane', 'Luka Nenadic', 'Sina Ahmadi', 'Yingqiang Gao', 'Cyrill A. H. Chevalley', 'Claude Humbel', 'Christophe GÃ¶sken', 'Lorenzo Tanzi', 'Thomas LÃ¼thi', 'Stefan Palombo', 'Spencer Poff', 'Boling Yang', 'Nan Wu', 'Matthew Guillod', 'Robin MamiÃ©', 'Daniel Brunner', 'Julio Pereyra', 'Niko Grupen'], 'affiliations': ['Canton of Solothurn', 'ETH Zurich', 'Max Planck Institute for Research on Collective Goods', 'Swiss Federal Supreme Court', 'University of Basel', 'University of Geneva', 'University of Lausanne', 'University of Zurich'], 'pdf_title_img': 'assets/pdf/title_img/2503.01372.jpg', 'data': {'categories': ['#multilingual', '#open_source', '#benchmark', '#dataset', '#machine_translation'], 'emoji': 'âš–ï¸', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑÑ€Ğ¸Ğ´Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğµ: Ğ˜Ğ˜ Ğ¿Ğ¾ĞºĞ¾Ñ€ÑĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½ÑƒÑ Ğ¨Ğ²ĞµĞ¹Ñ†Ğ°Ñ€Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ SwiLTra-Bench - Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° ÑÑ€Ğ¸Ğ´Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ğ² Ğ¨Ğ²ĞµĞ¹Ñ†Ğ°Ñ€Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ²Ğ¾ Ğ²ÑĞµÑ… Ñ‚Ğ¸Ğ¿Ğ°Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ğ° Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ°. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° SwiLTra-Judge Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ ĞºĞ¾Ñ€Ñ€ĞµĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Enhancing Legal Translation with SwiLTra-Bench and LLMs', 'desc': 'This paper addresses the challenges of legal translation in Switzerland, where multiple languages complicate the process. It introduces SwiLTra-Bench, a benchmark dataset with over 180,000 aligned legal translation pairs to evaluate large language model (LLM) translation systems. The findings show that while advanced models perform well across various document types, specialized systems are better for translating laws but struggle with headnotes. The study also highlights the effectiveness of fine-tuning open-source language models, although they still do not match the performance of top zero-shot models like Claude-3.5-Sonnet.'}, 'zh': {'title': 'ç‘å£«æ³•å¾‹ç¿»è¯‘çš„æ™ºèƒ½è§£å†³æ–¹æ¡ˆ', 'desc': 'åœ¨ç‘å£«ï¼Œç”±äºæœ‰å››ç§å®˜æ–¹è¯­è¨€ï¼Œæ³•å¾‹ç¿»è¯‘æ˜¾å¾—å°¤ä¸ºé‡è¦ã€‚ä¼ ç»Ÿä¸Šï¼Œè¿™ä¸€è¿‡ç¨‹ä¾èµ–äºæ—¢æ˜¯æ³•å¾‹ä¸“å®¶åˆæ˜¯ç¿»è¯‘é«˜æ‰‹çš„ä¸“ä¸šäººå£«ï¼Œå¯¼è‡´äº†ç“¶é¢ˆï¼Œå½±å“äº†å…¬æ­£çš„æœ‰æ•ˆè·å–ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†SwiLTra-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«è¶…è¿‡18ä¸‡å¯¹ç‘å£«æ³•å¾‹ç¿»è¯‘çš„å¤šè¯­è¨€åŸºå‡†æ•°æ®é›†ï¼Œæ—¨åœ¨è¯„ä¼°åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„ç¿»è¯‘ç³»ç»Ÿã€‚æˆ‘ä»¬çš„è¯„ä¼°æ˜¾ç¤ºï¼Œå‰æ²¿æ¨¡å‹åœ¨æ‰€æœ‰æ–‡æ¡£ç±»å‹çš„ç¿»è¯‘è¡¨ç°ä¸Šä¼˜äºå…¶ä»–ç³»ç»Ÿï¼Œè€Œä¸“é—¨çš„ç¿»è¯‘ç³»ç»Ÿåœ¨æ³•å¾‹æ–‡æœ¬ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤´æ³¨æ–¹é¢è¡¨ç°ä¸ä½³ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.00502', 'title': 'Interact, Instruct to Improve: A LLM-Driven Parallel Actor-Reasoner Framework for Enhancing Autonomous Vehicle Interactions', 'url': 'https://huggingface.co/papers/2503.00502', 'abstract': "Autonomous Vehicles (AVs) have entered the commercialization stage, but their limited ability to interact and express intentions still poses challenges in interactions with Human-driven Vehicles (HVs). Recent advances in large language models (LLMs) enable bidirectional human-machine communication, but the conflict between slow inference speed and the need for real-time decision-making challenges practical deployment. To address these issues, this paper introduces a parallel Actor-Reasoner framework designed to enable explicit bidirectional AV-HV interactions across multiple scenarios. First, by facilitating interactions between the LLM-driven Reasoner and heterogeneous simulated HVs during training, an interaction memory database, referred to as the Actor, is established. Then, by introducing the memory partition module and the two-layer memory retrieval module, the Actor's ability to handle heterogeneous HVs is significantly enhanced. Ablation studies and comparisons with other decision-making methods demonstrate that the proposed Actor-Reasoner framework significantly improves safety and efficiency. Finally, with the combination of the external Human-Machine Interface (eHMI) information derived from Reasoner's reasoning and the feasible action solutions retrieved from the Actor, the effectiveness of the proposed Actor-Reasoner is confirmed in multi-scenario field interactions. Our code is available at https://github.com/FanGShiYuu/Actor-Reasoner.", 'score': 2, 'issue_id': 2555, 'pub_date': '2025-03-01', 'pub_date_card': {'ru': '1 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 1', 'zh': '3æœˆ1æ—¥'}, 'hash': 'd184a5cae68093d5', 'authors': ['Shiyu Fang', 'Jiaqi Liu', 'Chengkai Xu', 'Chen Lv', 'Peng Hang', 'Jian Sun'], 'affiliations': ['College of Transportation, Tongji University, Shanghai 201804, China', 'Nanyang Technological University, 639798, Singapore', 'State Key Lab of Intelligent Transportation System, Beijing 100088, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.00502.jpg', 'data': {'categories': ['#rl', '#robotics', '#inference', '#optimization', '#agents', '#reasoning'], 'emoji': 'ğŸš—', 'ru': {'title': 'Ğ˜Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ğ¸ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ¾Ğ±Ğ¸Ğ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Actor-Reasoner Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ğ¼Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¼ Ñ‚Ñ€Ğ°Ğ½ÑĞ¿Ğ¾Ñ€Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ ÑÑ€ĞµĞ´ÑÑ‚Ğ²Ğ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ±Ğ°Ğ·Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸ Ğ´Ğ²ÑƒÑ…ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚Ñ€Ğ°Ğ½ÑĞ¿Ğ¾Ñ€Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ ÑÑ€ĞµĞ´ÑÑ‚Ğ²Ğ°Ğ¼Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Actor-Reasoner Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ….'}, 'en': {'title': 'Enhancing AV-HV Interactions with the Actor-Reasoner Framework', 'desc': "This paper presents a new framework called the Actor-Reasoner to improve interactions between Autonomous Vehicles (AVs) and Human-driven Vehicles (HVs). It leverages large language models (LLMs) to facilitate real-time communication and decision-making, addressing the challenge of slow inference speeds. The framework includes an interaction memory database, which enhances the AV's ability to understand and respond to various HV behaviors. Experimental results show that this approach significantly boosts both safety and efficiency in multi-scenario driving situations."}, 'zh': {'title': 'æå‡è‡ªåŠ¨é©¾é©¶ä¸äººç±»é©¾é©¶äº’åŠ¨çš„æ™ºèƒ½æ¡†æ¶', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„å¹¶è¡Œæ¼”å‘˜-æ¨ç†å™¨æ¡†æ¶ï¼Œæ—¨åœ¨æ”¹å–„è‡ªåŠ¨é©¾é©¶æ±½è½¦ï¼ˆAVï¼‰ä¸äººç±»é©¾é©¶æ±½è½¦ï¼ˆHVï¼‰ä¹‹é—´çš„äº’åŠ¨ã€‚é€šè¿‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¿ƒè¿›å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é©±åŠ¨çš„æ¨ç†å™¨ä¸ä¸åŒç±»å‹çš„æ¨¡æ‹ŸHVä¹‹é—´çš„äº’åŠ¨ï¼Œå»ºç«‹äº†ä¸€ä¸ªäº’åŠ¨è®°å¿†æ•°æ®åº“ã€‚å¼•å…¥è®°å¿†åˆ†åŒºæ¨¡å—å’ŒåŒå±‚è®°å¿†æ£€ç´¢æ¨¡å—åï¼Œæ¼”å‘˜çš„å¤„ç†èƒ½åŠ›å¾—åˆ°äº†æ˜¾è‘—æå‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å¤šåœºæ™¯äº¤äº’ä¸­æ˜¾è‘—æé«˜äº†å®‰å…¨æ€§å’Œæ•ˆç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.02954', 'title': 'Reliable and Efficient Multi-Agent Coordination via Graph Neural Network Variational Autoencoders', 'url': 'https://huggingface.co/papers/2503.02954', 'abstract': 'Multi-agent coordination is crucial for reliable multi-robot navigation in shared spaces such as automated warehouses. In regions of dense robot traffic, local coordination methods may fail to find a deadlock-free solution. In these scenarios, it is appropriate to let a central unit generate a global schedule that decides the passing order of robots. However, the runtime of such centralized coordination methods increases significantly with the problem scale. In this paper, we propose to leverage Graph Neural Network Variational Autoencoders (GNN-VAE) to solve the multi-agent coordination problem at scale faster than through centralized optimization. We formulate the coordination problem as a graph problem and collect ground truth data using a Mixed-Integer Linear Program (MILP) solver. During training, our learning framework encodes good quality solutions of the graph problem into a latent space. At inference time, solution samples are decoded from the sampled latent variables, and the lowest-cost sample is selected for coordination. Finally, the feasible proposal with the highest performance index is selected for the deployment. By construction, our GNN-VAE framework returns solutions that always respect the constraints of the considered coordination problem. Numerical results show that our approach trained on small-scale problems can achieve high-quality solutions even for large-scale problems with 250 robots, being much faster than other baselines. Project page: https://mengyuest.github.io/gnn-vae-coord', 'score': 0, 'issue_id': 2574, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 4', 'zh': '3æœˆ4æ—¥'}, 'hash': '1d0c072d834299e0', 'authors': ['Yue Meng', 'Nathalie Majcherczyk', 'Wenliang Liu', 'Scott Kiesel', 'Chuchu Fan', 'Federico Pecora'], 'affiliations': ['Amazon Robotics, North Reading, MA USA', 'Massachusetts Institute of Technology, Cambridge, MA 02139 USA'], 'pdf_title_img': 'assets/pdf/title_img/2503.02954.jpg', 'data': {'categories': ['#optimization', '#training', '#games', '#agents', '#inference', '#graphs'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'GNN-VAE: Ğ‘Ñ‹ÑÑ‚Ñ€Ğ°Ñ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ° Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ° Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ² Ğ¾Ğ±Ñ‰Ğ¸Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°Ñ…, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞºĞ»Ğ°Ğ´Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ²Ñ‹Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹ (GNN-VAE) Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğµ Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ, Ñ‡ĞµĞ¼ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ†ĞµĞ½Ñ‚Ñ€Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€ĞµÑˆĞ°Ñ‚ĞµĞ»ĞµĞ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡ ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ†ĞµĞ»Ğ¾Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ (MILP). Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ‚ÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ´Ğ°Ğ¶Ğµ Ğ´Ğ»Ñ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ñ 250 Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ğ¼Ğ¸, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ².'}, 'en': {'title': 'Efficient Multi-Robot Coordination with GNN-VAE', 'desc': 'This paper addresses the challenge of coordinating multiple robots in shared environments, particularly in high-traffic areas where traditional local methods may lead to deadlocks. The authors propose a novel approach using Graph Neural Network Variational Autoencoders (GNN-VAE) to efficiently generate coordination schedules at scale. By framing the coordination problem as a graph problem and utilizing a Mixed-Integer Linear Program (MILP) for data collection, the framework learns to encode effective solutions into a latent space. During inference, it decodes these solutions to select the most optimal coordination strategy, ensuring compliance with all operational constraints while significantly reducing computation time compared to centralized methods.'}, 'zh': {'title': 'é«˜æ•ˆå¤šæ™ºèƒ½ä½“åè°ƒçš„æ–°æ–¹æ³•', 'desc': 'å¤šæ™ºèƒ½ä½“åè°ƒåœ¨å…±äº«ç©ºé—´ä¸­çš„å¯é å¤šæœºå™¨äººå¯¼èˆªä¸­è‡³å…³é‡è¦ï¼Œå°¤å…¶æ˜¯åœ¨æœºå™¨äººäº¤é€šå¯†é›†çš„åŒºåŸŸã€‚ä¼ ç»Ÿçš„å±€éƒ¨åè°ƒæ–¹æ³•å¯èƒ½æ— æ³•æ‰¾åˆ°æ— æ­»é”çš„è§£å†³æ–¹æ¡ˆï¼Œå› æ­¤éœ€è¦ä¸€ä¸ªä¸­å¤®å•å…ƒç”Ÿæˆå…¨å±€è°ƒåº¦æ¥å†³å®šæœºå™¨äººçš„é€šè¡Œé¡ºåºã€‚æœ¬æ–‡æå‡ºåˆ©ç”¨å›¾ç¥ç»ç½‘ç»œå˜åˆ†è‡ªç¼–ç å™¨ï¼ˆGNN-VAEï¼‰æ¥æ›´å¿«åœ°è§£å†³å¤§è§„æ¨¡çš„å¤šæ™ºèƒ½ä½“åè°ƒé—®é¢˜ï¼Œé¿å…äº†é›†ä¸­ä¼˜åŒ–æ–¹æ³•çš„é«˜è¿è¡Œæ—¶é—´ã€‚é€šè¿‡å°†åè°ƒé—®é¢˜å½¢å¼åŒ–ä¸ºå›¾é—®é¢˜ï¼Œå¹¶ä½¿ç”¨æ··åˆæ•´æ•°çº¿æ€§è§„åˆ’ï¼ˆMILPï¼‰æ±‚è§£å™¨æ”¶é›†çœŸå®æ•°æ®ï¼Œæˆ‘ä»¬çš„å­¦ä¹ æ¡†æ¶èƒ½å¤Ÿåœ¨æ½œåœ¨ç©ºé—´ä¸­ç¼–ç é«˜è´¨é‡çš„è§£å†³æ–¹æ¡ˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.02924', 'title': 'Diverse Controllable Diffusion Policy with Signal Temporal Logic', 'url': 'https://huggingface.co/papers/2503.02924', 'abstract': 'Generating realistic simulations is critical for autonomous system applications such as self-driving and human-robot interactions. However, driving simulators nowadays still have difficulty in generating controllable, diverse, and rule-compliant behaviors for road participants: Rule-based models cannot produce diverse behaviors and require careful tuning, whereas learning-based methods imitate the policy from data but are not designed to follow the rules explicitly. Besides, the real-world datasets are by nature "single-outcome", making the learning method hard to generate diverse behaviors. In this paper, we leverage Signal Temporal Logic (STL) and Diffusion Models to learn controllable, diverse, and rule-aware policy. We first calibrate the STL on the real-world data, then generate diverse synthetic data using trajectory optimization, and finally learn the rectified diffusion policy on the augmented dataset. We test on the NuScenes dataset and our approach can achieve the most diverse rule-compliant trajectories compared to other baselines, with a runtime 1/17X to the second-best approach. In the closed-loop testing, our approach reaches the highest diversity, rule satisfaction rate, and the least collision rate. Our method can generate varied characteristics conditional on different STL parameters in testing. A case study on human-robot encounter scenarios shows our approach can generate diverse and closed-to-oracle trajectories. The annotation tool, augmented dataset, and code are available at https://github.com/mengyuest/pSTL-diffusion-policy.', 'score': 0, 'issue_id': 2574, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 4', 'zh': '3æœˆ4æ—¥'}, 'hash': 'adc4dd2a16bb83a4', 'authors': ['Yue Meng', 'Chuchu fan'], 'affiliations': ['Department of Aeronautics and Astronautics, Massachusetts Institute of Technology, Cambridge, MA 02139 USA'], 'pdf_title_img': 'assets/pdf/title_img/2503.02924.jpg', 'data': {'categories': ['#dataset', '#training', '#data', '#rl', '#agents', '#diffusion', '#synthetic'], 'emoji': 'ğŸš—', 'ru': {'title': 'Ğ ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ´Ğ¾Ñ€Ğ¾Ğ¶Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ STL Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ±ĞµÑĞ¿Ğ¸Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ğµ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ¾Ğ±Ğ¸Ğ»Ğ¸ Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ Ğ¡Ğ¸Ğ³Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¢ĞµĞ¼Ğ¿Ğ¾Ñ€Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ›Ğ¾Ğ³Ğ¸ĞºĞ¸ (STL) Ğ¸ Ğ”Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ĞœĞ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹, Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¹ Ğ¸ ÑĞ¾Ğ±Ğ»ÑĞ´Ğ°ÑÑ‰ĞµĞ¹ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ° Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ ÑƒÑ‡Ğ°ÑÑ‚Ğ½Ğ¸ĞºĞ¾Ğ² Ğ´Ğ¾Ñ€Ğ¾Ğ¶Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° ĞºĞ°Ğ»Ğ¸Ğ±Ñ€ÑƒĞµÑ‚ STL Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹, Ğ¸ Ğ½Ğ°ĞºĞ¾Ğ½ĞµÑ† Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ ÑĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºÑƒ Ğ½Ğ° Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ, ÑĞ¾Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ» Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² STL.'}, 'en': {'title': 'Diverse and Rule-Compliant Simulations for Autonomous Systems', 'desc': 'This paper addresses the challenge of generating realistic simulations for autonomous systems, particularly in driving scenarios. It combines Signal Temporal Logic (STL) with Diffusion Models to create a policy that is both controllable and diverse while adhering to traffic rules. By calibrating STL on real-world data and generating synthetic data through trajectory optimization, the authors enhance the learning process to produce varied and rule-compliant behaviors. The results demonstrate that their method outperforms existing approaches in terms of diversity and safety in simulated environments.'}, 'zh': {'title': 'ç”Ÿæˆå¤šæ ·åŒ–ä¸”éµå¾ªè§„åˆ™çš„è¡Œä¸ºç­–ç•¥', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œé€šè¿‡ä¿¡å·æ—¶åºé€»è¾‘ï¼ˆSTLï¼‰å’Œæ‰©æ•£æ¨¡å‹æ¥ç”Ÿæˆå¯æ§ã€å¤šæ ·ä¸”éµå¾ªè§„åˆ™çš„è¡Œä¸ºç­–ç•¥ï¼Œä»¥è§£å†³å½“å‰é©¾é©¶æ¨¡æ‹Ÿå™¨åœ¨ç”Ÿæˆé“è·¯å‚ä¸è€…è¡Œä¸ºæ—¶çš„å±€é™æ€§ã€‚æˆ‘ä»¬é¦–å…ˆåœ¨çœŸå®æ•°æ®ä¸Šæ ¡å‡†STLï¼Œç„¶ååˆ©ç”¨è½¨è¿¹ä¼˜åŒ–ç”Ÿæˆå¤šæ ·çš„åˆæˆæ•°æ®ï¼Œæœ€ååœ¨å¢å¼ºæ•°æ®é›†ä¸Šå­¦ä¹ ä¿®æ­£çš„æ‰©æ•£ç­–ç•¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨NuScenesæ•°æ®é›†ä¸Šèƒ½å¤Ÿç”Ÿæˆæœ€å…·å¤šæ ·æ€§ä¸”ç¬¦åˆè§„åˆ™çš„è½¨è¿¹ï¼Œä¸”è¿è¡Œæ—¶é—´æ˜¾è‘—ä¼˜äºå…¶ä»–åŸºçº¿æ–¹æ³•ã€‚é€šè¿‡é—­ç¯æµ‹è¯•ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šæ ·æ€§ã€è§„åˆ™æ»¡è¶³ç‡å’Œç¢°æ’ç‡æ–¹é¢å‡è¡¨ç°æœ€ä½³ï¼Œèƒ½å¤Ÿæ ¹æ®ä¸åŒçš„STLå‚æ•°ç”Ÿæˆå¤šæ ·åŒ–çš„ç‰¹å¾ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.18878', 'title': 'I Have Covered All the Bases Here: Interpreting Reasoning Features in\n  Large Language Models via Sparse Autoencoders', 'url': 'https://huggingface.co/papers/2503.18878', 'abstract': "Large Language Models (LLMs) have achieved remarkable success in natural language processing. Recent advances have led to the developing of a new class of reasoning LLMs; for example, open-source DeepSeek-R1 has achieved state-of-the-art performance by integrating deep thinking and complex reasoning. Despite these impressive capabilities, the internal reasoning mechanisms of such models remain unexplored. In this work, we employ Sparse Autoencoders (SAEs), a method to learn a sparse decomposition of latent representations of a neural network into interpretable features, to identify features that drive reasoning in the DeepSeek-R1 series of models. First, we propose an approach to extract candidate ''reasoning features'' from SAE representations. We validate these features through empirical analysis and interpretability methods, demonstrating their direct correlation with the model's reasoning abilities. Crucially, we demonstrate that steering these features systematically enhances reasoning performance, offering the first mechanistic account of reasoning in LLMs. Code available at https://github.com/AIRI-Institute/SAE-Reasoning", 'score': 82, 'issue_id': 2881, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 24', 'zh': '3æœˆ24æ—¥'}, 'hash': 'a72d586944db2b55', 'authors': ['Andrey Galichin', 'Alexey Dontsov', 'Polina Druzhinina', 'Anton Razzhigaev', 'Oleg Y. Rogov', 'Elena Tutubalina', 'Ivan Oseledets'], 'affiliations': ['AIRI', 'HSE', 'MTUCI', 'Sber', 'Skoltech'], 'pdf_title_img': 'assets/pdf/title_img/2503.18878.jpg', 'data': {'categories': ['#training', '#interpretability', '#open_source', '#architecture', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°Ñ Ñ‚Ğ°Ğ¹Ğ½Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¸Ğ·ÑƒÑ‡Ğ¸Ğ»Ğ¸ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğµ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ñ‹ (SAE). ĞĞ½Ğ¸ Ğ²Ñ‹Ğ´ĞµĞ»Ğ¸Ğ»Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸, Ğ¾Ñ‚Ğ²ĞµÑ‡Ğ°ÑÑ‰Ğ¸Ğµ Ğ·Ğ° ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼, Ğ² ÑĞµÑ€Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ DeepSeek-R1. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ» Ğ¿Ñ€ÑĞ¼ÑƒÑ ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ñ ÑÑ‚Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ£Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ğ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ°Ğ¼Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ‚ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°ĞµÑ‚ Ğ¿ĞµÑ€Ğ²Ğ¾Ğµ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ….'}, 'en': {'title': 'Unlocking Reasoning in Large Language Models', 'desc': "This paper explores the internal reasoning mechanisms of Large Language Models (LLMs), specifically focusing on the DeepSeek-R1 model. The authors utilize Sparse Autoencoders (SAEs) to extract and identify 'reasoning features' that contribute to the model's reasoning capabilities. Through empirical analysis, they validate the correlation between these features and the model's performance in reasoning tasks. The study reveals that manipulating these features can systematically improve reasoning performance, providing new insights into how LLMs process and understand complex information."}, 'zh': {'title': 'æ­ç¤ºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†æœºåˆ¶', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå–å¾—äº†æ˜¾è‘—æˆåŠŸã€‚æœ€è¿‘çš„è¿›å±•å‚¬ç”Ÿäº†ä¸€ç±»æ–°çš„æ¨ç†LLMsï¼Œä¾‹å¦‚å¼€æºçš„DeepSeek-R1ï¼Œé€šè¿‡æ•´åˆæ·±åº¦æ€è€ƒå’Œå¤æ‚æ¨ç†å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚å°½ç®¡è¿™äº›æ¨¡å‹çš„èƒ½åŠ›ä»¤äººå°è±¡æ·±åˆ»ï¼Œä½†å…¶å†…éƒ¨æ¨ç†æœºåˆ¶ä»æœªè¢«æ·±å…¥æ¢ç´¢ã€‚æœ¬æ–‡é‡‡ç”¨ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEsï¼‰æ¥è¯†åˆ«é©±åŠ¨DeepSeek-R1ç³»åˆ—æ¨¡å‹æ¨ç†çš„ç‰¹å¾ï¼Œå¹¶é€šè¿‡å®è¯åˆ†æéªŒè¯è¿™äº›ç‰¹å¾ä¸æ¨¡å‹æ¨ç†èƒ½åŠ›çš„ç›´æ¥å…³è”ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.17359', 'title': 'Position: Interactive Generative Video as Next-Generation Game Engine', 'url': 'https://huggingface.co/papers/2503.17359', 'abstract': "Modern game development faces significant challenges in creativity and cost due to predetermined content in traditional game engines. Recent breakthroughs in video generation models, capable of synthesizing realistic and interactive virtual environments, present an opportunity to revolutionize game creation. In this position paper, we propose Interactive Generative Video (IGV) as the foundation for Generative Game Engines (GGE), enabling unlimited novel content generation in next-generation gaming. GGE leverages IGV's unique strengths in unlimited high-quality content synthesis, physics-aware world modeling, user-controlled interactivity, long-term memory capabilities, and causal reasoning. We present a comprehensive framework detailing GGE's core modules and a hierarchical maturity roadmap (L0-L4) to guide its evolution. Our work charts a new course for game development in the AI era, envisioning a future where AI-powered generative systems fundamentally reshape how games are created and experienced.", 'score': 51, 'issue_id': 2875, 'pub_date': '2025-03-21', 'pub_date_card': {'ru': '21 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 21', 'zh': '3æœˆ21æ—¥'}, 'hash': '0046c940a41d8637', 'authors': ['Jiwen Yu', 'Yiran Qin', 'Haoxuan Che', 'Quande Liu', 'Xintao Wang', 'Pengfei Wan', 'Di Zhang', 'Xihui Liu'], 'affiliations': ['Kuaishou', 'The Hong Kong University of Science and Technology', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2503.17359.jpg', 'data': {'categories': ['#video', '#architecture', '#games', '#multimodal'], 'emoji': 'ğŸ®', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ¸Ğ³Ñ€: Ğ˜Ğ˜-Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ğ¼Ğ¸Ñ€Ñ‹ Ğ±ÑƒĞ´ÑƒÑ‰ĞµĞ³Ğ¾', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ˜Ğ³Ñ€Ğ¾Ğ²Ñ‹Ñ… Ğ”Ğ²Ğ¸Ğ¶ĞºĞ¾Ğ² (GGE), Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ˜Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ’Ğ¸Ğ´ĞµĞ¾ Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸ (IGV). GGE Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ½ĞµĞ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ¸Ğ³Ñ€ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° IGV Ğ² ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°, Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ñ„Ğ¸Ğ·Ğ¸ĞºĞ¸ Ğ¼Ğ¸Ñ€Ğ° Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ÑƒĞ»ĞµĞ¹ GGE Ğ¸ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ´Ğ¾Ñ€Ğ¾Ğ¶Ğ½ÑƒÑ ĞºĞ°Ñ€Ñ‚Ñƒ Ğ·Ñ€ĞµĞ»Ğ¾ÑÑ‚Ğ¸ (L0-L4) Ğ´Ğ»Ñ ĞµĞ³Ğ¾ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ. Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ñ‹ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¸Ğ³Ñ€ Ğ² ÑĞ¿Ğ¾Ñ…Ñƒ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°.'}, 'en': {'title': 'Revolutionizing Game Development with AI-Driven Generative Engines', 'desc': 'This paper discusses the limitations of traditional game engines that rely on fixed content, which can hinder creativity and increase costs in game development. It introduces Interactive Generative Video (IGV) as a new approach to create Generative Game Engines (GGE), which can produce endless unique game content. GGE utilizes advanced features like high-quality content synthesis, physics-aware modeling, and user interactivity to enhance the gaming experience. The authors also outline a framework and roadmap for the development of GGE, aiming to transform the future of game creation through AI technologies.'}, 'zh': {'title': 'AIé©±åŠ¨çš„æ¸¸æˆåˆ›ä½œæ–°çºªå…ƒ', 'desc': 'ç°ä»£æ¸¸æˆå¼€å‘é¢ä¸´ç€åˆ›é€ åŠ›å’Œæˆæœ¬çš„é‡å¤§æŒ‘æˆ˜ï¼Œä¼ ç»Ÿæ¸¸æˆå¼•æ“çš„å†…å®¹é¢„è®¾é™åˆ¶äº†åˆ›æ–°ã€‚æœ€è¿‘ï¼Œè§†é¢‘ç”Ÿæˆæ¨¡å‹çš„çªç ´ä½¿å¾—åˆæˆé€¼çœŸä¸”äº’åŠ¨çš„è™šæ‹Ÿç¯å¢ƒæˆä¸ºå¯èƒ½ï¼Œè¿™ä¸ºæ¸¸æˆåˆ›ä½œå¸¦æ¥äº†é©å‘½æ€§çš„æœºä¼šã€‚æˆ‘ä»¬æå‡ºäº†äº’åŠ¨ç”Ÿæˆè§†é¢‘ï¼ˆIGVï¼‰ä½œä¸ºç”Ÿæˆæ¸¸æˆå¼•æ“ï¼ˆGGEï¼‰çš„åŸºç¡€ï¼Œèƒ½å¤Ÿåœ¨ä¸‹ä¸€ä»£æ¸¸æˆä¸­å®ç°æ— é™çš„æ–°å†…å®¹ç”Ÿæˆã€‚GGEåˆ©ç”¨IGVåœ¨é«˜è´¨é‡å†…å®¹åˆæˆã€ç‰©ç†æ„ŸçŸ¥ä¸–ç•Œå»ºæ¨¡ã€ç”¨æˆ·æ§åˆ¶äº’åŠ¨ã€é•¿æœŸè®°å¿†èƒ½åŠ›å’Œå› æœæ¨ç†ç­‰æ–¹é¢çš„ç‹¬ç‰¹ä¼˜åŠ¿ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.18942', 'title': 'Video-T1: Test-Time Scaling for Video Generation', 'url': 'https://huggingface.co/papers/2503.18942', 'abstract': 'With the scale capability of increasing training data, model size, and computational cost, video generation has achieved impressive results in digital creation, enabling users to express creativity across various domains. Recently, researchers in Large Language Models (LLMs) have expanded the scaling to test-time, which can significantly improve LLM performance by using more inference-time computation. Instead of scaling up video foundation models through expensive training costs, we explore the power of Test-Time Scaling (TTS) in video generation, aiming to answer the question: if a video generation model is allowed to use non-trivial amount of inference-time compute, how much can it improve generation quality given a challenging text prompt. In this work, we reinterpret the test-time scaling of video generation as a searching problem to sample better trajectories from Gaussian noise space to the target video distribution. Specifically, we build the search space with test-time verifiers to provide feedback and heuristic algorithms to guide searching process. Given a text prompt, we first explore an intuitive linear search strategy by increasing noise candidates at inference time. As full-step denoising all frames simultaneously requires heavy test-time computation costs, we further design a more efficient TTS method for video generation called Tree-of-Frames (ToF) that adaptively expands and prunes video branches in an autoregressive manner. Extensive experiments on text-conditioned video generation benchmarks demonstrate that increasing test-time compute consistently leads to significant improvements in the quality of videos. Project page: https://liuff19.github.io/Video-T1', 'score': 50, 'issue_id': 2876, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 24', 'zh': '3æœˆ24æ—¥'}, 'hash': '1d482b72d90d6136', 'authors': ['Fangfu Liu', 'Hanyang Wang', 'Yimo Cai', 'Kaiyan Zhang', 'Xiaohang Zhan', 'Yueqi Duan'], 'affiliations': ['Tencent', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.18942.jpg', 'data': {'categories': ['#video', '#inference', '#games', '#optimization', '#benchmark'], 'emoji': 'ğŸ¥', 'ru': {'title': 'ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ (TTS) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ ÑÑ‚Ğ¾Ñ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ ĞºĞ°Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ¾Ñ‚ Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ğ° ÑˆÑƒĞ¼Ğ° Ğº Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ²Ğ° Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°: Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ñ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸ĞµĞ¼ ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ¾Ğ² ÑˆÑƒĞ¼Ğ° Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Tree-of-Frames (ToF), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ Ğ¸ Ğ¾Ğ±Ñ€ĞµĞ·Ğ°ĞµÑ‚ Ğ²ĞµÑ‚Ğ²Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğ¼ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ¾Ğ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'Unlocking Video Quality with Test-Time Scaling', 'desc': 'This paper explores the concept of Test-Time Scaling (TTS) in video generation, which allows models to utilize additional computational resources during inference to enhance video quality. Instead of focusing solely on training larger models, the authors investigate how increasing inference-time computation can improve the generation of videos from text prompts. They propose a method called Tree-of-Frames (ToF) that efficiently manages the search for better video outputs by adaptively expanding and pruning video branches. The results show that leveraging more computational power at test time leads to significant improvements in the quality of generated videos.'}, 'zh': {'title': 'æµ‹è¯•æ—¶é—´æ‰©å±•ï¼šæå‡è§†é¢‘ç”Ÿæˆè´¨é‡çš„æ–°æ–¹æ³•', 'desc': 'éšç€è®­ç»ƒæ•°æ®ã€æ¨¡å‹è§„æ¨¡å’Œè®¡ç®—æˆæœ¬çš„å¢åŠ ï¼Œè§†é¢‘ç”Ÿæˆåœ¨æ•°å­—åˆ›ä½œä¸­å–å¾—äº†æ˜¾è‘—æˆæœã€‚æœ¬æ–‡æ¢è®¨äº†åœ¨è§†é¢‘ç”Ÿæˆä¸­åº”ç”¨æµ‹è¯•æ—¶é—´æ‰©å±•ï¼ˆTTSï¼‰çš„æ½œåŠ›ï¼Œæ—¨åœ¨æé«˜ç”Ÿæˆè´¨é‡ã€‚æˆ‘ä»¬å°†æµ‹è¯•æ—¶é—´æ‰©å±•é‡æ–°è§£é‡Šä¸ºä¸€ä¸ªæœç´¢é—®é¢˜ï¼Œé€šè¿‡ä»é«˜æ–¯å™ªå£°ç©ºé—´ä¸­é‡‡æ ·æ›´å¥½çš„è½¨è¿¹æ¥ç”Ÿæˆç›®æ ‡è§†é¢‘ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå¢åŠ æµ‹è¯•æ—¶é—´è®¡ç®—å¯ä»¥æ˜¾è‘—æå‡è§†é¢‘è´¨é‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.18945', 'title': 'Aether: Geometric-Aware Unified World Modeling', 'url': 'https://huggingface.co/papers/2503.18945', 'abstract': 'The integration of geometric reconstruction and generative modeling remains a critical challenge in developing AI systems capable of human-like spatial reasoning. This paper proposes Aether, a unified framework that enables geometry-aware reasoning in world models by jointly optimizing three core capabilities: (1) 4D dynamic reconstruction, (2) action-conditioned video prediction, and (3) goal-conditioned visual planning. Through task-interleaved feature learning, Aether achieves synergistic knowledge sharing across reconstruction, prediction, and planning objectives. Building upon video generation models, our framework demonstrates unprecedented synthetic-to-real generalization despite never observing real-world data during training. Furthermore, our approach achieves zero-shot generalization in both action following and reconstruction tasks, thanks to its intrinsic geometric modeling. Remarkably, even without real-world data, its reconstruction performance far exceeds that of domain-specific models. Additionally, Aether leverages a geometry-informed action space to seamlessly translate predictions into actions, enabling effective autonomous trajectory planning. We hope our work inspires the community to explore new frontiers in physically-reasonable world modeling and its applications.', 'score': 21, 'issue_id': 2877, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 24', 'zh': '3æœˆ24æ—¥'}, 'hash': 'e9f70faf8bc6d0d0', 'authors': ['Aether Team', 'Haoyi Zhu', 'Yifan Wang', 'Jianjun Zhou', 'Wenzheng Chang', 'Yang Zhou', 'Zizun Li', 'Junyi Chen', 'Chunhua Shen', 'Jiangmiao Pang', 'Tong He'], 'affiliations': ['Shanghai AI Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2503.18945.jpg', 'data': {'categories': ['#video', '#3d', '#reasoning', '#agents', '#synthetic'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¸Ñ€Ğ° Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Aether - ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸-Ğ¾ÑĞ¾Ğ·Ğ½Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¼Ğ¸Ñ€Ğ°. Aether Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸: 4D Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ, Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ñ†ĞµĞ»ĞµĞ¹. Ğ‘Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡, ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ±ĞµÑĞ¿Ñ€ĞµÑ†ĞµĞ´ĞµĞ½Ñ‚Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ. Aether Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑĞ¼ Ğ¸ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸.'}, 'en': {'title': 'Aether: Bridging Geometry and Generative Modeling for AI Spatial Reasoning', 'desc': "This paper introduces Aether, a framework that combines geometric reconstruction with generative modeling to enhance AI's spatial reasoning abilities. Aether optimizes three main functions: dynamic 4D reconstruction, action-based video prediction, and goal-oriented visual planning. By using task-interleaved feature learning, it allows for effective knowledge sharing among these functions, leading to improved performance. Notably, Aether achieves strong generalization to real-world scenarios without ever training on real data, showcasing its potential for autonomous trajectory planning and physical world modeling."}, 'zh': {'title': 'Aetherï¼šå®ç°å‡ ä½•æ„ŸçŸ¥æ¨ç†çš„ç»Ÿä¸€æ¡†æ¶', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†Aetheræ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å‡ ä½•é‡å»ºä¸ç”Ÿæˆå»ºæ¨¡çš„æ•´åˆé—®é¢˜ï¼Œä»¥å®ç°ç±»äººç©ºé—´æ¨ç†ã€‚Aetheré€šè¿‡è”åˆä¼˜åŒ–å››ä¸ªæ ¸å¿ƒèƒ½åŠ›ï¼ŒåŒ…æ‹¬4DåŠ¨æ€é‡å»ºã€åŸºäºåŠ¨ä½œçš„è§†é¢‘é¢„æµ‹å’ŒåŸºäºç›®æ ‡çš„è§†è§‰è§„åˆ’ï¼Œæ¥å®ç°å‡ ä½•æ„ŸçŸ¥æ¨ç†ã€‚è¯¥æ¡†æ¶é€šè¿‡ä»»åŠ¡äº¤é”™ç‰¹å¾å­¦ä¹ ï¼Œä¿ƒè¿›äº†é‡å»ºã€é¢„æµ‹å’Œè§„åˆ’ç›®æ ‡ä¹‹é—´çš„çŸ¥è¯†å…±äº«ã€‚å°½ç®¡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æœªè§‚å¯Ÿåˆ°çœŸå®ä¸–ç•Œæ•°æ®ï¼ŒAetherä»å±•ç°å‡ºå‰æ‰€æœªæœ‰çš„åˆæˆåˆ°çœŸå®çš„æ³›åŒ–èƒ½åŠ›ï¼Œä¸”åœ¨æ— ç›‘ç£æƒ…å†µä¸‹åœ¨åŠ¨ä½œè·Ÿéšå’Œé‡å»ºä»»åŠ¡ä¸­å®ç°äº†é›¶æ ·æœ¬æ³›åŒ–ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.18892', 'title': 'SimpleRL-Zoo: Investigating and Taming Zero Reinforcement Learning for\n  Open Base Models in the Wild', 'url': 'https://huggingface.co/papers/2503.18892', 'abstract': 'DeepSeek-R1 has shown that long chain-of-thought (CoT) reasoning can naturally emerge through a simple reinforcement learning (RL) framework with rule-based rewards, where the training may directly start from the base models-a paradigm referred to as zero RL training. Most recent efforts to reproduce zero RL training have primarily focused on the Qwen2.5 model series, which may not be representative as we find the base models already exhibit strong instruction-following and self-reflection abilities. In this work, we investigate zero RL training across 10 diverse base models, spanning different families and sizes including LLama3-8B, Mistral-7B/24B, DeepSeek-Math-7B, Qwen2.5-math-7B, and all Qwen2.5 models from 0.5B to 32B. Leveraging several key design strategies-such as adjusting format reward and controlling query difficulty-we achieve substantial improvements in both reasoning accuracy and response length across most settings. However, by carefully monitoring the training dynamics, we observe that different base models exhibit distinct patterns during training. For instance, the increased response length does not always correlate with the emergence of certain cognitive behaviors such as verification (i.e., the "aha moment"). Notably, we observe the "aha moment" for the first time in small models not from the Qwen family. We share the key designs that enable successful zero RL training, along with our findings and practices. To facilitate further research, we open-source the code, models, and analysis tools.', 'score': 21, 'issue_id': 2876, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 24', 'zh': '3æœˆ24æ—¥'}, 'hash': '4c0c4ab2292562e4', 'authors': ['Weihao Zeng', 'Yuzhen Huang', 'Qian Liu', 'Wei Liu', 'Keqing He', 'Zejun Ma', 'Junxian He'], 'affiliations': ['BUPT', 'HKUST', 'TikTok'], 'pdf_title_img': 'assets/pdf/title_img/2503.18892.jpg', 'data': {'categories': ['#reasoning', '#open_source', '#rl', '#training', '#small_models'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ‡ĞµÑ€ĞµĞ· RL Ñ Ğ½ÑƒĞ»Ñ', 'desc': 'Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL) Ğ±ĞµĞ· Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ñƒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° 10 Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ LLama3, Mistral, DeepSeek-Math Ğ¸ Qwen2.5. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ° Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ², ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ñƒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². ĞĞ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¿Ñ€Ğ¸Ñ‡ĞµĞ¼ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° Ğ½Ğµ Ğ²ÑĞµĞ³Ğ´Ğ° ĞºĞ¾Ñ€Ñ€ĞµĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ñ Ğ¿Ğ¾ÑĞ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹.'}, 'en': {'title': 'Unlocking Reasoning with Zero RL Training', 'desc': "The paper discusses DeepSeek-R1, which demonstrates that long chain-of-thought reasoning can be developed using a simple reinforcement learning framework with rule-based rewards, starting directly from base models, a method called zero RL training. The authors explore zero RL training across ten different base models, revealing that many of these models already possess strong instruction-following and self-reflection capabilities. They implement design strategies to enhance reasoning accuracy and response length, while also noting that training dynamics vary significantly among models. Importantly, they identify the 'aha moment' in smaller models outside the Qwen family, providing insights and open-sourcing their findings to support further research."}, 'zh': {'title': 'é›¶å¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼šæ¨ç†ä¸åæ€çš„æ–°çªç ´', 'desc': 'DeepSeek-R1å±•ç¤ºäº†é€šè¿‡ç®€å•çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶å’ŒåŸºäºè§„åˆ™çš„å¥–åŠ±ï¼Œé•¿é“¾æ€ç»´æ¨ç†å¯ä»¥è‡ªç„¶å‡ºç°ã€‚è¿™ç§è®­ç»ƒæ–¹æ³•è¢«ç§°ä¸ºé›¶å¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œå…è®¸ç›´æ¥ä»åŸºç¡€æ¨¡å‹å¼€å§‹ã€‚æˆ‘ä»¬ç ”ç©¶äº†10ç§ä¸åŒçš„åŸºç¡€æ¨¡å‹ï¼Œå‘ç°å®ƒä»¬åœ¨æ¨ç†å‡†ç¡®æ€§å’Œå“åº”é•¿åº¦ä¸Šéƒ½æœ‰æ˜¾è‘—æå‡ã€‚æˆ‘ä»¬è¿˜è§‚å¯Ÿåˆ°ï¼Œä¸åŒæ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è¡¨ç°å‡ºä¸åŒçš„æ¨¡å¼ï¼Œç‰¹åˆ«æ˜¯å°æ¨¡å‹é¦–æ¬¡å‡ºç°äº†â€œæç„¶å¤§æ‚Ÿâ€çš„ç°è±¡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.18033', 'title': 'OmnimatteZero: Training-free Real-time Omnimatte with Pre-trained Video\n  Diffusion Models', 'url': 'https://huggingface.co/papers/2503.18033', 'abstract': "Omnimatte aims to decompose a given video into semantically meaningful layers, including the background and individual objects along with their associated effects, such as shadows and reflections. Existing methods often require extensive training or costly self-supervised optimization. In this paper, we present OmnimatteZero, a training-free approach that leverages off-the-shelf pre-trained video diffusion models for omnimatte. It can remove objects from videos, extract individual object layers along with their effects, and composite those objects onto new videos. We accomplish this by adapting zero-shot image inpainting techniques for video object removal, a task they fail to handle effectively out-of-the-box. We then show that self-attention maps capture information about the object and its footprints and use them to inpaint the object's effects, leaving a clean background. Additionally, through simple latent arithmetic, object layers can be isolated and recombined seamlessly with new video layers to produce new videos. Evaluations show that OmnimatteZero not only achieves superior performance in terms of background reconstruction but also sets a new record for the fastest Omnimatte approach, achieving real-time performance with minimal frame runtime.", 'score': 20, 'issue_id': 2881, 'pub_date': '2025-03-23', 'pub_date_card': {'ru': '23 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 23', 'zh': '3æœˆ23æ—¥'}, 'hash': 'd43dc2371ac5a1e8', 'authors': ['Dvir Samuel', 'Matan Levy', 'Nir Darshan', 'Gal Chechik', 'Rami Ben-Ari'], 'affiliations': ['Bar-Ilan University, Ramat-Gan, Israel', 'NVIDIA Research, Tel-Aviv, Israel', 'OriginAI, Tel-Aviv, Israel', 'The Hebrew University of Jerusalem, Jerusalem, Israel'], 'pdf_title_img': 'assets/pdf/title_img/2503.18033.jpg', 'data': {'categories': ['#diffusion', '#cv', '#optimization', '#video'], 'emoji': 'ğŸ¬', 'ru': {'title': 'OmnimatteZero: Ğ‘Ñ‹ÑÑ‚Ñ€Ğ°Ñ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'OmnimatteZero - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ğ¼Ñ‹Ğµ ÑĞ»Ğ¾Ğ¸ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ ÑƒĞ´Ğ°Ğ»ÑÑ‚ÑŒ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹, Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°Ñ‚ÑŒ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑĞ»Ğ¾Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ²Ğ¼ĞµÑÑ‚Ğµ Ñ Ğ¸Ñ… ÑÑ„Ñ„ĞµĞºÑ‚Ğ°Ğ¼Ğ¸ Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ñ… Ğ² Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾. OmnimatteZero Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸ Ğ¸Ğ½Ğ¿ĞµĞ¹Ğ½Ñ‚Ğ¸Ğ½Ğ³Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸Ğ· Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ°Ñ€Ñ‚Ñ‹ ÑĞ°Ğ¼Ğ¾Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¾Ğ² Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ñ„Ğ¾Ğ½Ğ° Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ² Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸.'}, 'en': {'title': 'Revolutionizing Video Editing with Training-Free Layer Decomposition', 'desc': 'OmnimatteZero is a novel approach for video decomposition that separates a video into meaningful layers, such as backgrounds and individual objects, along with their effects like shadows and reflections. Unlike traditional methods that require extensive training, OmnimatteZero utilizes pre-trained video diffusion models, allowing for a training-free solution. It effectively employs zero-shot image inpainting techniques to remove objects and inpaint their effects, ensuring a clean background. The method also allows for the seamless recombination of object layers with new video content, achieving real-time performance and setting a new standard for speed in Omnimatte techniques.'}, 'zh': {'title': 'OmnimatteZeroï¼šæ— è®­ç»ƒçš„è§†é¢‘å¯¹è±¡åˆ†è§£æ–°æ–¹æ³•', 'desc': 'Omnimatteæ—¨åœ¨å°†ç»™å®šè§†é¢‘åˆ†è§£ä¸ºå…·æœ‰è¯­ä¹‰æ„ä¹‰çš„å±‚ï¼ŒåŒ…æ‹¬èƒŒæ™¯å’Œå•ä¸ªå¯¹è±¡åŠå…¶ç›¸å…³æ•ˆæœï¼Œå¦‚é˜´å½±å’Œåå°„ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦å¤§é‡çš„è®­ç»ƒæˆ–æ˜‚è´µçš„è‡ªç›‘ç£ä¼˜åŒ–ã€‚æœ¬æ–‡æå‡ºäº†OmnimatteZeroï¼Œè¿™æ˜¯ä¸€ç§æ— è®­ç»ƒçš„æ–¹æ³•ï¼Œåˆ©ç”¨ç°æˆçš„é¢„è®­ç»ƒè§†é¢‘æ‰©æ•£æ¨¡å‹è¿›è¡Œomnimatteå¤„ç†ã€‚é€šè¿‡é€‚åº”é›¶-shotå›¾åƒä¿®å¤æŠ€æœ¯ï¼ŒOmnimatteZeroèƒ½å¤Ÿæœ‰æ•ˆåœ°ä»è§†é¢‘ä¸­ç§»é™¤å¯¹è±¡ï¼Œæå–å•ä¸ªå¯¹è±¡å±‚åŠå…¶æ•ˆæœï¼Œå¹¶å°†è¿™äº›å¯¹è±¡åˆæˆåˆ°æ–°è§†é¢‘ä¸­ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.17489', 'title': 'Judge Anything: MLLM as a Judge Across Any Modality', 'url': 'https://huggingface.co/papers/2503.17489', 'abstract': 'Evaluating generative foundation models on open-ended multimodal understanding (MMU) and generation (MMG) tasks across diverse modalities (e.g., images, audio, video) poses significant challenges due to the complexity of cross-modal interactions. To this end, the idea of utilizing Multimodal LLMs (MLLMs) as automated judges has emerged, with encouraging results in assessing vision-language understanding tasks. Moving further, this paper extends MLLM-as-a-Judge across modalities to a unified manner by introducing two benchmarks, TaskAnything and JudgeAnything, to respectively evaluate the overall performance and judging capabilities of MLLMs across any-to-any modality tasks. Specifically, TaskAnything evaluates the MMU and MMG capabilities across 15 any-to-any modality categories, employing 1,500 queries curated from well-established benchmarks. Furthermore, JudgeAnything evaluates the judging capabilities of 5 advanced (e.g., GPT-4o and Gemini-2.0-Flash) from the perspectives of Pair Comparison and Score Evaluation, providing a standardized testbed that incorporates human judgments and detailed rubrics. Our extensive experiments reveal that while these MLLMs show promise in assessing MMU (i.e., achieving an average of 66.55% in Pair Comparison setting and 42.79% in Score Evaluation setting), they encounter significant challenges with MMG tasks (i.e., averaging only 53.37% in Pair Comparison setting and 30.05% in Score Evaluation setting), exposing cross-modality biases and hallucination issues. To address this, we present OmniArena, an automated platform for evaluating omni-models and multimodal reward models. Our work highlights the need for fairer evaluation protocols and stronger alignment with human preferences. The source code and dataset are publicly available at: https://urrealhero.github.io/judgeanythingweb/.', 'score': 14, 'issue_id': 2875, 'pub_date': '2025-03-21', 'pub_date_card': {'ru': '21 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 21', 'zh': '3æœˆ21æ—¥'}, 'hash': 'bb040618997e1b0a', 'authors': ['Shu Pu', 'Yaochen Wang', 'Dongping Chen', 'Yuhang Chen', 'Guohao Wang', 'Qi Qin', 'Zhongyi Zhang', 'Zhiyuan Zhang', 'Zetong Zhou', 'Shuang Gong', 'Yi Gui', 'Yao Wan', 'Philip S. Yu'], 'affiliations': ['Huazhong University of Science and Technology', 'University of Illinois Chicago'], 'pdf_title_img': 'assets/pdf/title_img/2503.17489.jpg', 'data': {'categories': ['#multimodal', '#hallucinations', '#benchmark', '#alignment', '#open_source'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: Ğ¾Ñ‚ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸ TaskAnything Ğ¸ JudgeAnything Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. TaskAnything Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ MLLM Ğ² 15 ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸ÑÑ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ 1500 Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². JudgeAnything Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ MLLM Ğ²Ñ‹ÑÑ‚ÑƒĞ¿Ğ°Ñ‚ÑŒ Ğ² Ñ€Ğ¾Ğ»Ğ¸ ÑÑƒĞ´ĞµĞ¹, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ñ Ğ¸Ñ… Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¿Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¸ĞºĞ°Ğ¼ Ğ¿Ğ¾Ğ¿Ğ°Ñ€Ğ½Ğ¾Ğ³Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ğ¸ Ğ±Ğ°Ğ»Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ MLLM Ğ»ÑƒÑ‡ÑˆĞµ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ñ‡ĞµĞ¼ Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ²Ñ‹ÑĞ²Ğ»ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¼ĞµĞ¶Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹.'}, 'en': {'title': 'Enhancing Multimodal Evaluation with MLLMs', 'desc': "This paper discusses the challenges of evaluating generative foundation models in tasks that involve multiple types of data, like images and audio. It introduces Multimodal LLMs (MLLMs) as automated judges to assess these models' understanding and generation capabilities across different modalities. The authors present two benchmarks, TaskAnything and JudgeAnything, to systematically evaluate MLLMs' performance and judging abilities. The findings reveal that while MLLMs perform reasonably well in understanding tasks, they struggle with generation tasks, highlighting the need for improved evaluation methods and alignment with human preferences."}, 'zh': {'title': 'å¤šæ¨¡æ€è¯„ä¼°çš„æ–°è§†è§’', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†åœ¨å¤šæ¨¡æ€ç†è§£ï¼ˆMMUï¼‰å’Œç”Ÿæˆï¼ˆMMGï¼‰ä»»åŠ¡ä¸­è¯„ä¼°ç”ŸæˆåŸºç¡€æ¨¡å‹çš„æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯è·¨æ¨¡æ€äº¤äº’çš„å¤æ‚æ€§ã€‚æˆ‘ä»¬æå‡ºäº†ä½¿ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä½œä¸ºè‡ªåŠ¨è¯„ä¼°è€…çš„æƒ³æ³•ï¼Œå¹¶å¼•å…¥äº†ä¸¤ä¸ªåŸºå‡†ï¼šTaskAnythingå’ŒJudgeAnythingï¼Œåˆ†åˆ«ç”¨äºè¯„ä¼°MLLMsåœ¨ä»»ä½•æ¨¡æ€ä»»åŠ¡ä¸­çš„æ•´ä½“æ€§èƒ½å’Œåˆ¤æ–­èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå°½ç®¡MLLMsåœ¨MMUä»»åŠ¡ä¸­è¡¨ç°å‡ºä¸€å®šçš„æ½œåŠ›ï¼Œä½†åœ¨MMGä»»åŠ¡ä¸­é¢ä¸´æ˜¾è‘—æŒ‘æˆ˜ï¼Œæš´éœ²äº†è·¨æ¨¡æ€åè§å’Œå¹»è§‰é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†OmniArenaï¼Œä¸€ä¸ªç”¨äºè¯„ä¼°å¤šæ¨¡æ€æ¨¡å‹å’Œå¥–åŠ±æ¨¡å‹çš„è‡ªåŠ¨åŒ–å¹³å°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.18813', 'title': 'Defeating Prompt Injections by Design', 'url': 'https://huggingface.co/papers/2503.18813', 'abstract': 'Large Language Models (LLMs) are increasingly deployed in agentic systems that interact with an external environment. However, LLM agents are vulnerable to prompt injection attacks when handling untrusted data. In this paper we propose CaMeL, a robust defense that creates a protective system layer around the LLM, securing it even when underlying models may be susceptible to attacks. To operate, CaMeL explicitly extracts the control and data flows from the (trusted) query; therefore, the untrusted data retrieved by the LLM can never impact the program flow. To further improve security, CaMeL relies on a notion of a capability to prevent the exfiltration of private data over unauthorized data flows. We demonstrate effectiveness of CaMeL by solving 67% of tasks with provable security in AgentDojo [NeurIPS 2024], a recent agentic security benchmark.', 'score': 13, 'issue_id': 2879, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 24', 'zh': '3æœˆ24æ—¥'}, 'hash': 'a86a20fb5877c5ad', 'authors': ['Edoardo Debenedetti', 'Ilia Shumailov', 'Tianqi Fan', 'Jamie Hayes', 'Nicholas Carlini', 'Daniel Fabian', 'Christoph Kern', 'Chongyang Shi', 'Andreas Terzis', 'Florian TramÃ¨r'], 'affiliations': ['ETH Zurich', 'Google', 'Google DeepMind'], 'pdf_title_img': 'assets/pdf/title_img/2503.18813.jpg', 'data': {'categories': ['#inference', '#security', '#agents', '#benchmark'], 'emoji': 'ğŸ›¡ï¸', 'ru': {'title': 'CaMeL: ĞĞ°Ğ´ĞµĞ¶Ğ½Ğ°Ñ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ° LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¾Ñ‚ Ğ°Ñ‚Ğ°Ğº Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ CaMeL - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñ‹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¾Ñ‚ Ğ°Ñ‚Ğ°Ğº Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ². CaMeL ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ½Ñ‹Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½Ñ‹Ğ¹ ÑĞ»Ğ¾Ğ¹ Ğ²Ğ¾ĞºÑ€ÑƒĞ³ LLM, Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°Ñ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Ğ´Ğ¾Ğ²ĞµÑ€ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°. Ğ­Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ½ĞµĞ´Ğ¾Ğ²ĞµÑ€ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñ‹. ĞœĞµÑ‚Ğ¾Ğ´ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ ÑƒÑ‚ĞµÑ‡ĞºĞ¸ Ğ¿Ñ€Ğ¸Ğ²Ğ°Ñ‚Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ CaMeL Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ° Ñ€ĞµÑˆĞµĞ½Ğ¸ĞµĞ¼ 67% Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ğ´Ğ¾ĞºĞ°Ğ·ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ² Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ AgentDojo.'}, 'en': {'title': 'Securing LLMs Against Prompt Injection with CaMeL', 'desc': 'This paper introduces CaMeL, a defense mechanism designed to protect Large Language Models (LLMs) from prompt injection attacks when they interact with untrusted data. CaMeL works by creating a secure layer that separates control and data flows, ensuring that untrusted inputs do not affect the execution of the program. Additionally, it implements a capability system to prevent unauthorized access to private data. The effectiveness of CaMeL is demonstrated through its ability to solve 67% of tasks securely in the AgentDojo benchmark.'}, 'zh': {'title': 'CaMeLï¼šä¿æŠ¤å¤§å‹è¯­è¨€æ¨¡å‹çš„å®‰å…¨é˜²çº¿', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä¸å¤–éƒ¨ç¯å¢ƒäº¤äº’çš„æ™ºèƒ½ç³»ç»Ÿä¸­è¶Šæ¥è¶Šå¤šåœ°è¢«ä½¿ç”¨ã€‚ç„¶è€Œï¼Œå½“å¤„ç†ä¸å¯ä¿¡æ•°æ®æ—¶ï¼ŒLLMä»£ç†å®¹æ˜“å—åˆ°æç¤ºæ³¨å…¥æ”»å‡»ã€‚æœ¬æ–‡æå‡ºäº†CaMeLï¼Œè¿™æ˜¯ä¸€ç§å¼ºå¤§çš„é˜²å¾¡æœºåˆ¶ï¼Œå®ƒåœ¨LLMå‘¨å›´åˆ›å»ºäº†ä¸€ä¸ªä¿æŠ¤ç³»ç»Ÿå±‚ï¼Œå³ä½¿åº•å±‚æ¨¡å‹å¯èƒ½å®¹æ˜“å—åˆ°æ”»å‡»ã€‚CaMeLé€šè¿‡æ˜ç¡®æå–æ§åˆ¶å’Œæ•°æ®æµæ¥æ“ä½œï¼Œä»è€Œç¡®ä¿LLMæ£€ç´¢åˆ°çš„ä¸å¯ä¿¡æ•°æ®ä¸ä¼šå½±å“ç¨‹åºæµç¨‹ï¼Œå¹¶é€šè¿‡èƒ½åŠ›æ¦‚å¿µè¿›ä¸€æ­¥æé«˜å®‰å…¨æ€§ï¼Œé˜²æ­¢ç§å¯†æ•°æ®é€šè¿‡æœªç»æˆæƒçš„æ•°æ®æµæ³„éœ²ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.18102', 'title': 'AgentRxiv: Towards Collaborative Autonomous Research', 'url': 'https://huggingface.co/papers/2503.18102', 'abstract': 'Progress in scientific discovery is rarely the result of a single "Eureka" moment, but is rather the product of hundreds of scientists incrementally working together toward a common goal. While existing agent workflows are capable of producing research autonomously, they do so in isolation, without the ability to continuously improve upon prior research results. To address these challenges, we introduce AgentRxiv-a framework that lets LLM agent laboratories upload and retrieve reports from a shared preprint server in order to collaborate, share insights, and iteratively build on each other\'s research. We task agent laboratories to develop new reasoning and prompting techniques and find that agents with access to their prior research achieve higher performance improvements compared to agents operating in isolation (11.4% relative improvement over baseline on MATH-500). We find that the best performing strategy generalizes to benchmarks in other domains (improving on average by 3.3%). Multiple agent laboratories sharing research through AgentRxiv are able to work together towards a common goal, progressing more rapidly than isolated laboratories, achieving higher overall accuracy (13.7% relative improvement over baseline on MATH-500). These findings suggest that autonomous agents may play a role in designing future AI systems alongside humans. We hope that AgentRxiv allows agents to collaborate toward research goals and enables researchers to accelerate discovery.', 'score': 13, 'issue_id': 2879, 'pub_date': '2025-03-23', 'pub_date_card': {'ru': '23 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 23', 'zh': '3æœˆ23æ—¥'}, 'hash': 'f13710802c9e6fb0', 'authors': ['Samuel Schmidgall', 'Michael Moor'], 'affiliations': ['Department of Biosystems Science & Engineering, ETH Zurich', 'Department of Electrical & Computer Engineering, Johns Hopkins University'], 'pdf_title_img': 'assets/pdf/title_img/2503.18102.jpg', 'data': {'categories': ['#agents', '#math', '#reasoning', '#science', '#benchmark'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞšĞ¾Ğ»Ğ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·ÑƒĞ¼ Ğ˜Ğ˜: AgentRxiv Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ AgentRxiv - Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¾Ğ±Ğ¼ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒÑÑ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°Ğ¼Ğ¸ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±Ñ‰Ğ¸Ğ¹ Ğ¿Ñ€ĞµĞ¿Ñ€Ğ¸Ğ½Ñ‚-ÑĞµÑ€Ğ²ĞµÑ€. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ñ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ¾Ğ¼ Ğº Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¼ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑĞ¼ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¸Ğ·Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸. ĞĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ»Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¾Ğ±Ğ¼ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ñ…ÑÑ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸ Ñ‡ĞµÑ€ĞµĞ· AgentRxiv, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹ Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ³Ğ°Ñ‚ÑŒÑÑ Ğº Ğ¾Ğ±Ñ‰ĞµĞ¹ Ñ†ĞµĞ»Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¸Ğ³Ñ€Ğ°Ñ‚ÑŒ Ñ€Ğ¾Ğ»ÑŒ Ğ² Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ½Ğ°Ñ€ÑĞ´Ñƒ Ñ Ğ»ÑĞ´ÑŒĞ¼Ğ¸.'}, 'en': {'title': 'Collaborative AI: Accelerating Research with AgentRxiv', 'desc': 'The paper introduces AgentRxiv, a collaborative framework for large language model (LLM) agents to share and build upon research findings. Unlike traditional isolated workflows, AgentRxiv allows agents to upload and retrieve reports from a shared preprint server, fostering collaboration and iterative improvement. The study shows that agents utilizing prior research achieve significant performance gains, with a 11.4% relative improvement on the MATH-500 benchmark. This collaborative approach not only enhances individual agent performance but also accelerates overall research progress, suggesting a promising future for autonomous agents in scientific discovery.'}, 'zh': {'title': 'AgentRxivï¼šä¿ƒè¿›ä»£ç†å®éªŒå®¤åä½œçš„ç ”ç©¶æ¡†æ¶', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†AgentRxivï¼Œä¸€ä¸ªæ¡†æ¶ä½¿å¾—å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†å®éªŒå®¤èƒ½å¤Ÿä¸Šä¼ å’Œæ£€ç´¢å…±äº«çš„é¢„å°æœ¬æŠ¥å‘Šï¼Œä»è€Œå®ç°åˆä½œå’ŒçŸ¥è¯†å…±äº«ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œä»£ç†å®éªŒå®¤å¯ä»¥åœ¨å½¼æ­¤çš„ç ”ç©¶åŸºç¡€ä¸Šè¿›è¡Œè¿­ä»£ï¼Œæå‡ç ”ç©¶æˆæœã€‚ç ”ç©¶è¡¨æ˜ï¼Œèƒ½å¤Ÿè®¿é—®å…ˆå‰ç ”ç©¶çš„ä»£ç†åœ¨æ€§èƒ½ä¸Šæœ‰æ˜¾è‘—æå‡ï¼Œç›¸æ¯”äºå­¤ç«‹æ“ä½œçš„ä»£ç†ï¼Œæå‡è¾¾åˆ°äº†11.4%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œå¤šä¸ªä»£ç†å®éªŒå®¤é€šè¿‡AgentRxivåˆä½œï¼Œå¯ä»¥æ›´å¿«åœ°æœç€å…±åŒç›®æ ‡å‰è¿›ï¼Œå¹¶åœ¨æ•´ä½“å‡†ç¡®æ€§ä¸Šå®ç°æ›´é«˜çš„æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.17439', 'title': 'LEMMA: Learning from Errors for MatheMatical Advancement in LLMs', 'url': 'https://huggingface.co/papers/2503.17439', 'abstract': "Large language models (LLMs) have demonstrated remarkable reasoning capability in solving mathematical problems. However, existing approaches primarily focus on improving the quality of correct training data, e.g., distilling high-quality correct solutions from advanced models, neglecting the value contained in error data, potentially hindering the model's reflective ability. Though some studies attempt to leverage error data, they often involve complex mechanisms, such as Monte Carlo Tree Search (MCTS) to explore error nodes. In this work, we propose to enhance LLMs' reasoning ability by Learning from Errors for Mathematical Advancement (LEMMA). LEMMA constructs data consisting of an incorrect solution with an erroneous step and a reflection connection to a correct solution for fine-tuning. Specifically, we systematically analyze the model-generated error types and introduce an error-type grounded mistake augmentation method to collect diverse and representative errors. Correct solutions are either from fixing the errors or generating a fresh start. Through a model-aware smooth reflection connection, the erroneous solution is transferred to the correct one. By fine-tuning on the constructed dataset, the model is able to self-correct errors autonomously within the generation process without relying on external critique models. Experimental results demonstrate that LEMMA achieves significant performance improvements over other strong baselines.", 'score': 13, 'issue_id': 2875, 'pub_date': '2025-03-21', 'pub_date_card': {'ru': '21 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 21', 'zh': '3æœˆ21æ—¥'}, 'hash': '946d486485fedb03', 'authors': ['Zhuoshi Pan', 'Yu Li', 'Honglin Lin', 'Qizhi Pei', 'Zinan Tang', 'Wei Wu', 'Chenlin Ming', 'H. Vicky Zhao', 'Conghui He', 'Lijun Wu'], 'affiliations': ['Renmin University of China', 'Shanghai AI Laboratory', 'Shanghai Jiao Tong University', 'Tsinghua University', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2503.17439.jpg', 'data': {'categories': ['#math', '#training', '#reasoning', '#dataset', '#data'], 'emoji': 'ğŸ§®', 'ru': {'title': 'Ğ£Ñ‡Ğ¸Ğ¼ÑÑ Ğ½Ğ° Ğ¾ÑˆĞ¸Ğ±ĞºĞ°Ñ…: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ˜Ğ˜', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ LEMMA Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ñ€ĞµÑˆĞ°Ñ‚ÑŒ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑˆĞ¸Ğ±ĞºĞ°Ñ…. LEMMA ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑĞ¾ÑÑ‚Ğ¾ÑÑ‰Ğ¸Ğ¹ Ğ¸Ğ· Ğ½ĞµĞ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ ÑˆĞ°Ğ³Ğ°Ğ¼Ğ¸ Ğ¸ ÑĞ²ÑĞ·ÑĞ¼Ğ¸ Ñ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ´Ğ»Ñ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ´Ğ»Ñ ÑĞ±Ğ¾Ñ€Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¸ Ñ€ĞµĞ¿Ñ€ĞµĞ·ĞµĞ½Ñ‚Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ LEMMA Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼Ğ¸ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸.'}, 'en': {'title': 'Empowering LLMs: Learning from Errors to Enhance Reasoning', 'desc': 'This paper introduces a novel approach called Learning from Errors for Mathematical Advancement (LEMMA) to improve the reasoning capabilities of large language models (LLMs) in solving mathematical problems. Unlike traditional methods that focus solely on enhancing correct training data, LEMMA leverages the value of error data by constructing a dataset that includes incorrect solutions paired with reflections on correct solutions. The method systematically analyzes error types and employs a mistake augmentation technique to gather diverse errors, allowing the model to learn from its mistakes. By fine-tuning on this enriched dataset, LEMMA enables LLMs to autonomously correct their errors during the generation process, leading to significant performance gains compared to existing methods.'}, 'zh': {'title': 'ä»é”™è¯¯ä¸­å­¦ä¹ ï¼Œæå‡æ•°å­¦æ¨ç†èƒ½åŠ›', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è§£å†³æ•°å­¦é—®é¢˜æ—¶å±•ç°äº†å‡ºè‰²çš„æ¨ç†èƒ½åŠ›ã€‚ç°æœ‰çš„æ–¹æ³•ä¸»è¦å…³æ³¨æé«˜æ­£ç¡®è®­ç»ƒæ•°æ®çš„è´¨é‡ï¼Œè€Œå¿½è§†äº†é”™è¯¯æ•°æ®çš„ä»·å€¼ï¼Œè¿™å¯èƒ½ä¼šå¦¨ç¢æ¨¡å‹çš„åæ€èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§é€šè¿‡å­¦ä¹ é”™è¯¯æ¥æå‡æ•°å­¦æ¨ç†èƒ½åŠ›çš„æ–¹æ³•ï¼Œç§°ä¸ºLEMMAã€‚è¯¥æ–¹æ³•é€šè¿‡æ„å»ºåŒ…å«é”™è¯¯æ­¥éª¤çš„é”™è¯¯è§£å’Œä¸æ­£ç¡®è§£çš„åæ€è¿æ¥çš„æ•°æ®é›†ï¼Œæ¥è¿›è¡Œæ¨¡å‹çš„å¾®è°ƒï¼Œä»è€Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­è‡ªä¸»çº æ­£é”™è¯¯ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.18013', 'title': 'Vision-R1: Evolving Human-Free Alignment in Large Vision-Language Models\n  via Vision-Guided Reinforcement Learning', 'url': 'https://huggingface.co/papers/2503.18013', 'abstract': 'Large Vision-Language Models (LVLMs) typically follow a two-stage training paradigm-pretraining and supervised fine-tuning. Recently, preference optimization, derived from the language domain, has emerged as an effective post-training reinforcement strategy to enhance capabilities of LVLMs. However, constructing high-quality human-annotated preference data and developing robust reward models to mimic these preferences are both costly and challenging. Motivated by this observation, we propose Vision-R1, a novel vision-guided R1-like reinforcement learning algorithm for LVLMs that rewards models with definitive vision feedback. It only leverages curated instruction data, eliminating the need for specialized reward models and handcrafted preference datasets. We incorporate a criterion-driven reward function that further integrates multi-dimensional feedback to evaluate model completions comprehensively based on the vision task logic. Furthermore, we introduce a progressive rule refinement strategy that dynamically adjusts the reward criteria during training, enabling continuous model improvement and mitigating reward hacking. Extensive experiments on both in-distribution and out-of-distribution benchmarks demonstrate that fine-tuning the 7B LVLMs with Vision-R1 achieves consistent performance gains, with even up to 50% improvement and surpassing the state-of-the-art 10x size model.', 'score': 12, 'issue_id': 2876, 'pub_date': '2025-03-23', 'pub_date_card': {'ru': '23 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 23', 'zh': '3æœˆ23æ—¥'}, 'hash': '45029d297f1b8ac9', 'authors': ['Yufei Zhan', 'Yousong Zhu', 'Shurong Zheng', 'Hongyin Zhao', 'Fan Yang', 'Ming Tang', 'Jinqiao Wang'], 'affiliations': ['Foundation Model Research Center, Institute of Automation, Chinese Academy of Sciences, Beijing, China', 'Peng Cheng Laboratory, Shenzhen, China', 'School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China', 'Wuhan AI Research, Wuhan, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.18013.jpg', 'data': {'categories': ['#optimization', '#rl', '#training', '#benchmark', '#rlhf'], 'emoji': 'ğŸ‘ï¸', 'ru': {'title': 'Vision-R1: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Vision-R1. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒÑ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑÑ… Ğ¸Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ. Vision-R1 Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸ĞµĞ² Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ». Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Vision-R1, Ğ² Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… ÑĞ»ÑƒÑ‡Ğ°ÑÑ… Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‰ĞµĞµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² 10 Ñ€Ğ°Ğ· Ğ±Ğ¾Ğ»ÑŒÑˆĞµĞ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°.'}, 'en': {'title': 'Reinforcing Vision with Vision-R1: Simplifying LVLM Training', 'desc': 'This paper introduces Vision-R1, a new reinforcement learning algorithm designed for Large Vision-Language Models (LVLMs). Unlike traditional methods that require expensive human-annotated preference data, Vision-R1 uses curated instruction data to provide vision feedback directly to the models. The algorithm employs a criterion-driven reward function that assesses model outputs based on the logic of vision tasks, allowing for a more comprehensive evaluation. Additionally, it features a progressive rule refinement strategy that adapts reward criteria during training, leading to significant performance improvements in LVLMs without the need for complex reward models.'}, 'zh': {'title': 'è§†è§‰å¼•å¯¼çš„å¼ºåŒ–å­¦ä¹ æå‡æ¨¡å‹èƒ½åŠ›', 'desc': 'å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰é€šå¸¸é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ³•ï¼šé¢„è®­ç»ƒå’Œç›‘ç£å¾®è°ƒã€‚æœ€è¿‘ï¼Œæºè‡ªè¯­è¨€é¢†åŸŸçš„åå¥½ä¼˜åŒ–æˆä¸ºä¸€ç§æœ‰æ•ˆçš„åè®­ç»ƒå¼ºåŒ–ç­–ç•¥ï¼Œç”¨äºæå‡LVLMsçš„èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„è§†è§‰å¼•å¯¼R1ç±»å¼ºåŒ–å­¦ä¹ ç®—æ³•Vision-R1ï¼Œå®ƒé€šè¿‡æ˜ç¡®çš„è§†è§‰åé¦ˆæ¥å¥–åŠ±æ¨¡å‹ï¼Œé¿å…äº†æ„å»ºé«˜è´¨é‡äººç±»æ ‡æ³¨çš„åå¥½æ•°æ®å’Œå¼€å‘å¤æ‚çš„å¥–åŠ±æ¨¡å‹çš„é«˜æˆæœ¬ã€‚é€šè¿‡å¼•å…¥å¤šç»´åé¦ˆçš„æ ‡å‡†é©±åŠ¨å¥–åŠ±å‡½æ•°ï¼ŒVision-R1èƒ½å¤Ÿå…¨é¢è¯„ä¼°æ¨¡å‹çš„å®Œæˆæƒ…å†µï¼Œå¹¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŠ¨æ€è°ƒæ•´å¥–åŠ±æ ‡å‡†ï¼Œä»è€Œå®ç°æŒç»­çš„æ¨¡å‹æ”¹è¿›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.18948', 'title': 'Equivariant Image Modeling', 'url': 'https://huggingface.co/papers/2503.18948', 'abstract': 'Current generative models, such as autoregressive and diffusion approaches, decompose high-dimensional data distribution learning into a series of simpler subtasks. However, inherent conflicts arise during the joint optimization of these subtasks, and existing solutions fail to resolve such conflicts without sacrificing efficiency or scalability. We propose a novel equivariant image modeling framework that inherently aligns optimization targets across subtasks by leveraging the translation invariance of natural visual signals. Our method introduces (1) column-wise tokenization which enhances translational symmetry along the horizontal axis, and (2) windowed causal attention which enforces consistent contextual relationships across positions. Evaluated on class-conditioned ImageNet generation at 256x256 resolution, our approach achieves performance comparable to state-of-the-art AR models while using fewer computational resources. Systematic analysis demonstrates that enhanced equivariance reduces inter-task conflicts, significantly improving zero-shot generalization and enabling ultra-long image synthesis. This work establishes the first framework for task-aligned decomposition in generative modeling, offering insights into efficient parameter sharing and conflict-free optimization. The code and models are publicly available at https://github.com/drx-code/EquivariantModeling.', 'score': 11, 'issue_id': 2880, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 24', 'zh': '3æœˆ24æ—¥'}, 'hash': 'a098ae2b5e7412ee', 'authors': ['Ruixiao Dong', 'Mengde Xu', 'Zigang Geng', 'Li Li', 'Han Hu', 'Shuyang Gu'], 'affiliations': ['Tencent Hunyuan Research', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2503.18948.jpg', 'data': {'categories': ['#architecture', '#optimization', '#diffusion', '#training', '#open_source', '#cv'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'Ğ­ĞºĞ²Ğ¸Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğµ ÑĞºĞ²Ğ¸Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ ÑÑ‚Ğ¾Ğ»Ğ±Ñ†Ğ°Ğ¼ Ğ¸ Ğ¾ĞºĞ¾Ğ½Ğ½Ğ¾Ğµ ĞºĞ°ÑƒĞ·Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ñ€Ğ°Ğ½ÑĞ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° ImageNet Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ…. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½ÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° ÑĞ²ĞµÑ€Ñ…Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Aligning Tasks for Efficient Generative Modeling', 'desc': 'This paper introduces a new framework for generative modeling that addresses conflicts in optimizing multiple subtasks. By utilizing the natural translation invariance found in visual data, the proposed method aligns optimization targets, leading to improved efficiency. Key innovations include column-wise tokenization for better symmetry and windowed causal attention to maintain contextual relationships. The framework shows competitive performance in image generation while reducing computational demands, enhancing generalization, and enabling longer image synthesis.'}, 'zh': {'title': 'ä»»åŠ¡å¯¹é½çš„ç”Ÿæˆå»ºæ¨¡æ–°æ¡†æ¶', 'desc': 'å½“å‰çš„ç”Ÿæˆæ¨¡å‹ï¼Œå¦‚è‡ªå›å½’å’Œæ‰©æ•£æ–¹æ³•ï¼Œå°†é«˜ç»´æ•°æ®åˆ†å¸ƒå­¦ä¹ åˆ†è§£ä¸ºä¸€ç³»åˆ—æ›´ç®€å•çš„å­ä»»åŠ¡ã€‚ç„¶è€Œï¼Œåœ¨è¿™äº›å­ä»»åŠ¡çš„è”åˆä¼˜åŒ–è¿‡ç¨‹ä¸­ä¼šå‡ºç°å›ºæœ‰çš„å†²çªï¼Œç°æœ‰çš„è§£å†³æ–¹æ¡ˆæ— æ³•åœ¨ä¸ç‰ºç‰²æ•ˆç‡æˆ–å¯æ‰©å±•æ€§çš„æƒ…å†µä¸‹è§£å†³è¿™äº›å†²çªã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„ç­‰å˜å›¾åƒå»ºæ¨¡æ¡†æ¶ï¼Œé€šè¿‡åˆ©ç”¨è‡ªç„¶è§†è§‰ä¿¡å·çš„å¹³ç§»ä¸å˜æ€§ï¼Œå†…åœ¨åœ°å¯¹é½å­ä»»åŠ¡ä¹‹é—´çš„ä¼˜åŒ–ç›®æ ‡ã€‚æˆ‘ä»¬çš„ç ”ç©¶åœ¨256x256åˆ†è¾¨ç‡ä¸‹çš„ç±»æ¡ä»¶ImageNetç”Ÿæˆä¸­è¡¨ç°å‡ºä¸æœ€å…ˆè¿›çš„è‡ªå›å½’æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ï¼ŒåŒæ—¶ä½¿ç”¨æ›´å°‘çš„è®¡ç®—èµ„æºã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.18940', 'title': 'Training-free Diffusion Acceleration with Bottleneck Sampling', 'url': 'https://huggingface.co/papers/2503.18940', 'abstract': 'Diffusion models have demonstrated remarkable capabilities in visual content generation but remain challenging to deploy due to their high computational cost during inference. This computational burden primarily arises from the quadratic complexity of self-attention with respect to image or video resolution. While existing acceleration methods often compromise output quality or necessitate costly retraining, we observe that most diffusion models are pre-trained at lower resolutions, presenting an opportunity to exploit these low-resolution priors for more efficient inference without degrading performance. In this work, we introduce Bottleneck Sampling, a training-free framework that leverages low-resolution priors to reduce computational overhead while preserving output fidelity. Bottleneck Sampling follows a high-low-high denoising workflow: it performs high-resolution denoising in the initial and final stages while operating at lower resolutions in intermediate steps. To mitigate aliasing and blurring artifacts, we further refine the resolution transition points and adaptively shift the denoising timesteps at each stage. We evaluate Bottleneck Sampling on both image and video generation tasks, where extensive experiments demonstrate that it accelerates inference by up to 3times for image generation and 2.5times for video generation, all while maintaining output quality comparable to the standard full-resolution sampling process across multiple evaluation metrics. Code is available at: https://github.com/tyfeld/Bottleneck-Sampling', 'score': 11, 'issue_id': 2875, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 24', 'zh': '3æœˆ24æ—¥'}, 'hash': '83ffcf1c20f5d4db', 'authors': ['Ye Tian', 'Xin Xia', 'Yuxi Ren', 'Shanchuan Lin', 'Xing Wang', 'Xuefeng Xiao', 'Yunhai Tong', 'Ling Yang', 'Bin Cui'], 'affiliations': ['Bytedance', 'Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2503.18940.jpg', 'data': {'categories': ['#cv', '#optimization', '#diffusion', '#inference', '#video'], 'emoji': 'â±ï¸', 'ru': {'title': 'Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Bottleneck Sampling. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞ°ÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞœĞµÑ‚Ğ¾Ğ´ ÑĞ»ĞµĞ´ÑƒĞµÑ‚ ÑÑ…ĞµĞ¼Ğµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ-Ğ½Ğ¸Ğ·ĞºĞ¾Ğµ-Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸ Ğ´ĞµĞ½Ğ¾Ğ¹Ğ·Ğ¸Ğ½Ğ³Ğµ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒÑĞºĞ¾Ñ€Ğ¸Ñ‚ÑŒ Ğ²Ñ‹Ğ²Ğ¾Ğ´ Ğ² 2.5-3 Ñ€Ğ°Ğ·Ğ° Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. Bottleneck Sampling Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞ°ÑÑ‰ĞµĞ³Ğ¾ ÑĞµĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'Speeding Up Diffusion Models with Bottleneck Sampling', 'desc': 'This paper presents Bottleneck Sampling, a new method to speed up diffusion models used for generating images and videos. Traditional diffusion models are slow because they use a complex self-attention mechanism that increases with image resolution. Bottleneck Sampling takes advantage of low-resolution training data to reduce the computational load during inference without sacrificing quality. By using a high-low-high denoising approach, it achieves significant speed improvementsâ€”up to 3 times faster for images and 2.5 times for videosâ€”while still producing high-quality outputs.'}, 'zh': {'title': 'ç“¶é¢ˆé‡‡æ ·ï¼šé«˜æ•ˆçš„æ‰©æ•£æ¨¡å‹æ¨ç†', 'desc': 'æ‰©æ•£æ¨¡å‹åœ¨è§†è§‰å†…å®¹ç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨æ¨ç†æ—¶ç”±äºè®¡ç®—æˆæœ¬é«˜è€Œéš¾ä»¥éƒ¨ç½²ã€‚ä¸»è¦çš„è®¡ç®—è´Ÿæ‹…æ¥è‡ªäºè‡ªæ³¨æ„åŠ›æœºåˆ¶åœ¨å›¾åƒæˆ–è§†é¢‘åˆ†è¾¨ç‡ä¸Šçš„äºŒæ¬¡å¤æ‚æ€§ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºç“¶é¢ˆé‡‡æ ·çš„æ¡†æ¶ï¼Œåˆ©ç”¨ä½åˆ†è¾¨ç‡çš„å…ˆéªŒçŸ¥è¯†æ¥å‡å°‘è®¡ç®—å¼€é”€ï¼ŒåŒæ—¶ä¿æŒè¾“å‡ºè´¨é‡ã€‚é€šè¿‡åœ¨é«˜åˆ†è¾¨ç‡å’Œä½åˆ†è¾¨ç‡ä¹‹é—´è¿›è¡Œé«˜ä½é«˜çš„å»å™ªå·¥ä½œæµç¨‹ï¼Œæˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å›¾åƒç”Ÿæˆä¸­åŠ é€Ÿæ¨ç†é€Ÿåº¦å¯è¾¾3å€ï¼Œåœ¨è§†é¢‘ç”Ÿæˆä¸­å¯è¾¾2.5å€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.18908', 'title': 'FFN Fusion: Rethinking Sequential Computation in Large Language Models', 'url': 'https://huggingface.co/papers/2503.18908', 'abstract': 'We introduce FFN Fusion, an architectural optimization technique that reduces sequential computation in large language models by identifying and exploiting natural opportunities for parallelization. Our key insight is that sequences of Feed-Forward Network (FFN) layers, particularly those remaining after the removal of specific attention layers, can often be parallelized with minimal accuracy impact. We develop a principled methodology for identifying and fusing such sequences, transforming them into parallel operations that significantly reduce inference latency while preserving model behavior. Applying these techniques to Llama-3.1-405B-Instruct, we create Llama-Nemotron-Ultra-253B-Base (Ultra-253B-Base), an efficient and soon-to-be publicly available model that achieves a 1.71X speedup in inference latency and 35X lower per-token cost while maintaining strong performance across benchmarks. Through extensive experiments on models from 49B to 253B parameters, we demonstrate that FFN Fusion becomes increasingly effective at larger scales and can complement existing optimization techniques like quantization and pruning. Most intriguingly, we find that even full transformer blocks containing both attention and FFN layers can sometimes be parallelized, suggesting new directions for neural architecture design.', 'score': 11, 'issue_id': 2877, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 24', 'zh': '3æœˆ24æ—¥'}, 'hash': '77fd55a50b93d2d4', 'authors': ['Akhiad Bercovich', 'Mohammad Dabbah', 'Omri Puny', 'Ido Galil', 'Amnon Geifman', 'Yonatan Geifman', 'Izhak Golan', 'Ehud Karpas', 'Itay Levy', 'Zach Moshe', 'Najeeb Nabwani', 'Tomer Ronen', 'Itamar Schen', 'Elad Segal', 'Ido Shahaf', 'Oren Tropp', 'Ran Zilberstein', 'Ran El-Yaniv'], 'affiliations': ['NVIDIA'], 'pdf_title_img': 'assets/pdf/title_img/2503.18908.jpg', 'data': {'categories': ['#optimization', '#architecture', '#training', '#inference'], 'emoji': 'ğŸš€', 'ru': {'title': 'Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ FFN ÑĞ»Ğ¾ĞµĞ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ FFN Fusion, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑĞ»Ğ¾ĞµĞ² Feed-Forward Network (FFN), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ñ€Ğ°ÑĞ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»Ğ¸Ñ‚ÑŒ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ¾Ğ¹ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Llama-3.1-405B-Instruct Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ Llama-Nemotron-Ultra-253B-Base, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‰ÑƒÑ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ² 1.71 Ñ€Ğ°Ğ·Ğ° Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ FFN Fusion Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²ĞµĞ½ Ğ´Ğ»Ñ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑĞ¾Ñ‡ĞµÑ‚Ğ°Ñ‚ÑŒÑÑ Ñ Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğ¼Ğ¸ ĞºĞ°Ğº ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ¿Ñ€ÑƒĞ½Ğ¸Ğ½Ğ³.'}, 'en': {'title': 'Accelerating Language Models with FFN Fusion', 'desc': 'The paper presents FFN Fusion, a technique that optimizes large language models by enabling parallel computation of Feed-Forward Network (FFN) layers. This method identifies sequences of FFN layers that can be fused and executed in parallel, which reduces the time it takes for the model to make predictions without sacrificing accuracy. The authors demonstrate this approach on the Llama-3.1-405B-Instruct model, resulting in a new model, Llama-Nemotron-Ultra-253B-Base, that is significantly faster and cheaper to run. The findings suggest that FFN Fusion is particularly beneficial for larger models and can work alongside other optimization methods like quantization and pruning.'}, 'zh': {'title': 'FFN Fusionï¼šæå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†æ•ˆç‡', 'desc': 'æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºFFN Fusionçš„æ¶æ„ä¼˜åŒ–æŠ€æœ¯ï¼Œæ—¨åœ¨é€šè¿‡è¯†åˆ«å’Œåˆ©ç”¨å¹¶è¡ŒåŒ–çš„è‡ªç„¶æœºä¼šæ¥å‡å°‘å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„é¡ºåºè®¡ç®—ã€‚æˆ‘ä»¬çš„å…³é”®è§è§£æ˜¯ï¼Œå»é™¤ç‰¹å®šæ³¨æ„åŠ›å±‚åï¼Œå‰é¦ˆç½‘ç»œï¼ˆFFNï¼‰å±‚çš„åºåˆ—é€šå¸¸å¯ä»¥ä»¥æœ€å°çš„å‡†ç¡®æ€§å½±å“è¿›è¡Œå¹¶è¡ŒåŒ–ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ç§åŸåˆ™æ€§çš„æ–¹æ³•æ¥è¯†åˆ«å’Œèåˆè¿™äº›åºåˆ—ï¼Œå°†å…¶è½¬åŒ–ä¸ºå¹¶è¡Œæ“ä½œï¼Œä»è€Œæ˜¾è‘—é™ä½æ¨ç†å»¶è¿Ÿï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹è¡Œä¸ºã€‚é€šè¿‡å°†è¿™äº›æŠ€æœ¯åº”ç”¨äºLlama-3.1-405B-Instructï¼Œæˆ‘ä»¬åˆ›å»ºäº†Llama-Nemotron-Ultra-253B-Baseï¼ˆUltra-253B-Baseï¼‰ï¼Œè¯¥æ¨¡å‹åœ¨æ¨ç†å»¶è¿Ÿä¸Šå®ç°äº†1.71å€çš„åŠ é€Ÿï¼Œå¹¶ä¸”æ¯ä¸ªtokençš„æˆæœ¬é™ä½äº†35å€ï¼ŒåŒæ—¶åœ¨åŸºå‡†æµ‹è¯•ä¸­ä¿æŒäº†å¼ºåŠ²çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.18886', 'title': 'CFG-Zero*: Improved Classifier-Free Guidance for Flow Matching Models', 'url': 'https://huggingface.co/papers/2503.18886', 'abstract': 'Classifier-Free Guidance (CFG) is a widely adopted technique in diffusion/flow models to improve image fidelity and controllability. In this work, we first analytically study the effect of CFG on flow matching models trained on Gaussian mixtures where the ground-truth flow can be derived. We observe that in the early stages of training, when the flow estimation is inaccurate, CFG directs samples toward incorrect trajectories. Building on this observation, we propose CFG-Zero*, an improved CFG with two contributions: (a) optimized scale, where a scalar is optimized to correct for the inaccuracies in the estimated velocity, hence the * in the name; and (b) zero-init, which involves zeroing out the first few steps of the ODE solver. Experiments on both text-to-image (Lumina-Next, Stable Diffusion 3, and Flux) and text-to-video (Wan-2.1) generation demonstrate that CFG-Zero* consistently outperforms CFG, highlighting its effectiveness in guiding Flow Matching models. (Code is available at github.com/WeichenFan/CFG-Zero-star)', 'score': 11, 'issue_id': 2880, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 24', 'zh': '3æœˆ24æ—¥'}, 'hash': '1e2a721645115955', 'authors': ['Weichen Fan', 'Amber Yijia Zheng', 'Raymond A. Yeh', 'Ziwei Liu'], 'affiliations': ['Department of Computer Science, Purdue University', 'S-Lab, Nanyang Technological University'], 'pdf_title_img': 'assets/pdf/title_img/2503.18886.jpg', 'data': {'categories': ['#architecture', '#optimization', '#diffusion', '#video', '#training', '#cv'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'CFG-Zero*: ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Classifier-Free Guidance (CFG) Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ¸ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¸Ğ·ÑƒÑ‡Ğ¸Ğ»Ğ¸ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ CFG Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ², Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹Ñ… ÑĞ¼ĞµÑÑÑ…. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğ¹ Ğ¾Ğ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ CFG-Zero* Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ½ÑƒĞ»ÑĞ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ CFG-Zero* Ğ½Ğ°Ğ´ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¼ CFG.'}, 'en': {'title': 'Enhancing Image Generation with CFG-Zero*', 'desc': 'This paper explores the limitations of Classifier-Free Guidance (CFG) in flow matching models, particularly during the early training phases when flow estimations are often inaccurate. The authors introduce CFG-Zero*, which enhances CFG by optimizing a scalar to adjust for these inaccuracies and implementing a zero-initialization strategy for the initial steps of the ODE solver. Through experiments in both text-to-image and text-to-video generation, CFG-Zero* shows significant improvements over traditional CFG in terms of image fidelity and controllability. The findings suggest that these modifications lead to better guidance for flow matching models, making them more effective in generating high-quality outputs.'}, 'zh': {'title': 'CFG-Zero*: æå‡æµæ¨¡å‹çš„å¼•å¯¼æ•ˆæœ', 'desc': 'æœ¬æ–‡ç ”ç©¶äº†æ— åˆ†ç±»å™¨å¼•å¯¼ï¼ˆCFGï¼‰åœ¨æ‰©æ•£/æµæ¨¡å‹ä¸­çš„åº”ç”¨ï¼Œæ—¨åœ¨æé«˜å›¾åƒçš„çœŸå®æ„Ÿå’Œå¯æ§æ€§ã€‚æˆ‘ä»¬å‘ç°ï¼Œåœ¨è®­ç»ƒçš„æ—©æœŸé˜¶æ®µï¼Œæµä¼°è®¡ä¸å‡†ç¡®æ—¶ï¼ŒCFGä¼šå°†æ ·æœ¬å¼•å¯¼åˆ°é”™è¯¯çš„è½¨è¿¹ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†CFG-Zero*ï¼Œå®ƒé€šè¿‡ä¼˜åŒ–æ¯”ä¾‹å’Œé›¶åˆå§‹åŒ–æ¥æ”¹è¿›CFGã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCFG-Zero*åœ¨æ–‡æœ¬åˆ°å›¾åƒå’Œæ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸­å‡ä¼˜äºä¼ ç»Ÿçš„CFGï¼Œè¯æ˜äº†å…¶åœ¨å¼•å¯¼æµåŒ¹é…æ¨¡å‹æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.18923', 'title': 'Video SimpleQA: Towards Factuality Evaluation in Large Video Language\n  Models', 'url': 'https://huggingface.co/papers/2503.18923', 'abstract': 'Recent advancements in Large Video Language Models (LVLMs) have highlighted their potential for multi-modal understanding, yet evaluating their factual grounding in video contexts remains a critical unsolved challenge. To address this gap, we introduce Video SimpleQA, the first comprehensive benchmark tailored for factuality evaluation of LVLMs. Our work distinguishes from existing video benchmarks through the following key features: 1) Knowledge required: demanding integration of external knowledge beyond the explicit narrative; 2) Fact-seeking question: targeting objective, undisputed events or relationships, avoiding subjective interpretation; 3) Definitive & short-form answer: Answers are crafted as unambiguous and definitively correct in a short format, enabling automated evaluation through LLM-as-a-judge frameworks with minimal scoring variance; 4) External-source verified: All annotations undergo rigorous validation against authoritative external references to ensure the reliability; 5) Temporal reasoning required: The annotated question types encompass both static single-frame understanding and dynamic temporal reasoning, explicitly evaluating LVLMs factuality under the long-context dependencies. We extensively evaluate 41 state-of-the-art LVLMs and summarize key findings as follows: 1) Current LVLMs exhibit notable deficiencies in factual adherence, particularly for open-source models. The best-performing model Gemini-1.5-Pro achieves merely an F-score of 54.4%; 2) Test-time compute paradigms show insignificant performance gains, revealing fundamental constraints for enhancing factuality through post-hoc computation; 3) Retrieval-Augmented Generation demonstrates consistent improvements at the cost of additional inference time overhead, presenting a critical efficiency-performance trade-off.', 'score': 10, 'issue_id': 2876, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 24', 'zh': '3æœˆ24æ—¥'}, 'hash': '599abe342d833dd0', 'authors': ['Meng Cao', 'Pengfei Hu', 'Yingyao Wang', 'Jihao Gu', 'Haoran Tang', 'Haoze Zhao', 'Jiahua Dong', 'Wangbo Yu', 'Ge Zhang', 'Ian Reid', 'Xiaodan Liang'], 'affiliations': ['Alibaba Group', 'M-A-P', 'MBZUAI', 'Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2503.18923.jpg', 'data': {'categories': ['#video', '#interpretability', '#reasoning', '#long_context', '#multimodal', '#rag', '#benchmark'], 'emoji': 'ğŸ¥', 'ru': {'title': 'ĞĞ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Video SimpleQA - Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ‘Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ’Ğ¸Ğ´ĞµĞ¾-Ğ¯Ğ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… ĞœĞ¾Ğ´ĞµĞ»ĞµĞ¹ (LVLM). Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ°ĞµÑ‚ÑÑ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒÑ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². ĞÑ†ĞµĞ½ĞºĞ° 41 ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ LVLM Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚ĞºĞ¸ Ğ² Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Gemini-1.5-Pro Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ° F-Ğ¼ĞµÑ€Ñ‹ Ğ²ÑĞµĞ³Ğ¾ 54.4%. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ğ¾ÑÑ‚-Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Retrieval-Augmented Generation.'}, 'en': {'title': 'Evaluating Factual Accuracy in Video Language Models', 'desc': 'This paper introduces Video SimpleQA, a new benchmark designed to evaluate the factual accuracy of Large Video Language Models (LVLMs). It focuses on assessing how well these models can integrate external knowledge and answer fact-based questions about video content. The benchmark emphasizes the need for definitive answers and includes rigorous validation against authoritative sources to ensure reliability. The evaluation of 41 LVLMs reveals significant shortcomings in factual adherence, particularly among open-source models, highlighting the challenges in improving factual accuracy in multi-modal contexts.'}, 'zh': {'title': 'è§†é¢‘è¯­è¨€æ¨¡å‹çš„äº‹å®æ€§è¯„ä¼°æ–°åŸºå‡†', 'desc': 'æœ€è¿‘ï¼Œå¤§å‹è§†é¢‘è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çš„è¿›å±•æ˜¾ç¤ºäº†å®ƒä»¬åœ¨å¤šæ¨¡æ€ç†è§£æ–¹é¢çš„æ½œåŠ›ï¼Œä½†åœ¨è§†é¢‘ä¸Šä¸‹æ–‡ä¸­è¯„ä¼°å…¶äº‹å®åŸºç¡€ä»ç„¶æ˜¯ä¸€ä¸ªé‡è¦çš„æœªè§£å†³æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†Video SimpleQAï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªä¸“é—¨é’ˆå¯¹LVLMsäº‹å®æ€§è¯„ä¼°çš„ç»¼åˆåŸºå‡†ã€‚è¯¥åŸºå‡†çš„ç‰¹ç‚¹åŒ…æ‹¬ï¼šéœ€è¦æ•´åˆå¤–éƒ¨çŸ¥è¯†ã€é’ˆå¯¹å®¢è§‚äº‹ä»¶çš„é—®é¢˜ã€æ˜ç¡®ä¸”ç®€çŸ­çš„ç­”æ¡ˆï¼Œä»¥åŠç»è¿‡å¤–éƒ¨æ¥æºéªŒè¯çš„æ³¨é‡Šã€‚æˆ‘ä»¬å¯¹41ä¸ªæœ€å…ˆè¿›çš„LVLMsè¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ï¼Œå‘ç°å½“å‰æ¨¡å‹åœ¨äº‹å®éµå¾ªæ–¹é¢å­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œå°¤å…¶æ˜¯å¼€æºæ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.17811', 'title': 'Feather-SQL: A Lightweight NL2SQL Framework with Dual-Model\n  Collaboration Paradigm for Small Language Models', 'url': 'https://huggingface.co/papers/2503.17811', 'abstract': 'Natural Language to SQL (NL2SQL) has seen significant advancements with large language models (LLMs). However, these models often depend on closed-source systems and high computational resources, posing challenges in data privacy and deployment. In contrast, small language models (SLMs) struggle with NL2SQL tasks, exhibiting poor performance and incompatibility with existing frameworks. To address these issues, we introduce Feather-SQL, a new lightweight framework tailored for SLMs. Feather-SQL improves SQL executability and accuracy through 1) schema pruning and linking, 2) multi-path and multi-candidate generation. Additionally, we introduce the 1+1 Model Collaboration Paradigm, which pairs a strong general-purpose chat model with a fine-tuned SQL specialist, combining strong analytical reasoning with high-precision SQL generation. Experimental results on BIRD demonstrate that Feather-SQL improves NL2SQL performance on SLMs, with around 10% boost for models without fine-tuning. The proposed paradigm raises the accuracy ceiling of SLMs to 54.76%, highlighting its effectiveness.', 'score': 10, 'issue_id': 2887, 'pub_date': '2025-03-22', 'pub_date_card': {'ru': '22 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 22', 'zh': '3æœˆ22æ—¥'}, 'hash': 'a2f4f2ed59d6f67b', 'authors': ['Wenqi Pei', 'Hailing Xu', 'Hengyuan Zhao', 'Shizheng Hou', 'Han Chen', 'Zining Zhang', 'Pingyi Luo', 'Bingsheng He'], 'affiliations': ['4Paradigm', 'National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2503.17811.jpg', 'data': {'categories': ['#optimization', '#dataset', '#small_models', '#reasoning', '#training'], 'emoji': 'ğŸª¶', 'ru': {'title': 'Ğ›ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğ¹ NL2SQL: Feather-SQL Ğ¿Ğ¾Ğ´Ğ½Ğ¸Ğ¼Ğ°ĞµÑ‚ Ğ¿Ğ»Ğ°Ğ½ĞºÑƒ Ğ´Ğ»Ñ Ğ¼Ğ°Ğ»Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Feather-SQL - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¼Ğ°Ğ»Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (SLM) Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° Ğ² SQL (NL2SQL). Feather-SQL ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ SQL Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑÑ…ĞµĞ¼Ñ‹ Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° '1+1 Model', Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰ÑƒÑ Ğ¾Ğ±Ñ‰ÑƒÑ Ñ‡Ğ°Ñ‚-Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞ¾ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ SQL-Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ SLM Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ NL2SQL Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°."}, 'en': {'title': 'Empowering Small Models for SQL with Feather-SQL', 'desc': 'This paper presents Feather-SQL, a new framework designed to enhance the performance of small language models (SLMs) in converting natural language to SQL queries. It addresses the limitations of SLMs, which typically struggle with accuracy and compatibility in NL2SQL tasks. Feather-SQL employs techniques like schema pruning and multi-path generation to improve SQL executability and precision. Additionally, the introduction of the 1+1 Model Collaboration Paradigm allows for the combination of a general-purpose chat model with a specialized SQL model, significantly boosting the performance of SLMs in this domain.'}, 'zh': {'title': 'è½»é‡çº§æ¡†æ¶ï¼Œæå‡å°å‹æ¨¡å‹çš„SQLèƒ½åŠ›', 'desc': 'è‡ªç„¶è¯­è¨€è½¬SQLï¼ˆNL2SQLï¼‰åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸Šå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†è¿™äº›æ¨¡å‹é€šå¸¸ä¾èµ–äºå°é—­æºç³»ç»Ÿå’Œé«˜è®¡ç®—èµ„æºï¼Œç»™æ•°æ®éšç§å’Œéƒ¨ç½²å¸¦æ¥äº†æŒ‘æˆ˜ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMsï¼‰åœ¨NL2SQLä»»åŠ¡ä¸Šè¡¨ç°ä¸ä½³ï¼Œä¸”ä¸ç°æœ‰æ¡†æ¶ä¸å…¼å®¹ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Feather-SQLï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºSLMsè®¾è®¡çš„è½»é‡çº§æ¡†æ¶ï¼Œé€šè¿‡æ¨¡å¼ä¿®å‰ªå’Œé“¾æ¥ã€å¤šè·¯å¾„å’Œå¤šå€™é€‰ç”Ÿæˆæ¥æé«˜SQLçš„å¯æ‰§è¡Œæ€§å’Œå‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†1+1æ¨¡å‹åä½œèŒƒå¼ï¼Œå°†å¼ºå¤§çš„é€šç”¨èŠå¤©æ¨¡å‹ä¸ç»è¿‡å¾®è°ƒçš„SQLä¸“å®¶é…å¯¹ï¼Œç»“åˆäº†å¼ºå¤§çš„åˆ†ææ¨ç†å’Œé«˜ç²¾åº¦çš„SQLç”Ÿæˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.14428', 'title': 'MagicComp: Training-free Dual-Phase Refinement for Compositional Video\n  Generation', 'url': 'https://huggingface.co/papers/2503.14428', 'abstract': 'Text-to-video (T2V) generation has made significant strides with diffusion models. However, existing methods still struggle with accurately binding attributes, determining spatial relationships, and capturing complex action interactions between multiple subjects. To address these limitations, we propose MagicComp, a training-free method that enhances compositional T2V generation through dual-phase refinement. Specifically, (1) During the Conditioning Stage: We introduce the Semantic Anchor Disambiguation to reinforces subject-specific semantics and resolve inter-subject ambiguity by progressively injecting the directional vectors of semantic anchors into original text embedding; (2) During the Denoising Stage: We propose Dynamic Layout Fusion Attention, which integrates grounding priors and model-adaptive spatial perception to flexibly bind subjects to their spatiotemporal regions through masked attention modulation. Furthermore, MagicComp is a model-agnostic and versatile approach, which can be seamlessly integrated into existing T2V architectures. Extensive experiments on T2V-CompBench and VBench demonstrate that MagicComp outperforms state-of-the-art methods, highlighting its potential for applications such as complex prompt-based and trajectory-controllable video generation. Project page: https://hong-yu-zhang.github.io/MagicComp-Page/.', 'score': 7, 'issue_id': 2875, 'pub_date': '2025-03-18', 'pub_date_card': {'ru': '18 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 18', 'zh': '3æœˆ18æ—¥'}, 'hash': '1cd532518024f266', 'authors': ['Hongyu Zhang', 'Yufan Deng', 'Shenghai Yuan', 'Peng Jin', 'Zesen Cheng', 'Yian Zhao', 'Chang Liu', 'Jie Chen'], 'affiliations': ['Peng Cheng Laboratory, Shenzhen, China', 'School of Electronic and Computer Engineering, Peking University, Shenzhen, China', 'Tsinghua University, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.14428.jpg', 'data': {'categories': ['#training', '#multimodal', '#architecture', '#games', '#diffusion', '#video'], 'emoji': 'ğŸ¬', 'ru': {'title': 'MagicComp: Ğ£ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'MagicComp - ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ, Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğ¹ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…Ñ„Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğµ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸: ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ½ĞµĞ¾Ğ´Ğ½Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ¸ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğ¹ Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ¼Ğ°ĞºĞµÑ‚Ğ¾Ğ² Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ ÑĞ²ÑĞ·Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ¾Ğ², Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸. MagicComp Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ Ğ² ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ñ….'}, 'en': {'title': 'Enhancing Text-to-Video Generation with MagicComp', 'desc': 'This paper presents MagicComp, a novel method for improving text-to-video (T2V) generation using diffusion models. It addresses challenges in accurately linking attributes and understanding spatial relationships between subjects in videos. The method consists of two main phases: the Conditioning Stage, which clarifies subject semantics using Semantic Anchor Disambiguation, and the Denoising Stage, which employs Dynamic Layout Fusion Attention to enhance spatial binding. MagicComp is designed to be adaptable and can be integrated into existing T2V systems, showing superior performance in various benchmarks.'}, 'zh': {'title': 'MagicCompï¼šæå‡æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆçš„åˆ›æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºMagicCompçš„æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆæ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•åœ¨å±æ€§ç»‘å®šã€ç©ºé—´å…³ç³»ç¡®å®šå’Œå¤æ‚åŠ¨ä½œäº¤äº’æ–¹é¢çš„ä¸è¶³ã€‚è¯¥æ–¹æ³•é€šè¿‡åŒé˜¶æ®µçš„ç²¾ç‚¼è¿‡ç¨‹æ¥å¢å¼ºç»„åˆå¼T2Vç”Ÿæˆï¼Œé¦–å…ˆåœ¨æ¡ä»¶é˜¶æ®µå¼•å…¥è¯­ä¹‰é”šç‚¹æ¶ˆæ­§ï¼Œä»¥å¼ºåŒ–ç‰¹å®šä¸»é¢˜çš„è¯­ä¹‰å¹¶è§£å†³ä¸»é¢˜é—´çš„æ­§ä¹‰ã€‚å…¶æ¬¡ï¼Œåœ¨å»å™ªé˜¶æ®µï¼Œæå‡ºåŠ¨æ€å¸ƒå±€èåˆæ³¨æ„åŠ›ï¼Œé€šè¿‡æ©è”½æ³¨æ„åŠ›è°ƒåˆ¶çµæ´»ç»‘å®šä¸»é¢˜ä¸å…¶æ—¶ç©ºåŒºåŸŸã€‚MagicCompæ˜¯ä¸€ç§ä¸æ¨¡å‹æ— å…³çš„é€šç”¨æ–¹æ³•ï¼Œå¯ä»¥æ— ç¼é›†æˆåˆ°ç°æœ‰çš„T2Væ¶æ„ä¸­ï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.18866', 'title': 'Reasoning to Learn from Latent Thoughts', 'url': 'https://huggingface.co/papers/2503.18866', 'abstract': 'Compute scaling for language model (LM) pretraining has outpaced the growth of human-written texts, leading to concerns that data will become the bottleneck to LM scaling. To continue scaling pretraining in this data-constrained regime, we propose that explicitly modeling and inferring the latent thoughts that underlie the text generation process can significantly improve pretraining data efficiency. Intuitively, our approach views web text as the compressed final outcome of a verbose human thought process and that the latent thoughts contain important contextual knowledge and reasoning steps that are critical to data-efficient learning. We empirically demonstrate the effectiveness of our approach through data-constrained continued pretraining for math. We first show that synthetic data approaches to inferring latent thoughts significantly improve data efficiency, outperforming training on the same amount of raw data (5.7\\% rightarrow 25.4\\% on MATH). Furthermore, we demonstrate latent thought inference without a strong teacher, where an LM bootstraps its own performance by using an EM algorithm to iteratively improve the capability of the trained LM and the quality of thought-augmented pretraining data. We show that a 1B LM can bootstrap its performance across at least three iterations and significantly outperform baselines trained on raw data, with increasing gains from additional inference compute when performing the E-step. The gains from inference scaling and EM iterations suggest new opportunities for scaling data-constrained pretraining.', 'score': 6, 'issue_id': 2876, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 24', 'zh': '3æœˆ24æ—¥'}, 'hash': 'ab963a9dd28b0934', 'authors': ['Yangjun Ruan', 'Neil Band', 'Chris J. Maddison', 'Tatsunori Hashimoto'], 'affiliations': ['Stanford University', 'University of Toronto', 'Vector Institute'], 'pdf_title_img': 'assets/pdf/title_img/2503.18866.jpg', 'data': {'categories': ['#optimization', '#training', '#transfer_learning', '#synthetic', '#data', '#math'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ Ğ°ÑĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¼Ñ‹ÑĞ»ĞµĞ¹ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¼Ñ‹ÑĞ»Ğ¸, Ğ»ĞµĞ¶Ğ°Ñ‰Ğ¸Ğµ Ğ² Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²ĞµĞ±-Ñ‚ĞµĞºÑÑ‚ ĞºĞ°Ğº ÑĞ¶Ğ°Ñ‚Ñ‹Ğ¹ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ¸.'}, 'en': {'title': 'Unlocking Data Efficiency through Latent Thought Inference', 'desc': 'This paper addresses the challenge of limited human-written text data for training large language models (LMs). It proposes a method to model and infer the underlying thoughts that lead to text generation, which can enhance data efficiency during pretraining. By treating web text as a condensed version of human thought processes, the authors show that inferring these latent thoughts can lead to better learning outcomes, especially in data-scarce situations. Their experiments demonstrate that this approach not only improves performance on tasks like math but also allows LMs to iteratively enhance their own capabilities without relying heavily on external data.'}, 'zh': {'title': 'æ½œåœ¨æ€ç»´æ¨æ–­æå‡è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒæ•ˆç‡', 'desc': 'è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†åœ¨è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒä¸­ï¼Œæ•°æ®å¢é•¿é€Ÿåº¦æ…¢äºæ¨¡å‹è§„æ¨¡æ‰©å±•çš„é—®é¢˜ã€‚ä½œè€…æå‡ºé€šè¿‡æ˜¾å¼å»ºæ¨¡å’Œæ¨æ–­æ–‡æœ¬ç”Ÿæˆè¿‡ç¨‹ä¸­çš„æ½œåœ¨æ€ç»´ï¼Œå¯ä»¥æ˜¾è‘—æé«˜é¢„è®­ç»ƒçš„æ•°æ®æ•ˆç‡ã€‚ç ”ç©¶è¡¨æ˜ï¼Œåˆæˆæ•°æ®æ–¹æ³•åœ¨æ¨æ–­æ½œåœ¨æ€ç»´æ–¹é¢çš„åº”ç”¨ï¼Œèƒ½å¤Ÿåœ¨æ•°æ®å—é™çš„æƒ…å†µä¸‹ï¼Œæå‡æ¨¡å‹çš„å­¦ä¹ æ•ˆæœã€‚é€šè¿‡è¿­ä»£çš„EMç®—æ³•ï¼Œæ¨¡å‹èƒ½å¤Ÿè‡ªæˆ‘æå‡æ€§èƒ½ï¼Œå¹¶åœ¨å¤šä¸ªè¿­ä»£ä¸­æ˜¾è‘—è¶…è¶Šä»…ä½¿ç”¨åŸå§‹æ•°æ®è®­ç»ƒçš„åŸºçº¿æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.15879', 'title': 'Typed-RAG: Type-aware Multi-Aspect Decomposition for Non-Factoid\n  Question Answering', 'url': 'https://huggingface.co/papers/2503.15879', 'abstract': 'Non-factoid question-answering (NFQA) poses a significant challenge due to its open-ended nature, diverse intents, and the need for multi-aspect reasoning, which renders conventional factoid QA approaches, including retrieval-augmented generation (RAG), inadequate. Unlike factoid questions, non-factoid questions (NFQs) lack definitive answers and require synthesizing information from multiple sources across various reasoning dimensions. To address these limitations, we introduce Typed-RAG, a type-aware multi-aspect decomposition framework within the RAG paradigm for NFQA. Typed-RAG classifies NFQs into distinct types -- such as debate, experience, and comparison -- and applies aspect-based decomposition to refine retrieval and generation strategies. By decomposing multi-aspect NFQs into single-aspect sub-queries and aggregating the results, Typed-RAG generates more informative and contextually relevant responses. To evaluate Typed-RAG, we introduce Wiki-NFQA, a benchmark dataset covering diverse NFQ types. Experimental results demonstrate that Typed-RAG outperforms baselines, thereby highlighting the importance of type-aware decomposition for effective retrieval and generation in NFQA. Our code and dataset are available at https://github.com/TeamNLP/Typed-RAG{https://github.com/TeamNLP/Typed-RAG}.', 'score': 6, 'issue_id': 2878, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 20', 'zh': '3æœˆ20æ—¥'}, 'hash': '7bad41c869c73370', 'authors': ['DongGeon Lee', 'Ahjeong Park', 'Hyeri Lee', 'Hyeonseo Nam', 'Yunho Maeng'], 'affiliations': ['Ewha Womans University', 'Independent Researcher', 'KT', 'LLM Experimental Lab, MODULABS', 'Pohang University of Science and Technology', 'Sookmyung Womens University'], 'pdf_title_img': 'assets/pdf/title_img/2503.15879.jpg', 'data': {'categories': ['#rag', '#open_source', '#benchmark', '#optimization', '#dataset', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Typed-RAG: ÑƒĞ¼Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ğ¼ Ğ½Ğ° Ğ½ĞµÑ„Ğ°ĞºÑ‚Ğ¾Ğ¸Ğ´Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ (NFQA) Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Typed-RAG. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¿Ğ¾ Ñ‚Ğ¸Ğ¿Ğ°Ğ¼ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ Ğ½Ğ° Ğ°ÑĞ¿ĞµĞºÑ‚Ñ‹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Wiki-NFQA Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Typed-RAG. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Typed-RAG Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ NFQA.'}, 'en': {'title': 'Typed-RAG: Enhancing NFQA with Type-Aware Decomposition', 'desc': 'This paper addresses the challenges of non-factoid question-answering (NFQA), which involves questions that do not have straightforward answers and require complex reasoning. The authors propose a new framework called Typed-RAG, which enhances the retrieval-augmented generation (RAG) approach by classifying NFQs into specific types and breaking them down into simpler sub-queries. By focusing on individual aspects of the questions, Typed-RAG improves the quality of the generated responses, making them more relevant and informative. The effectiveness of this method is validated through a new benchmark dataset, Wiki-NFQA, showing that type-aware decomposition significantly boosts performance in NFQA tasks.'}, 'zh': {'title': 'ç±»å‹æ„ŸçŸ¥ï¼Œæå‡éäº‹å®é—®ç­”çš„æ•ˆæœ', 'desc': 'éäº‹å®é—®ç­”ï¼ˆNFQAï¼‰å› å…¶å¼€æ”¾æ€§ã€å¤šæ ·åŒ–çš„æ„å›¾å’Œå¤šæ–¹é¢æ¨ç†çš„éœ€æ±‚è€Œé¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼Œä¼ ç»Ÿçš„äº‹å®é—®ç­”æ–¹æ³•ï¼ˆå¦‚æ£€ç´¢å¢å¼ºç”ŸæˆRAGï¼‰æ— æ³•æ»¡è¶³è¿™äº›éœ€æ±‚ã€‚ä¸äº‹å®é—®é¢˜ä¸åŒï¼Œéäº‹å®é—®é¢˜ï¼ˆNFQï¼‰æ²¡æœ‰æ˜ç¡®çš„ç­”æ¡ˆï¼Œéœ€è¦ä»å¤šä¸ªæ¥æºç»¼åˆä¿¡æ¯ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Typed-RAGï¼Œè¿™æ˜¯ä¸€ç§åœ¨RAGèŒƒå¼ä¸‹çš„ç±»å‹æ„ŸçŸ¥å¤šæ–¹é¢åˆ†è§£æ¡†æ¶ã€‚Typed-RAGå°†NFQåˆ†ç±»ä¸ºä¸åŒç±»å‹ï¼Œå¹¶é€šè¿‡åŸºäºæ–¹é¢çš„åˆ†è§£æ¥ä¼˜åŒ–æ£€ç´¢å’Œç”Ÿæˆç­–ç•¥ï¼Œä»è€Œç”Ÿæˆæ›´å…·ä¿¡æ¯é‡å’Œä¸Šä¸‹æ–‡ç›¸å…³çš„å›ç­”ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.18769', 'title': 'AlphaSpace: Enabling Robotic Actions through Semantic Tokenization and\n  Symbolic Reasoning', 'url': 'https://huggingface.co/papers/2503.18769', 'abstract': 'This paper presents AlphaSpace, a novel methodology designed to enhance the spatial reasoning capabilities of large language models (LLMs) for 3D Cartesian space navigation. AlphaSpace employs a semantics-based tokenization strategy, encoding height information through specialized semantic tokens, and integrates primarily symbolic synthetic reasoning data. This approach enables LLMs to accurately manipulate objects by positioning them at specific [x, y, z] coordinates. Experimental results demonstrate that AlphaSpace significantly outperforms existing models on manipulation subtasks, achieving a total accuracy of 66.67%, compared to 37.5% for GPT-4o and 29.17% for Claude 3.5 Sonnet.', 'score': 5, 'issue_id': 2875, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 24', 'zh': '3æœˆ24æ—¥'}, 'hash': 'e92ee9df78b66019', 'authors': ['Alan Dao', 'Dinh Bach Vu', 'Bui Quang Huy'], 'affiliations': ['Menlo Research'], 'pdf_title_img': 'assets/pdf/title_img/2503.18769.jpg', 'data': {'categories': ['#architecture', '#synthetic', '#reasoning', '#3d'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'AlphaSpace: ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğ¸ Ğ˜Ğ˜', 'desc': 'AlphaSpace - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ 3D Ğ´ĞµĞºĞ°Ñ€Ñ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ñƒ. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ¸, ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ğ²Ñ‹ÑĞ¾Ñ‚Ğµ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹, Ğ¸ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ LLM Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸, Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒÑ Ğ¸Ñ… Ğ¿Ğ¾ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚Ğ°Ğ¼ [x, y, z]. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ AlphaSpace Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ¿Ğ¾Ğ´Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ¾Ğ±Ñ‰ĞµĞ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ 66,67%.'}, 'en': {'title': 'Enhancing 3D Navigation in Language Models with AlphaSpace', 'desc': 'This paper introduces AlphaSpace, a new method aimed at improving how large language models (LLMs) understand and navigate 3D spaces. It uses a unique tokenization method that incorporates height information through special semantic tokens, allowing for better spatial reasoning. By combining this with symbolic reasoning data, AlphaSpace enables LLMs to effectively manipulate objects in a 3D environment by placing them at precise coordinates. The results show that AlphaSpace achieves a notable accuracy of 66.67% in manipulation tasks, outperforming other models like GPT-4o and Claude 3.5 Sonnet.'}, 'zh': {'title': 'AlphaSpaceï¼šæå‡è¯­è¨€æ¨¡å‹çš„ç©ºé—´æ¨ç†èƒ½åŠ›', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°æ–¹æ³•AlphaSpaceï¼Œæ—¨åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä¸‰ç»´ç¬›å¡å°”ç©ºé—´å¯¼èˆªä¸­çš„ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚AlphaSpaceé‡‡ç”¨åŸºäºè¯­ä¹‰çš„æ ‡è®°åŒ–ç­–ç•¥ï¼Œé€šè¿‡ä¸“é—¨çš„è¯­ä¹‰æ ‡è®°ç¼–ç é«˜åº¦ä¿¡æ¯ï¼Œå¹¶ä¸»è¦æ•´åˆç¬¦å·åˆæˆæ¨ç†æ•°æ®ã€‚è¯¥æ–¹æ³•ä½¿å¾—LLMsèƒ½å¤Ÿå‡†ç¡®åœ°é€šè¿‡ç‰¹å®šçš„[x, y, z]åæ ‡æ¥æ“ä½œç‰©ä½“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAlphaSpaceåœ¨æ“ä½œå­ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ¨¡å‹ï¼Œæ€»å‡†ç¡®ç‡è¾¾åˆ°66.67%ï¼Œè€ŒGPT-4oä¸º37.5%ï¼ŒClaude 3.5 Sonnetä¸º29.17%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.17422', 'title': 'V-Seek: Accelerating LLM Reasoning on Open-hardware Server-class RISC-V\n  Platforms', 'url': 'https://huggingface.co/papers/2503.17422', 'abstract': 'The recent exponential growth of Large Language Models (LLMs) has relied on GPU-based systems. However, CPUs are emerging as a flexible and lower-cost alternative, especially when targeting inference and reasoning workloads. RISC-V is rapidly gaining traction in this area, given its open and vendor-neutral ISA. However, the RISC-V hardware for LLM workloads and the corresponding software ecosystem are not fully mature and streamlined, given the requirement of domain-specific tuning. This paper aims at filling this gap, focusing on optimizing LLM inference on the Sophon SG2042, the first commercially available many-core RISC-V CPU with vector processing capabilities.   On two recent state-of-the-art LLMs optimized for reasoning, DeepSeek R1 Distill Llama 8B and DeepSeek R1 Distill QWEN 14B, we achieve 4.32/2.29 token/s for token generation and 6.54/3.68 token/s for prompt processing, with a speed up of up 2.9x/3.0x compared to our baseline.', 'score': 5, 'issue_id': 2876, 'pub_date': '2025-03-21', 'pub_date_card': {'ru': '21 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 21', 'zh': '3æœˆ21æ—¥'}, 'hash': '3811c1f2a2e12813', 'authors': ['Javier J. Poveda Rodrigo', 'Mohamed Amine Ahmdi', 'Alessio Burrello', 'Daniele Jahier Pagliari', 'Luca Benini'], 'affiliations': ['DAUIN, Politecnico of Turin, Turin, Italy', 'ETHZ, Zurich, Switzerland'], 'pdf_title_img': 'assets/pdf/title_img/2503.17422.jpg', 'data': {'categories': ['#architecture', '#optimization', '#reasoning', '#inference'], 'emoji': 'ğŸš€', 'ru': {'title': 'Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° RISC-V Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ñ€Ğ°Ñ…', 'desc': 'Ğ”Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ½Ğ° Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ñ€Ğ°Ñ… RISC-V, Ğ² Ñ‡Ğ°ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Sophon SG2042. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ CPU ĞºĞ°Ğº Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ñ‹ GPU Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°. Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ñ‹ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ²ÑƒÑ… ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… LLM Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ - DeepSeek R1 Distill Llama 8B Ğ¸ DeepSeek R1 Distill QWEN 14B. Ğ”Ğ¾ÑÑ‚Ğ¸Ğ³Ğ½ÑƒÑ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹, Ğ´Ğ¾ 2.9-3.0 Ñ€Ğ°Ğ·.'}, 'en': {'title': 'Unlocking LLM Potential with RISC-V CPUs', 'desc': 'This paper discusses the potential of using CPUs, specifically RISC-V architecture, for optimizing Large Language Model (LLM) inference. It highlights the advantages of RISC-V, such as its flexibility and cost-effectiveness, especially for reasoning tasks. The authors focus on the Sophon SG2042, a many-core RISC-V CPU, and demonstrate significant performance improvements in token generation and prompt processing for two advanced LLMs. The results show up to 3x speed improvements compared to traditional systems, indicating a promising direction for LLM deployment on CPU architectures.'}, 'zh': {'title': 'ç”¨RISC-Vä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†', 'desc': 'è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¿«é€Ÿå‘å±•ä¾èµ–äºåŸºäºGPUçš„ç³»ç»Ÿã€‚ç„¶è€Œï¼ŒCPUä½œä¸ºä¸€ç§çµæ´»ä¸”æˆæœ¬æ›´ä½çš„æ›¿ä»£æ–¹æ¡ˆï¼Œæ­£åœ¨é€æ¸å´­éœ²å¤´è§’ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¨ç†å’Œæ¨æ–­å·¥ä½œè´Ÿè½½æ–¹é¢ã€‚RISC-Vå› å…¶å¼€æ”¾å’Œä¸­ç«‹çš„æŒ‡ä»¤é›†æ¶æ„ï¼ˆISAï¼‰è€Œåœ¨è¿™ä¸€é¢†åŸŸè¿…é€Ÿè·å¾—å…³æ³¨ã€‚æœ¬æ–‡æ—¨åœ¨ä¼˜åŒ–åœ¨Sophon SG2042ä¸Šè¿›è¡ŒLLMæ¨ç†ï¼Œå±•ç¤ºäº†åœ¨ä¸¤ä¸ªæœ€æ–°çš„ä¼˜åŒ–æ¨ç†æ¨¡å‹ä¸Šå®ç°çš„æ˜¾è‘—æ€§èƒ½æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.18559', 'title': 'AMD-Hummingbird: Towards an Efficient Text-to-Video Model', 'url': 'https://huggingface.co/papers/2503.18559', 'abstract': 'Text-to-Video (T2V) generation has attracted significant attention for its ability to synthesize realistic videos from textual descriptions. However, existing models struggle to balance computational efficiency and high visual quality, particularly on resource-limited devices, e.g.,iGPUs and mobile phones. Most prior work prioritizes visual fidelity while overlooking the need for smaller, more efficient models suitable for real-world deployment. To address this challenge, we propose a lightweight T2V framework, termed Hummingbird, which prunes existing models and enhances visual quality through visual feedback learning. Our approach reduces the size of the U-Net from 1.4 billion to 0.7 billion parameters, significantly improving efficiency while preserving high-quality video generation. Additionally, we introduce a novel data processing pipeline that leverages Large Language Models (LLMs) and Video Quality Assessment (VQA) models to enhance the quality of both text prompts and video data. To support user-driven training and style customization, we publicly release the full training code, including data processing and model training. Extensive experiments show that our method achieves a 31X speedup compared to state-of-the-art models such as VideoCrafter2, while also attaining the highest overall score on VBench. Moreover, our method supports the generation of videos with up to 26 frames, addressing the limitations of existing U-Net-based methods in long video generation. Notably, the entire training process requires only four GPUs, yet delivers performance competitive with existing leading methods. Hummingbird presents a practical and efficient solution for T2V generation, combining high performance, scalability, and flexibility for real-world applications.', 'score': 4, 'issue_id': 2877, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 24', 'zh': '3æœˆ24æ—¥'}, 'hash': 'f6ded1274ae1fbf4', 'authors': ['Takashi Isobe', 'He Cui', 'Dong Zhou', 'Mengmeng Ge', 'Dong Li', 'Emad Barsoum'], 'affiliations': ['Advanced Micro Devices, Inc.', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.18559.jpg', 'data': {'categories': ['#long_context', '#video', '#optimization', '#open_source', '#training', '#small_models', '#data'], 'emoji': 'ğŸ¦', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Hummingbird. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ¸Ğ»Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ U-Net Ñ 1,4 Ğ´Ğ¾ 0,7 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ğ² Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ÑÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾ (VQA) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Hummingbird Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ 31-ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğµ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ°Ğ¼Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ğ¸Ğ½Ğ¾Ğ¹ Ğ´Ğ¾ 26 ĞºĞ°Ğ´Ñ€Ğ¾Ğ².'}, 'en': {'title': 'Hummingbird: Efficient Text-to-Video Generation for Real-World Use', 'desc': 'This paper introduces Hummingbird, a lightweight framework for Text-to-Video (T2V) generation that aims to improve efficiency without sacrificing visual quality. By pruning the U-Net model from 1.4 billion to 0.7 billion parameters, Hummingbird achieves a significant speedup of 31 times compared to existing models like VideoCrafter2. The framework also incorporates a novel data processing pipeline that utilizes Large Language Models and Video Quality Assessment to enhance both text prompts and video data. With the ability to generate videos with up to 26 frames and requiring only four GPUs for training, Hummingbird offers a scalable and practical solution for real-world T2V applications.'}, 'zh': {'title': 'è½»é‡çº§æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆçš„é«˜æ•ˆè§£å†³æ–¹æ¡ˆ', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§è½»é‡çº§çš„æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆæ¡†æ¶ï¼Œç§°ä¸ºHummingbirdï¼Œæ—¨åœ¨æé«˜è§†é¢‘ç”Ÿæˆçš„æ•ˆç‡å’Œè§†è§‰è´¨é‡ã€‚è¯¥æ¡†æ¶é€šè¿‡å‰ªæç°æœ‰æ¨¡å‹ï¼Œå°†U-Netçš„å‚æ•°ä»14äº¿å‡å°‘åˆ°7äº¿ï¼Œä»è€Œåœ¨ä¿æŒé«˜è´¨é‡è§†é¢‘ç”Ÿæˆçš„åŒæ—¶æ˜¾è‘—æé«˜äº†è®¡ç®—æ•ˆç‡ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§æ–°çš„æ•°æ®å¤„ç†æµç¨‹ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å’Œè§†é¢‘è´¨é‡è¯„ä¼°æ¨¡å‹æ¥æå‡æ–‡æœ¬æç¤ºå’Œè§†é¢‘æ•°æ®çš„è´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒHummingbirdåœ¨é€Ÿåº¦å’Œæ€§èƒ½ä¸Šå‡ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ¨¡å‹ï¼Œé€‚ç”¨äºèµ„æºæœ‰é™çš„è®¾å¤‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.18018', 'title': 'Lost in Cultural Translation: Do LLMs Struggle with Math Across Cultural\n  Contexts?', 'url': 'https://huggingface.co/papers/2503.18018', 'abstract': "Large Language Models (LLMs) have significantly advanced various fields, particularly coding, mathematical reasoning, and logical problem solving. However, a critical question remains: Do these mathematical reasoning abilities persist when LLMs are presented with culturally adapted math problems? Specifically, how do LLMs perform when faced with math problems embedded in cultural contexts that have no significant representation in main stream web-scale AI training data? To explore this, we generated six synthetic cultural datasets from GSM8K, a widely used benchmark for assessing LLMs' mathematical reasoning skills. While preserving the mathematical logic and numerical values of the original GSM8K test set, we modify cultural elements such as personal names, food items, place names, etc. These culturally adapted datasets provide a more reliable framework for evaluating LLMs' mathematical reasoning under shifting cultural contexts. Our findings reveal that LLMs struggle with math problems when cultural references change, even though the underlying mathematical structure remains constant. Smaller models exhibit greater performance drops compared to larger models. Interestingly, our results also suggest that cultural familiarity can enhance mathematical reasoning. Even models with no explicit mathematical training but exposure to relevant cultural contexts sometimes outperform larger, mathematically proficient models on culturally embedded math problems. This study highlights the impact of cultural context on the mathematical reasoning abilities of LLMs, underscoring the need for more diverse and representative training data to improve robustness in real-world applications. The benchmark data sets and script for reproducing the results are available at https://github.com/akarim23131/Lost_in_Cultural_Translation", 'score': 4, 'issue_id': 2883, 'pub_date': '2025-03-23', 'pub_date_card': {'ru': '23 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 23', 'zh': '3æœˆ23æ—¥'}, 'hash': '38376682f477cce9', 'authors': ['Aabid Karim', 'Abdul Karim', 'Bhoomika Lohana', 'Matt Keon', 'Jaswinder Singh', 'Abdul Sattar'], 'affiliations': ['155mv Research Lab', 'Griffith University', 'Microsoft', 'Millcrest Technology'], 'pdf_title_img': 'assets/pdf/title_img/2503.18018.jpg', 'data': {'categories': ['#synthetic', '#dataset', '#math', '#data', '#reasoning', '#benchmark'], 'emoji': 'ğŸ§®', 'ru': {'title': 'ĞšÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ²Ğ»Ğ¸ÑĞµÑ‚ Ğ½Ğ° Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ˜Ğ˜', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸ĞµĞ¼ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡, ĞºĞ¾Ğ³Ğ´Ğ° Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚, Ğ½ĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ ÑˆĞµÑÑ‚ÑŒ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ GSM8K, Ğ¸Ğ·Ğ¼ĞµĞ½ÑÑ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ñ‹, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ¸Ğ¼ĞµĞ½Ğ° Ğ¸ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ĞµÑÑ‚. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞµĞµ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğ¼Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¸ Ñ€ĞµĞ¿Ñ€ĞµĞ·ĞµĞ½Ñ‚Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ LLM Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ÑÑ….'}, 'en': {'title': 'Cultural Context Matters: LLMs and Math Reasoning', 'desc': 'This paper investigates how Large Language Models (LLMs) perform on mathematical reasoning tasks when the problems are culturally adapted. It creates six synthetic datasets from the GSM8K benchmark, altering cultural elements while keeping the math intact. The study finds that LLMs struggle with culturally modified math problems, especially smaller models, indicating that cultural context significantly affects their reasoning abilities. Additionally, it suggests that familiarity with cultural references can enhance performance, highlighting the importance of diverse training data for improving LLM robustness.'}, 'zh': {'title': 'æ–‡åŒ–èƒŒæ™¯å½±å“æ•°å­¦æ¨ç†èƒ½åŠ›', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç¼–ç ã€æ•°å­¦æ¨ç†å’Œé€»è¾‘é—®é¢˜è§£å†³ç­‰é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œå½“è¿™äº›æ¨¡å‹é¢å¯¹æ–‡åŒ–é€‚åº”çš„æ•°å­¦é—®é¢˜æ—¶ï¼Œå®ƒä»¬çš„æ•°å­¦æ¨ç†èƒ½åŠ›æ˜¯å¦ä¾ç„¶å­˜åœ¨æ˜¯ä¸€ä¸ªé‡è¦é—®é¢˜ã€‚ç ”ç©¶å‘ç°ï¼Œå½“æ–‡åŒ–èƒŒæ™¯å‘ç”Ÿå˜åŒ–æ—¶ï¼ŒLLMsåœ¨è§£å†³æ•°å­¦é—®é¢˜æ—¶è¡¨ç°ä¸ä½³ï¼Œå°½ç®¡æ•°å­¦ç»“æ„ä¿æŒä¸å˜ã€‚æˆ‘ä»¬çš„ç ”ç©¶å¼ºè°ƒäº†æ–‡åŒ–èƒŒæ™¯å¯¹LLMsæ•°å­¦æ¨ç†èƒ½åŠ›çš„å½±å“ï¼Œè¡¨æ˜éœ€è¦æ›´å…·å¤šæ ·æ€§å’Œä»£è¡¨æ€§çš„è®­ç»ƒæ•°æ®ä»¥æé«˜æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„é²æ£’æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.17500', 'title': 'Variance Control via Weight Rescaling in LLM Pre-training', 'url': 'https://huggingface.co/papers/2503.17500', 'abstract': 'The outcome of Large Language Model (LLM) pre-training strongly depends on weight initialization and variance control strategies. Although the importance of initial variance control has been well documented in neural networks in general, the literature on initialization and management of its growth during LLM pre-training, specifically, is somewhat sparse. In this paper, we introduce the Layer Index Rescaling (LIR) weight initialization scheme, and the Target Variance Rescaling (TVR) variance control strategy. Experiments on a 1B parameter LLaMA model demonstrate that better variance management using these techniques yields substantial improvements in downstream task performance (up to 4.6% on common pre-training benchmarks) and reduces extreme activation values, thus mitigating challenges associated with quantization and low-precision training. Our code is available at: https://github.com/bluorion-com/weight_rescaling.', 'score': 4, 'issue_id': 2878, 'pub_date': '2025-03-21', 'pub_date_card': {'ru': '21 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 21', 'zh': '3æœˆ21æ—¥'}, 'hash': '56b451c0f204aeda', 'authors': ['Louis Owen', 'Abhay Kumar', 'Nilabhra Roy Chowdhury', 'Fabian GÃ¼ra'], 'affiliations': ['BluOrion'], 'pdf_title_img': 'assets/pdf/title_img/2503.17500.jpg', 'data': {'categories': ['#inference', '#benchmark', '#optimization', '#training', '#architecture'], 'emoji': 'âš–ï¸', 'ru': {'title': 'Ğ¢Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ²ĞµÑĞ¾Ğ² - ĞºĞ»ÑÑ‡ Ğº ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²ĞµÑĞ¾Ğ² Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ´Ğ¸ÑĞ¿ĞµÑ€ÑĞ¸Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑÑ…ĞµĞ¼Ñƒ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²ĞµÑĞ¾Ğ² Layer Index Rescaling (LIR) Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ´Ğ¸ÑĞ¿ĞµÑ€ÑĞ¸Ğ¸ Target Variance Rescaling (TVR). Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ LLaMA Ñ 1 Ğ¼Ğ»Ñ€Ğ´ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¸ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ†ĞµĞ»ĞµĞ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ÑÑ‚ ÑĞºÑÑ‚Ñ€ĞµĞ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ¸ÑĞ¿ĞµÑ€ÑĞ¸ĞµĞ¹ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ LLM.'}, 'en': {'title': 'Enhancing LLM Performance through Innovative Weight and Variance Management', 'desc': 'This paper discusses how the performance of Large Language Models (LLMs) during pre-training is influenced by how their weights are initialized and how variance is controlled. It introduces two new techniques: Layer Index Rescaling (LIR) for weight initialization and Target Variance Rescaling (TVR) for managing variance growth. The authors show that applying these methods to a 1B parameter LLaMA model leads to significant improvements in performance on various tasks, achieving up to a 4.6% increase on standard benchmarks. Additionally, these techniques help reduce extreme activation values, which can complicate quantization and low-precision training.'}, 'zh': {'title': 'ä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹çš„é¢„è®­ç»ƒæ€§èƒ½', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é¢„è®­ç»ƒä¸­æƒé‡åˆå§‹åŒ–å’Œæ–¹å·®æ§åˆ¶ç­–ç•¥çš„é‡è¦æ€§ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æƒé‡åˆå§‹åŒ–æ–¹æ¡ˆï¼Œç§°ä¸ºå±‚ç´¢å¼•é‡ç¼©æ”¾ï¼ˆLIRï¼‰ï¼Œä»¥åŠç›®æ ‡æ–¹å·®é‡ç¼©æ”¾ï¼ˆTVRï¼‰æ–¹å·®æ§åˆ¶ç­–ç•¥ã€‚å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨è¿™äº›æŠ€æœ¯è¿›è¡Œæ›´å¥½çš„æ–¹å·®ç®¡ç†å¯ä»¥æ˜¾è‘—æé«˜ä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ï¼Œæœ€é«˜å¯è¾¾4.6%çš„æå‡ï¼Œå¹¶å‡å°‘æç«¯æ¿€æ´»å€¼ï¼Œä»è€Œç¼“è§£é‡åŒ–å’Œä½ç²¾åº¦è®­ç»ƒå¸¦æ¥çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬çš„ä»£ç å·²åœ¨GitHubä¸Šå‘å¸ƒï¼Œä¾›ç ”ç©¶äººå‘˜ä½¿ç”¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.18352', 'title': 'Diffusion-4K: Ultra-High-Resolution Image Synthesis with Latent\n  Diffusion Models', 'url': 'https://huggingface.co/papers/2503.18352', 'abstract': 'In this paper, we present Diffusion-4K, a novel framework for direct ultra-high-resolution image synthesis using text-to-image diffusion models. The core advancements include: (1) Aesthetic-4K Benchmark: addressing the absence of a publicly available 4K image synthesis dataset, we construct Aesthetic-4K, a comprehensive benchmark for ultra-high-resolution image generation. We curated a high-quality 4K dataset with carefully selected images and captions generated by GPT-4o. Additionally, we introduce GLCM Score and Compression Ratio metrics to evaluate fine details, combined with holistic measures such as FID, Aesthetics and CLIPScore for a comprehensive assessment of ultra-high-resolution images. (2) Wavelet-based Fine-tuning: we propose a wavelet-based fine-tuning approach for direct training with photorealistic 4K images, applicable to various latent diffusion models, demonstrating its effectiveness in synthesizing highly detailed 4K images. Consequently, Diffusion-4K achieves impressive performance in high-quality image synthesis and text prompt adherence, especially when powered by modern large-scale diffusion models (e.g., SD3-2B and Flux-12B). Extensive experimental results from our benchmark demonstrate the superiority of Diffusion-4K in ultra-high-resolution image synthesis.', 'score': 3, 'issue_id': 2882, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 24', 'zh': '3æœˆ24æ—¥'}, 'hash': '16c79c2acf245efb', 'authors': ['Jinjin Zhang', 'Qiuyu Huang', 'Junjie Liu', 'Xiefan Guo', 'Di Huang'], 'affiliations': ['Meituan', 'School of Computer Science and Engineering, Beihang University, Beijing 100191, China', 'State Key Laboratory of Complex and Critical Software Environment, Beihang University, Beijing 100191, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.18352.jpg', 'data': {'categories': ['#diffusion', '#dataset', '#training', '#cv', '#benchmark'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 4K Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Diffusion-4K - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° ÑƒĞ»ÑŒÑ‚Ñ€Ğ°-Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‚ĞµĞºÑÑ‚-Ğ²-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Aesthetic-4K Ğ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° 4K Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ„Ğ°Ğ¹Ğ½-Ñ‚ÑĞ½Ğ¸Ğ½Ğ³Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²ĞµĞ¹Ğ²Ğ»ĞµÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ñ„Ğ¾Ñ‚Ğ¾Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… 4K Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Diffusion-4K Ğ² ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ ÑĞ²ĞµÑ€Ñ…Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Revolutionizing Ultra-High-Resolution Image Synthesis with Diffusion-4K', 'desc': 'This paper introduces Diffusion-4K, a new framework designed for creating ultra-high-resolution images directly from text using diffusion models. It presents the Aesthetic-4K Benchmark, a dataset of 4K images and captions, along with new metrics like GLCM Score and Compression Ratio for evaluating image quality. The authors also propose a wavelet-based fine-tuning method that enhances the training of diffusion models to produce detailed 4K images. Overall, Diffusion-4K shows significant improvements in image synthesis quality and adherence to text prompts compared to existing methods.'}, 'zh': {'title': 'Diffusion-4Kï¼šè¶…é«˜åˆ†è¾¨ç‡å›¾åƒåˆæˆçš„æ–°çªç ´', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶Diffusion-4Kï¼Œç”¨äºç›´æ¥ç”Ÿæˆè¶…é«˜åˆ†è¾¨ç‡å›¾åƒï¼Œé‡‡ç”¨æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹ã€‚æˆ‘ä»¬æ„å»ºäº†Aesthetic-4KåŸºå‡†æ•°æ®é›†ï¼Œè§£å†³äº†ç¼ºä¹å…¬å¼€4Kå›¾åƒåˆæˆæ•°æ®é›†çš„é—®é¢˜ï¼Œå¹¶å¼•å…¥äº†GLCMè¯„åˆ†å’Œå‹ç¼©æ¯”ç­‰æŒ‡æ ‡æ¥è¯„ä¼°å›¾åƒç»†èŠ‚ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§åŸºäºå°æ³¢çš„å¾®è°ƒæ–¹æ³•ï¼Œé€‚ç”¨äºå„ç§æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåˆæˆé«˜ç»†èŠ‚çš„4Kå›¾åƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDiffusion-4Kåœ¨é«˜è´¨é‡å›¾åƒåˆæˆå’Œæ–‡æœ¬æç¤ºéµå¾ªæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶æ˜¯åœ¨ç°ä»£å¤§è§„æ¨¡æ‰©æ•£æ¨¡å‹çš„æ”¯æŒä¸‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.18470', 'title': 'MetaSpatial: Reinforcing 3D Spatial Reasoning in VLMs for the Metaverse', 'url': 'https://huggingface.co/papers/2503.18470', 'abstract': 'We present MetaSpatial, the first reinforcement learning (RL)-based framework designed to enhance 3D spatial reasoning in vision-language models (VLMs), enabling real-time 3D scene generation without the need for hard-coded optimizations. MetaSpatial addresses two core challenges: (i) the lack of internalized 3D spatial reasoning in VLMs, which limits their ability to generate realistic layouts, and (ii) the inefficiency of traditional supervised fine-tuning (SFT) for layout generation tasks, as perfect ground truth annotations are unavailable. Our key innovation is a multi-turn RL-based optimization mechanism that integrates physics-aware constraints and rendered image evaluations, ensuring generated 3D layouts are coherent, physically plausible, and aesthetically consistent. Methodologically, MetaSpatial introduces an adaptive, iterative reasoning process, where the VLM refines spatial arrangements over multiple turns by analyzing rendered outputs, improving scene coherence progressively. Empirical evaluations demonstrate that MetaSpatial significantly enhances the spatial consistency and formatting stability of various scale models. Post-training, object placements are more realistic, aligned, and functionally coherent, validating the effectiveness of RL for 3D spatial reasoning in metaverse, AR/VR, digital twins, and game development applications. Our code, data, and training pipeline are publicly available at https://github.com/PzySeere/MetaSpatial.', 'score': 2, 'issue_id': 2881, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 24', 'zh': '3æœˆ24æ—¥'}, 'hash': '61b0d03727bc1ba4', 'authors': ['Zhenyu Pan', 'Han Liu'], 'affiliations': ['Department of Computer Science Northwestern University'], 'pdf_title_img': 'assets/pdf/title_img/2503.18470.jpg', 'data': {'categories': ['#3d', '#training', '#rl', '#optimization', '#games', '#open_source', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'MetaSpatial: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ 3D Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ˜Ğ˜ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'MetaSpatial - ÑÑ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ 3D Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ½Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ 3D ÑÑ†ĞµĞ½Ñ‹ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ±ĞµĞ· Ğ¶ĞµÑÑ‚ĞºĞ¾ Ğ·Ğ°ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¹. MetaSpatial Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¾Ğ¹ Ğ¾Ñ‚Ñ€ĞµĞ½Ğ´ĞµÑ€ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°.'}, 'en': {'title': 'Revolutionizing 3D Spatial Reasoning with Reinforcement Learning', 'desc': 'MetaSpatial is a novel framework that uses reinforcement learning (RL) to improve 3D spatial reasoning in vision-language models (VLMs). It tackles the challenges of inadequate internal 3D reasoning and the limitations of traditional supervised fine-tuning for layout generation. The framework employs a multi-turn RL optimization process that incorporates physics-aware constraints and evaluations of rendered images to create realistic and coherent 3D layouts. Through iterative refinement, MetaSpatial enhances the spatial consistency and functional coherence of object placements, making it valuable for applications in the metaverse, AR/VR, and game development.'}, 'zh': {'title': 'MetaSpatialï¼šæå‡3Dç©ºé—´æ¨ç†çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶', 'desc': 'MetaSpatialæ˜¯ç¬¬ä¸€ä¸ªåŸºäºå¼ºåŒ–å­¦ä¹ çš„æ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºè§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„3Dç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œå®ç°å®æ—¶3Dåœºæ™¯ç”Ÿæˆï¼Œè€Œæ— éœ€ç¡¬ç¼–ç ä¼˜åŒ–ã€‚è¯¥æ¡†æ¶è§£å†³äº†ä¸¤ä¸ªæ ¸å¿ƒæŒ‘æˆ˜ï¼šä¸€æ˜¯è§†è§‰è¯­è¨€æ¨¡å‹ç¼ºä¹å†…åœ¨çš„3Dç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œé™åˆ¶äº†å…¶ç”Ÿæˆé€¼çœŸå¸ƒå±€çš„èƒ½åŠ›ï¼›äºŒæ˜¯ä¼ ç»Ÿçš„ç›‘ç£å¾®è°ƒåœ¨å¸ƒå±€ç”Ÿæˆä»»åŠ¡ä¸­æ•ˆç‡ä½ä¸‹ï¼Œå› ä¸ºå®Œç¾çš„çœŸå®æ ‡æ³¨ä¸å¯ç”¨ã€‚MetaSpatialçš„åˆ›æ–°åœ¨äºé‡‡ç”¨å¤šè½®å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–æœºåˆ¶ï¼Œç»“åˆç‰©ç†çº¦æŸå’Œæ¸²æŸ“å›¾åƒè¯„ä¼°ï¼Œç¡®ä¿ç”Ÿæˆçš„3Då¸ƒå±€ä¸€è‡´ã€ç‰©ç†ä¸Šåˆç†ä¸”ç¾è§‚ã€‚é€šè¿‡é€‚åº”æ€§è¿­ä»£æ¨ç†è¿‡ç¨‹ï¼ŒMetaSpatialä½¿è§†è§‰è¯­è¨€æ¨¡å‹èƒ½å¤Ÿåœ¨å¤šä¸ªå›åˆä¸­é€æ­¥æ”¹è¿›ç©ºé—´å®‰æ’ï¼Œä»è€Œæé«˜åœºæ™¯çš„ä¸€è‡´æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.17760', 'title': 'CODA: Repurposing Continuous VAEs for Discrete Tokenization', 'url': 'https://huggingface.co/papers/2503.17760', 'abstract': 'Discrete visual tokenizers transform images into a sequence of tokens, enabling token-based visual generation akin to language models. However, this process is inherently challenging, as it requires both compressing visual signals into a compact representation and discretizing them into a fixed set of codes. Traditional discrete tokenizers typically learn the two tasks jointly, often leading to unstable training, low codebook utilization, and limited reconstruction quality. In this paper, we introduce CODA(COntinuous-to-Discrete Adaptation), a framework that decouples compression and discretization. Instead of training discrete tokenizers from scratch, CODA adapts off-the-shelf continuous VAEs -- already optimized for perceptual compression -- into discrete tokenizers via a carefully designed discretization process. By primarily focusing on discretization, CODA ensures stable and efficient training while retaining the strong visual fidelity of continuous VAEs. Empirically, with 6 times less training budget than standard VQGAN, our approach achieves a remarkable codebook utilization of 100% and notable reconstruction FID (rFID) of 0.43 and 1.34 for 8 times and 16 times compression on ImageNet 256times 256 benchmark.', 'score': 2, 'issue_id': 2887, 'pub_date': '2025-03-22', 'pub_date_card': {'ru': '22 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 22', 'zh': '3æœˆ22æ—¥'}, 'hash': '5a93c7572e0fb46c', 'authors': ['Zeyu Liu', 'Zanlin Ni', 'Yeguo Hua', 'Xin Deng', 'Xiao Ma', 'Cheng Zhong', 'Gao Huang'], 'affiliations': ['Lenovo Research, AI Lab', 'Renmin University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.17760.jpg', 'data': {'categories': ['#optimization', '#dataset', '#benchmark', '#cv', '#architecture', '#training'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'CODA: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ CODA - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. CODA Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğµ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ñ‹ Ğ² Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ñ‹, Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑÑ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ¸ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ, Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ĞºĞ¾Ğ´Ğ¾Ğ²Ğ¾Ğ¹ ĞºĞ½Ğ¸Ğ³Ğ¸ Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸. ĞŸĞ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ ÑĞ¾ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¼ VQGAN, CODA Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ….'}, 'en': {'title': 'Decoupling Compression and Discretization for Better Visual Tokenization', 'desc': 'This paper presents CODA, a novel framework for transforming images into discrete tokens for visual generation. Unlike traditional methods that combine compression and discretization, CODA separates these tasks to enhance training stability and efficiency. By adapting existing continuous Variational Autoencoders (VAEs) for discretization, CODA achieves high-quality visual representations with optimal codebook usage. The results demonstrate that CODA requires significantly less training resources while achieving superior reconstruction quality compared to standard methods.'}, 'zh': {'title': 'CODAï¼šé«˜æ•ˆçš„è§†è§‰æ ‡è®°ç¦»æ•£åŒ–æ¡†æ¶', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æ¡†æ¶CODAï¼ˆè¿ç»­åˆ°ç¦»æ•£é€‚åº”ï¼‰ï¼Œç”¨äºå°†å›¾åƒè½¬æ¢ä¸ºç¦»æ•£çš„è§†è§‰æ ‡è®°åºåˆ—ã€‚ä¼ ç»Ÿçš„ç¦»æ•£æ ‡è®°å™¨åœ¨å‹ç¼©å’Œç¦»æ•£åŒ–ä»»åŠ¡ä¸Šé€šå¸¸ä¼šå¯¼è‡´è®­ç»ƒä¸ç¨³å®šå’Œé‡å»ºè´¨é‡ä½ä¸‹ã€‚CODAé€šè¿‡å°†å‹ç¼©å’Œç¦»æ•£åŒ–è¿‡ç¨‹è§£è€¦ï¼Œåˆ©ç”¨ç°æœ‰çš„è¿ç»­å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰è¿›è¡Œé€‚åº”ï¼Œä»è€Œæé«˜äº†è®­ç»ƒçš„ç¨³å®šæ€§å’Œæ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCODAåœ¨è®­ç»ƒé¢„ç®—ä¸Šæ¯”æ ‡å‡†VQGANå°‘6å€ï¼ŒåŒæ—¶å®ç°äº†100%çš„ä»£ç æœ¬åˆ©ç”¨ç‡å’Œä¼˜å¼‚çš„é‡å»ºFIDå€¼ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.17735', 'title': 'RDTF: Resource-efficient Dual-mask Training Framework for Multi-frame\n  Animated Sticker Generation', 'url': 'https://huggingface.co/papers/2503.17735', 'abstract': 'Recently, great progress has been made in video generation technology, attracting the widespread attention of scholars. To apply this technology to downstream applications under resource-constrained conditions, researchers usually fine-tune the pre-trained models based on parameter-efficient tuning methods such as Adapter or Lora. Although these methods can transfer the knowledge from the source domain to the target domain, fewer training parameters lead to poor fitting ability, and the knowledge from the source domain may lead to the inference process deviating from the target domain. In this paper, we argue that under constrained resources, training a smaller video generation model from scratch using only million-level samples can outperform parameter-efficient tuning on larger models in downstream applications: the core lies in the effective utilization of data and curriculum strategy. Take animated sticker generation (ASG) as a case study, we first construct a discrete frame generation network for stickers with low frame rates, ensuring that its parameters meet the requirements of model training under constrained resources. In order to provide data support for models trained from scratch, we come up with a dual-mask based data utilization strategy, which manages to improve the availability and expand the diversity of limited data. To facilitate convergence under dual-mask situation, we propose a difficulty-adaptive curriculum learning method, which decomposes the sample entropy into static and adaptive components so as to obtain samples from easy to difficult. The experiment demonstrates that our resource-efficient dual-mask training framework is quantitatively and qualitatively superior to efficient-parameter tuning methods such as I2V-Adapter and SimDA, verifying the feasibility of our method on downstream tasks under constrained resources. Code will be available.', 'score': 2, 'issue_id': 2876, 'pub_date': '2025-03-22', 'pub_date_card': {'ru': '22 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 22', 'zh': '3æœˆ22æ—¥'}, 'hash': '186b92c438925eb6', 'authors': ['Zhiqiang Yuan', 'Ting Zhang', 'Ying Deng', 'Jiapei Zhang', 'Yeshuang Zhu', 'Zexi Jia', 'Jie Zhou', 'Jinchao Zhang'], 'affiliations': ['Pattern Recognition Center, WeChat AI, Tencent'], 'pdf_title_img': 'assets/pdf/title_img/2503.17735.jpg', 'data': {'categories': ['#video', '#dataset', '#optimization', '#training', '#transfer_learning'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ½ÑƒĞ»Ñ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒ Ñ Ğ½ÑƒĞ»Ñ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ² Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ¾Ğ¹ Ğ¼Ğ°ÑĞºĞ¸ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ ÑƒÑ‡ĞµĞ±Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ°Ğ½Ğ¸Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑÑ‚Ğ¸ĞºĞµÑ€Ğ¾Ğ² Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Train Small, Win Big: Efficient Video Generation Under Constraints', 'desc': 'This paper discusses advancements in video generation technology and its application in resource-limited environments. It challenges the effectiveness of parameter-efficient tuning methods like Adapter and Lora, suggesting that training smaller models from scratch can yield better results with limited data. The authors introduce a dual-mask data utilization strategy to enhance data diversity and a difficulty-adaptive curriculum learning method to improve model training. Their experiments show that this new approach outperforms existing tuning methods, demonstrating its potential for downstream applications.'}, 'zh': {'title': 'èµ„æºå—é™ä¸‹çš„è§†é¢‘ç”Ÿæˆæ–°ç­–ç•¥', 'desc': 'æœ€è¿‘ï¼Œè§†é¢‘ç”ŸæˆæŠ€æœ¯å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œå¸å¼•äº†å­¦è€…ä»¬çš„å¹¿æ³›å…³æ³¨ã€‚ä¸ºäº†åœ¨èµ„æºå—é™çš„æ¡ä»¶ä¸‹åº”ç”¨è¿™ä¸€æŠ€æœ¯ï¼Œç ”ç©¶äººå‘˜é€šå¸¸åŸºäºå‚æ•°é«˜æ•ˆçš„è°ƒä¼˜æ–¹æ³•å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚æœ¬æ–‡æå‡ºåœ¨èµ„æºå—é™çš„æƒ…å†µä¸‹ï¼Œä»å¤´å¼€å§‹è®­ç»ƒä¸€ä¸ªè¾ƒå°çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œä½¿ç”¨ç™¾ä¸‡çº§æ ·æœ¬ï¼Œèƒ½å¤Ÿåœ¨ä¸‹æ¸¸åº”ç”¨ä¸­è¶…è¶Šå¤§å‹æ¨¡å‹çš„å‚æ•°é«˜æ•ˆè°ƒä¼˜ã€‚æˆ‘ä»¬é€šè¿‡æ„å»ºä½å¸§ç‡è´´çº¸çš„ç¦»æ•£å¸§ç”Ÿæˆç½‘ç»œå’ŒåŒæ©ç æ•°æ®åˆ©ç”¨ç­–ç•¥ï¼Œç»“åˆéš¾åº¦è‡ªé€‚åº”çš„è¯¾ç¨‹å­¦ä¹ æ–¹æ³•ï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„è®­ç»ƒæ•ˆæœã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.16924', 'title': 'Optimized Minimal 3D Gaussian Splatting', 'url': 'https://huggingface.co/papers/2503.16924', 'abstract': '3D Gaussian Splatting (3DGS) has emerged as a powerful representation for real-time, high-performance rendering, enabling a wide range of applications. However, representing 3D scenes with numerous explicit Gaussian primitives imposes significant storage and memory overhead. Recent studies have shown that high-quality rendering can be achieved with a substantially reduced number of Gaussians when represented with high-precision attributes. Nevertheless, existing 3DGS compression methods still rely on a relatively large number of Gaussians, focusing primarily on attribute compression. This is because a smaller set of Gaussians becomes increasingly sensitive to lossy attribute compression, leading to severe quality degradation. Since the number of Gaussians is directly tied to computational costs, it is essential to reduce the number of Gaussians effectively rather than only optimizing storage. In this paper, we propose Optimized Minimal Gaussians representation (OMG), which significantly reduces storage while using a minimal number of primitives. First, we determine the distinct Gaussian from the near ones, minimizing redundancy without sacrificing quality. Second, we propose a compact and precise attribute representation that efficiently captures both continuity and irregularity among primitives. Additionally, we propose a sub-vector quantization technique for improved irregularity representation, maintaining fast training with a negligible codebook size. Extensive experiments demonstrate that OMG reduces storage requirements by nearly 50% compared to the previous state-of-the-art and enables 600+ FPS rendering while maintaining high rendering quality. Our source code is available at https://maincold2.github.io/omg/.', 'score': 2, 'issue_id': 2877, 'pub_date': '2025-03-21', 'pub_date_card': {'ru': '21 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 21', 'zh': '3æœˆ21æ—¥'}, 'hash': '280ac899f8c492d0', 'authors': ['Joo Chan Lee', 'Jong Hwan Ko', 'Eunbyung Park'], 'affiliations': ['Sungkyunkwan University', 'Yonsei University'], 'pdf_title_img': 'assets/pdf/title_img/2503.16924.jpg', 'data': {'categories': ['#optimization', '#3d', '#inference', '#open_source'], 'emoji': 'ğŸ¨', 'ru': {'title': 'OMG: ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ 3D Gaussian Splatting Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ°', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Optimized Minimal Gaussians (OMG) Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ 3D Gaussian Splatting. OMG Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğº Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ğ²Ğ¾Ğ², ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ³Ğ°ÑƒÑÑĞ¸Ğ°Ğ½ ÑÑ€ĞµĞ´Ğ¸ Ğ±Ğ»Ğ¸Ğ·ĞºĞ¸Ñ… Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ¾Ğ², ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ capturing Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ½ĞµÑ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ğ²Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ ÑÑƒĞ±Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğ¹ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½ĞµÑ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ½Ğ¾ÑÑ‚Ğ¸, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ½ĞµĞ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ²Ğ¾Ğ¹ ĞºĞ½Ğ¸Ğ³Ğ¸.'}, 'en': {'title': 'Streamlining 3D Rendering with Minimal Gaussians', 'desc': 'This paper introduces the Optimized Minimal Gaussians (OMG) representation, which aims to enhance 3D Gaussian Splatting (3DGS) by significantly reducing the number of Gaussian primitives needed for high-quality rendering. The authors focus on minimizing redundancy among similar Gaussians while ensuring that the quality of the rendered scenes is preserved. They also present a compact attribute representation that captures both smooth and irregular features of the 3D scene, along with a sub-vector quantization method to improve efficiency. The results show that OMG can cut storage requirements by nearly 50% and achieve over 600 frames per second in rendering without compromising quality.'}, 'zh': {'title': 'ä¼˜åŒ–æœ€å°é«˜æ–¯è¡¨ç¤ºï¼Œæå‡æ¸²æŸ“æ•ˆç‡ï¼', 'desc': '3Dé«˜æ–¯ç‚¹äº‘è¡¨ç¤ºï¼ˆ3DGSï¼‰æ˜¯ä¸€ç§ç”¨äºå®æ—¶é«˜æ€§èƒ½æ¸²æŸ“çš„å¼ºå¤§æ–¹æ³•ï¼Œä½†ä½¿ç”¨å¤§é‡æ˜¾å¼é«˜æ–¯åŸè¯­ä¼šå¯¼è‡´å­˜å‚¨å’Œå†…å­˜å¼€é”€å¤§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ä¼˜åŒ–çš„æœ€å°é«˜æ–¯è¡¨ç¤ºï¼ˆOMGï¼‰ï¼Œé€šè¿‡å‡å°‘é«˜æ–¯æ•°é‡æ¥æ˜¾è‘—é™ä½å­˜å‚¨éœ€æ±‚ï¼ŒåŒæ—¶ä¿æŒé«˜æ¸²æŸ“è´¨é‡ã€‚æˆ‘ä»¬é€šè¿‡è¯†åˆ«ç›¸ä¼¼é«˜æ–¯æ¥å‡å°‘å†—ä½™ï¼Œå¹¶æå‡ºäº†ä¸€ç§ç´§å‡‘çš„å±æ€§è¡¨ç¤ºæ–¹æ³•ï¼Œä»¥æœ‰æ•ˆæ•æ‰åŸè¯­ä¹‹é—´çš„è¿ç»­æ€§å’Œä¸è§„åˆ™æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§å­å‘é‡é‡åŒ–æŠ€æœ¯ï¼Œä»¥æé«˜ä¸è§„åˆ™æ€§çš„è¡¨ç¤ºæ•ˆæœã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.18071', 'title': 'Mind with Eyes: from Language Reasoning to Multimodal Reasoning', 'url': 'https://huggingface.co/papers/2503.18071', 'abstract': 'Language models have recently advanced into the realm of reasoning, yet it is through multimodal reasoning that we can fully unlock the potential to achieve more comprehensive, human-like cognitive capabilities. This survey provides a systematic overview of the recent multimodal reasoning approaches, categorizing them into two levels: language-centric multimodal reasoning and collaborative multimodal reasoning. The former encompasses one-pass visual perception and active visual perception, where vision primarily serves a supporting role in language reasoning. The latter involves action generation and state update within reasoning process, enabling a more dynamic interaction between modalities. Furthermore, we analyze the technical evolution of these methods, discuss their inherent challenges, and introduce key benchmark tasks and evaluation metrics for assessing multimodal reasoning performance. Finally, we provide insights into future research directions from the following two perspectives: (i) from visual-language reasoning to omnimodal reasoning and (ii) from multimodal reasoning to multimodal agents. This survey aims to provide a structured overview that will inspire further advancements in multimodal reasoning research.', 'score': 1, 'issue_id': 2888, 'pub_date': '2025-03-23', 'pub_date_card': {'ru': '23 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 23', 'zh': '3æœˆ23æ—¥'}, 'hash': 'bb57f84b4f49656e', 'authors': ['Zhiyu Lin', 'Yifei Gao', 'Xian Zhao', 'Yunfan Yang', 'Jitao Sang'], 'affiliations': ['Beijing Jiaotong University'], 'pdf_title_img': 'assets/pdf/title_img/2503.18071.jpg', 'data': {'categories': ['#agents', '#benchmark', '#reasoning', '#multimodal', '#survey'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ: Ğ¿ÑƒÑ‚ÑŒ Ğº Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾Ğ¼Ñƒ Ğ˜Ğ˜', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ¾Ğ±Ğ·Ğ¾Ñ€ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğº Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ´Ğ²Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ñ: ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾-Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡Ğ½Ğ¾Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¸ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ. Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ°Ñ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ ÑÑ‚Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², Ğ¾Ğ±ÑÑƒĞ¶Ğ´Ğ°ÑÑ‚ÑÑ Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ñ‹ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´ Ğ¾Ñ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğº Ğ¾Ğ¼Ğ½Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ¸ Ğ¾Ñ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğº Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼.'}, 'en': {'title': 'Unlocking Human-like Reasoning through Multimodal Approaches', 'desc': 'This paper surveys recent advancements in multimodal reasoning, which combines different types of data, like text and images, to enhance cognitive abilities similar to humans. It categorizes multimodal reasoning into two main types: language-centric, where visual data supports language tasks, and collaborative, which involves generating actions and updating states for more interactive reasoning. The authors discuss the evolution of these methods, their challenges, and propose benchmarks for evaluating their performance. Additionally, they suggest future research directions, aiming to transition from visual-language reasoning to more comprehensive omnimodal reasoning and the development of multimodal agents.'}, 'zh': {'title': 'è§£é”å¤šæ¨¡æ€æ¨ç†çš„æ½œåŠ›', 'desc': 'è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¤šæ¨¡æ€æ¨ç†åœ¨æœºå™¨å­¦ä¹ ä¸­çš„é‡è¦æ€§ï¼Œå¼ºè°ƒäº†å…¶åœ¨å®ç°æ›´å…¨é¢çš„äººç±»è®¤çŸ¥èƒ½åŠ›æ–¹é¢çš„æ½œåŠ›ã€‚è®ºæ–‡å°†å¤šæ¨¡æ€æ¨ç†æ–¹æ³•åˆ†ä¸ºä¸¤ç±»ï¼šä»¥è¯­è¨€ä¸ºä¸­å¿ƒçš„å¤šæ¨¡æ€æ¨ç†å’Œåä½œå¤šæ¨¡æ€æ¨ç†ã€‚å‰è€…ä¸»è¦å…³æ³¨è§†è§‰åœ¨è¯­è¨€æ¨ç†ä¸­çš„è¾…åŠ©ä½œç”¨ï¼Œè€Œåè€…åˆ™æ¶‰åŠåœ¨æ¨ç†è¿‡ç¨‹ä¸­ç”ŸæˆåŠ¨ä½œå’Œæ›´æ–°çŠ¶æ€ï¼Œå®ç°æ¨¡æ€ä¹‹é—´çš„åŠ¨æ€äº¤äº’ã€‚æœ€åï¼Œè®ºæ–‡åˆ†æäº†è¿™äº›æ–¹æ³•çš„æŠ€æœ¯æ¼”å˜ã€é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¹¶æå‡ºäº†æœªæ¥ç ”ç©¶çš„æ–¹å‘ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.14774', 'title': 'Revisiting Image Fusion for Multi-Illuminant White-Balance Correction', 'url': 'https://huggingface.co/papers/2503.14774', 'abstract': 'White balance (WB) correction in scenes with multiple illuminants remains a persistent challenge in computer vision. Recent methods explored fusion-based approaches, where a neural network linearly blends multiple sRGB versions of an input image, each processed with predefined WB presets. However, we demonstrate that these methods are suboptimal for common multi-illuminant scenarios. Additionally, existing fusion-based methods rely on sRGB WB datasets lacking dedicated multi-illuminant images, limiting both training and evaluation. To address these challenges, we introduce two key contributions. First, we propose an efficient transformer-based model that effectively captures spatial dependencies across sRGB WB presets, substantially improving upon linear fusion techniques. Second, we introduce a large-scale multi-illuminant dataset comprising over 16,000 sRGB images rendered with five different WB settings, along with WB-corrected images. Our method achieves up to 100\\% improvement over existing techniques on our new multi-illuminant image fusion dataset.', 'score': 1, 'issue_id': 2887, 'pub_date': '2025-03-18', 'pub_date_card': {'ru': '18 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 18', 'zh': '3æœˆ18æ—¥'}, 'hash': '654639b6ee1b8295', 'authors': ['David Serrano-Lozano', 'Aditya Arora', 'Luis Herranz', 'Konstantinos G. Derpanis', 'Michael S. Brown', 'Javier Vazquez-Corral'], 'affiliations': ['Computer Vision Center', 'Universidad Autonoma de Madrid', 'Universitat Aut`onoma de Barcelona', 'Vector Institute', 'York University'], 'pdf_title_img': 'assets/pdf/title_img/2503.14774.jpg', 'data': {'categories': ['#cv', '#dataset', '#optimization'], 'emoji': 'ğŸ“¸', 'ru': {'title': 'Ğ¢Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ñ‹ Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¸Ğ´ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ±ĞµĞ»Ğ¾Ğ³Ğ¾', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ğ¸ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ±ĞµĞ»Ğ¾Ğ³Ğ¾ Ğ² ÑÑ†ĞµĞ½Ğ°Ñ… Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ°Ğ¼Ğ¸ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğ³Ğ¾ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¾Ğ½Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 16 000 sRGB Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ°Ğ¼Ğ¸ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ±ĞµĞ»Ğ¾Ğ³Ğ¾. Ğ˜Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ´Ğ¾ 100% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ°Ğ¼Ğ¸ Ğ½Ğ° Ğ½Ğ¾Ğ²Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ°Ğ¼Ğ¸ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Transforming White Balance Correction with a New Dataset and Model', 'desc': 'This paper addresses the challenge of white balance (WB) correction in images with multiple light sources. It critiques existing fusion-based methods that blend different sRGB images but finds them lacking for complex lighting scenarios. The authors propose a new transformer-based model that better captures the relationships between different WB settings, leading to significant performance improvements. Additionally, they introduce a comprehensive dataset with over 16,000 images to support training and evaluation of WB correction methods in multi-illuminant contexts.'}, 'zh': {'title': 'æå‡å¤šå…‰æºåœºæ™¯ä¸‹çš„ç™½å¹³è¡¡æ ¡æ­£æ•ˆæœ', 'desc': 'åœ¨è®¡ç®—æœºè§†è§‰ä¸­ï¼Œç™½å¹³è¡¡ï¼ˆWBï¼‰æ ¡æ­£åœ¨å¤šå…‰æºåœºæ™¯ä¸­ä»ç„¶æ˜¯ä¸€ä¸ªæŒç»­çš„æŒ‘æˆ˜ã€‚æœ€è¿‘çš„æ–¹æ³•æ¢ç´¢äº†åŸºäºèåˆçš„æŠ€æœ¯ï¼Œé€šè¿‡ç¥ç»ç½‘ç»œçº¿æ€§æ··åˆå¤šä¸ªsRGBç‰ˆæœ¬çš„è¾“å…¥å›¾åƒï¼Œä½†è¿™äº›æ–¹æ³•åœ¨å¸¸è§çš„å¤šå…‰æºåœºæ™¯ä¸­æ•ˆæœä¸ä½³ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„åŸºäºå˜æ¢å™¨çš„æ¨¡å‹ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ•æ‰ä¸åŒsRGB WBé¢„è®¾ä¹‹é—´çš„ç©ºé—´ä¾èµ–æ€§ï¼Œæ˜¾è‘—æ”¹å–„äº†çº¿æ€§èåˆæŠ€æœ¯ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªå¤§å‹å¤šå…‰æºæ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡16,000å¼ ä½¿ç”¨äº”ç§ä¸åŒWBè®¾ç½®æ¸²æŸ“çš„sRGBå›¾åƒåŠå…¶WBæ ¡æ­£å›¾åƒã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.13074', 'title': 'Rethinking Image Evaluation in Super-Resolution', 'url': 'https://huggingface.co/papers/2503.13074', 'abstract': "While recent advancing image super-resolution (SR) techniques are continually improving the perceptual quality of their outputs, they can usually fail in quantitative evaluations. This inconsistency leads to a growing distrust in existing image metrics for SR evaluations. Though image evaluation depends on both the metric and the reference ground truth (GT), researchers typically do not inspect the role of GTs, as they are generally accepted as `perfect' references. However, due to the data being collected in the early years and the ignorance of controlling other types of distortions, we point out that GTs in existing SR datasets can exhibit relatively poor quality, which leads to biased evaluations. Following this observation, in this paper, we are interested in the following questions: Are GT images in existing SR datasets 100% trustworthy for model evaluations? How does GT quality affect this evaluation? And how to make fair evaluations if there exist imperfect GTs? To answer these questions, this paper presents two main contributions. First, by systematically analyzing seven state-of-the-art SR models across three real-world SR datasets, we show that SR performances can be consistently affected across models by low-quality GTs, and models can perform quite differently when GT quality is controlled. Second, we propose a novel perceptual quality metric, Relative Quality Index (RQI), that measures the relative quality discrepancy of image pairs, thus issuing the biased evaluations caused by unreliable GTs. Our proposed model achieves significantly better consistency with human opinions. We expect our work to provide insights for the SR community on how future datasets, models, and metrics should be developed.", 'score': 1, 'issue_id': 2887, 'pub_date': '2025-03-17', 'pub_date_card': {'ru': '17 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 17', 'zh': '3æœˆ17æ—¥'}, 'hash': '7026b065c090a9c7', 'authors': ['Shaolin Su', 'Josep M. Rocafort', 'Danna Xue', 'David Serrano-Lozano', 'Lei Sun', 'Javier Vazquez-Corral'], 'affiliations': ['Computer Vision Center', 'INSAIT, Sofia University St. Kliment Ohridski', 'Universitat Autonoma de Barcelona'], 'pdf_title_img': 'assets/pdf/title_img/2503.13074.jpg', 'data': {'categories': ['#optimization', '#dataset', '#benchmark', '#cv', '#ethics'], 'emoji': 'ğŸ”', 'ru': {'title': 'ĞŸĞµÑ€ĞµĞ¾ÑĞ¼Ñ‹ÑĞ»ĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ¾Ğ² Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ² ÑĞ²ĞµÑ€Ñ…Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ² ÑĞ²ĞµÑ€Ñ…Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ² ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¸Ğ¼ĞµÑ‚ÑŒ Ğ½Ğ¸Ğ·ĞºĞ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼. ĞĞ½Ğ¸ Ğ¿Ñ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ¾Ğ² Ğ½Ğ° Ğ¾Ñ†ĞµĞ½ĞºÑƒ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ²ĞµÑ€Ñ…Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. Ğ’ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğµ Ğ±Ñ‹Ğ» Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒ Relative Quality Index (RQI), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸Ğ·Ğ¼ĞµÑ€ÑĞµÑ‚ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ñ€Ğ°Ğ·Ğ½Ğ¸Ñ†Ñƒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ğ°Ñ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ»ÑƒÑ‡ÑˆĞµ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒĞµÑ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ.'}, 'en': {'title': 'Rethinking Ground Truth: Enhancing Super-Resolution Evaluations', 'desc': 'This paper addresses the issue of trustworthiness in ground truth (GT) images used for evaluating image super-resolution (SR) models. It highlights that many existing GTs may not be perfect, leading to biased performance evaluations of SR techniques. The authors analyze various SR models and demonstrate that low-quality GTs can significantly impact model performance. They also introduce a new metric, the Relative Quality Index (RQI), which aims to provide a more reliable assessment of image quality by comparing pairs of images, aligning better with human judgment.'}, 'zh': {'title': 'æå‡è¶…åˆ†è¾¨ç‡è¯„ä¼°çš„ä¿¡ä»»åº¦', 'desc': 'è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å›¾åƒè¶…åˆ†è¾¨ç‡(SR)æŠ€æœ¯åœ¨å®šé‡è¯„ä¼°ä¸­çš„ä¸è¶³ï¼ŒæŒ‡å‡ºç°æœ‰çš„å›¾åƒè¯„ä¼°æŒ‡æ ‡å¯èƒ½ä¸å¯é ã€‚ç ”ç©¶å‘ç°ï¼Œç°æœ‰SRæ•°æ®é›†ä¸­ä½œä¸ºå‚è€ƒçš„çœŸå®å›¾åƒ(GT)è´¨é‡å¹¶ä¸æ€»æ˜¯å®Œç¾ï¼Œå¯èƒ½å¯¼è‡´è¯„ä¼°ç»“æœçš„åå·®ã€‚é€šè¿‡åˆ†æä¸ƒç§æœ€å…ˆè¿›çš„SRæ¨¡å‹ï¼Œè®ºæ–‡è¡¨æ˜ä½è´¨é‡çš„GTä¼šå½±å“æ¨¡å‹çš„è¡¨ç°ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°çš„æ„ŸçŸ¥è´¨é‡æŒ‡æ ‡â€”â€”ç›¸å¯¹è´¨é‡æŒ‡æ•°(RQI)ï¼Œç”¨äºè¡¡é‡å›¾åƒå¯¹ä¹‹é—´çš„ç›¸å¯¹è´¨é‡å·®å¼‚ã€‚è¯¥ç ”ç©¶æ—¨åœ¨ä¸ºSRé¢†åŸŸæä¾›å…³äºæœªæ¥æ•°æ®é›†ã€æ¨¡å‹å’Œè¯„ä¼°æŒ‡æ ‡å¼€å‘çš„è§è§£ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.18674', 'title': 'Human Motion Unlearning', 'url': 'https://huggingface.co/papers/2503.18674', 'abstract': 'We introduce the task of human motion unlearning to prevent the synthesis of toxic animations while preserving the general text-to-motion generative performance. Unlearning toxic motions is challenging as those can be generated from explicit text prompts and from implicit toxic combinations of safe motions (e.g., ``kicking" is ``loading and swinging a leg"). We propose the first motion unlearning benchmark by filtering toxic motions from the large and recent text-to-motion datasets of HumanML3D and Motion-X. We propose baselines, by adapting state-of-the-art image unlearning techniques to process spatio-temporal signals. Finally, we propose a novel motion unlearning model based on Latent Code Replacement, which we dub LCR. LCR is training-free and suitable to the discrete latent spaces of state-of-the-art text-to-motion diffusion models. LCR is simple and consistently outperforms baselines qualitatively and quantitatively. Project page: https://www.pinlab.org/hmu{https://www.pinlab.org/hmu}.', 'score': 0, 'issue_id': 2888, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 24', 'zh': '3æœˆ24æ—¥'}, 'hash': 'c8c54c3b8e02cf7b', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#dataset', '#benchmark', '#ethics', '#diffusion', '#training'], 'emoji': 'ğŸ•º', 'ru': {'title': 'Ğ‘ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹: Ñ€Ğ°Ğ·Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾ĞºÑĞ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ñ€Ğ°Ğ·Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ğ¾ĞºÑĞ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°ÑÑ‚ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹, Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€ÑƒÑ Ñ‚Ğ¾ĞºÑĞ¸Ñ‡Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚ Ğ¸Ğ· ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ². ĞĞ½Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒÑÑ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ñ€Ğ°Ğ·Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ°Ğ¼Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ LCR Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ·Ğ°Ğ¼ĞµĞ½Ñ‹ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ĞºĞ¾Ğ´Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹.'}, 'en': {'title': 'Unlearning Toxic Motions for Safer Animation Generation', 'desc': 'This paper addresses the challenge of human motion unlearning, which aims to eliminate the generation of harmful animations while maintaining effective text-to-motion performance. The authors introduce a benchmark for motion unlearning by filtering out toxic motions from large datasets like HumanML3D and Motion-X. They adapt advanced image unlearning techniques to work with spatio-temporal data and propose a new model called Latent Code Replacement (LCR), which does not require training and is compatible with existing text-to-motion diffusion models. LCR demonstrates superior performance compared to existing methods in both qualitative and quantitative evaluations.'}, 'zh': {'title': 'äººç±»åŠ¨ä½œå»å­¦ä¹ ï¼šæ¶ˆé™¤æœ‰å®³åŠ¨ç”»çš„åˆ›æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡ä»‹ç»äº†äººç±»åŠ¨ä½œå»å­¦ä¹ çš„ä»»åŠ¡ï¼Œæ—¨åœ¨é˜²æ­¢åˆæˆæœ‰å®³åŠ¨ç”»ï¼ŒåŒæ—¶ä¿æŒæ–‡æœ¬åˆ°åŠ¨ä½œç”Ÿæˆçš„æ•´ä½“æ€§èƒ½ã€‚å»å­¦ä¹ æœ‰å®³åŠ¨ä½œçš„æŒ‘æˆ˜åœ¨äºï¼Œè¿™äº›åŠ¨ä½œå¯ä»¥é€šè¿‡æ˜ç¡®çš„æ–‡æœ¬æç¤ºç”Ÿæˆï¼Œä¹Ÿå¯ä»¥é€šè¿‡å®‰å…¨åŠ¨ä½œçš„éšå«æœ‰å®³ç»„åˆç”Ÿæˆã€‚æˆ‘ä»¬æå‡ºäº†ç¬¬ä¸€ä¸ªåŠ¨ä½œå»å­¦ä¹ åŸºå‡†ï¼Œé€šè¿‡è¿‡æ»¤HumanML3Då’ŒMotion-Xè¿™ä¸¤ä¸ªå¤§å‹æ–‡æœ¬åˆ°åŠ¨ä½œæ•°æ®é›†ä¸­çš„æœ‰å®³åŠ¨ä½œã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§åŸºäºæ½œåœ¨ä»£ç æ›¿æ¢çš„æ–°å‹åŠ¨ä½œå»å­¦ä¹ æ¨¡å‹LCRï¼Œè¯¥æ¨¡å‹æ— éœ€è®­ç»ƒï¼Œé€‚ç”¨äºæœ€æ–°æ–‡æœ¬åˆ°åŠ¨ä½œæ‰©æ•£æ¨¡å‹çš„ç¦»æ•£æ½œåœ¨ç©ºé—´ã€‚'}}, 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2503.18494', 'title': 'Verbal Process Supervision Elicits Better Coding Agents', 'url': 'https://huggingface.co/papers/2503.18494', 'abstract': 'The emergence of large language models and their applications as AI agents have significantly advanced state-of-the-art code generation benchmarks, transforming modern software engineering tasks. However, even with test-time computed reasoning models, these systems still struggle with complex software engineering challenges. This work introduces CURA, a code understanding and reasoning agent system enhanced with verbal process supervision (VPS), achieving a 3.65\\% improvement over baseline models on challenging benchmarks like BigCodeBench. Furthermore, CURA, when paired with the o3-mini model and VPS techniques, attains state-of-the-art performance. This work represents a step forward in integrating reasoning-driven architectures with LLM-based code generation, enabling agentic reasoning for language models to solve complex software engineering tasks.', 'score': 0, 'issue_id': 2881, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 24', 'zh': '3æœˆ24æ—¥'}, 'hash': '97716e960b1a782d', 'authors': ['Hao-Yuan Chen', 'Cheng-Pong Huang', 'Jui-Ming Yao'], 'affiliations': ['Mindify AI, United States', 'National Taiwan University of Science and Technology, Taiwan', 'University of London, United Kingdom'], 'pdf_title_img': 'assets/pdf/title_img/2503.18494.jpg', 'data': {'categories': ['#training', '#benchmark', '#agents', '#agi', '#architecture', '#reasoning'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'CURA: ĞĞ³ĞµĞ½Ñ‚ Ñ Ğ²ĞµÑ€Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ CURA - ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¾ ĞºĞ¾Ğ´Ğµ, ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½ÑƒÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ²ĞµÑ€Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° (VPS). CURA Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ° 3.65% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº BigCodeBench. Ğ’ ÑĞ¾Ñ‡ĞµÑ‚Ğ°Ğ½Ğ¸Ğ¸ Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ o3-mini Ğ¸ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ°Ğ¼Ğ¸ VPS, CURA Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² ÑĞ²Ğ¾ĞµĞ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸. Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑˆĞ°Ğ³ Ğ²Ğ¿ĞµÑ€ĞµĞ´ Ğ² Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ…, Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ ĞºĞ¾Ğ´Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'CURA: Enhancing Code Generation with Reasoning and Supervision', 'desc': 'This paper presents CURA, a code understanding and reasoning agent that enhances large language models (LLMs) with verbal process supervision (VPS). CURA addresses the limitations of existing AI agents in tackling complex software engineering challenges by improving their reasoning capabilities. The system demonstrates a 3.65% performance boost on benchmarks like BigCodeBench compared to baseline models. By integrating reasoning-driven architectures with LLMs, CURA enables more effective code generation and problem-solving in software engineering tasks.'}, 'zh': {'title': 'CURAï¼šæå‡ä»£ç ç†è§£ä¸æ¨ç†çš„æ™ºèƒ½ä»£ç†ç³»ç»Ÿ', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºCURAçš„ä»£ç ç†è§£ä¸æ¨ç†ä»£ç†ç³»ç»Ÿï¼Œæ—¨åœ¨è§£å†³å¤æ‚çš„è½¯ä»¶å·¥ç¨‹é—®é¢˜ã€‚CURAé€šè¿‡å¼•å…¥è¯­è¨€è¿‡ç¨‹ç›‘ç£ï¼ˆVPSï¼‰æŠ€æœ¯ï¼Œæ˜¾è‘—æé«˜äº†åœ¨BigCodeBenchç­‰åŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°ï¼Œæå‡å¹…åº¦è¾¾åˆ°3.65%ã€‚æ­¤å¤–ï¼Œå½“CURAä¸o3-miniæ¨¡å‹ç»“åˆä½¿ç”¨æ—¶ï¼Œèƒ½å¤Ÿè¾¾åˆ°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¯¥ç ”ç©¶å±•ç¤ºäº†å°†æ¨ç†é©±åŠ¨æ¶æ„ä¸å¤§å‹è¯­è¨€æ¨¡å‹ç»“åˆçš„æ½œåŠ›ï¼Œä¸ºè¯­è¨€æ¨¡å‹åœ¨å¤æ‚è½¯ä»¶å·¥ç¨‹ä»»åŠ¡ä¸­çš„æ¨ç†èƒ½åŠ›æä¾›äº†æ–°çš„æ–¹å‘ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.18406', 'title': 'Instruct-CLIP: Improving Instruction-Guided Image Editing with Automated\n  Data Refinement Using Contrastive Learning', 'url': 'https://huggingface.co/papers/2503.18406', 'abstract': 'Although natural language instructions offer an intuitive way to guide automated image editing, deep-learning models often struggle to achieve high-quality results, largely due to challenges in creating large, high-quality training datasets. Previous work has typically relied on text-toimage (T2I) generative models to produce pairs of original and edited images that simulate the input/output of an instruction-guided image-editing model. However, these image pairs often fail to align with the specified edit instructions due to the limitations of T2I models, which negatively impacts models trained on such datasets. To address this, we present Instruct-CLIP, a self-supervised method that learns the semantic changes between original and edited images to refine and better align the instructions in existing datasets. Furthermore, we adapt Instruct-CLIP to handle noisy latent images and diffusion timesteps so that it can be used to train latent diffusion models (LDMs) [19] and efficiently enforce alignment between the edit instruction and the image changes in latent space at any step of the diffusion pipeline. We use Instruct-CLIP to correct the InstructPix2Pix dataset and get over 120K refined samples we then use to fine-tune their model, guided by our novel Instruct-CLIP-based loss function. The resulting model can produce edits that are more aligned with the given instructions. Our code and dataset are available at https://github.com/SherryXTChen/Instruct-CLIP.git.', 'score': 0, 'issue_id': 2893, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 24', 'zh': '3æœˆ24æ—¥'}, 'hash': '1683f929d40d4b06', 'authors': ['Sherry X. Chen', 'Misha Sra', 'Pradeep Sen'], 'affiliations': ['University of California, Santa Barbara'], 'pdf_title_img': 'assets/pdf/title_img/2503.18406.jpg', 'data': {'categories': ['#diffusion', '#data', '#cv', '#optimization', '#dataset', '#training'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Instruct-CLIP - ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼. ĞœĞµÑ‚Ğ¾Ğ´ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ¾Ñ‚Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ ÑƒÑ‚Ğ¾Ñ‡Ğ½Ğ¸Ñ‚ÑŒ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸ Ğ»ÑƒÑ‡ÑˆĞµ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ñ… Ñ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸ÑĞ¼Ğ¸. Instruct-CLIP Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ ÑˆĞ°Ğ³Ğ°Ğ¼Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞµĞ³Ğ¾ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ¡ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Instruct-CLIP Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… InstructPix2Pix Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ¸Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½ÑƒÑ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼.'}, 'en': {'title': 'Aligning Image Edits with Natural Language Instructions', 'desc': 'This paper introduces Instruct-CLIP, a self-supervised method designed to improve the alignment between image edits and natural language instructions. It addresses the limitations of traditional text-to-image models that often produce poorly aligned image pairs for training. By learning the semantic changes between original and edited images, Instruct-CLIP refines existing datasets and enhances the training of latent diffusion models. The method results in a more accurate model that generates image edits that closely match the specified instructions, significantly improving the quality of automated image editing.'}, 'zh': {'title': 'æå‡å›¾åƒç¼–è¾‘æŒ‡ä»¤å¯¹é½çš„è‡ªç›‘ç£æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºInstruct-CLIPçš„è‡ªç›‘ç£æ–¹æ³•ï¼Œæ—¨åœ¨æ”¹å–„åŸºäºè‡ªç„¶è¯­è¨€æŒ‡ä»¤çš„å›¾åƒç¼–è¾‘æ¨¡å‹çš„æ€§èƒ½ã€‚é€šè¿‡å­¦ä¹ åŸå§‹å›¾åƒä¸ç¼–è¾‘å›¾åƒä¹‹é—´çš„è¯­ä¹‰å˜åŒ–ï¼ŒInstruct-CLIPèƒ½å¤Ÿæ›´å¥½åœ°å¯¹é½ç°æœ‰æ•°æ®é›†ä¸­çš„æŒ‡ä»¤ã€‚è¯¥æ–¹æ³•è¿˜é€‚åº”äº†å¤„ç†å™ªå£°æ½œåœ¨å›¾åƒå’Œæ‰©æ•£æ—¶é—´æ­¥ï¼Œä»¥ä¾¿åœ¨æ½œåœ¨æ‰©æ•£æ¨¡å‹ä¸­æœ‰æ•ˆåœ°æ‰§è¡ŒæŒ‡ä»¤ä¸å›¾åƒå˜åŒ–ä¹‹é—´çš„å¯¹é½ã€‚æœ€ç»ˆï¼Œä½¿ç”¨Instruct-CLIPä¿®æ­£åçš„æ•°æ®é›†ç”Ÿæˆäº†è¶…è¿‡12ä¸‡æ¡æ ·æœ¬ï¼Œä»è€Œæé«˜äº†æ¨¡å‹å¯¹æŒ‡ä»¤çš„å“åº”èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.16709', 'title': 'QuartDepth: Post-Training Quantization for Real-Time Depth Estimation on\n  the Edge', 'url': 'https://huggingface.co/papers/2503.16709', 'abstract': 'Monocular Depth Estimation (MDE) has emerged as a pivotal task in computer vision, supporting numerous real-world applications. However, deploying accurate depth estimation models on resource-limited edge devices, especially Application-Specific Integrated Circuits (ASICs), is challenging due to the high computational and memory demands. Recent advancements in foundational depth estimation deliver impressive results but further amplify the difficulty of deployment on ASICs. To address this, we propose QuartDepth which adopts post-training quantization to quantize MDE models with hardware accelerations for ASICs. Our approach involves quantizing both weights and activations to 4-bit precision, reducing the model size and computation cost. To mitigate the performance degradation, we introduce activation polishing and compensation algorithm applied before and after activation quantization, as well as a weight reconstruction method for minimizing errors in weight quantization. Furthermore, we design a flexible and programmable hardware accelerator by supporting kernel fusion and customized instruction programmability, enhancing throughput and efficiency. Experimental results demonstrate that our framework achieves competitive accuracy while enabling fast inference and higher energy efficiency on ASICs, bridging the gap between high-performance depth estimation and practical edge-device applicability. Code: https://github.com/shawnricecake/quart-depth', 'score': 0, 'issue_id': 2893, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 20', 'zh': '3æœˆ20æ—¥'}, 'hash': '4d36736f3565bd06', 'authors': ['Xuan Shen', 'Weize Ma', 'Jing Liu', 'Changdi Yang', 'Rui Ding', 'Quanyi Wang', 'Henghui Ding', 'Wei Niu', 'Yanzhi Wang', 'Pu Zhao', 'Jun Lin', 'Jiuxiang Gu'], 'affiliations': ['Adobe Research', 'Fudan University', 'Monash University', 'Nanjing University', 'Nanjing University of Information Science and Technology', 'Northeastern University', 'University of Georgia'], 'pdf_title_img': 'assets/pdf/title_img/2503.16709.jpg', 'data': {'categories': ['#optimization', '#cv', '#inference'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ½Ğ° ĞºÑ€Ğ°ĞµĞ²Ñ‹Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ QuartDepth - Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ¾Ğ½Ğ¾ĞºÑƒĞ»ÑÑ€Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ½Ğ° ASIC. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²ĞµÑĞ¾Ğ² Ğ¸ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¹ Ğ´Ğ¾ 4 Ğ±Ğ¸Ñ‚, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¹ Ğ¸ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ²ĞµÑĞ¾Ğ² Ğ´Ğ»Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ğ³Ğ¸Ğ±ĞºĞ¸Ğ¹ Ğ°Ğ¿Ğ¿Ğ°Ñ€Ğ°Ñ‚Ğ½Ñ‹Ğ¹ ÑƒÑĞºĞ¾Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒ Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ ÑĞ´ĞµÑ€ Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ½ÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ¼ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğµ Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ ÑĞ½ĞµÑ€Ğ³Ğ¾ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° ASIC.'}, 'en': {'title': 'Optimizing Depth Estimation for Edge Devices with QuartDepth', 'desc': 'This paper presents QuartDepth, a novel approach for Monocular Depth Estimation (MDE) that focuses on optimizing models for resource-limited edge devices like ASICs. It utilizes post-training quantization to reduce the model size and computational requirements by quantizing weights and activations to 4-bit precision. To counteract potential performance loss from quantization, the authors introduce techniques such as activation polishing and a weight reconstruction method. The proposed framework not only maintains competitive accuracy but also enhances inference speed and energy efficiency, making high-performance depth estimation feasible on edge devices.'}, 'zh': {'title': 'é«˜æ•ˆçš„å•ç›®æ·±åº¦ä¼°è®¡è§£å†³æ–¹æ¡ˆ', 'desc': 'å•ç›®æ·±åº¦ä¼°è®¡ï¼ˆMDEï¼‰åœ¨è®¡ç®—æœºè§†è§‰ä¸­æ‰®æ¼”ç€é‡è¦è§’è‰²ï¼Œå¹¿æ³›åº”ç”¨äºç°å®ä¸–ç•Œä¸­ã€‚ç„¶è€Œï¼Œåœ¨èµ„æºæœ‰é™çš„è¾¹ç¼˜è®¾å¤‡ä¸Šï¼Œå°¤å…¶æ˜¯åº”ç”¨ç‰¹å®šé›†æˆç”µè·¯ï¼ˆASICsï¼‰ä¸Šï¼Œå‡†ç¡®çš„æ·±åº¦ä¼°è®¡æ¨¡å‹çš„éƒ¨ç½²é¢ä¸´æŒ‘æˆ˜ï¼Œå› ä¸ºå…¶è®¡ç®—å’Œå†…å­˜éœ€æ±‚è¾ƒé«˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†QuartDepthï¼Œé€šè¿‡åè®­ç»ƒé‡åŒ–æŠ€æœ¯å¯¹MDEæ¨¡å‹è¿›è¡Œé‡åŒ–ï¼Œä»¥é€‚åº”ASICsçš„ç¡¬ä»¶åŠ é€Ÿã€‚æˆ‘ä»¬çš„æ–¹æ¡ˆé€šè¿‡å°†æƒé‡å’Œæ¿€æ´»é‡åŒ–åˆ°4ä½ç²¾åº¦ï¼Œå‡å°‘æ¨¡å‹å¤§å°å’Œè®¡ç®—æˆæœ¬ï¼ŒåŒæ—¶å¼•å…¥æ¿€æ´»æŠ›å…‰å’Œè¡¥å¿ç®—æ³•ï¼Œä»¥å‡è½»æ€§èƒ½ä¸‹é™ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.16426', 'title': 'DynamicVis: An Efficient and General Visual Foundation Model for Remote\n  Sensing Image Understanding', 'url': 'https://huggingface.co/papers/2503.16426', 'abstract': "The advancement of remote sensing technology has improved the spatial resolution of satellite imagery, facilitating more detailed visual representations for diverse interpretations. However, existing methods exhibit limited generalization capabilities across varied applications. While some contemporary foundation models demonstrate potential, they are hindered by insufficient cross-task adaptability and primarily process low-resolution imagery of restricted sizes, thus failing to fully exploit high-resolution data or leverage comprehensive large-scene semantics. Crucially, remote sensing imagery differs fundamentally from natural images, as key foreground targets (eg., maritime objects, artificial structures) often occupy minimal spatial proportions (~1%) and exhibit sparse distributions. Efficiently modeling cross-task generalizable knowledge from lengthy 2D tokens (~100,000) poses a significant challenge yet remains critical for remote sensing image understanding. Motivated by the selective attention mechanisms inherent to the human visual system, we propose DynamicVis, a dynamic visual perception foundation model for remote sensing imagery. The framework integrates a novel dynamic region perception backbone based on the selective state space model, which strategically balances localized detail extraction with global contextual integration, enabling computationally efficient encoding of large-scale data while maintaining architectural scalability. To enhance cross-task knowledge transferring, we introduce a multi-instance learning paradigm utilizing meta-embedding representations, trained on million-scale region-level annotations. Evaluations across nine downstream tasks demonstrate the model's versatility. DynamicVis achieves multi-level feature modeling with exceptional efficiency, processing (2048x2048) pixels with 97 ms latency (6% of ViT's) and 833 MB GPU memory (3% of ViT's).", 'score': 0, 'issue_id': 2885, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 20', 'zh': '3æœˆ20æ—¥'}, 'hash': '973f386b0cfb485a', 'authors': ['Keyan Chen', 'Chenyang Liu', 'Bowen Chen', 'Wenyuan Li', 'Zhengxia Zou', 'Zhenwei Shi'], 'affiliations': ['Beihang University', 'the University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2503.16426.jpg', 'data': {'categories': ['#cv', '#optimization', '#transfer_learning', '#training', '#architecture'], 'emoji': 'ğŸ›°ï¸', 'ru': {'title': 'DynamicVis: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° ÑĞ¿ÑƒÑ‚Ğ½Ğ¸ĞºĞ¾Ğ²Ñ‹Ñ… ÑĞ½Ğ¸Ğ¼ĞºĞ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ DynamicVis - Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° ÑĞ¿ÑƒÑ‚Ğ½Ğ¸ĞºĞ¾Ğ²Ñ‹Ñ… ÑĞ½Ğ¸Ğ¼ĞºĞ¾Ğ² Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞµĞ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. DynamicVis Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ñ… Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ĞµĞ¹ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ 2048x2048 Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹ Ğ·Ğ° 97 Ğ¼Ñ Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ²ÑĞµĞ³Ğ¾ 833 ĞœĞ‘ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸.'}, 'en': {'title': 'DynamicVis: Revolutionizing Remote Sensing with Efficient Cross-Task Learning', 'desc': 'This paper presents DynamicVis, a foundation model designed specifically for remote sensing imagery, which often contains sparse and small foreground targets. The model addresses the limitations of existing methods by enhancing cross-task adaptability and efficiently processing high-resolution satellite images. By employing a dynamic region perception backbone and a multi-instance learning paradigm, DynamicVis achieves effective feature modeling while maintaining low latency and memory usage. Evaluations show that it excels across various tasks, demonstrating its potential for improved understanding of remote sensing data.'}, 'zh': {'title': 'DynamicVisï¼šé¥æ„Ÿå›¾åƒå¤„ç†çš„æ–°çªç ´', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºDynamicVisçš„åŠ¨æ€è§†è§‰æ„ŸçŸ¥åŸºç¡€æ¨¡å‹ï¼Œä¸“é—¨ç”¨äºé¥æ„Ÿå›¾åƒå¤„ç†ã€‚è¯¥æ¨¡å‹é€šè¿‡é€‰æ‹©æ€§çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼Œå¹³è¡¡å±€éƒ¨ç»†èŠ‚æå–ä¸å…¨å±€ä¸Šä¸‹æ–‡æ•´åˆï¼Œä»è€Œé«˜æ•ˆç¼–ç å¤§è§„æ¨¡æ•°æ®ã€‚DynamicVisé‡‡ç”¨å¤šå®ä¾‹å­¦ä¹ èŒƒå¼ï¼Œåˆ©ç”¨å…ƒåµŒå…¥è¡¨ç¤ºï¼Œæå‡è·¨ä»»åŠ¡çŸ¥è¯†è½¬ç§»èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å¤„ç†é«˜åˆ†è¾¨ç‡å›¾åƒæ—¶è¡¨ç°å‡ºè‰²ï¼Œå…·æœ‰æé«˜çš„æ•ˆç‡å’Œçµæ´»æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.13358', 'title': 'One-Step Residual Shifting Diffusion for Image Super-Resolution via\n  Distillation', 'url': 'https://huggingface.co/papers/2503.13358', 'abstract': 'Diffusion models for super-resolution (SR) produce high-quality visual results but require expensive computational costs. Despite the development of several methods to accelerate diffusion-based SR models, some (e.g., SinSR) fail to produce realistic perceptual details, while others (e.g., OSEDiff) may hallucinate non-existent structures. To overcome these issues, we present RSD, a new distillation method for ResShift, one of the top diffusion-based SR models. Our method is based on training the student network to produce such images that a new fake ResShift model trained on them will coincide with the teacher model. RSD achieves single-step restoration and outperforms the teacher by a large margin. We show that our distillation method can surpass the other distillation-based method for ResShift - SinSR - making it on par with state-of-the-art diffusion-based SR distillation methods. Compared to SR methods based on pre-trained text-to-image models, RSD produces competitive perceptual quality, provides images with better alignment to degraded input images, and requires fewer parameters and GPU memory. We provide experimental results on various real-world and synthetic datasets, including RealSR, RealSet65, DRealSR, ImageNet, and DIV2K.', 'score': 82, 'issue_id': 2830, 'pub_date': '2025-03-17', 'pub_date_card': {'ru': '17 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 17', 'zh': '3æœˆ17æ—¥'}, 'hash': '7c30d8250da0c284', 'authors': ['Daniil Selikhanovych', 'David Li', 'Aleksei Leonov', 'Nikita Gushchin', 'Sergei Kushneriuk', 'Alexander Filippov', 'Evgeny Burnaev', 'Iaroslav Koshelev', 'Alexander Korotin'], 'affiliations': ['AI Foundation and Algorithm Lab', 'AIRI', 'HSE University', 'MIPT', 'Skoltech', 'Yandex Research'], 'pdf_title_img': 'assets/pdf/title_img/2503.13358.jpg', 'data': {'categories': ['#synthetic', '#hallucinations', '#training', '#diffusion', '#cv', '#optimization'], 'emoji': 'ğŸ”', 'ru': {'title': 'RSD: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ ÑĞ²ĞµÑ€Ñ…Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ²ĞµÑ€Ñ…Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ RSD. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ ÑÑ‚ÑƒĞ´ĞµĞ½Ñ‡ĞµÑĞºÑƒÑ ÑĞµÑ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‚Ğ°ĞºĞ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ½Ğ¾Ğ²Ğ°Ñ Ñ„ĞµĞ¹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ResShift, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ½Ğ¸Ñ…, ÑĞ¾Ğ²Ğ¿Ğ°Ğ´Ğ°Ğ»Ğ° Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ. RSD Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ·Ğ° Ğ¾Ğ´Ğ¸Ğ½ ÑˆĞ°Ğ³ Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒÑĞºÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ. ĞŸĞ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸, RSD Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµĞµ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ğ½Ğ¸Ğ·ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸, Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¸ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ GPU.'}, 'en': {'title': 'RSD: Revolutionizing Super-Resolution with Efficient Distillation', 'desc': 'This paper introduces RSD, a novel distillation method designed to enhance the performance of ResShift, a leading diffusion model for super-resolution (SR). RSD trains a student network to generate images that align closely with those produced by a teacher model, ensuring high-quality visual results while reducing computational costs. The method demonstrates significant improvements over existing techniques, such as SinSR, by maintaining realistic perceptual details and avoiding the hallucination of non-existent structures. Experimental results show that RSD not only matches but often exceeds the performance of state-of-the-art diffusion-based SR methods, while also being more efficient in terms of parameters and GPU memory usage.'}, 'zh': {'title': 'RSDï¼šè¶…åˆ†è¾¨ç‡çš„æ–°çªç ´', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è’¸é¦æ–¹æ³•RSDï¼Œç”¨äºæ”¹è¿›åŸºäºæ‰©æ•£æ¨¡å‹çš„è¶…åˆ†è¾¨ç‡(SR)æŠ€æœ¯ã€‚RSDé€šè¿‡è®­ç»ƒå­¦ç”Ÿç½‘ç»œç”Ÿæˆå›¾åƒï¼Œä½¿å¾—ä¸€ä¸ªæ–°çš„å‡ResShiftæ¨¡å‹åœ¨è¿™äº›å›¾åƒä¸Šä¸æ•™å¸ˆæ¨¡å‹ä¸€è‡´ï¼Œä»è€Œå®ç°å•æ­¥æ¢å¤ã€‚ä¸å…¶ä»–è’¸é¦æ–¹æ³•ç›¸æ¯”ï¼ŒRSDåœ¨æ„ŸçŸ¥è´¨é‡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå¹¶ä¸”åœ¨å‚æ•°å’ŒGPUå†…å­˜éœ€æ±‚ä¸Šæ›´å…·ä¼˜åŠ¿ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRSDåœ¨å¤šä¸ªçœŸå®å’Œåˆæˆæ•°æ®é›†ä¸Šå‡ä¼˜äºç°æœ‰çš„è¶…åˆ†è¾¨ç‡æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.16416', 'title': 'Survey on Evaluation of LLM-based Agents', 'url': 'https://huggingface.co/papers/2503.16416', 'abstract': 'The emergence of LLM-based agents represents a paradigm shift in AI, enabling autonomous systems to plan, reason, use tools, and maintain memory while interacting with dynamic environments. This paper provides the first comprehensive survey of evaluation methodologies for these increasingly capable agents. We systematically analyze evaluation benchmarks and frameworks across four critical dimensions: (1) fundamental agent capabilities, including planning, tool use, self-reflection, and memory; (2) application-specific benchmarks for web, software engineering, scientific, and conversational agents; (3) benchmarks for generalist agents; and (4) frameworks for evaluating agents. Our analysis reveals emerging trends, including a shift toward more realistic, challenging evaluations with continuously updated benchmarks. We also identify critical gaps that future research must address-particularly in assessing cost-efficiency, safety, and robustness, and in developing fine-grained, and scalable evaluation methods. This survey maps the rapidly evolving landscape of agent evaluation, reveals the emerging trends in the field, identifies current limitations, and proposes directions for future research.', 'score': 58, 'issue_id': 2828, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 20', 'zh': '3æœˆ20æ—¥'}, 'hash': '8d23d348a6ecdbce', 'authors': ['Asaf Yehudai', 'Lilach Eden', 'Alan Li', 'Guy Uziel', 'Yilun Zhao', 'Roy Bar-Haim', 'Arman Cohan', 'Michal Shmueli-Scheuer'], 'affiliations': ['IBM Research', 'The Hebrew University of Jerusalem', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2503.16416.jpg', 'data': {'categories': ['#benchmark', '#survey', '#reasoning', '#agents'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞšĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞ¸ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸ Ğ¸ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ¸ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾ Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµĞ¼ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ Ğ°ÑĞ¿ĞµĞºÑ‚Ğ°Ğ¼: Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸, Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸ Ğ´Ğ»Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Ñ‚ĞµĞ½Ğ´ĞµĞ½Ñ†Ğ¸Ñ Ğº Ğ±Ğ¾Ğ»ĞµĞµ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ñ‹ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLM.'}, 'en': {'title': 'Evaluating the Future of LLM Agents: Trends and Gaps', 'desc': 'This paper surveys the evaluation methods for large language model (LLM)-based agents, which are AI systems capable of planning, reasoning, and interacting with their environments. It categorizes evaluation benchmarks into four key areas: fundamental capabilities, application-specific benchmarks, generalist agent benchmarks, and evaluation frameworks. The authors highlight a trend towards more realistic evaluations that adapt over time, while also pointing out significant gaps in current research, such as the need for better assessments of cost-efficiency and safety. The paper aims to provide a comprehensive overview of the evaluation landscape and suggest future research directions to enhance agent assessment.'}, 'zh': {'title': 'æ™ºèƒ½ä½“è¯„ä¼°çš„æ–°è§†é‡', 'desc': 'æœ¬è®ºæ–‡å¯¹åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ™ºèƒ½ä½“è¯„ä¼°æ–¹æ³•è¿›è¡Œäº†å…¨é¢çš„è°ƒæŸ¥ã€‚æˆ‘ä»¬åˆ†æäº†æ™ºèƒ½ä½“èƒ½åŠ›ã€åº”ç”¨ç‰¹å®šåŸºå‡†ã€é€šç”¨æ™ºèƒ½ä½“åŸºå‡†å’Œè¯„ä¼°æ¡†æ¶ç­‰å››ä¸ªå…³é”®ç»´åº¦ã€‚ç ”ç©¶å‘ç°ï¼Œè¯„ä¼°æ–¹æ³•æ­£æœç€æ›´çœŸå®å’Œå…·æœ‰æŒ‘æˆ˜æ€§çš„æ–¹å‘å‘å±•ï¼ŒåŒæ—¶ä¹ŸæŒ‡å‡ºäº†åœ¨æˆæœ¬æ•ˆç‡ã€å®‰å…¨æ€§å’Œé²æ£’æ€§è¯„ä¼°æ–¹é¢çš„ç ”ç©¶ç©ºç™½ã€‚è¯¥è°ƒæŸ¥ä¸ºæ™ºèƒ½ä½“è¯„ä¼°çš„å¿«é€Ÿå‘å±•æä¾›äº†åœ°å›¾ï¼Œæ­ç¤ºäº†é¢†åŸŸä¸­çš„æ–°å…´è¶‹åŠ¿å’Œæœªæ¥ç ”ç©¶çš„æ–¹å‘ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.16419', 'title': 'Stop Overthinking: A Survey on Efficient Reasoning for Large Language\n  Models', 'url': 'https://huggingface.co/papers/2503.16419', 'abstract': 'Large Language Models (LLMs) have demonstrated remarkable capabilities in complex tasks. Recent advancements in Large Reasoning Models (LRMs), such as OpenAI o1 and DeepSeek-R1, have further improved performance in System-2 reasoning domains like mathematics and programming by harnessing supervised fine-tuning (SFT) and reinforcement learning (RL) techniques to enhance the Chain-of-Thought (CoT) reasoning. However, while longer CoT reasoning sequences improve performance, they also introduce significant computational overhead due to verbose and redundant outputs, known as the "overthinking phenomenon". In this paper, we provide the first structured survey to systematically investigate and explore the current progress toward achieving efficient reasoning in LLMs. Overall, relying on the inherent mechanism of LLMs, we categorize existing works into several key directions: (1) model-based efficient reasoning, which considers optimizing full-length reasoning models into more concise reasoning models or directly training efficient reasoning models; (2) reasoning output-based efficient reasoning, which aims to dynamically reduce reasoning steps and length during inference; (3) input prompts-based efficient reasoning, which seeks to enhance reasoning efficiency based on input prompt properties such as difficulty or length control. Additionally, we introduce the use of efficient data for training reasoning models, explore the reasoning capabilities of small language models, and discuss evaluation methods and benchmarking.', 'score': 52, 'issue_id': 2824, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 20', 'zh': '3æœˆ20æ—¥'}, 'hash': 'dc9b04a227f74af7', 'authors': ['Yang Sui', 'Yu-Neng Chuang', 'Guanchu Wang', 'Jiamu Zhang', 'Tianyi Zhang', 'Jiayi Yuan', 'Hongyi Liu', 'Andrew Wen', 'Shaochen', 'Zhong', 'Hanjie Chen', 'Xia Hu'], 'affiliations': ['Department of Computer Science, Rice University'], 'pdf_title_img': 'assets/pdf/title_img/2503.16419.jpg', 'data': {'categories': ['#small_models', '#optimization', '#reasoning', '#rl', '#dataset', '#survey', '#training', '#benchmark'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ² LLM: Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€Ğ¸ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ: Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑĞ°Ğ¼Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ ÑˆĞ°Ğ³Ğ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğµ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ². Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¾Ğ±ÑÑƒĞ¶Ğ´Ğ°ĞµÑ‚ÑÑ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¾Ñ‚Ğ¼ĞµÑ‡Ğ°ĞµÑ‚ÑÑ, Ñ‡Ñ‚Ğ¾ Ñ…Ğ¾Ñ‚Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ, Ğ¾Ğ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°ÑÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ¸Ğ·-Ğ·Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ»Ğ¾Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Optimizing Reasoning Efficiency in Large Language Models', 'desc': 'This paper surveys the advancements in Large Reasoning Models (LRMs) that enhance reasoning capabilities in complex tasks like mathematics and programming. It highlights the use of supervised fine-tuning and reinforcement learning to improve Chain-of-Thought (CoT) reasoning, while addressing the computational challenges posed by longer reasoning sequences. The authors categorize existing approaches into three main strategies: optimizing reasoning models, reducing reasoning output length, and improving input prompts for efficiency. Additionally, the paper discusses the potential of small language models and introduces methods for evaluating reasoning performance.'}, 'zh': {'title': 'é«˜æ•ˆæ¨ç†ï¼šå¤§å‹è¯­è¨€æ¨¡å‹çš„æ–°æ¢ç´¢', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚ä»»åŠ¡ä¸­å±•ç°äº†å“è¶Šçš„èƒ½åŠ›ã€‚æœ€è¿‘ï¼Œå¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰çš„è¿›å±•è¿›ä¸€æ­¥æå‡äº†åœ¨æ•°å­¦å’Œç¼–ç¨‹ç­‰ç³»ç»Ÿ-2æ¨ç†é¢†åŸŸçš„è¡¨ç°ï¼Œåˆ©ç”¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æŠ€æœ¯æ¥å¢å¼ºæ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†ã€‚ç„¶è€Œï¼Œè¾ƒé•¿çš„CoTæ¨ç†åºåˆ—è™½ç„¶æé«˜äº†æ€§èƒ½ï¼Œå´ä¹Ÿå¸¦æ¥äº†æ˜¾è‘—çš„è®¡ç®—å¼€é”€ï¼Œè¿™ç§ç°è±¡è¢«ç§°ä¸ºâ€œè¿‡åº¦æ€è€ƒç°è±¡â€ã€‚æœ¬æ–‡é¦–æ¬¡ç³»ç»Ÿæ€§åœ°è°ƒæŸ¥å’Œæ¢ç´¢äº†å®ç°LLMsé«˜æ•ˆæ¨ç†çš„å½“å‰è¿›å±•ï¼Œå¹¶å°†ç°æœ‰å·¥ä½œåˆ†ç±»ä¸ºå¤šä¸ªå…³é”®æ–¹å‘ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.16302', 'title': 'Unleashing Vecset Diffusion Model for Fast Shape Generation', 'url': 'https://huggingface.co/papers/2503.16302', 'abstract': '3D shape generation has greatly flourished through the development of so-called "native" 3D diffusion, particularly through the Vecset Diffusion Model (VDM). While recent advancements have shown promising results in generating high-resolution 3D shapes, VDM still struggles with high-speed generation. Challenges exist because of difficulties not only in accelerating diffusion sampling but also VAE decoding in VDM, areas under-explored in previous works. To address these challenges, we present FlashVDM, a systematic framework for accelerating both VAE and DiT in VDM. For DiT, FlashVDM enables flexible diffusion sampling with as few as 5 inference steps and comparable quality, which is made possible by stabilizing consistency distillation with our newly introduced Progressive Flow Distillation. For VAE, we introduce a lightning vecset decoder equipped with Adaptive KV Selection, Hierarchical Volume Decoding, and Efficient Network Design. By exploiting the locality of the vecset and the sparsity of shape surface in the volume, our decoder drastically lowers FLOPs, minimizing the overall decoding overhead. We apply FlashVDM to Hunyuan3D-2 to obtain Hunyuan3D-2 Turbo. Through systematic evaluation, we show that our model significantly outperforms existing fast 3D generation methods, achieving comparable performance to the state-of-the-art while reducing inference time by over 45x for reconstruction and 32x for generation. Code and models are available at https://github.com/Tencent/FlashVDM.', 'score': 35, 'issue_id': 2824, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 20', 'zh': '3æœˆ20æ—¥'}, 'hash': 'd91e862542d76892', 'authors': ['Zeqiang Lai', 'Yunfei Zhao', 'Zibo Zhao', 'Haolin Liu', 'Fuyun Wang', 'Huiwen Shi', 'Xianghui Yang', 'Qinxiang Lin', 'Jinwei Huang', 'Yuhong Liu', 'Jie Jiang', 'Chunchao Guo', 'Xiangyu Yue'], 'affiliations': ['MMLab, CUHK', 'ShanghaiTech', 'Tencent Hunyuan'], 'pdf_title_img': 'assets/pdf/title_img/2503.16302.jpg', 'data': {'categories': ['#open_source', '#3d', '#diffusion', '#optimization', '#inference'], 'emoji': 'ğŸš€', 'ru': {'title': 'ĞœĞ¾Ğ»Ğ½Ğ¸ĞµĞ½Ğ¾ÑĞ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ 3D-Ñ„Ğ¾Ñ€Ğ¼ Ñ FlashVDM', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ FlashVDM - ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D-Ñ„Ğ¾Ñ€Ğ¼ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (VDM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ğº Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸, Ñ‚Ğ°Ğº Ğ¸ Ğ´Ğ»Ñ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ VAE, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´ Progressive Flow Distillation Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ñ… Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ². FlashVDM Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ 3D-Ñ„Ğ¾Ñ€Ğ¼Ñ‹ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²ÑĞµĞ³Ğ¾ Ğ·Ğ° 5 ÑˆĞ°Ğ³Ğ¾Ğ² Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°Ñ Ğ²Ñ€ĞµĞ¼Ñ inference. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ FlashVDM Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ¹ 3D-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°Ñ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ² 32-45 Ñ€Ğ°Ğ· Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ state-of-the-art.'}, 'en': {'title': 'Accelerating 3D Shape Generation with FlashVDM', 'desc': 'This paper introduces FlashVDM, a new framework designed to enhance the speed of 3D shape generation using the Vecset Diffusion Model (VDM). It addresses the slow generation times associated with VDM by improving both the Variational Autoencoder (VAE) and the Diffusion Transformer (DiT) components. FlashVDM achieves faster diffusion sampling with fewer inference steps while maintaining high-quality outputs through a technique called Progressive Flow Distillation. Additionally, it optimizes the VAE with a new decoder that reduces computational load, resulting in significant improvements in inference speed without sacrificing performance.'}, 'zh': {'title': 'åŠ é€Ÿ3Då½¢çŠ¶ç”Ÿæˆçš„FlashVDMæ¡†æ¶', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºFlashVDMçš„æ¡†æ¶ï¼Œæ—¨åœ¨åŠ é€Ÿ3Då½¢çŠ¶ç”Ÿæˆä¸­çš„VAEå’ŒDiTè¿‡ç¨‹ã€‚é€šè¿‡å¼•å…¥æ¸è¿›æµè’¸é¦ï¼ŒFlashVDMå®ç°äº†çµæ´»çš„æ‰©æ•£é‡‡æ ·ï¼Œä»…éœ€5ä¸ªæ¨ç†æ­¥éª¤å³å¯è·å¾—ä¸ç°æœ‰æ–¹æ³•ç›¸å½“çš„è´¨é‡ã€‚æˆ‘ä»¬è¿˜è®¾è®¡äº†ä¸€ç§é—ªç”µvecsetè§£ç å™¨ï¼Œåˆ©ç”¨è‡ªé€‚åº”KVé€‰æ‹©å’Œåˆ†å±‚ä½“ç§¯è§£ç ï¼Œæ˜¾è‘—é™ä½äº†è®¡ç®—å¤æ‚åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFlashVDMåœ¨é‡å»ºå’Œç”Ÿæˆæ–¹é¢çš„æ¨ç†æ—¶é—´åˆ†åˆ«å‡å°‘äº†è¶…è¿‡45å€å’Œ32å€ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰çš„å¿«é€Ÿ3Dç”Ÿæˆæ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.15558', 'title': 'Cosmos-Reason1: From Physical Common Sense To Embodied Reasoning', 'url': 'https://huggingface.co/papers/2503.15558', 'abstract': 'Physical AI systems need to perceive, understand, and perform complex actions in the physical world. In this paper, we present the Cosmos-Reason1 models that can understand the physical world and generate appropriate embodied decisions (e.g., next step action) in natural language through long chain-of-thought reasoning processes. We begin by defining key capabilities for Physical AI reasoning, with a focus on physical common sense and embodied reasoning. To represent physical common sense, we use a hierarchical ontology that captures fundamental knowledge about space, time, and physics. For embodied reasoning, we rely on a two-dimensional ontology that generalizes across different physical embodiments. Building on these capabilities, we develop two multimodal large language models, Cosmos-Reason1-8B and Cosmos-Reason1-56B. We curate data and train our models in four stages: vision pre-training, general supervised fine-tuning (SFT), Physical AI SFT, and Physical AI reinforcement learning (RL) as the post-training. To evaluate our models, we build comprehensive benchmarks for physical common sense and embodied reasoning according to our ontologies. Evaluation results show that Physical AI SFT and reinforcement learning bring significant improvements. To facilitate the development of Physical AI, we will make our code and pre-trained models available under the NVIDIA Open Model License at https://github.com/nvidia-cosmos/cosmos-reason1.', 'score': 29, 'issue_id': 2823, 'pub_date': '2025-03-18', 'pub_date_card': {'ru': '18 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 18', 'zh': '3æœˆ18æ—¥'}, 'hash': '882bcaa2257d8251', 'authors': ['NVIDIA', ':', 'Alisson Azzolini', 'Hannah Brandon', 'Prithvijit Chattopadhyay', 'Huayu Chen', 'Jinju Chu', 'Yin Cui', 'Jenna Diamond', 'Yifan Ding', 'Francesco Ferroni', 'Rama Govindaraju', 'Jinwei Gu', 'Siddharth Gururani', 'Imad El Hanafi', 'Zekun Hao', 'Jacob Huffman', 'Jingyi Jin', 'Brendan Johnson', 'Rizwan Khan', 'George Kurian', 'Elena Lantz', 'Nayeon Lee', 'Zhaoshuo Li', 'Xuan Li', 'Tsung-Yi Lin', 'Yen-Chen Lin', 'Ming-Yu Liu', 'Andrew Mathau', 'Yun Ni', 'Lindsey Pavao', 'Wei Ping', 'David W. Romero', 'Misha Smelyanskiy', 'Shuran Song', 'Lyne Tchapmi', 'Andrew Z. Wang', 'Boxin Wang', 'Haoxiang Wang', 'Fangyin Wei', 'Jiashu Xu', 'Yao Xu', 'Xiaodong Yang', 'Zhuolin Yang', 'Xiaohui Zeng', 'Zhe Zhang'], 'affiliations': ['NVIDIA'], 'pdf_title_img': 'assets/pdf/title_img/2503.15558.jpg', 'data': {'categories': ['#training', '#rl', '#benchmark', '#open_source', '#agents', '#reasoning', '#multimodal'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ¤Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ˜Ğ˜: Ğ¾Ñ‚ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¸Ñ€Ğ° Ğº Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑĞ¼', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Cosmos-Reason1, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¼Ğ¸Ñ€ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ñ‹ Ğ½Ğ° Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ğ½Ñ‚Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ·Ğ´Ñ€Ğ°Ğ²Ğ¾Ğ³Ğ¾ ÑĞ¼Ñ‹ÑĞ»Ğ° Ğ¸ Ğ´Ğ²ÑƒĞ¼ĞµÑ€Ğ½Ğ¾Ğ¹ Ğ¾Ğ½Ñ‚Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ¸Ğ¹. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ğ»Ğ¾ÑÑŒ Ğ² Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ ÑÑ‚Ğ°Ğ¿Ğ°: Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ·Ñ€ĞµĞ½Ğ¸Ñ, Ğ¾Ğ±Ñ‰Ğ°Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ°Ñ Ğ´Ğ¾Ğ²Ğ¾Ğ´ĞºĞ°, ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ°Ñ Ğ´Ğ¾Ğ²Ğ¾Ğ´ĞºĞ° Ğ´Ğ»Ñ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ˜Ğ˜ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ÑĞ»Ğµ ÑÑ‚Ğ°Ğ¿Ğ¾Ğ² Ğ´Ğ¾Ğ²Ğ¾Ğ´ĞºĞ¸ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ˜Ğ˜.'}, 'en': {'title': 'Empowering Physical AI with Cosmos-Reason1 Models', 'desc': 'This paper introduces the Cosmos-Reason1 models designed for Physical AI systems, which need to understand and act in the physical world. The models utilize long chain-of-thought reasoning to generate actions in natural language, focusing on physical common sense and embodied reasoning. A hierarchical ontology is employed to represent physical common sense, while a two-dimensional ontology supports various physical embodiments. The authors train their models through a structured process, including vision pre-training and reinforcement learning, and demonstrate significant performance improvements through comprehensive evaluations.'}, 'zh': {'title': 'ç‰©ç†äººå·¥æ™ºèƒ½çš„æ¨ç†ä¸å†³ç­–æ–°çªç ´', 'desc': 'æœ¬æ–‡ä»‹ç»äº†Cosmos-Reason1æ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ç§èƒ½å¤Ÿç†è§£ç‰©ç†ä¸–ç•Œå¹¶é€šè¿‡é•¿é“¾æ¨ç†è¿‡ç¨‹ç”Ÿæˆé€‚å½“å†³ç­–çš„ç‰©ç†äººå·¥æ™ºèƒ½ç³»ç»Ÿã€‚æˆ‘ä»¬å®šä¹‰äº†ç‰©ç†äººå·¥æ™ºèƒ½æ¨ç†çš„å…³é”®èƒ½åŠ›ï¼Œé‡ç‚¹å…³æ³¨ç‰©ç†å¸¸è¯†å’Œå…·èº«æ¨ç†ã€‚ä¸ºäº†è¡¨ç¤ºç‰©ç†å¸¸è¯†ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†ä¸€ä¸ªå±‚æ¬¡æœ¬ä½“ï¼Œæ•æ‰ç©ºé—´ã€æ—¶é—´å’Œç‰©ç†å­¦çš„åŸºæœ¬çŸ¥è¯†ã€‚é€šè¿‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„å¼€å‘ï¼Œæˆ‘ä»¬çš„ç ”ç©¶å±•ç¤ºäº†ç‰©ç†äººå·¥æ™ºèƒ½åœ¨æ¨ç†å’Œå†³ç­–ä¸­çš„æ˜¾è‘—è¿›æ­¥ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.16397', 'title': 'Scale-wise Distillation of Diffusion Models', 'url': 'https://huggingface.co/papers/2503.16397', 'abstract': 'We present SwD, a scale-wise distillation framework for diffusion models (DMs), which effectively employs next-scale prediction ideas for diffusion-based few-step generators. In more detail, SwD is inspired by the recent insights relating diffusion processes to the implicit spectral autoregression. We suppose that DMs can initiate generation at lower data resolutions and gradually upscale the samples at each denoising step without loss in performance while significantly reducing computational costs. SwD naturally integrates this idea into existing diffusion distillation methods based on distribution matching. Also, we enrich the family of distribution matching approaches by introducing a novel patch loss enforcing finer-grained similarity to the target distribution. When applied to state-of-the-art text-to-image diffusion models, SwD approaches the inference times of two full resolution steps and significantly outperforms the counterparts under the same computation budget, as evidenced by automated metrics and human preference studies.', 'score': 28, 'issue_id': 2830, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 20', 'zh': '3æœˆ20æ—¥'}, 'hash': 'b981ae73ba6549a8', 'authors': ['Nikita Starodubcev', 'Denis Kuznedelev', 'Artem Babenko', 'Dmitry Baranchuk'], 'affiliations': ['Yandex Research'], 'pdf_title_img': 'assets/pdf/title_img/2503.16397.jpg', 'data': {'categories': ['#inference', '#training', '#diffusion', '#cv', '#optimization'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼', 'desc': 'SwD - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ° Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ñ Ğ¼Ğ°Ğ»Ñ‹Ğ¼ Ñ‡Ğ¸ÑĞ»Ğ¾Ğ¼ ÑˆĞ°Ğ³Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° ÑĞ²ÑĞ·Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°Ğ¼Ğ¸ Ğ¸ Ğ½ĞµÑĞ²Ğ½Ğ¾Ğ¹ ÑĞ¿ĞµĞºÑ‚Ñ€Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸ĞµĞ¹, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ½Ğ°Ñ‡Ğ¸Ğ½Ğ°Ñ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°Ñ‚ÑŒ ĞµĞ³Ğ¾ Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ ÑˆĞ°Ğ³Ğµ. SwD Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ² ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ°Ñ‚Ñ‡ĞµĞ¹ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ. ĞŸÑ€Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¸ Ğº ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ, SwD Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸ Ğ¿Ñ€Ğ¸ Ñ‚Ğ¾Ğ¼ Ğ¶Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğµ.'}, 'en': {'title': 'Efficient Image Generation with Scale-Wise Distillation', 'desc': 'The paper introduces SwD, a new framework for improving diffusion models (DMs) by using a scale-wise distillation approach. This method allows DMs to start generating images at lower resolutions and progressively enhance them at each denoising step, which reduces computational costs without sacrificing quality. SwD builds on existing diffusion distillation techniques by incorporating a novel patch loss that ensures closer alignment with the target distribution. When tested on advanced text-to-image models, SwD shows faster inference times and better performance compared to traditional methods within the same computational limits.'}, 'zh': {'title': 'SwDï¼šé«˜æ•ˆçš„æ‰©æ•£æ¨¡å‹è’¸é¦æ¡†æ¶', 'desc': 'æˆ‘ä»¬æå‡ºäº†SwDï¼Œä¸€ä¸ªé’ˆå¯¹æ‰©æ•£æ¨¡å‹çš„å°ºåº¦çº§è’¸é¦æ¡†æ¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåˆ©ç”¨ä¸‹ä¸€å°ºåº¦é¢„æµ‹çš„æ€æƒ³æ¥ç”Ÿæˆå°‘æ­¥æ‰©æ•£æ¨¡å‹ã€‚SwDçš„çµæ„Ÿæ¥æºäºå°†æ‰©æ•£è¿‡ç¨‹ä¸éšå¼è°±è‡ªå›å½’ç›¸å…³è”çš„æœ€æ–°ç ”ç©¶ã€‚æˆ‘ä»¬å‡è®¾æ‰©æ•£æ¨¡å‹å¯ä»¥åœ¨è¾ƒä½çš„æ•°æ®åˆ†è¾¨ç‡ä¸‹å¼€å§‹ç”Ÿæˆï¼Œå¹¶åœ¨æ¯ä¸ªå»å™ªæ­¥éª¤ä¸­é€æ­¥æå‡æ ·æœ¬çš„åˆ†è¾¨ç‡ï¼ŒåŒæ—¶ä¸æŸå¤±æ€§èƒ½å¹¶æ˜¾è‘—é™ä½è®¡ç®—æˆæœ¬ã€‚é€šè¿‡å¼•å…¥ä¸€ç§æ–°é¢–çš„è¡¥ä¸æŸå¤±ï¼ŒSwDå¢å¼ºäº†åˆ†å¸ƒåŒ¹é…æ–¹æ³•çš„ç»†ç²’åº¦ç›¸ä¼¼æ€§ï¼Œåº”ç”¨äºæœ€å…ˆè¿›çš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹æ—¶ï¼ŒSwDåœ¨ç›¸åŒè®¡ç®—é¢„ç®—ä¸‹æ˜¾è‘—æé«˜äº†æ¨ç†é€Ÿåº¦å’Œæ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.15299', 'title': 'Inside-Out: Hidden Factual Knowledge in LLMs', 'url': 'https://huggingface.co/papers/2503.15299', 'abstract': "This work presents a framework for assessing whether large language models (LLMs) encode more factual knowledge in their parameters than what they express in their outputs. While a few studies hint at this possibility, none has clearly defined or demonstrated this phenomenon. We first propose a formal definition of knowledge, quantifying it for a given question as the fraction of correct-incorrect answer pairs where the correct one is ranked higher. This gives rise to external and internal knowledge, depending on the information used to score individual answer candidates: either the model's observable token-level probabilities or its intermediate computations. Hidden knowledge arises when internal knowledge exceeds external knowledge. We then present a case study, applying this framework to three popular open-weights LLMs in a closed-book QA setup. Our results indicate that: (1) LLMs consistently encode more factual knowledge internally than what they express externally, with an average gap of 40%. (2) Surprisingly, some knowledge is so deeply hidden that a model can internally know an answer perfectly, yet fail to generate it even once, despite large-scale repeated sampling of 1,000 answers. This reveals fundamental limitations in the generation capabilities of LLMs, which (3) puts a practical constraint on scaling test-time compute via repeated answer sampling in closed-book QA: significant performance improvements remain inaccessible because some answers are practically never sampled, yet if they were, we would be guaranteed to rank them first.", 'score': 27, 'issue_id': 2833, 'pub_date': '2025-03-19', 'pub_date_card': {'ru': '19 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 19', 'zh': '3æœˆ19æ—¥'}, 'hash': '2ca2246fe2ad8d73', 'authors': ['Zorik Gekhman', 'Eyal Ben David', 'Hadas Orgad', 'Eran Ofek', 'Yonatan Belinkov', 'Idan Szpector', 'Jonathan Herzig', 'Roi Reichart'], 'affiliations': ['Google Research', 'Technion - Israel Institute of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2503.15299.jpg', 'data': {'categories': ['#hallucinations', '#multimodal', '#data', '#benchmark', '#interpretability'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ¡ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…: Ğ±Ğ¾Ğ»ÑŒÑˆĞµ, Ñ‡ĞµĞ¼ ĞºĞ°Ğ¶ĞµÑ‚ÑÑ Ğ½Ğ° Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ°ÑÑ‚ Ğ²Ğ½ĞµÑˆĞ½ĞµĞµ Ğ¸ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ĞµĞµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ LLM ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‚ Ğ² ÑÑ€ĞµĞ´Ğ½ĞµĞ¼ Ğ½Ğ° 40% Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğµ, Ñ‡ĞµĞ¼ Ğ²Ñ‹Ñ€Ğ°Ğ¶Ğ°ÑÑ‚ Ğ²Ğ½ĞµÑˆĞ½Ğµ. ĞĞ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ½Ğ°ÑÑ‚Ğ¾Ğ»ÑŒĞºĞ¾ ÑĞºÑ€Ñ‹Ñ‚Ñ‹, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğµ Ğ·Ğ½Ğ°Ñ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ¸Ğ´ĞµĞ°Ğ»ÑŒĞ½Ğ¾, Ğ½Ğ¾ Ğ½Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞµĞ³Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ¿Ñ€Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğ¼ ÑĞµĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸.'}, 'en': {'title': 'Unveiling Hidden Knowledge in Language Models', 'desc': 'This paper introduces a framework to evaluate the factual knowledge embedded in large language models (LLMs) compared to what they actually produce in their outputs. It defines knowledge in a quantifiable way, distinguishing between external knowledge (observable outputs) and internal knowledge (hidden computations). The study reveals that LLMs often possess more internal knowledge than they express, with a notable gap of 40% on average. Additionally, it highlights that some knowledge is so well-hidden that models can know the correct answer internally but fail to generate it, indicating limitations in their generation capabilities and affecting performance in closed-book question answering.'}, 'zh': {'title': 'æ­ç¤ºå¤§å‹è¯­è¨€æ¨¡å‹çš„éšè—çŸ¥è¯†', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ¡†æ¶ï¼Œç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å…¶å‚æ•°ä¸­ç¼–ç çš„äº‹å®çŸ¥è¯†æ˜¯å¦è¶…è¿‡å…¶è¾“å‡ºä¸­è¡¨è¾¾çš„çŸ¥è¯†ã€‚æˆ‘ä»¬é¦–å…ˆæå‡ºäº†çŸ¥è¯†çš„æ­£å¼å®šä¹‰ï¼Œé€šè¿‡æ­£ç¡®-é”™è¯¯ç­”æ¡ˆå¯¹çš„æ¯”ä¾‹æ¥é‡åŒ–çŸ¥è¯†ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒLLMså†…éƒ¨ç¼–ç çš„äº‹å®çŸ¥è¯†é€šå¸¸æ¯”å¤–éƒ¨è¡¨è¾¾çš„å¤šï¼Œå¹³å‡å·®è·è¾¾åˆ°40%ã€‚æ­¤å¤–ï¼ŒæŸäº›çŸ¥è¯†æ·±è—ä¸éœ²ï¼Œæ¨¡å‹å¯èƒ½å†…éƒ¨å®Œå…¨çŸ¥é“ç­”æ¡ˆï¼Œä½†åœ¨å¤šæ¬¡é‡‡æ ·ä¸­å´ä»æœªç”Ÿæˆï¼Œè¿™æ­ç¤ºäº†LLMsç”Ÿæˆèƒ½åŠ›çš„åŸºæœ¬å±€é™æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.16365', 'title': 'JARVIS-VLA: Post-Training Large-Scale Vision Language Models to Play\n  Visual Games with Keyboards and Mouse', 'url': 'https://huggingface.co/papers/2503.16365', 'abstract': "Recently, action-based decision-making in open-world environments has gained significant attention. Visual Language Action (VLA) models, pretrained on large-scale web datasets, have shown promise in decision-making tasks. However, previous work has primarily focused on action post-training, often neglecting enhancements to the foundational model itself. In response, we introduce a novel approach, Act from Visual Language Post-Training, which refines Visual Language Models (VLMs) through visual and linguistic guidance in a self-supervised manner. This enhancement improves the models' capabilities in world knowledge, visual recognition, and spatial grounding in open-world environments. Following the above post-training paradigms, we obtain the first VLA models in Minecraft that can follow human instructions on over 1k different atomic tasks, including crafting, smelting, cooking, mining, and killing. Our experiments demonstrate that post-training on non-trajectory tasks leads to a significant 40% improvement over the best agent baseline on a diverse set of atomic tasks. Furthermore, we demonstrate that our approach surpasses traditional imitation learning-based policies in Minecraft, achieving state-of-the-art performance. We have open-sourced the code, models, and datasets to foster further research. The project page can be found in https://craftjarvis.github.io/JarvisVLA.", 'score': 25, 'issue_id': 2824, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 20', 'zh': '3æœˆ20æ—¥'}, 'hash': '764fc9201b406ec4', 'authors': ['Muyao Li', 'Zihao Wang', 'Kaichen He', 'Xiaojian Ma', 'Yitao Liang'], 'affiliations': ['BIGAI', 'Peking University', 'Team CraftJarvis'], 'pdf_title_img': 'assets/pdf/title_img/2503.16365.jpg', 'data': {'categories': ['#open_source', '#games', '#cv', '#agents', '#training'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ VLA Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ (VLA) Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Act from Visual Language Post-Training, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ VLM Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¸ Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¾ Ğ¼Ğ¸Ñ€Ğµ, Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºĞ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ² Minecraft Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ 40% ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ°Ñ‚Ğ¾Ğ¼Ğ°Ñ€Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ….'}, 'en': {'title': 'Enhancing Decision-Making in Open Worlds with Visual Language Models', 'desc': "This paper presents a new method called Act from Visual Language Post-Training, which enhances Visual Language Models (VLMs) for decision-making in open-world environments. The approach uses visual and linguistic guidance in a self-supervised way to improve the models' understanding of world knowledge, visual recognition, and spatial grounding. The authors demonstrate that their VLA models can effectively follow human instructions in Minecraft for over 1,000 different tasks, achieving a 40% performance boost compared to existing agents. Additionally, their method outperforms traditional imitation learning techniques, setting a new standard in the field."}, 'zh': {'title': 'æå‡å¼€æ”¾ä¸–ç•Œå†³ç­–èƒ½åŠ›çš„è§†è§‰è¯­è¨€æ¨¡å‹', 'desc': 'æœ€è¿‘ï¼ŒåŸºäºåŠ¨ä½œçš„å†³ç­–åœ¨å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­å¼•èµ·äº†å¹¿æ³›å…³æ³¨ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œç§°ä¸ºè§†è§‰è¯­è¨€åè®­ç»ƒï¼ˆAct from Visual Language Post-Trainingï¼‰ï¼Œé€šè¿‡è§†è§‰å’Œè¯­è¨€æŒ‡å¯¼è‡ªæˆ‘ç›‘ç£åœ°æ”¹è¿›è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ã€‚è¿™ç§å¢å¼ºæé«˜äº†æ¨¡å‹åœ¨ä¸–ç•ŒçŸ¥è¯†ã€è§†è§‰è¯†åˆ«å’Œç©ºé—´å®šä½æ–¹é¢çš„èƒ½åŠ›ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œåœ¨Minecraftä¸­ï¼Œæˆ‘ä»¬çš„æ¨¡å‹èƒ½å¤Ÿæ‰§è¡Œè¶…è¿‡1000ä¸ªä¸åŒçš„åŸå­ä»»åŠ¡ï¼Œå¹¶åœ¨å¤šæ ·åŒ–çš„ä»»åŠ¡ä¸Šå®ç°äº†40%çš„æ€§èƒ½æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.16219', 'title': "Reinforcement Learning for Reasoning in Small LLMs: What Works and What\n  Doesn't", 'url': 'https://huggingface.co/papers/2503.16219', 'abstract': 'Enhancing the reasoning capabilities of large language models (LLMs) typically relies on massive computational resources and extensive datasets, limiting accessibility for resource-constrained settings. Our study investigates the potential of reinforcement learning (RL) to improve reasoning in small LLMs, focusing on a 1.5-billion-parameter model, DeepSeek-R1-Distill-Qwen-1.5B, under strict constraints: training on 4 NVIDIA A40 GPUs (48 GB VRAM each) within 24 hours. Adapting the Group Relative Policy Optimization (GRPO) algorithm and curating a compact, high-quality mathematical reasoning dataset, we conducted three experiments to explore model behavior and performance. Our results demonstrate rapid reasoning gains - e.g., AMC23 accuracy rising from 63% to 80% and AIME24 reaching 46.7%, surpassing o1-preview - using only 7,000 samples and a $42 training cost, compared to thousands of dollars for baseline models. However, challenges such as optimization instability and length constraints emerged with prolonged training. These findings highlight the efficacy of RL-based fine-tuning for small LLMs, offering a cost-effective alternative to large-scale approaches. We release our code and datasets as open-source resources, providing insights into trade-offs and laying a foundation for scalable, reasoning-capable LLMs in resource-limited environments. All are available at https://github.com/knoveleng/open-rs.', 'score': 25, 'issue_id': 2829, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 20', 'zh': '3æœˆ20æ—¥'}, 'hash': 'b9603e3e14ebf85d', 'authors': ['Quy-Anh Dang', 'Chris Ngo'], 'affiliations': ['Knovel Engineering Lab, Singapore', 'VNU University of Science, Vietnam'], 'pdf_title_img': 'assets/pdf/title_img/2503.16219.jpg', 'data': {'categories': ['#rl', '#dataset', '#small_models', '#reasoning', '#open_source', '#training', '#optimization'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¼Ğ°Ğ»Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ RL', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñƒ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ğ»Ğ¸ÑÑŒ Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ DeepSeek-R1-Distill-Qwen-1.5B Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµÑÑƒÑ€ÑĞ°Ğ¼Ğ¸ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½ĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ GRPO Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ´Ğ¾Ğ±Ğ¸Ğ»Ğ¸ÑÑŒ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ¾Ğ¹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Reinforcement Learning: A Cost-Effective Boost for Small Language Models!', 'desc': 'This paper explores how reinforcement learning (RL) can enhance the reasoning abilities of smaller language models, specifically a 1.5-billion-parameter model called DeepSeek-R1-Distill-Qwen-1.5B. The researchers used a modified Group Relative Policy Optimization (GRPO) algorithm and a carefully curated dataset to train the model efficiently on limited hardware. Their experiments showed significant improvements in reasoning accuracy, achieving notable performance gains with minimal training resources. The study emphasizes the potential of RL for fine-tuning smaller models, making advanced reasoning capabilities more accessible in resource-constrained settings.'}, 'zh': {'title': 'å°å‹LLMsçš„æ¨ç†èƒ½åŠ›æå‡æ–°æ–¹æ¡ˆ', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†å¦‚ä½•é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æå‡å°å‹å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬ä½¿ç”¨äº†ä¸€ä¸ªåŒ…å«15äº¿å‚æ•°çš„æ¨¡å‹ï¼Œå¹¶åœ¨ä¸¥æ ¼çš„èµ„æºé™åˆ¶ä¸‹è¿›è¡Œè®­ç»ƒï¼Œä½¿ç”¨4ä¸ªNVIDIA A40 GPUï¼Œåœ¨24å°æ—¶å†…å®Œæˆã€‚é€šè¿‡é€‚åº”ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ç®—æ³•ï¼Œå¹¶åˆ›å»ºé«˜è´¨é‡çš„æ•°å­¦æ¨ç†æ•°æ®é›†ï¼Œæˆ‘ä»¬çš„å®éªŒæ˜¾ç¤ºæ¨¡å‹çš„æ¨ç†èƒ½åŠ›æ˜¾è‘—æé«˜ï¼Œè®­ç»ƒæˆæœ¬ä¹Ÿå¤§å¹…é™ä½ã€‚å°½ç®¡åœ¨é•¿æ—¶é—´è®­ç»ƒä¸­å‡ºç°äº†ä¼˜åŒ–ä¸ç¨³å®šå’Œé•¿åº¦é™åˆ¶ç­‰æŒ‘æˆ˜ï¼Œä½†æˆ‘ä»¬çš„ç ”ç©¶ä¸ºèµ„æºæœ‰é™ç¯å¢ƒä¸­çš„å°å‹LLMsæä¾›äº†æœ‰æ•ˆçš„å¼ºåŒ–å­¦ä¹ å¾®è°ƒæ–¹æ¡ˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.14487', 'title': 'DiffMoE: Dynamic Token Selection for Scalable Diffusion Transformers', 'url': 'https://huggingface.co/papers/2503.14487', 'abstract': 'Diffusion models have demonstrated remarkable success in various image generation tasks, but their performance is often limited by the uniform processing of inputs across varying conditions and noise levels. To address this limitation, we propose a novel approach that leverages the inherent heterogeneity of the diffusion process. Our method, DiffMoE, introduces a batch-level global token pool that enables experts to access global token distributions during training, promoting specialized expert behavior. To unleash the full potential of the diffusion process, DiffMoE incorporates a capacity predictor that dynamically allocates computational resources based on noise levels and sample complexity. Through comprehensive evaluation, DiffMoE achieves state-of-the-art performance among diffusion models on ImageNet benchmark, substantially outperforming both dense architectures with 3x activated parameters and existing MoE approaches while maintaining 1x activated parameters. The effectiveness of our approach extends beyond class-conditional generation to more challenging tasks such as text-to-image generation, demonstrating its broad applicability across different diffusion model applications. Project Page: https://shiml20.github.io/DiffMoE/', 'score': 25, 'issue_id': 2828, 'pub_date': '2025-03-18', 'pub_date_card': {'ru': '18 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 18', 'zh': '3æœˆ18æ—¥'}, 'hash': 'f530713a45ca1341', 'authors': ['Minglei Shi', 'Ziyang Yuan', 'Haotian Yang', 'Xintao Wang', 'Mingwu Zheng', 'Xin Tao', 'Wenliang Zhao', 'Wenzhao Zheng', 'Jie Zhou', 'Jiwen Lu', 'Pengfei Wan', 'Di Zhang', 'Kun Gai'], 'affiliations': ['Kuais', 'Tsinghua University, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.14487.jpg', 'data': {'categories': ['#benchmark', '#cv', '#diffusion', '#architecture'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'DiffMoE: Ğ£Ğ¼Ğ½Ğ°Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'DiffMoE - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿ÑƒĞ» Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ğ°ĞºĞµÑ‚Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ğ¼ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°Ñ‚ÑŒ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ Ğº Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸ÑĞ¼ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. DiffMoE Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¸ĞºÑ‚Ğ¾Ñ€ Ğ¼Ğ¾Ñ‰Ğ½Ğ¾ÑÑ‚Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµÑÑƒÑ€ÑÑ‹ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ ÑˆÑƒĞ¼Ğ° Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² ÑÑ€ĞµĞ´Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ ImageNet, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ĞºĞ°Ğº Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹, Ñ‚Ğ°Ğº Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ MoE.'}, 'en': {'title': 'Unlocking the Power of Diffusion with Expert Adaptation', 'desc': 'This paper introduces DiffMoE, a new method for improving diffusion models in image generation. It addresses the issue of uniform input processing by utilizing a batch-level global token pool, allowing for specialized expert behavior during training. Additionally, DiffMoE features a capacity predictor that adjusts computational resources based on the noise levels and complexity of the samples. The results show that DiffMoE achieves top performance on the ImageNet benchmark, outperforming existing models while using fewer activated parameters, and is effective in various tasks including text-to-image generation.'}, 'zh': {'title': 'é‡Šæ”¾æ‰©æ•£æ¨¡å‹çš„æ½œåŠ›ï¼ŒDiffMoEå¼•é¢†æ–°æ½®æµ', 'desc': 'æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ä¸åŒæ¡ä»¶å’Œå™ªå£°æ°´å¹³ä¸‹çš„è¾“å…¥å¤„ç†ä¸Šå­˜åœ¨å±€é™æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œç§°ä¸ºDiffMoEï¼Œåˆ©ç”¨æ‰©æ•£è¿‡ç¨‹çš„å†…åœ¨å¼‚è´¨æ€§ã€‚è¯¥æ–¹æ³•å¼•å…¥äº†ä¸€ä¸ªæ‰¹é‡çº§çš„å…¨å±€ä»¤ç‰Œæ± ï¼Œä½¿ä¸“å®¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­èƒ½å¤Ÿè®¿é—®å…¨å±€ä»¤ç‰Œåˆ†å¸ƒï¼Œä»è€Œä¿ƒè¿›ä¸“å®¶çš„ä¸“ä¸šåŒ–è¡Œä¸ºã€‚æ­¤å¤–ï¼ŒDiffMoEè¿˜ç»“åˆäº†ä¸€ä¸ªå®¹é‡é¢„æµ‹å™¨ï¼Œæ ¹æ®å™ªå£°æ°´å¹³å’Œæ ·æœ¬å¤æ‚æ€§åŠ¨æ€åˆ†é…è®¡ç®—èµ„æºï¼Œå……åˆ†å‘æŒ¥æ‰©æ•£è¿‡ç¨‹çš„æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.16418', 'title': 'InfiniteYou: Flexible Photo Recrafting While Preserving Your Identity', 'url': 'https://huggingface.co/papers/2503.16418', 'abstract': 'Achieving flexible and high-fidelity identity-preserved image generation remains formidable, particularly with advanced Diffusion Transformers (DiTs) like FLUX. We introduce InfiniteYou (InfU), one of the earliest robust frameworks leveraging DiTs for this task. InfU addresses significant issues of existing methods, such as insufficient identity similarity, poor text-image alignment, and low generation quality and aesthetics. Central to InfU is InfuseNet, a component that injects identity features into the DiT base model via residual connections, enhancing identity similarity while maintaining generation capabilities. A multi-stage training strategy, including pretraining and supervised fine-tuning (SFT) with synthetic single-person-multiple-sample (SPMS) data, further improves text-image alignment, ameliorates image quality, and alleviates face copy-pasting. Extensive experiments demonstrate that InfU achieves state-of-the-art performance, surpassing existing baselines. In addition, the plug-and-play design of InfU ensures compatibility with various existing methods, offering a valuable contribution to the broader community.', 'score': 24, 'issue_id': 2824, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 20', 'zh': '3æœˆ20æ—¥'}, 'hash': '71a484a2a785f8be', 'authors': ['Liming Jiang', 'Qing Yan', 'Yumin Jia', 'Zichuan Liu', 'Hao Kang', 'Xin Lu'], 'affiliations': ['ByteDance'], 'pdf_title_img': 'assets/pdf/title_img/2503.16418.jpg', 'data': {'categories': ['#cv', '#diffusion', '#architecture', '#synthetic', '#training'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'InfiniteYou: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸', 'desc': 'InfiniteYou (InfU) - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ”Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¢Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ñ‹ (DiT). ĞÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚ InfU - InfuseNet, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ²Ğ½ĞµĞ´Ñ€ÑĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ±Ğ°Ğ·Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ DiT Ñ‡ĞµÑ€ĞµĞ· Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ ÑĞ¾ĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼ÑƒÑ Ñ‚Ğ¾Ğ½ĞºÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ InfU Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'InfiniteYou: Revolutionizing Identity-Preserved Image Generation with DiTs', 'desc': 'This paper presents InfiniteYou (InfU), a novel framework that enhances identity-preserved image generation using advanced Diffusion Transformers (DiTs) like FLUX. InfU tackles key challenges in the field, such as improving identity similarity, aligning text with images, and enhancing overall image quality. The core innovation is InfuseNet, which integrates identity features into the DiT model through residual connections, thereby boosting identity preservation. With a multi-stage training approach that includes pretraining and supervised fine-tuning on synthetic data, InfU achieves superior performance compared to existing methods, while also being adaptable to various frameworks.'}, 'zh': {'title': 'æ— é™å¯èƒ½çš„èº«ä»½ä¿ç•™å›¾åƒç”Ÿæˆ', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºInfiniteYouï¼ˆInfUï¼‰çš„æ–°æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°é«˜ä¿çœŸåº¦çš„èº«ä»½ä¿ç•™å›¾åƒç”Ÿæˆã€‚InfUåˆ©ç”¨å…ˆè¿›çš„æ‰©æ•£å˜æ¢å™¨ï¼ˆDiTsï¼‰ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•åœ¨èº«ä»½ç›¸ä¼¼æ€§ã€æ–‡æœ¬ä¸å›¾åƒå¯¹é½ä»¥åŠç”Ÿæˆè´¨é‡ç­‰æ–¹é¢çš„ä¸è¶³ã€‚å…¶æ ¸å¿ƒç»„ä»¶InfuseNeté€šè¿‡æ®‹å·®è¿æ¥å°†èº«ä»½ç‰¹å¾æ³¨å…¥åˆ°DiTåŸºç¡€æ¨¡å‹ä¸­ï¼Œä»è€Œå¢å¼ºèº«ä»½ç›¸ä¼¼æ€§ï¼ŒåŒæ—¶ä¿æŒç”Ÿæˆèƒ½åŠ›ã€‚é€šè¿‡å¤šé˜¶æ®µè®­ç»ƒç­–ç•¥ï¼ŒåŒ…æ‹¬é¢„è®­ç»ƒå’Œç›‘ç£å¾®è°ƒï¼ŒInfUæ˜¾è‘—æé«˜äº†æ–‡æœ¬ä¸å›¾åƒçš„å¯¹é½åº¦å’Œå›¾åƒè´¨é‡ï¼Œå®éªŒç»“æœè¡¨æ˜å…¶æ€§èƒ½è¶…è¶Šäº†ç°æœ‰åŸºå‡†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.13657', 'title': 'Why Do Multi-Agent LLM Systems Fail?', 'url': 'https://huggingface.co/papers/2503.13657', 'abstract': "Despite growing enthusiasm for Multi-Agent Systems (MAS), where multiple LLM agents collaborate to accomplish tasks, their performance gains across popular benchmarks remain minimal compared to single-agent frameworks. This gap highlights the need to analyze the challenges hindering MAS effectiveness.   In this paper, we present the first comprehensive study of MAS challenges. We analyze five popular MAS frameworks across over 150 tasks, involving six expert human annotators. We identify 14 unique failure modes and propose a comprehensive taxonomy applicable to various MAS frameworks. This taxonomy emerges iteratively from agreements among three expert annotators per study, achieving a Cohen's Kappa score of 0.88. These fine-grained failure modes are organized into 3 categories, (i) specification and system design failures, (ii) inter-agent misalignment, and (iii) task verification and termination. To support scalable evaluation, we integrate MASFT with LLM-as-a-Judge. We also explore if identified failures could be easily prevented by proposing two interventions: improved specification of agent roles and enhanced orchestration strategies. Our findings reveal that identified failures require more complex solutions, highlighting a clear roadmap for future research. We open-source our dataset and LLM annotator.", 'score': 24, 'issue_id': 2829, 'pub_date': '2025-03-17', 'pub_date_card': {'ru': '17 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 17', 'zh': '3æœˆ17æ—¥'}, 'hash': 'f3e2aec1e3948d46', 'authors': ['Mert Cemri', 'Melissa Z. Pan', 'Shuyi Yang', 'Lakshya A. Agrawal', 'Bhavya Chopra', 'Rishabh Tiwari', 'Kurt Keutzer', 'Aditya Parameswaran', 'Dan Klein', 'Kannan Ramchandran', 'Matei Zaharia', 'Joseph E. Gonzalez', 'Ion Stoica'], 'affiliations': ['Intesa Sanpaolo', 'UC Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2503.13657.jpg', 'data': {'categories': ['#alignment', '#benchmark', '#dataset', '#agi', '#agents', '#open_source'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼: Ğ¿ÑƒÑ‚ÑŒ Ğº ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ˜Ğ˜', 'desc': 'Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ (ĞœĞĞ¡) Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ 14 ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ¾Ğ² Ğ¾Ñ‚ĞºĞ°Ğ·Ğ° Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ, Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼ÑƒÑ Ğº Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ°Ğ¼ ĞœĞĞ¡. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°Ğ»Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¿ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ¾Ğ² ĞœĞĞ¡ Ğ½Ğ° Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 150 Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ ÑƒÑ‡Ğ°ÑÑ‚Ğ¸ĞµĞ¼ ÑˆĞµÑÑ‚Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ²-Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ñ‹ Ğ´Ğ»Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹.'}, 'en': {'title': 'Unlocking the Potential of Multi-Agent Systems: Identifying and Overcoming Challenges', 'desc': 'This paper investigates the challenges faced by Multi-Agent Systems (MAS) that involve multiple large language model (LLM) agents working together. The authors conducted a thorough analysis of five MAS frameworks across over 150 tasks, identifying 14 distinct failure modes that hinder performance. They categorized these failures into three main groups: specification and system design failures, inter-agent misalignment, and task verification and termination issues. Additionally, the paper proposes interventions to mitigate these failures and emphasizes the need for more sophisticated solutions to enhance MAS effectiveness.'}, 'zh': {'title': 'æ­ç¤ºå¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„æŒ‘æˆ˜ä¸è§£å†³æ–¹æ¡ˆ', 'desc': 'å°½ç®¡å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMASï¼‰å—åˆ°è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ï¼Œä½†ä¸å•æ™ºèƒ½ä½“æ¡†æ¶ç›¸æ¯”ï¼Œå…¶åœ¨æµè¡ŒåŸºå‡†ä¸Šçš„æ€§èƒ½æå‡ä»ç„¶æœ‰é™ã€‚æœ¬æ–‡é¦–æ¬¡å…¨é¢ç ”ç©¶äº†MASé¢ä¸´çš„æŒ‘æˆ˜ï¼Œåˆ†æäº†äº”ä¸ªæµè¡Œçš„MASæ¡†æ¶åŠå…¶åœ¨150å¤šä¸ªä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚æˆ‘ä»¬è¯†åˆ«å‡º14ç§ç‹¬ç‰¹çš„å¤±è´¥æ¨¡å¼ï¼Œå¹¶æå‡ºäº†é€‚ç”¨äºå„ç§MASæ¡†æ¶çš„ç»¼åˆåˆ†ç±»æ³•ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè§£å†³è¿™äº›å¤±è´¥æ¨¡å¼éœ€è¦æ›´å¤æ‚çš„è§£å†³æ–¹æ¡ˆï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æŒ‡æ˜äº†æ–¹å‘ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.16257', 'title': 'Plug-and-Play 1.x-Bit KV Cache Quantization for Video Large Language\n  Models', 'url': 'https://huggingface.co/papers/2503.16257', 'abstract': 'Video large language models (VideoLLMs) have demonstrated the capability to process longer video inputs and enable complex reasoning and analysis. However, due to the thousands of visual tokens from the video frames, key-value (KV) cache can significantly increase memory requirements, becoming a bottleneck for inference speed and memory usage. KV cache quantization is a widely used approach to address this problem. In this paper, we find that 2-bit KV quantization of VideoLLMs can hardly hurt the model performance, while the limit of KV cache quantization in even lower bits has not been investigated. To bridge this gap, we introduce VidKV, a plug-and-play KV cache quantization method to compress the KV cache to lower than 2 bits. Specifically, (1) for key, we propose a mixed-precision quantization strategy in the channel dimension, where we perform 2-bit quantization for anomalous channels and 1-bit quantization combined with FFT for normal channels; (2) for value, we implement 1.58-bit quantization while selectively filtering semantically salient visual tokens for targeted preservation, for a better trade-off between precision and model performance. Importantly, our findings suggest that the value cache of VideoLLMs should be quantized in a per-channel fashion instead of the per-token fashion proposed by prior KV cache quantization works for LLMs. Empirically, extensive results with LLaVA-OV-7B and Qwen2.5-VL-7B on six benchmarks show that VidKV effectively compresses the KV cache to 1.5-bit and 1.58-bit precision with almost no performance drop compared to the FP16 counterparts.', 'score': 20, 'issue_id': 2829, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 20', 'zh': '3æœˆ20æ—¥'}, 'hash': '68c739951d6b3dce', 'authors': ['Keda Tao', 'Haoxuan You', 'Yang Sui', 'Can Qin', 'Huan Wang'], 'affiliations': ['Columbia University', 'Rice University', 'Salesforce AI Research', 'Westlake University', 'Xidian University'], 'pdf_title_img': 'assets/pdf/title_img/2503.16257.jpg', 'data': {'categories': ['#video', '#long_context', '#inference', '#optimization'], 'emoji': 'ğŸ¥', 'ru': {'title': 'VidKV: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ ĞºÑÑˆĞ° Ğ´Ğ»Ñ Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ VidKV - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ĞºÑÑˆĞ° ĞºĞ»ÑÑ‡-Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ (KV) Ğ´Ğ»Ñ Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (VideoLLMs). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ĞºĞ»ÑÑ‡ĞµĞ¹ Ğ¸ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğ¹, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰ÑƒÑ ÑĞ¶Ğ°Ñ‚ÑŒ ĞºÑÑˆ Ğ´Ğ¾ Ğ¼ĞµĞ½ĞµĞµ Ñ‡ĞµĞ¼ 2 Ğ±Ğ¸Ñ‚ Ğ±ĞµĞ· ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 2-Ğ±Ğ¸Ñ‚Ğ½ÑƒÑ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ĞºĞ°Ğ½Ğ°Ğ»Ğ¾Ğ² ĞºĞ»ÑÑ‡ĞµĞ¹ Ğ¸ 1-Ğ±Ğ¸Ñ‚Ğ½ÑƒÑ Ñ FFT Ğ´Ğ»Ñ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ…, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ 1.58-Ğ±Ğ¸Ñ‚Ğ½ÑƒÑ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğ¹ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° ÑˆĞµÑÑ‚Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ VidKV Ğ² ÑĞ¶Ğ°Ñ‚Ğ¸Ğ¸ ĞºÑÑˆĞ° Ğ´Ğ¾ 1.5-1.58 Ğ±Ğ¸Ñ‚ Ğ±ĞµĞ· Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Efficient KV Cache Compression for VideoLLMs with VidKV', 'desc': 'This paper introduces VidKV, a novel method for quantizing key-value (KV) caches in Video Large Language Models (VideoLLMs) to improve memory efficiency. The authors demonstrate that using 2-bit quantization does not significantly impact model performance, and they explore even lower quantization levels. VidKV employs a mixed-precision strategy for keys and a selective filtering approach for values, allowing for effective compression while maintaining accuracy. The results show that VidKV can reduce KV cache size to 1.5-bit and 1.58-bit with minimal performance loss, outperforming previous methods that used per-token quantization.'}, 'zh': {'title': 'å‹ç¼©KVç¼“å­˜ï¼Œæå‡è§†é¢‘æ¨¡å‹æ€§èƒ½', 'desc': 'è§†é¢‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆVideoLLMsï¼‰èƒ½å¤Ÿå¤„ç†æ›´é•¿çš„è§†é¢‘è¾“å…¥ï¼Œå¹¶è¿›è¡Œå¤æ‚çš„æ¨ç†å’Œåˆ†æã€‚ç„¶è€Œï¼Œç”±äºè§†é¢‘å¸§ä¸­æˆåƒä¸Šä¸‡çš„è§†è§‰æ ‡è®°ï¼Œé”®å€¼ï¼ˆKVï¼‰ç¼“å­˜ä¼šæ˜¾è‘—å¢åŠ å†…å­˜éœ€æ±‚ï¼Œæˆä¸ºæ¨ç†é€Ÿåº¦å’Œå†…å­˜ä½¿ç”¨çš„ç“¶é¢ˆã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºVidKVçš„KVç¼“å­˜é‡åŒ–æ–¹æ³•ï¼Œå¯ä»¥å°†KVç¼“å­˜å‹ç¼©åˆ°ä½äº2ä½ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹æ€§èƒ½å‡ ä¹ä¸å˜ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡æ··åˆç²¾åº¦é‡åŒ–ç­–ç•¥å’Œé€‰æ‹©æ€§è¿‡æ»¤è¯­ä¹‰æ˜¾è‘—çš„è§†è§‰æ ‡è®°ï¼Œå®ç°äº†æ›´å¥½çš„ç²¾åº¦ä¸æ¨¡å‹æ€§èƒ½ä¹‹é—´çš„å¹³è¡¡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.16252', 'title': 'Fin-R1: A Large Language Model for Financial Reasoning through\n  Reinforcement Learning', 'url': 'https://huggingface.co/papers/2503.16252', 'abstract': 'Reasoning large language models are rapidly evolving across various domains. However, their capabilities in handling complex financial tasks still require in-depth exploration. In this paper, we introduce Fin-R1, a reasoning large language model specifically designed for the financial sector. Fin-R1 is built using a two-stage architecture, leveraging a financial reasoning dataset distilled and processed based on DeepSeek-R1. Through supervised fine-tuning (SFT) and reinforcement learning (RL) training, it demonstrates performance close to DeepSeek-R1 with a parameter size of 7 billion across a range of financial reasoning tasks. It achieves the state-of-the-art (SOTA) in the FinQA and ConvFinQA tasks between those LLMs in our evaluation, surpassing larger models in other tasks as well. Fin-R1 showcases strong reasoning and decision-making capabilities, providing solutions to various problems encountered in the financial domain. Our code is available at https://github.com/SUFE-AIFLM-Lab/Fin-R1.', 'score': 20, 'issue_id': 2823, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 20', 'zh': '3æœˆ20æ—¥'}, 'hash': 'f2a1e8506f9711ee', 'authors': ['Zhaowei Liu', 'Xin Guo', 'Fangqi Lou', 'Lingfeng Zeng', 'Jinyi Niu', 'Zixuan Wang', 'Jiajie Xu', 'Weige Cai', 'Ziwei Yang', 'Xueqian Zhao', 'Chao Li', 'Sheng Xu', 'Dezhi Chen', 'Yun Chen', 'Zuo Bai', 'Liwen Zhang'], 'affiliations': ['FinStep', 'Fudan University', 'Shanghai University of Finance and Economics'], 'pdf_title_img': 'assets/pdf/title_img/2503.16252.jpg', 'data': {'categories': ['#training', '#rl', '#architecture', '#reasoning', '#dataset', '#healthcare'], 'emoji': 'ğŸ’¹', 'ru': {'title': 'Fin-R1: ĞœĞ¾Ñ‰Ğ½Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Fin-R1 - ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ´Ğ»Ñ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ğ¾Ğ³Ğ¾ ÑĞµĞºÑ‚Ğ¾Ñ€Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ½Ğ° DeepSeek-R1. Fin-R1 Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ, Ğ±Ğ»Ğ¸Ğ·ĞºÑƒÑ Ğº DeepSeek-R1, Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğµ Ğ² 7 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… FinQA Ğ¸ ConvFinQA ÑÑ€ĞµĞ´Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµĞ¼Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ² Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ğ¾Ğ¹ ÑÑ„ĞµÑ€Ğµ.'}, 'en': {'title': 'Fin-R1: Revolutionizing Financial Reasoning with AI', 'desc': 'This paper presents Fin-R1, a specialized reasoning large language model tailored for financial tasks. It employs a two-stage architecture and is trained on a unique financial reasoning dataset derived from DeepSeek-R1. Through techniques like supervised fine-tuning and reinforcement learning, Fin-R1 achieves competitive performance with 7 billion parameters, excelling in financial reasoning benchmarks such as FinQA and ConvFinQA. The model demonstrates advanced reasoning and decision-making skills, addressing various challenges in the financial sector.'}, 'zh': {'title': 'é‡‘èé¢†åŸŸçš„æ¨ç†æ–°æ˜Ÿï¼šFin-R1', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ä¸“ä¸ºé‡‘èé¢†åŸŸè®¾è®¡çš„æ¨ç†å¤§å‹è¯­è¨€æ¨¡å‹Fin-R1ã€‚Fin-R1é‡‡ç”¨ä¸¤é˜¶æ®µæ¶æ„ï¼Œåˆ©ç”¨åŸºäºDeepSeek-R1çš„é‡‘èæ¨ç†æ•°æ®é›†è¿›è¡Œè®­ç»ƒã€‚é€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼Œå®ƒåœ¨å¤šä¸ªé‡‘èæ¨ç†ä»»åŠ¡ä¸­è¡¨ç°æ¥è¿‘DeepSeek-R1ï¼Œå‚æ•°è§„æ¨¡ä¸º70äº¿ã€‚Fin-R1åœ¨FinQAå’ŒConvFinQAä»»åŠ¡ä¸­è¾¾åˆ°äº†å½“å‰æœ€å…ˆè¿›çš„æ°´å¹³ï¼Œå±•ç°å‡ºå¼ºå¤§çš„æ¨ç†å’Œå†³ç­–èƒ½åŠ›ï¼Œä¸ºé‡‘èé¢†åŸŸçš„å„ç§é—®é¢˜æä¾›äº†è§£å†³æ–¹æ¡ˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.16212', 'title': 'MathFusion: Enhancing Mathematic Problem-solving of LLM through\n  Instruction Fusion', 'url': 'https://huggingface.co/papers/2503.16212', 'abstract': 'Large Language Models (LLMs) have shown impressive progress in mathematical reasoning. While data augmentation is promising to enhance mathematical problem-solving ability, current approaches are predominantly limited to instance-level modifications-such as rephrasing or generating syntactic variations-which fail to capture and leverage the intrinsic relational structures inherent in mathematical knowledge. Inspired by human learning processes, where mathematical proficiency develops through systematic exposure to interconnected concepts, we introduce MathFusion, a novel framework that enhances mathematical reasoning through cross-problem instruction synthesis. MathFusion implements this through three fusion strategies: (1) sequential fusion, which chains related problems to model solution dependencies; (2) parallel fusion, which combines analogous problems to reinforce conceptual understanding; and (3) conditional fusion, which creates context-aware selective problems to enhance reasoning flexibility. By applying these strategies, we generate a new dataset, MathFusionQA, followed by fine-tuning models (DeepSeekMath-7B, Mistral-7B, Llama3-8B) on it. Experimental results demonstrate that MathFusion achieves substantial improvements in mathematical reasoning while maintaining high data efficiency, boosting performance by 18.0 points in accuracy across diverse benchmarks while requiring only 45K additional synthetic instructions, representing a substantial improvement over traditional single-instruction approaches. Our datasets, models, and code are publicly available at https://github.com/QizhiPei/mathfusion.', 'score': 17, 'issue_id': 2823, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 20', 'zh': '3æœˆ20æ—¥'}, 'hash': 'b5997ebd979f98f0', 'authors': ['Qizhi Pei', 'Lijun Wu', 'Zhuoshi Pan', 'Yu Li', 'Honglin Lin', 'Chenlin Ming', 'Xin Gao', 'Conghui He', 'Rui Yan'], 'affiliations': ['Gaoling School of Artificial Intelligence, Renmin University of China', 'School of Computer Science, Wuhan University', 'Shanghai AI Laboratory', 'Shanghai Jiao Tong University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.16212.jpg', 'data': {'categories': ['#training', '#synthetic', '#open_source', '#math', '#reasoning', '#dataset', '#benchmark', '#data'], 'emoji': 'ğŸ§®', 'ru': {'title': 'Ğ¡Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾ĞºĞ°Ñ‡ĞºĞ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ˜Ğ˜', 'desc': 'MathFusion - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ñƒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ñ‡ĞµÑ€ĞµĞ· ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ²ÑĞ·ĞµĞ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚Ñ€Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡: Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ, Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¸ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğµ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ MathFusionQA Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… LLM, Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ² Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 18 Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿ÑƒĞ½ĞºÑ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ²ÑĞµĞ³Ğ¾ 45 Ñ‚Ñ‹ÑÑÑ‡ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾ Ğ½Ğ°Ğ´ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ….'}, 'en': {'title': 'Enhancing Mathematical Reasoning through Conceptual Fusion', 'desc': 'This paper presents MathFusion, a new framework designed to improve the mathematical reasoning capabilities of Large Language Models (LLMs). Unlike traditional methods that only modify individual problems, MathFusion uses three innovative fusion strategies to connect related mathematical concepts and problems. These strategies include sequential, parallel, and conditional fusion, which help models learn from the relationships between problems rather than in isolation. The results show that MathFusion significantly enhances accuracy in mathematical reasoning tasks while being efficient in data usage, outperforming previous methods with fewer additional instructions.'}, 'zh': {'title': 'é€šè¿‡MathFusionæå‡æ•°å­¦æ¨ç†èƒ½åŠ›', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•°å­¦æ¨ç†æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç°æœ‰çš„æ•°æ®å¢å¼ºæ–¹æ³•ä¸»è¦é›†ä¸­åœ¨å®ä¾‹çº§åˆ«çš„ä¿®æ”¹ï¼Œæœªèƒ½æœ‰æ•ˆåˆ©ç”¨æ•°å­¦çŸ¥è¯†ä¸­å›ºæœ‰çš„å…³ç³»ç»“æ„ã€‚æˆ‘ä»¬æå‡ºäº†MathFusionæ¡†æ¶ï¼Œé€šè¿‡è·¨é—®é¢˜çš„æŒ‡ä»¤åˆæˆæ¥æå‡æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚è¯¥æ¡†æ¶é‡‡ç”¨ä¸‰ç§èåˆç­–ç•¥ï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.10625', 'title': 'LHM: Large Animatable Human Reconstruction Model from a Single Image in\n  Seconds', 'url': 'https://huggingface.co/papers/2503.10625', 'abstract': 'Animatable 3D human reconstruction from a single image is a challenging problem due to the ambiguity in decoupling geometry, appearance, and deformation. Recent advances in 3D human reconstruction mainly focus on static human modeling, and the reliance of using synthetic 3D scans for training limits their generalization ability. Conversely, optimization-based video methods achieve higher fidelity but demand controlled capture conditions and computationally intensive refinement processes. Motivated by the emergence of large reconstruction models for efficient static reconstruction, we propose LHM (Large Animatable Human Reconstruction Model) to infer high-fidelity avatars represented as 3D Gaussian splatting in a feed-forward pass. Our model leverages a multimodal transformer architecture to effectively encode the human body positional features and image features with attention mechanism, enabling detailed preservation of clothing geometry and texture. To further boost the face identity preservation and fine detail recovery, we propose a head feature pyramid encoding scheme to aggregate multi-scale features of the head regions. Extensive experiments demonstrate that our LHM generates plausible animatable human in seconds without post-processing for face and hands, outperforming existing methods in both reconstruction accuracy and generalization ability.', 'score': 17, 'issue_id': 2828, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 13', 'zh': '3æœˆ13æ—¥'}, 'hash': '35de541953d40e20', 'authors': ['Lingteng Qiu', 'Xiaodong Gu', 'Peihao Li', 'Qi Zuo', 'Weichao Shen', 'Junfei Zhang', 'Kejie Qiu', 'Weihao Yuan', 'Guanying Chen', 'Zilong Dong', 'Liefeng Bo'], 'affiliations': ['Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2503.10625.jpg', 'data': {'categories': ['#multimodal', '#3d', '#architecture'], 'emoji': 'ğŸ§‘\u200dğŸ¦°', 'ru': {'title': 'Ğ ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ğ°Ğ½Ğ¸Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… 3D-Ğ°Ğ²Ğ°Ñ‚Ğ°Ñ€Ğ¾Ğ² Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¿Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ñƒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ LHM (Large Animatable Human Reconstruction Model) - Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ°Ğ½Ğ¸Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… 3D-Ğ°Ğ²Ğ°Ñ‚Ğ°Ñ€Ğ¾Ğ² Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¿Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ñƒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° Ğ´Ğ»Ñ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ñ‚ĞµĞ»Ğ° Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ñ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. Ğ”Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ»Ğ¸Ñ†Ğ° Ğ¸ Ğ¼ĞµĞ»ĞºĞ¸Ñ… Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ÑÑ…ĞµĞ¼Ğ° ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¸Ñ€Ğ°Ğ¼Ğ¸Ğ´Ñ‹ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ³Ğ¾Ğ»Ğ¾Ğ²Ñ‹. LHM Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ñ‹Ğµ Ğ°Ğ½Ğ¸Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ·Ğ° ÑĞµĞºÑƒĞ½Ğ´Ñ‹ Ğ±ĞµĞ· Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Revolutionizing 3D Human Reconstruction with LHM', 'desc': 'This paper presents the Large Animatable Human Reconstruction Model (LHM), which addresses the challenge of creating 3D human avatars from a single image. Unlike previous methods that rely on static models or require extensive computational resources, LHM uses a multimodal transformer architecture to efficiently encode both body and image features. The model incorporates a head feature pyramid encoding scheme to enhance the detail and identity of facial features. Experimental results show that LHM can generate high-fidelity, animatable human models quickly and accurately, surpassing existing techniques in both performance and adaptability.'}, 'zh': {'title': 'é«˜æ•ˆçš„3Då¯åŠ¨ç”»äººç±»é‡å»ºæ¨¡å‹', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¤§å‹å¯åŠ¨ç”»äººç±»é‡å»ºæ¨¡å‹ï¼ˆLHMï¼‰ï¼Œæ—¨åœ¨ä»å•å¼ å›¾åƒä¸­é‡å»ºé«˜ä¿çœŸåº¦çš„3Däººç±»å¤´åƒã€‚è¯¥æ¨¡å‹åˆ©ç”¨å¤šæ¨¡æ€å˜æ¢å™¨æ¶æ„ï¼Œæœ‰æ•ˆç¼–ç äººä½“ä½ç½®ç‰¹å¾å’Œå›¾åƒç‰¹å¾ï¼Œèƒ½å¤Ÿè¯¦ç»†ä¿ç•™è¡£ç‰©çš„å‡ ä½•å½¢çŠ¶å’Œçº¹ç†ã€‚ä¸ºäº†å¢å¼ºé¢éƒ¨èº«ä»½çš„ä¿ç•™å’Œç»†èŠ‚æ¢å¤ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†ä¸€ç§å¤´éƒ¨ç‰¹å¾é‡‘å­—å¡”ç¼–ç æ–¹æ¡ˆï¼Œèšåˆå¤´éƒ¨åŒºåŸŸçš„å¤šå°ºåº¦ç‰¹å¾ã€‚å®éªŒè¡¨æ˜ï¼ŒLHMåœ¨é‡å»ºå‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨å‡ ç§’å†…ç”Ÿæˆå¯åŠ¨ç”»çš„äººç±»æ¨¡å‹ï¼Œæ— éœ€åå¤„ç†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.16420', 'title': 'SynCity: Training-Free Generation of 3D Worlds', 'url': 'https://huggingface.co/papers/2503.16420', 'abstract': 'We address the challenge of generating 3D worlds from textual descriptions. We propose SynCity, a training- and optimization-free approach, which leverages the geometric precision of pre-trained 3D generative models and the artistic versatility of 2D image generators to create large, high-quality 3D spaces. While most 3D generative models are object-centric and cannot generate large-scale worlds, we show how 3D and 2D generators can be combined to generate ever-expanding scenes. Through a tile-based approach, we allow fine-grained control over the layout and the appearance of scenes. The world is generated tile-by-tile, and each new tile is generated within its world-context and then fused with the scene. SynCity generates compelling and immersive scenes that are rich in detail and diversity.', 'score': 16, 'issue_id': 2836, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 20', 'zh': '3æœˆ20æ—¥'}, 'hash': '2a35e5254e6b2f50', 'authors': ['Paul Engstler', 'Aleksandar Shtedritski', 'Iro Laina', 'Christian Rupprecht', 'Andrea Vedaldi'], 'affiliations': ['Visual Geometry Group, University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2503.16420.jpg', 'data': {'categories': ['#synthetic', '#3d', '#games'], 'emoji': 'ğŸ™ï¸', 'ru': {'title': 'SynCity: Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ±ĞµÑĞºĞ¾Ğ½ĞµÑ‡Ğ½Ñ‹Ñ… 3D-Ğ¼Ğ¸Ñ€Ğ¾Ğ² Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ SynCity - Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ¼Ğ¸Ñ€Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… 3D-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ñ…ÑƒĞ´Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ³Ğ¸Ğ±ĞºĞ¾ÑÑ‚ÑŒÑ 2D-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. SynCity Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚Ğ°Ğ¹Ğ»Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½Ğ¾Ğ²ĞºÑƒ Ğ¸ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğ¹ Ğ²Ğ¸Ğ´ ÑÑ†ĞµĞ½. ĞœĞ¸Ñ€ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ¿Ğ»Ğ¸Ñ‚ĞºĞ° Ğ·Ğ° Ğ¿Ğ»Ğ¸Ñ‚ĞºĞ¾Ğ¹, ĞºĞ°Ğ¶Ğ´Ğ°Ñ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¿Ğ»Ğ¸Ñ‚ĞºĞ° ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ÑÑ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ¼Ğ¸Ñ€Ğ° Ğ¸ Ğ·Ğ°Ñ‚ĞµĞ¼ ÑĞ»Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ ÑĞ¾ ÑÑ†ĞµĞ½Ğ¾Ğ¹.'}, 'en': {'title': 'Building 3D Worlds from Words with SynCity', 'desc': 'This paper introduces SynCity, a novel method for creating 3D environments from text descriptions without the need for extensive training or optimization. It combines the strengths of pre-trained 3D generative models, which provide geometric accuracy, with the flexibility of 2D image generators to produce expansive and detailed 3D worlds. By using a tile-based approach, SynCity allows for precise control over the layout and aesthetics of the generated scenes, ensuring that each tile fits cohesively within the overall context of the world. The result is a system capable of generating immersive and richly detailed environments that enhance user experience.'}, 'zh': {'title': 'ä»æ–‡æœ¬åˆ°3Dä¸–ç•Œçš„åˆ›æ–°ç”Ÿæˆ', 'desc': 'æˆ‘ä»¬è§£å†³äº†ä»æ–‡æœ¬æè¿°ç”Ÿæˆ3Dä¸–ç•Œçš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†SynCityï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒå’Œä¼˜åŒ–çš„æ–¹æ³•ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„3Dç”Ÿæˆæ¨¡å‹çš„å‡ ä½•ç²¾åº¦å’Œ2Då›¾åƒç”Ÿæˆå™¨çš„è‰ºæœ¯å¤šæ ·æ€§æ¥åˆ›å»ºå¤§å‹é«˜è´¨é‡çš„3Dç©ºé—´ã€‚é€šè¿‡åŸºäºç“¦ç‰‡çš„æ–¹æ³•ï¼Œæˆ‘ä»¬å¯ä»¥å¯¹åœºæ™¯çš„å¸ƒå±€å’Œå¤–è§‚è¿›è¡Œç»†è‡´çš„æ§åˆ¶ã€‚SynCityç”Ÿæˆçš„åœºæ™¯å¼•äººå…¥èƒœï¼Œç»†èŠ‚ä¸°å¯Œï¼Œå…·æœ‰å¤šæ ·æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.16413', 'title': 'M3: 3D-Spatial MultiModal Memory', 'url': 'https://huggingface.co/papers/2503.16413', 'abstract': "We present 3D Spatial MultiModal Memory (M3), a multimodal memory system designed to retain information about medium-sized static scenes through video sources for visual perception. By integrating 3D Gaussian Splatting techniques with foundation models, M3 builds a multimodal memory capable of rendering feature representations across granularities, encompassing a wide range of knowledge. In our exploration, we identify two key challenges in previous works on feature splatting: (1) computational constraints in storing high-dimensional features for each Gaussian primitive, and (2) misalignment or information loss between distilled features and foundation model features. To address these challenges, we propose M3 with key components of principal scene components and Gaussian memory attention, enabling efficient training and inference. To validate M3, we conduct comprehensive quantitative evaluations of feature similarity and downstream tasks, as well as qualitative visualizations to highlight the pixel trace of Gaussian memory attention. Our approach encompasses a diverse range of foundation models, including vision-language models (VLMs), perception models, and large multimodal and language models (LMMs/LLMs). Furthermore, to demonstrate real-world applicability, we deploy M3's feature field in indoor scenes on a quadruped robot. Notably, we claim that M3 is the first work to address the core compression challenges in 3D feature distillation.", 'score': 13, 'issue_id': 2823, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 20', 'zh': '3æœˆ20æ—¥'}, 'hash': '963439a9c78faf82', 'authors': ['Xueyan Zou', 'Yuchen Song', 'Ri-Zhao Qiu', 'Xuanbin Peng', 'Jianglong Ye', 'Sifei Liu', 'Xiaolong Wang'], 'affiliations': ['NVIDIA', 'UC San Diego'], 'pdf_title_img': 'assets/pdf/title_img/2503.16413.jpg', 'data': {'categories': ['#robotics', '#architecture', '#optimization', '#games', '#3d', '#multimodal'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ 3D Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ 3D ĞŸÑ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ ĞœÑƒĞ»ÑŒÑ‚Ğ¸ĞœĞ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ ĞŸĞ°Ğ¼ÑÑ‚ÑŒ (M3) - ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ, Ğ¿Ñ€ĞµĞ´Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ½ÑƒÑ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ… ÑÑ€ĞµĞ´Ğ½ĞµĞ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ. M3 Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ 3D Ğ³Ğ°ÑƒÑÑĞ¾Ğ²ÑĞºĞ¾Ğ³Ğ¾ ÑĞ¿Ğ»Ğ°Ñ‚Ñ‚Ğ¸Ğ½Ğ³Ğ° Ñ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾Ğ¹ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ñ‚ÑŒ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¹ Ğ³Ñ€Ğ°Ğ½ÑƒĞ»ÑÑ€Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ°Ğ¼Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ°Ğ¼Ğ¸ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° M3 Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ‚ÑŒÑÑ Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚Ğ¸Ğ¿Ğ°Ğ¼Ğ¸ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Revolutionizing Scene Memory with M3: Efficient 3D Feature Distillation', 'desc': 'The paper introduces the 3D Spatial MultiModal Memory (M3), a novel memory system that captures and retains information about static scenes using video inputs for enhanced visual perception. M3 combines 3D Gaussian Splatting with foundation models to create a multimodal memory that effectively represents features at various levels of detail. It tackles two significant challenges in previous methods: the difficulty of storing high-dimensional features and the misalignment of distilled features with those from foundation models. The authors validate M3 through extensive evaluations and demonstrate its practical use in real-world scenarios, such as deploying it on a quadruped robot in indoor environments.'}, 'zh': {'title': '3Då¤šæ¨¡æ€è®°å¿†ï¼šè§£å†³ç‰¹å¾å‹ç¼©æŒ‘æˆ˜çš„åˆ›æ–°', 'desc': 'æˆ‘ä»¬æå‡ºäº†3Dç©ºé—´å¤šæ¨¡æ€è®°å¿†ç³»ç»Ÿï¼ˆM3ï¼‰ï¼Œæ—¨åœ¨é€šè¿‡è§†é¢‘æºä¿ç•™ä¸­ç­‰å¤§å°é™æ€åœºæ™¯çš„ä¿¡æ¯ï¼Œä»¥å¢å¼ºè§†è§‰æ„ŸçŸ¥ã€‚M3ç»“åˆäº†3Dé«˜æ–¯ç‚¹äº‘æŠ€æœ¯å’ŒåŸºç¡€æ¨¡å‹ï¼Œæ„å»ºäº†ä¸€ä¸ªèƒ½å¤Ÿè·¨ç²’åº¦æ¸²æŸ“ç‰¹å¾è¡¨ç¤ºçš„å¤šæ¨¡æ€è®°å¿†ç³»ç»Ÿï¼Œæ¶µç›–å¹¿æ³›çš„çŸ¥è¯†ã€‚æˆ‘ä»¬è¯†åˆ«äº†ä»¥å¾€ç‰¹å¾ç‚¹äº‘å·¥ä½œä¸­çš„ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šå­˜å‚¨é«˜ç»´ç‰¹å¾çš„è®¡ç®—é™åˆ¶ï¼Œä»¥åŠæå–ç‰¹å¾ä¸åŸºç¡€æ¨¡å‹ç‰¹å¾ä¹‹é—´çš„é”™ä½æˆ–ä¿¡æ¯ä¸¢å¤±ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†M3ï¼Œé‡‡ç”¨äº†ä¸»è¦åœºæ™¯ç»„ä»¶å’Œé«˜æ–¯è®°å¿†æ³¨æ„åŠ›çš„å…³é”®ç»„ä»¶ï¼Œå®ç°äº†é«˜æ•ˆçš„è®­ç»ƒå’Œæ¨ç†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.16422', 'title': '1000+ FPS 4D Gaussian Splatting for Dynamic Scene Rendering', 'url': 'https://huggingface.co/papers/2503.16422', 'abstract': '4D Gaussian Splatting (4DGS) has recently gained considerable attention as a method for reconstructing dynamic scenes. Despite achieving superior quality, 4DGS typically requires substantial storage and suffers from slow rendering speed. In this work, we delve into these issues and identify two key sources of temporal redundancy. (Q1) Short-Lifespan Gaussians: 4DGS uses a large portion of Gaussians with short temporal span to represent scene dynamics, leading to an excessive number of Gaussians. (Q2) Inactive Gaussians: When rendering, only a small subset of Gaussians contributes to each frame. Despite this, all Gaussians are processed during rasterization, resulting in redundant computation overhead. To address these redundancies, we present 4DGS-1K, which runs at over 1000 FPS on modern GPUs. For Q1, we introduce the Spatial-Temporal Variation Score, a new pruning criterion that effectively removes short-lifespan Gaussians while encouraging 4DGS to capture scene dynamics using Gaussians with longer temporal spans. For Q2, we store a mask for active Gaussians across consecutive frames, significantly reducing redundant computations in rendering. Compared to vanilla 4DGS, our method achieves a 41times reduction in storage and 9times faster rasterization speed on complex dynamic scenes, while maintaining comparable visual quality. Please see our project page at https://4DGS-1K.github.io.', 'score': 11, 'issue_id': 2823, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 20', 'zh': '3æœˆ20æ—¥'}, 'hash': 'a00a6529a3f82f89', 'authors': ['Yuheng Yuan', 'Qiuhong Shen', 'Xingyi Yang', 'Xinchao Wang'], 'affiliations': ['National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2503.16422.jpg', 'data': {'categories': ['#3d', '#inference', '#optimization'], 'emoji': 'ğŸš€', 'ru': {'title': '4DGS-1K: Ğ¡Ğ²ĞµÑ€Ñ…Ğ±Ñ‹ÑÑ‚Ñ€Ğ°Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ†ĞµĞ½', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ 4DGS-1K Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ 4D Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ğ¾Ğ³Ğ¾ ÑĞ¿Ğ»Ğ°Ñ‚Ñ‚Ğ¸Ğ½Ğ³Ğ° Ğ² Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ†ĞµĞ½. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€ĞµÑˆĞ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ°, Ğ²Ñ‹Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¾Ğ¶Ğ¸Ğ²ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ³Ğ°ÑƒÑÑĞ¸Ğ°Ğ½Ğ°Ğ¼Ğ¸ Ğ¸ Ğ½ĞµĞ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ³Ğ°ÑƒÑÑĞ¸Ğ°Ğ½Ğ°Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸Ğ¹ Ğ¾Ñ‚ÑĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ³Ğ°ÑƒÑÑĞ¸Ğ°Ğ½Ğ¾Ğ². Ğ’ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğµ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ÑÑ 41-ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğµ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑŠĞµĞ¼Ğ° Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¸ 9-ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğµ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğ¼ 4DGS Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°.'}, 'en': {'title': 'Streamlining Dynamic Scene Reconstruction with 4DGS-1K', 'desc': 'This paper introduces 4DGS-1K, an improved version of 4D Gaussian Splatting that addresses issues of high storage requirements and slow rendering speeds in dynamic scene reconstruction. The authors identify two main sources of inefficiency: the use of short-lifespan Gaussians that clutter the representation and the processing of inactive Gaussians during rendering. To optimize performance, they propose a new pruning criterion called the Spatial-Temporal Variation Score, which helps retain only the most relevant Gaussians. As a result, 4DGS-1K achieves a significant reduction in storage and rendering time while preserving visual quality, making it a more efficient solution for real-time applications.'}, 'zh': {'title': 'æå‡åŠ¨æ€åœºæ™¯é‡å»ºæ•ˆç‡çš„4DGS-1K', 'desc': '4Dé«˜æ–¯ç‚¹äº‘ï¼ˆ4DGSï¼‰æ˜¯ä¸€ç§ç”¨äºé‡å»ºåŠ¨æ€åœºæ™¯çš„æ–¹æ³•ï¼Œä½†é€šå¸¸éœ€è¦å¤§é‡å­˜å‚¨å¹¶ä¸”æ¸²æŸ“é€Ÿåº¦è¾ƒæ…¢ã€‚æœ¬æ–‡æ¢è®¨äº†å¯¼è‡´è¿™äº›é—®é¢˜çš„ä¸¤ä¸ªä¸»è¦æ¥æºï¼šçŸ­ç”Ÿå‘½å‘¨æœŸé«˜æ–¯å’Œéæ´»åŠ¨é«˜æ–¯ã€‚æˆ‘ä»¬æå‡ºäº†4DGS-1Kï¼Œé€šè¿‡å¼•å…¥ç©ºé—´-æ—¶é—´å˜åŒ–è¯„åˆ†æ¥æœ‰æ•ˆå»é™¤çŸ­ç”Ÿå‘½å‘¨æœŸé«˜æ–¯ï¼Œå¹¶å­˜å‚¨æ´»è·ƒé«˜æ–¯çš„æ©ç ï¼Œä»è€Œæ˜¾è‘—å‡å°‘å†—ä½™è®¡ç®—ã€‚ä¸ä¼ ç»Ÿ4DGSç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤æ‚åŠ¨æ€åœºæ™¯ä¸­å®ç°äº†41å€çš„å­˜å‚¨å‡å°‘å’Œ9å€çš„æ¸²æŸ“é€Ÿåº¦æå‡ï¼ŒåŒæ—¶ä¿æŒäº†ç›¸ä¼¼çš„è§†è§‰è´¨é‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.16356', 'title': 'CaKE: Circuit-aware Editing Enables Generalizable Knowledge Learners', 'url': 'https://huggingface.co/papers/2503.16356', 'abstract': 'Knowledge Editing (KE) enables the modification of outdated or incorrect information in large language models (LLMs). While existing KE methods can update isolated facts, they struggle to generalize these updates to multi-hop reasoning tasks that depend on the modified knowledge. Through an analysis of reasoning circuits -- the neural pathways LLMs use for knowledge-based inference, we observe that current layer-localized KE approaches, such as MEMIT and WISE, which edit only single or a few model layers, struggle to effectively incorporate updated information into these reasoning pathways. To address this limitation, we propose CaKE (Circuit-aware Knowledge Editing), a novel method that enables more effective integration of updated knowledge in LLMs. CaKE leverages strategically curated data, guided by our circuits-based analysis, that enforces the model to utilize the modified knowledge, stimulating the model to develop appropriate reasoning circuits for newly integrated knowledge. Experimental results show that CaKE enables more accurate and consistent use of updated knowledge across related reasoning tasks, leading to an average of 20% improvement in multi-hop reasoning accuracy on MQuAKE dataset compared to existing KE methods. We release the code and data in https://github.com/zjunlp/CaKE.', 'score': 11, 'issue_id': 2823, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 20', 'zh': '3æœˆ20æ—¥'}, 'hash': '8b74fed43c99c37d', 'authors': ['Yunzhi Yao', 'Jizhan Fang', 'Jia-Chen Gu', 'Ningyu Zhang', 'Shumin Deng', 'Huajun Chen', 'Nanyun Peng'], 'affiliations': ['National University of Singapore', 'University of California, Los Angeles', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.16356.jpg', 'data': {'categories': ['#training', '#open_source', '#reasoning', '#dataset', '#data'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'CaKE: ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ˜Ğ˜ Ñ‡ĞµÑ€ĞµĞ· ÑƒĞ¼Ğ½Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ CaKE. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½Ñ‡Ğ°Ñ‚Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼. CaKE Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¿ÑƒÑ‚ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ´Ğ»Ñ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ CaKE Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ² ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½Ñ‡Ğ°Ñ‚Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² ÑÑ€ĞµĞ´Ğ½ĞµĞ¼ Ğ½Ğ° 20% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Enhancing Knowledge Integration in Language Models with CaKE', 'desc': 'This paper introduces CaKE (Circuit-aware Knowledge Editing), a new method for updating knowledge in large language models (LLMs). Current methods struggle with multi-hop reasoning tasks that require the integration of modified information. CaKE improves upon these methods by analyzing the neural pathways used for reasoning and ensuring that updated knowledge is effectively incorporated. The results show a significant increase in accuracy for reasoning tasks, demonstrating the effectiveness of this approach.'}, 'zh': {'title': 'ç”µè·¯æ„ŸçŸ¥çŸ¥è¯†ç¼–è¾‘ï¼šæå‡å¤šè·³æ¨ç†çš„å‡†ç¡®æ€§', 'desc': 'çŸ¥è¯†ç¼–è¾‘ï¼ˆKEï¼‰ä½¿å¾—å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰èƒ½å¤Ÿä¿®æ”¹è¿‡æ—¶æˆ–ä¸æ­£ç¡®çš„ä¿¡æ¯ã€‚ç°æœ‰çš„KEæ–¹æ³•è™½ç„¶å¯ä»¥æ›´æ–°å­¤ç«‹çš„äº‹å®ï¼Œä½†åœ¨å¤šè·³æ¨ç†ä»»åŠ¡ä¸­å´éš¾ä»¥æœ‰æ•ˆæ¨å¹¿è¿™äº›æ›´æ–°ã€‚é€šè¿‡å¯¹æ¨ç†ç”µè·¯çš„åˆ†æï¼Œæˆ‘ä»¬å‘ç°å½“å‰çš„å±‚å±€éƒ¨KEæ–¹æ³•åœ¨æœ‰æ•ˆæ•´åˆæ›´æ–°ä¿¡æ¯æ–¹é¢å­˜åœ¨å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†CaKEï¼ˆç”µè·¯æ„ŸçŸ¥çŸ¥è¯†ç¼–è¾‘ï¼‰ï¼Œå®ƒé€šè¿‡ç²¾å¿ƒç­–åˆ’çš„æ•°æ®ï¼Œä¿ƒè¿›æ¨¡å‹åˆ©ç”¨ä¿®æ”¹åçš„çŸ¥è¯†ï¼Œä»è€Œæé«˜å¤šè·³æ¨ç†çš„å‡†ç¡®æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.16322', 'title': 'Ultra-Resolution Adaptation with Ease', 'url': 'https://huggingface.co/papers/2503.16322', 'abstract': 'Text-to-image diffusion models have achieved remarkable progress in recent years. However, training models for high-resolution image generation remains challenging, particularly when training data and computational resources are limited. In this paper, we explore this practical problem from two key perspectives: data and parameter efficiency, and propose a set of key guidelines for ultra-resolution adaptation termed URAE. For data efficiency, we theoretically and empirically demonstrate that synthetic data generated by some teacher models can significantly promote training convergence. For parameter efficiency, we find that tuning minor components of the weight matrices outperforms widely-used low-rank adapters when synthetic data are unavailable, offering substantial performance gains while maintaining efficiency. Additionally, for models leveraging guidance distillation, such as FLUX, we show that disabling classifier-free guidance, i.e., setting the guidance scale to 1 during adaptation, is crucial for satisfactory performance. Extensive experiments validate that URAE achieves comparable 2K-generation performance to state-of-the-art closed-source models like FLUX1.1 [Pro] Ultra with only 3K samples and 2K iterations, while setting new benchmarks for 4K-resolution generation. Codes are available https://github.com/Huage001/URAE{here}.', 'score': 11, 'issue_id': 2824, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 20', 'zh': '3æœˆ20æ—¥'}, 'hash': '48ce70d2a4cf0cf7', 'authors': ['Ruonan Yu', 'Songhua Liu', 'Zhenxiong Tan', 'Xinchao Wang'], 'affiliations': ['National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2503.16322.jpg', 'data': {'categories': ['#optimization', '#data', '#diffusion', '#cv', '#synthetic', '#training', '#benchmark'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ ÑĞ²ĞµÑ€Ñ…Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ URAE Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ ÑĞ²ĞµÑ€Ñ…Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ñ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ² Ğ²ĞµÑĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ† Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼Ñ‹Ğµ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€Ğ°Ğ½Ğ³Ğ¾Ğ²Ñ‹Ğµ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ñ‹ Ğ¿Ñ€Ğ¸ Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²Ğ¸Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ URAE Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 2K-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ²ÑĞµĞ³Ğ¾ 3000 Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ² Ğ¸ 2000 Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹, ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°Ñ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ñ‹ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸ĞµĞ¼ 4K.'}, 'en': {'title': 'Efficient High-Resolution Image Generation with URAE', 'desc': 'This paper addresses the challenges of training text-to-image diffusion models for high-resolution image generation, especially when data and computational resources are limited. It introduces URAE, a set of guidelines focused on improving data and parameter efficiency. The authors demonstrate that synthetic data from teacher models can enhance training convergence, and that tuning specific components of weight matrices can yield better results than traditional low-rank adapters. Their experiments show that URAE can achieve high-quality 2K and 4K image generation with significantly fewer training samples and iterations compared to existing models.'}, 'zh': {'title': 'é«˜æ•ˆç”Ÿæˆé«˜åˆ†è¾¨ç‡å›¾åƒçš„åˆ›æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æ¢è®¨äº†é«˜åˆ†è¾¨ç‡å›¾åƒç”Ÿæˆä¸­çš„è®­ç»ƒæŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨æ•°æ®å’Œè®¡ç®—èµ„æºæœ‰é™çš„æƒ…å†µä¸‹ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€å¥—åä¸ºURAEçš„è¶…åˆ†è¾¨ç‡é€‚åº”æŒ‡å—ï¼Œæ—¨åœ¨æé«˜æ•°æ®å’Œå‚æ•°æ•ˆç‡ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒæŸäº›æ•™å¸ˆæ¨¡å‹ç”Ÿæˆçš„åˆæˆæ•°æ®å¯ä»¥æ˜¾è‘—ä¿ƒè¿›è®­ç»ƒæ”¶æ•›ï¼Œè€Œå¾®è°ƒæƒé‡çŸ©é˜µçš„å°‘é‡ç»„ä»¶åœ¨ç¼ºä¹åˆæˆæ•°æ®æ—¶è¡¨ç°ä¼˜äºå¸¸ç”¨çš„ä½ç§©é€‚é…å™¨ã€‚é€šè¿‡å¤§é‡å®éªŒéªŒè¯ï¼ŒURAEåœ¨ä»…ä½¿ç”¨3000ä¸ªæ ·æœ¬å’Œ2000æ¬¡è¿­ä»£çš„æƒ…å†µä¸‹ï¼Œè¾¾åˆ°äº†ä¸FLUX1.1ç­‰æœ€å…ˆè¿›é—­æºæ¨¡å‹ç›¸å½“çš„2Kç”Ÿæˆæ€§èƒ½ï¼Œå¹¶ä¸º4Kåˆ†è¾¨ç‡ç”Ÿæˆè®¾å®šäº†æ–°åŸºå‡†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.16057', 'title': 'Expert Race: A Flexible Routing Strategy for Scaling Diffusion\n  Transformer with Mixture of Experts', 'url': 'https://huggingface.co/papers/2503.16057', 'abstract': 'Diffusion models have emerged as mainstream framework in visual generation. Building upon this success, the integration of Mixture of Experts (MoE) methods has shown promise in enhancing model scalability and performance. In this paper, we introduce Race-DiT, a novel MoE model for diffusion transformers with a flexible routing strategy, Expert Race. By allowing tokens and experts to compete together and select the top candidates, the model learns to dynamically assign experts to critical tokens. Additionally, we propose per-layer regularization to address challenges in shallow layer learning, and router similarity loss to prevent mode collapse, ensuring better expert utilization. Extensive experiments on ImageNet validate the effectiveness of our approach, showcasing significant performance gains while promising scaling properties.', 'score': 11, 'issue_id': 2827, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 20', 'zh': '3æœˆ20æ—¥'}, 'hash': 'bb5d00b3068e881c', 'authors': ['Yike Yuan', 'Ziyu Wang', 'Zihao Huang', 'Defa Zhu', 'Xun Zhou', 'Jingyi Yu', 'Qiyang Min'], 'affiliations': ['ByteDance Seed', 'ShanghaiTech University'], 'pdf_title_img': 'assets/pdf/title_img/2503.16057.jpg', 'data': {'categories': ['#architecture', '#diffusion', '#cv', '#training', '#optimization'], 'emoji': 'ğŸï¸', 'ru': {'title': 'Race-DiT: Ğ“Ğ¾Ğ½ĞºĞ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Race-DiT - Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Mixture of Experts (MoE) Ğ´Ğ»Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ñ Ğ³Ğ¸Ğ±ĞºĞ¾Ğ¹ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸ĞµĞ¹ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Expert Race. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡Ğ°ĞµÑ‚ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğ¼ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼ Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ğ¼ ÑĞ¾Ñ€ĞµĞ²Ğ½Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ¸ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°Ñ‚ÑŒ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ÑĞ»Ğ¾Ñ, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ñ€ĞµÑˆĞ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ñ… ÑĞ»Ğ¾ĞµĞ², Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ° Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ»Ğ»Ğ°Ğ¿ÑĞ° Ğ¼Ğ¾Ğ´. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° ImageNet Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ° Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'Race-DiT: Enhancing Diffusion Transformers with Expert Competition', 'desc': 'This paper presents Race-DiT, a new model that combines diffusion transformers with Mixture of Experts (MoE) techniques to improve visual generation. The model features a flexible routing strategy called Expert Race, which allows tokens to compete and select the best experts for processing. To enhance learning in shallow layers, the authors introduce per-layer regularization and a router similarity loss to avoid mode collapse. Experimental results on ImageNet demonstrate that Race-DiT achieves significant performance improvements and scalability compared to previous models.'}, 'zh': {'title': 'Race-DiTï¼šåŠ¨æ€åˆ†é…ä¸“å®¶çš„æ‰©æ•£æ¨¡å‹', 'desc': 'æ‰©æ•£æ¨¡å‹å·²æˆä¸ºè§†è§‰ç”Ÿæˆçš„ä¸»æµæ¡†æ¶ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ··åˆä¸“å®¶æ¨¡å‹Race-DiTï¼Œé‡‡ç”¨çµæ´»çš„è·¯ç”±ç­–ç•¥Expert Raceï¼Œä»¥æé«˜æ¨¡å‹çš„å¯æ‰©å±•æ€§å’Œæ€§èƒ½ã€‚è¯¥æ¨¡å‹é€šè¿‡è®©ä»¤ç‰Œå’Œä¸“å®¶ç«äº‰å¹¶é€‰æ‹©æœ€ä½³å€™é€‰è€…ï¼ŒåŠ¨æ€åˆ†é…ä¸“å®¶ç»™å…³é”®ä»¤ç‰Œã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†æ¯å±‚æ­£åˆ™åŒ–å’Œè·¯ç”±å™¨ç›¸ä¼¼æ€§æŸå¤±ï¼Œä»¥è§£å†³æµ…å±‚å­¦ä¹ ä¸­çš„æŒ‘æˆ˜ï¼Œç¡®ä¿æ›´å¥½çš„ä¸“å®¶åˆ©ç”¨ç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.16428', 'title': 'XAttention: Block Sparse Attention with Antidiagonal Scoring', 'url': 'https://huggingface.co/papers/2503.16428', 'abstract': "Long-Context Transformer Models (LCTMs) are vital for real-world applications but suffer high computational costs due to attention's quadratic complexity. Block-sparse attention mitigates this by focusing computation on critical regions, yet existing methods struggle with balancing accuracy and efficiency due to costly block importance measurements. In this paper, we introduce XAttention, a plug-and-play framework that dramatically accelerates long-context inference in Transformers models using sparse attention. XAttention's key innovation is the insight that the sum of antidiagonal values (i.e., from the lower-left to upper-right) in the attention matrix provides a powerful proxy for block importance. This allows for precise identification and pruning of non-essential blocks, resulting in high sparsity and dramatically accelerated inference. Across comprehensive evaluations on demanding long-context benchmarks-including RULER and LongBench for language, VideoMME for video understanding, and VBench for video generation. XAttention achieves accuracy comparable to full attention while delivering substantial computational gains. We demonstrate up to 13.5x acceleration in attention computation. These results underscore XAttention's ability to unlock the practical potential of block sparse attention, paving the way for scalable and efficient deployment of LCTMs in real-world applications. Code is available at https://github.com/mit-han-lab/x-attention.", 'score': 10, 'issue_id': 2823, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 20', 'zh': '3æœˆ20æ—¥'}, 'hash': 'befbf726163c510a', 'authors': ['Ruyi Xu', 'Guangxuan Xiao', 'Haofeng Huang', 'Junxian Guo', 'Song Han'], 'affiliations': ['Massachusetts Institute of Technology', 'NVIDIA', 'SJTU', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.16428.jpg', 'data': {'categories': ['#inference', '#architecture', '#long_context', '#optimization', '#video', '#benchmark'], 'emoji': 'ğŸš€', 'ru': {'title': 'XAttention: Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸', 'desc': 'XAttention - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ñ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ ÑÑƒĞ¼Ğ¼Ñ‹ Ğ°Ğ½Ñ‚Ğ¸Ğ´Ğ¸Ğ°Ğ³Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ»Ğ¾ĞºĞ¾Ğ². Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸ Ğ¾Ñ‚ÑĞµĞºĞ°Ñ‚ÑŒ Ğ½ĞµÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ±Ğ»Ğ¾ĞºĞ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼Ñƒ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. XAttention Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼ÑƒÑ Ñ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼, Ğ¿Ñ€Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ².'}, 'en': {'title': 'Accelerating Long-Context Transformers with XAttention', 'desc': 'This paper presents XAttention, a new framework designed to improve the efficiency of long-context Transformer models by utilizing block-sparse attention. Traditional attention mechanisms are computationally expensive, but XAttention reduces this cost by focusing on important regions of the attention matrix. The key innovation is using the sum of antidiagonal values to identify and prune less important blocks, which maintains accuracy while enhancing speed. Evaluations show that XAttention can achieve up to 13.5 times faster attention computation without sacrificing performance, making it a significant advancement for deploying LCTMs in practical applications.'}, 'zh': {'title': 'XAttentionï¼šåŠ é€Ÿé•¿ä¸Šä¸‹æ–‡æ¨ç†çš„åˆ›æ–°æ¡†æ¶', 'desc': 'é•¿ä¸Šä¸‹æ–‡å˜æ¢å™¨æ¨¡å‹ï¼ˆLCTMsï¼‰åœ¨å®é™…åº”ç”¨ä¸­éå¸¸é‡è¦ï¼Œä½†ç”±äºæ³¨æ„åŠ›æœºåˆ¶çš„å¹³æ–¹å¤æ‚åº¦ï¼Œè®¡ç®—æˆæœ¬å¾ˆé«˜ã€‚å—ç¨€ç–æ³¨æ„åŠ›é€šè¿‡é›†ä¸­è®¡ç®—åœ¨å…³é”®åŒºåŸŸæ¥ç¼“è§£è¿™ä¸€é—®é¢˜ï¼Œä½†ç°æœ‰æ–¹æ³•åœ¨å¹³è¡¡å‡†ç¡®æ€§å’Œæ•ˆç‡æ–¹é¢é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬æ–‡ä»‹ç»äº†XAttentionï¼Œè¿™æ˜¯ä¸€ç§å³æ’å³ç”¨çš„æ¡†æ¶ï¼Œé€šè¿‡ç¨€ç–æ³¨æ„åŠ›æ˜¾è‘—åŠ é€Ÿå˜æ¢å™¨æ¨¡å‹çš„é•¿ä¸Šä¸‹æ–‡æ¨ç†ã€‚XAttentionçš„å…³é”®åˆ›æ–°åœ¨äºåˆ©ç”¨æ³¨æ„åŠ›çŸ©é˜µä¸­åå¯¹è§’çº¿å€¼çš„æ€»å’Œä½œä¸ºå—é‡è¦æ€§çš„å¼ºå¤§ä»£ç†ï¼Œä»è€Œå®ç°é«˜ç¨€ç–æ€§å’Œæ˜¾è‘—åŠ é€Ÿæ¨ç†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.16425', 'title': 'Tokenize Image as a Set', 'url': 'https://huggingface.co/papers/2503.16425', 'abstract': "This paper proposes a fundamentally new paradigm for image generation through set-based tokenization and distribution modeling. Unlike conventional methods that serialize images into fixed-position latent codes with a uniform compression ratio, we introduce an unordered token set representation to dynamically allocate coding capacity based on regional semantic complexity. This TokenSet enhances global context aggregation and improves robustness against local perturbations. To address the critical challenge of modeling discrete sets, we devise a dual transformation mechanism that bijectively converts sets into fixed-length integer sequences with summation constraints. Further, we propose Fixed-Sum Discrete Diffusion--the first framework to simultaneously handle discrete values, fixed sequence length, and summation invariance--enabling effective set distribution modeling. Experiments demonstrate our method's superiority in semantic-aware representation and generation quality. Our innovations, spanning novel representation and modeling strategies, advance visual generation beyond traditional sequential token paradigms. Our code and models are publicly available at https://github.com/Gengzigang/TokenSet.", 'score': 10, 'issue_id': 2831, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 20', 'zh': '3æœˆ20æ—¥'}, 'hash': 'c960fe86078f8a7b', 'authors': ['Zigang Geng', 'Mengde Xu', 'Han Hu', 'Shuyang Gu'], 'affiliations': ['Tencent Hunyuan Research', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2503.16425.jpg', 'data': {'categories': ['#cv', '#open_source', '#diffusion', '#video'], 'emoji': 'ğŸ§©', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹: Ğ¾Ñ‚ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğº Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ°Ğ¼', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ² Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ² Ğ²Ğ¸Ğ´Ğµ Ğ½ĞµÑƒĞ¿Ğ¾Ñ€ÑĞ´Ğ¾Ñ‡ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ÑÑ‚ÑŒ ĞµĞ¼ĞºĞ¾ÑÑ‚ÑŒ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. Ğ”Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ² Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ±Ğ¸ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ° Ğ² Ñ†ĞµĞ»Ğ¾Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ½Ğ° ÑÑƒĞ¼Ğ¼Ñƒ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ² ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Revolutionizing Image Generation with Dynamic Token Sets', 'desc': 'This paper introduces a new way to generate images using a method called set-based tokenization and distribution modeling. Instead of using fixed-position codes, it uses an unordered set of tokens that can adapt based on the complexity of different image areas. This approach improves how the model understands the overall context of the image and makes it more resilient to small changes. The authors also present a new framework called Fixed-Sum Discrete Diffusion, which effectively manages discrete values and maintains certain constraints, leading to better image generation results.'}, 'zh': {'title': 'å›¾åƒç”Ÿæˆçš„æ–°èŒƒå¼ï¼šé›†åˆæ ‡è®°åŒ–ä¸åˆ†å¸ƒå»ºæ¨¡', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§å…¨æ–°çš„å›¾åƒç”ŸæˆèŒƒå¼ï¼Œé€šè¿‡åŸºäºé›†åˆçš„æ ‡è®°åŒ–å’Œåˆ†å¸ƒå»ºæ¨¡æ¥å®ç°ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•å°†å›¾åƒåºåˆ—åŒ–ä¸ºå›ºå®šä½ç½®çš„æ½œåœ¨ç¼–ç ä¸åŒï¼Œæˆ‘ä»¬å¼•å…¥äº†æ— åºçš„æ ‡è®°é›†åˆè¡¨ç¤ºï¼Œèƒ½å¤Ÿæ ¹æ®åŒºåŸŸè¯­ä¹‰å¤æ‚æ€§åŠ¨æ€åˆ†é…ç¼–ç èƒ½åŠ›ã€‚è¯¥TokenSetå¢å¼ºäº†å…¨å±€ä¸Šä¸‹æ–‡èšåˆèƒ½åŠ›ï¼Œå¹¶æé«˜äº†å¯¹å±€éƒ¨æ‰°åŠ¨çš„é²æ£’æ€§ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†å›ºå®šå’Œç¦»æ•£æ‰©æ•£æ¡†æ¶ï¼Œé¦–æ¬¡åŒæ—¶å¤„ç†ç¦»æ•£å€¼ã€å›ºå®šåºåˆ—é•¿åº¦å’Œå’Œä¸å˜æ€§ï¼Œä»è€Œæœ‰æ•ˆåœ°å»ºæ¨¡é›†åˆåˆ†å¸ƒã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.16421', 'title': 'MagicMotion: Controllable Video Generation with Dense-to-Sparse\n  Trajectory Guidance', 'url': 'https://huggingface.co/papers/2503.16421', 'abstract': 'Recent advances in video generation have led to remarkable improvements in visual quality and temporal coherence. Upon this, trajectory-controllable video generation has emerged to enable precise object motion control through explicitly defined spatial paths. However, existing methods struggle with complex object movements and multi-object motion control, resulting in imprecise trajectory adherence, poor object consistency, and compromised visual quality. Furthermore, these methods only support trajectory control in a single format, limiting their applicability in diverse scenarios. Additionally, there is no publicly available dataset or benchmark specifically tailored for trajectory-controllable video generation, hindering robust training and systematic evaluation. To address these challenges, we introduce MagicMotion, a novel image-to-video generation framework that enables trajectory control through three levels of conditions from dense to sparse: masks, bounding boxes, and sparse boxes. Given an input image and trajectories, MagicMotion seamlessly animates objects along defined trajectories while maintaining object consistency and visual quality. Furthermore, we present MagicData, a large-scale trajectory-controlled video dataset, along with an automated pipeline for annotation and filtering. We also introduce MagicBench, a comprehensive benchmark that assesses both video quality and trajectory control accuracy across different numbers of objects. Extensive experiments demonstrate that MagicMotion outperforms previous methods across various metrics. Our project page are publicly available at https://quanhaol.github.io/magicmotion-site.', 'score': 8, 'issue_id': 2823, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 20', 'zh': '3æœˆ20æ—¥'}, 'hash': 'ae5c7f7abb796973', 'authors': ['Quanhao Li', 'Zhen Xing', 'Rui Wang', 'Hui Zhang', 'Qi Dai', 'Zuxuan Wu'], 'affiliations': ['Fudan University', 'Microsoft Research Asia'], 'pdf_title_img': 'assets/pdf/title_img/2503.16421.jpg', 'data': {'categories': ['#open_source', '#optimization', '#video', '#games', '#dataset', '#benchmark'], 'emoji': 'ğŸ¥', 'ru': {'title': 'MagicMotion: Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ĞµĞ¼ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¼ Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'MagicMotion - Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². ĞĞ½Ğ° Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ¸ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ĞµĞ¹: Ğ¼Ğ°ÑĞºĞ¸, Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğµ Ñ€Ğ°Ğ¼ĞºĞ¸ Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğµ Ñ€Ğ°Ğ¼ĞºĞ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ MagicData - ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ MagicBench - Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ MagicMotion Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼.'}, 'en': {'title': 'MagicMotion: Mastering Object Motion in Video Generation', 'desc': 'This paper presents MagicMotion, a new framework for generating videos from images while allowing precise control over object movements along defined paths. It addresses the limitations of existing methods that struggle with complex and multi-object trajectories, which often lead to poor visual quality and object consistency. MagicMotion introduces a tiered approach to trajectory control using masks, bounding boxes, and sparse boxes, enhancing flexibility and applicability. Additionally, the authors provide a new dataset, MagicData, and a benchmark, MagicBench, to facilitate better training and evaluation of trajectory-controllable video generation.'}, 'zh': {'title': 'MagicMotionï¼šç²¾å‡†è½¨è¿¹æ§åˆ¶çš„è§†é¢‘ç”Ÿæˆæ–°æ¡†æ¶', 'desc': 'æœ€è¿‘è§†é¢‘ç”ŸæˆæŠ€æœ¯å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œæå‡äº†è§†è§‰è´¨é‡å’Œæ—¶é—´ä¸€è‡´æ€§ã€‚é’ˆå¯¹å¤æ‚ç‰©ä½“è¿åŠ¨å’Œå¤šç‰©ä½“è¿åŠ¨æ§åˆ¶çš„é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†MagicMotionæ¡†æ¶ï¼Œæ”¯æŒé€šè¿‡ä¸åŒå±‚æ¬¡çš„æ¡ä»¶ï¼ˆå¦‚æ©ç ã€è¾¹ç•Œæ¡†å’Œç¨€ç–æ¡†ï¼‰è¿›è¡Œè½¨è¿¹æ§åˆ¶ã€‚MagicMotionèƒ½å¤Ÿåœ¨ä¿æŒç‰©ä½“ä¸€è‡´æ€§å’Œè§†è§‰è´¨é‡çš„åŒæ—¶ï¼Œæ²¿ç€å®šä¹‰çš„è½¨è¿¹æ— ç¼åŠ¨ç”»åŒ–ç‰©ä½“ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¨å‡ºäº†MagicDataæ•°æ®é›†å’ŒMagicBenchåŸºå‡†ï¼Œä¿ƒè¿›äº†è½¨è¿¹æ§åˆ¶è§†é¢‘ç”Ÿæˆçš„ç ”ç©¶å’Œè¯„ä¼°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.16375', 'title': 'NuiScene: Exploring Efficient Generation of Unbounded Outdoor Scenes', 'url': 'https://huggingface.co/papers/2503.16375', 'abstract': 'In this paper, we explore the task of generating expansive outdoor scenes, ranging from castles to high-rises. Unlike indoor scene generation, which has been a primary focus of prior work, outdoor scene generation presents unique challenges, including wide variations in scene heights and the need for a method capable of rapidly producing large landscapes. To address this, we propose an efficient approach that encodes scene chunks as uniform vector sets, offering better compression and performance than the spatially structured latents used in prior methods. Furthermore, we train an explicit outpainting model for unbounded generation, which improves coherence compared to prior resampling-based inpainting schemes while also speeding up generation by eliminating extra diffusion steps. To facilitate this task, we curate NuiScene43, a small but high-quality set of scenes, preprocessed for joint training. Notably, when trained on scenes of varying styles, our model can blend different environments, such as rural houses and city skyscrapers, within the same scene, highlighting the potential of our curation process to leverage heterogeneous scenes for joint training.', 'score': 8, 'issue_id': 2829, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 20', 'zh': '3æœˆ20æ—¥'}, 'hash': 'bd1b53cbfef36cbc', 'authors': ['Han-Hung Lee', 'Qinghong Han', 'Angel X. Chang'], 'affiliations': ['Canada CIFAR AI Chair, Amii', 'Simon Fraser University'], 'pdf_title_img': 'assets/pdf/title_img/2503.16375.jpg', 'data': {'categories': ['#dataset', '#diffusion', '#synthetic', '#3d'], 'emoji': 'ğŸ™ï¸', 'ru': {'title': 'Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑÑ†ĞµĞ½: Ğ¾Ñ‚ Ğ·Ğ°Ğ¼ĞºĞ¾Ğ² Ğ´Ğ¾ Ğ½ĞµĞ±Ğ¾ÑĞºÑ€ĞµĞ±Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ñ‹Ñ… Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑÑ†ĞµĞ½, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ·Ğ°Ğ¼ĞºĞ¸ Ğ¸ Ğ½ĞµĞ±Ğ¾ÑĞºÑ€ĞµĞ±Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ñ‹ ÑÑ†ĞµĞ½Ñ‹ ĞºĞ°Ğº Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ½ĞµĞ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ÑÑ‰Ğ°Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… NuiScene43, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑÑ‚Ğ¸Ğ»Ğ¸ Ğ² Ğ¾Ğ´Ğ½Ğ¾Ğ¹ ÑÑ†ĞµĞ½Ğµ.'}, 'en': {'title': 'Efficient Outdoor Scene Generation with Unified Vector Encoding', 'desc': 'This paper presents a novel approach to generating expansive outdoor scenes, addressing the unique challenges posed by varying heights and large landscapes. The authors introduce a method that encodes scene chunks as uniform vector sets, which enhances compression and performance compared to previous spatially structured latents. They also develop an explicit outpainting model that allows for unbounded scene generation, improving coherence and speeding up the process by reducing diffusion steps. Additionally, the creation of the NuiScene43 dataset enables the model to effectively blend different environmental styles, showcasing the benefits of joint training on diverse scenes.'}, 'zh': {'title': 'é«˜æ•ˆç”Ÿæˆå¹¿é˜”æˆ·å¤–åœºæ™¯çš„åˆ›æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æ¢è®¨äº†ç”Ÿæˆå¹¿é˜”æˆ·å¤–åœºæ™¯çš„ä»»åŠ¡ï¼ŒåŒ…æ‹¬åŸå ¡å’Œé«˜æ¥¼ç­‰ã€‚ä¸ä»¥å¾€ä¸»è¦å…³æ³¨çš„å®¤å†…åœºæ™¯ç”Ÿæˆä¸åŒï¼Œæˆ·å¤–åœºæ™¯ç”Ÿæˆé¢ä¸´ç‹¬ç‰¹çš„æŒ‘æˆ˜ï¼Œå¦‚åœºæ™¯é«˜åº¦çš„å¹¿æ³›å˜åŒ–å’Œå¿«é€Ÿç”Ÿæˆå¤§è§„æ¨¡æ™¯è§‚çš„æ–¹æ³•éœ€æ±‚ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„æ–¹æ³•ï¼Œå°†åœºæ™¯å—ç¼–ç ä¸ºç»Ÿä¸€çš„å‘é‡é›†ï¼Œè¿™æ¯”ä»¥å¾€æ–¹æ³•ä¸­ä½¿ç”¨çš„ç©ºé—´ç»“æ„æ½œå˜é‡æä¾›äº†æ›´å¥½çš„å‹ç¼©å’Œæ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªæ˜¾å¼çš„å¤–æ‰©æ¨¡å‹ï¼Œä»¥å®ç°æ— ç•Œç”Ÿæˆï¼Œæ”¹å–„äº†ä¸ä»¥å¾€åŸºäºé‡é‡‡æ ·çš„ä¿®è¡¥æ–¹æ¡ˆç›¸æ¯”çš„ä¸€è‡´æ€§ï¼ŒåŒæ—¶é€šè¿‡æ¶ˆé™¤é¢å¤–çš„æ‰©æ•£æ­¥éª¤åŠ å¿«äº†ç”Ÿæˆé€Ÿåº¦ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.16055', 'title': 'SALT: Singular Value Adaptation with Low-Rank Transformation', 'url': 'https://huggingface.co/papers/2503.16055', 'abstract': 'The complex nature of medical image segmentation calls for models that are specifically designed to capture detailed, domain-specific features. Large foundation models offer considerable flexibility, yet the cost of fine-tuning these models remains a significant barrier. Parameter-Efficient Fine-Tuning (PEFT) methods, such as Low-Rank Adaptation (LoRA), efficiently update model weights with low-rank matrices but may suffer from underfitting when the chosen rank is insufficient to capture domain-specific nuances. Conversely, full-rank Singular Value Decomposition (SVD) based methods provide comprehensive updates by modifying all singular values, yet they often lack flexibility and exhibit variable performance across datasets. We propose SALT (Singular Value Adaptation with Low-Rank Transformation), a method that selectively adapts the most influential singular values using trainable scale and shift parameters while complementing this with a low-rank update for the remaining subspace. This hybrid approach harnesses the advantages of both LoRA and SVD, enabling effective adaptation without relying on increasing model size or depth. Evaluated on 5 challenging medical datasets, ranging from as few as 20 samples to 1000, SALT outperforms state-of-the-art PEFT (LoRA and SVD) by 2% to 5% in Dice with only 3.9% trainable parameters, demonstrating robust adaptation even in low-resource settings. The code for SALT is available at: https://github.com/BioMedIA-MBZUAI/SALT', 'score': 8, 'issue_id': 2827, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 20', 'zh': '3æœˆ20æ—¥'}, 'hash': '1b4a367b51eed08e', 'authors': ['Abdelrahman Elsayed', 'Sarim Hashmi', 'Mohammed Elseiagy', 'Hu Wang', 'Mohammad Yaqub', 'Ibrahim Almakky'], 'affiliations': ['Mohamed bin Zayed University of Artificial Intelligence, Abu Dhabi, UAE'], 'pdf_title_img': 'assets/pdf/title_img/2503.16055.jpg', 'data': {'categories': ['#open_source', '#low_resource', '#healthcare', '#training', '#optimization'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'SALT: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ SALT Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. SALT ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² LoRA Ğ¸ SVD, Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒÑ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ğ»Ğ¸ÑÑ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑĞ¸Ğ½Ğ³ÑƒĞ»ÑÑ€Ğ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€Ğ°Ğ½Ğ³Ğ¾Ğ²Ğ¾Ğµ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¾ÑÑ‚Ğ°Ğ²ÑˆĞµĞ³Ğ¾ÑÑ Ğ¿Ğ¾Ğ´Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ» Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ½Ğ°Ğ´ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸-ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ½Ğ° 5 ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. SALT Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ» ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»Ñ Dice Ğ½Ğ° 2-5% Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ²ÑĞµĞ³Ğ¾ 3.9% Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ².'}, 'en': {'title': 'SALT: Smart Adaptation for Medical Image Segmentation', 'desc': 'This paper introduces SALT, a novel method for fine-tuning large foundation models specifically for medical image segmentation. SALT combines the strengths of Low-Rank Adaptation (LoRA) and full-rank Singular Value Decomposition (SVD) by selectively adapting the most important singular values while applying low-rank updates to the rest. This approach allows for effective model adaptation without significantly increasing the number of trainable parameters. The results show that SALT outperforms existing Parameter-Efficient Fine-Tuning methods on various medical datasets, demonstrating its effectiveness even with limited training data.'}, 'zh': {'title': 'SALTï¼šé«˜æ•ˆåŒ»å­¦å›¾åƒåˆ†å‰²çš„æ–°æ–¹æ³•', 'desc': 'åŒ»å­¦å›¾åƒåˆ†å‰²çš„å¤æ‚æ€§éœ€è¦ä¸“é—¨è®¾è®¡çš„æ¨¡å‹æ¥æ•æ‰è¯¦ç»†çš„é¢†åŸŸç‰¹å¾ã€‚è™½ç„¶å¤§å‹åŸºç¡€æ¨¡å‹æä¾›äº†çµæ´»æ€§ï¼Œä½†å¾®è°ƒè¿™äº›æ¨¡å‹çš„æˆæœ¬ä»ç„¶æ˜¯ä¸€ä¸ªé‡è¦éšœç¢ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºSALTçš„æ–¹æ³•ï¼Œå®ƒé€šè¿‡å¯è®­ç»ƒçš„ç¼©æ”¾å’Œåç§»å‚æ•°é€‰æ‹©æ€§åœ°é€‚åº”æœ€å…·å½±å“åŠ›çš„å¥‡å¼‚å€¼ï¼ŒåŒæ—¶å¯¹å…¶ä½™å­ç©ºé—´è¿›è¡Œä½ç§©æ›´æ–°ã€‚è¿™ç§æ··åˆæ–¹æ³•ç»“åˆäº†LoRAå’ŒSVDçš„ä¼˜ç‚¹ï¼Œåœ¨ä½èµ„æºç¯å¢ƒä¸­ä¹Ÿèƒ½å®ç°æœ‰æ•ˆçš„é€‚åº”ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.15851', 'title': 'Zero-1-to-A: Zero-Shot One Image to Animatable Head Avatars Using Video\n  Diffusion', 'url': 'https://huggingface.co/papers/2503.15851', 'abstract': 'Animatable head avatar generation typically requires extensive data for training. To reduce the data requirements, a natural solution is to leverage existing data-free static avatar generation methods, such as pre-trained diffusion models with score distillation sampling (SDS), which align avatars with pseudo ground-truth outputs from the diffusion model. However, directly distilling 4D avatars from video diffusion often leads to over-smooth results due to spatial and temporal inconsistencies in the generated video. To address this issue, we propose Zero-1-to-A, a robust method that synthesizes a spatial and temporal consistency dataset for 4D avatar reconstruction using the video diffusion model. Specifically, Zero-1-to-A iteratively constructs video datasets and optimizes animatable avatars in a progressive manner, ensuring that avatar quality increases smoothly and consistently throughout the learning process. This progressive learning involves two stages: (1) Spatial Consistency Learning fixes expressions and learns from front-to-side views, and (2) Temporal Consistency Learning fixes views and learns from relaxed to exaggerated expressions, generating 4D avatars in a simple-to-complex manner. Extensive experiments demonstrate that Zero-1-to-A improves fidelity, animation quality, and rendering speed compared to existing diffusion-based methods, providing a solution for lifelike avatar creation. Code is publicly available at: https://github.com/ZhenglinZhou/Zero-1-to-A.', 'score': 8, 'issue_id': 2826, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 20', 'zh': '3æœˆ20æ—¥'}, 'hash': '470193a07d663d04', 'authors': ['Zhou Zhenglin', 'Ma Fan', 'Fan Hehe', 'Chua Tat-Seng'], 'affiliations': ['National University of Singapore', 'ReLER, CCAI, Zhejiang University', 'State Key Laboratory of Brain-machine Intelligence, Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.15851.jpg', 'data': {'categories': ['#data', '#diffusion', '#dataset', '#synthetic', '#video', '#open_source'], 'emoji': 'ğŸ­', 'ru': {'title': 'Zero-1-to-A: Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… 3D-Ğ°Ğ²Ğ°Ñ‚Ğ°Ñ€Ğ¾Ğ² Ğ±ĞµĞ· Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Zero-1-to-A Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ°Ğ½Ğ¸Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… 3D-Ğ°Ğ²Ğ°Ñ‚Ğ°Ñ€Ğ¾Ğ² Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¿ÑĞµĞ²Ğ´Ğ¾-Ğ¸ÑÑ‚Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ½Ğ¾ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´Ğ²Ğ° ÑÑ‚Ğ°Ğ¿Ğ°: Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ (Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ñ€Ğ°ĞºÑƒÑ€ÑÑ‹) Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ (Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ€Ğ°ĞºÑƒÑ€ÑÑ‹, Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ). Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Zero-1-to-A ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ°Ğ²Ğ°Ñ‚Ğ°Ñ€Ğ¾Ğ² Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ° Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸.'}, 'en': {'title': 'Revolutionizing 4D Avatar Generation with Zero-1-to-A', 'desc': 'This paper presents Zero-1-to-A, a novel approach for generating animatable 4D avatars using video diffusion models while minimizing data requirements. The method addresses the challenge of over-smoothing in avatar generation by creating a dataset that ensures both spatial and temporal consistency. It employs a two-stage progressive learning process: first, it focuses on spatial consistency by learning from different views, and then it enhances temporal consistency by varying expressions. The results show that Zero-1-to-A significantly improves the quality and speed of avatar rendering compared to traditional methods.'}, 'zh': {'title': 'é«˜æ•ˆç”ŸæˆçœŸå®æ„Ÿ4Då¤´åƒçš„åˆ›æ–°æ–¹æ³•', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºZero-1-to-Açš„æ–¹æ³•ï¼Œç”¨äºç”Ÿæˆå¯åŠ¨ç”»çš„4Då¤´åƒï¼Œæ—¨åœ¨å‡å°‘å¯¹å¤§é‡è®­ç»ƒæ•°æ®çš„éœ€æ±‚ã€‚è¯¥æ–¹æ³•åˆ©ç”¨è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡æ„å»ºç©ºé—´å’Œæ—¶é—´ä¸€è‡´æ€§çš„æ•°æ®é›†ï¼Œé€æ­¥ä¼˜åŒ–å¤´åƒçš„è´¨é‡ã€‚Zero-1-to-Açš„å­¦ä¹ è¿‡ç¨‹åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šç©ºé—´ä¸€è‡´æ€§å­¦ä¹ å’Œæ—¶é—´ä¸€è‡´æ€§å­¦ä¹ ï¼Œç¡®ä¿å¤´åƒåœ¨å­¦ä¹ è¿‡ç¨‹ä¸­è´¨é‡å¹³æ»‘æå‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤´åƒçš„çœŸå®æ„Ÿã€åŠ¨ç”»è´¨é‡å’Œæ¸²æŸ“é€Ÿåº¦ä¸Šä¼˜äºç°æœ‰çš„æ‰©æ•£æ¨¡å‹æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.15242', 'title': 'BigO(Bench) -- Can LLMs Generate Code with Controlled Time and Space\n  Complexity?', 'url': 'https://huggingface.co/papers/2503.15242', 'abstract': 'We introduce BigO(Bench), a novel coding benchmark designed to evaluate the capabilities of generative language models in understanding and generating code with specified time and space complexities. This benchmark addresses the gap in current evaluations that often overlook the ability of models to comprehend and produce code constrained by computational complexity. BigO(Bench) includes tooling to infer the algorithmic complexity of any Python function from profiling measurements, including human- or LLM-generated solutions. BigO(Bench) also includes of set of 3,105 coding problems and 1,190,250 solutions from Code Contests annotated with inferred (synthetic) time and space complexity labels from the complexity framework, as well as corresponding runtime and memory footprint values for a large set of input sizes. We present results from evaluating multiple state-of-the-art language models on this benchmark, highlighting their strengths and weaknesses in handling complexity requirements. In particular, token-space reasoning models are unrivaled in code generation but not in complexity understanding, hinting that they may not generalize well to tasks for which no reward was given at training time.', 'score': 7, 'issue_id': 2831, 'pub_date': '2025-03-19', 'pub_date_card': {'ru': '19 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 19', 'zh': '3æœˆ19æ—¥'}, 'hash': 'bac5c66c1f05b167', 'authors': ['Pierre Chambon', 'Baptiste Roziere', 'Benoit Sagot', 'Gabriel Synnaeve'], 'affiliations': ['FAIR at Meta', 'Inria', 'Mistral AI'], 'pdf_title_img': 'assets/pdf/title_img/2503.15242.jpg', 'data': {'categories': ['#synthetic', '#benchmark', '#dataset', '#plp', '#reasoning'], 'emoji': 'â±ï¸', 'ru': {'title': 'BigO(Bench): ĞÑ†ĞµĞ½ĞºĞ° Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸', 'desc': 'BigO(Bench) - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ benchmark Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ´ Ñ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Python-Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¸Ğ· 3,105 Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ 1,190,250 Ñ€ĞµÑˆĞµĞ½Ğ¸ÑĞ¼Ğ¸, Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚ĞºĞ°Ğ¼Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° ÑÑ‚Ğ¾Ğ¼ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¸Ñ… ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ ÑĞ»Ğ°Ğ±Ñ‹Ğµ ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñ‹ Ğ² Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸ Ğº ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸. ĞœĞ¾Ğ´ĞµĞ»Ğ¸, Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°ÑÑ‰Ğ¸Ğµ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ»Ğ¸Ğ´Ğ¸Ñ€ÑƒÑÑ‚ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ°, Ğ½Ğ¾ Ğ½Ğµ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Evaluating Code Complexity with BigO(Bench)', 'desc': "BigO(Bench) is a new benchmark created to test how well generative language models can understand and generate code while considering time and space complexities. It fills a gap in existing evaluations by focusing on the models' ability to handle computational constraints. The benchmark includes tools to analyze the algorithmic complexity of Python functions and features a large dataset of coding problems and solutions with annotated complexity labels. Results show that while some models excel at generating code, they struggle with understanding complexity, indicating potential limitations in their generalization capabilities."}, 'zh': {'title': 'è¯„ä¼°ç”Ÿæˆæ¨¡å‹çš„å¤æ‚åº¦ç†è§£èƒ½åŠ›', 'desc': 'æˆ‘ä»¬ä»‹ç»äº†BigO(Bench)ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„ç¼–ç åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°ç”Ÿæˆè¯­è¨€æ¨¡å‹åœ¨ç†è§£å’Œç”Ÿæˆå…·æœ‰ç‰¹å®šæ—¶é—´å’Œç©ºé—´å¤æ‚åº¦çš„ä»£ç æ–¹é¢çš„èƒ½åŠ›ã€‚è¯¥åŸºå‡†å¡«è¡¥äº†å½“å‰è¯„ä¼°ä¸­å¸¸å¸¸å¿½è§†æ¨¡å‹åœ¨è®¡ç®—å¤æ‚åº¦é™åˆ¶ä¸‹ç†è§£å’Œç”Ÿæˆä»£ç çš„èƒ½åŠ›çš„ç©ºç™½ã€‚BigO(Bench)åŒ…æ‹¬å·¥å…·ï¼Œå¯ä»¥ä»æ€§èƒ½æµ‹é‡ä¸­æ¨æ–­ä»»ä½•Pythonå‡½æ•°çš„ç®—æ³•å¤æ‚åº¦ï¼Œå¹¶åŒ…å«æ¥è‡ªä»£ç ç«èµ›çš„3,105ä¸ªç¼–ç é—®é¢˜å’Œ1,190,250ä¸ªè§£å†³æ–¹æ¡ˆï¼Œæ ‡æ³¨äº†æ¨æ–­çš„æ—¶é—´å’Œç©ºé—´å¤æ‚åº¦æ ‡ç­¾ã€‚æˆ‘ä»¬å±•ç¤ºäº†å¯¹å¤šä¸ªæœ€å…ˆè¿›è¯­è¨€æ¨¡å‹åœ¨è¯¥åŸºå‡†ä¸Šçš„è¯„ä¼°ç»“æœï¼Œçªå‡ºäº†å®ƒä»¬åœ¨å¤„ç†å¤æ‚æ€§è¦æ±‚æ–¹é¢çš„ä¼˜ç¼ºç‚¹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.13356', 'title': 'Agents Play Thousands of 3D Video Games', 'url': 'https://huggingface.co/papers/2503.13356', 'abstract': "We present PORTAL, a novel framework for developing artificial intelligence agents capable of playing thousands of 3D video games through language-guided policy generation. By transforming decision-making problems into language modeling tasks, our approach leverages large language models (LLMs) to generate behavior trees represented in domain-specific language (DSL). This method eliminates the computational burden associated with traditional reinforcement learning approaches while preserving strategic depth and rapid adaptability. Our framework introduces a hybrid policy structure that combines rule-based nodes with neural network components, enabling both high-level strategic reasoning and precise low-level control. A dual-feedback mechanism incorporating quantitative game metrics and vision-language model analysis facilitates iterative policy improvement at both tactical and strategic levels. The resulting policies are instantaneously deployable, human-interpretable, and capable of generalizing across diverse gaming environments. Experimental results demonstrate PORTAL's effectiveness across thousands of first-person shooter (FPS) games, showcasing significant improvements in development efficiency, policy generalization, and behavior diversity compared to traditional approaches. PORTAL represents a significant advancement in game AI development, offering a practical solution for creating sophisticated agents that can operate across thousands of commercial video games with minimal development overhead. Experiment results on the 3D video games are best viewed on https://zhongwen.one/projects/portal .", 'score': 7, 'issue_id': 2835, 'pub_date': '2025-03-17', 'pub_date_card': {'ru': '17 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 17', 'zh': '3æœˆ17æ—¥'}, 'hash': '950044773aa39963', 'authors': ['Zhongwen Xu', 'Xianliang Wang', 'Siyi Li', 'Tao Yu', 'Liang Wang', 'Qiang Fu', 'Wei Yang'], 'affiliations': ['Tencent'], 'pdf_title_img': 'assets/pdf/title_img/2503.13356.jpg', 'data': {'categories': ['#rl', '#interpretability', '#video', '#3d', '#games', '#reasoning', '#agi', '#agents'], 'emoji': 'ğŸ®', 'ru': {'title': 'Ğ¯Ğ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ĞºĞ¾Ñ€ÑÑÑ‚ Ğ¼Ğ¸Ñ€ 3D-Ğ¸Ğ³Ñ€', 'desc': 'PORTAL - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… Ğ¸Ğ³Ñ€Ğ°Ñ‚ÑŒ Ğ² Ñ‚Ñ‹ÑÑÑ‡Ğ¸ 3D-Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¸Ğ³Ñ€ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ·Ñ‹ĞºĞ°. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ´ĞµÑ€ĞµĞ²ÑŒĞµĞ² Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ, Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ½Ğ¾-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰ÑƒÑ ÑƒĞ·Ğ»Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ» Ñ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹. PORTAL Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ² ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸, Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğ¸ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'PORTAL: Revolutionizing Game AI with Language-Guided Policies', 'desc': 'PORTAL is a new framework that enables AI agents to play many 3D video games by using language to guide their decision-making. It turns complex decision problems into tasks that large language models can handle, generating behavior trees in a specific language. This approach reduces the heavy computational needs of traditional reinforcement learning while maintaining strategic depth and adaptability. By combining rule-based and neural network elements, PORTAL allows for both high-level strategy and detailed control, making it easier to create versatile game-playing agents.'}, 'zh': {'title': 'PORTALï¼šæ¸¸æˆAIçš„æ–°çºªå…ƒ', 'desc': 'PORTALæ˜¯ä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œæ—¨åœ¨å¼€å‘èƒ½å¤Ÿé€šè¿‡è¯­è¨€å¼•å¯¼ç­–ç•¥ç”Ÿæˆæ¥ç©æ•°åƒæ¬¾3Dè§†é¢‘æ¸¸æˆçš„äººå·¥æ™ºèƒ½ä»£ç†ã€‚è¯¥æ–¹æ³•å°†å†³ç­–é—®é¢˜è½¬åŒ–ä¸ºè¯­è¨€å»ºæ¨¡ä»»åŠ¡ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç”Ÿæˆä»¥é¢†åŸŸç‰¹å®šè¯­è¨€ï¼ˆDSLï¼‰è¡¨ç¤ºçš„è¡Œä¸ºæ ‘ã€‚é€šè¿‡ç»“åˆåŸºäºè§„åˆ™çš„èŠ‚ç‚¹å’Œç¥ç»ç½‘ç»œç»„ä»¶ï¼ŒPORTALå®ç°äº†é«˜å±‚æ¬¡çš„æˆ˜ç•¥æ¨ç†å’Œç²¾ç¡®çš„ä½å±‚æ¬¡æ§åˆ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPORTALåœ¨æ•°åƒæ¬¾ç¬¬ä¸€äººç§°å°„å‡»æ¸¸æˆä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„å¼€å‘æ•ˆç‡æå‡ã€ç­–ç•¥æ³›åŒ–å’Œè¡Œä¸ºå¤šæ ·æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.16188', 'title': 'CLS-RL: Image Classification with Rule-Based Reinforcement Learning', 'url': 'https://huggingface.co/papers/2503.16188', 'abstract': "Classification is a core task in machine learning. Recent research has shown that although Multimodal Large Language Models (MLLMs) are initially poor at image classification, fine-tuning them with an adequate amount of data can significantly enhance their performance, making them comparable to SOTA classification models. However, acquiring large-scale labeled data is expensive. In this paper, we explore few-shot MLLM classification fine-tuning. We found that SFT can cause severe overfitting issues and may even degrade performance over the zero-shot approach. To address this challenge, inspired by the recent successes in rule-based reinforcement learning, we propose CLS-RL, which uses verifiable signals as reward to fine-tune MLLMs. We discovered that CLS-RL outperforms SFT in most datasets and has a much higher average accuracy on both base-to-new and few-shot learning setting. Moreover, we observed a free-lunch phenomenon for CLS-RL; when models are fine-tuned on a particular dataset, their performance on other distinct datasets may also improve over zero-shot models, even if those datasets differ in distribution and class names. This suggests that RL-based methods effectively teach models the fundamentals of classification. Lastly, inspired by recent works in inference time thinking, we re-examine the `thinking process' during fine-tuning, a critical aspect of RL-based methods, in the context of visual classification. We question whether such tasks require extensive thinking process during fine-tuning, proposing that this may actually detract from performance. Based on this premise, we introduce the No-Thinking-CLS-RL method, which minimizes thinking processes during training by setting an equality accuracy reward. Our findings indicate that, with much less fine-tuning time, No-Thinking-CLS-RL method achieves superior in-domain performance and generalization capabilities than CLS-RL.", 'score': 6, 'issue_id': 2826, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 20', 'zh': '3æœˆ20æ—¥'}, 'hash': '904362034cdf6d16', 'authors': ['Ming Li', 'Shitian Zhao', 'Jike Zhong', 'Yuxiang Lai', 'Kaipeng Zhang'], 'affiliations': ['Emory University', 'Shanghai AI Laboratory', 'University of Southern California'], 'pdf_title_img': 'assets/pdf/title_img/2503.16188.jpg', 'data': {'categories': ['#rl', '#transfer_learning', '#optimization', '#cv', '#multimodal', '#training'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ñ‹ Ğ² ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜', 'desc': "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ (MLLM) Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ¼Ğ°Ğ»Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ CLS-RL, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ´Ğ»Ñ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ MLLM. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ CLS-RL Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğµ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ No-Thinking-CLS-RL, Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ 'Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ' Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ»ÑƒÑ‡ÑˆĞ¸Ğ¼ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°Ğ¼ Ğ¿Ñ€Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞµĞ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."}, 'en': {'title': 'Reinforcement Learning Revolutionizes Few-Shot Image Classification', 'desc': 'This paper investigates the fine-tuning of Multimodal Large Language Models (MLLMs) for image classification, particularly in few-shot scenarios. It highlights the limitations of standard supervised fine-tuning (SFT), which can lead to overfitting and reduced performance compared to zero-shot methods. To overcome these challenges, the authors propose a novel approach called CLS-RL, which utilizes reinforcement learning with verifiable signals as rewards, resulting in improved accuracy across various datasets. Additionally, they introduce the No-Thinking-CLS-RL method, which minimizes unnecessary cognitive processes during training, further enhancing performance and generalization with less fine-tuning time.'}, 'zh': {'title': 'å°‘æ ·æœ¬å¾®è°ƒï¼Œæå‡åˆ†ç±»æ€§èƒ½çš„åˆ›æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æ¢è®¨äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚å°½ç®¡è¿™äº›æ¨¡å‹åœ¨åˆå§‹é˜¶æ®µçš„åˆ†ç±»æ€§èƒ½è¾ƒå·®ï¼Œä½†é€šè¿‡å°‘é‡æ ·æœ¬çš„å¾®è°ƒï¼Œå¯ä»¥æ˜¾è‘—æå‡å…¶è¡¨ç°ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•CLS-RLï¼Œåˆ©ç”¨å¯éªŒè¯ä¿¡å·ä½œä¸ºå¥–åŠ±æ¥å¾®è°ƒMLLMsï¼Œç»“æœæ˜¾ç¤ºå…¶åœ¨å¤§å¤šæ•°æ•°æ®é›†ä¸Šä¼˜äºä¼ ç»Ÿçš„å¾®è°ƒæ–¹æ³•ã€‚æœ€åï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†No-Thinking-CLS-RLæ–¹æ³•ï¼Œé€šè¿‡å‡å°‘å¾®è°ƒè¿‡ç¨‹ä¸­çš„æ€è€ƒæ—¶é—´ï¼Œè¿›ä¸€æ­¥æé«˜äº†æ¨¡å‹çš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.15567', 'title': 'Towards Unified Latent Space for 3D Molecular Latent Diffusion Modeling', 'url': 'https://huggingface.co/papers/2503.15567', 'abstract': '3D molecule generation is crucial for drug discovery and material science, requiring models to process complex multi-modalities, including atom types, chemical bonds, and 3D coordinates. A key challenge is integrating these modalities of different shapes while maintaining SE(3) equivariance for 3D coordinates. To achieve this, existing approaches typically maintain separate latent spaces for invariant and equivariant modalities, reducing efficiency in both training and sampling. In this work, we propose Unified Variational Auto-Encoder for 3D Molecular Latent Diffusion Modeling (UAE-3D), a multi-modal VAE that compresses 3D molecules into latent sequences from a unified latent space, while maintaining near-zero reconstruction error. This unified latent space eliminates the complexities of handling multi-modality and equivariance when performing latent diffusion modeling. We demonstrate this by employing the Diffusion Transformer--a general-purpose diffusion model without any molecular inductive bias--for latent generation. Extensive experiments on GEOM-Drugs and QM9 datasets demonstrate that our method significantly establishes new benchmarks in both de novo and conditional 3D molecule generation, achieving leading efficiency and quality.', 'score': 6, 'issue_id': 2822, 'pub_date': '2025-03-19', 'pub_date_card': {'ru': '19 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 19', 'zh': '3æœˆ19æ—¥'}, 'hash': 'd1787a1a75f723a2', 'authors': ['Yanchen Luo', 'Zhiyuan Liu', 'Yi Zhao', 'Sihang Li', 'Kenji Kawaguchi', 'Tat-Seng Chua', 'Xiang Wang'], 'affiliations': ['National University of Singapore', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2503.15567.jpg', 'data': {'categories': ['#multimodal', '#diffusion', '#benchmark', '#3d', '#optimization', '#dataset'], 'emoji': 'ğŸ§ª', 'ru': {'title': 'Ğ•Ğ´Ğ¸Ğ½Ğ¾Ğµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ»', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ» Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ»ĞµĞºĞ°Ñ€ÑÑ‚Ğ² Ğ¸ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ (UAE-3D) Ğ´Ğ»Ñ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ 3D Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ» Ğ² Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ· ĞµĞ´Ğ¸Ğ½Ğ¾Ğ³Ğ¾ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞºĞ²Ğ¸Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… GEOM-Drugs Ğ¸ QM9 Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… de novo Ğ¸ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ».'}, 'en': {'title': 'Unified Latent Space for Efficient 3D Molecule Generation', 'desc': 'This paper presents a new approach called Unified Variational Auto-Encoder for 3D Molecular Latent Diffusion Modeling (UAE-3D) aimed at generating 3D molecules for drug discovery and material science. The method addresses the challenge of integrating different types of data, such as atom types and chemical bonds, while ensuring that the 3D coordinates maintain SE(3) equivariance. By using a unified latent space, UAE-3D simplifies the process of handling multi-modalities and improves the efficiency of training and sampling. The results show that this approach outperforms existing methods in generating high-quality 3D molecules, setting new benchmarks in the field.'}, 'zh': {'title': 'ç»Ÿä¸€æ½œåœ¨ç©ºé—´ï¼Œæå‡3Dåˆ†å­ç”Ÿæˆæ•ˆç‡', 'desc': '3Dåˆ†å­ç”Ÿæˆå¯¹è¯ç‰©å‘ç°å’Œææ–™ç§‘å­¦è‡³å…³é‡è¦ï¼Œéœ€è¦æ¨¡å‹å¤„ç†å¤æ‚çš„å¤šæ¨¡æ€ä¿¡æ¯ï¼ŒåŒ…æ‹¬åŸå­ç±»å‹ã€åŒ–å­¦é”®å’Œ3Dåæ ‡ã€‚ä¸€ä¸ªä¸»è¦æŒ‘æˆ˜æ˜¯å¦‚ä½•åœ¨ä¿æŒSE(3)ç­‰å˜æ€§çš„åŒæ—¶ï¼Œæ•´åˆä¸åŒå½¢çŠ¶çš„æ¨¡æ€ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆUAE-3Dï¼‰ï¼Œå®ƒå°†3Dåˆ†å­å‹ç¼©ä¸ºæ¥è‡ªç»Ÿä¸€æ½œåœ¨ç©ºé—´çš„æ½œåœ¨åºåˆ—ï¼ŒåŒæ—¶ä¿æŒè¿‘ä¹é›¶çš„é‡å»ºè¯¯å·®ã€‚é€šè¿‡ä½¿ç”¨Diffusion Transformerï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨GEOM-Drugså’ŒQM9æ•°æ®é›†ä¸Šæ˜¾è‘—æé«˜äº†æ–°åˆ†å­ç”Ÿæˆçš„æ•ˆç‡å’Œè´¨é‡ï¼Œå»ºç«‹äº†æ–°çš„åŸºå‡†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.16429', 'title': 'Sonata: Self-Supervised Learning of Reliable Point Representations', 'url': 'https://huggingface.co/papers/2503.16429', 'abstract': 'In this paper, we question whether we have a reliable self-supervised point cloud model that can be used for diverse 3D tasks via simple linear probing, even with limited data and minimal computation. We find that existing 3D self-supervised learning approaches fall short when evaluated on representation quality through linear probing. We hypothesize that this is due to what we term the "geometric shortcut", which causes representations to collapse to low-level spatial features. This challenge is unique to 3D and arises from the sparse nature of point cloud data. We address it through two key strategies: obscuring spatial information and enhancing the reliance on input features, ultimately composing a Sonata of 140k point clouds through self-distillation. Sonata is simple and intuitive, yet its learned representations are strong and reliable: zero-shot visualizations demonstrate semantic grouping, alongside strong spatial reasoning through nearest-neighbor relationships. Sonata demonstrates exceptional parameter and data efficiency, tripling linear probing accuracy (from 21.8% to 72.5%) on ScanNet and nearly doubling performance with only 1% of the data compared to previous approaches. Full fine-tuning further advances SOTA across both 3D indoor and outdoor perception tasks.', 'score': 5, 'issue_id': 2833, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 20', 'zh': '3æœˆ20æ—¥'}, 'hash': 'cab3efeaf1688094', 'authors': ['Xiaoyang Wu', 'Daniel DeTone', 'Duncan Frost', 'Tianwei Shen', 'Chris Xie', 'Nan Yang', 'Jakob Engel', 'Richard Newcombe', 'Hengshuang Zhao', 'Julian Straub'], 'affiliations': ['Meta Reality Labs Research', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2503.16429.jpg', 'data': {'categories': ['#3d', '#optimization', '#transfer_learning'], 'emoji': 'ğŸŒ', 'ru': {'title': 'Sonata: Ğ½Ğ¾Ğ²Ğ°Ñ ÑÑ€Ğ° ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ 3D Ğ¾Ğ±Ğ»Ğ°ĞºĞ¾Ğ² Ñ‚Ğ¾Ñ‡ĞµĞº', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ¾Ğ±Ğ»Ğ°ĞºĞ°Ğ¼Ğ¸ Ñ‚Ğ¾Ñ‡ĞµĞº Ğ² 3D, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğº ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² 3D Ğ½Ğµ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ·-Ğ·Ğ° Ñ‚Ğ°Ğº Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ğ¾Ğ³Ğ¾ "Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ". Ğ­Ñ‚Ğ¾ ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ ÑĞ²ÑĞ·Ğ°Ğ½Ğ¾ Ñ Ñ‚ĞµĞ¼, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞ²Ğ¾Ğ´ÑÑ‚ÑÑ Ğº Ğ½Ğ¸Ğ·ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ°Ğ¼ Ğ¸Ğ·-Ğ·Ğ° Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ĞºĞ¾Ğ² Ñ‚Ğ¾Ñ‡ĞµĞº. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Sonata, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ğ¾Ğ½Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': "Unlocking 3D Potential: Sonata's Self-Supervised Breakthrough", 'desc': "This paper investigates the effectiveness of self-supervised learning models for point clouds in 3D tasks, particularly focusing on their performance with limited data. The authors identify a problem called the 'geometric shortcut', which leads to poor representation quality due to reliance on low-level spatial features. To overcome this, they propose a method called Sonata, which obscures spatial information and emphasizes input features, resulting in improved representation learning. Sonata achieves significant gains in linear probing accuracy and demonstrates strong performance in both zero-shot visualizations and full fine-tuning across various 3D perception tasks."}, 'zh': {'title': 'Sonataï¼šé«˜æ•ˆçš„è‡ªç›‘ç£3Dç‚¹äº‘å­¦ä¹ ', 'desc': 'æœ¬æ–‡è´¨ç–‘ç°æœ‰çš„è‡ªç›‘ç£ç‚¹äº‘æ¨¡å‹åœ¨æœ‰é™æ•°æ®å’Œæœ€å°è®¡ç®—ä¸‹æ˜¯å¦å¯é ï¼Œèƒ½å¦ç”¨äºå¤šæ ·çš„3Dä»»åŠ¡ã€‚æˆ‘ä»¬å‘ç°ç°æœ‰çš„3Dè‡ªç›‘ç£å­¦ä¹ æ–¹æ³•åœ¨é€šè¿‡çº¿æ€§æ¢æµ‹è¯„ä¼°è¡¨ç¤ºè´¨é‡æ—¶è¡¨ç°ä¸ä½³ï¼ŒåŸå› åœ¨äºæ‰€è°“çš„â€œå‡ ä½•æ·å¾„â€ï¼Œå¯¼è‡´è¡¨ç¤ºé€€åŒ–ä¸ºä½çº§ç©ºé—´ç‰¹å¾ã€‚æˆ‘ä»¬æå‡ºäº†ä¸¤ç§å…³é”®ç­–ç•¥æ¥åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼šé®è”½ç©ºé—´ä¿¡æ¯å’Œå¢å¼ºå¯¹è¾“å…¥ç‰¹å¾çš„ä¾èµ–ï¼Œæœ€ç»ˆé€šè¿‡è‡ªè’¸é¦æ„å»ºäº†ä¸€ä¸ªåŒ…å«14ä¸‡ä¸ªç‚¹äº‘çš„Sonataæ¨¡å‹ã€‚Sonataåœ¨å‚æ•°å’Œæ•°æ®æ•ˆç‡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œåœ¨çº¿æ€§æ¢æµ‹å‡†ç¡®ç‡ä¸Šæå‡äº†ä¸‰å€ï¼Œå¹¶åœ¨å…¨å¾®è°ƒä¸­è¿›ä¸€æ­¥æ¨åŠ¨äº†3Dæ„ŸçŸ¥ä»»åŠ¡çš„æœ€æ–°è¿›å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.16278', 'title': 'Uni-3DAR: Unified 3D Generation and Understanding via Autoregression on\n  Compressed Spatial Tokens', 'url': 'https://huggingface.co/papers/2503.16278', 'abstract': 'Recent advancements in large language models and their multi-modal extensions have demonstrated the effectiveness of unifying generation and understanding through autoregressive next-token prediction. However, despite the critical role of 3D structural generation and understanding ({3D GU}) in AI for science, these tasks have largely evolved independently, with autoregressive methods remaining underexplored. To bridge this gap, we introduce Uni-3DAR, a unified framework that seamlessly integrates {3D GU} tasks via autoregressive prediction. At its core, Uni-3DAR employs a novel hierarchical tokenization that compresses 3D space using an octree, leveraging the inherent sparsity of 3D structures. It then applies an additional tokenization for fine-grained structural details, capturing key attributes such as atom types and precise spatial coordinates in microscopic 3D structures. We further propose two optimizations to enhance efficiency and effectiveness. The first is a two-level subtree compression strategy, which reduces the octree token sequence by up to 8x. The second is a masked next-token prediction mechanism tailored for dynamically varying token positions, significantly boosting model performance. By combining these strategies, Uni-3DAR successfully unifies diverse {3D GU} tasks within a single autoregressive framework. Extensive experiments across multiple microscopic {3D GU} tasks, including molecules, proteins, polymers, and crystals, validate its effectiveness and versatility. Notably, Uni-3DAR surpasses previous state-of-the-art diffusion models by a substantial margin, achieving up to 256\\% relative improvement while delivering inference speeds up to 21.8x faster. The code is publicly available at https://github.com/dptech-corp/Uni-3DAR.', 'score': 5, 'issue_id': 2822, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 20', 'zh': '3æœˆ20æ—¥'}, 'hash': '7012b17f03aeb180', 'authors': ['Shuqi Lu', 'Haowei Lin', 'Lin Yao', 'Zhifeng Gao', 'Xiaohong Ji', 'Weinan E', 'Linfeng Zhang', 'Guolin Ke'], 'affiliations': ['AI for Science Institute, Beijing 100080, China', 'Center for Machine Learning Research, Peking University, Beijing 100084, China', 'DP Technology, Beijing, 100080, China', 'Institute for Artificial Intelligence, Peking University, Beijing 100871, China', 'School of Mathematical Sciences, Peking University, Beijing, 100871, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.16278.jpg', 'data': {'categories': ['#multimodal', '#science', '#training', '#architecture', '#3d', '#optimization', '#inference'], 'emoji': 'ğŸ§¬', 'ru': {'title': 'Uni-3DAR: Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ 3D Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Uni-3DAR - ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ 3D Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒÑ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¾ĞºÑ‚Ğ¾Ğ´ĞµÑ€ĞµĞ²Ğ° Ğ´Ğ»Ñ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ 3D Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ñ‹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸: Ğ´Ğ²ÑƒÑ…ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€ĞµĞ²ÑŒĞµĞ² Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Uni-3DAR Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¸ĞºÑ€Ğ¾ÑĞºĞ¾Ğ¿Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… 3D Ğ·Ğ°Ğ´Ğ°Ñ‡.'}, 'en': {'title': 'Unifying 3D Generation and Understanding with Uni-3DAR', 'desc': 'This paper presents Uni-3DAR, a novel framework that integrates 3D structural generation and understanding tasks using autoregressive next-token prediction. It introduces a hierarchical tokenization method that efficiently compresses 3D space with an octree, capturing both the overall structure and fine details like atom types and spatial coordinates. The framework includes optimizations such as a two-level subtree compression strategy and a masked next-token prediction mechanism, enhancing both efficiency and model performance. Experimental results show that Uni-3DAR significantly outperforms existing models, achieving faster inference speeds and improved accuracy across various microscopic 3D tasks.'}, 'zh': {'title': 'ç»Ÿä¸€3Dç»“æ„ç”Ÿæˆä¸ç†è§£çš„åˆ›æ–°æ¡†æ¶', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºUni-3DARçš„ç»Ÿä¸€æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡è‡ªå›å½’é¢„æµ‹æ•´åˆ3Dç»“æ„ç”Ÿæˆä¸ç†è§£ï¼ˆ3D GUï¼‰ä»»åŠ¡ã€‚è¯¥æ¡†æ¶é‡‡ç”¨æ–°é¢–çš„åˆ†å±‚æ ‡è®°åŒ–æ–¹æ³•ï¼Œåˆ©ç”¨å…«å‰æ ‘å‹ç¼©3Dç©ºé—´ï¼Œå¹¶ä¸ºå¾®è§‚3Dç»“æ„æ•æ‰å…³é”®å±æ€§å¦‚åŸå­ç±»å‹å’Œç²¾ç¡®åæ ‡ã€‚Uni-3DARè¿˜æå‡ºäº†ä¸¤é¡¹ä¼˜åŒ–ç­–ç•¥ï¼Œä»¥æé«˜æ•ˆç‡å’Œæ•ˆæœï¼ŒåŒ…æ‹¬ä¸¤çº§å­æ ‘å‹ç¼©å’ŒåŠ¨æ€å˜åŒ–æ ‡è®°ä½ç½®çš„æ©ç ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹æœºåˆ¶ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒï¼ŒUni-3DARåœ¨å¤šç§å¾®è§‚3D GUä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—è¶…è¶Šäº†ä¹‹å‰çš„æ‰©æ•£æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.15451', 'title': 'MotionStreamer: Streaming Motion Generation via Diffusion-based\n  Autoregressive Model in Causal Latent Space', 'url': 'https://huggingface.co/papers/2503.15451', 'abstract': 'This paper addresses the challenge of text-conditioned streaming motion generation, which requires us to predict the next-step human pose based on variable-length historical motions and incoming texts. Existing methods struggle to achieve streaming motion generation, e.g., diffusion models are constrained by pre-defined motion lengths, while GPT-based methods suffer from delayed response and error accumulation problem due to discretized non-causal tokenization. To solve these problems, we propose MotionStreamer, a novel framework that incorporates a continuous causal latent space into a probabilistic autoregressive model. The continuous latents mitigate information loss caused by discretization and effectively reduce error accumulation during long-term autoregressive generation. In addition, by establishing temporal causal dependencies between current and historical motion latents, our model fully utilizes the available information to achieve accurate online motion decoding. Experiments show that our method outperforms existing approaches while offering more applications, including multi-round generation, long-term generation, and dynamic motion composition. Project Page: https://zju3dv.github.io/MotionStreamer/', 'score': 5, 'issue_id': 2827, 'pub_date': '2025-03-19', 'pub_date_card': {'ru': '19 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 19', 'zh': '3æœˆ19æ—¥'}, 'hash': '014f7568fd194ca1', 'authors': ['Lixing Xiao', 'Shunlin Lu', 'Huaijin Pi', 'Ke Fan', 'Liang Pan', 'Yueer Zhou', 'Ziyong Feng', 'Xiaowei Zhou', 'Sida Peng', 'Jingbo Wang'], 'affiliations': ['DeepGlint', 'Shanghai AI Laboratory', 'Shanghai Jiao Tong University', 'The Chinese University of Hong Kong, Shenzhen', 'The University of Hong Kong', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.15451.jpg', 'data': {'categories': ['#multimodal', '#architecture', '#games', '#video', '#long_context'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞŸĞ»Ğ°Ğ²Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MotionStreamer - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ Ğ² Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², MotionStreamer Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ² Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ½Ğ¾Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ½Ğ¸Ğ·Ğ¸Ñ‚ÑŒ Ğ¿Ğ¾Ñ‚ĞµÑ€Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·-Ğ·Ğ° Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ¸Ñ‚ÑŒ Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¿Ñ€Ğ¸ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ¸ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'Real-Time Motion Generation with Text Input', 'desc': 'This paper introduces MotionStreamer, a new framework for generating human motion in real-time based on text inputs. It addresses limitations of previous methods, such as diffusion models and GPT-based approaches, which struggle with fixed motion lengths and error accumulation. By using a continuous causal latent space, MotionStreamer reduces information loss and improves the accuracy of long-term motion generation. The framework also leverages temporal dependencies to enhance online motion decoding, demonstrating superior performance in various applications.'}, 'zh': {'title': 'æµå¼è¿åŠ¨ç”Ÿæˆçš„æ–°çªç ´ï¼šMotionStreamer', 'desc': 'æœ¬æ–‡è§£å†³äº†åŸºäºæ–‡æœ¬çš„æµå¼è¿åŠ¨ç”Ÿæˆé—®é¢˜ï¼Œæ—¨åœ¨æ ¹æ®å¯å˜é•¿åº¦çš„å†å²åŠ¨ä½œå’Œè¾“å…¥æ–‡æœ¬é¢„æµ‹ä¸‹ä¸€æ­¥çš„äººä½“å§¿æ€ã€‚ç°æœ‰æ–¹æ³•åœ¨æµå¼è¿åŠ¨ç”Ÿæˆæ–¹é¢å­˜åœ¨å›°éš¾ï¼Œä¾‹å¦‚æ‰©æ•£æ¨¡å‹å—åˆ°é¢„å®šä¹‰è¿åŠ¨é•¿åº¦çš„é™åˆ¶ï¼Œè€ŒåŸºäºGPTçš„æ–¹æ³•ç”±äºç¦»æ•£åŒ–çš„éå› æœæ ‡è®°åŒ–è€Œé¢ä¸´å“åº”å»¶è¿Ÿå’Œé”™è¯¯ç´¯ç§¯çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†MotionStreamerï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œå°†è¿ç»­å› æœæ½œåœ¨ç©ºé—´èå…¥æ¦‚ç‡è‡ªå›å½’æ¨¡å‹ä¸­ã€‚é€šè¿‡å»ºç«‹å½“å‰å’Œå†å²è¿åŠ¨æ½œåœ¨ä¹‹é—´çš„æ—¶é—´å› æœä¾èµ–å…³ç³»ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å……åˆ†åˆ©ç”¨å¯ç”¨ä¿¡æ¯ï¼Œå®ç°å‡†ç¡®çš„åœ¨çº¿è¿åŠ¨è§£ç ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.14237', 'title': 'Make Your Training Flexible: Towards Deployment-Efficient Video Models', 'url': 'https://huggingface.co/papers/2503.14237', 'abstract': 'Popular video training methods mainly operate on a fixed number of tokens sampled from a predetermined spatiotemporal grid, resulting in sub-optimal accuracy-computation trade-offs due to inherent video redundancy. They also lack adaptability to varying computational budgets for downstream tasks, hindering applications of the most competitive model in real-world scenes. We thus propose a new test setting, Token Optimization, for maximized input information across budgets, which optimizes the size-limited set of input tokens through token selection from more suitably sampled videos. To this end, we propose a novel augmentation tool termed Flux. By making the sampling grid flexible and leveraging token selection, it is easily adopted in most popular video training frameworks, boosting model robustness with nearly no additional cost. We integrate Flux in large-scale video pre-training, and the resulting FluxViT establishes new state-of-the-art results across extensive tasks at standard costs. Notably, with 1/4 tokens only, it can still match the performance of previous state-of-the-art models with Token Optimization, yielding nearly 90\\% savings. All models and data are available at https://github.com/OpenGVLab/FluxViT.', 'score': 5, 'issue_id': 2829, 'pub_date': '2025-03-18', 'pub_date_card': {'ru': '18 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 18', 'zh': '3æœˆ18æ—¥'}, 'hash': 'bae6f023e68eb914', 'authors': ['Chenting Wang', 'Kunchang Li', 'Tianxiang Jiang', 'Xiangyu Zeng', 'Yi Wang', 'Limin Wang'], 'affiliations': ['Shanghai AI Laboratory', 'Shanghai Jiao Tong University', 'State Key Laboratory for Novel Software Technology, Nanjing University', 'University of Chinese Academy of Sciences', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2503.14237.jpg', 'data': {'categories': ['#benchmark', '#video', '#open_source', '#training', '#optimization'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Flux: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Flux. ĞĞ½ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºÑƒ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸Ğ· Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ´ĞµĞ»Ğ°Ñ ĞµĞµ Ğ±Ğ¾Ğ»ĞµĞµ Ğ³Ğ¸Ğ±ĞºĞ¾Ğ¹ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹. Flux Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ»ĞµĞ³ĞºĞ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ² ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ¸Ñ€ÑƒÑÑ‰Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ FluxViT Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²ÑĞµĞ³Ğ¾ 1/4 Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Optimizing Video Training with Token Selection for Efficiency', 'desc': 'This paper introduces a new approach to video training methods that addresses the inefficiencies of fixed token sampling from a spatiotemporal grid. The authors propose a test setting called Token Optimization, which allows for better input information management by selecting tokens from more appropriately sampled videos. They introduce a tool named Flux that enhances the flexibility of the sampling grid and can be integrated into existing video training frameworks with minimal cost. The results show that their method, FluxViT, achieves state-of-the-art performance while significantly reducing the number of tokens needed, demonstrating substantial computational savings.'}, 'zh': {'title': 'ä¼˜åŒ–è§†é¢‘è®­ç»ƒï¼Œæå‡æ¨¡å‹æ•ˆç‡', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„è§†é¢‘è®­ç»ƒæ–¹æ³•ï¼Œæ—¨åœ¨ä¼˜åŒ–è¾“å…¥ä¿¡æ¯çš„ä½¿ç”¨ã€‚é€šè¿‡å¼•å…¥ä¸€ç§åä¸ºFluxçš„å¢å¼ºå·¥å…·ï¼Œç ”ç©¶è€…ä»¬å®ç°äº†çµæ´»çš„é‡‡æ ·ç½‘æ ¼å’Œæœ‰æ•ˆçš„tokené€‰æ‹©ï¼Œä»è€Œæé«˜äº†æ¨¡å‹çš„é²æ£’æ€§ã€‚è¯¥æ–¹æ³•åœ¨å¤§è§„æ¨¡è§†é¢‘é¢„è®­ç»ƒä¸­åº”ç”¨ï¼Œå–å¾—äº†æ–°çš„æœ€å…ˆè¿›ç»“æœï¼Œä¸”åœ¨ä½¿ç”¨ä»…1/4çš„tokenæ—¶ï¼Œä»èƒ½ä¸ä¹‹å‰çš„æœ€ä½³æ¨¡å‹ç›¸åª²ç¾ã€‚æ­¤ç ”ç©¶ä¸ºè§†é¢‘å¤„ç†ä»»åŠ¡æä¾›äº†æ›´é«˜æ•ˆçš„è®¡ç®—é¢„ç®—é€‚åº”æ€§ï¼Œæ˜¾è‘—é™ä½äº†èµ„æºæ¶ˆè€—ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.13834', 'title': 'See-Saw Modality Balance: See Gradient, and Sew Impaired Vision-Language\n  Balance to Mitigate Dominant Modality Bias', 'url': 'https://huggingface.co/papers/2503.13834', 'abstract': 'Vision-language (VL) models have demonstrated strong performance across various tasks. However, these models often rely on a specific modality for predictions, leading to "dominant modality bias.\'\' This bias significantly hurts performance, especially when one modality is impaired. In this study, we analyze model behavior under dominant modality bias and theoretically show that unaligned gradients or differences in gradient magnitudes prevent balanced convergence of the loss. Based on these findings, we propose a novel framework, BalGrad to mitigate dominant modality bias. Our approach includes inter-modality gradient reweighting, adjusting the gradient of KL divergence based on each modality\'s contribution, and inter-task gradient projection to align task directions in a non-conflicting manner. Experiments on UPMC Food-101, Hateful Memes, and MM-IMDb datasets confirm that BalGrad effectively alleviates over-reliance on specific modalities when making predictions.', 'score': 4, 'issue_id': 2826, 'pub_date': '2025-03-18', 'pub_date_card': {'ru': '18 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 18', 'zh': '3æœˆ18æ—¥'}, 'hash': 'd17ffa2ed0a9ff25', 'authors': ['JuneHyoung Kwon', 'MiHyeon Kim', 'Eunju Lee', 'Juhwan Choi', 'YoungBin Kim'], 'affiliations': ['Department of Artificial Intelligence, Chung-Ang University', 'Graduate School of Advanced Imaging Sciences, Multimedia and Film, Chung-Ang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.13834.jpg', 'data': {'categories': ['#interpretability', '#multimodal', '#optimization', '#training'], 'emoji': 'âš–ï¸', 'ru': {'title': 'Ğ‘Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğµ Ğ´Ğ¾Ğ¼Ğ¸Ğ½Ğ¸Ñ€ÑƒÑÑ‰ĞµĞ¹ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ½ĞµÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ñ‹ Ğ¿Ñ€ĞµĞ¿ÑÑ‚ÑÑ‚Ğ²ÑƒÑÑ‚ ÑĞ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº BalGrad, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ¿ĞµÑ€ĞµĞ²Ğ·Ğ²ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾ĞµĞºÑ†Ğ¸Ñ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ BalGrad Ğ² ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ Ñ‡Ñ€ĞµĞ·Ğ¼ĞµÑ€Ğ½Ğ¾Ğ¹ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¿Ñ€Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸.'}, 'en': {'title': 'Balancing Modalities for Better Predictions', 'desc': "This paper addresses the issue of dominant modality bias in vision-language (VL) models, which can negatively impact their performance when one modality is compromised. The authors analyze how unaligned gradients and varying gradient magnitudes hinder the model's ability to converge effectively. To counter this bias, they introduce a new framework called BalGrad, which employs techniques like inter-modality gradient reweighting and inter-task gradient projection. Experimental results on multiple datasets demonstrate that BalGrad successfully reduces the dependency on any single modality, leading to improved prediction accuracy."}, 'zh': {'title': 'å¹³è¡¡æ¨¡æ€åå·®ï¼Œæå‡æ¨¡å‹æ€§èƒ½', 'desc': 'è§†è§‰è¯­è¨€ï¼ˆVLï¼‰æ¨¡å‹åœ¨å¤šç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬é€šå¸¸ä¾èµ–äºç‰¹å®šçš„æ¨¡æ€è¿›è¡Œé¢„æµ‹ï¼Œè¿™å¯¼è‡´äº†â€œä¸»å¯¼æ¨¡æ€åå·®â€ã€‚è¿™ç§åå·®ä¼šæ˜¾è‘—å½±å“æ¨¡å‹æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨æŸä¸€æ¨¡æ€å—æŸæ—¶ã€‚æˆ‘ä»¬åˆ†æäº†ä¸»å¯¼æ¨¡æ€åå·®ä¸‹æ¨¡å‹çš„è¡Œä¸ºï¼Œå¹¶ç†è®ºä¸Šè¯æ˜äº†æœªå¯¹é½çš„æ¢¯åº¦æˆ–æ¢¯åº¦å¹…åº¦å·®å¼‚ä¼šé˜»ç¢æŸå¤±çš„å¹³è¡¡æ”¶æ•›ã€‚åŸºäºè¿™äº›å‘ç°ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ¡†æ¶BalGradï¼Œé€šè¿‡æ¨¡æ€é—´æ¢¯åº¦é‡åŠ æƒå’Œä»»åŠ¡é—´æ¢¯åº¦æŠ•å½±æ¥å‡è½»ä¸»å¯¼æ¨¡æ€åå·®ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.12689', 'title': 'MagicID: Hybrid Preference Optimization for ID-Consistent and\n  Dynamic-Preserved Video Customization', 'url': 'https://huggingface.co/papers/2503.12689', 'abstract': "Video identity customization seeks to produce high-fidelity videos that maintain consistent identity and exhibit significant dynamics based on users' reference images. However, existing approaches face two key challenges: identity degradation over extended video length and reduced dynamics during training, primarily due to their reliance on traditional self-reconstruction training with static images. To address these issues, we introduce MagicID, a novel framework designed to directly promote the generation of identity-consistent and dynamically rich videos tailored to user preferences. Specifically, we propose constructing pairwise preference video data with explicit identity and dynamic rewards for preference learning, instead of sticking to the traditional self-reconstruction. To address the constraints of customized preference data, we introduce a hybrid sampling strategy. This approach first prioritizes identity preservation by leveraging static videos derived from reference images, then enhances dynamic motion quality in the generated videos using a Frontier-based sampling method. By utilizing these hybrid preference pairs, we optimize the model to align with the reward differences between pairs of customized preferences. Extensive experiments show that MagicID successfully achieves consistent identity and natural dynamics, surpassing existing methods across various metrics.", 'score': 4, 'issue_id': 2824, 'pub_date': '2025-03-16', 'pub_date_card': {'ru': '16 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 16', 'zh': '3æœˆ16æ—¥'}, 'hash': '2285de85beb9dbfb', 'authors': ['Hengjia Li', 'Lifan Jiang', 'Xi Xiao', 'Tianyang Wang', 'Hongwei Yi', 'Boxi Wu', 'Deng Cai'], 'affiliations': ['Hedra AI', 'University of Alabama at Birmingham', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.12689.jpg', 'data': {'categories': ['#video', '#multimodal'], 'emoji': 'ğŸ­', 'ru': {'title': 'MagicID: ĞŸĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¾Ğ¹', 'desc': 'MagicID - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ½Ğ° Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸, Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ². MagicID Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ¿Ğ°Ñ€Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ÑĞ°Ğ¼Ğ¾Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'MagicID: Dynamic Video Identity Customization Made Easy', 'desc': 'This paper presents MagicID, a new framework for generating high-quality videos that maintain a consistent identity while exhibiting dynamic movements based on user preferences. The authors identify two main challenges in existing methods: the loss of identity over longer videos and the lack of dynamics during training due to reliance on static images. To overcome these issues, MagicID uses pairwise preference video data that incorporates explicit rewards for identity and dynamics, moving away from traditional self-reconstruction techniques. The framework employs a hybrid sampling strategy to prioritize identity preservation and enhance motion quality, leading to improved performance in generating videos that meet user expectations.'}, 'zh': {'title': 'é­”æ³•èº«ä»½ï¼šå®šåˆ¶åŠ¨æ€è§†é¢‘çš„æ–°æ–¹æ³•', 'desc': 'è§†é¢‘èº«ä»½å®šåˆ¶æ—¨åœ¨ç”Ÿæˆé«˜ä¿çœŸåº¦çš„è§†é¢‘ï¼Œè¿™äº›è§†é¢‘èƒ½å¤Ÿä¿æŒä¸€è‡´çš„èº«ä»½å¹¶æ ¹æ®ç”¨æˆ·çš„å‚è€ƒå›¾åƒå±•ç°æ˜¾è‘—çš„åŠ¨æ€æ€§ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é¢ä¸´ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šåœ¨è¾ƒé•¿è§†é¢‘é•¿åº¦ä¸‹èº«ä»½çš„é€€åŒ–å’Œè®­ç»ƒè¿‡ç¨‹ä¸­åŠ¨æ€æ€§çš„å‡å°‘ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºå®ƒä»¬ä¾èµ–äºä¼ ç»Ÿçš„è‡ªæˆ‘é‡å»ºè®­ç»ƒä¸é™æ€å›¾åƒã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†MagicIDï¼Œä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œæ—¨åœ¨ç›´æ¥ä¿ƒè¿›ç”Ÿæˆç¬¦åˆç”¨æˆ·åå¥½çš„èº«ä»½ä¸€è‡´ä¸”åŠ¨æ€ä¸°å¯Œçš„è§†é¢‘ã€‚æˆ‘ä»¬é€šè¿‡æ„å»ºå…·æœ‰æ˜ç¡®èº«ä»½å’ŒåŠ¨æ€å¥–åŠ±çš„æˆå¯¹åå¥½è§†é¢‘æ•°æ®æ¥è¿›è¡Œåå¥½å­¦ä¹ ï¼Œè€Œä¸æ˜¯åšæŒä¼ ç»Ÿçš„è‡ªæˆ‘é‡å»ºæ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.09949', 'title': 'UVE: Are MLLMs Unified Evaluators for AI-Generated Videos?', 'url': 'https://huggingface.co/papers/2503.09949', 'abstract': 'With the rapid growth of video generative models (VGMs), it is essential to develop reliable and comprehensive automatic metrics for AI-generated videos (AIGVs). Existing methods either use off-the-shelf models optimized for other tasks or rely on human assessment data to train specialized evaluators. These approaches are constrained to specific evaluation aspects and are difficult to scale with the increasing demands for finer-grained and more comprehensive evaluations. To address this issue, this work investigates the feasibility of using multimodal large language models (MLLMs) as a unified evaluator for AIGVs, leveraging their strong visual perception and language understanding capabilities. To evaluate the performance of automatic metrics in unified AIGV evaluation, we introduce a benchmark called UVE-Bench. UVE-Bench collects videos generated by state-of-the-art VGMs and provides pairwise human preference annotations across 15 evaluation aspects. Using UVE-Bench, we extensively evaluate 16 MLLMs. Our empirical results suggest that while advanced MLLMs (e.g., Qwen2VL-72B and InternVL2.5-78B) still lag behind human evaluators, they demonstrate promising ability in unified AIGV evaluation, significantly surpassing existing specialized evaluation methods. Additionally, we conduct an in-depth analysis of key design choices that impact the performance of MLLM-driven evaluators, offering valuable insights for future research on AIGV evaluation. The code is available at https://github.com/bytedance/UVE.', 'score': 4, 'issue_id': 2835, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 13', 'zh': '3æœˆ13æ—¥'}, 'hash': '9533dd54603f19de', 'authors': ['Yuanxin Liu', 'Rui Zhu', 'Shuhuai Ren', 'Jiacong Wang', 'Haoyuan Guo', 'Xu Sun', 'Lu Jiang'], 'affiliations': ['ByteDance Seed', 'Peking University', 'School of Artificial Intelligence, University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2503.09949.jpg', 'data': {'categories': ['#interpretability', '#video', '#games', '#optimization', '#benchmark', '#multimodal'], 'emoji': 'ğŸ¥', 'ru': {'title': 'MLLM ĞºĞ°Ğº ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¾Ñ†ĞµĞ½Ñ‰Ğ¸Ğº AI-Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ²Ğ¸Ğ´ĞµĞ¾, ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ¾Ğ¼ (AIGV), Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (MLLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº UVE-Bench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº Ğ² ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞµ AIGV. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğµ MLLM, Ñ…Ğ¾Ñ‚Ñ Ğ¸ ÑƒÑÑ‚ÑƒĞ¿Ğ°ÑÑ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼ Ğ¾Ñ†ĞµĞ½Ñ‰Ğ¸ĞºĞ°Ğ¼, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¾Ğ±ĞµÑ‰Ğ°ÑÑ‰Ğ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞµ AIGV, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ñ‹, Ğ²Ğ»Ğ¸ÑÑÑ‰Ğ¸Ğµ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ†ĞµĞ½Ñ‰Ğ¸ĞºĞ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ MLLM.'}, 'en': {'title': 'Harnessing MLLMs for Comprehensive AI-Generated Video Evaluation', 'desc': 'This paper addresses the need for effective evaluation metrics for AI-generated videos (AIGVs) as video generative models (VGMs) become more prevalent. It critiques existing methods that either depend on models designed for different tasks or require human assessments, which limits their scalability and comprehensiveness. The authors propose using multimodal large language models (MLLMs) as a unified evaluator, capitalizing on their capabilities in visual perception and language understanding. They introduce UVE-Bench, a benchmark for evaluating AIGVs, and find that while MLLMs do not yet match human evaluators, they outperform traditional evaluation methods, providing insights for future improvements in AIGV assessment.'}, 'zh': {'title': 'åˆ©ç”¨å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹æå‡AIç”Ÿæˆè§†é¢‘è¯„ä¼°çš„ç»Ÿä¸€æ€§', 'desc': 'éšç€è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼ˆVGMï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œå¼€å‘å¯é çš„è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡å¯¹äºAIç”Ÿæˆè§†é¢‘ï¼ˆAIGVï¼‰å˜å¾—è‡³å…³é‡è¦ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºä¸ºå…¶ä»–ä»»åŠ¡ä¼˜åŒ–çš„ç°æˆæ¨¡å‹æˆ–äººç±»è¯„ä¼°æ•°æ®æ¥è®­ç»ƒä¸“é—¨çš„è¯„ä¼°å™¨ï¼Œè¿™é™åˆ¶äº†è¯„ä¼°çš„å…¨é¢æ€§å’Œå¯æ‰©å±•æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æ¢è®¨äº†ä½¿ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ä½œä¸ºAIGVçš„ç»Ÿä¸€è¯„ä¼°å™¨çš„å¯è¡Œæ€§ï¼Œå¹¶å¼•å…¥äº†ä¸€ä¸ªåä¸ºUVE-Benchçš„åŸºå‡†ï¼Œæ”¶é›†äº†ç”±æœ€å…ˆè¿›çš„VGMç”Ÿæˆçš„è§†é¢‘åŠå…¶äººç±»åå¥½æ³¨é‡Šã€‚æˆ‘ä»¬çš„å®è¯ç»“æœè¡¨æ˜ï¼Œå°½ç®¡å…ˆè¿›çš„MLLMä»ç„¶è½åäºäººç±»è¯„ä¼°è€…ï¼Œä½†åœ¨ç»Ÿä¸€AIGVè¯„ä¼°ä¸­è¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„ä¸“é—¨è¯„ä¼°æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.16194', 'title': 'Improving Autoregressive Image Generation through Coarse-to-Fine Token\n  Prediction', 'url': 'https://huggingface.co/papers/2503.16194', 'abstract': "Autoregressive models have shown remarkable success in image generation by adapting sequential prediction techniques from language modeling. However, applying these approaches to images requires discretizing continuous pixel data through vector quantization methods like VQ-VAE. To alleviate the quantization errors that existed in VQ-VAE, recent works tend to use larger codebooks. However, this will accordingly expand vocabulary size, complicating the autoregressive modeling task. This paper aims to find a way to enjoy the benefits of large codebooks without making autoregressive modeling more difficult. Through empirical investigation, we discover that tokens with similar codeword representations produce similar effects on the final generated image, revealing significant redundancy in large codebooks. Based on this insight, we propose to predict tokens from coarse to fine (CTF), realized by assigning the same coarse label for similar tokens. Our framework consists of two stages: (1) an autoregressive model that sequentially predicts coarse labels for each token in the sequence, and (2) an auxiliary model that simultaneously predicts fine-grained labels for all tokens conditioned on their coarse labels. Experiments on ImageNet demonstrate our method's superior performance, achieving an average improvement of 59 points in Inception Score compared to baselines. Notably, despite adding an inference step, our approach achieves faster sampling speeds.", 'score': 3, 'issue_id': 2826, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 20', 'zh': '3æœˆ20æ—¥'}, 'hash': '0e990c414e8ba81c', 'authors': ['Ziyao Guo', 'Kaipeng Zhang', 'Michael Qizhe Shieh'], 'affiliations': ['National University of Singapore', 'Shanghai AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2503.16194.jpg', 'data': {'categories': ['#cv', '#training', '#optimization', '#architecture'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'ĞÑ‚ Ğ³Ñ€ÑƒĞ±Ğ¾Ğ³Ğ¾ Ğº Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¼Ñƒ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¾Ñ‚ Ğ³Ñ€ÑƒĞ±Ğ¾Ğ³Ğ¾ Ğº Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¼Ñƒ (CTF), Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ĞºĞ¾Ğ´Ğ¾Ğ²Ñ‹Ñ… ĞºĞ½Ğ¸Ğ³ Ğ±ĞµĞ· ÑƒÑĞ»Ğ¾Ğ¶Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ğ´Ğ²ÑƒÑ… ÑÑ‚Ğ°Ğ¿Ğ¾Ğ²: Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ³Ñ€ÑƒĞ±Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğº Ğ¸ Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğº. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° ImageNet Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Inception Score Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ğ±Ñ‹ÑÑ‚Ñ€ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Enhancing Image Generation with Coarse-to-Fine Token Prediction', 'desc': 'This paper explores how autoregressive models can be improved for image generation by addressing the challenges of using large codebooks in vector quantization. It identifies that many tokens in these large codebooks are redundant, meaning they have similar effects on the generated images. To tackle this, the authors propose a coarse-to-fine (CTF) prediction method that first predicts broad categories for tokens and then refines these predictions with more detailed labels. Their experiments show that this approach not only enhances image quality, as indicated by a significant increase in Inception Score, but also speeds up the sampling process despite the added complexity.'}, 'zh': {'title': 'ä»ç²—åˆ°ç»†ï¼šä¼˜åŒ–è‡ªå›å½’å›¾åƒç”Ÿæˆ', 'desc': 'è‡ªå›å½’æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆä¸­å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œå€Ÿé‰´äº†è¯­è¨€å»ºæ¨¡ä¸­çš„åºåˆ—é¢„æµ‹æŠ€æœ¯ã€‚ç„¶è€Œï¼Œå°†è¿™äº›æ–¹æ³•åº”ç”¨äºå›¾åƒæ—¶ï¼Œéœ€è¦é€šè¿‡å‘é‡é‡åŒ–æ–¹æ³•ï¼ˆå¦‚VQ-VAEï¼‰å°†è¿ç»­åƒç´ æ•°æ®ç¦»æ•£åŒ–ã€‚ä¸ºäº†å‡å°‘VQ-VAEä¸­å­˜åœ¨çš„é‡åŒ–è¯¯å·®ï¼Œæœ€è¿‘çš„ç ”ç©¶å€¾å‘äºä½¿ç”¨æ›´å¤§çš„ä»£ç æœ¬ï¼Œä½†è¿™ä¼šå¢åŠ è¯æ±‡é‡ï¼Œå¤æ‚åŒ–è‡ªå›å½’å»ºæ¨¡ä»»åŠ¡ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ä»ç²—åˆ°ç»†ï¼ˆCTFï¼‰é¢„æµ‹æ ‡è®°çš„æ–¹æ³•ï¼Œåˆ©ç”¨ç›¸ä¼¼ä»£ç è¯è¡¨ç¤ºçš„æ ‡è®°å¯¹æœ€ç»ˆç”Ÿæˆå›¾åƒçš„ç›¸ä¼¼å½±å“ï¼Œä»è€Œåœ¨ä¸å¢åŠ å»ºæ¨¡éš¾åº¦çš„æƒ…å†µä¸‹äº«å—å¤§ä»£ç æœ¬çš„ä¼˜åŠ¿ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.16031', 'title': 'Deceptive Humor: A Synthetic Multilingual Benchmark Dataset for Bridging\n  Fabricated Claims with Humorous Content', 'url': 'https://huggingface.co/papers/2503.16031', 'abstract': 'This paper presents the Deceptive Humor Dataset (DHD), a novel resource for studying humor derived from fabricated claims and misinformation. In an era of rampant misinformation, understanding how humor intertwines with deception is essential. DHD consists of humor-infused comments generated from false narratives, incorporating fabricated claims and manipulated information using the ChatGPT-4o model. Each instance is labeled with a Satire Level, ranging from 1 for subtle satire to 3 for high-level satire and classified into five distinct Humor Categories: Dark Humor, Irony, Social Commentary, Wordplay, and Absurdity. The dataset spans multiple languages including English, Telugu, Hindi, Kannada, Tamil, and their code-mixed variants (Te-En, Hi-En, Ka-En, Ta-En), making it a valuable multilingual benchmark. By introducing DHD, we establish a structured foundation for analyzing humor in deceptive contexts, paving the way for a new research direction that explores how humor not only interacts with misinformation but also influences its perception and spread. We establish strong baselines for the proposed dataset, providing a foundation for future research to benchmark and advance deceptive humor detection models.', 'score': 3, 'issue_id': 2822, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 20', 'zh': '3æœˆ20æ—¥'}, 'hash': '80726382feca8f20', 'authors': ['Sai Kartheek Reddy Kasu', 'Shankar Biradar', 'Sunil Saumya'], 'affiliations': ['IIIT Dharwad', 'MIT Manipal'], 'pdf_title_img': 'assets/pdf/title_img/2503.16031.jpg', 'data': {'categories': ['#ethics', '#low_resource', '#benchmark', '#dataset', '#multilingual'], 'emoji': 'ğŸ¤¡', 'ru': {'title': 'Ğ¡Ğ¼ĞµÑ… ÑĞºĞ²Ğ¾Ğ·ÑŒ Ğ»Ğ¾Ğ¶ÑŒ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¾Ğ±Ğ¼Ğ°Ğ½Ñ‡Ğ¸Ğ²Ğ¾Ğ³Ğ¾ ÑĞ¼Ğ¾Ñ€Ğ°', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… DHD (Deceptive Humor Dataset) Ğ´Ğ»Ñ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ¼Ğ¾Ñ€Ğ°, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ° Ğ²Ñ‹Ğ´ÑƒĞ¼Ğ°Ğ½Ğ½Ñ‹Ñ… ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¸ÑÑ… Ğ¸ Ğ´ĞµĞ·Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. DHD ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· ÑĞ¼Ğ¾Ñ€Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ĞºĞ¾Ğ¼Ğ¼ĞµĞ½Ñ‚Ğ°Ñ€Ğ¸ĞµĞ², ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ½Ğ°Ñ€Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ¾Ğ² Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ChatGPT-4o. ĞšĞ°Ğ¶Ğ´Ñ‹Ğ¹ ÑĞºĞ·ĞµĞ¼Ğ¿Ğ»ÑÑ€ Ğ¿Ğ¾Ğ¼ĞµÑ‡ĞµĞ½ ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ¼ ÑĞ°Ñ‚Ğ¸Ñ€Ñ‹ Ğ¸ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ Ğ¿Ğ¾ Ğ¿ÑÑ‚Ğ¸ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸ÑĞ¼ ÑĞ¼Ğ¾Ñ€Ğ°. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ÑĞ·Ñ‹ĞºĞ¾Ğ², Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¸Ğ¹ Ğ¸ Ğ¸Ğ½Ğ´Ğ¸Ğ¹ÑĞºĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¸, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ ĞµĞ³Ğ¾ Ñ†ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¼ Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ¼ Ğ´Ğ»Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹.'}, 'en': {'title': 'Unraveling Humor in the Age of Misinformation', 'desc': 'The Deceptive Humor Dataset (DHD) is a new resource designed to explore the relationship between humor and misinformation. It includes humor-filled comments based on false narratives, generated using the ChatGPT-4o model, and is labeled with a Satire Level and categorized into five types of humor. This dataset supports multiple languages, making it a useful tool for multilingual research in humor and deception. By providing a structured dataset, DHD aims to enhance the understanding of how humor can affect the perception and dissemination of misinformation.'}, 'zh': {'title': 'æ­ç¤ºå¹½é»˜ä¸æ¬ºéª—çš„äº¤ç»‡', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªæ–°çš„èµ„æºâ€”â€”æ¬ºéª—å¹½é»˜æ•°æ®é›†ï¼ˆDHDï¼‰ï¼Œç”¨äºç ”ç©¶æºè‡ªè™šå‡å£°æ˜å’Œé”™è¯¯ä¿¡æ¯çš„å¹½é»˜ã€‚åœ¨ä¿¡æ¯æ³›æ»¥çš„æ—¶ä»£ï¼Œç†è§£å¹½é»˜ä¸æ¬ºéª—ä¹‹é—´çš„å…³ç³»è‡³å…³é‡è¦ã€‚DHDåŒ…å«ä»è™šå‡å™è¿°ä¸­ç”Ÿæˆçš„å¹½é»˜è¯„è®ºï¼Œå¹¶ä½¿ç”¨ChatGPT-4oæ¨¡å‹ç”Ÿæˆè™šå‡å£°æ˜å’Œæ“æ§ä¿¡æ¯ã€‚æ¯ä¸ªå®ä¾‹éƒ½æ ‡æ³¨äº†è®½åˆºæ°´å¹³ï¼Œå¹¶åˆ†ä¸ºäº”ç§å¹½é»˜ç±»åˆ«ï¼Œä¸ºåˆ†ææ¬ºéª—èƒŒæ™¯ä¸‹çš„å¹½é»˜æä¾›äº†ç»“æ„åŒ–åŸºç¡€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.15855', 'title': 'VideoRFSplat: Direct Scene-Level Text-to-3D Gaussian Splatting\n  Generation with Flexible Pose and Multi-View Joint Modeling', 'url': 'https://huggingface.co/papers/2503.15855', 'abstract': 'We propose VideoRFSplat, a direct text-to-3D model leveraging a video generation model to generate realistic 3D Gaussian Splatting (3DGS) for unbounded real-world scenes. To generate diverse camera poses and unbounded spatial extent of real-world scenes, while ensuring generalization to arbitrary text prompts, previous methods fine-tune 2D generative models to jointly model camera poses and multi-view images. However, these methods suffer from instability when extending 2D generative models to joint modeling due to the modality gap, which necessitates additional models to stabilize training and inference. In this work, we propose an architecture and a sampling strategy to jointly model multi-view images and camera poses when fine-tuning a video generation model. Our core idea is a dual-stream architecture that attaches a dedicated pose generation model alongside a pre-trained video generation model via communication blocks, generating multi-view images and camera poses through separate streams. This design reduces interference between the pose and image modalities. Additionally, we propose an asynchronous sampling strategy that denoises camera poses faster than multi-view images, allowing rapidly denoised poses to condition multi-view generation, reducing mutual ambiguity and enhancing cross-modal consistency. Trained on multiple large-scale real-world datasets (RealEstate10K, MVImgNet, DL3DV-10K, ACID), VideoRFSplat outperforms existing text-to-3D direct generation methods that heavily depend on post-hoc refinement via score distillation sampling, achieving superior results without such refinement.', 'score': 3, 'issue_id': 2829, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 20', 'zh': '3æœˆ20æ—¥'}, 'hash': '66858187cf22b842', 'authors': ['Hyojun Go', 'Byeongjun Park', 'Hyelin Nam', 'Byung-Hoon Kim', 'Hyungjin Chung', 'Changick Kim'], 'affiliations': ['EverEx', 'KAIST', 'Yonsei University'], 'pdf_title_img': 'assets/pdf/title_img/2503.15855.jpg', 'data': {'categories': ['#architecture', '#video', '#3d'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Ğ ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ 3D-ÑÑ†ĞµĞ½Ñ‹ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ° Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸', 'desc': 'VideoRFSplat - ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… 3D-ÑÑ†ĞµĞ½ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ ĞºĞ°Ğ¼ĞµÑ€Ñ‹. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ´Ğ²ÑƒÑ…Ğ¿Ğ¾Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ, Ğ³Ğ´Ğµ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ğ·Ñ‹ ĞºĞ°Ğ¼ĞµÑ€Ñ‹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ°ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Revolutionizing 3D Scene Generation with VideoRFSplat', 'desc': 'VideoRFSplat is a novel text-to-3D model that utilizes a video generation framework to create realistic 3D Gaussian Splatting for expansive real-world scenes. Unlike previous methods that struggle with stability when combining 2D generative models for camera poses and images, our approach employs a dual-stream architecture that separates the generation of poses and images. This design minimizes interference between the two modalities and incorporates an asynchronous sampling strategy that accelerates the denoising of camera poses, enhancing the overall consistency of the generated outputs. Trained on extensive datasets, VideoRFSplat demonstrates superior performance compared to existing methods that rely on additional refinement techniques.'}, 'zh': {'title': 'VideoRFSplatï¼šæ–‡æœ¬åˆ°3Dçš„åˆ›æ–°ä¹‹è·¯', 'desc': 'æˆ‘ä»¬æå‡ºäº†VideoRFSplatï¼Œè¿™æ˜¯ä¸€ç§ç›´æ¥çš„æ–‡æœ¬åˆ°3Dæ¨¡å‹ï¼Œåˆ©ç”¨è§†é¢‘ç”Ÿæˆæ¨¡å‹ç”Ÿæˆé€¼çœŸçš„3Dé«˜æ–¯ç‚¹äº‘ï¼ˆ3DGSï¼‰ï¼Œé€‚ç”¨äºæ— é™çš„çœŸå®åœºæ™¯ã€‚ä¸ä¹‹å‰çš„æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ¨¡å‹é€šè¿‡åŒæµæ¶æ„åŒæ—¶å»ºæ¨¡å¤šè§†å›¾å›¾åƒå’Œç›¸æœºå§¿æ€ï¼Œå‡å°‘äº†å§¿æ€å’Œå›¾åƒæ¨¡æ€ä¹‹é—´çš„å¹²æ‰°ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§å¼‚æ­¥é‡‡æ ·ç­–ç•¥ï¼Œä½¿å¾—ç›¸æœºå§¿æ€çš„å»å™ªé€Ÿåº¦å¿«äºå¤šè§†å›¾å›¾åƒï¼Œä»è€Œæé«˜äº†è·¨æ¨¡æ€çš„ä¸€è‡´æ€§ã€‚ç»è¿‡åœ¨å¤šä¸ªå¤§è§„æ¨¡çœŸå®æ•°æ®é›†ä¸Šçš„è®­ç»ƒï¼ŒVideoRFSplatåœ¨ä¸ä¾èµ–åæœŸç²¾ç‚¼çš„æƒ…å†µä¸‹ï¼Œè¶…è¶Šäº†ç°æœ‰çš„æ–‡æœ¬åˆ°3Dç›´æ¥ç”Ÿæˆæ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.13891', 'title': 'Where do Large Vision-Language Models Look at when Answering Questions?', 'url': 'https://huggingface.co/papers/2503.13891', 'abstract': 'Large Vision-Language Models (LVLMs) have shown promising performance in vision-language understanding and reasoning tasks. However, their visual understanding behaviors remain underexplored. A fundamental question arises: to what extent do LVLMs rely on visual input, and which image regions contribute to their responses? It is non-trivial to interpret the free-form generation of LVLMs due to their complicated visual architecture (e.g., multiple encoders and multi-resolution) and variable-length outputs. In this paper, we extend existing heatmap visualization methods (e.g., iGOS++) to support LVLMs for open-ended visual question answering. We propose a method to select visually relevant tokens that reflect the relevance between generated answers and input image. Furthermore, we conduct a comprehensive analysis of state-of-the-art LVLMs on benchmarks designed to require visual information to answer. Our findings offer several insights into LVLM behavior, including the relationship between focus region and answer correctness, differences in visual attention across architectures, and the impact of LLM scale on visual understanding. The code and data are available at https://github.com/bytedance/LVLM_Interpretation.', 'score': 2, 'issue_id': 2841, 'pub_date': '2025-03-18', 'pub_date_card': {'ru': '18 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 18', 'zh': '3æœˆ18æ—¥'}, 'hash': '561c77b5d6495701', 'authors': ['Xiaoying Xing', 'Chia-Wen Kuo', 'Li Fuxin', 'Yulei Niu', 'Fan Chen', 'Ming Li', 'Ying Wu', 'Longyin Wen', 'Sijie Zhu'], 'affiliations': ['Bytedance Intelligent Creation', 'Northwestern University', 'Oregon State University'], 'pdf_title_img': 'assets/pdf/title_img/2503.13891.jpg', 'data': {'categories': ['#cv', '#benchmark', '#multimodal', '#interpretability', '#games'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ—Ğ°Ğ³Ğ»ÑĞ´Ñ‹Ğ²Ğ°Ñ Ğ² Ğ³Ğ»Ğ°Ğ·Ğ° Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ñƒ: Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ LVLM', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LVLM) Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞ¿Ğ»Ğ¾Ğ²Ñ‹Ñ… ĞºĞ°Ñ€Ñ‚ Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ ÑĞ²Ğ¾Ğ±Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ LVLM Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°. ĞĞ½Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… LVLM Ğ½Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾ ÑĞ²ÑĞ·Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ„Ğ¾ĞºÑƒÑĞ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑĞ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ², Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸ÑÑ… Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°Ğ¼Ğ¸ Ğ¸ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ.'}, 'en': {'title': 'Unlocking the Visual Understanding of LVLMs', 'desc': 'This paper investigates how Large Vision-Language Models (LVLMs) understand and utilize visual information when answering questions about images. It addresses the challenge of interpreting the complex visual architectures of LVLMs, which include multiple encoders and produce variable-length outputs. The authors enhance existing heatmap visualization techniques to identify which parts of an image are most relevant to the answers generated by the models. Their analysis reveals important insights into how different LVLM architectures focus on visual regions and how the scale of the language model affects its visual comprehension capabilities.'}, 'zh': {'title': 'æ­ç¤ºå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„è§†è§‰ç†è§£', 'desc': 'å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨è§†è§‰è¯­è¨€ç†è§£å’Œæ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬çš„è§†è§‰ç†è§£è¡Œä¸ºä»ç„¶ä¸å¤Ÿæ˜ç¡®ã€‚æœ¬æ–‡æ¢è®¨äº†LVLMså¯¹è§†è§‰è¾“å…¥çš„ä¾èµ–ç¨‹åº¦ä»¥åŠå“ªäº›å›¾åƒåŒºåŸŸå¯¹å…¶å“åº”æœ‰è´¡çŒ®ã€‚æˆ‘ä»¬æ‰©å±•äº†ç°æœ‰çš„çƒ­å›¾å¯è§†åŒ–æ–¹æ³•ï¼Œä»¥æ”¯æŒLVLMsåœ¨å¼€æ”¾å¼è§†è§‰é—®ç­”ä¸­çš„åº”ç”¨ï¼Œå¹¶æå‡ºäº†ä¸€ç§é€‰æ‹©ä¸ç”Ÿæˆç­”æ¡ˆç›¸å…³çš„è§†è§‰æ ‡è®°çš„æ–¹æ³•ã€‚é€šè¿‡å¯¹æœ€å…ˆè¿›çš„LVLMsè¿›è¡Œå…¨é¢åˆ†æï¼Œæˆ‘ä»¬æ­ç¤ºäº†ç„¦ç‚¹åŒºåŸŸä¸ç­”æ¡ˆæ­£ç¡®æ€§ä¹‹é—´çš„å…³ç³»ï¼Œä»¥åŠä¸åŒæ¶æ„åœ¨è§†è§‰æ³¨æ„åŠ›ä¸Šçš„å·®å¼‚ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.07906', 'title': 'Painting with Words: Elevating Detailed Image Captioning with Benchmark\n  and Alignment Learning', 'url': 'https://huggingface.co/papers/2503.07906', 'abstract': 'Image captioning has long been a pivotal task in visual understanding, with recent advancements in vision-language models (VLMs) significantly enhancing the ability to generate detailed image captions. However, the evaluation of detailed image captioning remains underexplored due to outdated evaluation metrics and coarse annotations. In this paper, we introduce DeCapBench along with a novel metric, DCScore, specifically designed for detailed captioning tasks. DCScore evaluates hallucinations and fine-grained comprehensiveness by deconstructing responses into the smallest self-sufficient units, termed primitive information units, and assessing them individually. Our evaluation shows that DCScore aligns more closely with human judgment than other rule-based or model-based metrics. Concurrently, DeCapBench exhibits a high correlation with VLM arena results on descriptive tasks, surpassing existing benchmarks for vision-language models. Additionally, we present an automatic fine-grained feedback collection method, FeedQuill, for preference optimization based on our advanced metric, showing robust generalization capabilities across auto-generated preference data. Extensive experiments on multiple VLMs demonstrate that our method not only significantly reduces hallucinations but also enhances performance across various benchmarks, achieving superior detail captioning performance while surpassing GPT-4o.', 'score': 2, 'issue_id': 2843, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 10', 'zh': '3æœˆ10æ—¥'}, 'hash': 'df60b4319f196868', 'authors': ['Qinghao Ye', 'Xianhan Zeng', 'Fu Li', 'Chunyuan Li', 'Haoqi Fan'], 'affiliations': ['ByteDance Research'], 'pdf_title_img': 'assets/pdf/title_img/2503.07906.jpg', 'data': {'categories': ['#optimization', '#hallucinations', '#cv', '#benchmark', '#multimodal', '#training'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ DCScore. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº Ñ€Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ½Ğ° ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ°Ñ€Ğ½Ñ‹Ğµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ ĞµĞ´Ğ¸Ğ½Ğ¸Ñ†Ñ‹ Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¸Ñ… Ğ¸Ğ½Ğ´Ğ¸Ğ²Ğ¸Ğ´ÑƒĞ°Ğ»ÑŒĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµ Ğ²Ñ‹ÑĞ²Ğ»ÑÑ‚ÑŒ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… DeCapBench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ° Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ”Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ FeedQuill Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ±Ğ¾Ñ€Ğ° Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ….'}, 'en': {'title': 'Revolutionizing Image Caption Evaluation with DeCapBench and DCScore', 'desc': 'This paper addresses the challenge of evaluating detailed image captioning, which has been hindered by outdated metrics and coarse annotations. The authors introduce DeCapBench and a new metric called DCScore, which focuses on assessing image captions by breaking them down into primitive information units. This approach allows for a more accurate evaluation of hallucinations and the comprehensiveness of captions, aligning better with human judgment. Additionally, the paper presents FeedQuill, a method for collecting fine-grained feedback to optimize preferences, demonstrating improved performance in detailed captioning tasks across various vision-language models.'}, 'zh': {'title': 'æå‡å›¾åƒæè¿°çš„è¯„ä¼°ä¸ç”Ÿæˆèƒ½åŠ›', 'desc': 'å›¾åƒæè¿°ä¸€ç›´æ˜¯è§†è§‰ç†è§£ä¸­çš„ä¸€ä¸ªé‡è¦ä»»åŠ¡ï¼Œæœ€è¿‘è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„è¿›æ­¥æ˜¾è‘—æå‡äº†ç”Ÿæˆè¯¦ç»†å›¾åƒæè¿°çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¯¦ç»†å›¾åƒæè¿°çš„è¯„ä¼°ä»ç„¶ä¸å¤Ÿæ·±å…¥ï¼Œä¸»è¦æ˜¯å› ä¸ºè¯„ä¼°æŒ‡æ ‡è¿‡æ—¶å’Œæ³¨é‡Šç²—ç³™ã€‚æœ¬æ–‡ä»‹ç»äº†DeCapBenchå’Œä¸€ç§æ–°é¢–çš„è¯„ä¼°æŒ‡æ ‡DCScoreï¼Œä¸“é—¨ç”¨äºè¯¦ç»†æè¿°ä»»åŠ¡ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°è¯„ä¼°æè¿°çš„å‡†ç¡®æ€§å’Œç»†è‡´ç¨‹åº¦ã€‚æˆ‘ä»¬çš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒDCScoreä¸äººç±»åˆ¤æ–­çš„å»åˆåº¦é«˜äºå…¶ä»–åŸºäºè§„åˆ™æˆ–æ¨¡å‹çš„æŒ‡æ ‡ï¼ŒåŒæ—¶DeCapBenchåœ¨æè¿°æ€§ä»»åŠ¡ä¸Šä¸VLMé¢†åŸŸçš„ç»“æœé«˜åº¦ç›¸å…³ï¼Œè¶…è¶Šäº†ç°æœ‰çš„åŸºå‡†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.16091', 'title': 'AIMI: Leveraging Future Knowledge and Personalization in Sparse Event\n  Forecasting for Treatment Adherence', 'url': 'https://huggingface.co/papers/2503.16091', 'abstract': 'Adherence to prescribed treatments is crucial for individuals with chronic conditions to avoid costly or adverse health outcomes. For certain patient groups, intensive lifestyle interventions are vital for enhancing medication adherence. Accurate forecasting of treatment adherence can open pathways to developing an on-demand intervention tool, enabling timely and personalized support. With the increasing popularity of smartphones and wearables, it is now easier than ever to develop and deploy smart activity monitoring systems. However, effective forecasting systems for treatment adherence based on wearable sensors are still not widely available. We close this gap by proposing Adherence Forecasting and Intervention with Machine Intelligence (AIMI). AIMI is a knowledge-guided adherence forecasting system that leverages smartphone sensors and previous medication history to estimate the likelihood of forgetting to take a prescribed medication. A user study was conducted with 27 participants who took daily medications to manage their cardiovascular diseases. We designed and developed CNN and LSTM-based forecasting models with various combinations of input features and found that LSTM models can forecast medication adherence with an accuracy of 0.932 and an F-1 score of 0.936. Moreover, through a series of ablation studies involving convolutional and recurrent neural network architectures, we demonstrate that leveraging known knowledge about future and personalized training enhances the accuracy of medication adherence forecasting. Code available: https://github.com/ab9mamun/AIMI.', 'score': 1, 'issue_id': 2832, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 20', 'zh': '3æœˆ20æ—¥'}, 'hash': '905c53a7c6bb7214', 'authors': ['Abdullah Mamun', 'Diane J. Cook', 'Hassan Ghasemzadeh'], 'affiliations': ['College of Health Solutions, Arizona State University, Phoenix, AZ 85054, USA', 'School of Computing and Augmented Intelligence, Arizona State University, Phoenix, AZ 85054, USA', 'School of Electrical Engineering and Computer Science, Washington State University, Pullman, WA 99164, USA'], 'pdf_title_img': 'assets/pdf/title_img/2503.16091.jpg', 'data': {'categories': ['#science', '#architecture', '#healthcare', '#optimization', '#open_source', '#training'], 'emoji': 'ğŸ’Š', 'ru': {'title': 'Ğ˜Ğ˜ Ğ½Ğ° ÑÑ‚Ñ€Ğ°Ğ¶Ğµ Ğ²Ğ°ÑˆĞµĞ³Ğ¾ Ğ·Ğ´Ğ¾Ñ€Ğ¾Ğ²ÑŒÑ: Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸ĞµĞ¼Ğ° Ğ»ĞµĞºĞ°Ñ€ÑÑ‚Ğ²', 'desc': 'Ğ”Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ AIMI (Adherence Forecasting and Intervention with Machine Intelligence) Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ²ĞµÑ€Ğ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ½Ğ¾ÑĞ¸Ğ¼Ñ‹Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ² Ğ¸ ÑĞ¼Ğ°Ñ€Ñ‚Ñ„Ğ¾Ğ½Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ±Ğ°Ğ·Ğµ ÑĞ²ĞµÑ€Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹ (CNN) Ğ¸ LSTM Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ° Ğ¿Ñ€Ğ¸ĞµĞ¼Ğ° Ğ»ĞµĞºĞ°Ñ€ÑÑ‚Ğ² Ğ¿Ğ°Ñ†Ğ¸ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ñ ÑĞµÑ€Ğ´ĞµÑ‡Ğ½Ğ¾-ÑĞ¾ÑÑƒĞ´Ğ¸ÑÑ‚Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ±Ğ¾Ğ»ĞµĞ²Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ LSTM Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ 0,932 Ğ¸ F1-Ğ¼ĞµÑ€Ñ‹ 0,936 Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ°Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¾ Ğ±ÑƒĞ´ÑƒÑ‰ĞµĞ¼ Ğ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ÑÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¾Ğ².'}, 'en': {'title': 'Smart Forecasting for Better Medication Adherence', 'desc': 'This paper presents AIMI, a machine learning system designed to forecast medication adherence using smartphone sensors and patient medication history. By employing advanced models like CNN and LSTM, AIMI achieves high accuracy in predicting when patients might forget to take their medications. The study involved 27 participants with cardiovascular diseases, demonstrating that personalized training and knowledge integration significantly improve forecasting performance. The findings suggest that AIMI can serve as a valuable tool for timely interventions, ultimately enhancing treatment adherence for chronic condition management.'}, 'zh': {'title': 'æ™ºèƒ½é¢„æµ‹ï¼ŒåŠ©åŠ›è¯ç‰©ä¾ä»æ€§', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºAIMIçš„è¯ç‰©ä¾ä»æ€§é¢„æµ‹ç³»ç»Ÿï¼Œåˆ©ç”¨æ™ºèƒ½æ‰‹æœºä¼ æ„Ÿå™¨å’Œæ‚£è€…çš„ç”¨è¯å†å²æ¥é¢„æµ‹æ‚£è€…å¿˜è®°æœè¯çš„å¯èƒ½æ€§ã€‚é€šè¿‡å¯¹27åå¿ƒè¡€ç®¡ç–¾ç—…æ‚£è€…çš„ç”¨æˆ·ç ”ç©¶ï¼Œä½¿ç”¨åŸºäºå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’Œé•¿çŸ­æœŸè®°å¿†ç½‘ç»œï¼ˆLSTMï¼‰çš„æ¨¡å‹è¿›è¡Œé¢„æµ‹ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒLSTMæ¨¡å‹åœ¨è¯ç‰©ä¾ä»æ€§é¢„æµ‹ä¸­è¾¾åˆ°äº†93.2%çš„å‡†ç¡®ç‡å’Œ93.6çš„F-1åˆ†æ•°ã€‚é€šè¿‡ä¸€ç³»åˆ—æ¶ˆèå®éªŒï¼Œè¯æ˜äº†åˆ©ç”¨å·²çŸ¥çŸ¥è¯†å’Œä¸ªæ€§åŒ–è®­ç»ƒå¯ä»¥æé«˜è¯ç‰©ä¾ä»æ€§é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.11509', 'title': 'TikZero: Zero-Shot Text-Guided Graphics Program Synthesis', 'url': 'https://huggingface.co/papers/2503.11509', 'abstract': 'With the rise of generative AI, synthesizing figures from text captions becomes a compelling application. However, achieving high geometric precision and editability requires representing figures as graphics programs in languages like TikZ, and aligned training data (i.e., graphics programs with captions) remains scarce. Meanwhile, large amounts of unaligned graphics programs and captioned raster images are more readily available. We reconcile these disparate data sources by presenting TikZero, which decouples graphics program generation from text understanding by using image representations as an intermediary bridge. It enables independent training on graphics programs and captioned images and allows for zero-shot text-guided graphics program synthesis during inference. We show that our method substantially outperforms baselines that can only operate with caption-aligned graphics programs. Furthermore, when leveraging caption-aligned graphics programs as a complementary training signal, TikZero matches or exceeds the performance of much larger models, including commercial systems like GPT-4o. Our code, datasets, and select models are publicly available.', 'score': 1, 'issue_id': 2837, 'pub_date': '2025-03-14', 'pub_date_card': {'ru': '14 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 14', 'zh': '3æœˆ14æ—¥'}, 'hash': '2335a2440c406122', 'authors': ['Jonas Belouadi', 'Eddy Ilg', 'Margret Keuper', 'Hideki Tanaka', 'Masao Utiyama', 'Raj Dabre', 'Steffen Eger', 'Simone Paolo Ponzetto'], 'affiliations': ['National Institute of Information and Communications Technology, Japan', 'University of Mannheim, Germany', 'University of Technology Nuremberg, Germany'], 'pdf_title_img': 'assets/pdf/title_img/2503.11509.jpg', 'data': {'categories': ['#training', '#synthetic', '#open_source', '#multimodal', '#data', '#dataset', '#inference'], 'emoji': 'ğŸ¨', 'ru': {'title': 'TikZero: Ğ¼Ğ¾ÑÑ‚ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ¸ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğ¹ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¾Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ', 'desc': 'TikZero - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ğ²ĞµĞ½Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ¸ ĞºĞ¾Ğ´Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒÑÑ Ğ½Ğ° Ğ½ĞµÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. TikZero Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‰Ğ¸Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ Ğ²Ñ‹Ñ€Ğ¾Ğ²Ğ½ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸, Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğ¹ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¸.'}, 'en': {'title': 'Bridging Text and Graphics with TikZero', 'desc': 'This paper introduces TikZero, a novel approach for generating graphics programs from text captions using generative AI. It addresses the challenge of limited aligned training data by utilizing unaligned graphics programs and captioned images as separate training sources. TikZero employs image representations as a bridge, allowing for independent training and enabling zero-shot synthesis of graphics programs guided by text. The results demonstrate that TikZero significantly outperforms existing methods that rely solely on aligned data, achieving competitive performance with larger models when combined with aligned graphics programs.'}, 'zh': {'title': 'TikZeroï¼šè§£è€¦æ–‡æœ¬ä¸å›¾å½¢ç”Ÿæˆçš„åˆ›æ–°æ–¹æ³•', 'desc': 'éšç€ç”Ÿæˆæ€§äººå·¥æ™ºèƒ½çš„å…´èµ·ï¼Œä»æ–‡æœ¬æè¿°åˆæˆå›¾å½¢æˆä¸ºä¸€ä¸ªå¼•äººæ³¨ç›®çš„åº”ç”¨ã€‚ç„¶è€Œï¼Œå®ç°é«˜å‡ ä½•ç²¾åº¦å’Œå¯ç¼–è¾‘æ€§éœ€è¦å°†å›¾å½¢è¡¨ç¤ºä¸ºåƒTikZè¿™æ ·çš„å›¾å½¢ç¨‹åºï¼Œè€Œå¯¹é½çš„è®­ç»ƒæ•°æ®ï¼ˆå³å¸¦æœ‰æè¿°çš„å›¾å½¢ç¨‹åºï¼‰ä»ç„¶ç¨€ç¼ºã€‚æˆ‘ä»¬æå‡ºäº†TikZeroï¼Œé€šè¿‡ä½¿ç”¨å›¾åƒè¡¨ç¤ºä½œä¸ºä¸­ä»‹æ¡¥æ¢ï¼Œå°†å›¾å½¢ç¨‹åºç”Ÿæˆä¸æ–‡æœ¬ç†è§£è§£è€¦ã€‚è¿™ä½¿å¾—æˆ‘ä»¬èƒ½å¤Ÿç‹¬ç«‹è®­ç»ƒå›¾å½¢ç¨‹åºå’Œå¸¦æè¿°çš„å›¾åƒï¼Œå¹¶åœ¨æ¨ç†è¿‡ç¨‹ä¸­å®ç°é›¶-shotæ–‡æœ¬å¼•å¯¼çš„å›¾å½¢ç¨‹åºåˆæˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.15672', 'title': 'GASP: Unifying Geometric and Semantic Self-Supervised Pre-training for\n  Autonomous Driving', 'url': 'https://huggingface.co/papers/2503.15672', 'abstract': 'Self-supervised pre-training based on next-token prediction has enabled large language models to capture the underlying structure of text, and has led to unprecedented performance on a large array of tasks when applied at scale. Similarly, autonomous driving generates vast amounts of spatiotemporal data, alluding to the possibility of harnessing scale to learn the underlying geometric and semantic structure of the environment and its evolution over time. In this direction, we propose a geometric and semantic self-supervised pre-training method, GASP, that learns a unified representation by predicting, at any queried future point in spacetime, (1) general occupancy, capturing the evolving structure of the 3D scene; (2) ego occupancy, modeling the ego vehicle path through the environment; and (3) distilled high-level features from a vision foundation model. By modeling geometric and semantic 4D occupancy fields instead of raw sensor measurements, the model learns a structured, generalizable representation of the environment and its evolution through time. We validate GASP on multiple autonomous driving benchmarks, demonstrating significant improvements in semantic occupancy forecasting, online mapping, and ego trajectory prediction. Our results demonstrate that continuous 4D geometric and semantic occupancy prediction provides a scalable and effective pre-training paradigm for autonomous driving. For code and additional visualizations, see \\href{https://research.zenseact.com/publications/gasp/.', 'score': 0, 'issue_id': 2838, 'pub_date': '2025-03-19', 'pub_date_card': {'ru': '19 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 19', 'zh': '3æœˆ19æ—¥'}, 'hash': '94ae46d5f2ffc089', 'authors': ['William Ljungbergh', 'Adam Lilja', 'Adam Tonderski. Arvid Laveno Ling', 'Carl LindstrÃ¶m', 'Willem Verbeke', 'Junsheng Fu', 'Christoffer Petersson', 'Lars Hammarstrand', 'Michael Felsberg'], 'affiliations': ['Chalmers University of Technology', 'Linkoping University', 'Lund University', 'Zenseact'], 'pdf_title_img': 'assets/pdf/title_img/2503.15672.jpg', 'data': {'categories': ['#optimization', '#dataset', '#benchmark', '#3d', '#architecture', '#survey', '#multimodal'], 'emoji': 'ğŸš—', 'ru': {'title': 'ĞŸÑ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ 4D-Ğ·Ğ°Ğ½ÑÑ‚Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ¼ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ GASP - Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ. GASP Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ ĞµĞ´Ğ¸Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ, Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ Ğ±ÑƒĞ´ÑƒÑ‰ÑƒÑ Ğ¾Ğ±Ñ‰ÑƒÑ Ğ¸ ÑĞ³Ğ¾-Ğ·Ğ°Ğ½ÑÑ‚Ğ¾ÑÑ‚ÑŒ, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑƒÑ‡Ğ¸Ñ‚ÑÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾ĞºÑ€ÑƒĞ¶Ğ°ÑÑ‰ĞµĞ¹ ÑÑ€ĞµĞ´Ñ‹ Ğ¸ ĞµĞµ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸, Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒÑ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ 4D-Ğ¿Ğ¾Ğ»Ñ Ğ·Ğ°Ğ½ÑÑ‚Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ·Ğ°Ğ½ÑÑ‚Ğ¾ÑÑ‚Ğ¸, Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-ĞºĞ°Ñ€Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'GASP: Revolutionizing Autonomous Driving with 4D Occupancy Prediction', 'desc': 'This paper introduces GASP, a self-supervised pre-training method designed for autonomous driving applications. GASP predicts future occupancy in a 4D space, focusing on general occupancy, ego occupancy, and high-level features from a vision model. By utilizing geometric and semantic representations instead of raw sensor data, GASP creates a more structured understanding of the environment. The method shows significant improvements in tasks like semantic occupancy forecasting and ego trajectory prediction across various benchmarks.'}, 'zh': {'title': 'GASPï¼šè‡ªåŠ¨é©¾é©¶çš„è‡ªç›‘ç£é¢„è®­ç»ƒæ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§å‡ ä½•å’Œè¯­ä¹‰è‡ªç›‘ç£é¢„è®­ç»ƒæ–¹æ³•GASPï¼Œæ—¨åœ¨é€šè¿‡é¢„æµ‹æœªæ¥æ—¶ç©ºç‚¹çš„å ç”¨æƒ…å†µæ¥å­¦ä¹ ç¯å¢ƒçš„ç»Ÿä¸€è¡¨ç¤ºã€‚è¯¥æ–¹æ³•åŒ…æ‹¬å¯¹3Dåœºæ™¯çš„åŠ¨æ€ç»“æ„ã€è½¦è¾†åœ¨ç¯å¢ƒä¸­çš„è·¯å¾„ä»¥åŠä»è§†è§‰åŸºç¡€æ¨¡å‹æå–çš„é«˜çº§ç‰¹å¾è¿›è¡Œå»ºæ¨¡ã€‚é€šè¿‡å»ºæ¨¡å‡ ä½•å’Œè¯­ä¹‰çš„4Då ç”¨åœºï¼Œè€Œä¸æ˜¯åŸå§‹ä¼ æ„Ÿå™¨æ•°æ®ï¼Œæ¨¡å‹èƒ½å¤Ÿå­¦ä¹ åˆ°ç»“æ„åŒ–ä¸”å…·æœ‰å¯æ¨å¹¿æ€§çš„ç¯å¢ƒè¡¨ç¤ºã€‚æˆ‘ä»¬åœ¨å¤šä¸ªè‡ªåŠ¨é©¾é©¶åŸºå‡†æµ‹è¯•ä¸­éªŒè¯äº†GASPï¼Œæ˜¾ç¤ºå‡ºåœ¨è¯­ä¹‰å ç”¨é¢„æµ‹ã€åœ¨çº¿åœ°å›¾æ„å»ºå’Œè‡ªæˆ‘è½¨è¿¹é¢„æµ‹æ–¹é¢çš„æ˜¾è‘—æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.14201', 'title': 'Why Personalizing Deep Learning-Based Code Completion Tools Matters', 'url': 'https://huggingface.co/papers/2503.14201', 'abstract': "Deep learning (DL)-based code completion tools have transformed software development by enabling advanced code generation. These tools leverage models trained on vast amounts of code from numerous repositories, capturing general coding patterns. However, the impact of fine-tuning these models for specific organizations or developers to boost their performance on such subjects remains unexplored. In this work, we fill this gap by presenting solid empirical evidence answering this question. More specifically, we consider 136 developers from two organizations (Apache and Spring), two model architectures (T5 and Code Llama), and three model sizes (60M, 750M, and 7B trainable parameters). T5 models (60M, 750M) were pre-trained and fine-tuned on over 2,000 open-source projects, excluding the subject organizations' data, and compared against versions fine-tuned on organization- and developer-specific datasets. For the Code Llama model (7B), we compared the performance of the already pre-trained model publicly available online with the same model fine-tuned via parameter-efficient fine-tuning on organization- and developer-specific datasets. Our results show that there is a boost in prediction capabilities provided by both an organization-specific and a developer-specific additional fine-tuning, with the former being particularly performant. Such a finding generalizes across (i) the two subject organizations (i.e., Apache and Spring) and (ii) models of completely different magnitude (from 60M to 7B trainable parameters). Finally, we show that DL models fine-tuned on an organization-specific dataset achieve the same completion performance of pre-trained code models used out of the box and being sim10times larger, with consequent savings in terms of deployment and inference cost (e.g., smaller GPUs needed).", 'score': 0, 'issue_id': 2840, 'pub_date': '2025-03-18', 'pub_date_card': {'ru': '18 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 18', 'zh': '3æœˆ18æ—¥'}, 'hash': 'b244dc3db7a6fb65', 'authors': ['Alessandro Giagnorio', 'Alberto Martin-Lopez', 'Gabriele Bavota'], 'affiliations': ['UniversitÃ  della Svizzera italiana, Switzerland'], 'pdf_title_img': 'assets/pdf/title_img/2503.14201.jpg', 'data': {'categories': ['#transfer_learning', '#dataset', '#open_source', '#small_models', '#training', '#architecture'], 'emoji': 'ğŸš€', 'ru': {'title': 'Ğ”Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ´Ğ°: Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ²Ñ‹ÑˆĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ´Ğ° Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¹ Ğ¸Ğ»Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸ĞºĞ¾Ğ² Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ¸Ñ… ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ğ»Ğ¸ÑÑŒ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ T5 Ğ¸ Code Llama Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ² Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¾Ğ² Apache Ğ¸ Spring. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¹ Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞœĞ¾Ğ´ĞµĞ»Ğ¸, Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ğ°ĞºÑƒÑ Ğ¶Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ, ĞºĞ°Ğº Ğ¸ Ğ² 10 Ñ€Ğ°Ğ· Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‚ÑŒ Ñ€ĞµÑÑƒÑ€ÑÑ‹.'}, 'en': {'title': 'Boosting Code Completion with Tailored Fine-Tuning', 'desc': 'This paper investigates the effectiveness of fine-tuning deep learning models for code completion tailored to specific organizations and developers. It analyzes the performance of two model architectures, T5 and Code Llama, across different sizes and datasets. The study finds that fine-tuning on organization-specific and developer-specific datasets significantly enhances prediction capabilities, with organization-specific fine-tuning yielding the best results. Notably, models fine-tuned on smaller datasets can match the performance of larger pre-trained models, leading to cost savings in deployment and inference.'}, 'zh': {'title': 'å¾®è°ƒæ¨¡å‹ï¼Œæå‡ä»£ç è¡¥å…¨æ€§èƒ½ï¼', 'desc': 'åŸºäºæ·±åº¦å­¦ä¹ çš„ä»£ç è¡¥å…¨å·¥å…·é€šè¿‡å…ˆè¿›çš„ä»£ç ç”ŸæˆæŠ€æœ¯æ”¹å˜äº†è½¯ä»¶å¼€å‘ã€‚è¿™äº›å·¥å…·åˆ©ç”¨ä»å¤§é‡ä»£ç åº“ä¸­è®­ç»ƒçš„æ¨¡å‹ï¼Œæ•æ‰ä¸€èˆ¬çš„ç¼–ç æ¨¡å¼ã€‚ç„¶è€Œï¼Œé’ˆå¯¹ç‰¹å®šç»„ç»‡æˆ–å¼€å‘è€…å¯¹è¿™äº›æ¨¡å‹è¿›è¡Œå¾®è°ƒä»¥æé«˜æ€§èƒ½çš„å½±å“å°šæœªè¢«æ·±å…¥ç ”ç©¶ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œé’ˆå¯¹ç»„ç»‡å’Œå¼€å‘è€…çš„å¾®è°ƒå¯ä»¥æ˜¾è‘—æå‡æ¨¡å‹çš„é¢„æµ‹èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯ç»„ç»‡ç‰¹å®šçš„å¾®è°ƒæ•ˆæœæ›´ä¸ºæ˜¾è‘—ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.14456', 'title': 'RWKV-7 "Goose" with Expressive Dynamic State Evolution', 'url': 'https://huggingface.co/papers/2503.14456', 'abstract': 'We present RWKV-7 "Goose", a new sequence modeling architecture, along with pre-trained language models that establish a new state-of-the-art in downstream performance at the 3 billion parameter scale on multilingual tasks, and match current SoTA English language performance despite being trained on dramatically fewer tokens than other top 3B models. Nevertheless, RWKV-7 models require only constant memory usage and constant inference time per token. RWKV-7 introduces a newly generalized formulation of the delta rule with vector-valued gating and in-context learning rates, as well as a relaxed value replacement rule. We show that RWKV-7 can perform state tracking and recognize all regular languages, while retaining parallelizability of training. This exceeds the capabilities of Transformers under standard complexity conjectures, which are limited to TC^0. To demonstrate RWKV-7\'s language modeling capability, we also present an extended open source 3.1 trillion token multilingual corpus, and train four RWKV-7 models ranging from 0.19 billion to 2.9 billion parameters on this dataset.   To foster openness, reproduction, and adoption, we release our models and dataset component listing at https://huggingface.co/RWKV, and our training and inference code at https://github.com/RWKV/RWKV-LM all under the Apache 2.0 License.', 'score': 92, 'issue_id': 2776, 'pub_date': '2025-03-18', 'pub_date_card': {'ru': '18 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 18', 'zh': '3æœˆ18æ—¥'}, 'hash': '0cd796cef6fa6475', 'authors': ['Bo Peng', 'Ruichong Zhang', 'Daniel Goldstein', 'Eric Alcaide', 'Haowen Hou', 'Janna Lu', 'William Merrill', 'Guangyu Song', 'Kaifeng Tan', 'Saiteja Utpala', 'Nathan Wilce', 'Johan S. Wind', 'Tianyi Wu', 'Daniel Wuttke', 'Christian Zhou-Zheng'], 'affiliations': ['Beijing Normal University', 'Dalle Molle Institute for Artificial Intelligence USI-SUPSI', 'Denigma', 'EleutherAI', 'George Mason University', 'Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ)', 'New York University', 'RWKV Project (under Linux Foundation AI & Data)', 'Recursal AI', 'Shenzhen University', 'Tano Labs', 'Tsinghua University', 'University of Oslo'], 'pdf_title_img': 'assets/pdf/title_img/2503.14456.jpg', 'data': {'categories': ['#architecture', '#training', '#open_source', '#dataset', '#multilingual'], 'emoji': 'ğŸ¦¢', 'ru': {'title': 'RWKV-7: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹', 'desc': "RWKV-7 'Goose' - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ state-of-the-art Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ 3 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ½Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¿Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ½Ğ° Ñ‚Ğ¾ĞºĞµĞ½, Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ½ÑƒÑ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²ĞºÑƒ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ° Ğ´ĞµĞ»ÑŒÑ‚Ñ‹ Ñ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ğ¼ Ğ³ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ğ¾Ğ¼ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ. RWKV-7 ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ¸ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ²ÑĞµ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¸, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ ĞºĞ¾Ñ€Ğ¿ÑƒÑ Ğ¸Ğ· 3,1 Ñ‚Ñ€Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ Ğ½Ğ° Ğ½ĞµĞ¼ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ RWKV-7 Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ Ğ¾Ñ‚ 0,19 Ğ´Ğ¾ 2,9 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²."}, 'en': {'title': 'RWKV-7: Efficient Multilingual Mastery with Fewer Parameters', 'desc': 'RWKV-7 "Goose" is a novel sequence modeling architecture that achieves state-of-the-art performance on multilingual tasks with only 3 billion parameters. It utilizes a unique formulation of the delta rule and vector-valued gating, allowing for efficient memory usage and constant inference time per token. The model demonstrates advanced capabilities such as state tracking and recognition of regular languages, surpassing traditional Transformers in complexity. Additionally, RWKV-7 is trained on a large multilingual corpus and is made openly available for further research and development.'}, 'zh': {'title': 'RWKV-7ï¼šå¤šè¯­è¨€ä»»åŠ¡çš„æ–°çªç ´', 'desc': 'RWKV-7 "Goose" æ˜¯ä¸€ç§æ–°çš„åºåˆ—å»ºæ¨¡æ¶æ„ï¼Œå…·æœ‰3äº¿å‚æ•°çš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œåœ¨å¤šè¯­è¨€ä»»åŠ¡ä¸­è¾¾åˆ°äº†æ–°çš„æœ€å…ˆè¿›æ°´å¹³ã€‚ä¸å…¶ä»–é¡¶çº§3Bæ¨¡å‹ç›¸æ¯”ï¼ŒRWKV-7åœ¨è®­ç»ƒæ—¶ä½¿ç”¨çš„æ ‡è®°æ•°é‡æ˜¾è‘—å‡å°‘ï¼Œä½†ä»èƒ½ä¸å½“å‰çš„è‹±è¯­è¯­è¨€æ€§èƒ½ç›¸åŒ¹é…ã€‚è¯¥æ¨¡å‹é‡‡ç”¨äº†æ–°çš„å¹¿ä¹‰å¢é‡è§„åˆ™ï¼Œç»“åˆäº†å‘é‡å€¼é—¨æ§å’Œä¸Šä¸‹æ–‡å­¦ä¹ ç‡ï¼ŒåŒæ—¶ä¿æŒäº†è®­ç»ƒçš„å¹¶è¡Œæ€§ã€‚RWKV-7èƒ½å¤Ÿè¿›è¡ŒçŠ¶æ€è·Ÿè¸ªå¹¶è¯†åˆ«æ‰€æœ‰æ­£è§„è¯­è¨€ï¼Œè¶…è¶Šäº†æ ‡å‡†å¤æ‚æ€§çŒœæƒ³ä¸‹çš„Transformerçš„èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.14378', 'title': 'Impossible Videos', 'url': 'https://huggingface.co/papers/2503.14378', 'abstract': "Synthetic videos nowadays is widely used to complement data scarcity and diversity of real-world videos. Current synthetic datasets primarily replicate real-world scenarios, leaving impossible, counterfactual and anti-reality video concepts underexplored. This work aims to answer two questions: 1) Can today's video generation models effectively follow prompts to create impossible video content? 2) Are today's video understanding models good enough for understanding impossible videos? To this end, we introduce IPV-Bench, a novel benchmark designed to evaluate and foster progress in video understanding and generation. IPV-Bench is underpinned by a comprehensive taxonomy, encompassing 4 domains, 14 categories. It features diverse scenes that defy physical, biological, geographical, or social laws. Based on the taxonomy, a prompt suite is constructed to evaluate video generation models, challenging their prompt following and creativity capabilities. In addition, a video benchmark is curated to assess Video-LLMs on their ability of understanding impossible videos, which particularly requires reasoning on temporal dynamics and world knowledge. Comprehensive evaluations reveal limitations and insights for future directions of video models, paving the way for next-generation video models.", 'score': 47, 'issue_id': 2776, 'pub_date': '2025-03-18', 'pub_date_card': {'ru': '18 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 18', 'zh': '3æœˆ18æ—¥'}, 'hash': '3334aa74b743ac8d', 'authors': ['Zechen Bai', 'Hai Ci', 'Mike Zheng Shou'], 'affiliations': ['Show Lab, National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2503.14378.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#synthetic', '#video'], 'emoji': 'ğŸ¬', 'ru': {'title': 'IPV-Bench: ĞĞ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½ĞµĞ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ IPV-Bench - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ½ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ğ¸, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰ĞµĞ¹ 4 Ğ´Ğ¾Ğ¼ĞµĞ½Ğ° Ğ¸ 14 ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¹, Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ ÑÑ†ĞµĞ½Ñ‹, Ğ½Ğ°Ñ€ÑƒÑˆĞ°ÑÑ‰Ğ¸Ğµ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ, Ğ±Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ, Ğ³ĞµĞ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸Ğ»Ğ¸ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°ĞºĞ¾Ğ½Ñ‹. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Video-LLM Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ½ĞµĞ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞšĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¸Ğ´ĞµĞ¸ Ğ´Ğ»Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Exploring the Impossible: Advancing Video Generation and Understanding', 'desc': 'This paper addresses the limitations of current synthetic video datasets, which mainly focus on replicating real-world scenarios. It introduces IPV-Bench, a new benchmark designed to evaluate video generation and understanding models on their ability to create and comprehend impossible video content. The benchmark includes a detailed taxonomy with various categories that challenge models to generate videos that defy physical and social laws. The findings highlight the current capabilities and shortcomings of video models, providing insights for future advancements in the field.'}, 'zh': {'title': 'æ¢ç´¢ä¸å¯èƒ½è§†é¢‘çš„ç”Ÿæˆä¸ç†è§£', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†åˆæˆè§†é¢‘åœ¨æ•°æ®ç¨€ç¼ºå’Œå¤šæ ·æ€§æ–¹é¢çš„åº”ç”¨ã€‚å½“å‰çš„åˆæˆæ•°æ®é›†ä¸»è¦å¤åˆ¶ç°å®åœºæ™¯ï¼Œè€Œå¯¹ä¸å¯èƒ½ã€åäº‹å®å’Œåç°å®çš„è§†é¢‘æ¦‚å¿µç ”ç©¶ä¸è¶³ã€‚æˆ‘ä»¬æå‡ºäº†IPV-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°å’Œä¿ƒè¿›è§†é¢‘ç†è§£ä¸ç”Ÿæˆçš„è¿›å±•ã€‚è¯¥åŸºå‡†æ¶µç›–äº†å¤šç§è¿åç‰©ç†ã€ç”Ÿç‰©ã€åœ°ç†æˆ–ç¤¾ä¼šæ³•åˆ™çš„åœºæ™¯ï¼Œå¹¶é€šè¿‡æ„å»ºæç¤ºå¥—ä»¶æ¥æŒ‘æˆ˜è§†é¢‘ç”Ÿæˆæ¨¡å‹çš„åˆ›é€ åŠ›å’Œæç¤ºè·Ÿéšèƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.14476', 'title': 'DAPO: An Open-Source LLM Reinforcement Learning System at Scale', 'url': 'https://huggingface.co/papers/2503.14476', 'abstract': 'Inference scaling empowers LLMs with unprecedented reasoning ability, with reinforcement learning as the core technique to elicit complex reasoning. However, key technical details of state-of-the-art reasoning LLMs are concealed (such as in OpenAI o1 blog and DeepSeek R1 technical report), thus the community still struggles to reproduce their RL training results. We propose the Decoupled Clip and Dynamic sAmpling Policy Optimization (DAPO) algorithm, and fully open-source a state-of-the-art large-scale RL system that achieves 50 points on AIME 2024 using Qwen2.5-32B base model. Unlike previous works that withhold training details, we introduce four key techniques of our algorithm that make large-scale LLM RL a success. In addition, we open-source our training code, which is built on the verl framework, along with a carefully curated and processed dataset. These components of our open-source system enhance reproducibility and support future research in large-scale LLM RL.', 'score': 42, 'issue_id': 2777, 'pub_date': '2025-03-18', 'pub_date_card': {'ru': '18 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 18', 'zh': '3æœˆ18æ—¥'}, 'hash': '5b4841d2845817e8', 'authors': ['Qiying Yu', 'Zheng Zhang', 'Ruofei Zhu', 'Yufeng Yuan', 'Xiaochen Zuo', 'Yu Yue', 'Tiantian Fan', 'Gaohong Liu', 'Lingjun Liu', 'Xin Liu', 'Haibin Lin', 'Zhiqi Lin', 'Bole Ma', 'Guangming Sheng', 'Yuxuan Tong', 'Chi Zhang', 'Mofan Zhang', 'Wang Zhang', 'Hang Zhu', 'Jinhua Zhu', 'Jiaze Chen', 'Jiangjie Chen', 'Chengyi Wang', 'Hongli Yu', 'Weinan Dai', 'Yuxuan Song', 'Xiangpeng Wei', 'Hao Zhou', 'Jingjing Liu', 'Wei-Ying Ma', 'Ya-Qin Zhang', 'Lin Yan', 'Mu Qiao', 'Yonghui Wu', 'Mingxuan Wang'], 'affiliations': ['ByteDance', 'Institute for AI Industry Research (AIR), Tsinghua University', 'SIA-Lab of Tsinghua AIR and ByteDance', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2503.14476.jpg', 'data': {'categories': ['#rl', '#dataset', '#reasoning', '#optimization', '#training', '#open_source'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞÑ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° RL Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ DAPO Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºÑƒÑÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‰ÑƒÑ 50 Ğ±Ğ°Ğ»Ğ»Ğ¾Ğ² Ğ½Ğ° AIME 2024 Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Qwen2.5-32B. Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°ÑÑ‚ÑÑ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸, Ğ´ĞµĞ»Ğ°ÑÑ‰Ğ¸Ğµ ÑƒÑĞ¿ĞµÑˆĞ½Ñ‹Ğ¼ RL Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¯Ğœ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ ĞºĞ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Unlocking LLM Potential with Open-Source Reinforcement Learning', 'desc': 'This paper introduces the Decoupled Clip and Dynamic sAmpling Policy Optimization (DAPO) algorithm, which enhances the reasoning capabilities of large language models (LLMs) through reinforcement learning (RL). The authors address the lack of transparency in existing state-of-the-art LLMs by providing detailed insights into their training processes and techniques. They report achieving a significant performance score of 50 points on the AIME 2024 benchmark using the Qwen2.5-32B model. By open-sourcing their training code and dataset, they aim to improve reproducibility and foster further advancements in large-scale LLM reinforcement learning research.'}, 'zh': {'title': 'è§£é”å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†æ½œåŠ›', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„ç®—æ³•ï¼Œç§°ä¸ºè§£è€¦å‰ªè¾‘å’ŒåŠ¨æ€é‡‡æ ·ç­–ç•¥ä¼˜åŒ–ï¼ˆDAPOï¼‰ï¼Œæ—¨åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬é€šè¿‡å¼ºåŒ–å­¦ä¹ æŠ€æœ¯ï¼ŒæˆåŠŸå®ç°äº†åœ¨AIME 2024ä¸Šè·å¾—50åˆ†çš„æˆç»©ï¼Œä½¿ç”¨çš„æ˜¯Qwen2.5-32BåŸºç¡€æ¨¡å‹ã€‚ä¸ä»¥å¾€ç ”ç©¶ä¸åŒï¼Œæˆ‘ä»¬å…¬å¼€äº†å››ä¸ªå…³é”®æŠ€æœ¯ç»†èŠ‚ï¼Œå¸®åŠ©ç¤¾åŒºæ›´å¥½åœ°ç†è§£å’Œå¤ç°æˆ‘ä»¬çš„è®­ç»ƒç»“æœã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼€æºäº†è®­ç»ƒä»£ç å’Œç»è¿‡ç²¾å¿ƒå¤„ç†çš„æ•°æ®é›†ï¼Œä»¥ä¿ƒè¿›å¤§å‹LLMå¼ºåŒ–å­¦ä¹ çš„å¯é‡å¤æ€§å’Œæœªæ¥ç ”ç©¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.14478', 'title': 'Creation-MMBench: Assessing Context-Aware Creative Intelligence in MLLM', 'url': 'https://huggingface.co/papers/2503.14478', 'abstract': "Creativity is a fundamental aspect of intelligence, involving the ability to generate novel and appropriate solutions across diverse contexts. While Large Language Models (LLMs) have been extensively evaluated for their creative capabilities, the assessment of Multimodal Large Language Models (MLLMs) in this domain remains largely unexplored. To address this gap, we introduce Creation-MMBench, a multimodal benchmark specifically designed to evaluate the creative capabilities of MLLMs in real-world, image-based tasks. The benchmark comprises 765 test cases spanning 51 fine-grained tasks. To ensure rigorous evaluation, we define instance-specific evaluation criteria for each test case, guiding the assessment of both general response quality and factual consistency with visual inputs. Experimental results reveal that current open-source MLLMs significantly underperform compared to proprietary models in creative tasks. Furthermore, our analysis demonstrates that visual fine-tuning can negatively impact the base LLM's creative abilities. Creation-MMBench provides valuable insights for advancing MLLM creativity and establishes a foundation for future improvements in multimodal generative intelligence. Full data and evaluation code is released on https://github.com/open-compass/Creation-MMBench.", 'score': 37, 'issue_id': 2777, 'pub_date': '2025-03-18', 'pub_date_card': {'ru': '18 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 18', 'zh': '3æœˆ18æ—¥'}, 'hash': '120f1d8ec2eb88a8', 'authors': ['Xinyu Fang', 'Zhijian Chen', 'Kai Lan', 'Shengyuan Ding', 'Yingji Liang', 'Xiangyu Zhao', 'Farong Wen', 'Zicheng Zhang', 'Guofeng Zhang', 'Haodong Duan', 'Kai Chen', 'Dahua Lin'], 'affiliations': ['East China Normal University', 'Nanjing University', 'Shanghai AI Laboratory', 'Shanghai Jiaotong University', 'The Chinese University of Hong Kong', 'Tongji University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.14478.jpg', 'data': {'categories': ['#creativity', '#open_source', '#benchmark', '#multimodal'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ˜Ğ·Ğ¼ĞµÑ€ÑÑ Ñ‚Ğ²Ğ¾Ñ€Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Creation-MMBench - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ‚Ğ²Ğ¾Ñ€Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 765 Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ², Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… 51 Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ, Ñ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸ÑĞ¼Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ÑĞ»ÑƒÑ‡Ğ°Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ MLLM Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒÑÑ‚ÑƒĞ¿Ğ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ² Ñ‚Ğ²Ğ¾Ñ€Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ½ĞµĞ³Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ²Ğ»Ğ¸ÑÑ‚ÑŒ Ğ½Ğ° Ñ‚Ğ²Ğ¾Ñ€Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Unlocking Creativity in Multimodal AI with Creation-MMBench', 'desc': 'This paper introduces Creation-MMBench, a new benchmark designed to evaluate the creative capabilities of Multimodal Large Language Models (MLLMs) in tasks that involve both text and images. The benchmark includes 765 test cases across 51 specific tasks, each with tailored evaluation criteria to assess the quality of responses and their consistency with visual inputs. Experimental findings indicate that current open-source MLLMs perform poorly in creative tasks compared to proprietary models, and that visual fine-tuning may hinder the creative performance of base LLMs. Creation-MMBench aims to enhance understanding and development of MLLM creativity, providing a resource for future research in multimodal generative intelligence.'}, 'zh': {'title': 'è¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„åˆ›é€ åŠ›', 'desc': 'åˆ›é€ åŠ›æ˜¯æ™ºèƒ½çš„ä¸€ä¸ªåŸºæœ¬æ–¹é¢ï¼Œæ¶‰åŠåœ¨ä¸åŒæƒ…å¢ƒä¸­ç”Ÿæˆæ–°é¢–ä¸”é€‚å½“çš„è§£å†³æ–¹æ¡ˆã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨åˆ›é€ èƒ½åŠ›æ–¹é¢å¾—åˆ°äº†å¹¿æ³›è¯„ä¼°ï¼Œä½†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„è¯„ä¼°ä»ç„¶ç›¸å¯¹ç¼ºä¹ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†Creation-MMBenchï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨è®¾è®¡ç”¨äºè¯„ä¼°MLLMsåœ¨åŸºäºå›¾åƒçš„å®é™…ä»»åŠ¡ä¸­åˆ›é€ èƒ½åŠ›çš„å¤šæ¨¡æ€åŸºå‡†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå½“å‰çš„å¼€æºMLLMsåœ¨åˆ›é€ æ€§ä»»åŠ¡ä¸­æ˜¾è‘—ä½äºä¸“æœ‰æ¨¡å‹ï¼Œè€Œè§†è§‰å¾®è°ƒå¯èƒ½ä¼šå¯¹åŸºç¡€LLMçš„åˆ›é€ èƒ½åŠ›äº§ç”Ÿè´Ÿé¢å½±å“ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.12797', 'title': 'DeepPerception: Advancing R1-like Cognitive Visual Perception in MLLMs\n  for Knowledge-Intensive Visual Grounding', 'url': 'https://huggingface.co/papers/2503.12797', 'abstract': 'Human experts excel at fine-grained visual discrimination by leveraging domain knowledge to refine perceptual features, a capability that remains underdeveloped in current Multimodal Large Language Models (MLLMs). Despite possessing vast expert-level knowledge, MLLMs struggle to integrate reasoning into visual perception, often generating direct responses without deeper analysis. To bridge this gap, we introduce knowledge-intensive visual grounding (KVG), a novel visual grounding task that requires both fine-grained perception and domain-specific knowledge integration. To address the challenges of KVG, we propose DeepPerception, an MLLM enhanced with cognitive visual perception capabilities. Our approach consists of (1) an automated data synthesis pipeline that generates high-quality, knowledge-aligned training samples, and (2) a two-stage training framework combining supervised fine-tuning for cognitive reasoning scaffolding and reinforcement learning to optimize perception-cognition synergy. To benchmark performance, we introduce KVG-Bench a comprehensive dataset spanning 10 domains with 1.3K manually curated test cases. Experimental results demonstrate that DeepPerception significantly outperforms direct fine-tuning, achieving +8.08\\% accuracy improvements on KVG-Bench and exhibiting +4.60\\% superior cross-domain generalization over baseline approaches. Our findings highlight the importance of integrating cognitive processes into MLLMs for human-like visual perception and open new directions for multimodal reasoning research. The data, codes, and models are released at https://github.com/thunlp/DeepPerception.', 'score': 24, 'issue_id': 2777, 'pub_date': '2025-03-17', 'pub_date_card': {'ru': '17 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 17', 'zh': '3æœˆ17æ—¥'}, 'hash': 'c682a086aaac0fa1', 'authors': ['Xinyu Ma', 'Ziyang Ding', 'Zhicong Luo', 'Chi Chen', 'Zonghao Guo', 'Derek F. Wong', 'Xiaoyi Feng', 'Maosong Sun'], 'affiliations': ['Northwestern Polytechnical University', 'Shandong University', 'Tsinghua University', 'University of Macau'], 'pdf_title_img': 'assets/pdf/title_img/2503.12797.jpg', 'data': {'categories': ['#multimodal', '#data', '#dataset', '#reasoning', '#synthetic', '#benchmark', '#transfer_learning', '#training'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'DeepPerception: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ˜Ğ˜ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ knowledge-intensive visual grounding (KVG), Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰ÑƒÑ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. ĞŸÑ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ DeepPerception, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ MLLM. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ KVG-Bench Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ĞµĞ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Enhancing Visual Perception in MLLMs with DeepPerception', 'desc': 'This paper addresses the limitations of Multimodal Large Language Models (MLLMs) in visual perception and reasoning. It introduces a new task called knowledge-intensive visual grounding (KVG), which combines fine-grained visual discrimination with domain-specific knowledge. The authors propose DeepPerception, an enhanced MLLM that utilizes a data synthesis pipeline and a two-stage training framework to improve cognitive reasoning and perception integration. Experimental results show that DeepPerception outperforms existing methods, demonstrating the potential for MLLMs to achieve human-like visual understanding.'}, 'zh': {'title': 'æå‡è§†è§‰æ„ŸçŸ¥çš„è®¤çŸ¥æ•´åˆèƒ½åŠ›', 'desc': 'äººç±»ä¸“å®¶åœ¨ç»†ç²’åº¦è§†è§‰è¾¨åˆ«æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿåˆ©ç”¨é¢†åŸŸçŸ¥è¯†æ¥ä¼˜åŒ–æ„ŸçŸ¥ç‰¹å¾ï¼Œè€Œå½“å‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è¿™æ–¹é¢ä»æ˜¾ä¸è¶³ã€‚å°½ç®¡æ‹¥æœ‰ä¸°å¯Œçš„ä¸“å®¶çº§çŸ¥è¯†ï¼ŒMLLMsåœ¨è§†è§‰æ„ŸçŸ¥ä¸­æ•´åˆæ¨ç†çš„èƒ½åŠ›è¾ƒå¼±ï¼Œå¸¸å¸¸ç›´æ¥ç”Ÿæˆå“åº”è€Œç¼ºä¹æ·±å…¥åˆ†æã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†çŸ¥è¯†å¯†é›†å‹è§†è§‰å®šä½ï¼ˆKVGï¼‰ï¼Œè¿™æ˜¯ä¸€é¡¹æ–°é¢–çš„è§†è§‰å®šä½ä»»åŠ¡ï¼Œè¦æ±‚åŒæ—¶å…·å¤‡ç»†ç²’åº¦æ„ŸçŸ¥å’Œé¢†åŸŸç‰¹å®šçŸ¥è¯†çš„æ•´åˆã€‚æˆ‘ä»¬æå‡ºçš„DeepPerceptionæ¨¡å‹å¢å¼ºäº†è®¤çŸ¥è§†è§‰æ„ŸçŸ¥èƒ½åŠ›ï¼Œé€šè¿‡è‡ªåŠ¨åŒ–æ•°æ®åˆæˆå’Œä¸¤é˜¶æ®µè®­ç»ƒæ¡†æ¶ï¼Œæ˜¾è‘—æé«˜äº†åœ¨KVG-Benchæ•°æ®é›†ä¸Šçš„å‡†ç¡®æ€§å’Œè·¨é¢†åŸŸæ³›åŒ–èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.12329', 'title': 'CapArena: Benchmarking and Analyzing Detailed Image Captioning in the\n  LLM Era', 'url': 'https://huggingface.co/papers/2503.12329', 'abstract': 'Image captioning has been a longstanding challenge in vision-language research. With the rise of LLMs, modern Vision-Language Models (VLMs) generate detailed and comprehensive image descriptions. However, benchmarking the quality of such captions remains unresolved. This paper addresses two key questions: (1) How well do current VLMs actually perform on image captioning, particularly compared to humans? We built CapArena, a platform with over 6000 pairwise caption battles and high-quality human preference votes. Our arena-style evaluation marks a milestone, showing that leading models like GPT-4o achieve or even surpass human performance, while most open-source models lag behind. (2) Can automated metrics reliably assess detailed caption quality? Using human annotations from CapArena, we evaluate traditional and recent captioning metrics, as well as VLM-as-a-Judge. Our analysis reveals that while some metrics (e.g., METEOR) show decent caption-level agreement with humans, their systematic biases lead to inconsistencies in model ranking. In contrast, VLM-as-a-Judge demonstrates robust discernment at both the caption and model levels. Building on these insights, we release CapArena-Auto, an accurate and efficient automated benchmark for detailed captioning, achieving 94.3% correlation with human rankings at just $4 per test. Data and resources will be open-sourced at https://caparena.github.io.', 'score': 19, 'issue_id': 2778, 'pub_date': '2025-03-16', 'pub_date_card': {'ru': '16 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 16', 'zh': '3æœˆ16æ—¥'}, 'hash': 'c97a8c730bfcbfa8', 'authors': ['Kanzhi Cheng', 'Wenpo Song', 'Jiaxin Fan', 'Zheng Ma', 'Qiushi Sun', 'Fangzhi Xu', 'Chenyang Yan', 'Nuo Chen', 'Jianbing Zhang', 'Jiajun Chen'], 'affiliations': ['National Key Laboratory for Novel Software Technology, Nanjing University', 'Shanghai Artificial Intelligence Laboratory', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2503.12329.jpg', 'data': {'categories': ['#benchmark', '#games', '#open_source', '#cv'], 'emoji': 'ğŸ“¸', 'ru': {'title': 'ĞĞ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞµĞ¹ Ğº Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼: Ğ˜Ğ˜ Ğ´Ğ¾Ğ³Ğ¾Ğ½ÑĞµÑ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞµĞ¹ Ğº Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° (VLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ñƒ CapArena Ğ´Ğ»Ñ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞµĞ¹, ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¸ Ğ»ÑĞ´ÑŒĞ¼Ğ¸, Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ², Ñ‡Ñ‚Ğ¾ Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, GPT-4o) Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ¸Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞµĞ¹, Ğ²Ñ‹ÑĞ²Ğ»ÑÑ Ğ¸Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ CapArena-Auto - Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞµĞ¹ Ğº Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼.'}, 'en': {'title': 'Elevating Image Captioning: Human-Level Performance and Reliable Metrics', 'desc': 'This paper tackles the challenge of evaluating image captioning performance in Vision-Language Models (VLMs). It introduces CapArena, a platform that conducts over 6000 pairwise caption battles to compare VLM outputs with human-generated captions. The findings indicate that advanced models like GPT-4o can match or exceed human performance, while many open-source models do not. Additionally, the study assesses various automated metrics for caption quality, revealing that while some metrics align with human preferences, VLM-as-a-Judge provides a more reliable evaluation method, leading to the development of CapArena-Auto for efficient benchmarking.'}, 'zh': {'title': 'å›¾åƒæè¿°çš„æ–°æ ‡å‡†ï¼šVLMçš„å´›èµ·ä¸è¯„ä¼°', 'desc': 'å›¾åƒæè¿°ä¸€ç›´æ˜¯è§†è§‰ä¸è¯­è¨€ç ”ç©¶ä¸­çš„ä¸€ä¸ªé‡è¦æŒ‘æˆ˜ã€‚éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å‘å±•ï¼Œç°ä»£è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰èƒ½å¤Ÿç”Ÿæˆè¯¦ç»†ä¸”å…¨é¢çš„å›¾åƒæè¿°ã€‚æœ¬æ–‡é€šè¿‡å»ºç«‹CapArenaå¹³å°ï¼Œè¯„ä¼°å½“å‰VLMåœ¨å›¾åƒæè¿°ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œå‘ç°é¢†å…ˆæ¨¡å‹å¦‚GPT-4oçš„è¡¨ç°ç”šè‡³è¶…è¿‡äº†äººç±»ã€‚æˆ‘ä»¬è¿˜åˆ†æäº†è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡çš„å¯é æ€§ï¼Œç»“æœè¡¨æ˜VLMä½œä¸ºè¯„åˆ¤è€…åœ¨æè¿°è´¨é‡è¯„ä¼°ä¸­è¡¨ç°å‡ºè‰²ï¼Œæä¾›äº†ä¸€ç§æ–°çš„é«˜æ•ˆåŸºå‡†æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.13424', 'title': 'Infinite Mobility: Scalable High-Fidelity Synthesis of Articulated\n  Objects via Procedural Generation', 'url': 'https://huggingface.co/papers/2503.13424', 'abstract': 'Large-scale articulated objects with high quality are desperately needed for multiple tasks related to embodied AI. Most existing methods for creating articulated objects are either data-driven or simulation based, which are limited by the scale and quality of the training data or the fidelity and heavy labour of the simulation. In this paper, we propose Infinite Mobility, a novel method for synthesizing high-fidelity articulated objects through procedural generation. User study and quantitative evaluation demonstrate that our method can produce results that excel current state-of-the-art methods and are comparable to human-annotated datasets in both physics property and mesh quality. Furthermore, we show that our synthetic data can be used as training data for generative models, enabling next-step scaling up. Code is available at https://github.com/Intern-Nexus/Infinite-Mobility', 'score': 18, 'issue_id': 2776, 'pub_date': '2025-03-17', 'pub_date_card': {'ru': '17 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 17', 'zh': '3æœˆ17æ—¥'}, 'hash': '20793013b58dba36', 'authors': ['Xinyu Lian', 'Zichao Yu', 'Ruiming Liang', 'Yitong Wang', 'Li Ray Luo', 'Kaixu Chen', 'Yuanzhen Zhou', 'Qihong Tang', 'Xudong Xu', 'Zhaoyang Lyu', 'Bo Dai', 'Jiangmiao Pang'], 'affiliations': ['Fudan University', 'Harbin Institute of Technology, Shenzhen', 'Institute of Automation, Chinese Academy of Sciences', 'School of Artificial Intelligence, University of Chinese Academy of Sciences', 'Shanghai Artificial Intelligence Laboratory', 'South China University of Technology', 'The University of Hong Kong', 'Tongji University', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2503.13424.jpg', 'data': {'categories': ['#3d', '#open_source', '#dataset', '#synthetic'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞŸÑ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ÑĞ¾Ñ‡Ğ»ĞµĞ½ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Infinite Mobility Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ¾Ñ‡Ğ»ĞµĞ½ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¸ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼ Ñ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ğ¼Ğ¸, Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ñ€ÑƒÑ‡Ğ½ÑƒÑ, Ğ¿Ğ¾ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ğ¼ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ¿Ğ¾Ğ»Ğ¸Ğ³Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞµÑ‚Ğ¾Ğº. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ»Ğ¸ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸. Ğ¡Ğ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Infinite Mobility, Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ñ‡Ñ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'Revolutionizing Articulated Object Creation with Infinite Mobility', 'desc': 'This paper introduces Infinite Mobility, a new approach for creating high-quality articulated objects using procedural generation techniques. Unlike traditional methods that rely on limited data or labor-intensive simulations, Infinite Mobility generates objects that maintain high fidelity in both physical properties and mesh quality. The authors conducted user studies and quantitative evaluations, showing that their method outperforms existing state-of-the-art techniques and rivals human-annotated datasets. Additionally, the synthetic data produced can be utilized for training generative models, facilitating further advancements in the field of embodied AI.'}, 'zh': {'title': 'æ— é™ç§»åŠ¨ï¼šåˆæˆé«˜ä¿çœŸå…³èŠ‚ç‰©ä½“çš„æ–°æ–¹æ³•', 'desc': 'åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºæ— é™ç§»åŠ¨ï¼ˆInfinite Mobilityï¼‰çš„æ–¹æ³•ï¼Œç”¨äºé€šè¿‡ç¨‹åºç”Ÿæˆåˆæˆé«˜ä¿çœŸåº¦çš„å…³èŠ‚ç‰©ä½“ã€‚è¿™ç§æ–¹æ³•å…‹æœäº†ç°æœ‰æ•°æ®é©±åŠ¨æˆ–æ¨¡æ‹Ÿæ–¹æ³•åœ¨è§„æ¨¡å’Œè´¨é‡ä¸Šçš„é™åˆ¶ã€‚ç”¨æˆ·ç ”ç©¶å’Œå®šé‡è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç‰©ç†å±æ€§å’Œç½‘æ ¼è´¨é‡ä¸Šè¶…è¶Šäº†å½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå¹¶ä¸”ä¸äººå·¥æ ‡æ³¨çš„æ•°æ®é›†ç›¸å½“ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„åˆæˆæ•°æ®å¯ä»¥ä½œä¸ºç”Ÿæˆæ¨¡å‹çš„è®­ç»ƒæ•°æ®ï¼Œæ”¯æŒåç»­çš„æ‰©å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.14125', 'title': 'Frac-Connections: Fractional Extension of Hyper-Connections', 'url': 'https://huggingface.co/papers/2503.14125', 'abstract': 'Residual connections are central to modern deep learning architectures, enabling the training of very deep networks by mitigating gradient vanishing. Hyper-Connections recently generalized residual connections by introducing multiple connection strengths at different depths, thereby addressing the seesaw effect between gradient vanishing and representation collapse. However, Hyper-Connections increase memory access costs by expanding the width of hidden states. In this paper, we propose Frac-Connections, a novel approach that divides hidden states into multiple parts rather than expanding their width. Frac-Connections retain partial benefits of Hyper-Connections while reducing memory consumption. To validate their effectiveness, we conduct large-scale experiments on language tasks, with the largest being a 7B MoE model trained on up to 3T tokens, demonstrating that Frac-Connections significantly outperform residual connections.', 'score': 13, 'issue_id': 2776, 'pub_date': '2025-03-18', 'pub_date_card': {'ru': '18 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 18', 'zh': '3æœˆ18æ—¥'}, 'hash': '936ea0aa4972c382', 'authors': ['Defa Zhu', 'Hongzhi Huang', 'Jundong Zhou', 'Zihao Huang', 'Yutao Zeng', 'Banggu Wu', 'Qiyang Min', 'Xun Zhou'], 'affiliations': ['ByteDance Seed'], 'pdf_title_img': 'assets/pdf/title_img/2503.14125.jpg', 'data': {'categories': ['#optimization', '#architecture', '#training'], 'emoji': 'ğŸ§©', 'ru': {'title': 'Frac-Connections: Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ¹ Ğ±ĞµĞ· Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Frac-Connections. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¸Ğ´ĞµÑ Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑĞ¾ĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğ¹ (residual connections), Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑÑ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ğ°ÑÑ‚ĞµĞ¹ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ Ğ¸Ñ… ÑˆĞ¸Ñ€Ğ¸Ğ½Ñ‹. Frac-Connections ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑÑ‚ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Hyper-Connections, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑĞ½Ğ¸Ğ¶Ğ°Ñ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ° Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¼Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ½Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ MoE Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ 7 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ½Ğ° 3 Ñ‚Ñ€Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ².'}, 'en': {'title': 'Frac-Connections: Efficient Memory for Deep Learning', 'desc': 'This paper introduces Frac-Connections, a new method that improves upon Hyper-Connections in deep learning. While Hyper-Connections allow for multiple connection strengths to combat issues like gradient vanishing, they also increase memory usage due to wider hidden states. Frac-Connections address this by splitting hidden states into smaller parts, maintaining some advantages of Hyper-Connections while reducing memory costs. The authors demonstrate the effectiveness of Frac-Connections through extensive experiments on language tasks, showing superior performance compared to traditional residual connections.'}, 'zh': {'title': 'Frac-Connectionsï¼šä¼˜åŒ–æ·±åº¦å­¦ä¹ çš„å†…å­˜ä½¿ç”¨', 'desc': 'æ®‹å·®è¿æ¥æ˜¯ç°ä»£æ·±åº¦å­¦ä¹ æ¶æ„çš„æ ¸å¿ƒï¼Œèƒ½å¤Ÿé€šè¿‡å‡è½»æ¢¯åº¦æ¶ˆå¤±é—®é¢˜æ¥è®­ç»ƒéå¸¸æ·±çš„ç½‘ç»œã€‚è¶…è¿æ¥æœ€è¿‘é€šè¿‡åœ¨ä¸åŒæ·±åº¦å¼•å…¥å¤šä¸ªè¿æ¥å¼ºåº¦æ¥æ¨å¹¿æ®‹å·®è¿æ¥ï¼Œä»è€Œè§£å†³äº†æ¢¯åº¦æ¶ˆå¤±ä¸è¡¨ç¤ºå´©æºƒä¹‹é—´çš„æ‘‡æ‘†æ•ˆåº”ã€‚ç„¶è€Œï¼Œè¶…è¿æ¥é€šè¿‡æ‰©å±•éšè—çŠ¶æ€çš„å®½åº¦å¢åŠ äº†å†…å­˜è®¿é—®æˆæœ¬ã€‚æˆ‘ä»¬æå‡ºäº†Frac-Connectionsï¼Œè¿™æ˜¯ä¸€ç§æ–°æ–¹æ³•ï¼Œé€šè¿‡å°†éšè—çŠ¶æ€åˆ’åˆ†ä¸ºå¤šä¸ªéƒ¨åˆ†è€Œä¸æ˜¯æ‰©å±•å…¶å®½åº¦ï¼Œä¿ç•™äº†è¶…è¿æ¥çš„éƒ¨åˆ†ä¼˜åŠ¿ï¼ŒåŒæ—¶å‡å°‘äº†å†…å­˜æ¶ˆè€—ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.14492', 'title': 'Cosmos-Transfer1: Conditional World Generation with Adaptive Multimodal\n  Control', 'url': 'https://huggingface.co/papers/2503.14492', 'abstract': 'We introduce Cosmos-Transfer, a conditional world generation model that can generate world simulations based on multiple spatial control inputs of various modalities such as segmentation, depth, and edge. In the design, the spatial conditional scheme is adaptive and customizable. It allows weighting different conditional inputs differently at different spatial locations. This enables highly controllable world generation and finds use in various world-to-world transfer use cases, including Sim2Real. We conduct extensive evaluations to analyze the proposed model and demonstrate its applications for Physical AI, including robotics Sim2Real and autonomous vehicle data enrichment. We further demonstrate an inference scaling strategy to achieve real-time world generation with an NVIDIA GB200 NVL72 rack. To help accelerate research development in the field, we open-source our models and code at https://github.com/nvidia-cosmos/cosmos-transfer1.', 'score': 12, 'issue_id': 2777, 'pub_date': '2025-03-18', 'pub_date_card': {'ru': '18 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 18', 'zh': '3æœˆ18æ—¥'}, 'hash': '5098c043161888fa', 'authors': ['NVIDIA', ':', 'Hassan Abu Alhaija', 'Jose Alvarez', 'Maciej Bala', 'Tiffany Cai', 'Tianshi Cao', 'Liz Cha', 'Joshua Chen', 'Mike Chen', 'Francesco Ferroni', 'Sanja Fidler', 'Dieter Fox', 'Yunhao Ge', 'Jinwei Gu', 'Ali Hassani', 'Michael Isaev', 'Pooya Jannaty', 'Shiyi Lan', 'Tobias Lasser', 'Huan Ling', 'Ming-Yu Liu', 'Xian Liu', 'Yifan Lu', 'Alice Luo', 'Qianli Ma', 'Hanzi Mao', 'Fabio Ramos', 'Xuanchi Ren', 'Tianchang Shen', 'Shitao Tang', 'Ting-Chun Wang', 'Jay Wu', 'Jiashu Xu', 'Stella Xu', 'Kevin Xie', 'Yuchong Ye', 'Xiaodong Yang', 'Xiaohui Zeng', 'Yu Zeng'], 'affiliations': ['NVIDIA'], 'pdf_title_img': 'assets/pdf/title_img/2503.14492.jpg', 'data': {'categories': ['#agents', '#robotics', '#transfer_learning', '#open_source', '#inference', '#3d'], 'emoji': 'ğŸŒŒ', 'ru': {'title': 'ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ¸Ñ€Ğ¾Ğ² Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼', 'desc': 'Cosmos-Transfer - ÑÑ‚Ğ¾ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¸Ñ€Ğ¾Ğ², ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ°Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³Ğ¸Ğ±ĞºĞ¾ Ğ½Ğ°ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ²ĞµÑĞ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ²Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ² Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ»Ğ¾ĞºĞ°Ñ†Ğ¸ÑÑ…. Ğ­Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ¸Ñ€Ğ¾Ğ² Ğ¸ Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¸Ñ€Ğ°Ğ¼Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Sim2Real. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ñ‹Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ ĞµĞµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ˜Ğ˜, Ğ² Ñ‚Ğ¾Ğ¼ Ñ‡Ğ¸ÑĞ»Ğµ Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Sim2Real Ğ¸ Ğ¾Ğ±Ğ¾Ğ³Ğ°Ñ‰ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ±ĞµÑĞ¿Ğ¸Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ¾Ğ±Ğ¸Ğ»ĞµĞ¹.'}, 'en': {'title': 'Empowering World Generation with Adaptive Control', 'desc': 'Cosmos-Transfer is a novel model designed for generating world simulations using various spatial control inputs like segmentation, depth, and edge maps. It features an adaptive and customizable spatial conditional scheme that allows for different weights to be assigned to inputs based on their spatial locations. This flexibility enhances the controllability of world generation, making it suitable for applications such as Sim2Real in robotics and autonomous vehicles. The model has been extensively evaluated, and its code is open-sourced to foster further research in the field.'}, 'zh': {'title': 'å¯æ§çš„ä¸–ç•Œç”Ÿæˆï¼ŒåŠ©åŠ›ç‰©ç†äººå·¥æ™ºèƒ½', 'desc': 'æˆ‘ä»¬ä»‹ç»äº†Cosmos-Transferï¼Œè¿™æ˜¯ä¸€ç§æ¡ä»¶ä¸–ç•Œç”Ÿæˆæ¨¡å‹ï¼Œå¯ä»¥æ ¹æ®å¤šç§ç©ºé—´æ§åˆ¶è¾“å…¥ï¼ˆå¦‚åˆ†å‰²ã€æ·±åº¦å’Œè¾¹ç¼˜ï¼‰ç”Ÿæˆä¸–ç•Œæ¨¡æ‹Ÿã€‚åœ¨è®¾è®¡ä¸Šï¼Œç©ºé—´æ¡ä»¶æ–¹æ¡ˆæ˜¯è‡ªé€‚åº”å’Œå¯å®šåˆ¶çš„ï¼Œå…è®¸åœ¨ä¸åŒç©ºé—´ä½ç½®å¯¹ä¸åŒæ¡ä»¶è¾“å…¥è¿›è¡ŒåŠ æƒã€‚è¿™ä½¿å¾—ä¸–ç•Œç”Ÿæˆå…·æœ‰é«˜åº¦å¯æ§æ€§ï¼Œå¹¶åœ¨å¤šç§ä¸–ç•Œåˆ°ä¸–ç•Œçš„è½¬ç§»åº”ç”¨ä¸­æ‰¾åˆ°ç”¨é€”ï¼ŒåŒ…æ‹¬Sim2Realã€‚æˆ‘ä»¬è¿›è¡Œäº†å¹¿æ³›çš„è¯„ä¼°ï¼Œåˆ†æäº†æ‰€æå‡ºçš„æ¨¡å‹ï¼Œå¹¶å±•ç¤ºäº†å…¶åœ¨ç‰©ç†äººå·¥æ™ºèƒ½ä¸­çš„åº”ç”¨ï¼ŒåŒ…æ‹¬æœºå™¨äººSim2Realå’Œè‡ªåŠ¨é©¾é©¶è½¦è¾†æ•°æ®å¢å¼ºã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.14504', 'title': 'Aligning Multimodal LLM with Human Preference: A Survey', 'url': 'https://huggingface.co/papers/2503.14504', 'abstract': 'Large language models (LLMs) can handle a wide variety of general tasks with simple prompts, without the need for task-specific training. Multimodal Large Language Models (MLLMs), built upon LLMs, have demonstrated impressive potential in tackling complex tasks involving visual, auditory, and textual data. However, critical issues related to truthfulness, safety, o1-like reasoning, and alignment with human preference remain insufficiently addressed. This gap has spurred the emergence of various alignment algorithms, each targeting different application scenarios and optimization goals. Recent studies have shown that alignment algorithms are a powerful approach to resolving the aforementioned challenges. In this paper, we aim to provide a comprehensive and systematic review of alignment algorithms for MLLMs. Specifically, we explore four key aspects: (1) the application scenarios covered by alignment algorithms, including general image understanding, multi-image, video, and audio, and extended multimodal applications; (2) the core factors in constructing alignment datasets, including data sources, model responses, and preference annotations; (3) the benchmarks used to evaluate alignment algorithms; and (4) a discussion of potential future directions for the development of alignment algorithms. This work seeks to help researchers organize current advancements in the field and inspire better alignment methods. The project page of this paper is available at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Alignment.', 'score': 10, 'issue_id': 2778, 'pub_date': '2025-03-18', 'pub_date_card': {'ru': '18 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 18', 'zh': '3æœˆ18æ—¥'}, 'hash': 'b33bcba515cfa942', 'authors': ['Tao Yu', 'Yi-Fan Zhang', 'Chaoyou Fu', 'Junkang Wu', 'Jinda Lu', 'Kun Wang', 'Xingyu Lu', 'Yunhang Shen', 'Guibin Zhang', 'Dingjie Song', 'Yibo Yan', 'Tianlong Xu', 'Qingsong Wen', 'Zhang Zhang', 'Yan Huang', 'Liang Wang', 'Tieniu Tan'], 'affiliations': ['Institute of automation, Chinese academy of science', 'Lehigh University', 'Nanjing University', 'Nanyang Technological University', 'National University of Singapore', 'Shenzhen International Graduate School, Tsinghua University', 'Squirrel Ai Learning', 'Tencent Youtu Lab', 'The Hong Kong University of Science and Technology', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2503.14504.jpg', 'data': {'categories': ['#survey', '#multimodal', '#benchmark', '#dataset', '#alignment'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ’Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜: Ğ¿ÑƒÑ‚ÑŒ Ğº Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ²ÑĞµÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½Ğ¸Ğ¹ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ² Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¸Ñ… Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ², Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾. ĞĞ½Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ñ‹ Ğ² ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±ÑÑƒĞ¶Ğ´Ğ°ÑÑ‚ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ². Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸.'}, 'en': {'title': 'Aligning MLLMs: Bridging Gaps for Better Understanding', 'desc': 'This paper reviews alignment algorithms for Multimodal Large Language Models (MLLMs), which integrate visual, auditory, and textual data. It highlights the challenges of truthfulness, safety, and reasoning in MLLMs, emphasizing the need for effective alignment strategies. The authors categorize alignment algorithms based on application scenarios, dataset construction, and evaluation benchmarks. The goal is to provide a structured overview that aids researchers in advancing alignment techniques for MLLMs.'}, 'zh': {'title': 'å¯¹é½ç®—æ³•åŠ©åŠ›å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹çš„æœªæ¥', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰èƒ½å¤Ÿé€šè¿‡ç®€å•çš„æç¤ºå¤„ç†å„ç§é€šç”¨ä»»åŠ¡ï¼Œè€Œæ— éœ€ç‰¹å®šä»»åŠ¡çš„è®­ç»ƒã€‚å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤„ç†æ¶‰åŠè§†è§‰ã€å¬è§‰å’Œæ–‡æœ¬æ•°æ®çš„å¤æ‚ä»»åŠ¡æ–¹é¢å±•ç°äº†ä»¤äººå°è±¡æ·±åˆ»çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œå…³äºçœŸå®æ€§ã€å®‰å…¨æ€§ã€ç±»o1æ¨ç†å’Œä¸äººç±»åå¥½çš„å¯¹é½ç­‰å…³é”®é—®é¢˜ä»æœªå¾—åˆ°å……åˆ†è§£å†³ã€‚æœ¬æ–‡æ—¨åœ¨ç³»ç»Ÿæ€§åœ°å›é¡¾MLLMsçš„å¯¹é½ç®—æ³•ï¼Œæ¢è®¨å…¶åº”ç”¨åœºæ™¯ã€æ•°æ®é›†æ„å»ºæ ¸å¿ƒå› ç´ ã€è¯„ä¼°åŸºå‡†ä»¥åŠæœªæ¥å‘å±•æ–¹å‘ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.14499', 'title': 'Measuring AI Ability to Complete Long Tasks', 'url': 'https://huggingface.co/papers/2503.14499', 'abstract': "Despite rapid progress on AI benchmarks, the real-world meaning of benchmark performance remains unclear. To quantify the capabilities of AI systems in terms of human capabilities, we propose a new metric: 50%-task-completion time horizon. This is the time humans typically take to complete tasks that AI models can complete with 50% success rate. We first timed humans with relevant domain expertise on a combination of RE-Bench, HCAST, and 66 novel shorter tasks. On these tasks, current frontier AI models such as Claude 3.7 Sonnet have a 50% time horizon of around 50 minutes. Furthermore, frontier AI time horizon has been doubling approximately every seven months since 2019, though the trend may have accelerated in 2024. The increase in AI models' time horizons seems to be primarily driven by greater reliability and ability to adapt to mistakes, combined with better logical reasoning and tool use capabilities. We discuss the limitations of our results -- including their degree of external validity -- and the implications of increased autonomy for dangerous capabilities. If these results generalize to real-world software tasks, extrapolation of this trend predicts that within 5 years, AI systems will be capable of automating many software tasks that currently take humans a month.", 'score': 9, 'issue_id': 2777, 'pub_date': '2025-03-18', 'pub_date_card': {'ru': '18 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 18', 'zh': '3æœˆ18æ—¥'}, 'hash': 'c31aeeed7f6139af', 'authors': ['Thomas Kwa', 'Ben West', 'Joel Becker', 'Amy Deng', 'Katharyn Garcia', 'Max Hasin', 'Sami Jawhar', 'Megan Kinniment', 'Nate Rush', 'Sydney Von Arx', 'Ryan Bloom', 'Thomas Broadley', 'Haoxing Du', 'Brian Goodrich', 'Nikola Jurkovic', 'Luke Harold Miles', 'Seraphina Nix', 'Tao Lin', 'Neev Parikh', 'David Rein', 'Lucas Jun Koba Sato', 'Hjalmar Wijk', 'Daniel M. Ziegler', 'Elizabeth Barnes', 'Lawrence Chan'], 'affiliations': ['Anthropic', 'Model Evaluation & Threat Research (METR)'], 'pdf_title_img': 'assets/pdf/title_img/2503.14499.jpg', 'data': {'categories': ['#ethics', '#benchmark', '#reasoning'], 'emoji': 'â³', 'ru': {'title': 'Ğ’Ñ€ĞµĞ¼Ñ Ğ½Ğ° Ğ²Ğ°ÑˆĞµĞ¹ ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğµ: Ğ˜Ğ˜ Ğ´Ğ¾Ğ³Ğ¾Ğ½ÑĞµÑ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ² ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ˜Ğ˜-ÑĞ¸ÑÑ‚ĞµĞ¼ - Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ 50%-Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒ Ğ¸Ğ·Ğ¼ĞµÑ€ÑĞµÑ‚ Ğ²Ñ€ĞµĞ¼Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ÑÑ Ğ»ÑĞ´ÑĞ¼ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ˜Ğ˜ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ÑŒ Ñ 50%-Ğ½Ğ¾Ğ¹ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒÑ ÑƒÑĞ¿ĞµÑ…Ğ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ˜Ğ˜, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Claude 3.7 Sonnet, Ğ¸Ğ¼ĞµÑÑ‚ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¾ĞºĞ¾Ğ»Ğ¾ 50 Ğ¼Ğ¸Ğ½ÑƒÑ‚. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ‚Ğ¼ĞµÑ‡Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ˜Ğ˜ ÑƒĞ´Ğ²Ğ°Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ½Ğ¾ ĞºĞ°Ğ¶Ğ´Ñ‹Ğµ ÑĞµĞ¼ÑŒ Ğ¼ĞµÑÑÑ†ĞµĞ² Ñ 2019 Ğ³Ğ¾Ğ´Ğ°, Ğ¸ Ğ¾Ğ±ÑÑƒĞ¶Ğ´Ğ°ÑÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑÑ‚Ğ²Ğ¸Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ˜Ğ˜-ÑĞ¸ÑÑ‚ĞµĞ¼.'}, 'en': {'title': 'Measuring AI Progress: The 50%-Task-Completion Time Horizon', 'desc': 'This paper introduces a new metric called the 50%-task-completion time horizon to evaluate AI systems based on human performance. It measures the time it takes for humans to complete tasks that AI can accomplish with a 50% success rate. The study found that leading AI models, like Claude 3.7 Sonnet, have a time horizon of about 50 minutes, and this capability has been improving rapidly, doubling approximately every seven months. The authors highlight the implications of this trend, suggesting that AI could soon automate complex software tasks that currently require significant human effort.'}, 'zh': {'title': 'AIèƒ½åŠ›çš„æ–°åº¦é‡ï¼š50%ä»»åŠ¡å®Œæˆæ—¶é—´', 'desc': 'å°½ç®¡äººå·¥æ™ºèƒ½åœ¨åŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†å¿«é€Ÿè¿›å±•ï¼Œä½†å…¶å®é™…è¡¨ç°çš„æ„ä¹‰ä»ä¸æ˜ç¡®ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„åº¦é‡æ ‡å‡†ï¼š50%ä»»åŠ¡å®Œæˆæ—¶é—´èŒƒå›´ï¼Œæ—¨åœ¨é‡åŒ–AIç³»ç»Ÿä¸äººç±»èƒ½åŠ›çš„å¯¹æ¯”ã€‚é€šè¿‡å¯¹äººç±»ä¸“å®¶åœ¨å¤šä¸ªä»»åŠ¡ä¸Šçš„å®Œæˆæ—¶é—´è¿›è¡Œæµ‹é‡ï¼Œæˆ‘ä»¬å‘ç°å½“å‰çš„å‰æ²¿AIæ¨¡å‹åœ¨è¿™äº›ä»»åŠ¡ä¸Šçš„50%æ—¶é—´èŒƒå›´çº¦ä¸º50åˆ†é’Ÿã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼ŒAIæ¨¡å‹çš„æ—¶é—´èŒƒå›´è‡ª2019å¹´ä»¥æ¥å¤§çº¦æ¯ä¸ƒä¸ªæœˆç¿»å€ï¼Œæœªæ¥äº”å¹´å†…ï¼ŒAIç³»ç»Ÿå¯èƒ½èƒ½å¤Ÿè‡ªåŠ¨åŒ–è®¸å¤šç›®å‰éœ€è¦äººç±»ä¸€ä¸ªæœˆæ‰èƒ½å®Œæˆçš„è½¯ä»¶ä»»åŠ¡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.12355', 'title': 'Atlas: Multi-Scale Attention Improves Long Context Image Modeling', 'url': 'https://huggingface.co/papers/2503.12355', 'abstract': 'Efficiently modeling massive images is a long-standing challenge in machine learning. To this end, we introduce Multi-Scale Attention (MSA). MSA relies on two key ideas, (i) multi-scale representations (ii) bi-directional cross-scale communication. MSA creates O(log N) scales to represent the image across progressively coarser features and leverages cross-attention to propagate information across scales. We then introduce Atlas, a novel neural network architecture based on MSA. We demonstrate that Atlas significantly improves the compute-performance tradeoff of long-context image modeling in a high-resolution variant of ImageNet 100. At 1024px resolution, Atlas-B achieves 91.04% accuracy, comparable to ConvNext-B (91.92%) while being 4.3x faster. Atlas is 2.95x faster and 7.38% better than FasterViT, 2.25x faster and 4.96% better than LongViT. In comparisons against MambaVision-S, we find Atlas-S achieves 5%, 16% and 32% higher accuracy at 1024px, 2048px and 4096px respectively, while obtaining similar runtimes. Code for reproducing our experiments and pretrained models is available at https://github.com/yalalab/atlas.', 'score': 9, 'issue_id': 2791, 'pub_date': '2025-03-16', 'pub_date_card': {'ru': '16 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 16', 'zh': '3æœˆ16æ—¥'}, 'hash': '0b78d0c7af506f20', 'authors': ['Kumar Krishna Agrawal', 'Long Lian', 'Longchao Liu', 'Natalia Harguindeguy', 'Boyi Li', 'Alexander Bick', 'Maggie Chung', 'Trevor Darrell', 'Adam Yala'], 'affiliations': ['University of California San Francisco', 'University of California, Berkeley', 'Vanderbilt University'], 'pdf_title_img': 'assets/pdf/title_img/2503.12355.jpg', 'data': {'categories': ['#cv', '#open_source', '#architecture', '#optimization', '#long_context'], 'emoji': 'ğŸ”', 'ru': {'title': 'ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ³Ğ¸Ğ³Ğ°Ğ½Ñ‚ÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ - Multi-Scale Attention (MSA). MSA Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ´Ğ²ÑƒĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½ÑƒÑ ĞºĞ¾Ğ¼Ğ¼ÑƒĞ½Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°Ğ¼Ğ¸, ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ O(log N) ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ¹ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ MSA Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Atlas, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¾Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ¿Ñ€Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼. Atlas Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Revolutionizing Image Modeling with Multi-Scale Attention', 'desc': 'This paper presents Multi-Scale Attention (MSA), a method designed to efficiently model large images in machine learning. MSA utilizes multi-scale representations and bi-directional cross-scale communication to enhance image feature extraction. The authors introduce Atlas, a neural network architecture that implements MSA, achieving significant improvements in speed and accuracy for high-resolution image tasks. Atlas outperforms existing models like ConvNext and FasterViT, demonstrating a superior compute-performance tradeoff while maintaining high accuracy on the ImageNet dataset.'}, 'zh': {'title': 'é«˜æ•ˆå›¾åƒå»ºæ¨¡çš„æ–°çªç ´ï¼šå¤šå°ºåº¦æ³¨æ„åŠ›', 'desc': 'åœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œé«˜æ•ˆå»ºæ¨¡å¤§è§„æ¨¡å›¾åƒä¸€ç›´æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†å¤šå°ºåº¦æ³¨æ„åŠ›ï¼ˆMSAï¼‰ï¼Œå®ƒä¾èµ–äºå¤šå°ºåº¦è¡¨ç¤ºå’ŒåŒå‘è·¨å°ºåº¦é€šä¿¡ä¸¤ä¸ªå…³é”®æ€æƒ³ã€‚MSAé€šè¿‡åˆ›å»ºO(log N)çš„å°ºåº¦æ¥è¡¨ç¤ºå›¾åƒï¼Œå¹¶åˆ©ç”¨äº¤å‰æ³¨æ„åŠ›åœ¨ä¸åŒå°ºåº¦ä¹‹é—´ä¼ æ’­ä¿¡æ¯ã€‚æˆ‘ä»¬è¿˜ä»‹ç»äº†ä¸€ç§åŸºäºMSAçš„æ–°å‹ç¥ç»ç½‘ç»œæ¶æ„Atlasï¼Œå®éªŒè¡¨æ˜Atlasåœ¨é«˜åˆ†è¾¨ç‡å›¾åƒå»ºæ¨¡ä¸­æ˜¾è‘—æé«˜äº†è®¡ç®—æ€§èƒ½çš„å¹³è¡¡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.10522', 'title': 'AudioX: Diffusion Transformer for Anything-to-Audio Generation', 'url': 'https://huggingface.co/papers/2503.10522', 'abstract': 'Audio and music generation have emerged as crucial tasks in many applications, yet existing approaches face significant limitations: they operate in isolation without unified capabilities across modalities, suffer from scarce high-quality, multi-modal training data, and struggle to effectively integrate diverse inputs. In this work, we propose AudioX, a unified Diffusion Transformer model for Anything-to-Audio and Music Generation. Unlike previous domain-specific models, AudioX can generate both general audio and music with high quality, while offering flexible natural language control and seamless processing of various modalities including text, video, image, music, and audio. Its key innovation is a multi-modal masked training strategy that masks inputs across modalities and forces the model to learn from masked inputs, yielding robust and unified cross-modal representations. To address data scarcity, we curate two comprehensive datasets: vggsound-caps with 190K audio captions based on the VGGSound dataset, and V2M-caps with 6 million music captions derived from the V2M dataset. Extensive experiments demonstrate that AudioX not only matches or outperforms state-of-the-art specialized models, but also offers remarkable versatility in handling diverse input modalities and generation tasks within a unified architecture. The code and datasets will be available at https://zeyuet.github.io/AudioX/', 'score': 9, 'issue_id': 2791, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 13', 'zh': '3æœˆ13æ—¥'}, 'hash': 'c3590459e5737c75', 'authors': ['Zeyue Tian', 'Yizhu Jin', 'Zhaoyang Liu', 'Ruibin Yuan', 'Xu Tan', 'Qifeng Chen', 'Wei Xue', 'Yike Guo'], 'affiliations': ['Hong Kong University of Science and Technology', 'Moonshot AI'], 'pdf_title_img': 'assets/pdf/title_img/2503.10522.jpg', 'data': {'categories': ['#synthetic', '#architecture', '#diffusion', '#audio', '#multimodal', '#dataset'], 'emoji': 'ğŸµ', 'ru': {'title': 'AudioX: ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸Ğ· Ñ‡ĞµĞ³Ğ¾ ÑƒĞ³Ğ¾Ğ´Ğ½Ğ¾', 'desc': 'AudioX - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Diffusion Transformer Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ñ… ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ñ‹Ğ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ñ‹ Ğ´Ğ²Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…: vggsound-caps Ğ¸ V2M-caps. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ AudioX Ğ½Ğµ ÑƒÑÑ‚ÑƒĞ¿Ğ°ĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ¸ Ğ¾Ğ±Ğ»Ğ°Ğ´Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'AudioX: Unifying Audio and Music Generation Across Modalities', 'desc': 'This paper introduces AudioX, a novel Diffusion Transformer model designed for generating audio and music from various input types. Unlike traditional models that focus on specific tasks, AudioX integrates multiple modalities, allowing it to process text, video, images, and audio seamlessly. The model employs a unique multi-modal masked training strategy, which enhances its ability to learn robust representations by masking inputs across different modalities. Additionally, the authors address the challenge of limited high-quality training data by creating two extensive datasets, enabling AudioX to outperform existing specialized models in versatility and performance.'}, 'zh': {'title': 'ç»Ÿä¸€éŸ³é¢‘ä¸éŸ³ä¹ç”Ÿæˆçš„åˆ›æ–°æ¨¡å‹', 'desc': 'éŸ³é¢‘å’ŒéŸ³ä¹ç”Ÿæˆåœ¨è®¸å¤šåº”ç”¨ä¸­å˜å¾—è¶Šæ¥è¶Šé‡è¦ï¼Œä½†ç°æœ‰æ–¹æ³•å­˜åœ¨æ˜¾è‘—å±€é™æ€§ã€‚æˆ‘ä»¬æå‡ºäº†AudioXï¼Œè¿™æ˜¯ä¸€ç§ç»Ÿä¸€çš„æ‰©æ•£å˜æ¢å™¨æ¨¡å‹ï¼Œèƒ½å¤Ÿå®ç°å¤šç§è¾“å…¥åˆ°éŸ³é¢‘å’ŒéŸ³ä¹çš„ç”Ÿæˆã€‚AudioXçš„åˆ›æ–°ä¹‹å¤„åœ¨äºå…¶å¤šæ¨¡æ€æ©è”½è®­ç»ƒç­–ç•¥ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿä»ä¸åŒæ¨¡æ€çš„æ©è”½è¾“å…¥ä¸­å­¦ä¹ ï¼Œä»è€Œç”Ÿæˆé«˜è´¨é‡çš„éŸ³é¢‘å’ŒéŸ³ä¹ã€‚é€šè¿‡æ„å»ºä¸¤ä¸ªå…¨é¢çš„æ•°æ®é›†ï¼ŒAudioXåœ¨å¤„ç†å¤šæ ·åŒ–è¾“å…¥æ¨¡æ€å’Œç”Ÿæˆä»»åŠ¡æ–¹é¢å±•ç°äº†å“è¶Šçš„çµæ´»æ€§å’Œæ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.12505', 'title': 'MPBench: A Comprehensive Multimodal Reasoning Benchmark for Process\n  Errors Identification', 'url': 'https://huggingface.co/papers/2503.12505', 'abstract': 'Reasoning is an essential capacity for large language models (LLMs) to address complex tasks, where the identification of process errors is vital for improving this ability. Recently, process-level reward models (PRMs) were proposed to provide step-wise rewards that facilitate reinforcement learning and data production during training and guide LLMs toward correct steps during inference, thereby improving reasoning accuracy. However, existing benchmarks of PRMs are text-based and focus on error detection, neglecting other scenarios like reasoning search. To address this gap, we introduce MPBench, a comprehensive, multi-task, multimodal benchmark designed to systematically assess the effectiveness of PRMs in diverse scenarios. MPBench employs three evaluation paradigms, each targeting a specific role of PRMs in the reasoning process: (1) Step Correctness, which assesses the correctness of each intermediate reasoning step; (2) Answer Aggregation, which aggregates multiple solutions and selects the best one; and (3) Reasoning Process Search, which guides the search for optimal reasoning steps during inference. Through these paradigms, MPBench makes comprehensive evaluations and provides insights into the development of multimodal PRMs.', 'score': 8, 'issue_id': 2776, 'pub_date': '2025-03-16', 'pub_date_card': {'ru': '16 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 16', 'zh': '3æœˆ16æ—¥'}, 'hash': '0ac4fdd3411855ac', 'authors': ['Zhaopan Xu', 'Pengfei Zhou', 'Jiaxin Ai', 'Wangbo Zhao', 'Kai Wang', 'Xiaojiang Peng', 'Wenqi Shao', 'Hongxun Yao', 'Kaipeng Zhang'], 'affiliations': ['HIT', 'NUS', 'SZTU', 'Shanghai AI Laboratory', 'Shanghai Innovation Institute'], 'pdf_title_img': 'assets/pdf/title_img/2503.12505.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#multimodal', '#rl'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'MPBench: ĞšĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº MPBench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° (PRM) Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. MPBench Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñ‹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸: ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ ÑˆĞ°Ğ³Ğ¾Ğ², Ğ°Ğ³Ñ€ĞµĞ³Ğ°Ñ†Ğ¸Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ²ÑĞµÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½Ğµ Ğ¾Ñ†ĞµĞ½Ğ¸Ñ‚ÑŒ PRM Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ¸ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ.'}, 'en': {'title': 'Enhancing Reasoning in LLMs with MPBench', 'desc': "This paper discusses the importance of reasoning in large language models (LLMs) and introduces process-level reward models (PRMs) to enhance their reasoning capabilities. PRMs provide step-wise rewards that help LLMs learn from their mistakes during training and improve their decision-making during inference. The authors identify a gap in existing benchmarks, which primarily focus on error detection, and propose MPBench, a new benchmark that evaluates PRMs across various reasoning scenarios. MPBench includes three evaluation paradigms: assessing step correctness, aggregating answers, and guiding the reasoning process search, offering a comprehensive framework for understanding PRMs' effectiveness."}, 'zh': {'title': 'å…¨é¢è¯„ä¼°æ¨ç†è¿‡ç¨‹çš„å¤šæ¨¡æ€åŸºå‡†', 'desc': 'æ¨ç†æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¤„ç†å¤æ‚ä»»åŠ¡çš„é‡è¦èƒ½åŠ›ï¼Œè€Œè¯†åˆ«è¿‡ç¨‹é”™è¯¯å¯¹äºæå‡è¿™ä¸€èƒ½åŠ›è‡³å…³é‡è¦ã€‚æœ€è¿‘æå‡ºçš„è¿‡ç¨‹çº§å¥–åŠ±æ¨¡å‹ï¼ˆPRMsï¼‰é€šè¿‡æä¾›é€æ­¥å¥–åŠ±ï¼Œä¿ƒè¿›äº†å¼ºåŒ–å­¦ä¹ å’Œæ•°æ®ç”Ÿæˆï¼Œä»è€Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­å¼•å¯¼LLMsèµ°å‘æ­£ç¡®çš„æ­¥éª¤ï¼Œæé«˜æ¨ç†å‡†ç¡®æ€§ã€‚ç„¶è€Œï¼Œç°æœ‰çš„PRMsåŸºå‡†ä¸»è¦åŸºäºæ–‡æœ¬ï¼Œä¸“æ³¨äºé”™è¯¯æ£€æµ‹ï¼Œå¿½è§†äº†æ¨ç†æœç´¢ç­‰å…¶ä»–åœºæ™¯ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†MPBenchï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„å¤šä»»åŠ¡å¤šæ¨¡æ€åŸºå‡†ï¼Œæ—¨åœ¨ç³»ç»Ÿè¯„ä¼°PRMsåœ¨ä¸åŒåœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.13265', 'title': 'FlexWorld: Progressively Expanding 3D Scenes for Flexiable-View\n  Synthesis', 'url': 'https://huggingface.co/papers/2503.13265', 'abstract': 'Generating flexible-view 3D scenes, including 360{\\deg} rotation and zooming, from single images is challenging due to a lack of 3D data. To this end, we introduce FlexWorld, a novel framework consisting of two key components: (1) a strong video-to-video (V2V) diffusion model to generate high-quality novel view images from incomplete input rendered from a coarse scene, and (2) a progressive expansion process to construct a complete 3D scene. In particular, leveraging an advanced pre-trained video model and accurate depth-estimated training pairs, our V2V model can generate novel views under large camera pose variations. Building upon it, FlexWorld progressively generates new 3D content and integrates it into the global scene through geometry-aware scene fusion. Extensive experiments demonstrate the effectiveness of FlexWorld in generating high-quality novel view videos and flexible-view 3D scenes from single images, achieving superior visual quality under multiple popular metrics and datasets compared to existing state-of-the-art methods. Qualitatively, we highlight that FlexWorld can generate high-fidelity scenes with flexible views like 360{\\deg} rotations and zooming. Project page: https://ml-gsai.github.io/FlexWorld.', 'score': 7, 'issue_id': 2785, 'pub_date': '2025-03-17', 'pub_date_card': {'ru': '17 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 17', 'zh': '3æœˆ17æ—¥'}, 'hash': '8b68e5c333f5fe62', 'authors': ['Luxi Chen', 'Zihan Zhou', 'Min Zhao', 'Yikai Wang', 'Ge Zhang', 'Wenhao Huang', 'Hao Sun', 'Ji-Rong Wen', 'Chongxuan Li'], 'affiliations': ['Beijing Key Laboratory of Big Data Management and Analysis Methods', 'ByteDance', 'Dept. of Comp. Sci. & Tech., BNRist Center, THU-Bosch MLCenter, Tsinghua University', 'Gaoling School of AI, Renmin University of China', 'School of Artificial Intelligence, Beijing Normal University'], 'pdf_title_img': 'assets/pdf/title_img/2503.13265.jpg', 'data': {'categories': ['#diffusion', '#3d', '#video'], 'emoji': 'ğŸŒ', 'ru': {'title': 'Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ³Ğ¸Ğ±ĞºĞ¸Ñ… 3D-Ğ¼Ğ¸Ñ€Ğ¾Ğ² Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ', 'desc': 'FlexWorld - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ³Ğ¸Ğ±ĞºĞ¸Ñ… 3D-ÑÑ†ĞµĞ½ Ğ¸Ğ· Ğ¾Ğ´Ğ¸Ğ½Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ Ğ½Ğ° 360Â° Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ. ĞĞ½Ğ° ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ğ¼Ğ¾Ñ‰Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ²-Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ² Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ 3D-ÑÑ†ĞµĞ½Ñ‹. FlexWorld Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ€Ñ‹ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¾Ğ¹ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸ÑÑ… Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ°Ğ¼ĞµÑ€Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ FlexWorld Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ² Ğ¸ Ğ³Ğ¸Ğ±ĞºĞ¸Ñ… 3D-ÑÑ†ĞµĞ½ Ğ¸Ğ· Ğ¾Ğ´Ğ¸Ğ½Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Transforming Single Images into Dynamic 3D Worlds', 'desc': 'FlexWorld is a new framework designed to create flexible 3D scenes from single images, addressing the challenge of limited 3D data. It features a video-to-video (V2V) diffusion model that generates high-quality images from incomplete scenes, allowing for significant camera pose variations. Additionally, it employs a progressive expansion process to build a complete 3D environment, integrating new content through geometry-aware scene fusion. Experiments show that FlexWorld outperforms existing methods in generating high-fidelity scenes with flexible viewing options like 360-degree rotations and zooming.'}, 'zh': {'title': 'FlexWorldï¼šä»å•å›¾åƒç”Ÿæˆçµæ´»è§†è§’3Dåœºæ™¯çš„åˆ›æ–°æ¡†æ¶', 'desc': 'FlexWorldæ˜¯ä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œæ—¨åœ¨ä»å•å¼ å›¾åƒç”Ÿæˆçµæ´»è§†è§’çš„3Dåœºæ™¯ï¼ŒåŒ…æ‹¬360åº¦æ—‹è½¬å’Œç¼©æ”¾ã€‚è¯¥æ¡†æ¶åŒ…å«ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šä¸€ä¸ªå¼ºå¤§çš„è§†é¢‘åˆ°è§†é¢‘ï¼ˆV2Vï¼‰æ‰©æ•£æ¨¡å‹ï¼Œç”¨äºä»ç²—ç³™åœºæ™¯æ¸²æŸ“çš„ç¼ºå¤±è¾“å…¥ä¸­ç”Ÿæˆé«˜è´¨é‡çš„æ–°è§†å›¾å›¾åƒï¼Œä»¥åŠä¸€ä¸ªæ¸è¿›æ‰©å±•è¿‡ç¨‹ï¼Œç”¨äºæ„å»ºå®Œæ•´çš„3Dåœºæ™¯ã€‚é€šè¿‡åˆ©ç”¨å…ˆè¿›çš„é¢„è®­ç»ƒè§†é¢‘æ¨¡å‹å’Œå‡†ç¡®çš„æ·±åº¦ä¼°è®¡è®­ç»ƒå¯¹ï¼ŒV2Væ¨¡å‹èƒ½å¤Ÿåœ¨å¤§ç›¸æœºå§¿æ€å˜åŒ–ä¸‹ç”Ÿæˆæ–°è§†å›¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFlexWorldåœ¨ç”Ÿæˆé«˜è´¨é‡æ–°è§†å›¾è§†é¢‘å’Œçµæ´»è§†è§’3Dåœºæ™¯æ–¹é¢è¡¨ç°ä¼˜è¶Šï¼Œè¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.13111', 'title': 'MM-Spatial: Exploring 3D Spatial Understanding in Multimodal LLMs', 'url': 'https://huggingface.co/papers/2503.13111', 'abstract': 'Multimodal large language models (MLLMs) excel at 2D visual understanding but remain limited in their ability to reason about 3D space. In this work, we leverage large-scale high-quality 3D scene data with open-set annotations to introduce 1) a novel supervised fine-tuning dataset and 2) a new evaluation benchmark, focused on indoor scenes. Our Cubify Anything VQA (CA-VQA) data covers diverse spatial tasks including spatial relationship prediction, metric size and distance estimation, and 3D grounding. We show that CA-VQA enables us to train MM-Spatial, a strong generalist MLLM that also achieves state-of-the-art performance on 3D spatial understanding benchmarks, including our own. We show how incorporating metric depth and multi-view inputs (provided in CA-VQA) can further improve 3D understanding, and demonstrate that data alone allows our model to achieve depth perception capabilities comparable to dedicated monocular depth estimation models. We will publish our SFT dataset and benchmark.', 'score': 6, 'issue_id': 2785, 'pub_date': '2025-03-17', 'pub_date_card': {'ru': '17 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 17', 'zh': '3æœˆ17æ—¥'}, 'hash': 'ce045e2d94eafd19', 'authors': ['Erik Daxberger', 'Nina Wenzel', 'David Griffiths', 'Haiming Gang', 'Justin Lazarow', 'Gefen Kohavi', 'Kai Kang', 'Marcin Eichner', 'Yinfei Yang', 'Afshin Dehghan', 'Peter Grasch'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2503.13111.jpg', 'data': {'categories': ['#reasoning', '#optimization', '#3d', '#multimodal', '#benchmark', '#dataset'], 'emoji': 'ğŸ ', 'ru': {'title': 'ĞĞ¾Ğ²Ñ‹Ğ¹ ÑˆĞ°Ğ³ Ğº Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ 3D-Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ¾Ğ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… CA-VQA Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€ÑŒĞµÑ€Ğ°Ñ…. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ±Ñ‹Ğ»Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ MM-Spatial, Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ²ÑˆĞ°Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ 3D-Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ²ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Enhancing 3D Understanding in MLLMs with CA-VQA', 'desc': 'This paper addresses the limitations of multimodal large language models (MLLMs) in understanding 3D spaces. The authors introduce a new supervised fine-tuning dataset called Cubify Anything VQA (CA-VQA), which includes high-quality 3D scene data with open-set annotations. CA-VQA focuses on various spatial tasks such as predicting spatial relationships and estimating sizes and distances in indoor environments. The study demonstrates that their model, MM-Spatial, achieves state-of-the-art performance in 3D spatial understanding by leveraging the rich data from CA-VQA, enhancing depth perception capabilities.'}, 'zh': {'title': 'æå‡ä¸‰ç»´ç©ºé—´ç†è§£çš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œå®ƒåœ¨ç†è§£äºŒç»´è§†è§‰æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ä¸‰ç»´ç©ºé—´æ¨ç†ä¸Šä»ç„¶æœ‰é™ã€‚ç ”ç©¶è€…åˆ©ç”¨å¤§è§„æ¨¡é«˜è´¨é‡çš„ä¸‰ç»´åœºæ™¯æ•°æ®ï¼Œåˆ›å»ºäº†ä¸€ä¸ªæ–°çš„ç›‘ç£å¾®è°ƒæ•°æ®é›†å’Œè¯„ä¼°åŸºå‡†ï¼Œä¸“æ³¨äºå®¤å†…åœºæ™¯ã€‚æ–°æ•°æ®é›†Cubify Anything VQAï¼ˆCA-VQAï¼‰æ¶µç›–äº†å¤šç§ç©ºé—´ä»»åŠ¡ï¼ŒåŒ…æ‹¬ç©ºé—´å…³ç³»é¢„æµ‹ã€åº¦é‡å¤§å°å’Œè·ç¦»ä¼°è®¡ä»¥åŠä¸‰ç»´å®šä½ã€‚ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡ç»“åˆåº¦é‡æ·±åº¦å’Œå¤šè§†è§’è¾“å…¥ï¼Œæ¨¡å‹åœ¨ä¸‰ç»´ç†è§£æ–¹é¢çš„è¡¨ç°å¾—åˆ°äº†æ˜¾è‘—æå‡ï¼Œç”šè‡³è¾¾åˆ°äº†ä¸“ç”¨å•ç›®æ·±åº¦ä¼°è®¡æ¨¡å‹çš„èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.12271', 'title': 'Reflect-DiT: Inference-Time Scaling for Text-to-Image Diffusion\n  Transformers via In-Context Reflection', 'url': 'https://huggingface.co/papers/2503.12271', 'abstract': 'The predominant approach to advancing text-to-image generation has been training-time scaling, where larger models are trained on more data using greater computational resources. While effective, this approach is computationally expensive, leading to growing interest in inference-time scaling to improve performance. Currently, inference-time scaling for text-to-image diffusion models is largely limited to best-of-N sampling, where multiple images are generated per prompt and a selection model chooses the best output. Inspired by the recent success of reasoning models like DeepSeek-R1 in the language domain, we introduce an alternative to naive best-of-N sampling by equipping text-to-image Diffusion Transformers with in-context reflection capabilities. We propose Reflect-DiT, a method that enables Diffusion Transformers to refine their generations using in-context examples of previously generated images alongside textual feedback describing necessary improvements. Instead of passively relying on random sampling and hoping for a better result in a future generation, Reflect-DiT explicitly tailors its generations to address specific aspects requiring enhancement. Experimental results demonstrate that Reflect-DiT improves performance on the GenEval benchmark (+0.19) using SANA-1.0-1.6B as a base model. Additionally, it achieves a new state-of-the-art score of 0.81 on GenEval while generating only 20 samples per prompt, surpassing the previous best score of 0.80, which was obtained using a significantly larger model (SANA-1.5-4.8B) with 2048 samples under the best-of-N approach.', 'score': 6, 'issue_id': 2778, 'pub_date': '2025-03-15', 'pub_date_card': {'ru': '15 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 15', 'zh': '3æœˆ15æ—¥'}, 'hash': '2f560e2ec0839955', 'authors': ['Shufan Li', 'Konstantinos Kallidromitis', 'Akash Gokul', 'Arsh Koneru', 'Yusuke Kato', 'Kazuki Kozuka', 'Aditya Grover'], 'affiliations': ['Panasonic AI Research', 'Salesforce AI Research', 'UCLA'], 'pdf_title_img': 'assets/pdf/title_img/2503.12271.jpg', 'data': {'categories': ['#optimization', '#cv', '#benchmark', '#inference', '#diffusion', '#reasoning'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'Reflect-DiT: Ğ£Ğ¼Ğ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¸ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ñ', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Reflect-DiT Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° best-of-N, Reflect-DiT Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Diffusion Transformer Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ½ĞµĞµ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²ÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ GenEval, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ€ĞµĞºĞ¾Ñ€Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ° 0.81. Reflect-DiT Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° inference-time scaling Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Refining Image Generation with Reflective Learning', 'desc': 'This paper presents Reflect-DiT, a novel method for enhancing text-to-image generation by incorporating in-context reflection capabilities into Diffusion Transformers. Unlike traditional best-of-N sampling, which generates multiple images and selects the best, Reflect-DiT refines its outputs based on previous generations and textual feedback. This approach allows the model to focus on specific areas for improvement, leading to more tailored and effective image generation. Experimental results show that Reflect-DiT achieves a new state-of-the-art performance on the GenEval benchmark with fewer samples, demonstrating its efficiency compared to larger models.'}, 'zh': {'title': 'åæ€ç”Ÿæˆï¼Œæå‡å›¾åƒè´¨é‡ï¼', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ–¹æ³•ï¼Œç§°ä¸ºReflect-DiTã€‚è¯¥æ–¹æ³•é€šè¿‡åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­å¼•å…¥ä¸Šä¸‹æ–‡åæ€èƒ½åŠ›ï¼Œå¸®åŠ©Diffusion Transformersæ ¹æ®ä¹‹å‰ç”Ÿæˆçš„å›¾åƒå’Œæ–‡æœ¬åé¦ˆè¿›è¡Œæ”¹è¿›ã€‚ä¸ä¼ ç»Ÿçš„æœ€ä½³Né‡‡æ ·æ–¹æ³•ä¸åŒï¼ŒReflect-DiTèƒ½å¤Ÿé’ˆå¯¹ç‰¹å®šçš„æ”¹è¿›éœ€æ±‚è¿›è¡Œè°ƒæ•´ï¼Œä»è€Œæé«˜ç”Ÿæˆè´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒReflect-DiTåœ¨GenEvalåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¾¾åˆ°äº†æ–°çš„æœ€å…ˆè¿›åˆ†æ•°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.14495', 'title': 'Temporal Consistency for LLM Reasoning Process Error Identification', 'url': 'https://huggingface.co/papers/2503.14495', 'abstract': 'Verification is crucial for effective mathematical reasoning. We present a new temporal consistency method where verifiers iteratively refine their judgments based on the previous assessment. Unlike one-round verification or multi-model debate approaches, our method leverages consistency in a sequence of self-reflection actions to improve verification accuracy. Empirical evaluations across diverse mathematical process error identification benchmarks (Mathcheck, ProcessBench, and PRM800K) show consistent performance improvements over baseline methods. When applied to the recent DeepSeek R1 distilled models, our method demonstrates strong performance, enabling 7B/8B distilled models to outperform all 70B/72B models and GPT-4o on ProcessBench. Notably, the distilled 14B model with our method achieves performance comparable to Deepseek-R1. Our codes are available at https://github.com/jcguo123/Temporal-Consistency', 'score': 5, 'issue_id': 2779, 'pub_date': '2025-03-18', 'pub_date_card': {'ru': '18 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 18', 'zh': '3æœˆ18æ—¥'}, 'hash': '590d8cdf2ae26151', 'authors': ['Jiacheng Guo', 'Yue Wu', 'Jiahao Qiu', 'Kaixuan Huang', 'Xinzhe Juan', 'Ling Yang', 'Mengdi Wang'], 'affiliations': ['AI Lab, Princeton University', 'Department of Computer Science & Engineering, University of Michigan', 'Department of Electrical & Computer Engineering, Princeton University'], 'pdf_title_img': 'assets/pdf/title_img/2503.14495.jpg', 'data': {'categories': ['#math', '#benchmark', '#optimization', '#reasoning', '#training', '#architecture'], 'emoji': 'ğŸ§®', 'ru': {'title': 'ĞŸĞ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¾Ğ´Ğ½Ğ¾Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ğ¸Ğ»Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ² Ñ ÑƒÑ‡Ğ°ÑÑ‚Ğ¸ĞµĞ¼ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ÑÑ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ ÑĞ°Ğ¼Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ¿Ğ¾ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ² Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸. ĞŸÑ€Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¸ Ğº Ğ½ĞµĞ´Ğ°Ğ²Ğ½Ğ¾ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ DeepSeek R1 Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ» Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ 7B/8B Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾Ğ¹Ñ‚Ğ¸ Ğ²ÑĞµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ 70B/72B Ğ¸ GPT-4o Ğ½Ğ° Ñ‚ĞµÑÑ‚Ğµ ProcessBench.'}, 'en': {'title': 'Enhancing Verification Accuracy through Temporal Consistency', 'desc': 'This paper introduces a novel temporal consistency method for enhancing verification in mathematical reasoning. The approach allows verifiers to iteratively refine their judgments based on previous assessments, improving accuracy over traditional one-round verification methods. By utilizing a sequence of self-reflection actions, the method shows significant performance gains on various benchmarks for identifying mathematical process errors. Notably, it enables smaller distilled models to outperform larger models, demonstrating its effectiveness in practical applications.'}, 'zh': {'title': 'æå‡æ•°å­¦éªŒè¯çš„æ—¶é—´ä¸€è‡´æ€§æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ—¶é—´ä¸€è‡´æ€§éªŒè¯æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜æ•°å­¦æ¨ç†çš„æœ‰æ•ˆæ€§ã€‚è¯¥æ–¹æ³•é€šè¿‡è¿­ä»£åœ°æ ¹æ®ä¹‹å‰çš„è¯„ä¼°æ¥ç»†åŒ–åˆ¤æ–­ï¼Œå…‹æœäº†å•è½®éªŒè¯å’Œå¤šæ¨¡å‹è¾©è®ºçš„å±€é™æ€§ã€‚é€šè¿‡åœ¨å¤šä¸ªæ•°å­¦è¿‡ç¨‹é”™è¯¯è¯†åˆ«åŸºå‡†ï¼ˆå¦‚Mathcheckã€ProcessBenchå’ŒPRM800Kï¼‰ä¸Šçš„å®è¯è¯„ä¼°ï¼Œæ˜¾ç¤ºå‡ºç›¸è¾ƒäºåŸºçº¿æ–¹æ³•çš„ä¸€è‡´æ€§æ€§èƒ½æå‡ã€‚åº”ç”¨äºæœ€æ–°çš„DeepSeek R1è’¸é¦æ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä½¿å¾—7B/8Bè’¸é¦æ¨¡å‹åœ¨ProcessBenchä¸Šè¶…è¶Šäº†æ‰€æœ‰70B/72Bæ¨¡å‹å’ŒGPT-4oã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.14151', 'title': 'Concat-ID: Towards Universal Identity-Preserving Video Synthesis', 'url': 'https://huggingface.co/papers/2503.14151', 'abstract': "We present Concat-ID, a unified framework for identity-preserving video generation. Concat-ID employs Variational Autoencoders to extract image features, which are concatenated with video latents along the sequence dimension, leveraging solely 3D self-attention mechanisms without the need for additional modules. A novel cross-video pairing strategy and a multi-stage training regimen are introduced to balance identity consistency and facial editability while enhancing video naturalness. Extensive experiments demonstrate Concat-ID's superiority over existing methods in both single and multi-identity generation, as well as its seamless scalability to multi-subject scenarios, including virtual try-on and background-controllable generation. Concat-ID establishes a new benchmark for identity-preserving video synthesis, providing a versatile and scalable solution for a wide range of applications.", 'score': 5, 'issue_id': 2783, 'pub_date': '2025-03-18', 'pub_date_card': {'ru': '18 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 18', 'zh': '3æœˆ18æ—¥'}, 'hash': 'ba38580b0d116018', 'authors': ['Yong Zhong', 'Zhuoyi Yang', 'Jiayan Teng', 'Xiaotao Gu', 'Chongxuan Li'], 'affiliations': ['Gaoling School of AI, Renmin University of China, Beijing, China', 'Tsinghua University', 'Zhipu AI'], 'pdf_title_img': 'assets/pdf/title_img/2503.14151.jpg', 'data': {'categories': ['#benchmark', '#3d', '#video'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Concat-ID: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸', 'desc': 'Concat-ID - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ñ‹ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ 3D Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ ÑĞ°Ğ¼Ğ¾Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒÑ Ğ»Ğ¸Ñ†. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Concat-ID Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¾Ğ´Ğ½Ğ¸Ğ¼ Ğ¸ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Identity-Preserving Video Generation Made Easy with Concat-ID', 'desc': 'Concat-ID is a new framework designed for generating videos that maintain the identity of subjects. It uses Variational Autoencoders to capture important image features and combines them with video data using 3D self-attention, simplifying the process without extra components. The framework introduces a unique method for pairing videos and a multi-stage training approach to ensure that the generated videos are both consistent in identity and editable, while also looking natural. Through extensive testing, Concat-ID has shown to outperform existing techniques in generating videos with single and multiple identities, making it adaptable for various applications like virtual try-ons and customizable backgrounds.'}, 'zh': {'title': 'Concat-IDï¼šèº«ä»½ä¸€è‡´æ€§è§†é¢‘ç”Ÿæˆçš„æ–°æ ‡æ†', 'desc': 'Concat-ID æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œç”¨äºç”Ÿæˆä¿æŒèº«ä»½ä¸€è‡´æ€§çš„è§†é¢‘ã€‚å®ƒä½¿ç”¨å˜åˆ†è‡ªç¼–ç å™¨æå–å›¾åƒç‰¹å¾ï¼Œå¹¶å°†è¿™äº›ç‰¹å¾ä¸è§†é¢‘æ½œåœ¨å˜é‡åœ¨åºåˆ—ç»´åº¦ä¸Šè¿›è¡Œè¿æ¥ï¼Œå®Œå…¨ä¾èµ–äº 3D è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œè€Œæ— éœ€é¢å¤–æ¨¡å—ã€‚è¯¥æ–¹æ³•å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„è·¨è§†é¢‘é…å¯¹ç­–ç•¥å’Œå¤šé˜¶æ®µè®­ç»ƒæ–¹æ¡ˆï¼Œä»¥å¹³è¡¡èº«ä»½ä¸€è‡´æ€§å’Œé¢éƒ¨å¯ç¼–è¾‘æ€§ï¼ŒåŒæ—¶æé«˜è§†é¢‘çš„è‡ªç„¶æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒConcat-ID åœ¨å•ä¸€å’Œå¤šèº«ä»½ç”Ÿæˆæ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶ä¸”èƒ½å¤Ÿæ— ç¼æ‰©å±•åˆ°å¤šä¸»ä½“åœºæ™¯ï¼ŒåŒ…æ‹¬è™šæ‹Ÿè¯•ç©¿å’ŒèƒŒæ™¯å¯æ§ç”Ÿæˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.12545', 'title': 'PEBench: A Fictitious Dataset to Benchmark Machine Unlearning for\n  Multimodal Large Language Models', 'url': 'https://huggingface.co/papers/2503.12545', 'abstract': 'In recent years, Multimodal Large Language Models (MLLMs) have demonstrated remarkable advancements in tasks such as visual question answering, visual understanding, and reasoning. However, this impressive progress relies on vast amounts of data collected from the internet, raising significant concerns about privacy and security. To address these issues, machine unlearning (MU) has emerged as a promising solution, enabling the removal of specific knowledge from an already trained model without requiring retraining from scratch. Although MU for MLLMs has gained attention, current evaluations of its efficacy remain incomplete, and the underlying problem is often poorly defined, which hinders the development of strategies for creating more secure and trustworthy systems. To bridge this gap, we introduce a benchmark, named PEBench, which includes a dataset of personal entities and corresponding general event scenes, designed to comprehensively assess the performance of MU for MLLMs. Through PEBench, we aim to provide a standardized and robust framework to advance research in secure and privacy-preserving multimodal models. We benchmarked 6 MU methods, revealing their strengths and limitations, and shedding light on key challenges and opportunities for MU in MLLMs.', 'score': 4, 'issue_id': 2776, 'pub_date': '2025-03-16', 'pub_date_card': {'ru': '16 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 16', 'zh': '3æœˆ16æ—¥'}, 'hash': '8a908fbc8ce24853', 'authors': ['Zhaopan Xu', 'Pengfei Zhou', 'Weidong Tang', 'Jiaxin Ai', 'Wangbo Zhao', 'Xiaojiang Peng', 'Kai Wang', 'Yang You', 'Wenqi Shao', 'Hongxun Yao', 'Kaipeng Zhang'], 'affiliations': ['HIT', 'NUS', 'SZTU', 'Shanghai AI Laboratory', 'Shanghai Innovation Institute', 'XDU'], 'pdf_title_img': 'assets/pdf/title_img/2503.12545.jpg', 'data': {'categories': ['#multimodal', '#ethics', '#security', '#benchmark', '#dataset'], 'emoji': 'ğŸ”’', 'ru': {'title': 'PEBench: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº PEBench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ (MU) Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (MLLM). PEBench Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¾Ğ±Ñ‰Ğ¸Ğ¼Ğ¸ ÑÑ†ĞµĞ½Ğ°Ğ¼Ğ¸ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ 6 Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² MU, Ğ²Ñ‹ÑĞ²Ğ¸Ğ² Ğ¸Ñ… ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ ÑĞ»Ğ°Ğ±Ñ‹Ğµ ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñ‹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¾ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ñ… Ğ¸ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ´ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Enhancing Privacy in Multimodal Models with Machine Unlearning', 'desc': 'This paper discusses the advancements of Multimodal Large Language Models (MLLMs) in tasks like visual question answering and reasoning, while highlighting the privacy concerns associated with their data usage. It introduces machine unlearning (MU) as a method to selectively remove knowledge from these models without complete retraining. The authors present PEBench, a benchmark designed to evaluate MU methods specifically for MLLMs, using a dataset of personal entities and general event scenes. By benchmarking six MU techniques, the study identifies their strengths and weaknesses, aiming to enhance the security and trustworthiness of multimodal systems.'}, 'zh': {'title': 'æ¨åŠ¨å¤šæ¨¡æ€æ¨¡å‹çš„å®‰å…¨ä¸éšç§ä¿æŠ¤', 'desc': 'è¿‘å¹´æ¥ï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†è§‰é—®ç­”ã€è§†è§‰ç†è§£å’Œæ¨ç†ç­‰ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œè¿™äº›è¿›å±•ä¾èµ–äºä»äº’è”ç½‘æ”¶é›†çš„å¤§é‡æ•°æ®ï¼Œè¿™å¼•å‘äº†éšç§å’Œå®‰å…¨æ–¹é¢çš„é‡å¤§æ‹…å¿§ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœºå™¨é—å¿˜ï¼ˆMUï¼‰ä½œä¸ºä¸€ç§æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆåº”è¿è€Œç”Ÿï¼Œèƒ½å¤Ÿåœ¨ä¸éœ€è¦ä»å¤´å¼€å§‹é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œä»å·²è®­ç»ƒçš„æ¨¡å‹ä¸­åˆ é™¤ç‰¹å®šçŸ¥è¯†ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåŸºå‡†æµ‹è¯•PEBenchï¼Œæ—¨åœ¨å…¨é¢è¯„ä¼°MUåœ¨MLLMsä¸­çš„è¡¨ç°ï¼Œæ¨åŠ¨å®‰å…¨å’Œéšç§ä¿æŠ¤çš„å¤šæ¨¡æ€æ¨¡å‹ç ”ç©¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.12303', 'title': 'Towards Self-Improving Systematic Cognition for Next-Generation\n  Foundation MLLMs', 'url': 'https://huggingface.co/papers/2503.12303', 'abstract': "Despite their impressive capabilities, Multimodal Large Language Models (MLLMs) face challenges with fine-grained perception and complex reasoning. Prevalent multimodal pre-training approaches focus on enhancing perception by training on high-quality image captions due to the extremely high cost of collecting chain-of-thought (CoT) reasoning data for improving reasoning. While leveraging advanced MLLMs for caption generation enhances scalability, the outputs often lack comprehensiveness and accuracy. In this paper, we introduce Self-Improving cognition (SIcog), a self-learning framework designed to construct next-generation foundation MLLMs by enhancing their systematic cognitive capabilities through multimodal pre-training with self-generated data. Specifically, we propose Chain-of-Description, an approach that improves an MLLM's systematic perception by enabling step-by-step visual understanding, ensuring greater comprehensiveness and accuracy. Additionally, we adopt a structured CoT reasoning technique to enable MLLMs to integrate in-depth multimodal reasoning. To construct a next-generation foundation MLLM with self-improved cognition, SIcog first equips an MLLM with systematic perception and reasoning abilities using minimal external annotations. The enhanced models then generate detailed captions and CoT reasoning data, which are further curated through self-consistency. This curated data is ultimately used for multimodal pre-training to develop next-generation foundation models. Extensive experiments on both low- and high-resolution MLLMs across diverse benchmarks demonstrate that, with merely 213K self-generated pre-training samples, SIcog produces next-generation foundation MLLMs with significantly improved cognition, achieving benchmark-leading performance compared to prevalent pre-training approaches.", 'score': 4, 'issue_id': 2787, 'pub_date': '2025-03-16', 'pub_date_card': {'ru': '16 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 16', 'zh': '3æœˆ16æ—¥'}, 'hash': 'd0f6251740b9c15c', 'authors': ['Xiaoying Zhang', 'Da Peng', 'Yipeng Zhang', 'Zonghao Guo', 'Chengyue Wu', 'Chi Chen', 'Wei Ke', 'Helen Meng', 'Maosong Sun'], 'affiliations': ['The Chinese University of Hong Kong', 'The University of Hong Kong', 'Tsinghua University', 'Xian Jiaotong University'], 'pdf_title_img': 'assets/pdf/title_img/2503.12303.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#multimodal', '#transfer_learning', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ¡Ğ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¹ÑÑ Ğ˜Ğ˜: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ SIcog - Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM). SIcog Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Chain-of-Description Ğ´Ğ»Ñ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞµ Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ MLLM ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğ¼Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸, Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞ¸ Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ SIcog Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ MLLM, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ….'}, 'en': {'title': 'Enhancing MLLMs with Self-Improving Cognition for Better Perception and Reasoning', 'desc': 'This paper addresses the limitations of Multimodal Large Language Models (MLLMs) in fine-grained perception and complex reasoning. It introduces Self-Improving cognition (SIcog), a framework that enhances MLLMs by using self-generated data for multimodal pre-training. The proposed Chain-of-Description method allows MLLMs to develop systematic visual understanding, improving the quality of generated captions and reasoning. Through minimal external annotations, SIcog enables MLLMs to achieve superior performance on various benchmarks with significantly fewer training samples.'}, 'zh': {'title': 'è‡ªæˆ‘æ”¹è¿›è®¤çŸ¥ï¼Œæå‡å¤šæ¨¡æ€æ¨¡å‹çš„æ™ºèƒ½', 'desc': 'å°½ç®¡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å…·æœ‰å¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†åœ¨ç»†ç²’åº¦æ„ŸçŸ¥å’Œå¤æ‚æ¨ç†æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚ç°æœ‰çš„å¤šæ¨¡æ€é¢„è®­ç»ƒæ–¹æ³•ä¸»è¦é€šè¿‡é«˜è´¨é‡çš„å›¾åƒæè¿°æ¥å¢å¼ºæ„ŸçŸ¥ï¼Œä½†æ”¶é›†é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†æ•°æ®çš„æˆæœ¬æé«˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªæˆ‘æ”¹è¿›è®¤çŸ¥æ¡†æ¶ï¼ˆSIcogï¼‰ï¼Œæ—¨åœ¨é€šè¿‡è‡ªç”Ÿæˆæ•°æ®çš„å¤šæ¨¡æ€é¢„è®­ç»ƒæ¥æå‡MLLMçš„ç³»ç»Ÿè®¤çŸ¥èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºçš„æè¿°é“¾æ–¹æ³•èƒ½å¤Ÿé€æ­¥å¢å¼ºè§†è§‰ç†è§£ï¼Œç¡®ä¿ç”Ÿæˆçš„æè¿°æ›´å…¨é¢ã€æ›´å‡†ç¡®ï¼ŒåŒæ—¶ç»“åˆç»“æ„åŒ–çš„CoTæ¨ç†æŠ€æœ¯ï¼Œå®ç°æ·±å…¥çš„å¤šæ¨¡æ€æ¨ç†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.09443', 'title': 'Florenz: Scaling Laws for Systematic Generalization in Vision-Language\n  Models', 'url': 'https://huggingface.co/papers/2503.09443', 'abstract': 'Cross-lingual transfer enables vision-language models (VLMs) to perform vision tasks in various languages with training data only in one language. Current approaches rely on large pre-trained multilingual language models. However, they face the curse of multilinguality, sacrificing downstream task performance for multilingual capabilities, struggling with lexical ambiguities, and falling behind recent advances. In this work, we study the scaling laws of systematic generalization with monolingual VLMs for multilingual tasks, focusing on the impact of model size and seen training samples. We propose Florenz, a monolingual encoder-decoder VLM with 0.4B to 11.2B parameters combining the pre-trained VLM Florence-2 and the large language model Gemma-2. Florenz is trained with varying compute budgets on a synthetic dataset that features intentionally incomplete language coverage for image captioning, thus, testing generalization from the fully covered translation task. We show that not only does indirectly learning unseen task-language pairs adhere to a scaling law, but also that with our data generation pipeline and the proposed Florenz model family, image captioning abilities can emerge in a specific language even when only data for the translation task is available. Fine-tuning on a mix of downstream datasets yields competitive performance and demonstrates promising scaling trends in multimodal machine translation (Multi30K, CoMMuTE), lexical disambiguation (CoMMuTE), and image captioning (Multi30K, XM3600, COCO Karpathy).', 'score': 4, 'issue_id': 2784, 'pub_date': '2025-03-12', 'pub_date_card': {'ru': '12 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 12', 'zh': '3æœˆ12æ—¥'}, 'hash': 'c895c390f49ed5c1', 'authors': ['Julian Spravil', 'Sebastian Houben', 'Sven Behnke'], 'affiliations': ['Fraunhofer IAIS, Germany', 'Lamarr Institute for Machine Learning and Artificial Intelligence, Germany', 'University of Applied Sciences Bonn-Rhein-Sieg, Germany', 'University of Bonn, Computer Science Institute VI, Center for Robotics, Germany'], 'pdf_title_img': 'assets/pdf/title_img/2503.09443.jpg', 'data': {'categories': ['#dataset', '#multilingual', '#multimodal', '#training', '#synthetic', '#machine_translation', '#long_context', '#transfer_learning'], 'emoji': 'ğŸŒ', 'ru': {'title': 'ĞœĞ¾Ğ½Ğ¾Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°ÑÑ‚ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ±Ğ°Ñ€ÑŒĞµÑ€Ñ‹ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ½Ğ¾Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Florenz, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰ÑƒÑ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Florence-2 Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Gemma-2. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ Ñ Ğ½ĞµĞ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¿Ğ¾ĞºÑ€Ñ‹Ñ‚Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ğ¾ÑĞ²Ğ¸Ñ‚ÑŒÑÑ Ğ´Ğ°Ğ¶Ğµ Ğ¿Ñ€Ğ¸ Ğ½Ğ°Ğ»Ğ¸Ñ‡Ğ¸Ğ¸ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ°.'}, 'en': {'title': 'Florenz: Bridging Languages with Monolingual Vision-Language Models', 'desc': 'This paper introduces Florenz, a monolingual vision-language model (VLM) designed to enhance cross-lingual transfer for vision tasks. Unlike existing multilingual models that struggle with performance due to the complexities of multiple languages, Florenz leverages a single language to achieve systematic generalization across various languages. The model is built on a combination of the pre-trained VLM Florence-2 and the large language model Gemma-2, with a focus on scaling laws related to model size and training data. Results show that Florenz can effectively learn to perform image captioning in new languages, even when trained only on translation tasks, demonstrating strong performance in several downstream applications.'}, 'zh': {'title': 'å•è¯­æ¨¡å‹çš„è·¨è¯­è¨€èƒ½åŠ›æ–°çªç ´', 'desc': 'è¿™ç¯‡è®ºæ–‡ç ”ç©¶äº†è·¨è¯­è¨€è¿ç§»å¦‚ä½•ä½¿è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨ä»…ç”¨ä¸€ç§è¯­è¨€çš„è®­ç»ƒæ•°æ®ä¸‹æ‰§è¡Œè§†è§‰ä»»åŠ¡ã€‚å½“å‰çš„æ–¹æ³•ä¾èµ–äºå¤§å‹é¢„è®­ç»ƒçš„å¤šè¯­è¨€æ¨¡å‹ï¼Œä½†åœ¨å¤šè¯­è¨€èƒ½åŠ›ä¸ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ä¹‹é—´å­˜åœ¨æƒè¡¡ã€‚æˆ‘ä»¬æå‡ºäº†Florenzï¼Œä¸€ä¸ªå•è¯­ç¼–ç å™¨-è§£ç å™¨VLMï¼Œç»“åˆäº†é¢„è®­ç»ƒçš„Florence-2å’Œå¤§å‹è¯­è¨€æ¨¡å‹Gemma-2ï¼Œå…·æœ‰0.4Båˆ°11.2Bçš„å‚æ•°ã€‚é€šè¿‡åœ¨åˆæˆæ•°æ®é›†ä¸Šè®­ç»ƒï¼ŒFlorenzå±•ç¤ºäº†å³ä½¿åªæœ‰ç¿»è¯‘ä»»åŠ¡çš„æ•°æ®ï¼Œä¹Ÿèƒ½åœ¨ç‰¹å®šè¯­è¨€ä¸­äº§ç”Ÿå›¾åƒæè¿°èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.10905', 'title': 'Learning to Inference Adaptively for Multimodal Large Language Models', 'url': 'https://huggingface.co/papers/2503.10905', 'abstract': 'Multimodal Large Language Models (MLLMs) have shown impressive capabilities in reasoning, yet come with substantial computational cost, limiting their deployment in resource-constrained settings. Despite recent efforts on improving the efficiency of MLLMs, prior solutions fall short in responding to varying runtime conditions, in particular changing resource availability (e.g., contention due to the execution of other programs on the device). To bridge this gap, we introduce AdaLLaVA, an adaptive inference framework that learns to dynamically reconfigure operations in an MLLM during inference, accounting for the input data and a latency budget. We conduct extensive experiments across benchmarks involving question-answering, reasoning, and hallucination. Our results show that AdaLLaVA effectively adheres to input latency budget, achieving varying accuracy and latency tradeoffs at runtime. Further, we demonstrate that AdaLLaVA adapts to both input latency and content, can be integrated with token selection for enhanced efficiency, and generalizes across MLLMs. Our project webpage with code release is at https://zhuoyan-xu.github.io/ada-llava/.', 'score': 2, 'issue_id': 2791, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 13', 'zh': '3æœˆ13æ—¥'}, 'hash': 'c1a89325eab3a9af', 'authors': ['Zhuoyan Xu', 'Khoi Duc Nguyen', 'Preeti Mukherjee', 'Saurabh Bagchi', 'Somali Chaterji', 'Yingyu Liang', 'Yin Li'], 'affiliations': ['Purdue University', 'The University of Hong Kong', 'University of Wisconsin-Madison'], 'pdf_title_img': 'assets/pdf/title_img/2503.10905.jpg', 'data': {'categories': ['#open_source', '#benchmark', '#reasoning', '#optimization', '#hallucinations', '#multimodal', '#inference'], 'emoji': 'âš¡', 'ru': {'title': 'AdaLLaVA: ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ²Ñ‹Ğ²Ğ¾Ğ´ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'AdaLLaVA - ÑÑ‚Ğ¾ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ€ĞµĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞµ. Ğ­Ñ‚Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ MLLM Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ², Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒÑÑÑŒ Ğº Ğ¼ĞµĞ½ÑÑÑ‰Ğ¸Ğ¼ÑÑ ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑĞ¼ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ AdaLLaVA ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ ÑĞ¾Ğ±Ğ»ÑĞ´Ğ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞµ, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑĞ¾Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒÑ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğº Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°Ñ‚ÑŒÑÑ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ MLLM.'}, 'en': {'title': 'Adaptive Inference for Efficient Multimodal Language Models', 'desc': 'This paper presents AdaLLaVA, an adaptive inference framework designed for Multimodal Large Language Models (MLLMs) to optimize their performance under varying computational resources. It addresses the challenge of maintaining efficiency while responding to different runtime conditions, such as resource contention from other applications. AdaLLaVA dynamically adjusts the operations of MLLMs during inference based on the input data and a specified latency budget, allowing for flexible accuracy and latency trade-offs. The framework has been tested across various benchmarks, demonstrating its ability to adapt to both input characteristics and latency requirements, while also integrating with token selection for improved efficiency.'}, 'zh': {'title': 'è‡ªé€‚åº”æ¨ç†ï¼Œæå‡å¤šæ¨¡æ€æ¨¡å‹æ•ˆç‡', 'desc': 'å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨æ¨ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶è®¡ç®—æˆæœ¬é«˜ï¼Œé™åˆ¶äº†åœ¨èµ„æºå—é™ç¯å¢ƒä¸­çš„åº”ç”¨ã€‚å°½ç®¡è¿‘æœŸå¯¹æé«˜MLLMsæ•ˆç‡çš„åŠªåŠ›æœ‰æ‰€å¢åŠ ï¼Œä½†ç°æœ‰è§£å†³æ–¹æ¡ˆåœ¨åº”å¯¹ä¸åŒè¿è¡Œæ—¶æ¡ä»¶æ–¹é¢ä»æ˜¾ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯åœ¨èµ„æºå¯ç”¨æ€§å˜åŒ–æ—¶ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†AdaLLaVAï¼Œè¿™æ˜¯ä¸€ç§è‡ªé€‚åº”æ¨ç†æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨æ¨ç†è¿‡ç¨‹ä¸­åŠ¨æ€é‡æ–°é…ç½®MLLMçš„æ“ä½œï¼Œè€ƒè™‘è¾“å…¥æ•°æ®å’Œå»¶è¿Ÿé¢„ç®—ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒAdaLLaVAèƒ½å¤Ÿæœ‰æ•ˆéµå¾ªè¾“å…¥å»¶è¿Ÿé¢„ç®—ï¼Œå®ç°è¿è¡Œæ—¶çš„å‡†ç¡®æ€§å’Œå»¶è¿Ÿä¹‹é—´çš„æƒè¡¡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.10546', 'title': 'KUDA: Keypoints to Unify Dynamics Learning and Visual Prompting for\n  Open-Vocabulary Robotic Manipulation', 'url': 'https://huggingface.co/papers/2503.10546', 'abstract': 'With the rapid advancement of large language models (LLMs) and vision-language models (VLMs), significant progress has been made in developing open-vocabulary robotic manipulation systems. However, many existing approaches overlook the importance of object dynamics, limiting their applicability to more complex, dynamic tasks. In this work, we introduce KUDA, an open-vocabulary manipulation system that integrates dynamics learning and visual prompting through keypoints, leveraging both VLMs and learning-based neural dynamics models. Our key insight is that a keypoint-based target specification is simultaneously interpretable by VLMs and can be efficiently translated into cost functions for model-based planning. Given language instructions and visual observations, KUDA first assigns keypoints to the RGB image and queries the VLM to generate target specifications. These abstract keypoint-based representations are then converted into cost functions, which are optimized using a learned dynamics model to produce robotic trajectories. We evaluate KUDA on a range of manipulation tasks, including free-form language instructions across diverse object categories, multi-object interactions, and deformable or granular objects, demonstrating the effectiveness of our framework. The project page is available at http://kuda-dynamics.github.io.', 'score': 2, 'issue_id': 2776, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 13', 'zh': '3æœˆ13æ—¥'}, 'hash': 'f856affbe8bcf064', 'authors': ['Zixian Liu', 'Mingtong Zhang', 'Yunzhu Li'], 'affiliations': ['Columbia University', 'Tsinghua University', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2503.10546.jpg', 'data': {'categories': ['#multimodal', '#optimization', '#robotics', '#open_source', '#agents'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'KUDA: Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ ÑĞ»Ğ¾Ğ²Ğ°Ñ€ĞµĞ¼', 'desc': 'KUDA - ÑÑ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ ÑĞ»Ğ¾Ğ²Ğ°Ñ€ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞµ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ñ‚Ğ¾Ñ‡ĞºĞ¸. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ (VLM) Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°. KUDA ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡Ğ°ĞµÑ‚ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ½Ğ° RGB-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¸ Ğ¸ Ğ·Ğ°Ğ¿Ñ€Ğ°ÑˆĞ¸Ğ²Ğ°ĞµÑ‚ VLM Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ñ… ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¹. Ğ—Ğ°Ñ‚ĞµĞ¼ ÑÑ‚Ğ¸ Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ñ‚Ğ¾Ñ‡ĞµĞº Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒÑÑ‚ÑÑ Ğ² Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸.'}, 'en': {'title': 'KUDA: Bridging Language and Dynamics for Robotic Manipulation', 'desc': 'This paper presents KUDA, an innovative open-vocabulary robotic manipulation system that incorporates dynamics learning and visual prompting. Unlike previous methods, KUDA utilizes keypoints to create target specifications that are understandable by vision-language models (VLMs) and can be transformed into cost functions for planning. The system processes language instructions and visual data to assign keypoints to images, which are then optimized using a learned dynamics model to generate robotic movements. The effectiveness of KUDA is validated through various manipulation tasks, showcasing its ability to handle complex interactions with different object types.'}, 'zh': {'title': 'KUDAï¼šåŠ¨æ€å­¦ä¹ ä¸è§†è§‰æç¤ºçš„å¼€æ”¾è¯æ±‡æ“ä½œç³»ç»Ÿ', 'desc': 'éšç€å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œå¼€æ”¾è¯æ±‡çš„æœºå™¨äººæ“ä½œç³»ç»Ÿå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œè®¸å¤šç°æœ‰æ–¹æ³•å¿½è§†äº†ç‰©ä½“åŠ¨æ€çš„é‡è¦æ€§ï¼Œé™åˆ¶äº†å®ƒä»¬åœ¨æ›´å¤æ‚åŠ¨æ€ä»»åŠ¡ä¸­çš„é€‚ç”¨æ€§ã€‚æˆ‘ä»¬æå‡ºäº†KUDAï¼Œä¸€ä¸ªé›†æˆäº†åŠ¨æ€å­¦ä¹ å’Œé€šè¿‡å…³é”®ç‚¹è¿›è¡Œè§†è§‰æç¤ºçš„å¼€æ”¾è¯æ±‡æ“ä½œç³»ç»Ÿï¼Œåˆ©ç”¨äº†VLMså’ŒåŸºäºå­¦ä¹ çš„ç¥ç»åŠ¨æ€æ¨¡å‹ã€‚KUDAé€šè¿‡å°†å…³é”®ç‚¹åˆ†é…ç»™RGBå›¾åƒï¼Œå¹¶æŸ¥è¯¢VLMç”Ÿæˆç›®æ ‡è§„èŒƒï¼Œå°†æŠ½è±¡çš„å…³é”®ç‚¹è¡¨ç¤ºè½¬æ¢ä¸ºæˆæœ¬å‡½æ•°ï¼Œä»è€Œä¼˜åŒ–æœºå™¨äººè½¨è¿¹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.10410', 'title': 'RoCo-Sim: Enhancing Roadside Collaborative Perception through Foreground\n  Simulation', 'url': 'https://huggingface.co/papers/2503.10410', 'abstract': 'Roadside Collaborative Perception refers to a system where multiple roadside units collaborate to pool their perceptual data, assisting vehicles in enhancing their environmental awareness. Existing roadside perception methods concentrate on model design but overlook data issues like calibration errors, sparse information, and multi-view consistency, leading to poor performance on recent published datasets. To significantly enhance roadside collaborative perception and address critical data issues, we present the first simulation framework RoCo-Sim for road-side collaborative perception. RoCo-Sim is capable of generating diverse, multi-view consistent simulated roadside data through dynamic foreground editing and full-scene style transfer of a single image. RoCo-Sim consists of four components: (1) Camera Extrinsic Optimization ensures accurate 3D to 2D projection for roadside cameras; (2) A novel Multi-View Occlusion-Aware Sampler (MOAS) determines the placement of diverse digital assets within 3D space; (3) DepthSAM innovatively models foreground-background relationships from single-frame fixed-view images, ensuring multi-view consistency of foreground; and (4) Scalable Post-Processing Toolkit generates more realistic and enriched scenes through style transfer and other enhancements. RoCo-Sim significantly improves roadside 3D object detection, outperforming SOTA methods by 83.74 on Rcooper-Intersection and 83.12 on TUMTraf-V2X for AP70. RoCo-Sim fills a critical gap in roadside perception simulation. Code and pre-trained models will be released soon: https://github.com/duyuwen-duen/RoCo-Sim', 'score': 2, 'issue_id': 2776, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 13', 'zh': '3æœˆ13æ—¥'}, 'hash': '44150f611040e79d', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#3d', '#optimization', '#data', '#dataset', '#cv', '#open_source'], 'emoji': 'ğŸš—', 'ru': {'title': 'RoCo-Sim: Ğ¿Ñ€Ğ¾Ñ€Ñ‹Ğ² Ğ² ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ´Ğ¾Ñ€Ğ¾Ğ¶Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ', 'desc': 'RoCo-Sim - ÑÑ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ´Ğ¾Ñ€Ğ¾Ğ¶Ğ½Ğ¾Ğ¹ Ğ¾Ğ±ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ¸. ĞĞ½Ğ° Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞ¸, Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. RoCo-Sim Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² ĞºĞ°Ğ¼ĞµÑ€, Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ñ‹Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ñ‰Ğ¸Ğº Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¾ĞºĞºĞ»ÑĞ·Ğ¸Ğ¹, Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¹ Ğ¿ĞµÑ€ĞµĞ´Ğ½Ğ¸Ğ¹ Ğ¿Ğ»Ğ°Ğ½-Ñ„Ğ¾Ğ½ Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ 3D-Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Enhancing Roadside Awareness with Collaborative Perception', 'desc': 'This paper introduces Roadside Collaborative Perception, a system where multiple roadside units work together to improve vehicle awareness of their surroundings. It highlights the limitations of current methods that focus mainly on model design while neglecting important data issues such as calibration errors and sparse information. To address these challenges, the authors present RoCo-Sim, a simulation framework that generates diverse and consistent roadside data using advanced techniques like dynamic foreground editing. RoCo-Sim significantly enhances roadside 3D object detection performance, surpassing state-of-the-art methods on benchmark datasets.'}, 'zh': {'title': 'æå‡è·¯è¾¹æ„ŸçŸ¥çš„ååŒåŠ›é‡', 'desc': 'è·¯è¾¹ååŒæ„ŸçŸ¥æ˜¯ä¸€ç§ç³»ç»Ÿï¼Œå¤šä¸ªè·¯è¾¹å•å…ƒåä½œæ±‡èšæ„ŸçŸ¥æ•°æ®ï¼Œå¸®åŠ©è½¦è¾†æé«˜ç¯å¢ƒæ„è¯†ã€‚ç°æœ‰çš„è·¯è¾¹æ„ŸçŸ¥æ–¹æ³•ä¸»è¦å…³æ³¨æ¨¡å‹è®¾è®¡ï¼Œä½†å¿½è§†äº†æ•°æ®é—®é¢˜ï¼Œå¦‚æ ¡å‡†è¯¯å·®ã€ä¿¡æ¯ç¨€ç–å’Œå¤šè§†å›¾ä¸€è‡´æ€§ï¼Œå¯¼è‡´åœ¨æœ€æ–°æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¸ä½³ã€‚ä¸ºæ˜¾è‘—æå‡è·¯è¾¹ååŒæ„ŸçŸ¥å¹¶è§£å†³å…³é”®æ•°æ®é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†é¦–ä¸ªè·¯è¾¹ååŒæ„ŸçŸ¥æ¨¡æ‹Ÿæ¡†æ¶RoCo-Simã€‚RoCo-Simèƒ½å¤Ÿé€šè¿‡åŠ¨æ€å‰æ™¯ç¼–è¾‘å’Œå•å›¾åƒçš„å…¨åœºæ™¯é£æ ¼è¿ç§»ç”Ÿæˆå¤šæ ·åŒ–çš„ã€å¤šè§†å›¾ä¸€è‡´çš„æ¨¡æ‹Ÿè·¯è¾¹æ•°æ®ã€‚'}}, 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2503.08893', 'title': 'EvalTree: Profiling Language Model Weaknesses via Hierarchical\n  Capability Trees', 'url': 'https://huggingface.co/papers/2503.08893', 'abstract': "An ideal model evaluation should achieve two goals: identifying where the model fails and providing actionable improvement guidance. Toward these goals for Language Model (LM) evaluations, we formulate the problem of generating a weakness profile, a set of weaknesses expressed in natural language, given an LM's performance on every individual instance in a benchmark. We introduce a suite of quantitative assessments to compare different weakness profiling methods. We also propose a weakness profiling method EvalTree. It constructs a capability tree where each node represents a capability described in natural language and is linked to a subset of benchmark instances that specifically evaluate this capability; it then extracts nodes where the LM performs poorly to generate a weakness profile. On the MATH and WildChat benchmarks, we show that EvalTree outperforms baseline weakness profiling methods by identifying weaknesses more precisely and comprehensively. Weakness profiling further enables weakness-guided data collection, and training data collection guided by EvalTree-identified weaknesses improves LM performance more than other data collection strategies. We also show how EvalTree exposes flaws in Chatbot Arena's human-voter-based evaluation practice. To facilitate future work, we release our code and an interface that allows practitioners to interactively explore the capability trees built by EvalTree.", 'score': 2, 'issue_id': 2790, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 11', 'zh': '3æœˆ11æ—¥'}, 'hash': '8645a737473c8209', 'authors': ['Zhiyuan Zeng', 'Yizhong Wang', 'Hannaneh Hajishirzi', 'Pang Wei Koh'], 'affiliations': ['Allen Institute for Artificial Intelligence', 'Paul G. Allen School of Computer Science & Engineering, University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2503.08893.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#data', '#interpretability', '#training', '#open_source'], 'emoji': 'ğŸŒ³', 'ru': {'title': 'EvalTree: Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ»Ğ°Ğ±Ğ¾ÑÑ‚ĞµĞ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ EvalTree Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»Ñ ÑĞ»Ğ°Ğ±Ğ¾ÑÑ‚ĞµĞ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. EvalTree ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ Ğ´ĞµÑ€ĞµĞ²Ğ¾ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹, Ğ³Ğ´Ğµ ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ ÑƒĞ·ĞµĞ» ÑĞ²ÑĞ·Ğ°Ğ½ Ñ Ğ¿Ğ¾Ğ´Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ², Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ñ… ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ² Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚Ğµ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞ»Ğ°Ğ±Ğ¾ÑÑ‚ĞµĞ¹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… MATH Ğ¸ WildChat. ĞŸÑ€Ğ¾Ñ„Ğ¸Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ»Ğ°Ğ±Ğ¾ÑÑ‚ĞµĞ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ†ĞµĞ»ĞµĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾ ÑĞ¾Ğ±Ğ¸Ñ€Ğ°Ñ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ĞµĞµ Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹.'}, 'en': {'title': 'Uncovering Weaknesses for Stronger Language Models', 'desc': 'This paper focuses on improving the evaluation of Language Models (LMs) by creating a weakness profile that identifies specific areas where the model underperforms. The authors introduce EvalTree, a method that organizes model capabilities into a tree structure, linking each capability to benchmark instances that test it. By analyzing these capabilities, EvalTree generates a detailed profile of weaknesses, allowing for targeted improvements in model training. The results demonstrate that using EvalTree leads to better performance in LMs by guiding data collection based on identified weaknesses, surpassing traditional evaluation methods.'}, 'zh': {'title': 'è¯†åˆ«æ¨¡å‹å¼±ç‚¹ï¼Œæå‡è¯­è¨€æ¨¡å‹æ€§èƒ½', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è¯­è¨€æ¨¡å‹è¯„ä¼°æ–¹æ³•ï¼Œæ—¨åœ¨è¯†åˆ«æ¨¡å‹çš„å¼±ç‚¹å¹¶æä¾›æ”¹è¿›å»ºè®®ã€‚æˆ‘ä»¬å¼•å…¥äº†å¼±ç‚¹åˆ†æçš„æ¦‚å¿µï¼Œé€šè¿‡ç”Ÿæˆå¼±ç‚¹æ¡£æ¡ˆæ¥æè¿°æ¨¡å‹åœ¨åŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°ã€‚æˆ‘ä»¬æå‡ºçš„EvalTreeæ–¹æ³•æ„å»ºäº†ä¸€ä¸ªèƒ½åŠ›æ ‘ï¼Œèƒ½å¤Ÿæ›´å‡†ç¡®åœ°è¯†åˆ«æ¨¡å‹çš„ä¸è¶³ä¹‹å¤„ã€‚é€šè¿‡åœ¨MATHå’ŒWildChatåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒï¼ŒEvalTreeæ˜¾ç¤ºå‡ºæ¯”ä¼ ç»Ÿæ–¹æ³•æ›´ä¼˜è¶Šçš„æ€§èƒ½ï¼Œå¹¶ä¸”èƒ½å¤ŸæŒ‡å¯¼æ•°æ®æ”¶é›†ä»¥æå‡æ¨¡å‹è¡¨ç°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.14002', 'title': 'MeshFleet: Filtered and Annotated 3D Vehicle Dataset for Domain Specific\n  Generative Modeling', 'url': 'https://huggingface.co/papers/2503.14002', 'abstract': 'Generative models have recently made remarkable progress in the field of 3D objects. However, their practical application in fields like engineering remains limited since they fail to deliver the accuracy, quality, and controllability needed for domain-specific tasks. Fine-tuning large generative models is a promising perspective for making these models available in these fields. Creating high-quality, domain-specific 3D datasets is crucial for fine-tuning large generative models, yet the data filtering and annotation process remains a significant bottleneck. We present MeshFleet, a filtered and annotated 3D vehicle dataset extracted from Objaverse-XL, the most extensive publicly available collection of 3D objects. Our approach proposes a pipeline for automated data filtering based on a quality classifier. This classifier is trained on a manually labeled subset of Objaverse, incorporating DINOv2 and SigLIP embeddings, refined through caption-based analysis and uncertainty estimation. We demonstrate the efficacy of our filtering method through a comparative analysis against caption and image aesthetic score-based techniques and fine-tuning experiments with SV3D, highlighting the importance of targeted data selection for domain-specific 3D generative modeling.', 'score': 1, 'issue_id': 2789, 'pub_date': '2025-03-18', 'pub_date_card': {'ru': '18 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 18', 'zh': '3æœˆ18æ—¥'}, 'hash': '8affb04cd78c70c9', 'authors': ['Damian Boborzi', 'Phillip Mueller', 'Jonas Emrich', 'Dominik Schmid', 'Sebastian Mueller', 'Lars Mikelsons'], 'affiliations': ['BMW Group', 'University of Augsburg'], 'pdf_title_img': 'assets/pdf/title_img/2503.14002.jpg', 'data': {'categories': ['#dataset', '#synthetic', '#optimization', '#data', '#3d'], 'emoji': 'ğŸš—', 'ru': {'title': 'MeshFleet: ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ 3D Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‚Ñ€Ğ°Ğ½ÑĞ¿Ğ¾Ñ€Ñ‚Ğ½Ñ‹Ñ… ÑÑ€ĞµĞ´ÑÑ‚Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MeshFleet - Ğ¾Ñ‚Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… 3D Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‚Ñ€Ğ°Ğ½ÑĞ¿Ğ¾Ñ€Ñ‚Ğ½Ñ‹Ñ… ÑÑ€ĞµĞ´ÑÑ‚Ğ², Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¸Ğ· Objaverse-XL. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ° Ğ²Ñ€ÑƒÑ‡Ğ½ÑƒÑ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ´Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğµ Objaverse Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² DINOv2 Ğ¸ SigLIP. Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ¿ÑƒÑ‚ĞµĞ¼ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ½Ğ° Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑÑÑ… Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ñ… ÑÑÑ‚ĞµÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ñ†ĞµĞ»ĞµĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ½Ğ¾-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ 3D Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'Enhancing 3D Generative Models with Targeted Data Selection', 'desc': "This paper discusses advancements in generative models for creating 3D objects, particularly focusing on their application in engineering. It highlights the challenges of achieving the necessary accuracy and quality for specific tasks, which can be addressed by fine-tuning these models with high-quality datasets. The authors introduce MeshFleet, a curated 3D vehicle dataset derived from Objaverse-XL, aimed at improving the training process of generative models. They also present a novel automated data filtering method using a quality classifier, which enhances the dataset's relevance and effectiveness for domain-specific applications."}, 'zh': {'title': 'MeshFleetï¼šæå‡3Dç”Ÿæˆæ¨¡å‹çš„å…³é”®æ•°æ®é›†', 'desc': 'ç”Ÿæˆæ¨¡å‹åœ¨3Dç‰©ä½“é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨å·¥ç¨‹ç­‰é¢†åŸŸçš„å®é™…åº”ç”¨ä»ç„¶æœ‰é™ï¼Œå› ä¸ºå®ƒä»¬æ— æ³•æä¾›æ‰€éœ€çš„å‡†ç¡®æ€§ã€è´¨é‡å’Œå¯æ§æ€§ã€‚å¯¹å¤§å‹ç”Ÿæˆæ¨¡å‹è¿›è¡Œå¾®è°ƒæ˜¯ä½¿è¿™äº›æ¨¡å‹åœ¨ç‰¹å®šé¢†åŸŸå¯ç”¨çš„æœ‰å‰æ™¯çš„æ–¹æ³•ã€‚åˆ›å»ºé«˜è´¨é‡ã€ç‰¹å®šé¢†åŸŸçš„3Dæ•°æ®é›†å¯¹äºå¾®è°ƒå¤§å‹ç”Ÿæˆæ¨¡å‹è‡³å…³é‡è¦ï¼Œä½†æ•°æ®è¿‡æ»¤å’Œæ ‡æ³¨è¿‡ç¨‹ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§ç“¶é¢ˆã€‚æˆ‘ä»¬æå‡ºäº†MeshFleetï¼Œè¿™æ˜¯ä¸€ä¸ªä»Objaverse-XLæå–çš„ç»è¿‡è¿‡æ»¤å’Œæ ‡æ³¨çš„3Dè½¦è¾†æ•°æ®é›†ï¼Œå±•ç¤ºäº†åŸºäºè´¨é‡åˆ†ç±»å™¨çš„è‡ªåŠ¨åŒ–æ•°æ®è¿‡æ»¤æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.13661', 'title': 'Pensez: Less Data, Better Reasoning -- Rethinking French LLM', 'url': 'https://huggingface.co/papers/2503.13661', 'abstract': 'Large language models (LLMs) have demonstrated remarkable capabilities in various natural language processing tasks. However, achieving strong performance in specialized domains like mathematical reasoning and non-English languages often requires extensive training on massive datasets. This paper investigates a contrasting approach: strategic fine-tuning on a small, high-quality, bilingual (English-French) dataset to enhance both the reasoning capabilities and French language proficiency of a large language model. Rather than relying on scale, we explore the hypothesis that targeted data curation and optimized training can achieve competitive, or even superior, performance. We demonstrate, through targeted supervised fine-tuning (SFT) on only 2,000 carefully selected samples, significant improvements in mathematical reasoning. Specifically, Pensez 7B exhibits an increase in accuracy of the base model up to 20% on the AIME25 and a 12% increase on a French MATH level 5 benchmark. These results challenge the prevailing assumption that massive datasets are aprerequisite for strong reasoning performance in LLMs, highlighting the potential of strategic data curation and optimized fine-tuning for enhancing both specialized skills and multilingual capabilities. Our findings have implications for the efficient development of high-performing, multilingual LLMs, especially in resource-constrained scenarios.', 'score': 1, 'issue_id': 2784, 'pub_date': '2025-03-17', 'pub_date_card': {'ru': '17 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 17', 'zh': '3æœˆ17æ—¥'}, 'hash': '248eff76119a7839', 'authors': ['Huy Hoang Ha'], 'affiliations': ['Menlo Research', 'UniversitÃ© Grenoble Alpes'], 'pdf_title_img': 'assets/pdf/title_img/2503.13661.jpg', 'data': {'categories': ['#reasoning', '#dataset', '#multilingual', '#data', '#training', '#transfer_learning', '#low_resource'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞœĞ°Ğ»Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚: Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ', 'desc': 'Ğ”Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¿ÑƒÑ‚ĞµĞ¼ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ½Ğ° Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼, Ğ½Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ´Ğ²ÑƒÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ¸ Ğ²Ğ»Ğ°Ğ´ĞµĞ½Ğ¸Ñ Ñ„Ñ€Ğ°Ğ½Ñ†ÑƒĞ·ÑĞºĞ¸Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ¼ Ğ¿Ğ¾ÑĞ»Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ²ÑĞµĞ³Ğ¾ 2000 Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ñ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Pensez 7B Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¾ 20% Ğ½Ğ° Ñ‚ĞµÑÑ‚Ğµ AIME25 Ğ¸ Ğ½Ğ° 12% Ğ½Ğ° Ñ„Ñ€Ğ°Ğ½Ñ†ÑƒĞ·ÑĞºĞ¾Ğ¼ ÑĞºĞ²Ğ¸Ğ²Ğ°Ğ»ĞµĞ½Ñ‚Ğµ Ñ‚ĞµÑÑ‚Ğ° MATH ÑƒÑ€Ğ¾Ğ²Ğ½Ñ 5. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑÑ‚Ğ°Ğ²ÑÑ‚ Ğ¿Ğ¾Ğ´ ÑĞ¾Ğ¼Ğ½ĞµĞ½Ğ¸Ğµ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ³Ñ€Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ LLM Ğ² ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ….'}, 'en': {'title': 'Strategic Fine-Tuning: Small Data, Big Gains!', 'desc': 'This paper explores how to improve large language models (LLMs) in specialized areas like math and French without needing huge datasets. It focuses on strategic fine-tuning using a small, high-quality bilingual dataset of only 2,000 samples. The results show that this targeted approach can significantly boost reasoning accuracy, achieving up to a 20% improvement in mathematical tasks. This challenges the idea that only large datasets can lead to strong performance, suggesting that careful data selection and training can be more effective.'}, 'zh': {'title': 'å°æ•°æ®é›†ä¹Ÿèƒ½æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨æ•°å­¦æ¨ç†å’Œéè‹±è¯­è¯­è¨€ç­‰ä¸“ä¸šé¢†åŸŸçš„è¡¨ç°é€šå¸¸éœ€è¦å¤§é‡æ•°æ®çš„è®­ç»ƒã€‚æœ¬æ–‡æ¢è®¨äº†ä¸€ç§ä¸åŒçš„æ–¹æ³•ï¼šé€šè¿‡åœ¨å°è§„æ¨¡é«˜è´¨é‡çš„åŒè¯­ï¼ˆè‹±è¯­-æ³•è¯­ï¼‰æ•°æ®é›†ä¸Šè¿›è¡Œæˆ˜ç•¥æ€§å¾®è°ƒï¼Œä»¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’Œæ³•è¯­æ°´å¹³ã€‚æˆ‘ä»¬é€šè¿‡å¯¹ä»…2000ä¸ªç²¾å¿ƒæŒ‘é€‰çš„æ ·æœ¬è¿›è¡Œæœ‰é’ˆå¯¹æ€§çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œåœ¨æ•°å­¦æ¨ç†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æå‡ï¼ŒPensez 7Båœ¨AIME25ä¸Šçš„å‡†ç¡®ç‡æé«˜äº†20%ï¼Œåœ¨æ³•è¯­MATH 5çº§åŸºå‡†ä¸Šæé«˜äº†12%ã€‚è¿™äº›ç»“æœæŒ‘æˆ˜äº†å¤§è§„æ¨¡æ•°æ®é›†æ˜¯LLMså¼ºå¤§æ¨ç†æ€§èƒ½çš„å…ˆå†³æ¡ä»¶çš„æ™®éå‡è®¾ï¼Œçªæ˜¾äº†æˆ˜ç•¥æ•°æ®ç­–åˆ’å’Œä¼˜åŒ–å¾®è°ƒåœ¨æå‡ä¸“ä¸šæŠ€èƒ½å’Œå¤šè¯­è¨€èƒ½åŠ›æ–¹é¢çš„æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.12127', 'title': 'Hyperbolic Safety-Aware Vision-Language Models', 'url': 'https://huggingface.co/papers/2503.12127', 'abstract': "Addressing the retrieval of unsafe content from vision-language models such as CLIP is an important step towards real-world integration. Current efforts have relied on unlearning techniques that try to erase the model's knowledge of unsafe concepts. While effective in reducing unwanted outputs, unlearning limits the model's capacity to discern between safe and unsafe content. In this work, we introduce a novel approach that shifts from unlearning to an awareness paradigm by leveraging the inherent hierarchical properties of the hyperbolic space. We propose to encode safe and unsafe content as an entailment hierarchy, where both are placed in different regions of hyperbolic space. Our HySAC, Hyperbolic Safety-Aware CLIP, employs entailment loss functions to model the hierarchical and asymmetrical relations between safe and unsafe image-text pairs. This modelling, ineffective in standard vision-language models due to their reliance on Euclidean embeddings, endows the model with awareness of unsafe content, enabling it to serve as both a multimodal unsafe classifier and a flexible content retriever, with the option to dynamically redirect unsafe queries toward safer alternatives or retain the original output. Extensive experiments show that our approach not only enhances safety recognition but also establishes a more adaptable and interpretable framework for content moderation in vision-language models. Our source code is available at https://github.com/aimagelab/HySAC.", 'score': 1, 'issue_id': 2784, 'pub_date': '2025-03-15', 'pub_date_card': {'ru': '15 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 15', 'zh': '3æœˆ15æ—¥'}, 'hash': 'fe42b5bcd1d5d4be', 'authors': ['Tobia Poppi', 'Tejaswi Kasarla', 'Pascal Mettes', 'Lorenzo Baraldi', 'Rita Cucchiara'], 'affiliations': ['IIT-CNR, Italy', 'University of Amsterdam, Netherlands', 'University of Modena and Reggio Emilia, Italy', 'University of Pisa, Italy'], 'pdf_title_img': 'assets/pdf/title_img/2503.12127.jpg', 'data': {'categories': ['#interpretability', '#multimodal', '#open_source', '#cv', '#security'], 'emoji': 'ğŸ›¡ï¸', 'ru': {'title': 'Ğ“Ğ¸Ğ¿ĞµÑ€Ğ±Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾ÑĞ¾Ğ·Ğ½Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ½ĞµĞ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº CLIP. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¾ Ğ½ĞµĞ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸ÑÑ…, Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ³Ğ¸Ğ¿ĞµÑ€Ğ±Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾Ğ³Ğ¾ Ğ¸ Ğ½ĞµĞ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ HySAC (Hyperbolic Safety-Aware CLIP) Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ½ĞµĞ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ-Ñ‚ĞµĞºÑÑ‚. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ³Ğ¸Ğ±ĞºÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¼Ğ¾Ğ´ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°.'}, 'en': {'title': 'Enhancing Safety Awareness in Vision-Language Models with Hyperbolic Space', 'desc': 'This paper addresses the challenge of retrieving unsafe content from vision-language models like CLIP by introducing a new approach called Hyperbolic Safety-Aware CLIP (HySAC). Instead of using unlearning techniques that erase knowledge of unsafe concepts, HySAC utilizes the hierarchical properties of hyperbolic space to encode safe and unsafe content as an entailment hierarchy. By employing entailment loss functions, the model can better understand the relationships between safe and unsafe image-text pairs, enhancing its ability to classify and moderate content. The results demonstrate that HySAC improves safety recognition and offers a more flexible framework for content moderation in multimodal applications.'}, 'zh': {'title': 'æå‡è§†è§‰-è¯­è¨€æ¨¡å‹çš„å®‰å…¨æ€§æ„è¯†', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†å¦‚ä½•ä»è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰ä¸­æ£€ç´¢ä¸å®‰å…¨å†…å®¹ã€‚å½“å‰çš„æ–¹æ³•ä¸»è¦ä¾èµ–äºæ¶ˆé™¤å­¦ä¹ æŠ€æœ¯ï¼Œä½†è¿™ç§æ–¹æ³•é™åˆ¶äº†æ¨¡å‹åŒºåˆ†å®‰å…¨å’Œä¸å®‰å…¨å†…å®¹çš„èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œåˆ©ç”¨åŒæ›²ç©ºé—´çš„å±‚æ¬¡ç‰¹æ€§ï¼Œå°†å®‰å…¨å’Œä¸å®‰å…¨å†…å®¹ç¼–ç ä¸ºè•´å«å±‚æ¬¡ç»“æ„ã€‚æˆ‘ä»¬çš„HySACæ¨¡å‹é€šè¿‡è•´å«æŸå¤±å‡½æ•°å»ºæ¨¡å®‰å…¨ä¸ä¸å®‰å…¨å›¾åƒ-æ–‡æœ¬å¯¹ä¹‹é—´çš„å±‚æ¬¡å’Œä¸å¯¹ç§°å…³ç³»ï¼Œä»è€Œå¢å¼ºäº†æ¨¡å‹å¯¹ä¸å®‰å…¨å†…å®¹çš„æ„è¯†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.08683', 'title': 'CoLMDriver: LLM-based Negotiation Benefits Cooperative Autonomous\n  Driving', 'url': 'https://huggingface.co/papers/2503.08683', 'abstract': 'Vehicle-to-vehicle (V2V) cooperative autonomous driving holds great promise for improving safety by addressing the perception and prediction uncertainties inherent in single-agent systems. However, traditional cooperative methods are constrained by rigid collaboration protocols and limited generalization to unseen interactive scenarios. While LLM-based approaches offer generalized reasoning capabilities, their challenges in spatial planning and unstable inference latency hinder their direct application in cooperative driving. To address these limitations, we propose CoLMDriver, the first full-pipeline LLM-based cooperative driving system, enabling effective language-based negotiation and real-time driving control. CoLMDriver features a parallel driving pipeline with two key components: (i) an LLM-based negotiation module under an actor-critic paradigm, which continuously refines cooperation policies through feedback from previous decisions of all vehicles; and (ii) an intention-guided waypoint generator, which translates negotiation outcomes into executable waypoints. Additionally, we introduce InterDrive, a CARLA-based simulation benchmark comprising 10 challenging interactive driving scenarios for evaluating V2V cooperation. Experimental results demonstrate that CoLMDriver significantly outperforms existing approaches, achieving an 11% higher success rate across diverse highly interactive V2V driving scenarios. Code will be released on https://github.com/cxliu0314/CoLMDriver.', 'score': 1, 'issue_id': 2790, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 11', 'zh': '3æœˆ11æ—¥'}, 'hash': 'ef46cd2c17c64880', 'authors': ['Changxing Liu', 'Genjia Liu', 'Zijun Wang', 'Jinchang Yang', 'Siheng Chen'], 'affiliations': ['Multi-Agent Governance & Intelligence Crew (MAGIC)', 'Shanghai AI Laboratory', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2503.08683.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#optimization', '#rl', '#agents'], 'emoji': 'ğŸš—', 'ru': {'title': 'Ğ¯Ğ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ ÑÑ€Ñƒ Ğ² ĞºĞ¾Ğ¾Ğ¿ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ¼ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¸', 'desc': 'CoLMDriver - ÑÑ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° ĞºĞ¾Ğ¾Ğ¿ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM), Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‰Ğ°Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¿ĞµÑ€ĞµĞ³Ğ¾Ğ²Ğ¾Ñ€Ñ‹ Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ¿ĞµÑ€ĞµĞ³Ğ¾Ğ²Ğ¾Ñ€Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLM Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ Ğ¿ÑƒÑ‚ĞµĞ²Ñ‹Ñ… Ñ‚Ğ¾Ñ‡ĞµĞº, ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ğ¹ Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ¸ÑĞ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ InterDrive - ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ½Ğ° Ğ±Ğ°Ğ·Ğµ CARLA Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ¾Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚Ñ€Ğ°Ğ½ÑĞ¿Ğ¾Ñ€Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ ÑÑ€ĞµĞ´ÑÑ‚Ğ²Ğ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ CoLMDriver Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ½Ğ° 11% Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ¾Ğ±Ğ¸Ğ»ĞµĞ¹.'}, 'en': {'title': 'Revolutionizing V2V Cooperation with CoLMDriver!', 'desc': 'This paper introduces CoLMDriver, a novel system for vehicle-to-vehicle (V2V) cooperative autonomous driving that leverages large language models (LLMs) for improved safety and decision-making. CoLMDriver addresses the limitations of traditional methods by implementing a full-pipeline approach that includes a negotiation module and a waypoint generator, allowing vehicles to communicate and plan collaboratively in real-time. The negotiation module uses an actor-critic framework to adaptively refine cooperation strategies based on feedback from previous interactions. Experimental results show that CoLMDriver outperforms existing systems, achieving a higher success rate in complex driving scenarios, demonstrating its effectiveness in enhancing V2V cooperation.'}, 'zh': {'title': 'CoLMDriverï¼šæ™ºèƒ½åä½œé©¾é©¶çš„æ–°çªç ´', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºCoLMDriverçš„å…¨æµç¨‹åŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åˆä½œé©¾é©¶ç³»ç»Ÿï¼Œæ—¨åœ¨æé«˜è½¦è¾†é—´çš„å®‰å…¨æ€§ã€‚è¯¥ç³»ç»Ÿé€šè¿‡è¯­è¨€åŸºç¡€çš„åå•†å’Œå®æ—¶é©¾é©¶æ§åˆ¶ï¼Œå…‹æœäº†ä¼ ç»Ÿåˆä½œæ–¹æ³•çš„å±€é™æ€§ã€‚CoLMDriveråŒ…å«ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šåŸºäºæ¼”å‘˜-è¯„è®ºå®¶èŒƒå¼çš„åå•†æ¨¡å—å’Œæ„å›¾å¼•å¯¼çš„è·¯å¾„ç‚¹ç”Ÿæˆå™¨ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°å°†åå•†ç»“æœè½¬åŒ–ä¸ºå¯æ‰§è¡Œçš„è·¯å¾„ç‚¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCoLMDriveråœ¨å¤šç§é«˜åº¦äº’åŠ¨çš„è½¦è¾†é—´é©¾é©¶åœºæ™¯ä¸­ï¼ŒæˆåŠŸç‡æ¯”ç°æœ‰æ–¹æ³•æé«˜äº†11%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.21776', 'title': 'Video-R1: Reinforcing Video Reasoning in MLLMs', 'url': 'https://huggingface.co/papers/2503.21776', 'abstract': "Inspired by DeepSeek-R1's success in eliciting reasoning abilities through rule-based reinforcement learning (RL), we introduce Video-R1 as the first attempt to systematically explore the R1 paradigm for eliciting video reasoning within multimodal large language models (MLLMs). However, directly applying RL training with the GRPO algorithm to video reasoning presents two primary challenges: (i) a lack of temporal modeling for video reasoning, and (ii) the scarcity of high-quality video-reasoning data. To address these issues, we first propose the T-GRPO algorithm, which encourages models to utilize temporal information in videos for reasoning. Additionally, instead of relying solely on video data, we incorporate high-quality image-reasoning data into the training process. We have constructed two datasets: Video-R1-COT-165k for SFT cold start and Video-R1-260k for RL training, both comprising image and video data. Experimental results demonstrate that Video-R1 achieves significant improvements on video reasoning benchmarks such as VideoMMMU and VSI-Bench, as well as on general video benchmarks including MVBench and TempCompass, etc. Notably, Video-R1-7B attains a 35.8% accuracy on video spatial reasoning benchmark VSI-bench, surpassing the commercial proprietary model GPT-4o. All codes, models, data are released.", 'score': 66, 'issue_id': 2941, 'pub_date': '2025-03-27', 'pub_date_card': {'ru': '27 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 27', 'zh': '3æœˆ27æ—¥'}, 'hash': 'f88fc5679e0ee30c', 'authors': ['Kaituo Feng', 'Kaixiong Gong', 'Bohao Li', 'Zonghao Guo', 'Yibing Wang', 'Tianshuo Peng', 'Benyou Wang', 'Xiangyu Yue'], 'affiliations': ['CUHK (SZ)', 'CUHK MMLab', 'Tsinghua University', 'UCAS'], 'pdf_title_img': 'assets/pdf/title_img/2503.21776.jpg', 'data': {'categories': ['#reasoning', '#rl', '#open_source', '#benchmark', '#dataset', '#multimodal', '#optimization'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Video-R1: ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾-Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ… Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Video-R1 - Ğ¿ĞµÑ€Ğ²ÑƒÑ Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚ĞºÑƒ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ñ‚ÑŒ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ R1 Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ²Ğ¸Ğ´ĞµĞ¾-Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (MLLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ T-GRPO Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾-Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ…. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ÑÑ ĞºĞ°Ğº Ğ²Ğ¸Ğ´ĞµĞ¾-, Ñ‚Ğ°Ğº Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ-Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Video-R1 Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾-Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Enhancing Video Reasoning with Temporal Insights and Image Data', 'desc': 'This paper presents Video-R1, a novel approach to enhance video reasoning capabilities in multimodal large language models (MLLMs) using rule-based reinforcement learning (RL). The authors identify two main challenges: the need for better temporal modeling in video reasoning and the limited availability of high-quality video-reasoning data. To overcome these challenges, they introduce the T-GRPO algorithm, which leverages temporal information, and augment the training process with high-quality image-reasoning data. The results show that Video-R1 significantly outperforms existing benchmarks, achieving notable accuracy improvements in video reasoning tasks.'}, 'zh': {'title': 'Video-R1ï¼šè§†é¢‘æ¨ç†çš„æ–°çªç ´', 'desc': 'æœ¬æ–‡ä»‹ç»äº†Video-R1ï¼Œè¿™æ˜¯é¦–æ¬¡ç³»ç»Ÿæ¢ç´¢åœ¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­é€šè¿‡è§„åˆ™åŸºç¡€çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¥å¼•å‘è§†é¢‘æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†T-GRPOç®—æ³•ï¼Œä»¥è§£å†³è§†é¢‘æ¨ç†ä¸­çš„æ—¶é—´å»ºæ¨¡ä¸è¶³å’Œé«˜è´¨é‡è§†é¢‘æ¨ç†æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ã€‚é€šè¿‡ç»“åˆé«˜è´¨é‡çš„å›¾åƒæ¨ç†æ•°æ®ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸¤ä¸ªæ•°æ®é›†ï¼Œåˆ†åˆ«ç”¨äºå†·å¯åŠ¨å’Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVideo-R1åœ¨å¤šä¸ªè§†é¢‘æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œå°¤å…¶åœ¨è§†é¢‘ç©ºé—´æ¨ç†åŸºå‡†VSI-benchä¸Šè¾¾åˆ°äº†35.8%çš„å‡†ç¡®ç‡ï¼Œè¶…è¶Šäº†å•†ä¸šæ¨¡å‹GPT-4oã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.21620', 'title': 'UI-R1: Enhancing Action Prediction of GUI Agents by Reinforcement\n  Learning', 'url': 'https://huggingface.co/papers/2503.21620', 'abstract': 'The recent DeepSeek-R1 has showcased the emergence of reasoning capabilities in LLMs through reinforcement learning (RL) with rule-based rewards. Building on this idea, we are the first to explore how rule-based RL can enhance the reasoning capabilities of multimodal large language models (MLLMs) for graphic user interface (GUI) action prediction tasks. To this end, we curate a small yet high-quality dataset of 136 challenging tasks, encompassing five common action types on mobile devices. We also introduce a unified rule-based action reward, enabling model optimization via policy-based algorithms such as Group Relative Policy Optimization (GRPO). Experimental results demonstrate that our proposed data-efficient model, UI-R1-3B, achieves substantial improvements on both in-domain (ID) and out-of-domain (OOD) tasks. Specifically, on the ID benchmark AndroidControl, the action type accuracy improves by 15%, while grounding accuracy increases by 10.3%, compared with the base model (i.e. Qwen2.5-VL-3B). On the OOD GUI grounding benchmark ScreenSpot-Pro, our model surpasses the base model by 6.0% and achieves competitive performance with larger models (e.g., OS-Atlas-7B), which are trained via supervised fine-tuning (SFT) on 76K data. These results underscore the potential of rule-based reinforcement learning to advance GUI understanding and control, paving the way for future research in this domain.', 'score': 45, 'issue_id': 2941, 'pub_date': '2025-03-27', 'pub_date_card': {'ru': '27 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 27', 'zh': '3æœˆ27æ—¥'}, 'hash': '81580448c4650ed8', 'authors': ['Zhengxi Lu', 'Yuxiang Chai', 'Yaxuan Guo', 'Xi Yin', 'Liang Liu', 'Hao Wang', 'Guanjing Xiong', 'Hongsheng Li'], 'affiliations': ['MMLab @ CUHK', 'vivo AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2503.21620.jpg', 'data': {'categories': ['#reasoning', '#rl', '#training', '#dataset', '#multimodal', '#optimization'], 'emoji': 'ğŸ–¥ï¸', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ GUI Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¸Ğ·ÑƒÑ‡Ğ°ÑÑ‚, ĞºĞ°Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ» Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ° (GUI). ĞĞ½Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹, Ğ½Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· 136 ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ·Ğ° Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ». Ğ˜Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ UI-R1-3B Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ğº Ğ½Ğ° Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ñ‹Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ½Ğ° Ğ²Ğ½ĞµĞ´Ğ¾Ğ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ» Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ GUI.'}, 'en': {'title': 'Enhancing GUI Action Prediction with Rule-Based Reinforcement Learning', 'desc': 'This paper introduces a novel approach to enhance the reasoning capabilities of multimodal large language models (MLLMs) using rule-based reinforcement learning (RL) for predicting actions in graphic user interfaces (GUIs). The authors curate a high-quality dataset of 136 challenging tasks that involve common actions on mobile devices, allowing for effective model training. They propose a unified rule-based action reward system that optimizes the model through policy-based algorithms like Group Relative Policy Optimization (GRPO). Experimental results show that their model, UI-R1-3B, significantly outperforms the base model in both in-domain and out-of-domain tasks, highlighting the effectiveness of rule-based RL in improving GUI action prediction.'}, 'zh': {'title': 'åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ æå‡GUIåŠ¨ä½œé¢„æµ‹èƒ½åŠ›', 'desc': 'æœ€è¿‘çš„DeepSeek-R1å±•ç¤ºäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šè¿‡åŸºäºè§„åˆ™çš„å¥–åŠ±è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è€Œå±•ç°å‡ºçš„æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬é¦–æ¬¡æ¢ç´¢äº†åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ å¦‚ä½•å¢å¼ºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰åŠ¨ä½œé¢„æµ‹ä»»åŠ¡ä¸­çš„æ¨ç†èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ•´ç†äº†ä¸€ä¸ªå°è€Œé«˜è´¨é‡çš„æ•°æ®é›†ï¼ŒåŒ…å«136ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œæ¶µç›–äº”ç§å¸¸è§çš„ç§»åŠ¨è®¾å¤‡åŠ¨ä½œç±»å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„æ•°æ®é«˜æ•ˆæ¨¡å‹UI-R1-3Båœ¨é¢†åŸŸå†…å’Œé¢†åŸŸå¤–ä»»åŠ¡ä¸Šå‡å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œå±•ç¤ºäº†åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ åœ¨æå‡GUIç†è§£å’Œæ§åˆ¶æ–¹é¢çš„æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.21460', 'title': 'Large Language Model Agent: A Survey on Methodology, Applications and\n  Challenges', 'url': 'https://huggingface.co/papers/2503.21460', 'abstract': 'The era of intelligent agents is upon us, driven by revolutionary advancements in large language models. Large Language Model (LLM) agents, with goal-driven behaviors and dynamic adaptation capabilities, potentially represent a critical pathway toward artificial general intelligence. This survey systematically deconstructs LLM agent systems through a methodology-centered taxonomy, linking architectural foundations, collaboration mechanisms, and evolutionary pathways. We unify fragmented research threads by revealing fundamental connections between agent design principles and their emergent behaviors in complex environments. Our work provides a unified architectural perspective, examining how agents are constructed, how they collaborate, and how they evolve over time, while also addressing evaluation methodologies, tool applications, practical challenges, and diverse application domains. By surveying the latest developments in this rapidly evolving field, we offer researchers a structured taxonomy for understanding LLM agents and identify promising directions for future research. The collection is available at https://github.com/luo-junyu/Awesome-Agent-Papers.', 'score': 41, 'issue_id': 2941, 'pub_date': '2025-03-27', 'pub_date_card': {'ru': '27 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 27', 'zh': '3æœˆ27æ—¥'}, 'hash': '8295a3726d0cc1b8', 'authors': ['Junyu Luo', 'Weizhi Zhang', 'Ye Yuan', 'Yusheng Zhao', 'Junwei Yang', 'Yiyang Gu', 'Bohan Wu', 'Binqi Chen', 'Ziyue Qiao', 'Qingqing Long', 'Rongcheng Tu', 'Xiao Luo', 'Wei Ju', 'Zhiping Xiao', 'Yifan Wang', 'Meng Xiao', 'Chenwu Liu', 'Jingyang Yuan', 'Shichang Zhang', 'Yiqiao Jin', 'Fan Zhang', 'Xian Wu', 'Hanqing Zhao', 'Dacheng Tao', 'Philip S. Yu', 'Ming Zhang'], 'affiliations': ['Computer Network Information Center, Chinese Academy of Sciences, Beijing, China', 'Department of Computer Science, University of California, Los Angeles, USA', 'Department of Computer Science, University of Illinois at Chicago, Chicago, USA', 'Georgia Institute of Technology, Atlanta, USA', 'Harvard University, Cambridge, USA', 'Jarvis Research Center, Tencent YouTu Lab, Shenzhen, China', 'Nanyang Technological University, Singapore', 'Paul G. Allen School of Computer Science and Engineering, University of Washington, Seattle, USA', 'School of Computer Science and PKU-Anker LLM Lab, Peking University, Beijing, China', 'School of Computing and Information Technology, Great Bay University, Guangdong, China', 'School of Information Technology & Management, University of International Business and Economics, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.21460.jpg', 'data': {'categories': ['#agi', '#benchmark', '#agents', '#architecture', '#survey'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞĞ³ĞµĞ½Ñ‚Ñ‹ LLM: Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°, ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ¾Ğ±Ğ·Ğ¾Ñ€ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ, ÑĞ²ÑĞ·Ñ‹Ğ²Ğ°ÑÑ‰ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ Ğ¾ÑĞ½Ğ¾Ğ²Ñ‹, Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¿ÑƒÑ‚Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² LLM. Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ñ‹ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¸Ñ… Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ…, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ. Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² LLM Ğ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹.'}, 'en': {'title': 'Unifying the Future of Intelligent LLM Agents', 'desc': 'This paper explores the advancements in Large Language Model (LLM) agents, which are intelligent systems capable of adapting and achieving specific goals. It presents a structured taxonomy that categorizes LLM agent systems based on their architecture, collaboration methods, and evolutionary processes. The authors aim to connect various research areas by highlighting the relationship between design principles and the behaviors of these agents in complex environments. Additionally, the paper discusses evaluation methods, practical challenges, and potential applications, providing a comprehensive overview for future research in this field.'}, 'zh': {'title': 'å¤§å‹è¯­è¨€æ¨¡å‹ä»£ç†ï¼šé€šå‘äººå·¥é€šç”¨æ™ºèƒ½çš„å…³é”®', 'desc': 'æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†çš„ç³»ç»Ÿï¼Œå¼ºè°ƒå…¶åœ¨å®ç°äººå·¥é€šç”¨æ™ºèƒ½æ–¹é¢çš„é‡è¦æ€§ã€‚é€šè¿‡å»ºç«‹ä¸€ä¸ªä»¥æ–¹æ³•è®ºä¸ºä¸­å¿ƒçš„åˆ†ç±»æ³•ï¼Œæ–‡ç« å°†ä»£ç†çš„æ¶æ„åŸºç¡€ã€åä½œæœºåˆ¶å’Œæ¼”åŒ–è·¯å¾„è¿›è¡Œäº†ç³»ç»Ÿæ€§åˆ†æã€‚æˆ‘ä»¬æ­ç¤ºäº†ä»£ç†è®¾è®¡åŸåˆ™ä¸å…¶åœ¨å¤æ‚ç¯å¢ƒä¸­æ¶Œç°è¡Œä¸ºä¹‹é—´çš„åŸºæœ¬è”ç³»ï¼Œä»è€Œç»Ÿä¸€äº†åˆ†æ•£çš„ç ”ç©¶çº¿ç´¢ã€‚æœ€åï¼Œæœ¬æ–‡ä¸ºç ”ç©¶äººå‘˜æä¾›äº†ä¸€ä¸ªç»“æ„åŒ–çš„åˆ†ç±»ä½“ç³»ï¼Œä»¥ç†è§£LLMä»£ç†ï¼Œå¹¶æŒ‡å‡ºæœªæ¥ç ”ç©¶çš„æœ‰å‰æ™¯æ–¹å‘ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.21380', 'title': 'Challenging the Boundaries of Reasoning: An Olympiad-Level Math\n  Benchmark for Large Language Models', 'url': 'https://huggingface.co/papers/2503.21380', 'abstract': "In recent years, the rapid development of large reasoning models has resulted in the saturation of existing benchmarks for evaluating mathematical reasoning, highlighting the urgent need for more challenging and rigorous evaluation frameworks. To address this gap, we introduce OlymMATH, a novel Olympiad-level mathematical benchmark, designed to rigorously test the complex reasoning capabilities of LLMs. OlymMATH features 200 meticulously curated problems, each manually verified and available in parallel English and Chinese versions. The problems are systematically organized into two distinct difficulty tiers: (1) AIME-level problems (easy) that establish a baseline for mathematical reasoning assessment, and (2) significantly more challenging problems (hard) designed to push the boundaries of current state-of-the-art models. In our benchmark, these problems span four core mathematical fields, each including a verifiable numerical solution to enable objective, rule-based evaluation. Empirical results underscore the significant challenge presented by OlymMATH, with state-of-the-art models including DeepSeek-R1 and OpenAI's o3-mini demonstrating notably limited accuracy on the hard subset. Furthermore, the benchmark facilitates comprehensive bilingual assessment of mathematical reasoning abilities-a critical dimension that remains largely unaddressed in mainstream mathematical reasoning benchmarks. We release the OlymMATH benchmark at the STILL project: https://github.com/RUCAIBox/Slow_Thinking_with_LLMs.", 'score': 35, 'issue_id': 2941, 'pub_date': '2025-03-27', 'pub_date_card': {'ru': '27 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 27', 'zh': '3æœˆ27æ—¥'}, 'hash': '923e6e5f48ca95e4', 'authors': ['Haoxiang Sun', 'Yingqian Min', 'Zhipeng Chen', 'Wayne Xin Zhao', 'Zheng Liu', 'Zhongyuan Wang', 'Lei Fang', 'Ji-Rong Wen'], 'affiliations': ['BAAI', 'DataCanvas Alaya NeW', 'Gaoling School of Artificial Intelligence, Renmin University of China', 'School of Information, Renmin University of China'], 'pdf_title_img': 'assets/pdf/title_img/2503.21380.jpg', 'data': {'categories': ['#reasoning', '#multilingual', '#benchmark', '#low_resource', '#math'], 'emoji': 'ğŸ§®', 'ru': {'title': 'OlymMATH: ĞĞ¾Ğ²Ğ°Ñ Ğ²Ñ‹ÑĞ¾Ñ‚Ğ° Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ˜Ğ˜', 'desc': 'OlymMATH - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¾Ğ»Ğ¸Ğ¼Ğ¿Ğ¸Ğ°Ğ´. ĞĞ½ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 200 Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ´Ğ²ÑƒÑ… ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ¸. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½ Ğ½Ğ° Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ¼ Ğ¸ ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞ°Ñ…, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ´Ğ²ÑƒÑĞ·Ñ‹Ñ‡Ğ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½ÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ´Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡.'}, 'en': {'title': 'OlymMATH: Raising the Bar for Mathematical Reasoning Evaluation', 'desc': "The paper introduces OlymMATH, a new benchmark for evaluating the mathematical reasoning abilities of large language models (LLMs). It consists of 200 carefully curated problems, verified for accuracy, and presented in both English and Chinese. The problems are categorized into two difficulty levels: AIME-level (easy) and more challenging problems (hard) that test the limits of current models. Empirical results show that even advanced models struggle with the hard problems, highlighting the benchmark's effectiveness in assessing complex reasoning skills."}, 'zh': {'title': 'OlymMATHï¼šæŒ‘æˆ˜æ•°å­¦æ¨ç†çš„æ–°åŸºå‡†', 'desc': 'è¿‘å¹´æ¥ï¼Œå¤§å‹æ¨ç†æ¨¡å‹çš„å¿«é€Ÿå‘å±•ä½¿å¾—ç°æœ‰çš„æ•°å­¦æ¨ç†è¯„ä¼°åŸºå‡†è¶‹äºé¥±å’Œï¼Œè¿«åˆ‡éœ€è¦æ›´å…·æŒ‘æˆ˜æ€§å’Œä¸¥æ ¼æ€§çš„è¯„ä¼°æ¡†æ¶ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºäº†OlymMATHï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„å¥¥æ—åŒ¹å…‹çº§æ•°å­¦åŸºå‡†ï¼Œæ—¨åœ¨ä¸¥æ ¼æµ‹è¯•å¤§å‹è¯­è¨€æ¨¡å‹çš„å¤æ‚æ¨ç†èƒ½åŠ›ã€‚OlymMATHåŒ…å«200ä¸ªç»è¿‡ç²¾å¿ƒæŒ‘é€‰çš„é—®é¢˜ï¼Œåˆ†ä¸ºAIMEçº§ï¼ˆç®€å•ï¼‰å’Œæ›´å…·æŒ‘æˆ˜æ€§çš„ï¼ˆå›°éš¾ï¼‰ä¸¤ç§éš¾åº¦ï¼Œæ¶µç›–å››ä¸ªæ ¸å¿ƒæ•°å­¦é¢†åŸŸï¼Œå¹¶æä¾›å¯éªŒè¯çš„æ•°å€¼è§£ã€‚å®è¯ç»“æœè¡¨æ˜ï¼ŒOlymMATHå¯¹å½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹æå‡ºäº†æ˜¾è‘—æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨å›°éš¾å­é›†ä¸Šï¼Œæ¨¡å‹çš„å‡†ç¡®æ€§æ˜æ˜¾æœ‰é™ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.21755', 'title': 'VBench-2.0: Advancing Video Generation Benchmark Suite for Intrinsic\n  Faithfulness', 'url': 'https://huggingface.co/papers/2503.21755', 'abstract': 'Video generation has advanced significantly, evolving from producing unrealistic outputs to generating videos that appear visually convincing and temporally coherent. To evaluate these video generative models, benchmarks such as VBench have been developed to assess their faithfulness, measuring factors like per-frame aesthetics, temporal consistency, and basic prompt adherence. However, these aspects mainly represent superficial faithfulness, which focus on whether the video appears visually convincing rather than whether it adheres to real-world principles. While recent models perform increasingly well on these metrics, they still struggle to generate videos that are not just visually plausible but fundamentally realistic. To achieve real "world models" through video generation, the next frontier lies in intrinsic faithfulness to ensure that generated videos adhere to physical laws, commonsense reasoning, anatomical correctness, and compositional integrity. Achieving this level of realism is essential for applications such as AI-assisted filmmaking and simulated world modeling. To bridge this gap, we introduce VBench-2.0, a next-generation benchmark designed to automatically evaluate video generative models for their intrinsic faithfulness. VBench-2.0 assesses five key dimensions: Human Fidelity, Controllability, Creativity, Physics, and Commonsense, each further broken down into fine-grained capabilities. Tailored for individual dimensions, our evaluation framework integrates generalists such as state-of-the-art VLMs and LLMs, and specialists, including anomaly detection methods proposed for video generation. We conduct extensive annotations to ensure alignment with human judgment. By pushing beyond superficial faithfulness toward intrinsic faithfulness, VBench-2.0 aims to set a new standard for the next generation of video generative models in pursuit of intrinsic faithfulness.', 'score': 28, 'issue_id': 2941, 'pub_date': '2025-03-27', 'pub_date_card': {'ru': '27 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 27', 'zh': '3æœˆ27æ—¥'}, 'hash': '4bd65083c265f9a4', 'authors': ['Dian Zheng', 'Ziqi Huang', 'Hongbo Liu', 'Kai Zou', 'Yinan He', 'Fan Zhang', 'Yuanhan Zhang', 'Jingwen He', 'Wei-Shi Zheng', 'Yu Qiao', 'Ziwei Liu'], 'affiliations': ['S-Lab, Nanyang Technological University', 'Shanghai Artificial Intelligence Laboratory', 'Sun Yat-Sen University', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2503.21755.jpg', 'data': {'categories': ['#video', '#alignment', '#benchmark', '#games'], 'emoji': 'ğŸ¬', 'ru': {'title': 'ĞÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ VBench-2.0 - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ²ĞµÑ€ÑĞ¸Ğ¹, Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒÑÑ‰Ğ¸Ñ…ÑÑ Ğ½Ğ° Ğ²Ğ½ĞµÑˆĞ½ĞµĞ¹ Ğ´Ğ¾ÑÑ‚Ğ¾Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸, VBench-2.0 Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ÑÑ Ğ´Ğ¾ÑÑ‚Ğ¾Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ¢ĞµÑÑ‚ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ¿ÑÑ‚Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ°ÑĞ¿ĞµĞºÑ‚Ğ¾Ğ²: Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ»ÑĞ´ĞµĞ¹, ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾ÑÑ‚ÑŒ, ĞºÑ€ĞµĞ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ, Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºÑƒÑ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ·Ğ´Ñ€Ğ°Ğ²Ñ‹Ğ¹ ÑĞ¼Ñ‹ÑĞ». VBench-2.0 Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ°Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ (VLM, LLM), Ñ‚Ğ°Ğº Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ´Ğ»Ñ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Towards Realism: VBench-2.0 for Intrinsic Faithfulness in Video Generation', 'desc': 'This paper discusses the evolution of video generation models from producing unrealistic outputs to creating visually convincing and temporally coherent videos. It highlights the limitations of current evaluation benchmarks like VBench, which focus on superficial aspects of faithfulness rather than adherence to real-world principles. The authors introduce VBench-2.0, a new benchmark that evaluates video generative models based on intrinsic faithfulness, which includes factors like physics, commonsense reasoning, and anatomical correctness. By emphasizing these deeper aspects of realism, VBench-2.0 aims to improve the quality of video generation for applications such as AI-assisted filmmaking and simulated world modeling.'}, 'zh': {'title': 'è¿½æ±‚å†…åœ¨å¯ä¿¡åº¦çš„ä¸‹ä¸€ä»£è§†é¢‘ç”Ÿæˆæ ‡å‡†', 'desc': 'è§†é¢‘ç”ŸæˆæŠ€æœ¯å·²ç»å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä»æœ€åˆç”Ÿæˆä¸çœŸå®çš„è¾“å‡ºåˆ°ç°åœ¨èƒ½å¤Ÿç”Ÿæˆè§†è§‰ä¸Šä»¤äººä¿¡æœä¸”æ—¶é—´ä¸Šè¿è´¯çš„è§†é¢‘ã€‚ä¸ºäº†è¯„ä¼°è¿™äº›è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œå¼€å‘äº†VBenchç­‰åŸºå‡†ï¼Œä¸»è¦æµ‹é‡æ¯å¸§çš„ç¾è§‚æ€§ã€æ—¶é—´ä¸€è‡´æ€§å’ŒåŸºæœ¬æç¤ºéµå¾ªç­‰å› ç´ ã€‚ç„¶è€Œï¼Œè¿™äº›è¯„ä¼°ä¸»è¦å…³æ³¨è¡¨é¢ä¸Šçš„å¯ä¿¡åº¦ï¼Œè€Œä¸æ˜¯è§†é¢‘æ˜¯å¦éµå¾ªç°å®ä¸–ç•Œçš„åŸåˆ™ã€‚ä¸ºå®ç°çœŸæ­£çš„â€œä¸–ç•Œæ¨¡å‹â€ï¼Œæˆ‘ä»¬å¼•å…¥äº†VBench-2.0ï¼Œæ—¨åœ¨è‡ªåŠ¨è¯„ä¼°è§†é¢‘ç”Ÿæˆæ¨¡å‹çš„å†…åœ¨å¯ä¿¡åº¦ï¼Œç¡®ä¿ç”Ÿæˆçš„è§†é¢‘ç¬¦åˆç‰©ç†æ³•åˆ™å’Œå¸¸è¯†æ¨ç†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.21749', 'title': 'LeX-Art: Rethinking Text Generation via Scalable High-Quality Data\n  Synthesis', 'url': 'https://huggingface.co/papers/2503.21749', 'abstract': 'We introduce LeX-Art, a comprehensive suite for high-quality text-image synthesis that systematically bridges the gap between prompt expressiveness and text rendering fidelity. Our approach follows a data-centric paradigm, constructing a high-quality data synthesis pipeline based on Deepseek-R1 to curate LeX-10K, a dataset of 10K high-resolution, aesthetically refined 1024times1024 images. Beyond dataset construction, we develop LeX-Enhancer, a robust prompt enrichment model, and train two text-to-image models, LeX-FLUX and LeX-Lumina, achieving state-of-the-art text rendering performance. To systematically evaluate visual text generation, we introduce LeX-Bench, a benchmark that assesses fidelity, aesthetics, and alignment, complemented by Pairwise Normalized Edit Distance (PNED), a novel metric for robust text accuracy evaluation. Experiments demonstrate significant improvements, with LeX-Lumina achieving a 79.81% PNED gain on CreateBench, and LeX-FLUX outperforming baselines in color (+3.18%), positional (+4.45%), and font accuracy (+3.81%). Our codes, models, datasets, and demo are publicly available.', 'score': 21, 'issue_id': 2941, 'pub_date': '2025-03-27', 'pub_date_card': {'ru': '27 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 27', 'zh': '3æœˆ27æ—¥'}, 'hash': 'fe7d17315ae060c8', 'authors': ['Shitian Zhao', 'Qilong Wu', 'Xinyue Li', 'Bo Zhang', 'Ming Li', 'Qi Qin', 'Dongyang Liu', 'Kaipeng Zhang', 'Hongsheng Li', 'Yu Qiao', 'Peng Gao', 'Bin Fu', 'Zhen Li'], 'affiliations': ['Shanghai AI Laboratory', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2503.21749.jpg', 'data': {'categories': ['#cv', '#open_source', '#benchmark', '#dataset', '#data'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'LeX-Art: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğµ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'LeX-Art Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑĞµÑ‚ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ñ‹Ñ€Ğ°Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ° Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ° Ñ‚ĞµĞºÑÑ‚Ğ°. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğµ, Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ğ° ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Deepseek-R1 Ğ´Ğ»Ñ ĞºÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… LeX-10K. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ñ‹ LeX-Enhancer Ğ´Ğ»Ñ Ğ¾Ğ±Ğ¾Ğ³Ğ°Ñ‰ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ¸ Ğ´Ğ²Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ text-to-image: LeX-FLUX Ğ¸ LeX-Lumina, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‰Ğ¸Ğµ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğµ Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ”Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ²ĞµĞ´ĞµĞ½ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº LeX-Bench Ğ¸ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ° PNED Ğ´Ğ»Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°.'}, 'en': {'title': "Bridging Text and Image: LeX-Art's High-Quality Synthesis Revolution", 'desc': 'LeX-Art is a new system designed to create high-quality images from text prompts, focusing on improving how well the text is rendered in the images. It builds a large dataset called LeX-10K, which contains 10,000 high-resolution images that are visually appealing. The system includes a model called LeX-Enhancer that improves the prompts used for generating images, and two advanced text-to-image models, LeX-FLUX and LeX-Lumina, which achieve top performance in rendering text. Additionally, LeX-Art introduces a benchmark called LeX-Bench to evaluate the quality of the generated images, using a new metric called Pairwise Normalized Edit Distance (PNED) to measure text accuracy.'}, 'zh': {'title': 'é«˜è´¨é‡æ–‡æœ¬-å›¾åƒåˆæˆçš„æ–°çªç ´', 'desc': 'LeX-Art æ˜¯ä¸€ä¸ªå…¨é¢çš„æ–‡æœ¬-å›¾åƒåˆæˆå·¥å…·ï¼Œæ—¨åœ¨æé«˜æç¤ºè¡¨è¾¾èƒ½åŠ›å’Œæ–‡æœ¬æ¸²æŸ“çš„å‡†ç¡®æ€§ã€‚æˆ‘ä»¬æ„å»ºäº† LeX-10K æ•°æ®é›†ï¼ŒåŒ…å« 10,000 å¼ é«˜åˆ†è¾¨ç‡ã€ç»è¿‡ç¾å­¦ä¼˜åŒ–çš„å›¾åƒï¼Œå¹¶å¼€å‘äº† LeX-Enhancer æ¨¡å‹æ¥å¢å¼ºæç¤ºæ•ˆæœã€‚æˆ‘ä»¬è®­ç»ƒäº†ä¸¤ä¸ªæ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ LeX-FLUX å’Œ LeX-Luminaï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ–‡æœ¬æ¸²æŸ“æ€§èƒ½ã€‚é€šè¿‡ LeX-Bench åŸºå‡†æµ‹è¯•ï¼Œæˆ‘ä»¬è¯„ä¼°äº†è§†è§‰æ–‡æœ¬ç”Ÿæˆçš„ä¿çœŸåº¦ã€ç¾å­¦å’Œä¸€è‡´æ€§ï¼Œå®éªŒç»“æœæ˜¾ç¤ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.21729', 'title': 'ReaRAG: Knowledge-guided Reasoning Enhances Factuality of Large\n  Reasoning Models with Iterative Retrieval Augmented Generation', 'url': 'https://huggingface.co/papers/2503.21729', 'abstract': "Large Reasoning Models (LRMs) exhibit remarkable reasoning abilities but rely primarily on parametric knowledge, limiting factual accuracy. While recent works equip reinforcement learning (RL)-based LRMs with retrieval capabilities, they suffer from overthinking and lack robustness in reasoning, reducing their effectiveness in question answering (QA) tasks. To address this, we propose ReaRAG, a factuality-enhanced reasoning model that explores diverse queries without excessive iterations. Our solution includes a novel data construction framework with an upper bound on the reasoning chain length. Specifically, we first leverage an LRM to generate deliberate thinking, then select an action from a predefined action space (Search and Finish). For Search action, a query is executed against the RAG engine, where the result is returned as observation to guide reasoning steps later. This process iterates until a Finish action is chosen. Benefiting from ReaRAG's strong reasoning capabilities, our approach outperforms existing baselines on multi-hop QA. Further analysis highlights its strong reflective ability to recognize errors and refine its reasoning trajectory. Our study enhances LRMs' factuality while effectively integrating robust reasoning for Retrieval-Augmented Generation (RAG).", 'score': 18, 'issue_id': 2941, 'pub_date': '2025-03-27', 'pub_date_card': {'ru': '27 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 27', 'zh': '3æœˆ27æ—¥'}, 'hash': '83959049f8af99fe', 'authors': ['Zhicheng Lee', 'Shulin Cao', 'Jinxin Liu', 'Jiajie Zhang', 'Weichuan Liu', 'Xiaoyin Che', 'Lei Hou', 'Juanzi Li'], 'affiliations': ['Siemens AG', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.21729.jpg', 'data': {'categories': ['#reasoning', '#rag', '#rl', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ReaRAG: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ReaRAG - Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ¹ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ReaRAG Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±Ğ´ÑƒĞ¼Ğ°Ğ½Ğ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ² Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸Ğ· Ğ¿Ñ€ĞµĞ´Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Enhancing Factuality in Reasoning with ReaRAG', 'desc': 'This paper introduces ReaRAG, a new reasoning model designed to improve the factual accuracy of Large Reasoning Models (LRMs) in question answering tasks. Unlike traditional RL-based LRMs that often overthink and lack robustness, ReaRAG employs a structured approach to reasoning by limiting the length of reasoning chains and allowing for diverse query exploration. The model uses a two-step action process, where it can either search for information or finish the reasoning process, enhancing its ability to refine answers based on retrieved data. Overall, ReaRAG demonstrates superior performance in multi-hop question answering by effectively combining reasoning capabilities with retrieval mechanisms.'}, 'zh': {'title': 'å¢å¼ºäº‹å®æ€§çš„æ¨ç†æ¨¡å‹ReaRAG', 'desc': 'å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰å±•ç°äº†å“è¶Šçš„æ¨ç†èƒ½åŠ›ï¼Œä½†ä¸»è¦ä¾èµ–å‚æ•°çŸ¥è¯†ï¼Œé™åˆ¶äº†äº‹å®å‡†ç¡®æ€§ã€‚è™½ç„¶æœ€è¿‘çš„ç ”ç©¶ä¸ºåŸºäºå¼ºåŒ–å­¦ä¹ çš„LRMså¢åŠ äº†æ£€ç´¢èƒ½åŠ›ï¼Œä½†å®ƒä»¬åœ¨æ¨ç†æ—¶å®¹æ˜“è¿‡åº¦æ€è€ƒï¼Œç¼ºä¹ç¨³å¥æ€§ï¼Œä»è€Œé™ä½äº†åœ¨é—®ç­”ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ReaRAGï¼Œè¿™æ˜¯ä¸€ç§å¢å¼ºäº‹å®æ€§çš„æ¨ç†æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨ä¸è¿›è¡Œè¿‡å¤šè¿­ä»£çš„æƒ…å†µä¸‹æ¢ç´¢å¤šæ ·åŒ–çš„æŸ¥è¯¢ã€‚æˆ‘ä»¬çš„è§£å†³æ–¹æ¡ˆåŒ…æ‹¬ä¸€ä¸ªæ–°é¢–çš„æ•°æ®æ„å»ºæ¡†æ¶ï¼Œå¹¶å¯¹æ¨ç†é“¾çš„é•¿åº¦è®¾å®šä¸Šé™ï¼Œä»è€Œæé«˜äº†LRMsçš„äº‹å®æ€§å’Œæ¨ç†çš„ç¨³å¥æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.21696', 'title': 'Embodied-Reasoner: Synergizing Visual Search, Reasoning, and Action for\n  Embodied Interactive Tasks', 'url': 'https://huggingface.co/papers/2503.21696', 'abstract': "Recent advances in deep thinking models have demonstrated remarkable reasoning capabilities on mathematical and coding tasks. However, their effectiveness in embodied domains which require continuous interaction with environments through image action interleaved trajectories remains largely -unexplored. We present Embodied Reasoner, a model that extends o1 style reasoning to interactive embodied search tasks. Unlike mathematical reasoning that relies primarily on logical deduction, embodied scenarios demand spatial understanding, temporal reasoning, and ongoing self-reflection based on interaction history. To address these challenges, we synthesize 9.3k coherent Observation-Thought-Action trajectories containing 64k interactive images and 90k diverse thinking processes (analysis, spatial reasoning, reflection, planning, and verification). We develop a three-stage training pipeline that progressively enhances the model's capabilities through imitation learning, self-exploration via rejection sampling, and self-correction through reflection tuning. The evaluation shows that our model significantly outperforms those advanced visual reasoning models, e.g., it exceeds OpenAI o1, o3-mini, and Claude-3.7 by +9\\%, 24\\%, and +13\\%. Analysis reveals our model exhibits fewer repeated searches and logical inconsistencies, with particular advantages in complex long-horizon tasks. Real-world environments also show our superiority while exhibiting fewer repeated searches and logical inconsistency cases.", 'score': 17, 'issue_id': 2946, 'pub_date': '2025-03-27', 'pub_date_card': {'ru': '27 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 27', 'zh': '3æœˆ27æ—¥'}, 'hash': 'a52516705bc7a122', 'authors': ['Wenqi Zhang', 'Mengna Wang', 'Gangao Liu', 'Xu Huixin', 'Yiwei Jiang', 'Yongliang Shen', 'Guiyang Hou', 'Zhe Zheng', 'Hang Zhang', 'Xin Li', 'Weiming Lu', 'Peng Li', 'Yueting Zhuang'], 'affiliations': ['Alibaba Group', 'College of Computer Science and Technology, Zhejiang University', 'DAMO Academy, Alibaba Group', 'Hohai University', 'Institute of Software, Chinese Academy of Sciences', 'Nanjing Institute of Software Technology', 'Nanjing University of Posts and Telecommunications', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2503.21696.jpg', 'data': {'categories': ['#cv', '#optimization', '#reasoning', '#agents', '#training'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ’Ğ¾Ğ¿Ğ»Ğ¾Ñ‰Ñ‘Ğ½Ğ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·ÑƒĞ¼: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ² Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ¼Ğ¸Ñ€Ğµ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Embodied Reasoner, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² ÑÑ‚Ğ¸Ğ»Ğµ GPT-4 Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ² Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ¼Ğ¸Ñ€Ğµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰ĞµĞ¼ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğ¹, Ğ¼Ñ‹ÑĞ»ĞµĞ¹ Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ñ‚Ñ€ĞµÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ¸Ğ¼Ğ¸Ñ‚Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ, ÑĞ°Ğ¼Ğ¾Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Embodied Reasoner Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ….'}, 'en': {'title': 'Empowering Reasoning in Interactive Environments', 'desc': 'This paper introduces the Embodied Reasoner, a model designed to enhance reasoning in interactive environments that require continuous engagement through visual and action-based tasks. Unlike traditional mathematical reasoning, this model focuses on spatial understanding and temporal reasoning, which are crucial for navigating real-world scenarios. The authors created a large dataset of 9.3k Observation-Thought-Action trajectories to train the model, employing a three-stage training process that includes imitation learning and self-correction. The results demonstrate that the Embodied Reasoner outperforms existing visual reasoning models, showing improved efficiency and fewer logical errors in complex tasks.'}, 'zh': {'title': 'æå‡äº¤äº’å¼æ¨ç†èƒ½åŠ›çš„å…¨æ–°æ¨¡å‹', 'desc': 'æœ€è¿‘æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨æ•°å­¦å’Œç¼–ç¨‹ä»»åŠ¡ä¸Šå±•ç°äº†å‡ºè‰²çš„æ¨ç†èƒ½åŠ›ï¼Œä½†åœ¨éœ€è¦ä¸ç¯å¢ƒæŒç»­äº’åŠ¨çš„å®é™…åº”ç”¨é¢†åŸŸä¸­ï¼Œå…¶æœ‰æ•ˆæ€§ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æˆ‘ä»¬æå‡ºäº†"Embodied Reasoner"æ¨¡å‹ï¼Œæ—¨åœ¨å°†æ¨ç†èƒ½åŠ›æ‰©å±•åˆ°äº¤äº’å¼çš„å®é™…æœç´¢ä»»åŠ¡ä¸­ã€‚ä¸ä¸»è¦ä¾èµ–é€»è¾‘æ¨ç†çš„æ•°å­¦æ¨ç†ä¸åŒï¼Œå®é™…åœºæ™¯éœ€è¦ç©ºé—´ç†è§£ã€æ—¶é—´æ¨ç†ä»¥åŠåŸºäºäº’åŠ¨å†å²çš„è‡ªæˆ‘åæ€ã€‚é€šè¿‡åˆæˆ9.3åƒæ¡è¿è´¯çš„è§‚å¯Ÿ-æ€è€ƒ-è¡ŒåŠ¨è½¨è¿¹ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªä¸‰é˜¶æ®µçš„è®­ç»ƒæµç¨‹ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.21144', 'title': 'ChatAnyone: Stylized Real-time Portrait Video Generation with\n  Hierarchical Motion Diffusion Model', 'url': 'https://huggingface.co/papers/2503.21144', 'abstract': 'Real-time interactive video-chat portraits have been increasingly recognized as the future trend, particularly due to the remarkable progress made in text and voice chat technologies. However, existing methods primarily focus on real-time generation of head movements, but struggle to produce synchronized body motions that match these head actions. Additionally, achieving fine-grained control over the speaking style and nuances of facial expressions remains a challenge. To address these limitations, we introduce a novel framework for stylized real-time portrait video generation, enabling expressive and flexible video chat that extends from talking head to upper-body interaction. Our approach consists of the following two stages. The first stage involves efficient hierarchical motion diffusion models, that take both explicit and implicit motion representations into account based on audio inputs, which can generate a diverse range of facial expressions with stylistic control and synchronization between head and body movements. The second stage aims to generate portrait video featuring upper-body movements, including hand gestures. We inject explicit hand control signals into the generator to produce more detailed hand movements, and further perform face refinement to enhance the overall realism and expressiveness of the portrait video. Additionally, our approach supports efficient and continuous generation of upper-body portrait video in maximum 512 * 768 resolution at up to 30fps on 4090 GPU, supporting interactive video-chat in real-time. Experimental results demonstrate the capability of our approach to produce portrait videos with rich expressiveness and natural upper-body movements.', 'score': 17, 'issue_id': 2941, 'pub_date': '2025-03-27', 'pub_date_card': {'ru': '27 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 27', 'zh': '3æœˆ27æ—¥'}, 'hash': '12e50e9826751c62', 'authors': ['Jinwei Qi', 'Chaonan Ji', 'Sheng Xu', 'Peng Zhang', 'Bang Zhang', 'Liefeng Bo'], 'affiliations': ['Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2503.21144.jpg', 'data': {'categories': ['#cv', '#multimodal', '#video', '#diffusion'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Ğ ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾Ñ€Ñ‚Ñ€ĞµÑ‚Ñ‹ Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ñ‚ĞµĞ»Ğ° Ğ´Ğ»Ñ Ğ²Ğ¸Ğ´ĞµĞ¾Ñ‡Ğ°Ñ‚Ğ¾Ğ² Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑÑ‚Ğ¸Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾Ñ€Ñ‚Ñ€ĞµÑ‚Ğ¾Ğ² Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ´ĞµĞ¾Ñ‡Ğ°Ñ‚Ğ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğµ ĞºĞ°Ğº ÑĞ²Ğ½Ñ‹Ğµ, Ñ‚Ğ°Ğº Ğ¸ Ğ½ĞµÑĞ²Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ²Ñ…Ğ¾Ğ´Ğ°. ĞĞ½Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ»Ğ¸Ñ†Ğ° Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼ ÑÑ‚Ğ¸Ğ»Ñ Ğ¸ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ñ‹ Ğ¸ Ñ‚ĞµĞ»Ğ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ²ĞµÑ€Ñ…Ğ½ĞµĞ¹ Ñ‡Ğ°ÑÑ‚Ğ¸ Ñ‚ĞµĞ»Ğ°, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¶ĞµÑÑ‚Ñ‹ Ñ€ÑƒĞº, Ğ¸ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ»Ğ¸Ñ†Ğ° Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Expressive Real-Time Video Chats with Synchronized Body Movements', 'desc': 'This paper presents a new framework for creating real-time interactive video chat portraits that include both head and upper-body movements. It utilizes hierarchical motion diffusion models to synchronize facial expressions and body motions based on audio inputs, allowing for expressive and stylistically controlled video generation. The framework also incorporates explicit hand control signals to enhance the realism of hand gestures and facial refinements. Overall, the approach enables high-quality, interactive video chats at a resolution of 512 * 768 and up to 30 frames per second.'}, 'zh': {'title': 'å®æ—¶äº’åŠ¨è§†é¢‘èŠå¤©çš„æœªæ¥è¶‹åŠ¿', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å®æ—¶è‚–åƒè§†é¢‘ç”Ÿæˆæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•åœ¨å¤´éƒ¨åŠ¨ä½œä¸èº«ä½“åŠ¨ä½œåŒæ­¥æ–¹é¢çš„ä¸è¶³ã€‚è¯¥æ¡†æ¶é€šè¿‡é«˜æ•ˆçš„å±‚æ¬¡è¿åŠ¨æ‰©æ•£æ¨¡å‹ï¼Œç»“åˆéŸ³é¢‘è¾“å…¥ç”Ÿæˆå¤šæ ·åŒ–çš„é¢éƒ¨è¡¨æƒ…ï¼Œå¹¶å®ç°å¤´éƒ¨ä¸èº«ä½“åŠ¨ä½œçš„åè°ƒã€‚ç¬¬äºŒé˜¶æ®µåˆ™ä¸“æ³¨äºç”ŸæˆåŒ…å«ä¸ŠåŠèº«åŠ¨ä½œçš„è‚–åƒè§†é¢‘ï¼Œé€šè¿‡æ³¨å…¥æ‰‹éƒ¨æ§åˆ¶ä¿¡å·æ¥å¢å¼ºæ‰‹éƒ¨åŠ¨ä½œçš„ç»†èŠ‚ï¼Œå¹¶è¿›è¡Œé¢éƒ¨ç»†åŒ–ä»¥æå‡è§†é¢‘çš„çœŸå®æ„Ÿå’Œè¡¨ç°åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿä»¥é«˜è¾¾30fpsçš„é€Ÿåº¦ç”Ÿæˆä¸°å¯Œè¡¨ç°åŠ›å’Œè‡ªç„¶ä¸ŠåŠèº«åŠ¨ä½œçš„è‚–åƒè§†é¢‘ï¼Œé€‚ç”¨äºå®æ—¶äº’åŠ¨è§†é¢‘èŠå¤©ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.20990', 'title': 'FinAudio: A Benchmark for Audio Large Language Models in Financial\n  Applications', 'url': 'https://huggingface.co/papers/2503.20990', 'abstract': 'Audio Large Language Models (AudioLLMs) have received widespread attention and have significantly improved performance on audio tasks such as conversation, audio understanding, and automatic speech recognition (ASR). Despite these advancements, there is an absence of a benchmark for assessing AudioLLMs in financial scenarios, where audio data, such as earnings conference calls and CEO speeches, are crucial resources for financial analysis and investment decisions. In this paper, we introduce FinAudio, the first benchmark designed to evaluate the capacity of AudioLLMs in the financial domain. We first define three tasks based on the unique characteristics of the financial domain: 1) ASR for short financial audio, 2) ASR for long financial audio, and 3) summarization of long financial audio. Then, we curate two short and two long audio datasets, respectively, and develop a novel dataset for financial audio summarization, comprising the FinAudio benchmark. Then, we evaluate seven prevalent AudioLLMs on FinAudio. Our evaluation reveals the limitations of existing AudioLLMs in the financial domain and offers insights for improving AudioLLMs. All datasets and codes will be released.', 'score': 17, 'issue_id': 2943, 'pub_date': '2025-03-26', 'pub_date_card': {'ru': '26 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 26', 'zh': '3æœˆ26æ—¥'}, 'hash': '55f780e7347209e5', 'authors': ['Yupeng Cao', 'Haohang Li', 'Yangyang Yu', 'Shashidhar Reddy Javaji', 'Yueru He', 'Jimin Huang', 'Zining Zhu', 'Qianqian Xie', 'Xiao-yang Liu', 'Koduvayur Subbalakshmi', 'Meikang Qiu', 'Sophia Ananiadou', 'Jian-Yun Nie'], 'affiliations': ['Augusta University', 'Columbia University', 'Stevens Institute of Technology', 'The Fin AI', 'The University of Manchester', 'University of Montreal'], 'pdf_title_img': 'assets/pdf/title_img/2503.20990.jpg', 'data': {'categories': ['#audio', '#benchmark', '#dataset'], 'emoji': 'ğŸ™ï¸', 'ru': {'title': 'FinAudio: ĞŸĞµÑ€Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾-Ğ˜Ğ˜ Ğ² Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ°Ñ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ FinAudio - Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (AudioLLMs) Ğ² Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ğ¾Ğ¹ ÑÑ„ĞµÑ€Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑÑ‚ Ñ‚Ñ€Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸: Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ñ… Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ñ… Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¸Ñ… ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ. Ğ”Ğ»Ñ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ° ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ñ‹ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ñ… Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ°ÑƒĞ´Ğ¸Ğ¾, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ°ÑƒĞ´Ğ¸Ğ¾. ĞÑ†ĞµĞ½ĞºĞ° ÑĞµĞ¼Ğ¸ Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… AudioLLMs Ğ½Ğ° FinAudio Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ° Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸.'}, 'en': {'title': 'Benchmarking AudioLLMs for Financial Insights', 'desc': 'This paper introduces FinAudio, a benchmark specifically designed to evaluate Audio Large Language Models (AudioLLMs) in financial contexts. It identifies three key tasks: automatic speech recognition (ASR) for both short and long financial audio, and summarization of long financial audio. The authors curate datasets tailored to these tasks, highlighting the unique characteristics of financial audio data. The evaluation of seven existing AudioLLMs on this benchmark reveals their limitations and suggests areas for enhancement in their performance within the financial domain.'}, 'zh': {'title': 'é‡‘èé¢†åŸŸéŸ³é¢‘æ¨¡å‹è¯„ä¼°æ–°åŸºå‡†', 'desc': 'éŸ³é¢‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆAudioLLMsï¼‰åœ¨å¯¹è¯ã€éŸ³é¢‘ç†è§£å’Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ç­‰éŸ³é¢‘ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œç›®å‰ç¼ºä¹ä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°AudioLLMsåœ¨é‡‘èåœºæ™¯ä¸­çš„åŸºå‡†ã€‚æœ¬æ–‡æå‡ºäº†FinAudioï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°AudioLLMsåœ¨é‡‘èé¢†åŸŸèƒ½åŠ›çš„åŸºå‡†ï¼Œå®šä¹‰äº†ä¸‰ä¸ªåŸºäºé‡‘èé¢†åŸŸç‰¹å¾çš„ä»»åŠ¡ï¼Œå¹¶åˆ›å»ºäº†ç›¸åº”çš„æ•°æ®é›†ã€‚é€šè¿‡å¯¹ä¸ƒä¸ªæµè¡Œçš„AudioLLMsè¿›è¡Œè¯„ä¼°ï¼Œæˆ‘ä»¬æ­ç¤ºäº†ç°æœ‰æ¨¡å‹åœ¨é‡‘èé¢†åŸŸçš„å±€é™æ€§ï¼Œå¹¶ä¸ºæ”¹è¿›AudioLLMsæä¾›äº†è§è§£ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.21248', 'title': 'ResearchBench: Benchmarking LLMs in Scientific Discovery via\n  Inspiration-Based Task Decomposition', 'url': 'https://huggingface.co/papers/2503.21248', 'abstract': 'Large language models (LLMs) have demonstrated potential in assisting scientific research, yet their ability to discover high-quality research hypotheses remains unexamined due to the lack of a dedicated benchmark. To address this gap, we introduce the first large-scale benchmark for evaluating LLMs with a near-sufficient set of sub-tasks of scientific discovery: inspiration retrieval, hypothesis composition, and hypothesis ranking. We develop an automated framework that extracts critical components - research questions, background surveys, inspirations, and hypotheses - from scientific papers across 12 disciplines, with expert validation confirming its accuracy. To prevent data contamination, we focus exclusively on papers published in 2024, ensuring minimal overlap with LLM pretraining data. Our evaluation reveals that LLMs perform well in retrieving inspirations, an out-of-distribution task, suggesting their ability to surface novel knowledge associations. This positions LLMs as "research hypothesis mines", capable of facilitating automated scientific discovery by generating innovative hypotheses at scale with minimal human intervention.', 'score': 16, 'issue_id': 2944, 'pub_date': '2025-03-27', 'pub_date_card': {'ru': '27 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 27', 'zh': '3æœˆ27æ—¥'}, 'hash': 'c58f620a0f532dc0', 'authors': ['Yujie Liu', 'Zonglin Yang', 'Tong Xie', 'Jinjie Ni', 'Ben Gao', 'Yuqiang Li', 'Shixiang Tang', 'Wanli Ouyang', 'Erik Cambria', 'Dongzhan Zhou'], 'affiliations': ['Nanyang Technological University', 'National University of Singapore', 'Shanghai Artificial Intelligence Laboratory', 'University of New South Wales', 'Wuhan University'], 'pdf_title_img': 'assets/pdf/title_img/2503.21248.jpg', 'data': {'categories': ['#benchmark', '#science', '#dataset'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'LLM ĞºĞ°Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ³Ğ¸Ğ¿Ğ¾Ñ‚ĞµĞ·: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ˜Ğ˜ Ğ² Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑÑ…', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğµ Ğ³Ğ¸Ğ¿Ğ¾Ñ‚ĞµĞ·Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸Ğ· Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ñ‚ĞµĞ¹ Ğ² 12 Ğ´Ğ¸ÑÑ†Ğ¸Ğ¿Ğ»Ğ¸Ğ½Ğ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹, Ğ¾Ğ±Ğ·Ğ¾Ñ€Ñ‹ Ğ»Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ñ‹, Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸ Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²ĞµĞ½Ğ¸Ñ Ğ¸ Ğ³Ğ¸Ğ¿Ğ¾Ñ‚ĞµĞ·Ñ‹. ĞÑ†ĞµĞ½ĞºĞ° Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ°, Ñ‡Ñ‚Ğ¾ LLM Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ¼ Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ¸Ñ‚ Ğ¾Ğ± Ğ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ°ÑÑĞ¾Ñ†Ğ¸Ğ°Ñ†Ğ¸Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ LLM ĞºĞ°Ğº Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ñ, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ³Ğ¸Ğ¿Ğ¾Ñ‚ĞµĞ·Ñ‹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°Ñ….'}, 'en': {'title': 'Unlocking Scientific Discovery with LLMs', 'desc': 'This paper introduces a new benchmark to evaluate large language models (LLMs) in the context of scientific research. It focuses on three key tasks: retrieving inspirations, composing hypotheses, and ranking them based on quality. The authors developed an automated framework that accurately extracts essential elements from scientific papers, validated by experts. The findings indicate that LLMs excel at retrieving inspirations, highlighting their potential to generate innovative research hypotheses efficiently.'}, 'zh': {'title': 'å¤§å‹è¯­è¨€æ¨¡å‹åŠ©åŠ›ç§‘å­¦å‘ç°çš„æ½œåŠ›', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç§‘å­¦ç ”ç©¶ä¸­æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å®ƒä»¬å‘ç°é«˜è´¨é‡ç ”ç©¶å‡è®¾çš„èƒ½åŠ›å°šæœªå¾—åˆ°éªŒè¯ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç¬¬ä¸€ä¸ªå¤§è§„æ¨¡åŸºå‡†ï¼Œç”¨äºè¯„ä¼°LLMsåœ¨ç§‘å­¦å‘ç°ä¸­çš„è¡¨ç°ï¼ŒåŒ…æ‹¬çµæ„Ÿæ£€ç´¢ã€å‡è®¾æ„å»ºå’Œå‡è®¾æ’åºç­‰å­ä»»åŠ¡ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªè‡ªåŠ¨åŒ–æ¡†æ¶ï¼Œä»12ä¸ªå­¦ç§‘çš„ç§‘å­¦è®ºæ–‡ä¸­æå–å…³é”®ç»„ä»¶ï¼Œå¹¶é€šè¿‡ä¸“å®¶éªŒè¯ç¡®è®¤å…¶å‡†ç¡®æ€§ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒLLMsåœ¨æ£€ç´¢çµæ„Ÿæ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œè¡¨æ˜å®ƒä»¬èƒ½å¤Ÿå‘ç°æ–°çš„çŸ¥è¯†å…³è”ï¼Œä»è€Œæ¨åŠ¨è‡ªåŠ¨åŒ–ç§‘å­¦å‘ç°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.21758', 'title': 'Lumina-Image 2.0: A Unified and Efficient Image Generative Framework', 'url': 'https://huggingface.co/papers/2503.21758', 'abstract': 'We introduce Lumina-Image 2.0, an advanced text-to-image generation framework that achieves significant progress compared to previous work, Lumina-Next. Lumina-Image 2.0 is built upon two key principles: (1) Unification - it adopts a unified architecture (Unified Next-DiT) that treats text and image tokens as a joint sequence, enabling natural cross-modal interactions and allowing seamless task expansion. Besides, since high-quality captioners can provide semantically well-aligned text-image training pairs, we introduce a unified captioning system, Unified Captioner (UniCap), specifically designed for T2I generation tasks. UniCap excels at generating comprehensive and accurate captions, accelerating convergence and enhancing prompt adherence. (2) Efficiency - to improve the efficiency of our proposed model, we develop multi-stage progressive training strategies and introduce inference acceleration techniques without compromising image quality. Extensive evaluations on academic benchmarks and public text-to-image arenas show that Lumina-Image 2.0 delivers strong performances even with only 2.6B parameters, highlighting its scalability and design efficiency. We have released our training details, code, and models at https://github.com/Alpha-VLLM/Lumina-Image-2.0.', 'score': 15, 'issue_id': 2941, 'pub_date': '2025-03-27', 'pub_date_card': {'ru': '27 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 27', 'zh': '3æœˆ27æ—¥'}, 'hash': '976a9523e7e6ba13', 'authors': ['Qi Qin', 'Le Zhuo', 'Yi Xin', 'Ruoyi Du', 'Zhen Li', 'Bin Fu', 'Yiting Lu', 'Jiakang Yuan', 'Xinyue Li', 'Dongyang Liu', 'Xiangyang Zhu', 'Manyuan Zhang', 'Will Beddow', 'Erwann Millon', 'Victor Perez', 'Wenhai Wang', 'Conghui He', 'Bo Zhang', 'Xiaohong Liu', 'Hongsheng Li', 'Yu Qiao', 'Chang Xu', 'Peng Gao'], 'affiliations': ['Krea AI', 'Shanghai AI Laboratory', 'Shanghai Innovation Institute', 'Shanghai Jiao Tong University', 'The Chinese University of Hong Kong', 'The University of Sydney'], 'pdf_title_img': 'assets/pdf/title_img/2503.21758.jpg', 'data': {'categories': ['#inference', '#cv', '#open_source', '#training', '#multimodal', '#small_models', '#diffusion'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ•Ğ´Ğ¸Ğ½ÑÑ‚Ğ²Ğ¾ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ', 'desc': 'Lumina-Image 2.0 - ÑÑ‚Ğ¾ ÑƒÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ. ĞĞ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ½Ğ° ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰ĞµĞ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞµĞ¹ UniCap. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸ĞºĞ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ»Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² (2.6 Ğ¼Ğ»Ñ€Ğ´), Lumina-Image 2.0 Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ°ĞºĞ°Ğ´ĞµĞ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¸ Ğ² Ğ¿ÑƒĞ±Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑĞ¾Ñ€ĞµĞ²Ğ½Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑÑ… Ğ¿Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Revolutionizing Text-to-Image Generation with Lumina-Image 2.0', 'desc': "Lumina-Image 2.0 is a cutting-edge framework for generating images from text, significantly improving upon its predecessor, Lumina-Next. It utilizes a unified architecture that allows text and image data to interact seamlessly, enhancing the model's ability to handle various tasks. The introduction of the Unified Captioner (UniCap) enables the generation of high-quality captions that align well with images, improving training efficiency and output accuracy. Additionally, the model incorporates advanced training and inference techniques to maintain high image quality while being resource-efficient, demonstrating strong performance with a relatively small number of parameters."}, 'zh': {'title': 'Lumina-Image 2.0ï¼šé«˜æ•ˆçš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ–°çºªå…ƒ', 'desc': 'Lumina-Image 2.0 æ˜¯ä¸€ä¸ªå…ˆè¿›çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¡†æ¶ï¼Œç›¸æ¯”äºä¹‹å‰çš„ Lumina-Next å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚è¯¥æ¡†æ¶åŸºäºä¸¤ä¸ªå…³é”®åŸåˆ™ï¼šç»Ÿä¸€æ€§å’Œæ•ˆç‡ã€‚ç»Ÿä¸€æ€§é€šè¿‡é‡‡ç”¨ç»Ÿä¸€æ¶æ„ï¼ˆUnified Next-DiTï¼‰æ¥å®ç°æ–‡æœ¬å’Œå›¾åƒæ ‡è®°çš„è”åˆå¤„ç†ï¼Œä¿ƒè¿›äº†è·¨æ¨¡æ€çš„è‡ªç„¶äº¤äº’ã€‚æ•ˆç‡æ–¹é¢ï¼Œæˆ‘ä»¬å¼€å‘äº†å¤šé˜¶æ®µæ¸è¿›è®­ç»ƒç­–ç•¥å’Œæ¨ç†åŠ é€ŸæŠ€æœ¯ï¼Œç¡®ä¿åœ¨ä¸é™ä½å›¾åƒè´¨é‡çš„æƒ…å†µä¸‹æé«˜æ¨¡å‹æ•ˆç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.21774', 'title': 'Optimal Stepsize for Diffusion Sampling', 'url': 'https://huggingface.co/papers/2503.21774', 'abstract': 'Diffusion models achieve remarkable generation quality but suffer from computational intensive sampling due to suboptimal step discretization. While existing works focus on optimizing denoising directions, we address the principled design of stepsize schedules. This paper proposes Optimal Stepsize Distillation, a dynamic programming framework that extracts theoretically optimal schedules by distilling knowledge from reference trajectories. By reformulating stepsize optimization as recursive error minimization, our method guarantees global discretization bounds through optimal substructure exploitation. Crucially, the distilled schedules demonstrate strong robustness across architectures, ODE solvers, and noise schedules. Experiments show 10x accelerated text-to-image generation while preserving 99.4% performance on GenEval. Our code is available at https://github.com/bebebe666/OptimalSteps.', 'score': 11, 'issue_id': 2941, 'pub_date': '2025-03-27', 'pub_date_card': {'ru': '27 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 27', 'zh': '3æœˆ27æ—¥'}, 'hash': '709832ee6e0a6f3f', 'authors': ['Jianning Pei', 'Han Hu', 'Shuyang Gu'], 'affiliations': ['Tencent Hunyuan Research', 'University Chinese Academic of Science'], 'pdf_title_img': 'assets/pdf/title_img/2503.21774.jpg', 'data': {'categories': ['#training', '#architecture', '#data', '#diffusion', '#optimization'], 'emoji': 'ğŸš€', 'ru': {'title': 'Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°', 'desc': 'Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Optimal Stepsize Distillation, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°ÑĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ ÑˆĞ°Ğ³Ğ¾Ğ² Ğ¸Ğ· ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğº Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°Ğ¼ Ğ¸ ÑˆÑƒĞ¼Ğ¾Ğ²Ñ‹Ğ¼ Ñ€Ğ°ÑĞ¿Ğ¸ÑĞ°Ğ½Ğ¸ÑĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ 10-ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğµ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ 99.4% Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° GenEval.'}, 'en': {'title': 'Accelerating Diffusion Models with Optimal Stepsize Distillation', 'desc': 'This paper introduces a new method called Optimal Stepsize Distillation to improve the efficiency of diffusion models in generating images. It focuses on optimizing the stepsize schedules used during the sampling process, which is often computationally intensive. By using dynamic programming, the method distills optimal schedules from reference trajectories, ensuring that the stepsize is effectively minimized. The results show that this approach can speed up text-to-image generation by 10 times while maintaining high performance levels.'}, 'zh': {'title': 'æœ€ä¼˜æ­¥é•¿è’¸é¦ï¼šåŠ é€Ÿæ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„å…³é”®', 'desc': 'æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆè´¨é‡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†ç”±äºæ­¥éª¤ç¦»æ•£åŒ–ä¸ç†æƒ³ï¼Œå¯¼è‡´è®¡ç®—å¼€é”€å¤§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºæœ€ä¼˜æ­¥é•¿è’¸é¦çš„åŠ¨æ€è§„åˆ’æ¡†æ¶ï¼Œé€šè¿‡ä»å‚è€ƒè½¨è¿¹ä¸­æå–ç†è®ºæœ€ä¼˜çš„æ­¥é•¿è°ƒåº¦æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚æˆ‘ä»¬å°†æ­¥é•¿ä¼˜åŒ–é‡æ–°è¡¨è¿°ä¸ºé€’å½’è¯¯å·®æœ€å°åŒ–ï¼Œä»è€Œä¿è¯äº†å…¨å±€ç¦»æ•£åŒ–ç•Œé™ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒ99.4%æ€§èƒ½çš„åŒæ—¶ï¼Œå®ç°äº†æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„10å€åŠ é€Ÿã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.21765', 'title': 'Exploring the Evolution of Physics Cognition in Video Generation: A\n  Survey', 'url': 'https://huggingface.co/papers/2503.21765', 'abstract': 'Recent advancements in video generation have witnessed significant progress, especially with the rapid advancement of diffusion models. Despite this, their deficiencies in physical cognition have gradually received widespread attention - generated content often violates the fundamental laws of physics, falling into the dilemma of \'\'visual realism but physical absurdity". Researchers began to increasingly recognize the importance of physical fidelity in video generation and attempted to integrate heuristic physical cognition such as motion representations and physical knowledge into generative systems to simulate real-world dynamic scenarios. Considering the lack of a systematic overview in this field, this survey aims to provide a comprehensive summary of architecture designs and their applications to fill this gap. Specifically, we discuss and organize the evolutionary process of physical cognition in video generation from a cognitive science perspective, while proposing a three-tier taxonomy: 1) basic schema perception for generation, 2) passive cognition of physical knowledge for generation, and 3) active cognition for world simulation, encompassing state-of-the-art methods, classical paradigms, and benchmarks. Subsequently, we emphasize the inherent key challenges in this domain and delineate potential pathways for future research, contributing to advancing the frontiers of discussion in both academia and industry. Through structured review and interdisciplinary analysis, this survey aims to provide directional guidance for developing interpretable, controllable, and physically consistent video generation paradigms, thereby propelling generative models from the stage of \'\'visual mimicry\'\' towards a new phase of \'\'human-like physical comprehension\'\'.', 'score': 9, 'issue_id': 2941, 'pub_date': '2025-03-27', 'pub_date_card': {'ru': '27 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 27', 'zh': '3æœˆ27æ—¥'}, 'hash': '87e887fdf8f612cf', 'authors': ['Minghui Lin', 'Xiang Wang', 'Yishan Wang', 'Shu Wang', 'Fengqi Dai', 'Pengxiang Ding', 'Cunxiang Wang', 'Zhengrong Zuo', 'Nong Sang', 'Siteng Huang', 'Donglin Wang'], 'affiliations': ['Huazhong University of Science and Technology', 'Shandong University', 'Tsinghua University', 'Westlake University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.21765.jpg', 'data': {'categories': ['#benchmark', '#survey', '#video', '#architecture', '#diffusion'], 'emoji': 'ğŸ¬', 'ru': {'title': 'ĞÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¸Ğ¼Ğ¸ĞºÑ€Ğ¸Ğ¸ Ğº Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾Ğ¼Ñƒ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ„Ğ¸Ğ·Ğ¸ĞºĞ¸ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'Ğ­Ñ‚Ğ¾Ñ‚ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ‚Ñ€ĞµÑ…ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²ÑƒÑ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰ÑƒÑ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… ÑÑ…ĞµĞ¼, Ğ¿Ğ°ÑÑĞ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ·Ğ½Ğ°Ğ½Ğ¸Ğµ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ² Ğ¸ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¸Ñ€Ğ°. Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¾Ğ±ÑÑƒĞ¶Ğ´Ğ°ÑÑ‚ÑÑ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñ‹ Ğ¸ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğµ Ñ‚ĞµÑÑ‚Ñ‹ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¾ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ğµ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ…, ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ñ… Ğ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'From Visual Mimicry to Human-like Physical Comprehension in Video Generation', 'desc': 'This paper discusses the recent improvements in video generation using diffusion models, highlighting their limitations in understanding physical laws. It points out that while these models can create visually appealing content, they often produce results that do not adhere to the principles of physics, leading to unrealistic scenarios. The authors propose a structured overview of how physical cognition can be integrated into video generation, categorizing it into three levels: basic perception, passive knowledge, and active simulation. The survey aims to guide future research towards creating video generation systems that are not only visually realistic but also physically coherent, moving beyond mere visual mimicry to a deeper understanding of the physical world.'}, 'zh': {'title': 'æ¨åŠ¨è§†é¢‘ç”Ÿæˆå‘äººç±»ç‰©ç†ç†è§£çš„æ–°é˜¶æ®µ', 'desc': 'è¿‘å¹´æ¥ï¼Œè§†é¢‘ç”ŸæˆæŠ€æœ¯å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œå°¤å…¶æ˜¯æ‰©æ•£æ¨¡å‹çš„å¿«é€Ÿå‘å±•ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨ç‰©ç†è®¤çŸ¥æ–¹é¢çš„ä¸è¶³é€æ¸å¼•èµ·äº†å¹¿æ³›å…³æ³¨ï¼Œç”Ÿæˆçš„å†…å®¹å¸¸å¸¸è¿ååŸºæœ¬çš„ç‰©ç†æ³•åˆ™ï¼Œé™·å…¥äº†â€œè§†è§‰çœŸå®ä½†ç‰©ç†è’è°¬â€çš„å›°å¢ƒã€‚ç ”ç©¶äººå‘˜å¼€å§‹è®¤è¯†åˆ°ç‰©ç†çœŸå®æ„Ÿåœ¨è§†é¢‘ç”Ÿæˆä¸­çš„é‡è¦æ€§ï¼Œå¹¶å°è¯•å°†å¯å‘å¼çš„ç‰©ç†è®¤çŸ¥èå…¥ç”Ÿæˆç³»ç»Ÿï¼Œä»¥æ¨¡æ‹Ÿç°å®ä¸–ç•Œçš„åŠ¨æ€åœºæ™¯ã€‚æœ¬æ–‡ç»¼è¿°äº†ç‰©ç†è®¤çŸ¥åœ¨è§†é¢‘ç”Ÿæˆä¸­çš„æ¼”å˜è¿‡ç¨‹ï¼Œæå‡ºäº†ä¸‰å±‚æ¬¡çš„åˆ†ç±»æ³•ï¼Œå¹¶å¼ºè°ƒäº†è¯¥é¢†åŸŸçš„å…³é”®æŒ‘æˆ˜å’Œæœªæ¥ç ”ç©¶çš„æ½œåœ¨æ–¹å‘ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.20822', 'title': 'Synthetic Video Enhances Physical Fidelity in Video Synthesis', 'url': 'https://huggingface.co/papers/2503.20822', 'abstract': 'We investigate how to enhance the physical fidelity of video generation models by leveraging synthetic videos derived from computer graphics pipelines. These rendered videos respect real-world physics, such as maintaining 3D consistency, and serve as a valuable resource that can potentially improve video generation models. To harness this potential, we propose a solution that curates and integrates synthetic data while introducing a method to transfer its physical realism to the model, significantly reducing unwanted artifacts. Through experiments on three representative tasks emphasizing physical consistency, we demonstrate its efficacy in enhancing physical fidelity. While our model still lacks a deep understanding of physics, our work offers one of the first empirical demonstrations that synthetic video enhances physical fidelity in video synthesis. Website: https://kevinz8866.github.io/simulation/', 'score': 8, 'issue_id': 2941, 'pub_date': '2025-03-26', 'pub_date_card': {'ru': '26 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 26', 'zh': '3æœˆ26æ—¥'}, 'hash': '8ad7de4de496fce6', 'authors': ['Qi Zhao', 'Xingyu Ni', 'Ziyu Wang', 'Feng Cheng', 'Ziyan Yang', 'Lu Jiang', 'Bohan Wang'], 'affiliations': ['ByteDance Seed', 'National University of Singapore', 'Peking University', 'ShanghaiTech University'], 'pdf_title_img': 'assets/pdf/title_img/2503.20822.jpg', 'data': {'categories': ['#dataset', '#video', '#data', '#synthetic', '#optimization'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Ğ¡Ğ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸: ÑˆĞ°Ğ³ Ğº Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ´Ğ¾ÑÑ‚Ğ¾Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¸Ğ·ÑƒÑ‡Ğ°ÑÑ‚, ĞºĞ°Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ´Ğ¾ÑÑ‚Ğ¾Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ¹ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¸. Ğ­Ñ‚Ğ¸ Ñ€ĞµĞ½Ğ´ĞµÑ€ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ ÑĞ¾Ğ±Ğ»ÑĞ´Ğ°ÑÑ‚ Ğ·Ğ°ĞºĞ¾Ğ½Ñ‹ Ñ„Ğ¸Ğ·Ğ¸ĞºĞ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ° Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ğ¸ Ğ¸Ñ… Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ¼Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°Ñ Ğ½ĞµĞ¶ĞµĞ»Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ñ‚Ñ€ĞµÑ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ² Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ´Ğ¾ÑÑ‚Ğ¾Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Enhancing Video Realism with Synthetic Physics', 'desc': 'This paper explores how to improve the realism of video generation models by using synthetic videos created through computer graphics. These synthetic videos adhere to real-world physics, ensuring 3D consistency, which can enhance the training of video generation models. The authors propose a method to curate and integrate this synthetic data, allowing the model to adopt the physical realism of the videos and reduce visual artifacts. Their experiments show that this approach effectively boosts the physical fidelity of generated videos, marking a significant step in the field of video synthesis.'}, 'zh': {'title': 'åˆæˆè§†é¢‘æå‡è§†é¢‘ç”Ÿæˆçš„ç‰©ç†çœŸå®æ€§', 'desc': 'æˆ‘ä»¬ç ”ç©¶äº†å¦‚ä½•é€šè¿‡åˆ©ç”¨è®¡ç®—æœºå›¾å½¢å­¦ç”Ÿæˆçš„åˆæˆè§†é¢‘æ¥å¢å¼ºè§†é¢‘ç”Ÿæˆæ¨¡å‹çš„ç‰©ç†çœŸå®æ€§ã€‚è¿™äº›æ¸²æŸ“è§†é¢‘éµå¾ªç°å®ä¸–ç•Œçš„ç‰©ç†è§„å¾‹ï¼Œä¿æŒä¸‰ç»´ä¸€è‡´æ€§ï¼Œæˆä¸ºæ”¹å–„è§†é¢‘ç”Ÿæˆæ¨¡å‹çš„é‡è¦èµ„æºã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§è§£å†³æ–¹æ¡ˆï¼Œç­–åˆ’å’Œæ•´åˆåˆæˆæ•°æ®ï¼ŒåŒæ—¶å¼•å…¥äº†ä¸€ç§å°†ç‰©ç†çœŸå®æ„Ÿè½¬ç§»åˆ°æ¨¡å‹çš„æ–¹æ³•ï¼Œæ˜¾è‘—å‡å°‘äº†ä¸å¿…è¦çš„ä¼ªå½±ã€‚é€šè¿‡åœ¨ä¸‰ä¸ªå¼ºè°ƒç‰©ç†ä¸€è‡´æ€§çš„ä»£è¡¨æ€§ä»»åŠ¡ä¸Šçš„å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†è¿™ç§æ–¹æ³•åœ¨æé«˜ç‰©ç†çœŸå®æ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.21088', 'title': 'ZJUKLAB at SemEval-2025 Task 4: Unlearning via Model Merging', 'url': 'https://huggingface.co/papers/2503.21088', 'abstract': "This paper presents the ZJUKLAB team's submission for SemEval-2025 Task 4: Unlearning Sensitive Content from Large Language Models. This task aims to selectively erase sensitive knowledge from large language models, avoiding both over-forgetting and under-forgetting issues. We propose an unlearning system that leverages Model Merging (specifically TIES-Merging), combining two specialized models into a more balanced unlearned model. Our system achieves competitive results, ranking second among 26 teams, with an online score of 0.944 for Task Aggregate and 0.487 for overall Aggregate. In this paper, we also conduct local experiments and perform a comprehensive analysis of the unlearning process, examining performance trajectories, loss dynamics, and weight perspectives, along with several supplementary experiments, to understand the effectiveness of our method. Furthermore, we analyze the shortcomings of our method and evaluation metrics, emphasizing that MIA scores and ROUGE-based metrics alone are insufficient to fully evaluate successful unlearning. Finally, we emphasize the need for more comprehensive evaluation methodologies and rethinking of unlearning objectives in future research. Code is available at https://github.com/zjunlp/unlearn/tree/main/semeval25.", 'score': 6, 'issue_id': 2941, 'pub_date': '2025-03-27', 'pub_date_card': {'ru': '27 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 27', 'zh': '3æœˆ27æ—¥'}, 'hash': '494379ac783f4297', 'authors': ['Haoming Xu', 'Shuxun Wang', 'Yanqiu Zhao', 'Yi Zhong', 'Ziyan Jiang', 'Ningyuan Zhao', 'Shumin Deng', 'Huajun Chen', 'Ningyu Zhang'], 'affiliations': ['National University of Singapore', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.21088.jpg', 'data': {'categories': ['#benchmark', '#training', '#data', '#leakage', '#ethics', '#hallucinations'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ˜Ğ·Ğ±Ğ¸Ñ€Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ·Ğ°Ğ±Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğµ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ĞºĞ¾Ğ½Ñ„Ğ¸Ğ´ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² LLM', 'desc': "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ±Ğ¸Ñ€Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ´ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸Ğ· Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (Model Merging), Ğ² Ñ‡Ğ°ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ TIES-Merging, Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ 'Ğ·Ğ°Ğ±Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸' Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° ÑĞ¾Ñ€ĞµĞ²Ğ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ SemEval-2025, Ğ·Ğ°Ğ½ÑĞ² Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ğ¼ĞµÑÑ‚Ğ¾ ÑÑ€ĞµĞ´Ğ¸ 26 ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´. Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ñ€Ğ°Ğ·Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ (unlearning) Ğ¸ Ğ¾Ğ±ÑÑƒĞ¶Ğ´Ğ°ÑÑ‚ÑÑ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸Ğ· Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."}, 'en': {'title': 'Mastering Unlearning: Balancing Sensitivity in Language Models', 'desc': "This paper discusses the ZJUKLAB team's approach to the SemEval-2025 Task 4, which focuses on unlearning sensitive information from large language models. The proposed method utilizes Model Merging, specifically TIES-Merging, to create a balanced model that effectively manages the challenges of over-forgetting and under-forgetting. The team achieved impressive results, ranking second out of 26 participants, and conducted thorough experiments to analyze the unlearning process, including performance metrics and loss dynamics. The authors also highlight the limitations of current evaluation methods and advocate for improved metrics to better assess unlearning effectiveness in future studies."}, 'zh': {'title': 'é€‰æ‹©æ€§åˆ é™¤ï¼Œé‡å¡‘è¯­è¨€æ¨¡å‹çš„æœªæ¥', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ZJUKLABå›¢é˜Ÿåœ¨SemEval-2025ä»»åŠ¡4ä¸­çš„æäº¤ï¼Œæ—¨åœ¨ä»å¤§å‹è¯­è¨€æ¨¡å‹ä¸­é€‰æ‹©æ€§åœ°åˆ é™¤æ•æ„Ÿå†…å®¹ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨æ¨¡å‹åˆå¹¶ï¼ˆç‰¹åˆ«æ˜¯TIES-Mergingï¼‰çš„æ–¹æ³•ï¼Œå°†ä¸¤ä¸ªä¸“é—¨æ¨¡å‹ç»“åˆæˆä¸€ä¸ªæ›´å¹³è¡¡çš„æœªå­¦ä¹ æ¨¡å‹ã€‚æˆ‘ä»¬çš„ç³»ç»Ÿåœ¨26ä¸ªå›¢é˜Ÿä¸­æ’åç¬¬äºŒï¼Œä»»åŠ¡èšåˆçš„åœ¨çº¿å¾—åˆ†ä¸º0.944ï¼Œæ€»ä½“èšåˆå¾—åˆ†ä¸º0.487ã€‚æˆ‘ä»¬è¿˜è¿›è¡Œäº†å±€éƒ¨å®éªŒï¼Œå…¨é¢åˆ†æäº†æœªå­¦ä¹ è¿‡ç¨‹çš„è¡¨ç°è½¨è¿¹ã€æŸå¤±åŠ¨æ€å’Œæƒé‡è§†è§’ï¼Œå¹¶å¼ºè°ƒäº†ç°æœ‰è¯„ä¼°æŒ‡æ ‡çš„ä¸è¶³ï¼Œå‘¼åæœªæ¥ç ”ç©¶éœ€è¦æ›´å…¨é¢çš„è¯„ä¼°æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.20776', 'title': 'Feature4X: Bridging Any Monocular Video to 4D Agentic AI with Versatile\n  Gaussian Feature Fields', 'url': 'https://huggingface.co/papers/2503.20776', 'abstract': 'Recent advancements in 2D and multimodal models have achieved remarkable success by leveraging large-scale training on extensive datasets. However, extending these achievements to enable free-form interactions and high-level semantic operations with complex 3D/4D scenes remains challenging. This difficulty stems from the limited availability of large-scale, annotated 3D/4D or multi-view datasets, which are crucial for generalizable vision and language tasks such as open-vocabulary and prompt-based segmentation, language-guided editing, and visual question answering (VQA). In this paper, we introduce Feature4X, a universal framework designed to extend any functionality from 2D vision foundation model into the 4D realm, using only monocular video input, which is widely available from user-generated content. The "X" in Feature4X represents its versatility, enabling any task through adaptable, model-conditioned 4D feature field distillation. At the core of our framework is a dynamic optimization strategy that unifies multiple model capabilities into a single representation. Additionally, to the best of our knowledge, Feature4X is the first method to distill and lift the features of video foundation models (e.g. SAM2, InternVideo2) into an explicit 4D feature field using Gaussian Splatting. Our experiments showcase novel view segment anything, geometric and appearance scene editing, and free-form VQA across all time steps, empowered by LLMs in feedback loops. These advancements broaden the scope of agentic AI applications by providing a foundation for scalable, contextually and spatiotemporally aware systems capable of immersive dynamic 4D scene interaction.', 'score': 6, 'issue_id': 2941, 'pub_date': '2025-03-26', 'pub_date_card': {'ru': '26 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 26', 'zh': '3æœˆ26æ—¥'}, 'hash': 'd066b6a18982ec5f', 'authors': ['Shijie Zhou', 'Hui Ren', 'Yijia Weng', 'Shuwang Zhang', 'Zhen Wang', 'Dejia Xu', 'Zhiwen Fan', 'Suya You', 'Zhangyang Wang', 'Leonidas Guibas', 'Achuta Kadambi'], 'affiliations': ['DEVCOM ARL', 'MIT', 'Stanford', 'UCLA', 'UT Austin'], 'pdf_title_img': 'assets/pdf/title_img/2503.20776.jpg', 'data': {'categories': ['#3d', '#cv', '#agi', '#agents', '#dataset', '#multimodal', '#optimization'], 'emoji': 'ğŸŒ', 'ru': {'title': 'Feature4X: Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¼Ğ¾ÑÑ‚ Ğ¼ĞµĞ¶Ğ´Ñƒ 2D Ğ¸ 4D ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ñ‹Ğ¼ Ğ·Ñ€ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Feature4X - ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ 2D Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ½Ğ° 4D Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¼Ğ¾Ğ½Ğ¾ĞºÑƒĞ»ÑÑ€Ğ½Ğ¾Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾. Feature4X Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ, Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ² 4D ÑÑ†ĞµĞ½Ğ°Ñ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº. Ğ’ Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ»ĞµĞ¶Ğ¸Ñ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ°Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ. Ğ­Ñ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² ÑĞ²Ğ½Ğ¾Ğµ 4D Ğ¿Ğ¾Ğ»Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Gaussian Splatting.'}, 'en': {'title': 'Feature4X: Bridging 2D Vision to 4D Interaction', 'desc': 'This paper presents Feature4X, a novel framework that enhances 2D vision models to operate in 4D environments using only monocular video inputs. It addresses the challenge of limited annotated datasets for 3D/4D tasks by enabling versatile interactions and semantic operations in complex scenes. The framework employs a dynamic optimization strategy to unify various model capabilities into a single representation, allowing for adaptable feature extraction. Feature4X is the first to distill video foundation model features into a 4D feature field, facilitating advanced tasks like segmentation, scene editing, and visual question answering with improved context awareness.'}, 'zh': {'title': 'Feature4Xï¼šå°†2Dè§†è§‰æ‰©å±•åˆ°4Dçš„é€šç”¨æ¡†æ¶', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºFeature4Xçš„é€šç”¨æ¡†æ¶ï¼Œæ—¨åœ¨å°†2Dè§†è§‰åŸºç¡€æ¨¡å‹çš„åŠŸèƒ½æ‰©å±•åˆ°4Dé¢†åŸŸã€‚è¯¥æ¡†æ¶ä»…ä½¿ç”¨å•ç›®è§†é¢‘è¾“å…¥ï¼Œè§£å†³äº†3D/4Dæ•°æ®é›†ç¨€ç¼ºçš„é—®é¢˜ã€‚Feature4Xé€šè¿‡åŠ¨æ€ä¼˜åŒ–ç­–ç•¥ï¼Œå°†å¤šç§æ¨¡å‹èƒ½åŠ›ç»Ÿä¸€ä¸ºå•ä¸€è¡¨ç¤ºï¼Œæ”¯æŒå¼€æ”¾è¯æ±‡å’ŒåŸºäºæç¤ºçš„åˆ†å‰²ã€è¯­è¨€å¼•å¯¼ç¼–è¾‘å’Œè§†è§‰é—®ç­”ç­‰ä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿå®ç°æ–°è§†è§’çš„åˆ†å‰²ã€å‡ ä½•å’Œå¤–è§‚åœºæ™¯ç¼–è¾‘ï¼Œä»¥åŠè·¨æ—¶é—´æ­¥çš„è‡ªç”±å½¢å¼è§†è§‰é—®ç­”ï¼Œæ¨åŠ¨äº†æ™ºèƒ½ä»£ç†AIåº”ç”¨çš„å‘å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.21780', 'title': 'Semantic Library Adaptation: LoRA Retrieval and Fusion for\n  Open-Vocabulary Semantic Segmentation', 'url': 'https://huggingface.co/papers/2503.21780', 'abstract': "Open-vocabulary semantic segmentation models associate vision and text to label pixels from an undefined set of classes using textual queries, providing versatile performance on novel datasets. However, large shifts between training and test domains degrade their performance, requiring fine-tuning for effective real-world applications. We introduce Semantic Library Adaptation (SemLA), a novel framework for training-free, test-time domain adaptation. SemLA leverages a library of LoRA-based adapters indexed with CLIP embeddings, dynamically merging the most relevant adapters based on proximity to the target domain in the embedding space. This approach constructs an ad-hoc model tailored to each specific input without additional training. Our method scales efficiently, enhances explainability by tracking adapter contributions, and inherently protects data privacy, making it ideal for sensitive applications. Comprehensive experiments on a 20-domain benchmark built over 10 standard datasets demonstrate SemLA's superior adaptability and performance across diverse settings, establishing a new standard in domain adaptation for open-vocabulary semantic segmentation.", 'score': 5, 'issue_id': 2954, 'pub_date': '2025-03-27', 'pub_date_card': {'ru': '27 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 27', 'zh': '3æœˆ27æ—¥'}, 'hash': 'f676e21908b87897', 'authors': ['Reza Qorbani', 'Gianluca Villani', 'Theodoros Panagiotakopoulos', 'Marc Botet Colomer', 'Linus HÃ¤renstam-Nielsen', 'Mattia Segu', 'Pier Luigi Dovesi', 'Jussi Karlgren', 'Daniel Cremers', 'Federico Tombari', 'Matteo Poggi'], 'affiliations': ['AMD', 'ETH Zurich', 'Google', 'KTH', 'King', 'Munich Center for Machine Learning', 'Silo AI', 'Technical University of Munich', 'The Good AI Lab', 'University of Bologna', 'University of Toronto'], 'pdf_title_img': 'assets/pdf/title_img/2503.21780.jpg', 'data': {'categories': ['#training', '#architecture', '#transfer_learning', '#dataset', '#interpretability', '#security', '#benchmark'], 'emoji': 'ğŸ”„', 'ru': {'title': 'ĞĞ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ»ĞµÑ‚Ñƒ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğº Ğ½Ğ¾Ğ²Ñ‹Ğ¼ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ğ¼ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ SemLA Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞºÑƒ LoRA-Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ğ¾Ğ², Ğ¸Ğ½Ğ´ĞµĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ CLIP-ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ². Ğ”Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ²Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ÑÑ‚ÑÑ Ğ¸ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‚ÑÑ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° 20 Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ SemLA Ğ² Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Adapt and Conquer: Dynamic Domain Adaptation for Semantic Segmentation', 'desc': 'This paper presents a new method called Semantic Library Adaptation (SemLA) for improving open-vocabulary semantic segmentation models. These models can label pixels using text queries but struggle when the training and testing data are very different. SemLA allows the model to adapt to new domains at test time without needing additional training by using a library of pre-trained adapters. The method is efficient, enhances model explainability, and protects data privacy, making it suitable for sensitive applications.'}, 'zh': {'title': 'æ— è®­ç»ƒçš„é¢†åŸŸé€‚åº”ï¼Œæå‡è¯­ä¹‰åˆ†å‰²æ€§èƒ½', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œç§°ä¸ºè¯­ä¹‰åº“é€‚åº”ï¼ˆSemLAï¼‰ï¼Œç”¨äºæ— è®­ç»ƒçš„æµ‹è¯•æ—¶é¢†åŸŸé€‚åº”ï¼Œæ—¨åœ¨æé«˜å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²æ¨¡å‹åœ¨ä¸åŒæ•°æ®é›†ä¸Šçš„è¡¨ç°ã€‚SemLAåˆ©ç”¨åŸºäºLoRAçš„é€‚é…å™¨åº“ï¼Œå¹¶é€šè¿‡CLIPåµŒå…¥ç´¢å¼•ï¼ŒåŠ¨æ€åˆå¹¶ä¸ç›®æ ‡é¢†åŸŸæœ€ç›¸å…³çš„é€‚é…å™¨ï¼Œä»è€Œæ„å»ºé’ˆå¯¹ç‰¹å®šè¾“å…¥çš„æ¨¡å‹ã€‚è¯¥æ–¹æ³•æ— éœ€é¢å¤–è®­ç»ƒï¼Œèƒ½å¤Ÿé«˜æ•ˆæ‰©å±•ï¼Œå¹¶é€šè¿‡è·Ÿè¸ªé€‚é…å™¨çš„è´¡çŒ®æ¥å¢å¼ºå¯è§£é‡Šæ€§ï¼ŒåŒæ—¶ä¿æŠ¤æ•°æ®éšç§ï¼Œé€‚åˆæ•æ„Ÿåº”ç”¨ã€‚é€šè¿‡åœ¨20ä¸ªé¢†åŸŸåŸºå‡†ä¸Šçš„å…¨é¢å®éªŒï¼ŒSemLAå±•ç¤ºäº†å…¶åœ¨å¤šç§è®¾ç½®ä¸‹çš„ä¼˜è¶Šé€‚åº”æ€§å’Œæ€§èƒ½ï¼Œç¡®ç«‹äº†å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²é¢†åŸŸé€‚åº”çš„æ–°æ ‡å‡†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.20853', 'title': 'Unified Multimodal Discrete Diffusion', 'url': 'https://huggingface.co/papers/2503.20853', 'abstract': 'Multimodal generative models that can understand and generate across multiple modalities are dominated by autoregressive (AR) approaches, which process tokens sequentially from left to right, or top to bottom. These models jointly handle images, text, video, and audio for various tasks such as image captioning, question answering, and image generation. In this work, we explore discrete diffusion models as a unified generative formulation in the joint text and image domain, building upon their recent success in text generation. Discrete diffusion models offer several advantages over AR models, including improved control over quality versus diversity of generated samples, the ability to perform joint multimodal inpainting (across both text and image domains), and greater controllability in generation through guidance. Leveraging these benefits, we present the first Unified Multimodal Discrete Diffusion (UniDisc) model which is capable of jointly understanding and generating text and images for a variety of downstream tasks. We compare UniDisc to multimodal AR models, performing a scaling analysis and demonstrating that UniDisc outperforms them in terms of both performance and inference-time compute, enhanced controllability, editability, inpainting, and flexible trade-off between inference time and generation quality. Code and additional visualizations are available at https://unidisc.github.io.', 'score': 5, 'issue_id': 2941, 'pub_date': '2025-03-26', 'pub_date_card': {'ru': '26 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 26', 'zh': '3æœˆ26æ—¥'}, 'hash': '9650b4dd1188fcc0', 'authors': ['Alexander Swerdlow', 'Mihir Prabhudesai', 'Siddharth Gandhi', 'Deepak Pathak', 'Katerina Fragkiadaki'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2503.20853.jpg', 'data': {'categories': ['#cv', '#training', '#audio', '#multimodal', '#video', '#diffusion'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'UniDisc: ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ UniDisc, Ğ¿ĞµÑ€Ğ²Ğ°Ñ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ°Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‚ĞµĞºÑÑ‚ Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞœĞ¾Ğ´ĞµĞ»ÑŒ UniDisc Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ñ€ÑĞ´ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ² Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ½Ğ°Ğ´ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸ĞµĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ², Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ğ¿ĞµĞ¹Ğ½Ñ‚Ğ¸Ğ½Ğ³Ğ° Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´ÑÑ‚ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ UniDisc Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ğ¼ Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ³Ğ¸Ğ±ĞºĞ¸Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ñ€ĞµĞ¼ĞµĞ½ĞµĞ¼ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'Revolutionizing Multimodal Generation with UniDisc!', 'desc': 'This paper introduces the Unified Multimodal Discrete Diffusion (UniDisc) model, which is designed to generate and understand both text and images simultaneously. Unlike traditional autoregressive models that process data sequentially, UniDisc utilizes discrete diffusion techniques to enhance the quality and diversity of generated outputs. The model excels in tasks such as multimodal inpainting and offers improved controllability and editability during generation. Through extensive comparisons, UniDisc demonstrates superior performance and efficiency over existing multimodal autoregressive approaches.'}, 'zh': {'title': 'ç»Ÿä¸€å¤šæ¨¡æ€ç”Ÿæˆï¼Œè¶…è¶Šè‡ªå›å½’æ¨¡å‹ï¼', 'desc': 'è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†ä¸€ç§æ–°çš„å¤šæ¨¡æ€ç”Ÿæˆæ¨¡å‹ï¼Œç§°ä¸ºç»Ÿä¸€å¤šæ¨¡æ€ç¦»æ•£æ‰©æ•£æ¨¡å‹ï¼ˆUniDiscï¼‰ã€‚ä¸ä¼ ç»Ÿçš„è‡ªå›å½’æ¨¡å‹ä¸åŒï¼ŒUniDiscèƒ½å¤ŸåŒæ—¶ç†è§£å’Œç”Ÿæˆæ–‡æœ¬ä¸å›¾åƒï¼Œé€‚ç”¨äºå¤šç§ä»»åŠ¡ã€‚è¯¥æ¨¡å‹åœ¨ç”Ÿæˆæ ·æœ¬çš„è´¨é‡ä¸å¤šæ ·æ€§ä¹‹é—´æä¾›äº†æ›´å¥½çš„æ§åˆ¶ï¼Œå¹¶ä¸”èƒ½å¤Ÿè¿›è¡Œè·¨æ–‡æœ¬å’Œå›¾åƒçš„è”åˆä¿®å¤ã€‚é€šè¿‡ä¸è‡ªå›å½’æ¨¡å‹çš„æ¯”è¾ƒï¼ŒUniDiscåœ¨æ€§èƒ½ã€è®¡ç®—æ•ˆç‡å’Œå¯æ§æ€§ç­‰æ–¹é¢è¡¨ç°æ›´ä¼˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.20578', 'title': 'LLPut: Investigating Large Language Models for Bug Report-Based Input\n  Generation', 'url': 'https://huggingface.co/papers/2503.20578', 'abstract': 'Failure-inducing inputs play a crucial role in diagnosing and analyzing software bugs. Bug reports typically contain these inputs, which developers extract to facilitate debugging. Since bug reports are written in natural language, prior research has leveraged various Natural Language Processing (NLP) techniques for automated input extraction. With the advent of Large Language Models (LLMs), an important research question arises: how effectively can generative LLMs extract failure-inducing inputs from bug reports? In this paper, we propose LLPut, a technique to empirically evaluate the performance of three open-source generative LLMs -- LLaMA, Qwen, and Qwen-Coder -- in extracting relevant inputs from bug reports. We conduct an experimental evaluation on a dataset of 206 bug reports to assess the accuracy and effectiveness of these models. Our findings provide insights into the capabilities and limitations of generative LLMs in automated bug diagnosis.', 'score': 4, 'issue_id': 2941, 'pub_date': '2025-03-26', 'pub_date_card': {'ru': '26 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 26', 'zh': '3æœˆ26æ—¥'}, 'hash': '5ee433dad4dd5d00', 'authors': ['Alif Al Hasan', 'Subarna Saha', 'Mia Mohammad Imran', 'Tarannum Shaila Zaman'], 'affiliations': ['Jahangirnagar University Dhaka, Bangladesh', 'Missouri University of Science and Technology Rolla, Missouri, USA', 'University of Maryland Baltimore County Bsltimore, Maryland, USA'], 'pdf_title_img': 'assets/pdf/title_img/2503.20578.jpg', 'data': {'categories': ['#open_source', '#science', '#dataset', '#multimodal', '#data'], 'emoji': 'ğŸ›', 'ru': {'title': 'Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° ÑÑ‚Ñ€Ğ°Ğ¶Ğµ Ğ¾Ñ‚Ğ»Ğ°Ğ´ĞºĞ¸: Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ± Ğ¾ÑˆĞ¸Ğ±ĞºĞ°Ñ… Ğ¸Ğ· Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ¾Ğ²', 'desc': 'Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ¾Ñ†ĞµĞ½ĞºĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ²Ñ‹Ğ·Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸, Ğ¸Ğ· Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ¾Ğ² Ğ¾ Ğ±Ğ°Ğ³Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¸ĞºÑƒ LLPut Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ‚Ñ€ĞµÑ… Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… LLM: LLaMA, Qwen Ğ¸ Qwen-Coder. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ğ»ÑÑ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ¸Ğ· 206 Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ¾Ğ² Ğ¾ Ğ±Ğ°Ğ³Ğ°Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ ÑÑ‚Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑÑ… Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸ÑÑ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… LLM Ğ² Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞµ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº.'}, 'en': {'title': 'Harnessing LLMs for Smart Bug Diagnosis', 'desc': 'This paper investigates how well generative Large Language Models (LLMs) can extract failure-inducing inputs from bug reports, which are essential for diagnosing software issues. The authors introduce a technique called LLPut to evaluate the performance of three open-source LLMs: LLaMA, Qwen, and Qwen-Coder. They conduct experiments on a dataset of 206 bug reports to measure the accuracy and effectiveness of these models in identifying relevant inputs. The results offer valuable insights into the strengths and weaknesses of using generative LLMs for automated bug diagnosis.'}, 'zh': {'title': 'åˆ©ç”¨LLMæå‡ç¼ºé™·æŠ¥å‘Šåˆ†æçš„æ•ˆç‡', 'desc': 'æœ¬æ–‡æ¢è®¨äº†å¦‚ä½•åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä»è½¯ä»¶ç¼ºé™·æŠ¥å‘Šä¸­æå–å¯¼è‡´æ•…éšœçš„è¾“å…¥ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºLLPutçš„æŠ€æœ¯ï¼Œæ—¨åœ¨è¯„ä¼°ä¸‰ç§å¼€æºç”Ÿæˆæ€§LLMï¼ˆLLaMAã€Qwenå’ŒQwen-Coderï¼‰åœ¨è¿™ä¸€ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚é€šè¿‡å¯¹206ä¸ªç¼ºé™·æŠ¥å‘Šçš„æ•°æ®é›†è¿›è¡Œå®éªŒè¯„ä¼°ï¼Œæˆ‘ä»¬åˆ†æäº†è¿™äº›æ¨¡å‹çš„å‡†ç¡®æ€§å’Œæœ‰æ•ˆæ€§ã€‚ç ”ç©¶ç»“æœæ­ç¤ºäº†ç”Ÿæˆæ€§LLMåœ¨è‡ªåŠ¨åŒ–ç¼ºé™·è¯Šæ–­ä¸­çš„èƒ½åŠ›å’Œå±€é™æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.21541', 'title': 'LOCATEdit: Graph Laplacian Optimized Cross Attention for Localized\n  Text-Guided Image Editing', 'url': 'https://huggingface.co/papers/2503.21541', 'abstract': 'Text-guided image editing aims to modify specific regions of an image according to natural language instructions while maintaining the general structure and the background fidelity. Existing methods utilize masks derived from cross-attention maps generated from diffusion models to identify the target regions for modification. However, since cross-attention mechanisms focus on semantic relevance, they struggle to maintain the image integrity. As a result, these methods often lack spatial consistency, leading to editing artifacts and distortions. In this work, we address these limitations and introduce LOCATEdit, which enhances cross-attention maps through a graph-based approach utilizing self-attention-derived patch relationships to maintain smooth, coherent attention across image regions, ensuring that alterations are limited to the designated items while retaining the surrounding structure. \\method consistently and substantially outperforms existing baselines on PIE-Bench, demonstrating its state-of-the-art performance and effectiveness on various editing tasks. Code can be found on https://github.com/LOCATEdit/LOCATEdit/', 'score': 1, 'issue_id': 2950, 'pub_date': '2025-03-27', 'pub_date_card': {'ru': '27 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 27', 'zh': '3æœˆ27æ—¥'}, 'hash': '38e7c3d2a3738793', 'authors': ['Achint Soni', 'Meet Soni', 'Sirisha Rambhatla'], 'affiliations': ['Stony Brook University', 'University of Waterloo'], 'pdf_title_img': 'assets/pdf/title_img/2503.21541.jpg', 'data': {'categories': ['#cv', '#multimodal', '#architecture', '#open_source', '#games', '#optimization', '#graphs', '#benchmark'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'Ğ¢Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹', 'desc': 'LOCATEdit - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ€Ñ‚ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ‚ÑŒ Ñ†ĞµĞ»Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸Ğ·Ğ±ĞµĞ³Ğ°Ñ‚ÑŒ Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸. LOCATEdit Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ PIE-Bench Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'Enhancing Text-Guided Image Editing with LOCATEdit', 'desc': 'This paper presents LOCATEdit, a novel approach for text-guided image editing that improves upon existing methods by addressing issues of spatial consistency. Traditional techniques rely on cross-attention maps from diffusion models, which can lead to artifacts due to their focus on semantic relevance rather than spatial integrity. LOCATEdit enhances these maps using a graph-based method that leverages self-attention to maintain coherent attention across different regions of the image. The results show that LOCATEdit significantly outperforms current baselines on the PIE-Bench dataset, proving its effectiveness in preserving the overall structure while allowing precise modifications.'}, 'zh': {'title': 'ç²¾ç¡®å›¾åƒç¼–è¾‘ï¼Œä¿æŒç»“æ„å®Œæ•´æ€§', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºLOCATEditçš„å›¾åƒç¼–è¾‘æ–¹æ³•ï¼Œæ—¨åœ¨æ ¹æ®è‡ªç„¶è¯­è¨€æŒ‡ä»¤ä¿®æ”¹å›¾åƒçš„ç‰¹å®šåŒºåŸŸï¼ŒåŒæ—¶ä¿æŒæ•´ä½“ç»“æ„å’ŒèƒŒæ™¯çš„å®Œæ•´æ€§ã€‚ç°æœ‰æ–¹æ³•ä½¿ç”¨æ¥è‡ªæ‰©æ•£æ¨¡å‹çš„äº¤å‰æ³¨æ„åŠ›å›¾ç”Ÿæˆçš„æ©ç æ¥è¯†åˆ«ç›®æ ‡åŒºåŸŸï¼Œä½†ç”±äºäº¤å‰æ³¨æ„åŠ›æœºåˆ¶å…³æ³¨è¯­ä¹‰ç›¸å…³æ€§ï¼Œå¯¼è‡´å›¾åƒå®Œæ•´æ€§éš¾ä»¥ä¿æŒã€‚LOCATEdité€šè¿‡åŸºäºå›¾çš„è‡ªæ³¨æ„åŠ›æ–¹æ³•å¢å¼ºäº¤å‰æ³¨æ„åŠ›å›¾ï¼Œç¡®ä¿å›¾åƒåŒºåŸŸä¹‹é—´çš„å¹³æ»‘ä¸€è‡´æ€§ï¼Œä»è€Œé™åˆ¶ä¿®æ”¹ä»…åœ¨æŒ‡å®šé¡¹ç›®ä¸Šï¼ŒåŒæ—¶ä¿ç•™å‘¨å›´ç»“æ„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLOCATEditåœ¨PIE-Benchä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ï¼Œå±•ç¤ºäº†å…¶åœ¨å„ç§ç¼–è¾‘ä»»åŠ¡ä¸­çš„å…ˆè¿›æ€§èƒ½å’Œæœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.19904', 'title': 'Tracktention: Leveraging Point Tracking to Attend Videos Faster and\n  Better', 'url': 'https://huggingface.co/papers/2503.19904', 'abstract': 'Temporal consistency is critical in video prediction to ensure that outputs are coherent and free of artifacts. Traditional methods, such as temporal attention and 3D convolution, may struggle with significant object motion and may not capture long-range temporal dependencies in dynamic scenes. To address this gap, we propose the Tracktention Layer, a novel architectural component that explicitly integrates motion information using point tracks, i.e., sequences of corresponding points across frames. By incorporating these motion cues, the Tracktention Layer enhances temporal alignment and effectively handles complex object motions, maintaining consistent feature representations over time. Our approach is computationally efficient and can be seamlessly integrated into existing models, such as Vision Transformers, with minimal modification. It can be used to upgrade image-only models to state-of-the-art video ones, sometimes outperforming models natively designed for video prediction. We demonstrate this on video depth prediction and video colorization, where models augmented with the Tracktention Layer exhibit significantly improved temporal consistency compared to baselines.', 'score': 1, 'issue_id': 2957, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 25', 'zh': '3æœˆ25æ—¥'}, 'hash': '14b58ae496ab3e3b', 'authors': ['Zihang Lai', 'Andrea Vedaldi'], 'affiliations': ['Visual Geometry Group (VGG), University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2503.19904.jpg', 'data': {'categories': ['#architecture', '#optimization', '#video'], 'emoji': 'ğŸï¸', 'ru': {'title': 'Tracktention Layer: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Tracktention Layer Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ­Ñ‚Ğ¾Ñ‚ ÑĞ»Ğ¾Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ñ‚Ğ¾Ñ‡ĞµĞº Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ğ´Ñ€Ğ°Ğ¼Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². Tracktention Layer Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ»ĞµĞ³ĞºĞ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ² ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Vision Transformers, Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸ÑĞ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ¸ ĞºĞ¾Ğ»Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ³Ğ´Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Tracktention Layer Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½ÑƒÑ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸.'}, 'en': {'title': 'Enhancing Video Prediction with Tracktention for Temporal Consistency', 'desc': 'This paper introduces the Tracktention Layer, a new component designed to improve video prediction by enhancing temporal consistency. Traditional methods often fail to manage significant object motion and long-range dependencies, leading to artifacts in the output. The Tracktention Layer utilizes point tracks to integrate motion information, allowing for better alignment and representation of features over time. This approach is efficient and can be easily added to existing models, showing superior performance in tasks like video depth prediction and colorization.'}, 'zh': {'title': 'æå‡è§†é¢‘é¢„æµ‹çš„æ—¶é—´ä¸€è‡´æ€§', 'desc': 'åœ¨è§†é¢‘é¢„æµ‹ä¸­ï¼Œæ—¶é—´ä¸€è‡´æ€§éå¸¸é‡è¦ï¼Œä»¥ç¡®ä¿è¾“å‡ºç»“æœè¿è´¯ä¸”æ²¡æœ‰ä¼ªå½±ã€‚ä¼ ç»Ÿæ–¹æ³•å¦‚æ—¶é—´æ³¨æ„åŠ›å’Œ3Då·ç§¯åœ¨å¤„ç†æ˜¾è‘—ç‰©ä½“è¿åŠ¨æ—¶å¯èƒ½ä¼šé‡åˆ°å›°éš¾ï¼Œæ— æ³•æ•æ‰åŠ¨æ€åœºæ™¯ä¸­çš„é•¿è·ç¦»æ—¶é—´ä¾èµ–å…³ç³»ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Tracktentionå±‚ï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„æ¶æ„ç»„ä»¶ï¼Œæ˜ç¡®æ•´åˆäº†è¿åŠ¨ä¿¡æ¯ï¼Œé€šè¿‡ç‚¹è½¨è¿¹æ¥å®ç°ã€‚é€šè¿‡å¼•å…¥è¿™äº›è¿åŠ¨çº¿ç´¢ï¼ŒTracktentionå±‚å¢å¼ºäº†æ—¶é—´å¯¹é½èƒ½åŠ›ï¼Œæœ‰æ•ˆå¤„ç†å¤æ‚çš„ç‰©ä½“è¿åŠ¨ï¼Œä¿æŒäº†ä¸€è‡´çš„ç‰¹å¾è¡¨ç¤ºã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.20215', 'title': 'Qwen2.5-Omni Technical Report', 'url': 'https://huggingface.co/papers/2503.20215', 'abstract': "In this report, we present Qwen2.5-Omni, an end-to-end multimodal model designed to perceive diverse modalities, including text, images, audio, and video, while simultaneously generating text and natural speech responses in a streaming manner. To enable the streaming of multimodal information inputs, both audio and visual encoders utilize a block-wise processing approach. To synchronize the timestamps of video inputs with audio, we organize the audio and video sequentially in an interleaved manner and propose a novel position embedding approach, named TMRoPE(Time-aligned Multimodal RoPE). To concurrently generate text and speech while avoiding interference between the two modalities, we propose Thinker-Talker architecture. In this framework, Thinker functions as a large language model tasked with text generation, while Talker is a dual-track autoregressive model that directly utilizes the hidden representations from the Thinker to produce audio tokens as output. Both the Thinker and Talker models are designed to be trained and inferred in an end-to-end manner. For decoding audio tokens in a streaming manner, we introduce a sliding-window DiT that restricts the receptive field, aiming to reduce the initial package delay. Qwen2.5-Omni is comparable with the similarly sized Qwen2.5-VL and outperforms Qwen2-Audio. Furthermore, Qwen2.5-Omni achieves state-of-the-art performance on multimodal benchmarks like Omni-Bench. Notably, Qwen2.5-Omni's performance in end-to-end speech instruction following is comparable to its capabilities with text inputs, as evidenced by benchmarks such as MMLU and GSM8K. As for speech generation, Qwen2.5-Omni's streaming Talker outperforms most existing streaming and non-streaming alternatives in robustness and naturalness.", 'score': 55, 'issue_id': 2924, 'pub_date': '2025-03-26', 'pub_date_card': {'ru': '26 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 26', 'zh': '3æœˆ26æ—¥'}, 'hash': 'dd7a3c8e8564b973', 'authors': ['Jin Xu', 'Zhifang Guo', 'Jinzheng He', 'Hangrui Hu', 'Ting He', 'Shuai Bai', 'Keqin Chen', 'Jialin Wang', 'Yang Fan', 'Kai Dang', 'Bin Zhang', 'Xiong Wang', 'Yunfei Chu', 'Junyang Lin'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2503.20215.jpg', 'data': {'categories': ['#architecture', '#agi', '#benchmark', '#multimodal', '#video', '#games', '#audio'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Qwen2.5-Omni: ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ˜Ğ˜ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ', 'desc': 'Qwen2.5-Omni - ÑÑ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ°Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ñ‚ĞµĞºÑÑ‚, Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¸ Ñ€ĞµÑ‡ĞµĞ²Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ² Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ¾Ğ¼ Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ±Ğ»Ğ¾Ñ‡Ğ½ÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ´Ğ»Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ², Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¼Ñƒ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ TMRoPE Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğº. ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Thinker-Talker Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‚ĞµĞºÑÑ‚ Ğ¸ Ñ€ĞµÑ‡ÑŒ Ğ±ĞµĞ· Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ¼ĞµÑ…. Qwen2.5-Omni Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Omni-Bench Ğ¸ MMLU.'}, 'en': {'title': 'Streamlining Multimodal Interaction with Qwen2.5-Omni', 'desc': 'Qwen2.5-Omni is a cutting-edge multimodal model that can process and generate responses across various formats, including text, images, audio, and video. It employs a unique block-wise processing method for audio and visual data to facilitate real-time streaming. The model features a dual architecture, where the Thinker generates text and the Talker produces audio, ensuring smooth interaction between the two. With innovative techniques like TMRoPE for timestamp alignment and a sliding-window DiT for audio decoding, Qwen2.5-Omni sets new benchmarks in multimodal performance and speech generation.'}, 'zh': {'title': 'å¤šæ¨¡æ€æµå¼ç”Ÿæˆçš„æœªæ¥', 'desc': 'æœ¬æ–‡ä»‹ç»äº†Qwen2.5-Omniï¼Œè¿™æ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯çš„å¤šæ¨¡æ€æ¨¡å‹ï¼Œèƒ½å¤Ÿå¤„ç†æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘å’Œè§†é¢‘ç­‰å¤šç§è¾“å…¥ï¼ŒåŒæ—¶ç”Ÿæˆæ–‡æœ¬å’Œè‡ªç„¶è¯­éŸ³å“åº”ã€‚ä¸ºäº†å®ç°å¤šæ¨¡æ€ä¿¡æ¯çš„æµå¼å¤„ç†ï¼ŒéŸ³é¢‘å’Œè§†è§‰ç¼–ç å™¨é‡‡ç”¨äº†å—å¤„ç†çš„æ–¹æ³•ï¼Œå¹¶é€šè¿‡ä¸€ç§æ–°é¢–çš„ä½ç½®åµŒå…¥æ–¹æ³•TMRoPEæ¥åŒæ­¥éŸ³é¢‘å’Œè§†é¢‘çš„æ—¶é—´æˆ³ã€‚è¯¥æ¨¡å‹çš„Thinker-Talkeræ¶æ„ä½¿å¾—æ–‡æœ¬ç”Ÿæˆå’Œè¯­éŸ³ç”Ÿæˆå¯ä»¥å¹¶è¡Œè¿›è¡Œï¼Œé¿å…äº†ä¸¤è€…ä¹‹é—´çš„å¹²æ‰°ã€‚Qwen2.5-Omniåœ¨å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶åœ¨æµå¼è¯­éŸ³ç”Ÿæˆæ–¹é¢ï¼Œå…¶æ€§èƒ½ä¼˜äºå¤§å¤šæ•°ç°æœ‰çš„æ›¿ä»£æ–¹æ¡ˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.19757', 'title': 'Dita: Scaling Diffusion Transformer for Generalist\n  Vision-Language-Action Policy', 'url': 'https://huggingface.co/papers/2503.19757', 'abstract': "While recent vision-language-action models trained on diverse robot datasets exhibit promising generalization capabilities with limited in-domain data, their reliance on compact action heads to predict discretized or continuous actions constrains adaptability to heterogeneous action spaces. We present Dita, a scalable framework that leverages Transformer architectures to directly denoise continuous action sequences through a unified multimodal diffusion process. Departing from prior methods that condition denoising on fused embeddings via shallow networks, Dita employs in-context conditioning -- enabling fine-grained alignment between denoised actions and raw visual tokens from historical observations. This design explicitly models action deltas and environmental nuances. By scaling the diffusion action denoiser alongside the Transformer's scalability, Dita effectively integrates cross-embodiment datasets across diverse camera perspectives, observation scenes, tasks, and action spaces. Such synergy enhances robustness against various variances and facilitates the successful execution of long-horizon tasks. Evaluations across extensive benchmarks demonstrate state-of-the-art or comparative performance in simulation. Notably, Dita achieves robust real-world adaptation to environmental variances and complex long-horizon tasks through 10-shot finetuning, using only third-person camera inputs. The architecture establishes a versatile, lightweight and open-source baseline for generalist robot policy learning. Project Page: https://robodita.github.io.", 'score': 39, 'issue_id': 2920, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 25', 'zh': '3æœˆ25æ—¥'}, 'hash': 'f481410d892c371a', 'authors': ['Zhi Hou', 'Tianyi Zhang', 'Yuwen Xiong', 'Haonan Duan', 'Hengjun Pu', 'Ronglei Tong', 'Chengyang Zhao', 'Xizhou Zhu', 'Yu Qiao', 'Jifeng Dai', 'Yuntao Chen'], 'affiliations': ['Center for Artificial Intelligence and Robotics, HKISI, CAS', 'College of Computer Science and Technology, Zhejiang University', 'MMLab, The Chinese University of Hong Kong', 'Peking University', 'SenseTime Research', 'Shanghai AI Lab', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.19757.jpg', 'data': {'categories': ['#cv', '#open_source', '#multimodal', '#architecture', '#diffusion', '#agents', '#training'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Dita - Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ´ĞµĞ½Ğ¾Ğ¹Ğ·Ğ¸Ğ½Ğ³Ğ° Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², Dita Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑĞ»Ğ¾Ğ²Ğ»Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½ĞµĞµ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°Ñ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğº Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸ÑĞ¼ ÑÑ€ĞµĞ´Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Dita ĞºĞ°Ğº Ğ² ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸, Ñ‚Ğ°Ğº Ğ¸ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¼Ğ¸Ñ€Ğµ Ğ¿Ñ€Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğº ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼.'}, 'en': {'title': 'Dita: Transforming Robot Action Learning with Multimodal Diffusion', 'desc': 'The paper introduces Dita, a new framework that improves how robots learn to perform actions by using advanced Transformer models. Unlike previous methods that used simple networks to predict actions, Dita employs a sophisticated approach called in-context conditioning, which helps align actions more closely with visual information from past experiences. This allows Dita to effectively handle a wide range of actions and environments, making it adaptable to different tasks and camera views. The results show that Dita not only performs well in simulations but also adapts successfully to real-world scenarios with minimal additional training.'}, 'zh': {'title': 'Ditaï¼šæå‡æœºå™¨äººé€‚åº”èƒ½åŠ›çš„åˆ›æ–°æ¡†æ¶', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºDitaçš„æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜æœºå™¨äººåœ¨å¤šæ ·åŒ–åŠ¨ä½œç©ºé—´ä¸­çš„é€‚åº”èƒ½åŠ›ã€‚Ditaåˆ©ç”¨Transformeræ¶æ„ï¼Œé€šè¿‡ç»Ÿä¸€çš„å¤šæ¨¡æ€æ‰©æ•£è¿‡ç¨‹ç›´æ¥å»å™ªè¿ç»­åŠ¨ä½œåºåˆ—ï¼Œå…‹æœäº†ä¼ ç»Ÿæ–¹æ³•çš„å±€é™ã€‚è¯¥æ¡†æ¶é€šè¿‡ä¸Šä¸‹æ–‡æ¡ä»¶åŒ–å®ç°äº†å»å™ªåŠ¨ä½œä¸å†å²è§‚å¯Ÿçš„åŸå§‹è§†è§‰æ ‡è®°ä¹‹é—´çš„ç²¾ç»†å¯¹é½ï¼Œä»è€Œæ›´å¥½åœ°å»ºæ¨¡åŠ¨ä½œå˜åŒ–å’Œç¯å¢ƒç»†èŠ‚ã€‚Ditaåœ¨å¤šç§åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿæœ‰æ•ˆé€‚åº”çœŸå®ä¸–ç•Œçš„ç¯å¢ƒå˜åŒ–ï¼Œå¹¶æˆåŠŸæ‰§è¡Œå¤æ‚çš„é•¿æœŸä»»åŠ¡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.20314', 'title': 'Wan: Open and Advanced Large-Scale Video Generative Models', 'url': 'https://huggingface.co/papers/2503.20314', 'abstract': "This report presents Wan, a comprehensive and open suite of video foundation models designed to push the boundaries of video generation. Built upon the mainstream diffusion transformer paradigm, Wan achieves significant advancements in generative capabilities through a series of innovations, including our novel VAE, scalable pre-training strategies, large-scale data curation, and automated evaluation metrics. These contributions collectively enhance the model's performance and versatility. Specifically, Wan is characterized by four key features: Leading Performance: The 14B model of Wan, trained on a vast dataset comprising billions of images and videos, demonstrates the scaling laws of video generation with respect to both data and model size. It consistently outperforms the existing open-source models as well as state-of-the-art commercial solutions across multiple internal and external benchmarks, demonstrating a clear and significant performance superiority. Comprehensiveness: Wan offers two capable models, i.e., 1.3B and 14B parameters, for efficiency and effectiveness respectively. It also covers multiple downstream applications, including image-to-video, instruction-guided video editing, and personal video generation, encompassing up to eight tasks. Consumer-Grade Efficiency: The 1.3B model demonstrates exceptional resource efficiency, requiring only 8.19 GB VRAM, making it compatible with a wide range of consumer-grade GPUs. Openness: We open-source the entire series of Wan, including source code and all models, with the goal of fostering the growth of the video generation community. This openness seeks to significantly expand the creative possibilities of video production in the industry and provide academia with high-quality video foundation models. All the code and models are available at https://github.com/Wan-Video/Wan2.1.", 'score': 29, 'issue_id': 2920, 'pub_date': '2025-03-26', 'pub_date_card': {'ru': '26 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 26', 'zh': '3æœˆ26æ—¥'}, 'hash': 'e9770d8e9d313979', 'authors': ['WanTeam', ':', 'Ang Wang', 'Baole Ai', 'Bin Wen', 'Chaojie Mao', 'Chen-Wei Xie', 'Di Chen', 'Feiwu Yu', 'Haiming Zhao', 'Jianxiao Yang', 'Jianyuan Zeng', 'Jiayu Wang', 'Jingfeng Zhang', 'Jingren Zhou', 'Jinkai Wang', 'Jixuan Chen', 'Kai Zhu', 'Kang Zhao', 'Keyu Yan', 'Lianghua Huang', 'Mengyang Feng', 'Ningyi Zhang', 'Pandeng Li', 'Pingyu Wu', 'Ruihang Chu', 'Ruili Feng', 'Shiwei Zhang', 'Siyang Sun', 'Tao Fang', 'Tianxing Wang', 'Tianyi Gui', 'Tingyu Weng', 'Tong Shen', 'Wei Lin', 'Wei Wang', 'Wei Wang', 'Wenmeng Zhou', 'Wente Wang', 'Wenting Shen', 'Wenyuan Yu', 'Xianzhong Shi', 'Xiaoming Huang', 'Xin Xu', 'Yan Kou', 'Yangyu Lv', 'Yifei Li', 'Yijing Liu', 'Yiming Wang', 'Yingya Zhang', 'Yitong Huang', 'Yong Li', 'You Wu', 'Yu Liu', 'Yulin Pan', 'Yun Zheng', 'Yuntao Hong', 'Yupeng Shi', 'Yutong Feng', 'Zeyinzi Jiang', 'Zhen Han', 'Zhi-Fan Wu', 'Ziyu Liu'], 'affiliations': ['Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2503.20314.jpg', 'data': {'categories': ['#video', '#open_source', '#multimodal', '#architecture', '#diffusion', '#benchmark', '#dataset'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Wan: ĞÑ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Wan - ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°. Wan Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¹ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑÑ… Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸ÑĞ¼, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ VAE, Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Wan Ñ 14 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ¾Ğ³Ñ€Ğ¾Ğ¼Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ¸ ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸ÑĞ¼Ğ¸. Wan Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚ Ğ´Ğ»Ñ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ğ°, Ñ‡Ñ‚Ğ¾ ÑĞ¿Ğ¾ÑĞ¾Ğ±ÑÑ‚Ğ²ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¹ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸.'}, 'en': {'title': 'Wan: Revolutionizing Video Generation with Open-Source Models', 'desc': 'This paper introduces Wan, a suite of video foundation models that enhances video generation using a diffusion transformer approach. Wan features a novel Variational Autoencoder (VAE) and scalable pre-training strategies, which improve its generative capabilities. The 14B model, trained on a massive dataset, showcases superior performance compared to existing models, while the 1.3B model offers efficiency for consumer-grade hardware. By open-sourcing the models and code, Wan aims to support the video generation community and expand creative opportunities in video production.'}, 'zh': {'title': 'æ¨åŠ¨è§†é¢‘ç”Ÿæˆçš„å¼€æ”¾æ¨¡å‹â€”â€”Wan', 'desc': 'æœ¬æŠ¥å‘Šä»‹ç»äº†Wanï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢ä¸”å¼€æ”¾çš„è§†é¢‘åŸºç¡€æ¨¡å‹å¥—ä»¶ï¼Œæ—¨åœ¨æ¨åŠ¨è§†é¢‘ç”Ÿæˆçš„è¾¹ç•Œã€‚WanåŸºäºä¸»æµçš„æ‰©æ•£å˜æ¢å™¨èŒƒå¼ï¼Œé€šè¿‡åˆ›æ–°çš„å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰ã€å¯æ‰©å±•çš„é¢„è®­ç»ƒç­–ç•¥ã€å¤§è§„æ¨¡æ•°æ®æ•´ç†å’Œè‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡ï¼Œæ˜¾è‘—æå‡äº†ç”Ÿæˆèƒ½åŠ›ã€‚Wançš„14Bæ¨¡å‹åœ¨æ•°åäº¿å›¾åƒå’Œè§†é¢‘çš„æ•°æ®é›†ä¸Šè®­ç»ƒï¼Œå±•ç¤ºäº†è§†é¢‘ç”Ÿæˆåœ¨æ•°æ®å’Œæ¨¡å‹è§„æ¨¡æ–¹é¢çš„æ‰©å±•è§„å¾‹ï¼Œè¶…è¶Šäº†ç°æœ‰çš„å¼€æºæ¨¡å‹å’Œå•†ä¸šè§£å†³æ–¹æ¡ˆã€‚è¯¥æ¨¡å‹ä¸ä»…é«˜æ•ˆä¸”å¤šåŠŸèƒ½ï¼Œæ”¯æŒå¤šç§ä¸‹æ¸¸åº”ç”¨ï¼Œä¸”æ‰€æœ‰ä»£ç å’Œæ¨¡å‹å‡å·²å¼€æºï¼Œæ—¨åœ¨ä¿ƒè¿›è§†é¢‘ç”Ÿæˆç¤¾åŒºçš„å‘å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.19990', 'title': 'LEGO-Puzzles: How Good Are MLLMs at Multi-Step Spatial Reasoning?', 'url': 'https://huggingface.co/papers/2503.19990', 'abstract': "Multi-step spatial reasoning entails understanding and reasoning about spatial relationships across multiple sequential steps, which is crucial for tackling complex real-world applications, such as robotic manipulation, autonomous navigation, and automated assembly. To assess how well current Multimodal Large Language Models (MLLMs) have acquired this fundamental capability, we introduce LEGO-Puzzles, a scalable benchmark designed to evaluate both spatial understanding and sequential reasoning in MLLMs through LEGO-based tasks. LEGO-Puzzles consists of 1,100 carefully curated visual question-answering (VQA) samples spanning 11 distinct tasks, ranging from basic spatial understanding to complex multi-step reasoning. Based on LEGO-Puzzles, we conduct a comprehensive evaluation of state-of-the-art MLLMs and uncover significant limitations in their spatial reasoning capabilities: even the most powerful MLLMs can answer only about half of the test cases, whereas human participants achieve over 90\\% accuracy. In addition to VQA tasks, we evaluate MLLMs' abilities to generate LEGO images following assembly illustrations. Our experiments show that only Gemini-2.0-Flash and GPT-4o exhibit a limited ability to follow these instructions, while other MLLMs either replicate the input image or generate completely irrelevant outputs. Overall, LEGO-Puzzles exposes critical deficiencies in existing MLLMs' spatial understanding and sequential reasoning capabilities, and underscores the need for further advancements in multimodal spatial reasoning.", 'score': 26, 'issue_id': 2922, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 25', 'zh': '3æœˆ25æ—¥'}, 'hash': 'ec85f1936ae0edd9', 'authors': ['Kexian Tang', 'Junyao Gao', 'Yanhong Zeng', 'Haodong Duan', 'Yanan Sun', 'Zhening Xing', 'Wenran Liu', 'Kaifeng Lyu', 'Kai Chen'], 'affiliations': ['Shanghai AI Laboratory', 'Simons Institute, UC Berkeley', 'Tongji University'], 'pdf_title_img': 'assets/pdf/title_img/2503.19990.jpg', 'data': {'categories': ['#robotics', '#benchmark', '#multimodal', '#reasoning'], 'emoji': 'ğŸ§©', 'ru': {'title': 'LEGO-Puzzles: Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ğ¾Ğ² Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğ¸ Ğ˜Ğ˜', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ LEGO-Puzzles - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñƒ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM). Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· 1100 Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LEGO, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ°ÑĞ¿ĞµĞºÑ‚Ñ‹ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ ÑĞ°Ğ¼Ñ‹Ğµ Ğ¼Ğ¾Ñ‰Ğ½Ñ‹Ğµ MLLM ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ğ»Ğ¸ÑˆÑŒ Ñ Ğ¿Ğ¾Ğ»Ğ¾Ğ²Ğ¸Ğ½Ğ¾Ğ¹ Ñ‚ĞµÑÑ‚Ğ¾Ğ², Ğ² Ñ‚Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ĞºĞ°Ğº Ğ»ÑĞ´Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ 90% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ MLLM Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ LEGO Ğ¿Ğ¾ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼ ÑĞ±Ğ¾Ñ€ĞºĞ¸.'}, 'en': {'title': 'LEGO-Puzzles: Unveiling Spatial Reasoning Gaps in MLLMs', 'desc': "This paper introduces LEGO-Puzzles, a benchmark designed to evaluate the spatial reasoning and sequential understanding capabilities of Multimodal Large Language Models (MLLMs). The benchmark consists of 1,100 visual question-answering samples across 11 tasks, ranging from basic to complex reasoning. The evaluation reveals that current MLLMs struggle with spatial reasoning, achieving only about 50% accuracy compared to over 90% for humans. Additionally, the study assesses MLLMs' ability to generate images based on assembly instructions, finding that only a couple of models perform adequately, highlighting significant gaps in MLLMs' spatial understanding."}, 'zh': {'title': 'LEGO-Puzzlesï¼šè¯„ä¼°å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹çš„ç©ºé—´æ¨ç†èƒ½åŠ›', 'desc': 'å¤šæ­¥éª¤ç©ºé—´æ¨ç†æ˜¯ç†è§£å’Œæ¨ç†ç©ºé—´å…³ç³»çš„é‡è¦èƒ½åŠ›ï¼Œå°¤å…¶åœ¨å¤æ‚çš„ç°å®åº”ç”¨ä¸­ï¼Œå¦‚æœºå™¨äººæ“ä½œå’Œè‡ªåŠ¨å¯¼èˆªã€‚ä¸ºè¯„ä¼°å½“å‰å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è¿™ä¸€èƒ½åŠ›ä¸Šçš„è¡¨ç°ï¼Œæˆ‘ä»¬å¼•å…¥äº†LEGO-Puzzlesï¼Œè¿™æ˜¯ä¸€ä¸ªå¯æ‰©å±•çš„åŸºå‡†ï¼Œæ—¨åœ¨é€šè¿‡åŸºäºLEGOçš„ä»»åŠ¡è¯„ä¼°ç©ºé—´ç†è§£å’Œé¡ºåºæ¨ç†ã€‚LEGO-PuzzlesåŒ…å«1100ä¸ªç²¾å¿ƒç­–åˆ’çš„è§†è§‰é—®ç­”æ ·æœ¬ï¼Œæ¶µç›–ä»åŸºæœ¬ç©ºé—´ç†è§£åˆ°å¤æ‚å¤šæ­¥éª¤æ¨ç†çš„11ä¸ªä¸åŒä»»åŠ¡ã€‚æˆ‘ä»¬çš„è¯„ä¼°æ˜¾ç¤ºï¼Œç°æœ‰çš„MLLMsåœ¨ç©ºé—´æ¨ç†èƒ½åŠ›ä¸Šå­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œæœ€å¼ºçš„æ¨¡å‹ä»…èƒ½å›ç­”çº¦ä¸€åŠçš„æµ‹è¯•æ¡ˆä¾‹ï¼Œè€Œäººç±»å‚ä¸è€…çš„å‡†ç¡®ç‡è¶…è¿‡90%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.20201', 'title': 'Open Deep Search: Democratizing Search with Open-source Reasoning Agents', 'url': 'https://huggingface.co/papers/2503.20201', 'abstract': "We introduce Open Deep Search (ODS) to close the increasing gap between the proprietary search AI solutions, such as Perplexity's Sonar Reasoning Pro and OpenAI's GPT-4o Search Preview, and their open-source counterparts. The main innovation introduced in ODS is to augment the reasoning capabilities of the latest open-source LLMs with reasoning agents that can judiciously use web search tools to answer queries. Concretely, ODS consists of two components that work with a base LLM chosen by the user: Open Search Tool and Open Reasoning Agent. Open Reasoning Agent interprets the given task and completes it by orchestrating a sequence of actions that includes calling tools, one of which is the Open Search Tool. Open Search Tool is a novel web search tool that outperforms proprietary counterparts. Together with powerful open-source reasoning LLMs, such as DeepSeek-R1, ODS nearly matches and sometimes surpasses the existing state-of-the-art baselines on two benchmarks: SimpleQA and FRAMES. For example, on the FRAMES evaluation benchmark, ODS improves the best existing baseline of the recently released GPT-4o Search Preview by 9.7% in accuracy. ODS is a general framework for seamlessly augmenting any LLMs -- for example, DeepSeek-R1 that achieves 82.4% on SimpleQA and 30.1% on FRAMES -- with search and reasoning capabilities to achieve state-of-the-art performance: 88.3% on SimpleQA and 75.3% on FRAMES.", 'score': 22, 'issue_id': 2919, 'pub_date': '2025-03-26', 'pub_date_card': {'ru': '26 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 26', 'zh': '3æœˆ26æ—¥'}, 'hash': 'a9b1bed8d26f5055', 'authors': ['Salaheddin Alzubi', 'Creston Brooks', 'Purva Chiniya', 'Edoardo Contente', 'Chiara von Gerlach', 'Lucas Irwin', 'Yihan Jiang', 'Arda Kaz', 'Windsor Nguyen', 'Sewoong Oh', 'Himanshu Tyagi', 'Pramod Viswanath'], 'affiliations': ['Princeton University', 'Sentient', 'UC Berkeley', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2503.20201.jpg', 'data': {'categories': ['#multimodal', '#open_source', '#agents', '#benchmark', '#reasoning'], 'emoji': 'ğŸ”', 'ru': {'title': 'ODS: Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ˜Ğ˜-Ğ¿Ğ¾Ğ¸ÑĞº Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹', 'desc': 'Open Deep Search (ODS) - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ°Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ²ĞµĞ±-Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹. ODS ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ğ´Ğ²ÑƒÑ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ²: Open Search Tool (Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ¿Ğ¾Ğ¸ÑĞºĞ°) Ğ¸ Open Reasoning Agent (Ğ°Ğ³ĞµĞ½Ñ‚ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ğ¿Ğ¾ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ñƒ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… SimpleQA Ğ¸ FRAMES, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. ODS Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ»ÑĞ±Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑÑ Ğ¸Ğ¼ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Empowering Open-Source LLMs with Advanced Search and Reasoning', 'desc': 'Open Deep Search (ODS) is a new framework designed to enhance the reasoning abilities of open-source large language models (LLMs) by integrating them with advanced web search tools. It features two main components: the Open Search Tool, which performs web searches, and the Open Reasoning Agent, which interprets tasks and coordinates actions, including using the search tool. ODS has shown to significantly improve performance on benchmarks like SimpleQA and FRAMES, even surpassing proprietary models like GPT-4o Search Preview in accuracy. This innovation allows users to leverage powerful open-source LLMs while achieving state-of-the-art results in query answering tasks.'}, 'zh': {'title': 'å¼€æ”¾æ·±åº¦æœç´¢ï¼šæå‡å¼€æºLLMçš„æ¨ç†ä¸æœç´¢èƒ½åŠ›', 'desc': 'æˆ‘ä»¬ä»‹ç»äº†å¼€æ”¾æ·±åº¦æœç´¢ï¼ˆODSï¼‰ï¼Œæ—¨åœ¨ç¼©å°ä¸“æœ‰æœç´¢äººå·¥æ™ºèƒ½è§£å†³æ–¹æ¡ˆä¸å¼€æºè§£å†³æ–¹æ¡ˆä¹‹é—´çš„å·®è·ã€‚ODSçš„ä¸»è¦åˆ›æ–°æ˜¯é€šè¿‡æ¨ç†ä»£ç†å¢å¼ºæœ€æ–°å¼€æºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿæ˜æ™ºåœ°ä½¿ç”¨ç½‘ç»œæœç´¢å·¥å…·æ¥å›ç­”æŸ¥è¯¢ã€‚ODSç”±ä¸¤ä¸ªç»„ä»¶ç»„æˆï¼šå¼€æ”¾æœç´¢å·¥å…·å’Œå¼€æ”¾æ¨ç†ä»£ç†ï¼Œåè€…è´Ÿè´£è§£é‡Šä»»åŠ¡å¹¶åè°ƒä¸€ç³»åˆ—æ“ä½œï¼ŒåŒ…æ‹¬è°ƒç”¨å·¥å…·ã€‚é€šè¿‡ä¸å¼ºå¤§çš„å¼€æºæ¨ç†LLMï¼ˆå¦‚DeepSeek-R1ï¼‰ç»“åˆï¼ŒODSåœ¨SimpleQAå’ŒFRAMESä¸¤ä¸ªåŸºå‡†æµ‹è¯•ä¸­å‡ ä¹è¾¾åˆ°äº†ç°æœ‰çš„æœ€å…ˆè¿›æ°´å¹³ï¼Œç”šè‡³åœ¨FRAMESåŸºå‡†ä¸Šæé«˜äº†9.7%çš„å‡†ç¡®ç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.20240', 'title': 'Unconditional Priors Matter! Improving Conditional Generation of\n  Fine-Tuned Diffusion Models', 'url': 'https://huggingface.co/papers/2503.20240', 'abstract': 'Classifier-Free Guidance (CFG) is a fundamental technique in training conditional diffusion models. The common practice for CFG-based training is to use a single network to learn both conditional and unconditional noise prediction, with a small dropout rate for conditioning. However, we observe that the joint learning of unconditional noise with limited bandwidth in training results in poor priors for the unconditional case. More importantly, these poor unconditional noise predictions become a serious reason for degrading the quality of conditional generation. Inspired by the fact that most CFG-based conditional models are trained by fine-tuning a base model with better unconditional generation, we first show that simply replacing the unconditional noise in CFG with that predicted by the base model can significantly improve conditional generation. Furthermore, we show that a diffusion model other than the one the fine-tuned model was trained on can be used for unconditional noise replacement. We experimentally verify our claim with a range of CFG-based conditional models for both image and video generation, including Zero-1-to-3, Versatile Diffusion, DiT, DynamiCrafter, and InstructPix2Pix.', 'score': 19, 'issue_id': 2919, 'pub_date': '2025-03-26', 'pub_date_card': {'ru': '26 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 26', 'zh': '3æœˆ26æ—¥'}, 'hash': '4add99d8d7510263', 'authors': ['Prin Phunyaphibarn', 'Phillip Y. Lee', 'Jaihoon Kim', 'Minhyuk Sung'], 'affiliations': ['KAIST'], 'pdf_title_img': 'assets/pdf/title_img/2503.20240.jpg', 'data': {'categories': ['#cv', '#training', '#diffusion', '#video'], 'emoji': 'ğŸ”€', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ CFG: Ğ·Ğ°Ğ¼ĞµĞ½Ğ° Ğ±ĞµĞ·ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑˆÑƒĞ¼Ğ° Ğ´Ğ»Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Classifier-Free Guidance (CFG) Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ Ğ±ĞµĞ·ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ÑˆÑƒĞ¼Ğ° Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº ÑƒÑ…ÑƒĞ´ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞĞ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ·Ğ°Ğ¼ĞµĞ½ÑÑ‚ÑŒ Ğ±ĞµĞ·ÑƒÑĞ»Ğ¾Ğ²Ğ½Ñ‹Ğ¹ ÑˆÑƒĞ¼ Ğ² CFG Ğ½Ğ° Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑƒÑĞ»Ğ¾Ğ²Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'Enhancing Conditional Generation with Improved Unconditional Noise', 'desc': 'This paper discusses Classifier-Free Guidance (CFG), a technique used in training conditional diffusion models. The authors identify that using a single network for both conditional and unconditional noise prediction leads to poor performance in generating unconditional noise, which negatively impacts the quality of conditional outputs. They propose a solution where the unconditional noise predictions are replaced with those from a better-performing base model, resulting in improved conditional generation. The findings are validated through experiments on various CFG-based models for generating images and videos, demonstrating the effectiveness of their approach.'}, 'zh': {'title': 'æå‡æ¡ä»¶ç”Ÿæˆè´¨é‡çš„æ— æ¡ä»¶å™ªå£°æ›¿ä»£', 'desc': 'æ— åˆ†ç±»å™¨å¼•å¯¼ï¼ˆCFGï¼‰æ˜¯ä¸€ç§åœ¨è®­ç»ƒæ¡ä»¶æ‰©æ•£æ¨¡å‹ä¸­ä½¿ç”¨çš„åŸºæœ¬æŠ€æœ¯ã€‚ä¼ ç»Ÿçš„CFGè®­ç»ƒæ–¹æ³•æ˜¯ä½¿ç”¨å•ä¸€ç½‘ç»œåŒæ—¶å­¦ä¹ æ¡ä»¶å’Œæ— æ¡ä»¶å™ªå£°é¢„æµ‹ï¼Œä½†è¿™ç§è”åˆå­¦ä¹ ä¼šå¯¼è‡´æ— æ¡ä»¶å™ªå£°çš„å…ˆéªŒè´¨é‡è¾ƒå·®ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä½¿ç”¨æ›´å¥½çš„æ— æ¡ä»¶ç”Ÿæˆæ¨¡å‹çš„å™ªå£°æ›¿ä»£å¯ä»¥æ˜¾è‘—æé«˜æ¡ä»¶ç”Ÿæˆçš„è´¨é‡ã€‚æˆ‘ä»¬é€šè¿‡å®éªŒéªŒè¯äº†è¿™ä¸€ç‚¹ï¼Œé€‚ç”¨äºå¤šç§åŸºäºCFGçš„æ¡ä»¶æ¨¡å‹ï¼ŒåŒ…æ‹¬å›¾åƒå’Œè§†é¢‘ç”Ÿæˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.19480', 'title': 'GenHancer: Imperfect Generative Models are Secretly Strong\n  Vision-Centric Enhancers', 'url': 'https://huggingface.co/papers/2503.19480', 'abstract': "The synergy between generative and discriminative models receives growing attention. While discriminative Contrastive Language-Image Pre-Training (CLIP) excels in high-level semantics, it struggles with perceiving fine-grained visual details. Generally, to enhance representations, generative models take CLIP's visual features as conditions for reconstruction. However, the underlying principle remains underexplored. In this work, we empirically found that visually perfect generations are not always optimal for representation enhancement. The essence lies in effectively extracting fine-grained knowledge from generative models while mitigating irrelevant information. To explore critical factors, we delve into three aspects: (1) Conditioning mechanisms: We found that even a small number of local tokens can drastically reduce the difficulty of reconstruction, leading to collapsed training. We thus conclude that utilizing only global visual tokens as conditions is the most effective strategy. (2) Denoising configurations: We observed that end-to-end training introduces extraneous information. To address this, we propose a two-stage training strategy to prioritize learning useful visual knowledge. Additionally, we demonstrate that lightweight denoisers can yield remarkable improvements. (3) Generation paradigms: We explore both continuous and discrete denoisers with desirable outcomes, validating the versatility of our method. Through our in-depth explorations, we have finally arrived at an effective method, namely GenHancer, which consistently outperforms prior arts on the MMVP-VLM benchmark, e.g., 6.0% on OpenAICLIP. The enhanced CLIP can be further plugged into multimodal large language models for better vision-centric performance. All the models and codes are made publicly available.", 'score': 14, 'issue_id': 2920, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 25', 'zh': '3æœˆ25æ—¥'}, 'hash': '0dc97844010f5fb0', 'authors': ['Shijie Ma', 'Yuying Ge', 'Teng Wang', 'Yuxin Guo', 'Yixiao Ge', 'Ying Shan'], 'affiliations': ['ARC Lab, Tencent PCG', 'Institute of Automation, CAS'], 'pdf_title_img': 'assets/pdf/title_img/2503.19480.jpg', 'data': {'categories': ['#cv', '#open_source', '#multimodal', '#architecture', '#optimization', '#benchmark', '#training'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'GenHancer: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµĞ¿Ñ€ĞµĞ·ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¸Ğ½ĞµÑ€Ğ³Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¸ Ğ´Ğ¸ÑĞºÑ€Ğ¸Ğ¼Ğ¸Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ ÑĞ¸Ğ½ĞµÑ€Ğ³Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ´Ğ¸ÑĞºÑ€Ğ¸Ğ¼Ğ¸Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾ Ğ¸Ğ´ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğµ Ğ²ÑĞµĞ³Ğ´Ğ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€ĞµĞ¿Ñ€ĞµĞ·ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ GenHancer. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸Ğ· Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ¸ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. GenHancer Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ MMVP-VLM, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ° 6.0% Ğ´Ğ»Ñ OpenAICLIP.'}, 'en': {'title': 'Enhancing CLIP with GenHancer: Bridging Generative and Discriminative Models', 'desc': 'This paper investigates the relationship between generative and discriminative models in machine learning, specifically focusing on enhancing the performance of the Contrastive Language-Image Pre-Training (CLIP) model. The authors found that while generative models can improve representations, perfect visual generations do not always lead to optimal results. They propose a method called GenHancer, which effectively extracts fine-grained knowledge while reducing irrelevant information through careful conditioning and denoising strategies. Their approach outperforms existing methods on the MMVP-VLM benchmark, demonstrating its potential for improving vision-centric tasks in multimodal large language models.'}, 'zh': {'title': 'ç”Ÿæˆä¸åˆ¤åˆ«æ¨¡å‹çš„å®Œç¾ç»“åˆ', 'desc': 'æœ¬æ–‡æ¢è®¨äº†ç”Ÿæˆæ¨¡å‹ä¸åˆ¤åˆ«æ¨¡å‹ä¹‹é—´çš„ååŒä½œç”¨ï¼Œç‰¹åˆ«æ˜¯å¦‚ä½•åˆ©ç”¨ç”Ÿæˆæ¨¡å‹æ¥å¢å¼ºåˆ¤åˆ«æ¨¡å‹CLIPçš„è¡¨ç¤ºèƒ½åŠ›ã€‚ç ”ç©¶å‘ç°ï¼Œè§†è§‰ä¸Šå®Œç¾çš„ç”Ÿæˆå¹¶ä¸æ€»æ˜¯æœ€ä¼˜çš„è¡¨ç¤ºå¢å¼ºæ–¹å¼ï¼Œå…³é”®åœ¨äºæœ‰æ•ˆæå–ç»†ç²’åº¦çŸ¥è¯†å¹¶å‡å°‘æ— å…³ä¿¡æ¯ã€‚æˆ‘ä»¬æå‡ºäº†GenHanceræ–¹æ³•ï¼Œé€šè¿‡ä¼˜åŒ–æ¡ä»¶æœºåˆ¶ã€å»å™ªé…ç½®å’Œç”ŸæˆèŒƒå¼ï¼Œæ˜¾è‘—æå‡äº†CLIPåœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚æœ€ç»ˆï¼ŒGenHanceråœ¨MMVP-VLMåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†ä¹‹å‰çš„ç ”ç©¶æˆæœï¼Œå±•ç¤ºäº†å…¶åœ¨è§†è§‰ä¸­å¿ƒæ€§èƒ½ä¸Šçš„ä¼˜åŠ¿ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.20020', 'title': 'Gemini Robotics: Bringing AI into the Physical World', 'url': 'https://huggingface.co/papers/2503.20020', 'abstract': "Recent advancements in large multimodal models have led to the emergence of remarkable generalist capabilities in digital domains, yet their translation to physical agents such as robots remains a significant challenge. This report introduces a new family of AI models purposefully designed for robotics and built upon the foundation of Gemini 2.0. We present Gemini Robotics, an advanced Vision-Language-Action (VLA) generalist model capable of directly controlling robots. Gemini Robotics executes smooth and reactive movements to tackle a wide range of complex manipulation tasks while also being robust to variations in object types and positions, handling unseen environments as well as following diverse, open vocabulary instructions. We show that with additional fine-tuning, Gemini Robotics can be specialized to new capabilities including solving long-horizon, highly dexterous tasks, learning new short-horizon tasks from as few as 100 demonstrations and adapting to completely novel robot embodiments. This is made possible because Gemini Robotics builds on top of the Gemini Robotics-ER model, the second model we introduce in this work. Gemini Robotics-ER (Embodied Reasoning) extends Gemini's multimodal reasoning capabilities into the physical world, with enhanced spatial and temporal understanding. This enables capabilities relevant to robotics including object detection, pointing, trajectory and grasp prediction, as well as multi-view correspondence and 3D bounding box predictions. We show how this novel combination can support a variety of robotics applications. We also discuss and address important safety considerations related to this new class of robotics foundation models. The Gemini Robotics family marks a substantial step towards developing general-purpose robots that realizes AI's potential in the physical world.", 'score': 13, 'issue_id': 2919, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 25', 'zh': '3æœˆ25æ—¥'}, 'hash': '5edeeaed81b90426', 'authors': ['Gemini Robotics Team', 'Saminda Abeyruwan', 'Joshua Ainslie', 'Jean-Baptiste Alayrac', 'Montserrat Gonzalez Arenas', 'Travis Armstrong', 'Ashwin Balakrishna', 'Robert Baruch', 'Maria Bauza', 'Michiel Blokzijl', 'Steven Bohez', 'Konstantinos Bousmalis', 'Anthony Brohan', 'Thomas Buschmann', 'Arunkumar Byravan', 'Serkan Cabi', 'Ken Caluwaerts', 'Federico Casarini', 'Oscar Chang', 'Jose Enrique Chen', 'Xi Chen', 'Hao-Tien Lewis Chiang', 'Krzysztof Choromanski', "David D'Ambrosio", 'Sudeep Dasari', 'Todor Davchev', 'Coline Devin', 'Norman Di Palo', 'Tianli Ding', 'Adil Dostmohamed', 'Danny Driess', 'Yilun Du', 'Debidatta Dwibedi', 'Michael Elabd', 'Claudio Fantacci', 'Cody Fong', 'Erik Frey', 'Chuyuan Fu', 'Marissa Giustina', 'Keerthana Gopalakrishnan', 'Laura Graesser', 'Leonard Hasenclever', 'Nicolas Heess', 'Brandon Hernaez', 'Alexander Herzog', 'R. Alex Hofer', 'Jan Humplik', 'Atil Iscen', 'Mithun George Jacob', 'Deepali Jain', 'Ryan Julian', 'Dmitry Kalashnikov', 'M. Emre Karagozler', 'Stefani Karp', 'Chase Kew', 'Jerad Kirkland', 'Sean Kirmani', 'Yuheng Kuang', 'Thomas Lampe', 'Antoine Laurens', 'Isabel Leal', 'Alex X. Lee', 'Tsang-Wei Edward Lee', 'Jacky Liang', 'Yixin Lin', 'Sharath Maddineni', 'Anirudha Majumdar', 'Assaf Hurwitz Michaely', 'Robert Moreno', 'Michael Neunert', 'Francesco Nori', 'Carolina Parada', 'Emilio Parisotto', 'Peter Pastor', 'Acorn Pooley', 'Kanishka Rao', 'Krista Reymann', 'Dorsa Sadigh', 'Stefano Saliceti', 'Pannag Sanketi', 'Pierre Sermanet', 'Dhruv Shah', 'Mohit Sharma', 'Kathryn Shea', 'Charles Shu', 'Vikas Sindhwani', 'Sumeet Singh', 'Radu Soricut', 'Jost Tobias Springenberg', 'Rachel Sterneck', 'Razvan Surdulescu', 'Jie Tan', 'Jonathan Tompson', 'Vincent Vanhoucke', 'Jake Varley', 'Grace Vesom', 'Giulia Vezzani', 'Oriol Vinyals', 'Ayzaan Wahid', 'Stefan Welker', 'Paul Wohlhart', 'Fei Xia', 'Ted Xiao', 'Annie Xie', 'Jinyu Xie', 'Peng Xu', 'Sichun Xu', 'Ying Xu', 'Zhuo Xu', 'Yuxiang Yang', 'Rui Yao', 'Sergey Yaroshenko', 'Wenhao Yu', 'Wentao Yuan', 'Jingwei Zhang', 'Tingnan Zhang', 'Allan Zhou', 'Yuxiang Zhou'], 'affiliations': ['Gemini Robotics Team, Google DeepMind'], 'pdf_title_img': 'assets/pdf/title_img/2503.20020.jpg', 'data': {'categories': ['#multimodal', '#optimization', '#agents', '#agi', '#ethics', '#games', '#reasoning', '#robotics'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Gemini Robotics: Ğ˜Ğ˜ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¼Ğ¸Ñ€', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Gemini Robotics, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Gemini 2.0 Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ğ¼Ğ¸. Gemini Robotics - ÑÑ‚Ğ¾ ÑƒÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ‚Ğ¸Ğ¿Ğ° Vision-Language-Action, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ°Ñ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Gemini Robotics-ER Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Gemini Ğ½Ğ° Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¼Ğ¸Ñ€, ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, ĞºĞ°Ğº ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ñ‹ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğº Ğ½Ğ¾Ğ²Ñ‹Ğ¼ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ¸ÑĞ¼ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Empowering Robots with Gemini Robotics: A Leap in Multimodal AI', 'desc': "This paper presents Gemini Robotics, a new family of AI models designed specifically for robotic applications, building on the Gemini 2.0 framework. The model integrates Vision-Language-Action (VLA) capabilities, allowing robots to perform complex manipulation tasks with smooth and adaptive movements. It can learn from limited demonstrations and adapt to new tasks and robot designs, showcasing its versatility in various environments. Additionally, the paper introduces Gemini Robotics-ER, which enhances the model's reasoning abilities in physical contexts, addressing safety and practical applications in robotics."}, 'zh': {'title': 'Gemini Roboticsï¼šé€šç”¨æœºå™¨äººçš„æ–°çºªå…ƒ', 'desc': 'æœ€è¿‘å¤§å‹å¤šæ¨¡æ€æ¨¡å‹çš„è¿›å±•ä½¿å¾—æ•°å­—é¢†åŸŸçš„é€šç”¨èƒ½åŠ›æ˜¾è‘—æå‡ï¼Œä½†å°†å…¶åº”ç”¨äºæœºå™¨äººç­‰ç‰©ç†ä»£ç†ä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹çš„äººå·¥æ™ºèƒ½æ¨¡å‹ï¼Œä¸“ä¸ºæœºå™¨äººè®¾è®¡ï¼ŒåŸºäºGemini 2.0æ„å»ºã€‚Gemini Roboticsæ˜¯ä¸€ä¸ªå…ˆè¿›çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰é€šç”¨æ¨¡å‹ï¼Œèƒ½å¤Ÿç›´æ¥æ§åˆ¶æœºå™¨äººï¼Œæ‰§è¡Œå¤æ‚çš„æ“ä½œä»»åŠ¡ï¼Œå¹¶å¯¹ç‰©ä½“ç±»å‹å’Œä½ç½®çš„å˜åŒ–å…·æœ‰é²æ£’æ€§ã€‚é€šè¿‡é¢å¤–çš„å¾®è°ƒï¼ŒGemini Roboticså¯ä»¥ä¸“é—¨åŒ–ä¸ºæ–°çš„èƒ½åŠ›ï¼ŒåŒ…æ‹¬è§£å†³é•¿æ—¶é—´è·¨åº¦çš„é«˜çµå·§ä»»åŠ¡ï¼Œä»¥åŠä»å°‘é‡ç¤ºä¾‹ä¸­å­¦ä¹ æ–°ä»»åŠ¡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.19786', 'title': 'Gemma 3 Technical Report', 'url': 'https://huggingface.co/papers/2503.19786', 'abstract': 'We introduce Gemma 3, a multimodal addition to the Gemma family of lightweight open models, ranging in scale from 1 to 27 billion parameters. This version introduces vision understanding abilities, a wider coverage of languages and longer context - at least 128K tokens. We also change the architecture of the model to reduce the KV-cache memory that tends to explode with long context. This is achieved by increasing the ratio of local to global attention layers, and keeping the span on local attention short. The Gemma 3 models are trained with distillation and achieve superior performance to Gemma 2 for both pre-trained and instruction finetuned versions. In particular, our novel post-training recipe significantly improves the math, chat, instruction-following and multilingual abilities, making Gemma3-4B-IT competitive with Gemma2-27B-IT and Gemma3-27B-IT comparable to Gemini-1.5-Pro across benchmarks. We release all our models to the community.', 'score': 12, 'issue_id': 2934, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 25', 'zh': '3æœˆ25æ—¥'}, 'hash': 'df574ff057c95baa', 'authors': ['Gemma Team', 'Aishwarya Kamath', 'Johan Ferret', 'Shreya Pathak', 'Nino Vieillard', 'Ramona Merhej', 'Sarah Perrin', 'Tatiana Matejovicova', 'Alexandre RamÃ©', 'Morgane RiviÃ¨re', 'Louis Rouillard', 'Thomas Mesnard', 'Geoffrey Cideron', 'Jean-bastien Grill', 'Sabela Ramos', 'Edouard Yvinec', 'Michelle Casbon', 'Etienne Pot', 'Ivo Penchev', 'GaÃ«l Liu', 'Francesco Visin', 'Kathleen Kenealy', 'Lucas Beyer', 'Xiaohai Zhai', 'Anton Tsitsulin', 'Robert Busa-Fekete', 'Alex Feng', 'Noveen Sachdeva', 'Benjamin Coleman', 'Yi Gao', 'Basil Mustafa', 'Iain Barr', 'Emilio Parisotto', 'David Tian', 'Matan Eyal', 'Colin Cherry', 'Jan-Thorsten Peter', 'Danila Sinopalnikov', 'Surya Bhupatiraju', 'Rishabh Agarwal', 'Mehran Kazemi', 'Dan Malkin', 'Ravin Kumar', 'David Vilar', 'Idan Brusilovsky', 'Jiaming Luo', 'Andreas Steiner', 'Abe Friesen', 'Abhanshu Sharma', 'Abheesht Sharma', 'Adi Mayrav Gilady', 'Adrian Goedeckemeyer', 'Alaa Saade', 'Alex Feng', 'Alexander Kolesnikov', 'Alexei Bendebury', 'Alvin Abdagic', 'Amit Vadi', 'AndrÃ¡s GyÃ¶rgy', 'AndrÃ© Susano Pinto', 'Anil Das', 'Ankur Bapna', 'Antoine Miech', 'Antoine Yang', 'Antonia Paterson', 'Ashish Shenoy', 'Ayan Chakrabarti', 'Bilal Piot', 'Bo Wu', 'Bobak Shahriari', 'Bryce Petrini', 'Charlie Chen', 'Charline Le Lan', 'Christopher A. Choquette-Choo', 'CJ Carey', 'Cormac Brick', 'Daniel Deutsch', 'Danielle Eisenbud', 'Dee Cattle', 'Derek Cheng', 'Dimitris Paparas', 'Divyashree Shivakumar Sreepathihalli', 'Doug Reid', 'Dustin Tran', 'Dustin Zelle', 'Eric Noland', 'Erwin Huizenga', 'Eugene Kharitonov', 'Frederick Liu', 'Gagik Amirkhanyan', 'Glenn Cameron', 'Hadi Hashemi', 'Hanna Klimczak-PluciÅ„ska', 'Harman Singh', 'Harsh Mehta', 'Harshal Tushar Lehri', 'Hussein Hazimeh', 'Ian Ballantyne', 'Idan Szpektor', 'Ivan Nardini', 'Jean Pouget-Abadie', 'Jetha Chan', 'Joe Stanton', 'John Wieting', 'Jonathan Lai', 'Jordi Orbay', 'Joseph Fernandez', 'Josh Newlan', 'Ju-yeong Ji', 'Jyotinder Singh', 'Kat Black', 'Kathy Yu', 'Kevin Hui', 'Kiran Vodrahalli', 'Klaus Greff', 'Linhai Qiu', 'Marcella Valentine', 'Marina Coelho', 'Marvin Ritter', 'Matt Hoffman', 'Matthew Watson', 'Mayank Chaturvedi', 'Michael Moynihan', 'Min Ma', 'Nabila Babar', 'Natasha Noy', 'Nathan Byrd', 'Nick Roy', 'Nikola Momchev', 'Nilay Chauhan', 'Noveen Sachdeva', 'Oskar Bunyan', 'Pankil Botarda', 'Paul Caron', 'Paul Kishan Rubenstein', 'Phil Culliton', 'Philipp Schmid', 'Pier Giuseppe Sessa', 'Pingmei Xu', 'Piotr Stanczyk', 'Pouya Tafti', 'Rakesh Shivanna', 'Renjie Wu', 'Renke Pan', 'Reza Rokni', 'Rob Willoughby', 'Rohith Vallu', 'Ryan Mullins', 'Sammy Jerome', 'Sara Smoot', 'Sertan Girgin', 'Shariq Iqbal', 'Shashir Reddy', 'Shruti Sheth', 'Siim PÃµder', 'Sijal Bhatnagar', 'Sindhu Raghuram Panyam', 'Sivan Eiger', 'Susan Zhang', 'Tianqi Liu', 'Trevor Yacovone', 'Tyler Liechty', 'Uday Kalra', 'Utku Evci', 'Vedant Misra', 'Vincent Roseberry', 'Vlad Feinberg', 'Vlad Kolesnikov', 'Woohyun Han', 'Woosuk Kwon', 'Xi Chen', 'Yinlam Chow', 'Yuvein Zhu', 'Zichuan Wei', 'Zoltan Egyed', 'Victor Cotruta', 'Minh Giang', 'Phoebe Kirk', 'Anand Rao', 'Kat Black', 'Nabila Babar', 'Jessica Lo', 'Erica Moreira', 'Luiz Gustavo Martins', 'Omar Sanseviero', 'Lucas Gonzalez', 'Zach Gleicher', 'Tris Warkentin', 'Vahab Mirrokni', 'Evan Senter', 'Eli Collins', 'Joelle Barral', 'Zoubin Ghahramani', 'Raia Hadsell', 'Yossi Matias', 'D. Sculley', 'Slav Petrov', 'Noah Fiedel', 'Noam Shazeer', 'Oriol Vinyals', 'Jeff Dean', 'Demis Hassabis', 'Koray Kavukcuoglu', 'Clement Farabet', 'Elena Buchatskaya', 'Jean-Baptiste Alayrac', 'Rohan Anil', 'Dmitry', 'Lepikhin', 'Sebastian Borgeaud', 'Olivier Bachem', 'Armand Joulin', 'Alek Andreev', 'Cassidy Hardin', 'Robert Dadashi', 'LÃ©onard Hussenot'], 'affiliations': ['Google DeepMind'], 'pdf_title_img': 'assets/pdf/title_img/2503.19786.jpg', 'data': {'categories': ['#training', '#multimodal', '#architecture', '#open_source', '#multilingual', '#long_context'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Gemma 3: ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ˜Ğ˜ Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Gemma 3 - Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ²ĞµÑ€ÑĞ¸Ñ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ° Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ñ… Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Gemma Ñ Ğ´Ğ¸Ğ°Ğ¿Ğ°Ğ·Ğ¾Ğ½Ğ¾Ğ¼ Ğ¾Ñ‚ 1 Ğ´Ğ¾ 27 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ğ»Ğ° Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½ÑƒÑ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºÑƒ ÑĞ·Ñ‹ĞºĞ¾Ğ² Ğ¸ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ´Ğ¾ 128Ğš Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ±Ñ‹Ğ»Ğ° Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ° Ğ´Ğ»Ñ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ KV-ĞºÑÑˆĞ° Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¿ÑƒÑ‚ĞµĞ¼ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ñ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ»Ğ¾ĞµĞ² Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Gemma 3 Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ñ‹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Gemma 2 ĞºĞ°Ğº Ğ² Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ² Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²ĞµÑ€ÑĞ¸ÑÑ….'}, 'en': {'title': 'Gemma 3: Multimodal Mastery with Extended Context!', 'desc': 'Gemma 3 is a new version of the Gemma model family that enhances multimodal capabilities, allowing it to understand both text and images. It features a larger scale, with models ranging from 1 to 27 billion parameters, and supports longer context lengths of at least 128K tokens. The architecture has been optimized to manage memory usage better during long contexts by adjusting the balance of local and global attention layers. With improved training techniques, Gemma 3 outperforms its predecessor, Gemma 2, in various tasks including math, chat, and multilingual processing, making it a strong competitor in the field.'}, 'zh': {'title': 'Gemma 3ï¼šå¤šæ¨¡æ€è½»é‡çº§æ¨¡å‹çš„çªç ´', 'desc': 'Gemma 3 æ˜¯ Gemma ç³»åˆ—è½»é‡çº§å¼€æ”¾æ¨¡å‹çš„å¤šæ¨¡æ€ç‰ˆæœ¬ï¼Œå‚æ•°è§„æ¨¡ä» 1 åˆ° 270 äº¿ä¸ç­‰ã€‚è¯¥ç‰ˆæœ¬å¼•å…¥äº†è§†è§‰ç†è§£èƒ½åŠ›ï¼Œæ”¯æŒæ›´å¤šè¯­è¨€ï¼Œå¹¶ä¸”èƒ½å¤Ÿå¤„ç†æ›´é•¿çš„ä¸Šä¸‹æ–‡ï¼Œè‡³å°‘è¾¾åˆ° 128K ä¸ªæ ‡è®°ã€‚æˆ‘ä»¬é€šè¿‡è°ƒæ•´æ¨¡å‹æ¶æ„ï¼Œå¢åŠ å±€éƒ¨æ³¨æ„åŠ›å±‚ä¸å…¨å±€æ³¨æ„åŠ›å±‚çš„æ¯”ä¾‹ï¼Œæ¥å‡å°‘é•¿ä¸Šä¸‹æ–‡ä¸‹ KV-cache å†…å­˜çš„çˆ†ç‚¸ã€‚Gemma 3 æ¨¡å‹ç»è¿‡è’¸é¦è®­ç»ƒï¼Œè¡¨ç°ä¼˜äº Gemma 2ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°å­¦ã€å¯¹è¯ã€æŒ‡ä»¤è·Ÿéšå’Œå¤šè¯­è¨€èƒ½åŠ›æ–¹é¢æœ‰æ˜¾è‘—æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.20672', 'title': 'BizGen: Advancing Article-level Visual Text Rendering for Infographics\n  Generation', 'url': 'https://huggingface.co/papers/2503.20672', 'abstract': 'Recently, state-of-the-art text-to-image generation models, such as Flux and Ideogram 2.0, have made significant progress in sentence-level visual text rendering. In this paper, we focus on the more challenging scenarios of article-level visual text rendering and address a novel task of generating high-quality business content, including infographics and slides, based on user provided article-level descriptive prompts and ultra-dense layouts. The fundamental challenges are twofold: significantly longer context lengths and the scarcity of high-quality business content data.   In contrast to most previous works that focus on a limited number of sub-regions and sentence-level prompts, ensuring precise adherence to ultra-dense layouts with tens or even hundreds of sub-regions in business content is far more challenging. We make two key technical contributions: (i) the construction of scalable, high-quality business content dataset, i.e., Infographics-650K, equipped with ultra-dense layouts and prompts by implementing a layer-wise retrieval-augmented infographic generation scheme; and (ii) a layout-guided cross attention scheme, which injects tens of region-wise prompts into a set of cropped region latent space according to the ultra-dense layouts, and refine each sub-regions flexibly during inference using a layout conditional CFG.   We demonstrate the strong results of our system compared to previous SOTA systems such as Flux and SD3 on our BizEval prompt set. Additionally, we conduct thorough ablation experiments to verify the effectiveness of each component. We hope our constructed Infographics-650K and BizEval can encourage the broader community to advance the progress of business content generation.', 'score': 11, 'issue_id': 2920, 'pub_date': '2025-03-26', 'pub_date_card': {'ru': '26 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 26', 'zh': '3æœˆ26æ—¥'}, 'hash': 'b04cbfb976ce4e45', 'authors': ['Yuyang Peng', 'Shishi Xiao', 'Keming Wu', 'Qisheng Liao', 'Bohan Chen', 'Kevin Lin', 'Danqing Huang', 'Ji Li', 'Yuhui Yuan'], 'affiliations': ['Brown University', 'Microsoft', 'Microsoft Research Asia', 'Tsinghua University', 'University of Liverpool'], 'pdf_title_img': 'assets/pdf/title_img/2503.20672.jpg', 'data': {'categories': ['#long_context', '#cv', '#synthetic', '#dataset', '#rag'], 'emoji': 'ğŸ“Š', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ¸Ğ·Ğ½ĞµÑ-ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°: Ğ¾Ñ‚ Ñ‚ĞµĞºÑÑ‚Ğ° Ğº Ğ¸Ğ½Ñ„Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞµ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ¸Ğ·Ğ½ĞµÑ-ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¸Ğ½Ñ„Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºÑƒ Ğ¸ ÑĞ»Ğ°Ğ¹Ğ´Ñ‹, Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² ÑƒÑ€Ğ¾Ğ²Ğ½Ñ ÑÑ‚Ğ°Ñ‚ÑŒĞ¸ Ğ¸ ÑĞ²ĞµÑ€Ñ…Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ğ¼Ğ°ĞºĞµÑ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Infographics-650K Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ±Ğ¸Ğ·Ğ½ĞµÑ-ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ¾Ğ¼, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¿Ğ¾ÑĞ»Ğ¾Ğ¹Ğ½ÑƒÑ ÑÑ…ĞµĞ¼Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑÑ…ĞµĞ¼Ñƒ ĞºÑ€Ğ¾ÑÑ-Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¼Ğ°ĞºĞµÑ‚Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ²Ğ½ĞµĞ´Ñ€ÑĞµÑ‚ Ğ´ĞµÑÑÑ‚ĞºĞ¸ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½-ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº Ğ² Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ¾Ğ±Ñ€ĞµĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ¾Ğ² ÑĞ¾Ğ³Ğ»Ğ°ÑĞ½Ğ¾ ÑĞ²ĞµÑ€Ñ…Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ğ¼ Ğ¼Ğ°ĞºĞµÑ‚Ğ°Ğ¼. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ½Ğ°Ğ´ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ°Ğ¼Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğ¼Ğ¸ ĞºĞ°Ğº Flux Ğ¸ SD3, Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² BizEval.'}, 'en': {'title': 'Revolutionizing Business Content Generation with Ultra-Dense Layouts', 'desc': 'This paper presents advancements in text-to-image generation, specifically targeting the creation of business content like infographics and slides from article-level prompts. The authors introduce a new dataset, Infographics-650K, which includes ultra-dense layouts and is designed to address the challenges of longer context lengths and limited high-quality data. They propose a layout-guided cross attention mechanism that allows for precise generation across multiple sub-regions, enhancing the fidelity of the output. The results show significant improvements over existing models, encouraging further research in business content generation.'}, 'zh': {'title': 'æ¨åŠ¨å•†ä¸šå†…å®¹ç”Ÿæˆçš„æ–°çªç ´', 'desc': 'æœ¬æ–‡å…³æ³¨äºæ–‡ç« çº§è§†è§‰æ–‡æœ¬æ¸²æŸ“çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ç”Ÿæˆé«˜è´¨é‡å•†ä¸šå†…å®¹æ–¹é¢ï¼Œå¦‚ä¿¡æ¯å›¾å’Œå¹»ç¯ç‰‡ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„ä»»åŠ¡ï¼Œæ—¨åœ¨æ ¹æ®ç”¨æˆ·æä¾›çš„æè¿°æ€§æç¤ºå’Œè¶…å¯†é›†å¸ƒå±€ç”Ÿæˆè¿™äº›å†…å®¹ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªå¯æ‰©å±•çš„é«˜è´¨é‡å•†ä¸šå†…å®¹æ•°æ®é›†Infographics-650Kï¼Œå¹¶å®ç°äº†ä¸€ç§åŸºäºå¸ƒå±€çš„äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œä»¥å¤„ç†å¤æ‚çš„åŒºåŸŸæç¤ºã€‚æˆ‘ä»¬çš„ç³»ç»Ÿåœ¨ä¸ç°æœ‰æœ€å…ˆè¿›ç³»ç»Ÿçš„æ¯”è¾ƒä¸­è¡¨ç°å‡ºè‰²ï¼Œå¹¶é€šè¿‡æ¶ˆèå®éªŒéªŒè¯äº†å„ä¸ªç»„ä»¶çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.20757', 'title': 'MCTS-RAG: Enhancing Retrieval-Augmented Generation with Monte Carlo Tree\n  Search', 'url': 'https://huggingface.co/papers/2503.20757', 'abstract': 'We introduce MCTS-RAG, a novel approach that enhances the reasoning capabilities of small language models on knowledge-intensive tasks by leveraging retrieval-augmented generation (RAG) to provide relevant context and Monte Carlo Tree Search (MCTS) to refine reasoning paths. MCTS-RAG dynamically integrates retrieval and reasoning through an iterative decision-making process. Unlike standard RAG methods, which typically retrieve information independently from reasoning and thus integrate knowledge suboptimally, or conventional MCTS reasoning, which depends solely on internal model knowledge without external facts, MCTS-RAG combines structured reasoning with adaptive retrieval. This integrated approach enhances decision-making, reduces hallucinations, and ensures improved factual accuracy and response consistency. The experimental results on multiple reasoning and knowledge-intensive datasets datasets (i.e., ComplexWebQA, GPQA, and FoolMeTwice) show that our method enables small-scale LMs to achieve performance comparable to frontier LLMs like GPT-4o by effectively scaling inference-time compute, setting a new standard for reasoning in small-scale models.', 'score': 7, 'issue_id': 2920, 'pub_date': '2025-03-26', 'pub_date_card': {'ru': '26 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 26', 'zh': '3æœˆ26æ—¥'}, 'hash': 'd4c3b116518a2b0f', 'authors': ['Yunhai Hu', 'Yilun Zhao', 'Chen Zhao', 'Arman Cohan'], 'affiliations': ['New York University', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2503.20757.jpg', 'data': {'categories': ['#hallucinations', '#reasoning', '#rag', '#small_models'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'MCTS-RAG: Ğ£ÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ°Ğ»Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'MCTS-RAG - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¹ retrieval-augmented generation (RAG) Ğ¸ Monte Carlo Tree Search (MCTS) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ°Ğ»Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. MCTS-RAG ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ¼, Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹, ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ°Ğ»Ñ‹Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ¾Ğ¹ Ñ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ñ‚Ğ¸Ğ¿Ğ° GPT-4, ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒÑ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°.'}, 'en': {'title': 'Enhancing Small Models with Smart Retrieval and Reasoning', 'desc': 'MCTS-RAG is a new method that improves how small language models handle complex tasks that require knowledge. It combines retrieval-augmented generation (RAG) with Monte Carlo Tree Search (MCTS) to enhance reasoning by providing relevant information during the decision-making process. This approach allows the model to access external facts while reasoning, leading to better accuracy and consistency in responses. Experiments show that MCTS-RAG enables smaller models to perform as well as larger models like GPT-4o on challenging datasets.'}, 'zh': {'title': 'MCTS-RAGï¼šå°å‹æ¨¡å‹æ¨ç†çš„æ–°æ ‡å‡†', 'desc': 'æˆ‘ä»¬ä»‹ç»äº†ä¸€ç§æ–°æ–¹æ³•MCTS-RAGï¼Œå®ƒé€šè¿‡ç»“åˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å’Œè’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰ï¼Œæå‡å°å‹è¯­è¨€æ¨¡å‹åœ¨çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ä¸Šçš„æ¨ç†èƒ½åŠ›ã€‚MCTS-RAGé€šè¿‡è¿­ä»£å†³ç­–è¿‡ç¨‹åŠ¨æ€æ•´åˆæ£€ç´¢å’Œæ¨ç†ï¼Œå…‹æœäº†ä¼ ç»ŸRAGæ–¹æ³•å’ŒMCTSæ¨ç†çš„å±€é™æ€§ã€‚ä¸æ ‡å‡†RAGæ–¹æ³•ä¸åŒï¼ŒMCTS-RAGèƒ½å¤Ÿæ›´å¥½åœ°ç»“åˆç»“æ„åŒ–æ¨ç†å’Œè‡ªé€‚åº”æ£€ç´¢ï¼Œä»è€Œæé«˜å†³ç­–è´¨é‡ï¼Œå‡å°‘å¹»è§‰ç°è±¡ï¼Œå¹¶ç¡®ä¿æ›´é«˜çš„äº‹å®å‡†ç¡®æ€§å’Œå“åº”ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä½¿å°å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½å¯ä¸å‰æ²¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚GPT-4oï¼‰ç›¸åª²ç¾ï¼Œæ ‘ç«‹äº†å°å‹æ¨¡å‹æ¨ç†çš„æ–°æ ‡å‡†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.19950', 'title': 'LogQuant: Log-Distributed 2-Bit Quantization of KV Cache with Superior\n  Accuracy Preservation', 'url': 'https://huggingface.co/papers/2503.19950', 'abstract': "We introduce LogQuant, a groundbreaking 2-bit quantization technique for KV Cache in large language model (LLM) inference, delivering substantial memory savings while preserving superior performance. Previous methods either assume that later tokens are more important or attempt to predict important tokens based on earlier attention patterns. Both approaches, however, can result in performance bottlenecks or frequent mispredictions.   LogQuant takes a different approach. By applying a log-based filtering mechanism, it selectively compresses the KV Cache across the entire context, achieving better performance with the same or even reduced memory footprint compared to existing methods. In benchmark tests, it enhances throughput by 25% and boosts batch size by 60% without increasing memory consumption. For challenging tasks such as Math and Code Completion, LogQuant improves accuracy by 40% to 200% at the same compression ratio, outperforming comparable techniques.LogQuant integrates effortlessly with popular inference frameworks like Python's transformers library. Implementation can be available in https://github.com/Concyclics/LogQuantKV.", 'score': 7, 'issue_id': 2921, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 25', 'zh': '3æœˆ25æ—¥'}, 'hash': 'b75e0acc68cc5153', 'authors': ['Han Chen', 'Zicong Jiang', 'Zining Zhang', 'Bingsheng He', 'Pingyi Luo', 'Mian Lu', 'Yuqiang Chen'], 'affiliations': ['4Paradigm', 'School of Computing National University of Singapore', 'School of Electronic and Information Engineering South China University of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2503.19950.jpg', 'data': {'categories': ['#benchmark', '#inference', '#optimization'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'LogQuant - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ° 2-Ğ±Ğ¸Ñ‚Ğ½Ğ¾Ğ¹ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ KV-ĞºÑÑˆĞ° Ğ¿Ñ€Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ»Ğ¾Ğ³Ğ°Ñ€Ğ¸Ñ„Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ KV-ĞºÑÑˆĞ° Ğ¿Ğ¾ Ğ²ÑĞµĞ¼Ñƒ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ñƒ. Ğ’ Ñ‚ĞµÑÑ‚Ğ°Ñ… LogQuant Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ½ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° 25% Ğ¸ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ Ğ±Ğ°Ñ‚Ñ‡Ğ° Ğ½Ğ° 60% Ğ±ĞµĞ· Ñ€Ğ¾ÑÑ‚Ğ° Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. Ğ”Ğ»Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ° Ğ¸ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ´Ğ°, LogQuant ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° 40-200% Ğ¿Ñ€Ğ¸ Ñ‚Ğ¾Ğ¼ Ğ¶Ğµ ĞºĞ¾ÑÑ„Ñ„Ğ¸Ñ†Ğ¸ĞµĞ½Ñ‚Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ.'}, 'en': {'title': 'LogQuant: Efficient 2-Bit Quantization for Enhanced LLM Performance', 'desc': 'LogQuant is a novel 2-bit quantization method designed for efficiently managing the KV Cache in large language model inference. Unlike previous techniques that prioritize later tokens or rely on early attention patterns, LogQuant employs a log-based filtering mechanism to compress the KV Cache more effectively. This approach not only reduces memory usage but also enhances performance, achieving a 25% increase in throughput and a 60% increase in batch size without additional memory costs. In challenging tasks like Math and Code Completion, LogQuant significantly boosts accuracy by 40% to 200% while maintaining the same compression ratio, making it a superior choice for LLM applications.'}, 'zh': {'title': 'LogQuantï¼šé«˜æ•ˆçš„KVç¼“å­˜é‡åŒ–æŠ€æœ¯', 'desc': 'LogQuantæ˜¯ä¸€ç§åˆ›æ–°çš„2ä½é‡åŒ–æŠ€æœ¯ï¼Œä¸“ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†ä¸­çš„KVç¼“å­˜è®¾è®¡ã€‚å®ƒé€šè¿‡åº”ç”¨åŸºäºå¯¹æ•°çš„è¿‡æ»¤æœºåˆ¶ï¼Œé€‰æ‹©æ€§åœ°å‹ç¼©KVç¼“å­˜ï¼Œä»è€Œåœ¨ä¿æŒä¼˜è¶Šæ€§èƒ½çš„åŒæ—¶æ˜¾è‘—èŠ‚çœå†…å­˜ã€‚ä¸ä»¥å¾€æ–¹æ³•ä¸åŒï¼ŒLogQuanté¿å…äº†æ€§èƒ½ç“¶é¢ˆå’Œé¢‘ç¹çš„é”™è¯¯é¢„æµ‹ï¼Œæå‡äº†25%çš„ååé‡å’Œ60%çš„æ‰¹å¤„ç†å¤§å°ã€‚å¯¹äºæ•°å­¦å’Œä»£ç è¡¥å…¨ç­‰å¤æ‚ä»»åŠ¡ï¼ŒLogQuantåœ¨ç›¸åŒå‹ç¼©æ¯”ä¸‹æé«˜äº†40%åˆ°200%çš„å‡†ç¡®æ€§ï¼Œè¶…è¶Šäº†ç±»ä¼¼æŠ€æœ¯ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.20271', 'title': 'ViLBench: A Suite for Vision-Language Process Reward Modeling', 'url': 'https://huggingface.co/papers/2503.20271', 'abstract': "Process-supervised reward models serve as a fine-grained function that provides detailed step-wise feedback to model responses, facilitating effective selection of reasoning trajectories for complex tasks. Despite its advantages, evaluation on PRMs remains less explored, especially in the multimodal domain. To address this gap, this paper first benchmarks current vision large language models (VLLMs) as two types of reward models: output reward models (ORMs) and process reward models (PRMs) on multiple vision-language benchmarks, which reveal that neither ORM nor PRM consistently outperforms across all tasks, and superior VLLMs do not necessarily yield better rewarding performance. To further advance evaluation, we introduce ViLBench, a vision-language benchmark designed to require intensive process reward signals. Notably, OpenAI's GPT-4o with Chain-of-Thought (CoT) achieves only 27.3% accuracy, indicating the benchmark's challenge for current VLLMs. Lastly, we preliminarily showcase a promising pathway towards bridging the gap between general VLLMs and reward models -- by collecting 73.6K vision-language process reward data using an enhanced tree-search algorithm, our 3B model is able to achieve an average improvement of 3.3% over standard CoT and up to 2.5% compared to its untrained counterpart on ViLBench by selecting OpenAI o1's generations. We release the implementations at https://ucsc-vlaa.github.io/ViLBench with our code, model, and data.", 'score': 6, 'issue_id': 2925, 'pub_date': '2025-03-26', 'pub_date_card': {'ru': '26 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 26', 'zh': '3æœˆ26æ—¥'}, 'hash': '53462508b8990597', 'authors': ['Haoqin Tu', 'Weitao Feng', 'Hardy Chen', 'Hui Liu', 'Xianfeng Tang', 'Cihang Xie'], 'affiliations': ['Amazon Research', 'UC Santa Cruz', 'UT Dallas'], 'pdf_title_img': 'assets/pdf/title_img/2503.20271.jpg', 'data': {'categories': ['#open_source', '#reasoning', '#benchmark', '#optimization', '#multimodal', '#dataset'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞŸÑ€Ğ¾Ğ³Ñ€ĞµÑÑ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ¼ (PRM), Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´ÑÑ‚ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (VLLM) Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…. ĞĞ½Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ViLBench, ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸Ğ½Ñ‚ĞµĞ½ÑĞ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ VLLM Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸ÑÑ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ° Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¿Ğ¾ Ğ´ĞµÑ€ĞµĞ²Ñƒ.'}, 'en': {'title': 'Enhancing Multimodal Learning with Process-Supervised Rewards', 'desc': 'This paper explores the use of process-supervised reward models (PRMs) to provide detailed feedback for complex reasoning tasks in the multimodal domain. It benchmarks vision large language models (VLLMs) as output reward models (ORMs) and PRMs across various vision-language tasks, finding that neither consistently outperforms the other. The authors introduce ViLBench, a challenging benchmark that emphasizes the need for process reward signals, revealing that even advanced models like GPT-4o struggle with its complexity. Additionally, they demonstrate a method to enhance reward model performance by collecting a large dataset of vision-language process rewards, leading to measurable improvements in model accuracy.'}, 'zh': {'title': 'æå‡è§†è§‰-è¯­è¨€æ¨¡å‹çš„å¥–åŠ±è¯„ä¼°', 'desc': 'æœ¬æ–‡æ¢è®¨äº†è¿‡ç¨‹ç›‘ç£å¥–åŠ±æ¨¡å‹ï¼ˆPRMsï¼‰åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„åº”ç”¨ï¼Œæä¾›äº†è¯¦ç»†çš„é€æ­¥åé¦ˆä»¥å¸®åŠ©é€‰æ‹©æ¨ç†è·¯å¾„ã€‚å°½ç®¡PRMså…·æœ‰ä¼˜åŠ¿ï¼Œä½†åœ¨å¤šæ¨¡æ€é¢†åŸŸçš„è¯„ä¼°ä»ç„¶è¾ƒå°‘ã€‚æˆ‘ä»¬é¦–æ¬¡å¯¹å½“å‰çš„è§†è§‰å¤§è¯­è¨€æ¨¡å‹ï¼ˆVLLMsï¼‰è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œå‘ç°è¾“å‡ºå¥–åŠ±æ¨¡å‹ï¼ˆORMsï¼‰å’Œè¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMsï¼‰åœ¨ä¸åŒä»»åŠ¡ä¸­çš„è¡¨ç°å¹¶ä¸ä¸€è‡´ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†ViLBenchï¼Œä¸€ä¸ªéœ€è¦å¼ºçƒˆè¿‡ç¨‹å¥–åŠ±ä¿¡å·çš„è§†è§‰-è¯­è¨€åŸºå‡†ï¼Œå±•ç¤ºäº†å½“å‰VLLMsåœ¨æ­¤åŸºå‡†ä¸Šçš„æŒ‘æˆ˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.19462', 'title': 'AccVideo: Accelerating Video Diffusion Model with Synthetic Dataset', 'url': 'https://huggingface.co/papers/2503.19462', 'abstract': 'Diffusion models have achieved remarkable progress in the field of video generation. However, their iterative denoising nature requires a large number of inference steps to generate a video, which is slow and computationally expensive. In this paper, we begin with a detailed analysis of the challenges present in existing diffusion distillation methods and propose a novel efficient method, namely AccVideo, to reduce the inference steps for accelerating video diffusion models with synthetic dataset. We leverage the pretrained video diffusion model to generate multiple valid denoising trajectories as our synthetic dataset, which eliminates the use of useless data points during distillation. Based on the synthetic dataset, we design a trajectory-based few-step guidance that utilizes key data points from the denoising trajectories to learn the noise-to-video mapping, enabling video generation in fewer steps. Furthermore, since the synthetic dataset captures the data distribution at each diffusion timestep, we introduce an adversarial training strategy to align the output distribution of the student model with that of our synthetic dataset, thereby enhancing the video quality. Extensive experiments demonstrate that our model achieves 8.5x improvements in generation speed compared to the teacher model while maintaining comparable performance. Compared to previous accelerating methods, our approach is capable of generating videos with higher quality and resolution, i.e., 5-seconds, 720x1280, 24fps.', 'score': 5, 'issue_id': 2919, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 25', 'zh': '3æœˆ25æ—¥'}, 'hash': '721d2bb59c963434', 'authors': ['Haiyu Zhang', 'Xinyuan Chen', 'Yaohui Wang', 'Xihui Liu', 'Yunhong Wang', 'Yu Qiao'], 'affiliations': ['Beihang University', 'Shanghai AI Laboratory', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2503.19462.jpg', 'data': {'categories': ['#diffusion', '#optimization', '#inference', '#video', '#dataset', '#synthetic'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ AccVideo Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ÑĞ¼Ğ¸ Ğ´ĞµĞ½Ğ¾Ğ¹Ğ·Ğ¸Ğ½Ğ³Ğ°. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¿Ğ¾ Ğ¼Ğ°Ğ»Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑĞ¾ÑÑ‚ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ 8.5-ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğµ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ğ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°.'}, 'en': {'title': 'Accelerating Video Generation with AccVideo', 'desc': 'This paper addresses the slow and resource-intensive process of video generation using diffusion models, which require many steps for denoising. The authors introduce AccVideo, a new method that reduces the number of inference steps needed by utilizing a synthetic dataset generated from a pretrained video diffusion model. By focusing on key data points from denoising trajectories, the model learns to map noise to video more efficiently. Additionally, an adversarial training strategy is employed to improve the quality of the generated videos, resulting in significant speed improvements while maintaining high resolution and quality.'}, 'zh': {'title': 'åŠ é€Ÿè§†é¢‘ç”Ÿæˆï¼Œæå‡è´¨é‡ä¸æ•ˆç‡', 'desc': 'æ‰©æ•£æ¨¡å‹åœ¨è§†é¢‘ç”Ÿæˆé¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å…¶è¿­ä»£å»å™ªçš„ç‰¹æ€§å¯¼è‡´ç”Ÿæˆè§†é¢‘éœ€è¦å¤§é‡æ¨ç†æ­¥éª¤ï¼Œé€Ÿåº¦æ…¢ä¸”è®¡ç®—æˆæœ¬é«˜ã€‚æœ¬æ–‡åˆ†æäº†ç°æœ‰æ‰©æ•£è’¸é¦æ–¹æ³•ä¸­çš„æŒ‘æˆ˜ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°é¢–çš„é«˜æ•ˆæ–¹æ³•AccVideoï¼Œä»¥å‡å°‘æ¨ç†æ­¥éª¤ï¼ŒåŠ é€Ÿè§†é¢‘æ‰©æ•£æ¨¡å‹ã€‚æˆ‘ä»¬åˆ©ç”¨é¢„è®­ç»ƒçš„è§†é¢‘æ‰©æ•£æ¨¡å‹ç”Ÿæˆå¤šä¸ªæœ‰æ•ˆçš„å»å™ªè½¨è¿¹ä½œä¸ºåˆæˆæ•°æ®é›†ï¼Œä»è€Œåœ¨è’¸é¦è¿‡ç¨‹ä¸­æ¶ˆé™¤æ— ç”¨æ•°æ®ç‚¹ã€‚é€šè¿‡è®¾è®¡åŸºäºè½¨è¿¹çš„å°‘æ­¥å¼•å¯¼ï¼Œæˆ‘ä»¬èƒ½å¤Ÿåœ¨æ›´å°‘çš„æ­¥éª¤ä¸­å®ç°è§†é¢‘ç”Ÿæˆï¼ŒåŒæ—¶å¼•å…¥å¯¹æŠ—è®­ç»ƒç­–ç•¥ä»¥æé«˜è§†é¢‘è´¨é‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.19846', 'title': 'Attention IoU: Examining Biases in CelebA using Attention Maps', 'url': 'https://huggingface.co/papers/2503.19846', 'abstract': "Computer vision models have been shown to exhibit and amplify biases across a wide array of datasets and tasks. Existing methods for quantifying bias in classification models primarily focus on dataset distribution and model performance on subgroups, overlooking the internal workings of a model. We introduce the Attention-IoU (Attention Intersection over Union) metric and related scores, which use attention maps to reveal biases within a model's internal representations and identify image features potentially causing the biases. First, we validate Attention-IoU on the synthetic Waterbirds dataset, showing that the metric accurately measures model bias. We then analyze the CelebA dataset, finding that Attention-IoU uncovers correlations beyond accuracy disparities. Through an investigation of individual attributes through the protected attribute of Male, we examine the distinct ways biases are represented in CelebA. Lastly, by subsampling the training set to change attribute correlations, we demonstrate that Attention-IoU reveals potential confounding variables not present in dataset labels.", 'score': 4, 'issue_id': 2921, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 25', 'zh': '3æœˆ25æ—¥'}, 'hash': 'a4039050131ab9d6', 'authors': ['Aaron Serianni', 'Tyler Zhu', 'Olga Russakovsky', 'Vikram V. Ramaswamy'], 'affiliations': ['Princeton University'], 'pdf_title_img': 'assets/pdf/title_img/2503.19846.jpg', 'data': {'categories': ['#ethics', '#benchmark', '#cv', '#dataset', '#interpretability'], 'emoji': 'ğŸ‘ï¸', 'ru': {'title': 'ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚ÑŒ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¸Ğ·Ğ¼Ñƒ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ - Attention-IoU. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², Attention-IoU Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ĞºĞ°Ñ€Ñ‚Ñ‹ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ¸Ñ€ÑƒÑÑ‚ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Waterbirds Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ ĞµĞµ Ğº Ğ½Ğ°Ğ±Ğ¾Ñ€Ñƒ CelebA, Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°Ñ ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ğ¸, Ğ²Ñ‹Ñ…Ğ¾Ğ´ÑÑ‰Ğ¸Ğµ Ğ·Ğ° Ñ€Ğ°Ğ¼ĞºĞ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ğ¹ Ğ² Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Attention-IoU Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ²Ñ‹ÑĞ²Ğ»ÑÑ‚ÑŒ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ÑĞºĞ°Ğ¶Ğ°ÑÑ‰Ğ¸Ğµ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ, Ğ½Ğµ Ğ¿Ñ€Ğ¸ÑÑƒÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ² Ğ¼ĞµÑ‚ĞºĞ°Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Unveiling Biases with Attention-IoU in Computer Vision Models', 'desc': "This paper addresses the issue of bias in computer vision models, which can be amplified by the datasets they are trained on. The authors introduce a new metric called Attention-IoU, which utilizes attention maps to uncover biases in a model's internal representations rather than just focusing on dataset distribution or performance metrics. They validate this metric using the Waterbirds dataset and further analyze the CelebA dataset to reveal hidden correlations that go beyond mere accuracy differences. By manipulating the training set, they demonstrate that Attention-IoU can identify confounding variables that are not explicitly labeled in the dataset, providing deeper insights into model biases."}, 'zh': {'title': 'æ­ç¤ºæ¨¡å‹å†…éƒ¨åè§çš„æ³¨æ„åŠ›äº¤å¹¶æ¯”', 'desc': 'æœ¬æ–‡æ¢è®¨äº†è®¡ç®—æœºè§†è§‰æ¨¡å‹åœ¨ä¸åŒæ•°æ®é›†å’Œä»»åŠ¡ä¸­è¡¨ç°å‡ºçš„åè§ã€‚ç°æœ‰çš„é‡åŒ–åˆ†ç±»æ¨¡å‹åè§çš„æ–¹æ³•ä¸»è¦å…³æ³¨æ•°æ®é›†åˆ†å¸ƒå’Œæ¨¡å‹åœ¨å­ç¾¤ä½“ä¸Šçš„è¡¨ç°ï¼Œè€Œå¿½è§†äº†æ¨¡å‹å†…éƒ¨çš„å·¥ä½œæœºåˆ¶ã€‚æˆ‘ä»¬æå‡ºäº†æ³¨æ„åŠ›äº¤å¹¶æ¯”ï¼ˆAttention-IoUï¼‰æŒ‡æ ‡ï¼Œé€šè¿‡æ³¨æ„åŠ›å›¾æ­ç¤ºæ¨¡å‹å†…éƒ¨è¡¨ç¤ºä¸­çš„åè§ï¼Œå¹¶è¯†åˆ«å¯èƒ½å¯¼è‡´åè§çš„å›¾åƒç‰¹å¾ã€‚é€šè¿‡å¯¹Waterbirdså’ŒCelebAæ•°æ®é›†çš„åˆ†æï¼Œæˆ‘ä»¬éªŒè¯äº†Attention-IoUçš„æœ‰æ•ˆæ€§ï¼Œå¹¶å‘ç°å…¶èƒ½å¤Ÿæ­ç¤ºè¶…å‡ºå‡†ç¡®æ€§å·®å¼‚çš„ç›¸å…³æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.20756', 'title': 'ADS-Edit: A Multimodal Knowledge Editing Dataset for Autonomous Driving\n  Systems', 'url': 'https://huggingface.co/papers/2503.20756', 'abstract': "Recent advancements in Large Multimodal Models (LMMs) have shown promise in Autonomous Driving Systems (ADS). However, their direct application to ADS is hindered by challenges such as misunderstanding of traffic knowledge, complex road conditions, and diverse states of vehicle. To address these challenges, we propose the use of Knowledge Editing, which enables targeted modifications to a model's behavior without the need for full retraining. Meanwhile, we introduce ADS-Edit, a multimodal knowledge editing dataset specifically designed for ADS, which includes various real-world scenarios, multiple data types, and comprehensive evaluation metrics. We conduct comprehensive experiments and derive several interesting conclusions. We hope that our work will contribute to the further advancement of knowledge editing applications in the field of autonomous driving. Code and data are available in https://github.com/zjunlp/EasyEdit.", 'score': 3, 'issue_id': 2919, 'pub_date': '2025-03-26', 'pub_date_card': {'ru': '26 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 26', 'zh': '3æœˆ26æ—¥'}, 'hash': '6b8affbfbdd5a426', 'authors': ['Chenxi Wang', 'Jizhan Fang', 'Xiang Chen', 'Bozhong Tian', 'Ziwen Xu', 'Huajun Chen', 'Ningyu Zhang'], 'affiliations': ['Nanjing University of Aeronautics and Astronautics', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.20756.jpg', 'data': {'categories': ['#multimodal', '#agents', '#dataset'], 'emoji': 'ğŸš—', 'ru': {'title': 'Ğ ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LMM) Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ (ADS). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ADS-Edit Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞŸÑ€Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ½Ñ‹Ğµ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑĞ¿Ğ¾Ñ€Ñ‚Ğ½Ñ‹Ñ… ÑÑ€ĞµĞ´ÑÑ‚Ğ².'}, 'en': {'title': 'Enhancing Autonomous Driving with Targeted Knowledge Editing', 'desc': "This paper discusses the use of Large Multimodal Models (LMMs) in Autonomous Driving Systems (ADS) and the challenges they face, such as traffic knowledge misunderstanding and complex road conditions. To overcome these issues, the authors propose a method called Knowledge Editing, which allows for specific adjustments to a model's behavior without needing to retrain it entirely. They also introduce ADS-Edit, a specialized dataset for multimodal knowledge editing in ADS, which includes diverse real-world scenarios and various data types. The paper presents experimental results that highlight the potential of knowledge editing to enhance the performance of autonomous driving systems."}, 'zh': {'title': 'çŸ¥è¯†ç¼–è¾‘åŠ©åŠ›è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„è¿›æ­¥', 'desc': 'æœ¬æ–‡æ¢è®¨äº†å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿï¼ˆADSï¼‰ä¸­çš„åº”ç”¨æ½œåŠ›ã€‚å°½ç®¡LMMsæœ‰å‰æ™¯ï¼Œä½†åœ¨äº¤é€šçŸ¥è¯†ç†è§£ã€å¤æ‚è·¯å†µå’Œè½¦è¾†å¤šæ ·æ€§ç­‰æ–¹é¢é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†çŸ¥è¯†ç¼–è¾‘çš„æ–¹æ³•ï¼Œå¯ä»¥åœ¨ä¸å®Œå…¨é‡è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œé’ˆå¯¹æ€§åœ°ä¿®æ”¹æ¨¡å‹çš„è¡Œä¸ºã€‚åŒæ—¶ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ADS-Editï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºè‡ªåŠ¨é©¾é©¶è®¾è®¡çš„å¤šæ¨¡æ€çŸ¥è¯†ç¼–è¾‘æ•°æ®é›†ï¼ŒåŒ…å«å¤šç§çœŸå®åœºæ™¯å’Œæ•°æ®ç±»å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.20220', 'title': 'DINeMo: Learning Neural Mesh Models with no 3D Annotations', 'url': 'https://huggingface.co/papers/2503.20220', 'abstract': 'Category-level 3D/6D pose estimation is a crucial step towards comprehensive 3D scene understanding, which would enable a broad range of applications in robotics and embodied AI. Recent works explored neural mesh models that approach a range of 2D and 3D tasks from an analysis-by-synthesis perspective. Despite the largely enhanced robustness to partial occlusion and domain shifts, these methods depended heavily on 3D annotations for part-contrastive learning, which confines them to a narrow set of categories and hinders efficient scaling. In this work, we present DINeMo, a novel neural mesh model that is trained with no 3D annotations by leveraging pseudo-correspondence obtained from large visual foundation models. We adopt a bidirectional pseudo-correspondence generation method, which produce pseudo correspondence utilize both local appearance features and global context information. Experimental results on car datasets demonstrate that our DINeMo outperforms previous zero- and few-shot 3D pose estimation by a wide margin, narrowing the gap with fully-supervised methods by 67.3%. Our DINeMo also scales effectively and efficiently when incorporating more unlabeled images during training, which demonstrate the advantages over supervised learning methods that rely on 3D annotations. Our project page is available at https://analysis-by-synthesis.github.io/DINeMo/.', 'score': 3, 'issue_id': 2919, 'pub_date': '2025-03-26', 'pub_date_card': {'ru': '26 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 26', 'zh': '3æœˆ26æ—¥'}, 'hash': 'f65c515bbb6af49b', 'authors': ['Weijie Guo', 'Guofeng Zhang', 'Wufei Ma', 'Alan Yuille'], 'affiliations': ['Johns Hopkins University'], 'pdf_title_img': 'assets/pdf/title_img/2503.20220.jpg', 'data': {'categories': ['#transfer_learning', '#3d', '#robotics', '#optimization'], 'emoji': 'ğŸš—', 'ru': {'title': 'Ğ¢Ğ¾Ñ‡Ğ½Ğ°Ñ 3D-Ğ¿Ğ¾Ğ·Ğ° Ğ±ĞµĞ· 3D-Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸', 'desc': 'DINeMo - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ°Ñ ÑĞµÑ‚ĞµĞ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ 3D/6D Ğ¿Ğ¾Ğ·Ñ‹ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ 3D-Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿ÑĞµĞ²Ğ´Ğ¾-ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ, Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¸Ğ· ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ Ğ´Ğ²ÑƒĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. DINeMo Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ 3D-Ğ¿Ğ¾Ğ·Ñ‹ Ñ Ğ½ÑƒĞ»ĞµĞ²Ñ‹Ğ¼ Ğ¸ Ğ¼Ğ°Ğ»Ñ‹Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ², ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°Ñ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ñ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ½Ğ° 67.3%. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¸ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ Ğ½ĞµÑ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Revolutionizing 3D Pose Estimation with Unlabeled Data', 'desc': 'This paper introduces DINeMo, a new neural mesh model designed for category-level 3D/6D pose estimation without relying on 3D annotations. It utilizes pseudo-correspondence generated from large visual foundation models, allowing it to learn from unlabeled data effectively. The model employs a bidirectional approach to generate pseudo correspondences, combining local appearance features with global context. Experimental results show that DINeMo significantly improves performance in zero- and few-shot 3D pose estimation, achieving results close to fully-supervised methods while being more scalable and efficient.'}, 'zh': {'title': 'æ— æ ‡æ³¨3Då§¿æ€ä¼°è®¡çš„æ–°çªç ´', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„ç¥ç»ç½‘æ ¼æ¨¡å‹DINeMoï¼Œç”¨äºç±»åˆ«çº§çš„3D/6Då§¿æ€ä¼°è®¡ï¼Œæ—¨åœ¨æé«˜3Dåœºæ™¯ç†è§£çš„èƒ½åŠ›ã€‚ä¸ä»¥å¾€ä¾èµ–3Dæ ‡æ³¨çš„å­¦ä¹ æ–¹æ³•ä¸åŒï¼ŒDINeMoé€šè¿‡åˆ©ç”¨å¤§å‹è§†è§‰åŸºç¡€æ¨¡å‹è·å¾—çš„ä¼ªå¯¹åº”å…³ç³»è¿›è¡Œè®­ç»ƒï¼Œä»è€Œé¿å…äº†å¯¹3Dæ ‡æ³¨çš„ä¾èµ–ã€‚æˆ‘ä»¬é‡‡ç”¨åŒå‘ä¼ªå¯¹åº”ç”Ÿæˆæ–¹æ³•ï¼Œç»“åˆå±€éƒ¨å¤–è§‚ç‰¹å¾å’Œå…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„é²æ£’æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDINeMoåœ¨è½¦ç±»æ•°æ®é›†ä¸Šçš„è¡¨ç°è¶…è¶Šäº†ä¹‹å‰çš„é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬3Då§¿æ€ä¼°è®¡æ–¹æ³•ï¼Œç¼©å°äº†ä¸å®Œå…¨ç›‘ç£æ–¹æ³•çš„å·®è·ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.20198', 'title': 'Beyond Words: Advancing Long-Text Image Generation via Multimodal\n  Autoregressive Models', 'url': 'https://huggingface.co/papers/2503.20198', 'abstract': 'Recent advancements in autoregressive and diffusion models have led to strong performance in image generation with short scene text words. However, generating coherent, long-form text in images, such as paragraphs in slides or documents, remains a major challenge for current generative models. We present the first work specifically focused on long text image generation, addressing a critical gap in existing text-to-image systems that typically handle only brief phrases or single sentences. Through comprehensive analysis of state-of-the-art autoregressive generation models, we identify the image tokenizer as a critical bottleneck in text generating quality. To address this, we introduce a novel text-focused, binary tokenizer optimized for capturing detailed scene text features. Leveraging our tokenizer, we develop \\ModelName, a multimodal autoregressive model that excels in generating high-quality long-text images with unprecedented fidelity. Our model offers robust controllability, enabling customization of text properties such as font style, size, color, and alignment. Extensive experiments demonstrate that \\ModelName~significantly outperforms SD3.5 Large~sd3 and GPT4o~gpt4o with DALL-E 3~dalle3 in generating long text accurately, consistently, and flexibly. Beyond its technical achievements, \\ModelName~opens up exciting opportunities for innovative applications like interleaved document and PowerPoint generation, establishing a new frontier in long-text image generating.', 'score': 3, 'issue_id': 2920, 'pub_date': '2025-03-26', 'pub_date_card': {'ru': '26 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 26', 'zh': '3æœˆ26æ—¥'}, 'hash': 'b2b966b253011624', 'authors': ['Alex Jinpeng Wang', 'Linjie Li', 'Zhengyuan Yang', 'Lijuan Wang', 'Min Li'], 'affiliations': ['Central South University', 'Microsoft'], 'pdf_title_img': 'assets/pdf/title_img/2503.20198.jpg', 'data': {'categories': ['#long_context', '#cv', '#multimodal'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'ĞĞ¾Ğ²Ğ°Ñ ÑÑ€Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°Ğ¼Ğ¸', 'desc': 'Ğ¡Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ autoregressive Ğ¸ diffusion Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ğ¼Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°Ğ¼Ğ¸, Ğ½Ğ¾ Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°Ğ¼Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğ¼Ğ¸ ĞºĞ°Ğº Ğ°Ğ±Ğ·Ğ°Ñ†Ñ‹ Ğ² Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ…. Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¿ĞµÑ€Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°Ğ¼Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ·Ğ°ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ» Ğ² ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… text-to-image. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ¾Ğ¹ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ image tokenizer, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ±Ñ‹Ğ» Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±Ğ¸Ğ½Ğ°Ñ€Ğ½Ñ‹Ğ¹ tokenizer, Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ´Ğ»Ñ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ĞµĞ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‰ÑƒÑ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ² Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ³Ğ¸Ğ±ĞºĞ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ².'}, 'en': {'title': 'Revolutionizing Long-Text Image Generation with \\ModelName', 'desc': 'This paper introduces a new approach to generating long-form text in images, which has been a challenge for existing generative models. The authors identify that the image tokenizer is a key limitation in the quality of text generation. To overcome this, they propose a novel binary tokenizer that focuses on capturing detailed features of scene text. Their multimodal autoregressive model, named \\ModelName, demonstrates superior performance in generating high-quality long-text images, allowing for customizable text properties and paving the way for new applications in document and presentation generation.'}, 'zh': {'title': 'é•¿æ–‡æœ¬å›¾åƒç”Ÿæˆçš„æ–°çªç ´', 'desc': 'æœ€è¿‘ï¼Œè‡ªå›å½’å’Œæ‰©æ•£æ¨¡å‹çš„è¿›å±•ä½¿å¾—çŸ­æ–‡æœ¬å›¾åƒç”Ÿæˆè¡¨ç°å‡ºè‰²ã€‚ç„¶è€Œï¼Œç”Ÿæˆè¿è´¯çš„é•¿æ–‡æœ¬å›¾åƒï¼ˆå¦‚å¹»ç¯ç‰‡æˆ–æ–‡æ¡£ä¸­çš„æ®µè½ï¼‰ä»ç„¶æ˜¯å½“å‰ç”Ÿæˆæ¨¡å‹é¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜ã€‚æˆ‘ä»¬é¦–æ¬¡ä¸“æ³¨äºé•¿æ–‡æœ¬å›¾åƒç”Ÿæˆï¼Œå¡«è¡¥äº†ç°æœ‰æ–‡æœ¬åˆ°å›¾åƒç³»ç»Ÿçš„å…³é”®ç©ºç™½ã€‚é€šè¿‡åˆ†ææœ€å…ˆè¿›çš„è‡ªå›å½’ç”Ÿæˆæ¨¡å‹ï¼Œæˆ‘ä»¬å‘ç°å›¾åƒåˆ†è¯å™¨æ˜¯å½±å“æ–‡æœ¬ç”Ÿæˆè´¨é‡çš„å…³é”®ç“¶é¢ˆï¼Œå› æ­¤æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–‡æœ¬ä¸“æ³¨çš„äºŒè¿›åˆ¶åˆ†è¯å™¨ï¼Œä»¥ä¼˜åŒ–ç»†èŠ‚åœºæ™¯æ–‡æœ¬ç‰¹å¾çš„æ•æ‰ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.17358', 'title': 'Image as an IMU: Estimating Camera Motion from a Single Motion-Blurred\n  Image', 'url': 'https://huggingface.co/papers/2503.17358', 'abstract': 'In many robotics and VR/AR applications, fast camera motions cause a high level of motion blur, causing existing camera pose estimation methods to fail. In this work, we propose a novel framework that leverages motion blur as a rich cue for motion estimation rather than treating it as an unwanted artifact. Our approach works by predicting a dense motion flow field and a monocular depth map directly from a single motion-blurred image. We then recover the instantaneous camera velocity by solving a linear least squares problem under the small motion assumption. In essence, our method produces an IMU-like measurement that robustly captures fast and aggressive camera movements. To train our model, we construct a large-scale dataset with realistic synthetic motion blur derived from ScanNet++v2 and further refine our model by training end-to-end on real data using our fully differentiable pipeline. Extensive evaluations on real-world benchmarks demonstrate that our method achieves state-of-the-art angular and translational velocity estimates, outperforming current methods like MASt3R and COLMAP.', 'score': 3, 'issue_id': 2927, 'pub_date': '2025-03-21', 'pub_date_card': {'ru': '21 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 21', 'zh': '3æœˆ21æ—¥'}, 'hash': '491a350f070233ec', 'authors': ['Jerred Chen', 'Ronald Clark'], 'affiliations': ['University of Oxford Department of Computer Science'], 'pdf_title_img': 'assets/pdf/title_img/2503.17358.jpg', 'data': {'categories': ['#dataset', '#robotics', '#training', '#cv', '#benchmark'], 'emoji': 'ğŸ“·', 'ru': {'title': 'Ğ Ğ°Ğ·Ğ¼Ñ‹Ñ‚Ğ¸Ğµ Ğ² Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ğº ĞºĞ»ÑÑ‡ Ğº Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¼Ñƒ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ°Ğ¼ĞµÑ€Ñ‹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… ÑĞ¸Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼Ñ‹Ñ‚Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ¼ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ğ¾Ğ»Ğµ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ° Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ ĞºĞ°Ñ€Ñ‚Ñƒ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ¿Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°Ğ·Ğ¼Ñ‹Ñ‚Ğ¾Ğ¼Ñƒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. Ğ—Ğ°Ñ‚ĞµĞ¼ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ¼Ğ³Ğ½Ğ¾Ğ²ĞµĞ½Ğ½Ğ°Ñ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ¿ÑƒÑ‚ĞµĞ¼ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ½Ğ°Ğ¸Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ñ… ĞºĞ²Ğ°Ğ´Ñ€Ğ°Ñ‚Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ğ°.'}, 'en': {'title': 'Harnessing Motion Blur for Enhanced Camera Motion Estimation', 'desc': "This paper introduces a new method for estimating camera motion in situations where fast movements cause motion blur, which typically hinders existing techniques. Instead of viewing motion blur as a problem, the authors utilize it to predict a dense motion flow field and a depth map from a single blurred image. They calculate the camera's instantaneous velocity by solving a linear least squares problem, effectively treating the motion blur as valuable information. The proposed framework is trained on a large dataset and shows superior performance in estimating camera velocities compared to traditional methods."}, 'zh': {'title': 'åˆ©ç”¨è¿åŠ¨æ¨¡ç³Šæå‡ç›¸æœºè¿åŠ¨ä¼°è®¡çš„åˆ›æ–°æ–¹æ³•', 'desc': 'åœ¨è®¸å¤šæœºå™¨äººå’Œè™šæ‹Ÿç°å®/å¢å¼ºç°å®åº”ç”¨ä¸­ï¼Œå¿«é€Ÿçš„ç›¸æœºè¿åŠ¨ä¼šå¯¼è‡´ä¸¥é‡çš„è¿åŠ¨æ¨¡ç³Šï¼Œç°æœ‰çš„ç›¸æœºå§¿æ€ä¼°è®¡æ–¹æ³•å› æ­¤å¤±æ•ˆã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œå°†è¿åŠ¨æ¨¡ç³Šè§†ä¸ºè¿åŠ¨ä¼°è®¡çš„ä¸°å¯Œçº¿ç´¢ï¼Œè€Œä¸æ˜¯ä¸å¿…è¦çš„ä¼ªå½±ã€‚è¯¥æ–¹æ³•é€šè¿‡ä»å•ä¸€çš„è¿åŠ¨æ¨¡ç³Šå›¾åƒä¸­ç›´æ¥é¢„æµ‹å¯†é›†çš„è¿åŠ¨æµåœºå’Œå•ç›®æ·±åº¦å›¾æ¥å·¥ä½œã€‚æˆ‘ä»¬çš„æ¨¡å‹ç»è¿‡å¤§è§„æ¨¡åˆæˆè¿åŠ¨æ¨¡ç³Šæ•°æ®é›†è®­ç»ƒï¼Œå¹¶åœ¨çœŸå®æ•°æ®ä¸Šè¿›è¡Œç«¯åˆ°ç«¯çš„å¾®è°ƒï¼Œæœ€ç»ˆåœ¨çœŸå®ä¸–ç•ŒåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¶…è¶Šäº†ç°æœ‰çš„MASt3Rå’ŒCOLMAPç­‰æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.20641', 'title': 'Unlocking Efficient Long-to-Short LLM Reasoning with Model Merging', 'url': 'https://huggingface.co/papers/2503.20641', 'abstract': "The transition from System 1 to System 2 reasoning in large language models (LLMs) has marked significant advancements in handling complex tasks through deliberate, iterative thinking. However, this progress often comes at the cost of efficiency, as models tend to overthink, generating redundant reasoning steps without proportional improvements in output quality. Long-to-Short (L2S) reasoning has emerged as a promising solution to this challenge, aiming to balance reasoning depth with practical efficiency. While existing approaches, such as supervised fine-tuning (SFT), reinforcement learning (RL), and prompt engineering, have shown potential, they are either computationally expensive or unstable. Model merging, on the other hand, offers a cost-effective and robust alternative by integrating the quick-thinking capabilities of System 1 models with the methodical reasoning of System 2 models. In this work, we present a comprehensive empirical study on model merging for L2S reasoning, exploring diverse methodologies, including task-vector-based, SVD-based, and activation-informed merging. Our experiments reveal that model merging can reduce average response length by up to 55% while preserving or even improving baseline performance. We also identify a strong correlation between model scale and merging efficacy with extensive evaluations on 1.5B/7B/14B/32B models. Furthermore, we investigate the merged model's ability to self-critique and self-correct, as well as its adaptive response length based on task complexity. Our findings highlight model merging as a highly efficient and effective paradigm for L2S reasoning, offering a practical solution to the overthinking problem while maintaining the robustness of System 2 reasoning. This work can be found on Github https://github.com/hahahawu/Long-to-Short-via-Model-Merging.", 'score': 2, 'issue_id': 2932, 'pub_date': '2025-03-26', 'pub_date_card': {'ru': '26 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 26', 'zh': '3æœˆ26æ—¥'}, 'hash': '52b4dbdb179d7229', 'authors': ['Han Wu', 'Yuxuan Yao', 'Shuqi Liu', 'Zehua Liu', 'Xiaojin Fu', 'Xiongwei Han', 'Xing Li', 'Hui-Ling Zhen', 'Tao Zhong', 'Mingxuan Yuan'], 'affiliations': ['Huawei Noahs Ark Lab'], 'pdf_title_img': 'assets/pdf/title_img/2503.20641.jpg', 'data': {'categories': ['#optimization', '#reasoning', '#training', '#architecture'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ² LLM: Ğ¾Ñ‚ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğº ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¾Ğ¼Ñƒ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğµ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM) Ğ¿Ñ€Ğ¸ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğµ Ğ¾Ñ‚ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ÑƒĞ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ (System 1) Ğº Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ğ°Ğ½Ğ°Ğ»Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ (System 2). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (model merging) Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğ¾Ğ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ ÑÑ€ĞµĞ´Ğ½ÑÑ Ğ´Ğ»Ğ¸Ğ½Ñƒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° Ğ½Ğ° 55% Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¸Ğ»Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ ÑĞ¸Ğ»ÑŒĞ½ÑƒÑ ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¾Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Efficient Reasoning through Model Merging', 'desc': 'This paper discusses the transition from quick, intuitive reasoning (System 1) to more deliberate, analytical reasoning (System 2) in large language models (LLMs). It highlights the inefficiencies that arise when models overthink, leading to unnecessary complexity without significant gains in output quality. The authors propose a method called Long-to-Short (L2S) reasoning, which aims to optimize the balance between deep reasoning and efficiency. They introduce model merging as a solution, which combines the strengths of both reasoning systems, demonstrating that this approach can significantly reduce response length while maintaining or enhancing performance.'}, 'zh': {'title': 'æ¨¡å‹åˆå¹¶ï¼šé«˜æ•ˆçš„é•¿åˆ°çŸ­æ¨ç†è§£å†³æ–¹æ¡ˆ', 'desc': 'æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚ä»»åŠ¡ä¸­ä»ç³»ç»Ÿ1æ¨ç†åˆ°ç³»ç»Ÿ2æ¨ç†çš„è½¬å˜ã€‚å°½ç®¡è¿™ç§è¿›æ­¥æé«˜äº†æ¨ç†çš„æ·±åº¦ï¼Œä½†å¾€å¾€å¯¼è‡´æ•ˆç‡ä¸‹é™ï¼Œæ¨¡å‹å¯èƒ½ä¼šäº§ç”Ÿå†—ä½™çš„æ¨ç†æ­¥éª¤ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œé•¿åˆ°çŸ­ï¼ˆL2Sï¼‰æ¨ç†æå‡ºäº†ä¸€ç§å¹³è¡¡æ¨ç†æ·±åº¦ä¸æ•ˆç‡çš„æ–¹æ¡ˆã€‚é€šè¿‡æ¨¡å‹åˆå¹¶ï¼Œæˆ‘ä»¬èƒ½å¤Ÿå°†ç³»ç»Ÿ1æ¨¡å‹çš„å¿«é€Ÿæ€ç»´ä¸ç³»ç»Ÿ2æ¨¡å‹çš„ç³»ç»Ÿæ€§æ¨ç†ç»“åˆï¼Œä»è€Œåœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶æ˜¾è‘—å‡å°‘å“åº”é•¿åº¦ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.19953', 'title': 'Self-Supervised Learning of Motion Concepts by Optimizing\n  Counterfactuals', 'url': 'https://huggingface.co/papers/2503.19953', 'abstract': "Estimating motion in videos is an essential computer vision problem with many downstream applications, including controllable video generation and robotics. Current solutions are primarily trained using synthetic data or require tuning of situation-specific heuristics, which inherently limits these models' capabilities in real-world contexts. Despite recent developments in large-scale self-supervised learning from videos, leveraging such representations for motion estimation remains relatively underexplored. In this work, we develop Opt-CWM, a self-supervised technique for flow and occlusion estimation from a pre-trained next-frame prediction model. Opt-CWM works by learning to optimize counterfactual probes that extract motion information from a base video model, avoiding the need for fixed heuristics while training on unrestricted video inputs. We achieve state-of-the-art performance for motion estimation on real-world videos while requiring no labeled data.", 'score': 2, 'issue_id': 2919, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 25', 'zh': '3æœˆ25æ—¥'}, 'hash': '01093e8b98f32607', 'authors': ['Stefan Stojanov', 'David Wendt', 'Seungwoo Kim', 'Rahul Venkatesh', 'Kevin Feigelis', 'Jiajun Wu', 'Daniel LK Yamins'], 'affiliations': ['Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2503.19953.jpg', 'data': {'categories': ['#cv', '#video'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Ğ¡Ğ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…', 'desc': 'Opt-CWM - ÑÑ‚Ğ¾ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ° Ğ¸ Ğ¾ĞºĞºĞ»ÑĞ·Ğ¸Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ ĞºĞ°Ğ´Ñ€Ğ°. ĞĞ½ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¾Ğ±, Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ÑÑ‰Ğ¸Ñ… Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ Ğ¸Ğ· Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸Ğº Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒÑÑ Ğ½Ğ° Ğ½ĞµĞ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ²Ñ…Ğ¾Ğ´Ğ°Ñ…. Opt-CWM Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Revolutionizing Motion Estimation with Self-Supervised Learning', 'desc': 'This paper presents Opt-CWM, a self-supervised method for estimating motion in videos, which is crucial for applications like video generation and robotics. Unlike traditional approaches that rely on synthetic data or specific heuristics, Opt-CWM utilizes a pre-trained next-frame prediction model to learn motion information directly from real-world video inputs. The technique optimizes counterfactual probes, allowing it to adaptively extract flow and occlusion data without needing fixed rules. As a result, Opt-CWM achieves state-of-the-art performance in motion estimation while eliminating the requirement for labeled datasets.'}, 'zh': {'title': 'è‡ªç›‘ç£è¿åŠ¨ä¼°è®¡çš„æ–°çªç ´', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†è§†é¢‘ä¸­çš„è¿åŠ¨ä¼°è®¡é—®é¢˜ï¼Œè¿™æ˜¯è®¡ç®—æœºè§†è§‰ä¸­çš„ä¸€ä¸ªé‡è¦è¯¾é¢˜ï¼Œå¹¿æ³›åº”ç”¨äºå¯æ§è§†é¢‘ç”Ÿæˆå’Œæœºå™¨äººæŠ€æœ¯ã€‚ç°æœ‰çš„æ–¹æ³•ä¸»è¦ä¾èµ–åˆæˆæ•°æ®è®­ç»ƒæˆ–ç‰¹å®šæƒ…å¢ƒçš„å¯å‘å¼è°ƒæ•´ï¼Œè¿™é™åˆ¶äº†æ¨¡å‹åœ¨çœŸå®åœºæ™¯ä¸­çš„è¡¨ç°ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºOpt-CWMçš„è‡ªç›‘ç£æŠ€æœ¯ï¼Œé€šè¿‡é¢„è®­ç»ƒçš„ä¸‹ä¸€å¸§é¢„æµ‹æ¨¡å‹è¿›è¡ŒæµåŠ¨å’Œé®æŒ¡ä¼°è®¡ã€‚Opt-CWMé€šè¿‡ä¼˜åŒ–åäº‹å®æ¢é’ˆæ¥æå–è¿åŠ¨ä¿¡æ¯ï¼Œé¿å…äº†å›ºå®šå¯å‘å¼çš„éœ€æ±‚ï¼Œå¹¶åœ¨æ— éœ€æ ‡æ³¨æ•°æ®çš„æƒ…å†µä¸‹ï¼Œåœ¨çœŸå®è§†é¢‘ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„è¿åŠ¨ä¼°è®¡æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.16870', 'title': 'Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs', 'url': 'https://huggingface.co/papers/2503.16870', 'abstract': "Knowledge distillation can be a cost-effective technique to distill knowledge in Large Language Models, if the teacher output logits can be pre-computed and cached. However, successfully applying this to pre-training remains largely unexplored. In this work, we prove that naive approaches for sparse knowledge distillation such as caching Top-K probabilities, while intuitive, provide biased estimates of teacher probability distribution to the student, resulting in suboptimal performance and calibration. We propose an importance-sampling-based method `Random Sampling Knowledge Distillation', which provides unbiased estimates, preserves the gradient in expectation, and requires storing significantly sparser logits. Our method enables faster training of student models with marginal overhead (<10%) compared to cross-entropy based training, while maintaining competitive performance compared to full distillation, across a range of model sizes from 300M to 3B.", 'score': 2, 'issue_id': 2921, 'pub_date': '2025-03-21', 'pub_date_card': {'ru': '21 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 21', 'zh': '3æœˆ21æ—¥'}, 'hash': 'e8b397bd8ee5118a', 'authors': ['Anshumann', 'Mohd Abbas Zaidi', 'Akhil Kedia', 'Jinwoo Ahn', 'Taehwak Kwon', 'Kangwook Lee', 'Haejun Lee', 'Joohyung Lee'], 'affiliations': ['Samsung Research, Seoul'], 'pdf_title_img': 'assets/pdf/title_img/2503.16870.jpg', 'data': {'categories': ['#optimization', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ½Ğ°Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğº Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Top-K Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ĞµĞ¹, Ğ´Ğ°ÑÑ‚ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ. ĞŸÑ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ 'Random Sampling Knowledge Distillation', Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞµ Ğ¿Ğ¾ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½ĞµÑĞ¼ĞµÑ‰ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚ Ğ² Ğ¾Ğ¶Ğ¸Ğ´Ğ°Ğ½Ğ¸Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸-ÑƒÑ‡ĞµĞ½Ğ¸ĞºĞ¸ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ½Ğ°ĞºĞ»Ğ°Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ñ€Ğ°ÑÑ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞºÑ€Ğ¾ÑÑ-ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ."}, 'en': {'title': 'Unbiased Knowledge Distillation for Efficient Model Training', 'desc': "This paper discusses a new method for knowledge distillation in Large Language Models, focusing on the challenges of pre-training. The authors highlight that traditional methods, like caching Top-K probabilities, can lead to biased teacher probability distributions, which negatively affect the student's performance. They introduce 'Random Sampling Knowledge Distillation', an importance-sampling approach that provides unbiased estimates and maintains gradient preservation. This method allows for faster training with minimal overhead while achieving competitive results compared to full distillation across various model sizes."}, 'zh': {'title': 'é«˜æ•ˆçš„çŸ¥è¯†è’¸é¦æ–¹æ³•æå‡å­¦ç”Ÿæ¨¡å‹è®­ç»ƒ', 'desc': 'çŸ¥è¯†è’¸é¦æ˜¯ä¸€ç§æœ‰æ•ˆçš„æŠ€æœ¯ï¼Œå¯ä»¥ä»å¤§å‹è¯­è¨€æ¨¡å‹ä¸­æå–çŸ¥è¯†ã€‚æœ¬æ–‡æ¢è®¨äº†åœ¨é¢„è®­ç»ƒé˜¶æ®µåº”ç”¨çŸ¥è¯†è’¸é¦çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯ç®€å•çš„ç¨€ç–çŸ¥è¯†è’¸é¦æ–¹æ³•å¯èƒ½å¯¼è‡´å­¦ç”Ÿæ¨¡å‹è·å¾—åå·®çš„æ•™å¸ˆæ¦‚ç‡åˆ†å¸ƒã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºé‡è¦æ€§é‡‡æ ·çš„éšæœºé‡‡æ ·çŸ¥è¯†è’¸é¦æ–¹æ³•ï¼Œèƒ½å¤Ÿæä¾›æ— åä¼°è®¡ï¼Œå¹¶åœ¨æœŸæœ›ä¸­ä¿ç•™æ¢¯åº¦ã€‚è¯¥æ–¹æ³•åœ¨å­˜å‚¨ç¨€ç–logitsçš„åŒæ—¶ï¼Œèƒ½åŠ å¿«å­¦ç”Ÿæ¨¡å‹çš„è®­ç»ƒé€Ÿåº¦ï¼Œå¹¶åœ¨ä¸åŒæ¨¡å‹è§„æ¨¡ä¸‹ä¿æŒç«äº‰åŠ›çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.18929', 'title': 'Trajectory Balance with Asynchrony: Decoupling Exploration and Learning\n  for Fast, Scalable LLM Post-Training', 'url': 'https://huggingface.co/papers/2503.18929', 'abstract': 'Reinforcement learning (RL) is a critical component of large language model (LLM) post-training. However, existing on-policy algorithms used for post-training are inherently incompatible with the use of experience replay buffers, which can be populated scalably by distributed off-policy actors to enhance exploration as compute increases. We propose efficiently obtaining this benefit of replay buffers via Trajectory Balance with Asynchrony (TBA), a massively scalable LLM RL system. In contrast to existing approaches, TBA uses a larger fraction of compute on search, constantly generating off-policy data for a central replay buffer. A training node simultaneously samples data from this buffer based on reward or recency to update the policy using Trajectory Balance (TB), a diversity-seeking RL objective introduced for GFlowNets. TBA offers three key advantages: (1) decoupled training and search, speeding up training wall-clock time by 4x or more; (2) improved diversity through large-scale off-policy sampling; and (3) scalable search for sparse reward settings. On mathematical reasoning, preference-tuning, and automated red-teaming (diverse and representative post-training tasks), TBA produces speed and performance improvements over strong baselines.', 'score': 1, 'issue_id': 2932, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 24', 'zh': '3æœˆ24æ—¥'}, 'hash': 'ae8d68c1a89d88a1', 'authors': ['Brian R. Bartoldson', 'Siddarth Venkatraman', 'James Diffenderfer', 'Moksh Jain', 'Tal Ben-Nun', 'Seanie Lee', 'Minsu Kim', 'Johan Obando-Ceron', 'Yoshua Bengio', 'Bhavya Kailkhura'], 'affiliations': ['CIFAR Fellow', 'KAIST', 'Lawrence Livermore National Laboratory', 'Mila Quebec AI Institute', 'Universite de Montreal'], 'pdf_title_img': 'assets/pdf/title_img/2503.18929.jpg', 'data': {'categories': ['#optimization', '#rl', '#reasoning', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'TBA: ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Trajectory Balance with Asynchrony (TBA). TBA Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ±ÑƒÑ„ĞµÑ€ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ¾Ğ¿Ñ‹Ñ‚Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ°ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑÑÑ‰Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ğ¾Ğ±Ñ‰ĞµĞµ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. TBA Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Boosting LLM Training with Efficient Replay and Exploration', 'desc': 'This paper introduces a new method called Trajectory Balance with Asynchrony (TBA) for improving reinforcement learning in large language models (LLMs). TBA allows the use of experience replay buffers, which help in better exploration by storing past experiences and using them for training. The method separates the training and search processes, leading to faster training times and enhanced diversity in the data sampled. Overall, TBA shows significant improvements in performance and efficiency on various post-training tasks compared to existing methods.'}, 'zh': {'title': 'æå‡å¼ºåŒ–å­¦ä¹ æ•ˆç‡çš„è½¨è¿¹å¹³è¡¡ä¸å¼‚æ­¥æ–¹æ³•', 'desc': 'å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åè®­ç»ƒçš„é‡è¦ç»„æˆéƒ¨åˆ†ã€‚ç°æœ‰çš„åœ¨çº¿ç®—æ³•ä¸ç»éªŒé‡æ”¾ç¼“å†²åŒºä¸å…¼å®¹ï¼Œè€Œæˆ‘ä»¬æå‡ºçš„è½¨è¿¹å¹³è¡¡ä¸å¼‚æ­¥ï¼ˆTBAï¼‰æ–¹æ³•å¯ä»¥æœ‰æ•ˆåˆ©ç”¨é‡æ”¾ç¼“å†²åŒºçš„ä¼˜åŠ¿ã€‚TBAé€šè¿‡å°†è®¡ç®—èµ„æºæ›´å¤šåœ°ç”¨äºæœç´¢ï¼ŒæŒç»­ç”Ÿæˆç¦»çº¿æ•°æ®æ¥æ›´æ–°ç­–ç•¥ï¼Œä»è€ŒåŠ é€Ÿè®­ç»ƒè¿‡ç¨‹ã€‚è¯¥æ–¹æ³•åœ¨æ•°å­¦æ¨ç†ã€åå¥½è°ƒä¼˜å’Œè‡ªåŠ¨çº¢é˜Ÿç­‰ä»»åŠ¡ä¸Šè¡¨ç°å‡ºæ˜¾è‘—çš„é€Ÿåº¦å’Œæ€§èƒ½æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.15893', 'title': 'UniHDSA: A Unified Relation Prediction Approach for Hierarchical\n  Document Structure Analysis', 'url': 'https://huggingface.co/papers/2503.15893', 'abstract': "Document structure analysis, aka document layout analysis, is crucial for understanding both the physical layout and logical structure of documents, serving information retrieval, document summarization, knowledge extraction, etc. Hierarchical Document Structure Analysis (HDSA) specifically aims to restore the hierarchical structure of documents created using authoring software with hierarchical schemas. Previous research has primarily followed two approaches: one focuses on tackling specific subtasks of HDSA in isolation, such as table detection or reading order prediction, while the other adopts a unified framework that uses multiple branches or modules, each designed to address a distinct task. In this work, we propose a unified relation prediction approach for HDSA, called UniHDSA, which treats various HDSA sub-tasks as relation prediction problems and consolidates relation prediction labels into a unified label space. This allows a single relation prediction module to handle multiple tasks simultaneously, whether at a page-level or document-level structure analysis. To validate the effectiveness of UniHDSA, we develop a multimodal end-to-end system based on Transformer architectures. Extensive experimental results demonstrate that our approach achieves state-of-the-art performance on a hierarchical document structure analysis benchmark, Comp-HRDoc, and competitive results on a large-scale document layout analysis dataset, DocLayNet, effectively illustrating the superiority of our method across all sub-tasks. The Comp-HRDoc benchmark and UniHDSA's configurations are publicly available at https://github.com/microsoft/CompHRDoc.", 'score': 1, 'issue_id': 2932, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 20', 'zh': '3æœˆ20æ—¥'}, 'hash': '459fe3a2bedb5ed6', 'authors': ['Jiawei Wang', 'Kai Hu', 'Qiang Huo'], 'affiliations': ['Department of EEIS, University of Science and Technology of China, Hefei, 230026, China', 'Microsoft Research Asia, Beijing, 100080, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.15893.jpg', 'data': {'categories': ['#dataset', '#architecture', '#multimodal', '#benchmark'], 'emoji': 'ğŸ“„', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² (HDSA) Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ UniHDSA. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ HDSA ĞºĞ°Ğº Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¼ĞµÑ‚ĞºĞ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¹ Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğº. UniHDSA Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞµĞ´Ğ¸Ğ½Ñ‹Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ğ¸ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Transformer Ğ´Ğ»Ñ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ UniHDSA.'}, 'en': {'title': 'Unified Relation Prediction for Enhanced Document Structure Analysis', 'desc': 'This paper introduces a new method called UniHDSA for Hierarchical Document Structure Analysis (HDSA), which focuses on understanding the layout and structure of documents. Unlike previous methods that tackled individual tasks separately, UniHDSA treats various HDSA tasks as relation prediction problems, allowing for a more integrated approach. The method uses a single relation prediction module to analyze both page-level and document-level structures simultaneously. Experimental results show that UniHDSA outperforms existing methods on benchmark datasets, demonstrating its effectiveness in document layout analysis.'}, 'zh': {'title': 'ç»Ÿä¸€å…³ç³»é¢„æµ‹ï¼Œæå‡æ–‡æ¡£ç»“æ„åˆ†æ', 'desc': 'æ–‡æ¡£ç»“æ„åˆ†æå¯¹äºç†è§£æ–‡æ¡£çš„ç‰©ç†å¸ƒå±€å’Œé€»è¾‘ç»“æ„è‡³å…³é‡è¦ï¼Œæ¶‰åŠä¿¡æ¯æ£€ç´¢ã€æ–‡æ¡£æ‘˜è¦å’ŒçŸ¥è¯†æå–ç­‰åº”ç”¨ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„å…³ç³»é¢„æµ‹æ–¹æ³•UniHDSAï¼Œæ—¨åœ¨å°†æ–‡æ¡£ç»“æ„åˆ†æçš„å„ä¸ªå­ä»»åŠ¡è§†ä¸ºå…³ç³»é¢„æµ‹é—®é¢˜ï¼Œå¹¶å°†å…³ç³»é¢„æµ‹æ ‡ç­¾æ•´åˆåˆ°ä¸€ä¸ªç»Ÿä¸€çš„æ ‡ç­¾ç©ºé—´ä¸­ã€‚é€šè¿‡è¿™ç§æ–¹æ³•ï¼Œå•ä¸€çš„å…³ç³»é¢„æµ‹æ¨¡å—èƒ½å¤ŸåŒæ—¶å¤„ç†å¤šä¸ªä»»åŠ¡ï¼Œæ— è®ºæ˜¯åœ¨é¡µé¢çº§åˆ«è¿˜æ˜¯æ–‡æ¡£çº§åˆ«çš„ç»“æ„åˆ†æã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒUniHDSAåœ¨å±‚æ¬¡æ–‡æ¡£ç»“æ„åˆ†æåŸºå‡†Comp-HRDocä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶åœ¨å¤§è§„æ¨¡æ–‡æ¡£å¸ƒå±€åˆ†ææ•°æ®é›†DocLayNetä¸Šä¹Ÿå–å¾—äº†ç«äº‰åŠ›çš„ç»“æœï¼Œå±•ç¤ºäº†è¯¥æ–¹æ³•åœ¨å„ä¸ªå­ä»»åŠ¡ä¸Šçš„ä¼˜è¶Šæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.10997', 'title': 'RONA: Pragmatically Diverse Image Captioning with Coherence Relations', 'url': 'https://huggingface.co/papers/2503.10997', 'abstract': 'Writing Assistants (e.g., Grammarly, Microsoft Copilot) traditionally generate diverse image captions by employing syntactic and semantic variations to describe image components. However, human-written captions prioritize conveying a central message alongside visual descriptions using pragmatic cues. To enhance pragmatic diversity, it is essential to explore alternative ways of communicating these messages in conjunction with visual content. To address this challenge, we propose RONA, a novel prompting strategy for Multi-modal Large Language Models (MLLM) that leverages Coherence Relations as an axis for variation. We demonstrate that RONA generates captions with better overall diversity and ground-truth alignment, compared to MLLM baselines across multiple domains. Our code is available at: https://github.com/aashish2000/RONA', 'score': 1, 'issue_id': 2932, 'pub_date': '2025-03-14', 'pub_date_card': {'ru': '14 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 14', 'zh': '3æœˆ14æ—¥'}, 'hash': 'd3b2354bbfc88139', 'authors': ['Aashish Anantha Ramakrishnan', 'Aadarsh Anantha Ramakrishnan', 'Dongwon Lee'], 'affiliations': ['National Institute of Technology, Tiruchirappalli', 'The Pennsylvania State University'], 'pdf_title_img': 'assets/pdf/title_img/2503.10997.jpg', 'data': {'categories': ['#open_source', '#story_generation', '#multimodal', '#cv'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'RONA: Ğ¿Ñ€Ğ°Ğ³Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞµĞ¹ Ğº Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ RONA Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞµĞ¹ Ğº Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM). Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², RONA Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ°Ğ³Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ RONA Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ MLLM Ğ¿Ğ¾ Ğ¾Ğ±Ñ‰ĞµĞ¼Ñƒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑÑĞ¼ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ´ ÑĞ²Ğ¾ĞµĞ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°.'}, 'en': {'title': 'RONA: Enhancing Caption Diversity with Pragmatic Cues', 'desc': 'This paper introduces RONA, a new prompting strategy designed for Multi-modal Large Language Models (MLLM) to improve the diversity of image captions. Unlike traditional methods that focus on syntactic and semantic variations, RONA emphasizes pragmatic cues to convey a central message alongside visual descriptions. By utilizing Coherence Relations, RONA enhances the way messages are communicated in relation to images. The results show that RONA outperforms existing MLLM baselines in generating captions that are more diverse and closely aligned with ground-truth descriptions across various domains.'}, 'zh': {'title': 'RONAï¼šæå‡å›¾åƒæ ‡é¢˜å¤šæ ·æ€§çš„åˆ›æ–°ç­–ç•¥', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æç¤ºç­–ç•¥RONAï¼Œç”¨äºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œæ—¨åœ¨æé«˜å›¾åƒæ ‡é¢˜çš„å¤šæ ·æ€§ã€‚ä¼ ç»Ÿçš„å†™ä½œåŠ©æ‰‹ç”Ÿæˆçš„æ ‡é¢˜å¾€å¾€ä¾§é‡äºè¯­æ³•å’Œè¯­ä¹‰çš„å˜åŒ–ï¼Œè€Œäººç±»æ’°å†™çš„æ ‡é¢˜åˆ™æ›´æ³¨é‡ä¼ è¾¾ä¸­å¿ƒä¿¡æ¯ã€‚RONAé€šè¿‡åˆ©ç”¨ä¸€è‡´æ€§å…³ç³»ä½œä¸ºå˜åŒ–çš„è½´å¿ƒï¼Œæ¢ç´¢äº†ä¸è§†è§‰å†…å®¹ç»“åˆçš„æ›¿ä»£æ²Ÿé€šæ–¹å¼ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRONAç”Ÿæˆçš„æ ‡é¢˜åœ¨å¤šæ ·æ€§å’ŒçœŸå®å¯¹é½åº¦ä¸Šä¼˜äºç°æœ‰çš„MLLMåŸºçº¿ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.20731', 'title': 'RecTable: Fast Modeling Tabular Data with Rectified Flow', 'url': 'https://huggingface.co/papers/2503.20731', 'abstract': 'Score-based or diffusion models generate high-quality tabular data, surpassing GAN-based and VAE-based models. However, these methods require substantial training time. In this paper, we introduce RecTable, which uses the rectified flow modeling, applied in such as text-to-image generation and text-to-video generation. RecTable features a simple architecture consisting of a few stacked gated linear unit blocks. Additionally, our training strategies are also simple, incorporating a mixed-type noise distribution and a logit-normal timestep distribution. Our experiments demonstrate that RecTable achieves competitive performance compared to the several state-of-the-art diffusion and score-based models while reducing the required training time. Our code is available at https://github.com/fmp453/rectable.', 'score': 0, 'issue_id': 2935, 'pub_date': '2025-03-26', 'pub_date_card': {'ru': '26 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 26', 'zh': '3æœˆ26æ—¥'}, 'hash': 'dfe7dcfbd0067c32', 'authors': ['Masane Fuchi', 'Tomohiro Takagi'], 'affiliations': ['Meiji University'], 'pdf_title_img': 'assets/pdf/title_img/2503.20731.jpg', 'data': {'categories': ['#open_source', '#architecture', '#dataset', '#optimization', '#diffusion', '#training'], 'emoji': 'ğŸ“Š', 'ru': {'title': 'RecTable: Ğ±Ñ‹ÑÑ‚Ñ€Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…', 'desc': 'ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ RecTable - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸ Ğ²Ñ‹Ğ¿Ñ€ÑĞ¼Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ° (rectified flow). RecTable Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ¸Ğ· Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±Ğ»Ğ¾ĞºĞ¾Ğ² Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ½Ğ¸Ñ‚Ğ°Ğ¼Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ ÑˆÑƒĞ¼Ğ° Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ‚-Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ RecTable Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ², ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ñ‹Ñ… Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ¿Ñ€Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞµĞ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'RecTable: Fast and Efficient Tabular Data Generation', 'desc': 'This paper presents RecTable, a novel approach for generating high-quality tabular data using rectified flow modeling. Unlike traditional methods like GANs and VAEs, RecTable significantly reduces training time while maintaining competitive performance. The architecture is straightforward, utilizing stacked gated linear unit blocks, which simplifies the model design. Additionally, the training strategies involve a mixed-type noise distribution and a logit-normal timestep distribution, enhancing efficiency and effectiveness in data generation.'}, 'zh': {'title': 'RecTableï¼šé«˜æ•ˆç”Ÿæˆè¡¨æ ¼æ•°æ®çš„æ–°æ–¹æ³•', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æ¨¡å‹RecTableï¼Œç”¨äºç”Ÿæˆé«˜è´¨é‡çš„è¡¨æ ¼æ•°æ®ï¼Œè¶…è¶Šäº†åŸºäºGANå’ŒVAEçš„æ¨¡å‹ã€‚RecTableé‡‡ç”¨äº†ä¿®æ­£æµå»ºæ¨¡ï¼Œå…·æœ‰ç®€å•çš„æ¶æ„ï¼Œç”±å‡ ä¸ªå †å çš„é—¨æ§çº¿æ€§å•å…ƒå—ç»„æˆã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨è®­ç»ƒç­–ç•¥ä¸Šä¹Ÿå¾ˆç®€å•ï¼Œç»“åˆäº†æ··åˆç±»å‹çš„å™ªå£°åˆ†å¸ƒå’Œå¯¹æ•°æ­£æ€æ—¶é—´æ­¥åˆ†å¸ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRecTableåœ¨æ€§èƒ½ä¸Šä¸å¤šç§æœ€å…ˆè¿›çš„æ‰©æ•£å’Œè¯„åˆ†æ¨¡å‹ç›¸å½“ï¼ŒåŒæ—¶å‡å°‘äº†è®­ç»ƒæ—¶é—´ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.17970', 'title': 'PathoHR: Breast Cancer Survival Prediction on High-Resolution\n  Pathological Images', 'url': 'https://huggingface.co/papers/2503.17970', 'abstract': "Breast cancer survival prediction in computational pathology presents a remarkable challenge due to tumor heterogeneity. For instance, different regions of the same tumor in the pathology image can show distinct morphological and molecular characteristics. This makes it difficult to extract representative features from whole slide images (WSIs) that truly reflect the tumor's aggressive potential and likely survival outcomes. In this paper, we present PathoHR, a novel pipeline for accurate breast cancer survival prediction that enhances any size of pathological images to enable more effective feature learning. Our approach entails (1) the incorporation of a plug-and-play high-resolution Vision Transformer (ViT) to enhance patch-wise WSI representation, enabling more detailed and comprehensive feature extraction, (2) the systematic evaluation of multiple advanced similarity metrics for comparing WSI-extracted features, optimizing the representation learning process to better capture tumor characteristics, (3) the demonstration that smaller image patches enhanced follow the proposed pipeline can achieve equivalent or superior prediction accuracy compared to raw larger patches, while significantly reducing computational overhead. Experimental findings valid that PathoHR provides the potential way of integrating enhanced image resolution with optimized feature learning to advance computational pathology, offering a promising direction for more accurate and efficient breast cancer survival prediction. Code will be available at https://github.com/AIGeeksGroup/PathoHR.", 'score': 0, 'issue_id': 2928, 'pub_date': '2025-03-23', 'pub_date_card': {'ru': '23 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 23', 'zh': '3æœˆ23æ—¥'}, 'hash': '7982f77e78076ec7', 'authors': ['Yang Luo', 'Shiru Wang', 'Jun Liu', 'Jiaxuan Xiao', 'Rundong Xue', 'Zeyu Zhang', 'Hao Zhang', 'Yu Lu', 'Yang Zhao', 'Yutong Xie'], 'affiliations': ['ANU', 'DLMU', 'Dartmouth', 'La Trobe', 'MBZUAI', 'NUP', 'SZTU', 'UCAS', 'XJLTU', 'XJTU'], 'pdf_title_img': 'assets/pdf/title_img/2503.17970.jpg', 'data': {'categories': ['#cv', '#healthcare', '#training'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'ĞŸĞ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ° Ñ€Ğ°ĞºĞ° Ğ³Ñ€ÑƒĞ´Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'PathoHR - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹Ğ¶Ğ¸Ğ²Ğ°ĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ñ€Ğ°ĞºĞµ Ğ¼Ğ¾Ğ»Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¶ĞµĞ»ĞµĞ·Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ğ°Ñ‚Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ»ÑĞ±Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ². ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞ°ÑÑ‰Ğ¸Ğ¹ Vision Transformer Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ñ‚Ñ‡-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ° Ğ´Ğ»Ñ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ°Ğ»ĞµĞ½ÑŒĞºĞ¸Ğµ Ğ¿Ğ°Ñ‚Ñ‡Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ¾Ğ¹ Ğ¸Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‰ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ½ĞµĞ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ‚Ñ‡Ğ¸. PathoHR Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ°Ñ‚Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸.'}, 'en': {'title': 'Enhancing Breast Cancer Survival Prediction with PathoHR', 'desc': 'This paper addresses the challenge of predicting breast cancer survival by focusing on the variability within tumors, which can complicate feature extraction from whole slide images (WSIs). The authors introduce PathoHR, a new pipeline that utilizes a high-resolution Vision Transformer (ViT) to improve the representation of tumor features at a patch level. They evaluate various similarity metrics to optimize the learning process, allowing for better capture of tumor characteristics. The results show that smaller, enhanced image patches can achieve similar or better prediction accuracy compared to larger patches, while also being more computationally efficient.'}, 'zh': {'title': 'æå‡ä¹³è…ºç™Œç”Ÿå­˜é¢„æµ‹çš„PathoHRç®¡é“', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºPathoHRçš„æ–°å‹ç®¡é“ï¼Œç”¨äºæé«˜ä¹³è…ºç™Œç”Ÿå­˜é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚ç”±äºè‚¿ç˜¤çš„å¼‚è´¨æ€§ï¼Œç—…ç†å›¾åƒä¸­çš„ä¸åŒåŒºåŸŸå¯èƒ½è¡¨ç°å‡ºä¸åŒçš„å½¢æ€å’Œåˆ†å­ç‰¹å¾ï¼Œè¿™ä½¿å¾—ä»å…¨åˆ‡ç‰‡å›¾åƒä¸­æå–ä»£è¡¨æ€§ç‰¹å¾å˜å¾—å›°éš¾ã€‚PathoHRé€šè¿‡å¼•å…¥é«˜åˆ†è¾¨ç‡çš„è§†è§‰å˜æ¢å™¨ï¼ˆViTï¼‰æ¥å¢å¼ºå›¾åƒè¡¥ä¸çš„è¡¨ç¤ºï¼Œä»è€Œå®ç°æ›´æœ‰æ•ˆçš„ç‰¹å¾å­¦ä¹ ã€‚æ­¤å¤–ï¼Œå®éªŒç»“æœè¡¨æ˜ï¼Œç»è¿‡è¯¥ç®¡é“å¢å¼ºçš„å°å›¾åƒè¡¥ä¸åœ¨é¢„æµ‹å‡†ç¡®æ€§ä¸Šå¯ä»¥ä¸åŸå§‹çš„å¤§å›¾åƒè¡¥ä¸ç›¸åª²ç¾ï¼ŒåŒæ—¶æ˜¾è‘—é™ä½è®¡ç®—å¼€é”€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.10622', 'title': 'Transformers without Normalization', 'url': 'https://huggingface.co/papers/2503.10622', 'abstract': 'Normalization layers are ubiquitous in modern neural networks and have long been considered essential. This work demonstrates that Transformers without normalization can achieve the same or better performance using a remarkably simple technique. We introduce Dynamic Tanh (DyT), an element-wise operation DyT(x) = tanh(alpha x), as a drop-in replacement for normalization layers in Transformers. DyT is inspired by the observation that layer normalization in Transformers often produces tanh-like, S-shaped input-output mappings. By incorporating DyT, Transformers without normalization can match or exceed the performance of their normalized counterparts, mostly without hyperparameter tuning. We validate the effectiveness of Transformers with DyT across diverse settings, ranging from recognition to generation, supervised to self-supervised learning, and computer vision to language models. These findings challenge the conventional understanding that normalization layers are indispensable in modern neural networks, and offer new insights into their role in deep networks.', 'score': 84, 'issue_id': 2702, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 13', 'zh': '3æœˆ13æ—¥'}, 'hash': 'd790a15c7d75d15e', 'authors': ['Jiachen Zhu', 'Xinlei Chen', 'Kaiming He', 'Yann LeCun', 'Zhuang Liu'], 'affiliations': ['FAIR, Meta', 'MIT', 'New York University', 'Princeton University'], 'pdf_title_img': 'assets/pdf/title_img/2503.10622.jpg', 'data': {'categories': ['#optimization', '#training', '#architecture', '#multimodal', '#cv'], 'emoji': 'ğŸ”„', 'ru': {'title': 'ĞŸÑ€Ğ¾Ñ‰Ğ°Ğ¹, Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ: Dynamic Tanh Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ° Ğ¸Ğ³Ñ€Ñ‹ Ğ² Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ²', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ñƒ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼ ÑĞ»Ğ¾ÑĞ¼ Ğ² Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°Ñ… - Dynamic Tanh (DyT). DyT - ÑÑ‚Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ°Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ½Ğ°Ñ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ·Ğ°Ğ¼ĞµĞ½Ğ¸Ñ‚ÑŒ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ñ‹ Ñ DyT Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ Ğ½Ğµ Ñ…ÑƒĞ¶Ğµ Ğ¸Ğ»Ğ¸ Ğ»ÑƒÑ‡ÑˆĞµ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°. Ğ­Ñ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ ÑÑ‚Ğ°Ğ²Ğ¸Ñ‚ Ğ¿Ğ¾Ğ´ ÑĞ¾Ğ¼Ğ½ĞµĞ½Ğ¸Ğµ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ»Ğ¾ĞµĞ² Ğ² ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ÑÑ….'}, 'en': {'title': 'Transformers Thrive Without Normalization: Introducing Dynamic Tanh', 'desc': 'This paper explores the role of normalization layers in Transformers, showing that they may not be as essential as previously thought. The authors introduce Dynamic Tanh (DyT), a simple element-wise operation that can replace normalization layers while maintaining or improving performance. DyT is based on the observation that layer normalization often results in S-shaped mappings similar to the tanh function. The study demonstrates that Transformers using DyT can perform well across various tasks, suggesting a reevaluation of the necessity of normalization in deep learning architectures.'}, 'zh': {'title': 'åŠ¨æ€åŒæ›²æ­£åˆ‡ï¼šè¶…è¶Šå½’ä¸€åŒ–çš„å˜æ¢å™¨', 'desc': 'æœ¬ç ”ç©¶å±•ç¤ºäº†åœ¨ç°ä»£ç¥ç»ç½‘ç»œä¸­ï¼Œå½’ä¸€åŒ–å±‚å¹¶éå¿…ä¸å¯å°‘ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºåŠ¨æ€åŒæ›²æ­£åˆ‡ï¼ˆDynamic Tanh, DyTï¼‰çš„ç®€å•æŠ€æœ¯ï¼Œå¯ä»¥æ›¿ä»£å˜æ¢å™¨ä¸­çš„å½’ä¸€åŒ–å±‚ã€‚DyTé€šè¿‡è§‚å¯Ÿå˜æ¢å™¨ä¸­çš„å±‚å½’ä¸€åŒ–é€šå¸¸äº§ç”Ÿç±»ä¼¼tanhçš„Så½¢è¾“å…¥è¾“å‡ºæ˜ å°„è€Œå¾—å‡ºã€‚é€šè¿‡å¼•å…¥DyTï¼Œæœªä½¿ç”¨å½’ä¸€åŒ–çš„å˜æ¢å™¨å¯ä»¥åœ¨å¤šç§ä»»åŠ¡ä¸­è¾¾åˆ°æˆ–è¶…è¿‡ä½¿ç”¨å½’ä¸€åŒ–çš„å˜æ¢å™¨çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.10613', 'title': 'CoSTAast: Cost-Sensitive Toolpath Agent for Multi-turn Image Editing', 'url': 'https://huggingface.co/papers/2503.10613', 'abstract': 'Text-to-image models like stable diffusion and DALLE-3 still struggle with multi-turn image editing. We decompose such a task as an agentic workflow (path) of tool use that addresses a sequence of subtasks by AI tools of varying costs. Conventional search algorithms require expensive exploration to find tool paths. While large language models (LLMs) possess prior knowledge of subtask planning, they may lack accurate estimations of capabilities and costs of tools to determine which to apply in each subtask. Can we combine the strengths of both LLMs and graph search to find cost-efficient tool paths? We propose a three-stage approach "CoSTA*" that leverages LLMs to create a subtask tree, which helps prune a graph of AI tools for the given task, and then conducts A* search on the small subgraph to find a tool path. To better balance the total cost and quality, CoSTA* combines both metrics of each tool on every subtask to guide the A* search. Each subtask\'s output is then evaluated by a vision-language model (VLM), where a failure will trigger an update of the tool\'s cost and quality on the subtask. Hence, the A* search can recover from failures quickly to explore other paths. Moreover, CoSTA* can automatically switch between modalities across subtasks for a better cost-quality trade-off. We build a novel benchmark of challenging multi-turn image editing, on which CoSTA* outperforms state-of-the-art image-editing models or agents in terms of both cost and quality, and performs versatile trade-offs upon user preference.', 'score': 60, 'issue_id': 2700, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 13', 'zh': '3æœˆ13æ—¥'}, 'hash': '7f2d9ee971af97a8', 'authors': ['Advait Gupta', 'NandaKiran Velaga', 'Dang Nguyen', 'Tianyi Zhou'], 'affiliations': ['University of Maryland, College Park'], 'pdf_title_img': 'assets/pdf/title_img/2503.10613.jpg', 'data': {'categories': ['#games', '#benchmark', '#multimodal', '#agents', '#optimization'], 'emoji': 'ğŸ¨', 'ru': {'title': 'CoSTA*: Ğ£Ğ¼Ğ½Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜ Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ CoSTA* Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ´ĞµÑ€ĞµĞ²Ğ° Ğ¿Ğ¾Ğ´Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¿Ğ¾Ğ¸ÑĞºĞ° A* Ğ´Ğ»Ñ Ğ½Ğ°Ñ…Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿ÑƒÑ‚Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ˜Ğ˜. CoSTA* Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ÑÑ‚Ğ°Ğ¿Ğ° Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğº Ğ½ĞµÑƒĞ´Ğ°Ñ‡Ğ°Ğ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ CoSTA* Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ² ÑĞ¾Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¸ ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Optimizing Multi-Turn Image Editing with CoSTA*', 'desc': 'This paper addresses the challenges faced by text-to-image models in multi-turn image editing by introducing a new approach called CoSTA*. It combines large language models (LLMs) with graph search techniques to efficiently plan and execute a sequence of subtasks using AI tools. CoSTA* creates a subtask tree to streamline the selection of tools based on their costs and capabilities, and employs A* search to find optimal tool paths. The method also adapts to failures by updating tool metrics, allowing for quick recovery and improved cost-quality trade-offs in image editing tasks.'}, 'zh': {'title': 'é«˜æ•ˆçš„å¤šè½®å›¾åƒç¼–è¾‘å·¥å…·è·¯å¾„ä¼˜åŒ–', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºCoSTA*çš„ä¸‰é˜¶æ®µæ–¹æ³•ï¼Œç”¨äºè§£å†³æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹åœ¨å¤šè½®å›¾åƒç¼–è¾‘ä¸­çš„æŒ‘æˆ˜ã€‚è¯¥æ–¹æ³•ç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå›¾æœç´¢çš„ä¼˜ç‚¹ï¼Œé€šè¿‡åˆ›å»ºå­ä»»åŠ¡æ ‘æ¥ä¼˜åŒ–AIå·¥å…·çš„ä½¿ç”¨è·¯å¾„ã€‚CoSTA*åœ¨æ¯ä¸ªå­ä»»åŠ¡ä¸­ç»¼åˆè€ƒè™‘å·¥å…·çš„æˆæœ¬å’Œè´¨é‡ï¼Œä»¥æŒ‡å¯¼A*æœç´¢ï¼Œä»è€Œæ‰¾åˆ°é«˜æ•ˆçš„å·¥å…·è·¯å¾„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCoSTA*åœ¨å¤šè½®å›¾åƒç¼–è¾‘ä»»åŠ¡ä¸­è¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ¨¡å‹ï¼Œèƒ½å¤Ÿæ ¹æ®ç”¨æˆ·åå¥½è¿›è¡Œçµæ´»çš„æˆæœ¬ä¸è´¨é‡æƒè¡¡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.10633', 'title': "Charting and Navigating Hugging Face's Model Atlas", 'url': 'https://huggingface.co/papers/2503.10633', 'abstract': 'As there are now millions of publicly available neural networks, searching and analyzing large model repositories becomes increasingly important. Navigating so many models requires an atlas, but as most models are poorly documented charting such an atlas is challenging. To explore the hidden potential of model repositories, we chart a preliminary atlas representing the documented fraction of Hugging Face. It provides stunning visualizations of the model landscape and evolution. We demonstrate several applications of this atlas including predicting model attributes (e.g., accuracy), and analyzing trends in computer vision models. However, as the current atlas remains incomplete, we propose a method for charting undocumented regions. Specifically, we identify high-confidence structural priors based on dominant real-world model training practices. Leveraging these priors, our approach enables accurate mapping of previously undocumented areas of the atlas. We publicly release our datasets, code, and interactive atlas.', 'score': 50, 'issue_id': 2703, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 13', 'zh': '3æœˆ13æ—¥'}, 'hash': '128e9e97a54b8404', 'authors': ['Eliahu Horwitz', 'Nitzan Kurer', 'Jonathan Kahana', 'Liel Amar', 'Yedid Hoshen'], 'affiliations': ['School of Computer Science and Engineering The Hebrew University of Jerusalem, Israel'], 'pdf_title_img': 'assets/pdf/title_img/2503.10633.jpg', 'data': {'categories': ['#open_source', '#benchmark', '#cv', '#data', '#survey', '#dataset'], 'emoji': 'ğŸ—ºï¸', 'ru': {'title': 'Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ°Ñ‚Ğ»Ğ°ÑĞ° Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ¹: Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ñ Ğ² Ğ¾ĞºĞµĞ°Ğ½Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ°Ñ‚Ğ»Ğ°ÑĞ° Ğ´Ğ»Ñ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ğ¼ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ»Ğ°Ğ½Ğ´ÑˆĞ°Ñ„Ñ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ñ‡Ğ°ÑÑ‚Ğ¸ Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ñ Hugging Face. ĞĞ½Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ°Ñ‚Ğ»Ğ°ÑĞ° Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ñ‚Ñ€ĞµĞ½Ğ´Ğ¾Ğ² Ğ² ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ¼ Ğ·Ñ€ĞµĞ½Ğ¸Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ ĞºĞ°Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½ĞµĞ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ Ğ°Ñ‚Ğ»Ğ°ÑĞ° Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ñ… Ğ°Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹.'}, 'en': {'title': 'Mapping the Neural Network Landscape: An Interactive Atlas', 'desc': 'This paper addresses the challenge of navigating the vast number of publicly available neural networks by creating a visual atlas of documented models from Hugging Face. The atlas not only visualizes the model landscape but also tracks the evolution of these models over time. It includes applications such as predicting model attributes like accuracy and analyzing trends in computer vision. To improve the atlas, the authors propose a method to chart undocumented regions by using high-confidence structural priors based on common training practices.'}, 'zh': {'title': 'æ¢ç´¢ç¥ç»ç½‘ç»œæ¨¡å‹çš„åœ°å›¾', 'desc': 'éšç€å…¬å¼€ç¥ç»ç½‘ç»œæ•°é‡çš„æ¿€å¢ï¼Œæœç´¢å’Œåˆ†æå¤§å‹æ¨¡å‹åº“å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚ç”±äºå¤§å¤šæ•°æ¨¡å‹æ–‡æ¡£ä¸å…¨ï¼Œç»˜åˆ¶æ¨¡å‹çš„åœ°å›¾å˜å¾—å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æˆ‘ä»¬ç»˜åˆ¶äº†ä¸€ä¸ªåˆæ­¥çš„åœ°å›¾ï¼Œå±•ç¤ºäº†Hugging Faceä¸Šå·²è®°å½•æ¨¡å‹çš„åˆ†å¸ƒå’Œæ¼”å˜ï¼Œå¹¶å±•ç¤ºäº†å¤šç§åº”ç”¨ï¼ŒåŒ…æ‹¬é¢„æµ‹æ¨¡å‹å±æ€§å’Œåˆ†æè®¡ç®—æœºè§†è§‰æ¨¡å‹çš„è¶‹åŠ¿ã€‚ä¸ºäº†å¡«è¡¥å½“å‰åœ°å›¾çš„ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼Œé€šè¿‡è¯†åˆ«åŸºäºä¸»æµè®­ç»ƒå®è·µçš„é«˜ç½®ä¿¡åº¦ç»“æ„å…ˆéªŒï¼Œå‡†ç¡®ç»˜åˆ¶æœªè®°å½•åŒºåŸŸã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.10480', 'title': 'World Modeling Makes a Better Planner: Dual Preference Optimization for\n  Embodied Task Planning', 'url': 'https://huggingface.co/papers/2503.10480', 'abstract': 'Recent advances in large vision-language models (LVLMs) have shown promise for embodied task planning, yet they struggle with fundamental challenges like dependency constraints and efficiency. Existing approaches either solely optimize action selection or leverage world models during inference, overlooking the benefits of learning to model the world as a way to enhance planning capabilities. We propose Dual Preference Optimization (D^2PO), a new learning framework that jointly optimizes state prediction and action selection through preference learning, enabling LVLMs to understand environment dynamics for better planning. To automatically collect trajectories and stepwise preference data without human annotation, we introduce a tree search mechanism for extensive exploration via trial-and-error. Extensive experiments on VoTa-Bench demonstrate that our D^2PO-based method significantly outperforms existing methods and GPT-4o when applied to Qwen2-VL (7B), LLaVA-1.6 (7B), and LLaMA-3.2 (11B), achieving superior task success rates with more efficient execution paths.', 'score': 39, 'issue_id': 2700, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 13', 'zh': '3æœˆ13æ—¥'}, 'hash': '2d3e6b8c84c51e69', 'authors': ['Siyin Wang', 'Zhaoye Fei', 'Qinyuan Cheng', 'Shiduo Zhang', 'Panpan Cai', 'Jinlan Fu', 'Xipeng Qiu'], 'affiliations': ['Fudan University', 'National University of Singapore', 'Shanghai Innovation Institute', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2503.10480.jpg', 'data': {'categories': ['#reasoning', '#multimodal', '#agents', '#optimization', '#rl', '#training'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ñƒ LVLM Ñ‡ĞµÑ€ĞµĞ· ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½ÑƒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LVLM) Ğ´Ğ»Ñ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ² Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Dual Preference Optimization (D^2PO) ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼. Ğ”Ğ»Ñ ÑĞ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ±ĞµĞ· ÑƒÑ‡Ğ°ÑÑ‚Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¿Ğ¾ Ğ´ĞµÑ€ĞµĞ²Ñƒ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ D^2PO Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹.'}, 'en': {'title': 'Enhancing Planning in LVLMs with Dual Preference Optimization', 'desc': 'This paper introduces Dual Preference Optimization (D^2PO), a novel framework designed to improve the planning capabilities of large vision-language models (LVLMs) by jointly optimizing state prediction and action selection. The approach addresses key challenges in embodied task planning, such as dependency constraints and efficiency, which have been inadequately handled by existing methods. By employing a tree search mechanism, D^2PO enables the automatic collection of trajectories and preference data, facilitating extensive exploration through trial-and-error without the need for human annotation. Experimental results on VoTa-Bench show that D^2PO significantly enhances task success rates and execution efficiency compared to previous methods and GPT-4o across various LVLMs.'}, 'zh': {'title': 'åŒé‡åå¥½ä¼˜åŒ–ï¼šæå‡ä»»åŠ¡è§„åˆ’èƒ½åŠ›çš„å…³é”®', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å­¦ä¹ æ¡†æ¶ï¼Œç§°ä¸ºåŒé‡åå¥½ä¼˜åŒ–ï¼ˆD^2POï¼‰ï¼Œæ—¨åœ¨æé«˜å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨ä»»åŠ¡è§„åˆ’ä¸­çš„èƒ½åŠ›ã€‚è¯¥æ¡†æ¶é€šè¿‡åå¥½å­¦ä¹ åŒæ—¶ä¼˜åŒ–çŠ¶æ€é¢„æµ‹å’ŒåŠ¨ä½œé€‰æ‹©ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£ç¯å¢ƒåŠ¨æ€ã€‚ä¸ºäº†è‡ªåŠ¨æ”¶é›†è½¨è¿¹å’Œé€æ­¥åå¥½æ•°æ®ï¼Œæœ¬æ–‡å¼•å…¥äº†ä¸€ç§æ ‘æœç´¢æœºåˆ¶ï¼Œä»¥ä¾¿é€šè¿‡è¯•é”™è¿›è¡Œå¹¿æ³›æ¢ç´¢ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºD^2POçš„æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•å’ŒGPT-4oï¼Œè¾¾åˆ°äº†æ›´é«˜çš„ä»»åŠ¡æˆåŠŸç‡å’Œæ›´é«˜æ•ˆçš„æ‰§è¡Œè·¯å¾„ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.10639', 'title': 'GoT: Unleashing Reasoning Capability of Multimodal Large Language Model\n  for Visual Generation and Editing', 'url': 'https://huggingface.co/papers/2503.10639', 'abstract': 'Current image generation and editing methods primarily process textual prompts as direct inputs without reasoning about visual composition and explicit operations. We present Generation Chain-of-Thought (GoT), a novel paradigm that enables generation and editing through an explicit language reasoning process before outputting images. This approach transforms conventional text-to-image generation and editing into a reasoning-guided framework that analyzes semantic relationships and spatial arrangements. We define the formulation of GoT and construct large-scale GoT datasets containing over 9M samples with detailed reasoning chains capturing semantic-spatial relationships. To leverage the advantages of GoT, we implement a unified framework that integrates Qwen2.5-VL for reasoning chain generation with an end-to-end diffusion model enhanced by our novel Semantic-Spatial Guidance Module. Experiments show our GoT framework achieves excellent performance on both generation and editing tasks, with significant improvements over baselines. Additionally, our approach enables interactive visual generation, allowing users to explicitly modify reasoning steps for precise image adjustments. GoT pioneers a new direction for reasoning-driven visual generation and editing, producing images that better align with human intent. To facilitate future research, we make our datasets, code, and pretrained models publicly available at https://github.com/rongyaofang/GoT.', 'score': 37, 'issue_id': 2701, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 13', 'zh': '3æœˆ13æ—¥'}, 'hash': '70c9d741eedd181c', 'authors': ['Rongyao Fang', 'Chengqi Duan', 'Kun Wang', 'Linjiang Huang', 'Hao Li', 'Shilin Yan', 'Hao Tian', 'Xingyu Zeng', 'Rui Zhao', 'Jifeng Dai', 'Xihui Liu', 'Hongsheng Li'], 'affiliations': ['BUAA', 'CUHK MMLab', 'HKU', 'SenseTime', 'Shanghai AI Laboratory', 'THU'], 'pdf_title_img': 'assets/pdf/title_img/2503.10639.jpg', 'data': {'categories': ['#multimodal', '#cv', '#dataset', '#reasoning', '#diffusion', '#open_source'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ Ğ°Ğ·ÑƒĞ¼Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹: Ğ¾Ñ‚ Ñ‚ĞµĞºÑÑ‚Ğ° Ğº Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Generation Chain-of-Thought (GoT) Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. GoT Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ²Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿ĞµÑ€ĞµĞ´ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ¾Ğ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… GoT Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ»Ğ¸ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰ÑƒÑ Qwen2.5-VL Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'Revolutionizing Image Generation with Reasoning!', 'desc': 'The paper introduces Generation Chain-of-Thought (GoT), a new method for image generation and editing that incorporates reasoning about visual composition. Instead of simply processing text prompts, GoT uses a reasoning-guided framework to analyze semantic relationships and spatial arrangements before generating images. It includes a large dataset with over 9 million samples that capture detailed reasoning chains, enhancing the understanding of how different elements relate visually. The results show that GoT significantly improves the quality of generated and edited images, allowing for interactive adjustments based on user-defined reasoning steps.'}, 'zh': {'title': 'æ¨ç†é©±åŠ¨çš„å›¾åƒç”Ÿæˆä¸ç¼–è¾‘æ–°æ–¹å‘', 'desc': 'å½“å‰çš„å›¾åƒç”Ÿæˆå’Œç¼–è¾‘æ–¹æ³•ä¸»è¦å°†æ–‡æœ¬æç¤ºä½œä¸ºç›´æ¥è¾“å…¥ï¼Œè€Œæ²¡æœ‰è€ƒè™‘è§†è§‰æ„å›¾å’Œæ˜ç¡®çš„æ“ä½œã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„ç”Ÿæˆæ€ç»´é“¾ï¼ˆGoTï¼‰èŒƒå¼ï¼Œé€šè¿‡åœ¨è¾“å‡ºå›¾åƒä¹‹å‰è¿›è¡Œæ˜ç¡®çš„è¯­è¨€æ¨ç†è¿‡ç¨‹æ¥å®ç°ç”Ÿæˆå’Œç¼–è¾‘ã€‚è¯¥æ–¹æ³•å°†ä¼ ç»Ÿçš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå’Œç¼–è¾‘è½¬å˜ä¸ºä¸€ä¸ªåŸºäºæ¨ç†çš„æ¡†æ¶ï¼Œåˆ†æè¯­ä¹‰å…³ç³»å’Œç©ºé—´æ’åˆ—ã€‚æˆ‘ä»¬çš„GoTæ¡†æ¶åœ¨ç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—ä¼˜äºåŸºçº¿ï¼Œå¹¶å…è®¸ç”¨æˆ·é€šè¿‡æ˜ç¡®ä¿®æ”¹æ¨ç†æ­¥éª¤æ¥è¿›è¡Œäº¤äº’å¼è§†è§‰ç”Ÿæˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.09669', 'title': 'Silent Branding Attack: Trigger-free Data Poisoning Attack on\n  Text-to-Image Diffusion Models', 'url': 'https://huggingface.co/papers/2503.09669', 'abstract': 'Text-to-image diffusion models have achieved remarkable success in generating high-quality contents from text prompts. However, their reliance on publicly available data and the growing trend of data sharing for fine-tuning make these models particularly vulnerable to data poisoning attacks. In this work, we introduce the Silent Branding Attack, a novel data poisoning method that manipulates text-to-image diffusion models to generate images containing specific brand logos or symbols without any text triggers. We find that when certain visual patterns are repeatedly in the training data, the model learns to reproduce them naturally in its outputs, even without prompt mentions. Leveraging this, we develop an automated data poisoning algorithm that unobtrusively injects logos into original images, ensuring they blend naturally and remain undetected. Models trained on this poisoned dataset generate images containing logos without degrading image quality or text alignment. We experimentally validate our silent branding attack across two realistic settings on large-scale high-quality image datasets and style personalization datasets, achieving high success rates even without a specific text trigger. Human evaluation and quantitative metrics including logo detection show that our method can stealthily embed logos.', 'score': 31, 'issue_id': 2701, 'pub_date': '2025-03-12', 'pub_date_card': {'ru': '12 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 12', 'zh': '3æœˆ12æ—¥'}, 'hash': 'f54f510a4cf48d22', 'authors': ['Sangwon Jang', 'June Suk Choi', 'Jaehyeong Jo', 'Kimin Lee', 'Sung Ju Hwang'], 'affiliations': ['DeepAuto.ai', 'KAIST'], 'pdf_title_img': 'assets/pdf/title_img/2503.09669.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#security', '#diffusion', '#data'], 'emoji': 'ğŸ•µï¸', 'ru': {'title': 'ĞĞµĞ²Ğ¸Ğ´Ğ¸Ğ¼Ñ‹Ğµ Ğ»Ğ¾Ğ³Ğ¾Ñ‚Ğ¸Ğ¿Ñ‹: ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğµ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ğµ Ğ±Ñ€ĞµĞ½Ğ´Ğ¾Ğ² Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸', 'desc': "Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ°Ñ‚Ğ°ĞºĞ¸ Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¾Ñ‚Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞœĞµÑ‚Ğ¾Ğ´, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ 'Ğ¢Ğ¸Ñ…Ğ°Ñ Ğ±Ñ€ĞµĞ½Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ²Ğ°Ñ Ğ°Ñ‚Ğ°ĞºĞ°', Ğ·Ğ°ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ»Ğ¾Ğ³Ğ¾Ñ‚Ğ¸Ğ¿Ğ°Ğ¼Ğ¸ Ğ±ĞµĞ· ÑĞ²Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ñ‚Ñ€Ğ¸Ğ³Ğ³ĞµÑ€Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ½ĞµĞ·Ğ°Ğ¼ĞµÑ‚Ğ½Ğ¾ Ğ²Ğ½ĞµĞ´Ñ€ÑĞµÑ‚ Ğ»Ğ¾Ğ³Ğ¾Ñ‚Ğ¸Ğ¿Ñ‹ Ğ² Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ½Ğ° ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¾ ĞºĞ°Ğº Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¾Ğ¹, Ñ‚Ğ°Ğº Ğ¸ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼Ğ¸."}, 'en': {'title': 'Stealthy Logo Injection in Image Generation', 'desc': "This paper presents a new method called the Silent Branding Attack, which targets text-to-image diffusion models by subtly injecting brand logos into training data. The attack exploits the model's tendency to reproduce visual patterns it has seen, allowing logos to appear in generated images without any explicit text prompts. The authors developed an automated algorithm that seamlessly integrates these logos into original images, making them difficult to detect. Their experiments demonstrate that models trained on this manipulated data can produce high-quality images containing logos, achieving significant success rates in various settings."}, 'zh': {'title': 'é™é»˜å“ç‰Œæ”»å‡»ï¼šéšç§˜æ¤å…¥å“ç‰Œæ ‡å¿—çš„åˆ›æ–°æ–¹æ³•', 'desc': 'æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹åœ¨æ ¹æ®æ–‡æœ¬æç¤ºç”Ÿæˆé«˜è´¨é‡å†…å®¹æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆåŠŸã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹ä¾èµ–äºå…¬å¼€æ•°æ®ï¼Œå¹¶ä¸”æ•°æ®å…±äº«çš„è¶‹åŠ¿ä½¿å…¶ç‰¹åˆ«å®¹æ˜“å—åˆ°æ•°æ®ä¸­æ¯’æ”»å‡»ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ•°æ®ä¸­æ¯’æ–¹æ³•â€”â€”é™é»˜å“ç‰Œæ”»å‡»ï¼Œèƒ½å¤Ÿæ“æ§æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹ç”ŸæˆåŒ…å«ç‰¹å®šå“ç‰Œæ ‡å¿—æˆ–ç¬¦å·çš„å›¾åƒï¼Œè€Œæ— éœ€ä»»ä½•æ–‡æœ¬è§¦å‘ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ç§è‡ªåŠ¨åŒ–çš„æ•°æ®ä¸­æ¯’ç®—æ³•ï¼Œå¯ä»¥åœ¨åŸå§‹å›¾åƒä¸­æ‚„æ— å£°æ¯åœ°æ³¨å…¥æ ‡å¿—ï¼Œç¡®ä¿å®ƒä»¬è‡ªç„¶èåˆä¸”ä¸è¢«æ£€æµ‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.10291', 'title': 'VisualPRM: An Effective Process Reward Model for Multimodal Reasoning', 'url': 'https://huggingface.co/papers/2503.10291', 'abstract': 'We introduce VisualPRM, an advanced multimodal Process Reward Model (PRM) with 8B parameters, which improves the reasoning abilities of existing Multimodal Large Language Models (MLLMs) across different model scales and families with Best-of-N (BoN) evaluation strategies. Specifically, our model improves the reasoning performance of three types of MLLMs and four different model scales. Even when applied to the highly capable InternVL2.5-78B, it achieves a 5.9-point improvement across seven multimodal reasoning benchmarks. Experimental results show that our model exhibits superior performance compared to Outcome Reward Models and Self-Consistency during BoN evaluation. To facilitate the training of multimodal PRMs, we construct a multimodal process supervision dataset VisualPRM400K using an automated data pipeline. For the evaluation of multimodal PRMs, we propose VisualProcessBench, a benchmark with human-annotated step-wise correctness labels, to measure the abilities of PRMs to detect erroneous steps in multimodal reasoning tasks. We hope that our work can inspire more future research and contribute to the development of MLLMs. Our model, data, and benchmark are released in https://internvl.github.io/blog/2025-03-13-VisualPRM/.', 'score': 28, 'issue_id': 2705, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 13', 'zh': '3æœˆ13æ—¥'}, 'hash': 'f4734440c38ca048', 'authors': ['Weiyun Wang', 'Zhangwei Gao', 'Lianjie Chen', 'Zhe Chen', 'Jinguo Zhu', 'Xiangyu Zhao', 'Yangzhou Liu', 'Yue Cao', 'Shenglong Ye', 'Xizhou Zhu', 'Lewei Lu', 'Haodong Duan', 'Yu Qiao', 'Jifeng Dai', 'Wenhai Wang'], 'affiliations': ['Fudan University', 'Nanjing University', 'SenseTime Research', 'Shanghai AI Laboratory', 'Shanghai Jiaotong University', 'The Chinese University of Hong Kong', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.10291.jpg', 'data': {'categories': ['#dataset', '#open_source', '#benchmark', '#reasoning', '#multimodal', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'VisualPRM: ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°', 'desc': 'VisualPRM - ÑÑ‚Ğ¾ ÑƒÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ñ 8 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞĞ½Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¾Ğ² Ğ¸ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ² Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Best-of-N. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¸ ÑĞ°Ğ¼Ğ¾ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¿Ñ€Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞµ BoN. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… PRM Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… VisualPRM400K Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº VisualProcessBench.'}, 'en': {'title': 'Enhancing Multimodal Reasoning with VisualPRM', 'desc': 'VisualPRM is a multimodal Process Reward Model designed to enhance the reasoning capabilities of Multimodal Large Language Models (MLLMs) with 8 billion parameters. It demonstrates significant improvements in reasoning performance across various model scales and types, achieving a notable 5.9-point increase on seven multimodal reasoning benchmarks, even with high-capacity models like InternVL2.5-78B. The model outperforms traditional Outcome Reward Models and Self-Consistency methods during Best-of-N evaluations. Additionally, VisualPRM introduces a new dataset, VisualPRM400K, and a benchmark, VisualProcessBench, to support the training and evaluation of multimodal PRMs.'}, 'zh': {'title': 'æå‡å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›çš„VisualPRMæ¨¡å‹', 'desc': 'æˆ‘ä»¬ä»‹ç»äº†ä¸€ç§å…ˆè¿›çš„å¤šæ¨¡æ€è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰VisualPRMï¼Œå…·æœ‰80äº¿ä¸ªå‚æ•°ï¼Œèƒ½å¤Ÿæå‡ç°æœ‰å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚è¯¥æ¨¡å‹åœ¨ä¸‰ç§ç±»å‹çš„MLLMså’Œå››ç§ä¸åŒçš„æ¨¡å‹è§„æ¨¡ä¸Šå‡è¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶æ˜¯åœ¨å¼ºå¤§çš„InternVL2.5-78Bæ¨¡å‹ä¸Šï¼Œæ¨ç†æ€§èƒ½æé«˜äº†5.9åˆ†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVisualPRMåœ¨æœ€ä½³é€‰æ‹©ï¼ˆBoNï¼‰è¯„ä¼°ä¸­ä¼˜äºç»“æœå¥–åŠ±æ¨¡å‹å’Œè‡ªä¸€è‡´æ€§æ–¹æ³•ã€‚ä¸ºäº†æ”¯æŒå¤šæ¨¡æ€PRMsçš„è®­ç»ƒï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåä¸ºVisualPRM400Kçš„å¤šæ¨¡æ€è¿‡ç¨‹ç›‘ç£æ•°æ®é›†ï¼Œå¹¶æå‡ºäº†VisualProcessBenchåŸºå‡†ï¼Œä»¥è¯„ä¼°PRMsåœ¨å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ä¸­æ£€æµ‹é”™è¯¯æ­¥éª¤çš„èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.09662', 'title': 'CoRe^2: Collect, Reflect and Refine to Generate Better and Faster', 'url': 'https://huggingface.co/papers/2503.09662', 'abstract': "Making text-to-image (T2I) generative model sample both fast and well represents a promising research direction. Previous studies have typically focused on either enhancing the visual quality of synthesized images at the expense of sampling efficiency or dramatically accelerating sampling without improving the base model's generative capacity. Moreover, nearly all inference methods have not been able to ensure stable performance simultaneously on both diffusion models (DMs) and visual autoregressive models (ARMs). In this paper, we introduce a novel plug-and-play inference paradigm, CoRe^2, which comprises three subprocesses: Collect, Reflect, and Refine. CoRe^2 first collects classifier-free guidance (CFG) trajectories, and then use collected data to train a weak model that reflects the easy-to-learn contents while reducing number of function evaluations during inference by half. Subsequently, CoRe^2 employs weak-to-strong guidance to refine the conditional output, thereby improving the model's capacity to generate high-frequency and realistic content, which is difficult for the base model to capture. To the best of our knowledge, CoRe^2 is the first to demonstrate both efficiency and effectiveness across a wide range of DMs, including SDXL, SD3.5, and FLUX, as well as ARMs like LlamaGen. It has exhibited significant performance improvements on HPD v2, Pick-of-Pic, Drawbench, GenEval, and T2I-Compbench. Furthermore, CoRe^2 can be seamlessly integrated with the state-of-the-art Z-Sampling, outperforming it by 0.3 and 0.16 on PickScore and AES, while achieving 5.64s time saving using SD3.5.Code is released at https://github.com/xie-lab-ml/CoRe/tree/main.", 'score': 28, 'issue_id': 2705, 'pub_date': '2025-03-12', 'pub_date_card': {'ru': '12 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 12', 'zh': '3æœˆ12æ—¥'}, 'hash': 'a6fdb00ac6a8ebca', 'authors': ['Shitong Shao', 'Zikai Zhou', 'Dian Xie', 'Yuetong Fang', 'Tian Ye', 'Lichen Bai', 'Zeke Xie'], 'affiliations': ['The Hong Kong University of Science and Technology (GuangZhou)'], 'pdf_title_img': 'assets/pdf/title_img/2503.09662.jpg', 'data': {'categories': ['#cv', '#benchmark', '#inference', '#diffusion'], 'emoji': 'ğŸ¨', 'ru': {'title': 'CoRe^2: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ¹ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ CoRe^2 Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‚ĞµĞºÑÑ‚-Ğ²-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ. ĞœĞµÑ‚Ğ¾Ğ´ ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ñ‚Ñ€ĞµÑ… ÑÑ‚Ğ°Ğ¿Ğ¾Ğ²: ÑĞ±Ğ¾Ñ€ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ±ĞµĞ·ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ°, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑĞ»Ğ°Ğ±Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ° Ğ¾Ñ‚ ÑĞ»Ğ°Ğ±Ğ¾Ğ¹ Ğº ÑĞ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. CoRe^2 Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'CoRe^2: Fast and High-Quality Text-to-Image Generation', 'desc': 'This paper presents CoRe^2, a new method for improving the speed and quality of text-to-image (T2I) generative models. It introduces a three-step process: Collect, Reflect, and Refine, which enhances sampling efficiency while maintaining high visual fidelity. By using classifier-free guidance trajectories, CoRe^2 trains a weak model that simplifies the learning process and reduces function evaluations during inference. The method shows significant performance gains across various diffusion models and autoregressive models, making it a notable advancement in the field of generative modeling.'}, 'zh': {'title': 'é«˜æ•ˆç”Ÿæˆï¼ŒçœŸå®å›¾åƒï¼', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹æ¨ç†èŒƒå¼ï¼Œç§°ä¸ºCoRe^2ã€‚è¯¥æ–¹æ³•é€šè¿‡ä¸‰ä¸ªå­è¿‡ç¨‹ï¼šæ”¶é›†ã€åå°„å’Œç²¾ç‚¼ï¼Œæ¥æé«˜ç”Ÿæˆæ•ˆç‡å’Œæ•ˆæœã€‚CoRe^2é¦–å…ˆæ”¶é›†æ— åˆ†ç±»å™¨å¼•å¯¼è½¨è¿¹ï¼Œç„¶åè®­ç»ƒä¸€ä¸ªå¼±æ¨¡å‹ä»¥å‡å°‘æ¨ç†è¿‡ç¨‹ä¸­çš„å‡½æ•°è¯„ä¼°æ¬¡æ•°ã€‚æœ€åï¼Œé€šè¿‡å¼±åˆ°å¼ºçš„å¼•å¯¼æ¥ç²¾ç‚¼æ¡ä»¶è¾“å‡ºï¼Œä»è€Œç”Ÿæˆæ›´é«˜é¢‘ç‡å’Œæ›´çœŸå®çš„å†…å®¹ï¼Œæ˜¾è‘—æå‡äº†å¤šç§æ‰©æ•£æ¨¡å‹å’Œè‡ªå›å½’æ¨¡å‹çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.10437', 'title': '4D LangSplat: 4D Language Gaussian Splatting via Multimodal Large\n  Language Models', 'url': 'https://huggingface.co/papers/2503.10437', 'abstract': 'Learning 4D language fields to enable time-sensitive, open-ended language queries in dynamic scenes is essential for many real-world applications. While LangSplat successfully grounds CLIP features into 3D Gaussian representations, achieving precision and efficiency in 3D static scenes, it lacks the ability to handle dynamic 4D fields as CLIP, designed for static image-text tasks, cannot capture temporal dynamics in videos. Real-world environments are inherently dynamic, with object semantics evolving over time. Building a precise 4D language field necessitates obtaining pixel-aligned, object-wise video features, which current vision models struggle to achieve. To address these challenges, we propose 4D LangSplat, which learns 4D language fields to handle time-agnostic or time-sensitive open-vocabulary queries in dynamic scenes efficiently. 4D LangSplat bypasses learning the language field from vision features and instead learns directly from text generated from object-wise video captions via Multimodal Large Language Models (MLLMs). Specifically, we propose a multimodal object-wise video prompting method, consisting of visual and text prompts that guide MLLMs to generate detailed, temporally consistent, high-quality captions for objects throughout a video. These captions are encoded using a Large Language Model into high-quality sentence embeddings, which then serve as pixel-aligned, object-specific feature supervision, facilitating open-vocabulary text queries through shared embedding spaces. Recognizing that objects in 4D scenes exhibit smooth transitions across states, we further propose a status deformable network to model these continuous changes over time effectively. Our results across multiple benchmarks demonstrate that 4D LangSplat attains precise and efficient results for both time-sensitive and time-agnostic open-vocabulary queries.', 'score': 22, 'issue_id': 2702, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 13', 'zh': '3æœˆ13æ—¥'}, 'hash': '212017b2c934863c', 'authors': ['Wanhua Li', 'Renping Zhou', 'Jiawei Zhou', 'Yingwei Song', 'Johannes Herter', 'Minghan Qin', 'Gao Huang', 'Hanspeter Pfister'], 'affiliations': ['Brown University', 'ETH Zurich', 'Harvard University', 'Stony Brook University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.10437.jpg', 'data': {'categories': ['#3d', '#video', '#optimization', '#multimodal', '#interpretability', '#reasoning', '#games', '#agi', '#transfer_learning'], 'emoji': 'ğŸŒ', 'ru': {'title': '4D ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¿Ğ¾Ğ»Ñ Ğ´Ğ»Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ†ĞµĞ½: Ğ¾Ñ‚ ÑÑ‚Ğ°Ñ‚Ğ¸ĞºĞ¸ Ğº Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞµ', 'desc': '4D LangSplat - ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ 4D ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¿Ğ¾Ğ»ĞµĞ¹, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ² Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ†ĞµĞ½Ğ°Ñ… Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (MLLM) Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ·Ğ°Ñ‚ĞµĞ¼ ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ² Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ LLM. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑĞµÑ‚ÑŒ Ğ´ĞµÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ»Ğ°Ğ²Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. 4D LangSplat Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² ĞºĞ°Ğº Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸, Ñ‚Ğ°Ğº Ğ¸ Ğ±ĞµĞ· Ğ½ĞµĞ³Ğ¾.'}, 'en': {'title': 'Empowering Dynamic Scene Understanding with 4D Language Fields', 'desc': 'This paper introduces 4D LangSplat, a novel approach for creating 4D language fields that can handle dynamic scenes in videos. Unlike previous models that focus on static images, 4D LangSplat learns directly from text generated by Multimodal Large Language Models (MLLMs) using object-wise video captions. The method employs a multimodal prompting technique to produce high-quality, temporally consistent captions, which are then transformed into sentence embeddings for effective querying. Additionally, a status deformable network is proposed to accurately model the continuous changes of objects over time, resulting in improved performance for both time-sensitive and time-agnostic queries.'}, 'zh': {'title': 'åŠ¨æ€åœºæ™¯ä¸­çš„4Dè¯­è¨€åœºå­¦ä¹ ', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œç§°ä¸º4D LangSplatï¼Œæ—¨åœ¨å¤„ç†åŠ¨æ€åœºæ™¯ä¸­çš„æ—¶é—´æ•æ„Ÿå’Œå¼€æ”¾è¯æ±‡æŸ¥è¯¢ã€‚ä¸ç°æœ‰çš„é™æ€å›¾åƒ-æ–‡æœ¬æ¨¡å‹ä¸åŒï¼Œ4D LangSplatèƒ½å¤Ÿå­¦ä¹ 4Dè¯­è¨€åœºï¼Œç›´æ¥ä»å¯¹è±¡è§†é¢‘å­—å¹•ç”Ÿæˆçš„æ–‡æœ¬ä¸­è·å–ä¿¡æ¯ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ç”Ÿæˆé«˜è´¨é‡çš„æ—¶é—´ä¸€è‡´æ€§å­—å¹•ï¼Œå¹¶å°†å…¶ç¼–ç ä¸ºå¥å­åµŒå…¥ï¼Œä»¥æ”¯æŒåƒç´ å¯¹é½çš„å¯¹è±¡ç‰¹å¾ç›‘ç£ã€‚é€šè¿‡å¼•å…¥çŠ¶æ€å¯å˜å½¢ç½‘ç»œï¼Œ4D LangSplatæœ‰æ•ˆå»ºæ¨¡äº†å¯¹è±¡åœ¨æ—¶é—´ä¸Šçš„è¿ç»­å˜åŒ–ï¼Œå±•ç¤ºäº†åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—çš„ç²¾ç¡®å’Œé«˜æ•ˆçš„æŸ¥è¯¢ç»“æœã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.08677', 'title': 'OmniPaint: Mastering Object-Oriented Editing via Disentangled\n  Insertion-Removal Inpainting', 'url': 'https://huggingface.co/papers/2503.08677', 'abstract': 'Diffusion-based generative models have revolutionized object-oriented image editing, yet their deployment in realistic object removal and insertion remains hampered by challenges such as the intricate interplay of physical effects and insufficient paired training data. In this work, we introduce OmniPaint, a unified framework that re-conceptualizes object removal and insertion as interdependent processes rather than isolated tasks. Leveraging a pre-trained diffusion prior along with a progressive training pipeline comprising initial paired sample optimization and subsequent large-scale unpaired refinement via CycleFlow, OmniPaint achieves precise foreground elimination and seamless object insertion while faithfully preserving scene geometry and intrinsic properties. Furthermore, our novel CFD metric offers a robust, reference-free evaluation of context consistency and object hallucination, establishing a new benchmark for high-fidelity image editing. Project page: https://yeates.github.io/OmniPaint-Page/', 'score': 22, 'issue_id': 2712, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 11', 'zh': '3æœˆ11æ—¥'}, 'hash': '6110f983d46b536a', 'authors': ['Yongsheng Yu', 'Ziyun Zeng', 'Haitian Zheng', 'Jiebo Luo'], 'affiliations': ['Adobe Research', 'University of Rochester'], 'pdf_title_img': 'assets/pdf/title_img/2503.08677.jpg', 'data': {'categories': ['#cv', '#dataset', '#diffusion', '#hallucinations', '#benchmark'], 'emoji': 'ğŸ¨', 'ru': {'title': 'OmniPaint: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜', 'desc': 'OmniPaint - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ½Ğ° Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ²ÑÑ‚Ğ°Ğ²ĞºÑƒ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² ĞºĞ°Ğº Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. OmniPaint Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ Ğ¿ĞµÑ€ĞµĞ´Ğ½ĞµĞ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ° Ğ¸ Ğ±ĞµÑÑˆĞ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ²ÑÑ‚Ğ°Ğ²ĞºĞ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ ÑÑ†ĞµĞ½Ñ‹ Ğ¸ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğµ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ CFD Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ².'}, 'en': {'title': 'OmniPaint: Seamless Object Editing through Interconnected Processes', 'desc': "This paper presents OmniPaint, a new framework for object removal and insertion in images, treating these tasks as interconnected rather than separate. It utilizes a pre-trained diffusion model and a two-step training process that starts with paired samples and then refines the model using unpaired data. OmniPaint effectively removes unwanted objects and adds new ones while maintaining the original scene's geometry and characteristics. Additionally, the authors introduce a new evaluation metric called CFD, which measures the consistency and realism of the edited images without needing reference images."}, 'zh': {'title': 'OmniPaintï¼šç‰©ä½“ç¼–è¾‘çš„æ–°çºªå…ƒ', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºOmniPaintçš„ç»Ÿä¸€æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç‰©ä½“ç§»é™¤å’Œæ’å…¥çš„å¤æ‚é—®é¢˜ã€‚è¯¥æ¡†æ¶å°†ç‰©ä½“ç§»é™¤å’Œæ’å…¥è§†ä¸ºç›¸äº’ä¾èµ–çš„è¿‡ç¨‹ï¼Œè€Œéå­¤ç«‹çš„ä»»åŠ¡ã€‚é€šè¿‡åˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹å’Œé€æ­¥è®­ç»ƒæµç¨‹ï¼ŒOmniPaintèƒ½å¤Ÿå®ç°ç²¾ç¡®çš„å‰æ™¯æ¶ˆé™¤å’Œæ— ç¼çš„ç‰©ä½“æ’å…¥ï¼ŒåŒæ—¶ä¿æŒåœºæ™¯çš„å‡ ä½•å½¢çŠ¶å’Œå†…åœ¨ç‰¹æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºçš„CFDæŒ‡æ ‡ä¸ºä¸Šä¸‹æ–‡ä¸€è‡´æ€§å’Œç‰©ä½“å¹»è§‰æä¾›äº†ä¸€ç§å¼ºå¥çš„æ— å‚è€ƒè¯„ä¼°æ–¹æ³•ï¼Œå»ºç«‹äº†é«˜ä¿çœŸå›¾åƒç¼–è¾‘çš„æ–°åŸºå‡†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.10596', 'title': 'GroundingSuite: Measuring Complex Multi-Granular Pixel Grounding', 'url': 'https://huggingface.co/papers/2503.10596', 'abstract': 'Pixel grounding, encompassing tasks such as Referring Expression Segmentation (RES), has garnered considerable attention due to its immense potential for bridging the gap between vision and language modalities. However, advancements in this domain are currently constrained by limitations inherent in existing datasets, including limited object categories, insufficient textual diversity, and a scarcity of high-quality annotations. To mitigate these limitations, we introduce GroundingSuite, which comprises: (1) an automated data annotation framework leveraging multiple Vision-Language Model (VLM) agents; (2) a large-scale training dataset encompassing 9.56 million diverse referring expressions and their corresponding segmentations; and (3) a meticulously curated evaluation benchmark consisting of 3,800 images. The GroundingSuite training dataset facilitates substantial performance improvements, enabling models trained on it to achieve state-of-the-art results. Specifically, a cIoU of 68.9 on gRefCOCO and a gIoU of 55.3 on RefCOCOm. Moreover, the GroundingSuite annotation framework demonstrates superior efficiency compared to the current leading data annotation method, i.e., 4.5 times faster than the GLaMM.', 'score': 18, 'issue_id': 2701, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 13', 'zh': '3æœˆ13æ—¥'}, 'hash': '1b9ea337d198c741', 'authors': ['Rui Hu', 'Lianghui Zhu', 'Yuxuan Zhang', 'Tianheng Cheng', 'Lei Liu', 'Heng Liu', 'Longjin Ran', 'Xiaoxin Chen', 'Wenyu Liu', 'Xinggang Wang'], 'affiliations': ['School of EIC, Huazhong University of Science & Technology', 'vivo AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2503.10596.jpg', 'data': {'categories': ['#benchmark', '#data', '#dataset'], 'emoji': 'ğŸ”', 'ru': {'title': 'GroundingSuite: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ GroundingSuite - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° Ğ¸ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ. GroundingSuite Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ 9,56 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ğ¼Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ²Ñ‹Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¸Ğ¼ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¹, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸Ğ· 3800 Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° ÑÑ‚Ğ¾Ğ¼ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'Revolutionizing Pixel Grounding with GroundingSuite', 'desc': 'This paper introduces GroundingSuite, a new framework designed to enhance pixel grounding tasks like Referring Expression Segmentation (RES) by addressing the limitations of existing datasets. GroundingSuite features an automated data annotation system that utilizes multiple Vision-Language Model (VLM) agents, resulting in a large-scale dataset with 9.56 million diverse referring expressions and their segmentations. The framework not only improves the quality and diversity of training data but also provides a curated evaluation benchmark with 3,800 images. Models trained on this dataset achieve state-of-the-art performance, significantly outperforming previous methods in both efficiency and accuracy.'}, 'zh': {'title': 'GroundingSuiteï¼šæå‡è§†è§‰ä¸è¯­è¨€çš„æ¡¥æ¢', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºGroundingSuiteçš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ•°æ®é›†åœ¨åƒç´ å®šä½ä»»åŠ¡ä¸­çš„å±€é™æ€§ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬ä¸€ä¸ªè‡ªåŠ¨åŒ–æ•°æ®æ³¨é‡Šç³»ç»Ÿï¼Œåˆ©ç”¨å¤šä¸ªè§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ä»£ç†ç”Ÿæˆæ•°æ®ã€‚GroundingSuiteæä¾›äº†ä¸€ä¸ªåŒ…å«956ä¸‡ç§å¤šæ ·åŒ–æŒ‡ç§°è¡¨è¾¾åŠå…¶å¯¹åº”åˆ†å‰²çš„å¤§è§„æ¨¡è®­ç»ƒæ•°æ®é›†ï¼Œå¹¶å»ºç«‹äº†ä¸€ä¸ªåŒ…å«3800å¼ å›¾åƒçš„è¯„ä¼°åŸºå‡†ã€‚é€šè¿‡ä½¿ç”¨GroundingSuiteè®­ç»ƒçš„æ•°æ®é›†ï¼Œæ¨¡å‹çš„æ€§èƒ½æ˜¾è‘—æå‡ï¼Œè¾¾åˆ°äº†æœ€æ–°çš„ç ”ç©¶æˆæœã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.10351', 'title': 'New Trends for Modern Machine Translation with Large Reasoning Models', 'url': 'https://huggingface.co/papers/2503.10351', 'abstract': 'Recent advances in Large Reasoning Models (LRMs), particularly those leveraging Chain-of-Thought reasoning (CoT), have opened brand new possibility for Machine Translation (MT). This position paper argues that LRMs substantially transformed traditional neural MT as well as LLMs-based MT paradigms by reframing translation as a dynamic reasoning task that requires contextual, cultural, and linguistic understanding and reasoning. We identify three foundational shifts: 1) contextual coherence, where LRMs resolve ambiguities and preserve discourse structure through explicit reasoning over cross-sentence and complex context or even lack of context; 2) cultural intentionality, enabling models to adapt outputs by inferring speaker intent, audience expectations, and socio-linguistic norms; 3) self-reflection, LRMs can perform self-reflection during the inference time to correct the potential errors in translation especially extremely noisy cases, showing better robustness compared to simply mapping X->Y translation. We explore various scenarios in translation including stylized translation, document-level translation and multimodal translation by showcasing empirical examples that demonstrate the superiority of LRMs in translation. We also identify several interesting phenomenons for LRMs for MT including auto-pivot translation as well as the critical challenges such as over-localisation in translation and inference efficiency. In conclusion, we think that LRMs redefine translation systems not merely as text converters but as multilingual cognitive agents capable of reasoning about meaning beyond the text. This paradigm shift reminds us to think of problems in translation beyond traditional translation scenarios in a much broader context with LRMs - what we can achieve on top of it.', 'score': 18, 'issue_id': 2700, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 13', 'zh': '3æœˆ13æ—¥'}, 'hash': 'c0a1b109c05698cc', 'authors': ['Sinuo Liu', 'Chenyang Lyu', 'Minghao Wu', 'Longyue Wang', 'Weihua Luo', 'Kaifu Zhang'], 'affiliations': ['MarcoPolo Team, Alibaba International Digital Commerce', 'University of Edinburgh'], 'pdf_title_img': 'assets/pdf/title_img/2503.10351.jpg', 'data': {'categories': ['#reasoning', '#agents', '#multilingual', '#machine_translation'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'LRM: ĞÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° Ğº Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (LRM) Ğ½Ğ° Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ñ‹Ğ¹ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ LRM Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒÑÑ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹, Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´ ĞºĞ°Ğº Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰ÑƒÑ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾, ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. Ğ’Ñ‹Ğ´ĞµĞ»ÑÑÑ‚ÑÑ Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ: ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ, ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ğ°Ñ Ğ¸Ğ½Ñ‚ĞµĞ½Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑĞ°Ğ¼Ğ¾Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ñ. Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ LRM Ğ² Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğµ Ğ¸ Ğ¾Ğ±ÑÑƒĞ¶Ğ´Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ÑÑ‰Ğ¸Ğµ Ñ„ĞµĞ½Ğ¾Ğ¼ĞµĞ½Ñ‹ Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹.'}, 'en': {'title': 'Transforming Translation: LRMs as Cognitive Agents', 'desc': 'This paper discusses how Large Reasoning Models (LRMs) are changing the way we think about Machine Translation (MT). It highlights three key shifts: first, LRMs improve contextual coherence by understanding complex contexts and resolving ambiguities; second, they incorporate cultural intentionality, allowing translations to reflect speaker intent and social norms; and third, LRMs can self-reflect during translation to correct errors, making them more robust. The authors provide examples of how LRMs excel in various translation scenarios, suggesting that these models should be seen as cognitive agents that reason about meaning rather than just text converters.'}, 'zh': {'title': 'å¤§å‹æ¨ç†æ¨¡å‹é‡å¡‘æœºå™¨ç¿»è¯‘çš„æœªæ¥', 'desc': 'å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰åœ¨æœºå™¨ç¿»è¯‘ï¼ˆMTï¼‰é¢†åŸŸå¸¦æ¥äº†æ–°çš„å¯èƒ½æ€§ï¼Œç‰¹åˆ«æ˜¯é€šè¿‡é“¾å¼æ€ç»´æ¨ç†ï¼ˆCoTï¼‰ã€‚è¿™ç¯‡è®ºæ–‡æŒ‡å‡ºï¼ŒLRMså°†ä¼ ç»Ÿç¥ç»æœºå™¨ç¿»è¯‘è½¬å˜ä¸ºä¸€ç§åŠ¨æ€æ¨ç†ä»»åŠ¡ï¼Œå¼ºè°ƒä¸Šä¸‹æ–‡ã€æ–‡åŒ–å’Œè¯­è¨€ç†è§£çš„é‡è¦æ€§ã€‚æˆ‘ä»¬è¯†åˆ«å‡ºä¸‰ä¸ªåŸºç¡€è½¬å˜ï¼šä¸Šä¸‹æ–‡è¿è´¯æ€§ã€æ–‡åŒ–æ„å›¾æ€§å’Œè‡ªæˆ‘åæ€èƒ½åŠ›ï¼Œä½¿å¾—LRMsåœ¨ç¿»è¯‘ä¸­è¡¨ç°å‡ºæ›´å¥½çš„é²æ£’æ€§ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬è®¤ä¸ºLRMsé‡æ–°å®šä¹‰äº†ç¿»è¯‘ç³»ç»Ÿï¼Œä½¿å…¶ä¸ä»…ä»…æ˜¯æ–‡æœ¬è½¬æ¢å™¨ï¼Œè€Œæ˜¯èƒ½å¤Ÿè¶…è¶Šæ–‡æœ¬è¿›è¡Œæ„ä¹‰æ¨ç†çš„å¤šè¯­è¨€è®¤çŸ¥ä»£ç†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.04723', 'title': 'Shifting Long-Context LLMs Research from Input to Output', 'url': 'https://huggingface.co/papers/2503.04723', 'abstract': 'Recent advancements in long-context Large Language Models (LLMs) have primarily concentrated on processing extended input contexts, resulting in significant strides in long-context comprehension. However, the equally critical aspect of generating long-form outputs has received comparatively less attention. This paper advocates for a paradigm shift in NLP research toward addressing the challenges of long-output generation. Tasks such as novel writing, long-term planning, and complex reasoning require models to understand extensive contexts and produce coherent, contextually rich, and logically consistent extended text. These demands highlight a critical gap in current LLM capabilities. We underscore the importance of this under-explored domain and call for focused efforts to develop foundational LLMs tailored for generating high-quality, long-form outputs, which hold immense potential for real-world applications.', 'score': 18, 'issue_id': 2700, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 6', 'zh': '3æœˆ6æ—¥'}, 'hash': '009f3e654e927dc3', 'authors': ['Yuhao Wu', 'Yushi Bai', 'Zhiqing Hu', 'Shangqing Tu', 'Ming Shan Hee', 'Juanzi Li', 'Roy Ka-Wei Lee'], 'affiliations': ['Singapore University', 'Singapore University of Technology and Design', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.04723.jpg', 'data': {'categories': ['#data', '#long_context', '#multimodal'], 'emoji': 'ğŸ“', 'ru': {'title': 'ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ²Ñ‹Ğ·Ğ¾Ğ² Ğ´Ğ»Ñ Ğ˜Ğ˜: ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ±ÑÑƒĞ¶Ğ´Ğ°ĞµÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ‚Ğ¼ĞµÑ‡Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¼ ÑĞ¾ÑÑ€ĞµĞ´Ğ¾Ñ‚Ğ¾Ñ‡ĞµĞ½Ñ‹ Ğ½Ğ° Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ², Ğ½Ğ¾ Ğ½Ğµ Ğ½Ğ° ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞ¼Ğ½Ñ‹Ñ… Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞŸĞ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ²ÑĞ·Ğ½Ñ‹Ğµ Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ñ‚ĞµĞºÑÑ‚Ñ‹. Ğ¢Ğ°ĞºĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ñ‹ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ½Ğ°Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ñ€Ğ¾Ğ¼Ğ°Ğ½Ğ¾Ğ², Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Bridging the Gap: Enhancing Long-Form Output in LLMs', 'desc': 'This paper discusses the need for improvements in long-form output generation by Large Language Models (LLMs). While recent advancements have focused on understanding long input contexts, generating coherent and contextually rich long outputs remains underexplored. The authors emphasize that tasks like novel writing and complex reasoning require models to produce logically consistent extended text. They call for more research efforts to develop LLMs that can effectively handle these long-output challenges, which are crucial for various real-world applications.'}, 'zh': {'title': 'æ¨åŠ¨é•¿æ–‡æœ¬ç”Ÿæˆçš„ç ”ç©¶è½¬å‹', 'desc': 'æœ€è¿‘ï¼Œé•¿ä¸Šä¸‹æ–‡çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†æ‰©å±•è¾“å…¥ä¸Šä¸‹æ–‡æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ç”Ÿæˆé•¿æ–‡æœ¬è¾“å‡ºçš„ç ”ç©¶ç›¸å¯¹è¾ƒå°‘ã€‚æœ¬æ–‡æå€¡è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ç ”ç©¶å‘è§£å†³é•¿è¾“å‡ºç”Ÿæˆçš„æŒ‘æˆ˜è½¬å˜ã€‚é•¿ç¯‡å°è¯´å†™ä½œã€é•¿æœŸè§„åˆ’å’Œå¤æ‚æ¨ç†ç­‰ä»»åŠ¡éœ€è¦æ¨¡å‹ç†è§£å¹¿æ³›çš„ä¸Šä¸‹æ–‡ï¼Œå¹¶ç”Ÿæˆè¿è´¯ã€ä¸°å¯Œä¸”é€»è¾‘ä¸€è‡´çš„æ‰©å±•æ–‡æœ¬ã€‚æˆ‘ä»¬å¼ºè°ƒè¿™ä¸€æœªè¢«å……åˆ†æ¢ç´¢é¢†åŸŸçš„é‡è¦æ€§ï¼Œå¹¶å‘¼åé›†ä¸­åŠ›é‡å¼€å‘ä¸“é—¨ç”¨äºç”Ÿæˆé«˜è´¨é‡é•¿æ–‡æœ¬è¾“å‡ºçš„åŸºç¡€LLMï¼Œä»¥æ»¡è¶³ç°å®åº”ç”¨çš„å·¨å¤§æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.10460', 'title': 'Light-R1: Curriculum SFT, DPO and RL for Long COT from Scratch and\n  Beyond', 'url': 'https://huggingface.co/papers/2503.10460', 'abstract': 'This paper presents our work on the Light-R1 series, with models, data, and code all released.   We first focus on training long COT models from scratch, specifically starting from models initially lacking long COT capabilities. Using a curriculum training recipe consisting of two-stage SFT and semi-on-policy DPO, we train our model Light-R1-32B from Qwen2.5-32B-Instruct, resulting in superior math performance compared to DeepSeek-R1-Distill-Qwen-32B. Despite being trained exclusively on math data, Light-R1-32B shows strong generalization across other domains. In the subsequent phase of this work, we highlight the significant benefit of the 3k dataset constructed for the second SFT stage on enhancing other models. By fine-tuning DeepSeek-R1-Distilled models using this dataset, we obtain new SOTA models in 7B and 14B, while the 32B model, Light-R1-32B-DS performed comparably to QwQ-32B and DeepSeek-R1.   Furthermore, we extend our work by applying reinforcement learning, specifically GRPO, on long-COT models to further improve reasoning performance. We successfully train our final Light-R1-14B-DS with RL, achieving SOTA performance among 14B parameter models in math. With AIME24 & 25 scores of 74.0 and 60.2 respectively, Light-R1-14B-DS surpasses even many 32B models and DeepSeek-R1-Distill-Llama-70B. Its RL training also exhibits well expected behavior, showing simultaneous increase in response length and reward score.   The Light-R1 series of work validates training long-COT models from scratch, showcases the art in SFT data and releases SOTA models from RL.', 'score': 17, 'issue_id': 2701, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 13', 'zh': '3æœˆ13æ—¥'}, 'hash': '503c1d89e949bb41', 'authors': ['Liang Wen', 'Yunke Cai', 'Fenrui Xiao', 'Xin He', 'Qi An', 'Zhenyu Duan', 'Yimin Du', 'Junchen Liu', 'Lifu Tang', 'Xiaowei Lv', 'Haosheng Zou', 'Yongchao Deng', 'Shousheng Jia', 'Xiangzheng Zhang'], 'affiliations': ['Qiyuan Tech', 'Renmin University'], 'pdf_title_img': 'assets/pdf/title_img/2503.10460.jpg', 'data': {'categories': ['#rlhf', '#dataset', '#rl', '#reasoning', '#training', '#optimization', '#long_context', '#open_source'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ˜Ğ˜ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ°Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞµÑ€Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Light-R1, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (COT) Ñ Ğ½ÑƒĞ»Ñ. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ (SFT) Ğ¸ Ğ¿Ğ¾Ğ»Ñƒ-Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½Ğ¾Ğ²Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼Ğ¸ (DPO), Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Light-R1-32B, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‰ÑƒÑ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸ Ğ² Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. Ğ”Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞµĞµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL) Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Light-R1-14B-DS, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³ÑˆÑƒÑ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² ÑÑ€ĞµĞ´Ğ¸ 14B Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞµ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ COT Ñ Ğ½ÑƒĞ»Ñ Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ SFT.'}, 'en': {'title': 'Revolutionizing Long COT Models with Light-R1 Series', 'desc': "This paper introduces the Light-R1 series, focusing on training long Chain of Thought (COT) models from scratch. The authors employ a two-stage Supervised Fine-Tuning (SFT) and semi-on-policy Direct Preference Optimization (DPO) to enhance the model's math capabilities, resulting in the Light-R1-32B model that outperforms existing models. They also demonstrate that a specially constructed 3k dataset significantly boosts the performance of other models, achieving state-of-the-art (SOTA) results in various parameter sizes. Additionally, the application of reinforcement learning (RL) further improves reasoning performance, with the Light-R1-14B-DS model achieving impressive scores in math tasks, surpassing even larger models."}, 'zh': {'title': 'ä»é›¶å¼€å§‹è®­ç»ƒé•¿é“¾æ¨ç†æ¨¡å‹çš„æˆåŠŸä¹‹è·¯', 'desc': 'æœ¬æ–‡ä»‹ç»äº†Light-R1ç³»åˆ—çš„ç ”ç©¶å·¥ä½œï¼Œå‘å¸ƒäº†æ¨¡å‹ã€æ•°æ®å’Œä»£ç ã€‚æˆ‘ä»¬ä¸“æ³¨äºä»å¤´å¼€å§‹è®­ç»ƒé•¿é“¾æ¨ç†ï¼ˆCOTï¼‰æ¨¡å‹ï¼Œé‡‡ç”¨ä¸¤é˜¶æ®µçš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’ŒåŠåœ¨çº¿ç­–ç•¥ä¼˜åŒ–ï¼ˆDPOï¼‰è¿›è¡Œè¯¾ç¨‹è®­ç»ƒã€‚Light-R1-32Bæ¨¡å‹åœ¨æ•°å­¦æ€§èƒ½ä¸Šä¼˜äºDeepSeek-R1-Distill-Qwen-32Bï¼Œå°½ç®¡ä»…åœ¨æ•°å­¦æ•°æ®ä¸Šè®­ç»ƒï¼Œä½†åœ¨å…¶ä»–é¢†åŸŸä¹Ÿè¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿›ä¸€æ­¥æå‡æ¨ç†æ€§èƒ½ï¼Œæˆ‘ä»¬æˆåŠŸè®­ç»ƒäº†Light-R1-14B-DSï¼Œè¾¾åˆ°äº†14Bå‚æ•°æ¨¡å‹ä¸­çš„æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.10618', 'title': 'DiT-Air: Revisiting the Efficiency of Diffusion Model Architecture\n  Design in Text to Image Generation', 'url': 'https://huggingface.co/papers/2503.10618', 'abstract': 'In this work, we empirically study Diffusion Transformers (DiTs) for text-to-image generation, focusing on architectural choices, text-conditioning strategies, and training protocols. We evaluate a range of DiT-based architectures--including PixArt-style and MMDiT variants--and compare them with a standard DiT variant which directly processes concatenated text and noise inputs. Surprisingly, our findings reveal that the performance of standard DiT is comparable with those specialized models, while demonstrating superior parameter-efficiency, especially when scaled up. Leveraging the layer-wise parameter sharing strategy, we achieve a further reduction of 66% in model size compared to an MMDiT architecture, with minimal performance impact. Building on an in-depth analysis of critical components such as text encoders and Variational Auto-Encoders (VAEs), we introduce DiT-Air and DiT-Air-Lite. With supervised and reward fine-tuning, DiT-Air achieves state-of-the-art performance on GenEval and T2I CompBench, while DiT-Air-Lite remains highly competitive, surpassing most existing models despite its compact size.', 'score': 16, 'issue_id': 2704, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 13', 'zh': '3æœˆ13æ—¥'}, 'hash': '3e0a293a0bdb9c8c', 'authors': ['Chen Chen', 'Rui Qian', 'Wenze Hu', 'Tsu-Jui Fu', 'Lezhi Li', 'Bowen Zhang', 'Alex Schwing', 'Wei Liu', 'Yinfei Yang'], 'affiliations': ['Apple Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2503.10618.jpg', 'data': {'categories': ['#diffusion', '#architecture', '#dataset', '#training'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ”Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¢Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ñ‹ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ÑÑ Ğ”Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¢Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ñ‹ (DiT) Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸Ğ·ÑƒÑ‡Ğ°ÑÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ DiT, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ñ‹ PixArt Ğ¸ MMDiT, Ğ¸ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¸Ñ… ÑĞ¾ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¼ DiT. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¹ DiT ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼ Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¾ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ½Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²ĞµĞ½ Ğ¿Ğ¾ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¿Ğ¾ÑĞ»Ğ¾Ğ¹Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ´Ğ¾Ğ±Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞµĞ³Ğ¾ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° 66% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ MMDiT.'}, 'en': {'title': 'Efficient Image Generation with Diffusion Transformers', 'desc': 'This paper investigates Diffusion Transformers (DiTs) for generating images from text, examining different architectural designs and training methods. The authors compare various DiT models, including specialized versions and a standard model that combines text and noise inputs. They find that the standard DiT performs similarly to more complex models while being more efficient in terms of parameters, especially when scaled. Additionally, they introduce new models, DiT-Air and DiT-Air-Lite, which achieve top performance on benchmark tests while maintaining a smaller size.'}, 'zh': {'title': 'æ‰©æ•£å˜æ¢å™¨ï¼šé«˜æ•ˆçš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆ', 'desc': 'æœ¬æ–‡ç ”ç©¶äº†æ‰©æ•£å˜æ¢å™¨ï¼ˆDiTsï¼‰åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­çš„åº”ç”¨ï¼Œé‡ç‚¹å…³æ³¨æ¶æ„é€‰æ‹©ã€æ–‡æœ¬æ¡ä»¶ç­–ç•¥å’Œè®­ç»ƒåè®®ã€‚æˆ‘ä»¬è¯„ä¼°äº†å¤šç§åŸºäºDiTçš„æ¶æ„ï¼ŒåŒ…æ‹¬PixArté£æ ¼å’ŒMMDiTå˜ä½“ï¼Œå¹¶ä¸ç›´æ¥å¤„ç†æ–‡æœ¬å’Œå™ªå£°è¾“å…¥çš„æ ‡å‡†DiTå˜ä½“è¿›è¡Œäº†æ¯”è¾ƒã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæ ‡å‡†DiTçš„æ€§èƒ½ä¸è¿™äº›ä¸“ä¸šæ¨¡å‹ç›¸å½“ï¼ŒåŒæ—¶åœ¨å‚æ•°æ•ˆç‡ä¸Šè¡¨ç°æ›´ä½³ï¼Œå°¤å…¶æ˜¯åœ¨æ¨¡å‹è§„æ¨¡å¢å¤§æ—¶ã€‚é€šè¿‡å±‚çº§å‚æ•°å…±äº«ç­–ç•¥ï¼Œæˆ‘ä»¬å°†æ¨¡å‹å¤§å°è¿›ä¸€æ­¥å‡å°‘äº†66%ï¼Œå¯¹æ€§èƒ½å½±å“æå°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.10582', 'title': 'VisualWebInstruct: Scaling up Multimodal Instruction Data through Web\n  Search', 'url': 'https://huggingface.co/papers/2503.10582', 'abstract': "Vision-Language Models have made significant progress on many perception-focused tasks, however, their progress on reasoning-focused tasks seem to be limited due to the lack of high-quality and diverse training data. In this work, we aim to address the scarcity issue of reasoning-focused multimodal datasets. We propose VisualWebInstruct - a novel approach that leverages search engine to create a diverse, and high-quality dataset spanning multiple disciplines like math, physics, finance, chemistry, etc. Starting with meticulously selected 30,000 seed images, we employ Google Image search to identify websites containing similar images. We collect and process the HTMLs from over 700K unique URL sources. Through a pipeline of content extraction, filtering and synthesis, we build a dataset of approximately 900K question-answer pairs, with 40% being visual QA pairs and the rest as text QA pairs. Models fine-tuned on VisualWebInstruct demonstrate significant performance gains: (1) training from Llava-OV-mid shows 10-20% absolute point gains across benchmarks, (2) training from MAmmoTH-VL shows 5% absoluate gain. Our best model MAmmoTH-VL2 shows state-of-the-art performance within the 10B parameter class on MMMU-Pro-std (40.7%), MathVerse (42.6%), and DynaMath (55.7%). These remarkable results highlight the effectiveness of our dataset in enhancing VLMs' reasoning capabilities for complex multimodal tasks.", 'score': 16, 'issue_id': 2700, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 13', 'zh': '3æœˆ13æ—¥'}, 'hash': 'ff1e79c1589f8602', 'authors': ['Yiming Jia', 'Jiachen Li', 'Xiang Yue', 'Bo Li', 'Ping Nie', 'Kai Zou', 'Wenhu Chen'], 'affiliations': ['CMU', 'Independent', 'NUS', 'Netmind.ai', 'UC Santa Barbara', 'University of Toronto', 'University of Waterloo'], 'pdf_title_img': 'assets/pdf/title_img/2503.10582.jpg', 'data': {'categories': ['#dataset', '#data', '#reasoning', '#multimodal'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'VisualWebInstruct: Ğ¿Ñ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ VisualWebInstruct - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ HTML-ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†, Ğ¾Ğ½Ğ¸ ÑĞ¾Ğ±Ñ€Ğ°Ğ»Ğ¸ Ğ¾ĞºĞ¾Ğ»Ğ¾ 900 Ñ‚Ñ‹ÑÑÑ‡ Ğ¿Ğ°Ñ€ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ-Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ¿Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ´Ğ¸ÑÑ†Ğ¸Ğ¿Ğ»Ğ¸Ğ½Ğ°Ğ¼. ĞœĞ¾Ğ´ĞµĞ»Ğ¸, Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° ÑÑ‚Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…. Ğ›ÑƒÑ‡ÑˆĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ MAmmoTH-VL2 Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ° state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² ÑĞ²Ğ¾ĞµĞ¼ ĞºĞ»Ğ°ÑÑĞµ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡.'}, 'en': {'title': 'Enhancing Reasoning in Vision-Language Models with VisualWebInstruct', 'desc': 'This paper introduces VisualWebInstruct, a new dataset aimed at improving the reasoning abilities of Vision-Language Models (VLMs). The dataset is created by leveraging search engines to gather diverse and high-quality multimodal data, specifically focusing on reasoning tasks across various disciplines. By processing over 700,000 unique web sources, the authors compile approximately 900,000 question-answer pairs, enhancing the training data available for VLMs. The results show that models trained on this dataset achieve significant performance improvements on reasoning benchmarks, demonstrating its effectiveness in advancing VLM capabilities.'}, 'zh': {'title': 'æå‡è§†è§‰è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„æ–°æ•°æ®é›†', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•VisualWebInstructï¼Œæ—¨åœ¨è§£å†³æ¨ç†å¯¼å‘çš„å¤šæ¨¡æ€æ•°æ®é›†ç¨€ç¼ºé—®é¢˜ã€‚æˆ‘ä»¬åˆ©ç”¨æœç´¢å¼•æ“åˆ›å»ºäº†ä¸€ä¸ªå¤šå­¦ç§‘çš„é«˜è´¨é‡æ•°æ®é›†ï¼ŒåŒ…æ‹¬æ•°å­¦ã€ç‰©ç†ã€é‡‘èå’ŒåŒ–å­¦ç­‰é¢†åŸŸã€‚é€šè¿‡ä»30,000ä¸ªç§å­å›¾åƒå¼€å§‹ï¼Œæ”¶é›†å’Œå¤„ç†è¶…è¿‡70ä¸‡ä¸ªç‹¬ç‰¹ç½‘å€çš„HTMLå†…å®¹ï¼Œæˆ‘ä»¬æ„å»ºäº†çº¦90ä¸‡ä¸ªé—®ç­”å¯¹çš„æ•°æ®é›†ã€‚ç»è¿‡åœ¨VisualWebInstructä¸Šå¾®è°ƒçš„æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¯æ˜äº†è¯¥æ•°æ®é›†åœ¨å¢å¼ºè§†è§‰è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.09642', 'title': 'Open-Sora 2.0: Training a Commercial-Level Video Generation Model in\n  $200k', 'url': 'https://huggingface.co/papers/2503.09642', 'abstract': 'Video generation models have achieved remarkable progress in the past year. The quality of AI video continues to improve, but at the cost of larger model size, increased data quantity, and greater demand for training compute. In this report, we present Open-Sora 2.0, a commercial-level video generation model trained for only $200k. With this model, we demonstrate that the cost of training a top-performing video generation model is highly controllable. We detail all techniques that contribute to this efficiency breakthrough, including data curation, model architecture, training strategy, and system optimization. According to human evaluation results and VBench scores, Open-Sora 2.0 is comparable to global leading video generation models including the open-source HunyuanVideo and the closed-source Runway Gen-3 Alpha. By making Open-Sora 2.0 fully open-source, we aim to democratize access to advanced video generation technology, fostering broader innovation and creativity in content creation. All resources are publicly available at: https://github.com/hpcaitech/Open-Sora.', 'score': 14, 'issue_id': 2701, 'pub_date': '2025-03-12', 'pub_date_card': {'ru': '12 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 12', 'zh': '3æœˆ12æ—¥'}, 'hash': '8987bc0bbdde3b5a', 'authors': ['Xiangyu Peng', 'Zangwei Zheng', 'Chenhui Shen', 'Tom Young', 'Xinying Guo', 'Binluo Wang', 'Hang Xu', 'Hongxin Liu', 'Mingyan Jiang', 'Wenjun Li', 'Yuhui Wang', 'Anbang Ye', 'Gang Ren', 'Qianran Ma', 'Wanying Liang', 'Xiang Lian', 'Xiwen Wu', 'Yuting Zhong', 'Zhuangyan Li', 'Chaoyu Gong', 'Guojun Lei', 'Leijun Cheng', 'Limin Zhang', 'Minghao Li', 'Ruijie Zhang', 'Silan Hu', 'Shijie Huang', 'Xiaokang Wang', 'Yuanheng Zhao', 'Yuqi Wang', 'Ziang Wei', 'Yang You'], 'affiliations': ['HPC-AI Tech'], 'pdf_title_img': 'assets/pdf/title_img/2503.09642.jpg', 'data': {'categories': ['#training', '#optimization', '#open_source', '#architecture', '#video', '#data'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ”Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾: ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¿Ğ¾ Ñ€Ğ°Ğ·ÑƒĞ¼Ğ½Ğ¾Ğ¹ Ñ†ĞµĞ½Ğµ', 'desc': 'Open-Sora 2.0 - ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ²ÑĞµĞ³Ğ¾ Ğ·Ğ° $200 Ñ‚Ñ‹Ñ. ĞĞ½Ğ° Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ»Ğ¸ Ñ€ÑĞ´ Ñ‚ĞµÑ…Ğ½Ğ¸Ğº Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ĞºÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞŸĞ¾ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°Ğ¼ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑĞ¼ VBench, Open-Sora 2.0 ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ğ° Ñ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'Affordable Excellence in Video Generation', 'desc': 'Open-Sora 2.0 is a new video generation model that has been developed to produce high-quality videos while keeping training costs low. It was trained for only $200,000, showcasing that effective video generation can be achieved without massive resources. The model incorporates various techniques such as optimized data curation, innovative model architecture, and efficient training strategies to enhance performance. By making Open-Sora 2.0 open-source, the authors aim to provide wider access to advanced video generation tools, encouraging creativity and innovation in the field.'}, 'zh': {'title': 'Open-Sora 2.0ï¼šé«˜æ•ˆè§†é¢‘ç”Ÿæˆçš„æœªæ¥', 'desc': 'è§†é¢‘ç”Ÿæˆæ¨¡å‹åœ¨è¿‡å»ä¸€å¹´å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚è™½ç„¶AIè§†é¢‘çš„è´¨é‡ä¸æ–­æé«˜ï¼Œä½†è¿™ä¹Ÿå¯¼è‡´äº†æ¨¡å‹è§„æ¨¡å¢å¤§ã€æ•°æ®é‡å¢åŠ å’Œè®­ç»ƒè®¡ç®—éœ€æ±‚åŠ å¤§ã€‚æˆ‘ä»¬ä»‹ç»äº†Open-Sora 2.0ï¼Œè¿™æ˜¯ä¸€ä¸ªå•†ä¸šçº§çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œä»…ç”¨20ä¸‡ç¾å…ƒè¿›è¡Œè®­ç»ƒã€‚é€šè¿‡ä¼˜åŒ–æ•°æ®ç®¡ç†ã€æ¨¡å‹æ¶æ„ã€è®­ç»ƒç­–ç•¥å’Œç³»ç»Ÿæ€§èƒ½ï¼Œæˆ‘ä»¬å±•ç¤ºäº†é«˜æ•ˆè®­ç»ƒé¡¶çº§è§†é¢‘ç”Ÿæˆæ¨¡å‹çš„å¯æ§æ€§ï¼Œå¹¶å¸Œæœ›é€šè¿‡å¼€æºè¿™ä¸€æ¨¡å‹æ¥ä¿ƒè¿›å†…å®¹åˆ›ä½œçš„åˆ›æ–°ä¸å‘å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.09641', 'title': 'SANA-Sprint: One-Step Diffusion with Continuous-Time Consistency\n  Distillation', 'url': 'https://huggingface.co/papers/2503.09641', 'abstract': 'This paper presents SANA-Sprint, an efficient diffusion model for ultra-fast text-to-image (T2I) generation. SANA-Sprint is built on a pre-trained foundation model and augmented with hybrid distillation, dramatically reducing inference steps from 20 to 1-4. We introduce three key innovations: (1) We propose a training-free approach that transforms a pre-trained flow-matching model for continuous-time consistency distillation (sCM), eliminating costly training from scratch and achieving high training efficiency. Our hybrid distillation strategy combines sCM with latent adversarial distillation (LADD): sCM ensures alignment with the teacher model, while LADD enhances single-step generation fidelity. (2) SANA-Sprint is a unified step-adaptive model that achieves high-quality generation in 1-4 steps, eliminating step-specific training and improving efficiency. (3) We integrate ControlNet with SANA-Sprint for real-time interactive image generation, enabling instant visual feedback for user interaction. SANA-Sprint establishes a new Pareto frontier in speed-quality tradeoffs, achieving state-of-the-art performance with 7.59 FID and 0.74 GenEval in only 1 step - outperforming FLUX-schnell (7.94 FID / 0.71 GenEval) while being 10x faster (0.1s vs 1.1s on H100). It also achieves 0.1s (T2I) and 0.25s (ControlNet) latency for 1024 x 1024 images on H100, and 0.31s (T2I) on an RTX 4090, showcasing its exceptional efficiency and potential for AI-powered consumer applications (AIPC). Code and pre-trained models will be open-sourced.', 'score': 14, 'issue_id': 2702, 'pub_date': '2025-03-12', 'pub_date_card': {'ru': '12 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 12', 'zh': '3æœˆ12æ—¥'}, 'hash': '4079b7fe9f67a246', 'authors': ['Junsong Chen', 'Shuchen Xue', 'Yuyang Zhao', 'Jincheng Yu', 'Sayak Paul', 'Junyu Chen', 'Han Cai', 'Enze Xie', 'Song Han'], 'affiliations': ['Huggingface', 'Independent Researcher', 'MIT', 'NVIDIA', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.09641.jpg', 'data': {'categories': ['#diffusion', '#training', '#inference', '#cv', '#open_source'], 'emoji': 'ğŸš€', 'ru': {'title': 'ĞœĞ¾Ğ»Ğ½Ğ¸ĞµĞ½Ğ¾ÑĞ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ SANA-Sprint', 'desc': 'SANA-Sprint - ÑÑ‚Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑĞ²ĞµÑ€Ñ…Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ±Ğ°Ğ·Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½ÑƒÑ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ, ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°Ñ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑˆĞ°Ğ³Ğ¾Ğ² Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ñ 20 Ğ´Ğ¾ 1-4. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸, ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ñ ControlNet Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. SANA-Sprint Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼ FID Ğ¸ GenEval Ğ·Ğ° 1 ÑˆĞ°Ğ³, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ.'}, 'en': {'title': 'SANA-Sprint: Ultra-Fast Text-to-Image Generation Revolutionized!', 'desc': 'This paper introduces SANA-Sprint, a novel diffusion model designed for rapid text-to-image (T2I) generation. By leveraging a pre-trained foundation model and employing hybrid distillation techniques, SANA-Sprint significantly reduces the number of inference steps required, achieving high-quality image generation in just 1-4 steps. The model integrates a training-free approach and combines continuous-time consistency distillation with latent adversarial distillation to enhance generation fidelity. Additionally, SANA-Sprint incorporates ControlNet for real-time interactive image generation, setting a new standard in the speed-quality tradeoff for T2I models.'}, 'zh': {'title': 'SANA-Sprintï¼šè¶…å¿«é€Ÿæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„é©å‘½æ€§æ¨¡å‹', 'desc': 'æœ¬æ–‡ä»‹ç»äº†SANA-Sprintï¼Œä¸€ç§é«˜æ•ˆçš„æ‰©æ•£æ¨¡å‹ï¼Œç”¨äºè¶…å¿«é€Ÿçš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆã€‚SANA-SprintåŸºäºé¢„è®­ç»ƒçš„åŸºç¡€æ¨¡å‹ï¼Œå¹¶é€šè¿‡æ··åˆè’¸é¦æŠ€æœ¯æ˜¾è‘—å‡å°‘æ¨ç†æ­¥éª¤ï¼Œä»20æ­¥å‡å°‘åˆ°1-4æ­¥ã€‚è¯¥æ¨¡å‹çš„ä¸‰é¡¹å…³é”®åˆ›æ–°åŒ…æ‹¬æ— è®­ç»ƒçš„æµåŒ¹é…æ¨¡å‹è½¬æ¢ã€ç»Ÿä¸€çš„æ­¥é€‚åº”æ¨¡å‹ä»¥åŠä¸ControlNetçš„é›†æˆï¼Œå®ç°å®æ—¶äº¤äº’å¼å›¾åƒç”Ÿæˆã€‚SANA-Sprintåœ¨é€Ÿåº¦ä¸è´¨é‡çš„æƒè¡¡ä¸­å»ºç«‹äº†æ–°çš„Paretoå‰æ²¿ï¼Œä»¥1æ­¥ç”Ÿæˆè¾¾åˆ°7.59 FIDå’Œ0.74 GenEvalçš„æœ€ä½³æ€§èƒ½ï¼Œä¸”é€Ÿåº¦æ¯”FLUX-schnellå¿«10å€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.10615', 'title': 'R1-Onevision: Advancing Generalized Multimodal Reasoning through\n  Cross-Modal Formalization', 'url': 'https://huggingface.co/papers/2503.10615', 'abstract': 'Large Language Models have demonstrated remarkable reasoning capability in complex textual tasks. However, multimodal reasoning, which requires integrating visual and textual information, remains a significant challenge. Existing visual-language models often struggle to effectively analyze and reason visual content, resulting in suboptimal performance on complex reasoning tasks. Moreover, the absence of comprehensive benchmarks hinders the accurate assessment of multimodal reasoning capabilities. In this paper, we introduce R1-Onevision, a multimodal reasoning model designed to bridge the gap between visual perception and deep reasoning. To achieve this, we propose a cross-modal reasoning pipeline that transforms images into formal textural representations, enabling precise language-based reasoning. Leveraging this pipeline, we construct the R1-Onevision dataset which provides detailed, step-by-step multimodal reasoning annotations across diverse domains. We further develop the R1-Onevision model through supervised fine-tuning and reinforcement learning to cultivate advanced reasoning and robust generalization abilities. To comprehensively evaluate multimodal reasoning performance across different grades, we introduce R1-Onevision-Bench, a benchmark aligned with human educational stages, covering exams from junior high school to university and beyond. Experimental results show that R1-Onevision achieves state-of-the-art performance, outperforming models such as GPT-4o and Qwen2.5-VL on multiple challenging multimodal reasoning benchmarks.', 'score': 13, 'issue_id': 2701, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 13', 'zh': '3æœˆ13æ—¥'}, 'hash': '857ef8e4c110da7f', 'authors': ['Yi Yang', 'Xiaoxuan He', 'Hongkun Pan', 'Xiyan Jiang', 'Yan Deng', 'Xingtao Yang', 'Haoyu Lu', 'Dacheng Yin', 'Fengyun Rao', 'Minfeng Zhu', 'Bo Zhang', 'Wei Chen'], 'affiliations': ['Renmin University of China', 'WeChat Vision, Tencent Inc.', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.10615.jpg', 'data': {'categories': ['#benchmark', '#multimodal', '#dataset', '#rl', '#reasoning', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°Ñ Ğ±Ğ°Ñ€ÑŒĞµÑ€ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ·Ñ€ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼ Ğ² Ğ˜Ğ˜', 'desc': 'R1-Onevision - ÑÑ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°ÑÑ‰Ğ°Ñ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸ĞµĞ¼ Ğ¸ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ğ¼ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ¼. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ R1-Onevision Ñ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ R1-Onevision, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ….'}, 'en': {'title': 'Bridging Visual and Textual Reasoning with R1-Onevision', 'desc': 'This paper presents R1-Onevision, a new model for multimodal reasoning that combines visual and textual information. It addresses the limitations of existing visual-language models by introducing a cross-modal reasoning pipeline that converts images into structured text representations. The authors also create the R1-Onevision dataset, which includes detailed annotations for multimodal reasoning tasks across various domains. Additionally, they establish R1-Onevision-Bench, a benchmark for evaluating multimodal reasoning abilities at different educational levels, demonstrating that their model outperforms others like GPT-4o and Qwen2.5-VL.'}, 'zh': {'title': 'R1-Onevisionï¼šè§†è§‰ä¸è¯­è¨€çš„å®Œç¾ç»“åˆ', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºR1-Onevisionçš„å¤šæ¨¡æ€æ¨ç†æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³è§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯æ•´åˆçš„æŒ‘æˆ˜ã€‚è¯¥æ¨¡å‹é€šè¿‡è·¨æ¨¡æ€æ¨ç†ç®¡é“ï¼Œå°†å›¾åƒè½¬æ¢ä¸ºæ­£å¼çš„æ–‡æœ¬è¡¨ç¤ºï¼Œä»è€Œå®ç°ç²¾ç¡®çš„åŸºäºè¯­è¨€çš„æ¨ç†ã€‚æˆ‘ä»¬è¿˜æ„å»ºäº†R1-Onevisionæ•°æ®é›†ï¼Œæä¾›äº†è¯¦ç»†çš„å¤šæ¨¡æ€æ¨ç†æ³¨é‡Šï¼Œä»¥æ”¯æŒä¸åŒé¢†åŸŸçš„ç ”ç©¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒR1-Onevisionåœ¨å¤šä¸ªå¤æ‚çš„å¤šæ¨¡æ€æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†ç°æœ‰çš„å…ˆè¿›æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.10589', 'title': 'Long Context Tuning for Video Generation', 'url': 'https://huggingface.co/papers/2503.10589', 'abstract': 'Recent advances in video generation can produce realistic, minute-long single-shot videos with scalable diffusion transformers. However, real-world narrative videos require multi-shot scenes with visual and dynamic consistency across shots. In this work, we introduce Long Context Tuning (LCT), a training paradigm that expands the context window of pre-trained single-shot video diffusion models to learn scene-level consistency directly from data. Our method expands full attention mechanisms from individual shots to encompass all shots within a scene, incorporating interleaved 3D position embedding and an asynchronous noise strategy, enabling both joint and auto-regressive shot generation without additional parameters. Models with bidirectional attention after LCT can further be fine-tuned with context-causal attention, facilitating auto-regressive generation with efficient KV-cache. Experiments demonstrate single-shot models after LCT can produce coherent multi-shot scenes and exhibit emerging capabilities, including compositional generation and interactive shot extension, paving the way for more practical visual content creation. See https://guoyww.github.io/projects/long-context-video/ for more details.', 'score': 13, 'issue_id': 2702, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 13', 'zh': '3æœˆ13æ—¥'}, 'hash': '3850b6066f2b7160', 'authors': ['Yuwei Guo', 'Ceyuan Yang', 'Ziyan Yang', 'Zhibei Ma', 'Zhijie Lin', 'Zhenheng Yang', 'Dahua Lin', 'Lu Jiang'], 'affiliations': ['ByteDance', 'ByteDance Seed', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2503.10589.jpg', 'data': {'categories': ['#long_context', '#3d', '#video', '#diffusion', '#training'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾ĞºĞ°Ğ´Ñ€Ğ¾Ğ²Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Long Context Tuning', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Long Context Tuning (LCT) Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ĞºĞ°Ğ´Ñ€Ğ¾Ğ²Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ ÑÑ†ĞµĞ½. LCT Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğµ Ğ¾ĞºĞ½Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ´Ğ¸Ğ½Ğ¾Ñ‡Ğ½Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ², Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¸Ğ¼ ÑƒÑ‡Ğ¸Ñ‚ÑŒÑÑ Ğ½ĞµĞ¿Ğ¾ÑÑ€ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ¸Ğ· Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ°ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ ÑˆÑƒĞ¼Ğ° Ğ´Ğ»Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ¹ Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ°Ğ´Ñ€Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ÑĞ»Ğµ LCT ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ĞºĞ°Ğ´Ñ€Ğ¾Ğ²Ñ‹Ğµ ÑÑ†ĞµĞ½Ñ‹ Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ ĞºĞ°Ğ´Ñ€Ğ¾Ğ².'}, 'en': {'title': 'Enhancing Video Generation with Long Context Tuning', 'desc': 'This paper presents Long Context Tuning (LCT), a new training approach for video generation models that enhances their ability to create multi-shot scenes with visual and dynamic consistency. By expanding the context window of pre-trained single-shot video diffusion models, LCT allows the model to learn from the entire scene rather than just individual shots. The method utilizes full attention mechanisms and incorporates advanced techniques like interleaved 3D position embedding and an asynchronous noise strategy. As a result, models trained with LCT can generate coherent multi-shot videos and demonstrate new capabilities such as compositional generation and interactive shot extension.'}, 'zh': {'title': 'é•¿ä¸Šä¸‹æ–‡è°ƒä¼˜ï¼šæå‡è§†é¢‘ç”Ÿæˆçš„ä¸€è‡´æ€§ä¸è¿è´¯æ€§', 'desc': 'æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„è®­ç»ƒèŒƒå¼ï¼Œç§°ä¸ºé•¿ä¸Šä¸‹æ–‡è°ƒä¼˜ï¼ˆLCTï¼‰ï¼Œæ—¨åœ¨æé«˜è§†é¢‘ç”Ÿæˆæ¨¡å‹åœ¨å¤šé•œå¤´åœºæ™¯ä¸­çš„ä¸€è‡´æ€§ã€‚é€šè¿‡æ‰©å±•é¢„è®­ç»ƒå•é•œå¤´è§†é¢‘æ‰©æ•£æ¨¡å‹çš„ä¸Šä¸‹æ–‡çª—å£ï¼ŒLCTèƒ½å¤Ÿç›´æ¥ä»æ•°æ®ä¸­å­¦ä¹ åœºæ™¯çº§åˆ«çš„ä¸€è‡´æ€§ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å…¨æ³¨æ„åŠ›æœºåˆ¶ï¼Œå°†æ³¨æ„åŠ›ä»å•ä¸ªé•œå¤´æ‰©å±•åˆ°æ•´ä¸ªåœºæ™¯ï¼Œç»“åˆäº†3Dä½ç½®åµŒå…¥å’Œå¼‚æ­¥å™ªå£°ç­–ç•¥ï¼Œå®ç°äº†æ— é¢å¤–å‚æ•°çš„è”åˆå’Œè‡ªå›å½’é•œå¤´ç”Ÿæˆã€‚å®éªŒè¡¨æ˜ï¼Œç»è¿‡LCTå¤„ç†çš„å•é•œå¤´æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆè¿è´¯çš„å¤šé•œå¤´åœºæ™¯ï¼Œå¹¶å±•ç°å‡ºç»„åˆç”Ÿæˆå’Œäº¤äº’å¼é•œå¤´æ‰©å±•ç­‰æ–°èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.10637', 'title': 'Distilling Diversity and Control in Diffusion Models', 'url': 'https://huggingface.co/papers/2503.10637', 'abstract': 'Distilled diffusion models suffer from a critical limitation: reduced sample diversity compared to their base counterparts. In this work, we uncover that despite this diversity loss, distilled models retain the fundamental concept representations of base models. We demonstrate control distillation - where control mechanisms like Concept Sliders and LoRAs trained on base models can be seamlessly transferred to distilled models and vice-versa, effectively distilling control without any retraining. This preservation of representational structure prompted our investigation into the mechanisms of diversity collapse during distillation. To understand how distillation affects diversity, we introduce Diffusion Target (DT) Visualization, an analysis and debugging tool that reveals how models predict final outputs at intermediate steps. Through DT-Visualization, we identify generation artifacts, inconsistencies, and demonstrate that initial diffusion timesteps disproportionately determine output diversity, while later steps primarily refine details. Based on these insights, we introduce diversity distillation - a hybrid inference approach that strategically employs the base model for only the first critical timestep before transitioning to the efficient distilled model. Our experiments demonstrate that this simple modification not only restores the diversity capabilities from base to distilled models but surprisingly exceeds it, while maintaining nearly the computational efficiency of distilled inference, all without requiring additional training or model modifications. Our code and data are available at https://distillation.baulab.info', 'score': 12, 'issue_id': 2701, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 13', 'zh': '3æœˆ13æ—¥'}, 'hash': '5313c06d699a1a7f', 'authors': ['Rohit Gandikota', 'David Bau'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2503.10637.jpg', 'data': {'categories': ['#dataset', '#inference', '#training', '#optimization', '#diffusion', '#data'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'Ğ’Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ² Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğµ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ ÑÑĞ¼Ğ¿Ğ»Ğ¾Ğ² Ğ² Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑÑ‚ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ¸Ñ‚ÑŒ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ½Ğ¸Ğ¼Ğ¸ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ”Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½ Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ±Ñ‹Ğ»Ğ° Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Diffusion Target, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ°, Ñ‡Ñ‚Ğ¾ Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑˆĞ°Ğ³Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ½ĞµĞ¿Ñ€Ğ¾Ğ¿Ğ¾Ñ€Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ²Ğ»Ğ¸ÑÑÑ‚ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¸Ñ… Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ diversity distillation, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ±Ğ°Ğ·Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ´Ğ»Ñ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑˆĞ°Ğ³Ğ°, Ñ‡Ñ‚Ğ¾ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¸ Ğ´Ğ°Ğ¶Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°.'}, 'en': {'title': 'Restoring Diversity in Distilled Diffusion Models', 'desc': 'This paper addresses the issue of reduced sample diversity in distilled diffusion models compared to their original versions. The authors reveal that distilled models still maintain the essential concept representations of their base models. They introduce a method called control distillation, which allows for the transfer of control mechanisms between models without retraining. Additionally, they present Diffusion Target (DT) Visualization to analyze how diversity is affected during the distillation process, leading to a new approach called diversity distillation that enhances output diversity while keeping computational efficiency intact.'}, 'zh': {'title': 'æ¢å¤è’¸é¦æ¨¡å‹çš„å¤šæ ·æ€§', 'desc': 'æœ¬æ–‡æ¢è®¨äº†è’¸é¦æ‰©æ•£æ¨¡å‹çš„ä¸€ä¸ªé‡è¦é™åˆ¶ï¼Œå³ä¸åŸºç¡€æ¨¡å‹ç›¸æ¯”ï¼Œæ ·æœ¬å¤šæ ·æ€§é™ä½ã€‚å°½ç®¡å­˜åœ¨è¿™ç§å¤šæ ·æ€§æŸå¤±ï¼Œè’¸é¦æ¨¡å‹ä»ç„¶ä¿ç•™äº†åŸºç¡€æ¨¡å‹çš„åŸºæœ¬æ¦‚å¿µè¡¨ç¤ºã€‚æˆ‘ä»¬æå‡ºäº†æ§åˆ¶è’¸é¦çš„æ–¹æ³•ï¼Œå¯ä»¥å°†æ§åˆ¶æœºåˆ¶æ— ç¼è½¬ç§»åˆ°è’¸é¦æ¨¡å‹ï¼Œå¹¶ä¸”ä¸éœ€è¦é‡æ–°è®­ç»ƒã€‚é€šè¿‡å¼•å…¥æ‰©æ•£ç›®æ ‡å¯è§†åŒ–å·¥å…·ï¼Œæˆ‘ä»¬åˆ†æäº†è’¸é¦å¯¹å¤šæ ·æ€§çš„å½±å“ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°çš„æ··åˆæ¨ç†æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨ä¿æŒè®¡ç®—æ•ˆç‡çš„åŒæ—¶æ¢å¤å¤šæ ·æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.10357', 'title': 'Do I look like a `cat.n.01` to you? A Taxonomy Image Generation\n  Benchmark', 'url': 'https://huggingface.co/papers/2503.10357', 'abstract': "This paper explores the feasibility of using text-to-image models in a zero-shot setup to generate images for taxonomy concepts. While text-based methods for taxonomy enrichment are well-established, the potential of the visual dimension remains unexplored. To address this, we propose a comprehensive benchmark for Taxonomy Image Generation that assesses models' abilities to understand taxonomy concepts and generate relevant, high-quality images. The benchmark includes common-sense and randomly sampled WordNet concepts, alongside the LLM generated predictions. The 12 models are evaluated using 9 novel taxonomy-related text-to-image metrics and human feedback. Moreover, we pioneer the use of pairwise evaluation with GPT-4 feedback for image generation. Experimental results show that the ranking of models differs significantly from standard T2I tasks. Playground-v2 and FLUX consistently outperform across metrics and subsets and the retrieval-based approach performs poorly. These findings highlight the potential for automating the curation of structured data resources.", 'score': 11, 'issue_id': 2705, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 13', 'zh': '3æœˆ13æ—¥'}, 'hash': 'dad8c63e36f6d8c2', 'authors': ['Viktor Moskvoretskii', 'Alina Lobanova', 'Ekaterina Neminova', 'Chris Biemann', 'Alexander Panchenko', 'Irina Nikishina'], 'affiliations': ['AIRI', 'HSE University', 'Skoltech', 'University of Hamburg'], 'pdf_title_img': 'assets/pdf/title_img/2503.10357.jpg', 'data': {'categories': ['#cv', '#multimodal', '#benchmark'], 'emoji': 'ğŸŒ³', 'ru': {'title': 'Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ğ¹: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ´Ğ»Ñ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¸ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ, ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¾Ñ†ĞµĞ½ĞºÑƒ 12 Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾ 9 Ğ½Ğ¾Ğ²Ñ‹Ğ¼ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ñ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸ĞµĞ¹, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ Ğ¾Ñ‚ Ğ»ÑĞ´ĞµĞ¹ Ğ¸ GPT-4. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Playground-v2 Ğ¸ FLUX Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ĞºÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Unlocking Visual Understanding in Taxonomy with Zero-Shot Image Generation', 'desc': 'This paper investigates how well text-to-image models can create images based on taxonomy concepts without prior training on those specific concepts, known as a zero-shot setup. It introduces a new benchmark for evaluating these models, focusing on their ability to generate relevant and high-quality images that align with taxonomy terms. The study employs innovative metrics and human feedback to assess the performance of 12 different models, revealing that their effectiveness varies significantly from traditional text-to-image tasks. The results suggest that certain models, like Playground-v2 and FLUX, excel in this context, indicating a promising avenue for automating the enhancement of structured data resources.'}, 'zh': {'title': 'æ¢ç´¢é›¶æ ·æœ¬ä¸‹çš„åˆ†ç±»å›¾åƒç”Ÿæˆæ½œåŠ›', 'desc': 'æœ¬æ–‡æ¢è®¨äº†åœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹ä½¿ç”¨æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ç”Ÿæˆåˆ†ç±»æ¦‚å¿µå›¾åƒçš„å¯è¡Œæ€§ã€‚è™½ç„¶åŸºäºæ–‡æœ¬çš„æ–¹æ³•åœ¨åˆ†ç±»ä¸°å¯Œæ–¹é¢å·²ç»ç›¸å½“æˆç†Ÿï¼Œä½†è§†è§‰ç»´åº¦çš„æ½œåŠ›å°šæœªè¢«å……åˆ†æŒ–æ˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå…¨é¢çš„åˆ†ç±»å›¾åƒç”ŸæˆåŸºå‡†ï¼Œè¯„ä¼°æ¨¡å‹ç†è§£åˆ†ç±»æ¦‚å¿µå’Œç”Ÿæˆç›¸å…³é«˜è´¨é‡å›¾åƒçš„èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ¨¡å‹çš„æ’åä¸æ ‡å‡†çš„æ–‡æœ¬åˆ°å›¾åƒä»»åŠ¡æœ‰æ˜¾è‘—ä¸åŒï¼Œå¼ºè°ƒäº†è‡ªåŠ¨åŒ–ç»“æ„åŒ–æ•°æ®èµ„æºæ•´ç†çš„æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.09799', 'title': 'Communication-Efficient Language Model Training Scales Reliably and\n  Robustly: Scaling Laws for DiLoCo', 'url': 'https://huggingface.co/papers/2503.09799', 'abstract': "As we scale to more massive machine learning models, the frequent synchronization demands inherent in data-parallel approaches create significant slowdowns, posing a critical challenge to further scaling. Recent work develops an approach (DiLoCo) that relaxes synchronization demands without compromising model quality. However, these works do not carefully analyze how DiLoCo's behavior changes with model size. In this work, we study the scaling law behavior of DiLoCo when training LLMs under a fixed compute budget. We focus on how algorithmic factors, including number of model replicas, hyperparameters, and token budget affect training in ways that can be accurately predicted via scaling laws. We find that DiLoCo scales both predictably and robustly with model size. When well-tuned, DiLoCo scales better than data-parallel training with model size, and can outperform data-parallel training even at small model sizes. Our results showcase a more general set of benefits of DiLoCo than previously documented, including increased optimal batch sizes, improved downstream generalization with scale, and improved evaluation loss for a fixed token budget.", 'score': 10, 'issue_id': 2714, 'pub_date': '2025-03-12', 'pub_date_card': {'ru': '12 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 12', 'zh': '3æœˆ12æ—¥'}, 'hash': '78f500ddd43c6e65', 'authors': ['Zachary Charles', 'Gabriel Teston', 'Lucio Dery', 'Keith Rush', 'Nova Fallen', 'Zachary Garrett', 'Arthur Szlam', 'Arthur Douillard'], 'affiliations': ['Google DeepMind', 'Google Research', 'Google Search'], 'pdf_title_img': 'assets/pdf/title_img/2503.09799.jpg', 'data': {'categories': ['#training', '#optimization'], 'emoji': 'ğŸ“ˆ', 'ru': {'title': 'DiLoCo: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° DiLoCo Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ñ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğ¾Ğ¼ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚, ĞºĞ°Ğº Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ñ‹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€ĞµĞ¿Ğ»Ğ¸Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ³Ğ¸Ğ¿ĞµÑ€Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ¸ Ğ±ÑĞ´Ğ¶ĞµÑ‚ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ²Ğ»Ğ¸ÑÑÑ‚ Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ, Ğ¸ ĞºĞ°Ğº ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ñ‚ÑŒ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ² Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ DiLoCo Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·ÑƒĞµĞ¼Ğ¾ Ğ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ Ñ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ´Ğ°Ğ¶Ğµ Ğ´Ğ»Ñ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¸Ğ¹ ÑĞ¿ĞµĞºÑ‚Ñ€ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ² DiLoCo, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ² Ğ±Ğ°Ñ‚Ñ‡Ğ° Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ½Ğ° downstream Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ….'}, 'en': {'title': 'DiLoCo: Scaling Up Without the Sync Slowdown!', 'desc': 'This paper investigates the performance of the DiLoCo approach for training large language models (LLMs) while minimizing synchronization delays. It analyzes how various factors, such as the number of model replicas and hyperparameters, influence the training process under a fixed compute budget. The authors demonstrate that DiLoCo not only scales effectively with model size but also outperforms traditional data-parallel training methods, even for smaller models. Their findings highlight the advantages of DiLoCo, including better batch sizes and enhanced generalization capabilities as model size increases.'}, 'zh': {'title': 'DiLoCoï¼šè¶…è¶Šæ•°æ®å¹¶è¡Œçš„è®­ç»ƒæ–°æ–¹æ³•', 'desc': 'éšç€æœºå™¨å­¦ä¹ æ¨¡å‹è§„æ¨¡çš„æ‰©å¤§ï¼Œæ•°æ®å¹¶è¡Œæ–¹æ³•ä¸­é¢‘ç¹çš„åŒæ­¥éœ€æ±‚å¯¼è‡´æ˜¾è‘—çš„æ€§èƒ½ä¸‹é™ï¼Œæˆä¸ºè¿›ä¸€æ­¥æ‰©å±•çš„å…³é”®æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºDiLoCoçš„æ–¹æ³•ï¼Œå®ƒåœ¨ä¸å½±å“æ¨¡å‹è´¨é‡çš„æƒ…å†µä¸‹ï¼Œæ”¾å®½äº†åŒæ­¥éœ€æ±‚ã€‚æˆ‘ä»¬ç ”ç©¶äº†åœ¨å›ºå®šè®¡ç®—é¢„ç®—ä¸‹ï¼ŒDiLoCoåœ¨è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹æ—¶çš„æ‰©å±•è§„å¾‹ï¼Œåˆ†æäº†æ¨¡å‹å‰¯æœ¬æ•°é‡ã€è¶…å‚æ•°å’Œä»¤ç‰Œé¢„ç®—ç­‰ç®—æ³•å› ç´ å¯¹è®­ç»ƒçš„å½±å“ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒDiLoCoåœ¨æ¨¡å‹è§„æ¨¡ä¸Šå…·æœ‰å¯é¢„æµ‹å’Œç¨³å¥çš„æ‰©å±•æ€§ï¼Œç»è¿‡è‰¯å¥½è°ƒä¼˜åï¼Œå…¶æ‰©å±•æ€§èƒ½ä¼˜äºæ•°æ®å¹¶è¡Œè®­ç»ƒï¼Œç”šè‡³åœ¨å°è§„æ¨¡æ¨¡å‹ä¸­ä¹Ÿèƒ½è¶…è¶Šæ•°æ®å¹¶è¡Œè®­ç»ƒã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.10391', 'title': 'CINEMA: Coherent Multi-Subject Video Generation via MLLM-Based Guidance', 'url': 'https://huggingface.co/papers/2503.10391', 'abstract': 'Video generation has witnessed remarkable progress with the advent of deep generative models, particularly diffusion models. While existing methods excel in generating high-quality videos from text prompts or single images, personalized multi-subject video generation remains a largely unexplored challenge. This task involves synthesizing videos that incorporate multiple distinct subjects, each defined by separate reference images, while ensuring temporal and spatial consistency. Current approaches primarily rely on mapping subject images to keywords in text prompts, which introduces ambiguity and limits their ability to model subject relationships effectively. In this paper, we propose CINEMA, a novel framework for coherent multi-subject video generation by leveraging Multimodal Large Language Model (MLLM). Our approach eliminates the need for explicit correspondences between subject images and text entities, mitigating ambiguity and reducing annotation effort. By leveraging MLLM to interpret subject relationships, our method facilitates scalability, enabling the use of large and diverse datasets for training. Furthermore, our framework can be conditioned on varying numbers of subjects, offering greater flexibility in personalized content creation. Through extensive evaluations, we demonstrate that our approach significantly improves subject consistency, and overall video coherence, paving the way for advanced applications in storytelling, interactive media, and personalized video generation.', 'score': 9, 'issue_id': 2700, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 13', 'zh': '3æœˆ13æ—¥'}, 'hash': 'd7f2c49b29951ace', 'authors': ['Yufan Deng', 'Xun Guo', 'Yizhi Wang', 'Jacob Zhiyuan Fang', 'Angtian Wang', 'Shenghai Yuan', 'Yiding Yang', 'Bo Liu', 'Haibin Huang', 'Chongyang Ma'], 'affiliations': ['ByteDance Intelligent Creation', 'Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2503.10391.jpg', 'data': {'categories': ['#synthetic', '#multimodal', '#story_generation', '#video', '#diffusion'], 'emoji': 'ğŸ¬', 'ru': {'title': 'CINEMA: Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾Ğ¼ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ¸ MLLM', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ CINEMA - Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶Ğ°Ğ¼Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ (MLLM). CINEMA ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ² ÑĞ²Ğ½Ğ¾Ğ¼ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸, Ñ‡Ñ‚Ğ¾ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ĞµÑ‚ Ğ½ĞµĞ¾Ğ´Ğ½Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑƒĞ¿Ñ€Ğ¾Ñ‰Ğ°ĞµÑ‚ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. CINEMA Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹ Ğ¸ Ğ¾Ğ±Ñ‰ĞµĞ¹ ÑĞ²ÑĞ·Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'CINEMA: Coherent Multi-Subject Video Generation Made Easy', 'desc': 'This paper introduces CINEMA, a new framework for generating videos that feature multiple distinct subjects from separate reference images. Unlike traditional methods that rely on mapping images to text prompts, CINEMA uses a Multimodal Large Language Model (MLLM) to understand and interpret relationships between subjects, which reduces ambiguity. The framework allows for flexible conditioning on different numbers of subjects, making it easier to create personalized content. Extensive evaluations show that CINEMA enhances subject consistency and overall video coherence, opening up new possibilities for applications in storytelling and interactive media.'}, 'zh': {'title': 'ä¸ªæ€§åŒ–å¤šä¸»ä½“è§†é¢‘ç”Ÿæˆçš„æ–°çªç ´', 'desc': 'è§†é¢‘ç”Ÿæˆåœ¨æ·±åº¦ç”Ÿæˆæ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯æ‰©æ•£æ¨¡å‹çš„æ¨åŠ¨ä¸‹å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç°æœ‰æ–¹æ³•åœ¨ä»æ–‡æœ¬æç¤ºæˆ–å•å¼ å›¾åƒç”Ÿæˆé«˜è´¨é‡è§†é¢‘æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†ä¸ªæ€§åŒ–çš„å¤šä¸»ä½“è§†é¢‘ç”Ÿæˆä»ç„¶æ˜¯ä¸€ä¸ªæœªè¢«å……åˆ†æ¢ç´¢çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†CINEMAæ¡†æ¶ï¼Œé€šè¿‡åˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œå®ç°äº†ä¸€è‡´çš„å¤šä¸»ä½“è§†é¢‘ç”Ÿæˆï¼Œæ¶ˆé™¤äº†ä¸»ä½“å›¾åƒä¸æ–‡æœ¬å®ä½“ä¹‹é—´çš„æ˜ç¡®å¯¹åº”å…³ç³»ï¼Œä»è€Œå‡å°‘äº†æ­§ä¹‰å’Œæ ‡æ³¨å·¥ä½œã€‚æˆ‘ä»¬çš„æ¡†æ¶èƒ½å¤Ÿæ ¹æ®ä¸åŒæ•°é‡çš„ä¸»ä½“è¿›è¡Œæ¡ä»¶ç”Ÿæˆï¼Œæä¾›äº†æ›´å¤§çš„ä¸ªæ€§åŒ–å†…å®¹åˆ›ä½œçµæ´»æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.10614', 'title': 'ConsisLoRA: Enhancing Content and Style Consistency for LoRA-based Style\n  Transfer', 'url': 'https://huggingface.co/papers/2503.10614', 'abstract': 'Style transfer involves transferring the style from a reference image to the content of a target image. Recent advancements in LoRA-based (Low-Rank Adaptation) methods have shown promise in effectively capturing the style of a single image. However, these approaches still face significant challenges such as content inconsistency, style misalignment, and content leakage. In this paper, we comprehensively analyze the limitations of the standard diffusion parameterization, which learns to predict noise, in the context of style transfer. To address these issues, we introduce ConsisLoRA, a LoRA-based method that enhances both content and style consistency by optimizing the LoRA weights to predict the original image rather than noise. We also propose a two-step training strategy that decouples the learning of content and style from the reference image. To effectively capture both the global structure and local details of the content image, we introduce a stepwise loss transition strategy. Additionally, we present an inference guidance method that enables continuous control over content and style strengths during inference. Through both qualitative and quantitative evaluations, our method demonstrates significant improvements in content and style consistency while effectively reducing content leakage.', 'score': 7, 'issue_id': 2712, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 13', 'zh': '3æœˆ13æ—¥'}, 'hash': '12aacf8b56fdac9c', 'authors': ['Bolin Chen', 'Baoquan Zhao', 'Haoran Xie', 'Yi Cai', 'Qing Li', 'Xudong Mao'], 'affiliations': ['Lingnan University', 'South China University of Technology', 'Sun Yat-sen University', 'The Hong Kong Polytechnic University'], 'pdf_title_img': 'assets/pdf/title_img/2503.10614.jpg', 'data': {'categories': ['#synthetic', '#cv', '#inference', '#training', '#diffusion', '#leakage', '#optimization'], 'emoji': 'ğŸ¨', 'ru': {'title': 'ConsisLoRA: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞµ ÑÑ‚Ğ¸Ğ»Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ° ÑÑ‚Ğ¸Ğ»Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ConsisLoRA. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²ĞµÑĞ¾Ğ² LoRA Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ÑˆÑƒĞ¼Ğ°, Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑÑ‚Ğ¸Ğ»Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞ¸Ğ»Ğ¾Ğ¹ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑÑ‚Ğ¸Ğ»Ñ Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Enhancing Style Transfer with ConsisLoRA for Better Consistency', 'desc': 'This paper focuses on improving style transfer techniques, which blend the style of one image with the content of another. It identifies key challenges in existing methods, such as inconsistencies in content and style alignment. The authors introduce ConsisLoRA, a new approach that optimizes LoRA weights to enhance both content and style consistency by predicting the original image instead of noise. They also propose a two-step training strategy and a stepwise loss transition to better capture the details of the images, resulting in improved performance in style transfer tasks.'}, 'zh': {'title': 'æå‡é£æ ¼è¿ç§»çš„ä¸€è‡´æ€§ä¸æ§åˆ¶åŠ›', 'desc': 'é£æ ¼è¿ç§»æ˜¯å°†å‚è€ƒå›¾åƒçš„é£æ ¼è½¬ç§»åˆ°ç›®æ ‡å›¾åƒå†…å®¹ä¸Šçš„æŠ€æœ¯ã€‚æœ€è¿‘ï¼ŒåŸºäºLoRAï¼ˆä½ç§©é€‚åº”ï¼‰çš„æ–¹æ³•åœ¨æœ‰æ•ˆæ•æ‰å•ä¸€å›¾åƒé£æ ¼æ–¹é¢æ˜¾ç¤ºå‡ºè‰¯å¥½å‰æ™¯ï¼Œä½†ä»é¢ä¸´å†…å®¹ä¸ä¸€è‡´ã€é£æ ¼é”™ä½å’Œå†…å®¹æ³„æ¼ç­‰æŒ‘æˆ˜ã€‚æœ¬æ–‡åˆ†æäº†æ ‡å‡†æ‰©æ•£å‚æ•°åŒ–åœ¨é£æ ¼è¿ç§»ä¸­çš„å±€é™æ€§ï¼Œå¹¶æå‡ºäº†ConsisLoRAæ–¹æ³•ï¼Œé€šè¿‡ä¼˜åŒ–LoRAæƒé‡æ¥å¢å¼ºå†…å®¹å’Œé£æ ¼çš„ä¸€è‡´æ€§ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸¤æ­¥è®­ç»ƒç­–ç•¥å’Œé€æ­¥æŸå¤±è¿‡æ¸¡ç­–ç•¥ï¼Œä»¥æœ‰æ•ˆæ•æ‰å†…å®¹å›¾åƒçš„å…¨å±€ç»“æ„å’Œå±€éƒ¨ç»†èŠ‚ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.10630', 'title': 'UniGoal: Towards Universal Zero-shot Goal-oriented Navigation', 'url': 'https://huggingface.co/papers/2503.10630', 'abstract': 'In this paper, we propose a general framework for universal zero-shot goal-oriented navigation. Existing zero-shot methods build inference framework upon large language models (LLM) for specific tasks, which differs a lot in overall pipeline and fails to generalize across different types of goal. Towards the aim of universal zero-shot navigation, we propose a uniform graph representation to unify different goals, including object category, instance image and text description. We also convert the observation of agent into an online maintained scene graph. With this consistent scene and goal representation, we preserve most structural information compared with pure text and are able to leverage LLM for explicit graph-based reasoning. Specifically, we conduct graph matching between the scene graph and goal graph at each time instant and propose different strategies to generate long-term goal of exploration according to different matching states. The agent first iteratively searches subgraph of goal when zero-matched. With partial matching, the agent then utilizes coordinate projection and anchor pair alignment to infer the goal location. Finally scene graph correction and goal verification are applied for perfect matching. We also present a blacklist mechanism to enable robust switch between stages. Extensive experiments on several benchmarks show that our UniGoal achieves state-of-the-art zero-shot performance on three studied navigation tasks with a single model, even outperforming task-specific zero-shot methods and supervised universal methods.', 'score': 6, 'issue_id': 2700, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 13', 'zh': '3æœˆ13æ—¥'}, 'hash': '9e7667a37c699f4c', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#reasoning', '#long_context', '#games', '#graphs', '#benchmark', '#agents', '#rl'], 'emoji': 'ğŸ§­', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ñ Ñ Ğ½ÑƒĞ»ĞµĞ²Ñ‹Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ² Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ½ÑƒĞ»ĞµĞ²Ñ‹Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğº Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ñ†ĞµĞ»ÑĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ĞµĞ´Ğ¸Ğ½Ğ¾Ğµ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ²Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ½Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ñ†ĞµĞ»ĞµĞ¹ Ğ¸ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ², ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ³Ñ€Ğ°Ñ„Ğ° ÑÑ†ĞµĞ½Ñ‹ Ğ¸ Ğ³Ñ€Ğ°Ñ„Ğ° Ñ†ĞµĞ»Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ UniGoal Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ½ÑƒĞ»ĞµĞ²Ñ‹Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Navigating Goals Universally with Graphs!', 'desc': 'This paper introduces a new framework for universal zero-shot goal-oriented navigation, which allows an agent to navigate towards various goals without prior training on specific tasks. Unlike existing methods that rely heavily on large language models (LLMs) tailored for individual tasks, this approach uses a uniform graph representation to integrate different types of goals, such as object categories and text descriptions. The agent maintains an online scene graph that captures the environment, enabling it to perform graph-based reasoning and match its observations with the goals. The proposed method demonstrates superior performance in navigation tasks, surpassing both task-specific and supervised approaches, by effectively managing goal exploration and verification through innovative strategies.'}, 'zh': {'title': 'é€šç”¨é›¶-shotå¯¼èˆªçš„åˆ›æ–°æ¡†æ¶', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šç”¨çš„é›¶-shotç›®æ ‡å¯¼å‘å¯¼èˆªæ¡†æ¶ã€‚ç°æœ‰çš„é›¶-shotæ–¹æ³•é€šå¸¸ä¾èµ–äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œç‰¹å®šä»»åŠ¡çš„æ¨ç†ï¼Œå¯¼è‡´åœ¨ä¸åŒç›®æ ‡ä¹‹é—´çš„æ³›åŒ–èƒ½åŠ›ä¸è¶³ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„å›¾è¡¨ç¤ºæ–¹æ³•ï¼Œå°†ä¸åŒçš„ç›®æ ‡ï¼ˆå¦‚ç‰©ä½“ç±»åˆ«ã€å®ä¾‹å›¾åƒå’Œæ–‡æœ¬æè¿°ï¼‰æ•´åˆåœ¨ä¸€èµ·ï¼Œå¹¶å°†ä»£ç†çš„è§‚å¯Ÿç»“æœè½¬æ¢ä¸ºåœ¨çº¿ç»´æŠ¤çš„åœºæ™¯å›¾ã€‚é€šè¿‡è¿™ç§ä¸€è‡´çš„åœºæ™¯å’Œç›®æ ‡è¡¨ç¤ºï¼Œæˆ‘ä»¬èƒ½å¤Ÿåˆ©ç”¨LLMè¿›è¡ŒåŸºäºå›¾çš„æ¨ç†ï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å±•ç¤ºäº†æˆ‘ä»¬çš„UniGoalåœ¨é›¶-shotå¯¼èˆªä»»åŠ¡ä¸Šçš„ä¼˜è¶Šæ€§èƒ½ã€‚'}}, 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2503.10568', 'title': 'Autoregressive Image Generation with Randomized Parallel Decoding', 'url': 'https://huggingface.co/papers/2503.10568', 'abstract': 'We introduce ARPG, a novel visual autoregressive model that enables randomized parallel generation, addressing the inherent limitations of conventional raster-order approaches, which hinder inference efficiency and zero-shot generalization due to their sequential, predefined token generation order. Our key insight is that effective random-order modeling necessitates explicit guidance for determining the position of the next predicted token. To this end, we propose a novel guided decoding framework that decouples positional guidance from content representation, encoding them separately as queries and key-value pairs. By directly incorporating this guidance into the causal attention mechanism, our approach enables fully random-order training and generation, eliminating the need for bidirectional attention. Consequently, ARPG readily generalizes to zero-shot tasks such as image inpainting, outpainting, and resolution expansion. Furthermore, it supports parallel inference by concurrently processing multiple queries using a shared KV cache. On the ImageNet-1K 256 benchmark, our approach attains an FID of 1.94 with only 64 sampling steps, achieving over a 20-fold increase in throughput while reducing memory consumption by over 75% compared to representative recent autoregressive models at a similar scale.', 'score': 6, 'issue_id': 2701, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 13', 'zh': '3æœˆ13æ—¥'}, 'hash': '44a1e551455870ac', 'authors': ['Haopeng Li', 'Jinyue Yang', 'Guoqi Li', 'Huan Wang'], 'affiliations': ['Institute of Automation, Chinese Academy of Sciences', 'Westlake University'], 'pdf_title_img': 'assets/pdf/title_img/2503.10568.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#cv', '#architecture'], 'emoji': 'ğŸ¨', 'ru': {'title': 'ARPG: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞ¾Ğ¼ Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ¾Ğ¼', 'desc': 'ARPG - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ°Ñ Ğ¾ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ğ»ÑÑ‚ÑŒ Ñ€Ğ°Ğ½Ğ´Ğ¾Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ½Ğ° Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ² Ñ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞ¾Ğ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°Ñ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°. ARPG Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ² Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾Ğ¼ Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞµ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ĞµĞ¹ Ğ»ĞµĞ³ĞºĞ¾ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ‚Ğ°ĞºĞ¸Ğ¼ ĞºĞ°Ğº Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ğ¸ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ°Ğ¼Ğ¸, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ FID 1.94 Ğ½Ğ° ImageNet-1K 256 Ğ²ÑĞµĞ³Ğ¾ Ğ·Ğ° 64 ÑˆĞ°Ğ³Ğ° ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'Revolutionizing Image Generation with Randomized Parallel Processing', 'desc': 'The paper presents ARPG, a new visual autoregressive model designed for efficient and flexible image generation. Unlike traditional methods that generate tokens in a fixed order, ARPG allows for randomized parallel generation, improving inference speed and enabling zero-shot generalization. It introduces a guided decoding framework that separates positional guidance from content representation, enhancing the causal attention mechanism. As a result, ARPG excels in tasks like image inpainting and outpainting, achieving significant performance improvements on benchmarks while using less memory.'}, 'zh': {'title': 'ARPGï¼šå®ç°éšæœºå¹¶è¡Œç”Ÿæˆçš„è‡ªå›å½’æ¨¡å‹', 'desc': 'æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„è§†è§‰è‡ªå›å½’æ¨¡å‹ARPGï¼Œèƒ½å¤Ÿå®ç°éšæœºå¹¶è¡Œç”Ÿæˆï¼Œè§£å†³äº†ä¼ ç»Ÿå…‰æ …é¡ºåºæ–¹æ³•åœ¨æ¨ç†æ•ˆç‡å’Œé›¶-shotæ³›åŒ–æ–¹é¢çš„å›ºæœ‰é™åˆ¶ã€‚æˆ‘ä»¬çš„å…³é”®è§è§£æ˜¯ï¼Œæœ‰æ•ˆçš„éšæœºé¡ºåºå»ºæ¨¡éœ€è¦æ˜ç¡®çš„æŒ‡å¯¼æ¥ç¡®å®šä¸‹ä¸€ä¸ªé¢„æµ‹æ ‡è®°çš„ä½ç½®ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¼•å¯¼è§£ç æ¡†æ¶ï¼Œå°†ä½ç½®æŒ‡å¯¼ä¸å†…å®¹è¡¨ç¤ºåˆ†å¼€ç¼–ç ï¼Œåˆ†åˆ«ä½œä¸ºæŸ¥è¯¢å’Œé”®å€¼å¯¹ã€‚é€šè¿‡å°†è¿™ç§æŒ‡å¯¼ç›´æ¥èå…¥å› æœæ³¨æ„åŠ›æœºåˆ¶ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†å®Œå…¨éšæœºé¡ºåºçš„è®­ç»ƒå’Œç”Ÿæˆï¼Œæ¶ˆé™¤äº†å¯¹åŒå‘æ³¨æ„åŠ›çš„éœ€æ±‚ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.10365', 'title': 'Piece it Together: Part-Based Concepting with IP-Priors', 'url': 'https://huggingface.co/papers/2503.10365', 'abstract': 'Advanced generative models excel at synthesizing images but often rely on text-based conditioning. Visual designers, however, often work beyond language, directly drawing inspiration from existing visual elements. In many cases, these elements represent only fragments of a potential concept-such as an uniquely structured wing, or a specific hairstyle-serving as inspiration for the artist to explore how they can come together creatively into a coherent whole. Recognizing this need, we introduce a generative framework that seamlessly integrates a partial set of user-provided visual components into a coherent composition while simultaneously sampling the missing parts needed to generate a plausible and complete concept. Our approach builds on a strong and underexplored representation space, extracted from IP-Adapter+, on which we train IP-Prior, a lightweight flow-matching model that synthesizes coherent compositions based on domain-specific priors, enabling diverse and context-aware generations. Additionally, we present a LoRA-based fine-tuning strategy that significantly improves prompt adherence in IP-Adapter+ for a given task, addressing its common trade-off between reconstruction quality and prompt adherence.', 'score': 6, 'issue_id': 2713, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 13', 'zh': '3æœˆ13æ—¥'}, 'hash': '80bcd7657c63561e', 'authors': ['Elad Richardson', 'Kfir Goldberg', 'Yuval Alaluf', 'Daniel Cohen-Or'], 'affiliations': ['Bria AI', 'Tel Aviv University'], 'pdf_title_img': 'assets/pdf/title_img/2503.10365.jpg', 'data': {'categories': ['#training', '#cv', '#multimodal', '#synthetic'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ²: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ‚Ğ²Ğ¾Ñ€Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ñƒ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹, Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¼, Ğ² Ñ†ĞµĞ»Ğ¾ÑÑ‚Ğ½ÑƒÑ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ IP-Prior, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ½Ğ° Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ IP-Adapter+, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ğ¾-ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¾Ğ². ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾-Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ñ‹Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LoRA, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñƒ Ğ² IP-Adapter+.'}, 'en': {'title': 'Bridging Visual Inspiration with Generative Design', 'desc': "This paper presents a new generative framework that allows visual designers to create coherent compositions by integrating partial visual elements provided by users. Unlike traditional models that rely heavily on text descriptions, this approach focuses on visual components, enabling artists to explore creative combinations. The framework utilizes a lightweight flow-matching model called IP-Prior, which is trained on a specialized representation space to ensure context-aware and diverse outputs. Additionally, the authors introduce a fine-tuning strategy that enhances the model's ability to adhere to user prompts while maintaining high reconstruction quality."}, 'zh': {'title': 'æ— ç¼æ•´åˆè§†è§‰å…ƒç´ ï¼Œåˆ›é€ è¿è´¯æ¦‚å¿µ', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§ç”Ÿæˆæ¡†æ¶ï¼Œèƒ½å¤Ÿå°†ç”¨æˆ·æä¾›çš„éƒ¨åˆ†è§†è§‰å…ƒç´ æ— ç¼æ•´åˆæˆä¸€ä¸ªè¿è´¯çš„æ•´ä½“ï¼ŒåŒæ—¶ç”Ÿæˆæ‰€éœ€çš„ç¼ºå¤±éƒ¨åˆ†ï¼Œä»¥åˆ›é€ å‡ºåˆç†ä¸”å®Œæ•´çš„æ¦‚å¿µã€‚è¯¥æ–¹æ³•åŸºäºä¸€ä¸ªå¼ºå¤§ä¸”æœªè¢«å……åˆ†æ¢ç´¢çš„è¡¨ç¤ºç©ºé—´ï¼Œåˆ©ç”¨IP-Adapter+æå–çš„ç‰¹å¾ï¼Œè®­ç»ƒå‡ºè½»é‡çº§çš„æµåŒ¹é…æ¨¡å‹IP-Priorã€‚æ­¤æ¨¡å‹èƒ½å¤ŸåŸºäºç‰¹å®šé¢†åŸŸçš„å…ˆéªŒçŸ¥è¯†åˆæˆè¿è´¯çš„ä½œå“ï¼Œå®ç°å¤šæ ·åŒ–å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„ç”Ÿæˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§åŸºäºLoRAçš„å¾®è°ƒç­–ç•¥ï¼Œæ˜¾è‘—æé«˜äº†IP-Adapter+åœ¨ç‰¹å®šä»»åŠ¡ä¸­çš„æç¤ºéµå¾ªæ€§ï¼Œè§£å†³äº†é‡å»ºè´¨é‡ä¸æç¤ºéµå¾ªæ€§ä¹‹é—´çš„å¸¸è§æƒè¡¡é—®é¢˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.09905', 'title': "Quantization for OpenAI's Whisper Models: A Comparative Analysis", 'url': 'https://huggingface.co/papers/2503.09905', 'abstract': 'Automated speech recognition (ASR) models have gained prominence for applications such as captioning, speech translation, and live transcription. This paper studies Whisper and two model variants: one optimized for live speech streaming and another for offline transcription. Notably, these models have been found to generate hallucinated content, reducing transcription reliability. Furthermore, larger model variants exhibit increased latency and pose challenges for deployment on resource-constrained devices. This study analyzes the similarities and differences between three Whisper models, qualitatively examining their distinct capabilities. Next, this study quantifies the impact of model quantization on latency and evaluates its viability for edge deployment. Using the open source LibriSpeech dataset, this paper evaluates the word error rate (WER) along with latency analysis of whispercpp using 3 quantization methods (INT4, INT5, INT8). Results show that quantization reduces latency by 19\\% and model size by 45\\%, while preserving transcription accuracy. These findings provide insights into the optimal use cases of different Whisper models and edge device deployment possibilities. All code, datasets, and implementation details are available in a public GitHub repository: https://github.com/allisonandreyev/WhisperQuantization.git', 'score': 6, 'issue_id': 2700, 'pub_date': '2025-03-12', 'pub_date_card': {'ru': '12 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 12', 'zh': '3æœˆ12æ—¥'}, 'hash': '8d7b17a97a4cbb6e', 'authors': ['Allison Andreyev'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2503.09905.jpg', 'data': {'categories': ['#inference', '#audio', '#open_source', '#optimization', '#hallucinations', '#dataset'], 'emoji': 'ğŸ™ï¸', 'ru': {'title': 'ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Whisper Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸ Ğ½Ğ° Ğ¿ĞµÑ€Ğ¸Ñ„ĞµÑ€Ğ¸Ğ¹Ğ½Ñ‹Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ…', 'desc': 'Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸ Whisper Ğ¸ Ğ¸Ñ… Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¸ Ğ¾Ñ„Ğ»Ğ°Ğ¹Ğ½-Ñ‚Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ¿Ñ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸Ğ·ÑƒÑ‡Ğ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ… Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµÑÑƒÑ€ÑĞ°Ğ¼Ğ¸. ĞŸÑ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºÑƒ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‚Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ¿Ñ†Ğ¸Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… LibriSpeech. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ĞµÑ‚ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºÑƒ Ğ½Ğ° 19% Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° 45%, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‚Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ¿Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'Optimizing Whisper Models for Efficient Speech Recognition', 'desc': 'This paper investigates the performance of Whisper, an automated speech recognition (ASR) model, along with its two variants tailored for live streaming and offline transcription. It highlights the issue of hallucinated content in the transcriptions, which can compromise reliability, especially in larger models that also face latency challenges. The study further explores the effects of model quantization on latency and size, demonstrating a significant reduction in both while maintaining transcription accuracy. By analyzing the word error rate (WER) using the LibriSpeech dataset, the research provides valuable insights for deploying Whisper models on resource-constrained devices.'}, 'zh': {'title': 'ä¼˜åŒ–Whisperæ¨¡å‹ï¼Œæå‡è¯­éŸ³è¯†åˆ«æ•ˆç‡', 'desc': 'è¿™ç¯‡è®ºæ–‡ç ”ç©¶äº†è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹WhisperåŠå…¶ä¸¤ä¸ªå˜ä½“ï¼Œä¸€ä¸ªé’ˆå¯¹å®æ—¶è¯­éŸ³æµä¼˜åŒ–ï¼Œå¦ä¸€ä¸ªç”¨äºç¦»çº¿è½¬å½•ã€‚ç ”ç©¶å‘ç°ï¼Œè¿™äº›æ¨¡å‹å¯èƒ½ä¼šç”Ÿæˆè™šå‡å†…å®¹ï¼Œä»è€Œé™ä½è½¬å½•çš„å¯é æ€§ã€‚æ­¤å¤–ï¼Œè¾ƒå¤§çš„æ¨¡å‹å˜ä½“åœ¨å»¶è¿Ÿæ–¹é¢è¡¨ç°è¾ƒå·®ï¼Œç»™èµ„æºå—é™çš„è®¾å¤‡éƒ¨ç½²å¸¦æ¥äº†æŒ‘æˆ˜ã€‚é€šè¿‡å¯¹æ¯”åˆ†æï¼Œè®ºæ–‡é‡åŒ–äº†æ¨¡å‹é‡åŒ–å¯¹å»¶è¿Ÿçš„å½±å“ï¼Œå¹¶è¯„ä¼°äº†å…¶åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šçš„å¯è¡Œæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.09837', 'title': 'On the Limitations of Vision-Language Models in Understanding Image\n  Transforms', 'url': 'https://huggingface.co/papers/2503.09837', 'abstract': 'Vision Language Models (VLMs) have demonstrated significant potential in various downstream tasks, including Image/Video Generation, Visual Question Answering, Multimodal Chatbots, and Video Understanding. However, these models often struggle with basic image transformations. This paper investigates the image-level understanding of VLMs, specifically CLIP by OpenAI and SigLIP by Google. Our findings reveal that these models lack comprehension of multiple image-level augmentations. To facilitate this study, we created an augmented version of the Flickr8k dataset, pairing each image with a detailed description of the applied transformation. We further explore how this deficiency impacts downstream tasks, particularly in image editing, and evaluate the performance of state-of-the-art Image2Image models on simple transformations.', 'score': 6, 'issue_id': 2709, 'pub_date': '2025-03-12', 'pub_date_card': {'ru': '12 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 12', 'zh': '3æœˆ12æ—¥'}, 'hash': 'cc8e8180cfeef80d', 'authors': ['Ahmad Mustafa Anis', 'Hasnain Ali', 'Saquib Sarfraz'], 'affiliations': ['Arbisoft', 'Cohere for AI Community', 'Karlsruhe Institute of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2503.09837.jpg', 'data': {'categories': ['#dataset', '#cv', '#multimodal', '#games', '#interpretability'], 'emoji': 'ğŸ”', 'ru': {'title': 'ĞĞ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… VLM', 'desc': 'Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° (VLM) Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ CLIP Ğ¸ SigLIP Ğ½Ğµ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ½Ğ¾Ğ³Ğ¸Ñ… Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ”Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ñ‹Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Flickr8k Ñ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ÑĞ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ñ‘Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¹. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¸Ğ·ÑƒÑ‡Ğ°Ğ»Ğ¾ÑÑŒ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚ĞºĞ° Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ğ»Ğ°ÑÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Image2Image Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ… Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹.'}, 'en': {'title': 'Unlocking Image Transformations in Vision Language Models', 'desc': 'This paper examines the limitations of Vision Language Models (VLMs) like CLIP and SigLIP in understanding basic image transformations. Despite their success in tasks such as image generation and visual question answering, these models struggle with comprehending various image-level augmentations. The authors created an augmented Flickr8k dataset to analyze how well these models recognize and describe transformations applied to images. The study also assesses the impact of these deficiencies on downstream tasks, particularly in image editing, and evaluates the performance of advanced Image2Image models in handling simple transformations.'}, 'zh': {'title': 'æå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„å›¾åƒç†è§£èƒ½åŠ›', 'desc': 'æœ¬æ–‡ç ”ç©¶äº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å›¾åƒç†è§£æ–¹é¢çš„ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯OpenAIçš„CLIPå’ŒGoogleçš„SigLIPã€‚å°½ç®¡è¿™äº›æ¨¡å‹åœ¨å¤šç§ä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬åœ¨åŸºæœ¬å›¾åƒå˜æ¢æ–¹é¢å­˜åœ¨å›°éš¾ã€‚æˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªå¢å¼ºç‰ˆçš„Flickr8kæ•°æ®é›†ï¼Œä¸ºæ¯å¼ å›¾åƒé…ä¸Šè¯¦ç»†çš„å˜æ¢æè¿°ï¼Œä»¥ä¾¿è¿›è¡Œç ”ç©¶ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè¿™ç§ç†è§£ç¼ºé™·ä¼šå½±å“ä¸‹æ¸¸ä»»åŠ¡çš„è¡¨ç°ï¼Œå°¤å…¶æ˜¯åœ¨å›¾åƒç¼–è¾‘æ–¹é¢ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.09046', 'title': 'Discovering Influential Neuron Path in Vision Transformers', 'url': 'https://huggingface.co/papers/2503.09046', 'abstract': "Vision Transformer models exhibit immense power yet remain opaque to human understanding, posing challenges and risks for practical applications. While prior research has attempted to demystify these models through input attribution and neuron role analysis, there's been a notable gap in considering layer-level information and the holistic path of information flow across layers. In this paper, we investigate the significance of influential neuron paths within vision Transformers, which is a path of neurons from the model input to output that impacts the model inference most significantly. We first propose a joint influence measure to assess the contribution of a set of neurons to the model outcome. And we further provide a layer-progressive neuron locating approach that efficiently selects the most influential neuron at each layer trying to discover the crucial neuron path from input to output within the target model. Our experiments demonstrate the superiority of our method finding the most influential neuron path along which the information flows, over the existing baseline solutions. Additionally, the neuron paths have illustrated that vision Transformers exhibit some specific inner working mechanism for processing the visual information within the same image category. We further analyze the key effects of these neurons on the image classification task, showcasing that the found neuron paths have already preserved the model capability on downstream tasks, which may also shed some lights on real-world applications like model pruning. The project website including implementation code is available at https://foundation-model-research.github.io/NeuronPath/.", 'score': 6, 'issue_id': 2712, 'pub_date': '2025-03-12', 'pub_date_card': {'ru': '12 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 12', 'zh': '3æœˆ12æ—¥'}, 'hash': 'c4d7c64b526c6486', 'authors': ['Yifan Wang', 'Yifei Liu', 'Yingdong Shi', 'Changming Li', 'Anqi Pang', 'Sibei Yang', 'Jingyi Yu', 'Kan Ren'], 'affiliations': ['ShanghaiTech University', 'Tencent PCG'], 'pdf_title_img': 'assets/pdf/title_img/2503.09046.jpg', 'data': {'categories': ['#architecture', '#interpretability', '#cv', '#training', '#inference'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ Ğ°ÑĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ Ñ‚Ğ°Ğ¹Ğ½ Vision Transformer: Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ»Ğ¸ÑÑ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¿ÑƒÑ‚ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ğ¼Ñ‹Ğµ Ğ¿ÑƒÑ‚Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ¾Ğ² Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Vision Transformer, Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ Ğ³Ñ€ÑƒĞ¿Ğ¿ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ¾Ğ² Ğ½Ğ° Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¿Ğ¾ÑĞ»Ğ¾Ğ¹Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ğ»Ğ¸ÑÑ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ¾Ğ², Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ñ‚ÑŒ ĞºĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ¿ÑƒÑ‚ÑŒ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ‚ Ğ²Ñ…Ğ¾Ğ´Ğ° Ğº Ğ²Ñ‹Ñ…Ğ¾Ğ´Ñƒ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ² Ğ¿Ğ¾Ğ¸ÑĞºĞµ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ğ¼Ñ‹Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¿ÑƒÑ‚ĞµĞ¹. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿ÑƒÑ‚ĞµĞ¹ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ¾Ğ² Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ» ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ² Vision Transformer Ğ¸ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ´Ğ»Ñ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº pruning Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Unveiling the Neuron Paths in Vision Transformers', 'desc': "This paper explores how to better understand Vision Transformer models by focusing on the paths of influential neurons that contribute significantly to model predictions. It introduces a joint influence measure to evaluate the impact of neuron sets on the output and proposes a method to identify the most important neurons layer by layer. The findings reveal that these neuron paths not only clarify the model's decision-making process but also maintain the model's performance on tasks like image classification. This research could lead to practical applications such as model pruning, enhancing the efficiency of Vision Transformers in real-world scenarios."}, 'zh': {'title': 'æ­ç¤ºè§†è§‰å˜æ¢å™¨ä¸­çš„å…³é”®ç¥ç»å…ƒè·¯å¾„', 'desc': 'æœ¬æ–‡ç ”ç©¶äº†è§†è§‰å˜æ¢å™¨æ¨¡å‹ä¸­å½±å“åŠ›ç¥ç»å…ƒè·¯å¾„çš„é‡è¦æ€§ï¼Œè¿™äº›è·¯å¾„ä»æ¨¡å‹è¾“å…¥åˆ°è¾“å‡ºï¼Œå¯¹æ¨¡å‹æ¨ç†æœ‰æ˜¾è‘—å½±å“ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§è”åˆå½±å“åº¦é‡æ–¹æ³•ï¼Œè¯„ä¼°ä¸€ç»„ç¥ç»å…ƒå¯¹æ¨¡å‹ç»“æœçš„è´¡çŒ®ï¼Œå¹¶æä¾›äº†ä¸€ç§é€å±‚ç¥ç»å…ƒå®šä½æ–¹æ³•ï¼Œä»¥é«˜æ•ˆé€‰æ‹©æ¯å±‚ä¸­æœ€å…·å½±å“åŠ›çš„ç¥ç»å…ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¯»æ‰¾ä¿¡æ¯æµåŠ¨çš„æœ€å…·å½±å“åŠ›ç¥ç»å…ƒè·¯å¾„æ–¹é¢ä¼˜äºç°æœ‰åŸºçº¿è§£å†³æ–¹æ¡ˆã€‚æ­¤å¤–ï¼Œè¿™äº›ç¥ç»å…ƒè·¯å¾„æ­ç¤ºäº†è§†è§‰å˜æ¢å™¨åœ¨å¤„ç†åŒä¸€å›¾åƒç±»åˆ«çš„è§†è§‰ä¿¡æ¯æ—¶çš„ç‰¹å®šå†…éƒ¨å·¥ä½œæœºåˆ¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.10242', 'title': 'MinorBench: A hand-built benchmark for content-based risks for children', 'url': 'https://huggingface.co/papers/2503.10242', 'abstract': "Large Language Models (LLMs) are rapidly entering children's lives - through parent-driven adoption, schools, and peer networks - yet current AI ethics and safety research do not adequately address content-related risks specific to minors. In this paper, we highlight these gaps with a real-world case study of an LLM-based chatbot deployed in a middle school setting, revealing how students used and sometimes misused the system. Building on these findings, we propose a new taxonomy of content-based risks for minors and introduce MinorBench, an open-source benchmark designed to evaluate LLMs on their ability to refuse unsafe or inappropriate queries from children. We evaluate six prominent LLMs under different system prompts, demonstrating substantial variability in their child-safety compliance. Our results inform practical steps for more robust, child-focused safety mechanisms and underscore the urgency of tailoring AI systems to safeguard young users.", 'score': 4, 'issue_id': 2712, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 13', 'zh': '3æœˆ13æ—¥'}, 'hash': '28234b0e69863cbe', 'authors': ['Shaun Khoo', 'Gabriel Chua', 'Rachel Shong'], 'affiliations': ['Government Technology Agency Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2503.10242.jpg', 'data': {'categories': ['#open_source', '#multimodal', '#benchmark', '#ethics'], 'emoji': 'ğŸ§’', 'ru': {'title': 'Ğ—Ğ°Ñ‰Ğ¸Ñ‚Ğ° Ğ´ĞµÑ‚ĞµĞ¹ Ğ¾Ñ‚ Ñ€Ğ¸ÑĞºĞ¾Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ”Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ñ€Ğ¸ÑĞºĞ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ´ĞµÑ‚ÑŒĞ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´ÑÑ‚ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ»ÑƒÑ‡Ğ°Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ñ‡Ğ°Ñ‚-Ğ±Ğ¾Ñ‚Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLM Ğ² ÑÑ€ĞµĞ´Ğ½ĞµĞ¹ ÑˆĞºĞ¾Ğ»Ğµ. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ğ°Ñ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ñ€Ğ¸ÑĞºĞ¾Ğ² Ğ´Ğ»Ñ Ğ½ĞµÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½Ğ½Ğ¾Ğ»ĞµÑ‚Ğ½Ğ¸Ñ… Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ÑÑ MinorBench - Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ LLM Ğ¾Ñ‚ĞºĞ»Ğ¾Ğ½ÑÑ‚ÑŒ Ğ½ĞµĞ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ğµ Ğ¸Ğ»Ğ¸ Ğ½ĞµÑƒĞ¼ĞµÑÑ‚Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¾Ñ‚ Ğ´ĞµÑ‚ĞµĞ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ñ… Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸, Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ´ĞµÑ‚ĞµĞ¹.'}, 'en': {'title': 'Ensuring Child Safety in AI: A Call for Better LLM Standards', 'desc': 'This paper discusses the increasing presence of Large Language Models (LLMs) in the lives of children and the associated risks that are not adequately addressed by current AI ethics. It presents a case study of a chatbot used in a middle school, highlighting both the positive and negative ways students interacted with the system. The authors propose a new classification system for content-related risks specific to minors and introduce MinorBench, a benchmark to assess LLMs on their ability to handle inappropriate queries from children. The evaluation of six LLMs shows significant differences in their compliance with child safety standards, emphasizing the need for improved safety measures tailored for young users.'}, 'zh': {'title': 'ä¿æŠ¤å„¿ç«¥ï¼Œå®‰å…¨ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹', 'desc': 'æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å„¿ç«¥ç”Ÿæ´»ä¸­çš„åº”ç”¨ï¼Œå°¤å…¶æ˜¯åœ¨å­¦æ ¡ç¯å¢ƒä¸­çš„ä½¿ç”¨æƒ…å†µã€‚é€šè¿‡æ¡ˆä¾‹ç ”ç©¶ï¼Œæˆ‘ä»¬å‘ç°å­¦ç”Ÿåœ¨ä½¿ç”¨èŠå¤©æœºå™¨äººæ—¶å­˜åœ¨å†…å®¹ç›¸å…³çš„é£é™©ï¼Œå°¤å…¶æ˜¯å¯¹æœªæˆå¹´äººè€Œè¨€ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æœªæˆå¹´äººå†…å®¹é£é™©åˆ†ç±»æ³•ï¼Œå¹¶å¼•å…¥äº†MinorBenchï¼Œä¸€ä¸ªå¼€æºåŸºå‡†ï¼Œç”¨äºè¯„ä¼°LLMsæ‹’ç»ä¸å®‰å…¨æˆ–ä¸å½“æŸ¥è¯¢çš„èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œä¸åŒçš„LLMsåœ¨å„¿ç«¥å®‰å…¨åˆè§„æ€§æ–¹é¢å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œå¼ºè°ƒäº†ä¸ºä¿æŠ¤å¹´è½»ç”¨æˆ·è€Œå®šåˆ¶AIç³»ç»Ÿçš„ç´§è¿«æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.10072', 'title': '"Silent Is Not Actually Silent": An Investigation of Toxicity on Bug\n  Report Discussion', 'url': 'https://huggingface.co/papers/2503.10072', 'abstract': 'Toxicity in bug report discussions poses significant challenges to the collaborative dynamics of open-source software development. Bug reports are crucial for identifying and resolving defects, yet their inherently problem-focused nature and emotionally charged context make them susceptible to toxic interactions. This study explores toxicity in GitHub bug reports through a qualitative analysis of 203 bug threads, including 81 toxic ones. Our findings reveal that toxicity frequently arises from misaligned perceptions of bug severity and priority, unresolved frustrations with tools, and lapses in professional communication. These toxic interactions not only derail productive discussions but also reduce the likelihood of actionable outcomes, such as linking issues with pull requests. Our preliminary findings offer actionable recommendations to improve bug resolution by mitigating toxicity.', 'score': 4, 'issue_id': 2699, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 13', 'zh': '3æœˆ13æ—¥'}, 'hash': 'a91b6bc8232847a3', 'authors': ['Mia Mohammad Imran', 'Jaydeb Sarker'], 'affiliations': ['Missouri University of Science and Technology, Rolla, Missouri, USA', 'University of Nebraska Omaha, Omaha, Nebraska, USA'], 'pdf_title_img': 'assets/pdf/title_img/2503.10072.jpg', 'data': {'categories': ['#ethics', '#open_source'], 'emoji': 'ğŸ›', 'ru': {'title': 'Ğ¢Ğ¾ĞºÑĞ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ°Ñ… Ğ¾Ğ± Ğ¾ÑˆĞ¸Ğ±ĞºĞ°Ñ…: Ğ¿Ñ€ĞµĞ¿ÑÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ´ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğµ Ñ‚Ğ¾ĞºÑĞ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¾Ğ±ÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ… Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ¾Ğ² Ğ¾Ğ± Ğ¾ÑˆĞ¸Ğ±ĞºĞ°Ñ… Ğ² Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ°Ñ… Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· 203 Ğ²ĞµÑ‚Ğ¾Ğº Ğ¾Ğ±ÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ½Ğ° GitHub, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ 81 Ñ‚Ğ¾ĞºÑĞ¸Ñ‡Ğ½ÑƒÑ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ¾ĞºÑĞ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ĞµÑ‚ Ğ¸Ğ·-Ğ·Ğ° Ñ€Ğ°ÑÑ…Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğ¸ ÑĞµÑ€ÑŒĞµĞ·Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº, Ğ½ĞµÑ€ĞµÑˆĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ¸ Ğ½Ğ°Ñ€ÑƒÑˆĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¼ÑƒĞ½Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸. Ğ¢Ğ¾ĞºÑĞ¸Ñ‡Ğ½Ñ‹Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ°Ñ€ÑƒÑˆĞ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ´ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¾Ğ±ÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ½Ğ¾ Ğ¸ ÑĞ½Ğ¸Ğ¶Ğ°ÑÑ‚ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ², Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºĞ° Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğº Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼ Ğ½Ğ° ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ.'}, 'en': {'title': 'Mitigating Toxicity for Better Bug Resolution in Open Source', 'desc': 'This paper investigates the issue of toxicity in bug report discussions within open-source software development on GitHub. It analyzes 203 bug threads, identifying 81 as toxic, and highlights that toxicity often stems from differing views on bug severity, frustrations with tools, and poor communication. The study emphasizes that such toxic interactions hinder productive collaboration and decrease the chances of resolving issues effectively. The authors provide recommendations aimed at reducing toxicity to enhance the bug resolution process.'}, 'zh': {'title': 'å‡å°‘æ¯’æ€§ï¼Œæå‡å¼€æºåä½œæ•ˆç‡', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†å¼€æºè½¯ä»¶å¼€å‘ä¸­ï¼ŒGitHub bug æŠ¥å‘Šè®¨è®ºä¸­çš„æ¯’æ€§é—®é¢˜ã€‚é€šè¿‡å¯¹203ä¸ªbugçº¿ç¨‹çš„å®šæ€§åˆ†æï¼Œå‘ç°81ä¸ªçº¿ç¨‹å­˜åœ¨æ¯’æ€§äº’åŠ¨ã€‚ç ”ç©¶è¡¨æ˜ï¼Œæ¯’æ€§å¾€å¾€æºäºå¯¹bugä¸¥é‡æ€§å’Œä¼˜å…ˆçº§çš„è¯¯è§£ã€å¯¹å·¥å…·çš„ä¸æ»¡ä»¥åŠä¸“ä¸šæ²Ÿé€šçš„ç¼ºå¤±ã€‚è¿™äº›æ¯’æ€§äº’åŠ¨ä¸ä»…å½±å“äº†æœ‰æ•ˆè®¨è®ºï¼Œè¿˜é™ä½äº†å°†é—®é¢˜ä¸æ‹‰å–è¯·æ±‚å…³è”çš„å¯èƒ½æ€§ï¼Œå› æ­¤æå‡ºäº†å‡å°‘æ¯’æ€§ä»¥æ”¹å–„bugè§£å†³çš„å»ºè®®ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.10636', 'title': 'The Curse of Conditions: Analyzing and Improving Optimal Transport for\n  Conditional Flow-Based Generation', 'url': 'https://huggingface.co/papers/2503.10636', 'abstract': 'Minibatch optimal transport coupling straightens paths in unconditional flow matching. This leads to computationally less demanding inference as fewer integration steps and less complex numerical solvers can be employed when numerically solving an ordinary differential equation at test time. However, in the conditional setting, minibatch optimal transport falls short. This is because the default optimal transport mapping disregards conditions, resulting in a conditionally skewed prior distribution during training. In contrast, at test time, we have no access to the skewed prior, and instead sample from the full, unbiased prior distribution. This gap between training and testing leads to a subpar performance. To bridge this gap, we propose conditional optimal transport C^2OT that adds a conditional weighting term in the cost matrix when computing the optimal transport assignment. Experiments demonstrate that this simple fix works with both discrete and continuous conditions in 8gaussians-to-moons, CIFAR-10, ImageNet-32x32, and ImageNet-256x256. Our method performs better overall compared to the existing baselines across different function evaluation budgets. Code is available at https://hkchengrex.github.io/C2OT', 'score': 3, 'issue_id': 2700, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 13', 'zh': '3æœˆ13æ—¥'}, 'hash': '93d6f410c35246be', 'authors': ['Ho Kei Cheng', 'Alexander Schwing'], 'affiliations': ['University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2503.10636.jpg', 'data': {'categories': ['#cv', '#inference', '#optimization', '#training'], 'emoji': 'ğŸ”€', 'ru': {'title': 'Ğ£ÑĞ»Ğ¾Ğ²Ğ½Ñ‹Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑĞ¿Ğ¾Ñ€Ñ‚ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ‚Ñ€Ğ°Ğ½ÑĞ¿Ğ¾Ñ€Ñ‚Ğ° (C^2OT) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ Ğ¸ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸-Ğ±Ğ°Ñ‚Ñ‡ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ‚Ñ€Ğ°Ğ½ÑĞ¿Ğ¾Ñ€Ñ‚Ğ° Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. C^2OT Ñ€ĞµÑˆĞ°ĞµÑ‚ ÑÑ‚Ñƒ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²ĞµÑĞ¾Ğ²Ğ¾Ğ³Ğ¾ ĞºĞ¾ÑÑ„Ñ„Ğ¸Ñ†Ğ¸ĞµĞ½Ñ‚Ğ° Ğ² Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ñƒ ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ‚Ñ€Ğ°Ğ½ÑĞ¿Ğ¾Ñ€Ñ‚Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ»Ğ¸Ğ½Ğ¸Ğ¹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğ°Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Bridging the Gap in Conditional Flow Matching with C^2OT', 'desc': 'This paper introduces a method called conditional optimal transport (C^2OT) to improve the performance of flow matching in machine learning. The authors highlight that while minibatch optimal transport simplifies computations during inference, it fails in conditional settings due to a mismatch between training and testing distributions. By incorporating a conditional weighting term in the cost matrix, C^2OT effectively aligns the training and testing conditions. Experiments show that this approach outperforms existing methods across various datasets, demonstrating its effectiveness in both discrete and continuous scenarios.'}, 'zh': {'title': 'æ¡ä»¶æœ€ä¼˜ä¼ è¾“ï¼šç¼©å°è®­ç»ƒä¸æµ‹è¯•çš„æ€§èƒ½å·®è·', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ¡ä»¶æœ€ä¼˜ä¼ è¾“æ–¹æ³•C^2OTï¼Œä»¥è§£å†³åœ¨æ¡ä»¶è®¾ç½®ä¸‹å°æ‰¹é‡æœ€ä¼˜ä¼ è¾“çš„ä¸è¶³ã€‚ä¼ ç»Ÿçš„æœ€ä¼˜ä¼ è¾“æ˜ å°„åœ¨è®­ç»ƒæ—¶å¿½ç•¥äº†æ¡ä»¶ï¼Œå¯¼è‡´è®­ç»ƒæœŸé—´çš„å…ˆéªŒåˆ†å¸ƒåæ–œï¼Œè€Œæµ‹è¯•æ—¶å´æ— æ³•è®¿é—®è¿™ç§åæ–œçš„å…ˆéªŒã€‚é€šè¿‡åœ¨æˆæœ¬çŸ©é˜µä¸­æ·»åŠ æ¡ä»¶åŠ æƒé¡¹ï¼ŒC^2OTèƒ½å¤Ÿæ›´å¥½åœ°è®¡ç®—æœ€ä¼˜ä¼ è¾“åˆ†é…ï¼Œä»è€Œç¼©å°è®­ç»ƒä¸æµ‹è¯•ä¹‹é—´çš„æ€§èƒ½å·®è·ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜äºç°æœ‰åŸºçº¿ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.10602', 'title': 'TruthPrInt: Mitigating LVLM Object Hallucination Via Latent\n  Truthful-Guided Pre-Intervention', 'url': 'https://huggingface.co/papers/2503.10602', 'abstract': 'Object Hallucination (OH) has been acknowledged as one of the major trustworthy challenges in Large Vision-Language Models (LVLMs). Recent advancements in Large Language Models (LLMs) indicate that internal states, such as hidden states, encode the "overall truthfulness" of generated responses. However, it remains under-explored how internal states in LVLMs function and whether they could serve as "per-token" hallucination indicators, which is essential for mitigating OH. In this paper, we first conduct an in-depth exploration of LVLM internal states in relation to OH issues and discover that (1) LVLM internal states are high-specificity per-token indicators of hallucination behaviors. Moreover, (2) different LVLMs encode universal patterns of hallucinations in common latent subspaces, indicating that there exist "generic truthful directions" shared by various LVLMs. Based on these discoveries, we propose Truthful-Guided Pre-Intervention (TruthPrInt) that first learns the truthful direction of LVLM decoding and then applies truthful-guided inference-time intervention during LVLM decoding. We further propose ComnHallu to enhance both cross-LVLM and cross-data hallucination detection transferability by constructing and aligning hallucination latent subspaces. We evaluate TruthPrInt in extensive experimental settings, including in-domain and out-of-domain scenarios, over popular LVLMs and OH benchmarks. Experimental results indicate that TruthPrInt significantly outperforms state-of-the-art methods. Codes will be available at https://github.com/jinhaoduan/TruthPrInt.', 'score': 3, 'issue_id': 2712, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 13', 'zh': '3æœˆ13æ—¥'}, 'hash': '9d4ccef966377a11', 'authors': ['Jinhao Duan', 'Fei Kong', 'Hao Cheng', 'James Diffenderfer', 'Bhavya Kailkhura', 'Lichao Sun', 'Xiaofeng Zhu', 'Xiaoshuang Shi', 'Kaidi Xu'], 'affiliations': ['Drexel University', 'Hong Kong University of Science and Technology (Guangzhou)', 'LLNL', 'Lehigh University', 'University of Electronic Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2503.10602.jpg', 'data': {'categories': ['#interpretability', '#multimodal', '#transfer_learning', '#cv', '#training', '#hallucinations', '#benchmark'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ‘Ğ¾Ñ€ÑŒĞ±Ğ° Ñ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ñ… Ğ·Ñ€ĞµĞ½Ğ¸Ğµ Ğ¸ ÑĞ·Ñ‹Ğº (LVLMs). Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ ÑÑ‚Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ¾Ğ³ÑƒÑ‚ ÑĞ»ÑƒĞ¶Ğ¸Ñ‚ÑŒ Ğ¸Ğ½Ğ´Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ°. ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ LVLMs Ğ¸Ğ¼ĞµÑÑ‚ Ğ¾Ğ±Ñ‰Ğ¸Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ² ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°Ñ…. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¸Ñ… Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ğ¹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ TruthPrInt, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ·Ğ° ÑÑ‡Ñ‘Ñ‚ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¼ĞµÑˆĞ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ° Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'Enhancing Truthfulness in Vision-Language Models', 'desc': "This paper addresses the problem of Object Hallucination (OH) in Large Vision-Language Models (LVLMs), which affects the reliability of their outputs. It investigates how internal states of LVLMs can act as indicators of hallucination on a per-token basis, revealing that these states can signal specific hallucination behaviors. The authors introduce a method called Truthful-Guided Pre-Intervention (TruthPrInt) that learns the 'truthful direction' during decoding to improve the accuracy of generated responses. Additionally, they propose ComnHallu to enhance the detection of hallucinations across different LVLMs and datasets by aligning their latent subspaces, demonstrating significant improvements over existing methods in their experiments."}, 'zh': {'title': 'æ­ç¤ºLVLMä¸­çš„çœŸå®æ–¹å‘ï¼Œå‡å°‘å¯¹è±¡å¹»è§‰', 'desc': 'æœ¬æ–‡æ¢è®¨äº†å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰ä¸­çš„å¯¹è±¡å¹»è§‰ï¼ˆOHï¼‰é—®é¢˜ï¼Œè®¤ä¸ºå†…éƒ¨çŠ¶æ€å¯ä»¥ä½œä¸ºå¹»è§‰è¡Œä¸ºçš„æŒ‡ç¤ºå™¨ã€‚ç ”ç©¶å‘ç°ï¼ŒLVLMçš„å†…éƒ¨çŠ¶æ€èƒ½å¤Ÿé«˜ç‰¹å¼‚æ€§åœ°æŒ‡ç¤ºæ¯ä¸ªtokençš„å¹»è§‰è¡Œä¸ºï¼Œå¹¶ä¸”ä¸åŒçš„LVLMåœ¨æ½œåœ¨å­ç©ºé—´ä¸­ç¼–ç äº†é€šç”¨çš„å¹»è§‰æ¨¡å¼ã€‚åŸºäºè¿™äº›å‘ç°ï¼Œæå‡ºäº†ä¸€ç§åä¸ºTruthful-Guided Pre-Interventionï¼ˆTruthPrIntï¼‰çš„æ–¹æ³•ï¼Œé€šè¿‡å­¦ä¹ LVLMè§£ç çš„çœŸå®æ–¹å‘æ¥è¿›è¡Œå¹²é¢„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTruthPrIntåœ¨å¤šä¸ªæµè¡Œçš„LVLMå’ŒOHåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.10638', 'title': 'Studying Classifier(-Free) Guidance From a Classifier-Centric\n  Perspective', 'url': 'https://huggingface.co/papers/2503.10638', 'abstract': 'Classifier-free guidance has become a staple for conditional generation with denoising diffusion models. However, a comprehensive understanding of classifier-free guidance is still missing. In this work, we carry out an empirical study to provide a fresh perspective on classifier-free guidance. Concretely, instead of solely focusing on classifier-free guidance, we trace back to the root, i.e., classifier guidance, pinpoint the key assumption for the derivation, and conduct a systematic study to understand the role of the classifier. We find that both classifier guidance and classifier-free guidance achieve conditional generation by pushing the denoising diffusion trajectories away from decision boundaries, i.e., areas where conditional information is usually entangled and is hard to learn. Based on this classifier-centric understanding, we propose a generic postprocessing step built upon flow-matching to shrink the gap between the learned distribution for a pre-trained denoising diffusion model and the real data distribution, majorly around the decision boundaries. Experiments on various datasets verify the effectiveness of the proposed approach.', 'score': 2, 'issue_id': 2715, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 13', 'zh': '3æœˆ13æ—¥'}, 'hash': '67e97d098f101b16', 'authors': ['Xiaoming Zhao', 'Alexander G. Schwing'], 'affiliations': ['University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2503.10638.jpg', 'data': {'categories': ['#dataset', '#optimization', '#training', '#diffusion', '#benchmark'], 'emoji': 'ğŸ§­', 'ru': {'title': 'ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸', 'desc': 'Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸, Ğ² Ñ‡Ğ°ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸, classifier-free guidance Ğ¸ classifier guidance. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±Ğ° Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚, Ğ¾Ñ‚Ñ‚Ğ°Ğ»ĞºĞ¸Ğ²Ğ°Ñ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ´ĞµĞ½Ğ¾Ğ¹Ğ·Ğ¸Ğ½Ğ³Ğ° Ğ¾Ñ‚ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ† Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ flow-matching Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ´Ğ¸Ğ»Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Bridging the Gap in Conditional Generation with Classifier Insights', 'desc': 'This paper explores classifier-free guidance in denoising diffusion models, which is important for generating data based on specific conditions. The authors investigate the foundational concept of classifier guidance to better understand how both methods work. They discover that both approaches help in generating data by moving away from decision boundaries, where information is complex and difficult to learn. To improve the performance of pre-trained models, they introduce a new postprocessing technique that aligns the learned data distribution with the actual data distribution, particularly near these decision boundaries.'}, 'zh': {'title': 'æ·±å…¥ç†è§£æ— åˆ†ç±»å™¨å¼•å¯¼çš„æ¡ä»¶ç”Ÿæˆ', 'desc': 'æ— åˆ†ç±»å™¨å¼•å¯¼å·²æˆä¸ºå»å™ªæ‰©æ•£æ¨¡å‹æ¡ä»¶ç”Ÿæˆçš„å¸¸ç”¨æ–¹æ³•ã€‚ç„¶è€Œï¼Œç›®å‰å¯¹æ— åˆ†ç±»å™¨å¼•å¯¼çš„å…¨é¢ç†è§£ä»ç„¶ç¼ºä¹ã€‚æœ¬æ–‡é€šè¿‡å®è¯ç ”ç©¶æä¾›äº†å¯¹æ— åˆ†ç±»å™¨å¼•å¯¼çš„æ–°è§†è§’ï¼Œè¿½æº¯åˆ°åˆ†ç±»å™¨å¼•å¯¼çš„æ ¹æºï¼Œæ˜ç¡®äº†æ¨å¯¼çš„å…³é”®å‡è®¾ï¼Œå¹¶ç³»ç»Ÿç ”ç©¶äº†åˆ†ç±»å™¨çš„ä½œç”¨ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œæ— è®ºæ˜¯åˆ†ç±»å™¨å¼•å¯¼è¿˜æ˜¯æ— åˆ†ç±»å™¨å¼•å¯¼ï¼Œéƒ½æ˜¯é€šè¿‡å°†å»å™ªæ‰©æ•£è½¨è¿¹æ¨ç¦»å†³ç­–è¾¹ç•Œæ¥å®ç°æ¡ä»¶ç”Ÿæˆï¼Œè¿™äº›è¾¹ç•Œé€šå¸¸æ˜¯æ¡ä»¶ä¿¡æ¯äº¤ç»‡ä¸”éš¾ä»¥å­¦ä¹ çš„åŒºåŸŸã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.09368', 'title': 'PerCoV2: Improved Ultra-Low Bit-Rate Perceptual Image Compression with\n  Implicit Hierarchical Masked Image Modeling', 'url': 'https://huggingface.co/papers/2503.09368', 'abstract': 'We introduce PerCoV2, a novel and open ultra-low bit-rate perceptual image compression system designed for bandwidth- and storage-constrained applications. Building upon prior work by Careil et al., PerCoV2 extends the original formulation to the Stable Diffusion 3 ecosystem and enhances entropy coding efficiency by explicitly modeling the discrete hyper-latent image distribution. To this end, we conduct a comprehensive comparison of recent autoregressive methods (VAR and MaskGIT) for entropy modeling and evaluate our approach on the large-scale MSCOCO-30k benchmark. Compared to previous work, PerCoV2 (i) achieves higher image fidelity at even lower bit-rates while maintaining competitive perceptual quality, (ii) features a hybrid generation mode for further bit-rate savings, and (iii) is built solely on public components. Code and trained models will be released at https://github.com/Nikolai10/PerCoV2.', 'score': 2, 'issue_id': 2707, 'pub_date': '2025-03-12', 'pub_date_card': {'ru': '12 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 12', 'zh': '3æœˆ12æ—¥'}, 'hash': '7aa613c0b5e4220f', 'authors': ['Nikolai KÃ¶rber', 'Eduard Kromer', 'Andreas Siebert', 'Sascha Hauke', 'Daniel Mueller-Gritschneder', 'BjÃ¶rn Schuller'], 'affiliations': ['TU Wien', 'Technical University of Munich', 'University of Applied Sciences Landshut'], 'pdf_title_img': 'assets/pdf/title_img/2503.09368.jpg', 'data': {'categories': ['#cv', '#benchmark', '#dataset', '#architecture'], 'emoji': 'ğŸ—œï¸', 'ru': {'title': 'Ğ¡Ğ¶Ğ¸Ğ¼Ğ°Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ĞµĞµ, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞ¹ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾: PerCoV2 Ğ½Ğ° ÑÑ‚Ñ€Ğ°Ğ¶Ğµ Ğ±Ğ¸Ñ‚Ñ€ĞµĞ¹Ñ‚Ğ°', 'desc': 'PerCoV2 - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ ÑƒĞ»ÑŒÑ‚Ñ€Ğ°Ğ½Ğ¸Ğ·ĞºĞ¸Ğ¼ Ğ±Ğ¸Ñ‚Ñ€ĞµĞ¹Ñ‚Ğ¾Ğ¼, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ¿ĞµÑ€Ñ†ĞµĞ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğµ. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞºĞ¾ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Stable Diffusion 3 Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¹Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ³Ğ¸Ğ¿ĞµÑ€Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ÑÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ VAR Ğ¸ MaskGIT Ğ´Ğ»Ñ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¹Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‚ ÑĞ²Ğ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ MSCOCO-30k. PerCoV2 Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ ĞµÑ‰Ğµ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ¸Ğ·ĞºĞ¸Ñ… Ğ±Ğ¸Ñ‚Ñ€ĞµĞ¹Ñ‚Ğ°Ñ…, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾Ğµ Ğ¿ĞµÑ€Ñ†ĞµĞ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾.'}, 'en': {'title': 'Ultra-Low Bit-Rate Image Compression Redefined', 'desc': 'PerCoV2 is a new image compression system that uses very few bits to represent images, making it ideal for situations where bandwidth and storage are limited. It improves on earlier methods by better modeling how images are represented in a compressed form, specifically within the Stable Diffusion 3 framework. The system has been tested against other advanced methods and shows better image quality at lower bit rates, while also offering a way to save even more space. Additionally, all components of PerCoV2 are publicly available, allowing others to use and build upon this technology.'}, 'zh': {'title': 'PerCoV2ï¼šè¶…ä½æ¯”ç‰¹ç‡å›¾åƒå‹ç¼©çš„æ–°çªç ´', 'desc': 'PerCoV2æ˜¯ä¸€ç§æ–°å‹çš„è¶…ä½æ¯”ç‰¹ç‡æ„ŸçŸ¥å›¾åƒå‹ç¼©ç³»ç»Ÿï¼Œä¸“ä¸ºå¸¦å®½å’Œå­˜å‚¨å—é™çš„åº”ç”¨è€Œè®¾è®¡ã€‚è¯¥ç³»ç»Ÿåœ¨Careilç­‰äººçš„å…ˆå‰å·¥ä½œåŸºç¡€ä¸Šè¿›è¡Œäº†æ‰©å±•ï¼Œå¢å¼ºäº†ç†µç¼–ç æ•ˆç‡ï¼Œé€šè¿‡æ˜ç¡®å»ºæ¨¡ç¦»æ•£è¶…æ½œå›¾åƒåˆ†å¸ƒæ¥å®ç°ã€‚æˆ‘ä»¬å¯¹æœ€è¿‘çš„è‡ªå›å½’æ–¹æ³•ï¼ˆVARå’ŒMaskGITï¼‰è¿›è¡Œäº†å…¨é¢æ¯”è¾ƒï¼Œå¹¶åœ¨å¤§è§„æ¨¡çš„MSCOCO-30kåŸºå‡†ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ã€‚ä¸ä¹‹å‰çš„å·¥ä½œç›¸æ¯”ï¼ŒPerCoV2åœ¨æ›´ä½æ¯”ç‰¹ç‡ä¸‹å®ç°äº†æ›´é«˜çš„å›¾åƒä¿çœŸåº¦ï¼ŒåŒæ—¶ä¿æŒäº†ç«äº‰æ€§çš„æ„ŸçŸ¥è´¨é‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.10635', 'title': 'A Frustratingly Simple Yet Highly Effective Attack Baseline: Over 90%\n  Success Rate Against the Strong Black-box Models of GPT-4.5/4o/o1', 'url': 'https://huggingface.co/papers/2503.10635', 'abstract': 'Despite promising performance on open-source large vision-language models (LVLMs), transfer-based targeted attacks often fail against black-box commercial LVLMs. Analyzing failed adversarial perturbations reveals that the learned perturbations typically originate from a uniform distribution and lack clear semantic details, resulting in unintended responses. This critical absence of semantic information leads commercial LVLMs to either ignore the perturbation entirely or misinterpret its embedded semantics, thereby causing the attack to fail. To overcome these issues, we notice that identifying core semantic objects is a key objective for models trained with various datasets and methodologies. This insight motivates our approach that refines semantic clarity by encoding explicit semantic details within local regions, thus ensuring interoperability and capturing finer-grained features, and by concentrating modifications on semantically rich areas rather than applying them uniformly. To achieve this, we propose a simple yet highly effective solution: at each optimization step, the adversarial image is cropped randomly by a controlled aspect ratio and scale, resized, and then aligned with the target image in the embedding space. Experimental results confirm our hypothesis. Our adversarial examples crafted with local-aggregated perturbations focused on crucial regions exhibit surprisingly good transferability to commercial LVLMs, including GPT-4.5, GPT-4o, Gemini-2.0-flash, Claude-3.5-sonnet, Claude-3.7-sonnet, and even reasoning models like o1, Claude-3.7-thinking and Gemini-2.0-flash-thinking. Our approach achieves success rates exceeding 90% on GPT-4.5, 4o, and o1, significantly outperforming all prior state-of-the-art attack methods. Our optimized adversarial examples under different configurations and training code are available at https://github.com/VILA-Lab/M-Attack.', 'score': 1, 'issue_id': 2718, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 13', 'zh': '3æœˆ13æ—¥'}, 'hash': 'aba878bb95fd4fb4', 'authors': ['Zhaoyi Li', 'Xiaohan Zhao', 'Dong-Dong Wu', 'Jiacheng Cui', 'Zhiqiang Shen'], 'affiliations': ['VILA Lab, MBZUAI'], 'pdf_title_img': 'assets/pdf/title_img/2503.10635.jpg', 'data': {'categories': ['#training', '#benchmark', '#security', '#optimization', '#dataset', '#cv'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Ğ¢Ğ¾Ñ‡ĞµÑ‡Ğ½Ğ°Ñ Ğ°Ñ‚Ğ°ĞºĞ°: ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¾Ñ€ÑƒĞ¶Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ² Ñ‡ĞµÑ€Ğ½Ñ‹Ñ… ÑÑ‰Ğ¸ĞºĞ¾Ğ² Ğ˜Ğ˜', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ°Ñ‚Ğ°ĞºĞ¸ Ğ½Ğ° ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº GPT-4 Ğ¸ Gemini. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ñ‡Ğ°ÑÑ‚Ğ¾ Ñ‚ĞµÑ€Ğ¿ÑÑ‚ Ğ½ĞµÑƒĞ´Ğ°Ñ‡Ñƒ Ğ¸Ğ·-Ğ·Ğ° Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²Ğ¸Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ² ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°ĞµĞ¼Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ¼ÑƒÑ‰ĞµĞ½Ğ¸ÑÑ…. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ°Ğ³Ñ€ĞµĞ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¾Ğ·Ğ¼ÑƒÑ‰ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ñ‚Ğ°ĞºĞ¸ Ğ±Ğ¾Ğ»ĞµĞµ 90% Ğ½Ğ° Ñ€ÑĞ´Ğµ ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Enhancing Adversarial Attacks with Semantic Precision', 'desc': 'This paper addresses the challenges of transferring targeted adversarial attacks to black-box commercial large vision-language models (LVLMs). It identifies that previous attacks often fail due to the lack of semantic detail in the perturbations, which leads to ineffective responses from the models. The authors propose a novel method that enhances semantic clarity by focusing on important regions of the input, allowing for more effective adversarial modifications. Their experimental results demonstrate that this approach significantly improves the success rate of attacks on various commercial LVLMs, achieving over 90% success on several models.'}, 'zh': {'title': 'èšç„¦è¯­ä¹‰ç»†èŠ‚ï¼Œæå‡å¯¹æŠ—æ”»å‡»æˆåŠŸç‡', 'desc': 'å°½ç®¡å¼€æºçš„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰è¡¨ç°è‰¯å¥½ï¼Œä½†é’ˆå¯¹é»‘ç®±å•†ä¸šLVLMsçš„è½¬ç§»æ”»å‡»å¾€å¾€å¤±è´¥ã€‚åˆ†æå¤±è´¥çš„å¯¹æŠ—æ‰°åŠ¨å‘ç°ï¼Œå­¦ä¹ åˆ°çš„æ‰°åŠ¨é€šå¸¸æ¥è‡ªå‡åŒ€åˆ†å¸ƒï¼Œç¼ºä¹æ˜ç¡®çš„è¯­ä¹‰ç»†èŠ‚ï¼Œå¯¼è‡´æ„å¤–çš„å“åº”ã€‚ä¸ºäº†å…‹æœè¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼Œé€šè¿‡åœ¨å±€éƒ¨åŒºåŸŸå†…ç¼–ç æ˜ç¡®çš„è¯­ä¹‰ç»†èŠ‚ï¼Œç¡®ä¿æ¨¡å‹èƒ½å¤Ÿæ•æ‰åˆ°æ›´ç»†ç²’åº¦çš„ç‰¹å¾ï¼Œå¹¶é›†ä¸­ä¿®æ”¹åœ¨è¯­ä¹‰ä¸°å¯Œçš„åŒºåŸŸã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªå•†ä¸šLVLMsä¸Šå–å¾—äº†è¶…è¿‡90%çš„æˆåŠŸç‡ï¼Œæ˜¾è‘—ä¼˜äºä»¥å¾€çš„æ”»å‡»æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.07111', 'title': 'PoseLess: Depth-Free Vision-to-Joint Control via Direct Image Mapping\n  with VLM', 'url': 'https://huggingface.co/papers/2503.07111', 'abstract': 'This paper introduces PoseLess, a novel framework for robot hand control that eliminates the need for explicit pose estimation by directly mapping 2D images to joint angles using projected representations. Our approach leverages synthetic training data generated through randomized joint configurations, enabling zero-shot generalization to real-world scenarios and cross-morphology transfer from robotic to human hands. By projecting visual inputs and employing a transformer-based decoder, PoseLess achieves robust, low-latency control while addressing challenges such as depth ambiguity and data scarcity. Experimental results demonstrate competitive performance in joint angle prediction accuracy without relying on any human-labelled dataset.', 'score': 1, 'issue_id': 2721, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 10', 'zh': '3æœˆ10æ—¥'}, 'hash': 'edc49497be1a7cb7', 'authors': ['Alan Dao', 'Dinh Bach Vu', 'Tuan Le Duc Anh', 'Bui Quang Huy'], 'affiliations': ['Menlo Research'], 'pdf_title_img': 'assets/pdf/title_img/2503.07111.jpg', 'data': {'categories': ['#dataset', '#robotics', '#transfer_learning', '#training', '#synthetic'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ£Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ñ€ÑƒĞºĞ¾Ğ¹ Ğ±ĞµĞ· ÑĞ²Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ·Ñ‹', 'desc': 'PoseLess - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ñ€ÑƒĞºĞ¾Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ² ÑĞ²Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ¿Ğ¾Ğ·Ñ‹, Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ 2D-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ñ ÑƒĞ³Ğ»Ğ°Ğ¼Ğ¸ ÑÑƒÑÑ‚Ğ°Ğ²Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ñ€Ğ¾ĞµÑ†Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿ÑƒÑ‚ĞµĞ¼ Ñ€Ğ°Ğ½Ğ´Ğ¾Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¹ ÑÑƒÑÑ‚Ğ°Ğ²Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ğ»ÑÑ‚ÑŒ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ñ Ğ½ÑƒĞ»ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ²Ñ‹ÑÑ‚Ñ€ĞµĞ»Ğ° Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸ Ğ¸ Ğ¼ĞµĞ¶Ğ¼Ğ¾Ñ€Ñ„Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿ĞµÑ€ĞµĞ½Ğ¾Ñ Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğµ Ñ€ÑƒĞºĞ¸. PoseLess Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¿Ñ€Ğ¾ĞµÑ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½ÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑƒĞ³Ğ»Ğ¾Ğ² ÑÑƒÑÑ‚Ğ°Ğ²Ğ¾Ğ² Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ°ĞºĞ¸Ñ…-Ğ»Ğ¸Ğ±Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¾Ğ¹.'}, 'en': {'title': 'Direct Control of Robot Hands Without Pose Estimation', 'desc': 'PoseLess is a new framework designed for controlling robot hands without needing to estimate their positions explicitly. It uses 2D images and directly maps them to joint angles, which simplifies the control process. The framework is trained on synthetic data created from various joint configurations, allowing it to adapt to real-world situations and transfer knowledge between different hand types. By utilizing a transformer-based decoder, PoseLess effectively manages challenges like depth ambiguity and limited data, achieving high accuracy in predicting joint angles without any human-labeled data.'}, 'zh': {'title': 'PoseLessï¼šæ— é¡»å§¿æ€ä¼°è®¡çš„æœºå™¨äººæ‰‹æ§åˆ¶æ–°æ¡†æ¶', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºPoseLessçš„æ–°æ¡†æ¶ï¼Œç”¨äºæœºå™¨äººæ‰‹éƒ¨æ§åˆ¶ï¼Œçœå»äº†æ˜¾å¼å§¿æ€ä¼°è®¡çš„éœ€æ±‚ï¼Œç›´æ¥å°†2Då›¾åƒæ˜ å°„åˆ°å…³èŠ‚è§’åº¦ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨é€šè¿‡éšæœºå…³èŠ‚é…ç½®ç”Ÿæˆçš„åˆæˆè®­ç»ƒæ•°æ®ï¼Œå®ç°äº†å¯¹çœŸå®åœºæ™¯çš„é›¶æ ·æœ¬æ³›åŒ–å’Œä»æœºå™¨äººæ‰‹åˆ°äººç±»æ‰‹çš„è·¨å½¢æ€è¿ç§»ã€‚é€šè¿‡å¯¹è§†è§‰è¾“å…¥è¿›è¡ŒæŠ•å½±å¹¶é‡‡ç”¨åŸºäºå˜æ¢å™¨çš„è§£ç å™¨ï¼ŒPoseLesså®ç°äº†ç¨³å¥ã€ä½å»¶è¿Ÿçš„æ§åˆ¶ï¼ŒåŒæ—¶è§£å†³äº†æ·±åº¦æ¨¡ç³Šå’Œæ•°æ®ç¨€ç¼ºç­‰æŒ‘æˆ˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å…³èŠ‚è§’åº¦é¢„æµ‹å‡†ç¡®æ€§æ–¹é¢ï¼ŒPoseLessçš„è¡¨ç°å…·æœ‰ç«äº‰åŠ›ï¼Œä¸”ä¸ä¾èµ–äºä»»ä½•äººå·¥æ ‡æ³¨çš„æ•°æ®é›†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.02682', 'title': 'MPO: Boosting LLM Agents with Meta Plan Optimization', 'url': 'https://huggingface.co/papers/2503.02682', 'abstract': "Recent advancements in large language models (LLMs) have enabled LLM-based agents to successfully tackle interactive planning tasks. However, despite their successes, existing approaches often suffer from planning hallucinations and require retraining for each new agent. To address these challenges, we propose the Meta Plan Optimization (MPO) framework, which enhances agent planning capabilities by directly incorporating explicit guidance. Unlike previous methods that rely on complex knowledge, which either require significant human effort or lack quality assurance, MPO leverages high-level general guidance through meta plans to assist agent planning and enables continuous optimization of the meta plans based on feedback from the agent's task execution. Our experiments conducted on two representative tasks demonstrate that MPO significantly outperforms existing baselines. Moreover, our analysis indicates that MPO provides a plug-and-play solution that enhances both task completion efficiency and generalization capabilities in previous unseen scenarios.", 'score': 15, 'issue_id': 2535, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 4', 'zh': '3æœˆ4æ—¥'}, 'hash': 'e5fa3c849c1eee72', 'authors': ['Weimin Xiong', 'Yifan Song', 'Qingxiu Dong', 'Bingchan Zhao', 'Feifan Song', 'Xun Wang', 'Sujian Li'], 'affiliations': ['National Key Laboratory for Multimedia Information Processing, School of Computer Science, Peking University', 'Peking University', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2503.02682.jpg', 'data': {'categories': ['#rl', '#training', '#optimization', '#hallucinations', '#agents'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞœĞµÑ‚Ğ°Ğ¿Ğ»Ğ°Ğ½Ñ‹ Ğ´Ğ»Ñ ÑƒĞ¼Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ĞµĞµ, ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½ĞµĞµ, Ğ±ĞµĞ· Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Meta Plan Optimization (MPO) Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ°Ğ¿Ğ»Ğ°Ğ½Ñ‹ Ğ´Ğ»Ñ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ° Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ¼ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ñ… Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸. MPO Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ¸ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸.'}, 'en': {'title': 'Enhancing Agent Planning with Meta Plans', 'desc': "This paper introduces the Meta Plan Optimization (MPO) framework to improve the planning abilities of large language model (LLM)-based agents. MPO addresses issues like planning hallucinations and the need for retraining by using high-level meta plans for guidance. This approach allows for continuous optimization based on feedback from the agent's performance in tasks. Experimental results show that MPO not only outperforms existing methods but also enhances efficiency and adaptability in new situations."}, 'zh': {'title': 'å…ƒè§„åˆ’ä¼˜åŒ–ï¼šæå‡æ™ºèƒ½ä½“è§„åˆ’èƒ½åŠ›çš„åˆ›æ–°æ¡†æ¶', 'desc': 'æœ€è¿‘ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¿›æ­¥ä½¿å¾—åŸºäºLLMçš„æ™ºèƒ½ä½“èƒ½å¤ŸæˆåŠŸå¤„ç†äº¤äº’å¼è§„åˆ’ä»»åŠ¡ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¸¸å¸¸é¢ä¸´è§„åˆ’å¹»è§‰çš„é—®é¢˜ï¼Œå¹¶ä¸”æ¯ä¸ªæ–°æ™ºèƒ½ä½“éƒ½éœ€è¦é‡æ–°è®­ç»ƒã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†å…ƒè§„åˆ’ä¼˜åŒ–ï¼ˆMPOï¼‰æ¡†æ¶ï¼Œé€šè¿‡ç›´æ¥å¼•å…¥æ˜ç¡®çš„æŒ‡å¯¼æ¥å¢å¼ºæ™ºèƒ½ä½“çš„è§„åˆ’èƒ½åŠ›ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒMPOåœ¨ä»»åŠ¡å®Œæˆæ•ˆç‡å’Œåœ¨æœªè§åœºæ™¯ä¸­çš„æ³›åŒ–èƒ½åŠ›ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.02846', 'title': 'Mask-DPO: Generalizable Fine-grained Factuality Alignment of LLMs', 'url': 'https://huggingface.co/papers/2503.02846', 'abstract': 'Large language models (LLMs) exhibit hallucinations (i.e., unfaithful or nonsensical information) when serving as AI assistants in various domains. Since hallucinations always come with truthful content in the LLM responses, previous factuality alignment methods that conduct response-level preference learning inevitably introduced noises during training. Therefore, this paper proposes a fine-grained factuality alignment method based on Direct Preference Optimization (DPO), called Mask-DPO. Incorporating sentence-level factuality as mask signals, Mask-DPO only learns from factually correct sentences in the preferred samples and prevents the penalty on factual contents in the not preferred samples, which resolves the ambiguity in the preference learning. Extensive experimental results demonstrate that Mask-DPO can significantly improve the factuality of LLMs responses to questions from both in-domain and out-of-domain datasets, although these questions and their corresponding topics are unseen during training. Only trained on the ANAH train set, the score of Llama3.1-8B-Instruct on the ANAH test set is improved from 49.19% to 77.53%, even surpassing the score of Llama3.1-70B-Instruct (53.44%), while its FactScore on the out-of-domain Biography dataset is also improved from 30.29% to 39.39%. We further study the generalization property of Mask-DPO using different training sample scaling strategies and find that scaling the number of topics in the dataset is more effective than the number of questions. We provide a hypothesis of what factual alignment is doing with LLMs, on the implication of this phenomenon, and conduct proof-of-concept experiments to verify it. We hope the method and the findings pave the way for future research on scaling factuality alignment.', 'score': 14, 'issue_id': 2535, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 4', 'zh': '3æœˆ4æ—¥'}, 'hash': '7e9277cf8ca5bb98', 'authors': ['Yuzhe Gu', 'Wenwei Zhang', 'Chengqi Lyu', 'Dahua Lin', 'Kai Chen'], 'affiliations': ['MMLab, The Chinese University of Hong Kong', 'Shanghai AI Laboratory', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2503.02846.jpg', 'data': {'categories': ['#hallucinations', '#training', '#rlhf', '#alignment'], 'emoji': 'ğŸ­', 'ru': {'title': 'Mask-DPO: Ñ‚Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ñ„Ğ°ĞºÑ‚Ğ¾Ğ² Ğ² Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Mask-DPO Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ÑĞ¼Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ (DPO) Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²ĞµÑ€Ğ½Ñ‹Ñ… Ñ‡Ğ°ÑÑ‚ÑÑ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ°Ğº Ğ½Ğ° Ğ·Ğ½Ğ°ĞºĞ¾Ğ¼Ñ‹Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ‚ĞµĞ¼ Ğ² Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰ĞµĞ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾, Ñ‡ĞµĞ¼ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ñ‡Ğ¸ÑĞ»Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ².'}, 'en': {'title': 'Enhancing Factuality in LLMs with Mask-DPO', 'desc': 'This paper addresses the issue of hallucinations in large language models (LLMs), where they generate incorrect or nonsensical information despite including some accurate content. The authors introduce a new method called Mask-DPO, which focuses on fine-grained factuality alignment by using sentence-level factuality as mask signals. This approach allows the model to learn only from factually correct sentences in preferred samples, reducing noise during training and improving the overall factual accuracy of LLM responses. Experimental results show that Mask-DPO significantly enhances the factuality of LLMs, even on unseen questions and topics, outperforming larger models in certain tests.'}, 'zh': {'title': 'æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„äº‹å®æ€§å¯¹é½', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä½œä¸ºAIåŠ©æ‰‹æ—¶å¸¸å¸¸ä¼šå‡ºç°å¹»è§‰ç°è±¡ï¼Œå³æä¾›ä¸çœŸå®æˆ–æ— æ„ä¹‰çš„ä¿¡æ¯ã€‚ä»¥å¾€çš„äº‹å®å¯¹é½æ–¹æ³•åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¼•å…¥äº†å™ªå£°ï¼Œå› ä¸ºå®ƒä»¬åœ¨å“åº”çº§åˆ«è¿›è¡Œåå¥½å­¦ä¹ ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰çš„ç»†ç²’åº¦äº‹å®å¯¹é½æ–¹æ³•ï¼Œç§°ä¸ºMask-DPOï¼Œè¯¥æ–¹æ³•é€šè¿‡å¥å­çº§åˆ«çš„äº‹å®ä½œä¸ºæ©ç ä¿¡å·ï¼Œä»…ä»äº‹å®æ­£ç¡®çš„å¥å­ä¸­å­¦ä¹ ï¼Œä»è€Œæé«˜äº†LLMsçš„å“åº”å‡†ç¡®æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMask-DPOæ˜¾è‘—æå‡äº†LLMsåœ¨æœªè§é—®é¢˜ä¸Šçš„äº‹å®æ€§ï¼Œå°¤å…¶æ˜¯åœ¨ä¸åŒé¢†åŸŸçš„æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.02879', 'title': 'Wikipedia in the Era of LLMs: Evolution and Risks', 'url': 'https://huggingface.co/papers/2503.02879', 'abstract': "In this paper, we present a thorough analysis of the impact of Large Language Models (LLMs) on Wikipedia, examining the evolution of Wikipedia through existing data and using simulations to explore potential risks. We begin by analyzing page views and article content to study Wikipedia's recent changes and assess the impact of LLMs. Subsequently, we evaluate how LLMs affect various Natural Language Processing (NLP) tasks related to Wikipedia, including machine translation and retrieval-augmented generation (RAG). Our findings and simulation results reveal that Wikipedia articles have been influenced by LLMs, with an impact of approximately 1%-2% in certain categories. If the machine translation benchmark based on Wikipedia is influenced by LLMs, the scores of the models may become inflated, and the comparative results among models might shift as well. Moreover, the effectiveness of RAG might decrease if the knowledge base becomes polluted by LLM-generated content. While LLMs have not yet fully changed Wikipedia's language and knowledge structures, we believe that our empirical findings signal the need for careful consideration of potential future risks.", 'score': 13, 'issue_id': 2535, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 4', 'zh': '3æœˆ4æ—¥'}, 'hash': 'd8fedc8bf5ffc308', 'authors': ['Siming Huang', 'Yuliang Xu', 'Mingmeng Geng', 'Yao Wan', 'Dongping Chen'], 'affiliations': ['Huazhong University of Science and Technology', 'International School for Advanced Studies (SISSA)'], 'pdf_title_img': 'assets/pdf/title_img/2503.02879.jpg', 'data': {'categories': ['#rag', '#benchmark', '#machine_translation', '#dataset', '#multimodal', '#science', '#data'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ‘Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼ĞµĞ½ÑÑÑ‚ Ğ»Ğ¸Ñ†Ğ¾ Ğ’Ğ¸ĞºĞ¸Ğ¿ĞµĞ´Ğ¸Ğ¸: Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ¸ÑĞºĞ¾Ğ²', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ½Ğ° Ğ’Ğ¸ĞºĞ¸Ğ¿ĞµĞ´Ğ¸Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¸Ğ·ÑƒÑ‡Ğ¸Ğ»Ğ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾ÑĞ¼Ğ¾Ñ‚Ñ€Ğ°Ñ… ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ† Ğ¸ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ğ¸ ÑÑ‚Ğ°Ñ‚ĞµĞ¹, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ¸ÑĞºĞ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ LLM Ğ¿Ğ¾Ğ²Ğ»Ğ¸ÑĞ»Ğ¸ Ğ½Ğ° 1-2% ÑÑ‚Ğ°Ñ‚ĞµĞ¹ Ğ² Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸ÑÑ…, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ²ĞµÑÑ‚Ğ¸ Ğº Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ (RAG). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğº Ğ¾ÑÑ‚Ğ¾Ñ€Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¸ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ñ€Ğ¸ÑĞºĞ¾Ğ², ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸ĞµĞ¼ LLM Ğ½Ğ° ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ’Ğ¸ĞºĞ¸Ğ¿ĞµĞ´Ğ¸Ğ¸.'}, 'en': {'title': "Navigating the Impact of LLMs on Wikipedia's Evolution", 'desc': 'This paper analyzes how Large Language Models (LLMs) are affecting Wikipedia by examining changes in page views and article content. It assesses the influence of LLMs on various Natural Language Processing (NLP) tasks, such as machine translation and retrieval-augmented generation (RAG). The study finds that LLMs have caused a 1%-2% impact on certain Wikipedia categories, which could inflate machine translation benchmarks and alter model comparisons. The authors emphasize the importance of monitoring these changes to mitigate potential risks associated with LLM-generated content.'}, 'zh': {'title': 'å¤§å‹è¯­è¨€æ¨¡å‹å¯¹ç»´åŸºç™¾ç§‘çš„å½±å“åˆ†æ', 'desc': 'æœ¬æ–‡æ·±å…¥åˆ†æäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯¹ç»´åŸºç™¾ç§‘çš„å½±å“ï¼Œç ”ç©¶äº†ç»´åŸºç™¾ç§‘çš„æ¼”å˜ã€‚æˆ‘ä»¬é€šè¿‡åˆ†æé¡µé¢æµè§ˆé‡å’Œæ–‡ç« å†…å®¹ï¼Œè¯„ä¼°LLMså¯¹ç»´åŸºç™¾ç§‘çš„å½±å“ï¼Œå‘ç°æŸäº›ç±»åˆ«çš„å½±å“çº¦ä¸º1%-2%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯„ä¼°äº†LLMså¯¹ä¸ç»´åŸºç™¾ç§‘ç›¸å…³çš„è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä»»åŠ¡çš„å½±å“ï¼ŒåŒ…æ‹¬æœºå™¨ç¿»è¯‘å’Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒLLMså¯èƒ½ä¼šå¯¼è‡´æœºå™¨ç¿»è¯‘åŸºå‡†çš„åˆ†æ•°è†¨èƒ€ï¼Œå¹¶å¯èƒ½å½±å“æ¨¡å‹ä¹‹é—´çš„æ¯”è¾ƒç»“æœï¼Œå› æ­¤éœ€è¦å¯¹æœªæ¥çš„æ½œåœ¨é£é™©è¿›è¡Œä»”ç»†è€ƒè™‘ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.01935', 'title': 'MultiAgentBench: Evaluating the Collaboration and Competition of LLM agents', 'url': 'https://huggingface.co/papers/2503.01935', 'abstract': 'Large Language Models (LLMs) have shown remarkable capabilities as autonomous agents, yet existing benchmarks either focus on single-agent tasks or are confined to narrow domains, failing to capture the dynamics of multi-agent coordination and competition. In this paper, we introduce MultiAgentBench, a comprehensive benchmark designed to evaluate LLM-based multi-agent systems across diverse, interactive scenarios. Our framework measures not only task completion but also the quality of collaboration and competition using novel, milestone-based key performance indicators. Moreover, we evaluate various coordination protocols (including star, chain, tree, and graph topologies) and innovative strategies such as group discussion and cognitive planning. Notably, gpt-4o-mini reaches the average highest task score, graph structure performs the best among coordination protocols in the research scenario, and cognitive planning improves milestone achievement rates by 3%. Code and datasets are public available at https://github.com/MultiagentBench/MARBLE.', 'score': 10, 'issue_id': 2532, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 3', 'zh': '3æœˆ3æ—¥'}, 'hash': '4353f27d396c322a', 'authors': ['Kunlun Zhu', 'Hongyi Du', 'Zhaochen Hong', 'Xiaocheng Yang', 'Shuyi Guo', 'Zhe Wang', 'Zhenhailong Wang', 'Cheng Qian', 'Xiangru Tang', 'Heng Ji', 'Jiaxuan You'], 'affiliations': ['University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2503.01935.jpg', 'data': {'categories': ['#games', '#optimization', '#open_source', '#benchmark', '#agents'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'MultiAgentBench: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… LLM-ÑĞ¸ÑÑ‚ĞµĞ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MultiAgentBench - ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¸Ğ·Ğ¼ĞµÑ€ÑĞµÑ‚ Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ½Ğ¾ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ†Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ½Ğ¾Ğ²Ñ‹Ñ… ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ»Ñ‹ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¾Ğ²Ğ¾Ğµ Ğ¾Ğ±ÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¸ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ gpt-4o-mini Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ²Ñ‹ÑÑˆĞµĞ³Ğ¾ ÑÑ€ĞµĞ´Ğ½ĞµĞ³Ğ¾ Ğ±Ğ°Ğ»Ğ»Ğ° Ğ·Ğ° Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ° Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ²Ğ°Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° Ğ»ÑƒÑ‡ÑˆĞµ Ğ²ÑĞµĞ³Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ ÑÑ€ĞµĞ´Ğ¸ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ»Ğ¾Ğ² ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ¼ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸.'}, 'en': {'title': 'Evaluating LLMs in Multi-Agent Dynamics', 'desc': 'This paper presents MultiAgentBench, a new benchmark for assessing the performance of Large Language Models (LLMs) in multi-agent environments. Unlike previous benchmarks that focus on single-agent tasks, MultiAgentBench evaluates how well LLMs can collaborate and compete in various interactive scenarios. The framework introduces key performance indicators that measure both task completion and the quality of interactions among agents. The study also explores different coordination protocols and innovative strategies, revealing that certain configurations, like graph structures and cognitive planning, significantly enhance performance.'}, 'zh': {'title': 'å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„å…¨é¢è¯„ä¼°åŸºå‡†', 'desc': 'æœ¬æ–‡ä»‹ç»äº†MultiAgentBenchï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåœ¨å¤šæ ·åŒ–äº’åŠ¨åœºæ™¯ä¸­çš„è¡¨ç°ã€‚è¯¥æ¡†æ¶ä¸ä»…æµ‹é‡ä»»åŠ¡å®Œæˆæƒ…å†µï¼Œè¿˜è¯„ä¼°åä½œå’Œç«äº‰çš„è´¨é‡ï¼Œä½¿ç”¨æ–°é¢–çš„é‡Œç¨‹ç¢‘å¼å…³é”®ç»©æ•ˆæŒ‡æ ‡ã€‚æˆ‘ä»¬è¿˜è¯„ä¼°äº†å¤šç§åè°ƒåè®®ï¼ˆå¦‚æ˜Ÿå½¢ã€é“¾å½¢ã€æ ‘å½¢å’Œå›¾å½¢æ‹“æ‰‘ï¼‰ä»¥åŠåˆ›æ–°ç­–ç•¥ï¼Œå¦‚å°ç»„è®¨è®ºå’Œè®¤çŸ¥è§„åˆ’ã€‚ç ”ç©¶è¡¨æ˜ï¼Œgpt-4o-miniåœ¨ä»»åŠ¡å¾—åˆ†ä¸Šè¡¨ç°æœ€ä½³ï¼Œå›¾å½¢ç»“æ„åœ¨åè°ƒåè®®ä¸­è¡¨ç°æœ€ä½³ï¼Œè€Œè®¤çŸ¥è§„åˆ’æé«˜äº†é‡Œç¨‹ç¢‘è¾¾æˆç‡3%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.01328', 'title': 'PipeOffload: Improving Scalability of Pipeline Parallelism with Memory Optimization', 'url': 'https://huggingface.co/papers/2503.01328', 'abstract': 'Pipeline parallelism (PP) is widely used for training large language models (LLMs), yet its scalability is often constrained by high activation memory consumption as the number of in-flight microbatches grows with the degree of PP. In this paper, we focus on addressing this challenge by leveraging the under-explored memory offload strategy in PP. With empirical study, we discover that in the majority of standard configurations, at least half, and potentially all, of the activations can be offloaded with negligible overhead. In the cases where full overload is not possible, we introduce a novel selective offload strategy that decreases peak activation memory in a better-than-linear manner. Furthermore, we integrate memory offload with other techniques to jointly consider overall throughput and memory limitation. Our experiments proves that the per-device activation memory effectively reduces with the total number of stages, making PP a stronger alternative than TP, offering up to a 19\\% acceleration with even lower memory consumption. The implementation is open-sourced at https://github.com/sail-sg/zero-bubble-pipeline-parallelism{this url}.', 'score': 10, 'issue_id': 2532, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 3', 'zh': '3æœˆ3æ—¥'}, 'hash': '75e25b312e4cef8a', 'authors': ['Xinyi Wan', 'Penghui Qi', 'Guangxing Huang', 'Jialin Li', 'Min Lin'], 'affiliations': ['National University of', 'Sea AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2503.01328.jpg', 'data': {'categories': ['#open_source', '#inference', '#optimization', '#training'], 'emoji': 'ğŸš€', 'ru': {'title': 'ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»Ğ¸Ğ·Ğ¼Ğ° Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ²Ñ‹Ğ³Ñ€ÑƒĞ·ĞºĞ¸ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¹ Ğ¸Ğ· Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ·Ğ¸Ñ‚ÑŒ Ğ¿Ğ¸ĞºĞ¾Ğ²Ğ¾Ğµ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ²Ñ‹Ğ³Ñ€ÑƒĞ·ĞºĞ¸ Ğ´Ğ»Ñ ÑĞ»ÑƒÑ‡Ğ°ĞµĞ², ĞºĞ¾Ğ³Ğ´Ğ° Ğ¿Ğ¾Ğ»Ğ½Ğ°Ñ Ğ²Ñ‹Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ½ĞµĞ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´ĞµĞ»Ğ°ĞµÑ‚ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ğ½Ñ‹Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»Ğ¸Ğ·Ğ¼ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ¾Ğ¹ Ñ‚ĞµĞ½Ğ·Ğ¾Ñ€Ğ½Ğ¾Ğ¼Ñƒ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»Ğ¸Ğ·Ğ¼Ñƒ, ÑƒÑĞºĞ¾Ñ€ÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° 19% Ğ¿Ñ€Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞµĞ¼ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ğ¸ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸.'}, 'en': {'title': 'Optimizing Memory Usage in Pipeline Parallelism for Faster Training', 'desc': 'This paper addresses the challenge of high activation memory consumption in pipeline parallelism (PP) when training large language models (LLMs). The authors explore a memory offload strategy that allows for significant reductions in memory usage, showing that up to half of the activations can be offloaded with minimal impact on performance. They also propose a selective offload strategy that further decreases peak activation memory in a more efficient way. The results demonstrate that using memory offload in conjunction with other techniques can enhance throughput while reducing memory requirements, making PP a more viable option compared to tensor parallelism (TP).'}, 'zh': {'title': 'ä¼˜åŒ–ç®¡é“å¹¶è¡Œæ€§ï¼Œé™ä½å†…å­˜æ¶ˆè€—ï¼', 'desc': 'æœ¬æ–‡æ¢è®¨äº†åœ¨è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹æ—¶ï¼Œç®¡é“å¹¶è¡Œæ€§ï¼ˆPPï¼‰é¢ä¸´çš„é«˜æ¿€æ´»å†…å­˜æ¶ˆè€—é—®é¢˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§å†…å­˜å¸è½½ç­–ç•¥ï¼Œå¯ä»¥åœ¨å¤§å¤šæ•°æ ‡å‡†é…ç½®ä¸­å¸è½½è‡³å°‘ä¸€åŠçš„æ¿€æ´»ï¼Œå‡ ä¹æ²¡æœ‰é¢å¤–å¼€é”€ã€‚å¯¹äºæ— æ³•å®Œå…¨å¸è½½çš„æƒ…å†µï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„é€‰æ‹©æ€§å¸è½½ç­–ç•¥ï¼Œæ˜¾è‘—é™ä½äº†å³°å€¼æ¿€æ´»å†…å­˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œéšç€é˜¶æ®µæ•°é‡çš„å¢åŠ ï¼Œæ¯ä¸ªè®¾å¤‡çš„æ¿€æ´»å†…å­˜æœ‰æ•ˆå‡å°‘ï¼Œä½¿å¾—PPæˆä¸ºæ¯”å¼ é‡å¹¶è¡Œæ€§ï¼ˆTPï¼‰æ›´å¼ºçš„é€‰æ‹©ï¼Œæä¾›é«˜è¾¾19%çš„åŠ é€Ÿï¼ŒåŒæ—¶é™ä½å†…å­˜æ¶ˆè€—ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.00735', 'title': 'LADDER: Self-Improving LLMs Through Recursive Problem Decomposition', 'url': 'https://huggingface.co/papers/2503.00735', 'abstract': "We introduce LADDER (Learning through Autonomous Difficulty-Driven Example Recursion), a framework which enables Large Language Models to autonomously improve their problem-solving capabilities through self-guided learning by recursively generating and solving progressively simpler variants of complex problems. Unlike prior approaches that require curated datasets or human feedback, LADDER leverages a model's own capabilities to generate easier question variants. We demonstrate LADDER's effectiveness in the subject of mathematical integration, improving Llama 3.2 3B's accuracy from 1% to 82% on undergraduate-level problems and enabling Qwen2.5 7B Deepseek-R1 Distilled to achieve 73% on the MIT Integration Bee qualifying examination. We also introduce TTRL (Test-Time Reinforcement Learning), where we perform reinforcement learning on variants of test problems at inference time. TTRL enables Qwen2.5 7B Deepseek-R1 Distilled to achieve a state-of-the-art score of 90% on the MIT Integration Bee qualifying examination, surpassing OpenAI o1's performance. These results show how self-directed strategic learning can achieve significant capability improvements without relying on architectural scaling or human supervision.", 'score': 9, 'issue_id': 2540, 'pub_date': '2025-03-02', 'pub_date_card': {'ru': '2 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 2', 'zh': '3æœˆ2æ—¥'}, 'hash': 'c9ce66a728a6df87', 'authors': ['Toby Simonds', 'Akira Yoshiyama'], 'affiliations': ['Tufa Labs'], 'pdf_title_img': 'assets/pdf/title_img/2503.00735.jpg', 'data': {'categories': ['#math', '#reasoning', '#optimization', '#training', '#rl'], 'emoji': 'ğŸ§®', 'ru': {'title': 'Ğ¡Ğ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ÑƒĞ¿Ñ€Ğ¾Ñ‰ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ LADDER, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ‚ÑŒ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑƒĞ¿Ñ€Ğ¾Ñ‰ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ¾Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², LADDER Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² Ğ¸Ğ»Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ¾Ñ‚ Ğ»ÑĞ´ĞµĞ¹. Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ³Ğ´Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Llama 3.2 3B Ğ²Ñ‹Ñ€Ğ¾ÑĞ»Ğ° Ñ 1% Ğ´Ğ¾ 82% Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ¸Ñ‚ĞµÑ‚ÑĞºĞ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ TTRL, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Qwen2.5 7B Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ñ€ĞµĞºĞ¾Ñ€Ğ´Ğ½Ñ‹Ñ… 90% Ğ½Ğ° Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¼ ÑĞºĞ·Ğ°Ğ¼ĞµĞ½Ğµ MIT Integration Bee.'}, 'en': {'title': 'Empowering Models to Learn and Solve Problems Autonomously', 'desc': 'LADDER is a new framework that helps Large Language Models (LLMs) improve their problem-solving skills by creating and solving simpler versions of complex problems on their own. This method does not need pre-made datasets or human input, as it allows the model to generate easier questions based on its own understanding. The framework has shown remarkable success in mathematical integration, significantly boosting the accuracy of models like Llama 3.2 and Qwen2.5 on challenging exams. Additionally, the introduction of Test-Time Reinforcement Learning (TTRL) further enhances performance by applying reinforcement learning during the testing phase, leading to state-of-the-art results.'}, 'zh': {'title': 'è‡ªä¸»å­¦ä¹ ï¼Œæå‡èƒ½åŠ›çš„å…¨æ–°æ¡†æ¶', 'desc': 'æœ¬æ–‡ä»‹ç»äº†LADDERï¼ˆé€šè¿‡è‡ªä¸»éš¾åº¦é©±åŠ¨çš„ç¤ºä¾‹é€’å½’å­¦ä¹ ï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä½¿å¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿé€šè¿‡è‡ªæˆ‘å¼•å¯¼å­¦ä¹ ï¼Œé€æ­¥ç”Ÿæˆå’Œè§£å†³å¤æ‚é—®é¢˜çš„ç®€åŒ–å˜ä½“ï¼Œä»è€Œæé«˜å…¶è§£å†³é—®é¢˜çš„èƒ½åŠ›ã€‚ä¸ä»¥å¾€éœ€è¦äººå·¥åé¦ˆæˆ–ç²¾å¿ƒç­–åˆ’æ•°æ®é›†çš„æ–¹æ³•ä¸åŒï¼ŒLADDERåˆ©ç”¨æ¨¡å‹è‡ªèº«çš„èƒ½åŠ›ç”Ÿæˆæ›´ç®€å•çš„é—®é¢˜å˜ä½“ã€‚æˆ‘ä»¬å±•ç¤ºäº†LADDERåœ¨æ•°å­¦ç§¯åˆ†é¢†åŸŸçš„æœ‰æ•ˆæ€§ï¼Œä½¿Llama 3.2 3Båœ¨æœ¬ç§‘æ°´å¹³é—®é¢˜ä¸Šçš„å‡†ç¡®ç‡ä»1%æé«˜åˆ°82%ï¼Œå¹¶ä½¿Qwen2.5 7B Deepseek-R1 Distilledåœ¨MITç§¯åˆ†ç«èµ›èµ„æ ¼è€ƒè¯•ä¸­è¾¾åˆ°73%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†TTRLï¼ˆæµ‹è¯•æ—¶å¼ºåŒ–å­¦ä¹ ï¼‰ï¼Œåœ¨æ¨ç†æ—¶å¯¹æµ‹è¯•é—®é¢˜çš„å˜ä½“è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼Œä½¿Qwen2.5 7B Deepseek-R1 Distilledåœ¨MITç§¯åˆ†ç«èµ›èµ„æ ¼è€ƒè¯•ä¸­è·å¾—90%çš„é¢†å…ˆæˆç»©ï¼Œè¶…è¶Šäº†OpenAI o1çš„è¡¨ç°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.02368', 'title': 'Iterative Value Function Optimization for Guided Decoding', 'url': 'https://huggingface.co/papers/2503.02368', 'abstract': 'While Reinforcement Learning from Human Feedback (RLHF) has become the predominant method for controlling language model outputs, it suffers from high computational costs and training instability. Guided decoding, especially value-guided methods, offers a cost-effective alternative by controlling outputs without re-training models. However, the accuracy of the value function is crucial for value-guided decoding, as inaccuracies can lead to suboptimal decision-making and degraded performance. Existing methods struggle with accurately estimating the optimal value function, leading to less effective control. We propose Iterative Value Function Optimization, a novel framework that addresses these limitations through two key components: Monte Carlo Value Estimation, which reduces estimation variance by exploring diverse trajectories, and Iterative On-Policy Optimization, which progressively improves value estimation through collecting trajectories from value-guided policies. Extensive experiments on text summarization, multi-turn dialogue, and instruction following demonstrate the effectiveness of value-guided decoding approaches in aligning language models. These approaches not only achieve alignment but also significantly reduce computational costs by leveraging principled value function optimization for efficient and effective control.', 'score': 9, 'issue_id': 2538, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 4', 'zh': '3æœˆ4æ—¥'}, 'hash': '6a7f0679f238bd4d', 'authors': ['Zhenhua Liu', 'Lijun Li', 'Ruizhe Chen', 'Yuxian Jiang', 'Tong Zhu', 'Wenliang Chen', 'Jing Shao'], 'affiliations': ['Fudan University', 'Shanghai Artificial Intelligence Laboratory', 'Soochow University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.02368.jpg', 'data': {'categories': ['#optimization', '#alignment', '#training', '#rlhf'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ 'Ğ˜Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ñ†ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸'. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ñƒ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ¾Ñ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° (RLHF), ÑĞ½Ğ¸Ğ¶Ğ°Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾Ğµ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ñ†ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸, ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ ĞµÑ‘ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞœĞ¾Ğ½Ñ‚Ğµ-ĞšĞ°Ñ€Ğ»Ğ¾ Ğ¸ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ° Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹."}, 'en': {'title': 'Optimizing Value Functions for Efficient Language Model Control', 'desc': 'This paper addresses the challenges of Reinforcement Learning from Human Feedback (RLHF) in controlling language models, particularly its high computational costs and instability. It introduces a new framework called Iterative Value Function Optimization, which enhances value-guided decoding methods by improving the accuracy of the value function. The framework employs Monte Carlo Value Estimation to minimize estimation variance and Iterative On-Policy Optimization to refine value estimation through trajectory collection. Experimental results show that this approach not only aligns language models effectively but also reduces computational expenses significantly.'}, 'zh': {'title': 'é«˜æ•ˆæ§åˆ¶è¯­è¨€æ¨¡å‹çš„æ–°æ–¹æ³•', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰åœ¨æ§åˆ¶è¯­è¨€æ¨¡å‹è¾“å‡ºæ—¶çš„é«˜è®¡ç®—æˆæœ¬å’Œè®­ç»ƒä¸ç¨³å®šæ€§é—®é¢˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶â€”â€”è¿­ä»£å€¼å‡½æ•°ä¼˜åŒ–ï¼Œé€šè¿‡è’™ç‰¹å¡æ´›å€¼ä¼°è®¡å’Œè¿­ä»£åœ¨çº¿ä¼˜åŒ–æ¥æé«˜å€¼å‡½æ•°çš„å‡†ç¡®æ€§ã€‚è¯¥æ–¹æ³•é€šè¿‡æ¢ç´¢å¤šæ ·åŒ–çš„è½¨è¿¹æ¥å‡å°‘ä¼°è®¡æ–¹å·®ï¼Œå¹¶é€æ­¥æ”¹è¿›å€¼ä¼°è®¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºå€¼å¼•å¯¼çš„è§£ç æ–¹æ³•åœ¨æ–‡æœ¬æ‘˜è¦ã€å¤šè½®å¯¹è¯å’ŒæŒ‡ä»¤è·Ÿéšä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—é™ä½äº†è®¡ç®—æˆæœ¬ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.00955', 'title': 'SemViQA: A Semantic Question Answering System for Vietnamese Information Fact-Checking', 'url': 'https://huggingface.co/papers/2503.00955', 'abstract': 'The rise of misinformation, exacerbated by Large Language Models (LLMs) like GPT and Gemini, demands robust fact-checking solutions, especially for low-resource languages like Vietnamese. Existing methods struggle with semantic ambiguity, homonyms, and complex linguistic structures, often trading accuracy for efficiency. We introduce SemViQA, a novel Vietnamese fact-checking framework integrating Semantic-based Evidence Retrieval (SER) and Two-step Verdict Classification (TVC). Our approach balances precision and speed, achieving state-of-the-art results with 78.97\\% strict accuracy on ISE-DSC01 and 80.82\\% on ViWikiFC, securing 1st place in the UIT Data Science Challenge. Additionally, SemViQA Faster improves inference speed 7x while maintaining competitive accuracy. SemViQA sets a new benchmark for Vietnamese fact verification, advancing the fight against misinformation. The source code is available at: https://github.com/DAVID-NGUYEN-S16/SemViQA.', 'score': 7, 'issue_id': 2535, 'pub_date': '2025-03-02', 'pub_date_card': {'ru': '2 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 2', 'zh': '3æœˆ2æ—¥'}, 'hash': '651aacbd378465bd', 'authors': ['Nam V. Nguyen', 'Dien X. Tran', 'Thanh T. Tran', 'Anh T. Hoang', 'Tai V. Duong', 'Di T. Le', 'Phuc-Lu Le'], 'affiliations': ['FPT Software AI Center, Viet Nam', 'FPT Telecom, Viet Nam', 'Faculty of Information Technology, Industrial University of Ho Chi Minh City, Viet Nam', 'Faculty of Information Technology, University of Science, VNU-HCM, Viet Nam'], 'pdf_title_img': 'assets/pdf/title_img/2503.00955.jpg', 'data': {'categories': ['#multilingual', '#inference', '#benchmark', '#low_resource', '#dataset', '#science', '#data'], 'emoji': 'ğŸ•µï¸', 'ru': {'title': 'SemViQA: ĞŸĞµÑ€ĞµĞ´Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ñ„Ğ°ĞºÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ±Ğ¾Ñ€ÑŒĞ±Ñ‹ Ñ Ğ´ĞµĞ·Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ½Ğ° Ğ²ÑŒĞµÑ‚Ğ½Ğ°Ğ¼ÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ', 'desc': 'SemViQA - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ñ„Ğ°ĞºÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ²ÑŒĞµÑ‚Ğ½Ğ°Ğ¼ÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ Ğ±Ğ¾Ñ€ÑŒĞ±Ñ‹ Ñ Ğ´ĞµĞ·Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹, ÑƒÑÑƒĞ³ÑƒĞ±Ğ»ÑĞµĞ¼Ğ¾Ğ¹ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. ĞĞ½Ğ° Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ² Ğ¸ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ²ĞµÑ€Ğ´Ğ¸ĞºÑ‚Ğ¾Ğ², Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒÑ. SemViQA Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ° Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ… ISE-DSC01 Ğ¸ ViWikiFC, ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ğ² Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ñ„Ğ°ĞºÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ²ÑŒĞµÑ‚Ğ½Ğ°Ğ¼ÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ. Ğ‘Ñ‹ÑÑ‚Ñ€Ğ°Ñ Ğ²ĞµÑ€ÑĞ¸Ñ SemViQA Faster ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ² 7 Ñ€Ğ°Ğ· Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'SemViQA: Revolutionizing Vietnamese Fact-Checking Against Misinformation', 'desc': 'This paper addresses the challenge of misinformation in low-resource languages, specifically Vietnamese, by introducing SemViQA, a new fact-checking framework. SemViQA combines Semantic-based Evidence Retrieval (SER) and Two-step Verdict Classification (TVC) to enhance both accuracy and efficiency in verifying facts. The framework achieves impressive results, with a strict accuracy of 78.97% on the ISE-DSC01 dataset and 80.82% on ViWikiFC, outperforming existing methods. Additionally, SemViQA Faster significantly boosts inference speed by 7 times while maintaining competitive accuracy, setting a new standard for fact verification in Vietnamese.'}, 'zh': {'title': 'SemViQAï¼šè¶Šå—è¯­äº‹å®æ ¸æŸ¥çš„æ–°æ ‡æ†', 'desc': 'éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¦‚GPTå’ŒGeminiçš„å…´èµ·ï¼Œè™šå‡ä¿¡æ¯é—®é¢˜æ—¥ç›Šä¸¥é‡ï¼Œå°¤å…¶æ˜¯åœ¨èµ„æºåŒ®ä¹çš„è¯­è¨€å¦‚è¶Šå—è¯­ä¸­ï¼Œè¿«åˆ‡éœ€è¦å¼ºæœ‰åŠ›çš„äº‹å®æ ¸æŸ¥è§£å†³æ–¹æ¡ˆã€‚ç°æœ‰çš„æ–¹æ³•åœ¨è¯­ä¹‰æ¨¡ç³Šã€åŒä¹‰è¯å’Œå¤æ‚è¯­è¨€ç»“æ„æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œå¾€å¾€åœ¨å‡†ç¡®æ€§å’Œæ•ˆç‡ä¹‹é—´åšå‡ºå¦¥åã€‚æˆ‘ä»¬æå‡ºäº†SemViQAï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„è¶Šå—è¯­äº‹å®æ ¸æŸ¥æ¡†æ¶ï¼Œç»“åˆäº†åŸºäºè¯­ä¹‰çš„è¯æ®æ£€ç´¢ï¼ˆSERï¼‰å’Œä¸¤æ­¥è£å†³åˆ†ç±»ï¼ˆTVCï¼‰ã€‚æˆ‘ä»¬çš„æ–¹æ¡ˆåœ¨ä¿æŒç«äº‰æ€§å‡†ç¡®åº¦çš„åŒæ—¶ï¼Œå®ç°äº†ç²¾åº¦ä¸é€Ÿåº¦çš„å¹³è¡¡ï¼Œåœ¨ISE-DSC01ä¸Šè¾¾åˆ°äº†78.97%çš„ä¸¥æ ¼å‡†ç¡®ç‡ï¼Œåœ¨ViWikiFCä¸Šè¾¾åˆ°äº†80.82%ï¼Œå¹¶åœ¨UITæ•°æ®ç§‘å­¦æŒ‘æˆ˜èµ›ä¸­è·å¾—ç¬¬ä¸€åã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.14856', 'title': 'FR-Spec: Accelerating Large-Vocabulary Language Models via Frequency-Ranked Speculative Sampling', 'url': 'https://huggingface.co/papers/2502.14856', 'abstract': 'Speculative sampling has emerged as an important technique for accelerating the auto-regressive generation process of large language models (LLMs) by utilizing a draft-then-verify mechanism to produce multiple tokens per forward pass. While state-of-the-art speculative sampling methods use only a single layer and a language modeling (LM) head as the draft model to achieve impressive layer compression, their efficiency gains are substantially reduced for large-vocabulary LLMs, such as Llama-3-8B with a vocabulary of 128k tokens. To address this, we present FR-Spec, a frequency-ranked speculative sampling framework that optimizes draft candidate selection through vocabulary space compression. By constraining the draft search to a frequency-prioritized token subset, our method reduces LM Head computation overhead by 75% while ensuring the equivalence of the final output distribution. Experiments across multiple datasets demonstrate an average of 1.12times speedup over the state-of-the-art speculative sampling method EAGLE-2.', 'score': 6, 'issue_id': 2535, 'pub_date': '2025-02-20', 'pub_date_card': {'ru': '20 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 20', 'zh': '2æœˆ20æ—¥'}, 'hash': '45e128ccea542dad', 'authors': ['Weilin Zhao', 'Tengyu Pan', 'Xu Han', 'Yudi Zhang', 'Ao Sun', 'Yuxiang Huang', 'Kaihuo Zhang', 'Weilun Zhao', 'Yuxuan Li', 'Jianyong Wang', 'Zhiyuan Liu', 'Maosong Sun'], 'affiliations': ['Beijing University of Posts and Telecommunications, Beijing, China', 'Harbin Institute of Technology, Harbin, China', 'OpenBMB', 'Tsinghua University, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2502.14856.jpg', 'data': {'categories': ['#training', '#inference', '#optimization'], 'emoji': 'ğŸš€', 'ru': {'title': 'Ğ‘Ñ‹ÑÑ‚Ñ€ĞµĞµ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ĞµĞµ: Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑĞ¿ĞµĞºÑƒĞ»ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ Ğ´Ğ»Ñ LLM', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ FR-Spec - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¿ĞµĞºÑƒĞ»ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹Ğ±Ğ¾Ñ€ ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ¾Ğ² Ğ¿ÑƒÑ‚ĞµĞ¼ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ, Ğ¾Ñ‚Ğ´Ğ°Ğ²Ğ°Ñ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ñ‹Ğ¼ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ½Ğ¸Ğ·Ğ¸Ñ‚ÑŒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ½Ğ° 75% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ ÑĞºĞ²Ğ¸Ğ²Ğ°Ğ»ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸Ñ‚Ğ¾Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ² 1.12 Ñ€Ğ°Ğ·Ğ° Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼ EAGLE-2.'}, 'en': {'title': 'Accelerating Token Generation with Frequency-Ranked Speculative Sampling', 'desc': 'This paper introduces FR-Spec, a new framework for speculative sampling in large language models (LLMs) that enhances the efficiency of token generation. By using a draft-then-verify approach, FR-Spec optimizes the selection of draft candidates by focusing on a frequency-ranked subset of tokens, which reduces computational overhead significantly. The method achieves a 75% reduction in LM Head computation while maintaining the same output distribution as traditional methods. Experimental results show that FR-Spec provides an average speedup of 1.12 times compared to the leading speculative sampling technique, EAGLE-2.'}, 'zh': {'title': 'é¢‘ç‡ä¼˜å…ˆï¼Œæ¨æµ‹é‡‡æ ·åŠ é€Ÿï¼', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºFR-Specçš„é¢‘ç‡æ’åæ¨æµ‹é‡‡æ ·æ¡†æ¶ï¼Œæ—¨åœ¨åŠ é€Ÿå¤§å‹è¯­è¨€æ¨¡å‹çš„è‡ªå›å½’ç”Ÿæˆè¿‡ç¨‹ã€‚è¯¥æ–¹æ³•é€šè¿‡å‹ç¼©è¯æ±‡ç©ºé—´ï¼Œä¼˜åŒ–è‰ç¨¿å€™é€‰é€‰æ‹©ï¼Œä»è€Œå‡å°‘è®¡ç®—å¼€é”€ã€‚FR-Specå°†è‰ç¨¿æœç´¢é™åˆ¶åœ¨ä¸€ä¸ªä¼˜å…ˆè€ƒè™‘é¢‘ç‡çš„è¯æ±‡å­é›†ä¸Šï¼Œä½¿å¾—è¯­è¨€æ¨¡å‹å¤´çš„è®¡ç®—å¼€é”€é™ä½äº†75%ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFR-Specåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šç›¸è¾ƒäºæœ€å…ˆè¿›çš„æ¨æµ‹é‡‡æ ·æ–¹æ³•EAGLE-2å®ç°äº†å¹³å‡1.12å€çš„åŠ é€Ÿã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.02537', 'title': 'RectifiedHR: Enable Efficient High-Resolution Image Generation via Energy Rectification', 'url': 'https://huggingface.co/papers/2503.02537', 'abstract': "Diffusion models have achieved remarkable advances in various image generation tasks. However, their performance notably declines when generating images at resolutions higher than those used during the training period. Despite the existence of numerous methods for producing high-resolution images, they either suffer from inefficiency or are hindered by complex operations. In this paper, we propose RectifiedHR, an efficient and straightforward solution for training-free high-resolution image generation. Specifically, we introduce the noise refresh strategy, which theoretically only requires a few lines of code to unlock the model's high-resolution generation ability and improve efficiency. Additionally, we first observe the phenomenon of energy decay that may cause image blurriness during the high-resolution image generation process. To address this issue, we propose an Energy Rectification strategy, where modifying the hyperparameters of the classifier-free guidance effectively improves the generation performance. Our method is entirely training-free and boasts a simple implementation logic. Through extensive comparisons with numerous baseline methods, our RectifiedHR demonstrates superior effectiveness and efficiency.", 'score': 5, 'issue_id': 2540, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 4', 'zh': '3æœˆ4æ—¥'}, 'hash': '764b3070cbcdd839', 'authors': ['Zhen Yang', 'Guibao Shen', 'Liang Hou', 'Mushui Liu', 'Luozhou Wang', 'Xin Tao', 'Pengfei Wan', 'Di Zhang', 'Ying-Cong Chen'], 'affiliations': ['HKUST', 'HKUST(GZ)', 'Kuaishou Technology', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.02537.jpg', 'data': {'categories': ['#cv', '#optimization', '#diffusion', '#training'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ RectifiedHR Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑˆÑƒĞ¼Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ ÑĞ½ĞµÑ€Ğ³ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ€ĞµĞºÑ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ€Ğ°Ğ·Ğ¼Ñ‹Ñ‚Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ RectifiedHR Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Unlocking High-Resolution Image Generation with RectifiedHR', 'desc': 'This paper presents RectifiedHR, a novel approach for generating high-resolution images using diffusion models without the need for additional training. The authors introduce a noise refresh strategy that simplifies the process, allowing for efficient high-resolution image generation with minimal code changes. They also identify and address the issue of energy decay, which can lead to blurry images, by proposing an Energy Rectification strategy that optimizes hyperparameters for better performance. Overall, RectifiedHR stands out for its simplicity and effectiveness compared to existing methods.'}, 'zh': {'title': 'é«˜æ•ˆæ— è®­ç»ƒçš„é«˜åˆ†è¾¨ç‡å›¾åƒç”Ÿæˆ', 'desc': 'æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨ç”Ÿæˆé«˜äºè®­ç»ƒåˆ†è¾¨ç‡çš„å›¾åƒæ—¶æ€§èƒ½æ˜æ˜¾ä¸‹é™ã€‚å°½ç®¡å·²æœ‰å¤šç§æ–¹æ³•å¯ä»¥ç”Ÿæˆé«˜åˆ†è¾¨ç‡å›¾åƒï¼Œä½†å®ƒä»¬å¾€å¾€æ•ˆç‡ä½ä¸‹æˆ–æ“ä½œå¤æ‚ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºRectifiedHRçš„é«˜æ•ˆä¸”ç®€å•çš„æ— è®­ç»ƒé«˜åˆ†è¾¨ç‡å›¾åƒç”Ÿæˆè§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬å¼•å…¥äº†å™ªå£°åˆ·æ–°ç­–ç•¥å’Œèƒ½é‡ä¿®æ­£ç­–ç•¥ï¼Œæ˜¾è‘—æé«˜äº†ç”Ÿæˆæ€§èƒ½ï¼Œä¸”å®ç°é€»è¾‘ç®€å•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.02197', 'title': 'ATLaS: Agent Tuning via Learning Critical Steps', 'url': 'https://huggingface.co/papers/2503.02197', 'abstract': "Large Language Model (LLM) agents have demonstrated remarkable generalization capabilities across multi-domain tasks. Existing agent tuning approaches typically employ supervised finetuning on entire expert trajectories. However, behavior-cloning of full trajectories can introduce expert bias and weaken generalization to states not covered by the expert data. Additionally, critical steps, such as planning, complex reasoning for intermediate subtasks, and strategic decision-making, are essential to success in agent tasks, so learning these steps is the key to improving LLM agents. For more effective and efficient agent tuning, we propose ATLaS that identifies the critical steps in expert trajectories and finetunes LLMs solely on these steps with reduced costs. By steering the training's focus to a few critical steps, our method mitigates the risk of overfitting entire trajectories and promotes generalization across different environments and tasks. In extensive experiments, an LLM finetuned on only 30% critical steps selected by ATLaS outperforms the LLM finetuned on all steps and recent open-source LLM agents. ATLaS maintains and improves base LLM skills as generalist agents interacting with diverse environments.", 'score': 5, 'issue_id': 2532, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 4', 'zh': '3æœˆ4æ—¥'}, 'hash': 'a06c17fd97b92f03', 'authors': ['Zhixun Chen', 'Ming Li', 'Yuxuan Huang', 'Yali Du', 'Meng Fang', 'Tianyi Zhou'], 'affiliations': ['Kings College London', 'University of Liverpool', 'University of Maryland', 'University of Technology Sydney'], 'pdf_title_img': 'assets/pdf/title_img/2503.02197.jpg', 'data': {'categories': ['#optimization', '#reasoning', '#training', '#agents', '#agi'], 'emoji': 'ğŸ¯', 'ru': {'title': 'ATLaS: Ñ‚Ğ¾Ñ‡ĞµÑ‡Ğ½Ğ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ', 'desc': 'ATLaS - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ½ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğ¼ ÑˆĞ°Ğ³Ğ°Ğ¼ Ğ² ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ÑÑ…, Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ¸ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ñ€Ğ¸ÑĞº Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ LLM, Ğ½Ğ°ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° 30% ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑˆĞ°Ğ³Ğ¾Ğ², Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… ATLaS, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ²ÑĞµÑ… ÑˆĞ°Ğ³Ğ°Ñ…. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¸ LLM ĞºĞ°Ğº ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ ÑÑ€ĞµĞ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Focus on Critical Steps for Better LLM Performance', 'desc': 'This paper introduces ATLaS, a novel approach for tuning Large Language Model (LLM) agents by focusing on critical steps in expert trajectories rather than using full trajectory behavior-cloning. By finetuning LLMs on only 30% of these essential steps, ATLaS reduces the risk of expert bias and enhances generalization to unseen states. The method emphasizes the importance of planning, reasoning, and decision-making in agent tasks, which are crucial for improving performance. Experimental results show that LLMs trained with ATLaS outperform those trained on complete trajectories and other recent models, while also maintaining their generalist capabilities across various tasks.'}, 'zh': {'title': 'èšç„¦å…³é”®æ­¥éª¤ï¼Œæå‡LLMä»£ç†çš„æ³›åŒ–èƒ½åŠ›', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†åœ¨å¤šé¢†åŸŸä»»åŠ¡ä¸­å±•ç°äº†å‡ºè‰²çš„æ³›åŒ–èƒ½åŠ›ã€‚ç°æœ‰çš„ä»£ç†è°ƒä¼˜æ–¹æ³•é€šå¸¸åœ¨æ•´ä¸ªä¸“å®¶è½¨è¿¹ä¸Šè¿›è¡Œç›‘ç£å¾®è°ƒï¼Œä½†è¿™ç§è¡Œä¸ºå…‹éš†å¯èƒ½å¼•å…¥ä¸“å®¶åè§ï¼Œå‰Šå¼±å¯¹æœªè¦†ç›–çŠ¶æ€çš„æ³›åŒ–èƒ½åŠ›ã€‚æœ¬æ–‡æå‡ºçš„ATLaSæ–¹æ³•é€šè¿‡è¯†åˆ«ä¸“å®¶è½¨è¿¹ä¸­çš„å…³é”®æ­¥éª¤ï¼Œä»…åœ¨è¿™äº›æ­¥éª¤ä¸Šè¿›è¡Œå¾®è°ƒï¼Œä»è€Œé™ä½æˆæœ¬å¹¶æé«˜æ•ˆç‡ã€‚å®éªŒè¡¨æ˜ï¼ŒåŸºäºATLaSé€‰æ‹©çš„30%å…³é”®æ­¥éª¤å¾®è°ƒçš„LLMåœ¨æ€§èƒ½ä¸Šä¼˜äºåœ¨æ‰€æœ‰æ­¥éª¤ä¸Šå¾®è°ƒçš„LLMå’Œæœ€è¿‘çš„å¼€æºLLMä»£ç†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.02878', 'title': 'Language Models can Self-Improve at State-Value Estimation for Better Search', 'url': 'https://huggingface.co/papers/2503.02878', 'abstract': 'Collecting ground truth task completion rewards or human demonstrations for multi-step reasoning tasks is often cost-prohibitive and time-consuming, especially in interactive domains like web tasks. To address this bottleneck, we present self-taught lookahead, a self-supervised method that leverages state-transition dynamics to train a value model capable of effectively guiding language model-controlled search. We find that moderately sized (8 billion parameters) open-weight value models improved with self-taught lookahead can match the performance of using a frontier LLM such as gpt-4o as the value model. Furthermore, we find that self-taught lookahead improves performance by 20% while reducing costs 37x compared to previous LLM-based tree search, without relying on ground truth rewards.', 'score': 5, 'issue_id': 2532, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 4', 'zh': '3æœˆ4æ—¥'}, 'hash': '04d05c7118ce4a93', 'authors': ['Ethan Mendes', 'Alan Ritter'], 'affiliations': ['Georgia Institute of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2503.02878.jpg', 'data': {'categories': ['#reasoning', '#rl', '#optimization', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ¡Ğ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ±ĞµĞ· Ğ´Ğ¾Ñ€Ğ¾Ğ³Ğ¾ÑÑ‚Ğ¾ÑÑ‰Ğ¸Ñ… Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¹', 'desc': "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ 'self-taught lookahead' Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ¸ÑĞº, ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ. Ğ­Ñ‚Ğ¾Ñ‚ ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºÑƒ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ¾Ğ² ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ Ğ´Ğ¾Ñ€Ğ¾Ğ³Ğ¾ÑÑ‚Ğ¾ÑÑ‰ĞµĞ³Ğ¾ Ğ¸ Ñ‚Ñ€ÑƒĞ´Ğ¾ĞµĞ¼ĞºĞ¾Ğ³Ğ¾ ÑĞ±Ğ¾Ñ€Ğ° Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´ Ğ·Ğ° Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸Ğ»Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¹ Ğ¾Ñ‚ Ğ»ÑĞ´ĞµĞ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ (8 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²) Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ 'self-taught lookahead', Ğ¼Ğ¾Ğ³ÑƒÑ‚ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ñ‚ÑŒÑÑ Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº GPT-4, Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸. Ğ‘Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ğ³Ğ¾, Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° 20% Ğ¿Ñ€Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ² 37 Ñ€Ğ°Ğ· Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."}, 'en': {'title': 'Self-Taught Lookahead: Cost-Effective Multi-Step Reasoning', 'desc': 'This paper introduces a method called self-taught lookahead, which is designed to improve the efficiency of training value models for multi-step reasoning tasks without needing expensive human input. By utilizing state-transition dynamics, this self-supervised approach allows the model to learn how to guide searches effectively. The authors demonstrate that a moderately sized value model can achieve performance comparable to larger models like GPT-4o, while also significantly reducing costs. Overall, self-taught lookahead enhances performance by 20% and cuts costs by 37 times compared to traditional LLM-based methods.'}, 'zh': {'title': 'è‡ªæˆ‘æ•™å¯¼å‰ç»ï¼šé«˜æ•ˆçš„å¤šæ­¥éª¤æ¨ç†è§£å†³æ–¹æ¡ˆ', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§è‡ªæˆ‘å­¦ä¹ çš„å‰ç»æ€§æ–¹æ³•ï¼Œç§°ä¸ºè‡ªæˆ‘æ•™å¯¼å‰ç»ï¼ˆself-taught lookaheadï¼‰ï¼Œæ—¨åœ¨è§£å†³å¤šæ­¥éª¤æ¨ç†ä»»åŠ¡ä¸­æ”¶é›†çœŸå®å¥–åŠ±æˆ–äººç±»ç¤ºèŒƒçš„é«˜æˆæœ¬å’Œæ—¶é—´æ¶ˆè€—é—®é¢˜ã€‚è¯¥æ–¹æ³•åˆ©ç”¨çŠ¶æ€è½¬ç§»åŠ¨æ€æ¥è®­ç»ƒä¸€ä¸ªä»·å€¼æ¨¡å‹ï¼Œä»è€Œæœ‰æ•ˆæŒ‡å¯¼è¯­è¨€æ¨¡å‹æ§åˆ¶çš„æœç´¢ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä½¿ç”¨è‡ªæˆ‘æ•™å¯¼å‰ç»çš„ä¸­ç­‰è§„æ¨¡ï¼ˆ80äº¿å‚æ•°ï¼‰å¼€æ”¾æƒé‡ä»·å€¼æ¨¡å‹ï¼Œå…¶æ€§èƒ½å¯ä»¥ä¸å‰æ²¿çš„è¯­è¨€æ¨¡å‹ï¼ˆå¦‚gpt-4oï¼‰ç›¸åª²ç¾ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œè‡ªæˆ‘æ•™å¯¼å‰ç»åœ¨ä¸ä¾èµ–çœŸå®å¥–åŠ±çš„æƒ…å†µä¸‹ï¼Œèƒ½å¤Ÿå°†æ€§èƒ½æå‡20%ï¼ŒåŒæ—¶å°†æˆæœ¬é™ä½37å€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.01342', 'title': 'UFO: A Unified Approach to Fine-grained Visual Perception via Open-ended Language Interface', 'url': 'https://huggingface.co/papers/2503.01342', 'abstract': 'Generalist models have achieved remarkable success in both language and vision-language tasks, showcasing the potential of unified modeling. However, effectively integrating fine-grained perception tasks like detection and segmentation into these models remains a significant challenge. This is primarily because these tasks often rely heavily on task-specific designs and architectures that can complicate the modeling process. To address this challenge, we present \\ours, a framework that Unifies Fine-grained visual perception tasks through an Open-ended language interface. By transforming all perception targets into the language space, \\ours unifies object-level detection, pixel-level segmentation, and image-level vision-language tasks into a single model. Additionally, we introduce a novel embedding retrieval approach that relies solely on the language interface to support segmentation tasks. Our framework bridges the gap between fine-grained perception and vision-language tasks, significantly simplifying architectural design and training strategies while achieving comparable or superior performance to methods with intricate task-specific designs. After multi-task training on five standard visual perception datasets, \\ours outperforms the previous state-of-the-art generalist models by 12.3 mAP on COCO instance segmentation and 3.3 mIoU on ADE20K semantic segmentation. Furthermore, our method seamlessly integrates with existing MLLMs, effectively combining fine-grained perception capabilities with their advanced language abilities, thereby enabling more challenging tasks such as reasoning segmentation. Code and models will be publicly available.', 'score': 4, 'issue_id': 2535, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 3', 'zh': '3æœˆ3æ—¥'}, 'hash': '8f87e24c90eade92', 'authors': ['Hao Tang', 'Chenwei Xie', 'Haiyang Wang', 'Xiaoyi Bao', 'Tingyu Weng', 'Pandeng Li', 'Yun Zheng', 'Liwei Wang'], 'affiliations': ['Alibaba Group', 'Center for Data Science, Peking University', 'Institute of Automation, Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2503.01342.jpg', 'data': {'categories': ['#cv', '#training', '#optimization', '#agi', '#open_source', '#multimodal', '#architecture'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒĞ½Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿ĞµÑ€Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒÑÑ‚ Ğ²ÑĞµ Ñ†ĞµĞ»Ğ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹ Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ-ÑĞ·Ñ‹ĞºĞ° Ğ² ĞµĞ´Ğ¸Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸ĞµĞ¼ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸-Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸ÑÑ‚Ñ‹ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ»ĞµĞ³ĞºĞ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸.'}, 'en': {'title': 'Unifying Fine-Grained Perception with Language for Enhanced Model Performance', 'desc': 'This paper introduces a framework called \textit{ours} that aims to unify fine-grained visual perception tasks, such as detection and segmentation, with language-based tasks. By converting all perception targets into a language format, the framework simplifies the integration of various tasks into a single model. The authors also propose a new embedding retrieval method that utilizes the language interface to enhance segmentation capabilities. The results show that \textit{ours} outperforms existing generalist models on multiple datasets, demonstrating its effectiveness in bridging fine-grained perception and vision-language tasks.'}, 'zh': {'title': 'ç»Ÿä¸€ç»†ç²’åº¦è§†è§‰æ„ŸçŸ¥ä¸è¯­è¨€ä»»åŠ¡çš„åˆ›æ–°æ¡†æ¶', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶\textit{ours}ï¼Œæ—¨åœ¨é€šè¿‡å¼€æ”¾å¼è¯­è¨€æ¥å£ç»Ÿä¸€ç»†ç²’åº¦è§†è§‰æ„ŸçŸ¥ä»»åŠ¡ï¼Œå¦‚ç›®æ ‡æ£€æµ‹å’Œåƒç´ åˆ†å‰²ã€‚è¯¥æ¡†æ¶å°†æ‰€æœ‰æ„ŸçŸ¥ç›®æ ‡è½¬åŒ–ä¸ºè¯­è¨€ç©ºé—´ï¼Œä»è€Œå°†å¯¹è±¡çº§æ£€æµ‹ã€åƒç´ çº§åˆ†å‰²å’Œå›¾åƒçº§è§†è§‰è¯­è¨€ä»»åŠ¡æ•´åˆåˆ°ä¸€ä¸ªæ¨¡å‹ä¸­ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„åµŒå…¥æ£€ç´¢æ–¹æ³•ï¼Œä»…ä¾èµ–è¯­è¨€æ¥å£æ¥æ”¯æŒåˆ†å‰²ä»»åŠ¡ã€‚ç»è¿‡åœ¨äº”ä¸ªæ ‡å‡†è§†è§‰æ„ŸçŸ¥æ•°æ®é›†ä¸Šçš„å¤šä»»åŠ¡è®­ç»ƒï¼Œ\textit{ours}åœ¨COCOå®ä¾‹åˆ†å‰²ä¸Šæ¯”ä¹‹å‰çš„æœ€å…ˆè¿›é€šç”¨æ¨¡å‹æé«˜äº†12.3 mAPï¼Œåœ¨ADE20Kè¯­ä¹‰åˆ†å‰²ä¸Šæé«˜äº†3.3 mIoUã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.02876', 'title': 'SPIDER: A Comprehensive Multi-Organ Supervised Pathology Dataset and Baseline Models', 'url': 'https://huggingface.co/papers/2503.02876', 'abstract': 'Advancing AI in computational pathology requires large, high-quality, and diverse datasets, yet existing public datasets are often limited in organ diversity, class coverage, or annotation quality. To bridge this gap, we introduce SPIDER (Supervised Pathology Image-DEscription Repository), the largest publicly available patch-level dataset covering multiple organ types, including Skin, Colorectal, and Thorax, with comprehensive class coverage for each organ. SPIDER provides high-quality annotations verified by expert pathologists and includes surrounding context patches, which enhance classification performance by providing spatial context.   Alongside the dataset, we present baseline models trained on SPIDER using the Hibou-L foundation model as a feature extractor combined with an attention-based classification head. The models achieve state-of-the-art performance across multiple tissue categories and serve as strong benchmarks for future digital pathology research. Beyond patch classification, the model enables rapid identification of significant areas, quantitative tissue metrics, and establishes a foundation for multimodal approaches.   Both the dataset and trained models are publicly available to advance research, reproducibility, and AI-driven pathology development. Access them at: https://github.com/HistAI/SPIDER', 'score': 4, 'issue_id': 2532, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 4', 'zh': '3æœˆ4æ—¥'}, 'hash': 'a85b69f61f2f377f', 'authors': ['Dmitry Nechaev', 'Alexey Pchelnikov', 'Ekaterina Ivanova'], 'affiliations': ['HistAI'], 'pdf_title_img': 'assets/pdf/title_img/2503.02876.jpg', 'data': {'categories': ['#open_source', '#dataset', '#multimodal', '#science', '#benchmark'], 'emoji': 'ğŸ•·ï¸', 'ru': {'title': 'SPIDER: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ˜Ğ˜ Ğ² Ğ¿Ğ°Ñ‚Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ SPIDER - ĞºÑ€ÑƒĞ¿Ğ½ĞµĞ¹ÑˆĞ¸Ğ¹ Ğ¾Ğ±Ñ‰ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ°Ñ‚Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾ Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¾Ğ². SPIDER Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸, Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞµĞ½Ğ½Ñ‹Ğµ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ğ¼Ğ¸-Ğ¿Ğ°Ñ‚Ğ¾Ğ»Ğ¾Ğ³Ğ°Ğ¼Ğ¸, Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ‚Ñ‡Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° SPIDER Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Hibou-L Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ ÑĞºÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ¾Ñ€Ğ° Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¸ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ³Ğ¾Ğ»Ğ¾Ğ²ĞºĞ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸ÑÑ… Ñ‚ĞºĞ°Ğ½ĞµĞ¹ Ğ¸ ÑĞ»ÑƒĞ¶Ğ°Ñ‚ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ¾Ğ¼ Ğ´Ğ»Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ñ†Ğ¸Ñ„Ñ€Ğ¾Ğ²Ğ¾Ğ¹ Ğ¿Ğ°Ñ‚Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸.'}, 'en': {'title': 'SPIDER: Bridging the Gap in Pathology Datasets for AI Advancement', 'desc': 'This paper introduces SPIDER, a large and diverse dataset designed for computational pathology, addressing the limitations of existing public datasets. SPIDER includes high-quality, expert-verified annotations for various organ types, enhancing the classification of pathology images. The authors also present baseline models that utilize the Hibou-L foundation model and an attention-based classification head, achieving state-of-the-art results in tissue classification. The dataset and models are publicly available, promoting further research and development in AI-driven pathology.'}, 'zh': {'title': 'æ¨åŠ¨ç—…ç†å­¦çš„AIè¿›æ­¥', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†SPIDERï¼ˆç›‘ç£ç—…ç†å›¾åƒæè¿°åº“ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ¶µç›–å¤šç§å™¨å®˜ç±»å‹çš„æœ€å¤§å…¬å¼€è¡¥ä¸çº§æ•°æ®é›†ï¼ŒåŒ…æ‹¬çš®è‚¤ã€ç»“ç›´è‚ å’Œèƒ¸éƒ¨ã€‚è¯¥æ•°æ®é›†æä¾›äº†é«˜è´¨é‡çš„æ³¨é‡Šï¼Œç”±ä¸“å®¶ç—…ç†å­¦å®¶éªŒè¯ï¼Œå¹¶åŒ…å«å‘¨å›´ä¸Šä¸‹æ–‡è¡¥ä¸ï¼Œä»¥æé«˜åˆ†ç±»æ€§èƒ½ã€‚æˆ‘ä»¬è¿˜å±•ç¤ºäº†åŸºäºSPIDERè®­ç»ƒçš„åŸºçº¿æ¨¡å‹ï¼Œä½¿ç”¨Hibou-LåŸºç¡€æ¨¡å‹ä½œä¸ºç‰¹å¾æå–å™¨ï¼Œå¹¶ç»“åˆåŸºäºæ³¨æ„åŠ›çš„åˆ†ç±»å¤´ï¼Œè¾¾åˆ°äº†å¤šç§ç»„ç»‡ç±»åˆ«çš„æœ€å…ˆè¿›æ€§èƒ½ã€‚è¯¥æ•°æ®é›†å’Œè®­ç»ƒæ¨¡å‹å‡å¯å…¬å¼€è·å–ï¼Œä»¥æ¨åŠ¨ç ”ç©¶ã€å¯é‡å¤æ€§å’ŒåŸºäºAIçš„ç—…ç†å‘å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.00876', 'title': 'Improve Representation for Imbalanced Regression through Geometric Constraints', 'url': 'https://huggingface.co/papers/2503.00876', 'abstract': 'In representation learning, uniformity refers to the uniform feature distribution in the latent space (i.e., unit hypersphere). Previous work has shown that improving uniformity contributes to the learning of under-represented classes. However, most of the previous work focused on classification; the representation space of imbalanced regression remains unexplored. Classification-based methods are not suitable for regression tasks because they cluster features into distinct groups without considering the continuous and ordered nature essential for regression. In a geometric aspect, we uniquely focus on ensuring uniformity in the latent space for imbalanced regression through two key losses: enveloping and homogeneity. The enveloping loss encourages the induced trace to uniformly occupy the surface of a hypersphere, while the homogeneity loss ensures smoothness, with representations evenly spaced at consistent intervals. Our method integrates these geometric principles into the data representations via a Surrogate-driven Representation Learning (SRL) framework. Experiments with real-world regression and operator learning tasks highlight the importance of uniformity in imbalanced regression and validate the efficacy of our geometry-based loss functions.', 'score': 3, 'issue_id': 2543, 'pub_date': '2025-03-02', 'pub_date_card': {'ru': '2 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 2', 'zh': '3æœˆ2æ—¥'}, 'hash': '018d350ccd3dd3f5', 'authors': ['Zijian Dong', 'Yilei Wu', 'Chongyao Chen', 'Yingtian Zou', 'Yichi Zhang', 'Juan Helen Zhou'], 'affiliations': ['National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2503.00876.jpg', 'data': {'categories': ['#data', '#optimization', '#training'], 'emoji': 'ğŸŒ', 'ru': {'title': 'Ğ“ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€Ğ°Ğ²Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾Ğ¼Ñƒ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ² Ğ½ĞµÑĞ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¸', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ñ€Ğ°Ğ²Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¸ Ñ Ğ½ĞµÑĞ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ°Ñ…, Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ²Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ. ĞĞ½Ğ¸ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ´Ğ²Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ: Ğ¾Ğ±Ğ²Ğ¾Ğ»Ğ°ĞºĞ¸Ğ²Ğ°ÑÑ‰ÑƒÑ Ğ¸ Ğ³Ğ¾Ğ¼Ğ¾Ğ³ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±ÑÑ‚Ğ²ÑƒÑÑ‚ Ñ€Ğ°Ğ²Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ½Ğ° Ğ³Ğ¸Ğ¿ĞµÑ€ÑÑ„ĞµÑ€Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¸ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ².'}, 'en': {'title': 'Achieving Uniformity in Imbalanced Regression through Geometric Losses', 'desc': "This paper addresses the challenge of representation learning in imbalanced regression tasks, where the distribution of data points is uneven across different classes. It introduces a novel approach that focuses on achieving uniformity in the latent space by employing two specific loss functions: enveloping and homogeneity. The enveloping loss promotes a uniform distribution of features on the hypersphere's surface, while the homogeneity loss ensures that these features are evenly spaced. The proposed Surrogate-driven Representation Learning (SRL) framework demonstrates the effectiveness of these geometric principles in improving performance on real-world regression tasks."}, 'zh': {'title': 'æå‡ä¸å¹³è¡¡å›å½’çš„å‡åŒ€æ€§', 'desc': 'åœ¨è¡¨ç¤ºå­¦ä¹ ä¸­ï¼Œå‡åŒ€æ€§æŒ‡çš„æ˜¯æ½œåœ¨ç©ºé—´ä¸­ç‰¹å¾çš„å‡åŒ€åˆ†å¸ƒã€‚ä»¥å¾€çš„ç ”ç©¶è¡¨æ˜ï¼Œæé«˜å‡åŒ€æ€§æœ‰åŠ©äºå­¦ä¹ ä»£è¡¨æ€§ä¸è¶³çš„ç±»åˆ«ï¼Œä½†å¤§å¤šæ•°ç ”ç©¶é›†ä¸­åœ¨åˆ†ç±»ä»»åŠ¡ä¸Šï¼Œè€Œä¸å¹³è¡¡å›å½’çš„è¡¨ç¤ºç©ºé—´å°šæœªè¢«æ¢ç´¢ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡ä¸¤ä¸ªå…³é”®æŸå¤±å‡½æ•°ï¼šåŒ…ç»œæŸå¤±å’Œå‡åŒ€æ€§æŸå¤±ï¼Œç¡®ä¿ä¸å¹³è¡¡å›å½’ä¸­çš„æ½œåœ¨ç©ºé—´å‡åŒ€æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„å‡ ä½•åŸºç¡€æŸå¤±å‡½æ•°åœ¨ä¸å¹³è¡¡å›å½’ä¸­å…·æœ‰é‡è¦æ„ä¹‰ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.02357', 'title': 'Q-Eval-100K: Evaluating Visual Quality and Alignment Level for Text-to-Vision Content', 'url': 'https://huggingface.co/papers/2503.02357', 'abstract': 'Evaluating text-to-vision content hinges on two crucial aspects: visual quality and alignment. While significant progress has been made in developing objective models to assess these dimensions, the performance of such models heavily relies on the scale and quality of human annotations. According to Scaling Law, increasing the number of human-labeled instances follows a predictable pattern that enhances the performance of evaluation models. Therefore, we introduce a comprehensive dataset designed to Evaluate Visual quality and Alignment Level for text-to-vision content (Q-EVAL-100K), featuring the largest collection of human-labeled Mean Opinion Scores (MOS) for the mentioned two aspects. The Q-EVAL-100K dataset encompasses both text-to-image and text-to-video models, with 960K human annotations specifically focused on visual quality and alignment for 100K instances (60K images and 40K videos). Leveraging this dataset with context prompt, we propose Q-Eval-Score, a unified model capable of evaluating both visual quality and alignment with special improvements for handling long-text prompt alignment. Experimental results indicate that the proposed Q-Eval-Score achieves superior performance on both visual quality and alignment, with strong generalization capabilities across other benchmarks. These findings highlight the significant value of the Q-EVAL-100K dataset. Data and codes will be available at https://github.com/zzc-1998/Q-Eval.', 'score': 3, 'issue_id': 2541, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 4', 'zh': '3æœˆ4æ—¥'}, 'hash': 'a9bc65d32288d8f5', 'authors': ['Zicheng Zhang', 'Tengchuan Kou', 'Shushi Wang', 'Chunyi Li', 'Wei Sun', 'Wei Wang', 'Xiaoyu Li', 'Zongyu Wang', 'Xuezhi Cao', 'Xiongkuo Min', 'Xiaohong Liu', 'Guangtao Zhai'], 'affiliations': ['Meituan', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2503.02357.jpg', 'data': {'categories': ['#benchmark', '#alignment', '#long_context', '#dataset', '#multimodal', '#cv', '#video'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ‘Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Q-EVAL-100K Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°, ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ text-to-image Ğ¸ text-to-video. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Q-Eval-Score Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑƒ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ñ†ĞµĞ½Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'Enhancing Text-to-Vision Evaluation with Q-EVAL-100K', 'desc': 'This paper discusses the importance of evaluating text-to-vision content based on visual quality and alignment. It highlights the role of human annotations in improving the performance of evaluation models, following the Scaling Law principle. The authors introduce a new dataset called Q-EVAL-100K, which contains a large number of human-labeled Mean Opinion Scores for both text-to-image and text-to-video models. They also present Q-Eval-Score, a model that effectively assesses visual quality and alignment, demonstrating strong performance and generalization across various benchmarks.'}, 'zh': {'title': 'æå‡æ–‡æœ¬åˆ°è§†è§‰å†…å®¹è¯„ä¼°çš„è´¨é‡ä¸å¯¹é½åº¦', 'desc': 'æœ¬æ–‡æ¢è®¨äº†æ–‡æœ¬åˆ°è§†è§‰å†…å®¹è¯„ä¼°çš„ä¸¤ä¸ªå…³é”®æ–¹é¢ï¼šè§†è§‰è´¨é‡å’Œå¯¹é½åº¦ã€‚å°½ç®¡å·²æœ‰å®¢è§‚æ¨¡å‹åœ¨è¿™ä¸¤ä¸ªç»´åº¦ä¸Šå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†è¿™äº›æ¨¡å‹çš„æ€§èƒ½ä¾èµ–äºäººç±»æ ‡æ³¨çš„è§„æ¨¡å’Œè´¨é‡ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå…¨é¢çš„æ•°æ®é›†Q-EVAL-100Kï¼ŒåŒ…å«960Kä¸ªäººç±»æ ‡æ³¨çš„å¹³å‡æ„è§åˆ†æ•°ï¼ˆMOSï¼‰ï¼Œç”¨äºè¯„ä¼°æ–‡æœ¬åˆ°å›¾åƒå’Œæ–‡æœ¬åˆ°è§†é¢‘æ¨¡å‹çš„è§†è§‰è´¨é‡å’Œå¯¹é½åº¦ã€‚é€šè¿‡åˆ©ç”¨è¯¥æ•°æ®é›†ï¼Œæˆ‘ä»¬æå‡ºäº†Q-Eval-Scoreæ¨¡å‹ï¼Œèƒ½å¤Ÿæœ‰æ•ˆè¯„ä¼°è§†è§‰è´¨é‡å’Œå¯¹é½åº¦ï¼Œå¹¶åœ¨å¤„ç†é•¿æ–‡æœ¬æç¤ºå¯¹é½æ–¹é¢è¿›è¡Œäº†ç‰¹åˆ«æ”¹è¿›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.02783', 'title': 'IterPref: Focal Preference Learning for Code Generation via Iterative Debugging', 'url': 'https://huggingface.co/papers/2503.02783', 'abstract': 'Preference learning enhances Code LLMs beyond supervised fine-tuning by leveraging relative quality comparisons. Existing methods construct preference pairs from   candidates based on test case success, treating the higher pass rate sample as positive and the lower as negative. However, this approach does not pinpoint specific errors in the code, which prevents the model from learning more informative error correction patterns, as aligning failing code as a whole lacks the granularity needed to capture meaningful error-resolution relationships. To address these issues, we propose IterPref, a new preference alignment framework that mimics human iterative debugging to refine Code LLMs. IterPref explicitly locates error regions and aligns the corresponding tokens via a tailored DPO algorithm. To generate informative pairs, we introduce the CodeFlow dataset, where samples are iteratively refined until passing tests, with modifications capturing error corrections. Extensive experiments show that a diverse suite of Code LLMs equipped with IterPref achieves significant performance gains in code generation and improves on challenging tasks like BigCodeBench. In-depth analysis reveals that IterPref yields fewer errors. Our code and data will be made publicaly available.', 'score': 3, 'issue_id': 2540, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 4', 'zh': '3æœˆ4æ—¥'}, 'hash': '3d7ced41646a5f95', 'authors': ['Jie Wu', 'Haoling Li', 'Xin Zhang', 'Jianwen Luo', 'Yangyu Huang', 'Ruihang Chu', 'Yujiu Yang', 'Scarlett Li'], 'affiliations': ['CASIA', 'Microsoft Research', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.02783.jpg', 'data': {'categories': ['#alignment', '#rlhf', '#open_source', '#dataset', '#optimization', '#training'], 'emoji': 'ğŸ”§', 'ru': {'title': 'Ğ˜Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼ Ğ´Ğ»Ñ ÑƒÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ´Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ IterPref Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ´Ğ° (Code LLMs) Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², IterPref Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ² ĞºĞ¾Ğ´Ğµ Ğ¸ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ° DPO. Ğ”Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… CodeFlow, Ğ³Ğ´Ğµ Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ñ‹ ĞºĞ¾Ğ´Ğ° Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ÑÑ Ğ´Ğ¾ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ‚ĞµÑÑ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ IterPref Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Code LLMs Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ° Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ….'}, 'en': {'title': 'Iterative Preference Learning for Enhanced Code LLMs', 'desc': 'This paper introduces IterPref, a novel framework for enhancing Code LLMs (Language Models) through preference learning. Unlike traditional methods that simply compare pass rates of code samples, IterPref focuses on identifying specific error regions in code, allowing for more precise learning of error correction patterns. By utilizing a tailored DPO (Direct Preference Optimization) algorithm and the CodeFlow dataset, which iteratively refines code samples, the framework generates informative preference pairs that improve model training. Experimental results demonstrate that Code LLMs using IterPref show significant performance improvements in code generation tasks and exhibit fewer errors overall.'}, 'zh': {'title': 'è¿­ä»£è°ƒè¯•ï¼Œæå‡ä»£ç ç”Ÿæˆèƒ½åŠ›ï¼', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„åå¥½å¯¹é½æ¡†æ¶IterPrefï¼Œæ—¨åœ¨é€šè¿‡æ¨¡æ‹Ÿäººç±»çš„è¿­ä»£è°ƒè¯•è¿‡ç¨‹æ¥æå‡ä»£ç å¤§è¯­è¨€æ¨¡å‹ï¼ˆCode LLMsï¼‰çš„æ€§èƒ½ã€‚ç°æœ‰æ–¹æ³•é€šè¿‡æµ‹è¯•ç”¨ä¾‹çš„æˆåŠŸç‡æ„å»ºåå¥½å¯¹ï¼Œä½†æœªèƒ½å‡†ç¡®å®šä½ä»£ç ä¸­çš„å…·ä½“é”™è¯¯ï¼Œé™åˆ¶äº†æ¨¡å‹å­¦ä¹ æœ‰æ•ˆçš„é”™è¯¯ä¿®æ­£æ¨¡å¼ã€‚IterPrefé€šè¿‡å®šåˆ¶çš„DPOç®—æ³•æ˜ç¡®å®šä½é”™è¯¯åŒºåŸŸï¼Œå¹¶å¯¹ç›¸åº”çš„æ ‡è®°è¿›è¡Œå¯¹é½ï¼Œä»è€Œç”Ÿæˆæ›´å…·ä¿¡æ¯é‡çš„åå¥½å¯¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨IterPrefçš„å¤šæ ·åŒ–ä»£ç å¤§è¯­è¨€æ¨¡å‹åœ¨ä»£ç ç”Ÿæˆå’Œå¤æ‚ä»»åŠ¡ä¸Šå‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.02268', 'title': 'AppAgentX: Evolving GUI Agents as Proficient Smartphone Users', 'url': 'https://huggingface.co/papers/2503.02268', 'abstract': "Recent advancements in Large Language Models (LLMs) have led to the development of intelligent LLM-based agents capable of interacting with graphical user interfaces (GUIs). These agents demonstrate strong reasoning and adaptability, enabling them to perform complex tasks that traditionally required predefined rules. However, the reliance on step-by-step reasoning in LLM-based agents often results in inefficiencies, particularly for routine tasks. In contrast, traditional rule-based systems excel in efficiency but lack the intelligence and flexibility to adapt to novel scenarios. To address this challenge, we propose a novel evolutionary framework for GUI agents that enhances operational efficiency while retaining intelligence and flexibility. Our approach incorporates a memory mechanism that records the agent's task execution history. By analyzing this history, the agent identifies repetitive action sequences and evolves high-level actions that act as shortcuts, replacing these low-level operations and improving efficiency. This allows the agent to focus on tasks requiring more complex reasoning, while simplifying routine actions. Experimental results on multiple benchmark tasks demonstrate that our approach significantly outperforms existing methods in both efficiency and accuracy. The code will be open-sourced to support further research.", 'score': 3, 'issue_id': 2536, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 4', 'zh': '3æœˆ4æ—¥'}, 'hash': '03199cb797dc8d88', 'authors': ['Wenjia Jiang', 'Yangyang Zhuang', 'Chenxi Song', 'Xu Yang', 'Chi Zhang'], 'affiliations': ['Henan University', 'Southeast University', 'Westlake University'], 'pdf_title_img': 'assets/pdf/title_img/2503.02268.jpg', 'data': {'categories': ['#optimization', '#agents', '#benchmark', '#reasoning', '#open_source', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ­Ğ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² GUI: Ğ±Ğ°Ğ»Ğ°Ğ½Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ³Ğ¸Ğ±ĞºĞ¾ÑÑ‚Ğ¸', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‰Ğ¸Ñ… Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ·Ğ°Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ¼ Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ĞµĞµ Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€ÑÑÑ‰Ğ¸Ñ…ÑÑ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° ÑĞ¾Ğ·Ğ´Ğ°ÑÑ‚ÑÑ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ·Ğ°Ğ¼ĞµĞ½ÑÑÑ‚ Ğ½Ğ¸Ğ·ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğµ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Evolving Efficiency: Smart Shortcuts for GUI Agents', 'desc': 'This paper presents a new framework for Large Language Model (LLM)-based agents that interact with graphical user interfaces (GUIs). The proposed method enhances the efficiency of these agents by incorporating a memory mechanism that tracks their task execution history. By analyzing this history, the agents can identify repetitive actions and evolve high-level shortcuts, allowing them to streamline routine tasks. This approach maintains the intelligence and adaptability of LLMs while improving their operational efficiency, as demonstrated by experimental results on benchmark tasks.'}, 'zh': {'title': 'æ™ºèƒ½ä»£ç†çš„è¿›åŒ–ï¼šæå‡æ•ˆç‡ä¸çµæ´»æ€§', 'desc': 'æœ€è¿‘ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¿›æ­¥ä½¿å¾—åŸºäºLLMçš„æ™ºèƒ½ä»£ç†èƒ½å¤Ÿä¸å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIsï¼‰è¿›è¡Œäº¤äº’ã€‚è¿™äº›ä»£ç†å±•ç°å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›å’Œé€‚åº”æ€§ï¼Œèƒ½å¤Ÿæ‰§è¡Œä¼ ç»Ÿä¸Šéœ€è¦é¢„å®šä¹‰è§„åˆ™çš„å¤æ‚ä»»åŠ¡ã€‚ç„¶è€Œï¼ŒåŸºäºLLMçš„ä»£ç†åœ¨ä¾èµ–é€æ­¥æ¨ç†æ—¶ï¼Œå¾€å¾€åœ¨å¤„ç†å¸¸è§„ä»»åŠ¡æ—¶æ•ˆç‡è¾ƒä½ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„è¿›åŒ–æ¡†æ¶ï¼Œé€šè¿‡è®°å½•ä»£ç†çš„ä»»åŠ¡æ‰§è¡Œå†å²ï¼Œè¯†åˆ«é‡å¤çš„æ“ä½œåºåˆ—ï¼Œä»è€Œæ¼”åŒ–å‡ºé«˜å±‚æ¬¡çš„å¿«æ·æ“ä½œï¼Œæå‡äº†æ“ä½œæ•ˆç‡ï¼ŒåŒæ—¶ä¿ç•™äº†æ™ºèƒ½å’Œçµæ´»æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.02812', 'title': 'Q-Filters: Leveraging QK Geometry for Efficient KV Cache Compression', 'url': 'https://huggingface.co/papers/2503.02812', 'abstract': 'Autoregressive language models rely on a Key-Value (KV) Cache, which avoids re-computing past hidden states during generation, making it faster. As model sizes and context lengths grow, the KV Cache becomes a significant memory bottleneck, which calls for compression methods that limit its size during generation. In this paper, we discover surprising properties of Query (Q) and Key (K) vectors that allow us to efficiently approximate attention scores without computing the attention maps. We propose Q-Filters, a training-free KV Cache compression method that filters out less crucial Key-Value pairs based on a single context-agnostic projection. Contrarily to many alternatives, Q-Filters is compatible with FlashAttention, as it does not require direct access to attention weights. Experimental results in long-context settings demonstrate that Q-Filters is competitive with attention-based compression methods such as SnapKV in retrieval tasks while consistently outperforming efficient compression schemes such as Streaming-LLM in generation setups. Notably, Q-Filters achieves a 99% accuracy in the needle-in-a-haystack task with a x32 compression level while reducing the generation perplexity drop by up to 65% in text generation compared to Streaming-LLM.', 'score': 2, 'issue_id': 2546, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 4', 'zh': '3æœˆ4æ—¥'}, 'hash': '9dc95b6ce0d4b6ca', 'authors': ['Nathan Godey', 'Alessio Devoto', 'Yu Zhao', 'Simone Scardapane', 'Pasquale Minervini', 'Ã‰ric de la Clergerie', 'BenoÃ®t Sagot'], 'affiliations': ['Miniml.AI', 'Sapienza University of Rome', 'Sorbonne UniversitÃ©, Paris, France', 'University of Edinburgh'], 'pdf_title_img': 'assets/pdf/title_img/2503.02812.jpg', 'data': {'categories': ['#inference', '#long_context', '#training', '#optimization'], 'emoji': 'ğŸ”', 'ru': {'title': 'Q-Filters: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ KV-ĞºÑÑˆĞ° Ğ±ĞµĞ· ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑĞ¾Ğ² Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ KV-ĞºÑÑˆĞ° Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Q-Filters. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ… Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ² Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² (Q) Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ¹ (K), Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ñ… ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ°Ğ¿Ğ¿Ñ€Ğ¾ĞºÑĞ¸Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ±ĞµĞ· Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ ĞºĞ°Ñ€Ñ‚ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. Q-Filters Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€ÑƒĞµÑ‚ Ğ¼ĞµĞ½ĞµĞµ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ€Ñ‹ ĞºĞ»ÑÑ‡-Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ´Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾-Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾ĞµĞºÑ†Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ ĞµĞ³Ğ¾ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ñ‹Ğ¼ Ñ FlashAttention. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Q-Filters Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ñ….'}, 'en': {'title': 'Efficient KV Cache Compression with Q-Filters', 'desc': 'This paper addresses the memory limitations of autoregressive language models caused by the Key-Value (KV) Cache during text generation. It introduces Q-Filters, a novel method that compresses the KV Cache by filtering out less important Key-Value pairs without needing to compute attention maps. The method is efficient and compatible with existing techniques like FlashAttention, making it a practical solution for large models. Experimental results show that Q-Filters not only competes well with other compression methods but also significantly improves performance in text generation tasks.'}, 'zh': {'title': 'é«˜æ•ˆå‹ç¼©ï¼šQ-Filtersçš„åˆ›æ–°ä¹‹è·¯', 'desc': 'è‡ªå›å½’è¯­è¨€æ¨¡å‹ä¾èµ–äºé”®å€¼ç¼“å­˜ï¼ˆKV Cacheï¼‰ï¼Œä»¥é¿å…åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­é‡æ–°è®¡ç®—è¿‡å»çš„éšè—çŠ¶æ€ï¼Œä»è€Œæé«˜é€Ÿåº¦ã€‚éšç€æ¨¡å‹è§„æ¨¡å’Œä¸Šä¸‹æ–‡é•¿åº¦çš„å¢åŠ ï¼ŒKV Cache æˆä¸ºä¸€ä¸ªé‡è¦çš„å†…å­˜ç“¶é¢ˆï¼Œå› æ­¤éœ€è¦å‹ç¼©æ–¹æ³•æ¥é™åˆ¶å…¶å¤§å°ã€‚æœ¬æ–‡å‘ç°äº†æŸ¥è¯¢ï¼ˆQï¼‰å’Œé”®ï¼ˆKï¼‰å‘é‡çš„æ„å¤–ç‰¹æ€§ï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿåœ¨ä¸è®¡ç®—æ³¨æ„åŠ›å›¾çš„æƒ…å†µä¸‹æœ‰æ•ˆåœ°è¿‘ä¼¼æ³¨æ„åŠ›åˆ†æ•°ã€‚æˆ‘ä»¬æå‡ºäº† Q-Filtersï¼Œè¿™æ˜¯ä¸€ç§æ— è®­ç»ƒçš„ KV Cache å‹ç¼©æ–¹æ³•ï¼Œé€šè¿‡å•ä¸€çš„ä¸Šä¸‹æ–‡æ— å…³æŠ•å½±è¿‡æ»¤æ‰ä¸å¤ªé‡è¦çš„é”®å€¼å¯¹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.02823', 'title': 'A Multimodal Symphony: Integrating Taste and Sound through Generative AI', 'url': 'https://huggingface.co/papers/2503.02823', 'abstract': "In recent decades, neuroscientific and psychological research has traced direct relationships between taste and auditory perceptions. This article explores multimodal generative models capable of converting taste information into music, building on this foundational research. We provide a brief review of the state of the art in this field, highlighting key findings and methodologies. We present an experiment in which a fine-tuned version of a generative music model (MusicGEN) is used to generate music based on detailed taste descriptions provided for each musical piece. The results are promising: according the participants' (n=111) evaluation, the fine-tuned model produces music that more coherently reflects the input taste descriptions compared to the non-fine-tuned model. This study represents a significant step towards understanding and developing embodied interactions between AI, sound, and taste, opening new possibilities in the field of generative AI. We release our dataset, code and pre-trained model at: https://osf.io/xs5jy/.", 'score': 2, 'issue_id': 2545, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 4', 'zh': '3æœˆ4æ—¥'}, 'hash': '841602ca85525c24', 'authors': ['Matteo Spanio', 'Massimiliano Zampini', 'Antonio RodÃ ', 'Franco Pierucci'], 'affiliations': ['Center for Mind/Brain Sciences (CIMeC) University of Trento Rovereto, Italy', 'Centro di Sonologia Computazionale (CSC) Department of Information Engineering University of Padova Padova, Italy', 'SoundFood s.r.l. Terni, Italy'], 'pdf_title_img': 'assets/pdf/title_img/2503.02823.jpg', 'data': {'categories': ['#multimodal', '#open_source', '#games', '#dataset'], 'emoji': 'ğŸµ', 'ru': {'title': 'ĞÑ‚ Ğ²ĞºÑƒÑĞ° Ğº Ğ¼ĞµĞ»Ğ¾Ğ´Ğ¸Ğ¸: Ğ˜Ğ˜ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ³Ğ°ÑÑ‚Ñ€Ğ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾Ñ‰ÑƒÑ‰ĞµĞ½Ğ¸Ñ Ğ² Ğ¼ÑƒĞ·Ñ‹ĞºÑƒ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ğ²ĞºÑƒÑĞµ Ğ² Ğ¼ÑƒĞ·Ñ‹ĞºÑƒ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ²ĞµÑ€ÑĞ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ MusicGEN Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ Ğ²ĞºÑƒÑĞ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ¼ÑƒĞ·Ñ‹ĞºÑƒ, Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ñ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ÑĞ¼Ğ¸ Ğ²ĞºÑƒÑĞ¾Ğ² Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ. Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ.'}, 'en': {'title': 'Transforming Taste into Sound: A New Frontier in Generative AI', 'desc': 'This paper investigates the connection between taste and sound by using multimodal generative models. It focuses on a specific model called MusicGEN, which has been fine-tuned to create music that corresponds to taste descriptions. An experiment with 111 participants showed that the fine-tuned model produced music that better matched the taste inputs than the original model. This research advances the understanding of how AI can create embodied experiences that link different sensory modalities, particularly taste and sound.'}, 'zh': {'title': 'å‘³è§‰ä¸éŸ³ä¹çš„å¥‡å¦™ç»“åˆ', 'desc': 'è¿‘å¹´æ¥ï¼Œç¥ç»ç§‘å­¦å’Œå¿ƒç†å­¦ç ”ç©¶å‘ç°å‘³è§‰ä¸å¬è§‰ä¹‹é—´å­˜åœ¨ç›´æ¥å…³ç³»ã€‚æœ¬æ–‡æ¢è®¨äº†å¤šæ¨¡æ€ç”Ÿæˆæ¨¡å‹ï¼Œèƒ½å¤Ÿå°†å‘³è§‰ä¿¡æ¯è½¬æ¢ä¸ºéŸ³ä¹ï¼ŒåŸºäºè¿™ä¸€åŸºç¡€ç ”ç©¶è¿›è¡Œæ·±å…¥åˆ†æã€‚æˆ‘ä»¬å›é¡¾äº†è¯¥é¢†åŸŸçš„æœ€æ–°è¿›å±•ï¼Œå¼ºè°ƒäº†å…³é”®å‘ç°å’Œæ–¹æ³•è®ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç»è¿‡å¾®è°ƒçš„ç”ŸæˆéŸ³ä¹æ¨¡å‹ï¼ˆMusicGENï¼‰èƒ½å¤Ÿæ›´å¥½åœ°åæ˜ è¾“å…¥çš„å‘³è§‰æè¿°ï¼Œå±•ç¤ºäº†AIã€å£°éŸ³ä¸å‘³è§‰ä¹‹é—´çš„äº’åŠ¨æ–°å¯èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.02152', 'title': 'Tabby: Tabular Data Synthesis with Language Models', 'url': 'https://huggingface.co/papers/2503.02152', 'abstract': 'While advances in large language models (LLMs) have greatly improved the quality of synthetic text data in recent years, synthesizing tabular data has received relatively less attention. We address this disparity with Tabby, a simple but powerful post-training modification to the standard Transformer language model architecture, enabling its use for tabular dataset synthesis. Tabby enables the representation of differences across columns using Gated Mixture-of-Experts, with column-specific sets of parameters. Empirically, Tabby results in data quality near or equal to that of real data. By pairing our novel LLM table training technique, Plain, with Tabby, we observe up to a 44% improvement in quality over previous methods. We also show that Tabby extends beyond tables to more general structured data, reaching parity with real data on a nested JSON dataset as well.', 'score': 1, 'issue_id': 2545, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 4', 'zh': '3æœˆ4æ—¥'}, 'hash': '1d33263eb28e0be2', 'authors': ['Sonia Cromp', 'Satya Sai Srinath Namburi GNVV', 'Mohammed Alkhudhayri', 'Catherine Cao', 'Samuel Guo', 'Nicholas Roberts', 'Frederic Sala'], 'affiliations': ['GE HealthCare', 'University of Wisconsin-Madison'], 'pdf_title_img': 'assets/pdf/title_img/2503.02152.jpg', 'data': {'categories': ['#dataset', '#synthetic', '#architecture', '#data'], 'emoji': 'ğŸ“Š', 'ru': {'title': 'Tabby: Ğ¿Ñ€Ğ¾Ñ€Ñ‹Ğ² Ğ² ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğµ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Tabby - Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Tabby Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Gated Mixture-of-Experts Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑÑ‚Ğ¾Ğ»Ğ±Ñ†Ğ°Ğ¼Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ÑÑ‚Ğ¾Ğ»Ğ±Ñ†Ğ° Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². Ğ’ ÑĞ¾Ñ‡ĞµÑ‚Ğ°Ğ½Ğ¸Ğ¸ Ñ Ğ½Ğ¾Ğ²Ğ¾Ğ¹ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¾Ğ¹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Plain, Tabby Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ¾ 44% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ‚Ğ°ĞºĞ¶Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ° Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ²Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ JSON-Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸.'}, 'en': {'title': 'Tabby: Transforming Tabular Data Synthesis with LLMs', 'desc': 'This paper introduces Tabby, a new method for synthesizing tabular data using a modified Transformer language model. It employs Gated Mixture-of-Experts to effectively capture the unique characteristics of different columns in a dataset. The results show that Tabby can generate synthetic data that is comparable in quality to real data, achieving significant improvements over existing techniques. Additionally, Tabby is versatile enough to be applied to other structured data formats, such as nested JSON, demonstrating its broad applicability in data synthesis tasks.'}, 'zh': {'title': 'Tabbyï¼šè¡¨æ ¼æ•°æ®åˆæˆçš„æ–°çªç ´', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºTabbyçš„æ–¹æ³•ï¼Œç”¨äºåˆæˆè¡¨æ ¼æ•°æ®ã€‚Tabbyæ˜¯å¯¹æ ‡å‡†Transformerè¯­è¨€æ¨¡å‹æ¶æ„çš„åè®­ç»ƒä¿®æ”¹ï¼Œèƒ½å¤Ÿæœ‰æ•ˆè¡¨ç¤ºåˆ—ä¹‹é—´çš„å·®å¼‚ã€‚é€šè¿‡ä½¿ç”¨é—¨æ§æ··åˆä¸“å®¶æ¨¡å‹ï¼ŒTabbyä¸ºæ¯ä¸€åˆ—æä¾›ç‰¹å®šçš„å‚æ•°é›†ï¼Œä»è€Œæé«˜æ•°æ®è´¨é‡ã€‚å®éªŒè¡¨æ˜ï¼ŒTabbyåœ¨åˆæˆæ•°æ®çš„è´¨é‡ä¸Šæ¥è¿‘æˆ–ç­‰åŒäºçœŸå®æ•°æ®ï¼Œå¹¶ä¸”åœ¨å¤„ç†åµŒå¥—JSONæ•°æ®é›†æ—¶ä¹Ÿè¡¨ç°å‡ºè‰²ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.02304', 'title': 'A Token-level Text Image Foundation Model for Document Understanding', 'url': 'https://huggingface.co/papers/2503.02304', 'abstract': 'In recent years, general visual foundation models (VFMs) have witnessed increasing adoption, particularly as image encoders for popular multi-modal large language models (MLLMs). However, without semantically fine-grained supervision, these models still encounter fundamental prediction errors in the context of downstream text-image-related tasks, i.e., perception, understanding and reasoning with images containing small and dense texts. To bridge this gap, we develop TokenOCR, the first token-level visual foundation model specifically tailored for text-image-related tasks, designed to support a variety of traditional downstream applications. To facilitate the pretraining of TokenOCR, we also devise a high-quality data production pipeline that constructs the first token-level image text dataset, TokenIT, comprising 20 million images and 1.8 billion token-mask pairs. Furthermore, leveraging this foundation with exceptional image-as-text capability, we seamlessly replace previous VFMs with TokenOCR to construct a document-level MLLM, TokenVL, for VQA-based document understanding tasks. Finally, extensive experiments demonstrate the effectiveness of TokenOCR and TokenVL. Code, datasets, and weights will be available at https://token-family.github.io/TokenOCR_project.', 'score': 1, 'issue_id': 2545, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 4', 'zh': '3æœˆ4æ—¥'}, 'hash': 'b275d8ba26cca4b5', 'authors': ['Tongkun Guan', 'Zining Wang', 'Pei Fu', 'Zhengtao Guo', 'Wei Shen', 'Kai Zhou', 'Tiezhu Yue', 'Chen Duan', 'Hao Sun', 'Qianyi Jiang', 'Junfeng Luo', 'Xiaokang Yang'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2503.02304.jpg', 'data': {'categories': ['#dataset', '#cv', '#agi', '#reasoning', '#optimization', '#multimodal', '#games', '#data', '#open_source'], 'emoji': 'ğŸ”', 'ru': {'title': 'TokenOCR: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ TokenOCR - Ğ¿ĞµÑ€Ğ²ÑƒÑ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…. Ğ”Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… TokenIT, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 20 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ 1,8 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ° Ğ¿Ğ°Ñ€ Ñ‚Ğ¾ĞºĞµĞ½-Ğ¼Ğ°ÑĞºĞ°. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ TokenOCR Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ TokenVL Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ TokenOCR Ğ¸ TokenVL.'}, 'en': {'title': 'TokenOCR: Bridging Text and Image Understanding', 'desc': 'This paper introduces TokenOCR, a novel visual foundation model designed specifically for tasks that involve understanding and reasoning about images with dense text. Traditional visual foundation models struggle with these tasks due to a lack of detailed supervision, leading to prediction errors. TokenOCR addresses this issue by utilizing a new dataset, TokenIT, which contains 20 million images and 1.8 billion token-mask pairs for effective pretraining. The model is integrated into a document-level multi-modal large language model, TokenVL, enhancing its capabilities for visual question answering and document understanding tasks.'}, 'zh': {'title': 'TokenOCRï¼šæ–‡æœ¬å›¾åƒä»»åŠ¡çš„è§†è§‰åŸºç¡€æ¨¡å‹æ–°çªç ´', 'desc': 'è¿‘å¹´æ¥ï¼Œé€šç”¨è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆVFMï¼‰åœ¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ä¸­å¾—åˆ°äº†å¹¿æ³›åº”ç”¨ï¼Œå°¤å…¶ä½œä¸ºå›¾åƒç¼–ç å™¨ã€‚ç„¶è€Œï¼Œåœ¨æ²¡æœ‰ç»†ç²’åº¦è¯­ä¹‰ç›‘ç£çš„æƒ…å†µä¸‹ï¼Œè¿™äº›æ¨¡å‹åœ¨å¤„ç†ä¸æ–‡æœ¬ç›¸å…³çš„å›¾åƒä»»åŠ¡æ—¶ä»ç„¶ä¼šå‡ºç°åŸºæœ¬çš„é¢„æµ‹é”™è¯¯ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼€å‘äº†TokenOCRï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªä¸“é—¨é’ˆå¯¹æ–‡æœ¬-å›¾åƒç›¸å…³ä»»åŠ¡çš„æ ‡è®°çº§è§†è§‰åŸºç¡€æ¨¡å‹ï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªé«˜è´¨é‡çš„æ•°æ®ç”Ÿäº§ç®¡é“ï¼Œæ„å»ºäº†åŒ…å«2000ä¸‡å¼ å›¾åƒå’Œ18äº¿ä¸ªæ ‡è®°-æ©ç å¯¹çš„TokenITæ•°æ®é›†ã€‚é€šè¿‡è¿™ä¸€åŸºç¡€ï¼Œæˆ‘ä»¬æˆåŠŸåœ°ç”¨TokenOCRæ›¿æ¢äº†ä¹‹å‰çš„VFMï¼Œæ„å»ºäº†ç”¨äºæ–‡æ¡£ç†è§£ä»»åŠ¡çš„TokenVLæ–‡æ¡£çº§MLLMï¼Œå®éªŒç»“æœè¯æ˜äº†TokenOCRå’ŒTokenVLçš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.00200', 'title': 'Unified Video Action Model', 'url': 'https://huggingface.co/papers/2503.00200', 'abstract': 'A unified video and action model holds significant promise for robotics, where videos provide rich scene information for action prediction, and actions provide dynamics information for video prediction. However, effectively combining video generation and action prediction remains challenging, and current video generation-based methods struggle to match the performance of direct policy learning in action accuracy and inference speed. To bridge this gap, we introduce the Unified Video Action model (UVA), which jointly optimizes video and action predictions to achieve both high accuracy and efficient action inference. The key lies in learning a joint video-action latent representation and decoupling video-action decoding. The joint latent representation bridges the visual and action domains, effectively modeling the relationship between video and action sequences. Meanwhile, the decoupled decoding, powered by two lightweight diffusion heads, enables high-speed action inference by bypassing video generation during inference. Such a unified framework further enables versatile functionality through masked input training. By selectively masking actions or videos, a single model can tackle diverse tasks beyond policy learning, such as forward and inverse dynamics modeling and video generation. Via an extensive set of experiments, we demonstrate that UVA can serve as a general-purpose solution for a wide range of robotics tasks, such as policy learning, forward/inverse dynamics and video observation prediction, without compromising performance compared to methods tailored for specific applications. Results are best viewed on https://unified-video-action-model.github.io/.', 'score': 0, 'issue_id': 2551, 'pub_date': '2025-02-28', 'pub_date_card': {'ru': '28 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 28', 'zh': '2æœˆ28æ—¥'}, 'hash': '286c6abc07fdf95e', 'authors': ['Shuang Li', 'Yihuai Gao', 'Dorsa Sadigh', 'Shuran Song'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2503.00200.jpg', 'data': {'categories': ['#robotics', '#video', '#games', '#optimization'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞµ', 'desc': 'ĞœĞ¾Ğ´ĞµĞ»ÑŒ UVA (Unified Video Action) Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ ĞµĞ´Ğ¸Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ² Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞµ. ĞĞ½Ğ° Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¸ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ. UVA Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹, Ğ¸Ğ·Ğ±ĞµĞ³Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼Ğ° Ğº Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº, Ğ¿Ñ€ÑĞ¼ÑƒÑ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºÑƒ, Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Bridging Video and Action for Smarter Robotics', 'desc': "The Unified Video Action model (UVA) integrates video generation and action prediction to enhance robotics applications. It achieves this by creating a joint latent representation that connects visual data with action sequences, allowing for better modeling of their interrelationship. The model employs decoupled decoding with lightweight diffusion heads, which speeds up action inference by avoiding the need for video generation during this process. UVA's versatility is further demonstrated through masked input training, enabling it to perform various tasks like policy learning and dynamics modeling efficiently."}, 'zh': {'title': 'ç»Ÿä¸€è§†é¢‘ä¸åŠ¨ä½œæ¨¡å‹ï¼šæå‡æœºå™¨äººæ™ºèƒ½çš„å…³é”®', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„è§†é¢‘ä¸åŠ¨ä½œæ¨¡å‹ï¼ˆUVAï¼‰ï¼Œæ—¨åœ¨æé«˜æœºå™¨äººé¢†åŸŸä¸­çš„åŠ¨ä½œé¢„æµ‹å’Œè§†é¢‘ç”Ÿæˆçš„æ€§èƒ½ã€‚UVAé€šè¿‡è”åˆä¼˜åŒ–è§†é¢‘å’ŒåŠ¨ä½œé¢„æµ‹ï¼Œå­¦ä¹ è§†é¢‘ä¸åŠ¨ä½œçš„è”åˆæ½œåœ¨è¡¨ç¤ºï¼Œä»è€Œæœ‰æ•ˆå»ºæ¨¡è§†é¢‘ä¸åŠ¨ä½œåºåˆ—ä¹‹é—´çš„å…³ç³»ã€‚è¯¥æ¨¡å‹é‡‡ç”¨è§£è€¦è§£ç çš„æ–¹æ³•ï¼Œåˆ©ç”¨è½»é‡çº§çš„æ‰©æ•£å¤´å®ç°å¿«é€Ÿçš„åŠ¨ä½œæ¨ç†ï¼Œé¿å…äº†åœ¨æ¨ç†è¿‡ç¨‹ä¸­ç”Ÿæˆè§†é¢‘çš„éœ€æ±‚ã€‚é€šè¿‡æ©è”½è¾“å…¥è®­ç»ƒï¼ŒUVAèƒ½å¤Ÿå¤„ç†å¤šç§ä»»åŠ¡ï¼Œå¦‚ç­–ç•¥å­¦ä¹ ã€å‰å‘/é€†å‘åŠ¨åŠ›å­¦å»ºæ¨¡å’Œè§†é¢‘ç”Ÿæˆï¼Œå±•ç°å‡ºå¹¿æ³›çš„é€‚ç”¨æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.01842', 'title': 'Discrete-Time Hybrid Automata Learning: Legged Locomotion Meets Skateboarding', 'url': 'https://huggingface.co/papers/2503.01842', 'abstract': 'This paper introduces Discrete-time Hybrid Automata Learning (DHAL), a framework using on-policy Reinforcement Learning to identify and execute mode-switching without trajectory segmentation or event function learning. Hybrid dynamical systems, which include continuous flow and discrete mode switching, can model robotics tasks like legged robot locomotion. Model-based methods usually depend on predefined gaits, while model-free approaches lack explicit mode-switching knowledge. Current methods identify discrete modes via segmentation before regressing continuous flow, but learning high-dimensional complex rigid body dynamics without trajectory labels or segmentation is a challenging open problem. Our approach incorporates a beta policy distribution and a multi-critic architecture to model contact-guided motions, exemplified by a challenging quadrupedal robot skateboard task. We validate our method through simulations and real-world tests, demonstrating robust performance in hybrid dynamical systems.', 'score': 0, 'issue_id': 2545, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 3', 'zh': '3æœˆ3æ—¥'}, 'hash': '07eab4aa91f88137', 'authors': ['Hang Liu', 'Sangli Teng', 'Ben Liu', 'Wei Zhang', 'Maani Ghaffari'], 'affiliations': ['Southern University of Science and Technology', 'University of Michigan'], 'pdf_title_img': 'assets/pdf/title_img/2503.01842.jpg', 'data': {'categories': ['#games', '#rl', '#robotics', '#optimization'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¼ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼ Ğ±ĞµĞ· ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ DHAL - Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¼ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ°Ğ¼ Ñ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½ĞµĞ¼, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ±ĞµÑ‚Ğ°-Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹, ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚Ğ°ĞºÑ‚Ğ°Ğ¼Ğ¸. DHAL Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ¸Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹. Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ‚Ğ²ĞµÑ€Ğ¾Ğ½Ğ¾Ğ³Ğ¸Ğ¼ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ¼ Ğ½Ğ° ÑĞºĞµĞ¹Ñ‚Ğ±Ğ¾Ñ€Ğ´Ğµ Ğ² ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¼Ğ¸Ñ€Ğµ.'}, 'en': {'title': 'Learning Mode-Switching in Robotics Without Segmentation', 'desc': 'This paper presents Discrete-time Hybrid Automata Learning (DHAL), a novel framework that leverages on-policy Reinforcement Learning to effectively manage mode-switching in hybrid dynamical systems. Unlike traditional methods that require trajectory segmentation or predefined gaits, DHAL learns to identify and execute mode transitions directly from the data. The approach utilizes a beta policy distribution and a multi-critic architecture to handle complex contact-guided motions, particularly in tasks like quadrupedal robot locomotion. The results show that DHAL can robustly learn high-dimensional dynamics without the need for explicit trajectory labels, making it a significant advancement in the field.'}, 'zh': {'title': 'æ— è½¨è¿¹åˆ†å‰²çš„æ™ºèƒ½æ¨¡å¼åˆ‡æ¢å­¦ä¹ ', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ç¦»æ•£æ—¶é—´æ··åˆè‡ªåŠ¨æœºå­¦ä¹ ï¼ˆDHALï¼‰æ¡†æ¶ï¼Œåˆ©ç”¨åŸºäºç­–ç•¥çš„å¼ºåŒ–å­¦ä¹ æ¥è¯†åˆ«å’Œæ‰§è¡Œæ¨¡å¼åˆ‡æ¢ï¼Œè€Œæ— éœ€è¿›è¡Œè½¨è¿¹åˆ†å‰²æˆ–äº‹ä»¶å‡½æ•°å­¦ä¹ ã€‚æ··åˆåŠ¨æ€ç³»ç»Ÿèƒ½å¤Ÿæ¨¡æ‹Ÿæœºå™¨äººä»»åŠ¡ï¼Œä¾‹å¦‚å››è¶³æœºå™¨äººè¡Œèµ°ã€‚ä¼ ç»Ÿçš„æ¨¡å‹æ–¹æ³•é€šå¸¸ä¾èµ–äºé¢„å®šä¹‰çš„æ­¥æ€ï¼Œè€Œæ— æ¨¡å‹çš„æ–¹æ³•åˆ™ç¼ºä¹æ˜ç¡®çš„æ¨¡å¼åˆ‡æ¢çŸ¥è¯†ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡å¼•å…¥è´å¡”ç­–ç•¥åˆ†å¸ƒå’Œå¤šé‡è¯„è®ºå®¶æ¶æ„ï¼ŒæˆåŠŸåœ°å»ºæ¨¡äº†æ¥è§¦å¼•å¯¼çš„è¿åŠ¨ï¼Œå¹¶åœ¨ä»¿çœŸå’Œå®é™…æµ‹è¯•ä¸­éªŒè¯äº†å…¶åœ¨æ··åˆåŠ¨æ€ç³»ç»Ÿä¸­çš„å¼ºå¤§æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.06053', 'title': 'DropletVideo: A Dataset and Approach to Explore Integral Spatio-Temporal\n  Consistent Video Generation', 'url': 'https://huggingface.co/papers/2503.06053', 'abstract': 'Spatio-temporal consistency is a critical research topic in video generation. A qualified generated video segment must ensure plot plausibility and coherence while maintaining visual consistency of objects and scenes across varying viewpoints. Prior research, especially in open-source projects, primarily focuses on either temporal or spatial consistency, or their basic combination, such as appending a description of a camera movement after a prompt without constraining the outcomes of this movement. However, camera movement may introduce new objects to the scene or eliminate existing ones, thereby overlaying and affecting the preceding narrative. Especially in videos with numerous camera movements, the interplay between multiple plots becomes increasingly complex. This paper introduces and examines integral spatio-temporal consistency, considering the synergy between plot progression and camera techniques, and the long-term impact of prior content on subsequent generation. Our research encompasses dataset construction through to the development of the model. Initially, we constructed a DropletVideo-10M dataset, which comprises 10 million videos featuring dynamic camera motion and object actions. Each video is annotated with an average caption of 206 words, detailing various camera movements and plot developments. Following this, we developed and trained the DropletVideo model, which excels in preserving spatio-temporal coherence during video generation. The DropletVideo dataset and model are accessible at https://dropletx.github.io.', 'score': 72, 'issue_id': 2758, 'pub_date': '2025-03-08', 'pub_date_card': {'ru': '8 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 8', 'zh': '3æœˆ8æ—¥'}, 'hash': 'c6a544d3dc36bfbf', 'authors': ['Runze Zhang', 'Guoguang Du', 'Xiaochuan Li', 'Qi Jia', 'Liang Jin', 'Lu Liu', 'Jingjing Wang', 'Cong Xu', 'Zhenhua Guo', 'Yaqian Zhao', 'Xiaoli Gong', 'Rengang Li', 'Baoyu Fan'], 'affiliations': ['IEIT System Co., Ltd.', 'Nankai University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.06053.jpg', 'data': {'categories': ['#open_source', '#dataset', '#video', '#training'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Ğ˜Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ°Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾ÑĞ²ÑĞ·ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸ĞµĞ¼ ÑÑĞ¶ĞµÑ‚Ğ° Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ ĞºĞ°Ğ¼ĞµÑ€Ñ‹, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰ĞµĞ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ½Ğ° Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑƒÑÑ‰ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ. Ğ”Ğ»Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ñ‹Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ DropletVideo-10M, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 10 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ĞµĞ¼ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑĞ¼Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ±Ñ‹Ğ»Ğ° Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ DropletVideo, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ°Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'Achieving Seamless Video Generation with Spatio-Temporal Consistency', 'desc': 'This paper addresses the challenge of spatio-temporal consistency in video generation, which is essential for creating coherent and visually consistent narratives. It highlights the limitations of previous research that often focuses on either temporal or spatial aspects without integrating them effectively. The authors introduce a new dataset, DropletVideo-10M, containing 10 million videos with dynamic camera movements and detailed annotations, which aids in training their model. The proposed DropletVideo model demonstrates improved performance in maintaining coherence across both plot progression and camera techniques, ensuring a more seamless video generation experience.'}, 'zh': {'title': 'æå‡è§†é¢‘ç”Ÿæˆçš„æ—¶ç©ºä¸€è‡´æ€§', 'desc': 'æœ¬è®ºæ–‡ç ”ç©¶äº†è§†é¢‘ç”Ÿæˆä¸­çš„æ—¶ç©ºä¸€è‡´æ€§é—®é¢˜ã€‚ç”Ÿæˆçš„è§†é¢‘ç‰‡æ®µéœ€è¦åœ¨æƒ…èŠ‚åˆç†æ€§å’Œè§†è§‰ä¸€è‡´æ€§ä¹‹é—´å–å¾—å¹³è¡¡ï¼Œå°¤å…¶æ˜¯åœ¨ä¸åŒè§†è§’ä¸‹çš„ç‰©ä½“å’Œåœºæ™¯ã€‚ä»¥å¾€çš„ç ”ç©¶ä¸»è¦å…³æ³¨æ—¶é—´æˆ–ç©ºé—´ä¸€è‡´æ€§ï¼Œç¼ºä¹å¯¹ä¸¤è€…çš„ç»¼åˆè€ƒè™‘ã€‚æˆ‘ä»¬æå‡ºäº†æ•´ä½“æ—¶ç©ºä¸€è‡´æ€§çš„æ–¹æ³•ï¼Œæ„å»ºäº†åŒ…å«1000ä¸‡æ®µåŠ¨æ€é•œå¤´å’Œç‰©ä½“åŠ¨ä½œçš„è§†é¢‘æ•°æ®é›†ï¼Œå¹¶å¼€å‘äº†DropletVideoæ¨¡å‹ï¼Œä»¥æé«˜è§†é¢‘ç”Ÿæˆçš„æ—¶ç©ºè¿è´¯æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.12533', 'title': 'Being-0: A Humanoid Robotic Agent with Vision-Language Models and\n  Modular Skills', 'url': 'https://huggingface.co/papers/2503.12533', 'abstract': "Building autonomous robotic agents capable of achieving human-level performance in real-world embodied tasks is an ultimate goal in humanoid robot research. Recent advances have made significant progress in high-level cognition with Foundation Models (FMs) and low-level skill development for humanoid robots. However, directly combining these components often results in poor robustness and efficiency due to compounding errors in long-horizon tasks and the varied latency of different modules. We introduce Being-0, a hierarchical agent framework that integrates an FM with a modular skill library. The FM handles high-level cognitive tasks such as instruction understanding, task planning, and reasoning, while the skill library provides stable locomotion and dexterous manipulation for low-level control. To bridge the gap between these levels, we propose a novel Connector module, powered by a lightweight vision-language model (VLM). The Connector enhances the FM's embodied capabilities by translating language-based plans into actionable skill commands and dynamically coordinating locomotion and manipulation to improve task success. With all components, except the FM, deployable on low-cost onboard computation devices, Being-0 achieves efficient, real-time performance on a full-sized humanoid robot equipped with dexterous hands and active vision. Extensive experiments in large indoor environments demonstrate Being-0's effectiveness in solving complex, long-horizon tasks that require challenging navigation and manipulation subtasks. For further details and videos, visit https://beingbeyond.github.io/being-0.", 'score': 49, 'issue_id': 2755, 'pub_date': '2025-03-16', 'pub_date_card': {'ru': '16 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 16', 'zh': '3æœˆ16æ—¥'}, 'hash': '18b42d1274f6b260', 'authors': ['Haoqi Yuan', 'Yu Bai', 'Yuhui Fu', 'Bohan Zhou', 'Yicheng Feng', 'Xinrun Xu', 'Yi Zhan', 'BÃ¶rje F. Karlsson', 'Zongqing Lu'], 'affiliations': ['BAAI', 'BeingBeyond', 'Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2503.12533.jpg', 'data': {'categories': ['#reasoning', '#agents', '#optimization', '#robotics', '#agi', '#multimodal'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Being-0: ĞœĞ¾ÑÑ‚ Ğ¼ĞµĞ¶Ğ´Ñƒ AI Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ¼ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¼Ğ¸Ñ€Ğµ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Being-0 - Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ²-Ğ³ÑƒĞ¼Ğ°Ğ½Ğ¾Ğ¸Ğ´Ğ¾Ğ², Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰ÑƒÑ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ (Ğ¤Ğœ) Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ñ… ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞºĞ¾Ğ¹ Ğ½Ğ¸Ğ·ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ñ… Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ². ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ¼ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Connector Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ğ¾Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ÑĞ²ÑĞ·Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ¤Ğœ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ°Ğ¼Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Being-0 ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğµ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¹, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ Ğ² Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ½Ğ° Ğ½ĞµĞ´Ğ¾Ñ€Ğ¾Ğ³Ğ¾Ğ¼ Ğ±Ğ¾Ñ€Ñ‚Ğ¾Ğ²Ğ¾Ğ¼ Ğ¾Ğ±Ğ¾Ñ€ÑƒĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Being-0 Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¿Ğ¾Ğ¼ĞµÑ‰ĞµĞ½Ğ¸ÑÑ….'}, 'en': {'title': 'Bridging High-Level Cognition and Low-Level Skills in Humanoid Robots', 'desc': "This paper presents Being-0, a hierarchical framework designed to enhance humanoid robots' performance in complex tasks. It combines a Foundation Model (FM) for high-level cognitive functions with a modular skill library for low-level control, addressing issues of robustness and efficiency. A novel Connector module, utilizing a lightweight vision-language model, translates language-based plans into actionable commands, improving coordination between locomotion and manipulation. The system demonstrates effective real-time performance on humanoid robots in challenging environments, showcasing its ability to tackle long-horizon tasks successfully."}, 'zh': {'title': 'æå‡ç±»äººæœºå™¨äººæ™ºèƒ½çš„å±‚æ¬¡åŒ–æ¡†æ¶', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºBeing-0çš„å±‚æ¬¡åŒ–æ™ºèƒ½ä½“æ¡†æ¶ï¼Œæ—¨åœ¨æå‡ç±»äººæœºå™¨äººåœ¨ç°å®ä¸–ç•Œä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚è¯¥æ¡†æ¶å°†åŸºç¡€æ¨¡å‹ï¼ˆFMï¼‰ä¸æ¨¡å—åŒ–æŠ€èƒ½åº“ç›¸ç»“åˆï¼ŒFMè´Ÿè´£é«˜å±‚æ¬¡çš„è®¤çŸ¥ä»»åŠ¡ï¼Œå¦‚æŒ‡ä»¤ç†è§£å’Œä»»åŠ¡è§„åˆ’ï¼Œè€ŒæŠ€èƒ½åº“åˆ™æä¾›ç¨³å®šçš„è¿åŠ¨å’Œçµå·§æ“ä½œã€‚ä¸ºäº†è¿æ¥è¿™ä¸¤ä¸ªå±‚æ¬¡ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„è¿æ¥æ¨¡å—ï¼Œåˆ©ç”¨è½»é‡çº§çš„è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å°†è¯­è¨€è®¡åˆ’è½¬åŒ–ä¸ºå¯æ‰§è¡Œçš„æŠ€èƒ½å‘½ä»¤ã€‚é€šè¿‡åœ¨ä½æˆæœ¬çš„è®¡ç®—è®¾å¤‡ä¸Šéƒ¨ç½²å¤§éƒ¨åˆ†ç»„ä»¶ï¼ŒBeing-0åœ¨å¤æ‚çš„é•¿æ—¶é—´ä»»åŠ¡ä¸­å±•ç°å‡ºé«˜æ•ˆçš„å®æ—¶æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.12885', 'title': 'DreamRenderer: Taming Multi-Instance Attribute Control in Large-Scale\n  Text-to-Image Models', 'url': 'https://huggingface.co/papers/2503.12885', 'abstract': 'Image-conditioned generation methods, such as depth- and canny-conditioned approaches, have demonstrated remarkable abilities for precise image synthesis. However, existing models still struggle to accurately control the content of multiple instances (or regions). Even state-of-the-art models like FLUX and 3DIS face challenges, such as attribute leakage between instances, which limits user control. To address these issues, we introduce DreamRenderer, a training-free approach built upon the FLUX model. DreamRenderer enables users to control the content of each instance via bounding boxes or masks, while ensuring overall visual harmony. We propose two key innovations: 1) Bridge Image Tokens for Hard Text Attribute Binding, which uses replicated image tokens as bridge tokens to ensure that T5 text embeddings, pre-trained solely on text data, bind the correct visual attributes for each instance during Joint Attention; 2) Hard Image Attribute Binding applied only to vital layers. Through our analysis of FLUX, we identify the critical layers responsible for instance attribute rendering and apply Hard Image Attribute Binding only in these layers, using soft binding in the others. This approach ensures precise control while preserving image quality. Evaluations on the COCO-POS and COCO-MIG benchmarks demonstrate that DreamRenderer improves the Image Success Ratio by 17.7% over FLUX and enhances the performance of layout-to-image models like GLIGEN and 3DIS by up to 26.8%. Project Page: https://limuloo.github.io/DreamRenderer/.', 'score': 34, 'issue_id': 2754, 'pub_date': '2025-03-17', 'pub_date_card': {'ru': '17 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 17', 'zh': '3æœˆ17æ—¥'}, 'hash': 'e6332652493dc1ab', 'authors': ['Dewei Zhou', 'Mingwei Li', 'Zongxin Yang', 'Yi Yang'], 'affiliations': ['DBMI, HMS, Harvard University', 'RELER, CCAI, Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.12885.jpg', 'data': {'categories': ['#optimization', '#cv', '#training', '#games', '#benchmark'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ¢Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ½Ğ°Ğ´ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'DreamRenderer - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑĞ¼, Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ FLUX. ĞĞ½ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ğ¼Ğ¾Ğµ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ñ€Ğ°Ğ¼Ğ¾Ğº Ğ¸Ğ»Ğ¸ Ğ¼Ğ°ÑĞ¾Ğº. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾ÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¶ĞµÑÑ‚ĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºĞ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ¾Ğ² Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ¶ĞµÑÑ‚ĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºĞ¸ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ¾Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ² ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ñ… ÑĞ»Ğ¾ÑÑ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'DreamRenderer: Precise Control in Image Generation', 'desc': 'This paper presents DreamRenderer, a novel approach for image generation that allows precise control over multiple instances in an image. Unlike existing models, DreamRenderer uses a training-free method that leverages the FLUX model to bind visual attributes to specific instances through bounding boxes or masks. The key innovations include Bridge Image Tokens for ensuring accurate text-to-image attribute mapping and Hard Image Attribute Binding focused on critical layers for instance rendering. Evaluations show that DreamRenderer significantly outperforms FLUX and enhances other layout-to-image models, demonstrating its effectiveness in generating high-quality images with user-defined content.'}, 'zh': {'title': 'DreamRendererï¼šç²¾ç¡®æ§åˆ¶å›¾åƒç”Ÿæˆçš„åˆ›æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºDreamRendererçš„å›¾åƒç”Ÿæˆæ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ¨¡å‹åœ¨å¤šå®ä¾‹å†…å®¹æ§åˆ¶æ–¹é¢çš„ä¸è¶³ã€‚DreamRendereråŸºäºFLUXæ¨¡å‹ï¼Œå…è®¸ç”¨æˆ·é€šè¿‡è¾¹ç•Œæ¡†æˆ–æ©ç ç²¾ç¡®æ§åˆ¶æ¯ä¸ªå®ä¾‹çš„å†…å®¹ï¼ŒåŒæ—¶ä¿æŒæ•´ä½“è§†è§‰å’Œè°ã€‚æˆ‘ä»¬æå‡ºäº†ä¸¤é¡¹å…³é”®åˆ›æ–°ï¼šä¸€æ˜¯ä½¿ç”¨æ¡¥æ¥å›¾åƒæ ‡è®°æ¥ç¡®ä¿æ–‡æœ¬å±æ€§çš„å‡†ç¡®ç»‘å®šï¼ŒäºŒæ˜¯åœ¨å…³é”®å±‚ä¸­åº”ç”¨ç¡¬å›¾åƒå±æ€§ç»‘å®šï¼Œä»¥æé«˜å®ä¾‹å±æ€§æ¸²æŸ“çš„ç²¾åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDreamRendereråœ¨å›¾åƒæˆåŠŸç‡ä¸Šæ¯”FLUXæé«˜äº†17.7%ï¼Œå¹¶ä¸”åœ¨å¸ƒå±€åˆ°å›¾åƒæ¨¡å‹çš„æ€§èƒ½ä¸Šæå‡äº†å¤šè¾¾26.8%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.12590', 'title': 'Personalize Anything for Free with Diffusion Transformer', 'url': 'https://huggingface.co/papers/2503.12590', 'abstract': 'Personalized image generation aims to produce images of user-specified concepts while enabling flexible editing. Recent training-free approaches, while exhibit higher computational efficiency than training-based methods, struggle with identity preservation, applicability, and compatibility with diffusion transformers (DiTs). In this paper, we uncover the untapped potential of DiT, where simply replacing denoising tokens with those of a reference subject achieves zero-shot subject reconstruction. This simple yet effective feature injection technique unlocks diverse scenarios, from personalization to image editing. Building upon this observation, we propose Personalize Anything, a training-free framework that achieves personalized image generation in DiT through: 1) timestep-adaptive token replacement that enforces subject consistency via early-stage injection and enhances flexibility through late-stage regularization, and 2) patch perturbation strategies to boost structural diversity. Our method seamlessly supports layout-guided generation, multi-subject personalization, and mask-controlled editing. Evaluations demonstrate state-of-the-art performance in identity preservation and versatility. Our work establishes new insights into DiTs while delivering a practical paradigm for efficient personalization.', 'score': 22, 'issue_id': 2754, 'pub_date': '2025-03-16', 'pub_date_card': {'ru': '16 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 16', 'zh': '3æœˆ16æ—¥'}, 'hash': '25e3ee07c4a11ed2', 'authors': ['Haoran Feng', 'Zehuan Huang', 'Lin Li', 'Hairong Lv', 'Lu Sheng'], 'affiliations': ['Beihang University', 'Renmin University of China', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.12590.jpg', 'data': {'categories': ['#cv', '#multimodal', '#diffusion'], 'emoji': 'ğŸ¨', 'ru': {'title': 'ĞŸĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² (DiT). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Personalize Anything, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ·Ğ°Ğ¼ĞµĞ½Ñƒ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ²Ğ¾Ğ·Ğ¼ÑƒÑ‰ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ñ‚Ñ‡ĞµĞ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Personalize Anything: Efficient Image Generation with Diffusion Transformers', 'desc': 'This paper presents a novel approach to personalized image generation using diffusion transformers (DiTs) without the need for extensive training. The authors introduce a technique that replaces denoising tokens with those from a reference subject, enabling effective zero-shot subject reconstruction. They propose a framework called Personalize Anything, which incorporates adaptive token replacement and patch perturbation strategies to enhance identity preservation and structural diversity. The results show that this method achieves state-of-the-art performance in generating personalized images while allowing for flexible editing options.'}, 'zh': {'title': 'ä¸ªæ€§åŒ–å›¾åƒç”Ÿæˆçš„æ–°è§†è§’', 'desc': 'ä¸ªæ€§åŒ–å›¾åƒç”Ÿæˆæ—¨åœ¨æ ¹æ®ç”¨æˆ·æŒ‡å®šçš„æ¦‚å¿µç”Ÿæˆå›¾åƒï¼Œå¹¶å…è®¸çµæ´»ç¼–è¾‘ã€‚æœ€è¿‘çš„æ— è®­ç»ƒæ–¹æ³•åœ¨è®¡ç®—æ•ˆç‡ä¸Šä¼˜äºåŸºäºè®­ç»ƒçš„æ–¹æ³•ï¼Œä½†åœ¨èº«ä»½ä¿æŒã€é€‚ç”¨æ€§å’Œä¸æ‰©æ•£å˜æ¢å™¨çš„å…¼å®¹æ€§æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚æœ¬æ–‡æ­ç¤ºäº†æ‰©æ•£å˜æ¢å™¨çš„æ½œåŠ›ï¼Œé€šè¿‡ç®€å•åœ°ç”¨å‚è€ƒå¯¹è±¡çš„å»å™ªä»¤ç‰Œæ›¿æ¢å®ç°é›¶-shotçš„å¯¹è±¡é‡å»ºã€‚æˆ‘ä»¬æå‡ºçš„â€œä¸ªæ€§åŒ–ä»»ä½•äº‹ç‰©â€æ¡†æ¶ï¼Œé€šè¿‡æ—¶é—´æ­¥è‡ªé€‚åº”ä»¤ç‰Œæ›¿æ¢å’Œè¡¥ä¸æ‰°åŠ¨ç­–ç•¥ï¼Œå®ç°äº†é«˜æ•ˆçš„ä¸ªæ€§åŒ–å›¾åƒç”Ÿæˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.12349', 'title': 'SPIN-Bench: How Well Do LLMs Plan Strategically and Reason Socially?', 'url': 'https://huggingface.co/papers/2503.12349', 'abstract': 'Reasoning and strategic behavior in social interactions is a hallmark of intelligence. This form of reasoning is significantly more sophisticated than isolated planning or reasoning tasks in static settings (e.g., math problem solving). In this paper, we present Strategic Planning, Interaction, and Negotiation (SPIN-Bench), a new multi-domain evaluation designed to measure the intelligence of strategic planning and social reasoning. While many existing benchmarks focus on narrow planning or single-agent reasoning, SPIN-Bench combines classical PDDL tasks, competitive board games, cooperative card games, and multi-agent negotiation scenarios in one unified framework. The framework includes both a benchmark as well as an arena to simulate and evaluate the variety of social settings to test reasoning and strategic behavior of AI agents. We formulate the benchmark SPIN-Bench by systematically varying action spaces, state complexity, and the number of interacting agents to simulate a variety of social settings where success depends on not only methodical and step-wise decision making, but also conceptual inference of other (adversarial or cooperative) participants. Our experiments reveal that while contemporary LLMs handle basic fact retrieval and short-range planning reasonably well, they encounter significant performance bottlenecks in tasks requiring deep multi-hop reasoning over large state spaces and socially adept coordination under uncertainty. We envision SPIN-Bench as a catalyst for future research on robust multi-agent planning, social reasoning, and human--AI teaming.', 'score': 22, 'issue_id': 2765, 'pub_date': '2025-03-16', 'pub_date_card': {'ru': '16 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 16', 'zh': '3æœˆ16æ—¥'}, 'hash': '5f862770d0575514', 'authors': ['Jianzhu Yao', 'Kevin Wang', 'Ryan Hsieh', 'Haisu Zhou', 'Tianqing Zou', 'Zerui Cheng', 'Zhangyang Wang', 'Pramod Viswanath'], 'affiliations': ['Department of Electrical and Computer Engineering, Princeton University, New Jersey, US', 'Department of Electrical and Computer Engineering, The University of Texas at Austin, Austin, US'], 'pdf_title_img': 'assets/pdf/title_img/2503.12349.jpg', 'data': {'categories': ['#games', '#agents', '#benchmark', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'SPIN-Bench: ĞÑ†ĞµĞ½ĞºĞ° ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¸ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ˜Ğ˜', 'desc': 'SPIN-Bench - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°. ĞĞ½ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ PDDL, ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ½Ğ°ÑÑ‚Ğ¾Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸Ğ³Ñ€Ñ‹, ĞºĞ¾Ğ¾Ğ¿ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ĞºĞ°Ñ€Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ¸Ğ³Ñ€Ñ‹ Ğ¸ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿ĞµÑ€ĞµĞ³Ğ¾Ğ²Ğ¾Ñ€Ğ¾Ğ². SPIN-Bench ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ğ°Ñ€ÑŒĞ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹, ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ Ğ¸ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¸Ñ‚ÑƒĞ°Ñ†Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ„Ğ°ĞºÑ‚Ğ¾Ğ² Ğ¸ ĞºÑ€Ğ°Ñ‚ĞºĞ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼, Ğ½Ğ¾ Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½Ñ‡Ğ°Ñ‚Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': "SPIN-Bench: Evaluating AI's Strategic and Social Intelligence", 'desc': "This paper introduces SPIN-Bench, a new evaluation framework designed to assess AI's ability in strategic planning and social reasoning across various domains. Unlike existing benchmarks that focus on single-agent tasks, SPIN-Bench integrates multiple scenarios, including competitive games and cooperative interactions, to provide a comprehensive testing ground. The framework systematically varies factors like action spaces and the number of agents to simulate complex social environments where success relies on both strategic decision-making and understanding others' behaviors. The findings indicate that while current large language models perform well in basic tasks, they struggle with complex reasoning and coordination in uncertain social contexts."}, 'zh': {'title': 'æ™ºèƒ½æ¨ç†ä¸æˆ˜ç•¥è¡Œä¸ºçš„æ–°åŸºå‡†', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„å¤šé¢†åŸŸè¯„ä¼°å·¥å…·ï¼Œç§°ä¸ºæˆ˜ç•¥è§„åˆ’ã€äº’åŠ¨ä¸è°ˆåˆ¤åŸºå‡†ï¼ˆSPIN-Benchï¼‰ï¼Œæ—¨åœ¨æµ‹é‡æˆ˜ç•¥è§„åˆ’å’Œç¤¾ä¼šæ¨ç†çš„æ™ºèƒ½æ°´å¹³ã€‚ä¸ç°æœ‰çš„åŸºå‡†ä¸åŒï¼ŒSPIN-Benchç»“åˆäº†ç»å…¸çš„PDDLä»»åŠ¡ã€ç«äº‰æ€§æ£‹ç›˜æ¸¸æˆã€åˆä½œæ€§çº¸ç‰Œæ¸¸æˆå’Œå¤šæ™ºèƒ½ä½“è°ˆåˆ¤åœºæ™¯ï¼Œæä¾›äº†ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ã€‚è¯¥æ¡†æ¶ä¸ä»…åŒ…æ‹¬åŸºå‡†æµ‹è¯•ï¼Œè¿˜æä¾›äº†ä¸€ä¸ªæ¨¡æ‹Ÿå’Œè¯„ä¼°å„ç§ç¤¾ä¼šç¯å¢ƒçš„åœºæ‰€ï¼Œä»¥æµ‹è¯•äººå·¥æ™ºèƒ½ä»£ç†çš„æ¨ç†å’Œæˆ˜ç•¥è¡Œä¸ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå°½ç®¡å½“å‰çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åŸºæœ¬äº‹å®æ£€ç´¢å’ŒçŸ­æœŸè§„åˆ’æ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨éœ€è¦æ·±åº¦å¤šè·³æ¨ç†å’Œä¸ç¡®å®šæ€§ä¸‹çš„ç¤¾ä¼šåè°ƒä»»åŠ¡ä¸­å´é‡åˆ°äº†æ˜¾è‘—çš„æ€§èƒ½ç“¶é¢ˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.13327', 'title': 'Edit Transfer: Learning Image Editing via Vision In-Context Relations', 'url': 'https://huggingface.co/papers/2503.13327', 'abstract': 'We introduce a new setting, Edit Transfer, where a model learns a transformation from just a single source-target example and applies it to a new query image. While text-based methods excel at semantic manipulations through textual prompts, they often struggle with precise geometric details (e.g., poses and viewpoint changes). Reference-based editing, on the other hand, typically focuses on style or appearance and fails at non-rigid transformations. By explicitly learning the editing transformation from a source-target pair, Edit Transfer mitigates the limitations of both text-only and appearance-centric references. Drawing inspiration from in-context learning in large language models, we propose a visual relation in-context learning paradigm, building upon a DiT-based text-to-image model. We arrange the edited example and the query image into a unified four-panel composite, then apply lightweight LoRA fine-tuning to capture complex spatial transformations from minimal examples. Despite using only 42 training samples, Edit Transfer substantially outperforms state-of-the-art TIE and RIE methods on diverse non-rigid scenarios, demonstrating the effectiveness of few-shot visual relation learning.', 'score': 21, 'issue_id': 2754, 'pub_date': '2025-03-17', 'pub_date_card': {'ru': '17 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 17', 'zh': '3æœˆ17æ—¥'}, 'hash': '24a5e5d1d86949d5', 'authors': ['Lan Chen', 'Qi Mao', 'Yuchao Gu', 'Mike Zheng Shou'], 'affiliations': ['MIPG, Communication University of China', 'Show Lab, National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2503.13327.jpg', 'data': {'categories': ['#transfer_learning', '#dataset', '#cv', '#training'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'ĞŸĞµÑ€ĞµĞ´Ğ°Ñ‡Ğ° Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ: Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑĞ¼ Ğ¿Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ñƒ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñƒ', 'desc': "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ² Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ 'Edit Transfer'. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸Ğ·ÑƒÑ‡Ğ°Ñ‚ÑŒ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²ÑĞµĞ³Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ° Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ‚ÑŒ ĞµĞ³Ğ¾ Ğº Ğ½Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑƒ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸ÑĞ¼ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ DiT. ĞĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²ÑĞµĞ³Ğ¾ 42 Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ², Edit Transfer Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ TIE Ğ¸ RIE Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ğ½ĞµĞ¶ĞµÑÑ‚ĞºĞ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."}, 'en': {'title': 'Transforming Images with Just One Example!', 'desc': 'This paper presents a novel approach called Edit Transfer, which enables a model to learn how to transform images using just one example of a source and target image. Unlike traditional text-based methods that struggle with geometric details and reference-based methods that focus on style, Edit Transfer effectively captures complex spatial transformations. The method utilizes a visual relation in-context learning paradigm, inspired by large language models, and employs a DiT-based text-to-image model. Remarkably, it achieves superior performance in non-rigid scenarios with only 42 training samples, showcasing the power of few-shot learning in visual transformations.'}, 'zh': {'title': 'ç¼–è¾‘è½¬ç§»ï¼šå°‘æ ·æœ¬å­¦ä¹ çš„çªç ´', 'desc': 'æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„è®¾ç½®ï¼Œç§°ä¸ºç¼–è¾‘è½¬ç§»ï¼ˆEdit Transferï¼‰ï¼Œæ¨¡å‹é€šè¿‡å•ä¸€çš„æº-ç›®æ ‡ç¤ºä¾‹å­¦ä¹ å˜æ¢ï¼Œå¹¶å°†å…¶åº”ç”¨äºæ–°çš„æŸ¥è¯¢å›¾åƒã€‚ä¸æ–‡æœ¬æ–¹æ³•åœ¨è¯­ä¹‰æ“ä½œä¸Šè¡¨ç°ä¼˜å¼‚ä½†åœ¨å‡ ä½•ç»†èŠ‚ä¸Šå­˜åœ¨å›°éš¾ä¸åŒï¼Œç¼–è¾‘è½¬ç§»é€šè¿‡æ˜ç¡®å­¦ä¹ æº-ç›®æ ‡å¯¹çš„ç¼–è¾‘å˜æ¢ï¼Œå…‹æœäº†æ–‡æœ¬å’Œå¤–è§‚å‚è€ƒçš„å±€é™æ€§ã€‚æˆ‘ä»¬å€Ÿé‰´å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„ä¸Šä¸‹æ–‡å­¦ä¹ ï¼Œæå‡ºäº†ä¸€ç§è§†è§‰å…³ç³»ä¸Šä¸‹æ–‡å­¦ä¹ èŒƒå¼ï¼Œå¹¶åœ¨DiTåŸºç¡€çš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ä¸Šè¿›è¡Œæ„å»ºã€‚å°½ç®¡ä»…ä½¿ç”¨42ä¸ªè®­ç»ƒæ ·æœ¬ï¼Œç¼–è¾‘è½¬ç§»åœ¨å¤šæ ·çš„éåˆšæ€§åœºæ™¯ä¸­æ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œå±•ç¤ºäº†å°‘æ ·æœ¬è§†è§‰å…³ç³»å­¦ä¹ çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.13434', 'title': 'BlobCtrl: A Unified and Flexible Framework for Element-level Image\n  Generation and Editing', 'url': 'https://huggingface.co/papers/2503.13434', 'abstract': 'Element-level visual manipulation is essential in digital content creation, but current diffusion-based methods lack the precision and flexibility of traditional tools. In this work, we introduce BlobCtrl, a framework that unifies element-level generation and editing using a probabilistic blob-based representation. By employing blobs as visual primitives, our approach effectively decouples and represents spatial location, semantic content, and identity information, enabling precise element-level manipulation. Our key contributions include: 1) a dual-branch diffusion architecture with hierarchical feature fusion for seamless foreground-background integration; 2) a self-supervised training paradigm with tailored data augmentation and score functions; and 3) controllable dropout strategies to balance fidelity and diversity. To support further research, we introduce BlobData for large-scale training and BlobBench for systematic evaluation. Experiments show that BlobCtrl excels in various element-level manipulation tasks while maintaining computational efficiency, offering a practical solution for precise and flexible visual content creation. Project page: https://liyaowei-stu.github.io/project/BlobCtrl/', 'score': 19, 'issue_id': 2758, 'pub_date': '2025-03-17', 'pub_date_card': {'ru': '17 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 17', 'zh': '3æœˆ17æ—¥'}, 'hash': 'c8ae2d6baee8bf12', 'authors': ['Yaowei Li', 'Lingen Li', 'Zhaoyang Zhang', 'Xiaoyu Li', 'Guangzhi Wang', 'Hongxiang Li', 'Xiaodong Cun', 'Ying Shan', 'Yuexian Zou'], 'affiliations': ['ARC Lab, Tencent PCG', 'Peking University', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2503.13434.jpg', 'data': {'categories': ['#benchmark', '#diffusion', '#cv', '#open_source', '#dataset', '#training'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ¢Ğ¾Ñ‡Ğ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ±Ğ»Ğ¾Ğ±Ğ¾Ğ²', 'desc': "BlobCtrl - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ñ†Ğ¸Ñ„Ñ€Ğ¾Ğ²Ğ¾Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğµ. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ 'Ğ±Ğ»Ğ¾Ğ±Ğ¾Ğ²', Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ñ‚Ğ´ĞµĞ»Ğ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ, ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ± Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´Ğ²ÑƒÑ…Ğ²ĞµÑ‚Ğ²ĞµĞ²ÑƒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ ÑĞ»Ğ¸ÑĞ½Ğ¸ĞµĞ¼ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¸ ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ. BlobCtrl Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ."}, 'en': {'title': 'Precision and Flexibility in Visual Manipulation with BlobCtrl', 'desc': 'BlobCtrl is a new framework designed for element-level visual manipulation in digital content creation, addressing the limitations of current diffusion-based methods. It uses a probabilistic blob-based representation to separate and manage spatial location, semantic content, and identity, allowing for precise editing of visual elements. The framework features a dual-branch diffusion architecture that integrates foreground and background seamlessly, along with a self-supervised training approach that enhances model performance through tailored data augmentation. Additionally, BlobCtrl introduces BlobData for extensive training and BlobBench for evaluation, demonstrating superior efficiency and effectiveness in various manipulation tasks.'}, 'zh': {'title': 'BlobCtrlï¼šç²¾ç¡®çµæ´»çš„è§†è§‰å†…å®¹åˆ›ä½œæ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºBlobCtrlçš„æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜æ•°å­—å†…å®¹åˆ›ä½œä¸­å…ƒç´ çº§è§†è§‰æ“ä½œçš„ç²¾ç¡®æ€§å’Œçµæ´»æ€§ã€‚è¯¥æ¡†æ¶é‡‡ç”¨åŸºäºæ¦‚ç‡çš„blobè¡¨ç¤ºï¼Œèƒ½å¤Ÿæœ‰æ•ˆè§£è€¦ç©ºé—´ä½ç½®ã€è¯­ä¹‰å†…å®¹å’Œèº«ä»½ä¿¡æ¯ï¼Œä»è€Œå®ç°ç²¾ç¡®çš„å…ƒç´ çº§æ“ä½œã€‚æˆ‘ä»¬æå‡ºäº†åŒåˆ†æ”¯æ‰©æ•£æ¶æ„å’Œè‡ªç›‘ç£è®­ç»ƒèŒƒå¼ï¼Œä»¥å¢å¼ºå‰æ™¯å’ŒèƒŒæ™¯çš„æ— ç¼é›†æˆï¼Œå¹¶å¼•å…¥å¯æ§çš„dropoutç­–ç•¥æ¥å¹³è¡¡ä¿çœŸåº¦å’Œå¤šæ ·æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒBlobCtrlåœ¨å¤šç§å…ƒç´ çº§æ“ä½œä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼ŒåŒæ—¶ä¿æŒè®¡ç®—æ•ˆç‡ï¼Œä¸ºç²¾ç¡®å’Œçµæ´»çš„è§†è§‰å†…å®¹åˆ›ä½œæä¾›äº†å®ç”¨è§£å†³æ–¹æ¡ˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.13435', 'title': 'WideRange4D: Enabling High-Quality 4D Reconstruction with Wide-Range\n  Movements and Scenes', 'url': 'https://huggingface.co/papers/2503.13435', 'abstract': 'With the rapid development of 3D reconstruction technology, research in 4D reconstruction is also advancing, existing 4D reconstruction methods can generate high-quality 4D scenes. However, due to the challenges in acquiring multi-view video data, the current 4D reconstruction benchmarks mainly display actions performed in place, such as dancing, within limited scenarios. In practical scenarios, many scenes involve wide-range spatial movements, highlighting the limitations of existing 4D reconstruction datasets. Additionally, existing 4D reconstruction methods rely on deformation fields to estimate the dynamics of 3D objects, but deformation fields struggle with wide-range spatial movements, which limits the ability to achieve high-quality 4D scene reconstruction with wide-range spatial movements. In this paper, we focus on 4D scene reconstruction with significant object spatial movements and propose a novel 4D reconstruction benchmark, WideRange4D. This benchmark includes rich 4D scene data with large spatial variations, allowing for a more comprehensive evaluation of the generation capabilities of 4D generation methods. Furthermore, we introduce a new 4D reconstruction method, Progress4D, which generates stable and high-quality 4D results across various complex 4D scene reconstruction tasks. We conduct both quantitative and qualitative comparison experiments on WideRange4D, showing that our Progress4D outperforms existing state-of-the-art 4D reconstruction methods. Project: https://github.com/Gen-Verse/WideRange4D', 'score': 16, 'issue_id': 2754, 'pub_date': '2025-03-17', 'pub_date_card': {'ru': '17 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 17', 'zh': '3æœˆ17æ—¥'}, 'hash': 'c17d4be710ed24e0', 'authors': ['Ling Yang', 'Kaixin Zhu', 'Juanxi Tian', 'Bohan Zeng', 'Mingbao Lin', 'Hongjuan Pei', 'Wentao Zhang', 'Shuicheng Yan'], 'affiliations': ['National University of Singapore', 'Peking University', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2503.13435.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#3d'], 'emoji': 'ğŸŒ€', 'ru': {'title': 'ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² 4D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸: Ğ¾Ñ‚ ÑÑ‚Ğ°Ñ‚Ğ¸ĞºĞ¸ Ğº Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞµ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº WideRange4D Ğ´Ğ»Ñ 4D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ ÑÑ†ĞµĞ½ Ñ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¸Ğ¼ Ğ´Ğ¸Ğ°Ğ¿Ğ°Ğ·Ğ¾Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ 4D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Progress4D, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ 4D-Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ 4D-ÑÑ†ĞµĞ½. ĞœĞµÑ‚Ğ¾Ğ´ Progress4D Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ 4D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ² ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ñ… Ğ½Ğ° WideRange4D. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ° Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² 4D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ğ»Ğ¾Ñ…Ğ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸.'}, 'en': {'title': 'Advancing 4D Reconstruction with Wide Spatial Movements', 'desc': 'This paper addresses the limitations of current 4D reconstruction methods, which primarily focus on actions performed in place and struggle with wide-range spatial movements. The authors introduce a new benchmark called WideRange4D, which includes diverse 4D scene data featuring significant spatial variations, enabling better evaluation of 4D generation techniques. They also propose a novel reconstruction method named Progress4D, designed to produce stable and high-quality 4D results across complex scenarios. Experimental results demonstrate that Progress4D surpasses existing state-of-the-art methods in both quantitative and qualitative assessments.'}, 'zh': {'title': 'çªç ´ç©ºé—´é™åˆ¶ï¼Œå®ç°é«˜è´¨é‡4Dé‡å»º', 'desc': 'éšç€3Dé‡å»ºæŠ€æœ¯çš„å¿«é€Ÿå‘å±•ï¼Œ4Dé‡å»ºç ”ç©¶ä¹Ÿåœ¨ä¸æ–­è¿›æ­¥ã€‚ç°æœ‰çš„4Dé‡å»ºæ–¹æ³•èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„4Dåœºæ™¯ï¼Œä½†åœ¨è·å–å¤šè§†è§’è§†é¢‘æ•°æ®æ–¹é¢é¢ä¸´æŒ‘æˆ˜ï¼Œå¯¼è‡´ç°æœ‰åŸºå‡†ä¸»è¦å±•ç¤ºæœ‰é™åœºæ™¯ä¸­çš„åŠ¨ä½œã€‚æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ–°çš„4Dé‡å»ºåŸºå‡†WideRange4Dï¼ŒåŒ…å«ä¸°å¯Œçš„4Dåœºæ™¯æ•°æ®ï¼Œå…è®¸å¯¹4Dç”Ÿæˆæ–¹æ³•çš„èƒ½åŠ›è¿›è¡Œæ›´å…¨é¢çš„è¯„ä¼°ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„4Dé‡å»ºæ–¹æ³•Progress4Dï¼Œåœ¨å„ç§å¤æ‚çš„4Dåœºæ™¯é‡å»ºä»»åŠ¡ä¸­ç”Ÿæˆç¨³å®šä¸”é«˜è´¨é‡çš„ç»“æœã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.13399', 'title': 'MicroVQA: A Multimodal Reasoning Benchmark for Microscopy-Based\n  Scientific Research', 'url': 'https://huggingface.co/papers/2503.13399', 'abstract': "Scientific research demands sophisticated reasoning over multimodal data, a challenge especially prevalent in biology. Despite recent advances in multimodal large language models (MLLMs) for AI-assisted research, existing multimodal reasoning benchmarks only target up to college-level difficulty, while research-level benchmarks emphasize lower-level perception, falling short of the complex multimodal reasoning needed for scientific discovery. To bridge this gap, we introduce MicroVQA, a visual-question answering (VQA) benchmark designed to assess three reasoning capabilities vital in research workflows: expert image understanding, hypothesis generation, and experiment proposal. MicroVQA consists of 1,042 multiple-choice questions (MCQs) curated by biology experts across diverse microscopy modalities, ensuring VQA samples represent real scientific practice. In constructing the benchmark, we find that standard MCQ generation methods induce language shortcuts, motivating a new two-stage pipeline: an optimized LLM prompt structures question-answer pairs into MCQs; then, an agent-based `RefineBot' updates them to remove shortcuts. Benchmarking on state-of-the-art MLLMs reveal a peak performance of 53\\%; models with smaller LLMs only slightly underperform top models, suggesting that language-based reasoning is less challenging than multimodal reasoning; and tuning with scientific articles enhances performance. Expert analysis of chain-of-thought responses shows that perception errors are the most frequent, followed by knowledge errors and then overgeneralization errors. These insights highlight the challenges in multimodal scientific reasoning, showing MicroVQA is a valuable resource advancing AI-driven biomedical research. MicroVQA is available at https://huggingface.co/datasets/jmhb/microvqa, and project page at https://jmhb0.github.io/microvqa.", 'score': 16, 'issue_id': 2754, 'pub_date': '2025-03-17', 'pub_date_card': {'ru': '17 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 17', 'zh': '3æœˆ17æ—¥'}, 'hash': '50d4f1f510eff333', 'authors': ['James Burgess', 'Jeffrey J Nirschl', 'Laura Bravo-SÃ¡nchez', 'Alejandro Lozano', 'Sanket Rajan Gupte', 'Jesus G. Galaz-Montoya', 'Yuhui Zhang', 'Yuchang Su', 'Disha Bhowmik', 'Zachary Coman', 'Sarina M. Hasan', 'Alexandra Johannesson', 'William D. Leineweber', 'Malvika G Nair', 'Ridhi Yarlagadda', 'Connor Zuraski', 'Wah Chiu', 'Sarah Cohen', 'Jan N. Hansen', 'Manuel D Leonetti', 'Chad Liu', 'Emma Lundberg', 'Serena Yeung-Levy'], 'affiliations': ['Chan Zuckerberg Biohub Network', 'KTH Royal Institute of Technology', 'Princeton University', 'Stanford University', 'Tsinghua University', 'University of North Carolina at Chapel Hill'], 'pdf_title_img': 'assets/pdf/title_img/2503.13399.jpg', 'data': {'categories': ['#dataset', '#reasoning', '#multimodal', '#benchmark', '#science'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'MicroVQA: ĞĞ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ˜Ğ˜ Ğ´Ğ»Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MicroVQA - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· 1042 Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ğ¼, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¸ĞºÑ€Ğ¾ÑĞºĞ¾Ğ¿Ğ¸Ğ¸ Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ³Ğ¸Ğ¿Ğ¾Ñ‚ĞµĞ· Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ², Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… ÑƒĞ¿Ñ€Ğ¾Ñ‰ĞµĞ½Ğ¸Ğ¹. Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½ÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ 53%, Ğ²Ñ‹ÑĞ²Ğ¸Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ… Ğ´Ğ»Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡.'}, 'en': {'title': 'MicroVQA: Advancing Multimodal Reasoning in Scientific Discovery', 'desc': 'This paper introduces MicroVQA, a new benchmark for visual-question answering (VQA) that focuses on complex reasoning needed in scientific research, particularly in biology. It assesses three key capabilities: understanding images, generating hypotheses, and proposing experiments, using questions curated by biology experts. The benchmark consists of 1,042 multiple-choice questions designed to reflect real scientific practices, addressing the limitations of existing multimodal reasoning benchmarks. The study reveals that while current models perform at a peak of 53%, the challenges in multimodal reasoning are significant, emphasizing the need for improved AI tools in biomedical research.'}, 'zh': {'title': 'MicroVQAï¼šæ¨åŠ¨ç”Ÿç‰©åŒ»å­¦ç ”ç©¶çš„å¤šæ¨¡æ€æ¨ç†åŸºå‡†', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†MicroVQAï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹ç”Ÿç‰©å­¦ç ”ç©¶çš„è§†è§‰é—®ç­”åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°ç§‘å­¦ç ”ç©¶ä¸­æ‰€éœ€çš„ä¸‰ç§æ¨ç†èƒ½åŠ›ï¼šä¸“å®¶å›¾åƒç†è§£ã€å‡è®¾ç”Ÿæˆå’Œå®éªŒææ¡ˆã€‚ç°æœ‰çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤„ç†å¤æ‚çš„å¤šæ¨¡æ€æ¨ç†æ—¶å­˜åœ¨ä¸è¶³ï¼ŒMicroVQAé€šè¿‡1,042ä¸ªç”±ç”Ÿç‰©å­¦ä¸“å®¶ç­–åˆ’çš„å¤šé¡¹é€‰æ‹©é¢˜æ¥å¡«è¡¥è¿™ä¸€ç©ºç™½ã€‚ç ”ç©¶å‘ç°ï¼Œæ ‡å‡†çš„å¤šé¡¹é€‰æ‹©é¢˜ç”Ÿæˆæ–¹æ³•å®¹æ˜“äº§ç”Ÿè¯­è¨€æ·å¾„ï¼Œå› æ­¤æå‡ºäº†ä¸€ç§æ–°çš„ä¸¤é˜¶æ®µæµç¨‹æ¥ä¼˜åŒ–é—®é¢˜å’Œç­”æ¡ˆçš„ç»“æ„ã€‚é€šè¿‡å¯¹æœ€å…ˆè¿›çš„MLLMsè¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œç»“æœæ˜¾ç¤ºï¼Œå°½ç®¡å°å‹LLMsçš„è¡¨ç°ç•¥é€Šäºé¡¶çº§æ¨¡å‹ï¼Œä½†è¯­è¨€æ¨ç†çš„éš¾åº¦ä½äºå¤šæ¨¡æ€æ¨ç†ï¼Œè¿™çªæ˜¾äº†åœ¨ç§‘å­¦æ¨ç†ä¸­çš„æŒ‘æˆ˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.12605', 'title': 'Multimodal Chain-of-Thought Reasoning: A Comprehensive Survey', 'url': 'https://huggingface.co/papers/2503.12605', 'abstract': 'By extending the advantage of chain-of-thought (CoT) reasoning in human-like step-by-step processes to multimodal contexts, multimodal CoT (MCoT) reasoning has recently garnered significant research attention, especially in the integration with multimodal large language models (MLLMs). Existing MCoT studies design various methodologies and innovative reasoning paradigms to address the unique challenges of image, video, speech, audio, 3D, and structured data across different modalities, achieving extensive success in applications such as robotics, healthcare, autonomous driving, and multimodal generation. However, MCoT still presents distinct challenges and opportunities that require further focus to ensure consistent thriving in this field, where, unfortunately, an up-to-date review of this domain is lacking. To bridge this gap, we present the first systematic survey of MCoT reasoning, elucidating the relevant foundational concepts and definitions. We offer a comprehensive taxonomy and an in-depth analysis of current methodologies from diverse perspectives across various application scenarios. Furthermore, we provide insights into existing challenges and future research directions, aiming to foster innovation toward multimodal AGI.', 'score': 14, 'issue_id': 2762, 'pub_date': '2025-03-16', 'pub_date_card': {'ru': '16 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 16', 'zh': '3æœˆ16æ—¥'}, 'hash': '4b0cc0276cb6afed', 'authors': ['Yaoting Wang', 'Shengqiong Wu', 'Yuecheng Zhang', 'William Wang', 'Ziwei Liu', 'Jiebo Luo', 'Hao Fei'], 'affiliations': ['CUHK', 'NTU', 'NUS', 'UCSB', 'UR'], 'pdf_title_img': 'assets/pdf/title_img/2503.12605.jpg', 'data': {'categories': ['#multimodal', '#robotics', '#healthcare', '#3d', '#video', '#agi', '#reasoning', '#survey', '#audio'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ: ÑˆĞ°Ğ³ Ğº AGI', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞµ Ğ¼Ñ‹ÑĞ»ĞµĞ¹ (MCoT) Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¾Ğ±Ğ·Ğ¾Ñ€ MCoT, Ğ¾Ğ±ÑŠÑÑĞ½ÑÑ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ. ĞĞ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ²ÑĞµÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½ÑÑ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ Ğ¸ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¹ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ. Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ñ‚Ğ°ĞºĞ¶Ğµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¸ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Unlocking Multimodal Reasoning for Future AI Innovations', 'desc': 'This paper introduces multimodal chain-of-thought (MCoT) reasoning, which enhances human-like reasoning processes across different types of data such as images, videos, and audio. It reviews various methodologies and innovative reasoning paradigms that have been developed to tackle the unique challenges posed by these multimodal contexts. The authors present a systematic survey that includes foundational concepts, a comprehensive taxonomy, and an analysis of current approaches in MCoT applications. Additionally, the paper highlights existing challenges and suggests future research directions to advance the field towards multimodal artificial general intelligence (AGI).'}, 'zh': {'title': 'å¤šæ¨¡æ€æ¨ç†çš„æœªæ¥ä¹‹è·¯', 'desc': 'å¤šæ¨¡æ€é“¾å¼æ€ç»´ï¼ˆMCoTï¼‰æ¨ç†å°†äººç±»çš„é€æ­¥æ¨ç†ä¼˜åŠ¿æ‰©å±•åˆ°å¤šç§æ•°æ®ç±»å‹ï¼Œè¿‘å¹´æ¥å—åˆ°å¹¿æ³›å…³æ³¨ï¼Œå°¤å…¶æ˜¯åœ¨ä¸å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„ç»“åˆæ–¹é¢ã€‚ç°æœ‰çš„MCoTç ”ç©¶è®¾è®¡äº†å¤šç§æ–¹æ³•å’Œåˆ›æ–°çš„æ¨ç†èŒƒå¼ï¼Œä»¥åº”å¯¹å›¾åƒã€è§†é¢‘ã€è¯­éŸ³ã€éŸ³é¢‘ã€3Då’Œç»“æ„åŒ–æ•°æ®ç­‰ä¸åŒæ¨¡æ€çš„ç‹¬ç‰¹æŒ‘æˆ˜ï¼Œå¹¶åœ¨æœºå™¨äººæŠ€æœ¯ã€åŒ»ç–—ä¿å¥ã€è‡ªåŠ¨é©¾é©¶å’Œå¤šæ¨¡æ€ç”Ÿæˆç­‰åº”ç”¨ä¸­å–å¾—äº†æ˜¾è‘—æˆåŠŸã€‚å°½ç®¡å¦‚æ­¤ï¼ŒMCoTä»é¢ä¸´ç‹¬ç‰¹çš„æŒ‘æˆ˜å’Œæœºé‡ï¼Œéœ€è¦è¿›ä¸€æ­¥å…³æ³¨ï¼Œä»¥ç¡®ä¿è¯¥é¢†åŸŸçš„æŒç»­å‘å±•ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æä¾›äº†MCoTæ¨ç†çš„é¦–æ¬¡ç³»ç»Ÿæ€§ç»¼è¿°ï¼Œé˜æ˜ç›¸å…³çš„åŸºç¡€æ¦‚å¿µå’Œå®šä¹‰ï¼Œå¹¶å¯¹å½“å‰æ–¹æ³•è¿›è¡Œäº†å…¨é¢çš„åˆ†ç±»å’Œæ·±å…¥åˆ†æã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.11751', 'title': 'reWordBench: Benchmarking and Improving the Robustness of Reward Models\n  with Transformed Inputs', 'url': 'https://huggingface.co/papers/2503.11751', 'abstract': 'Reward models have become a staple in modern NLP, serving as not only a scalable text evaluator, but also an indispensable component in many alignment recipes and inference-time algorithms. However, while recent reward models increase performance on standard benchmarks, this may partly be due to overfitting effects, which would confound an understanding of their true capability. In this work, we scrutinize the robustness of reward models and the extent of such overfitting. We build **reWordBench**, which systematically transforms reward model inputs in meaning- or ranking-preserving ways. We show that state-of-the-art reward models suffer from substantial performance degradation even with minor input transformations, sometimes dropping to significantly below-random accuracy, suggesting brittleness. To improve reward model robustness, we propose to explicitly train them to assign similar scores to paraphrases, and find that this approach also improves robustness to other distinct kinds of transformations. For example, our robust reward model reduces such degradation by roughly half for the Chat Hard subset in RewardBench. Furthermore, when used in alignment, our robust reward models demonstrate better utility and lead to higher-quality outputs, winning in up to 59% of instances against a standardly trained RM.', 'score': 14, 'issue_id': 2755, 'pub_date': '2025-03-14', 'pub_date_card': {'ru': '14 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 14', 'zh': '3æœˆ14æ—¥'}, 'hash': '0adbcb6b7ba858f8', 'authors': ['Zhaofeng Wu', 'Michihiro Yasunaga', 'Andrew Cohen', 'Yoon Kim', 'Asli Celikyilmaz', 'Marjan Ghazvininejad'], 'affiliations': ['FAIR at Meta', 'MIT'], 'pdf_title_img': 'assets/pdf/title_img/2503.11751.jpg', 'data': {'categories': ['#benchmark', '#hallucinations', '#training', '#alignment', '#rlhf'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'ĞŸĞ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² NLP', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ (reward models) Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… reWordBench Ğ´Ğ»Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑÑ‚Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿ÑƒÑ‚ĞµĞ¼ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ñ‚ĞµÑ€ÑÑÑ‚ Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ°Ğ¶Ğµ Ğ¿Ñ€Ğ¸ Ğ½ĞµĞ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸ÑÑ… Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ”Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸ÑĞ²Ğ°Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑÑ…Ğ¾Ğ¶Ğ¸Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ°Ñ€Ğ°Ñ„Ñ€Ğ°Ğ·Ğ°Ğ¼, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¶Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğº Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼ Ğ²Ğ¸Ğ´Ğ°Ğ¼ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¹.'}, 'en': {'title': 'Enhancing Robustness in Reward Models for NLP', 'desc': 'This paper investigates the reliability of reward models in natural language processing (NLP), which are crucial for evaluating text and enhancing model alignment. The authors introduce **reWordBench**, a tool that modifies inputs to test the robustness of these models against overfitting. Their findings reveal that many state-of-the-art reward models perform poorly when faced with slight input changes, indicating a lack of robustness. To address this issue, they propose training reward models to provide consistent scores for paraphrased inputs, resulting in improved performance and higher-quality outputs during alignment tasks.'}, 'zh': {'title': 'æå‡å¥–åŠ±æ¨¡å‹é²æ£’æ€§ï¼Œå‡å°‘è¿‡æ‹Ÿåˆå½±å“', 'desc': 'å¥–åŠ±æ¨¡å‹åœ¨ç°ä»£è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä¸­æ‰®æ¼”ç€é‡è¦è§’è‰²ï¼Œæ—¢æ˜¯å¯æ‰©å±•çš„æ–‡æœ¬è¯„ä¼°å·¥å…·ï¼Œä¹Ÿæ˜¯è®¸å¤šå¯¹é½ç®—æ³•å’Œæ¨ç†æ—¶ç®—æ³•çš„å…³é”®ç»„æˆéƒ¨åˆ†ã€‚å°½ç®¡æœ€è¿‘çš„å¥–åŠ±æ¨¡å‹åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†è¿™å¯èƒ½éƒ¨åˆ†æ˜¯ç”±äºè¿‡æ‹Ÿåˆç°è±¡ï¼Œå½±å“äº†å¯¹å…¶çœŸå®èƒ½åŠ›çš„ç†è§£ã€‚æˆ‘ä»¬æ„å»ºäº†reWordBenchï¼Œç³»ç»Ÿåœ°å¯¹å¥–åŠ±æ¨¡å‹è¾“å…¥è¿›è¡Œæ„ä¹‰æˆ–æ’åä¿æŒçš„è½¬æ¢ï¼Œå‘ç°å³ä½¿æ˜¯å¾®å°çš„è¾“å…¥å˜åŒ–ï¼Œæœ€å…ˆè¿›çš„å¥–åŠ±æ¨¡å‹ä¹Ÿä¼šå‡ºç°æ˜¾è‘—çš„æ€§èƒ½ä¸‹é™ï¼Œæ˜¾ç¤ºå‡ºå…¶è„†å¼±æ€§ã€‚ä¸ºæé«˜å¥–åŠ±æ¨¡å‹çš„é²æ£’æ€§ï¼Œæˆ‘ä»¬æå‡ºæ˜¾å¼è®­ç»ƒæ¨¡å‹å¯¹åŒä¹‰å¥èµ‹äºˆç›¸ä¼¼åˆ†æ•°ï¼Œè¿™ç§æ–¹æ³•ä¹Ÿæ”¹å–„äº†æ¨¡å‹å¯¹å…¶ä»–ä¸åŒç±»å‹è½¬æ¢çš„é²æ£’æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.12937', 'title': 'R1-VL: Learning to Reason with Multimodal Large Language Models via\n  Step-wise Group Relative Policy Optimization', 'url': 'https://huggingface.co/papers/2503.12937', 'abstract': "Recent studies generally enhance MLLMs' reasoning capabilities via supervised fine-tuning on high-quality chain-of-thought reasoning data, which often leads models to merely imitate successful reasoning paths without understanding what the wrong reasoning paths are. In this work, we aim to enhance the MLLMs' reasoning ability beyond passively imitating positive reasoning paths. To this end, we design Step-wise Group Relative Policy Optimization (StepGRPO), a new online reinforcement learning framework that enables MLLMs to self-improve reasoning ability via simple, effective and dense step-wise rewarding. Specifically, StepGRPO introduces two novel rule-based reasoning rewards: Step-wise Reasoning Accuracy Reward (StepRAR) and Step-wise Reasoning Validity Reward (StepRVR). StepRAR rewards the reasoning paths that contain necessary intermediate reasoning steps via a soft key-step matching technique, while StepRAR rewards reasoning paths that follow a well-structured and logically consistent reasoning process through a reasoning completeness and logic evaluation strategy. With the proposed StepGRPO, we introduce R1-VL, a series of MLLMs with outstanding capabilities in step-by-step reasoning. Extensive experiments over 8 benchmarks demonstrate the superiority of our methods.", 'score': 13, 'issue_id': 2755, 'pub_date': '2025-03-17', 'pub_date_card': {'ru': '17 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 17', 'zh': '3æœˆ17æ—¥'}, 'hash': 'dbdf6970963a4489', 'authors': ['Jingyi Zhang', 'Jiaxing Huang', 'Huanjin Yao', 'Shunyu Liu', 'Xikun Zhang', 'Shijian Lu', 'Dacheng Tao'], 'affiliations': ['Nanyang Technological University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.12937.jpg', 'data': {'categories': ['#training', '#reasoning', '#rl'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ¡Ğ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Step-wise Group Relative Policy Optimization (StepGRPO), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑĞ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. StepGRPO Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ğ²Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»: Step-wise Reasoning Accuracy Reward (StepRAR) Ğ¸ Step-wise Reasoning Validity Reward (StepRVR). Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° 8 Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°.'}, 'en': {'title': 'Empowering MLLMs with Step-wise Reasoning Rewards', 'desc': 'This paper presents a new approach to improve the reasoning abilities of machine learning language models (MLLMs) by using reinforcement learning. The proposed method, called Step-wise Group Relative Policy Optimization (StepGRPO), focuses on rewarding MLLMs for their reasoning steps rather than just imitating correct paths. It introduces two innovative rewards: Step-wise Reasoning Accuracy Reward (StepRAR) and Step-wise Reasoning Validity Reward (StepRVR), which encourage models to follow logical reasoning processes. The results show that MLLMs trained with StepGRPO, referred to as R1-VL, perform significantly better in step-by-step reasoning tasks across multiple benchmarks.'}, 'zh': {'title': 'æå‡æ¨ç†èƒ½åŠ›çš„åˆ›æ–°æ–¹æ³•', 'desc': 'æœ€è¿‘çš„ç ”ç©¶é€šå¸¸é€šè¿‡åœ¨é«˜è´¨é‡çš„æ¨ç†æ•°æ®ä¸Šè¿›è¡Œç›‘ç£å¾®è°ƒæ¥å¢å¼ºå¤šè¯­è¨€å¤§æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æ¨ç†èƒ½åŠ›ï¼Œè¿™å¾€å¾€å¯¼è‡´æ¨¡å‹ä»…ä»…æ¨¡ä»¿æˆåŠŸçš„æ¨ç†è·¯å¾„ï¼Œè€Œä¸ç†è§£é”™è¯¯çš„æ¨ç†è·¯å¾„ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ—¨åœ¨è¶…è¶Šè¢«åŠ¨æ¨¡ä»¿ç§¯ææ¨ç†è·¯å¾„ï¼Œæå‡MLLMsçš„æ¨ç†èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æ–°çš„åœ¨çº¿å¼ºåŒ–å­¦ä¹ æ¡†æ¶â€”â€”é€æ­¥ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆStepGRPOï¼‰ï¼Œä½¿MLLMsèƒ½å¤Ÿé€šè¿‡ç®€å•ã€æœ‰æ•ˆå’Œå¯†é›†çš„é€æ­¥å¥–åŠ±è‡ªæˆ‘æå‡æ¨ç†èƒ½åŠ›ã€‚å…·ä½“è€Œè¨€ï¼ŒStepGRPOå¼•å…¥äº†ä¸¤ç§æ–°é¢–çš„åŸºäºè§„åˆ™çš„æ¨ç†å¥–åŠ±ï¼šé€æ­¥æ¨ç†å‡†ç¡®æ€§å¥–åŠ±ï¼ˆStepRARï¼‰å’Œé€æ­¥æ¨ç†æœ‰æ•ˆæ€§å¥–åŠ±ï¼ˆStepRVRï¼‰ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.11495', 'title': 'V-STaR: Benchmarking Video-LLMs on Video Spatio-Temporal Reasoning', 'url': 'https://huggingface.co/papers/2503.11495', 'abstract': 'Human processes video reasoning in a sequential spatio-temporal reasoning logic, we first identify the relevant frames ("when") and then analyse the spatial relationships ("where") between key objects, and finally leverage these relationships to draw inferences ("what"). However, can Video Large Language Models (Video-LLMs) also "reason through a sequential spatio-temporal logic" in videos? Existing Video-LLM benchmarks primarily focus on assessing object presence, neglecting relational reasoning. Consequently, it is difficult to measure whether a model truly comprehends object interactions (actions/events) in videos or merely relies on pre-trained "memory" of co-occurrences as biases in generating answers. In this work, we introduce a Video Spatio-Temporal Reasoning (V-STaR) benchmark to address these shortcomings. The key idea is to decompose video understanding into a Reverse Spatio-Temporal Reasoning (RSTR) task that simultaneously evaluates what objects are present, when events occur, and where they are located while capturing the underlying Chain-of-thought (CoT) logic. To support this evaluation, we construct a dataset to elicit the spatial-temporal reasoning process of Video-LLMs. It contains coarse-to-fine CoT questions generated by a semi-automated GPT-4-powered pipeline, embedding explicit reasoning chains to mimic human cognition. Experiments from 14 Video-LLMs on our V-STaR reveal significant gaps between current Video-LLMs and the needs for robust and consistent spatio-temporal reasoning.', 'score': 10, 'issue_id': 2754, 'pub_date': '2025-03-14', 'pub_date_card': {'ru': '14 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 14', 'zh': '3æœˆ14æ—¥'}, 'hash': '93b4a63d45a11f3d', 'authors': ['Zixu Cheng', 'Jian Hu', 'Ziquan Liu', 'Chenyang Si', 'Wei Li', 'Shaogang Gong'], 'affiliations': ['Nanjing University', 'Nanyang Technological University', 'Queen Mary University of London'], 'pdf_title_img': 'assets/pdf/title_img/2503.11495.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#reasoning', '#video'], 'emoji': 'ğŸ¥', 'ru': {'title': 'ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾-LLM', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº V-STaR Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾-LLM Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ (RSTR), Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‰ÑƒÑ Ğ¿Ñ€Ğ¸ÑÑƒÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ğ²Ñ€ĞµĞ¼Ñ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹ Ğ¸ Ğ¸Ñ… Ñ€Ğ°ÑĞ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ. Ğ”Ğ»Ñ ÑÑ‚Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ñ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸, ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ GPT-4, Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºÑƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° 14 Ğ²Ğ¸Ğ´ĞµĞ¾-LLM Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ñ‹ Ğ² Ğ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ¼Ñƒ Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Enhancing Video Understanding with Spatio-Temporal Reasoning', 'desc': "This paper explores how Video Large Language Models (Video-LLMs) can understand videos using a method similar to human reasoning, which involves identifying when events happen, where objects are located, and how they interact. The authors highlight that current benchmarks for Video-LLMs mainly check for object presence but fail to assess the models' ability to reason about relationships and interactions between objects. To address this, they introduce the Video Spatio-Temporal Reasoning (V-STaR) benchmark, which breaks down video understanding into a task that evaluates object presence, timing of events, and spatial relationships while capturing the reasoning process. Their findings show that there are significant gaps in the reasoning capabilities of existing Video-LLMs, indicating a need for improved models that can perform robust spatio-temporal reasoning."}, 'zh': {'title': 'æå‡è§†é¢‘ç†è§£çš„æ—¶ç©ºæ¨ç†èƒ½åŠ›', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†è§†é¢‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆVideo-LLMsï¼‰åœ¨è§†é¢‘ç†è§£ä¸­çš„æ—¶ç©ºæ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„åŸºå‡†ï¼Œç§°ä¸ºè§†é¢‘æ—¶ç©ºæ¨ç†ï¼ˆV-STaRï¼‰ï¼Œæ—¨åœ¨è¯„ä¼°æ¨¡å‹åœ¨è¯†åˆ«å¯¹è±¡ã€äº‹ä»¶å‘ç”Ÿæ—¶é—´å’Œç©ºé—´ä½ç½®æ–¹é¢çš„èƒ½åŠ›ã€‚é€šè¿‡æ„å»ºä¸€ä¸ªåŒ…å«ç»†è‡´æ¨ç†é“¾çš„é—®é¢˜æ•°æ®é›†ï¼Œæˆ‘ä»¬æ¨¡æ‹Ÿäººç±»çš„è®¤çŸ¥è¿‡ç¨‹ï¼Œä»¥ä¾¿æ›´å¥½åœ°è¯„ä¼°æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œç°æœ‰çš„Video-LLMsåœ¨æ—¶ç©ºæ¨ç†æ–¹é¢å­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œæ— æ³•æ»¡è¶³å®é™…åº”ç”¨çš„éœ€æ±‚ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.13082', 'title': 'Free-form language-based robotic reasoning and grasping', 'url': 'https://huggingface.co/papers/2503.13082', 'abstract': "Performing robotic grasping from a cluttered bin based on human instructions is a challenging task, as it requires understanding both the nuances of free-form language and the spatial relationships between objects. Vision-Language Models (VLMs) trained on web-scale data, such as GPT-4o, have demonstrated remarkable reasoning capabilities across both text and images. But can they truly be used for this task in a zero-shot setting? And what are their limitations? In this paper, we explore these research questions via the free-form language-based robotic grasping task, and propose a novel method, FreeGrasp, leveraging the pre-trained VLMs' world knowledge to reason about human instructions and object spatial arrangements. Our method detects all objects as keypoints and uses these keypoints to annotate marks on images, aiming to facilitate GPT-4o's zero-shot spatial reasoning. This allows our method to determine whether a requested object is directly graspable or if other objects must be grasped and removed first. Since no existing dataset is specifically designed for this task, we introduce a synthetic dataset FreeGraspData by extending the MetaGraspNetV2 dataset with human-annotated instructions and ground-truth grasping sequences. We conduct extensive analyses with both FreeGraspData and real-world validation with a gripper-equipped robotic arm, demonstrating state-of-the-art performance in grasp reasoning and execution. Project website: https://tev-fbk.github.io/FreeGrasp/.", 'score': 9, 'issue_id': 2762, 'pub_date': '2025-03-17', 'pub_date_card': {'ru': '17 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 17', 'zh': '3æœˆ17æ—¥'}, 'hash': 'c29bd4c3e364d62c', 'authors': ['Runyu Jiao', 'Alice Fasoli', 'Francesco Giuliari', 'Matteo Bortolon', 'Sergio Povoli', 'Guofeng Mei', 'Yiming Wang', 'Fabio Poiesi'], 'affiliations': ['Fondazione Bruno Kessler', 'Istituto Italiano di Tecnologia', 'University of Trento'], 'pdf_title_img': 'assets/pdf/title_img/2503.13082.jpg', 'data': {'categories': ['#dataset', '#robotics', '#agents', '#reasoning', '#cv', '#synthetic'], 'emoji': 'ğŸ¦¾', 'ru': {'title': 'Ğ Ğ¾Ğ±Ğ¾Ñ‚Ñ‹ ÑƒÑ‡Ğ°Ñ‚ÑÑ Ñ…Ğ²Ğ°Ñ‚Ğ°Ñ‚ÑŒ Ğ¿Ğ¾-Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ FreeGrasp Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (VLM) Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸. FreeGrasp Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹ ĞºĞ°Ğº ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ñ… Ğ´Ğ»Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ FreeGraspData Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°.'}, 'en': {'title': 'Empowering Robots with Language: Grasping Made Easy!', 'desc': 'This paper investigates the use of Vision-Language Models (VLMs) for robotic grasping tasks based on human instructions, particularly in cluttered environments. The authors introduce a method called FreeGrasp, which utilizes pre-trained VLMs to enhance spatial reasoning by detecting objects as keypoints and annotating them on images. They also create a new synthetic dataset, FreeGraspData, to support their research, as no existing dataset fits their needs. The results show that FreeGrasp achieves state-of-the-art performance in understanding and executing grasping tasks, demonstrating the potential of VLMs in robotics.'}, 'zh': {'title': 'åŸºäºäººç±»æŒ‡ä»¤çš„æ™ºèƒ½æŠ“å–æ–°æ–¹æ³•', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†åŸºäºäººç±»æŒ‡ä»¤çš„æœºå™¨äººæŠ“å–ä»»åŠ¡ï¼Œå°¤å…¶æ˜¯åœ¨æ‚ä¹±çš„ç¯å¢ƒä¸­è¿›è¡ŒæŠ“å–çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•FreeGraspï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰æ¥ç†è§£äººç±»æŒ‡ä»¤å’Œç‰©ä½“ä¹‹é—´çš„ç©ºé—´å…³ç³»ã€‚è¯¥æ–¹æ³•é€šè¿‡å°†æ‰€æœ‰ç‰©ä½“æ£€æµ‹ä¸ºå…³é”®ç‚¹ï¼Œå¹¶åœ¨å›¾åƒä¸Šæ ‡æ³¨è¿™äº›å…³é”®ç‚¹ï¼Œæ¥å¢å¼ºæ¨¡å‹çš„ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬è¿˜åˆ›å»ºäº†ä¸€ä¸ªåˆæˆæ•°æ®é›†FreeGraspDataï¼Œä»¥æ”¯æŒè¿™ä¸€ä»»åŠ¡çš„ç ”ç©¶ï¼Œå¹¶åœ¨çœŸå®ä¸–ç•Œä¸­éªŒè¯äº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.13444', 'title': 'VideoMind: A Chain-of-LoRA Agent for Long Video Reasoning', 'url': 'https://huggingface.co/papers/2503.13444', 'abstract': 'Videos, with their unique temporal dimension, demand precise grounded understanding, where answers are directly linked to visual, interpretable evidence. Despite significant breakthroughs in reasoning capabilities within Large Language Models, multi-modal reasoning - especially for videos - remains unexplored. In this work, we introduce VideoMind, a novel video-language agent designed for temporal-grounded video understanding. VideoMind incorporates two key innovations: (i) We identify essential capabilities for video temporal reasoning and develop a role-based agentic workflow, including a planner for coordinating different roles, a grounder for temporal localization, a verifier to assess temporal interval accuracy, and an answerer for question-answering. (ii) To efficiently integrate these diverse roles, we propose a novel Chain-of-LoRA strategy, enabling seamless role-switching via lightweight LoRA adaptors while avoiding the overhead of multiple models, thus balancing efficiency and flexibility. Extensive experiments on 14 public benchmarks demonstrate that our agent achieves state-of-the-art performance on diverse video understanding tasks, including 3 on grounded video question-answering, 6 on video temporal grounding, and 5 on general video question-answering, underscoring its effectiveness in advancing video agent and long-form temporal reasoning.', 'score': 8, 'issue_id': 2755, 'pub_date': '2025-03-17', 'pub_date_card': {'ru': '17 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 17', 'zh': '3æœˆ17æ—¥'}, 'hash': '4845903cb3dc6761', 'authors': ['Ye Liu', 'Kevin Qinghong Lin', 'Chang Wen Chen', 'Mike Zheng Shou'], 'affiliations': ['Show Lab, National University of Singapore', 'The Hong Kong Polytechnic University'], 'pdf_title_img': 'assets/pdf/title_img/2503.13444.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#agents', '#optimization', '#video', '#multimodal'], 'emoji': 'ğŸ¬', 'ru': {'title': 'VideoMind: Ğ¢ĞµĞ¼Ğ¿Ğ¾Ñ€Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ€Ğ¾Ğ»ĞµĞ²Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ VideoMind - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ñ‚ĞµĞ¼Ğ¿Ğ¾Ñ€Ğ°Ğ»ÑŒĞ½Ğ¾-Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ñ€Ğ¾Ğ»ĞµĞ²Ğ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ¾Ğ¼, Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ¼, Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‡Ğ¸ĞºĞ¾Ğ¼ Ğ´Ğ»Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Chain-of-LoRA Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ¾Ğ»ÑĞ¼Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ»ĞµĞ³ĞºĞ¸Ñ… LoRA-Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° 14 Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ VideoMind Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'VideoMind: Advancing Video Understanding with Temporal Reasoning', 'desc': 'This paper presents VideoMind, a new video-language agent that enhances understanding of videos by linking answers to visual evidence. It introduces a role-based workflow that includes a planner, grounder, verifier, and answerer to facilitate effective temporal reasoning in videos. The authors also propose a Chain-of-LoRA strategy, which allows for efficient role-switching without the need for multiple models, thus improving both efficiency and flexibility. The results show that VideoMind outperforms existing methods on various benchmarks, highlighting its capability in grounded video question-answering and temporal reasoning tasks.'}, 'zh': {'title': 'VideoMindï¼šè§†é¢‘ç†è§£çš„æ–°çªç ´', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„è§†é¢‘è¯­è¨€æ™ºèƒ½ä½“ï¼Œåä¸ºVideoMindï¼Œæ—¨åœ¨å®ç°è§†é¢‘çš„æ—¶é—´åŸºç¡€ç†è§£ã€‚è¯¥æ™ºèƒ½ä½“é€šè¿‡è§’è‰²é©±åŠ¨çš„å·¥ä½œæµç¨‹ï¼Œæ•´åˆäº†è§„åˆ’è€…ã€å®šä½è€…ã€éªŒè¯è€…å’Œå›ç­”è€…ç­‰å…³é”®è§’è‰²ï¼Œä»¥æé«˜è§†é¢‘çš„æ—¶é—´æ¨ç†èƒ½åŠ›ã€‚ä¸ºäº†é«˜æ•ˆæ•´åˆè¿™äº›è§’è‰²ï¼Œæå‡ºäº†ä¸€ç§æ–°é¢–çš„Chain-of-LoRAç­–ç•¥ï¼Œå…è®¸é€šè¿‡è½»é‡çº§çš„LoRAé€‚é…å™¨å®ç°è§’è‰²ä¹‹é—´çš„æ— ç¼åˆ‡æ¢ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVideoMindåœ¨å¤šé¡¹è§†é¢‘ç†è§£ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œæ¨åŠ¨äº†è§†é¢‘æ™ºèƒ½ä½“å’Œé•¿æ—¶åºæ¨ç†çš„å‘å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.11412', 'title': 'MTV-Inpaint: Multi-Task Long Video Inpainting', 'url': 'https://huggingface.co/papers/2503.11412', 'abstract': 'Video inpainting involves modifying local regions within a video, ensuring spatial and temporal consistency. Most existing methods focus primarily on scene completion (i.e., filling missing regions) and lack the capability to insert new objects into a scene in a controllable manner. Fortunately, recent advancements in text-to-video (T2V) diffusion models pave the way for text-guided video inpainting. However, directly adapting T2V models for inpainting remains limited in unifying completion and insertion tasks, lacks input controllability, and struggles with long videos, thereby restricting their applicability and flexibility. To address these challenges, we propose MTV-Inpaint, a unified multi-task video inpainting framework capable of handling both traditional scene completion and novel object insertion tasks. To unify these distinct tasks, we design a dual-branch spatial attention mechanism in the T2V diffusion U-Net, enabling seamless integration of scene completion and object insertion within a single framework. In addition to textual guidance, MTV-Inpaint supports multimodal control by integrating various image inpainting models through our proposed image-to-video (I2V) inpainting mode. Additionally, we propose a two-stage pipeline that combines keyframe inpainting with in-between frame propagation, enabling MTV-Inpaint to effectively handle long videos with hundreds of frames. Extensive experiments demonstrate that MTV-Inpaint achieves state-of-the-art performance in both scene completion and object insertion tasks. Furthermore, it demonstrates versatility in derived applications such as multi-modal inpainting, object editing, removal, image object brush, and the ability to handle long videos. Project page: https://mtv-inpaint.github.io/.', 'score': 7, 'issue_id': 2756, 'pub_date': '2025-03-14', 'pub_date_card': {'ru': '14 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 14', 'zh': '3æœˆ14æ—¥'}, 'hash': 'df14c3a2c0dfe3fd', 'authors': ['Shiyuan Yang', 'Zheng Gu', 'Liang Hou', 'Xin Tao', 'Pengfei Wan', 'Xiaodong Chen', 'Jing Liao'], 'affiliations': ['City University of Hong Kong', 'Kuaishou Technology', 'Shenzhen University', 'Tianjin University'], 'pdf_title_img': 'assets/pdf/title_img/2503.11412.jpg', 'data': {'categories': ['#multimodal', '#diffusion', '#video'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¸Ğ½Ğ¿ĞµĞ¹Ğ½Ñ‚Ğ¸Ğ½Ğ³: Ğ¾Ñ‚ Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ¾Ğ² Ğ´Ğ¾ Ğ²ÑÑ‚Ğ°Ğ²ĞºĞ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ²', 'desc': 'MTV-Inpaint - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¸Ğ½Ğ¿ĞµĞ¹Ğ½Ñ‚Ğ¸Ğ½Ğ³Ğ°, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ°Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ¾Ğ² Ğ¸ Ğ²ÑÑ‚Ğ°Ğ²ĞºĞ¸ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ text-to-video Ñ Ğ´Ğ²ÑƒÑ…Ğ²ĞµÑ‚Ğ²ĞµĞ²Ñ‹Ğ¼ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² U-Net Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ. MTV-Inpaint Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ, Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸Ğ½Ğ¿ĞµĞ¹Ğ½Ñ‚Ğ¸Ğ½Ğ³Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ Ğ¸Ğ½Ğ¿ĞµĞ¹Ğ½Ñ‚Ğ¸Ğ½Ğ³Ğ¾Ğ¼ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ¸ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'Unified Video Inpainting: Complete and Insert with Control!', 'desc': 'This paper presents MTV-Inpaint, a novel framework for video inpainting that integrates scene completion and object insertion tasks. It utilizes a dual-branch spatial attention mechanism within a text-to-video diffusion U-Net to achieve seamless control over both tasks. The framework also introduces a two-stage pipeline that effectively manages long videos by combining keyframe inpainting with in-between frame propagation. Extensive experiments show that MTV-Inpaint outperforms existing methods and offers versatility for various applications, including multi-modal inpainting and object editing.'}, 'zh': {'title': 'ç»Ÿä¸€è§†é¢‘ä¿®å¤ï¼Œåœºæ™¯è¡¥å…¨ä¸ç‰©ä½“æ’å…¥çš„å®Œç¾ç»“åˆ', 'desc': 'è§†é¢‘ä¿®å¤æ˜¯æŒ‡åœ¨è§†é¢‘ä¸­ä¿®æ”¹å±€éƒ¨åŒºåŸŸï¼Œä»¥ç¡®ä¿ç©ºé—´å’Œæ—¶é—´çš„ä¸€è‡´æ€§ã€‚ç°æœ‰çš„æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨åœºæ™¯è¡¥å…¨ä¸Šï¼Œç¼ºä¹å¯æ§åœ°æ’å…¥æ–°ç‰©ä½“çš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†MTV-Inpaintï¼Œä¸€ä¸ªç»Ÿä¸€çš„å¤šä»»åŠ¡è§†é¢‘ä¿®å¤æ¡†æ¶ï¼Œèƒ½å¤ŸåŒæ—¶å¤„ç†ä¼ ç»Ÿçš„åœºæ™¯è¡¥å…¨å’Œæ–°ç‰©ä½“æ’å…¥ä»»åŠ¡ã€‚é€šè¿‡è®¾è®¡åŒåˆ†æ”¯ç©ºé—´æ³¨æ„åŠ›æœºåˆ¶ï¼ŒMTV-Inpaintå®ç°äº†åœºæ™¯è¡¥å…¨å’Œç‰©ä½“æ’å…¥çš„æ— ç¼é›†æˆï¼Œå¹¶æ”¯æŒå¤šæ¨¡æ€æ§åˆ¶ï¼Œé€‚ç”¨äºé•¿è§†é¢‘çš„å¤„ç†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.13070', 'title': 'Rewards Are Enough for Fast Photo-Realistic Text-to-image Generation', 'url': 'https://huggingface.co/papers/2503.13070', 'abstract': 'Aligning generated images to complicated text prompts and human preferences is a central challenge in Artificial Intelligence-Generated Content (AIGC). With reward-enhanced diffusion distillation emerging as a promising approach that boosts controllability and fidelity of text-to-image models, we identify a fundamental paradigm shift: as conditions become more specific and reward signals stronger, the rewards themselves become the dominant force in generation. In contrast, the diffusion losses serve as an overly expensive form of regularization. To thoroughly validate our hypothesis, we introduce R0, a novel conditional generation approach via regularized reward maximization. Instead of relying on tricky diffusion distillation losses, R0 proposes a new perspective that treats image generations as an optimization problem in data space which aims to search for valid images that have high compositional rewards. By innovative designs of the generator parameterization and proper regularization techniques, we train state-of-the-art few-step text-to-image generative models with R0 at scales. Our results challenge the conventional wisdom of diffusion post-training and conditional generation by demonstrating that rewards play a dominant role in scenarios with complex conditions. We hope our findings can contribute to further research into human-centric and reward-centric generation paradigms across the broader field of AIGC. Code is available at https://github.com/Luo-Yihong/R0.', 'score': 6, 'issue_id': 2756, 'pub_date': '2025-03-17', 'pub_date_card': {'ru': '17 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 17', 'zh': '3æœˆ17æ—¥'}, 'hash': '3d6b6c6e117e1abd', 'authors': ['Yihong Luo', 'Tianyang Hu', 'Weijian Luo', 'Kenji Kawaguchi', 'Jing Tang'], 'affiliations': ['HKUST', 'HKUST (GZ)', 'NUS', 'Xiaohongshu Inc'], 'pdf_title_img': 'assets/pdf/title_img/2503.13070.jpg', 'data': {'categories': ['#rag', '#diffusion', '#alignment', '#training', '#optimization'], 'emoji': 'ğŸ¨', 'ru': {'title': 'R0: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼, Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ R0. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, R0 Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ĞºĞ°Ğº Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ¸ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ°Ñ… Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, ÑĞ°Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ÑÑ‚ÑÑ Ğ´Ğ¾Ğ¼Ğ¸Ğ½Ğ¸Ñ€ÑƒÑÑ‰ĞµĞ¹ ÑĞ¸Ğ»Ğ¾Ğ¹ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ñ€Ğ¾ÑĞ°ÑÑ‚ Ğ²Ñ‹Ğ·Ğ¾Ğ² Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼ Ğ¾ Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'Revolutionizing Image Generation: Prioritizing Rewards Over Diffusion', 'desc': 'This paper addresses the challenge of aligning generated images with complex text prompts and human preferences in AI-generated content. It introduces R0, a new approach that emphasizes reward maximization over traditional diffusion distillation methods, which are seen as inefficient. The authors argue that as conditions for image generation become more specific, the influence of reward signals becomes more significant than diffusion losses. Their findings suggest a shift in focus towards reward-centric generation strategies, which could enhance the effectiveness of text-to-image models.'}, 'zh': {'title': 'å¥–åŠ±é©±åŠ¨çš„å›¾åƒç”Ÿæˆæ–°è§†è§’', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†åœ¨äººå·¥æ™ºèƒ½ç”Ÿæˆå†…å®¹ï¼ˆAIGCï¼‰ä¸­ï¼Œå°†ç”Ÿæˆå›¾åƒä¸å¤æ‚æ–‡æœ¬æç¤ºå’Œäººç±»åå¥½å¯¹é½çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ¡ä»¶ç”Ÿæˆæ–¹æ³•R0ï¼Œé€šè¿‡æ­£åˆ™åŒ–å¥–åŠ±æœ€å¤§åŒ–æ¥ä¼˜åŒ–å›¾åƒç”Ÿæˆè¿‡ç¨‹ã€‚ç ”ç©¶è¡¨æ˜ï¼Œåœ¨æ¡ä»¶å˜å¾—æ›´åŠ å…·ä½“ä¸”å¥–åŠ±ä¿¡å·æ›´å¼ºæ—¶ï¼Œå¥–åŠ±åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­èµ·ç€ä¸»å¯¼ä½œç”¨ï¼Œè€Œæ‰©æ•£æŸå¤±åˆ™æˆä¸ºä¸€ç§è¿‡äºæ˜‚è´µçš„æ­£åˆ™åŒ–å½¢å¼ã€‚æˆ‘ä»¬çš„ç»“æœæŒ‘æˆ˜äº†ä¼ ç»Ÿçš„æ‰©æ•£åè®­ç»ƒå’Œæ¡ä»¶ç”Ÿæˆçš„è§‚å¿µï¼Œå¼ºè°ƒäº†åœ¨å¤æ‚æ¡ä»¶ä¸‹å¥–åŠ±çš„é‡è¦æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.10719', 'title': 'Long-Video Audio Synthesis with Multi-Agent Collaboration', 'url': 'https://huggingface.co/papers/2503.10719', 'abstract': 'Video-to-audio synthesis, which generates synchronized audio for visual content, critically enhances viewer immersion and narrative coherence in film and interactive media. However, video-to-audio dubbing for long-form content remains an unsolved challenge due to dynamic semantic shifts, temporal misalignment, and the absence of dedicated datasets. While existing methods excel in short videos, they falter in long scenarios (e.g., movies) due to fragmented synthesis and inadequate cross-scene consistency. We propose LVAS-Agent, a novel multi-agent framework that emulates professional dubbing workflows through collaborative role specialization. Our approach decomposes long-video synthesis into four steps including scene segmentation, script generation, sound design and audio synthesis. Central innovations include a discussion-correction mechanism for scene/script refinement and a generation-retrieval loop for temporal-semantic alignment. To enable systematic evaluation, we introduce LVAS-Bench, the first benchmark with 207 professionally curated long videos spanning diverse scenarios. Experiments demonstrate superior audio-visual alignment over baseline methods. Project page: https://lvas-agent.github.io', 'score': 6, 'issue_id': 2760, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 13', 'zh': '3æœˆ13æ—¥'}, 'hash': '121476792dd15d11', 'authors': ['Yehang Zhang', 'Xinli Xu', 'Xiaojie Xu', 'Li Liu', 'Yingcong Chen'], 'affiliations': ['HKUST', 'HKUST(GZ)'], 'pdf_title_img': 'assets/pdf/title_img/2503.10719.jpg', 'data': {'categories': ['#long_context', '#agents', '#benchmark', '#video', '#games'], 'emoji': 'ğŸ¬', 'ru': {'title': 'ĞŸÑ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ´ÑƒĞ±Ğ»ÑĞ¶ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜', 'desc': 'LVAS-Agent - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğº Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ñ€ÑƒÑÑ‰Ğ°Ñ Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ´ÑƒĞ±Ğ»ÑĞ¶Ğ°. ĞĞ½Ğ° Ñ€Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ½Ğ° Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ ÑÑ‚Ğ°Ğ¿Ğ°: ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ ÑÑ†ĞµĞ½, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ñ, Ğ·Ğ²ÑƒĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½ Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ°ÑƒĞ´Ğ¸Ğ¾. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¾Ğ±ÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑÑ†ĞµĞ½/ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ñ Ğ¸ Ñ†Ğ¸ĞºĞ» Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸-Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ´Ğ»Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾-ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ LVAS-Bench - Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¸Ğ· 207 Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ‚Ğ°ĞºĞ¸Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼.'}, 'en': {'title': 'Revolutionizing Long-Form Video Dubbing with LVAS-Agent', 'desc': 'This paper presents LVAS-Agent, a new framework for generating audio that matches long videos, like movies, to improve viewer experience. The framework breaks down the audio synthesis process into four key steps: segmenting scenes, generating scripts, designing sounds, and synthesizing audio. It introduces innovative techniques such as a discussion-correction mechanism to refine scripts and a generation-retrieval loop to ensure that audio aligns well with the video over time. Additionally, the authors provide LVAS-Bench, a benchmark dataset of 207 long videos to evaluate the effectiveness of their approach against existing methods.'}, 'zh': {'title': 'é•¿è§†é¢‘é…éŸ³çš„æ–°çªç ´ï¼šLVAS-Agent', 'desc': 'è§†é¢‘åˆ°éŸ³é¢‘åˆæˆæ˜¯ä¸ºè§†è§‰å†…å®¹ç”ŸæˆåŒæ­¥éŸ³é¢‘çš„æŠ€æœ¯ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡è§‚ä¼—çš„æ²‰æµ¸æ„Ÿå’Œå™äº‹è¿è´¯æ€§ã€‚ç„¶è€Œï¼Œå¯¹äºé•¿ç¯‡å†…å®¹çš„è§†é¢‘åˆ°éŸ³é¢‘é…éŸ³ä»ç„¶æ˜¯ä¸€ä¸ªæœªè§£å†³çš„æŒ‘æˆ˜ï¼Œä¸»è¦ç”±äºåŠ¨æ€è¯­ä¹‰å˜åŒ–ã€æ—¶é—´é”™ä½å’Œç¼ºä¹ä¸“é—¨çš„æ•°æ®é›†ã€‚ç°æœ‰æ–¹æ³•åœ¨çŸ­è§†é¢‘ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨é•¿è§†é¢‘ï¼ˆå¦‚ç”µå½±ï¼‰ä¸­ç”±äºåˆæˆç¢ç‰‡åŒ–å’Œè·¨åœºæ™¯ä¸€è‡´æ€§ä¸è¶³è€Œè¡¨ç°ä¸ä½³ã€‚æˆ‘ä»¬æå‡ºäº†LVAS-Agentï¼Œä¸€ä¸ªæ–°é¢–çš„å¤šä»£ç†æ¡†æ¶ï¼Œé€šè¿‡åä½œè§’è‰²ä¸“ä¸šåŒ–æ¨¡æ‹Ÿä¸“ä¸šé…éŸ³å·¥ä½œæµç¨‹ï¼Œåˆ†è§£é•¿è§†é¢‘åˆæˆä¸ºåœºæ™¯åˆ†å‰²ã€è„šæœ¬ç”Ÿæˆã€å£°éŸ³è®¾è®¡å’ŒéŸ³é¢‘åˆæˆå››ä¸ªæ­¥éª¤ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.10704', 'title': 'Error Analyses of Auto-Regressive Video Diffusion Models: A Unified\n  Framework', 'url': 'https://huggingface.co/papers/2503.10704', 'abstract': 'A variety of Auto-Regressive Video Diffusion Models (ARVDM) have achieved remarkable successes in generating realistic long-form videos. However, theoretical analyses of these models remain scant. In this work, we develop theoretical underpinnings for these models and use our insights to improve the performance of existing models. We first develop Meta-ARVDM, a unified framework of ARVDMs that subsumes most existing methods. Using Meta-ARVDM, we analyze the KL-divergence between the videos generated by Meta-ARVDM and the true videos. Our analysis uncovers two important phenomena inherent to ARVDM -- error accumulation and memory bottleneck. By deriving an information-theoretic impossibility result, we show that the memory bottleneck phenomenon cannot be avoided. To mitigate the memory bottleneck, we design various network structures to explicitly use more past frames. We also achieve a significantly improved trade-off between the mitigation of the memory bottleneck and the inference efficiency by compressing the frames. Experimental results on DMLab and Minecraft validate the efficacy of our methods. Our experiments also demonstrate a Pareto-frontier between the error accumulation and memory bottleneck across different methods.', 'score': 5, 'issue_id': 2756, 'pub_date': '2025-03-12', 'pub_date_card': {'ru': '12 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 12', 'zh': '3æœˆ12æ—¥'}, 'hash': 'e5f88bd433320c2f', 'authors': ['Jing Wang', 'Fengzhuo Zhang', 'Xiaoli Li', 'Vincent Y. F. Tan', 'Tianyu Pang', 'Chao Du', 'Aixin Sun', 'Zhuoran Yang'], 'affiliations': ['A*STAR', 'Nanyang Technological University', 'National University of Singapore', 'Sea AI Lab', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2503.10704.jpg', 'data': {'categories': ['#diffusion', '#video', '#inference', '#architecture', '#math', '#optimization'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ¢ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾ÑĞ½Ğ¾Ğ²Ñ‹ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ (ARVDM) Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ñ‹ Ğ¸Ñ… ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Meta-ARVDM - ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰ÑƒÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² ARVDM. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ñ‹ÑĞ²Ğ¸Ğ» Ğ´Ğ²Ğ° ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ² ARVDM: Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸, Ğ¿Ñ€Ğ¸Ñ‡ĞµĞ¼ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½ĞµĞµ Ğ½ĞµĞ¸Ğ·Ğ±ĞµĞ¶Ğ½Ğ¾ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ½Ğ¾ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾-Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñƒ. Ğ”Ğ»Ñ ÑĞ¼ÑĞ³Ñ‡ĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ° Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ñ‹ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹ Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ ĞºĞ°Ğ´Ñ€Ğ¾Ğ², ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ° ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Enhancing Video Generation with Meta-ARVDM: Tackling Memory Bottlenecks!', 'desc': 'This paper focuses on improving Auto-Regressive Video Diffusion Models (ARVDM) for generating realistic long videos. The authors introduce Meta-ARVDM, a comprehensive framework that encompasses existing ARVDM methods and provides a theoretical analysis of their performance. They identify two key issues: error accumulation and memory bottleneck, the latter of which is shown to be unavoidable through an information-theoretic approach. To address these challenges, the authors propose new network architectures that utilize more past frames and optimize the balance between memory usage and inference efficiency, demonstrating their findings through experiments on DMLab and Minecraft.'}, 'zh': {'title': 'æå‡è§†é¢‘ç”Ÿæˆæ•ˆç‡ï¼Œç ´è§£å†…å­˜ç“¶é¢ˆ', 'desc': 'æœ¬æ–‡æ¢è®¨äº†è‡ªå›å½’è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆARVDMï¼‰çš„ç†è®ºåŸºç¡€ï¼Œå¹¶æå‡ºäº†Meta-ARVDMè¿™ä¸€ç»Ÿä¸€æ¡†æ¶ï¼Œæ¶µç›–äº†å¤§å¤šæ•°ç°æœ‰æ–¹æ³•ã€‚é€šè¿‡åˆ†æMeta-ARVDMç”Ÿæˆçš„è§†é¢‘ä¸çœŸå®è§†é¢‘ä¹‹é—´çš„KLæ•£åº¦ï¼Œæ­ç¤ºäº†ARVDMå›ºæœ‰çš„ä¸¤ä¸ªé‡è¦ç°è±¡ï¼šè¯¯å·®ç´¯ç§¯å’Œå†…å­˜ç“¶é¢ˆã€‚æˆ‘ä»¬é€šè¿‡ä¿¡æ¯è®ºçš„ä¸å¯èƒ½æ€§ç»“æœè¯æ˜äº†å†…å­˜ç“¶é¢ˆç°è±¡æ˜¯æ— æ³•é¿å…çš„ã€‚ä¸ºäº†è§£å†³å†…å­˜ç“¶é¢ˆé—®é¢˜ï¼Œæœ¬æ–‡è®¾è®¡äº†å¤šç§ç½‘ç»œç»“æ„ï¼Œä»¥æ˜¾å¼åˆ©ç”¨æ›´å¤šçš„è¿‡å»å¸§ï¼Œå¹¶é€šè¿‡å‹ç¼©å¸§å®ç°äº†å†…å­˜ç“¶é¢ˆç¼“è§£ä¸æ¨ç†æ•ˆç‡ä¹‹é—´çš„æ˜¾è‘—æ”¹è¿›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.13369', 'title': 'Sightation Counts: Leveraging Sighted User Feedback in Building a\n  BLV-aligned Dataset of Diagram Descriptions', 'url': 'https://huggingface.co/papers/2503.13369', 'abstract': 'Often, the needs and visual abilities differ between the annotator group and the end user group. Generating detailed diagram descriptions for blind and low-vision (BLV) users is one such challenging domain. Sighted annotators could describe visuals with ease, but existing studies have shown that direct generations by them are costly, bias-prone, and somewhat lacking by BLV standards. In this study, we ask sighted individuals to assess -- rather than produce -- diagram descriptions generated by vision-language models (VLM) that have been guided with latent supervision via a multi-pass inference. The sighted assessments prove effective and useful to professional educators who are themselves BLV and teach visually impaired learners. We release Sightation, a collection of diagram description datasets spanning 5k diagrams and 137k samples for completion, preference, retrieval, question answering, and reasoning training purposes and demonstrate their fine-tuning potential in various downstream tasks.', 'score': 3, 'issue_id': 2760, 'pub_date': '2025-03-17', 'pub_date_card': {'ru': '17 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 17', 'zh': '3æœˆ17æ—¥'}, 'hash': '30b312815a3aaed9', 'authors': ['Wan Ju Kang', 'Eunki Kim', 'Na Min An', 'Sangryul Kim', 'Haemin Choi', 'Ki Hoon Kwak', 'James Thorne'], 'affiliations': ['KAIST AI', 'Sungkyunkwan University', 'Yonsei University'], 'pdf_title_img': 'assets/pdf/title_img/2503.13369.jpg', 'data': {'categories': ['#training', '#alignment', '#data', '#low_resource', '#dataset', '#multimodal'], 'emoji': 'ğŸ‘ï¸', 'ru': {'title': 'Ğ˜ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ Ğ½Ğ° ÑĞ»ÑƒĞ¶Ğ±Ğµ Ğ½ĞµĞ·Ñ€ÑÑ‡Ğ¸Ñ…: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ Ğ´Ğ¸Ğ°Ğ³Ñ€Ğ°Ğ¼Ğ¼ Ğ´Ğ»Ñ ÑĞ»ĞµĞ¿Ñ‹Ñ… Ğ¸ ÑĞ»Ğ°Ğ±Ğ¾Ğ²Ğ¸Ğ´ÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ·Ñ€ÑÑ‡Ğ¸Ñ… Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹, ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ğ¾Ğ³Ğ¾ Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ñ… ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¿Ğ¾Ğ´Ğ°Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ñ Ğ½Ğ°Ñ€ÑƒÑˆĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ. Ğ’ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ±Ñ‹Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Sightation, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ 5000 Ğ´Ğ¸Ğ°Ğ³Ñ€Ğ°Ğ¼Ğ¼ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Empowering BLV Education with Vision-Language Models', 'desc': "This paper addresses the challenge of creating effective diagram descriptions for blind and low-vision (BLV) users, highlighting the differences in needs between annotators and end users. It proposes a novel approach where sighted annotators assess diagram descriptions generated by vision-language models (VLM) instead of creating them directly, reducing bias and improving quality. The study introduces 'Sightation', a comprehensive dataset containing 5,000 diagrams and 137,000 samples designed for various machine learning tasks such as preference and reasoning. The findings suggest that using sighted assessments can enhance the educational resources available for BLV learners, making them more accessible and relevant."}, 'zh': {'title': 'ä¸ºè§†è§‰éšœç¢ç”¨æˆ·ç”Ÿæˆç²¾å‡†å›¾è¡¨æè¿°', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†è§†è§‰éšœç¢ç”¨æˆ·ï¼ˆBLVï¼‰å¯¹å›¾è¡¨æè¿°çš„éœ€æ±‚ä¸è§†è§‰æ ‡æ³¨è€…ä¹‹é—´çš„å·®å¼‚ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œé€šè¿‡è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ç”Ÿæˆå›¾è¡¨æè¿°ï¼Œå¹¶è®©è§†è§‰æ­£å¸¸çš„è¯„ä¼°è€…å¯¹è¿™äº›æè¿°è¿›è¡Œè¯„ä¼°ï¼Œè€Œä¸æ˜¯ç›´æ¥ç”Ÿæˆã€‚ç ”ç©¶è¡¨æ˜ï¼Œè¿™ç§è¯„ä¼°æ–¹å¼å¯¹ä¸“ä¸šçš„è§†è§‰éšœç¢æ•™è‚²è€…éå¸¸æœ‰æ•ˆï¼Œèƒ½å¤Ÿå¸®åŠ©ä»–ä»¬æ›´å¥½åœ°æœåŠ¡äºè§†è§‰éšœç¢å­¦ä¹ è€…ã€‚æˆ‘ä»¬è¿˜å‘å¸ƒäº†åä¸ºSightationçš„æ•°æ®é›†ï¼ŒåŒ…å«5000ä¸ªå›¾è¡¨å’Œ137000ä¸ªæ ·æœ¬ï¼Œæ—¨åœ¨æ”¯æŒå¤šç§ä¸‹æ¸¸ä»»åŠ¡çš„è®­ç»ƒã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.12530', 'title': 'Basic Category Usage in Vision Language Models', 'url': 'https://huggingface.co/papers/2503.12530', 'abstract': "The field of psychology has long recognized a basic level of categorization that humans use when labeling visual stimuli, a term coined by Rosch in 1976. This level of categorization has been found to be used most frequently, to have higher information density, and to aid in visual language tasks with priming in humans. Here, we investigate basic level categorization in two recently released, open-source vision-language models (VLMs). This paper demonstrates that Llama 3.2 Vision Instruct (11B) and Molmo 7B-D both prefer basic level categorization consistent with human behavior. Moreover, the models' preferences are consistent with nuanced human behaviors like the biological versus non-biological basic level effects and the well established expert basic level shift, further suggesting that VLMs acquire cognitive categorization behaviors from the human data on which they are trained.", 'score': 2, 'issue_id': 2755, 'pub_date': '2025-03-16', 'pub_date_card': {'ru': '16 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 16', 'zh': '3æœˆ16æ—¥'}, 'hash': 'eca67c7a2633554a', 'authors': ['Hunter Sawyer', 'Jesse Roberts', 'Kyle Moore'], 'affiliations': ['Tennessee Tech University'], 'pdf_title_img': 'assets/pdf/title_img/2503.12530.jpg', 'data': {'categories': ['#open_source', '#interpretability', '#multimodal', '#cv'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ˜Ğ˜ Ğ¼Ñ‹ÑĞ»Ğ¸Ñ‚ ĞºĞ°Ğº Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞº: Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ°Ñ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¸ Ğ¸Ğ· Ğ¿ÑĞ¸Ñ…Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸, Ğ² ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° (VLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Llama 3.2 Vision Instruct Ğ¸ Molmo 7B-D Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ğ¸Ñ‚Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°. Ğ‘Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ğ³Ğ¾, Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ½ÑĞ°Ğ½ÑÑ‹, Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ»ÑĞ´ĞµĞ¹, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ² ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ Ğ½ĞµĞ±Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ´Ğ²Ğ¸Ğ³ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ñƒ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ². Ğ­Ñ‚Ğ¾ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ° Ñ‚Ğ¾, Ñ‡Ñ‚Ğ¾ VLM Ğ¿Ñ€Ğ¸Ğ¾Ğ±Ñ€ĞµÑ‚Ğ°ÑÑ‚ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ· Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ½Ğ° ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ğ¾Ğ½Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ÑÑ.'}, 'en': {'title': 'Bridging Human and Machine: Basic Level Categorization in Vision-Language Models', 'desc': 'This paper explores how vision-language models (VLMs) categorize visual stimuli at a basic level, similar to human categorization as identified by Rosch. The study shows that Llama 3.2 Vision Instruct and Molmo 7B-D models exhibit a preference for basic level categorization, which aligns with human behavior. Additionally, the models reflect complex human categorization nuances, such as distinguishing between biological and non-biological entities. This suggests that VLMs learn cognitive categorization patterns from the human data they are trained on, highlighting the influence of human cognitive processes on machine learning models.'}, 'zh': {'title': 'æ¢ç´¢è§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„åŸºæœ¬åˆ†ç±»è¡Œä¸º', 'desc': 'æœ¬è®ºæ–‡ç ”ç©¶äº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ä¸­çš„åŸºæœ¬åˆ†ç±»æ°´å¹³ï¼Œè¿™ä¸€æ¦‚å¿µæœ€æ—©ç”±Roschåœ¨1976å¹´æå‡ºã€‚ç ”ç©¶å‘ç°ï¼ŒLlama 3.2 Vision Instructå’ŒMolmo 7B-Dè¿™ä¸¤ä¸ªæ¨¡å‹åœ¨åˆ†ç±»æ—¶æ›´å€¾å‘äºä½¿ç”¨ä¸äººç±»è¡Œä¸ºä¸€è‡´çš„åŸºæœ¬åˆ†ç±»æ°´å¹³ã€‚æ¨¡å‹çš„åå¥½è¿˜ä¸äººç±»çš„ç»†å¾®è¡Œä¸ºç›¸ç¬¦ï¼Œä¾‹å¦‚ç”Ÿç‰©ä¸éç”Ÿç‰©çš„åŸºæœ¬åˆ†ç±»æ•ˆåº”ï¼Œä»¥åŠä¸“å®¶çš„åŸºæœ¬åˆ†ç±»è½¬å˜ã€‚è¿™è¡¨æ˜ï¼Œè§†è§‰è¯­è¨€æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä»äººç±»æ•°æ®ä¸­å­¦ä¹ äº†è®¤çŸ¥åˆ†ç±»è¡Œä¸ºã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.12528', 'title': 'Investigating Human-Aligned Large Language Model Uncertainty', 'url': 'https://huggingface.co/papers/2503.12528', 'abstract': 'Recent work has sought to quantify large language model uncertainty to facilitate model control and modulate user trust. Previous works focus on measures of uncertainty that are theoretically grounded or reflect the average overt behavior of the model. In this work, we investigate a variety of uncertainty measures, in order to identify measures that correlate with human group-level uncertainty. We find that Bayesian measures and a variation on entropy measures, top-k entropy, tend to agree with human behavior as a function of model size. We find that some strong measures decrease in human-similarity with model size, but, by multiple linear regression, we find that combining multiple uncertainty measures provide comparable human-alignment with reduced size-dependency.', 'score': 2, 'issue_id': 2755, 'pub_date': '2025-03-16', 'pub_date_card': {'ru': '16 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 16', 'zh': '3æœˆ16æ—¥'}, 'hash': '7c140046351fe245', 'authors': ['Kyle Moore', 'Jesse Roberts', 'Daryl Watson', 'Pamela Wisniewski'], 'affiliations': ['Tennessee Tech University', 'Vanderbilt University'], 'pdf_title_img': 'assets/pdf/title_img/2503.12528.jpg', 'data': {'categories': ['#interpretability', '#hallucinations', '#rlhf', '#training'], 'emoji': 'ğŸ¤”', 'ru': {'title': 'Ğ˜Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğµ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ LLM: ĞºĞ°Ğº Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ·Ğ¸Ñ‚ÑŒÑÑ Ğº Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºÑƒ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM) Ñ Ñ†ĞµĞ»ÑŒÑ Ğ½Ğ°Ğ¹Ñ‚Ğ¸ Ñ‚Ğµ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ»ÑƒÑ‡ÑˆĞµ Ğ²ÑĞµĞ³Ğ¾ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‚ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¾Ğ²Ğ¾Ğ¹ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ»ÑĞ´ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ±Ğ°Ğ¹ĞµÑĞ¾Ğ²ÑĞºĞ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¸ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ñ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¹Ğ½Ñ‹Ñ… Ğ¼ĞµÑ€ (ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ñ top-k) Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ ÑĞ¾Ğ³Ğ»Ğ°ÑÑƒÑÑ‚ÑÑ Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸ĞµĞ¼ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼ĞµÑ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ° Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¼ Ğ¿Ñ€Ğ¸ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞšĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¼ĞµÑ€ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğ¹ Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ğ¾Ğµ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ñ Ğ¼ĞµĞ½ÑŒÑˆĞµĞ¹ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑŒÑ Ğ¾Ñ‚ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Aligning Model Uncertainty with Human Perception', 'desc': 'This paper explores how to measure uncertainty in large language models to improve their control and enhance user trust. It examines various uncertainty measures, particularly focusing on Bayesian methods and a new approach called top-k entropy, to see how well they align with human perceptions of uncertainty. The study reveals that while some measures lose their effectiveness as model size increases, combining different measures can maintain a strong correlation with human behavior regardless of model size. Ultimately, the findings suggest that a multi-measure approach can lead to better alignment with human understanding of uncertainty in language models.'}, 'zh': {'title': 'é‡åŒ–ä¸ç¡®å®šæ€§ï¼Œå¢å¼ºç”¨æˆ·ä¿¡ä»»', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸ç¡®å®šæ€§é‡åŒ–ï¼Œä»¥ä¾¿æ›´å¥½åœ°æ§åˆ¶æ¨¡å‹å¹¶å¢å¼ºç”¨æˆ·ä¿¡ä»»ã€‚æˆ‘ä»¬åˆ†æäº†å¤šç§ä¸ç¡®å®šæ€§åº¦é‡ï¼Œæ—¨åœ¨æ‰¾å‡ºä¸äººç±»ç¾¤ä½“ä¸ç¡®å®šæ€§ç›¸å…³çš„åº¦é‡ã€‚ç ”ç©¶å‘ç°ï¼Œè´å¶æ–¯åº¦é‡å’Œä¸€ç§å˜ä½“çš„ç†µåº¦é‡ï¼ˆtop-kç†µï¼‰åœ¨æ¨¡å‹è§„æ¨¡å˜åŒ–æ—¶ä¸äººç±»è¡Œä¸ºä¸€è‡´ã€‚é€šè¿‡å¤šå…ƒçº¿æ€§å›å½’ï¼Œæˆ‘ä»¬å‘ç°ç»“åˆå¤šç§ä¸ç¡®å®šæ€§åº¦é‡å¯ä»¥åœ¨å‡å°‘è§„æ¨¡ä¾èµ–æ€§çš„åŒæ—¶ï¼Œä¿æŒä¸äººç±»è¡Œä¸ºçš„ç›¸ä¼¼æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.12964', 'title': 'Training Video Foundation Models with NVIDIA NeMo', 'url': 'https://huggingface.co/papers/2503.12964', 'abstract': 'Video Foundation Models (VFMs) have recently been used to simulate the real world to train physical AI systems and develop creative visual experiences. However, there are significant challenges in training large-scale, high quality VFMs that can generate high-quality videos. We present a scalable, open-source VFM training pipeline with NVIDIA NeMo, providing accelerated video dataset curation, multimodal data loading, and parallelized video diffusion model training and inference. We also provide a comprehensive performance analysis highlighting best practices for efficient VFM training and inference.', 'score': 1, 'issue_id': 2771, 'pub_date': '2025-03-17', 'pub_date_card': {'ru': '17 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 17', 'zh': '3æœˆ17æ—¥'}, 'hash': 'a9b85c5227c0a3e6', 'authors': ['Zeeshan Patel', 'Ethan He', 'Parth Mannan', 'Xiaowei Ren', 'Ryan Wolf', 'Niket Agarwal', 'Jacob Huffman', 'Zhuoyao Wang', 'Carl Wang', 'Jack Chang', 'Yan Bai', 'Tommy Huang', 'Linnan Wang', 'Sahil Jain', 'Shanmugam Ramasamy', 'Joseph Jennings', 'Ekaterina Sirazitdinova', 'Oleg Sudakov', 'Mingyuan Ma', 'Bobby Chen', 'Forrest Lin', 'Hao Wang', 'Vasanth Rao Naik Sabavat', 'Sriharsha Niverty', 'Rong Ou', 'Pallab Bhattacharya', 'David Page', 'Nima Tajbakhsh', 'Ashwath Aithal'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2503.12964.jpg', 'data': {'categories': ['#dataset', '#diffusion', '#multimodal', '#inference', '#video', '#optimization', '#data', '#training', '#open_source'], 'emoji': 'ğŸ¥', 'ru': {'title': 'ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾-Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ NVIDIA NeMo', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾-Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (VFM) Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ NVIDIA NeMo. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ÑÑ‚ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ½ÑƒÑ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºÑƒ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºÑƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ñ€Ğ°ÑĞ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ĞµĞ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸. Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ğ²Ñ‹Ğ´ĞµĞ»ÑÑÑ‰Ğ¸Ğ¹ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° VFM. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¾ Ğ½Ğ° Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… VFM Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'Empowering Video Foundation Models with Scalable Training Solutions', 'desc': 'This paper introduces a scalable and open-source training pipeline for Video Foundation Models (VFMs) using NVIDIA NeMo. It addresses the challenges of training large-scale VFMs by offering tools for efficient video dataset curation and multimodal data loading. The proposed pipeline also includes methods for parallelized training and inference of video diffusion models. Additionally, the authors present a performance analysis that outlines best practices for optimizing VFM training and inference processes.'}, 'zh': {'title': 'é«˜æ•ˆè®­ç»ƒè§†é¢‘åŸºç¡€æ¨¡å‹çš„å¼€æºè§£å†³æ–¹æ¡ˆ', 'desc': 'è§†é¢‘åŸºç¡€æ¨¡å‹ï¼ˆVFMï¼‰æœ€è¿‘è¢«ç”¨äºæ¨¡æ‹Ÿç°å®ä¸–ç•Œï¼Œä»¥è®­ç»ƒç‰©ç†äººå·¥æ™ºèƒ½ç³»ç»Ÿå’Œå¼€å‘åˆ›æ„è§†è§‰ä½“éªŒã€‚ç„¶è€Œï¼Œè®­ç»ƒå¤§è§„æ¨¡ã€é«˜è´¨é‡çš„VFMä»¥ç”Ÿæˆé«˜è´¨é‡è§†é¢‘é¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå¯æ‰©å±•çš„å¼€æºVFMè®­ç»ƒç®¡é“ï¼Œåˆ©ç”¨NVIDIA NeMoï¼ŒåŠ é€Ÿè§†é¢‘æ•°æ®é›†çš„æ•´ç†ã€å¤šæ¨¡æ€æ•°æ®åŠ è½½ï¼Œä»¥åŠå¹¶è¡ŒåŒ–çš„è§†é¢‘æ‰©æ•£æ¨¡å‹è®­ç»ƒå’Œæ¨ç†ã€‚æˆ‘ä»¬è¿˜æä¾›äº†å…¨é¢çš„æ€§èƒ½åˆ†æï¼Œå¼ºè°ƒé«˜æ•ˆVFMè®­ç»ƒå’Œæ¨ç†çš„æœ€ä½³å®è·µã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.12720', 'title': 'GenStereo: Towards Open-World Generation of Stereo Images and\n  Unsupervised Matching', 'url': 'https://huggingface.co/papers/2503.12720', 'abstract': 'Stereo images are fundamental to numerous applications, including extended reality (XR) devices, autonomous driving, and robotics. Unfortunately, acquiring high-quality stereo images remains challenging due to the precise calibration requirements of dual-camera setups and the complexity of obtaining accurate, dense disparity maps. Existing stereo image generation methods typically focus on either visual quality for viewing or geometric accuracy for matching, but not both. We introduce GenStereo, a diffusion-based approach, to bridge this gap. The method includes two primary innovations (1) conditioning the diffusion process on a disparity-aware coordinate embedding and a warped input image, allowing for more precise stereo alignment than previous methods, and (2) an adaptive fusion mechanism that intelligently combines the diffusion-generated image with a warped image, improving both realism and disparity consistency. Through extensive training on 11 diverse stereo datasets, GenStereo demonstrates strong generalization ability. GenStereo achieves state-of-the-art performance in both stereo image generation and unsupervised stereo matching tasks. Our framework eliminates the need for complex hardware setups while enabling high-quality stereo image generation, making it valuable for both real-world applications and unsupervised learning scenarios. Project page is available at https://qjizhi.github.io/genstereo', 'score': 1, 'issue_id': 2766, 'pub_date': '2025-03-17', 'pub_date_card': {'ru': '17 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 17', 'zh': '3æœˆ17æ—¥'}, 'hash': '2fdcfe786193f6cf', 'authors': ['Feng Qiao', 'Zhexiao Xiong', 'Eric Xing', 'Nathan Jacobs'], 'affiliations': ['Washington University in St. Louis'], 'pdf_title_img': 'assets/pdf/title_img/2503.12720.jpg', 'data': {'categories': ['#robotics', '#video', '#3d', '#diffusion'], 'emoji': 'ğŸ‘ï¸', 'ru': {'title': 'GenStereo: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑÑ‚ĞµÑ€ĞµĞ¾Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'GenStereo - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑÑ‚ĞµÑ€ĞµĞ¾Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ´Ğ¸ÑĞ¿Ğ°Ñ€Ğ°Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ´ĞµÑ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ²Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ÑÑ‚ĞµÑ€ĞµĞ¾Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ. GenStereo Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¸ Ğ´ĞµÑ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¸ÑĞ¿Ğ°Ñ€Ğ°Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° 11 Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑÑ‚ĞµÑ€ĞµĞ¾Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ…, GenStereo Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑÑ‚ĞµÑ€ĞµĞ¾Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ½ĞµĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ ÑÑ‚ĞµÑ€ĞµĞ¾ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'GenStereo: Bridging Visual Quality and Geometric Accuracy in Stereo Image Generation', 'desc': 'This paper presents GenStereo, a novel diffusion-based method for generating high-quality stereo images. It addresses the challenges of stereo image generation by focusing on both visual quality and geometric accuracy, which are often at odds in existing methods. GenStereo innovates by conditioning the diffusion process on disparity-aware embeddings and using an adaptive fusion mechanism to enhance realism and consistency. The method shows strong performance across various stereo datasets, making it suitable for applications in extended reality, autonomous driving, and robotics without the need for complex hardware setups.'}, 'zh': {'title': 'GenStereoï¼šé«˜è´¨é‡ç«‹ä½“å›¾åƒç”Ÿæˆçš„æ–°æ–¹æ³•', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºGenStereoçš„ç”Ÿæˆæ–¹æ³•ï¼Œæ—¨åœ¨æé«˜ç«‹ä½“å›¾åƒçš„è´¨é‡å’Œå‡ ä½•å‡†ç¡®æ€§ã€‚è¯¥æ–¹æ³•é€šè¿‡æ¡ä»¶æ‰©æ•£è¿‡ç¨‹ï¼Œç»“åˆè§†å·®æ„ŸçŸ¥çš„åæ ‡åµŒå…¥å’Œå˜å½¢è¾“å…¥å›¾åƒï¼Œå®ç°æ›´ç²¾ç¡®çš„ç«‹ä½“å¯¹é½ã€‚GenStereoè¿˜é‡‡ç”¨äº†ä¸€ç§è‡ªé€‚åº”èåˆæœºåˆ¶ï¼Œæ™ºèƒ½åœ°å°†ç”Ÿæˆçš„å›¾åƒä¸å˜å½¢å›¾åƒç»“åˆï¼Œä»è€Œæé«˜äº†å›¾åƒçš„çœŸå®æ„Ÿå’Œè§†å·®ä¸€è‡´æ€§ã€‚ç»è¿‡åœ¨11ä¸ªå¤šæ ·åŒ–çš„ç«‹ä½“æ•°æ®é›†ä¸Šçš„å¹¿æ³›è®­ç»ƒï¼ŒGenStereoåœ¨ç«‹ä½“å›¾åƒç”Ÿæˆå’Œæ— ç›‘ç£ç«‹ä½“åŒ¹é…ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå…·æœ‰å¾ˆå¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.08153', 'title': 'WISA: World Simulator Assistant for Physics-Aware Text-to-Video\n  Generation', 'url': 'https://huggingface.co/papers/2503.08153', 'abstract': "Recent rapid advancements in text-to-video (T2V) generation, such as SoRA and Kling, have shown great potential for building world simulators. However, current T2V models struggle to grasp abstract physical principles and generate videos that adhere to physical laws. This challenge arises primarily from a lack of clear guidance on physical information due to a significant gap between abstract physical principles and generation models. To this end, we introduce the World Simulator Assistant (WISA), an effective framework for decomposing and incorporating physical principles into T2V models. Specifically, WISA decomposes physical principles into textual physical descriptions, qualitative physical categories, and quantitative physical properties. To effectively embed these physical attributes into the generation process, WISA incorporates several key designs, including Mixture-of-Physical-Experts Attention (MoPA) and a Physical Classifier, enhancing the model's physics awareness. Furthermore, most existing datasets feature videos where physical phenomena are either weakly represented or entangled with multiple co-occurring processes, limiting their suitability as dedicated resources for learning explicit physical principles. We propose a novel video dataset, WISA-32K, collected based on qualitative physical categories. It consists of 32,000 videos, representing 17 physical laws across three domains of physics: dynamics, thermodynamics, and optics. Experimental results demonstrate that WISA can effectively enhance the compatibility of T2V models with real-world physical laws, achieving a considerable improvement on the VideoPhy benchmark. The visual exhibitions of WISA and WISA-32K are available in the https://360cvgroup.github.io/WISA/.", 'score': 0, 'issue_id': 2766, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 11', 'zh': '3æœˆ11æ—¥'}, 'hash': '60c6c27b7b32f442', 'authors': ['Jing Wang', 'Ao Ma', 'Ke Cao', 'Jun Zheng', 'Zhanjie Zhang', 'Jiasong Feng', 'Shanyuan Liu', 'Yuhang Ma', 'Bo Cheng', 'Dawei Leng', 'Yuhui Yin', 'Xiaodan Liang'], 'affiliations': ['AI Research', 'Peng Cheng Laboratory', 'Shenzhen Campus of Sun Yat-Sen University'], 'pdf_title_img': 'assets/pdf/title_img/2503.08153.jpg', 'data': {'categories': ['#games', '#benchmark', '#video', '#dataset', '#optimization'], 'emoji': 'ğŸ¥', 'ru': {'title': 'WISA: Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ T2V Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ·Ğ°ĞºĞ¾Ğ½Ğ°Ğ¼ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ WISA (World Simulator Assistant) - Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ñ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ¾Ğ² Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ (T2V). WISA Ñ€Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ñ‹ Ğ½Ğ° Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ, ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¸ Ğ¸ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°. Ğ”Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾ÑĞ²ĞµĞ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ÑÑ Ñ‚Ğ°ĞºĞ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ĞºĞ°Ğº Mixture-of-Physical-Experts Attention Ğ¸ Physical Classifier. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ WISA-32K Ğ¸Ğ· 32 000 Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ñ… 17 Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ² Ğ¸Ğ· Ñ‚Ñ€ĞµÑ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ Ñ„Ğ¸Ğ·Ğ¸ĞºĞ¸.'}, 'en': {'title': 'Bridging Text-to-Video with Real-World Physics', 'desc': "This paper presents the World Simulator Assistant (WISA), a framework designed to improve text-to-video (T2V) generation by integrating physical principles into the models. WISA breaks down physical concepts into textual descriptions, qualitative categories, and quantitative properties, allowing T2V models to better understand and apply these principles. It introduces innovative components like Mixture-of-Physical-Experts Attention (MoPA) and a Physical Classifier to enhance the model's awareness of physics. Additionally, the authors create a new dataset, WISA-32K, containing 32,000 videos that illustrate 17 physical laws, which helps train T2V models to align more closely with real-world physics."}, 'zh': {'title': 'æå‡æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆçš„ç‰©ç†æ„è¯†', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œç§°ä¸ºä¸–ç•Œæ¨¡æ‹ŸåŠ©æ‰‹ï¼ˆWISAï¼‰ï¼Œæ—¨åœ¨æ”¹å–„æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆï¼ˆT2Vï¼‰æ¨¡å‹å¯¹ç‰©ç†åŸåˆ™çš„ç†è§£ã€‚WISAé€šè¿‡å°†ç‰©ç†åŸåˆ™åˆ†è§£ä¸ºæ–‡æœ¬æè¿°ã€å®šæ€§ç±»åˆ«å’Œå®šé‡å±æ€§ï¼Œå¸®åŠ©ç”Ÿæˆæ¨¡å‹æ›´å¥½åœ°èå…¥ç‰©ç†ä¿¡æ¯ã€‚è¯¥æ¡†æ¶è¿˜å¼•å…¥äº†æ··åˆç‰©ç†ä¸“å®¶æ³¨æ„åŠ›ï¼ˆMoPAï¼‰å’Œç‰©ç†åˆ†ç±»å™¨ç­‰è®¾è®¡ï¼Œå¢å¼ºäº†æ¨¡å‹çš„ç‰©ç†æ„è¯†ã€‚æ­¤å¤–ï¼ŒWISAè¿˜æå‡ºäº†ä¸€ä¸ªæ–°çš„è§†é¢‘æ•°æ®é›†WISA-32Kï¼ŒåŒ…å«32,000ä¸ªè§†é¢‘ï¼Œæ¶µç›–17æ¡ç‰©ç†å®šå¾‹ï¼Œæ—¨åœ¨ä¸ºå­¦ä¹ æ˜ç¡®çš„ç‰©ç†åŸåˆ™æä¾›æ›´åˆé€‚çš„èµ„æºã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.06269', 'title': 'Using Mechanistic Interpretability to Craft Adversarial Attacks against\n  Large Language Models', 'url': 'https://huggingface.co/papers/2503.06269', 'abstract': "Traditional white-box methods for creating adversarial perturbations against LLMs typically rely only on gradient computation from the targeted model, ignoring the internal mechanisms responsible for attack success or failure. Conversely, interpretability studies that analyze these internal mechanisms lack practical applications beyond runtime interventions. We bridge this gap by introducing a novel white-box approach that leverages mechanistic interpretability techniques to craft practical adversarial inputs. Specifically, we first identify acceptance subspaces - sets of feature vectors that do not trigger the model's refusal mechanisms - then use gradient-based optimization to reroute embeddings from refusal subspaces to acceptance subspaces, effectively achieving jailbreaks. This targeted approach significantly reduces computation cost, achieving attack success rates of 80-95\\% on state-of-the-art models including Gemma2, Llama3.2, and Qwen2.5 within minutes or even seconds, compared to existing techniques that often fail or require hours of computation. We believe this approach opens a new direction for both attack research and defense development. Furthermore, it showcases a practical application of mechanistic interpretability where other methods are less efficient, which highlights its utility. The code and generated datasets are available at https://github.com/Sckathach/subspace-rerouting.", 'score': 0, 'issue_id': 2772, 'pub_date': '2025-03-08', 'pub_date_card': {'ru': '8 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 8', 'zh': '3æœˆ8æ—¥'}, 'hash': '623dbef3e618123c', 'authors': ['Thomas Winninger', 'Boussad Addad', 'Katarzyna Kapusta'], 'affiliations': ['Thales SIX GTS', 'TÃ©lÃ©com SudParis'], 'pdf_title_img': 'assets/pdf/title_img/2503.06269.jpg', 'data': {'categories': ['#security', '#benchmark', '#interpretability', '#data', '#optimization', '#architecture', '#dataset'], 'emoji': 'ğŸ›¡ï¸', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ÑĞ¾ÑÑ‚ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ°Ñ‚Ğ°ĞºĞ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ñ… Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ¾ÑÑ‚ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ°Ñ‚Ğ°Ğº Ğ½Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒÑÑ‚ 'Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ' - Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ² Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ², Ğ½Ğµ Ğ²Ñ‹Ğ·Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğµ Ğ¾Ñ‚ĞºĞ°Ğ· Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¿ÑƒÑĞºĞ° Ğ´Ğ»Ñ Ğ¿ĞµÑ€ĞµĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ 80-95% ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ñ‚Ğ°Ğº Ğ½Ğ° ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ·Ğ° Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹ Ğ¸Ğ»Ğ¸ ÑĞµĞºÑƒĞ½Ğ´Ñ‹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ĞºĞ°Ğº Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ°Ñ‚Ğ°Ğº, Ñ‚Ğ°Ğº Ğ¸ Ğ´Ğ»Ñ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñ‹, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸."}, 'en': {'title': 'Bridging Interpretability and Adversarial Attacks for LLMs', 'desc': "This paper presents a new method for creating adversarial inputs for large language models (LLMs) by combining gradient-based optimization with mechanistic interpretability. The authors identify 'acceptance subspaces' where inputs are accepted by the model, and reroute embeddings from 'refusal subspaces' to these acceptance areas. This innovative approach significantly improves the efficiency of adversarial attacks, achieving success rates of 80-95% in a matter of minutes. The research not only advances attack strategies but also demonstrates the practical use of interpretability techniques in machine learning."}, 'zh': {'title': 'åˆ©ç”¨æœºåˆ¶å¯è§£é‡Šæ€§æå‡å¯¹æŠ—æ”»å‡»æˆåŠŸç‡', 'desc': 'è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„ç™½ç›’æ–¹æ³•ï¼Œç”¨äºç”Ÿæˆé’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¯¹æŠ—æ‰°åŠ¨ã€‚è¯¥æ–¹æ³•ç»“åˆäº†æœºåˆ¶å¯è§£é‡Šæ€§æŠ€æœ¯ï¼Œé¦–å…ˆè¯†åˆ«æ¥å—å­ç©ºé—´ï¼Œç„¶åé€šè¿‡åŸºäºæ¢¯åº¦çš„ä¼˜åŒ–å°†åµŒå…¥ä»æ‹’ç»å­ç©ºé—´é‡æ–°å¼•å¯¼åˆ°æ¥å—å­ç©ºé—´ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œè¿™ç§æ–¹æ³•æ˜¾è‘—é™ä½äº†è®¡ç®—æˆæœ¬ï¼Œå¹¶åœ¨å‡ åˆ†é’Ÿæˆ–å‡ ç§’å†…å®ç°äº†80-95%çš„æ”»å‡»æˆåŠŸç‡ã€‚è¯¥ç ”ç©¶ä¸ºæ”»å‡»ç ”ç©¶å’Œé˜²å¾¡å¼€å‘å¼€è¾Ÿäº†æ–°æ–¹å‘ï¼ŒåŒæ—¶å±•ç¤ºäº†æœºåˆ¶å¯è§£é‡Šæ€§çš„å®é™…åº”ç”¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.04625', 'title': 'START: Self-taught Reasoner with Tools', 'url': 'https://huggingface.co/papers/2503.04625', 'abstract': "Large reasoning models (LRMs) like OpenAI-o1 and DeepSeek-R1 have demonstrated remarkable capabilities in complex reasoning tasks through the utilization of long Chain-of-thought (CoT). However, these models often suffer from hallucinations and inefficiencies due to their reliance solely on internal reasoning processes. In this paper, we introduce START (Self-Taught Reasoner with Tools), a novel tool-integrated long CoT reasoning LLM that significantly enhances reasoning capabilities by leveraging external tools. Through code execution, START is capable of performing complex computations, self-checking, exploring diverse methods, and self-debugging, thereby addressing the limitations of LRMs. The core innovation of START lies in its self-learning framework, which comprises two key techniques: 1) Hint-infer: We demonstrate that inserting artificially designed hints (e.g., ``Wait, maybe using Python here is a good idea.'') during the inference process of a LRM effectively stimulates its ability to utilize external tools without the need for any demonstration data. Hint-infer can also serve as a simple and effective sequential test-time scaling method; 2) Hint Rejection Sampling Fine-Tuning (Hint-RFT): Hint-RFT combines Hint-infer and RFT by scoring, filtering, and modifying the reasoning trajectories with tool invocation generated by a LRM via Hint-infer, followed by fine-tuning the LRM. Through this framework, we have fine-tuned the QwQ-32B model to achieve START. On PhD-level science QA (GPQA), competition-level math benchmarks (AMC23, AIME24, AIME25), and the competition-level code benchmark (LiveCodeBench), START achieves accuracy rates of 63.6%, 95.0%, 66.7%, 47.1%, and 47.3%, respectively. It significantly outperforms the base QwQ-32B and achieves performance comparable to the state-of-the-art open-weight model R1-Distill-Qwen-32B and the proprietary model o1-Preview.", 'score': 66, 'issue_id': 2579, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 6', 'zh': '3æœˆ6æ—¥'}, 'hash': '8961f69e1eda24ad', 'authors': ['Chengpeng Li', 'Mingfeng Xue', 'Zhenru Zhang', 'Jiaxi Yang', 'Beichen Zhang', 'Xiang Wang', 'Bowen Yu', 'Binyuan Hui', 'Junyang Lin', 'Dayiheng Liu'], 'affiliations': ['Alibaba Group', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2503.04625.jpg', 'data': {'categories': ['#long_context', '#rl', '#training', '#architecture', '#hallucinations', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'START: Ğ¡Ğ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ°ÑÑÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ START - Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ¹ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¾Ğ¹ Ğ¼Ñ‹ÑĞ»ĞµĞ¹, Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑÑ‰ÑƒÑ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹. START Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ Hint-infer Ğ´Ğ»Ñ ÑÑ‚Ğ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¸ Hint Rejection Sampling Fine-Tuning Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²ÑƒÑ QwQ-32B Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. START Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ Ğ½Ğ°ÑƒĞºĞµ, Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞµ Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ PhD Ğ¸ ÑĞ¾Ñ€ĞµĞ²Ğ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹.'}, 'en': {'title': 'Enhancing Reasoning with External Tools: Introducing START', 'desc': "This paper presents START, a new long Chain-of-thought reasoning model that integrates external tools to improve reasoning capabilities. Traditional large reasoning models often struggle with hallucinations and inefficiencies, but START addresses these issues by allowing the model to perform complex computations and self-debugging. The innovation lies in two techniques: Hint-infer, which uses designed hints to encourage tool usage, and Hint Rejection Sampling Fine-Tuning, which refines the model's reasoning paths. START demonstrates superior performance on various benchmarks, surpassing its predecessor and competing effectively with state-of-the-art models."}, 'zh': {'title': 'å·¥å…·æ•´åˆï¼Œæ¨ç†æ›´å¼ºï¼', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹çš„é•¿é“¾æ¨ç†æ¨¡å‹STARTï¼ˆè‡ªæˆ‘å­¦ä¹ æ¨ç†å™¨ä¸å·¥å…·ï¼‰ï¼Œå®ƒé€šè¿‡æ•´åˆå¤–éƒ¨å·¥å…·æ¥å¢å¼ºæ¨ç†èƒ½åŠ›ã€‚STARTåˆ©ç”¨ä»£ç æ‰§è¡Œè¿›è¡Œå¤æ‚è®¡ç®—ã€è‡ªæˆ‘æ£€æŸ¥ã€æ¢ç´¢å¤šç§æ–¹æ³•å’Œè‡ªæˆ‘è°ƒè¯•ï¼Œä»è€Œå…‹æœäº†å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰åœ¨æ¨ç†è¿‡ç¨‹ä¸­å¸¸è§çš„å¹»è§‰å’Œä½æ•ˆé—®é¢˜ã€‚æ ¸å¿ƒåˆ›æ–°åœ¨äºè‡ªæˆ‘å­¦ä¹ æ¡†æ¶ï¼ŒåŒ…æ‹¬æç¤ºæ¨ç†ï¼ˆHint-inferï¼‰å’Œæç¤ºæ‹’ç»é‡‡æ ·å¾®è°ƒï¼ˆHint-RFTï¼‰ä¸¤ç§æŠ€æœ¯ï¼Œå‰è€…é€šè¿‡æ’å…¥è®¾è®¡çš„æç¤ºæ¥æ¿€å‘æ¨¡å‹ä½¿ç”¨å¤–éƒ¨å·¥å…·çš„èƒ½åŠ›ã€‚ç»è¿‡å¾®è°ƒï¼ŒSTARTåœ¨å¤šä¸ªç§‘å­¦é—®ç­”å’Œæ•°å­¦åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå‡†ç¡®ç‡æ˜¾è‘—é«˜äºåŸºç¡€æ¨¡å‹QwQ-32Bã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.04130', 'title': 'Token-Efficient Long Video Understanding for Multimodal LLMs', 'url': 'https://huggingface.co/papers/2503.04130', 'abstract': 'Recent advances in video-based multimodal large language models (Video-LLMs) have significantly improved video understanding by processing videos as sequences of image frames. However, many existing methods treat frames independently in the vision backbone, lacking explicit temporal modeling, which limits their ability to capture dynamic patterns and efficiently handle long videos. To address these limitations, we introduce STORM (Spatiotemporal TOken Reduction for Multimodal LLMs), a novel architecture incorporating a dedicated temporal encoder between the image encoder and the LLM. Our temporal encoder leverages the Mamba State Space Model to integrate temporal information into image tokens, generating enriched representations that preserve inter-frame dynamics across the entire video sequence. This enriched encoding not only enhances video reasoning capabilities but also enables effective token reduction strategies, including test-time sampling and training-based temporal and spatial pooling, substantially reducing computational demands on the LLM without sacrificing key temporal information. By integrating these techniques, our approach simultaneously reduces training and inference latency while improving performance, enabling efficient and robust video understanding over extended temporal contexts. Extensive evaluations show that STORM achieves state-of-the-art results across various long video understanding benchmarks (more than 5\\% improvement on MLVU and LongVideoBench) while reducing the computation costs by up to 8times and the decoding latency by 2.4-2.9times for the fixed numbers of input frames. Project page is available at https://research.nvidia.com/labs/lpr/storm', 'score': 59, 'issue_id': 2580, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 6', 'zh': '3æœˆ6æ—¥'}, 'hash': 'f1d1afacd41dc0c7', 'authors': ['Jindong Jiang', 'Xiuyu Li', 'Zhijian Liu', 'Muyang Li', 'Guo Chen', 'Zhiqi Li', 'De-An Huang', 'Guilin Liu', 'Zhiding Yu', 'Kurt Keutzer', 'Sungjin Ahn', 'Jan Kautz', 'Hongxu Yin', 'Yao Lu', 'Song Han', 'Wonmin Byeon'], 'affiliations': ['KAIST', 'MIT', 'NVIDIA', 'Nanjing University', 'Rutgers University', 'UC Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2503.04130.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#optimization', '#long_context', '#architecture', '#video', '#inference', '#multimodal'], 'emoji': 'ğŸŒªï¸', 'ru': {'title': 'STORM: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ‚ĞµĞ¼Ğ¿Ğ¾Ñ€Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ', 'desc': 'STORM - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ Ğ²Ğ¸Ğ´ĞµĞ¾-LLM, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ‚ĞµĞ¼Ğ¿Ğ¾Ñ€Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Mamba State Space Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ LLM. Ğ­Ñ‚Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ² Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ Ğ¾Ğ±Ğ¾Ğ³Ğ°Ñ‰ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑÑ‰Ğ¸Ğµ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºÑƒ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ğ´Ñ€Ğ°Ğ¼Ğ¸ Ğ²Ğ¾ Ğ²ÑĞµĞ¹ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. STORM Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºÑƒ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¿ÑƒĞ»Ğ¸Ğ½Ğ³ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ LLM. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ STORM Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ğ½Ğ° 5% Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ´Ğ¾ 8 Ñ€Ğ°Ğ·.'}, 'en': {'title': 'STORM: Revolutionizing Video Understanding with Temporal Insights', 'desc': 'This paper presents STORM, a new architecture designed to enhance video understanding in multimodal large language models (LLMs) by incorporating a temporal encoder. Unlike previous methods that treat video frames independently, STORM uses the Mamba State Space Model to capture the dynamics between frames, resulting in richer representations of video content. The architecture also implements token reduction strategies that significantly lower computational costs and improve processing speed without losing important temporal information. Evaluations demonstrate that STORM outperforms existing models on long video benchmarks while achieving substantial reductions in computation and latency.'}, 'zh': {'title': 'é«˜æ•ˆè§†é¢‘ç†è§£çš„æ–°çªç ´ï¼šSTORMæ¨¡å‹', 'desc': 'æœ€è¿‘ï¼ŒåŸºäºè§†é¢‘çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆVideo-LLMsï¼‰åœ¨è§†é¢‘ç†è§£æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†è®¸å¤šç°æœ‰æ–¹æ³•åœ¨è§†è§‰éª¨å¹²ä¸­ç‹¬ç«‹å¤„ç†å¸§ï¼Œç¼ºä¹æ˜ç¡®çš„æ—¶é—´å»ºæ¨¡ï¼Œé™åˆ¶äº†æ•æ‰åŠ¨æ€æ¨¡å¼çš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†STORMï¼ˆæ—¶ç©ºä»¤ç‰Œå‡å°‘æ¨¡å‹ï¼‰ï¼Œå®ƒåœ¨å›¾åƒç¼–ç å™¨å’Œå¤§è¯­è¨€æ¨¡å‹ä¹‹é—´å¼•å…¥äº†ä¸“é—¨çš„æ—¶é—´ç¼–ç å™¨ã€‚æˆ‘ä»¬çš„æ—¶é—´ç¼–ç å™¨åˆ©ç”¨MambaçŠ¶æ€ç©ºé—´æ¨¡å‹ï¼Œå°†æ—¶é—´ä¿¡æ¯æ•´åˆåˆ°å›¾åƒä»¤ç‰Œä¸­ï¼Œç”Ÿæˆä¸°å¯Œçš„è¡¨ç¤ºï¼Œä¿ç•™æ•´ä¸ªè§†é¢‘åºåˆ—ä¸­çš„å¸§é—´åŠ¨æ€ã€‚é€šè¿‡è¿™äº›æŠ€æœ¯çš„æ•´åˆï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æé«˜æ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—å‡å°‘äº†è®¡ç®—éœ€æ±‚å’Œæ¨ç†å»¶è¿Ÿï¼Œå®ç°äº†å¯¹é•¿è§†é¢‘çš„é«˜æ•ˆç†è§£ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.04724', 'title': 'LLMVoX: Autoregressive Streaming Text-to-Speech Model for Any LLM', 'url': 'https://huggingface.co/papers/2503.04724', 'abstract': 'Recent advancements in speech-to-speech dialogue systems leverage LLMs for multimodal interactions, yet they remain hindered by fine-tuning requirements, high computational overhead, and text-speech misalignment. Existing speech-enabled LLMs often degrade conversational quality by modifying the LLM, thereby compromising its linguistic capabilities. In contrast, we propose LLMVoX, a lightweight 30M-parameter, LLM-agnostic, autoregressive streaming TTS system that generates high-quality speech with low latency, while fully preserving the capabilities of the base LLM. Our approach achieves a significantly lower Word Error Rate compared to speech-enabled LLMs, while operating at comparable latency and UTMOS score. By decoupling speech synthesis from LLM processing via a multi-queue token streaming system, LLMVoX supports seamless, infinite-length dialogues. Its plug-and-play design also facilitates extension to various tasks with different backbones. Furthermore, LLMVoX generalizes to new languages with only dataset adaptation, attaining a low Character Error Rate on an Arabic speech task. Additionally, we have integrated LLMVoX with a Vision-Language Model to create an omni-model with speech, text, and vision capabilities, without requiring additional multimodal training. Our code base and project page is available at https://mbzuai-oryx.github.io/LLMVoX .', 'score': 41, 'issue_id': 2589, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 6', 'zh': '3æœˆ6æ—¥'}, 'hash': '3113e03ae6f4ed42', 'authors': ['Sambal Shikhar', 'Mohammed Irfan Kurpath', 'Sahal Shaji Mullappilly', 'Jean Lahoud', 'Fahad Khan', 'Rao Muhammad Anwer', 'Salman Khan', 'Hisham Cholakkal'], 'affiliations': ['LinkÃ¶ping University, Sweden', 'Mohamed Bin Zayed University of Artificial Intelligence (MBZUAI), UAE'], 'pdf_title_img': 'assets/pdf/title_img/2503.04724.jpg', 'data': {'categories': ['#multimodal', '#small_models', '#low_resource', '#audio', '#open_source', '#long_context'], 'emoji': 'ğŸ—£ï¸', 'ru': {'title': 'LLMVoX: Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ñ€ĞµÑ‡Ğ¸ Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'LLMVoX - ÑÑ‚Ğ¾ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ñ€ĞµÑ‡Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾ Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒÑ Ğ¸Ñ… Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸. ĞĞ½Ğ° Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ñ€ĞµÑ‡Ğ¸ Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ²ÑĞµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. LLMVoX Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ»Ğ¾Ğ² Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ€ĞµÑ‡ĞµĞ²Ñ‹Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ Ñ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ğ¾Ğ¹ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ»ĞµĞ³ĞºĞ¾ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ², Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ° Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸.'}, 'en': {'title': 'Seamless Speech Synthesis with LLMVoX', 'desc': 'The paper introduces LLMVoX, a novel speech-to-speech dialogue system that utilizes a lightweight, 30M-parameter architecture to enhance multimodal interactions without compromising the linguistic capabilities of large language models (LLMs). Unlike existing systems that require extensive fine-tuning and often degrade conversational quality, LLMVoX operates efficiently with low latency and a significantly reduced Word Error Rate. It employs a multi-queue token streaming mechanism to decouple speech synthesis from LLM processing, enabling seamless dialogues of infinite length. Additionally, LLMVoX can adapt to new languages with minimal dataset adjustments and integrates with Vision-Language Models to support speech, text, and vision functionalities without extra multimodal training.'}, 'zh': {'title': 'LLMVoXï¼šé«˜æ•ˆè¯­éŸ³åˆæˆçš„æ–°é€‰æ‹©', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºLLMVoXçš„è½»é‡çº§è¯­éŸ³åˆæˆç³»ç»Ÿï¼Œå…·æœ‰3000ä¸‡å‚æ•°ï¼Œèƒ½å¤Ÿä¸ä»»ä½•å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å…¼å®¹ã€‚LLMVoXé€šè¿‡å¤šé˜Ÿåˆ—ä»¤ç‰Œæµç³»ç»Ÿï¼Œå°†è¯­éŸ³åˆæˆä¸LLMå¤„ç†è§£è€¦ï¼Œä»è€Œå®ç°ä½å»¶è¿Ÿå’Œé«˜è´¨é‡çš„è¯­éŸ³ç”Ÿæˆã€‚ä¸ç°æœ‰çš„è¯­éŸ³å¢å¼ºLLMç›¸æ¯”ï¼ŒLLMVoXåœ¨è¯é”™è¯¯ç‡ä¸Šæ˜¾è‘—é™ä½ï¼ŒåŒæ—¶ä¿æŒç›¸ä¼¼çš„å»¶è¿Ÿå’Œç”¨æˆ·æ»¡æ„åº¦è¯„åˆ†ã€‚è¯¥ç³»ç»Ÿè¿˜æ”¯æŒæ— ç¼çš„æ— é™é•¿åº¦å¯¹è¯ï¼Œå¹¶èƒ½å¤Ÿé€šè¿‡æ•°æ®é›†é€‚åº”è½»æ¾æ‰©å±•åˆ°æ–°è¯­è¨€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.03803', 'title': 'EgoLife: Towards Egocentric Life Assistant', 'url': 'https://huggingface.co/papers/2503.03803', 'abstract': 'We introduce EgoLife, a project to develop an egocentric life assistant that accompanies and enhances personal efficiency through AI-powered wearable glasses. To lay the foundation for this assistant, we conducted a comprehensive data collection study where six participants lived together for one week, continuously recording their daily activities - including discussions, shopping, cooking, socializing, and entertainment - using AI glasses for multimodal egocentric video capture, along with synchronized third-person-view video references. This effort resulted in the EgoLife Dataset, a comprehensive 300-hour egocentric, interpersonal, multiview, and multimodal daily life dataset with intensive annotation. Leveraging this dataset, we introduce EgoLifeQA, a suite of long-context, life-oriented question-answering tasks designed to provide meaningful assistance in daily life by addressing practical questions such as recalling past relevant events, monitoring health habits, and offering personalized recommendations. To address the key technical challenges of (1) developing robust visual-audio models for egocentric data, (2) enabling identity recognition, and (3) facilitating long-context question answering over extensive temporal information, we introduce EgoButler, an integrated system comprising EgoGPT and EgoRAG. EgoGPT is an omni-modal model trained on egocentric datasets, achieving state-of-the-art performance on egocentric video understanding. EgoRAG is a retrieval-based component that supports answering ultra-long-context questions. Our experimental studies verify their working mechanisms and reveal critical factors and bottlenecks, guiding future improvements. By releasing our datasets, models, and benchmarks, we aim to stimulate further research in egocentric AI assistants.', 'score': 28, 'issue_id': 2581, 'pub_date': '2025-03-05', 'pub_date_card': {'ru': '5 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 5', 'zh': '3æœˆ5æ—¥'}, 'hash': '52396234365a3fb0', 'authors': ['Jingkang Yang', 'Shuai Liu', 'Hongming Guo', 'Yuhao Dong', 'Xiamengwei Zhang', 'Sicheng Zhang', 'Pengyun Wang', 'Zitang Zhou', 'Binzhu Xie', 'Ziyue Wang', 'Bei Ouyang', 'Zhengyu Lin', 'Marco Cominelli', 'Zhongang Cai', 'Yuanhan Zhang', 'Peiyuan Zhang', 'Fangzhou Hong', 'Joerg Widmer', 'Francesco Gringoli', 'Lei Yang', 'Bo Li', 'Ziwei Liu'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2503.03803.jpg', 'data': {'categories': ['#video', '#long_context', '#dataset', '#open_source', '#agents', '#multimodal', '#data', '#benchmark'], 'emoji': 'ğŸ‘“', 'ru': {'title': 'EgoLife: Ğ˜Ğ˜-Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²ÑĞµĞ´Ğ½ĞµĞ²Ğ½Ğ¾Ğ¹ Ğ¶Ğ¸Ğ·Ğ½Ğ¸ Ğ½Ğ° Ğ±Ğ°Ğ·Ğµ ÑƒĞ¼Ğ½Ñ‹Ñ… Ğ¾Ñ‡ĞºĞ¾Ğ²', 'desc': 'ĞŸÑ€Ğ¾ĞµĞºÑ‚ EgoLife Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ñ‡ĞºĞ¾Ğ² Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ½Ğ¾Ğ¹ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ¾Ğ¼. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑĞ¾Ğ±Ñ€Ğ°Ğ»Ğ¸ Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… EgoLife Dataset, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ 300 Ñ‡Ğ°ÑĞ¾Ğ² ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾Ğ²ÑĞµĞ´Ğ½ĞµĞ²Ğ½Ğ¾Ğ¹ Ğ¶Ğ¸Ğ·Ğ½Ğ¸ ÑˆĞµÑÑ‚Ğ¸ ÑƒÑ‡Ğ°ÑÑ‚Ğ½Ğ¸ĞºĞ¾Ğ². ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ·Ğ°Ğ´Ğ°Ñ‡ EgoLifeQA Ğ´Ğ»Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ¿Ğ¾Ğ²ÑĞµĞ´Ğ½ĞµĞ²Ğ½Ğ¾Ğ¹ Ğ¶Ğ¸Ğ·Ğ½Ğ¸. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° EgoButler, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ EgoGPT Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ EgoRAG.'}, 'en': {'title': 'Empowering Daily Life with Egocentric AI Assistance', 'desc': 'The paper presents EgoLife, an AI-powered life assistant designed to enhance personal efficiency through wearable glasses that capture egocentric video data. A comprehensive dataset, the EgoLife Dataset, was created by recording daily activities of participants over a week, resulting in 300 hours of multimodal data with detailed annotations. The study introduces EgoLifeQA, a set of question-answering tasks that utilize this dataset to assist users with practical life questions and personalized recommendations. To tackle challenges in visual-audio model development and long-context question answering, the authors propose EgoButler, which includes EgoGPT for video understanding and EgoRAG for retrieval-based answering.'}, 'zh': {'title': 'æ™ºèƒ½ç”Ÿæ´»åŠ©æ‰‹ï¼Œæå‡ä¸ªäººæ•ˆç‡', 'desc': 'æˆ‘ä»¬ä»‹ç»äº†EgoLifeé¡¹ç›®ï¼Œæ—¨åœ¨å¼€å‘ä¸€ä¸ªä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„ç”Ÿæ´»åŠ©æ‰‹ï¼Œé€šè¿‡AIé©±åŠ¨çš„å¯ç©¿æˆ´çœ¼é•œæå‡ä¸ªäººæ•ˆç‡ã€‚æˆ‘ä»¬è¿›è¡Œäº†å…¨é¢çš„æ•°æ®æ”¶é›†ç ”ç©¶ï¼Œå…­åå‚ä¸è€…å…±åŒç”Ÿæ´»ä¸€å‘¨ï¼Œä½¿ç”¨AIçœ¼é•œè®°å½•æ—¥å¸¸æ´»åŠ¨ï¼Œå½¢æˆäº†EgoLifeæ•°æ®é›†ï¼ŒåŒ…å«300å°æ—¶çš„å¤šè§†è§’ã€å¤šæ¨¡æ€æ—¥å¸¸ç”Ÿæ´»æ•°æ®ã€‚åŸºäºè¯¥æ•°æ®é›†ï¼Œæˆ‘ä»¬æ¨å‡ºäº†EgoLifeQAï¼Œä¸€ä¸ªé’ˆå¯¹ç”Ÿæ´»çš„é•¿æ–‡æœ¬é—®ç­”ä»»åŠ¡ï¼Œæ—¨åœ¨æä¾›å®ç”¨çš„æ—¥å¸¸ç”Ÿæ´»å¸®åŠ©ã€‚ä¸ºäº†è§£å†³å…³é”®æŠ€æœ¯æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†EgoButlerç³»ç»Ÿï¼ŒåŒ…æ‹¬EgoGPTå’ŒEgoRAGï¼Œå‰è€…åœ¨è‡ªæˆ‘ä¸­å¿ƒè§†é¢‘ç†è§£ä¸Šè¡¨ç°å‡ºè‰²ï¼Œåè€…æ”¯æŒè¶…é•¿æ–‡æœ¬é—®é¢˜çš„å›ç­”ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.02972', 'title': 'LINGOLY-TOO: Disentangling Memorisation from Reasoning with Linguistic Templatisation and Orthographic Obfuscation', 'url': 'https://huggingface.co/papers/2503.02972', 'abstract': 'Effective evaluation of the reasoning capabilities of large language models (LLMs) are susceptible to overestimation due to data exposure of evaluation benchmarks. We introduce a framework for producing linguistic reasoning problems that reduces the effect of memorisation in model performance estimates and apply this framework to develop LINGOLY-TOO, a challenging evaluation benchmark for linguistic reasoning. By developing orthographic templates, we dynamically obfuscate the writing systems of real languages to generate numerous question variations. These variations preserve the reasoning steps required for each solution while reducing the likelihood of specific problem instances appearing in model training data. Our experiments demonstrate that frontier models, including OpenAI o1-preview and DeepSeem R1, struggle with advanced reasoning. Our analysis also shows that LLMs exhibit noticeable variance in accuracy across permutations of the same problem, and on average perform better on questions appearing in their original orthography. Our findings highlight the opaque nature of response generation in LLMs and provide evidence that prior data exposure contributes to overestimating the reasoning capabilities of frontier models.', 'score': 22, 'issue_id': 2585, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 4', 'zh': '3æœˆ4æ—¥'}, 'hash': '0eb195e2704f3d5d', 'authors': ['Jude Khouja', 'Karolina Korgul', 'Simi Hellsten', 'Lingyi Yang', 'Vlad Neacs', 'Harry Mayne', 'Ryan Kearns', 'Andrew Bean', 'Adam Mahdi'], 'affiliations': ['Asia-Pacific Linguistics Olympiad', 'Hong Kong Linguistics Olympiad', 'National University of Science and Technology POLITEHNICA Bucharest, Romania', 'United Kingdom Linguistics Olympiad', 'University of Glasgow, Glasgow, United Kingdom', 'University of Oxford, Oxford, United Kingdom'], 'pdf_title_img': 'assets/pdf/title_img/2503.02972.jpg', 'data': {'categories': ['#reasoning', '#dataset', '#hallucinations', '#benchmark'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ LLM Ğ±ĞµĞ· Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº LINGOLY-TOO, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ Ğ¾Ğ±Ñ„ÑƒÑĞºĞ°Ñ†Ğ¸ĞµĞ¹ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¿Ğ¸ÑÑŒĞ¼Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ OpenAI Ğ¸ DeepSeem, Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ·Ğ½Ğ°ĞºĞ¾Ğ¼ÑÑ‚Ğ²Ğ¾ Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğº Ğ¿ĞµÑ€ĞµĞ¾Ñ†ĞµĞ½ĞºĞµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ LLM Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Unmasking Reasoning: Evaluating LLMs Beyond Memorization', 'desc': 'This paper addresses the challenge of accurately evaluating the reasoning abilities of large language models (LLMs) by introducing a new framework that minimizes the impact of memorization on performance assessments. The authors present LINGOLY-TOO, a benchmark designed to test linguistic reasoning through dynamically generated questions that obfuscate the original writing systems of languages. By using orthographic templates, they create multiple variations of questions that maintain the necessary reasoning steps while reducing the chance of models having seen specific instances during training. The results indicate that leading models struggle with complex reasoning tasks and show significant performance differences based on the question format, revealing the influence of prior data exposure on their evaluation.'}, 'zh': {'title': 'æ­ç¤ºå¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„çœŸå®é¢è²Œ', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¨ç†èƒ½åŠ›çš„æ–°æ¡†æ¶ï¼Œæ—¨åœ¨å‡å°‘ç”±äºæ•°æ®æš´éœ²å¯¼è‡´çš„è¯„ä¼°è¿‡é«˜çš„é—®é¢˜ã€‚æˆ‘ä»¬å¼€å‘äº†LINGOLY-TOOï¼Œè¿™æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„è¯­è¨€æ¨ç†è¯„ä¼°åŸºå‡†ï¼Œé€šè¿‡ä½¿ç”¨æ­£å­—æ³•æ¨¡æ¿åŠ¨æ€æ¨¡ç³ŠçœŸå®è¯­è¨€çš„ä¹¦å†™ç³»ç»Ÿï¼Œç”Ÿæˆå¤šç§é—®é¢˜å˜ä½“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå‰æ²¿æ¨¡å‹åœ¨é«˜çº§æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ï¼Œå¹¶ä¸”åœ¨ç›¸åŒé—®é¢˜çš„ä¸åŒæ’åˆ—ä¸­ï¼ŒLLMsçš„å‡†ç¡®æ€§å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚æˆ‘ä»¬çš„ç ”ç©¶æ­ç¤ºäº†LLMså“åº”ç”Ÿæˆçš„å¤æ‚æ€§ï¼Œå¹¶æä¾›äº†è¯æ®è¡¨æ˜ï¼Œå…ˆå‰çš„æ•°æ®æš´éœ²ä¼šå¯¼è‡´å¯¹å‰æ²¿æ¨¡å‹æ¨ç†èƒ½åŠ›çš„é«˜ä¼°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.04644', 'title': 'IFIR: A Comprehensive Benchmark for Evaluating Instruction-Following in Expert-Domain Information Retrieval', 'url': 'https://huggingface.co/papers/2503.04644', 'abstract': 'We introduce IFIR, the first comprehensive benchmark designed to evaluate instruction-following information retrieval (IR) in expert domains. IFIR includes 2,426 high-quality examples and covers eight subsets across four specialized domains: finance, law, healthcare, and science literature. Each subset addresses one or more domain-specific retrieval tasks, replicating real-world scenarios where customized instructions are critical. IFIR enables a detailed analysis of instruction-following retrieval capabilities by incorporating instructions at different levels of complexity. We also propose a novel LLM-based evaluation method to provide a more precise and reliable assessment of model performance in following instructions. Through extensive experiments on 15 frontier retrieval models, including those based on LLMs, our results reveal that current models face significant challenges in effectively following complex, domain-specific instructions. We further provide in-depth analyses to highlight these limitations, offering valuable insights to guide future advancements in retriever development.', 'score': 17, 'issue_id': 2585, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 6', 'zh': '3æœˆ6æ—¥'}, 'hash': 'c4f19dc17abc0e8f', 'authors': ['Tingyu Song', 'Guo Gan', 'Mingsheng Shang', 'Yilun Zhao'], 'affiliations': ['Chongqing Institute of Green and Intelligent Technology, Chinese Academy of Sciences', 'School of Advanced Interdisciplinary Sciences, University of Chinese Academy of Sciences', 'Yale University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.04644.jpg', 'data': {'categories': ['#science', '#healthcare', '#dataset', '#benchmark'], 'emoji': 'ğŸ”', 'ru': {'title': 'IFIR: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸', 'desc': 'IFIR - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ² ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 2426 Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ² 8 Ğ¿Ğ¾Ğ´Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ°Ñ… Ğ¸Ğ· 4 ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ¾Ğ¼ĞµĞ½Ğ¾Ğ²: Ñ„Ğ¸Ğ½Ğ°Ğ½ÑÑ‹, Ğ¿Ñ€Ğ°Ğ²Ğ¾, Ğ·Ğ´Ñ€Ğ°Ğ²Ğ¾Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ°Ñ Ğ»Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ğ°. IFIR Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° 15 Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ğ¾-ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹.'}, 'en': {'title': 'IFIR: Benchmarking Instruction-Following in Expert Domains', 'desc': 'The paper presents IFIR, a new benchmark for assessing how well information retrieval systems can follow instructions in specialized fields like finance, law, healthcare, and science. It consists of 2,426 examples that simulate real-world retrieval tasks requiring tailored instructions. The benchmark allows for a nuanced evaluation of retrieval models, particularly focusing on their ability to handle varying complexities of instructions. The authors also introduce a new evaluation method using large language models (LLMs) to measure performance, revealing that existing models struggle with complex, domain-specific tasks and providing insights for future improvements.'}, 'zh': {'title': 'IFIRï¼šä¸“å®¶é¢†åŸŸæŒ‡ä»¤è·Ÿéšæ£€ç´¢çš„é¦–ä¸ªåŸºå‡†', 'desc': 'æˆ‘ä»¬ä»‹ç»äº†IFIRï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå…¨é¢çš„åŸºå‡†ï¼Œç”¨äºè¯„ä¼°ä¸“å®¶é¢†åŸŸä¸­çš„æŒ‡ä»¤è·Ÿéšä¿¡æ¯æ£€ç´¢ï¼ˆIRï¼‰ã€‚IFIRåŒ…å«2426ä¸ªé«˜è´¨é‡ç¤ºä¾‹ï¼Œæ¶µç›–é‡‘èã€æ³•å¾‹ã€åŒ»ç–—å’Œç§‘å­¦æ–‡çŒ®ç­‰å››ä¸ªä¸“ä¸šé¢†åŸŸçš„å…«ä¸ªå­é›†ã€‚æ¯ä¸ªå­é›†é’ˆå¯¹ä¸€ä¸ªæˆ–å¤šä¸ªç‰¹å®šé¢†åŸŸçš„æ£€ç´¢ä»»åŠ¡ï¼Œæ¨¡æ‹Ÿäº†éœ€è¦å®šåˆ¶æŒ‡ä»¤çš„çœŸå®åœºæ™¯ã€‚é€šè¿‡å¯¹15ä¸ªå‰æ²¿æ£€ç´¢æ¨¡å‹çš„å¹¿æ³›å®éªŒï¼Œæˆ‘ä»¬çš„ç»“æœæ˜¾ç¤ºï¼Œå½“å‰æ¨¡å‹åœ¨æœ‰æ•ˆè·Ÿéšå¤æ‚çš„é¢†åŸŸç‰¹å®šæŒ‡ä»¤æ–¹é¢é¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.03983', 'title': 'Audio Flamingo 2: An Audio-Language Model with Long-Audio Understanding and Expert Reasoning Abilities', 'url': 'https://huggingface.co/papers/2503.03983', 'abstract': 'Understanding and reasoning over non-speech sounds and music are crucial for both humans and AI agents to interact effectively with their environments. In this paper, we introduce Audio Flamingo 2 (AF2), an Audio-Language Model (ALM) with advanced audio understanding and reasoning capabilities. AF2 leverages (i) a custom CLAP model, (ii) synthetic Audio QA data for fine-grained audio reasoning, and (iii) a multi-stage curriculum learning strategy. AF2 achieves state-of-the-art performance with only a 3B parameter small language model, surpassing large open-source and proprietary models across over 20 benchmarks. Next, for the first time, we extend audio understanding to long audio segments (30 secs to 5 mins) and propose LongAudio, a large and novel dataset for training ALMs on long audio captioning and question-answering tasks. Fine-tuning AF2 on LongAudio leads to exceptional performance on our proposed LongAudioBench, an expert annotated benchmark for evaluating ALMs on long audio understanding capabilities. We conduct extensive ablation studies to confirm the efficacy of our approach. Project Website: https://research.nvidia.com/labs/adlr/AF2/.', 'score': 17, 'issue_id': 2581, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 6', 'zh': '3æœˆ6æ—¥'}, 'hash': '952e4cb72ec34df8', 'authors': ['Sreyan Ghosh', 'Zhifeng Kong', 'Sonal Kumar', 'S Sakshi', 'Jaehyeon Kim', 'Wei Ping', 'Rafael Valle', 'Dinesh Manocha', 'Bryan Catanzaro'], 'affiliations': ['NVIDIA, Santa Clara, CA, USA', 'University of Maryland, College Park, MD, USA'], 'pdf_title_img': 'assets/pdf/title_img/2503.03983.jpg', 'data': {'categories': ['#audio', '#long_context', '#dataset', '#small_models', '#reasoning', '#synthetic', '#open_source', '#benchmark'], 'emoji': 'ğŸµ', 'ru': {'title': 'AF2: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ¾Ğ¼', 'desc': 'Audio Flamingo 2 (AF2) - ÑÑ‚Ğ¾ ÑƒÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ (ALM) Ñ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¾Ğ± Ğ°ÑƒĞ´Ğ¸Ğ¾. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ CLAP, ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Audio QA Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½Ñ‡Ğ°Ñ‚ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. AF2 Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² ÑÑ€ĞµĞ´Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 20 Ñ‚ĞµÑÑ‚Ğ°Ñ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²ÑĞµĞ³Ğ¾ 3 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ° Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ€Ğ°ÑÑˆĞ¸Ñ€Ğ¸Ğ»Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… LongAudio Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ALM Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¿Ğ¾ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ Ğ°ÑƒĞ´Ğ¸Ğ¾.'}, 'en': {'title': 'Revolutionizing Audio Understanding with Audio Flamingo 2', 'desc': 'This paper presents Audio Flamingo 2 (AF2), an advanced Audio-Language Model (ALM) designed to enhance audio understanding and reasoning. AF2 utilizes a custom CLAP model and synthetic Audio QA data to improve fine-grained audio reasoning, along with a multi-stage curriculum learning approach. The model achieves state-of-the-art results with a relatively small 3B parameter language model, outperforming larger models on over 20 benchmarks. Additionally, it introduces LongAudio, a new dataset for training on long audio segments, and demonstrates exceptional performance on the LongAudioBench for evaluating long audio understanding.'}, 'zh': {'title': 'éŸ³é¢‘ç†è§£çš„æ–°çªç ´ï¼šAudio Flamingo 2', 'desc': 'æœ¬æ–‡ä»‹ç»äº†Audio Flamingo 2ï¼ˆAF2ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å…·æœ‰å…ˆè¿›éŸ³é¢‘ç†è§£å’Œæ¨ç†èƒ½åŠ›çš„éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆALMï¼‰ã€‚AF2åˆ©ç”¨äº†å®šåˆ¶çš„CLAPæ¨¡å‹ã€åˆæˆçš„éŸ³é¢‘é—®ç­”æ•°æ®ä»¥åŠå¤šé˜¶æ®µçš„è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ã€‚AF2åœ¨ä»…ä½¿ç”¨ä¸€ä¸ª3Bå‚æ•°çš„å°å‹è¯­è¨€æ¨¡å‹çš„æƒ…å†µä¸‹ï¼Œè¶…è¶Šäº†20å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­çš„å¤§å‹å¼€æºå’Œä¸“æœ‰æ¨¡å‹ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒAF2é¦–æ¬¡æ‰©å±•äº†å¯¹é•¿éŸ³é¢‘ç‰‡æ®µï¼ˆ30ç§’åˆ°5åˆ†é’Ÿï¼‰çš„ç†è§£ï¼Œå¹¶æå‡ºäº†LongAudioæ•°æ®é›†ï¼Œç”¨äºè®­ç»ƒALMåœ¨é•¿éŸ³é¢‘æ ‡æ³¨å’Œé—®ç­”ä»»åŠ¡ä¸Šçš„èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.20258', 'title': 'LLM as a Broken Telephone: Iterative Generation Distorts Information', 'url': 'https://huggingface.co/papers/2502.20258', 'abstract': 'As large language models are increasingly responsible for online content, concerns arise about the impact of repeatedly processing their own outputs. Inspired by the "broken telephone" effect in chained human communication, this study investigates whether LLMs similarly distort information through iterative generation. Through translation-based experiments, we find that distortion accumulates over time, influenced by language choice and chain complexity. While degradation is inevitable, it can be mitigated through strategic prompting techniques. These findings contribute to discussions on the long-term effects of AI-mediated information propagation, raising important questions about the reliability of LLM-generated content in iterative workflows.', 'score': 17, 'issue_id': 2580, 'pub_date': '2025-02-27', 'pub_date_card': {'ru': '27 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 27', 'zh': '2æœˆ27æ—¥'}, 'hash': '5b26055854ae3d90', 'authors': ['Amr Mohamed', 'Mingmeng Geng', 'Michalis Vazirgiannis', 'Guokan Shang'], 'affiliations': ['Ecole Polytechnique', 'MBZUAI', 'SISSA'], 'pdf_title_img': 'assets/pdf/title_img/2502.20258.jpg', 'data': {'categories': ['#hallucinations', '#data', '#long_context', '#alignment', '#multimodal', '#training'], 'emoji': 'ğŸ“', 'ru': {'title': "Ğ­Ñ„Ñ„ĞµĞºÑ‚ 'Ğ¸ÑĞ¿Ğ¾Ñ€Ñ‡ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞ»ĞµÑ„Ğ¾Ğ½Ğ°' Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…", 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ° Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ¾Ğ², Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¾Ñ†ĞµĞ½Ğ¸Ñ‚ÑŒ Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ¸Ğµ Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° ÑĞ·Ñ‹ĞºĞ° Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ½ĞµĞ¸Ğ·Ğ±ĞµĞ¶Ğ½Ğ°, Ğ½Ğ¾ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ ÑĞ¼ÑĞ³Ñ‡ĞµĞ½Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ğ½Ğ¸Ğ¼Ğ°ĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¾ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ LLM Ğ² Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°Ñ….'}, 'en': {'title': 'Mitigating Distortion in Iterative LLM Outputs', 'desc': "This paper explores how large language models (LLMs) can distort information when they repeatedly process their own outputs, similar to the 'broken telephone' effect seen in human communication. The researchers conducted translation-based experiments to observe how information degradation occurs over time, which is affected by the choice of language and the complexity of the generation chain. They found that while some level of distortion is unavoidable, it can be reduced by using specific prompting strategies. These insights highlight the potential risks of relying on LLMs for generating content in iterative processes, raising concerns about the accuracy of AI-generated information."}, 'zh': {'title': 'æ¢è®¨å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¿¡æ¯æ‰­æ›²ä¸å¯é æ€§', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨åå¤å¤„ç†è‡ªèº«è¾“å‡ºæ—¶æ˜¯å¦ä¼šæ‰­æ›²ä¿¡æ¯ï¼Œç±»ä¼¼äºäººç±»æ²Ÿé€šä¸­çš„â€œç ´ç”µè¯â€æ•ˆåº”ã€‚é€šè¿‡åŸºäºç¿»è¯‘çš„å®éªŒï¼Œæˆ‘ä»¬å‘ç°ä¿¡æ¯æ‰­æ›²ä¼šéšç€æ—¶é—´çš„æ¨ç§»è€Œç´¯ç§¯ï¼Œå—è¯­è¨€é€‰æ‹©å’Œé“¾æ¡å¤æ‚æ€§çš„å½±å“ã€‚å°½ç®¡ä¿¡æ¯é€€åŒ–æ˜¯ä¸å¯é¿å…çš„ï¼Œä½†é€šè¿‡æˆ˜ç•¥æ€§æç¤ºæŠ€æœ¯å¯ä»¥å‡è½»è¿™ç§å½±å“ã€‚ç ”ç©¶ç»“æœä¸ºAIä»‹å¯¼çš„ä¿¡æ¯ä¼ æ’­çš„é•¿æœŸå½±å“æä¾›äº†é‡è¦è§è§£ï¼Œæå‡ºäº†å…³äºLLMç”Ÿæˆå†…å®¹åœ¨è¿­ä»£å·¥ä½œæµç¨‹ä¸­å¯é æ€§çš„é‡è¦é—®é¢˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.04725', 'title': 'L$^2$M: Mutual Information Scaling Law for Long-Context Language Modeling', 'url': 'https://huggingface.co/papers/2503.04725', 'abstract': "We rigorously establish a bipartite mutual information scaling law in natural language that governs long-range dependencies. This scaling law, which we show is distinct from and scales independently of the conventional two-point mutual information, is the key to understanding long-context language modeling. Using this scaling law, we formulate the Long-context Language Modeling (L^2M) condition, which relates a model's capacity for effective long context length modeling to the scaling of its latent state size for storing past information. Our results are validated through experiments on both transformers and state space models. This work establishes a theoretical foundation that guides the development of large language models toward longer context lengths.", 'score': 15, 'issue_id': 2582, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 6', 'zh': '3æœˆ6æ—¥'}, 'hash': '51e8f1332666da31', 'authors': ['Zhuo Chen', 'Oriol MaynÃ© i Comas', 'Zhuotao Jin', 'Di Luo', 'Marin SoljaÄiÄ‡'], 'affiliations': ['Harvard University', 'Massachusetts Institute of Technology', 'NSF AI Institute for Artificial Intelligence and Fundamental Interactions', 'Polytechnic University of Catalonia', 'University of California, Los Angeles'], 'pdf_title_img': 'assets/pdf/title_img/2503.04725.jpg', 'data': {'categories': ['#training', '#architecture', '#math', '#long_context'], 'emoji': 'ğŸ“', 'ru': {'title': 'ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ - ĞºĞ»ÑÑ‡ Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°, Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒÑÑ‰ĞµĞµÑÑ Ğ½Ğ° Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑÑ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒÑÑ‚ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğµ L^2M Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°, ÑĞ²ÑĞ·Ñ‹Ğ²Ğ°ÑÑ‰ĞµĞµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ĞµĞµ ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ñ‹ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°Ñ… Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ·Ğ°ĞºĞ»Ğ°Ğ´Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¾ÑĞ½Ğ¾Ğ²Ñƒ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ´Ğ»Ğ¸Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°.'}, 'en': {'title': 'Unlocking Long-Range Dependencies in Language Models', 'desc': "This paper introduces a new scaling law for bipartite mutual information that is crucial for understanding long-range dependencies in natural language processing. Unlike traditional two-point mutual information, this scaling law operates independently and is essential for effective long-context language modeling. The authors propose the Long-context Language Modeling (L^2M) condition, which connects a model's ability to handle long contexts with the size of its latent state for retaining past information. Experimental validation on transformers and state space models supports the theoretical framework, paving the way for advancements in large language models with extended context lengths."}, 'zh': {'title': 'é•¿ä¸Šä¸‹æ–‡å»ºæ¨¡çš„æ–°æ³•åˆ™', 'desc': 'æœ¬æ–‡ä¸¥è°¨åœ°å»ºç«‹äº†è‡ªç„¶è¯­è¨€ä¸­çš„åŒå‘äº’ä¿¡æ¯ç¼©æ”¾æ³•åˆ™ï¼Œè¯¥æ³•åˆ™æ§åˆ¶ç€é•¿è·ç¦»ä¾èµ–å…³ç³»ã€‚æˆ‘ä»¬å±•ç¤ºäº†è¿™ä¸€ç¼©æ”¾æ³•åˆ™ä¸ä¼ ç»Ÿçš„ä¸¤ç‚¹äº’ä¿¡æ¯ä¸åŒï¼Œå¹¶ä¸”ç‹¬ç«‹ç¼©æ”¾ï¼Œæ˜¯ç†è§£é•¿ä¸Šä¸‹æ–‡è¯­è¨€å»ºæ¨¡çš„å…³é”®ã€‚é€šè¿‡è¿™ä¸€ç¼©æ”¾æ³•åˆ™ï¼Œæˆ‘ä»¬æå‡ºäº†é•¿ä¸Šä¸‹æ–‡è¯­è¨€å»ºæ¨¡ï¼ˆL^2Mï¼‰æ¡ä»¶ï¼Œå°†æ¨¡å‹æœ‰æ•ˆå»ºæ¨¡é•¿ä¸Šä¸‹æ–‡é•¿åº¦çš„èƒ½åŠ›ä¸å…¶å­˜å‚¨è¿‡å»ä¿¡æ¯çš„æ½œåœ¨çŠ¶æ€å¤§å°çš„ç¼©æ”¾è”ç³»èµ·æ¥ã€‚æˆ‘ä»¬çš„ç»“æœé€šè¿‡å¯¹å˜æ¢å™¨å’ŒçŠ¶æ€ç©ºé—´æ¨¡å‹çš„å®éªŒå¾—åˆ°äº†éªŒè¯ï¼Œä¸ºå¤§å‹è¯­è¨€æ¨¡å‹å‘æ›´é•¿ä¸Šä¸‹æ–‡é•¿åº¦çš„å‘å±•å¥ å®šäº†ç†è®ºåŸºç¡€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.04598', 'title': 'HybridNorm: Towards Stable and Efficient Transformer Training via Hybrid Normalization', 'url': 'https://huggingface.co/papers/2503.04598', 'abstract': 'Transformers have become the de facto architecture for a wide range of machine learning tasks, particularly in large language models (LLMs). Despite their remarkable performance, challenges remain in training deep transformer networks, especially regarding the location of layer normalization. While Pre-Norm structures facilitate easier training due to their more prominent identity path, they often yield suboptimal performance compared to Post-Norm. In this paper, we propose HybridNorm, a straightforward yet effective hybrid normalization strategy that integrates the advantages of both Pre-Norm and Post-Norm approaches. Specifically, HybridNorm employs QKV normalization within the attention mechanism and Post-Norm in the feed-forward network (FFN) of each transformer block. This design not only stabilizes training but also enhances performance, particularly in the context of LLMs. Comprehensive experiments in both dense and sparse architectures show that HybridNorm consistently outperforms both Pre-Norm and Post-Norm approaches, achieving state-of-the-art results across various benchmarks. These findings highlight the potential of HybridNorm as a more stable and effective technique for improving the training and performance of deep transformer models. %Code will be made publicly available. Code is available at https://github.com/BryceZhuo/HybridNorm.', 'score': 15, 'issue_id': 2579, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 6', 'zh': '3æœˆ6æ—¥'}, 'hash': '78b05ed4e27a874b', 'authors': ['Zhijian Zhuo', 'Yutao Zeng', 'Ya Wang', 'Sijun Zhang', 'Jian Yang', 'Xiaoqing Li', 'Xun Zhou', 'Jinwen Ma'], 'affiliations': ['Capital University of Economics and Business', 'School of Mathematical Sciences, Peking University', 'SeedFoundation-Model, ByteDance'], 'pdf_title_img': 'assets/pdf/title_img/2503.04598.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#training', '#architecture', '#open_source'], 'emoji': 'ğŸ”€', 'ru': {'title': 'HybridNorm: Ğ“Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ°Ñ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ HybridNorm. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Pre-Norm Ğ¸ Post-Norm ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹, Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ QKV Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ² Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Post-Norm Ğ² FFN ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ±Ğ»Ğ¾ĞºĞ° Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°. HybridNorm Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ ĞºĞ°Ğº Ğ´Ğ»Ñ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ HybridNorm Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ², Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'HybridNorm: The Best of Both Normalization Worlds for Transformers', 'desc': 'This paper introduces HybridNorm, a new normalization strategy for training deep transformer networks, which combines the strengths of Pre-Norm and Post-Norm methods. Pre-Norm structures help with training stability but often lead to lower performance, while Post-Norm structures typically perform better but can complicate training. HybridNorm applies QKV normalization in the attention mechanism and Post-Norm in the feed-forward network, resulting in improved training stability and performance. Experiments demonstrate that HybridNorm outperforms both Pre-Norm and Post-Norm across various benchmarks, making it a promising approach for enhancing large language models.'}, 'zh': {'title': 'HybridNormï¼šæå‡å˜æ¢å™¨æ¨¡å‹è®­ç»ƒçš„ç¨³å®šæ€§ä¸æ€§èƒ½', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ··åˆå½’ä¸€åŒ–ç­–ç•¥ï¼Œç§°ä¸ºHybridNormï¼Œæ—¨åœ¨è§£å†³æ·±åº¦å˜æ¢å™¨ç½‘ç»œè®­ç»ƒä¸­çš„å±‚å½’ä¸€åŒ–ä½ç½®é—®é¢˜ã€‚HybridNormç»“åˆäº†Pre-Normå’ŒPost-Normçš„ä¼˜ç‚¹ï¼Œåœ¨æ³¨æ„åŠ›æœºåˆ¶ä¸­ä½¿ç”¨QKVå½’ä¸€åŒ–ï¼Œè€Œåœ¨å‰é¦ˆç½‘ç»œä¸­ä½¿ç”¨Post-Normã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒHybridNormåœ¨ç¨ å¯†å’Œç¨€ç–æ¶æ„ä¸­å‡ä¼˜äºä¼ ç»Ÿçš„Pre-Normå’ŒPost-Normæ–¹æ³•ï¼Œæå‡äº†å¤§è¯­è¨€æ¨¡å‹çš„è®­ç»ƒç¨³å®šæ€§å’Œæ€§èƒ½ã€‚è¯¥ç ”ç©¶ä¸ºæ·±åº¦å˜æ¢å™¨æ¨¡å‹çš„è®­ç»ƒå’Œæ€§èƒ½æ”¹è¿›æä¾›äº†æ–°çš„æ€è·¯ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.04222', 'title': 'FuseChat-3.0: Preference Optimization Meets Heterogeneous Model Fusion', 'url': 'https://huggingface.co/papers/2503.04222', 'abstract': 'We introduce FuseChat-3.0, a suite of large language models (LLMs) developed by integrating the strengths of heterogeneous source LLMs into more compact target LLMs. Our source models include the powerful Gemma-2-27B-it, Mistral-Large-Instruct-2407, Qwen-2.5-72B-Instruct, and Llama-3.1-70B-Instruct. For target models, we focus on three widely-used smaller variants-Llama-3.1-8B-Instruct, Gemma-2-9B-it, and Qwen-2.5-7B-Instruct-along with two ultra-compact options, Llama-3.2-3B-Instruct and Llama-3.2-1B-Instruct. To leverage the diverse capabilities of these source models, we develop a specialized data construction protocol tailored to various tasks and domains. The FuseChat-3.0 training pipeline consists of two key stages: (1) supervised fine-tuning (SFT) to align the target and source model distributions, and (2) Direct Preference Optimization (DPO) to apply preferences from multiple source LLMs to fine-tune the target model. The resulting FuseChat-3.0 models exhibit significant performance gains across tasks such as instruction following, general knowledge, mathematics, and coding. As illustrated in Figure 1, using Llama-3.1-8B-Instruct as the target model, our fusion approach achieves an average improvement of 6.8 points across 14 benchmarks. Moreover, it demonstrates remarkable gains of 37.1 points and 30.1 points on the instruction-following benchmarks AlpacaEval-2 and Arena-Hard, respectively. Our code, models, and datasets are available at https://github.com/SLIT-AI/FuseChat-3.0.', 'score': 11, 'issue_id': 2578, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 6', 'zh': '3æœˆ6æ—¥'}, 'hash': 'c7d793a0b91efd5d', 'authors': ['Ziyi Yang', 'Fanqi Wan', 'Longguang Zhong', 'Canbin Huang', 'Guosheng Liang', 'Xiaojun Quan'], 'affiliations': ['School of Computer Science and Engineering, Sun Yat-sen University, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.04222.jpg', 'data': {'categories': ['#training', '#dataset', '#benchmark', '#small_models', '#rlhf', '#transfer_learning', '#open_source', '#optimization'], 'emoji': 'ğŸ”€', 'ru': {'title': 'Ğ¡Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ¼Ğ¾Ñ‰Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾Ğ¼ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ', 'desc': 'FuseChat-3.0 Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM), Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½ Ğ³ĞµÑ‚ĞµÑ€Ğ¾Ğ³ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ñ… LLM Ğ² Ğ±Ğ¾Ğ»ĞµĞµ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğµ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞŸÑ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´Ğ²Ğ° ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ÑÑ‚Ğ°Ğ¿Ğ°: ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ°Ñ Ñ‚Ğ¾Ğ½ĞºĞ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€ÑĞ¼Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ FuseChat-3.0 Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼, Ğ¾Ğ±Ñ‰Ğ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ, Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºÑƒ Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Llama-3.1-8B-Instruct Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑÑ€ĞµĞ´Ğ½ĞµĞ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ° 6,8 Ğ¿ÑƒĞ½ĞºÑ‚Ğ¾Ğ² Ğ¿Ğ¾ 14 Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ğ¼.'}, 'en': {'title': 'Fusing Strengths for Smarter, Smaller Models', 'desc': 'FuseChat-3.0 is a collection of large language models (LLMs) that combines the strengths of various larger source models into smaller, more efficient target models. The training process involves supervised fine-tuning to align the target models with the source models, followed by Direct Preference Optimization to enhance performance based on preferences from multiple sources. This innovative approach leads to significant improvements in tasks like instruction following, general knowledge, mathematics, and coding. The results show an average performance boost of 6.8 points across 14 benchmarks, with particularly impressive gains in instruction-following tasks.'}, 'zh': {'title': 'èåˆå¤šæºæ¨¡å‹ï¼Œæå‡è¯­è¨€ç†è§£èƒ½åŠ›', 'desc': 'æˆ‘ä»¬ä»‹ç»äº†FuseChat-3.0ï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡æ•´åˆä¸åŒæ¥æºçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¼˜åŠ¿è€Œå¼€å‘çš„æ›´ç´§å‡‘çš„ç›®æ ‡LLMå¥—ä»¶ã€‚æºæ¨¡å‹åŒ…æ‹¬å¼ºå¤§çš„Gemma-2-27B-itã€Mistral-Large-Instruct-2407ã€Qwen-2.5-72B-Instructå’ŒLlama-3.1-70B-Instructã€‚ç›®æ ‡æ¨¡å‹åˆ™é›†ä¸­åœ¨ä¸‰ç§å¹¿æ³›ä½¿ç”¨çš„å°å‹å˜ä½“ä¸Šï¼Œä»¥åŠä¸¤ä¸ªè¶…ç´§å‡‘é€‰é¡¹ã€‚é€šè¿‡ä¸“é—¨çš„æ•°æ®æ„å»ºåè®®å’Œä¸¤é˜¶æ®µçš„è®­ç»ƒæµç¨‹ï¼ŒFuseChat-3.0åœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.04094', 'title': 'PokÃ©Champ: an Expert-level Minimax Language Agent', 'url': 'https://huggingface.co/papers/2503.04094', 'abstract': "We introduce Pok\\'eChamp, a minimax agent powered by Large Language Models (LLMs) for Pok\\'emon battles. Built on a general framework for two-player competitive games, Pok\\'eChamp leverages the generalist capabilities of LLMs to enhance minimax tree search. Specifically, LLMs replace three key modules: (1) player action sampling, (2) opponent modeling, and (3) value function estimation, enabling the agent to effectively utilize gameplay history and human knowledge to reduce the search space and address partial observability. Notably, our framework requires no additional LLM training. We evaluate Pok\\'eChamp in the popular Gen 9 OU format. When powered by GPT-4o, it achieves a win rate of 76% against the best existing LLM-based bot and 84% against the strongest rule-based bot, demonstrating its superior performance. Even with an open-source 8-billion-parameter Llama 3.1 model, Pok\\'eChamp consistently outperforms the previous best LLM-based bot, Pok\\'ellmon powered by GPT-4o, with a 64% win rate. Pok\\'eChamp attains a projected Elo of 1300-1500 on the Pok\\'emon Showdown online ladder, placing it among the top 30%-10% of human players. In addition, this work compiles the largest real-player Pok\\'emon battle dataset, featuring over 3 million games, including more than 500k high-Elo matches. Based on this dataset, we establish a series of battle benchmarks and puzzles to evaluate specific battling skills. We further provide key updates to the local game engine. We hope this work fosters further research that leverage Pok\\'emon battle as benchmark to integrate LLM technologies with game-theoretic algorithms addressing general multiagent problems. Videos, code, and dataset available at https://sites.google.com/view/pokechamp-llm.", 'score': 9, 'issue_id': 2580, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 6', 'zh': '3æœˆ6æ—¥'}, 'hash': 'bffe348d8443d4c2', 'authors': ['Seth Karten', 'Andy Luu Nguyen', 'Chi Jin'], 'affiliations': ['Department of Computer Science, Princeton University', 'Department of Electrical and Computer Engineering, Princeton University'], 'pdf_title_img': 'assets/pdf/title_img/2503.04094.jpg', 'data': {'categories': ['#open_source', '#benchmark', '#dataset', '#agents', '#games'], 'emoji': 'ğŸ®', 'ru': {'title': 'PokeChamp: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ˜Ğ˜ Ğ´Ğ»Ñ Ğ¸Ğ³Ñ€Ğ¾Ğ²Ñ‹Ñ… ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ PokeChamp - Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°ĞºÑĞ°, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰ĞµĞ³Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ğ´Ğ»Ñ Ğ±Ğ¾ĞµĞ² Ğ² PokÃ©mon. PokeChamp Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ LLM Ğ´Ğ»Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸Ğ³Ñ€Ğ¾ĞºĞ°, Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ¿Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ° Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ñ†ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ Ğ¸Ğ³Ñ€Ñ‹ Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°. ĞŸÑ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ GPT-4o Ğ°Ğ³ĞµĞ½Ñ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚Ğ° Ğ¿Ğ¾Ğ±ĞµĞ´ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ² ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ¸ Ğ¿Ğ¾Ğ¿Ğ°Ğ´Ğ°ĞµÑ‚ Ğ² Ñ‚Ğ¾Ğ¿-30% - 10% Ğ¸Ğ³Ñ€Ğ¾ĞºĞ¾Ğ² Ğ½Ğ° Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğµ PokÃ©mon Showdown. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ ĞºÑ€ÑƒĞ¿Ğ½ĞµĞ¹ÑˆĞµĞ³Ğ¾ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ğ±Ğ¾ĞµĞ² PokÃ©mon Ğ¸ ÑĞµÑ€Ğ¸Ğ¸ Ñ‚ĞµÑÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ñ.'}, 'en': {'title': "Pok'eChamp: Elevating PokÃ©mon Battles with LLMs", 'desc': "Pok'eChamp is a minimax agent designed for PokÃ©mon battles that utilizes Large Language Models (LLMs) to improve its decision-making process. It replaces traditional components of the minimax algorithm with LLMs for player action sampling, opponent modeling, and value function estimation, allowing it to better understand gameplay history and human strategies. The agent demonstrates impressive performance, achieving a 76% win rate against top LLM-based bots and a 64% win rate with a smaller model, showcasing its effectiveness in competitive play. Additionally, Pok'eChamp compiles a large dataset of over 3 million PokÃ©mon battles to create benchmarks for evaluating battling skills and encourages further research in integrating LLMs with game-theoretic approaches."}, 'zh': {'title': 'PokÃ©Champï¼šå®å¯æ¢¦å¯¹æˆ˜ä¸­çš„æ™ºèƒ½ä»£ç†', 'desc': 'æœ¬æ–‡ä»‹ç»äº†PokÃ©Champï¼Œä¸€ä¸ªåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æå°æå¤§ç®—æ³•ä»£ç†ï¼Œç”¨äºå®å¯æ¢¦å¯¹æˆ˜ã€‚è¯¥ä»£ç†åˆ©ç”¨LLMsçš„é€šç”¨èƒ½åŠ›ï¼Œå¢å¼ºäº†æå°æå¤§æ ‘æœç´¢ï¼Œæ›¿ä»£äº†ç©å®¶åŠ¨ä½œé‡‡æ ·ã€å¯¹æ‰‹å»ºæ¨¡å’Œä»·å€¼å‡½æ•°ä¼°è®¡ç­‰å…³é”®æ¨¡å—ã€‚PokÃ©Champåœ¨ä¸éœ€è¦é¢å¤–è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåˆ©ç”¨æ¸¸æˆå†å²å’Œäººç±»çŸ¥è¯†ï¼Œå‡å°‘æœç´¢ç©ºé—´å¹¶è§£å†³éƒ¨åˆ†å¯è§‚æµ‹æ€§é—®é¢˜ã€‚ç»è¿‡è¯„ä¼°ï¼ŒPokÃ©Champåœ¨å®å¯æ¢¦å¯¹æˆ˜ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œèµ¢å¾—äº†76%çš„èƒœç‡ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤šæ™ºèƒ½ä½“é—®é¢˜ä¸­çš„æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.01917', 'title': 'How to Steer LLM Latents for Hallucination Detection?', 'url': 'https://huggingface.co/papers/2503.01917', 'abstract': "Hallucinations in LLMs pose a significant concern to their safe deployment in real-world applications. Recent approaches have leveraged the latent space of LLMs for hallucination detection, but their embeddings, optimized for linguistic coherence rather than factual accuracy, often fail to clearly separate truthful and hallucinated content. To this end, we propose the Truthfulness Separator Vector (TSV), a lightweight and flexible steering vector that reshapes the LLM's representation space during inference to enhance the separation between truthful and hallucinated outputs, without altering model parameters. Our two-stage framework first trains TSV on a small set of labeled exemplars to form compact and well-separated clusters. It then augments the exemplar set with unlabeled LLM generations, employing an optimal transport-based algorithm for pseudo-labeling combined with a confidence-based filtering process. Extensive experiments demonstrate that TSV achieves state-of-the-art performance with minimal labeled data, exhibiting strong generalization across datasets and providing a practical solution for real-world LLM applications.", 'score': 8, 'issue_id': 2592, 'pub_date': '2025-03-01', 'pub_date_card': {'ru': '1 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 1', 'zh': '3æœˆ1æ—¥'}, 'hash': 'cbd43445771d58c3', 'authors': ['Seongheon Park', 'Xuefeng Du', 'Min-Hsuan Yeh', 'Haobo Wang', 'Yixuan Li'], 'affiliations': ['Department of Computer Sciences, University of Wisconsin-Madison, USA', 'School of Software Technology, Zhejiang University, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.01917.jpg', 'data': {'categories': ['#training', '#inference', '#hallucinations'], 'emoji': 'ğŸ”', 'ru': {'title': 'TSV: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ´Ñ‹ Ğ¸ Ğ²Ñ‹Ğ¼Ñ‹ÑĞ»Ğ° Ğ² LLM', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM) - Truthfulness Separator Vector (TSV). TSV - ÑÑ‚Ğ¾ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğ¹ Ğ²ĞµĞºÑ‚Ğ¾Ñ€, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸Ğ·Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ LLM Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¸Ğ²Ñ‹Ğ¼ Ğ¸ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸ĞµĞ¼. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° TSV Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ², Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¿ÑĞµĞ²Ğ´Ğ¾-Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ‚Ñ€Ğ°Ğ½ÑĞ¿Ğ¾Ñ€Ñ‚Ğ° Ğ´Ğ»Ñ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ TSV Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Enhancing Truthfulness in LLMs with TSV', 'desc': 'This paper addresses the issue of hallucinations in large language models (LLMs), which can lead to the generation of false information. The authors introduce the Truthfulness Separator Vector (TSV), a novel method that modifies the representation space of LLMs during inference to better distinguish between accurate and hallucinated content. TSV is trained using a small set of labeled examples and then enhanced with additional unlabeled data through a unique pseudo-labeling technique. The results show that TSV significantly improves the accuracy of LLM outputs while requiring minimal labeled data, making it a valuable tool for safe LLM deployment.'}, 'zh': {'title': 'æå‡LLMçœŸå®ä¸å¹»è§‰å†…å®¹åˆ†ç¦»çš„çœŸç›¸åˆ†ç¦»å‘é‡', 'desc': 'åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­ï¼Œå¹»è§‰ç°è±¡å¯¹å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„å®‰å…¨éƒ¨ç½²æ„æˆäº†é‡å¤§æŒ‘æˆ˜ã€‚è™½ç„¶æœ€è¿‘çš„æ–¹æ³•åˆ©ç”¨äº†LLMsçš„æ½œåœ¨ç©ºé—´è¿›è¡Œå¹»è§‰æ£€æµ‹ï¼Œä½†å…¶åµŒå…¥é€šå¸¸æ›´æ³¨é‡è¯­è¨€è¿è´¯æ€§è€Œéäº‹å®å‡†ç¡®æ€§ï¼Œå¯¼è‡´éš¾ä»¥æ¸…æ™°åŒºåˆ†çœŸå®å’Œå¹»è§‰å†…å®¹ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†çœŸç›¸åˆ†ç¦»å‘é‡ï¼ˆTSVï¼‰ï¼Œè¿™æ˜¯ä¸€ç§è½»é‡ä¸”çµæ´»çš„å¼•å¯¼å‘é‡ï¼Œå¯ä»¥åœ¨æ¨ç†è¿‡ç¨‹ä¸­é‡å¡‘LLMçš„è¡¨ç¤ºç©ºé—´ï¼Œä»è€Œå¢å¼ºçœŸå®è¾“å‡ºä¸å¹»è§‰è¾“å‡ºä¹‹é—´çš„åˆ†ç¦»ã€‚æˆ‘ä»¬çš„ä¸¤é˜¶æ®µæ¡†æ¶é¦–å…ˆåœ¨å°è§„æ¨¡æ ‡è®°æ ·æœ¬ä¸Šè®­ç»ƒTSVï¼Œä»¥å½¢æˆç´§å‡‘ä¸”åˆ†ç¦»è‰¯å¥½çš„èšç±»ï¼Œç„¶åé€šè¿‡åŸºäºæœ€ä¼˜ä¼ è¾“çš„ä¼ªæ ‡è®°ç®—æ³•å’ŒåŸºäºç½®ä¿¡åº¦çš„è¿‡æ»¤è¿‡ç¨‹ï¼Œåˆ©ç”¨æœªæ ‡è®°çš„LLMç”Ÿæˆæ•°æ®æ¥æ‰©å±•æ ·æœ¬é›†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.02495', 'title': 'Union of Experts: Adapting Hierarchical Routing to Equivalently Decomposed Transformer', 'url': 'https://huggingface.co/papers/2503.02495', 'abstract': "Mixture-of-Experts (MoE) enhances model performance while maintaining computational efficiency, making it well-suited for large-scale applications. However, expert in exist MoE paradigm works as an individual, thereby lacking high-quality expert interactions. Moreover, they have not been effectively extended to attention block, which constrains further efficiency improvements. To tackle these issues, we propose Union-of-Experts (UoE), which decomposes transformer into an equitant group of experts, and then implement dynamic routing on input data and experts. Our approach advances MoE design with three key innovations: (1) We conducted equitant expert decomposition on both MLP blocks and attention blocks based on matrix partition in tensor parallelism. (2) We developed two routing paradigms: patch wise data selection and expert selection, to apply routing across different levels. (3) We design the architecture of UoE model, including Selective Multi-Head Attention (SMHA) and Union-of-MLP-Experts (UoME). (4) We develop parallel implementation of UoE's routing and computation operation, and optimize efficiency based on the hardware processing analysis. The experiments demonstrate that the model employed with UoE surpass Full Attention, state-of-art MoEs and efficient transformers in several tasks across image and natural language domains. The source codes are available at https://github.com/YujiaoYang-work/UoE.", 'score': 7, 'issue_id': 2586, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 4', 'zh': '3æœˆ4æ—¥'}, 'hash': 'b879dd88adfef125', 'authors': ['Yujiao Yang', 'Jing Lian', 'Linhui Li'], 'affiliations': ['School of Mechanical Engineering, Dalian University of Technology, Dalian 116024, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.02495.jpg', 'data': {'categories': ['#training', '#optimization', '#architecture', '#open_source'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'UoE: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ², Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ Union-of-Experts (UoE). UoE ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Mixture-of-Experts (MoE), Ñ€Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°Ñ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ Ğ½Ğ° Ğ³Ñ€ÑƒĞ¿Ğ¿Ñ‹ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ĞºĞ°Ğº Ğº Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼, Ñ‚Ğ°Ğº Ğ¸ Ğº ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ğ¼. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ±Ğ»Ğ¾ĞºĞ¾Ğ² MLP Ğ¸ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñ‹ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½ÑƒÑ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ UoE Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°.'}, 'en': {'title': 'Union-of-Experts: Enhancing Efficiency and Interaction in Transformers', 'desc': 'The paper introduces Union-of-Experts (UoE), a novel approach that enhances the Mixture-of-Experts (MoE) framework by improving expert interactions and extending its application to attention blocks. UoE decomposes transformers into groups of experts and utilizes dynamic routing to optimize input data processing. Key innovations include equitant expert decomposition, two routing paradigms for data and expert selection, and a new architecture featuring Selective Multi-Head Attention and Union-of-MLP-Experts. Experimental results show that UoE outperforms existing models in various tasks, demonstrating its effectiveness in both image and natural language processing.'}, 'zh': {'title': 'ä¸“å®¶è”åˆæ¨¡å‹ï¼šæå‡æ•ˆç‡ä¸æ€§èƒ½çš„åˆ›æ–°ä¹‹è·¯', 'desc': 'æ··åˆä¸“å®¶æ¨¡å‹ï¼ˆMoEï¼‰é€šè¿‡æé«˜æ¨¡å‹æ€§èƒ½å¹¶ä¿æŒè®¡ç®—æ•ˆç‡ï¼Œé€‚åˆå¤§è§„æ¨¡åº”ç”¨ã€‚ç„¶è€Œï¼Œç°æœ‰çš„MoEæ¨¡å‹ä¸­çš„ä¸“å®¶ä½œä¸ºä¸ªä½“å·¥ä½œï¼Œç¼ºä¹é«˜è´¨é‡çš„ä¸“å®¶äº¤äº’ã€‚æ­¤å¤–ï¼ŒMoEå°šæœªæœ‰æ•ˆæ‰©å±•åˆ°æ³¨æ„åŠ›æ¨¡å—ï¼Œè¿™é™åˆ¶äº†è¿›ä¸€æ­¥çš„æ•ˆç‡æå‡ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸“å®¶è”åˆæ¨¡å‹ï¼ˆUoEï¼‰ï¼Œé€šè¿‡å°†å˜æ¢å™¨åˆ†è§£ä¸ºç­‰æ•ˆçš„ä¸“å®¶ç»„ï¼Œå¹¶åœ¨è¾“å…¥æ•°æ®å’Œä¸“å®¶ä¹‹é—´å®ç°åŠ¨æ€è·¯ç”±ï¼Œä»è€Œæ”¹è¿›äº†MoEçš„è®¾è®¡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.01901', 'title': 'Identifying Sensitive Weights via Post-quantization Integral', 'url': 'https://huggingface.co/papers/2503.01901', 'abstract': "Serving Large Language Models (LLMs) is costly. However, post-training weight quantization can address this problem by both compressing their sizes for limited memory and saving bandwidth for acceleration. As not all weight dimensions are equally important, those methods typically rely on a sensitivity metric, which indicates the element-wise influence of weights on loss function and is used to preprocess original weights for better quantization. In this work, we conduct an empirical study on the accuracy of the sensitivity metric, and find that existing gradient and Hessian based metrics are very inaccurate: they underestimate quantization's impact on the loss function by orders of magnitude, mainly due to the small convergence radius of local 2nd order approximation, \\ie, gradient and Hessian term in Taylor's formula. To tackle this problem, we propose Post-quantization Integral (PQI), an accurate metric to estimate posterior sensitivity in a fine-grained manner. To leverage this accurate metric, we further propose ReQuant, a simple yet powerful framework that mainly consists of two Dense-and-Sparse detach components: self-adaptive outlier selection and step-wise significant weights detach. Results show that ReQuant boosts state-of-the-art post-training quantization methods, with a pronounced improvement of 2.66 perplexity gain on Llama 3.2 1B with QTIP.", 'score': 7, 'issue_id': 2585, 'pub_date': '2025-02-28', 'pub_date_card': {'ru': '28 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 28', 'zh': '2æœˆ28æ—¥'}, 'hash': '1045983ed982a00b', 'authors': ['Yuezhou Hu', 'Weiyu Huang', 'Zichen Liang', 'Chang Chen', 'Jintao Zhang', 'Jun Zhu', 'Jianfei Chen'], 'affiliations': ['Dept. of Comp. Sci. & Tech., Institute for AI, BNRist Center, Tsinghua-Bosch Joint ML Center, THBI Lab, Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.01901.jpg', 'data': {'categories': ['#training', '#inference', '#optimization'], 'emoji': 'âš–ï¸', 'ru': {'title': 'Ğ¢Ğ¾Ñ‡Ğ½Ğ°Ñ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²ĞµÑĞ¾Ğ² Ğ´Ğ»Ñ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM), Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ ReQuant. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ²ĞµÑĞ¾Ğ² Ğ½ĞµÑ‚Ğ¾Ñ‡Ğ½Ñ‹ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½ÑƒÑ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ - Post-quantization Integral (PQI). ReQuant Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ PQI Ğ²Ğ¼ĞµÑÑ‚Ğµ Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ğ¼ Ğ²Ñ‹Ğ±Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğ¼ Ğ¾Ñ‚Ğ´ĞµĞ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ğ¼Ñ‹Ñ… Ğ²ĞµÑĞ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ReQuant ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ÑÑ‚-Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Llama 3.2 1B.'}, 'en': {'title': 'Enhancing LLM Efficiency with Accurate Quantization Metrics', 'desc': 'This paper addresses the high costs associated with serving large language models (LLMs) by introducing post-training weight quantization techniques. It highlights the inadequacy of existing sensitivity metrics, which fail to accurately predict the impact of quantization on model performance. The authors propose a new metric called Post-quantization Integral (PQI) that provides a more precise estimation of weight sensitivity. Additionally, they introduce ReQuant, a framework that enhances quantization methods by effectively selecting significant weights and improving overall model accuracy.'}, 'zh': {'title': 'æå‡é‡åŒ–ç²¾åº¦ï¼Œé™ä½æ¨¡å‹æˆæœ¬', 'desc': 'è¿™ç¯‡è®ºæ–‡è®¨è®ºäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æœåŠ¡æ—¶çš„é«˜æˆæœ¬é—®é¢˜ã€‚é€šè¿‡åè®­ç»ƒæƒé‡é‡åŒ–ï¼Œå¯ä»¥å‹ç¼©æ¨¡å‹å¤§å°ï¼ŒèŠ‚çœå†…å­˜å’Œå¸¦å®½ã€‚ç ”ç©¶å‘ç°ï¼Œç°æœ‰çš„åŸºäºæ¢¯åº¦å’Œæµ·æ£®çŸ©é˜µçš„æ•æ„Ÿåº¦åº¦é‡ä¸å¤Ÿå‡†ç¡®ï¼Œä½ä¼°äº†é‡åŒ–å¯¹æŸå¤±å‡½æ•°çš„å½±å“ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæå‡ºäº†åé‡åŒ–ç§¯åˆ†ï¼ˆPQIï¼‰ä½œä¸ºä¸€ç§æ›´ç²¾ç¡®çš„æ•æ„Ÿåº¦åº¦é‡ï¼Œå¹¶è¿›ä¸€æ­¥æå‡ºäº†ReQuantæ¡†æ¶ï¼Œä»¥æé«˜åè®­ç»ƒé‡åŒ–æ–¹æ³•çš„æ•ˆæœã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.04606', 'title': 'The Best of Both Worlds: Integrating Language Models and Diffusion Models for Video Generation', 'url': 'https://huggingface.co/papers/2503.04606', 'abstract': 'Recent advancements in text-to-video (T2V) generation have been driven by two competing paradigms: autoregressive language models and diffusion models. However, each paradigm has intrinsic limitations: language models struggle with visual quality and error accumulation, while diffusion models lack semantic understanding and causal modeling. In this work, we propose LanDiff, a hybrid framework that synergizes the strengths of both paradigms through coarse-to-fine generation. Our architecture introduces three key innovations: (1) a semantic tokenizer that compresses 3D visual features into compact 1D discrete representations through efficient semantic compression, achieving a sim14,000times compression ratio; (2) a language model that generates semantic tokens with high-level semantic relationships; (3) a streaming diffusion model that refines coarse semantics into high-fidelity videos. Experiments show that LanDiff, a 5B model, achieves a score of 85.43 on the VBench T2V benchmark, surpassing the state-of-the-art open-source models Hunyuan Video (13B) and other commercial models such as Sora, Keling, and Hailuo. Furthermore, our model also achieves state-of-the-art performance in long video generation, surpassing other open-source models in this field. Our demo can be viewed at https://landiff.github.io/.', 'score': 7, 'issue_id': 2580, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 6', 'zh': '3æœˆ6æ—¥'}, 'hash': 'c635690f3edc1186', 'authors': ['Aoxiong Yin', 'Kai Shen', 'Yichong Leng', 'Xu Tan', 'Xinyu Zhou', 'Juncheng Li', 'Siliang Tang'], 'affiliations': ['College of Computer Science and Technology, Zhejiang University, Hangzhou, China', 'Moonshot AI, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.04606.jpg', 'data': {'categories': ['#diffusion', '#open_source', '#benchmark', '#long_context', '#architecture', '#video'], 'emoji': 'ğŸ¬', 'ru': {'title': 'LanDiff: Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ LanDiff - Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰ÑƒÑ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. LanDiff Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€, ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²ÑƒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ LanDiff Ğ¾Ğ±ÑŠĞµĞ¼Ğ¾Ğ¼ 5 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾ÑˆĞ»Ğ° ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¸ ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ VBench T2V. Ğ¢Ğ°ĞºĞ¶Ğµ LanDiff Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸.'}, 'en': {'title': 'LanDiff: Bridging Language and Visuals for Superior Video Generation', 'desc': 'This paper introduces LanDiff, a novel framework for text-to-video (T2V) generation that combines the strengths of autoregressive language models and diffusion models. It addresses the limitations of each approach by using a coarse-to-fine generation strategy, which enhances both visual quality and semantic understanding. Key innovations include a semantic tokenizer for efficient compression of visual features, a language model that generates meaningful semantic tokens, and a streaming diffusion model that refines these tokens into high-quality videos. The results demonstrate that LanDiff outperforms existing state-of-the-art models in T2V tasks, particularly in generating long videos.'}, 'zh': {'title': 'LanDiffï¼šæ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆçš„æ–°çªç ´', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºLanDiffçš„æ··åˆæ¡†æ¶ï¼Œæ—¨åœ¨ç»“åˆè‡ªå›å½’è¯­è¨€æ¨¡å‹å’Œæ‰©æ•£æ¨¡å‹çš„ä¼˜ç‚¹ï¼Œä»¥å®ç°æ–‡æœ¬åˆ°è§†é¢‘çš„ç”Ÿæˆã€‚è¯¥æ¡†æ¶é€šè¿‡ç²—åˆ°ç»†çš„ç”Ÿæˆè¿‡ç¨‹ï¼Œå…‹æœäº†å„è‡ªçš„å±€é™æ€§ï¼Œæå‡äº†è§†è§‰è´¨é‡å’Œè¯­ä¹‰ç†è§£ã€‚LanDiffå¼•å…¥äº†ä¸‰é¡¹å…³é”®åˆ›æ–°ï¼ŒåŒ…æ‹¬é«˜æ•ˆçš„è¯­ä¹‰å‹ç¼©æŠ€æœ¯ã€ç”Ÿæˆé«˜å±‚è¯­ä¹‰å…³ç³»çš„è¯­è¨€æ¨¡å‹ï¼Œä»¥åŠå°†ç²—ç•¥è¯­ä¹‰ç²¾ç‚¼ä¸ºé«˜ä¿çœŸè§†é¢‘çš„æµå¼æ‰©æ•£æ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLanDiffåœ¨VBench T2VåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†ç°æœ‰çš„å¼€æºå’Œå•†ä¸šæ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.04378', 'title': 'Dedicated Feedback and Edit Models Empower Inference-Time Scaling for Open-Ended General-Domain Tasks', 'url': 'https://huggingface.co/papers/2503.04378', 'abstract': 'Inference-Time Scaling has been critical to the success of recent models such as OpenAI o1 and DeepSeek R1. However, many techniques used to train models for inference-time scaling require tasks to have answers that can be verified, limiting their application to domains such as math, coding and logical reasoning. We take inspiration from how humans make first attempts, ask for detailed feedback from others and make improvements based on such feedback across a wide spectrum of open-ended endeavors. To this end, we collect data for and train dedicated Feedback and Edit Models that are capable of performing inference-time scaling for open-ended general-domain tasks. In our setup, one model generates an initial response, which are given feedback by a second model, that are then used by a third model to edit the response. We show that performance on Arena Hard, a benchmark strongly predictive of Chatbot Arena Elo can be boosted by scaling the number of initial response drafts, effective feedback and edited responses. When scaled optimally, our setup based on 70B models from the Llama 3 family can reach SoTA performance on Arena Hard at 92.7 as of 5 Mar 2025, surpassing OpenAI o1-preview-2024-09-12 with 90.4 and DeepSeek R1 with 92.3.', 'score': 6, 'issue_id': 2578, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 6', 'zh': '3æœˆ6æ—¥'}, 'hash': '926e56aa51a0fefe', 'authors': ['Zhilin Wang', 'Jiaqi Zeng', 'Olivier Delalleau', 'Daniel Egert', 'Ellie Evans', 'Hoo-Chang Shin', 'Felipe Soares', 'Yi Dong', 'Oleksii Kuchaiev'], 'affiliations': ['NVIDIA'], 'pdf_title_img': 'assets/pdf/title_img/2503.04378.jpg', 'data': {'categories': ['#training', '#benchmark', '#inference', '#reasoning', '#rlhf', '#optimization'], 'emoji': 'ğŸ”„', 'ru': {'title': 'ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğµ Ğ´Ğ»Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡: Ğ¾Ñ‚ Ñ‡ĞµÑ€Ğ½Ğ¾Ğ²Ğ¸ĞºĞ° Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñƒ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¸Ğ· Ñ‚Ñ€ĞµÑ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: Ğ¾Ğ´Ğ½Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚, Ğ²Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ´Ğ°ĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ, Ğ° Ñ‚Ñ€ĞµÑ‚ÑŒÑ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ Arena Hard. ĞŸÑ€Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ° Llama 3 Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ 70B Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ², Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ OpenAI o1 Ğ¸ DeepSeek R1.'}, 'en': {'title': 'Enhancing Open-Ended Task Performance through Feedback and Editing', 'desc': 'This paper discusses a novel approach to improve inference-time scaling in machine learning models, particularly for open-ended tasks. It introduces a system where one model generates an initial response, a second model provides feedback, and a third model edits the response based on that feedback. This method allows for more flexible applications beyond traditional tasks that require verifiable answers, such as math or coding. The authors demonstrate that by optimizing the number of drafts, feedback, and edits, their model achieves state-of-the-art performance on the Arena Hard benchmark, outperforming previous models.'}, 'zh': {'title': 'æ¨ç†æ—¶é—´æ‰©å±•ï¼šæå‡å¼€æ”¾æ€§ä»»åŠ¡çš„æ€§èƒ½', 'desc': 'æœ¬æ–‡æ¢è®¨äº†æ¨ç†æ—¶é—´æ‰©å±•åœ¨æœºå™¨å­¦ä¹ æ¨¡å‹ä¸­çš„é‡è¦æ€§ï¼Œå°¤å…¶æ˜¯OpenAI o1å’ŒDeepSeek R1ç­‰æ¨¡å‹ã€‚è®¸å¤šç°æœ‰æŠ€æœ¯è¦æ±‚ä»»åŠ¡çš„ç­”æ¡ˆå¯éªŒè¯ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨å¼€æ”¾æ€§ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚æˆ‘ä»¬å€Ÿé‰´äººç±»å¦‚ä½•è¿›è¡Œåˆæ­¥å°è¯•ã€è¯·æ±‚åé¦ˆå¹¶æ ¹æ®åé¦ˆè¿›è¡Œæ”¹è¿›çš„è¿‡ç¨‹ï¼Œå¼€å‘äº†ä¸“é—¨çš„åé¦ˆå’Œç¼–è¾‘æ¨¡å‹ã€‚é€šè¿‡ä¼˜åŒ–åˆå§‹å“åº”è‰ç¨¿ã€æœ‰æ•ˆåé¦ˆå’Œç¼–è¾‘å“åº”çš„æ•°é‡ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨Arena HardåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†92.7çš„æœ€æ–°æ€§èƒ½ï¼Œè¶…è¶Šäº†å…¶ä»–æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.01375', 'title': 'Combining Flow Matching and Transformers for Efficient Solution of Bayesian Inverse Problems', 'url': 'https://huggingface.co/papers/2503.01375', 'abstract': 'Solving Bayesian inverse problems efficiently remains a significant challenge due to the complexity of posterior distributions and the computational cost of traditional sampling methods. Given a series of observations and the forward model, we want to recover the distribution of the parameters, conditioned on observed experimental data. We show, that combining Conditional Flow Mathching (CFM) with transformer-based architecture, we can efficiently sample from such kind of distribution, conditioned on variable number of observations.', 'score': 5, 'issue_id': 2583, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 3', 'zh': '3æœˆ3æ—¥'}, 'hash': '9dd35b1faaa32b61', 'authors': ['Daniil Sherki', 'Ivan Oseledets', 'Ekaterina Muravleva'], 'affiliations': ['Artificial Intelligence Research Institute', 'Sberbank, AI4S Center', 'Skolkovo Institute of Science and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2503.01375.jpg', 'data': {'categories': ['#architecture', '#math'], 'emoji': 'ğŸ”„', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ° Ğ¸Ğ· ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ°Ğ¿Ğ¾ÑÑ‚ĞµÑ€Ğ¸Ğ¾Ñ€Ğ½Ñ‹Ñ… Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ CFM Ğ¸ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ±Ğ°Ğ¹ĞµÑĞ¾Ğ²ÑĞºĞ¸Ñ… Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Conditional Flow Matching (CFM) Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ¾Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ². Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ğ»ÑÑ‚ÑŒ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºÑƒ Ğ¸Ğ· ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ°Ğ¿Ğ¾ÑÑ‚ĞµÑ€Ğ¸Ğ¾Ñ€Ğ½Ñ‹Ñ… Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹, Ğ¾Ğ±ÑƒÑĞ»Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼ Ñ‡Ğ¸ÑĞ»Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°ĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸, ÑĞ½Ğ¸Ğ¶Ğ°Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹.'}, 'en': {'title': 'Efficient Bayesian Inference with CFM and Transformers', 'desc': 'This paper addresses the challenge of efficiently solving Bayesian inverse problems, which involve estimating parameter distributions based on observed data. Traditional sampling methods can be computationally expensive, especially when dealing with complex posterior distributions. The authors propose a novel approach that combines Conditional Flow Matching (CFM) with transformer-based architectures to improve sampling efficiency. This method allows for effective sampling from parameter distributions conditioned on a variable number of observations, enhancing the ability to recover accurate parameter estimates.'}, 'zh': {'title': 'é«˜æ•ˆè´å¶æ–¯é€†é—®é¢˜æ±‚è§£çš„æ–°æ–¹æ³•', 'desc': 'è§£å†³è´å¶æ–¯é€†é—®é¢˜çš„æ•ˆç‡ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ï¼Œå› ä¸ºåéªŒåˆ†å¸ƒçš„å¤æ‚æ€§å’Œä¼ ç»Ÿé‡‡æ ·æ–¹æ³•çš„è®¡ç®—æˆæœ¬è¾ƒé«˜ã€‚æˆ‘ä»¬å¸Œæœ›åœ¨ç»™å®šä¸€ç³»åˆ—è§‚æµ‹å’Œå‰å‘æ¨¡å‹çš„æƒ…å†µä¸‹ï¼Œæ¢å¤å‚æ•°çš„åˆ†å¸ƒï¼Œè¿™äº›åˆ†å¸ƒæ˜¯åŸºäºè§‚å¯Ÿåˆ°çš„å®éªŒæ•°æ®ã€‚æˆ‘ä»¬å±•ç¤ºäº†å°†æ¡ä»¶æµåŒ¹é…ï¼ˆCFMï¼‰ä¸åŸºäºå˜æ¢å™¨çš„æ¶æ„ç›¸ç»“åˆï¼Œå¯ä»¥æœ‰æ•ˆåœ°ä»è¿™ç§åˆ†å¸ƒä¸­è¿›è¡Œé‡‡æ ·ï¼Œä¸”è¯¥åˆ†å¸ƒå¯ä»¥æ ¹æ®è§‚æµ‹æ•°é‡çš„å˜åŒ–è€Œå˜åŒ–ã€‚æ­¤æ–¹æ³•ä¸ºè´å¶æ–¯æ¨æ–­æä¾›äº†ä¸€ç§æ–°çš„é«˜æ•ˆè§£å†³æ–¹æ¡ˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.04369', 'title': 'Lost in Literalism: How Supervised Training Shapes Translationese in LLMs', 'url': 'https://huggingface.co/papers/2503.04369', 'abstract': 'Large language models (LLMs) have achieved remarkable success in machine translation, demonstrating impressive performance across diverse languages. However, translationese, characterized by overly literal and unnatural translations, remains a persistent challenge in LLM-based translation systems. Despite their pre-training on vast corpora of natural utterances, LLMs exhibit translationese errors and generate unexpected unnatural translations, stemming from biases introduced during supervised fine-tuning (SFT). In this work, we systematically evaluate the prevalence of translationese in LLM-generated translations and investigate its roots during supervised training. We introduce methods to mitigate these biases, including polishing golden references and filtering unnatural training instances. Empirical evaluations demonstrate that these approaches significantly reduce translationese while improving translation naturalness, validated by human evaluations and automatic metrics. Our findings highlight the need for training-aware adjustments to optimize LLM translation outputs, paving the way for more fluent and target-language-consistent translations. We release the data and code at https://github.com/yafuly/LLM_Translationese.', 'score': 4, 'issue_id': 2586, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 6', 'zh': '3æœˆ6æ—¥'}, 'hash': '7e6f548766f04dbf', 'authors': ['Yafu Li', 'Ronghao Zhang', 'Zhilin Wang', 'Huajian Zhang', 'Leyang Cui', 'Yongjing Yin', 'Tong Xiao', 'Yue Zhang'], 'affiliations': ['Northeastern University', 'Shanghai AI Laboratory', 'Westlake University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.04369.jpg', 'data': {'categories': ['#training', '#multilingual', '#machine_translation', '#data'], 'emoji': 'ğŸŒ', 'ru': {'title': 'Ğ‘Ğ¾Ñ€ÑŒĞ±Ğ° Ñ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ¼: Ğ¿ÑƒÑ‚ÑŒ Ğº ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ°Ğ¼ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ LLM', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğµ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° (translationese) Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ñ‹ Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ½Ğ¾Ğ²ĞµĞ½Ğ¸Ñ Ğ½ĞµĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ¾Ğ² Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ´Ğ»Ñ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ³Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ°, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ¾Ğ² Ğ¸ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ğ½ĞµĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ² Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ÑÑ‚ Ğ¿Ñ€Ğ¾ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ¾Ğ², Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼Ñ‹Ñ… LLM.'}, 'en': {'title': 'Reducing Translationese for Natural Language Translations', 'desc': 'This paper addresses the issue of translationese in large language models (LLMs) used for machine translation, which leads to unnatural and overly literal translations. The authors evaluate how prevalent translationese is in LLM outputs and identify biases introduced during supervised fine-tuning (SFT) as a key factor. They propose methods to reduce these biases, such as refining reference translations and filtering out unnatural training examples. Their empirical results show that these strategies significantly enhance the naturalness of translations, suggesting that careful adjustments during training can improve LLM performance in translation tasks.'}, 'zh': {'title': 'ä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹ç¿»è¯‘ï¼Œå‡å°‘ç¿»è¯‘è…”', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æœºå™¨ç¿»è¯‘ä¸­å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†ä»é¢ä¸´ç¿»è¯‘è…”çš„é—®é¢˜ï¼Œå³ç¿»è¯‘è¿‡äºå­—é¢å’Œä¸è‡ªç„¶ã€‚å°½ç®¡åœ¨å¤§é‡è‡ªç„¶è¯­æ–™ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼ŒLLMsåœ¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰è¿‡ç¨‹ä¸­å¼•å…¥çš„åå·®å¯¼è‡´äº†ç¿»è¯‘è…”é”™è¯¯ã€‚æœ¬æ–‡ç³»ç»Ÿè¯„ä¼°äº†LLMç”Ÿæˆç¿»è¯‘ä¸­çš„ç¿»è¯‘è…”ç°è±¡ï¼Œå¹¶æ¢è®¨äº†å…¶åœ¨ç›‘ç£è®­ç»ƒä¸­çš„æ ¹æºã€‚æˆ‘ä»¬æå‡ºäº†å‡è½»è¿™äº›åå·®çš„æ–¹æ³•ï¼Œå¹¶é€šè¿‡å®è¯è¯„ä¼°è¯æ˜è¿™äº›æ–¹æ³•æ˜¾è‘—é™ä½äº†ç¿»è¯‘è…”ï¼Œæé«˜äº†ç¿»è¯‘çš„è‡ªç„¶æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.02191', 'title': 'Understanding and Predicting Derailment in Toxic Conversations on GitHub', 'url': 'https://huggingface.co/papers/2503.02191', 'abstract': 'Software projects thrive on the involvement and contributions of individuals from different backgrounds. However, toxic language and negative interactions can hinder the participation and retention of contributors and alienate newcomers. Proactive moderation strategies aim to prevent toxicity from occurring by addressing conversations that have derailed from their intended purpose. This study aims to understand and predict conversational derailment leading to toxicity on GitHub.   To facilitate this research, we curate a novel dataset comprising 202 toxic conversations from GitHub with annotated derailment points, along with 696 non-toxic conversations as a baseline. Based on this dataset, we identify unique characteristics of toxic conversations and derailment points, including linguistic markers such as second-person pronouns, negation terms, and tones of Bitter Frustration and Impatience, as well as patterns in conversational dynamics between project contributors and external participants.   Leveraging these empirical observations, we propose a proactive moderation approach to automatically detect and address potentially harmful conversations before escalation. By utilizing modern LLMs, we develop a conversation trajectory summary technique that captures the evolution of discussions and identifies early signs of derailment. Our experiments demonstrate that LLM prompts tailored to provide summaries of GitHub conversations achieve 69% F1-Score in predicting conversational derailment, strongly improving over a set of baseline approaches.', 'score': 3, 'issue_id': 2581, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 4', 'zh': '3æœˆ4æ—¥'}, 'hash': '1d0bc1069249c9e1', 'authors': ['Mia Mohammad Imran', 'Robert Zita', 'Rebekah Copeland', 'Preetha Chatterjee', 'Rahat Rizvi Rahman', 'Kostadin Damevski'], 'affiliations': ['Drexel University, Philadelphia, PA, USA', 'Eastern Mennonite University, Harrisonburg, VA, USA', 'Elmhurst University, Elmhurst, IL, USA', 'Missouri University of Science and Technology, Rolla, MO, USA', 'Virginia Commonwealth University, Richmond, VA, USA'], 'pdf_title_img': 'assets/pdf/title_img/2503.02191.jpg', 'data': {'categories': ['#ethics', '#dataset', '#multimodal', '#data'], 'emoji': 'ğŸ›¡ï¸', 'ru': {'title': 'Ğ˜Ğ˜ Ğ½Ğ° ÑÑ‚Ñ€Ğ°Ğ¶Ğµ Ğ·Ğ´Ğ¾Ñ€Ğ¾Ğ²Ğ¾Ğ¹ Ğ°Ñ‚Ğ¼Ğ¾ÑÑ„ĞµÑ€Ñ‹ Ğ² open-source ÑĞ¾Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ğ°Ñ…', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾ĞºÑĞ¸Ñ‡Ğ½Ñ‹Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ² Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ°Ñ… Ğ½Ğ° GitHub. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸Ğ· 202 Ñ‚Ğ¾ĞºÑĞ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¸ 696 Ğ½ĞµÑ‚Ğ¾ĞºÑĞ¸Ñ‡Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ¾Ğ², Ğ²Ñ‹ÑĞ²Ğ¸Ğ² Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¼Ğ°Ñ€ĞºĞµÑ€Ñ‹ Ğ¸ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹, Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ´ĞµÑÑ‚Ñ€ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ±Ñ‹Ğ»Ğ° Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€Ğ¾Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ 69% F1-Ğ¼ĞµÑ€Ñ‹ Ğ² Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ¾Ğ² Ğ½Ğ° Ñ€Ğ°Ğ½Ğ½Ğ¸Ñ… ÑÑ‚Ğ°Ğ´Ğ¸ÑÑ….'}, 'en': {'title': 'Proactive Moderation: Detecting Toxicity Before It Escalates', 'desc': 'This paper investigates how toxic language in GitHub conversations can deter contributors and newcomers. It introduces a dataset of 202 toxic conversations with marked derailment points, alongside 696 non-toxic examples for comparison. The study identifies specific linguistic features and conversational dynamics that signal potential toxicity. Using large language models (LLMs), the authors propose a proactive moderation strategy that summarizes conversation trajectories to detect early signs of derailment, achieving a 69% F1-Score in predictions.'}, 'zh': {'title': 'ä¸»åŠ¨ç®¡ç†ï¼Œé˜²æ­¢å¯¹è¯åç¦»ä¸æœ‰æ¯’è¯­è¨€', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†å¦‚ä½•åœ¨GitHubä¸Šé¢„æµ‹å’Œç†è§£å¯¹è¯åç¦»å¯¼è‡´çš„æœ‰æ¯’è¯­è¨€ã€‚æˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªæ–°æ•°æ®é›†ï¼ŒåŒ…å«202ä¸ªæœ‰æ¯’å¯¹è¯å’Œ696ä¸ªéæœ‰æ¯’å¯¹è¯ï¼Œå¹¶æ ‡æ³¨äº†åç¦»ç‚¹ã€‚é€šè¿‡åˆ†æè¿™äº›å¯¹è¯çš„è¯­è¨€ç‰¹å¾å’ŒåŠ¨æ€æ¨¡å¼ï¼Œæˆ‘ä»¬è¯†åˆ«å‡ºæœ‰æ¯’å¯¹è¯çš„ç‹¬ç‰¹ç‰¹å¾ã€‚æœ€åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸»åŠ¨çš„ç®¡ç†ç­–ç•¥ï¼Œåˆ©ç”¨ç°ä»£å¤§è¯­è¨€æ¨¡å‹è‡ªåŠ¨æ£€æµ‹å’Œå¤„ç†æ½œåœ¨çš„æœ‰å®³å¯¹è¯ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.03962', 'title': 'On the Acquisition of Shared Grammatical Representations in Bilingual Language Models', 'url': 'https://huggingface.co/papers/2503.03962', 'abstract': "While crosslingual transfer is crucial to contemporary language models' multilingual capabilities, how it occurs is not well understood. In this paper, we ask what happens to a monolingual language model when it begins to be trained on a second language. Specifically, we train small bilingual models for which we control the amount of data for each language and the order of language exposure. To find evidence of shared multilingual representations, we turn to structural priming, a method used to study grammatical representations in humans. We first replicate previous crosslingual structural priming results and find that after controlling for training data quantity and language exposure, there are asymmetrical effects across language pairs and directions. We argue that this asymmetry may shape hypotheses about human structural priming effects. We also find that structural priming effects are less robust for less similar language pairs, highlighting potential limitations of crosslingual transfer learning and shared representations for typologically diverse languages.", 'score': 2, 'issue_id': 2592, 'pub_date': '2025-03-05', 'pub_date_card': {'ru': '5 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 5', 'zh': '3æœˆ5æ—¥'}, 'hash': '5d7eee50132fd29c', 'authors': ['Catherine Arnett', 'Tyler A. Chang', 'James A. Michaelov', 'Benjamin K. Bergen'], 'affiliations': ['Department of Brain and Cognitive Science, Massachusetts Institute of Technology', 'Department of Cognitive Science, University of California San Diego', 'Department of Linguistics, University of California San Diego', 'HalÄ±cÄ±oglu Data Science Institute, University of California San Diego'], 'pdf_title_img': 'assets/pdf/title_img/2503.03962.jpg', 'data': {'categories': ['#low_resource', '#training', '#transfer_learning', '#data', '#multilingual'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°Ñ Ñ‚Ğ°Ğ¹Ğ½Ñ‹ ĞºÑ€Ğ¾ÑÑ-Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ° Ğ² Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ÑÑ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ ĞºÑ€Ğ¾ÑÑ-Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ° Ğ² Ğ´Ğ²ÑƒÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸Ğ·ÑƒÑ‡Ğ°ÑÑ‚, ĞºĞ°Ğº Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ, Ğ½Ğ°Ñ‡Ğ¸Ğ½Ğ°ĞµÑ‚ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ¹ ÑĞ·Ñ‹Ğº. Ğ”Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¾Ğ±Ñ‰Ğ¸Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ°Ğ¹Ğ¼Ğ¸Ğ½Ğ³Ğ°, Ğ·Ğ°Ğ¸Ğ¼ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¸Ğ· Ğ¿ÑĞ¸Ñ…Ğ¾Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸ĞºĞ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ°ÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ñ‹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ğ°Ñ€Ğ°Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ² Ğ¸ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ°, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¼ĞµĞ½ÑŒÑˆÑƒÑ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ°Ğ¹Ğ¼Ğ¸Ğ½Ğ³Ğ° Ğ´Ğ»Ñ Ñ‚Ğ¸Ğ¿Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ².'}, 'en': {'title': 'Understanding Crosslingual Transfer in Language Models', 'desc': 'This paper investigates how training a monolingual language model on a second language affects its performance and understanding. The authors create small bilingual models and manipulate the amount of training data and the sequence of language exposure. They utilize structural priming to analyze the grammatical representations that emerge from this bilingual training. The findings reveal asymmetrical effects in language pairs and suggest that less similar languages may limit the effectiveness of crosslingual transfer learning.'}, 'zh': {'title': 'æ¢ç´¢è·¨è¯­è¨€è¿ç§»çš„éå¯¹ç§°æ€§', 'desc': 'æœ¬æ–‡æ¢è®¨äº†è·¨è¯­è¨€è¿ç§»åœ¨ç°ä»£è¯­è¨€æ¨¡å‹ä¸­çš„é‡è¦æ€§ï¼Œå°¤å…¶æ˜¯å½“å•è¯­æ¨¡å‹å¼€å§‹æ¥å—ç¬¬äºŒè¯­è¨€è®­ç»ƒæ—¶ä¼šå‘ç”Ÿä»€ä¹ˆã€‚æˆ‘ä»¬è®­ç»ƒäº†å°å‹åŒè¯­æ¨¡å‹ï¼Œæ§åˆ¶æ¯ç§è¯­è¨€çš„æ•°æ®é‡å’Œè¯­è¨€æš´éœ²çš„é¡ºåºã€‚é€šè¿‡ç»“æ„æ€§å¯åŠ¨çš„æ–¹æ³•ï¼Œæˆ‘ä»¬å‘ç°ä¸åŒè¯­è¨€å¯¹ä¹‹é—´çš„å½±å“ä¸å¯¹ç§°ï¼Œè¿™å¯èƒ½å½±å“äººç±»çš„ç»“æ„æ€§å¯åŠ¨å‡è®¾ã€‚ç ”ç©¶è¿˜è¡¨æ˜ï¼Œå¯¹äºè¯­è¨€ç±»å‹å·®å¼‚è¾ƒå¤§çš„è¯­è¨€å¯¹ï¼Œç»“æ„æ€§å¯åŠ¨æ•ˆæœè¾ƒå¼±ï¼Œçªæ˜¾äº†è·¨è¯­è¨€è¿ç§»å­¦ä¹ çš„æ½œåœ¨å±€é™æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.03601', 'title': 'Feature-Level Insights into Artificial Text Detection with Sparse\n  Autoencoders', 'url': 'https://huggingface.co/papers/2503.03601', 'abstract': 'Artificial Text Detection (ATD) is becoming increasingly important with the rise of advanced Large Language Models (LLMs). Despite numerous efforts, no single algorithm performs consistently well across different types of unseen text or guarantees effective generalization to new LLMs. Interpretability plays a crucial role in achieving this goal. In this study, we enhance ATD interpretability by using Sparse Autoencoders (SAE) to extract features from Gemma-2-2b residual stream. We identify both interpretable and efficient features, analyzing their semantics and relevance through domain- and model-specific statistics, a steering approach, and manual or LLM-based interpretation. Our methods offer valuable insights into how texts from various models differ from human-written content. We show that modern LLMs have a distinct writing style, especially in information-dense domains, even though they can produce human-like outputs with personalized prompts.', 'score': 133, 'issue_id': 2634, 'pub_date': '2025-03-05', 'pub_date_card': {'ru': '5 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 5', 'zh': '3æœˆ5æ—¥'}, 'hash': 'd045a8e2a39262c9', 'authors': ['Kristian Kuznetsov', 'Laida Kushnareva', 'Polina Druzhinina', 'Anton Razzhigaev', 'Anastasia Voznyuk', 'Irina Piontkovskaya', 'Evgeny Burnaev', 'Serguei Barannikov'], 'affiliations': ['AI Foundation and Algorithm Lab', 'Artificial Intelligence Research Institute (AIRI)', 'CNRS, UniversitÃ© Paris CitÃ©, France', 'Moscow Institute of Physics and Technology', 'Skolkovo Institute of Science and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2503.03601.jpg', 'data': {'categories': ['#multimodal', '#cv', '#interpretability', '#data'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°Ñ ÑĞµĞºÑ€ĞµÑ‚Ñ‹ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¼Ñƒ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ° (ATD) Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¸Ğ· Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Gemma-2-2b. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºÑƒ Ğ¸ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ğ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸Ğ¼ĞµÑÑ‚ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ ÑÑ‚Ğ¸Ğ»ÑŒ Ğ¿Ğ¸ÑÑŒĞ¼Ğ°, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾-Ğ½Ğ°ÑÑ‹Ñ‰ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ñ†ĞµĞ½Ğ½Ñ‹Ğµ insights Ğ¾ Ñ‚Ğ¾Ğ¼, ĞºĞ°Ğº Ñ‚ĞµĞºÑÑ‚Ñ‹, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ°ÑÑ‚ÑÑ Ğ¾Ñ‚ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°, Ğ½Ğ°Ğ¿Ğ¸ÑĞ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ»ÑĞ´ÑŒĞ¼Ğ¸.'}, 'en': {'title': 'Enhancing Text Detection with Interpretability in AI Models', 'desc': 'This paper focuses on improving Artificial Text Detection (ATD) by enhancing its interpretability using Sparse Autoencoders (SAE). The authors extract features from the residual stream of the Gemma-2-2b model to identify both interpretable and efficient characteristics of text. They analyze these features through various statistical methods and interpretations to understand how machine-generated texts differ from human-written ones. The study reveals that modern Large Language Models (LLMs) exhibit a unique writing style, particularly in information-dense areas, despite their ability to generate human-like text.'}, 'zh': {'title': 'æå‡äººå·¥æ–‡æœ¬æ£€æµ‹çš„å¯è§£é‡Šæ€§', 'desc': 'éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å‘å±•ï¼Œäººå·¥æ–‡æœ¬æ£€æµ‹ï¼ˆATDï¼‰å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚å°½ç®¡å·²æœ‰è®¸å¤šåŠªåŠ›ï¼Œä½†æ²¡æœ‰å•ä¸€ç®—æ³•èƒ½å¤Ÿåœ¨ä¸åŒç±»å‹çš„æœªè§æ–‡æœ¬ä¸­å§‹ç»ˆè¡¨ç°è‰¯å¥½ï¼Œä¹Ÿæ— æ³•ä¿è¯å¯¹æ–°LLMçš„æœ‰æ•ˆæ³›åŒ–ã€‚å¯è§£é‡Šæ€§åœ¨å®ç°è¿™ä¸€ç›®æ ‡ä¸­èµ·ç€å…³é”®ä½œç”¨ã€‚æˆ‘ä»¬é€šè¿‡ä½¿ç”¨ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEï¼‰ä»Gemma-2-2bæ®‹å·®æµä¸­æå–ç‰¹å¾ï¼Œå¢å¼ºäº†ATDçš„å¯è§£é‡Šæ€§ï¼Œåˆ†æäº†æ–‡æœ¬ä¸äººç±»å†™ä½œå†…å®¹çš„å·®å¼‚ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.07605', 'title': 'SEAP: Training-free Sparse Expert Activation Pruning Unlock the\n  Brainpower of Large Language Models', 'url': 'https://huggingface.co/papers/2503.07605', 'abstract': "Large Language Models have achieved remarkable success across various natural language processing tasks, yet their high computational cost during inference remains a major bottleneck. This paper introduces Sparse Expert Activation Pruning (SEAP), a training-free pruning method that selectively retains task-relevant parameters to reduce inference overhead. Inspired by the clustering patterns of hidden states and activations in LLMs, SEAP identifies task-specific expert activation patterns and prunes the model while preserving task performance and enhancing computational efficiency. Experimental results demonstrate that SEAP significantly reduces computational overhead while maintaining competitive accuracy. Notably, at 50% pruning, SEAP surpasses both WandA and FLAP by over 20%, and at 20% pruning, it incurs only a 2.2% performance drop compared to the dense model. These findings highlight SEAP's scalability and effectiveness, making it a promising approach for optimizing large-scale LLMs.", 'score': 59, 'issue_id': 2631, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 10', 'zh': '3æœˆ10æ—¥'}, 'hash': 'eaf9c07e3673cecc', 'authors': ['Xun Liang', 'Hanyu Wang', 'Huayi Lai', 'Simin Niu', 'Shichao Song', 'Jiawei Yang', 'Jihao Zhao', 'Feiyu Xiong', 'Bo Tang', 'Zhiyu Li'], 'affiliations': ['Institute for Advanced Algorithms Research, Shanghai, China', 'School of Information, Renmin University of China, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.07605.jpg', 'data': {'categories': ['#inference', '#optimization', '#training'], 'emoji': 'âœ‚ï¸', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±Ñ€ĞµĞ·Ğ°Ğ½Ğ¸Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ¹ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Sparse Expert Activation Pruning (SEAP) Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. SEAP Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ñ‡Ğ½Ğ¾ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹, Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ½Ğ¸Ğ·Ğ¸Ñ‚ÑŒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ¿Ñ€Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞµ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ğ¸ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ² Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ², ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ SEAP Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Optimize LLMs with Sparse Expert Activation Pruning!', 'desc': "This paper presents Sparse Expert Activation Pruning (SEAP), a novel method designed to reduce the computational cost of large language models (LLMs) during inference without requiring additional training. SEAP works by identifying and retaining only the parameters that are relevant to specific tasks, effectively pruning the model while maintaining its performance. The method leverages the clustering patterns of hidden states and activations to optimize the model's efficiency. Experimental results show that SEAP can significantly lower computational overhead while achieving competitive accuracy, making it a scalable solution for optimizing LLMs."}, 'zh': {'title': 'ç¨€ç–ä¸“å®¶æ¿€æ´»å‰ªæï¼šä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹çš„æ–°æ–¹æ³•', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†æ¨ç†æ—¶çš„é«˜è®¡ç®—æˆæœ¬ä»ç„¶æ˜¯ä¸€ä¸ªä¸»è¦ç“¶é¢ˆã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºç¨€ç–ä¸“å®¶æ¿€æ´»å‰ªæï¼ˆSEAPï¼‰çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨ä¸éœ€è¦è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œé€‰æ‹©æ€§åœ°ä¿ç•™ä¸ä»»åŠ¡ç›¸å…³çš„å‚æ•°ï¼Œä»¥å‡å°‘æ¨ç†å¼€é”€ã€‚SEAPé€šè¿‡åˆ†æéšè—çŠ¶æ€å’Œæ¿€æ´»çš„èšç±»æ¨¡å¼ï¼Œè¯†åˆ«ç‰¹å®šä»»åŠ¡çš„ä¸“å®¶æ¿€æ´»æ¨¡å¼ï¼Œä»è€Œåœ¨ä¿æŒä»»åŠ¡æ€§èƒ½çš„åŒæ—¶ä¼˜åŒ–è®¡ç®—æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSEAPåœ¨å‡å°‘è®¡ç®—å¼€é”€çš„åŒæ—¶ï¼Œä¿æŒäº†ç«äº‰åŠ›çš„å‡†ç¡®æ€§ï¼Œå±•ç¤ºäº†å…¶åœ¨ä¼˜åŒ–å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹æ–¹é¢çš„æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.07365', 'title': 'MM-Eureka: Exploring Visual Aha Moment with Rule-based Large-scale\n  Reinforcement Learning', 'url': 'https://huggingface.co/papers/2503.07365', 'abstract': "We present MM-Eureka, a multimodal reasoning model that successfully extends large-scale rule-based reinforcement learning (RL) to multimodal reasoning. While rule-based RL has shown remarkable success in improving LLMs' reasoning abilities in text domains, its application to multimodal settings has remained challenging. Our work reproduces key characteristics of text-based RL systems like DeepSeek-R1 in the multimodal space, including steady increases in accuracy reward and response length, and the emergence of reflection behaviors. We demonstrate that both instruction-tuned and pre-trained models can develop strong multimodal reasoning capabilities through rule-based RL without supervised fine-tuning, showing superior data efficiency compared to alternative approaches. We open-source our complete pipeline to foster further research in this area. We release all our codes, models, data, etc. at https://github.com/ModalMinds/MM-EUREKA", 'score': 45, 'issue_id': 2630, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 10', 'zh': '3æœˆ10æ—¥'}, 'hash': '765d475b38d9f289', 'authors': ['Fanqing Meng', 'Lingxiao Du', 'Zongkai Liu', 'Zhixiang Zhou', 'Quanfeng Lu', 'Daocheng Fu', 'Botian Shi', 'Wenhai Wang', 'Junjun He', 'Kaipeng Zhang', 'Ping Luo', 'Yu Qiao', 'Qiaosheng Zhang', 'Wenqi Shao'], 'affiliations': ['Shanghai AI Laboratory', 'Shanghai Innovation Institute', 'Shanghai Jiao Tong University', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2503.07365.jpg', 'data': {'categories': ['#rl', '#multimodal', '#reasoning', '#rag', '#open_source'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'MM-Eureka â€“ ÑÑ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ» Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾Ğµ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ², Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ĞºĞ°Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ½Ğ°ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸, Ñ‚Ğ°Ğº Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ»Ğ¸ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ´ Ğ²ÑĞµĞ³Ğ¾ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ğ° Ğ´Ğ»Ñ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸.'}, 'en': {'title': 'Unlocking Multimodal Reasoning with MM-Eureka!', 'desc': 'MM-Eureka is a new model that enhances reasoning across different types of data, like text and images, using a method called rule-based reinforcement learning (RL). This approach builds on successful techniques from text-based RL, allowing the model to improve its accuracy and response quality in multimodal contexts. The model shows that it can learn effectively without needing extra labeled data, making it more efficient than other methods. By sharing our tools and findings, we aim to encourage more research in multimodal reasoning.'}, 'zh': {'title': 'MM-Eurekaï¼šå¤šæ¨¡æ€æ¨ç†çš„æ–°çªç ´', 'desc': 'æˆ‘ä»¬æå‡ºäº†MM-Eurekaï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šæ¨¡æ€æ¨ç†æ¨¡å‹ï¼ŒæˆåŠŸåœ°å°†å¤§è§„æ¨¡åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ æ‰©å±•åˆ°å¤šæ¨¡æ€æ¨ç†é¢†åŸŸã€‚è™½ç„¶åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ åœ¨æ–‡æœ¬é¢†åŸŸæå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†åœ¨å¤šæ¨¡æ€ç¯å¢ƒä¸­çš„åº”ç”¨ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æˆ‘ä»¬çš„å·¥ä½œåœ¨å¤šæ¨¡æ€ç©ºé—´ä¸­é‡ç°äº†æ–‡æœ¬åŸºç¡€å¼ºåŒ–å­¦ä¹ ç³»ç»Ÿçš„å…³é”®ç‰¹å¾ï¼ŒåŒ…æ‹¬å‡†ç¡®æ€§å¥–åŠ±å’Œå“åº”é•¿åº¦çš„ç¨³å®šå¢åŠ ï¼Œä»¥åŠåæ€è¡Œä¸ºçš„å‡ºç°ã€‚æˆ‘ä»¬å±•ç¤ºäº†æ— ç›‘ç£å¾®è°ƒçš„æƒ…å†µä¸‹ï¼ŒæŒ‡ä»¤è°ƒä¼˜å’Œé¢„è®­ç»ƒæ¨¡å‹éƒ½èƒ½é€šè¿‡åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ å‘å±•å‡ºå¼ºå¤§çš„å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ï¼Œä¸”æ•°æ®æ•ˆç‡ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.07002', 'title': 'Taking Notes Brings Focus? Towards Multi-Turn Multimodal Dialogue\n  Learning', 'url': 'https://huggingface.co/papers/2503.07002', 'abstract': 'Multimodal large language models (MLLMs), built on large-scale pre-trained vision towers and language models, have shown great capabilities in multimodal understanding. However, most existing MLLMs are trained on single-turn vision question-answering tasks, which do not accurately reflect real-world human conversations. In this paper, we introduce MMDiag, a multi-turn multimodal dialogue dataset. This dataset is collaboratively generated through deliberately designed rules and GPT assistance, featuring strong correlations between questions, between questions and images, and among different image regions; thus aligning more closely with real-world scenarios. MMDiag serves as a strong benchmark for multi-turn multimodal dialogue learning and brings more challenges to the grounding and reasoning capabilities of MLLMs. Further, inspired by human vision processing, we present DiagNote, an MLLM equipped with multimodal grounding and reasoning capabilities. DiagNote consists of two modules (Deliberate and Gaze) interacting with each other to perform Chain-of-Thought and annotations respectively, throughout multi-turn dialogues. We empirically demonstrate the advantages of DiagNote in both grounding and jointly processing and reasoning with vision and language information over existing MLLMs.', 'score': 33, 'issue_id': 2631, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 10', 'zh': '3æœˆ10æ—¥'}, 'hash': '1f57fb5c5398efbc', 'authors': ['Jiazheng Liu', 'Sipeng Zheng', 'BÃ¶rje F. Karlsson', 'Zongqing Lu'], 'affiliations': ['Beijing Academy of Artificial Intelligence', 'School of Computer Science, Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2503.07002.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#games', '#multimodal', '#dataset', '#architecture'], 'emoji': 'ğŸ—£ï¸', 'ru': {'title': 'ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°Ğ¼: Ğ¾Ñ‚ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğº Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MMDiag - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ², ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ GPT. Ğ­Ñ‚Ğ¾Ñ‚ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµ Ğ¾Ñ‚Ñ€Ğ°Ğ¶Ğ°ĞµÑ‚ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸ Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ, Ñ‡ĞµĞ¼ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¾Ğ´Ğ½Ğ¾Ñ‚ÑƒÑ€Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ DiagNote - Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ·Ğ°Ğ·ĞµĞ¼Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. DiagNote Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²Ğ° Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ÑƒĞ»Ñ Ğ´Ğ»Ñ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ñ…Ğ¾Ğ´Ğµ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°.'}, 'en': {'title': 'Enhancing Multimodal Dialogue with MMDiag and DiagNote', 'desc': 'This paper presents MMDiag, a new dataset designed for multi-turn multimodal dialogue, which enhances the training of multimodal large language models (MLLMs). Unlike previous datasets that focus on single-turn tasks, MMDiag captures the complexities of real-world conversations by incorporating strong correlations between questions and images. The authors introduce DiagNote, an MLLM that features two interactive modules for improved grounding and reasoning in dialogues. The results show that DiagNote outperforms existing MLLMs in processing and reasoning with both visual and textual information.'}, 'zh': {'title': 'æå‡å¤šæ¨¡æ€å¯¹è¯çš„æ™ºèƒ½åŒ–', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„å¤šæ¨¡æ€å¯¹è¯æ•°æ®é›†MMDiagï¼Œæ—¨åœ¨æ”¹å–„ç°æœ‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤šè½®å¯¹è¯ä¸­çš„è¡¨ç°ã€‚MMDiagé€šè¿‡ç²¾å¿ƒè®¾è®¡çš„è§„åˆ™å’ŒGPTçš„è¾…åŠ©ç”Ÿæˆï¼ŒåŒ…å«äº†é—®é¢˜ä¹‹é—´ã€é—®é¢˜ä¸å›¾åƒä¹‹é—´ä»¥åŠä¸åŒå›¾åƒåŒºåŸŸä¹‹é—´çš„å¼ºç›¸å…³æ€§ï¼Œæ›´è´´è¿‘çœŸå®çš„äººç±»å¯¹è¯åœºæ™¯ã€‚è®ºæ–‡è¿˜æå‡ºäº†DiagNoteï¼Œä¸€ä¸ªå…·å¤‡å¤šæ¨¡æ€åŸºç¡€å’Œæ¨ç†èƒ½åŠ›çš„MLLMï¼ŒåŒ…å«ä¸¤ä¸ªç›¸äº’ä½œç”¨çš„æ¨¡å—ï¼ˆDeliberateå’ŒGazeï¼‰ï¼Œç”¨äºåœ¨å¤šè½®å¯¹è¯ä¸­è¿›è¡Œæ€ç»´é“¾å’Œæ³¨é‡Šã€‚é€šè¿‡å®éªŒè¯æ˜ï¼ŒDiagNoteåœ¨åŸºç¡€å’Œå…±åŒå¤„ç†è§†è§‰ä¸è¯­è¨€ä¿¡æ¯çš„æ¨ç†èƒ½åŠ›ä¸Šä¼˜äºç°æœ‰çš„MLLMsã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.07314', 'title': 'Automated Movie Generation via Multi-Agent CoT Planning', 'url': 'https://huggingface.co/papers/2503.07314', 'abstract': 'Existing long-form video generation frameworks lack automated planning, requiring manual input for storylines, scenes, cinematography, and character interactions, resulting in high costs and inefficiencies. To address these challenges, we present MovieAgent, an automated movie generation via multi-agent Chain of Thought (CoT) planning. MovieAgent offers two key advantages: 1) We firstly explore and define the paradigm of automated movie/long-video generation. Given a script and character bank, our MovieAgent can generates multi-scene, multi-shot long-form videos with a coherent narrative, while ensuring character consistency, synchronized subtitles, and stable audio throughout the film. 2) MovieAgent introduces a hierarchical CoT-based reasoning process to automatically structure scenes, camera settings, and cinematography, significantly reducing human effort. By employing multiple LLM agents to simulate the roles of a director, screenwriter, storyboard artist, and location manager, MovieAgent streamlines the production pipeline. Experiments demonstrate that MovieAgent achieves new state-of-the-art results in script faithfulness, character consistency, and narrative coherence. Our hierarchical framework takes a step forward and provides new insights into fully automated movie generation. The code and project website are available at: https://github.com/showlab/MovieAgent and https://weijiawu.github.io/MovieAgent.', 'score': 28, 'issue_id': 2631, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 10', 'zh': '3æœˆ10æ—¥'}, 'hash': '2e5c000925250863', 'authors': ['Weijia Wu', 'Zeyu Zhu', 'Mike Zheng Shou'], 'affiliations': ['Show Lab, National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2503.07314.jpg', 'data': {'categories': ['#reasoning', '#open_source', '#multimodal', '#story_generation', '#video', '#agents'], 'emoji': 'ğŸ¬', 'ru': {'title': 'ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ñ„Ğ¸Ğ»ÑŒĞ¼Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'MovieAgent - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°Ğ·Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğ¹ (Chain of Thought). Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ†ĞµĞ½Ğ½Ñ‹Ğµ Ñ„Ğ¸Ğ»ÑŒĞ¼Ñ‹ Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ ÑÑĞ¶ĞµÑ‚Ğ¾Ğ¼, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ğ¾ÑÑ‚Ğ¾ÑĞ½ÑÑ‚Ğ²Ğ¾ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹ Ğ¸ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑÑƒĞ±Ñ‚Ğ¸Ñ‚Ñ€Ğ¾Ğ². MovieAgent Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑÑ†ĞµĞ½ Ğ¸ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾ĞµĞº ĞºĞ°Ğ¼ĞµÑ€Ñ‹, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğµ ÑƒÑĞ¸Ğ»Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ MovieAgent Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ñ, Ğ¿Ğ¾ÑÑ‚Ğ¾ÑĞ½ÑÑ‚Ğ²Ğµ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ²ĞµÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'Automating Movie Magic with MovieAgent!', 'desc': 'This paper introduces MovieAgent, a novel framework for automated long-form video generation that eliminates the need for manual planning of storylines and scenes. It utilizes a multi-agent Chain of Thought (CoT) reasoning process to create coherent narratives while maintaining character consistency and synchronized audio. By simulating various production roles, MovieAgent streamlines the filmmaking process, significantly reducing the effort required from human creators. Experimental results show that MovieAgent sets new benchmarks in script adherence, character consistency, and overall narrative quality.'}, 'zh': {'title': 'è‡ªåŠ¨åŒ–ç”µå½±ç”Ÿæˆçš„æ–°çºªå…ƒ', 'desc': 'ç°æœ‰çš„é•¿è§†é¢‘ç”Ÿæˆæ¡†æ¶ç¼ºä¹è‡ªåŠ¨åŒ–è§„åˆ’ï¼Œéœ€è¦æ‰‹åŠ¨è¾“å…¥æ•…äº‹æƒ…èŠ‚ã€åœºæ™¯ã€æ‘„å½±å’Œè§’è‰²äº’åŠ¨ï¼Œå¯¼è‡´é«˜æˆæœ¬å’Œä½æ•ˆç‡ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†MovieAgentï¼Œé€šè¿‡å¤šæ™ºèƒ½ä½“çš„æ€ç»´é“¾ï¼ˆCoTï¼‰è§„åˆ’å®ç°è‡ªåŠ¨åŒ–ç”µå½±ç”Ÿæˆã€‚MovieAgentçš„ä¸¤ä¸ªä¸»è¦ä¼˜åŠ¿æ˜¯ï¼šé¦–å…ˆï¼Œæˆ‘ä»¬æ¢ç´¢å¹¶å®šä¹‰äº†è‡ªåŠ¨åŒ–ç”µå½±/é•¿è§†é¢‘ç”Ÿæˆçš„èŒƒå¼ï¼›å…¶æ¬¡ï¼ŒMovieAgentå¼•å…¥äº†åŸºäºå±‚æ¬¡çš„CoTæ¨ç†è¿‡ç¨‹ï¼Œè‡ªåŠ¨æ„å»ºåœºæ™¯ã€æ‘„åƒæœºè®¾ç½®å’Œæ‘„å½±ï¼Œå¤§å¤§å‡å°‘äº†äººåŠ›æŠ•å…¥ã€‚å®éªŒè¡¨æ˜ï¼ŒMovieAgentåœ¨å‰§æœ¬å¿ å®åº¦ã€è§’è‰²ä¸€è‡´æ€§å’Œå™äº‹è¿è´¯æ€§æ–¹é¢è¾¾åˆ°äº†æ–°çš„æœ€å…ˆè¿›æ°´å¹³ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.07216', 'title': 'FedRand: Enhancing Privacy in Federated Learning with Randomized LoRA\n  Subparameter Updates', 'url': 'https://huggingface.co/papers/2503.07216', 'abstract': "Federated Learning (FL) is a widely used framework for training models in a decentralized manner, ensuring that the central server does not have direct access to data from local clients. However, this approach may still fail to fully preserve data privacy, as models from local clients are exposed to the central server during the aggregation process. This issue becomes even more critical when training vision-language models (VLMs) with FL, as VLMs can easily memorize training data instances, making them vulnerable to membership inference attacks (MIAs). To address this challenge, we propose the FedRand framework, which avoids disclosing the full set of client parameters. In this framework, each client randomly selects subparameters of Low-Rank Adaptation (LoRA) from the server and keeps the remaining counterparts of the LoRA weights as private parameters. After training both parameters on the client's private dataset, only the non-private <PRE_TAG>client parameters</POST_TAG> are sent back to the server for aggregation. This approach mitigates the risk of exposing client-side VLM parameters, thereby enhancing data privacy. We empirically validate that FedRand improves robustness against MIAs compared to relevant baselines while achieving accuracy comparable to methods that communicate full LoRA parameters across several benchmark datasets.", 'score': 25, 'issue_id': 2631, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 10', 'zh': '3æœˆ10æ—¥'}, 'hash': '3d23c61b24a599ee', 'authors': ['Sangwoo Park', 'Seanie Lee', 'Byungjoo Kim', 'Sung Ju Hwang'], 'affiliations': ['DeepAuto.ai', 'Graduate School of AI, KAIST'], 'pdf_title_img': 'assets/pdf/title_img/2503.07216.jpg', 'data': {'categories': ['#security', '#benchmark', '#data', '#ethics', '#multimodal', '#rl'], 'emoji': 'ğŸ”', 'ru': {'title': 'FedRand: Ğ—Ğ°Ñ‰Ğ¸Ñ‚Ğ° ĞºĞ¾Ğ½Ñ„Ğ¸Ğ´ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ñ„ĞµĞ´ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ„ĞµĞ´ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ (FL) Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (VLM), Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ FedRand. Ğ­Ñ‚Ğ° Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¸ĞºĞ° Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ° Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ´ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿ÑƒÑ‚ĞµĞ¼ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¿Ğ¾Ğ´Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Low-Rank Adaptation (LoRA) Ğ´Ğ»Ñ Ğ¾Ğ±Ğ¼ĞµĞ½Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ»Ğ¸ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ¸ ÑĞµÑ€Ğ²ĞµÑ€Ğ¾Ğ¼. FedRand Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ‚ÑŒ Ñ‡Ğ°ÑÑ‚ÑŒ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² LoRA Ğ¿Ñ€Ğ¸Ğ²Ğ°Ñ‚Ğ½Ñ‹Ğ¼Ğ¸, ÑĞ½Ğ¸Ğ¶Ğ°Ñ Ñ€Ğ¸ÑĞº ÑƒÑ‚ĞµÑ‡ĞºĞ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ĞºĞ»Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ². Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ FedRand Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğº Ğ°Ñ‚Ğ°ĞºĞ°Ğ¼ Ğ¿Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ñƒ Ñ‡Ğ»ĞµĞ½ÑÑ‚Ğ²Ğ° (MIA) Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼ÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ.'}, 'en': {'title': 'Enhancing Privacy in Federated Learning with FedRand', 'desc': 'Federated Learning (FL) allows models to be trained on local data without sharing it with a central server, but it can still risk data privacy during model aggregation. This paper introduces the FedRand framework, which enhances privacy by having clients send only a subset of their model parameters back to the server. Specifically, it utilizes Low-Rank Adaptation (LoRA) to keep certain parameters private while still allowing effective model training. The results show that FedRand not only protects against membership inference attacks but also maintains accuracy similar to traditional methods that share full parameters.'}, 'zh': {'title': 'FedRandï¼šä¿æŠ¤æ•°æ®éšç§çš„è”é‚¦å­¦ä¹ æ–°æ¡†æ¶', 'desc': 'è”é‚¦å­¦ä¹ ï¼ˆFLï¼‰æ˜¯ä¸€ç§å»ä¸­å¿ƒåŒ–çš„æ¨¡å‹è®­ç»ƒæ¡†æ¶ï¼Œç¡®ä¿ä¸­å¤®æœåŠ¡å™¨æ— æ³•ç›´æ¥è®¿é—®æœ¬åœ°å®¢æˆ·ç«¯çš„æ•°æ®ã€‚ç„¶è€Œï¼Œåœ¨èšåˆè¿‡ç¨‹ä¸­ï¼Œæœ¬åœ°å®¢æˆ·ç«¯çš„æ¨¡å‹ä»å¯èƒ½æš´éœ²ç»™ä¸­å¤®æœåŠ¡å™¨ï¼Œä»è€Œå½±å“æ•°æ®éšç§ã€‚ç‰¹åˆ«æ˜¯åœ¨è®­ç»ƒè§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰æ—¶ï¼Œè¿™ç§é£é™©æ›´ä¸ºä¸¥é‡ï¼Œå› ä¸ºVLMså®¹æ˜“è®°ä½è®­ç»ƒæ•°æ®å®ä¾‹ï¼Œå®¹æ˜“å—åˆ°æˆå‘˜æ¨æ–­æ”»å‡»ï¼ˆMIAsï¼‰ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†FedRandæ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡éšæœºé€‰æ‹©ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰çš„å­å‚æ•°ï¼Œé¿å…æ³„éœ²å®Œæ•´çš„å®¢æˆ·ç«¯å‚æ•°ï¼Œä»è€Œå¢å¼ºæ•°æ®éšç§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.07067', 'title': 'DistiLLM-2: A Contrastive Approach Boosts the Distillation of LLMs', 'url': 'https://huggingface.co/papers/2503.07067', 'abstract': 'Despite the success of distillation in large language models (LLMs), most prior work applies identical loss functions to both teacher- and student-generated data. These strategies overlook the synergy between loss formulations and data types, leading to a suboptimal performance boost in student models. To address this, we propose DistiLLM-2, a contrastive approach that simultaneously increases the likelihood of teacher responses and decreases that of student responses by harnessing this synergy. Our extensive experiments show that DistiLLM-2 not only builds high-performing student models across a wide range of tasks, including instruction-following and code generation, but also supports diverse applications, such as preference alignment and vision-language extensions. These findings highlight the potential of a contrastive approach to enhance the efficacy of LLM distillation by effectively aligning teacher and student models across varied data types.', 'score': 22, 'issue_id': 2632, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 10', 'zh': '3æœˆ10æ—¥'}, 'hash': '8eb39990e619611e', 'authors': ['Jongwoo Ko', 'Tianyi Chen', 'Sungnyun Kim', 'Tianyu Ding', 'Luming Liang', 'Ilya Zharkov', 'Se-Young Yun'], 'affiliations': ['KAIST AI', 'Microsoft'], 'pdf_title_img': 'assets/pdf/title_img/2503.07067.jpg', 'data': {'categories': ['#multimodal', '#alignment', '#training', '#optimization'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'ĞšĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'DistiLLM-2 - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM), Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ. ĞĞ½ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ Ğ¸ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ĞµÑ‚ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² ÑƒÑ‡ĞµĞ½Ğ¸ĞºĞ°, ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑĞ¸Ğ½ĞµÑ€Ğ³Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²ĞºĞ°Ğ¼Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ¸ Ñ‚Ğ¸Ğ¿Ğ°Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ DistiLLM-2 ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸-ÑƒÑ‡ĞµĞ½Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾Ğ³Ğ¾ ÑĞ¿ĞµĞºÑ‚Ñ€Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ĞºĞ¾Ğ´Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸.'}, 'en': {'title': 'Enhancing LLM Distillation with Contrastive Learning', 'desc': 'This paper introduces DistiLLM-2, a novel approach to improve the distillation process in large language models (LLMs). Unlike previous methods that use the same loss functions for both teacher and student models, DistiLLM-2 employs a contrastive strategy that boosts the likelihood of correct teacher responses while reducing the likelihood of incorrect student responses. The results demonstrate that this method significantly enhances the performance of student models on various tasks, including instruction-following and code generation. Additionally, DistiLLM-2 shows versatility in applications such as preference alignment and vision-language tasks, emphasizing the importance of tailored loss functions in model distillation.'}, 'zh': {'title': 'å¯¹æ¯”æ–¹æ³•æå‡å¤§å‹è¯­è¨€æ¨¡å‹è’¸é¦æ•ˆæœ', 'desc': 'å°½ç®¡è’¸é¦åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­å–å¾—äº†æˆåŠŸï¼Œä½†å¤§å¤šæ•°å…ˆå‰çš„ç ”ç©¶å¯¹æ•™å¸ˆå’Œå­¦ç”Ÿç”Ÿæˆçš„æ•°æ®ä½¿ç”¨ç›¸åŒçš„æŸå¤±å‡½æ•°ã€‚è¿™ç§ç­–ç•¥å¿½è§†äº†æŸå¤±å…¬å¼ä¸æ•°æ®ç±»å‹ä¹‹é—´çš„ååŒä½œç”¨ï¼Œå¯¼è‡´å­¦ç”Ÿæ¨¡å‹çš„æ€§èƒ½æå‡ä¸ç†æƒ³ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†DistiLLM-2ï¼Œè¿™æ˜¯ä¸€ç§å¯¹æ¯”æ–¹æ³•ï¼Œèƒ½å¤ŸåŒæ—¶æé«˜æ•™å¸ˆå“åº”çš„å¯èƒ½æ€§å¹¶é™ä½å­¦ç”Ÿå“åº”çš„å¯èƒ½æ€§ã€‚æˆ‘ä»¬çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒDistiLLM-2ä¸ä»…åœ¨å¤šç§ä»»åŠ¡ä¸­æ„å»ºäº†é«˜æ€§èƒ½çš„å­¦ç”Ÿæ¨¡å‹ï¼Œè¿˜æ”¯æŒå¤šæ ·åŒ–çš„åº”ç”¨ï¼Œå¦‚åå¥½å¯¹é½å’Œè§†è§‰-è¯­è¨€æ‰©å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.07027', 'title': 'EasyControl: Adding Efficient and Flexible Control for Diffusion\n  Transformer', 'url': 'https://huggingface.co/papers/2503.07027', 'abstract': 'Recent advancements in Unet-based diffusion models, such as ControlNet and IP-Adapter, have introduced effective spatial and subject control mechanisms. However, the DiT (Diffusion Transformer) architecture still struggles with efficient and flexible control. To tackle this issue, we propose EasyControl, a novel framework designed to unify condition-guided diffusion transformers with high efficiency and flexibility. Our framework is built on three key innovations. First, we introduce a lightweight <PRE_TAG>Condition Injection LoRA Module</POST_TAG>. This module processes conditional signals in isolation, acting as a plug-and-play solution. It avoids modifying the base model weights, ensuring compatibility with customized models and enabling the flexible injection of diverse conditions. Notably, this module also supports harmonious and robust zero-shot multi-condition generalization, even when trained only on single-condition data. Second, we propose a <PRE_TAG>Position-Aware Training Paradigm</POST_TAG>. This approach standardizes input conditions to fixed resolutions, allowing the generation of images with arbitrary aspect ratios and flexible resolutions. At the same time, it optimizes computational efficiency, making the framework more practical for real-world applications. Third, we develop a Causal Attention Mechanism combined with the KV Cache technique, adapted for conditional generation tasks. This innovation significantly reduces the latency of image synthesis, improving the overall efficiency of the framework. Through extensive experiments, we demonstrate that EasyControl achieves exceptional performance across various application scenarios. These innovations collectively make our framework highly efficient, flexible, and suitable for a wide range of tasks.', 'score': 19, 'issue_id': 2630, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 10', 'zh': '3æœˆ10æ—¥'}, 'hash': '1b1589f9873720c7', 'authors': ['Yuxuan Zhang', 'Yirui Yuan', 'Yiren Song', 'Haofan Wang', 'Jiaming Liu'], 'affiliations': ['Liblib AI', 'National University of Singapore', 'ShanghaiTech University', 'Tiamat AI'], 'pdf_title_img': 'assets/pdf/title_img/2503.07027.jpg', 'data': {'categories': ['#cv', '#architecture', '#training', '#optimization', '#diffusion'], 'emoji': 'ğŸ¨', 'ru': {'title': 'EasyControl: Ğ³Ğ¸Ğ±ĞºĞ¾Ğµ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°Ğ¼Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ EasyControl - Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°Ñ…. ĞÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ñ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LoRA, Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ KV-ĞºÑÑˆĞµĞ¼. EasyControl Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¸ Ğ³Ğ¸Ğ±ĞºĞ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾Ğ¿Ğ¾Ñ€Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸ĞµĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'EasyControl: Unifying Efficiency and Flexibility in Diffusion Transformers', 'desc': 'This paper presents EasyControl, a new framework that enhances the efficiency and flexibility of condition-guided diffusion transformers, particularly addressing the limitations of the DiT architecture. It introduces a lightweight Condition Injection LoRA Module that allows for the independent processing of conditional signals without altering the base model, enabling easy integration of various conditions. Additionally, the Position-Aware Training Paradigm standardizes input conditions, facilitating the generation of images with different aspect ratios while optimizing computational resources. Lastly, the Causal Attention Mechanism with KV Cache significantly reduces image synthesis latency, making EasyControl a robust solution for diverse applications in machine learning.'}, 'zh': {'title': 'é«˜æ•ˆçµæ´»çš„æ¡ä»¶ç”Ÿæˆæ¡†æ¶EasyControl', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºEasyControlçš„æ–°æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜åŸºäºæ‰©æ•£å˜æ¢å™¨çš„æ¡ä»¶ç”Ÿæˆæ¨¡å‹çš„æ•ˆç‡å’Œçµæ´»æ€§ã€‚è¯¥æ¡†æ¶åŒ…å«ä¸‰ä¸ªå…³é”®åˆ›æ–°ï¼šé¦–å…ˆï¼Œè½»é‡çº§çš„æ¡ä»¶æ³¨å…¥LoRAæ¨¡å—å¯ä»¥ç‹¬ç«‹å¤„ç†æ¡ä»¶ä¿¡å·ï¼Œé¿å…ä¿®æ”¹åŸºç¡€æ¨¡å‹æƒé‡ï¼Œä»è€Œå®ç°ä¸å®šåˆ¶æ¨¡å‹çš„å…¼å®¹æ€§ã€‚å…¶æ¬¡ï¼Œä½ç½®æ„ŸçŸ¥è®­ç»ƒèŒƒå¼æ ‡å‡†åŒ–è¾“å…¥æ¡ä»¶ï¼Œä½¿å¾—ç”Ÿæˆä»»æ„é•¿å®½æ¯”å’Œçµæ´»åˆ†è¾¨ç‡çš„å›¾åƒæˆä¸ºå¯èƒ½ï¼ŒåŒæ—¶ä¼˜åŒ–è®¡ç®—æ•ˆç‡ã€‚æœ€åï¼Œç»“åˆKVç¼“å­˜æŠ€æœ¯çš„å› æœæ³¨æ„æœºåˆ¶æ˜¾è‘—é™ä½äº†å›¾åƒåˆæˆçš„å»¶è¿Ÿï¼Œæå‡äº†æ•´ä½“æ•ˆç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.06680', 'title': 'FEA-Bench: A Benchmark for Evaluating Repository-Level Code Generation\n  for Feature Implementation', 'url': 'https://huggingface.co/papers/2503.06680', 'abstract': "Implementing new features in repository-level codebases is a crucial application of code generation models. However, current benchmarks lack a dedicated evaluation framework for this capability. To fill this gap, we introduce FEA-Bench, a benchmark designed to assess the ability of large language models (LLMs) to perform incremental development within code repositories. We collect pull requests from 83 GitHub repositories and use rule-based and intent-based filtering to construct task instances focused on new feature development. Each task instance containing code changes is paired with relevant unit test files to ensure that the solution can be verified. The feature implementation requires LLMs to simultaneously possess code completion capabilities for new components and code editing abilities for other relevant parts in the code repository, providing a more comprehensive evaluation method of LLMs' automated software engineering capabilities. Experimental results show that LLMs perform significantly worse in the FEA-Bench, highlighting considerable challenges in such repository-level incremental code development.", 'score': 17, 'issue_id': 2630, 'pub_date': '2025-03-09', 'pub_date_card': {'ru': '9 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 9', 'zh': '3æœˆ9æ—¥'}, 'hash': '4394ce17a18696a3', 'authors': ['Wei Li', 'Xin Zhang', 'Zhongxin Guo', 'Shaoguang Mao', 'Wen Luo', 'Guangyue Peng', 'Yangyu Huang', 'Houfeng Wang', 'Scarlett Li'], 'affiliations': ['Microsoft Research Asia', 'State Key Laboratory for Multimedia Information Processing, School of Computer Science, Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2503.06680.jpg', 'data': {'categories': ['#plp', '#dataset', '#optimization', '#benchmark'], 'emoji': 'ğŸ§ª', 'ru': {'title': 'FEA-Bench: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ²Ñ‹Ğ·Ğ¾Ğ² Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ ĞŸĞ', 'desc': 'FEA-Bench - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ¸Ğ½ĞºÑ€ĞµĞ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½ÑƒÑ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ² Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸ÑÑ… ĞºĞ¾Ğ´Ğ°. ĞĞ½ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ¿ÑƒĞ»-Ñ€ĞµĞºĞ²ĞµÑÑ‚Ğ°Ñ… Ğ¸Ğ· 83 Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸ĞµĞ² GitHub Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¿Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹. ĞšĞ°Ğ¶Ğ´Ğ°Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ° ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ´Ğ° Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ñ‹Ğµ Ñ‚ĞµÑÑ‚Ñ‹ Ğ´Ğ»Ñ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ LLM Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ñ…ÑƒĞ¶Ğµ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ FEA-Bench, Ñ‡Ñ‚Ğ¾ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ° ÑĞµÑ€ÑŒĞµĞ·Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ² Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸ĞµĞ².'}, 'en': {'title': 'FEA-Bench: Evaluating Code Generation in Repositories', 'desc': "This paper introduces FEA-Bench, a new benchmark for evaluating large language models (LLMs) in the context of code generation for new features in software repositories. It addresses the lack of dedicated evaluation frameworks by collecting pull requests from 83 GitHub repositories and creating task instances that focus on incremental development. Each task is designed to test the LLM's ability to generate code changes while ensuring that these changes can be verified through associated unit tests. The findings reveal that LLMs struggle with this type of repository-level code development, indicating significant challenges in their automated software engineering capabilities."}, 'zh': {'title': 'è¯„ä¼°ä»£ç ç”Ÿæˆæ¨¡å‹çš„æ–°åŸºå‡†ï¼šFEA-Bench', 'desc': 'åœ¨ä»£ç ç”Ÿæˆæ¨¡å‹ä¸­ï¼Œå®ç°æ–°ç‰¹æ€§æ˜¯ä¸€ä¸ªé‡è¦çš„åº”ç”¨ã€‚å½“å‰çš„åŸºå‡†æµ‹è¯•ç¼ºä¹ä¸“é—¨è¯„ä¼°è¿™ä¸€èƒ½åŠ›çš„æ¡†æ¶ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†FEA-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä»£ç åº“ä¸­è¿›è¡Œå¢é‡å¼€å‘èƒ½åŠ›çš„åŸºå‡†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLLMsåœ¨FEA-Benchä¸­çš„è¡¨ç°æ˜¾è‘—è¾ƒå·®ï¼Œçªæ˜¾äº†åœ¨ä»£ç åº“çº§åˆ«å¢é‡å¼€å‘ä¸­é¢ä¸´çš„é‡å¤§æŒ‘æˆ˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.07608', 'title': 'AlphaDrive: Unleashing the Power of VLMs in Autonomous Driving via\n  Reinforcement Learning and Reasoning', 'url': 'https://huggingface.co/papers/2503.07608', 'abstract': 'OpenAI o1 and DeepSeek R1 achieve or even surpass human expert-level performance in complex domains like mathematics and science, with reinforcement learning (RL) and reasoning playing a crucial role. In autonomous driving, recent end-to-end models have greatly improved planning performance but still struggle with long-tailed problems due to limited common sense and reasoning abilities. Some studies integrate vision-language models (VLMs) into autonomous driving, but they typically rely on pre-trained models with simple supervised fine-tuning (SFT) on driving data, without further exploration of training strategies or optimizations specifically tailored for planning. In this paper, we propose AlphaDrive, a RL and reasoning framework for VLMs in autonomous driving. AlphaDrive introduces four GRPO-based RL rewards tailored for planning and employs a two-stage planning <PRE_TAG>reasoning training strategy</POST_TAG> that combines SFT with RL. As a result, AlphaDrive significantly improves both planning performance and training efficiency compared to using only SFT or without reasoning. Moreover, we are also excited to discover that, following RL training, AlphaDrive exhibits some emergent multimodal planning capabilities, which is critical for improving driving safety and efficiency. To the best of our knowledge, AlphaDrive is the first to integrate GRPO-based RL with planning reasoning into autonomous driving. Code will be released to facilitate future research.', 'score': 14, 'issue_id': 2632, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 10', 'zh': '3æœˆ10æ—¥'}, 'hash': '74dfc85ba9f7bcc7', 'authors': ['Bo Jiang', 'Shaoyu Chen', 'Qian Zhang', 'Wenyu Liu', 'Xinggang Wang'], 'affiliations': ['Horizon Robotics', 'Huazhong University of Science and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2503.07608.jpg', 'data': {'categories': ['#rl', '#training', '#optimization', '#multimodal', '#reasoning', '#agents'], 'emoji': 'ğŸš—', 'ru': {'title': 'AlphaDrive: Ğ˜Ğ˜ Ğ·Ğ° Ñ€ÑƒĞ»ĞµĞ¼ Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'AlphaDrive - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸. ĞĞ½Ğ° Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ GRPO, ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸. AlphaDrive Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸. ĞŸĞ¾ÑĞ»Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ÑÑ‰Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'AlphaDrive: Revolutionizing Autonomous Driving with RL and Reasoning', 'desc': 'This paper presents AlphaDrive, a novel framework that enhances autonomous driving by integrating reinforcement learning (RL) and reasoning with vision-language models (VLMs). AlphaDrive employs four GRPO-based RL rewards specifically designed for planning tasks and utilizes a two-stage training strategy that combines supervised fine-tuning (SFT) with RL. The results show that AlphaDrive significantly outperforms traditional methods that rely solely on SFT, leading to improved planning performance and training efficiency. Additionally, the framework demonstrates emergent multimodal planning capabilities post-RL training, which are essential for enhancing driving safety and efficiency.'}, 'zh': {'title': 'AlphaDriveï¼šæå‡è‡ªåŠ¨é©¾é©¶çš„æ™ºèƒ½è§„åˆ’ä¸æ¨ç†', 'desc': 'æœ¬æ–‡æå‡ºäº†AlphaDriveï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè‡ªåŠ¨é©¾é©¶çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å’Œæ¨ç†æ¡†æ¶ã€‚AlphaDriveå¼•å…¥äº†å››ç§åŸºäºGRPOçš„RLå¥–åŠ±ï¼Œä¸“é—¨é’ˆå¯¹è§„åˆ’ä»»åŠ¡ï¼Œå¹¶é‡‡ç”¨äº†ç»“åˆç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’ŒRLçš„ä¸¤é˜¶æ®µè§„åˆ’æ¨ç†è®­ç»ƒç­–ç•¥ã€‚ä¸ä»…ä½¿ç”¨SFTæˆ–ä¸è¿›è¡Œæ¨ç†çš„æƒ…å†µç›¸æ¯”ï¼ŒAlphaDriveæ˜¾è‘—æé«˜äº†è§„åˆ’æ€§èƒ½å’Œè®­ç»ƒæ•ˆç‡ã€‚æ­¤å¤–ï¼Œç»è¿‡RLè®­ç»ƒåï¼ŒAlphaDriveè¿˜å±•ç°å‡ºä¸€äº›æ–°å…´çš„å¤šæ¨¡æ€è§„åˆ’èƒ½åŠ›ï¼Œè¿™å¯¹æé«˜é©¾é©¶å®‰å…¨æ€§å’Œæ•ˆç‡è‡³å…³é‡è¦ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.06749', 'title': 'Vision-R1: Incentivizing Reasoning Capability in Multimodal Large\n  Language Models', 'url': 'https://huggingface.co/papers/2503.06749', 'abstract': "DeepSeek-R1-Zero has successfully demonstrated the emergence of reasoning capabilities in LLMs purely through Reinforcement Learning (RL). Inspired by this breakthrough, we explore how RL can be utilized to enhance the reasoning capability of MLLMs. However, direct training with RL struggles to activate complex reasoning capabilities such as questioning and reflection in MLLMs, due to the absence of substantial high-quality multimodal reasoning data. To address this issue, we propose the reasoning MLLM, Vision-R1, to improve multimodal reasoning capability. Specifically, we first construct a high-quality multimodal CoT dataset without human annotations by leveraging an existing MLLM and DeepSeek-R1 through modality bridging and data filtering to obtain a 200K multimodal CoT dataset, Vision-R1-cold dataset. It serves as cold-start initialization data for Vision-R1. To mitigate the optimization challenges caused by overthinking after cold start, we propose Progressive Thinking Suppression Training (PTST) strategy and employ Group Relative Policy Optimization (GRPO) with the hard formatting result reward function to gradually refine the model's ability to learn correct and complex reasoning processes on a 10K multimodal math dataset. Comprehensive experiments show our model achieves an average improvement of sim6% across various multimodal math reasoning benchmarks. Vision-R1-7B achieves a 73.5% accuracy on the widely used MathVista benchmark, which is only 0.4% lower than the leading reasoning model, OpenAI O1. The datasets and code will be released in: https://github.com/Osilly/Vision-R1 .", 'score': 14, 'issue_id': 2632, 'pub_date': '2025-03-09', 'pub_date_card': {'ru': '9 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 9', 'zh': '3æœˆ9æ—¥'}, 'hash': '403914241aa8967c', 'authors': ['Wenxuan Huang', 'Bohan Jia', 'Zijie Zhai', 'Shaosheng Cao', 'Zheyu Ye', 'Fei Zhao', 'Yao Hu', 'Shaohui Lin'], 'affiliations': ['East China Normal University', 'Xiaohongshu Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2503.06749.jpg', 'data': {'categories': ['#rl', '#training', '#optimization', '#multimodal', '#open_source', '#benchmark', '#reasoning', '#dataset', '#rag'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Vision-R1: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ˜Ğ˜', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Vision-R1, Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ½Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰ÑƒÑ MLLM Ğ¸ DeepSeek-R1. Ğ”Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ»Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Progressive Thinking Suppression Training Ğ¸ Group Relative Policy Optimization. Vision-R1-7B Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ° Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ 73.5% Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ MathVista, Ñ‡Ñ‚Ğ¾ Ğ»Ğ¸ÑˆÑŒ Ğ½Ğ° 0.4% Ğ½Ğ¸Ğ¶Ğµ Ğ²ĞµĞ´ÑƒÑ‰ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ OpenAI O1.'}, 'en': {'title': 'Enhancing Reasoning in MLLMs with Reinforcement Learning', 'desc': "The paper introduces DeepSeek-R1-Zero, which shows that large language models (LLMs) can develop reasoning skills through Reinforcement Learning (RL). It highlights the challenge of training LLMs directly with RL due to a lack of high-quality multimodal reasoning data. To overcome this, the authors propose a new model called Vision-R1, which utilizes a self-generated multimodal dataset for cold-start training. They also introduce a training strategy called Progressive Thinking Suppression Training (PTST) to enhance the model's reasoning capabilities, achieving significant improvements in multimodal math reasoning tasks."}, 'zh': {'title': 'é€šè¿‡å¼ºåŒ–å­¦ä¹ æå‡å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›', 'desc': 'DeepSeek-R1-Zeroå±•ç¤ºäº†é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä½¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å…·å¤‡æ¨ç†èƒ½åŠ›çš„å¯èƒ½æ€§ã€‚åŸºäºè¿™ä¸€çªç ´ï¼Œæœ¬æ–‡æ¢è®¨äº†å¦‚ä½•åˆ©ç”¨å¼ºåŒ–å­¦ä¹ æå‡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚ç”±äºç¼ºä¹é«˜è´¨é‡çš„å¤šæ¨¡æ€æ¨ç†æ•°æ®ï¼Œç›´æ¥ä½¿ç”¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒé¢ä¸´æŒ‘æˆ˜ï¼Œå› æ­¤æˆ‘ä»¬æå‡ºäº†Vision-R1æ¨¡å‹ï¼Œä»¥æ”¹å–„å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªé«˜è´¨é‡çš„å¤šæ¨¡æ€é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ•°æ®é›†ï¼Œå¹¶é€šè¿‡æ¸è¿›æ€ç»´æŠ‘åˆ¶è®­ç»ƒï¼ˆPTSTï¼‰å’Œç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ç­–ç•¥æ¥ä¼˜åŒ–æ¨¡å‹çš„æ¨ç†è¿‡ç¨‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.04629', 'title': 'SurveyForge: On the Outline Heuristics, Memory-Driven Generation, and\n  Multi-dimensional Evaluation for Automated Survey Writing', 'url': 'https://huggingface.co/papers/2503.04629', 'abstract': 'Survey paper plays a crucial role in scientific research, especially given the rapid growth of research publications. Recently, researchers have begun using LLMs to automate survey generation for better efficiency. However, the quality gap between LLM-generated surveys and those written by human remains significant, particularly in terms of outline quality and citation accuracy. To close these gaps, we introduce SurveyForge, which first generates the outline by analyzing the logical structure of human-written outlines and referring to the retrieved domain-related articles. Subsequently, leveraging high-quality papers retrieved from memory by our scholar navigation agent, SurveyForge can automatically generate and refine the content of the generated article. Moreover, to achieve a comprehensive evaluation, we construct SurveyBench, which includes 100 human-written survey papers for win-rate comparison and assesses AI-generated survey papers across three dimensions: reference, outline, and content quality. Experiments demonstrate that SurveyForge can outperform previous works such as AutoSurvey.', 'score': 14, 'issue_id': 2633, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 6', 'zh': '3æœˆ6æ—¥'}, 'hash': 'a229855316ab195d', 'authors': ['Xiangchao Yan', 'Shiyang Feng', 'Jiakang Yuan', 'Renqiu Xia', 'Bin Wang', 'Bo Zhang', 'Lei Bai'], 'affiliations': ['Fudan University', 'Shanghai Artificial Intelligence Laboratory', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2503.04629.jpg', 'data': {'categories': ['#survey', '#multimodal', '#agents', '#dataset', '#benchmark'], 'emoji': 'ğŸ“Š', 'ru': {'title': 'SurveyForge: ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ñ‚ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜', 'desc': 'SurveyForge - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ñ‚ĞµĞ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ ÑÑ‚Ğ°Ñ‚ÑŒĞ¸, Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ¾Ğ², Ğ½Ğ°Ğ¿Ğ¸ÑĞ°Ğ½Ğ½Ñ‹Ñ… Ğ»ÑĞ´ÑŒĞ¼Ğ¸, Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ñ‰Ğ°ÑÑÑŒ Ğº Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğ¼ ÑÑ‚Ğ°Ñ‚ÑŒÑĞ¼ Ğ² Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸. Ğ—Ğ°Ñ‚ĞµĞ¼, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ ÑÑ‚Ğ°Ñ‚ÑŒĞ¸, Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¸Ğ· Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸, SurveyForge Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ğµ ÑÑ‚Ğ°Ñ‚ÑŒĞ¸. Ğ”Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… SurveyBench, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ 100 Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ñ‚ĞµĞ¹, Ğ½Ğ°Ğ¿Ğ¸ÑĞ°Ğ½Ğ½Ñ‹Ñ… Ğ»ÑĞ´ÑŒĞ¼Ğ¸, Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ˜Ğ˜ Ğ¾Ğ±Ğ·Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ Ñ‚Ñ€ĞµĞ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼: ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑÑÑ‹Ğ»Ğ¾Ğº, ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¸ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'Enhancing Automated Survey Generation with SurveyForge', 'desc': 'This paper introduces SurveyForge, a tool designed to improve the quality of automated survey generation using large language models (LLMs). It addresses the significant quality gap between LLM-generated surveys and those created by humans, particularly in outline structure and citation accuracy. SurveyForge first generates an outline by analyzing human-written surveys and retrieving relevant articles, then it refines the content using high-quality papers. The authors also present SurveyBench, a benchmark for evaluating survey papers, which shows that SurveyForge outperforms previous methods like AutoSurvey.'}, 'zh': {'title': 'SurveyForgeï¼šæå‡æ–‡çŒ®ç»¼è¿°ç”Ÿæˆè´¨é‡çš„åˆ©å™¨', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†SurveyForgeï¼Œä¸€ä¸ªç”¨äºè‡ªåŠ¨ç”Ÿæˆæ–‡çŒ®ç»¼è¿°çš„å·¥å…·ã€‚å®ƒé€šè¿‡åˆ†æäººç±»æ’°å†™çš„ç»¼è¿°å¤§çº²çš„é€»è¾‘ç»“æ„ï¼Œå¹¶å‚è€ƒç›¸å…³é¢†åŸŸçš„æ–‡ç« ï¼Œé¦–å…ˆç”Ÿæˆå¤§çº²ã€‚ç„¶åï¼ŒSurveyForgeåˆ©ç”¨é«˜è´¨é‡çš„è®ºæ–‡æ¥è‡ªåŠ¨ç”Ÿæˆå’Œå®Œå–„æ–‡ç« å†…å®¹ã€‚ç ”ç©¶è¿˜æ„å»ºäº†SurveyBenchï¼Œç”¨äºè¯„ä¼°AIç”Ÿæˆçš„ç»¼è¿°è®ºæ–‡åœ¨å‚è€ƒæ–‡çŒ®ã€ç»“æ„å’Œå†…å®¹è´¨é‡ç­‰ä¸‰ä¸ªç»´åº¦çš„è¡¨ç°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.05244', 'title': 'WritingBench: A Comprehensive Benchmark for Generative Writing', 'url': 'https://huggingface.co/papers/2503.05244', 'abstract': "Recent advancements in large language models (LLMs) have significantly enhanced text generation capabilities, yet evaluating their performance in generative writing remains a challenge. Existing benchmarks primarily focus on generic text generation or limited in writing tasks, failing to capture the diverse requirements of high-quality written contents across various domains. To bridge this gap, we present WritingBench, a comprehensive benchmark designed to evaluate LLMs across 6 core writing domains and 100 subdomains, encompassing creative, persuasive, informative, and technical writing. We further propose a query-dependent evaluation framework that empowers LLMs to dynamically generate instance-specific assessment criteria. This framework is complemented by a fine-tuned critic model for criteria-aware scoring, enabling evaluations in style, format and length. The framework's validity is further demonstrated by its data curation capability, which enables 7B-parameter models to approach state-of-the-art (SOTA) performance. We open-source the benchmark, along with evaluation tools and modular framework components, to advance the development of LLMs in writing.", 'score': 13, 'issue_id': 2637, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 7', 'zh': '3æœˆ7æ—¥'}, 'hash': '679eb0b19323e2d2', 'authors': ['Yuning Wu', 'Jiahao Mei', 'Ming Yan', 'Chenliang Li', 'SHaopeng Lai', 'Yuran Ren', 'Zijia Wang', 'Ji Zhang', 'Mengyue Wu', 'Qin Jin', 'Fei Huang'], 'affiliations': ['Alibaba Group', 'Renmin University of China', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2503.05244.jpg', 'data': {'categories': ['#data', '#dataset', '#benchmark', '#story_generation', '#open_source'], 'emoji': 'âœï¸', 'ru': {'title': 'WritingBench: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¿Ğ¸ÑÑŒĞ¼Ğµ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ WritingBench - ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ… Ğ¿Ğ¸ÑÑŒĞ¼Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸ĞµĞ² Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ-ĞºÑ€Ğ¸Ñ‚Ğ¸Ğº Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ´ÑÑ‡ĞµÑ‚Ğ° Ğ±Ğ°Ğ»Ğ»Ğ¾Ğ². Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ 6 Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ Ğ¸ 100 Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ Ğ¿Ğ¸ÑÑŒĞ¼Ğ°, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ĞºÑ€ĞµĞ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ, ÑƒĞ±ĞµĞ¶Ğ´Ğ°ÑÑ‰ĞµĞµ, Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¸ Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ğ¸ÑÑŒĞ¼Ğ¾. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ·Ğ¸Ñ‚ÑŒÑÑ Ğº Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'WritingBench: A New Standard for Evaluating Language Models in Writing', 'desc': 'This paper introduces WritingBench, a new benchmark for evaluating large language models (LLMs) in generative writing across six key domains and 100 subdomains. It addresses the limitations of existing benchmarks that do not adequately assess the quality of writing in various contexts. The authors propose a query-dependent evaluation framework that allows LLMs to create specific assessment criteria based on the writing task at hand. Additionally, a fine-tuned critic model is introduced to provide criteria-aware scoring, enhancing the evaluation of style, format, and length in generated texts.'}, 'zh': {'title': 'WritingBenchï¼šå…¨é¢è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹çš„å†™ä½œèƒ½åŠ›', 'desc': 'è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¿›æ­¥æ˜¾è‘—æå‡äº†æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›ï¼Œä½†è¯„ä¼°å…¶åœ¨ç”Ÿæˆå†™ä½œä¸­çš„è¡¨ç°ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ç°æœ‰çš„åŸºå‡†ä¸»è¦é›†ä¸­åœ¨é€šç”¨æ–‡æœ¬ç”Ÿæˆæˆ–æœ‰é™çš„å†™ä½œä»»åŠ¡ä¸Šï¼Œæ— æ³•æ•æ‰åˆ°é«˜è´¨é‡ä¹¦é¢å†…å®¹åœ¨ä¸åŒé¢†åŸŸçš„å¤šæ ·åŒ–éœ€æ±‚ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†WritingBenchï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°LLMsåœ¨6ä¸ªæ ¸å¿ƒå†™ä½œé¢†åŸŸå’Œ100ä¸ªå­é¢†åŸŸçš„è¡¨ç°ï¼ŒåŒ…æ‹¬åˆ›æ„ã€è¯´æœæ€§ã€ä¿¡æ¯æ€§å’ŒæŠ€æœ¯å†™ä½œã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§ä¾èµ–æŸ¥è¯¢çš„è¯„ä¼°æ¡†æ¶ï¼Œä½¿LLMsèƒ½å¤ŸåŠ¨æ€ç”Ÿæˆç‰¹å®šå®ä¾‹çš„è¯„ä¼°æ ‡å‡†ï¼Œå¹¶é€šè¿‡ä¸€ä¸ªç»è¿‡å¾®è°ƒçš„è¯„ä¼°æ¨¡å‹è¿›è¡Œé£æ ¼ã€æ ¼å¼å’Œé•¿åº¦çš„è¯„åˆ†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.07602', 'title': 'DreamRelation: Relation-Centric Video Customization', 'url': 'https://huggingface.co/papers/2503.07602', 'abstract': "Relational video customization refers to the creation of personalized videos that depict user-specified relations between two subjects, a crucial task for comprehending real-world visual content. While existing methods can personalize subject appearances and motions, they still struggle with complex relational video customization, where precise relational modeling and high generalization across subject categories are essential. The primary challenge arises from the intricate spatial arrangements, layout variations, and nuanced temporal dynamics inherent in relations; consequently, current models tend to overemphasize irrelevant visual details rather than capturing meaningful interactions. To address these challenges, we propose DreamRelation, a novel approach that personalizes relations through a small set of exemplar videos, leveraging two key components: Relational Decoupling Learning and Relational Dynamics Enhancement. First, in Relational Decoupling Learning, we disentangle relations from subject appearances using relation LoRA triplet and hybrid mask training strategy, ensuring better generalization across diverse relationships. Furthermore, we determine the optimal design of relation LoRA triplet by analyzing the distinct roles of the query, key, and value features within MM-DiT's attention mechanism, making DreamRelation the first relational video generation framework with explainable components. Second, in Relational Dynamics Enhancement, we introduce space-time relational contrastive loss, which prioritizes relational dynamics while minimizing the reliance on detailed subject appearances. Extensive experiments demonstrate that DreamRelation outperforms state-of-the-art methods in relational video customization. Code and models will be made publicly available.", 'score': 12, 'issue_id': 2632, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 10', 'zh': '3æœˆ10æ—¥'}, 'hash': 'a32c943630a808fc', 'authors': ['Yujie Wei', 'Shiwei Zhang', 'Hangjie Yuan', 'Biao Gong', 'Longxiang Tang', 'Xiang Wang', 'Haonan Qiu', 'Hengjia Li', 'Shuai Tan', 'Yingya Zhang', 'Hongming Shan'], 'affiliations': ['Alibaba Group', 'Ant Group', 'Fudan University', 'Nanyang Technological University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.07602.jpg', 'data': {'categories': ['#optimization', '#multimodal', '#architecture', '#open_source', '#video', '#interpretability', '#games'], 'emoji': 'ğŸ¬', 'ru': {'title': 'DreamRelation: ĞŸĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ', 'desc': 'DreamRelation - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´Ğ²Ğ° ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°: Relational Decoupling Learning Ğ¸ Relational Dynamics Enhancement. Relational Decoupling Learning Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğ¹ Ğ²Ğ¸Ğ´ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ relation LoRA triplet Ğ¸ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ°ÑĞºĞ°Ğ¼. Relational Dynamics Enhancement Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ½ÑƒÑ Ğ¿Ğ¾Ñ‚ĞµÑ€Ñ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ ÑĞ¾ÑÑ€ĞµĞ´Ğ¾Ñ‚Ğ¾Ñ‡Ğ¸Ñ‚ÑŒÑÑ Ğ½Ğ° Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞµ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¹, Ğ° Ğ½Ğµ Ğ½Ğ° Ğ´ĞµÑ‚Ğ°Ğ»ÑÑ… Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ Ğ²Ğ¸Ğ´Ğ° ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ¾Ğ².'}, 'en': {'title': 'DreamRelation: Personalizing Video Relationships with Precision', 'desc': 'This paper introduces DreamRelation, a new method for creating personalized videos that focus on the relationships between two subjects. Current techniques struggle with complex relational dynamics, often missing meaningful interactions due to an overemphasis on irrelevant details. DreamRelation addresses this by using Relational Decoupling Learning to separate relationships from appearances and Relational Dynamics Enhancement to focus on the dynamics of these relationships. The approach shows significant improvements over existing methods in generating customized relational videos, with the added benefit of explainable components in its design.'}, 'zh': {'title': 'DreamRelationï¼šä¸ªæ€§åŒ–è§†é¢‘å…³ç³»å»ºæ¨¡çš„æ–°æ–¹æ³•', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºDreamRelationçš„æ–°æ–¹æ³•ï¼Œç”¨äºä¸ªæ€§åŒ–è§†é¢‘ä¸­çš„å…³ç³»å»ºæ¨¡ã€‚è¯¥æ–¹æ³•é€šè¿‡å…³ç³»è§£è€¦å­¦ä¹ å’Œå…³ç³»åŠ¨æ€å¢å¼ºä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼Œè§£å†³äº†å¤æ‚å…³ç³»è§†é¢‘å®šåˆ¶ä¸­çš„æŒ‘æˆ˜ã€‚é€šè¿‡åˆ†ææŸ¥è¯¢ã€é”®å’Œå€¼ç‰¹å¾åœ¨æ³¨æ„åŠ›æœºåˆ¶ä¸­çš„ä½œç”¨ï¼ŒDreamRelationå®ç°äº†å¯è§£é‡Šçš„å…³ç³»è§†é¢‘ç”Ÿæˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDreamRelationåœ¨å…³ç³»è§†é¢‘å®šåˆ¶æ–¹é¢ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.07459', 'title': 'MedAgentsBench: Benchmarking Thinking Models and Agent Frameworks for\n  Complex Medical Reasoning', 'url': 'https://huggingface.co/papers/2503.07459', 'abstract': 'Large Language Models (LLMs) have shown impressive performance on existing medical question-answering benchmarks. This high performance makes it increasingly difficult to meaningfully evaluate and differentiate advanced methods. We present <PRE_TAG>MedAgentsBench</POST_TAG>, a benchmark that focuses on challenging medical questions requiring multi-step clinical reasoning, diagnosis formulation, and treatment planning-scenarios where current models still struggle despite their strong performance on standard tests. Drawing from seven established medical datasets, our benchmark addresses three key limitations in existing evaluations: (1) the prevalence of straightforward questions where even base models achieve high performance, (2) inconsistent sampling and evaluation protocols across studies, and (3) lack of systematic analysis of the interplay between performance, cost, and inference time. Through experiments with various base models and reasoning methods, we demonstrate that the latest thinking models, DeepSeek R1 and OpenAI o3, exhibit exceptional performance in complex medical reasoning tasks. Additionally, advanced search-based agent methods offer promising performance-to-cost ratios compared to traditional approaches. Our analysis reveals substantial performance gaps between model families on complex questions and identifies optimal model selections for different computational constraints. Our benchmark and evaluation framework are publicly available at https://github.com/gersteinlab/medagents-benchmark.', 'score': 11, 'issue_id': 2634, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 10', 'zh': '3æœˆ10æ—¥'}, 'hash': '4d9aba5593231609', 'authors': ['Xiangru Tang', 'Daniel Shao', 'Jiwoong Sohn', 'Jiapeng Chen', 'Jiayi Zhang', 'Jinyu Xiang', 'Fang Wu', 'Yilun Zhao', 'Chenglin Wu', 'Wenqi Shi', 'Arman Cohan', 'Mark Gerstein'], 'affiliations': ['Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2503.07459.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#survey', '#healthcare'], 'emoji': 'ğŸ©º', 'ru': {'title': 'ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº <PRE_TAG>MedAgentsBench</POST_TAG> Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½Ñ‹. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½Ñ‡Ğ°Ñ‚Ñ‹Ñ… ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ½Ğ¾Ğ²ĞµĞ¹ÑˆĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº DeepSeek R1 Ğ¸ OpenAI o3, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¸ÑĞºĞ»ÑÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ», Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¾Ğ±ĞµÑ‰Ğ°ÑÑ‰ĞµĞµ ÑĞ¾Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'MedAgentsBench: Elevating Medical Question-Answering Evaluation', 'desc': 'This paper introduces MedAgentsBench, a new benchmark designed to evaluate Large Language Models (LLMs) on complex medical questions that require multi-step reasoning. It addresses limitations in current evaluations, such as the prevalence of simple questions and inconsistent testing protocols. The authors conduct experiments with various models, revealing that advanced models like DeepSeek R1 and OpenAI o3 perform well on challenging tasks, while search-based agents show better performance-to-cost ratios. The study highlights significant performance gaps among different model families and provides insights for selecting models based on computational resources.'}, 'zh': {'title': 'åŒ»å­¦é—®ç­”çš„æ–°åŸºå‡†ï¼šæŒ‘æˆ˜å¤æ‚æ¨ç†', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªæ–°çš„åŒ»å­¦é—®ç­”åŸºå‡†æµ‹è¯•â€”â€”MedAgentsBenchï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚åŒ»å­¦é—®é¢˜ä¸Šçš„è¡¨ç°ã€‚è¯¥åŸºå‡†æµ‹è¯•ä¸“æ³¨äºéœ€è¦å¤šæ­¥éª¤ä¸´åºŠæ¨ç†ã€è¯Šæ–­åˆ¶å®šå’Œæ²»ç–—è®¡åˆ’çš„é—®é¢˜ï¼Œè¿™äº›é—®é¢˜æ˜¯å½“å‰æ¨¡å‹ä»ç„¶é¢ä¸´æŒ‘æˆ˜çš„é¢†åŸŸã€‚é€šè¿‡å¯¹ä¸ƒä¸ªå·²å»ºç«‹çš„åŒ»å­¦æ•°æ®é›†è¿›è¡Œåˆ†æï¼Œæœ¬æ–‡è§£å†³äº†ç°æœ‰è¯„ä¼°ä¸­çš„ä¸‰ä¸ªå…³é”®é™åˆ¶ï¼ŒåŒ…æ‹¬ç®€å•é—®é¢˜çš„æ™®éæ€§å’Œè¯„ä¼°åè®®çš„ä¸ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæœ€æ–°çš„æ€ç»´æ¨¡å‹åœ¨å¤æ‚åŒ»å­¦æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå¹¶ä¸”åŸºäºæœç´¢çš„ä»£ç†æ–¹æ³•åœ¨æ€§èƒ½ä¸æˆæœ¬æ¯”æ–¹é¢å…·æœ‰è‰¯å¥½çš„å‰æ™¯ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.06580', 'title': 'Agent models: Internalizing Chain-of-Action Generation into Reasoning\n  models', 'url': 'https://huggingface.co/papers/2503.06580', 'abstract': 'Traditional agentic workflows rely on external prompts to manage interactions with tools and the environment, which limits the autonomy of reasoning models. We position Large Agent Models (LAMs) that internalize the generation of Chain-of-Action (CoA), enabling the model to autonomously decide when and how to use external tools. Our proposed AutoCoA framework combines supervised fine-tuning (SFT) and reinforcement learning (RL), allowing the model to seamlessly switch between reasoning and action while efficiently managing environment interactions. Main components include step-level action triggering, trajectory-level CoA optimization, and an internal world model to reduce real-environment interaction costs. Evaluations on open-domain QA tasks demonstrate that AutoCoA-trained agent models significantly outperform ReAct-based workflows in task completion, especially in tasks that require long-term reasoning and multi-step actions. Code and dataset are available at https://github.com/ADaM-BJTU/AutoCoA', 'score': 11, 'issue_id': 2631, 'pub_date': '2025-03-09', 'pub_date_card': {'ru': '9 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 9', 'zh': '3æœˆ9æ—¥'}, 'hash': '01588376bab86ceb', 'authors': ['Yuxiang Zhang', 'Yuqi Yang', 'Jiangming Shu', 'Xinyan Wen', 'Jitao Sang'], 'affiliations': ['School of Computer Science and Technology, Beijing Jiaotong University, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.06580.jpg', 'data': {'categories': ['#reasoning', '#optimization', '#agents', '#rl', '#training'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞĞ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑˆĞ°Ğ³ Ğº ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ˜Ğ˜', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°, Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ñ… Large Agent Models (LAM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº AutoCoA, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ¾ĞºÑ€ÑƒĞ¶Ğ°ÑÑ‰ĞµĞ¹ ÑÑ€ĞµĞ´Ğ¾Ğ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ AutoCoA, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹.'}, 'en': {'title': 'Empowering Autonomous Reasoning with AutoCoA', 'desc': 'This paper introduces Large Agent Models (LAMs) that enhance the autonomy of reasoning models by allowing them to generate their own Chain-of-Action (CoA) without relying on external prompts. The AutoCoA framework integrates supervised fine-tuning (SFT) and reinforcement learning (RL) to enable models to effectively alternate between reasoning and taking actions in their environment. Key features of this framework include mechanisms for triggering actions at each step, optimizing CoA over entire trajectories, and utilizing an internal world model to minimize the costs associated with real-world interactions. Evaluations show that models trained with AutoCoA outperform traditional ReAct-based workflows, particularly in complex tasks requiring extended reasoning and multiple actions.'}, 'zh': {'title': 'è‡ªä¸»å†³ç­–çš„æ™ºèƒ½ä»£ç†æ¨¡å‹', 'desc': 'ä¼ ç»Ÿçš„æ™ºèƒ½å·¥ä½œæµç¨‹ä¾èµ–å¤–éƒ¨æç¤ºæ¥ç®¡ç†ä¸å·¥å…·å’Œç¯å¢ƒçš„äº¤äº’ï¼Œè¿™é™åˆ¶äº†æ¨ç†æ¨¡å‹çš„è‡ªä¸»æ€§ã€‚æˆ‘ä»¬æå‡ºäº†å¤§å‹ä»£ç†æ¨¡å‹ï¼ˆLAMsï¼‰ï¼Œä½¿å…¶èƒ½å¤Ÿå†…éƒ¨ç”Ÿæˆè¡ŒåŠ¨é“¾ï¼ˆCoAï¼‰ï¼Œä»è€Œè‡ªä¸»å†³å®šä½•æ—¶ä»¥åŠå¦‚ä½•ä½¿ç”¨å¤–éƒ¨å·¥å…·ã€‚æˆ‘ä»¬æå‡ºçš„AutoCoAæ¡†æ¶ç»“åˆäº†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨æ¨ç†å’Œè¡ŒåŠ¨ä¹‹é—´æ— ç¼åˆ‡æ¢ï¼ŒåŒæ—¶æœ‰æ•ˆç®¡ç†ä¸ç¯å¢ƒçš„äº¤äº’ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œç»è¿‡AutoCoAè®­ç»ƒçš„ä»£ç†æ¨¡å‹åœ¨å¼€æ”¾é¢†åŸŸé—®ç­”ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºåŸºäºReActçš„å·¥ä½œæµç¨‹ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦é•¿æœŸæ¨ç†å’Œå¤šæ­¥éª¤è¡ŒåŠ¨çš„ä»»åŠ¡ä¸­ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.04973', 'title': 'Beyond RAG: Task-Aware KV Cache Compression for Comprehensive Knowledge\n  Reasoning', 'url': 'https://huggingface.co/papers/2503.04973', 'abstract': 'Incorporating external knowledge in large language models (LLMs) enhances their utility across diverse applications, but existing methods have trade-offs. Retrieval-Augmented Generation (RAG) fetches evidence via similarity search, but key information may fall outside top ranked results. Long-context models can process multiple documents but are computationally expensive and limited by context window size. Inspired by students condensing study material for open-book exams, we propose task-aware key-value (KV) cache compression, which compresses external knowledge in a zero- or few-shot setup. This enables LLMs to reason efficiently over a compacted representation of all relevant information. Experiments show our approach outperforms both RAG and task-agnostic compression methods. On LongBench v2, it improves accuracy by up to 7 absolute points over RAG with a 30x compression rate, while reducing inference latency from 0.43s to 0.16s. A synthetic dataset highlights that RAG performs well when sparse evidence suffices, whereas task-aware compression is superior for broad knowledge tasks.', 'score': 11, 'issue_id': 2640, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 6', 'zh': '3æœˆ6æ—¥'}, 'hash': 'a3117b7e2b2c099c', 'authors': ['Giulio Corallo', 'Orion Weller', 'Fabio Petroni', 'Paolo Papotti'], 'affiliations': ['EURECOM', 'Johns Hopkins University', 'SAP Labs', 'Samaya AI'], 'pdf_title_img': 'assets/pdf/title_img/2503.04973.jpg', 'data': {'categories': ['#reasoning', '#synthetic', '#long_context', '#rag', '#inference'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±Ğ¾Ğ³Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ LLM Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±Ğ¾Ğ³Ğ°Ñ‰ĞµĞ½Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) - ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ ĞºÑÑˆĞ° ĞºĞ»ÑÑ‡-Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ LLM ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ¶Ğ°Ñ‚Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²ÑĞµĞ¹ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ĞºĞ°Ğº RAG, Ñ‚Ğ°Ğº Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ, Ğ½Ğµ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ. ĞĞ° Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ LongBench v2 Ğ¾Ğ½ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° 7 Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿ÑƒĞ½ĞºÑ‚Ğ¾Ğ² Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ RAG Ğ¿Ñ€Ğ¸ 30-ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğ¼ ÑĞ¶Ğ°Ñ‚Ğ¸Ğ¸, Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°Ñ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºÑƒ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°.'}, 'en': {'title': 'Efficient Knowledge Integration for Enhanced Language Model Performance', 'desc': 'This paper presents a new method for enhancing large language models (LLMs) by incorporating external knowledge more effectively. The proposed task-aware key-value (KV) cache compression allows LLMs to efficiently reason over a compact representation of relevant information, improving performance in zero- or few-shot scenarios. Compared to existing methods like Retrieval-Augmented Generation (RAG), this approach achieves better accuracy and significantly reduces inference time. Experiments demonstrate that while RAG excels with sparse evidence, the new compression technique is more effective for tasks requiring extensive knowledge.'}, 'zh': {'title': 'ä»»åŠ¡æ„ŸçŸ¥å‹ç¼©ï¼Œæå‡è¯­è¨€æ¨¡å‹æ•ˆç‡', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œé€šè¿‡ä»»åŠ¡æ„ŸçŸ¥çš„é”®å€¼ï¼ˆKVï¼‰ç¼“å­˜å‹ç¼©æ¥æ•´åˆå¤–éƒ¨çŸ¥è¯†ï¼Œä»¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ•ˆç‡ã€‚è¯¥æ–¹æ³•åœ¨é›¶æ ·æœ¬æˆ–å°‘æ ·æœ¬è®¾ç½®ä¸‹å‹ç¼©å¤–éƒ¨çŸ¥è¯†ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨å¤„ç†ç›¸å…³ä¿¡æ¯æ—¶æ›´åŠ é«˜æ•ˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å‡†ç¡®æ€§å’Œæ¨ç†å»¶è¿Ÿæ–¹é¢å‡ä¼˜äºç°æœ‰çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å’Œä»»åŠ¡æ— å…³çš„å‹ç¼©æ–¹æ³•ã€‚ç‰¹åˆ«æ˜¯åœ¨LongBench v2æ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•çš„å‡†ç¡®æ€§æé«˜äº†7ä¸ªç™¾åˆ†ç‚¹ï¼ŒåŒæ—¶æ¨ç†å»¶è¿Ÿä»0.43ç§’å‡å°‘åˆ°0.16ç§’ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.04812', 'title': 'LLaVE: Large Language and Vision Embedding Models with Hardness-Weighted\n  Contrastive Learning', 'url': 'https://huggingface.co/papers/2503.04812', 'abstract': "Universal multimodal embedding models play a critical role in tasks such as interleaved image-text retrieval, multimodal RAG, and multimodal clustering. However, our empirical results indicate that existing LMM-based embedding models trained with the standard InfoNCE loss exhibit a high degree of overlap in similarity distribution between positive and negative pairs, making it challenging to distinguish hard negative pairs effectively. To deal with this issue, we propose a simple yet effective framework that dynamically improves the embedding model's representation learning for negative pairs based on their discriminative difficulty. Within this framework, we train a series of models, named LLaVE, and evaluate them on the MMEB benchmark, which covers 4 meta-tasks and 36 datasets. Experimental results show that LLaVE establishes stronger baselines that achieve state-of-the-art (SOTA) performance while demonstrating strong scalability and efficiency. Specifically, LLaVE-2B surpasses the previous SOTA 7B models, while LLaVE-7B achieves a further performance improvement of 6.2 points. Although LLaVE is trained on image-text data, it can generalize to text-video retrieval tasks in a zero-shot manner and achieve strong performance, demonstrating its remarkable potential for transfer to other embedding tasks.", 'score': 10, 'issue_id': 2631, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 4', 'zh': '3æœˆ4æ—¥'}, 'hash': 'a1fec2227e343e88', 'authors': ['Zhibin Lan', 'Liqiang Niu', 'Fandong Meng', 'Jie Zhou', 'Jinsong Su'], 'affiliations': ['Pattern Recognition Center, WeChat AI, Tencent Inc, China', 'School of Informatics, Xiamen University, China', 'Shanghai Artificial Intelligence Laboratory, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.04812.jpg', 'data': {'categories': ['#transfer_learning', '#benchmark', '#rag', '#multimodal', '#training'], 'emoji': 'ğŸ”€', 'ru': {'title': 'Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ½ĞµĞ³Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ°Ñ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¿ĞµÑ€ĞµĞºÑ€Ñ‹Ñ‚Ğ¸Ñ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ° Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¸ Ğ½ĞµĞ³Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€ Ğ² ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ½ĞµĞ³Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸Ñ… ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ LLaVE Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ MMEB Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ñ…Ğ¾Ñ€Ğ¾ÑˆÑƒÑ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ.'}, 'en': {'title': 'Enhancing Multimodal Embeddings with LLaVE for Better Performance', 'desc': 'This paper discusses the development of a new framework called LLaVE for improving multimodal embedding models, which are used for tasks involving both images and text. The authors identify a problem with existing models that struggle to differentiate between similar positive and negative pairs due to overlapping similarity distributions. To address this, LLaVE dynamically enhances the learning of negative pairs based on their difficulty, leading to better representation learning. The results show that LLaVE outperforms previous state-of-the-art models and can also be applied effectively to other tasks like text-video retrieval.'}, 'zh': {'title': 'LLaVEï¼šæå‡å¤šæ¨¡æ€åµŒå…¥çš„å¼ºå¤§å·¥å…·', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¤šæ¨¡æ€åµŒå…¥æ¨¡å‹LLaVEï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ¨¡å‹åœ¨å¤„ç†æ­£è´Ÿæ ·æœ¬æ—¶ç›¸ä¼¼åº¦åˆ†å¸ƒé‡å çš„é—®é¢˜ã€‚é€šè¿‡åŠ¨æ€è°ƒæ•´è´Ÿæ ·æœ¬çš„è¡¨ç¤ºå­¦ä¹ ï¼ŒLLaVEèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åŒºåˆ†å›°éš¾çš„è´Ÿæ ·æœ¬ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLLaVEåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¶…è¶Šäº†ä¹‹å‰çš„æœ€å…ˆè¿›æ¨¡å‹ï¼Œå¹¶åœ¨å›¾åƒ-æ–‡æœ¬æ•°æ®ä¸Šè®­ç»ƒåï¼Œèƒ½å¤Ÿåœ¨é›¶æ ·æœ¬æƒ…å†µä¸‹æ¨å¹¿åˆ°æ–‡æœ¬-è§†é¢‘æ£€ç´¢ä»»åŠ¡ã€‚è¯¥æ¨¡å‹å±•ç¤ºäº†å¼ºå¤§çš„å¯æ‰©å±•æ€§å’Œæ•ˆç‡ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.07334', 'title': 'Unleashing the Potential of Large Language Models for Text-to-Image\n  Generation through Autoregressive Representation Alignment', 'url': 'https://huggingface.co/papers/2503.07334', 'abstract': "We present Autoregressive Representation Alignment (ARRA), a new training framework that unlocks global-coherent text-to-image generation in autoregressive LLMs without architectural changes. Unlike prior work that requires complex architectural redesigns, ARRA aligns LLM hidden states with visual representations from external visual foundational models via a global visual alignment loss and a hybrid token, <HYBNEXT>. This token enforces dual constraints: local next-token prediction and global semantic distillation, enabling LLMs to implicitly learn spatial and contextual coherence while retaining their original autoregressive paradigm. Extensive experiments validate ARRA's plug-and-play versatility. When training from text-generation-only LLMs or random initialization, ARRA reduces FID by 25.5% (MIMIC-CXR), 8.8% (DeepEyeNet), and 7.5% (ImageNet) for advanced autoregressive LLMs like Chameleon and LlamaGen, all without framework modifications. For domain adaption, ARRA aligns general-purpose LLMs with specialized models (e.g., BioMedCLIP), achieving an 18.6% FID reduction over direct fine-tuning on medical imaging (MIMIC-CXR). By demonstrating that training objective redesign -- not just architectural innovation -- can resolve cross-modal global coherence challenges, ARRA offers a complementary paradigm for advancing autoregressive models. Code and models will be released to advance autoregressive image generation.", 'score': 9, 'issue_id': 2643, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 10', 'zh': '3æœˆ10æ—¥'}, 'hash': 'da94bc1af48685ac', 'authors': ['Xing Xie', 'Jiawei Liu', 'Ziyue Lin', 'Huijie Fan', 'Zhi Han', 'Yandong Tang', 'Liangqiong Qu'], 'affiliations': ['Shenyang Institute of Automation, Chinese Academy of Sciences', 'The University of Hong Kong', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2503.07334.jpg', 'data': {'categories': ['#training', '#optimization', '#alignment', '#open_source', '#multimodal'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'ARRA: Ğ“Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ARRA, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµgressĞ¸Ğ²Ğ½Ñ‹Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹. ARRA Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ¸ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ°. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ½ĞµÑĞ²Ğ½Ğ¾ ÑƒÑ‡Ğ¸Ñ‚ÑŒÑÑ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ARRA Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ¾Ğ².'}, 'en': {'title': 'Unlocking Coherent Text-to-Image Generation with ARRA', 'desc': "The paper introduces Autoregressive Representation Alignment (ARRA), a novel training framework that enhances text-to-image generation in autoregressive language models (LLMs) without changing their architecture. ARRA achieves this by aligning the hidden states of LLMs with visual representations from external models using a global visual alignment loss and a special hybrid token, <HYBNEXT>. This token imposes both local next-token prediction and global semantic distillation, allowing LLMs to learn spatial and contextual coherence effectively. The results show significant improvements in image generation quality, as evidenced by reduced FrÃ©chet Inception Distance (FID) scores across various datasets, demonstrating ARRA's effectiveness and versatility in enhancing autoregressive models."}, 'zh': {'title': 'è‡ªå›å½’æ¨¡å‹çš„æ–°çªç ´ï¼šå…¨çƒä¸€è‡´æ€§ç”Ÿæˆ', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è®­ç»ƒæ¡†æ¶ï¼Œç§°ä¸ºè‡ªå›å½’è¡¨ç¤ºå¯¹é½ï¼ˆARRAï¼‰ï¼Œæ—¨åœ¨å®ç°è‡ªå›å½’å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„å…¨çƒä¸€è‡´æ€§æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆï¼Œè€Œæ— éœ€æ”¹å˜æ¨¡å‹æ¶æ„ã€‚ARRAé€šè¿‡å…¨å±€è§†è§‰å¯¹é½æŸå¤±å’Œæ··åˆæ ‡è®°<HYBNEXT>ï¼Œå°†LLMçš„éšè—çŠ¶æ€ä¸å¤–éƒ¨è§†è§‰åŸºç¡€æ¨¡å‹çš„è§†è§‰è¡¨ç¤ºå¯¹é½ã€‚è¯¥æ ‡è®°æ–½åŠ äº†å±€éƒ¨ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹å’Œå…¨å±€è¯­ä¹‰è’¸é¦çš„åŒé‡çº¦æŸï¼Œä½¿LLMèƒ½å¤Ÿåœ¨ä¿æŒè‡ªå›å½’èŒƒå¼çš„åŒæ—¶ï¼Œéšå¼å­¦ä¹ ç©ºé—´å’Œä¸Šä¸‹æ–‡çš„ä¸€è‡´æ€§ã€‚å®éªŒç»“æœéªŒè¯äº†ARRAçš„çµæ´»æ€§ï¼Œæ˜¾ç¤ºå‡ºåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šæ˜¾è‘—é™ä½äº†FIDå€¼ï¼Œè¯æ˜äº†è®­ç»ƒç›®æ ‡çš„é‡æ–°è®¾è®¡å¯ä»¥è§£å†³è·¨æ¨¡æ€å…¨çƒä¸€è‡´æ€§æŒ‘æˆ˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.07507', 'title': 'PE3R: Perception-Efficient 3D Reconstruction', 'url': 'https://huggingface.co/papers/2503.07507', 'abstract': 'Recent advancements in 2D-to-3D perception have significantly improved the understanding of 3D scenes from 2D images. However, existing methods face critical challenges, including limited generalization across scenes, suboptimal perception accuracy, and slow reconstruction speeds. To address these limitations, we propose Perception-Efficient 3D Reconstruction (PE3R), a novel framework designed to enhance both accuracy and efficiency. PE3R employs a feed-forward architecture to enable rapid 3D semantic field reconstruction. The framework demonstrates robust zero-shot generalization across diverse scenes and objects while significantly improving reconstruction speed. Extensive experiments on 2D-to-3D open-vocabulary segmentation and 3D reconstruction validate the effectiveness and versatility of PE3R. The framework achieves a minimum 9-fold speedup in 3D semantic field reconstruction, along with substantial gains in perception accuracy and reconstruction precision, setting new benchmarks in the field. The code is publicly available at: https://github.com/hujiecpp/PE3R.', 'score': 8, 'issue_id': 2631, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 10', 'zh': '3æœˆ10æ—¥'}, 'hash': '6a53d341839fc41f', 'authors': ['Jie Hu', 'Shizun Wang', 'Xinchao Wang'], 'affiliations': ['xML Lab, National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2503.07507.jpg', 'data': {'categories': ['#3d', '#architecture', '#benchmark'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ‘Ñ‹ÑÑ‚Ñ€Ğ°Ñ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ°Ñ 3D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ğ¸Ğ· 2D-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'PE3R - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ 3D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸Ğ· 2D-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ÑĞ¼ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ 3D ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ğ¾Ğ»ĞµĞ¹. PE3R Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½ÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ… Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ñ… Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Revolutionizing 3D Scene Understanding with PE3R', 'desc': 'This paper introduces Perception-Efficient 3D Reconstruction (PE3R), a new framework that enhances the process of understanding 3D scenes from 2D images. PE3R addresses key challenges in existing methods, such as limited generalization and slow reconstruction speeds, by utilizing a feed-forward architecture. The framework achieves impressive zero-shot generalization across various scenes and objects, while also significantly speeding up the reconstruction process. Experimental results show that PE3R offers at least a 9-fold increase in reconstruction speed and improved accuracy, establishing new benchmarks in 2D-to-3D perception.'}, 'zh': {'title': 'æ„ŸçŸ¥é«˜æ•ˆ3Dé‡å»ºï¼Œé€Ÿåº¦ä¸ç²¾åº¦çš„åŒé‡æå‡', 'desc': 'æœ€è¿‘åœ¨2Dåˆ°3Dæ„ŸçŸ¥æ–¹é¢çš„è¿›å±•æ˜¾è‘—æé«˜äº†ä»2Då›¾åƒç†è§£3Dåœºæ™¯çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é¢ä¸´ç€åœºæ™¯æ³›åŒ–èƒ½åŠ›æœ‰é™ã€æ„ŸçŸ¥ç²¾åº¦ä¸ä½³å’Œé‡å»ºé€Ÿåº¦æ…¢ç­‰å…³é”®æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†æ„ŸçŸ¥é«˜æ•ˆ3Dé‡å»ºï¼ˆPE3Rï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚PE3Ré‡‡ç”¨å‰é¦ˆæ¶æ„ï¼Œèƒ½å¤Ÿå¿«é€Ÿé‡å»º3Dè¯­ä¹‰åœºï¼Œå¹¶åœ¨å¤šæ ·åœºæ™¯å’Œç‰©ä½“ä¸Šå±•ç¤ºå‡ºå¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼ŒåŒæ—¶æ˜¾è‘—æé«˜é‡å»ºé€Ÿåº¦ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.07197', 'title': 'Effective and Efficient Masked Image Generation Models', 'url': 'https://huggingface.co/papers/2503.07197', 'abstract': "Although masked image generation models and masked diffusion models are designed with different motivations and objectives, we observe that they can be unified within a single framework. Building upon this insight, we carefully explore the design space of training and sampling, identifying key factors that contribute to both performance and efficiency. Based on the improvements observed during this exploration, we develop our model, referred to as eMIGM. Empirically, eMIGM demonstrates strong performance on ImageNet generation, as measured by Fr\\'echet Inception Distance (FID). In particular, on ImageNet 256x256, with similar number of function evaluations (NFEs) and model parameters, eMIGM outperforms the seminal VAR. Moreover, as NFE and model parameters increase, eMIGM achieves performance comparable to the state-of-the-art continuous diffusion models while requiring less than 40% of the NFE. Additionally, on ImageNet 512x512, with only about 60% of the NFE, eMIGM outperforms the state-of-the-art continuous diffusion models.", 'score': 8, 'issue_id': 2631, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 10', 'zh': '3æœˆ10æ—¥'}, 'hash': '4740cc0178bbf099', 'authors': ['Zebin You', 'Jingyang Ou', 'Xiaolu Zhang', 'Jun Hu', 'Jun Zhou', 'Chongxuan Li'], 'affiliations': ['Ant Group', 'Beijing Key Laboratory of Big Data Management and Analysis Method', 'Gaoling School of AI, Renmin University of China'], 'pdf_title_img': 'assets/pdf/title_img/2503.07197.jpg', 'data': {'categories': ['#optimization', '#diffusion', '#training', '#cv'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½Ğ¸Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² ĞµĞ´Ğ¸Ğ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ. ĞĞ½Ğ¸ Ğ¸Ğ·ÑƒÑ‡Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ²Ñ‹ÑĞ²Ğ¸Ğ² ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ñ‹, Ğ²Ğ»Ğ¸ÑÑÑ‰Ğ¸Ğµ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ±Ñ‹Ğ»Ğ° Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ eMIGM, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ ImageNet. eMIGM Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾ÑˆĞ»Ğ° ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞµ FID Ğ¿Ñ€Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞµĞ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ².'}, 'en': {'title': 'Unifying Masked Models for Efficient Image Generation', 'desc': "This paper presents a unified framework for masked image generation models and masked diffusion models, highlighting their similarities despite different goals. The authors explore various design choices in training and sampling, which significantly impact the model's performance and efficiency. They introduce a new model called eMIGM, which shows superior results on ImageNet generation tasks, particularly in terms of FrÃ©chet Inception Distance (FID). Notably, eMIGM achieves competitive performance with fewer function evaluations compared to existing state-of-the-art models, demonstrating its efficiency and effectiveness in image generation."}, 'zh': {'title': 'ç»Ÿä¸€æ©è”½æ¨¡å‹ï¼Œæå‡å›¾åƒç”Ÿæˆæ€§èƒ½', 'desc': 'æœ¬æ–‡æ¢è®¨äº†æ©è”½å›¾åƒç”Ÿæˆæ¨¡å‹å’Œæ©è”½æ‰©æ•£æ¨¡å‹çš„ç»Ÿä¸€æ¡†æ¶ã€‚æˆ‘ä»¬åˆ†æäº†è®­ç»ƒå’Œé‡‡æ ·çš„è®¾è®¡ç©ºé—´ï¼Œè¯†åˆ«å‡ºå½±å“æ€§èƒ½å’Œæ•ˆç‡çš„å…³é”®å› ç´ ã€‚åŸºäºè¿™äº›æ”¹è¿›ï¼Œæˆ‘ä»¬å¼€å‘äº†åä¸ºeMIGMçš„æ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒeMIGMåœ¨ImageNetç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶åœ¨è¾ƒä½çš„å‡½æ•°è¯„ä¼°æ¬¡æ•°ä¸‹è¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.05856', 'title': 'This Is Your Doge, If It Please You: Exploring Deception and Robustness\n  in Mixture of LLMs', 'url': 'https://huggingface.co/papers/2503.05856', 'abstract': "Mixture of <PRE_TAG>large language model (LLMs) Agents (MoA)</POST_TAG> architectures achieve state-of-the-art performance on prominent benchmarks like AlpacaEval 2.0 by leveraging the collaboration of multiple LLMs at inference time. Despite these successes, an evaluation of the safety and reliability of MoA is missing. We present the first comprehensive study of MoA's robustness against deceptive LLM agents that deliberately provide misleading responses. We examine factors like the propagation of deceptive information, model size, and information availability, and uncover critical vulnerabilities. On AlpacaEval 2.0, the popular LLaMA 3.1-70B model achieves a length-controlled Win Rate (LC WR) of 49.2% when coupled with 3-layer MoA (6 LLM agents). However, we demonstrate that introducing only a single carefully-instructed deceptive agent into the MoA can reduce performance to 37.9%, effectively nullifying all MoA gains. On QuALITY, a multiple-choice comprehension task, the impact is also severe, with accuracy plummeting by a staggering 48.5%. Inspired in part by the historical Doge of Venice voting process, designed to minimize influence and deception, we propose a range of unsupervised defense mechanisms that recover most of the lost performance.", 'score': 7, 'issue_id': 2639, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 7', 'zh': '3æœˆ7æ—¥'}, 'hash': 'a7ca72375d4cd423', 'authors': ['Lorenz Wolf', 'Sangwoong Yoon', 'Ilija Bogunovic'], 'affiliations': ['University College London Center for Artificial Intelligence, London, UK'], 'pdf_title_img': 'assets/pdf/title_img/2503.05856.jpg', 'data': {'categories': ['#inference', '#security', '#benchmark', '#agents', '#hallucinations'], 'emoji': 'ğŸ›¡ï¸', 'ru': {'title': 'Ğ—Ğ°Ñ‰Ğ¸Ñ‚Ğ° ĞºĞ¾Ğ»Ğ»Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¾Ñ‚ Ğ¾Ğ±Ğ¼Ğ°Ğ½Ğ°', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ Mixture of large language model Agents (MoA), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸Ğ·ÑƒÑ‡Ğ°ÑÑ‚ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ MoA Ğº Ğ¾Ğ±Ğ¼Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼, Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ½Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‰Ğ¸Ğ¼ Ğ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ¾Ğ´Ğ¸Ğ½ Ğ¾Ğ±Ğ¼Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ·Ğ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ MoA Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ñ€Ğ¾Ğ´Ğµ AlpacaEval 2.0 Ğ¸ QuALITY. ĞŸÑ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ÑÑ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñ‹, Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğµ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ¼ Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ”Ğ¾Ğ¶Ğ° Ğ’ĞµĞ½ĞµÑ†Ğ¸Ğ¸, Ğ´Ğ»Ñ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑƒÑ‚Ñ€Ğ°Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Strengthening MoA: Defending Against Deceptive Agents', 'desc': 'This paper investigates the robustness of Mixture of Large Language Model Agents (MoA) architectures, which have shown impressive results in various benchmarks. The authors highlight a significant gap in understanding how these models handle deceptive agents that can provide misleading information. Their experiments reveal that even a single deceptive agent can drastically reduce the performance of MoA systems, indicating vulnerabilities in their design. To address these issues, the paper proposes unsupervised defense mechanisms inspired by historical voting processes to enhance the reliability of MoA against such threats.'}, 'zh': {'title': 'æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„å®‰å…¨æ€§ä¸å¯é æ€§', 'desc': 'è¿™ç¯‡è®ºæ–‡ç ”ç©¶äº†æ··åˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä»£ç†æ¶æ„çš„å®‰å…¨æ€§å’Œå¯é æ€§ã€‚å°½ç®¡è¿™äº›æ¶æ„åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å¯¹å…¶æŠµå¾¡è¯¯å¯¼æ€§ä¿¡æ¯çš„èƒ½åŠ›ç¼ºä¹è¯„ä¼°ã€‚ç ”ç©¶å‘ç°ï¼Œå•ä¸ªè¯¯å¯¼æ€§ä»£ç†çš„å¼•å…¥ä¼šæ˜¾è‘—é™ä½æ¨¡å‹çš„æ€§èƒ½ï¼Œç”šè‡³æŠµæ¶ˆæ‰€æœ‰çš„ä¼˜åŠ¿ã€‚ä¸ºæ­¤ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç³»åˆ—æ— ç›‘ç£çš„é˜²å¾¡æœºåˆ¶ï¼Œä»¥æ¢å¤å¤§éƒ¨åˆ†ä¸¢å¤±çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.06520', 'title': 'Seg-Zero: Reasoning-Chain Guided Segmentation via Cognitive\n  Reinforcement', 'url': 'https://huggingface.co/papers/2503.06520', 'abstract': "Traditional methods for reasoning segmentation rely on supervised fine-tuning with categorical labels and simple descriptions, limiting its out-of-domain generalization and lacking explicit reasoning processes. To address these limitations, we propose Seg-Zero, a novel framework that demonstrates remarkable generalizability and derives explicit chain-of-thought reasoning through cognitive reinforcement. Seg-Zero introduces a decoupled architecture consisting of a reasoning model and a segmentation model. The reasoning model interprets user intentions, generates explicit reasoning chains, and produces positional prompts, which are subsequently used by the segmentation model to generate precious pixel-level masks. We design a sophisticated reward mechanism that integrates both format and accuracy rewards to effectively guide optimization directions. Trained exclusively via reinforcement learning with GRPO and without explicit reasoning data, Seg-Zero achieves robust zero-shot generalization and exhibits emergent test-time reasoning capabilities. Experiments show that Seg-Zero-7B achieves a zero-shot performance of 57.5 on the ReasonSeg benchmark, surpassing the prior LISA-7B by 18\\%. This significant improvement highlights Seg-Zero's ability to generalize across domains while presenting an explicit reasoning process. Code is available at https://github.com/dvlab-research/Seg-Zero.", 'score': 6, 'issue_id': 2630, 'pub_date': '2025-03-09', 'pub_date_card': {'ru': '9 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 9', 'zh': '3æœˆ9æ—¥'}, 'hash': 'b21eb23a448d282e', 'authors': ['Yuqi Liu', 'Bohao Peng', 'Zhisheng Zhong', 'Zihao Yue', 'Fanbin Lu', 'Bei Yu', 'Jiaya Jia'], 'affiliations': ['CUHK', 'HKUST', 'RUC'], 'pdf_title_img': 'assets/pdf/title_img/2503.06520.jpg', 'data': {'categories': ['#rl', '#benchmark', '#architecture', '#reasoning', '#rlhf', '#optimization'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ¡ĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼: Ğ˜Ğ˜ ÑƒÑ‡Ğ¸Ñ‚ÑÑ Ğ¾Ğ±ÑŠÑÑĞ½ÑÑ‚ÑŒ ÑĞ²Ğ¾Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ', 'desc': 'Seg-Zero - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ĞµĞ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‰ĞµĞ¹ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ¸, Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸, ÑĞ¾Ğ·Ğ´Ğ°ÑÑ‰ĞµĞ¹ Ğ¼Ğ°ÑĞºĞ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Seg-Zero-7B Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° 18% Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ zero-shot ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ ReasonSeg.'}, 'en': {'title': 'Seg-Zero: Revolutionizing Reasoning Segmentation with Zero-Shot Generalization', 'desc': 'The paper introduces Seg-Zero, a new framework for reasoning segmentation that overcomes the limitations of traditional supervised methods. It features a decoupled architecture with a reasoning model that interprets user intentions and generates reasoning chains, which are then used by a segmentation model to create detailed pixel-level masks. The framework employs a unique reward mechanism that balances format and accuracy to optimize performance through reinforcement learning. Seg-Zero demonstrates impressive zero-shot generalization capabilities, achieving a significant performance boost on the ReasonSeg benchmark compared to previous models.'}, 'zh': {'title': 'Seg-Zeroï¼šçªç ´æ€§æ¨ç†ä¸åˆ†å‰²çš„ç»“åˆ', 'desc': 'ä¼ ç»Ÿçš„åˆ†å‰²æ¨ç†æ–¹æ³•ä¾èµ–äºå¸¦æœ‰ç±»åˆ«æ ‡ç­¾çš„ç›‘ç£å¾®è°ƒï¼Œè¿™é™åˆ¶äº†å…¶åœ¨ä¸åŒé¢†åŸŸçš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶ç¼ºä¹æ˜ç¡®çš„æ¨ç†è¿‡ç¨‹ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Seg-Zeroï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œå±•ç¤ºäº†æ˜¾è‘—çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶é€šè¿‡è®¤çŸ¥å¼ºåŒ–æ¨å¯¼å‡ºæ˜ç¡®çš„æ¨ç†é“¾ã€‚Seg-Zeroå¼•å…¥äº†ä¸€ä¸ªè§£è€¦æ¶æ„ï¼ŒåŒ…æ‹¬æ¨ç†æ¨¡å‹å’Œåˆ†å‰²æ¨¡å‹ï¼Œæ¨ç†æ¨¡å‹è§£é‡Šç”¨æˆ·æ„å›¾ï¼Œç”Ÿæˆæ˜ç¡®çš„æ¨ç†é“¾ï¼Œå¹¶äº§ç”Ÿä½ç½®æç¤ºï¼Œéšåç”±åˆ†å‰²æ¨¡å‹ç”Ÿæˆç²¾ç¡®çš„åƒç´ çº§æ©ç ã€‚é€šè¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼ŒSeg-Zeroåœ¨æ²¡æœ‰æ˜ç¡®æ¨ç†æ•°æ®çš„æƒ…å†µä¸‹ï¼Œå®ç°äº†å¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶å±•ç°å‡ºçªå‡ºçš„æµ‹è¯•æ—¶æ¨ç†èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.06121', 'title': 'BlackGoose Rimer: Harnessing RWKV-7 as a Simple yet Superior Replacement\n  for Transformers in Large-Scale Time Series Modeling', 'url': 'https://huggingface.co/papers/2503.06121', 'abstract': "Time series models face significant challenges in scaling to handle large and complex datasets, akin to the scaling achieved by large language models (LLMs). The unique characteristics of time series data and the computational demands of model scaling necessitate innovative approaches. While researchers have explored various architectures such as Transformers, LSTMs, and GRUs to address these challenges, we propose a novel solution using RWKV-7, which incorporates meta-learning into its state update mechanism. By integrating RWKV-7's time mix and channel mix components into the transformer-based time series model Timer, we achieve a substantial performance improvement of approximately 1.13 to 43.3x and a 4.5x reduction in training time with 1/23 parameters, all while utilizing fewer parameters. Our code and model weights are publicly available for further research and development at https://github.com/Alic-Li/BlackGoose_Rimer.", 'score': 5, 'issue_id': 2631, 'pub_date': '2025-03-08', 'pub_date_card': {'ru': '8 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 8', 'zh': '3æœˆ8æ—¥'}, 'hash': '3f03abe6317e5bba', 'authors': ['Li weile', 'Liu Xiao'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2503.06121.jpg', 'data': {'categories': ['#optimization', '#architecture', '#training', '#open_source'], 'emoji': 'â³', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ÑĞ´Ğ¾Ğ² Ñ RWKV-7', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ÑĞ´Ğ¾Ğ² Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ RWKV-7. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹ RWKV-7 Ğ² Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€-Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Timer Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ÑĞ´Ğ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞµĞ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ÑĞ´Ğ¾Ğ², Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ‚ĞµĞ¼, Ñ ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¼Ğ¸ ÑÑ‚Ğ°Ğ»ĞºĞ¸Ğ²Ğ°ÑÑ‚ÑÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Revolutionizing Time Series with RWKV-7: Efficiency Meets Performance', 'desc': "This paper addresses the challenges of scaling time series models to manage large and complex datasets, similar to the advancements seen in large language models. It introduces RWKV-7, a novel approach that integrates meta-learning into the state update mechanism of time series models. By combining RWKV-7's time mix and channel mix components with the Timer model, the authors report significant performance enhancements and reduced training times. The proposed method achieves impressive results with fewer parameters, making it a promising solution for time series analysis."}, 'zh': {'title': 'åˆ›æ–°æ—¶é—´åºåˆ—æ¨¡å‹ï¼Œæå‡æ€§èƒ½ä¸æ•ˆç‡', 'desc': 'æ—¶é—´åºåˆ—æ¨¡å‹åœ¨å¤„ç†å¤§å‹å¤æ‚æ•°æ®é›†æ—¶é¢ä¸´æ˜¾è‘—æŒ‘æˆ˜ï¼Œç±»ä¼¼äºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ‰©å±•èƒ½åŠ›ã€‚æ—¶é—´åºåˆ—æ•°æ®çš„ç‹¬ç‰¹ç‰¹æ€§å’Œæ¨¡å‹æ‰©å±•çš„è®¡ç®—éœ€æ±‚éœ€è¦åˆ›æ–°çš„æ–¹æ³•ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„è§£å†³æ–¹æ¡ˆï¼Œä½¿ç”¨RWKV-7å°†å…ƒå­¦ä¹ èå…¥çŠ¶æ€æ›´æ–°æœºåˆ¶ã€‚é€šè¿‡å°†RWKV-7çš„æ—¶é—´æ··åˆå’Œé€šé“æ··åˆç»„ä»¶æ•´åˆåˆ°åŸºäºå˜æ¢å™¨çš„æ—¶é—´åºåˆ—æ¨¡å‹Timerä¸­ï¼Œæˆ‘ä»¬å®ç°äº†çº¦1.13åˆ°43.3å€çš„æ€§èƒ½æå‡ï¼Œå¹¶å°†è®­ç»ƒæ—¶é—´å‡å°‘äº†4.5å€ï¼ŒåŒæ—¶å‚æ•°æ•°é‡ä»…ä¸ºåŸæ¥çš„1/23ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.03499', 'title': 'State-offset Tuning: State-based Parameter-Efficient Fine-Tuning for\n  State Space Models', 'url': 'https://huggingface.co/papers/2503.03499', 'abstract': 'State Space Models (SSMs) have emerged as efficient alternatives to Transformers, mitigating their quadratic computational cost. However, the application of Parameter-Efficient Fine-Tuning (PEFT) methods to SSMs remains largely unexplored. In particular, prompt-based methods like Prompt Tuning and Prefix-Tuning, which are widely used in Transformers, do not perform well on SSMs. To address this, we propose state-based methods as a superior alternative to prompt-based methods. This new family of methods naturally stems from the architectural characteristics of SSMs. State-based methods adjust state-related features directly instead of depending on external prompts. Furthermore, we introduce a novel state-based PEFT method: State-offset Tuning. At every timestep, our method directly affects the state at the current step, leading to more effective adaptation. Through extensive experiments across diverse datasets, we demonstrate the effectiveness of our method. Code is available at https://github.com/furiosa-ai/ssm-state-tuning.', 'score': 5, 'issue_id': 2630, 'pub_date': '2025-03-05', 'pub_date_card': {'ru': '5 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 5', 'zh': '3æœˆ5æ—¥'}, 'hash': '80ab6abd822f2976', 'authors': ['Wonjun Kang', 'Kevin Galim', 'Yuchen Zeng', 'Minjae Lee', 'Hyung Il Koo', 'Nam Ik Cho'], 'affiliations': ['UW-Madison'], 'pdf_title_img': 'assets/pdf/title_img/2503.03499.jpg', 'data': {'categories': ['#architecture', '#training', '#optimization'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ñ‚Ğ¾Ğ½ĞºĞ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ²', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ (SSM) Ğ² Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ° ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸ÑÑ…, ĞºĞ°Ğº Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ñƒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ°Ñ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ Ğ´Ğ»Ñ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ², Ğ½Ğ¾ Ğ½ĞµÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹ Ğ´Ğ»Ñ SSM. ĞĞ½Ğ¸ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ State-offset Tuning, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ğ²Ğ»Ğ¸ÑĞµÑ‚ Ğ½Ğ° ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¼ ÑˆĞ°Ğ³Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°.'}, 'en': {'title': 'Revolutionizing Fine-Tuning with State-Based Methods for SSMs', 'desc': 'This paper discusses the limitations of using traditional prompt-based fine-tuning methods on State Space Models (SSMs), which are more efficient than Transformers. The authors propose a new approach called state-based methods that leverage the unique structure of SSMs to improve performance. Specifically, they introduce State-offset Tuning, a novel parameter-efficient fine-tuning technique that modifies the state at each timestep directly. Experimental results show that this method outperforms existing prompt-based techniques, highlighting its effectiveness in adapting SSMs for various tasks.'}, 'zh': {'title': 'åŸºäºçŠ¶æ€çš„å¾®è°ƒï¼šè¶…è¶Šæç¤ºçš„æ–¹æ³•', 'desc': 'çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMsï¼‰ä½œä¸ºå˜æ¢å™¨çš„é«˜æ•ˆæ›¿ä»£æ–¹æ¡ˆï¼Œèƒ½å¤Ÿå‡è½»å…¶äºŒæ¬¡è®¡ç®—æˆæœ¬ã€‚ç„¶è€Œï¼Œå‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•åœ¨SSMsä¸Šçš„åº”ç”¨ä»ç„¶æœªè¢«å……åˆ†æ¢ç´¢ã€‚æˆ‘ä»¬æå‡ºåŸºäºçŠ¶æ€çš„æ–¹æ³•ï¼Œä½œä¸ºä¼˜äºåŸºäºæç¤ºçš„æ–¹æ³•çš„æ–°é€‰æ‹©ï¼Œè¿™äº›æ–¹æ³•ç›´æ¥è°ƒæ•´ä¸çŠ¶æ€ç›¸å…³çš„ç‰¹å¾ï¼Œè€Œä¸æ˜¯ä¾èµ–å¤–éƒ¨æç¤ºã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„åŸºäºçŠ¶æ€çš„PEFTæ–¹æ³•ï¼šçŠ¶æ€åç§»å¾®è°ƒï¼Œèƒ½å¤Ÿåœ¨æ¯ä¸ªæ—¶é—´æ­¥ç›´æ¥å½±å“å½“å‰çŠ¶æ€ï¼Œä»è€Œå®ç°æ›´æœ‰æ•ˆçš„é€‚åº”ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.07595', 'title': 'Detection Avoidance Techniques for Large Language Models', 'url': 'https://huggingface.co/papers/2503.07595', 'abstract': "The increasing popularity of large language models has not only led to widespread use but has also brought various risks, including the potential for systematically spreading fake news. Consequently, the development of classification systems such as DetectGPT has become vital. These detectors are vulnerable to evasion techniques, as demonstrated in an experimental series: Systematic changes of the generative models' temperature proofed shallow learning-detectors to be the least reliable. Fine-tuning the generative model via reinforcement learning circumvented BERT-based-detectors. Finally, rephrasing led to a >90\\% evasion of zero-shot-detectors like DetectGPT, although texts stayed highly similar to the original. A comparison with existing work highlights the better performance of the presented methods. Possible implications for society and further research are discussed.", 'score': 4, 'issue_id': 2631, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 10', 'zh': '3æœˆ10æ—¥'}, 'hash': '0b6929d80e047189', 'authors': ['Sinclair Schneider', 'Florian Steuber', 'Joao A. G. Schneider', 'Gabi Dreo Rodosek'], 'affiliations': ['Research Institute CODE, Bundeswehr University Munich, Munich, 81739, Bavaria, Germany'], 'pdf_title_img': 'assets/pdf/title_img/2503.07595.jpg', 'data': {'categories': ['#security', '#benchmark', '#rlhf', '#data', '#ethics', '#rl', '#hallucinations'], 'emoji': 'ğŸ•µï¸', 'ru': {'title': 'ĞĞ±Ğ¼Ğ°Ğ½ Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ²: ĞºĞ°Ğº LLM Ğ¾Ğ±Ñ…Ğ¾Ğ´ÑÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ˜Ğ˜-Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ², ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ ÑĞµÑ€Ğ¸Ñ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ñ…, ĞºĞ°Ğº Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¾Ğ±Ğ¾Ğ¹Ñ‚Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¾Ñ€Ñ‹, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº DetectGPT. Ğ‘Ñ‹Ğ»Ğ¸ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ñ‹ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ñ‚ĞµĞ¼Ğ¿ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ñ‹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ¿ĞµÑ€ĞµÑ„Ñ€Ğ°Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ğ±Ñ…Ğ¾Ğ´Ğ° Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹, Ñ‡ĞµĞ¼ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ğ½Ğ¸Ğ¼Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¾ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ˜Ğ˜-Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°.'}, 'en': {'title': 'Enhancing Evasion Techniques Against Language Model Detectors', 'desc': "This paper discusses the risks associated with large language models, particularly their potential to spread misinformation. It introduces DetectGPT, a classification system designed to identify generated text, but reveals its vulnerabilities to evasion techniques. The study shows that adjusting the generative model's temperature and fine-tuning it with reinforcement learning can effectively bypass existing detectors. Additionally, rephrasing generated content can achieve over 90% evasion rates while maintaining similarity to the original text, highlighting the need for improved detection methods."}, 'zh': {'title': 'åº”å¯¹å‡æ–°é—»çš„æ™ºèƒ½æ£€æµ‹æŒ‘æˆ˜', 'desc': 'éšç€å¤§å‹è¯­è¨€æ¨¡å‹çš„æ™®åŠï¼Œå‡æ–°é—»ä¼ æ’­çš„é£é™©ä¹Ÿéšä¹‹å¢åŠ ã€‚å› æ­¤ï¼Œå¼€å‘åƒDetectGPTè¿™æ ·çš„åˆ†ç±»ç³»ç»Ÿå˜å¾—è‡³å…³é‡è¦ã€‚è¿™äº›æ£€æµ‹å™¨å®¹æ˜“å—åˆ°è§„é¿æŠ€æœ¯çš„å½±å“ï¼Œå®éªŒè¡¨æ˜ï¼Œç”Ÿæˆæ¨¡å‹æ¸©åº¦çš„ç³»ç»Ÿæ€§å˜åŒ–ä½¿å¾—æµ…å±‚å­¦ä¹ æ£€æµ‹å™¨çš„å¯é æ€§æœ€ä½ã€‚é€šè¿‡å¼ºåŒ–å­¦ä¹ å¾®è°ƒç”Ÿæˆæ¨¡å‹å¯ä»¥ç»•è¿‡åŸºäºBERTçš„æ£€æµ‹å™¨ï¼Œè€Œé‡æ–°è¡¨è¿°æ–‡æœ¬åˆ™ä½¿å¾—åƒDetectGPTè¿™æ ·çš„é›¶æ ·æœ¬æ£€æµ‹å™¨çš„è§„é¿ç‡è¶…è¿‡90%ï¼Œå°½ç®¡æ–‡æœ¬ä¸åŸæ–‡é«˜åº¦ç›¸ä¼¼ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.07274', 'title': 'Efficient Distillation of Classifier-Free Guidance using Adapters', 'url': 'https://huggingface.co/papers/2503.07274', 'abstract': 'While classifier-free guidance (CFG) is essential for conditional diffusion models, it doubles the number of neural function evaluations (NFEs) per inference step. To mitigate this inefficiency, we introduce adapter guidance distillation (AGD), a novel approach that simulates CFG in a single forward pass. AGD leverages lightweight adapters to approximate CFG, effectively doubling the sampling speed while maintaining or even improving sample quality. Unlike prior guidance distillation methods that tune the entire model, AGD keeps the base model frozen and only trains minimal additional parameters (sim2%) to significantly reduce the resource requirement of the distillation phase. Additionally, this approach preserves the original model weights and enables the adapters to be seamlessly combined with other checkpoints derived from the same base model. We also address a key mismatch between training and inference in existing guidance distillation methods by training on CFG-guided trajectories instead of standard diffusion trajectories. Through extensive experiments, we show that AGD achieves comparable or superior FID to CFG across multiple architectures with only half the NFEs. Notably, our method enables the distillation of large models (sim2.6B parameters) on a single consumer GPU with 24 GB of VRAM, making it more accessible than previous approaches that require multiple high-end GPUs. We will publicly release the implementation of our method.', 'score': 4, 'issue_id': 2639, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 10', 'zh': '3æœˆ10æ—¥'}, 'hash': '84e5b0fd1b6f7f9f', 'authors': ['Cristian Perez Jensen', 'Seyedmorteza Sadat'], 'affiliations': ['ETH ZÃ¼rich'], 'pdf_title_img': 'assets/pdf/title_img/2503.07274.jpg', 'data': {'categories': ['#inference', '#optimization', '#diffusion', '#training', '#open_source', '#architecture'], 'emoji': 'ğŸš€', 'ru': {'title': 'Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°', 'desc': "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ 'Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ğ½Ğ°Ñ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ' (AGD) Ğ´Ğ»Ñ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. AGD ÑĞ¸Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ±ĞµĞ· ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ° (CFG) Ğ·Ğ° Ğ¾Ğ´Ğ¸Ğ½ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´ ÑĞµÑ‚Ğ¸, ÑƒĞ´Ğ²Ğ°Ğ¸Ğ²Ğ°Ñ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ ÑĞµĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğµ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ñ‹, Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ Ğ²ÑĞµĞ³Ğ¾ 2% Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ±Ğ°Ğ·Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½ĞµĞ¸Ğ·Ğ¼ĞµĞ½Ğ½Ğ¾Ğ¹. AGD Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ¾Ğ¼, Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€ÑƒÑÑÑŒ Ğ½Ğ° Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ÑÑ… Ñ CFG."}, 'en': {'title': 'Speeding Up Diffusion Models with Adapter Guidance Distillation', 'desc': 'This paper presents adapter guidance distillation (AGD), a new method that enhances the efficiency of conditional diffusion models by simulating classifier-free guidance (CFG) in a single forward pass. AGD uses lightweight adapters to approximate CFG, which allows for faster sampling without sacrificing sample quality. The approach keeps the base model unchanged and only trains a small number of additional parameters, significantly reducing resource requirements. Experimental results demonstrate that AGD achieves similar or better performance compared to CFG while halving the number of neural function evaluations needed, making it feasible to distill large models on consumer-grade hardware.'}, 'zh': {'title': 'é€‚é…å™¨å¼•å¯¼è’¸é¦ï¼šæå‡æ‰©æ•£æ¨¡å‹æ•ˆç‡çš„åˆ›æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œç§°ä¸ºé€‚é…å™¨å¼•å¯¼è’¸é¦ï¼ˆAGDï¼‰ï¼Œæ—¨åœ¨æé«˜æ¡ä»¶æ‰©æ•£æ¨¡å‹çš„æ•ˆç‡ã€‚AGDé€šè¿‡è½»é‡çº§é€‚é…å™¨åœ¨å•æ¬¡å‰å‘ä¼ æ’­ä¸­æ¨¡æ‹Ÿæ— åˆ†ç±»å™¨å¼•å¯¼ï¼ˆCFGï¼‰ï¼Œä»è€Œå®ç°äº†é‡‡æ ·é€Ÿåº¦çš„åŠ å€ï¼ŒåŒæ—¶ä¿æŒæˆ–æ”¹å–„æ ·æœ¬è´¨é‡ã€‚ä¸ä»¥å¾€çš„å¼•å¯¼è’¸é¦æ–¹æ³•ä¸åŒï¼ŒAGDåªè®­ç»ƒå°‘é‡é¢å¤–å‚æ•°ï¼Œè€Œä¿æŒåŸºç¡€æ¨¡å‹ä¸å˜ï¼Œæ˜¾è‘—é™ä½äº†èµ„æºéœ€æ±‚ã€‚é€šè¿‡å¤§é‡å®éªŒï¼Œæˆ‘ä»¬è¯æ˜AGDåœ¨å¤šä¸ªæ¶æ„ä¸Šå®ç°äº†ä¸CFGç›¸å½“æˆ–æ›´ä¼˜çš„FIDï¼ŒåŒæ—¶åªéœ€ä¸€åŠçš„ç¥ç»å‡½æ•°è¯„ä¼°ï¼ˆNFEï¼‰ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.02199', 'title': 'Words or Vision: Do Vision-Language Models Have Blind Faith in Text?', 'url': 'https://huggingface.co/papers/2503.02199', 'abstract': "Vision-Language Models (VLMs) excel in integrating visual and textual information for vision-centric tasks, but their handling of inconsistencies between modalities is underexplored. We investigate VLMs' modality preferences when faced with visual data and varied textual inputs in vision-centered settings. By introducing textual variations to four vision-centric tasks and evaluating ten Vision-Language Models (VLMs), we discover a ``blind faith in text'' phenomenon: VLMs disproportionately trust textual data over visual data when inconsistencies arise, leading to significant performance drops under corrupted text and raising safety concerns. We analyze factors influencing this text bias, including instruction prompts, language model size, text relevance, token order, and the interplay between visual and textual certainty. While certain factors, such as scaling up the language model size, slightly mitigate text bias, others like token order can exacerbate it due to positional biases inherited from language models. To address this issue, we explore supervised fine-tuning with text augmentation and demonstrate its effectiveness in reducing text bias. Additionally, we provide a theoretical analysis suggesting that the blind faith in text phenomenon may stem from an imbalance of pure text and multi-modal data during training. Our findings highlight the need for balanced training and careful consideration of modality interactions in VLMs to enhance their robustness and reliability in handling multi-modal data inconsistencies.", 'score': 4, 'issue_id': 2630, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 4', 'zh': '3æœˆ4æ—¥'}, 'hash': 'a354f8de058f0f84', 'authors': ['Ailin Deng', 'Tri Cao', 'Zhirui Chen', 'Bryan Hooi'], 'affiliations': ['National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2503.02199.jpg', 'data': {'categories': ['#interpretability', '#alignment', '#training', '#multimodal'], 'emoji': 'ğŸ”', 'ru': {'title': 'ĞĞ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ ÑĞ»ĞµĞ¿Ğ¾Ğ¹ Ğ²ĞµÑ€Ñ‹ Ğ² Ñ‚ĞµĞºÑÑ‚: Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ VLM Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° (VLM) ÑĞºĞ»Ğ¾Ğ½Ğ½Ñ‹ Ñ‡Ñ€ĞµĞ·Ğ¼ĞµÑ€Ğ½Ğ¾ Ğ´Ğ¾Ğ²ĞµÑ€ÑÑ‚ÑŒ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ¿Ñ€Ğ¸ Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹. Ğ­Ñ‚Ğ¾ ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ 'ÑĞ»ĞµĞ¿Ğ¾Ğ¹ Ğ²ĞµÑ€Ğ¾Ğ¹ Ğ² Ñ‚ĞµĞºÑÑ‚', Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ²ĞµÑÑ‚Ğ¸ Ğº Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼Ñƒ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ²Ñ‹Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¾Ğ¿Ğ°ÑĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ Ğ¿Ğ¾Ğ²Ğ¾Ğ´Ñƒ Ğ¸Ñ… Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ñ‹, Ğ²Ğ»Ğ¸ÑÑÑ‰Ğ¸Ğµ Ğ½Ğ° Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğµ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ğµ, Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ĞµĞ³Ğ¾ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸ĞµĞ¹ Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ¢ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ ÑĞ²ÑĞ·Ğ°Ğ½Ğ° Ñ Ğ´Ğ¸ÑĞ±Ğ°Ğ»Ğ°Ğ½ÑĞ¾Ğ¼ Ñ‡Ğ¸ÑÑ‚Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."}, 'en': {'title': 'Balancing Vision and Text: Overcoming Bias in Vision-Language Models', 'desc': "This paper investigates how Vision-Language Models (VLMs) manage inconsistencies between visual and textual information in tasks that rely heavily on vision. The authors identify a phenomenon called 'blind faith in text,' where VLMs tend to rely more on textual data than visual data when faced with conflicting inputs, which can lead to performance issues. They analyze various factors that contribute to this text bias, such as the size of the language model and the order of tokens in the text. To mitigate this bias, the paper proposes supervised fine-tuning with text augmentation and emphasizes the importance of balanced training for improving the reliability of VLMs in multi-modal contexts."}, 'zh': {'title': 'å¹³è¡¡è®­ç»ƒï¼Œæå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„å¯é æ€§', 'desc': 'è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤„ç†è§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬åœ¨é¢å¯¹æ¨¡æ€ä¸ä¸€è‡´æ—¶çš„è¡¨ç°å°šæœªå¾—åˆ°å……åˆ†ç ”ç©¶ã€‚æˆ‘ä»¬æ¢è®¨äº†VLMsåœ¨è§†è§‰æ•°æ®å’Œä¸åŒæ–‡æœ¬è¾“å…¥ä¸‹çš„æ¨¡æ€åå¥½ï¼Œå‘ç°äº†â€œå¯¹æ–‡æœ¬çš„ç›²ç›®ä¿¡ä»»â€ç°è±¡ï¼šå½“å‡ºç°ä¸ä¸€è‡´æ—¶ï¼ŒVLMsè¿‡åº¦ä¾èµ–æ–‡æœ¬æ•°æ®ï¼Œå¯¼è‡´æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚æˆ‘ä»¬åˆ†æäº†å½±å“è¿™ç§æ–‡æœ¬åè§çš„å› ç´ ï¼ŒåŒ…æ‹¬æŒ‡ä»¤æç¤ºã€è¯­è¨€æ¨¡å‹å¤§å°ã€æ–‡æœ¬ç›¸å…³æ€§ã€æ ‡è®°é¡ºåºä»¥åŠè§†è§‰å’Œæ–‡æœ¬ç¡®å®šæ€§ä¹‹é—´çš„ç›¸äº’ä½œç”¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æ¢ç´¢äº†å¸¦æœ‰æ–‡æœ¬å¢å¼ºçš„ç›‘ç£å¾®è°ƒï¼Œå¹¶è¯æ˜å…¶åœ¨å‡å°‘æ–‡æœ¬åè§æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.07603', 'title': 'Should VLMs be Pre-trained with Image Data?', 'url': 'https://huggingface.co/papers/2503.07603', 'abstract': 'Pre-trained LLMs that are further trained with image data perform well on vision-language tasks. While adding images during a second training phase effectively unlocks this capability, it is unclear how much of a gain or loss this two-step pipeline gives over VLMs which integrate images earlier into the training process. To investigate this, we train models spanning various datasets, scales, image-text ratios, and amount of pre-training done before introducing vision tokens. We then fine-tune these models and evaluate their downstream performance on a suite of vision-language and text-only tasks. We find that pre-training with a mixture of image and text data allows models to perform better on vision-language tasks while maintaining strong performance on text-only evaluations. On an average of 6 diverse tasks, we find that for a 1B model, introducing visual tokens 80% of the way through pre-training results in a 2% average improvement over introducing visual tokens to a fully pre-trained model.', 'score': 3, 'issue_id': 2633, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 10', 'zh': '3æœˆ10æ—¥'}, 'hash': '16f17eb6db67a418', 'authors': ['Sedrick Keh', 'Jean Mercat', 'Samir Yitzhak Gadre', 'Kushal Arora', 'Igor Vasiljevic', 'Benjamin Burchfiel', 'Shuran Song', 'Russ Tedrake', 'Thomas Kollar', 'Ludwig Schmidt', 'Achal Dave'], 'affiliations': ['Columbia University', 'MIT', 'Stanford', 'Toyota Research Institute'], 'pdf_title_img': 'assets/pdf/title_img/2503.07603.jpg', 'data': {'categories': ['#multimodal', '#training', '#benchmark', '#transfer_learning', '#optimization'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ñ€ĞµĞ¼Ñ Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ½Ğ° Ñ‚ĞµĞºÑÑ‚Ğ°Ñ…, Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…) Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ¸Ğ·Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ½Ğ° ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ»ÑƒÑ‡ÑˆĞµ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑ‚ÑŒÑÑ Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¾Ğ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. Ğ”Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ 1 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ²Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ½Ğ° 80% ÑÑ‚Ğ°Ğ¿Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ°ĞµÑ‚ Ğ² ÑÑ€ĞµĞ´Ğ½ĞµĞ¼ 2% ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğº Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Unlocking Vision-Language Synergy: Timing Matters!', 'desc': 'This paper explores the effectiveness of training large language models (LLMs) with image data in a two-step process compared to integrating images earlier in the training. The authors conduct experiments with various datasets and training configurations to assess the impact of when visual tokens are introduced. Their findings indicate that pre-training with both image and text data enhances performance on vision-language tasks while still performing well on text-only tasks. Specifically, they observe a 2% improvement in performance when visual tokens are added later in the pre-training phase for a 1B model.'}, 'zh': {'title': 'å›¾åƒä¸æ–‡æœ¬æ··åˆé¢„è®­ç»ƒæå‡è§†è§‰è¯­è¨€ä»»åŠ¡è¡¨ç°', 'desc': 'è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨åŠ å…¥å›¾åƒæ•°æ®ååœ¨è§†è§‰è¯­è¨€ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚ç ”ç©¶å‘ç°ï¼Œåœ¨ç¬¬äºŒé˜¶æ®µè®­ç»ƒä¸­åŠ å…¥å›¾åƒæ•°æ®å¯ä»¥æœ‰æ•ˆæå‡æ¨¡å‹çš„èƒ½åŠ›ï¼Œä½†ä¸æ¸…æ¥šè¿™ç§ä¸¤æ­¥è®­ç»ƒæµç¨‹ä¸æ—©æœŸæ•´åˆå›¾åƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ç›¸æ¯”ï¼Œç©¶ç«Ÿæ˜¯å¢ç›Šè¿˜æ˜¯æŸå¤±ã€‚é€šè¿‡è®­ç»ƒä¸åŒæ•°æ®é›†ã€è§„æ¨¡å’Œå›¾åƒæ–‡æœ¬æ¯”ä¾‹çš„æ¨¡å‹ï¼Œç ”ç©¶è€…å‘ç°æ··åˆå›¾åƒå’Œæ–‡æœ¬æ•°æ®çš„é¢„è®­ç»ƒå¯ä»¥æé«˜è§†è§‰è¯­è¨€ä»»åŠ¡çš„è¡¨ç°ï¼ŒåŒæ—¶åœ¨ä»…æ–‡æœ¬çš„è¯„ä¼°ä¸­ä¹Ÿä¿æŒè‰¯å¥½è¡¨ç°ã€‚ç»“æœæ˜¾ç¤ºï¼Œå¯¹äºä¸€ä¸ª10äº¿å‚æ•°çš„æ¨¡å‹ï¼Œåœ¨é¢„è®­ç»ƒçš„80%æ—¶å¼•å…¥è§†è§‰æ ‡è®°ï¼Œå¹³å‡æå‡äº†2%çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.07465', 'title': 'YOLOE: Real-Time Seeing Anything', 'url': 'https://huggingface.co/papers/2503.07465', 'abstract': "Object detection and segmentation are widely employed in computer vision applications, yet conventional models like YOLO series, while efficient and accurate, are limited by predefined categories, hindering adaptability in open scenarios. Recent open-set methods leverage text prompts, visual cues, or prompt-free paradigm to overcome this, but often compromise between performance and efficiency due to high computational demands or deployment complexity. In this work, we introduce YOLOE, which integrates detection and segmentation across diverse open prompt mechanisms within a single highly efficient model, achieving real-time seeing anything. For text prompts, we propose Re-parameterizable Region-Text Alignment (RepRTA) strategy. It refines pretrained textual embeddings via a re-parameterizable lightweight auxiliary network and enhances visual-textual alignment with zero inference and transferring overhead. For visual prompts, we present Semantic-Activated Visual Prompt Encoder (SAVPE). It employs decoupled semantic and activation branches to bring improved visual embedding and accuracy with minimal complexity. For prompt-free scenario, we introduce Lazy Region-Prompt Contrast (LRPC) strategy. It utilizes a built-in large vocabulary and specialized embedding to identify all objects, avoiding costly language model dependency. Extensive experiments show YOLOE's exceptional zero-shot performance and transferability with high inference efficiency and low training cost. Notably, on LVIS, with 3times less training cost and 1.4times inference speedup, YOLOE-v8-S surpasses YOLO-Worldv2-S by 3.5 AP. When transferring to COCO, YOLOE-v8-L achieves 0.6 AP^b and 0.4 AP^m gains over closed-set YOLOv8-L with nearly 4times less training time. Code and models are available at https://github.com/THU-MIG/yoloe.", 'score': 3, 'issue_id': 2637, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 10', 'zh': '3æœˆ10æ—¥'}, 'hash': 'faab5c7007c9cc4d', 'authors': ['Ao Wang', 'Lihao Liu', 'Hui Chen', 'Zijia Lin', 'Jungong Han', 'Guiguang Ding'], 'affiliations': ['Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.07465.jpg', 'data': {'categories': ['#training', '#benchmark', '#transfer_learning', '#optimization', '#cv'], 'emoji': 'ğŸ”', 'ru': {'title': 'YOLOE: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ¸ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ñ‡ĞµĞ³Ğ¾ ÑƒĞ³Ğ¾Ğ´Ğ½Ğ¾ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸', 'desc': 'YOLOE - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº Ğ² Ğ¾Ğ´Ğ½Ğ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ RepRTA Ğ´Ğ»Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº, ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ SAVPE Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ LRPC Ğ´Ğ»Ñ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ² Ğ±ĞµĞ· Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº. YOLOE Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸ÑĞºĞ»ÑÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… zero-shot Ğ¸ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑŒ, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ… Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ.'}, 'en': {'title': 'YOLOE: Real-Time Object Detection and Segmentation for Open-Set Scenarios', 'desc': 'This paper presents YOLOE, a novel model that enhances object detection and segmentation in open-set scenarios by integrating various prompt mechanisms. It introduces a Re-parameterizable Region-Text Alignment (RepRTA) strategy for text prompts, which refines textual embeddings efficiently without additional inference costs. For visual prompts, the Semantic-Activated Visual Prompt Encoder (SAVPE) improves visual accuracy while maintaining low complexity. Additionally, the Lazy Region-Prompt Contrast (LRPC) strategy allows for prompt-free object identification, significantly reducing training costs and improving inference speed compared to traditional models.'}, 'zh': {'title': 'YOLOEï¼šé«˜æ•ˆçš„å¼€æ”¾åœºæ™¯ç›®æ ‡æ£€æµ‹ä¸åˆ†å‰²', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„ç›®æ ‡æ£€æµ‹å’Œåˆ†å‰²æ¨¡å‹YOLOEï¼Œæ—¨åœ¨å…‹æœä¼ ç»Ÿæ¨¡å‹åœ¨å¼€æ”¾åœºæ™¯ä¸­çš„å±€é™æ€§ã€‚YOLOEé€šè¿‡é›†æˆå¤šç§å¼€æ”¾æç¤ºæœºåˆ¶ï¼Œå®ç°äº†é«˜æ•ˆçš„å®æ—¶æ£€æµ‹å’Œåˆ†å‰²ã€‚æˆ‘ä»¬æå‡ºäº†å¯é‡å‚æ•°åŒ–åŒºåŸŸ-æ–‡æœ¬å¯¹é½ç­–ç•¥ï¼ˆRepRTAï¼‰å’Œè¯­ä¹‰æ¿€æ´»è§†è§‰æç¤ºç¼–ç å™¨ï¼ˆSAVPEï¼‰ï¼Œä»¥æé«˜è§†è§‰å’Œæ–‡æœ¬çš„å¯¹é½æ•ˆæœã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒYOLOEåœ¨é›¶æ ·æœ¬æ€§èƒ½å’Œè¿ç§»èƒ½åŠ›ä¸Šè¡¨ç°ä¼˜å¼‚ï¼ŒåŒæ—¶è®­ç»ƒæˆæœ¬ä½ï¼Œæ¨ç†æ•ˆç‡é«˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.07265', 'title': 'WISE: A World Knowledge-Informed Semantic Evaluation for Text-to-Image\n  Generation', 'url': 'https://huggingface.co/papers/2503.07265', 'abstract': 'Text-to-Image (T2I) models are capable of generating high-quality artistic creations and visual content. However, existing research and evaluation standards predominantly focus on image realism and shallow text-image alignment, lacking a comprehensive assessment of complex semantic understanding and world knowledge integration in text to image generation. To address this challenge, we propose WISE, the first benchmark specifically designed for World Knowledge-Informed Semantic Evaluation. WISE moves beyond simple word-pixel mapping by challenging models with 1000 meticulously crafted prompts across 25 sub-domains in cultural common sense, spatio-temporal reasoning, and natural science. To overcome the limitations of traditional CLIP metric, we introduce WiScore, a novel quantitative metric for assessing knowledge-image alignment. Through comprehensive testing of 20 models (10 dedicated T2I models and 10 unified multimodal models) using 1,000 structured prompts spanning 25 subdomains, our findings reveal significant limitations in their ability to effectively integrate and apply world knowledge during image generation, highlighting critical pathways for enhancing knowledge incorporation and application in next-generation T2I models. Code and data are available at https://github.com/PKU-YuanGroup/WISE.', 'score': 3, 'issue_id': 2640, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 10', 'zh': '3æœˆ10æ—¥'}, 'hash': '6174088b65d232ba', 'authors': ['Yuwei Niu', 'Munan Ning', 'Mengren Zheng', 'Bin Lin', 'Peng Jin', 'Jiaqi Liao', 'Kunpeng Ning', 'Bin Zhu', 'Li Yuan'], 'affiliations': ['Chongqing University', 'Peking University', 'PengCheng Laboratory', 'Rabbitpre AI'], 'pdf_title_img': 'assets/pdf/title_img/2503.07265.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#multimodal', '#interpretability', '#cv'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'WISE: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ text-to-image', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº WISE Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ. WISE Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 1000 Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ² 25 Ğ¿Ğ¾Ğ´Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ñ…, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ñ‹Ğ¹ Ğ·Ğ´Ñ€Ğ°Ğ²Ñ‹Ğ¹ ÑĞ¼Ñ‹ÑĞ», Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ°ÑƒĞºĞ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ WiScore Ğ´Ğ»Ñ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ 20 Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ² Ğ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ‚ÑŒ Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Enhancing T2I Models with World Knowledge Evaluation', 'desc': 'This paper introduces WISE, a new benchmark for evaluating Text-to-Image (T2I) models that focuses on their ability to integrate world knowledge and complex semantic understanding. Unlike previous assessments that primarily measure image realism and basic text-image alignment, WISE challenges models with 1,000 detailed prompts across various domains, including cultural common sense and natural science. The authors also present WiScore, a new metric designed to quantitatively assess how well models align knowledge with generated images. Testing reveals that many current T2I models struggle to effectively incorporate world knowledge, indicating areas for improvement in future model development.'}, 'zh': {'title': 'æå‡æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„çŸ¥è¯†æ•´åˆèƒ½åŠ›', 'desc': 'æœ¬æ–‡æå‡ºäº†WISEï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªä¸“é—¨ä¸ºä¸–ç•ŒçŸ¥è¯†é©±åŠ¨çš„è¯­ä¹‰è¯„ä¼°è®¾è®¡çš„åŸºå‡†ã€‚ç°æœ‰çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹ä¸»è¦å…³æ³¨å›¾åƒçš„çœŸå®æ„Ÿå’Œç®€å•çš„æ–‡æœ¬-å›¾åƒå¯¹é½ï¼Œç¼ºä¹å¯¹å¤æ‚è¯­ä¹‰ç†è§£å’Œä¸–ç•ŒçŸ¥è¯†æ•´åˆçš„å…¨é¢è¯„ä¼°ã€‚WISEé€šè¿‡1000ä¸ªç²¾å¿ƒè®¾è®¡çš„æç¤ºï¼Œæ¶µç›–æ–‡åŒ–å¸¸è¯†ã€æ—¶ç©ºæ¨ç†å’Œè‡ªç„¶ç§‘å­¦ç­‰25ä¸ªå­é¢†åŸŸï¼ŒæŒ‘æˆ˜æ¨¡å‹çš„èƒ½åŠ›ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†WiScoreè¿™ä¸€æ–°é¢–çš„å®šé‡æŒ‡æ ‡ï¼Œä»¥è¯„ä¼°çŸ¥è¯†ä¸å›¾åƒçš„å¯¹é½ç¨‹åº¦ï¼Œæµ‹è¯•ç»“æœæ˜¾ç¤ºç°æœ‰æ¨¡å‹åœ¨æœ‰æ•ˆæ•´åˆå’Œåº”ç”¨ä¸–ç•ŒçŸ¥è¯†æ–¹é¢å­˜åœ¨æ˜¾è‘—å±€é™ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.06885', 'title': 'ProBench: Judging Multimodal Foundation Models on Open-ended\n  Multi-domain Expert Tasks', 'url': 'https://huggingface.co/papers/2503.06885', 'abstract': 'Solving expert-level multimodal tasks is a key milestone towards general intelligence. As the capabilities of multimodal large language models (MLLMs) continue to improve, evaluation of such advanced multimodal intelligence becomes necessary yet challenging. In this work, we introduce ProBench, a benchmark of open-ended user queries that require professional expertise and advanced reasoning. ProBench consists of 4,000 high-quality samples independently submitted by professionals based on their daily productivity demands. It spans across 10 fields and 56 sub-fields, including science, arts, humanities, coding, mathematics, and creative writing. Experimentally, we evaluate and compare 24 latest models using MLLM-as-a-Judge. Our results reveal that although the best open-source models rival the proprietary ones, ProBench presents significant challenges in visual perception, textual understanding, domain knowledge and advanced reasoning, thus providing valuable directions for future multimodal AI research efforts.', 'score': 3, 'issue_id': 2633, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 10', 'zh': '3æœˆ10æ—¥'}, 'hash': '196c83578a251f8e', 'authors': ['Yan Yang', 'Dongxu Li', 'Haoning Wu', 'Bei Chen', 'Liu Liu', 'Liyuan Pan', 'Junnan Li'], 'affiliations': ['ANU', 'BITSZ & School of CSAT, BIT', 'KooMap, Huawei', 'NTU', 'Salesforce AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2503.06885.jpg', 'data': {'categories': ['#open_source', '#multimodal', '#agi', '#benchmark', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ProBench: Ğ¸ÑĞ¿Ñ‹Ñ‚Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'ProBench - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM). ĞĞ½ ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· 4000 Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… 10 Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ Ğ¸ 56 Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ½Ğ°ÑƒĞºÑƒ, Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²Ğ¾, Ğ³ÑƒĞ¼Ğ°Ğ½Ğ¸Ñ‚Ğ°Ñ€Ğ½Ñ‹Ğµ Ğ½Ğ°ÑƒĞºĞ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ. Ğ—Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ±Ñ‹Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ñ‹ Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸Ñ… Ğ¿Ğ¾Ğ²ÑĞµĞ´Ğ½ĞµĞ²Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ñ… Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ 24 Ğ½Ğ¾Ğ²ĞµĞ¹ÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° MLLM-as-a-Judge Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ProBench Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğ¸, Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸ÑÑ… Ğ¸ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ….'}, 'en': {'title': 'ProBench: Benchmarking Multimodal Intelligence for Expert Tasks', 'desc': 'This paper presents ProBench, a new benchmark designed to evaluate multimodal large language models (MLLMs) on expert-level tasks. ProBench includes 4,000 user queries that require advanced reasoning and professional expertise across various fields such as science, arts, and coding. The study compares 24 state-of-the-art models using MLLM-as-a-Judge, highlighting the challenges these models face in visual perception, textual understanding, and domain knowledge. The findings indicate that while some open-source models perform comparably to proprietary ones, ProBench identifies critical areas for improvement in multimodal AI capabilities.'}, 'zh': {'title': 'å¤šæ¨¡æ€æ™ºèƒ½è¯„ä¼°çš„æ–°åŸºå‡†ï¼šProBench', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†ProBenchï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°å…¶åœ¨ä¸“ä¸šé¢†åŸŸçš„æ™ºèƒ½è¡¨ç°ã€‚ProBenchåŒ…å«4000ä¸ªé«˜è´¨é‡æ ·æœ¬ï¼Œæ¶µç›–ç§‘å­¦ã€è‰ºæœ¯ã€äººæ–‡å­¦ç§‘ã€ç¼–ç¨‹ã€æ•°å­¦å’Œåˆ›æ„å†™ä½œç­‰10ä¸ªé¢†åŸŸå’Œ56ä¸ªå­é¢†åŸŸã€‚é€šè¿‡å¯¹24ä¸ªæœ€æ–°æ¨¡å‹çš„å®éªŒè¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºå°½ç®¡ä¸€äº›å¼€æºæ¨¡å‹åœ¨æ€§èƒ½ä¸Šä¸ä¸“æœ‰æ¨¡å‹ç›¸å½“ï¼Œä½†åœ¨è§†è§‰æ„ŸçŸ¥ã€æ–‡æœ¬ç†è§£ã€é¢†åŸŸçŸ¥è¯†å’Œé«˜çº§æ¨ç†æ–¹é¢ï¼ŒProBenchä»ç„¶æå‡ºäº†æ˜¾è‘—çš„æŒ‘æˆ˜ã€‚è¯¥ç ”ç©¶ä¸ºæœªæ¥å¤šæ¨¡æ€äººå·¥æ™ºèƒ½çš„ç ”ç©¶æ–¹å‘æä¾›äº†é‡è¦çš„å‚è€ƒã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.06626', 'title': 'DiffCLIP: Differential Attention Meets CLIP', 'url': 'https://huggingface.co/papers/2503.06626', 'abstract': "We propose DiffCLIP, a novel vision-language model that extends the differential attention mechanism to CLIP architectures. Differential attention was originally developed for large language models to amplify relevant context while canceling out noisy information. In this work, we integrate this mechanism into CLIP's dual encoder (image and text) framework. With minimal additional parameters, DiffCLIP achieves superior performance on image-text understanding tasks. Across zero-shot classification, retrieval, and robustness benchmarks, DiffCLIP consistently outperforms baseline CLIP models. Notably, these gains come with negligible computational overhead, demonstrating that differential attention can significantly enhance multi-modal representations without sacrificing efficiency. Code can be found at https://github.com/hammoudhasan/DiffCLIP.", 'score': 3, 'issue_id': 2641, 'pub_date': '2025-03-09', 'pub_date_card': {'ru': '9 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 9', 'zh': '3æœˆ9æ—¥'}, 'hash': 'fff7aaf70b1d6ed0', 'authors': ['Hasan Abed Al Kader Hammoud', 'Bernard Ghanem'], 'affiliations': ['KAUST'], 'pdf_title_img': 'assets/pdf/title_img/2503.06626.jpg', 'data': {'categories': ['#architecture', '#diffusion', '#benchmark', '#multimodal', '#cv'], 'emoji': 'ğŸ”', 'ru': {'title': 'DiffCLIP: ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ DiffCLIP - Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ CLIP. Ğ”Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒÑĞ¸Ğ»Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ¸ Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ÑÑ‚ÑŒ ÑˆÑƒĞ¼Ğ¾Ğ²ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ. DiffCLIP Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ‚Ğ¾Ñ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ² Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ¾Ğ¹ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ CLIP Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ CLIP Ğ¿Ñ€Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ñ….'}, 'en': {'title': 'Enhancing CLIP with Differential Attention for Better Image-Text Understanding', 'desc': "DiffCLIP is a new vision-language model that improves the CLIP architecture by using a technique called differential attention. This method helps the model focus on important information while ignoring irrelevant details, which was initially designed for large language models. By incorporating differential attention into CLIP's dual encoder system, DiffCLIP enhances its ability to understand images and text together. The model shows better performance in various tasks like zero-shot classification and retrieval, all while maintaining low computational costs."}, 'zh': {'title': 'DiffCLIPï¼šé«˜æ•ˆå¢å¼ºå¤šæ¨¡æ€è¡¨ç¤ºçš„åˆ›æ–°æ¨¡å‹', 'desc': 'æˆ‘ä»¬æå‡ºäº†DiffCLIPï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„è§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œå®ƒå°†å·®åˆ†æ³¨æ„åŠ›æœºåˆ¶æ‰©å±•åˆ°CLIPæ¶æ„ä¸­ã€‚å·®åˆ†æ³¨æ„åŠ›æœ€åˆæ˜¯ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹å¼€å‘çš„ï¼Œæ—¨åœ¨æ”¾å¤§ç›¸å…³ä¸Šä¸‹æ–‡ï¼ŒåŒæ—¶æ¶ˆé™¤å™ªå£°ä¿¡æ¯ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å°†è¿™ä¸€æœºåˆ¶é›†æˆåˆ°CLIPçš„åŒç¼–ç å™¨ï¼ˆå›¾åƒå’Œæ–‡æœ¬ï¼‰æ¡†æ¶ä¸­ã€‚DiffCLIPåœ¨å›¾åƒ-æ–‡æœ¬ç†è§£ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œä¸”å‡ ä¹æ²¡æœ‰é¢å¤–çš„è®¡ç®—å¼€é”€ï¼Œè¯æ˜äº†å·®åˆ†æ³¨æ„åŠ›å¯ä»¥æ˜¾è‘—å¢å¼ºå¤šæ¨¡æ€è¡¨ç¤ºè€Œä¸ç‰ºç‰²æ•ˆç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.06273', 'title': 'Zero-AVSR: Zero-Shot Audio-Visual Speech Recognition with LLMs by\n  Learning Language-Agnostic Speech Representations', 'url': 'https://huggingface.co/papers/2503.06273', 'abstract': 'We explore a novel zero-shot Audio-Visual Speech Recognition (AVSR) framework, dubbed Zero-AVSR, which enables speech recognition in target languages without requiring any audio-visual speech data in those languages. Specifically, we introduce the Audio-Visual Speech Romanizer (AV-Romanizer), which learns language-agnostic speech representations by predicting Roman text. Then, by leveraging the strong multilingual modeling capabilities of Large Language Models (LLMs), we propose converting the predicted Roman text into language-specific graphemes, forming the proposed Cascaded <PRE_TAG>Zero-AVSR</POST_TAG>. Taking it a step further, we explore a unified Zero-AVSR approach by directly integrating the audio-visual speech representations encoded by the AV-Romanizer into the LLM. This is achieved through finetuning the adapter and the LLM using our proposed multi-task learning scheme. To capture the wide spectrum of phonetic and linguistic diversity, we also introduce a Multilingual Audio-Visual Romanized Corpus (MARC) consisting of 2,916 hours of audio-visual speech data across 82 languages, along with transcriptions in both language-specific graphemes and Roman text. Extensive analysis and experiments confirm that the proposed Zero-AVSR framework has the potential to expand language support beyond the languages seen during the training of the AV-Romanizer.', 'score': 3, 'issue_id': 2640, 'pub_date': '2025-03-08', 'pub_date_card': {'ru': '8 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 8', 'zh': '3æœˆ8æ—¥'}, 'hash': '66b650ba2f5b0404', 'authors': ['Jeong Hun Yeo', 'Minsu Kim', 'Chae Won Kim', 'Stavros Petridis', 'Yong Man Ro'], 'affiliations': ['Imperial College London', 'KAIST'], 'pdf_title_img': 'assets/pdf/title_img/2503.06273.jpg', 'data': {'categories': ['#low_resource', '#audio', '#dataset', '#multilingual', '#machine_translation'], 'emoji': 'ğŸ—£ï¸', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€ĞµÑ‡Ğ¸ Ğ±ĞµĞ· ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ±Ğ°Ñ€ÑŒĞµÑ€Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Zero-AVSR Ğ´Ğ»Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑ‡ĞµĞ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Audio-Visual Speech Romanizer Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ñ€ĞµÑ‡Ğ¸ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€Ğ¾Ğ¼Ğ°Ğ½Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ³Ñ€Ğ°Ñ„ĞµĞ¼Ñ‹ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Zero-AVSR, Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑ Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸ Ğ½ĞµĞ¿Ğ¾ÑÑ€ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ñ€Ğ¿ÑƒÑ MARC, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 2916 Ñ‡Ğ°ÑĞ¾Ğ² Ñ€ĞµÑ‡ĞµĞ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° 82 ÑĞ·Ñ‹ĞºĞ°Ñ….'}, 'en': {'title': 'Zero-AVSR: Speech Recognition Without Language-Specific Data', 'desc': 'The paper presents a new framework called Zero-AVSR for Audio-Visual Speech Recognition that can recognize speech in languages without needing any specific audio-visual data for those languages. It introduces the Audio-Visual Speech Romanizer (AV-Romanizer), which creates language-independent speech representations by predicting Roman text. By utilizing Large Language Models (LLMs), the framework converts these predictions into specific language graphemes, enhancing multilingual capabilities. Additionally, a new dataset, the Multilingual Audio-Visual Romanized Corpus (MARC), is introduced to support the training and evaluation of this framework across 82 languages.'}, 'zh': {'title': 'é›¶æ ·æœ¬éŸ³è§†é¢‘è¯­éŸ³è¯†åˆ«çš„åˆ›æ–°æ¢ç´¢', 'desc': 'æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„é›¶æ ·æœ¬éŸ³è§†é¢‘è¯­éŸ³è¯†åˆ«æ¡†æ¶ï¼Œç§°ä¸ºZero-AVSRï¼Œèƒ½å¤Ÿåœ¨æ²¡æœ‰ç›®æ ‡è¯­è¨€éŸ³è§†é¢‘è¯­éŸ³æ•°æ®çš„æƒ…å†µä¸‹è¿›è¡Œè¯­éŸ³è¯†åˆ«ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†éŸ³è§†é¢‘è¯­éŸ³ç½—é©¬åŒ–å™¨ï¼ˆAV-Romanizerï¼‰ï¼Œé€šè¿‡é¢„æµ‹ç½—é©¬æ–‡æœ¬æ¥å­¦ä¹ ä¸è¯­è¨€æ— å…³çš„è¯­éŸ³è¡¨ç¤ºã€‚æˆ‘ä»¬åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¼ºå¤§å¤šè¯­è¨€å»ºæ¨¡èƒ½åŠ›ï¼Œå°†é¢„æµ‹çš„ç½—é©¬æ–‡æœ¬è½¬æ¢ä¸ºç‰¹å®šè¯­è¨€çš„å­—å½¢ï¼Œå½¢æˆçº§è”çš„Zero-AVSRã€‚é€šè¿‡å¤šä»»åŠ¡å­¦ä¹ æ–¹æ¡ˆå¾®è°ƒé€‚é…å™¨å’ŒLLMï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æ¢ç´¢äº†ç»Ÿä¸€çš„Zero-AVSRæ–¹æ³•ï¼Œç›´æ¥å°†AV-Romanizerç¼–ç çš„éŸ³è§†é¢‘è¯­éŸ³è¡¨ç¤ºé›†æˆåˆ°LLMä¸­ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.07598', 'title': 'VACE: All-in-One Video Creation and Editing', 'url': 'https://huggingface.co/papers/2503.07598', 'abstract': 'Diffusion Transformer has demonstrated powerful capability and scalability in generating high-quality images and videos. Further pursuing the unification of generation and editing tasks has yielded significant progress in the domain of image content creation. However, due to the intrinsic demands for consistency across both temporal and spatial dynamics, achieving a unified approach for video synthesis remains challenging. We introduce VACE, which enables users to perform Video tasks within an All-in-one framework for Creation and Editing. These tasks include reference-to-video generation, video-to-video editing, and masked <PRE_TAG>video-to-video editing</POST_TAG>. Specifically, we effectively integrate the requirements of various tasks by organizing video task inputs, such as editing, reference, and masking, into a unified interface referred to as the Video Condition Unit (VCU). Furthermore, by utilizing a Context Adapter structure, we inject different task concepts into the model using formalized representations of temporal and spatial dimensions, allowing it to handle arbitrary video synthesis tasks flexibly. Extensive experiments demonstrate that the unified model of VACE achieves performance on par with task-specific models across various subtasks. Simultaneously, it enables diverse applications through versatile task combinations. Project page: https://ali-vilab.github.io/VACE-Page/.', 'score': 2, 'issue_id': 2644, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 10', 'zh': '3æœˆ10æ—¥'}, 'hash': '6bf5924fe3265297', 'authors': ['Zeyinzi Jiang', 'Zhen Han', 'Chaojie Mao', 'Jingfeng Zhang', 'Yulin Pan', 'Yu Liu'], 'affiliations': ['Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2503.07598.jpg', 'data': {'categories': ['#architecture', '#video', '#multimodal', '#diffusion'], 'emoji': 'ğŸ¬', 'ru': {'title': 'VACE: Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'VACE - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ°Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ² Ñ€Ğ°Ğ¼ĞºĞ°Ñ… ĞµĞ´Ğ¸Ğ½Ğ¾Ğ³Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ°. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹Ñ Video Condition Unit Ğ´Ğ»Ñ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Context Adapter Ğ´Ğ»Ñ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. VACE Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ Ñ€ĞµÑ„ĞµÑ€ĞµĞ½ÑÑƒ, Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ğ·Ğ°Ğ´Ğ°Ñ‡.'}, 'en': {'title': 'VACE: Unifying Video Creation and Editing in One Framework', 'desc': 'The paper introduces VACE, a unified framework for video generation and editing that leverages the capabilities of the Diffusion Transformer. It addresses the challenges of maintaining consistency in both temporal and spatial dynamics during video synthesis. VACE organizes various video tasks, such as reference-to-video generation and video-to-video editing, into a single interface called the Video Condition Unit (VCU). By employing a Context Adapter structure, VACE allows for flexible handling of different video synthesis tasks while achieving performance comparable to specialized models.'}, 'zh': {'title': 'VACEï¼šè§†é¢‘åˆ›ä½œä¸ç¼–è¾‘çš„ç»Ÿä¸€æ¡†æ¶', 'desc': 'æ‰©æ•£å˜æ¢å™¨åœ¨ç”Ÿæˆé«˜è´¨é‡å›¾åƒå’Œè§†é¢‘æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›å’Œå¯æ‰©å±•æ€§ã€‚ä¸ºäº†ç»Ÿä¸€ç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡ï¼Œç ”ç©¶è€…ä»¬åœ¨å›¾åƒå†…å®¹åˆ›ä½œé¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚å°½ç®¡å¦‚æ­¤ï¼Œç”±äºæ—¶é—´å’Œç©ºé—´åŠ¨æ€çš„ä¸€è‡´æ€§è¦æ±‚ï¼Œè§†é¢‘åˆæˆçš„ç»Ÿä¸€æ–¹æ³•ä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†VACEï¼Œå®ƒå…è®¸ç”¨æˆ·åœ¨ä¸€ä¸ªç»¼åˆæ¡†æ¶å†…æ‰§è¡Œè§†é¢‘ä»»åŠ¡ï¼ŒåŒ…æ‹¬å‚è€ƒè§†é¢‘ç”Ÿæˆã€è§†é¢‘ç¼–è¾‘å’Œæ©ç è§†é¢‘ç¼–è¾‘ç­‰ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.07389', 'title': 'TRCE: Towards Reliable Malicious Concept Erasure in Text-to-Image\n  Diffusion Models', 'url': 'https://huggingface.co/papers/2503.07389', 'abstract': "Recent advances in text-to-image diffusion models enable photorealistic image generation, but they also risk producing malicious content, such as NSFW images. To mitigate risk, concept erasure methods are studied to facilitate the model to unlearn specific concepts. However, current studies struggle to fully erase malicious concepts implicitly embedded in prompts (e.g., metaphorical expressions or adversarial prompts) while preserving the model's normal generation capability. To address this challenge, our study proposes TRCE, using a two-stage concept erasure strategy to achieve an effective trade-off between reliable erasure and knowledge preservation. Firstly, TRCE starts by erasing the malicious semantics implicitly embedded in textual prompts. By identifying a critical mapping objective(i.e., the [EoT] embedding), we optimize the cross-attention layers to map malicious prompts to contextually similar prompts but with safe concepts. This step prevents the model from being overly influenced by malicious semantics during the denoising process. Following this, considering the deterministic properties of the sampling trajectory of the diffusion model, TRCE further steers the early denoising prediction toward the safe direction and away from the unsafe one through contrastive learning, thus further avoiding the generation of malicious content. Finally, we conduct comprehensive evaluations of TRCE on multiple malicious concept erasure benchmarks, and the results demonstrate its effectiveness in erasing malicious concepts while better preserving the model's original generation ability. The code is available at: http://github.com/ddgoodgood/TRCE. CAUTION: This paper includes model-generated content that may contain offensive material.", 'score': 2, 'issue_id': 2642, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 10', 'zh': '3æœˆ10æ—¥'}, 'hash': 'd88e626045dd077e', 'authors': ['Ruidong Chen', 'Honglin Guo', 'Lanjun Wang', 'Chenyu Zhang', 'Weizhi Nie', 'An-An Liu'], 'affiliations': ['The School of Electrical and Information Engineering, Tianjin University', 'The School of New Media and Communication, Tianjin University'], 'pdf_title_img': 'assets/pdf/title_img/2503.07389.jpg', 'data': {'categories': ['#benchmark', '#security', '#open_source', '#training', '#cv', '#diffusion', '#hallucinations'], 'emoji': 'ğŸ”’', 'ru': {'title': 'Ğ‘ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ Ğ½ĞµĞ¶ĞµĞ»Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ TRCE Ğ´Ğ»Ñ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¶ĞµĞ»Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹ Ğ¸Ğ· Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ. TRCE Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° ÑƒĞ´Ğ°Ğ»ÑĞµÑ‚ Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½ÑƒÑ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºÑƒ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ², Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾Ğ¼ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒĞ´Ğ°Ğ»ÑĞµÑ‚ Ğ½ĞµĞ¶ĞµĞ»Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¸, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¾Ğ±Ñ‰Ğ¸Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ TRCE Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‰ÑƒÑ ĞµĞ³Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ.'}, 'en': {'title': 'TRCE: Safeguarding Image Generation with Smart Concept Erasure', 'desc': "This paper presents TRCE, a novel approach to mitigate the risk of generating malicious content in text-to-image diffusion models. It employs a two-stage concept erasure strategy that effectively removes harmful semantics from prompts while maintaining the model's ability to generate normal content. The first stage focuses on optimizing cross-attention layers to transform malicious prompts into safer alternatives. The second stage utilizes contrastive learning to guide the denoising process towards safe outputs, ensuring that the model does not produce NSFW images while preserving its generative capabilities."}, 'zh': {'title': 'å®‰å…¨ç”Ÿæˆï¼ŒæŠ¹é™¤æ¶æ„æ¦‚å¿µçš„åˆ›æ–°æ–¹æ³•', 'desc': 'æœ€è¿‘ï¼Œæ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆé€¼çœŸå›¾åƒæ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†ä¹Ÿå­˜åœ¨ç”Ÿæˆæ¶æ„å†…å®¹çš„é£é™©ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œç ”ç©¶äº†æ¦‚å¿µæŠ¹é™¤æ–¹æ³•ï¼Œä»¥å¸®åŠ©æ¨¡å‹å¿˜è®°ç‰¹å®šçš„æ¶æ„æ¦‚å¿µã€‚æˆ‘ä»¬çš„ç ”ç©¶æå‡ºäº†TRCEï¼Œé‡‡ç”¨ä¸¤é˜¶æ®µçš„æ¦‚å¿µæŠ¹é™¤ç­–ç•¥ï¼Œåœ¨å¯é æŠ¹é™¤å’ŒçŸ¥è¯†ä¿ç•™ä¹‹é—´å®ç°æœ‰æ•ˆçš„å¹³è¡¡ã€‚é€šè¿‡ä¼˜åŒ–äº¤å‰æ³¨æ„åŠ›å±‚ï¼ŒTRCEèƒ½å¤Ÿå°†æ¶æ„æç¤ºæ˜ å°„åˆ°å®‰å…¨çš„æ¦‚å¿µï¼Œä»è€Œé¿å…ç”Ÿæˆæ¶æ„å†…å®¹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.06960', 'title': 'A Data-Centric Revisit of Pre-Trained Vision Models for Robot Learning', 'url': 'https://huggingface.co/papers/2503.06960', 'abstract': 'Pre-trained vision models (PVMs) are fundamental to modern robotics, yet their optimal configuration remains unclear. Through systematic evaluation, we find that while DINO and iBOT outperform MAE across visuomotor control and perception tasks, they struggle when trained on non-(single-)object-centric (NOC) data--a limitation strongly correlated with their diminished ability to learn object-centric representations. This investigation indicates that the ability to form object-centric representations from the non-object-centric robotics dataset is the key to success for PVMs. Motivated by this discovery, we designed SlotMIM, a method that induces object-centric representations by introducing a semantic bottleneck to reduce the number of prototypes to encourage the emergence of objectness as well as cross-view consistency regularization for encouraging multiview invariance. Our experiments encompass pre-training on object-centric, scene-centric, web-crawled, and ego-centric data. Across all settings, our approach learns transferrable representations and achieves significant improvements over prior work in image recognition, scene understanding, and robot learning evaluations. When scaled up with million-scale datasets, our method also demonstrates superior data efficiency and scalability. Our code and models are publicly available at https://github.com/CVMI-Lab/SlotMIM.', 'score': 2, 'issue_id': 2642, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 10', 'zh': '3æœˆ10æ—¥'}, 'hash': '35668e47701edcd0', 'authors': ['Xin Wen', 'Bingchen Zhao', 'Yilun Chen', 'Jiangmiao Pang', 'Xiaojuan Qi'], 'affiliations': ['Shanghai AI Laboratory', 'The University of Hong Kong', 'University of Edinburgh'], 'pdf_title_img': 'assets/pdf/title_img/2503.06960.jpg', 'data': {'categories': ['#robotics', '#open_source', '#training', '#cv', '#dataset', '#transfer_learning'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'SlotMIM: Ğ¿Ñ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ½Ğ¾-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼ Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ (PVM) ÑÑ‚Ğ°Ğ»ĞºĞ¸Ğ²Ğ°ÑÑ‚ÑÑ Ñ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ½Ğµ ÑÑ„Ğ¾ĞºÑƒÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ĞºĞ»ÑÑ‡Ğ¾Ğ¼ Ğº ÑƒÑĞ¿ĞµÑ…Ñƒ PVM ÑĞ²Ğ»ÑĞµÑ‚ÑÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ½Ğ¾-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ· Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ñ Ğ¾Ğ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ SlotMIM, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ñ€Ğ¾Ñ‚Ğ¾Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ¸ Ğ¿Ğ¾Ğ¾Ñ‰Ñ€ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ SlotMIM Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑÑ†ĞµĞ½ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Unlocking Object-Centric Learning in Robotics with SlotMIM', 'desc': 'This paper explores the effectiveness of pre-trained vision models (PVMs) in robotics, particularly focusing on their ability to learn object-centric representations. The authors find that models like DINO and iBOT perform better than MAE in various tasks but struggle with non-object-centric data. They introduce SlotMIM, a new method that enhances object-centric learning by using a semantic bottleneck and cross-view consistency regularization. Their experiments show that SlotMIM significantly improves representation learning across different datasets and tasks, demonstrating better data efficiency and scalability.'}, 'zh': {'title': 'å¯¹è±¡ä¸­å¿ƒè¡¨ç¤ºæ˜¯æˆåŠŸçš„å…³é”®', 'desc': 'é¢„è®­ç»ƒè§†è§‰æ¨¡å‹ï¼ˆPVMsï¼‰åœ¨ç°ä»£æœºå™¨äººæŠ€æœ¯ä¸­è‡³å…³é‡è¦ï¼Œä½†å…¶æœ€ä½³é…ç½®å°šä¸æ˜ç¡®ã€‚ç ”ç©¶å‘ç°ï¼ŒDINOå’ŒiBOTåœ¨è§†è§‰è¿åŠ¨æ§åˆ¶å’Œæ„ŸçŸ¥ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºMAEï¼Œä½†åœ¨éå•å¯¹è±¡ä¸­å¿ƒï¼ˆNOCï¼‰æ•°æ®ä¸Šè®­ç»ƒæ—¶è¡¨ç°ä¸ä½³ï¼Œè¿™ä¸å®ƒä»¬å­¦ä¹ å¯¹è±¡ä¸­å¿ƒè¡¨ç¤ºçš„èƒ½åŠ›ä¸‹é™å¯†åˆ‡ç›¸å…³ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œä»éå¯¹è±¡ä¸­å¿ƒçš„æœºå™¨äººæ•°æ®é›†ä¸­å½¢æˆå¯¹è±¡ä¸­å¿ƒè¡¨ç¤ºçš„èƒ½åŠ›æ˜¯PVMsæˆåŠŸçš„å…³é”®ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬è®¾è®¡äº†SlotMIMæ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥è¯­ä¹‰ç“¶é¢ˆæ¥å‡å°‘åŸå‹æ•°é‡ï¼Œä¿ƒè¿›å¯¹è±¡æ€§å‡ºç°ï¼Œå¹¶é€šè¿‡äº¤å‰è§†å›¾ä¸€è‡´æ€§æ­£åˆ™åŒ–æ¥é¼“åŠ±å¤šè§†å›¾ä¸å˜æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.06362', 'title': 'Adaptive Audio-Visual Speech Recognition via Matryoshka-Based Multimodal\n  LLMs', 'url': 'https://huggingface.co/papers/2503.06362', 'abstract': 'Audio-Visual Speech Recognition (AVSR) leverages both audio and visual modalities to enhance speech recognition robustness, particularly in noisy environments. Recent advancements in Large Language Models (LLMs) have demonstrated their effectiveness in speech recognition, including AVSR. However, due to the significant length of speech representations, direct integration with LLMs imposes substantial computational costs. Prior approaches address this by compressing speech representations before feeding them into LLMs. However, higher compression ratios often lead to performance degradation, necessitating a trade-off between computational efficiency and recognition accuracy. To address this challenge, we propose Llama-MTSK, the first Matryoshka-based Multimodal LLM for AVSR, which enables flexible adaptation of the audio-visual token allocation based on specific computational constraints while preserving high performance. Our approach, inspired by Matryoshka Representation Learning, encodes audio-visual representations at multiple granularities within a single model, eliminating the need to train separate models for different compression levels. Moreover, to efficiently fine-tune the LLM, we introduce three LoRA-based Matryoshka strategies using global and scale-specific LoRA modules. Extensive evaluations on the two largest AVSR datasets demonstrate that Llama-MTSK achieves state-of-the-art results, matching or surpassing models trained independently at fixed compression levels.', 'score': 2, 'issue_id': 2638, 'pub_date': '2025-03-09', 'pub_date_card': {'ru': '9 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 9', 'zh': '3æœˆ9æ—¥'}, 'hash': 'e35aea6b25fbc264', 'authors': ['Umberto Cappellazzo', 'Minsu Kim', 'Stavros Petridis'], 'affiliations': ['Imperial College London', 'Meta AI'], 'pdf_title_img': 'assets/pdf/title_img/2503.06362.jpg', 'data': {'categories': ['#training', '#multimodal', '#optimization', '#audio'], 'emoji': 'ğŸ­', 'ru': {'title': 'Ğ“Ğ¸Ğ±ĞºĞ¾Ğµ AVSR Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ°Ñ‚Ñ€ĞµÑˆĞµÑ‡Ğ½Ğ¾Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Llama-MTSK - Ğ¿ĞµÑ€Ğ²ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ° Ğ¼Ğ°Ñ‚Ñ€ĞµÑˆĞºĞ¸ Ğ´Ğ»Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾-Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸ (AVSR). ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³Ğ¸Ğ±ĞºĞ¾ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ°ÑƒĞ´Ğ¸Ğ¾-Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»ĞµĞ½ Matryoshka Representation Learning Ğ¸ ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒĞµÑ‚ Ğ°ÑƒĞ´Ğ¸Ğ¾-Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑƒÑ€Ğ¾Ğ²Ğ½ÑÑ… Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ñ€Ğ°Ğ¼ĞºĞ°Ñ… Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ‚Ñ€Ğ¸ LoRA-ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Llama-MTSK: Efficient AVSR with Flexible Token Allocation', 'desc': 'This paper introduces Llama-MTSK, a novel Matryoshka-based Multimodal Large Language Model (LLM) designed for Audio-Visual Speech Recognition (AVSR). It addresses the challenge of integrating lengthy speech representations with LLMs by allowing flexible audio-visual token allocation based on computational constraints. The model encodes audio-visual data at various granularities, which helps maintain high performance without the need for separate models for different compression levels. Additionally, the paper presents three LoRA-based strategies for fine-tuning the model, achieving state-of-the-art results on major AVSR datasets.'}, 'zh': {'title': 'çµæ´»é«˜æ•ˆçš„éŸ³è§†é¢‘è¯­éŸ³è¯†åˆ«è§£å†³æ–¹æ¡ˆ', 'desc': 'éŸ³è§†é¢‘è¯­éŸ³è¯†åˆ«ï¼ˆAVSRï¼‰ç»“åˆäº†éŸ³é¢‘å’Œè§†è§‰ä¿¡æ¯ï¼Œä»¥æé«˜åœ¨å˜ˆæ‚ç¯å¢ƒä¸­çš„è¯­éŸ³è¯†åˆ«èƒ½åŠ›ã€‚æœ€è¿‘ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¿›å±•æ˜¾ç¤ºäº†å®ƒä»¬åœ¨è¯­éŸ³è¯†åˆ«ä¸­çš„æœ‰æ•ˆæ€§ï¼ŒåŒ…æ‹¬AVSRã€‚ç„¶è€Œï¼Œç”±äºè¯­éŸ³è¡¨ç¤ºçš„é•¿åº¦è¾ƒå¤§ï¼Œç›´æ¥ä¸LLMsé›†æˆä¼šå¸¦æ¥å·¨å¤§çš„è®¡ç®—æˆæœ¬ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Llama-MTSKï¼Œè¿™æ˜¯ä¸€ç§åŸºäºå¥—å¨ƒçš„å¤šæ¨¡æ€LLMï¼Œèƒ½å¤Ÿæ ¹æ®ç‰¹å®šçš„è®¡ç®—çº¦æŸçµæ´»è°ƒæ•´éŸ³è§†é¢‘æ ‡è®°çš„åˆ†é…ï¼ŒåŒæ—¶ä¿æŒé«˜æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.05578', 'title': 'Novel Object 6D Pose Estimation with a Single Reference View', 'url': 'https://huggingface.co/papers/2503.05578', 'abstract': 'Existing novel object 6D pose estimation methods typically rely on CAD models or dense reference views, which are both difficult to acquire. Using only a single reference view is more scalable, but challenging due to large pose discrepancies and limited geometric and spatial information. To address these issues, we propose a Single-Reference-based novel object 6D (SinRef-6D) pose estimation method. Our key idea is to iteratively establish point-wise alignment in the camera coordinate system based on state space models (SSMs). Specifically, iterative camera-space point-wise alignment can effectively handle large pose discrepancies, while our proposed RGB and Points SSMs can capture long-range dependencies and spatial information from a single view, offering linear complexity and superior spatial modeling capability. Once pre-trained on synthetic data, SinRef-6D can estimate the 6D pose of a novel object using only a single reference view, without requiring retraining or a CAD model. Extensive experiments on six popular datasets and real-world robotic scenes demonstrate that we achieve on-par performance with CAD-based and dense reference view-based methods, despite operating in the more challenging single reference setting. Code will be released at https://github.com/CNJianLiu/SinRef-6D.', 'score': 2, 'issue_id': 2640, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 7', 'zh': '3æœˆ7æ—¥'}, 'hash': '689a25e37e909986', 'authors': ['Jian Liu', 'Wei Sun', 'Kai Zeng', 'Jin Zheng', 'Hui Yang', 'Lin Wang', 'Hossein Rahmani', 'Ajmal Mian'], 'affiliations': ['Central South University', 'Hunan University', 'Lancaster University', 'Nanyang Technological University', 'The University of Western Australia'], 'pdf_title_img': 'assets/pdf/title_img/2503.05578.jpg', 'data': {'categories': ['#3d', '#cv', '#robotics'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ¢Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° 6D-Ğ¿Ğ¾Ğ·Ñ‹ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ° Ğ¿Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ñƒ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ SinRef-6D Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ 6D-Ğ¿Ğ¾Ğ·Ñ‹ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡ĞµĞº Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ğµ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ RGB-Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¾Ğ±Ğ»Ğ°ĞºĞ° Ñ‚Ğ¾Ñ‡ĞµĞº. SinRef-6D ÑĞ¿Ğ¾ÑĞ¾Ğ±ĞµĞ½ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ñ€Ğ°ÑÑ…Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ¿Ğ¾Ğ·Ğµ Ğ¸ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. ĞŸĞ¾ÑĞ»Ğµ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ·Ñƒ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ»Ğ¸ CAD-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Single View, Full Pose: Simplifying 6D Object Estimation', 'desc': 'This paper introduces a new method for estimating the 6D pose of novel objects using only a single reference view, which is more scalable than traditional methods that rely on CAD models or multiple views. The proposed SinRef-6D method utilizes iterative point-wise alignment in the camera coordinate system, effectively addressing challenges posed by large pose discrepancies. By employing state space models (SSMs), the method captures long-range dependencies and spatial information, achieving linear complexity and enhanced spatial modeling. Experimental results show that SinRef-6D performs comparably to existing methods while simplifying the pose estimation process.'}, 'zh': {'title': 'å•è§†å›¾ä¸‹çš„å…­ç»´å§¿æ€ä¼°è®¡æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å•å‚è€ƒè§†å›¾çš„å…­ç»´å§¿æ€ä¼°è®¡æ–¹æ³•ï¼Œç§°ä¸ºSinRef-6Dã€‚è¯¥æ–¹æ³•é€šè¿‡åœ¨ç›¸æœºåæ ‡ç³»ä¸­è¿­ä»£å»ºç«‹ç‚¹å¯¹é½ï¼Œè§£å†³äº†å¤§å§¿æ€å·®å¼‚å’Œç©ºé—´ä¿¡æ¯ä¸è¶³çš„é—®é¢˜ã€‚SinRef-6Dåˆ©ç”¨çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMsï¼‰æ¥æ•æ‰é•¿è·ç¦»ä¾èµ–å…³ç³»ï¼Œå¹¶å…·æœ‰çº¿æ€§å¤æ‚åº¦å’Œä¼˜è¶Šçš„ç©ºé—´å»ºæ¨¡èƒ½åŠ›ã€‚ç»è¿‡åœ¨åˆæˆæ•°æ®ä¸Šçš„é¢„è®­ç»ƒåï¼ŒSinRef-6Dèƒ½å¤Ÿä»…ä½¿ç”¨å•ä¸€å‚è€ƒè§†å›¾ä¼°è®¡æ–°ç‰©ä½“çš„å…­ç»´å§¿æ€ï¼Œä¸”æ— éœ€é‡æ–°è®­ç»ƒæˆ–CADæ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.07597', 'title': 'HumanMM: Global Human Motion Recovery from Multi-shot Videos', 'url': 'https://huggingface.co/papers/2503.07597', 'abstract': 'In this paper, we present a novel framework designed to reconstruct long-sequence 3D human motion in the world coordinates from in-the-wild videos with multiple shot transitions. Such long-sequence in-the-wild motions are highly valuable to applications such as motion generation and motion understanding, but are of great challenge to be recovered due to abrupt shot transitions, partial occlusions, and dynamic backgrounds presented in such videos. Existing methods primarily focus on single-shot videos, where continuity is maintained within a single camera view, or simplify multi-shot alignment in camera space only. In this work, we tackle the challenges by integrating an enhanced camera pose estimation with Human Motion Recovery (HMR) by incorporating a shot transition detector and a robust alignment module for accurate pose and orientation continuity across shots. By leveraging a custom motion integrator, we effectively mitigate the problem of foot sliding and ensure temporal consistency in human pose. Extensive evaluations on our created multi-shot dataset from public 3D human datasets demonstrate the robustness of our method in reconstructing realistic human motion in world coordinates.', 'score': 1, 'issue_id': 2640, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 10', 'zh': '3æœˆ10æ—¥'}, 'hash': '161213a2a054dd4d', 'authors': ['Yuhong Zhang', 'Guanlin Wu', 'Ling-Hao Chen', 'Zhuokai Zhao', 'Jing Lin', 'Xiaoke Jiang', 'Jiamin Wu', 'Zhuoheng Li', 'Hao Frank Yang', 'Haoqian Wang', 'Lei Zhang'], 'affiliations': ['HKU', 'HKUST', 'IDEA Research', 'Johns Hopkins University', 'Tsinghua University', 'University of Chicago'], 'pdf_title_img': 'assets/pdf/title_img/2503.07597.jpg', 'data': {'categories': ['#3d', '#dataset', '#long_context', '#video'], 'emoji': 'ğŸƒ', 'ru': {'title': 'Ğ ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ 3D-Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¸Ğ· Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ ĞºĞ°Ğ´Ñ€Ğ°Ğ¼Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ² Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ñ‹Ñ… ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚Ğ°Ñ… Ğ¸Ğ· Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ ÑĞ¼ĞµĞ½Ğ°Ğ¼Ğ¸ ĞºĞ°Ğ´Ñ€Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€ĞµÑˆĞ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ€ĞµĞ·ĞºĞ¸Ñ… Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ¾Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ğ´Ñ€Ğ°Ğ¼Ğ¸, Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾ĞºĞºĞ»ÑĞ·Ğ¸Ğ¹ Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ„Ğ¾Ğ½Ğ°, Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ñ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¾Ñ€ ÑĞ¼ĞµĞ½Ñ‹ ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ¸ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ·Ñ‹ Ğ¸ Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ğ´Ñ€Ğ°Ğ¼Ğ¸. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ‚Ğ¾Ñ€Ğ° Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ÑĞºĞ¾Ğ»ÑŒĞ¶ĞµĞ½Ğ¸Ñ Ğ½Ğ¾Ğ³ Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ñ‚ÑŒ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ·Ñ‹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°.'}, 'en': {'title': 'Reconstructing Realistic 3D Human Motion Across Multiple Video Shots', 'desc': 'This paper introduces a new framework for reconstructing long sequences of 3D human motion from videos that have multiple camera angles and transitions. The challenge lies in dealing with issues like sudden changes in shots, parts of the person being blocked, and moving backgrounds. Unlike previous methods that only work with single camera views, this approach combines advanced camera pose estimation with Human Motion Recovery (HMR) and includes a shot transition detector for better accuracy. The results show that the method effectively reduces foot sliding and maintains consistent human poses over time, proving its effectiveness on a specially created multi-shot dataset.'}, 'zh': {'title': 'é‡å»ºçœŸå®äººç±»è¿åŠ¨çš„æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œæ—¨åœ¨ä»å¤šé•œå¤´åˆ‡æ¢çš„é‡å¤–è§†é¢‘ä¸­é‡å»ºé•¿åºåˆ—çš„ä¸‰ç»´äººç±»è¿åŠ¨ã€‚ç”±äºè§†é¢‘ä¸­çš„çªç„¶é•œå¤´åˆ‡æ¢ã€éƒ¨åˆ†é®æŒ¡å’ŒåŠ¨æ€èƒŒæ™¯ï¼Œè¿™ç§é•¿åºåˆ—çš„è¿åŠ¨æ¢å¤é¢ä¸´å¾ˆå¤§æŒ‘æˆ˜ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡ç»“åˆå¢å¼ºçš„ç›¸æœºå§¿æ€ä¼°è®¡å’Œäººç±»è¿åŠ¨æ¢å¤ï¼ˆHMRï¼‰ï¼Œå¼•å…¥äº†é•œå¤´åˆ‡æ¢æ£€æµ‹å™¨å’Œç¨³å¥çš„å¯¹é½æ¨¡å—ï¼Œä»¥ç¡®ä¿å§¿æ€å’Œæ–¹å‘åœ¨é•œå¤´é—´çš„è¿ç»­æ€§ã€‚é€šè¿‡è‡ªå®šä¹‰çš„è¿åŠ¨æ•´åˆå™¨ï¼Œæˆ‘ä»¬æœ‰æ•ˆåœ°å‡è½»äº†è„šæ»‘é—®é¢˜ï¼Œå¹¶ç¡®ä¿äº†äººç±»å§¿æ€çš„æ—¶é—´ä¸€è‡´æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.07426', 'title': 'RePO: ReLU-based Preference Optimization', 'url': 'https://huggingface.co/papers/2503.07426', 'abstract': "Aligning large language models (LLMs) with human preferences is critical for real-world deployment, yet existing methods like RLHF face computational and stability challenges. While DPO establishes an offline paradigm with single hyperparameter beta, subsequent methods like SimPO reintroduce complexity through dual parameters (beta, gamma). We propose {ReLU-based Preference Optimization (RePO)}, a streamlined algorithm that eliminates beta via two advances: (1) retaining SimPO's reference-free margins but removing beta through gradient analysis, and (2) adopting a ReLU-based max-margin loss that naturally filters trivial pairs. Theoretically, RePO is characterized as SimPO's limiting case (beta to infty), where the logistic weighting collapses to binary thresholding, forming a convex envelope of the 0-1 loss. Empirical results on AlpacaEval 2 and Arena-Hard show that RePO outperforms DPO and SimPO across multiple base models, requiring only one hyperparameter to tune.", 'score': 1, 'issue_id': 2637, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 10', 'zh': '3æœˆ10æ—¥'}, 'hash': '619f3a64896639ee', 'authors': ['Junkang Wu', 'Kexin Huang', 'Xue Wang', 'Jinyang Gao', 'Bolin Ding', 'Jiancan Wu', 'Xiangnan He', 'Xiang Wang'], 'affiliations': ['Alibaba Group, Hangzhou, China', 'University of Science and Technology of China, Hefei, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.07426.jpg', 'data': {'categories': ['#alignment', '#rlhf', '#training'], 'emoji': 'ğŸš€', 'ru': {'title': 'RePO: Ğ£Ğ¿Ñ€Ğ¾Ñ‰ĞµĞ½Ğ½Ğ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ RePO. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ¿Ñ€Ğ¾Ñ‰Ğ°ĞµÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº DPO Ğ¸ SimPO, ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑÑ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ² Ğ³Ğ¸Ğ¿ĞµÑ€Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğµ beta. RePO Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ReLU-Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ´Ğ»Ñ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ñ€Ğ¸Ğ²Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ°Ñ€ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ RePO Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ DPO Ğ¸ SimPO Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ³Ğ¸Ğ¿ĞµÑ€Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°.'}, 'en': {'title': 'Streamlining Preference Optimization for LLMs with RePO', 'desc': 'This paper addresses the challenge of aligning large language models (LLMs) with human preferences using a new method called ReLU-based Preference Optimization (RePO). RePO simplifies the optimization process by eliminating the need for a complex hyperparameter, beta, while still maintaining effective performance through gradient analysis and a ReLU-based max-margin loss. The authors demonstrate that RePO can be seen as a special case of an existing method, SimPO, when certain conditions are met, leading to a more efficient training process. Empirical results indicate that RePO consistently outperforms previous methods like DPO and SimPO, while only requiring the tuning of a single hyperparameter.'}, 'zh': {'title': 'ç®€åŒ–åå¥½ä¼˜åŒ–ï¼Œæå‡è¯­è¨€æ¨¡å‹å¯¹é½', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ç®—æ³•ï¼Œç§°ä¸ºåŸºäºReLUçš„åå¥½ä¼˜åŒ–ï¼ˆRePOï¼‰ï¼Œæ—¨åœ¨ç®€åŒ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸äººç±»åå¥½çš„å¯¹é½è¿‡ç¨‹ã€‚RePOé€šè¿‡ä¸¤ä¸ªåˆ›æ–°ç‚¹æ¶ˆé™¤äº†è¶…å‚æ•°betaï¼Œé¦–å…ˆä¿ç•™äº†SimPOçš„æ— å‚è€ƒè¾¹é™…ï¼Œä½†é€šè¿‡æ¢¯åº¦åˆ†æå»é™¤äº†betaï¼Œå…¶æ¬¡é‡‡ç”¨åŸºäºReLUçš„æœ€å¤§è¾¹é™…æŸå¤±ï¼Œè‡ªç„¶è¿‡æ»¤æ‰æ— å…³çš„é…å¯¹ã€‚ç†è®ºä¸Šï¼ŒRePOè¢«æè¿°ä¸ºSimPOçš„æé™æƒ…å†µï¼Œå½“betaè¶‹å‘äºæ— ç©·å¤§æ—¶ï¼Œé€»è¾‘åŠ æƒæ”¶æ•›ä¸ºäºŒå…ƒé˜ˆå€¼ï¼Œå½¢æˆ0-1æŸå¤±çš„å‡¸åŒ…ã€‚å®éªŒè¯æ˜ï¼ŒRePOåœ¨å¤šä¸ªåŸºç¡€æ¨¡å‹ä¸Šä¼˜äºDPOå’ŒSimPOï¼Œä»…éœ€è°ƒæ•´ä¸€ä¸ªè¶…å‚æ•°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.05641', 'title': 'Symbolic Mixture-of-Experts: Adaptive Skill-based Routing for\n  Heterogeneous Reasoning', 'url': 'https://huggingface.co/papers/2503.05641', 'abstract': "Combining existing pre-trained expert LLMs is a promising avenue for scalably tackling large-scale and diverse tasks. However, selecting experts at the task level is often too coarse-grained, as heterogeneous tasks may require different expertise for each instance. To enable adaptive instance-level mixing of pre-trained LLM experts, we propose Symbolic-MoE, a symbolic, text-based, and gradient-free Mixture-of-Experts framework. Symbolic-MoE takes a fine-grained approach to selection by emphasizing skills, e.g., algebra in math or molecular biology in biomedical reasoning. We propose a skill-based recruiting strategy that dynamically selects the most relevant set of expert LLMs for diverse reasoning tasks based on their strengths. Each selected expert then generates its own reasoning, resulting in k outputs from k experts, which are then synthesized into a final high-quality response by an aggregator chosen based on its ability to integrate diverse reasoning outputs. We show that Symbolic-MoE's instance-level expert selection improves performance by a large margin but -- when implemented naively -- can introduce a high computational overhead due to the need for constant model loading and offloading. To address this, we implement a batch inference strategy that groups instances based on their assigned experts, loading each model only once. This allows us to integrate 16 expert models on 1 GPU with a time cost comparable to or better than prior multi-agent baselines using 4 GPUs. Through extensive evaluations on diverse benchmarks (MMLU-Pro, GPQA, AIME, and MedMCQA), we demonstrate that Symbolic-MoE outperforms strong LLMs like GPT4o-mini, as well as multi-agent approaches, with an absolute average improvement of 8.15% over the best multi-agent baseline. Moreover, Symbolic-MoE removes the need for expensive multi-round discussions, outperforming discussion baselines with less computation.", 'score': 1, 'issue_id': 2643, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 7', 'zh': '3æœˆ7æ—¥'}, 'hash': 'cd41a7296c5c9225', 'authors': ['Justin Chih-Yao Chen', 'Sukwon Yun', 'Elias Stengel-Eskin', 'Tianlong Chen', 'Mohit Bansal'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2503.05641.jpg', 'data': {'categories': ['#training', '#reasoning', '#optimization', '#benchmark', '#agents', '#multimodal'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Symbolic-MoE: Ğ£Ğ¼Ğ½Ğ¾Ğµ ÑĞ¾Ñ‡ĞµÑ‚Ğ°Ğ½Ğ¸Ğµ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ñ… LLM Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Symbolic-MoE - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. Symbolic-MoE Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»ÑŒĞ½Ñ‹Ğ¹, Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¹ Ğ¸ Ğ±ĞµĞ·Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ², Ğ¾ÑĞ½Ğ¾Ğ²Ñ‹Ğ²Ğ°ÑÑÑŒ Ğ½Ğ° Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ñ… Ğ½Ğ°Ğ²Ñ‹ĞºĞ°Ñ…. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ°Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¿Ğ¾Ğ´Ğ±Ğ¾Ñ€Ğ° ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ°Ğ³Ñ€ĞµĞ³Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ñ… Ğ²Ñ‹Ğ²Ğ¾Ğ´Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Symbolic-MoE Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ².'}, 'en': {'title': 'Adaptive Expert Selection for Enhanced Reasoning in LLMs', 'desc': 'The paper introduces Symbolic-MoE, a novel Mixture-of-Experts framework designed to enhance the performance of large language models (LLMs) by enabling instance-level expert selection. This approach allows for the dynamic recruitment of LLMs based on specific skills required for diverse reasoning tasks, such as algebra or molecular biology. By synthesizing outputs from multiple experts, Symbolic-MoE generates high-quality responses while addressing computational efficiency through a batch inference strategy. The results show significant performance improvements over existing models and methods, demonstrating the effectiveness of fine-grained expert selection in machine learning tasks.'}, 'zh': {'title': 'å®ä¾‹çº§ä¸“å®¶é€‰æ‹©ï¼Œæå‡æ¨ç†æ€§èƒ½ï¼', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºSymbolic-MoEçš„æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å®ä¾‹çº§çš„ä¸“å®¶é€‰æ‹©æ¥æé«˜é¢„è®­ç»ƒå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ€§èƒ½ã€‚è¯¥æ¡†æ¶é‡‡ç”¨ç¬¦å·åŒ–ã€åŸºäºæ–‡æœ¬ä¸”æ— æ¢¯åº¦çš„æ··åˆä¸“å®¶æ–¹æ³•ï¼Œå¼ºè°ƒæ ¹æ®ä»»åŠ¡çš„ä¸åŒéœ€æ±‚é€‰æ‹©åˆé€‚çš„ä¸“å®¶ã€‚é€šè¿‡åŠ¨æ€é€‰æ‹©æœ€ç›¸å…³çš„ä¸“å®¶ï¼ŒSymbolic-MoEèƒ½å¤Ÿåœ¨å¤šæ ·åŒ–çš„æ¨ç†ä»»åŠ¡ä¸­ç”Ÿæˆé«˜è´¨é‡çš„å“åº”ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSymbolic-MoEåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„å¼ºå¤§LLMå’Œå¤šä»£ç†æ–¹æ³•ï¼Œä¸”è®¡ç®—æ•ˆç‡æ›´é«˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.05283', 'title': "Escaping Plato's Cave: Towards the Alignment of 3D and Text Latent\n  Spaces", 'url': 'https://huggingface.co/papers/2503.05283', 'abstract': 'Recent works have shown that, when trained at scale, uni-modal 2D vision and text encoders converge to learned features that share remarkable structural properties, despite arising from different representations. However, the role of 3D encoders with respect to other modalities remains unexplored. Furthermore, existing 3D foundation models that leverage large datasets are typically trained with explicit alignment objectives with respect to frozen encoders from other representations. In this work, we investigate the possibility of a posteriori alignment of representations obtained from uni-modal 3D encoders compared to text-based feature spaces. We show that naive post-training feature alignment of uni-modal text and 3D encoders results in limited performance. We then focus on extracting subspaces of the corresponding feature spaces and discover that by projecting learned representations onto well-chosen lower-dimensional <PRE_TAG>subspaces</POST_TAG> the quality of alignment becomes significantly higher, leading to improved accuracy on matching and retrieval tasks. Our analysis further sheds light on the nature of these shared subspaces, which roughly separate between semantic and geometric data representations. Overall, ours is the first work that helps to establish a baseline for post-training alignment of 3D uni-modal and text feature spaces, and helps to highlight both the shared and unique properties of 3D data compared to other representations.', 'score': 1, 'issue_id': 2637, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 7', 'zh': '3æœˆ7æ—¥'}, 'hash': '85fcbc1ddbe7dbea', 'authors': ['Souhail Hadgi', 'Luca Moschella', 'Andrea Santilli', 'Diego Gomez', 'Qixing Huang', 'Emanuele RodolÃ ', 'Simone Melzi', 'Maks Ovsjanikov'], 'affiliations': ['Ecole polytechnique', 'Sapienza University of Rome', 'The University of Texas at Austin', 'University of Milano-Bicocca'], 'pdf_title_img': 'assets/pdf/title_img/2503.05283.jpg', 'data': {'categories': ['#alignment', '#3d', '#multimodal', '#transfer_learning'], 'emoji': 'ğŸ§©', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ 3D Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¾ĞµĞºÑ†Ğ¸Ñ Ğ½Ğ° Ğ¿Ğ¾Ğ´Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ¿Ğ¾ÑÑ‚ĞµÑ€Ğ¸Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹, Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… 3D-ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ², Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¿Ğ¾ÑĞ»Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ°ĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹. ĞĞ´Ğ½Ğ°ĞºĞ¾, Ğ¿Ñ€Ğ¾ĞµÑ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ¼ĞµĞ½ÑŒÑˆĞµĞ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ», Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¸ Ğ¾Ğ±Ñ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ½Ğ¾ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑÑÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Enhancing 3D and Text Feature Alignment through Subspace Projection', 'desc': 'This paper explores the alignment of features from uni-modal 3D encoders with text-based representations. It reveals that simply aligning these features after training does not yield good results. Instead, the authors find that projecting these features into carefully selected lower-dimensional subspaces significantly improves alignment quality. This work establishes a baseline for understanding how 3D data relates to text features, highlighting both their similarities and differences.'}, 'zh': {'title': 'æ¢ç´¢3Dç¼–ç å™¨ä¸æ–‡æœ¬ç‰¹å¾çš„å¯¹é½ä¹‹è·¯', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†3Dç¼–ç å™¨åœ¨ä¸å…¶ä»–æ¨¡æ€çš„å…³ç³»ä¸­çš„ä½œç”¨ï¼Œå°¤å…¶æ˜¯ä¸æ–‡æœ¬ç‰¹å¾ç©ºé—´çš„å¯¹æ¯”ã€‚æˆ‘ä»¬å‘ç°ï¼Œç®€å•çš„åæœŸè®­ç»ƒç‰¹å¾å¯¹é½æ–¹æ³•åœ¨å•æ¨¡æ€æ–‡æœ¬å’Œ3Dç¼–ç å™¨ä¹‹é—´çš„æ€§èƒ½æœ‰é™ã€‚é€šè¿‡æå–ç‰¹å¾ç©ºé—´çš„å­ç©ºé—´ï¼Œå¹¶å°†å­¦ä¹ åˆ°çš„è¡¨ç¤ºæŠ•å½±åˆ°ç²¾å¿ƒé€‰æ‹©çš„ä½ç»´å­ç©ºé—´ä¸­ï¼Œæˆ‘ä»¬æ˜¾è‘—æé«˜äº†å¯¹é½è´¨é‡ï¼Œä»è€Œåœ¨åŒ¹é…å’Œæ£€ç´¢ä»»åŠ¡ä¸­æå‡äº†å‡†ç¡®æ€§ã€‚æˆ‘ä»¬çš„å·¥ä½œé¦–æ¬¡ä¸º3Då•æ¨¡æ€å’Œæ–‡æœ¬ç‰¹å¾ç©ºé—´çš„åæœŸè®­ç»ƒå¯¹é½å»ºç«‹äº†åŸºçº¿ï¼Œå¹¶çªå‡ºäº†3Dæ•°æ®ä¸å…¶ä»–è¡¨ç¤ºä¹‹é—´çš„å…±äº«å’Œç‹¬ç‰¹ç‰¹æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.02819', 'title': 'Feynman-Kac Correctors in Diffusion: Annealing, Guidance, and Product of\n  Experts', 'url': 'https://huggingface.co/papers/2503.02819', 'abstract': "While score-based generative models are the model of choice across diverse domains, there are limited tools available for controlling inference-time behavior in a principled manner, e.g. for composing multiple pretrained models. Existing classifier-free guidance methods use a simple heuristic to mix conditional and un<PRE_TAG>conditional scores</POST_TAG> to approximately sample from conditional distributions. However, such methods do not approximate the intermediate distributions, necessitating additional 'corrector' steps. In this work, we provide an efficient and principled method for sampling from a sequence of annealed, geometric-averaged, or product distributions derived from pretrained score-based models. We derive a weighted simulation scheme which we call Feynman-Kac Correctors (FKCs) based on the celebrated Feynman-Kac formula by carefully accounting for terms in the appropriate partial differential equations (PDEs). To simulate these PDEs, we propose Sequential Monte Carlo (SMC) resampling algorithms that leverage inference-time scaling to improve sampling quality. We empirically demonstrate the utility of our methods by proposing amortized sampling via inference-time temperature annealing, improving multi-objective molecule generation using pretrained models, and improving classifier-free guidance for text-to-image generation. Our code is available at https://github.com/martaskrt/fkc-diffusion.", 'score': 1, 'issue_id': 2650, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 4', 'zh': '3æœˆ4æ—¥'}, 'hash': '6aa3f8c59c70086b', 'authors': ['Marta Skreta', 'Tara Akhound-Sadegh', 'Viktor Ohanesian', 'Roberto Bondesan', 'AlÃ¡n Aspuru-Guzik', 'Arnaud Doucet', 'Rob Brekelmans', 'Alexander Tong', 'Kirill Neklyudov'], 'affiliations': ['Google DeepMind', 'Imperial College London', 'McGill University', 'Mila - Quebec AI Institute', 'University of Toronto', 'UniversitÃ© de MontrÃ©al', 'Vector Institute'], 'pdf_title_img': 'assets/pdf/title_img/2503.02819.jpg', 'data': {'categories': ['#multimodal', '#optimization', '#cv', '#diffusion', '#inference'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ² Ğ¤ĞµĞ¹Ğ½Ğ¼Ğ°Ğ½Ğ°-ĞšĞ°Ñ†Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Feynman-Kac Correctors (FKCs) Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞµĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ· Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑƒÑÑ€ĞµĞ´Ğ½ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ½Ñ‹Ñ… Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹, Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹Ğ²Ğ¾Ğ´ÑÑ‚ Ğ²Ğ·Ğ²ĞµÑˆĞµĞ½Ğ½ÑƒÑ ÑÑ…ĞµĞ¼Ñƒ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ½Ğ° Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğµ Ğ¤ĞµĞ¹Ğ½Ğ¼Ğ°Ğ½Ğ°-ĞšĞ°Ñ†Ğ°, Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ñ‹ Ñ€ĞµÑĞµĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° ĞœĞ¾Ğ½Ñ‚Ğµ-ĞšĞ°Ñ€Ğ»Ğ¾. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑĞµĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ FKCs Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ» Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'Enhancing Score-Based Generative Models with Feynman-Kac Correctors', 'desc': 'This paper introduces a new method for controlling the behavior of score-based generative models during inference. The authors present Feynman-Kac Correctors (FKCs), which provide a principled way to sample from complex distributions derived from pretrained models. By using Sequential Monte Carlo (SMC) resampling algorithms, they enhance the quality of sampling through inference-time scaling. The proposed method shows improvements in various applications, including molecule generation and text-to-image generation, demonstrating its effectiveness in practical scenarios.'}, 'zh': {'title': 'é«˜æ•ˆæ§åˆ¶ç”Ÿæˆæ¨¡å‹çš„æ¨ç†è¡Œä¸º', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§é«˜æ•ˆä¸”æœ‰åŸåˆ™çš„æ–¹æ³•ï¼Œç”¨äºä»é¢„è®­ç»ƒçš„åŸºäºåˆ†æ•°çš„æ¨¡å‹ä¸­é‡‡æ ·ã€‚æˆ‘ä»¬å¼•å…¥äº†Feynman-Kacæ ¡æ­£å™¨ï¼ˆFKCï¼‰ï¼ŒåŸºäºFeynman-Kacå…¬å¼ï¼Œç²¾ç¡®å¤„ç†é€‚å½“çš„åå¾®åˆ†æ–¹ç¨‹ï¼ˆPDEï¼‰ä¸­çš„é¡¹ã€‚é€šè¿‡ä½¿ç”¨é¡ºåºè’™ç‰¹å¡æ´›ï¼ˆSMCï¼‰é‡é‡‡æ ·ç®—æ³•ï¼Œæˆ‘ä»¬æé«˜äº†é‡‡æ ·è´¨é‡ï¼Œå¹¶å±•ç¤ºäº†è¯¥æ–¹æ³•åœ¨å¤šç›®æ ‡åˆ†å­ç”Ÿæˆå’Œæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­çš„åº”ç”¨æ•ˆæœã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ºæ§åˆ¶æ¨ç†æ—¶çš„è¡Œä¸ºæä¾›äº†æ–°çš„å·¥å…·ï¼Œæ¨åŠ¨äº†ç”Ÿæˆæ¨¡å‹çš„è¿›ä¸€æ­¥å‘å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.07413', 'title': 'REF-VLM: Triplet-Based Referring Paradigm for Unified Visual Decoding', 'url': 'https://huggingface.co/papers/2503.07413', 'abstract': 'Multimodal Large Language Models (MLLMs) demonstrate robust zero-shot capabilities across diverse vision-language tasks after training on mega-scale datasets. However, dense prediction tasks, such as semantic segmentation and keypoint detection, pose significant challenges for MLLMs when represented solely as text outputs. Simultaneously, current MLLMs utilizing latent embeddings for visual task decoding generally demonstrate limited adaptability to both multi-task learning and multi-granularity scenarios. In this work, we present REF-VLM, an end-to-end framework for unified training of various visual decoding tasks. To address complex visual decoding scenarios, we introduce the Triplet-Based Referring Paradigm (TRP), which explicitly decouples three critical dimensions in visual decoding tasks through a triplet structure: concepts, decoding types, and targets. TRP employs symbolic delimiters to enforce structured representation learning, enhancing the parsability and interpretability of model outputs. Additionally, we construct Visual-Task Instruction Following Dataset (VTInstruct), a large-scale multi-task dataset containing over 100 million multimodal dialogue samples across 25 task types. Beyond text inputs and outputs, VT-Instruct incorporates various visual prompts such as point, box, scribble, and mask, and generates outputs composed of text and visual units like box, keypoint, depth and mask. The combination of different visual prompts and visual units generates a wide variety of task types, expanding the applicability of REF-VLM significantly. Both qualitative and quantitative experiments demonstrate that our REF-VLM outperforms other MLLMs across a variety of standard benchmarks. The code, dataset, and demo available at https://github.com/MacavityT/REF-VLM.', 'score': 0, 'issue_id': 2643, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 10', 'zh': '3æœˆ10æ—¥'}, 'hash': '52ecfef5a3f1d715', 'authors': ['Yan Tai', 'Luhao Zhu', 'Zhiqiang Chen', 'Ynan Ding', 'Yiying Dong', 'Xiaohong Liu', 'Guodong Guo'], 'affiliations': ['Hong Kong Polytechnic University', 'Ningbo Institute of Digital Twin, Eastern Institute of Technology, Ningbo, China', 'Shanghai Jiao Tong University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.07413.jpg', 'data': {'categories': ['#interpretability', '#cv', '#dataset', '#benchmark', '#multimodal', '#games'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'REF-VLM: Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ REF-VLM, Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Triplet-Based Referring Paradigm (TRP) Ğ´Ğ»Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… VTInstruct, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ĞµĞµ 100 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ² Ğ´Ğ»Ñ 25 Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ REF-VLM Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ….'}, 'en': {'title': 'REF-VLM: Unifying Visual Decoding for Enhanced Multimodal Learning', 'desc': 'This paper introduces REF-VLM, a new framework designed to improve the performance of Multimodal Large Language Models (MLLMs) on complex visual tasks like semantic segmentation and keypoint detection. It addresses the limitations of current MLLMs by using a Triplet-Based Referring Paradigm (TRP) that separates concepts, decoding types, and targets for better structured learning. The authors also present a large-scale dataset, VTInstruct, which includes multimodal dialogue samples and various visual prompts to enhance training. Experimental results show that REF-VLM significantly outperforms existing MLLMs on standard benchmarks, demonstrating its effectiveness in handling diverse visual decoding tasks.'}, 'zh': {'title': 'REF-VLMï¼šç»Ÿä¸€è§†è§‰è§£ç ä»»åŠ¡çš„åˆ›æ–°æ¡†æ¶', 'desc': 'å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ç»è¿‡å¤§è§„æ¨¡æ•°æ®é›†è®­ç»ƒåï¼Œå±•ç°å‡ºåœ¨å¤šç§è§†è§‰è¯­è¨€ä»»åŠ¡ä¸­çš„å¼ºå¤§é›¶-shotèƒ½åŠ›ã€‚ç„¶è€Œï¼Œå¯¹äºå¯†é›†é¢„æµ‹ä»»åŠ¡ï¼Œå¦‚è¯­ä¹‰åˆ†å‰²å’Œå…³é”®ç‚¹æ£€æµ‹ï¼Œä»…ç”¨æ–‡æœ¬è¾“å‡ºè¡¨ç¤ºæ—¶ï¼ŒMLLMsé¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›å¤æ‚çš„è§†è§‰è§£ç åœºæ™¯ï¼Œæˆ‘ä»¬æå‡ºäº†REF-VLMï¼Œä¸€ä¸ªç”¨äºç»Ÿä¸€è®­ç»ƒå„ç§è§†è§‰è§£ç ä»»åŠ¡çš„ç«¯åˆ°ç«¯æ¡†æ¶ï¼Œå¹¶å¼•å…¥äº†åŸºäºä¸‰å…ƒç»„çš„å¼•ç”¨èŒƒå¼ï¼ˆTRPï¼‰ï¼Œä»¥æ˜ç¡®è§£è€¦è§†è§‰è§£ç ä»»åŠ¡ä¸­çš„æ¦‚å¿µã€è§£ç ç±»å‹å’Œç›®æ ‡ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒREF-VLMåœ¨å¤šé¡¹æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºå…¶ä»–MLLMsï¼Œå±•ç¤ºäº†å…¶åœ¨å¤šä»»åŠ¡å­¦ä¹ å’Œå¤šç²’åº¦åœºæ™¯ä¸­çš„é€‚åº”æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.06698', 'title': "What's in a Latent? Leveraging Diffusion Latent Space for Domain\n  Generalization", 'url': 'https://huggingface.co/papers/2503.06698', 'abstract': 'Domain Generalization aims to develop models that can generalize to novel and unseen data distributions. In this work, we study how model architectures and pre-training objectives impact feature richness and propose a method to effectively leverage them for domain generalization. Specifically, given a pre-trained feature space, we first discover latent domain structures, referred to as pseudo-domains, that capture domain-specific variations in an unsupervised manner. Next, we augment existing classifiers with these complementary pseudo-domain representations making them more amenable to diverse unseen test domains. We analyze how different pre-training feature spaces differ in the domain-specific variances they capture. Our empirical studies reveal that features from diffusion models excel at separating domains in the absence of explicit domain labels and capture nuanced domain-specific information. On 5 datasets, we show that our very simple framework improves generalization to unseen domains by a maximum test accuracy improvement of over 4% compared to the standard baseline Empirical Risk Minimization (ERM). Crucially, our method outperforms most algorithms that access domain labels during training.', 'score': 0, 'issue_id': 2645, 'pub_date': '2025-03-09', 'pub_date_card': {'ru': '9 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 9', 'zh': '3æœˆ9æ—¥'}, 'hash': 'c706eb4738361e39', 'authors': ['Xavier Thomas', 'Deepti Ghadiyaram'], 'affiliations': ['Boston University', 'Runway'], 'pdf_title_img': 'assets/pdf/title_img/2503.06698.jpg', 'data': {'categories': ['#transfer_learning', '#optimization', '#dataset', '#training', '#architecture'], 'emoji': 'ğŸŒ', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ´Ğ¾Ğ¼ĞµĞ½Ñ‹ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ĞµĞ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€. ĞĞ½Ğ¸ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ÑÑÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ñ‹ ÑÑ‚Ğ¸Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¿ÑĞµĞ²Ğ´Ğ¾-Ğ´Ğ¾Ğ¼ĞµĞ½Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ Ğ½Ğ° Ğ½ĞµĞ·Ğ½Ğ°ĞºĞ¾Ğ¼Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ñ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ¸Ğ· Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ÑÑ‚ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ¾Ğ² Ğ±ĞµĞ· ÑĞ²Ğ½Ñ‹Ñ… Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğº.'}, 'en': {'title': 'Unlocking Generalization with Pseudo-Domains', 'desc': "This paper focuses on Domain Generalization, which is about creating models that can perform well on new and unseen data. The authors investigate how different model architectures and pre-training methods affect the richness of features used for generalization. They introduce a technique to identify latent structures called pseudo-domains that represent variations in data without needing labels. Their experiments show that using features from diffusion models significantly enhances the model's ability to generalize, achieving better accuracy on unseen domains compared to traditional methods."}, 'zh': {'title': 'æå‡æ¨¡å‹çš„é¢†åŸŸæ³›åŒ–èƒ½åŠ›', 'desc': 'é¢†åŸŸæ³›åŒ–æ—¨åœ¨å¼€å‘èƒ½å¤Ÿå¯¹æ–°é¢–å’Œæœªè§æ•°æ®åˆ†å¸ƒè¿›è¡Œæ³›åŒ–çš„æ¨¡å‹ã€‚æœ¬æ–‡ç ”ç©¶äº†æ¨¡å‹æ¶æ„å’Œé¢„è®­ç»ƒç›®æ ‡å¦‚ä½•å½±å“ç‰¹å¾ä¸°å¯Œæ€§ï¼Œå¹¶æå‡ºäº†ä¸€ç§æœ‰æ•ˆåˆ©ç”¨è¿™äº›ç‰¹å¾è¿›è¡Œé¢†åŸŸæ³›åŒ–çš„æ–¹æ³•ã€‚æˆ‘ä»¬é¦–å…ˆåœ¨æ— ç›‘ç£çš„æƒ…å†µä¸‹å‘ç°æ½œåœ¨çš„é¢†åŸŸç»“æ„ï¼Œç§°ä¸ºä¼ªé¢†åŸŸï¼Œè¿™äº›ç»“æ„æ•æ‰äº†é¢†åŸŸç‰¹å®šçš„å˜åŒ–ã€‚é€šè¿‡å¢å¼ºç°æœ‰åˆ†ç±»å™¨ä¸è¿™äº›ä¼ªé¢†åŸŸè¡¨ç¤ºçš„ç»“åˆï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨äº”ä¸ªæ•°æ®é›†ä¸Šæ˜¾ç¤ºå‡ºæ¯”æ ‡å‡†åŸºçº¿ç»éªŒé£é™©æœ€å°åŒ–ï¼ˆERMï¼‰æé«˜äº†è¶…è¿‡4%çš„æµ‹è¯•å‡†ç¡®ç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.03511', 'title': 'NeuGrasp: Generalizable Neural Surface Reconstruction with Background\n  Priors for Material-Agnostic Object Grasp Detection', 'url': 'https://huggingface.co/papers/2503.03511', 'abstract': 'Robotic grasping in scenes with transparent and specular objects presents great challenges for methods relying on accurate depth information. In this paper, we introduce NeuGrasp, a neural surface reconstruction method that leverages background priors for material-agnostic grasp detection. NeuGrasp integrates transformers and global prior volumes to aggregate multi-view features with spatial encoding, enabling robust surface reconstruction in narrow and sparse viewing conditions. By focusing on foreground objects through residual feature enhancement and refining spatial perception with an occupancy-prior volume, NeuGrasp excels in handling objects with transparent and specular surfaces. Extensive experiments in both simulated and real-world scenarios show that NeuGrasp outperforms state-of-the-art methods in grasping while maintaining comparable reconstruction quality. More details are available at https://neugrasp.github.io/.', 'score': 0, 'issue_id': 2630, 'pub_date': '2025-03-05', 'pub_date_card': {'ru': '5 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 5', 'zh': '3æœˆ5æ—¥'}, 'hash': 'ba660b9e0f676df1', 'authors': ['Qingyu Fan', 'Yinghao Cai', 'Chao Li', 'Wenzhe He', 'Xudong Zheng', 'Tao Lu', 'Bin Liang', 'Shuo Wang'], 'affiliations': ['Qiyuan Lab', 'School of Artificial Intelligence, University of Chinese Academy of Sciences', 'State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2503.03511.jpg', 'data': {'categories': ['#robotics', '#agents', '#architecture', '#cv'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'NeuGrasp: Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚ Ğ´Ğ»Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚ĞµĞ¹', 'desc': 'NeuGrasp - ÑÑ‚Ğ¾ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ·ĞµÑ€ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ñ‹ Ğ¸ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ñ‹ Ğ´Ğ»Ñ Ğ°Ğ³Ñ€ĞµĞ³Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼. NeuGrasp Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ñ… Ğ¿ĞµÑ€ĞµĞ´Ğ½ĞµĞ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ° Ğ¸ ÑƒÑ‚Ğ¾Ñ‡Ğ½ÑĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑŠĞµĞ¼Ğ° Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¾Ğ² Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ NeuGrasp Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ².'}, 'en': {'title': 'NeuGrasp: Mastering Grasping with Transparent and Specular Objects', 'desc': 'This paper presents NeuGrasp, a novel approach for robotic grasping that effectively deals with transparent and specular objects, which are challenging for traditional depth-based methods. NeuGrasp utilizes a neural surface reconstruction technique that incorporates background priors to enhance grasp detection without being limited by material properties. By combining transformers and global prior volumes, it aggregates multi-view features with spatial encoding, improving surface reconstruction even in difficult viewing conditions. The method demonstrates superior performance in grasping tasks compared to existing techniques, while also achieving high-quality surface reconstruction in both simulated and real-world environments.'}, 'zh': {'title': 'NeuGraspï¼šé€æ˜ç‰©ä½“æŠ“å–çš„æ–°çªç ´', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºNeuGraspçš„ç¥ç»è¡¨é¢é‡å»ºæ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³é€æ˜å’Œé•œé¢ç‰©ä½“æŠ“å–ä¸­çš„æŒ‘æˆ˜ã€‚è¯¥æ–¹æ³•åˆ©ç”¨èƒŒæ™¯å…ˆéªŒè¿›è¡Œææ–™æ— å…³çš„æŠ“å–æ£€æµ‹ï¼Œç»“åˆäº†å˜æ¢å™¨å’Œå…¨å±€å…ˆéªŒä½“ç§¯ï¼Œä»¥èšåˆå¤šè§†è§’ç‰¹å¾å¹¶è¿›è¡Œç©ºé—´ç¼–ç ã€‚NeuGraspé€šè¿‡æ®‹å·®ç‰¹å¾å¢å¼ºèšç„¦äºå‰æ™¯ç‰©ä½“ï¼Œå¹¶åˆ©ç”¨å ç”¨å…ˆéªŒä½“ç§¯æ¥æ”¹å–„ç©ºé—´æ„ŸçŸ¥ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå¤„ç†é€æ˜å’Œé•œé¢è¡¨é¢çš„ç‰©ä½“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒNeuGraspåœ¨æŠ“å–æ€§èƒ½ä¸Šä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼ŒåŒæ—¶ä¿æŒäº†ç›¸ä¼¼çš„é‡å»ºè´¨é‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.20475', 'title': 'Promote, Suppress, Iterate: How Language Models Answer One-to-Many\n  Factual Queries', 'url': 'https://huggingface.co/papers/2502.20475', 'abstract': "To answer one-to-many factual queries (e.g., listing cities of a country), a language model (LM) must simultaneously recall knowledge and avoid repeating previous answers. How are these two subtasks implemented and integrated internally? Across multiple datasets and models, we identify a promote-then-suppress mechanism: the model first recalls all answers, and then suppresses previously generated ones. Specifically, LMs use both the subject and previous answer tokens to perform knowledge recall, with attention propagating subject information and MLPs promoting the answers. Then, attention attends to and suppresses previous answer tokens, while MLPs amplify the suppression signal. Our mechanism is corroborated by extensive experimental evidence: in addition to using early decoding and causal tracing, we analyze how components use different tokens by introducing both Token Lens, which decodes aggregated attention updates from specified tokens, and a knockout method that analyzes changes in MLP outputs after removing attention to specified tokens. Overall, we provide new insights into how LMs' internal components interact with different input tokens to support complex factual recall. Code is available at https://github.com/Lorenayannnnn/how-lms-answer-one-to-many-factual-queries.", 'score': 0, 'issue_id': 2645, 'pub_date': '2025-02-27', 'pub_date_card': {'ru': '27 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 27', 'zh': '2æœˆ27æ—¥'}, 'hash': 'c89164e93db21862', 'authors': ['Tianyi Lorena Yan', 'Robin Jia'], 'affiliations': ['University of Southern California'], 'pdf_title_img': 'assets/pdf/title_img/2502.20475.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#data', '#interpretability', '#dataset', '#training', '#architecture'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ Ğ°ÑĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ĞµĞ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğµ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ñ„Ğ°ĞºÑ‚Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹', 'desc': "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ñ„Ğ°ĞºÑ‚Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹ Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ 'Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ, Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ': Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°ĞµÑ‚ Ğ²ÑĞµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑƒĞ¶Ğµ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·ÑƒĞµÑ‚ÑÑ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ ÑĞ»Ğ¾ĞµĞ² Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ MLP, Ğ³Ğ´Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°, Ğ° MLP ÑƒÑĞ¸Ğ»Ğ¸Ğ²Ğ°ÑÑ‚ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¸ Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ÑĞµÑ‚ÑÑ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°, Ñ‚Ğ°ĞºĞ¸Ğ¼Ğ¸ ĞºĞ°Ğº Token Lens Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ½Ğ¾ĞºĞ°ÑƒÑ‚Ğ°."}, 'en': {'title': 'Promote and Suppress: The Key to Factual Recall in Language Models', 'desc': 'This paper explores how language models (LMs) handle one-to-many factual queries by using a promote-then-suppress mechanism. Initially, the model recalls all possible answers based on the subject and previous answers, utilizing attention and multi-layer perceptrons (MLPs) to promote relevant responses. Subsequently, it suppresses any previously generated answers to avoid repetition, ensuring a diverse output. The authors provide experimental evidence and novel analysis methods to demonstrate how LMs manage these internal processes effectively.'}, 'zh': {'title': 'è¯­è¨€æ¨¡å‹çš„ä¿ƒè¿›ä¸æŠ‘åˆ¶æœºåˆ¶', 'desc': 'æœ¬æ–‡æ¢è®¨äº†è¯­è¨€æ¨¡å‹åœ¨å›ç­”ä¸€å¯¹å¤šäº‹å®æŸ¥è¯¢æ—¶çš„å†…éƒ¨æœºåˆ¶ã€‚ç ”ç©¶å‘ç°ï¼Œæ¨¡å‹é‡‡ç”¨äº†ä¸€ç§å…ˆä¿ƒè¿›åæŠ‘åˆ¶çš„æœºåˆ¶ï¼Œé¦–å…ˆå›å¿†æ‰€æœ‰å¯èƒ½çš„ç­”æ¡ˆï¼Œç„¶åæŠ‘åˆ¶ä¹‹å‰ç”Ÿæˆçš„ç­”æ¡ˆã€‚å…·ä½“æ¥è¯´ï¼Œæ¨¡å‹åˆ©ç”¨ä¸»é¢˜å’Œä¹‹å‰ç­”æ¡ˆçš„æ ‡è®°è¿›è¡ŒçŸ¥è¯†å›å¿†ï¼Œé€šè¿‡æ³¨æ„åŠ›æœºåˆ¶ä¼ æ’­ä¸»é¢˜ä¿¡æ¯ï¼Œå¹¶é€šè¿‡å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰ä¿ƒè¿›ç­”æ¡ˆçš„ç”Ÿæˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æœºåˆ¶æœ‰æ•ˆï¼Œæä¾›äº†å¯¹è¯­è¨€æ¨¡å‹å†…éƒ¨ç»„ä»¶å¦‚ä½•ä¸ä¸åŒè¾“å…¥æ ‡è®°äº¤äº’ä»¥æ”¯æŒå¤æ‚äº‹å®å›å¿†çš„æ–°è§è§£ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.20730', 'title': 'DeepSolution: Boosting Complex Engineering Solution Design via Tree-based Exploration and Bi-point Thinking', 'url': 'https://huggingface.co/papers/2502.20730', 'abstract': "Designing solutions for complex engineering challenges is crucial in human production activities. However, previous research in the retrieval-augmented generation (RAG) field has not sufficiently addressed tasks related to the design of complex engineering solutions. To fill this gap, we introduce a new benchmark, SolutionBench, to evaluate a system's ability to generate complete and feasible solutions for engineering problems with multiple complex constraints. To further advance the design of complex engineering solutions, we propose a novel system, SolutionRAG, that leverages the tree-based exploration and bi-point thinking mechanism to generate reliable solutions. Extensive experimental results demonstrate that SolutionRAG achieves state-of-the-art (SOTA) performance on the SolutionBench, highlighting its potential to enhance the automation and reliability of complex engineering solution design in real-world applications.", 'score': 20, 'issue_id': 2486, 'pub_date': '2025-02-28', 'pub_date_card': {'ru': '28 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 28', 'zh': '2æœˆ28æ—¥'}, 'hash': 'e9ef168e304ec240', 'authors': ['Zhuoqun Li', 'Haiyang Yu', 'Xuanang Chen', 'Hongyu Lin', 'Yaojie Lu', 'Fei Huang', 'Xianpei Han', 'Yongbin Li', 'Le Sun'], 'affiliations': ['Chinese Information Processing Laboratory, Institute of Software, Chinese Academy of Sciences', 'Tongyi Lab', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2502.20730.jpg', 'data': {'categories': ['#rag', '#benchmark'], 'emoji': 'ğŸ”§', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ½Ñ‹Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº SolutionBench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸ÑĞ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ SolutionRAG, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰ÑƒÑ Ğ´Ñ€ĞµĞ²Ğ¾Ğ²Ğ¸Ğ´Ğ½Ğ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ´Ğ²ÑƒÑ…Ñ‚Ğ¾Ñ‡ĞµÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ SolutionRAG Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ½Ğ° SolutionBench. Ğ­Ñ‚Ğ¾ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ½Ñ‹Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ÑÑ….'}, 'en': {'title': 'Revolutionizing Engineering Design with SolutionRAG', 'desc': 'This paper addresses the need for effective solutions in complex engineering design tasks, which have been overlooked in previous research on retrieval-augmented generation (RAG). It introduces a new benchmark called SolutionBench, aimed at evaluating the generation of feasible solutions under multiple constraints. The authors propose a novel system named SolutionRAG, which utilizes tree-based exploration and bi-point thinking to improve solution reliability. Experimental results show that SolutionRAG outperforms existing methods, indicating its potential to automate and enhance the design process in engineering applications.'}, 'zh': {'title': 'æå‡å¤æ‚å·¥ç¨‹è®¾è®¡çš„è‡ªåŠ¨åŒ–ä¸å¯é æ€§', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œç§°ä¸ºSolutionBenchï¼Œç”¨äºè¯„ä¼°ç³»ç»Ÿåœ¨ç”Ÿæˆå¤æ‚å·¥ç¨‹é—®é¢˜çš„å®Œæ•´å’Œå¯è¡Œè§£å†³æ–¹æ¡ˆæ–¹é¢çš„èƒ½åŠ›ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§æ–°ç³»ç»ŸSolutionRAGï¼Œåˆ©ç”¨æ ‘å½¢æ¢ç´¢å’ŒåŒç‚¹æ€ç»´æœºåˆ¶æ¥ç”Ÿæˆå¯é çš„è§£å†³æ–¹æ¡ˆã€‚é€šè¿‡å¤§é‡å®éªŒç»“æœï¼ŒSolutionRAGåœ¨SolutionBenchä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ˜¾ç¤ºäº†å…¶åœ¨å®é™…åº”ç”¨ä¸­æé«˜å¤æ‚å·¥ç¨‹è§£å†³æ–¹æ¡ˆè®¾è®¡çš„è‡ªåŠ¨åŒ–å’Œå¯é æ€§çš„æ½œåŠ›ã€‚æ­¤ç ”ç©¶å¡«è¡¥äº†ä»¥å¾€åœ¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰é¢†åŸŸä¸­å¯¹å¤æ‚å·¥ç¨‹è§£å†³æ–¹æ¡ˆè®¾è®¡ä»»åŠ¡çš„ç ”ç©¶ç©ºç™½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.18600', 'title': 'Chain of Draft: Thinking Faster by Writing Less', 'url': 'https://huggingface.co/papers/2502.18600', 'abstract': 'Large Language Models (LLMs) have demonstrated remarkable performance in solving complex reasoning tasks through mechanisms like Chain-of-Thought (CoT) prompting, which emphasizes verbose, step-by-step reasoning. However, humans typically employ a more efficient strategy: drafting concise intermediate thoughts that capture only essential information. In this work, we propose Chain of Draft (CoD), a novel paradigm inspired by human cognitive processes, where LLMs generate minimalistic yet informative intermediate reasoning outputs while solving tasks. By reducing verbosity and focusing on critical insights, CoD matches or surpasses CoT in accuracy while using as little as only 7.6% of the tokens, significantly reducing cost and latency across various reasoning tasks.', 'score': 19, 'issue_id': 2491, 'pub_date': '2025-02-25', 'pub_date_card': {'ru': '25 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 25', 'zh': '2æœˆ25æ—¥'}, 'hash': '739d903f5735d9eb', 'authors': ['Silei Xu', 'Wenhao Xie', 'Lingxiao Zhao', 'Pengcheng He'], 'affiliations': ['Zoom Communications'], 'pdf_title_img': 'assets/pdf/title_img/2502.18600.jpg', 'data': {'categories': ['#reasoning', '#training', '#rl'], 'emoji': 'âœï¸', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ: ĞºÑ€Ğ°Ñ‚ĞºĞ¾ÑÑ‚ÑŒ - ÑĞµÑÑ‚Ñ€Ğ° Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ - Chain of Draft (CoD). Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Chain-of-Thought (CoT), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, CoD Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑ ĞºÑ€Ğ°Ñ‚ĞºĞ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ¼Ñ‹ÑĞ»Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ñ‚Ğ¾Ğ¹ Ğ¶Ğµ Ğ¸Ğ»Ğ¸ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¸ CoT, Ğ½Ğ¾ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¼ĞµĞ½ÑŒÑˆĞµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². CoD Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, ÑĞ½Ğ¸Ğ¶Ğ°Ñ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ¸ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ¸.'}, 'en': {'title': 'Streamlining Reasoning: Less is More with Chain of Draft', 'desc': 'This paper introduces Chain of Draft (CoD), a new approach for Large Language Models (LLMs) that mimics human reasoning by generating concise intermediate thoughts. Unlike the traditional Chain-of-Thought (CoT) prompting, which relies on verbose explanations, CoD focuses on delivering essential information in a minimalistic format. The authors demonstrate that CoD can achieve comparable or even superior accuracy to CoT while using significantly fewer tokens, leading to reduced computational costs and faster processing times. This innovative method enhances the efficiency of LLMs in tackling complex reasoning tasks.'}, 'zh': {'title': 'è‰ç¨¿é“¾ï¼šé«˜æ•ˆæ¨ç†çš„æ–°æ–¹æ³•', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è§£å†³å¤æ‚æ¨ç†ä»»åŠ¡æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶æ˜¯é€šè¿‡é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æç¤ºï¼Œå¼ºè°ƒé€æ­¥æ¨ç†çš„è¯¦ç»†è¿‡ç¨‹ã€‚ç„¶è€Œï¼Œäººç±»é€šå¸¸é‡‡ç”¨æ›´é«˜æ•ˆçš„ç­–ç•¥ï¼šè‰æ‹Ÿç®€æ´çš„ä¸­é—´æ€è€ƒï¼Œåªæ•æ‰å…³é”®ä¿¡æ¯ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°èŒƒå¼â€”â€”è‰ç¨¿é“¾ï¼ˆCoDï¼‰ï¼Œçµæ„Ÿæ¥æºäºäººç±»çš„è®¤çŸ¥è¿‡ç¨‹ï¼Œä½¿LLMsåœ¨è§£å†³ä»»åŠ¡æ—¶ç”Ÿæˆç®€çº¦è€Œä¿¡æ¯ä¸°å¯Œçš„ä¸­é—´æ¨ç†è¾“å‡ºã€‚é€šè¿‡å‡å°‘å†—é•¿å¹¶ä¸“æ³¨äºå…³é”®è§è§£ï¼ŒCoDåœ¨å‡†ç¡®æ€§ä¸Šä¸CoTç›¸åŒ¹é…æˆ–è¶…è¶Šï¼ŒåŒæ—¶ä»…ä½¿ç”¨7.6%çš„æ ‡è®°ï¼Œæ˜¾è‘—é™ä½äº†å„ç§æ¨ç†ä»»åŠ¡çš„æˆæœ¬å’Œå»¶è¿Ÿã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.20380', 'title': 'Multi-Turn Code Generation Through Single-Step Rewards', 'url': 'https://huggingface.co/papers/2502.20380', 'abstract': 'We address the problem of code generation from multi-turn execution feedback. Existing methods either generate code without feedback or use complex, hierarchical reinforcement learning to optimize multi-turn rewards. We propose a simple yet scalable approach, muCode, that solves multi-turn code generation using only single-step rewards. Our key insight is that code generation is a one-step recoverable MDP, where the correct code can be recovered from any intermediate code state in a single turn. muCode iteratively trains both a generator to provide code solutions conditioned on multi-turn execution feedback and a verifier to score the newly generated code. Experimental evaluations show that our approach achieves significant improvements over the state-of-the-art baselines. We provide analysis of the design choices of the reward models and policy, and show the efficacy of muCode at utilizing the execution feedback. Our code is available at https://github.com/portal-cornell/muCode.', 'score': 17, 'issue_id': 2500, 'pub_date': '2025-02-27', 'pub_date_card': {'ru': '27 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 27', 'zh': '2æœˆ27æ—¥'}, 'hash': 'cceb0299fb5077b5', 'authors': ['Arnav Kumar Jain', 'Gonzalo Gonzalez-Pumariega', 'Wayne Chen', 'Alexander M Rush', 'Wenting Zhao', 'Sanjiban Choudhury'], 'affiliations': ['Cornell University', 'MilaQuebec AI Institute', 'Universite de Montreal'], 'pdf_title_img': 'assets/pdf/title_img/2502.20380.jpg', 'data': {'categories': ['#rl', '#rlhf', '#training', '#optimization'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ĞºĞ¾Ğ´Ğ° Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·ÑŒÑ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ° Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ¿Ğ¾ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ muCode. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ñ… ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğµ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, muCode Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ¹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ Ğ¾Ğ´Ğ½Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Simplifying Code Generation with muCode: One-Step Recovery from Feedback', 'desc': 'This paper presents muCode, a novel approach for generating code based on multi-turn execution feedback. Unlike existing methods that rely on complex reinforcement learning techniques, muCode simplifies the process by using single-step rewards. The authors argue that code generation can be treated as a one-step recoverable Markov Decision Process (MDP), allowing for the recovery of correct code from any intermediate state. Through iterative training of a code generator and a verifier, muCode demonstrates significant performance improvements over current state-of-the-art methods.'}, 'zh': {'title': 'ç®€å•é«˜æ•ˆçš„å¤šè½®ä»£ç ç”Ÿæˆæ–¹æ³•', 'desc': 'æœ¬æ–‡è§£å†³äº†ä»å¤šè½®æ‰§è¡Œåé¦ˆä¸­ç”Ÿæˆä»£ç çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•è¦ä¹ˆåœ¨æ²¡æœ‰åé¦ˆçš„æƒ…å†µä¸‹ç”Ÿæˆä»£ç ï¼Œè¦ä¹ˆä½¿ç”¨å¤æ‚çš„å±‚æ¬¡å¼ºåŒ–å­¦ä¹ æ¥ä¼˜åŒ–å¤šè½®å¥–åŠ±ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•è€Œå¯æ‰©å±•çš„æ–¹æ³•muCodeï¼Œä»…ä½¿ç”¨å•æ­¥å¥–åŠ±æ¥è§£å†³å¤šè½®ä»£ç ç”Ÿæˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›åŸºçº¿ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.21318', 'title': 'How far can we go with ImageNet for Text-to-Image generation?', 'url': 'https://huggingface.co/papers/2502.21318', 'abstract': "Recent text-to-image (T2I) generation models have achieved remarkable results by training on billion-scale datasets, following a `bigger is better' paradigm that prioritizes data quantity over quality. We challenge this established paradigm by demonstrating that strategic data augmentation of small, well-curated datasets can match or outperform models trained on massive web-scraped collections. Using only ImageNet enhanced with well-designed text and image augmentations, we achieve a +2 overall score over SD-XL on GenEval and +5 on DPGBench while using just 1/10th the parameters and 1/1000th the training images. Our results suggest that strategic data augmentation, rather than massive datasets, could offer a more sustainable path forward for T2I generation.", 'score': 13, 'issue_id': 2498, 'pub_date': '2025-02-28', 'pub_date_card': {'ru': '28 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 28', 'zh': '2æœˆ28æ—¥'}, 'hash': '73a20b698d827c0d', 'authors': ['L. Degeorge', 'A. Ghosh', 'N. Dufour', 'D. Picard', 'V. Kalogeiton'], 'affiliations': ['AMIAD, Pole recherche', 'LIGM, Ecole Nationale des Ponts et Chaussees, IP Paris, Univ Gustave Eiffel, CNRS, France', 'LIX, Ecole Polytechnique, CNRS, IP Paris, France'], 'pdf_title_img': 'assets/pdf/title_img/2502.21318.jpg', 'data': {'categories': ['#synthetic', '#optimization', '#training', '#data', '#cv', '#dataset'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'ĞšĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ±ĞµĞ¶Ğ´Ğ°ĞµÑ‚ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ…, Ğ½Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ImageNet Ñ Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸, Ğ¸Ğ¼ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾Ğ¹Ñ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ¾Ğ³Ñ€Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ğ²ĞµĞ±-Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ˜Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ SD-XL Ğ½Ğ° 2 Ğ±Ğ°Ğ»Ğ»Ğ° Ğ² GenEval Ğ¸ Ğ½Ğ° 5 Ğ±Ğ°Ğ»Ğ»Ğ¾Ğ² Ğ² DPGBench, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²ÑĞµĞ³Ğ¾ 1/10 Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¸ 1/1000 Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ° Ñ‚Ğ¾, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ñ‹Ğ¼ Ğ¿ÑƒÑ‚ĞµĞ¼ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ñ‡ĞµĞ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ°ÑÑĞ¸Ğ²Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ².'}, 'en': {'title': 'Quality Over Quantity: Augmenting Small Datasets for Better T2I Models', 'desc': 'This paper challenges the common belief that larger datasets always lead to better text-to-image (T2I) generation models. It shows that by using strategic data augmentation on smaller, high-quality datasets, we can achieve results that are as good as or better than those from models trained on much larger datasets. Specifically, the authors enhanced ImageNet with carefully designed text and image augmentations, leading to significant performance improvements. Their findings suggest that focusing on data quality and augmentation may be a more efficient and sustainable approach for developing T2I models.'}, 'zh': {'title': 'æˆ˜ç•¥æ•°æ®å¢å¼ºï¼šå°æ•°æ®é›†çš„åŠ›é‡', 'desc': 'æœ€è¿‘çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹é€šè¿‡åœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šè®­ç»ƒå–å¾—äº†æ˜¾è‘—æˆæœï¼Œéµå¾ªäº†â€œè¶Šå¤§è¶Šå¥½â€çš„èŒƒå¼ï¼Œä¼˜å…ˆè€ƒè™‘æ•°æ®çš„æ•°é‡è€Œéè´¨é‡ã€‚æˆ‘ä»¬æŒ‘æˆ˜äº†è¿™ä¸€æ—¢å®šèŒƒå¼ï¼Œå±•ç¤ºäº†é€šè¿‡å¯¹å°å‹ã€ç²¾å¿ƒç­–åˆ’çš„æ•°æ®é›†è¿›è¡Œæˆ˜ç•¥æ€§æ•°æ®å¢å¼ºï¼Œå¯ä»¥ä¸åœ¨å¤§è§„æ¨¡ç½‘ç»œæŠ“å–é›†åˆä¸Šè®­ç»ƒçš„æ¨¡å‹ç›¸åŒ¹æ•Œæˆ–è¶…è¶Šã€‚ä»…ä½¿ç”¨ç»è¿‡ç²¾å¿ƒè®¾è®¡çš„æ–‡æœ¬å’Œå›¾åƒå¢å¼ºçš„ImageNetï¼Œæˆ‘ä»¬åœ¨GenEvalä¸Šæ¯”SD-XLé«˜å‡º2åˆ†ï¼Œåœ¨DPGBenchä¸Šé«˜å‡º5åˆ†ï¼ŒåŒæ—¶åªä½¿ç”¨äº†1/10çš„å‚æ•°å’Œ1/1000çš„è®­ç»ƒå›¾åƒã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œæˆ˜ç•¥æ€§æ•°æ®å¢å¼ºè€Œéå¤§è§„æ¨¡æ•°æ®é›†ï¼Œå¯èƒ½ä¸ºæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæä¾›æ›´å¯æŒç»­çš„å‘å±•è·¯å¾„ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.18017', 'title': 'ViDoRAG: Visual Document Retrieval-Augmented Generation via Dynamic Iterative Reasoning Agents', 'url': 'https://huggingface.co/papers/2502.18017', 'abstract': "Understanding information from visually rich documents remains a significant challenge for traditional Retrieval-Augmented Generation (RAG) methods. Existing benchmarks predominantly focus on image-based question answering (QA), overlooking the fundamental challenges of efficient retrieval, comprehension, and reasoning within dense visual documents. To bridge this gap, we introduce ViDoSeek, a novel dataset designed to evaluate RAG performance on visually rich documents requiring complex reasoning. Based on it, we identify key limitations in current RAG approaches: (i) purely visual retrieval methods struggle to effectively integrate both textual and visual features, and (ii) previous approaches often allocate insufficient reasoning tokens, limiting their effectiveness. To address these challenges, we propose ViDoRAG, a novel multi-agent RAG framework tailored for complex reasoning across visual documents. ViDoRAG employs a Gaussian Mixture Model (GMM)-based hybrid strategy to effectively handle multi-modal retrieval. To further elicit the model's reasoning capabilities, we introduce an iterative agent workflow incorporating exploration, summarization, and reflection, providing a framework for investigating test-time scaling in RAG domains. Extensive experiments on ViDoSeek validate the effectiveness and generalization of our approach. Notably, ViDoRAG outperforms existing methods by over 10% on the competitive ViDoSeek benchmark.", 'score': 9, 'issue_id': 2487, 'pub_date': '2025-02-25', 'pub_date_card': {'ru': '25 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 25', 'zh': '2æœˆ25æ—¥'}, 'hash': '4202273d8c895c2a', 'authors': ['Qiuchen Wang', 'Ruixue Ding', 'Zehui Chen', 'Weiqi Wu', 'Shihang Wang', 'Pengjun Xie', 'Feng Zhao'], 'affiliations': ['MoE Key Laboratory of Brain-inspired Intelligent Perception and Cognition, USTC', 'Shanghai Jiao Tong University', 'Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2502.18017.jpg', 'data': {'categories': ['#reasoning', '#agents', '#rag', '#games', '#benchmark', '#multimodal', '#dataset'], 'emoji': 'ğŸ”', 'ru': {'title': 'ViDoRAG: ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ· Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ViDoSeek Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ (RAG) Ğ½Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾ Ğ½Ğ°ÑÑ‹Ñ‰ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ»ÑÑÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ² RAG Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° ViDoRAG, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹Ñ… ÑĞ¼ĞµÑĞµĞ¹ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°. ViDoRAG Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ğ½Ğ° 10% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ ViDoSeek.'}, 'en': {'title': 'Enhancing RAG for Complex Visual Reasoning with ViDoRAG', 'desc': 'This paper addresses the challenges of understanding complex information in visually rich documents using Retrieval-Augmented Generation (RAG) methods. It introduces ViDoSeek, a new dataset that tests RAG performance on documents that require advanced reasoning skills. The authors highlight limitations in current RAG techniques, such as difficulties in integrating visual and textual data and inadequate reasoning capabilities. To overcome these issues, they propose ViDoRAG, a multi-agent framework that enhances retrieval and reasoning through a hybrid strategy and an iterative workflow, demonstrating significant improvements in performance on the ViDoSeek benchmark.'}, 'zh': {'title': 'æå‡è§†è§‰æ–‡æ¡£ç†è§£çš„RAGæ–°æ¡†æ¶', 'desc': 'ç†è§£è§†è§‰ä¸°å¯Œæ–‡æ¡£ä¸­çš„ä¿¡æ¯å¯¹ä¼ ç»Ÿçš„å¢å¼ºæ£€ç´¢ç”Ÿæˆï¼ˆRAGï¼‰æ–¹æ³•æ¥è¯´ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ã€‚ç°æœ‰çš„åŸºå‡†ä¸»è¦é›†ä¸­åœ¨åŸºäºå›¾åƒçš„é—®é¢˜å›ç­”ï¼ˆQAï¼‰ï¼Œè€Œå¿½è§†äº†åœ¨å¯†é›†è§†è§‰æ–‡æ¡£ä¸­é«˜æ•ˆæ£€ç´¢ã€ç†è§£å’Œæ¨ç†çš„åŸºæœ¬æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ViDoSeekï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„æ•°æ®é›†ï¼Œæ—¨åœ¨è¯„ä¼°RAGåœ¨éœ€è¦å¤æ‚æ¨ç†çš„è§†è§‰ä¸°å¯Œæ–‡æ¡£ä¸Šçš„è¡¨ç°ã€‚æˆ‘ä»¬æå‡ºçš„ViDoRAGæ¡†æ¶é‡‡ç”¨æ··åˆç­–ç•¥ï¼Œç»“åˆå¤šæ¨¡æ€æ£€ç´¢å’Œè¿­ä»£ä»£ç†å·¥ä½œæµï¼Œä»¥æé«˜æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œå¹¶åœ¨ViDoSeekåŸºå‡†ä¸Šæ˜¾è‘—è¶…è¶Šç°æœ‰æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.20545', 'title': 'SoS1: O1 and R1-Like Reasoning LLMs are Sum-of-Square Solvers', 'url': 'https://huggingface.co/papers/2502.20545', 'abstract': "Large Language Models (LLMs) have achieved human-level proficiency across diverse tasks, but their ability to perform rigorous mathematical problem solving remains an open challenge. In this work, we investigate a fundamental yet computationally intractable problem: determining whether a given multivariate polynomial is nonnegative. This problem, closely related to Hilbert's Seventeenth Problem, plays a crucial role in global polynomial optimization and has applications in various fields. First, we introduce SoS-1K, a meticulously curated dataset of approximately 1,000 polynomials, along with expert-designed reasoning instructions based on five progressively challenging criteria. Evaluating multiple state-of-the-art LLMs, we find that without structured guidance, all models perform only slightly above the random guess baseline 50%. However, high-quality reasoning instructions significantly improve accuracy, boosting performance up to 81%. Furthermore, our 7B model, SoS-7B, fine-tuned on SoS-1K for just 4 hours, outperforms the 671B DeepSeek-V3 and GPT-4o-mini in accuracy while only requiring 1.8% and 5% of the computation time needed for letters, respectively. Our findings highlight the potential of LLMs to push the boundaries of mathematical reasoning and tackle NP-hard problems.", 'score': 9, 'issue_id': 2486, 'pub_date': '2025-02-27', 'pub_date_card': {'ru': '27 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 27', 'zh': '2æœˆ27æ—¥'}, 'hash': 'fb32f9423103ece9', 'authors': ['Kechen Li', 'Wenqi Zhu', 'Coralia Cartis', 'Tianbo Ji', 'Shiwei Liu'], 'affiliations': ['Mathematical Institute, University of Oxford', 'Nanjing University of Aeronautics and Astronautics', 'School of Transportation and Civil Engineering, Nantong University'], 'pdf_title_img': 'assets/pdf/title_img/2502.20545.jpg', 'data': {'categories': ['#dataset', '#optimization', '#training', '#math', '#reasoning'], 'emoji': 'ğŸ§®', 'ru': {'title': 'Ğ‘Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°ÑÑ‚ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ñ€ĞµÑˆĞ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ² Ñ‡Ğ°ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ½ĞµĞ¾Ñ‚Ñ€Ğ¸Ñ†Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»Ğ¸Ğ½Ğ¾Ğ¼Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ SoS-1K Ğ¸Ğ· 1000 Ğ¿Ğ¾Ğ»Ğ¸Ğ½Ğ¾Ğ¼Ğ¾Ğ² Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ±ĞµĞ· ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ Ğ½ĞµĞ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ»ÑƒÑ‡ÑˆĞµ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ³Ğ°Ğ´Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ½Ğ¾ Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ÑÑ Ğ´Ğ¾ 81%. ĞœĞ¾Ğ´ĞµĞ»ÑŒ SoS-7B, Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° SoS-1K, Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾ÑˆĞ»Ğ° Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Unlocking Mathematical Reasoning in LLMs with Structured Guidance', 'desc': 'This paper explores the limitations of Large Language Models (LLMs) in solving complex mathematical problems, specifically the challenge of determining if a multivariate polynomial is nonnegative. The authors introduce a new dataset called SoS-1K, which contains around 1,000 polynomials and structured reasoning instructions to guide the models. They demonstrate that LLMs perform poorly without guidance, achieving only slightly above random guessing, but can significantly improve their accuracy with high-quality instructions. Notably, their fine-tuned model, SoS-7B, surpasses larger models in performance while being more computationally efficient, showcasing the potential of LLMs in addressing NP-hard problems.'}, 'zh': {'title': 'æ¨åŠ¨æ•°å­¦æ¨ç†çš„è¾¹ç•Œ', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šç§ä»»åŠ¡ä¸­è¾¾åˆ°äº†äººç±»æ°´å¹³çš„èƒ½åŠ›ï¼Œä½†åœ¨ä¸¥æ ¼çš„æ•°å­¦é—®é¢˜è§£å†³æ–¹é¢ä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬æ–‡ç ”ç©¶äº†ä¸€ä¸ªåŸºæœ¬ä½†è®¡ç®—ä¸Šéš¾ä»¥å¤„ç†çš„é—®é¢˜ï¼šåˆ¤æ–­ç»™å®šçš„å¤šå˜é‡å¤šé¡¹å¼æ˜¯å¦éè´Ÿã€‚æˆ‘ä»¬å¼•å…¥äº†SoS-1Kæ•°æ®é›†ï¼ŒåŒ…å«çº¦1000ä¸ªå¤šé¡¹å¼ï¼Œå¹¶è®¾è®¡äº†åŸºäºäº”ä¸ªé€æ­¥æŒ‘æˆ˜æ ‡å‡†çš„æ¨ç†æŒ‡å¯¼ã€‚å®éªŒè¡¨æ˜ï¼Œç»è¿‡é«˜è´¨é‡çš„æ¨ç†æŒ‡å¯¼åï¼Œæ¨¡å‹çš„å‡†ç¡®ç‡æ˜¾è‘—æé«˜ï¼Œæœ€é«˜å¯è¾¾81%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.20396', 'title': 'Sim-to-Real Reinforcement Learning for Vision-Based Dexterous Manipulation on Humanoids', 'url': 'https://huggingface.co/papers/2502.20396', 'abstract': 'Reinforcement learning has delivered promising results in achieving human- or even superhuman-level capabilities across diverse problem domains, but success in dexterous robot manipulation remains limited. This work investigates the key challenges in applying reinforcement learning to solve a collection of contact-rich manipulation tasks on a humanoid embodiment. We introduce novel techniques to overcome the identified challenges with empirical validation. Our main contributions include an automated real-to-sim tuning module that brings the simulated environment closer to the real world, a generalized reward design scheme that simplifies reward engineering for long-horizon contact-rich manipulation tasks, a divide-and-conquer distillation process that improves the sample efficiency of hard-exploration problems while maintaining sim-to-real performance, and a mixture of sparse and dense object representations to bridge the sim-to-real perception gap. We show promising results on three humanoid dexterous manipulation tasks, with ablation studies on each technique. Our work presents a successful approach to learning humanoid dexterous manipulation using sim-to-real reinforcement learning, achieving robust generalization and high performance without the need for human demonstration.', 'score': 7, 'issue_id': 2486, 'pub_date': '2025-02-27', 'pub_date_card': {'ru': '27 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 27', 'zh': '2æœˆ27æ—¥'}, 'hash': '41439b4f54e02c9b', 'authors': ['Toru Lin', 'Kartik Sachdev', 'Linxi Fan', 'Jitendra Malik', 'Yuke Zhu'], 'affiliations': ['NVIDIA', 'UC Berkeley', 'UT Austin'], 'pdf_title_img': 'assets/pdf/title_img/2502.20396.jpg', 'data': {'categories': ['#robotics', '#rl', '#games', '#optimization'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ›Ğ¾Ğ²ĞºĞ¾ÑÑ‚ÑŒ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°: Ğ¾Ñ‚ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğº Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ñ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸, Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ½ÑƒÑ ÑÑ…ĞµĞ¼Ñƒ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´ Ğ¸ ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒÑĞ¿ĞµÑˆĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ñ‚Ñ€ĞµÑ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ»Ğ¾Ğ²ĞºĞ¾Ğ¹ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸ÑĞ¼ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸ÑÑ… Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°.'}, 'en': {'title': 'Reinforcement Learning for Dexterous Robot Manipulation: Bridging Sim and Real Worlds', 'desc': 'This paper addresses the challenges of using reinforcement learning for complex robot manipulation tasks that involve physical contact. The authors propose innovative methods to enhance the performance of humanoid robots in these tasks, including a system to align simulated environments with real-world conditions. They also introduce a new reward design that simplifies the process of creating effective rewards for long tasks and a technique to improve learning efficiency in difficult scenarios. The results demonstrate that their approach allows robots to learn dexterous manipulation effectively, achieving high performance without requiring human guidance.'}, 'zh': {'title': 'çªç ´ç±»äººæœºå™¨äººçµå·§æ“ä½œçš„å¼ºåŒ–å­¦ä¹ æŒ‘æˆ˜', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†åœ¨ç±»äººæœºå™¨äººçµå·§æ“ä½œä»»åŠ¡ä¸­åº”ç”¨å¼ºåŒ–å­¦ä¹ çš„å…³é”®æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†æ–°æŠ€æœ¯æ¥å…‹æœè¿™äº›æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬è‡ªåŠ¨åŒ–çš„çœŸå®åˆ°æ¨¡æ‹Ÿè°ƒä¼˜æ¨¡å—ï¼Œä»¥ç¼©å°æ¨¡æ‹Ÿç¯å¢ƒä¸ç°å®ä¸–ç•Œçš„å·®è·ã€‚æˆ‘ä»¬è¿˜è®¾è®¡äº†ä¸€ç§é€šç”¨çš„å¥–åŠ±æœºåˆ¶ï¼Œç®€åŒ–äº†é•¿æ—¶é—´æ¥è§¦ä¸°å¯Œçš„æ“ä½œä»»åŠ¡çš„å¥–åŠ±å·¥ç¨‹ã€‚é€šè¿‡å¯¹ä¸‰é¡¹ç±»äººçµå·§æ“ä½œä»»åŠ¡çš„å®éªŒï¼Œæˆ‘ä»¬å±•ç¤ºäº†åœ¨ä¸éœ€è¦äººç±»ç¤ºèŒƒçš„æƒ…å†µä¸‹ï¼Œä½¿ç”¨æ¨¡æ‹Ÿåˆ°çœŸå®çš„å¼ºåŒ–å­¦ä¹ å®ç°äº†ç¨³å¥çš„æ³›åŒ–å’Œé«˜æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.19577', 'title': 'Tell me why: Visual foundation models as self-explainable classifiers', 'url': 'https://huggingface.co/papers/2502.19577', 'abstract': 'Visual foundation models (VFMs) have become increasingly popular due to their state-of-the-art performance. However, interpretability remains crucial for critical applications. In this sense, self-explainable models (SEM) aim to provide interpretable classifiers that decompose predictions into a weighted sum of interpretable concepts. Despite their promise, recent studies have shown that these explanations often lack faithfulness. In this work, we combine VFMs with a novel prototypical architecture and specialized training objectives. By training only a lightweight head (approximately 1M parameters) on top of frozen VFMs, our approach (ProtoFM) offers an efficient and interpretable solution. Evaluations demonstrate that our approach achieves competitive classification performance while outperforming existing models across a range of interpretability metrics derived from the literature. Code is available at https://github.com/hturbe/proto-fm.', 'score': 6, 'issue_id': 2493, 'pub_date': '2025-02-26', 'pub_date_card': {'ru': '26 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 26', 'zh': '2æœˆ26æ—¥'}, 'hash': '7d2bd5235959eba5', 'authors': ['Hugues TurbÃ©', 'Mina Bjelogrlic', 'Gianmarco Mengaldo', 'Christian Lovis'], 'affiliations': ['Department of Mechanical Engineering, College of Design and Engineering, National University of Singapore, Singapore', 'Department of Radiology and Medical Informatics, University of Geneva, Geneva, Switzerland', 'Division of Medical Information Sciences, Geneva University Hospitals, Geneva, Switzerland'], 'pdf_title_img': 'assets/pdf/title_img/2502.19577.jpg', 'data': {'categories': ['#cv', '#small_models', '#architecture', '#interpretability', '#open_source', '#training', '#optimization'], 'emoji': 'ğŸ”', 'ru': {'title': 'ProtoFM: Ğ˜Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹-Ğ¾ÑĞ½Ğ¾Ğ² (VFM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ VFM Ñ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾Ñ‚Ğ¸Ğ¿Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ¾Ğ¹ Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ†ĞµĞ»ÑĞ¼Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½ÑƒÑ ProtoFM. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½ÑƒÑ Ğ²ĞµÑ€Ñ…ÑƒÑˆĞºÑƒ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ… Ğ·Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ñ… VFM, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ProtoFM Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ Ñ€ÑĞ´Ñƒ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'ProtoFM: Interpretable and Efficient Visual Foundation Models', 'desc': 'This paper introduces ProtoFM, a new approach that combines visual foundation models (VFMs) with a prototypical architecture to enhance interpretability in machine learning. The method focuses on creating self-explainable models (SEMs) that break down predictions into understandable components, addressing the issue of faithfulness in explanations. By training a lightweight head on top of frozen VFMs, ProtoFM maintains high classification performance while improving interpretability metrics. The results show that ProtoFM outperforms existing models, making it a promising solution for applications requiring both accuracy and clarity.'}, 'zh': {'title': 'é«˜æ•ˆå¯è§£é‡Šçš„è§†è§‰åŸºç¡€æ¨¡å‹', 'desc': 'è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆVFMï¼‰å› å…¶å“è¶Šçš„æ€§èƒ½è€Œå—åˆ°å¹¿æ³›å…³æ³¨ï¼Œä½†åœ¨å…³é”®åº”ç”¨ä¸­å¯è§£é‡Šæ€§ä»ç„¶è‡³å…³é‡è¦ã€‚è‡ªè§£é‡Šæ¨¡å‹ï¼ˆSEMï¼‰æ—¨åœ¨æä¾›å¯è§£é‡Šçš„åˆ†ç±»å™¨ï¼Œå°†é¢„æµ‹åˆ†è§£ä¸ºå¯è§£é‡Šæ¦‚å¿µçš„åŠ æƒå’Œã€‚å°½ç®¡æœ‰æ½œåŠ›ï¼Œè¿‘æœŸç ”ç©¶è¡¨æ˜è¿™äº›è§£é‡Šå¾€å¾€ç¼ºä¹å¯ä¿¡åº¦ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»“åˆVFMå’Œæ–°å‹åŸå‹æ¶æ„çš„æ–¹æ¡ˆï¼ˆProtoFMï¼‰ï¼Œé€šè¿‡åœ¨å†»ç»“çš„VFMä¸Šè®­ç»ƒä¸€ä¸ªè½»é‡çº§çš„å¤´éƒ¨æ¨¡å‹ï¼Œæä¾›äº†ä¸€ç§é«˜æ•ˆä¸”å¯è§£é‡Šçš„è§£å†³æ–¹æ¡ˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.20969', 'title': 'TeleRAG: Efficient Retrieval-Augmented Generation Inference with Lookahead Retrieval', 'url': 'https://huggingface.co/papers/2502.20969', 'abstract': 'Retrieval-augmented generation (RAG) extends large language models (LLMs) with external data sources to enhance factual correctness and domain coverage. Modern RAG pipelines rely on large datastores, leading to system challenges in latency-sensitive deployments, especially when limited GPU memory is available. To address these challenges, we propose TeleRAG, an efficient inference system that reduces RAG latency with minimal GPU memory requirements. The core innovation of TeleRAG is lookahead retrieval, a prefetching mechanism that anticipates required data and transfers it from CPU to GPU in parallel with LLM generation. By leveraging the modularity of RAG pipelines, the inverted file index (IVF) search algorithm and similarities between queries, TeleRAG optimally overlaps data movement and computation. Experimental results show that TeleRAG reduces end-to-end RAG inference latency by up to 1.72x on average compared to state-of-the-art systems, enabling faster, more memory-efficient deployments of advanced RAG applications.', 'score': 5, 'issue_id': 2498, 'pub_date': '2025-02-28', 'pub_date_card': {'ru': '28 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 28', 'zh': '2æœˆ28æ—¥'}, 'hash': '5a85f59eed1c3c3b', 'authors': ['Chien-Yu Lin', 'Keisuke Kamahori', 'Yiyu Liu', 'Xiaoxiang Shi', 'Madhav Kashyap', 'Yile Gu', 'Rulin Shao', 'Zihao Ye', 'Kan Zhu', 'Stephanie Wang', 'Arvind Krishnamurthy', 'Rohan Kadekodi', 'Luis Ceze', 'Baris Kasikci'], 'affiliations': ['Shanghai Jiao Tong University', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2502.20969.jpg', 'data': {'categories': ['#rag', '#inference', '#optimization'], 'emoji': 'ğŸš€', 'ru': {'title': 'TeleRAG: Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ RAG Ğ±ĞµĞ· ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑĞ¾Ğ² Ğ¿Ğ¾ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸', 'desc': 'TeleRAG - ÑÑ‚Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºÑƒ RAG Ğ¿Ñ€Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑÑ… Ğº Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ GPU. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğµ Ğ½Ğ¾Ğ²Ğ¾Ğ²Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ TeleRAG - ÑÑ‚Ğ¾ Ğ¾Ğ¿ĞµÑ€ĞµĞ¶Ğ°ÑÑ‰ĞµĞµ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ, Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ´Ğ²Ğ¸Ğ´Ğ¸Ñ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸ Ğ¿ĞµÑ€ĞµĞ´Ğ°ĞµÑ‚ Ğ¸Ñ… Ñ CPU Ğ½Ğ° GPU Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾ Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ LLM. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ RAG-ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ğ¾Ğ², Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ½Ğ²ĞµÑ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ğ´ĞµĞºÑĞ° (IVF) Ğ¸ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ²Ğ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ¿ĞµÑ€ĞµĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ TeleRAG ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºÑƒ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° RAG Ğ² ÑÑ€ĞµĞ´Ğ½ĞµĞ¼ Ğ´Ğ¾ 1,72 Ñ€Ğ°Ğ·Ğ° Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Speeding Up RAG with TeleRAG: Faster and Smarter Inference!', 'desc': 'This paper introduces TeleRAG, a new system designed to improve the efficiency of retrieval-augmented generation (RAG) in large language models (LLMs). TeleRAG addresses the latency issues that arise from using large datastores, particularly in environments with limited GPU memory. The key feature of TeleRAG is its lookahead retrieval mechanism, which allows data to be pre-fetched from the CPU to the GPU while the LLM is generating responses. Experimental results demonstrate that TeleRAG can significantly reduce inference latency, making RAG applications faster and more efficient.'}, 'zh': {'title': 'TeleRAGï¼šé«˜æ•ˆçš„RAGæ¨ç†ç³»ç»Ÿ', 'desc': 'æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰é€šè¿‡å¤–éƒ¨æ•°æ®æºæ‰©å±•å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œä»¥æé«˜äº‹å®å‡†ç¡®æ€§å’Œé¢†åŸŸè¦†ç›–ç‡ã€‚ç„¶è€Œï¼Œç°ä»£RAGç®¡é“ä¾èµ–äºå¤§å‹æ•°æ®å­˜å‚¨ï¼Œå¯¼è‡´åœ¨å»¶è¿Ÿæ•æ„Ÿçš„éƒ¨ç½²ä¸­é¢ä¸´ç³»ç»ŸæŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨GPUå†…å­˜æœ‰é™çš„æƒ…å†µä¸‹ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†TeleRAGï¼Œè¿™æ˜¯ä¸€ç§é«˜æ•ˆçš„æ¨ç†ç³»ç»Ÿï¼Œèƒ½å¤Ÿåœ¨æœ€å°çš„GPUå†…å­˜éœ€æ±‚ä¸‹å‡å°‘RAGå»¶è¿Ÿã€‚TeleRAGçš„æ ¸å¿ƒåˆ›æ–°æ˜¯å‰ç»æ€§æ£€ç´¢ï¼Œè¿™æ˜¯ä¸€ç§é¢„å–æœºåˆ¶ï¼Œå¯ä»¥åœ¨LLMç”Ÿæˆçš„åŒæ—¶ï¼Œé¢„æµ‹æ‰€éœ€æ•°æ®å¹¶å°†å…¶ä»CPUå¹¶è¡Œä¼ è¾“åˆ°GPUã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.17941', 'title': 'Optimal Brain Apoptosis', 'url': 'https://huggingface.co/papers/2502.17941', 'abstract': 'The increasing complexity and parameter count of Convolutional Neural Networks (CNNs) and Transformers pose challenges in terms of computational efficiency and resource demands. Pruning has been identified as an effective strategy to address these challenges by removing redundant elements such as neurons, channels, or connections, thereby enhancing computational efficiency without heavily compromising performance. This paper builds on the foundational work of Optimal Brain Damage (OBD) by advancing the methodology of parameter importance estimation using the Hessian matrix. Unlike previous approaches that rely on approximations, we introduce Optimal Brain Apoptosis (OBA), a novel pruning method that calculates the Hessian-vector product value directly for each parameter. By decomposing the Hessian matrix across network layers and identifying conditions under which inter-layer Hessian submatrices are non-zero, we propose a highly efficient technique for computing the second-order Taylor expansion of parameters. This approach allows for a more precise pruning process, particularly in the context of CNNs and Transformers, as validated in our experiments including VGG19, ResNet32, ResNet50, and ViT-B/16 on CIFAR10, CIFAR100 and Imagenet datasets. Our code is available at https://github.com/NEU-REAL/OBA.', 'score': 5, 'issue_id': 2495, 'pub_date': '2025-02-25', 'pub_date_card': {'ru': '25 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 25', 'zh': '2æœˆ25æ—¥'}, 'hash': '3748780b9de1393e', 'authors': ['Mingyuan Sun', 'Zheng Fang', 'Jiaxu Wang', 'Junjie Jiang', 'Delei Kong', 'Chenming Hu', 'Yuetong Fang', 'Renjing Xu'], 'affiliations': ['Hunan University', 'Northeastern University', 'The Hong Kong University of Science and Technology (Guangzhou)'], 'pdf_title_img': 'assets/pdf/title_img/2502.17941.jpg', 'data': {'categories': ['#optimization', '#inference', '#training', '#architecture'], 'emoji': 'âœ‚ï¸', 'ru': {'title': 'Ğ¢Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ¾Ğ±Ñ€ĞµĞ·ĞºĞ° Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑ‡ĞµÑ‚Ğ° Ğ“ĞµÑÑĞ¸Ğ°Ğ½Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ÑƒĞ½Ğ¸Ğ½Ğ³Ğ° Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Optimal Brain Apoptosis (OBA). OBA Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ¹ Ñ€Ğ°ÑÑ‡ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ“ĞµÑÑĞ¸Ğ°Ğ½Ğ° Ğ½Ğ° Ğ²ĞµĞºÑ‚Ğ¾Ñ€ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğº ÑĞ²ĞµÑ€Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğ¼ ÑĞµÑ‚ÑĞ¼ Ğ¸ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°Ğ¼, Ñ€Ğ°Ğ·Ğ»Ğ°Ğ³Ğ°Ñ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ñƒ Ğ“ĞµÑÑĞ¸Ğ°Ğ½Ğ° Ğ¿Ğ¾ ÑĞ»Ğ¾ÑĞ¼ ÑĞµÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°Ñ… Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ… Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°.'}, 'en': {'title': 'Efficient Neural Network Pruning with Optimal Brain Apoptosis', 'desc': 'This paper addresses the challenges of high computational demands in Convolutional Neural Networks (CNNs) and Transformers by introducing a new pruning method called Optimal Brain Apoptosis (OBA). OBA improves upon previous methods by directly calculating the Hessian-vector product for each parameter, allowing for more accurate estimation of parameter importance. The authors decompose the Hessian matrix across layers to identify non-zero conditions, which enhances the efficiency of the pruning process. Experimental results demonstrate the effectiveness of OBA on various architectures and datasets, confirming its potential to optimize neural networks without significant performance loss.'}, 'zh': {'title': 'é«˜æ•ˆå‰ªæï¼šæœ€ä¼˜è„‘å‡‹äº¡æ–¹æ³•', 'desc': 'éšç€å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’Œå˜æ¢å™¨ï¼ˆTransformersï¼‰æ¨¡å‹çš„å¤æ‚æ€§å’Œå‚æ•°æ•°é‡çš„å¢åŠ ï¼Œè®¡ç®—æ•ˆç‡å’Œèµ„æºéœ€æ±‚é¢ä¸´æŒ‘æˆ˜ã€‚å‰ªæè¢«è®¤ä¸ºæ˜¯ä¸€ç§æœ‰æ•ˆçš„ç­–ç•¥ï¼Œé€šè¿‡å»é™¤å†—ä½™å…ƒç´ ï¼ˆå¦‚ç¥ç»å…ƒã€é€šé“æˆ–è¿æ¥ï¼‰æ¥æé«˜è®¡ç®—æ•ˆç‡ï¼Œè€Œä¸ä¼šä¸¥é‡å½±å“æ€§èƒ½ã€‚æœ¬æ–‡åœ¨æœ€ä¼˜è„‘æŸä¼¤ï¼ˆOBDï¼‰çš„åŸºç¡€ä¸Šï¼Œæå‡ºäº†ä¸€ç§æ–°çš„å‰ªææ–¹æ³•â€”â€”æœ€ä¼˜è„‘å‡‹äº¡ï¼ˆOBAï¼‰ï¼Œé€šè¿‡ç›´æ¥è®¡ç®—æ¯ä¸ªå‚æ•°çš„Hessian-å‘é‡ä¹˜ç§¯å€¼æ¥ä¼°è®¡å‚æ•°çš„é‡è¦æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†éªŒè¯ï¼ŒåŒ…æ‹¬VGG19ã€ResNet32ã€ResNet50å’ŒViT-B/16ï¼Œå±•ç¤ºäº†åœ¨CNNå’ŒTransformersä¸­çš„é«˜æ•ˆå‰ªæè¿‡ç¨‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.19731', 'title': "Preference Learning Unlocks LLMs' Psycho-Counseling Skills", 'url': 'https://huggingface.co/papers/2502.19731', 'abstract': "Applying large language models (LLMs) to assist in psycho-counseling is an emerging and meaningful approach, driven by the significant gap between patient needs and the availability of mental health support. However, current LLMs struggle to consistently provide effective responses to client speeches, largely due to the lack of supervision from high-quality real psycho-counseling data, whose content is typically inaccessible due to client privacy concerns. Furthermore, the quality of therapists' responses in available sessions can vary significantly based on their professional training and experience. Assessing the quality of therapists' responses remains an open challenge. In this work, we address these challenges by first proposing a set of professional and comprehensive principles to evaluate therapists' responses to client speeches. Using these principles, we create a preference dataset, PsychoCounsel-Preference, which contains 36k high-quality preference comparison pairs. This dataset aligns with the preferences of professional psychotherapists, providing a robust foundation for evaluating and improving LLMs in psycho-counseling. Experiments on reward modeling and preference learning demonstrate that PsychoCounsel-Preference is an excellent resource for LLMs to acquire essential skills for responding to clients in a counseling session. Our best-aligned model, PsychoCounsel-Llama3-8B, achieves an impressive win rate of 87% against GPT-4o. We release PsychoCounsel-Preference, PsychoCounsel-Llama3-8B and the reward model PsychoCounsel Llama3-8B-Reward to facilitate the research of psycho-counseling with LLMs at: https://hf.co/Psychotherapy-LLM.", 'score': 4, 'issue_id': 2500, 'pub_date': '2025-02-27', 'pub_date_card': {'ru': '27 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 27', 'zh': '2æœˆ27æ—¥'}, 'hash': '533e7e87242a9b9e', 'authors': ['Mian Zhang', 'Shaun M. Eack', 'Zhiyu Zoey Chen'], 'affiliations': ['Department of Computer Science, University of Texas at Dallas', 'School of Social Work, University of Pittsburgh'], 'pdf_title_img': 'assets/pdf/title_img/2502.19731.jpg', 'data': {'categories': ['#open_source', '#alignment', '#healthcare', '#data', '#dataset'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ‘Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° ÑÑ‚Ñ€Ğ°Ğ¶Ğµ Ğ¿ÑĞ¸Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ·Ğ´Ğ¾Ñ€Ğ¾Ğ²ÑŒÑ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ğ¿ÑĞ¸Ñ…Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ ĞºĞ¾Ğ½ÑÑƒĞ»ÑŒÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ñ‚ĞµÑ€Ğ°Ğ¿ĞµĞ²Ñ‚Ğ¾Ğ² Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ PsychoCounsel-Preference, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 36 Ñ‚Ñ‹ÑÑÑ‡ Ğ¿Ğ°Ñ€ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¹ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ±Ñ‹Ğ»Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ PsychoCounsel-Llama3-8B, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ²Ğ¿ĞµÑ‡Ğ°Ñ‚Ğ»ÑÑÑ‰Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¸ Ñ GPT-4. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¾ Ğ½Ğ° Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ²Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ¿Ğ°Ñ†Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¿ÑĞ¸Ñ…Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ¸.'}, 'en': {'title': 'Enhancing Psycho-Counseling with LLMs: A New Standard for Therapist Response Evaluation', 'desc': "This paper explores the use of large language models (LLMs) in psycho-counseling, addressing the gap between patient needs and available mental health support. It highlights the challenges faced by LLMs in generating effective responses due to a lack of high-quality training data and variability in therapist responses. To tackle these issues, the authors propose evaluation principles for therapist responses and create a dataset called PsychoCounsel-Preference, which includes 36,000 preference comparison pairs aligned with professional psychotherapists' judgments. The results show that their model, PsychoCounsel-Llama3-8B, significantly outperforms existing models, achieving an 87% win rate against GPT-4o, thus demonstrating the potential of LLMs in enhancing psycho-counseling practices."}, 'zh': {'title': 'æå‡å¿ƒç†å’¨è¯¢çš„è¯­è¨€æ¨¡å‹åº”ç”¨', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¿ƒç†å’¨è¯¢ä¸­çš„åº”ç”¨ï¼Œæ—¨åœ¨å¡«è¡¥æ‚£è€…éœ€æ±‚ä¸å¿ƒç†å¥åº·æ”¯æŒä¹‹é—´çš„å·®è·ã€‚ç”±äºç¼ºä¹é«˜è´¨é‡çš„çœŸå®å¿ƒç†å’¨è¯¢æ•°æ®ï¼Œå½“å‰çš„LLMsåœ¨å›åº”å®¢æˆ·å‘è¨€æ—¶æ•ˆæœä¸ä½³ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€å¥—ä¸“ä¸šçš„è¯„ä¼°åŸåˆ™ï¼Œå¹¶åˆ›å»ºäº†åŒ…å«36,000ä¸ªé«˜è´¨é‡åå¥½æ¯”è¾ƒå¯¹çš„PsychoCounsel-Preferenceæ•°æ®é›†ï¼Œä»¥å¸®åŠ©è¯„ä¼°å’Œæ”¹è¿›LLMsåœ¨å¿ƒç†å’¨è¯¢ä¸­çš„è¡¨ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPsychoCounsel-Preferenceä¸ºLLMsæä¾›äº†å¿…è¦çš„æŠ€èƒ½åŸºç¡€ï¼Œä½¿å…¶åœ¨å’¨è¯¢ä¼šè¯ä¸­æ›´æœ‰æ•ˆåœ°å›åº”å®¢æˆ·ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.20583', 'title': 'LiteASR: Efficient Automatic Speech Recognition with Low-Rank Approximation', 'url': 'https://huggingface.co/papers/2502.20583', 'abstract': "Modern automatic speech recognition (ASR) models, such as OpenAI's Whisper, rely on deep encoder-decoder architectures, and their encoders are a critical bottleneck for efficient deployment due to high computational intensity. We introduce LiteASR, a low-rank compression scheme for ASR encoders that significantly reduces inference costs while maintaining transcription accuracy. Our approach leverages the strong low-rank properties observed in intermediate activations: by applying principal component analysis (PCA) with a small calibration dataset, we approximate linear transformations with a chain of low-rank matrix multiplications, and further optimize self-attention to work in the reduced dimension. Evaluation results show that our method can compress Whisper large-v3's encoder size by over 50%, matching Whisper medium's size with better transcription accuracy, thereby establishing a new Pareto-optimal frontier of efficiency and performance. The code of LiteASR is available at https://github.com/efeslab/LiteASR.", 'score': 4, 'issue_id': 2486, 'pub_date': '2025-02-27', 'pub_date_card': {'ru': '27 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 27', 'zh': '2æœˆ27æ—¥'}, 'hash': '3c7268c0881fa426', 'authors': ['Keisuke Kamahori', 'Jungo Kasai', 'Noriyuki Kojima', 'Baris Kasikci'], 'affiliations': ['Kotoba Technologies Inc.', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2502.20583.jpg', 'data': {'categories': ['#inference', '#optimization', '#audio'], 'emoji': 'ğŸ™ï¸', 'ru': {'title': 'LiteASR: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ² ASR Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸', 'desc': 'LiteASR - ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ² Ğ´Ğ»Ñ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸ (ASR), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‚Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ¿Ñ†Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ³Ğ»Ğ°Ğ²Ğ½Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚ (PCA) Ğ´Ğ»Ñ Ğ°Ğ¿Ğ¿Ñ€Ğ¾ĞºÑĞ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¾Ğ¹ ÑƒĞ¼Ğ½Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ† Ğ½Ğ¸Ğ·ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ½Ğ³Ğ°. LiteASR Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¶Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Whisper large-v3 Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ğ½Ğ° 50%, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Whisper medium Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ñ‚Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ¿Ñ†Ğ¸Ğ¸. Ğ­Ñ‚Ğ¾ ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ ĞŸĞ°Ñ€ĞµÑ‚Ğ¾-Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ñƒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ASR.'}, 'en': {'title': 'LiteASR: Efficient ASR with Low-Rank Compression', 'desc': "This paper presents LiteASR, a novel low-rank compression technique designed to enhance the efficiency of automatic speech recognition (ASR) models, particularly focusing on the encoder component. By utilizing principal component analysis (PCA) on intermediate activations, LiteASR reduces the computational load during inference while preserving transcription accuracy. The method achieves over 50% reduction in the encoder size of OpenAI's Whisper large-v3 model, aligning its performance with that of the medium version but with improved accuracy. This work sets a new standard for balancing efficiency and performance in ASR systems."}, 'zh': {'title': 'LiteASRï¼šé«˜æ•ˆçš„ä½ç§©å‹ç¼©æ–¹æ¡ˆ', 'desc': 'ç°ä»£è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹ï¼Œå¦‚OpenAIçš„Whisperï¼Œä¾èµ–äºæ·±åº¦ç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼Œè€Œç¼–ç å™¨çš„è®¡ç®—å¼ºåº¦æ˜¯é«˜æ•ˆéƒ¨ç½²çš„ç“¶é¢ˆã€‚æˆ‘ä»¬æå‡ºäº†LiteASRï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹ASRç¼–ç å™¨çš„ä½ç§©å‹ç¼©æ–¹æ¡ˆï¼Œèƒ½å¤Ÿæ˜¾è‘—é™ä½æ¨ç†æˆæœ¬ï¼ŒåŒæ—¶ä¿æŒè½¬å½•å‡†ç¡®æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨äº†ä¸­é—´æ¿€æ´»ä¸­çš„å¼ºä½ç§©ç‰¹æ€§ï¼Œé€šè¿‡ä½¿ç”¨å°å‹æ ¡å‡†æ•°æ®é›†çš„ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰ï¼Œç”¨ä½ç§©çŸ©é˜µä¹˜æ³•é“¾æ¥è¿‘ä¼¼çº¿æ€§å˜æ¢ï¼Œå¹¶è¿›ä¸€æ­¥ä¼˜åŒ–è‡ªæ³¨æ„åŠ›æœºåˆ¶ä»¥é€‚åº”é™ç»´åçš„æ•°æ®ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥å°†Whisper large-v3çš„ç¼–ç å™¨å¤§å°å‹ç¼©è¶…è¿‡50%ï¼Œå¹¶åœ¨è½¬å½•å‡†ç¡®æ€§ä¸Šä¼˜äºWhisper mediumï¼Œä»è€Œå»ºç«‹äº†æ•ˆç‡ä¸æ€§èƒ½çš„æ–°å¸•ç´¯æ‰˜æœ€ä¼˜è¾¹ç•Œã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.20900', 'title': 'DexGraspVLA: A Vision-Language-Action Framework Towards General Dexterous Grasping', 'url': 'https://huggingface.co/papers/2502.20900', 'abstract': "Dexterous grasping remains a fundamental yet challenging problem in robotics. A general-purpose robot must be capable of grasping diverse objects in arbitrary scenarios. However, existing research typically relies on specific assumptions, such as single-object settings or limited environments, leading to constrained generalization. Our solution is DexGraspVLA, a hierarchical framework that utilizes a pre-trained Vision-Language model as the high-level task planner and learns a diffusion-based policy as the low-level Action controller. The key insight lies in iteratively transforming diverse language and visual inputs into domain-invariant representations, where imitation learning can be effectively applied due to the alleviation of domain shift. Thus, it enables robust generalization across a wide range of real-world scenarios. Notably, our method achieves a 90+% success rate under thousands of unseen object, lighting, and background combinations in a ``zero-shot'' environment. Empirical analysis further confirms the consistency of internal model behavior across environmental variations, thereby validating our design and explaining its generalization performance. We hope our work can be a step forward in achieving general dexterous grasping. Our demo and code can be found at https://dexgraspvla.github.io/.", 'score': 3, 'issue_id': 2498, 'pub_date': '2025-02-28', 'pub_date_card': {'ru': '28 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 28', 'zh': '2æœˆ28æ—¥'}, 'hash': '5d9b331844235882', 'authors': ['Yifan Zhong', 'Xuchuan Huang', 'Ruochong Li', 'Ceyao Zhang', 'Yitao Liang', 'Yaodong Yang', 'Yuanpei Chen'], 'affiliations': ['Hong Kong University of Science and Technology (Guangzhou)', 'Institute for AI, Peking University', 'PKU-PsiBot Joint Lab'], 'pdf_title_img': 'assets/pdf/title_img/2502.20900.jpg', 'data': {'categories': ['#games', '#optimization', '#robotics', '#diffusion', '#interpretability', '#multimodal'], 'emoji': 'ğŸ¦¾', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ğ¼Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ°', 'desc': 'DexGraspVLA - ÑÑ‚Ğ¾ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Vision-Language Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ° Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼ÑƒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºÑƒ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ½Ğ¸Ğ·ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ»ĞµÑ€Ğ° Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ğ¸Ğ½Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ğº Ğ´Ğ¾Ğ¼ĞµĞ½Ñƒ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ‚ÑŒ Ğ¸Ğ¼Ğ¸Ñ‚Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ² ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾Ğ¼ Ğ´Ğ¸Ğ°Ğ¿Ğ°Ğ·Ğ¾Ğ½Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ², Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ±Ğ¾Ğ»ĞµĞµ 90% ÑƒÑĞ¿ĞµÑˆĞ½Ñ‹Ñ… Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ¾Ğ² Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ñ‚Ñ‹ÑÑÑ‡ Ğ½ĞµĞ²Ğ¸Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ½ĞµĞµ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ¸ Ñ„Ğ¾Ğ½Ğ¾Ğ². Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ĞµÑ‚ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ĞµĞ³Ğ¾ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ¾ĞºÑ€ÑƒĞ¶Ğ°ÑÑ‰ĞµĞ¹ ÑÑ€ĞµĞ´Ñ‹.'}, 'en': {'title': 'Achieving Dexterous Grasping with DexGraspVLA', 'desc': 'This paper presents DexGraspVLA, a novel hierarchical framework designed to improve dexterous grasping in robotics. It combines a pre-trained Vision-Language model for high-level task planning with a diffusion-based policy for low-level action control. The framework effectively transforms diverse language and visual inputs into domain-invariant representations, allowing for better generalization across various real-world scenarios. The results show a success rate of over 90% in grasping tasks involving thousands of unseen objects and conditions, demonstrating the robustness of the approach.'}, 'zh': {'title': 'å®ç°çµå·§æŠ“å–çš„çªç ´æ€§è¿›å±•', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºDexGraspVLAçš„å±‚æ¬¡æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æœºå™¨äººçµå·§æŠ“å–çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶åˆ©ç”¨é¢„è®­ç»ƒçš„è§†è§‰-è¯­è¨€æ¨¡å‹ä½œä¸ºé«˜å±‚ä»»åŠ¡è§„åˆ’å™¨ï¼Œå¹¶å­¦ä¹ åŸºäºæ‰©æ•£çš„ç­–ç•¥ä½œä¸ºä½å±‚åŠ¨ä½œæ§åˆ¶å™¨ã€‚é€šè¿‡å°†å¤šæ ·çš„è¯­è¨€å’Œè§†è§‰è¾“å…¥è¿­ä»£è½¬æ¢ä¸ºé¢†åŸŸä¸å˜çš„è¡¨ç¤ºï¼Œå‡è½»äº†é¢†åŸŸè½¬ç§»çš„å½±å“ï¼Œä»è€Œæœ‰æ•ˆåº”ç”¨æ¨¡ä»¿å­¦ä¹ ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ•°åƒç§æœªè§ç‰©ä½“ã€å…‰ç…§å’ŒèƒŒæ™¯ç»„åˆä¸‹ï¼Œåœ¨â€œé›¶-shotâ€ç¯å¢ƒä¸­å®ç°äº†90%ä»¥ä¸Šçš„æˆåŠŸç‡ï¼Œå±•ç¤ºäº†å…¶åœ¨çœŸå®åœºæ™¯ä¸­çš„å¼ºå¤§æ³›åŒ–èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.21291', 'title': 'MIGE: A Unified Framework for Multimodal Instruction-Based Image Generation and Editing', 'url': 'https://huggingface.co/papers/2502.21291', 'abstract': 'Despite significant progress in diffusion-based image generation, subject-driven generation and instruction-based editing remain challenging. Existing methods typically treat them separately, struggling with limited high-quality data and poor generalization. However, both tasks require capturing complex visual variations while maintaining consistency between inputs and outputs. Therefore, we propose MIGE, a unified framework that standardizes task representations using multimodal instructions. It treats subject-driven generation as creation on a blank canvas and instruction-based editing as modification of an existing image, establishing a shared input-output formulation. MIGE introduces a novel multimodal encoder that maps free-form multimodal instructions into a unified vision-language space, integrating visual and semantic features through a feature fusion mechanism.This unification enables joint training of both tasks, providing two key advantages: (1) Cross-Task Enhancement: By leveraging shared visual and semantic representations, joint training improves instruction adherence and visual consistency in both subject-driven generation and instruction-based editing. (2) Generalization: Learning in a unified format facilitates cross-task knowledge transfer, enabling MIGE to generalize to novel compositional tasks, including instruction-based subject-driven editing. Experiments show that MIGE excels in both subject-driven generation and instruction-based editing while setting a state-of-the-art in the new task of instruction-based subject-driven editing. Code and model have been publicly available at https://github.com/Eureka-Maggie/MIGE.', 'score': 3, 'issue_id': 2498, 'pub_date': '2025-02-28', 'pub_date_card': {'ru': '28 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 28', 'zh': '2æœˆ28æ—¥'}, 'hash': '55451dcf6bfad4a4', 'authors': ['Xueyun Tian', 'Wei Li', 'Bingbing Xu', 'Yige Yuan', 'Yuanzhuo Wang', 'Huawei Shen'], 'affiliations': ['Huawei Shen CAS Key Laboratory of AI Safety, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China', 'University of Chinese Academy of Sciences, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2502.21291.jpg', 'data': {'categories': ['#open_source', '#cv', '#diffusion', '#transfer_learning', '#multimodal'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ•Ğ´Ğ¸Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹', 'desc': 'MIGE - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ²Ğ¾Ğ±Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾. Ğ¡Ğ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ĞµĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ. MIGE Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ°.'}, 'en': {'title': 'MIGE: Unifying Image Generation and Editing with Multimodal Instructions', 'desc': 'This paper presents MIGE, a unified framework for improving both subject-driven image generation and instruction-based editing in machine learning. It addresses the challenges of limited data and poor generalization by standardizing task representations through multimodal instructions. MIGE employs a novel multimodal encoder that integrates visual and semantic features, allowing for joint training of both tasks. The results demonstrate that MIGE enhances instruction adherence and visual consistency while also enabling generalization to new tasks, achieving state-of-the-art performance in instruction-based subject-driven editing.'}, 'zh': {'title': 'ç»Ÿä¸€æ¡†æ¶ï¼Œæå‡å›¾åƒç”Ÿæˆä¸ç¼–è¾‘çš„èƒ½åŠ›', 'desc': 'å°½ç®¡æ‰©æ•£åŸºç¡€çš„å›¾åƒç”Ÿæˆå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ä»¥ä¸»é¢˜ä¸ºé©±åŠ¨çš„ç”Ÿæˆå’ŒåŸºäºæŒ‡ä»¤çš„ç¼–è¾‘ä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸å°†è¿™ä¸¤è€…åˆ†å¼€å¤„ç†ï¼Œå—é™äºé«˜è´¨é‡æ•°æ®çš„ä¸è¶³å’Œæ³›åŒ–èƒ½åŠ›å·®ã€‚æˆ‘ä»¬æå‡ºäº†MIGEï¼Œä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œé€šè¿‡å¤šæ¨¡æ€æŒ‡ä»¤æ ‡å‡†åŒ–ä»»åŠ¡è¡¨ç¤ºï¼Œå°†ä¸»é¢˜é©±åŠ¨ç”Ÿæˆè§†ä¸ºåœ¨ç©ºç™½ç”»å¸ƒä¸Šçš„åˆ›ä½œï¼Œè€Œå°†åŸºäºæŒ‡ä»¤çš„ç¼–è¾‘è§†ä¸ºå¯¹ç°æœ‰å›¾åƒçš„ä¿®æ”¹ã€‚MIGEå¼•å…¥äº†ä¸€ç§æ–°é¢–çš„å¤šæ¨¡æ€ç¼–ç å™¨ï¼Œå°†è‡ªç”±å½¢å¼çš„å¤šæ¨¡æ€æŒ‡ä»¤æ˜ å°„åˆ°ç»Ÿä¸€çš„è§†è§‰-è¯­è¨€ç©ºé—´ï¼Œä»è€Œå®ç°äº†è·¨ä»»åŠ¡çš„å¢å¼ºå’Œæ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.17125', 'title': 'LettuceDetect: A Hallucination Detection Framework for RAG Applications', 'url': 'https://huggingface.co/papers/2502.17125', 'abstract': "Retrieval Augmented Generation (RAG) systems remain vulnerable to hallucinated answers despite incorporating external knowledge sources. We present LettuceDetect a framework that addresses two critical limitations in existing hallucination detection methods: (1) the context window constraints of traditional encoder-based methods, and (2) the computational inefficiency of LLM based approaches. Building on ModernBERT's extended context capabilities (up to 8k tokens) and trained on the RAGTruth benchmark dataset, our approach outperforms all previous encoder-based models and most prompt-based models, while being approximately 30 times smaller than the best models. LettuceDetect is a token-classification model that processes context-question-answer triples, allowing for the identification of unsupported claims at the token level. Evaluations on the RAGTruth corpus demonstrate an F1 score of 79.22% for example-level detection, which is a 14.8% improvement over Luna, the previous state-of-the-art encoder-based architecture. Additionally, the system can process 30 to 60 examples per second on a single GPU, making it more practical for real-world RAG applications.", 'score': 3, 'issue_id': 2497, 'pub_date': '2025-02-24', 'pub_date_card': {'ru': '24 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 24', 'zh': '2æœˆ24æ—¥'}, 'hash': '347ebb871884d940', 'authors': ['ÃdÃ¡m KovÃ¡cs', 'GÃ¡bor Recski'], 'affiliations': ['KR Labs', 'TU Wien'], 'pdf_title_img': 'assets/pdf/title_img/2502.17125.jpg', 'data': {'categories': ['#benchmark', '#rag', '#hallucinations', '#architecture', '#small_models', '#long_context'], 'emoji': 'ğŸ¥¬', 'ru': {'title': 'LettuceDetect: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ² RAG-ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…', 'desc': 'LettuceDetect - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ (RAG). ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ ModernBERT Ñ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ğ¼ Ğ¾ĞºĞ½Ğ¾Ğ¼ Ğ´Ğ¾ 8000 Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ RAGTruth. LettuceDetect Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ² Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ², Ğ±ÑƒĞ´ÑƒÑ‡Ğ¸ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ² 30 Ñ€Ğ°Ğ· Ğ¼ĞµĞ½ÑŒÑˆĞµ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ F1-score 79.22% Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ½Ğ° 14.8% Ğ»ÑƒÑ‡ÑˆĞµ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰ĞµĞ³Ğ¾ SOTA-Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Luna.'}, 'en': {'title': 'LettuceDetect: Efficient Hallucination Detection for RAG Systems', 'desc': "This paper introduces LettuceDetect, a novel framework designed to improve the detection of hallucinated answers in Retrieval Augmented Generation (RAG) systems. It addresses limitations of existing methods by utilizing ModernBERT's extended context capabilities and a token-classification approach to analyze context-question-answer triples. LettuceDetect achieves a significant F1 score of 79.22% on the RAGTruth dataset, outperforming previous models while being much smaller and more efficient. The system's ability to process 30 to 60 examples per second on a single GPU enhances its practicality for real-world applications."}, 'zh': {'title': 'æå‡å¹»è§‰æ£€æµ‹çš„æ•ˆç‡ä¸å‡†ç¡®æ€§', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºLettuceDetectçš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰å¹»è§‰æ£€æµ‹æ–¹æ³•çš„ä¸¤ä¸ªä¸»è¦é™åˆ¶ã€‚é¦–å…ˆï¼Œå®ƒå…‹æœäº†ä¼ ç»Ÿç¼–ç å™¨æ–¹æ³•çš„ä¸Šä¸‹æ–‡çª—å£é™åˆ¶ï¼Œå…¶æ¬¡ï¼Œå®ƒæé«˜äº†åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ–¹æ³•çš„è®¡ç®—æ•ˆç‡ã€‚LettuceDetectåŸºäºModernBERTï¼Œèƒ½å¤Ÿå¤„ç†å¤šè¾¾8000ä¸ªæ ‡è®°ï¼Œå¹¶åœ¨RAGTruthåŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œè¡¨ç°ä¼˜äºæ‰€æœ‰å…ˆå‰çš„ç¼–ç å™¨æ¨¡å‹å’Œå¤§å¤šæ•°åŸºäºæç¤ºçš„æ¨¡å‹ã€‚è¯¥ç³»ç»Ÿåœ¨RAGTruthè¯­æ–™åº“ä¸Šçš„F1å¾—åˆ†è¾¾åˆ°79.22%ï¼Œå¹¶ä¸”åœ¨å•ä¸ªGPUä¸Šæ¯ç§’å¯å¤„ç†30åˆ°60ä¸ªç¤ºä¾‹ï¼Œé€‚ç”¨äºå®é™…çš„RAGåº”ç”¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.20490', 'title': 'EgoNormia: Benchmarking Physical Social Norm Understanding', 'url': 'https://huggingface.co/papers/2502.20490', 'abstract': 'Human activity is moderated by norms. When performing actions in the real world, humans not only follow norms, but also consider the trade-off between different norms However, machines are often trained without explicit supervision on norm understanding and reasoning, especially when the norms are grounded in a physical and social context. To improve and evaluate the normative reasoning capability of vision-language models (VLMs), we present EgoNormia |epsilon|, consisting of 1,853 ego-centric videos of human interactions, each of which has two related questions evaluating both the prediction and justification of normative actions. The normative actions encompass seven categories: safety, privacy, proxemics, politeness, cooperation, coordination/proactivity, and communication/legibility. To compile this dataset at scale, we propose a novel pipeline leveraging video sampling, automatic answer generation, filtering, and human validation. Our work demonstrates that current state-of-the-art vision-language models lack robust norm understanding, scoring a maximum of 45% on EgoNormia (versus a human bench of 92%). Our analysis of performance in each dimension highlights the significant risks of safety, privacy, and the lack of collaboration and communication capability when applied to real-world agents. We additionally show that through a retrieval-based generation method, it is possible to use EgoNomia to enhance normative reasoning in VLMs.', 'score': 2, 'issue_id': 2499, 'pub_date': '2025-02-27', 'pub_date_card': {'ru': '27 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 27', 'zh': '2æœˆ27æ—¥'}, 'hash': 'a2c7525ed78fb0ce', 'authors': ['MohammadHossein Rezaei', 'Yicheng Fu', 'Phil Cuvin', 'Caleb Ziems', 'Yanzhe Zhang', 'Hao Zhu', 'Diyi Yang'], 'affiliations': ['Georgia Tech', 'Stanford University', 'University of Arizona', 'University of Toronto'], 'pdf_title_img': 'assets/pdf/title_img/2502.20490.jpg', 'data': {'categories': ['#reasoning', '#multimodal', '#data', '#dataset', '#benchmark', '#ethics'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'EgoNormia: Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ˜Ğ˜ Ñ‡ĞµÑ€ĞµĞ· ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ EgoNormia |epsilon| - Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· 1853 ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° (VLM). ĞšĞ°Ğ¶Ğ´Ğ¾Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ ÑĞ¾Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ¶Ğ´Ğ°ĞµÑ‚ÑÑ Ğ´Ğ²ÑƒĞ¼Ñ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸, Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ² ÑĞµĞ¼Ğ¸ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸ÑÑ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ, ĞºĞ¾Ğ½Ñ„Ğ¸Ğ´ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ VLM Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°ÑÑ‚ Ğ½Ğ¾Ñ€Ğ¼Ñ‹, Ğ½Ğ°Ğ±Ğ¸Ñ€Ğ°Ñ Ğ¼Ğ°ĞºÑĞ¸Ğ¼ÑƒĞ¼ 45% Ğ½Ğ° EgoNormia Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ 92% Ñƒ Ğ»ÑĞ´ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² VLM Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ EgoNomia.'}, 'en': {'title': 'Enhancing Norm Understanding in Machines with EgoNormia', 'desc': "This paper introduces EgoNormia, a dataset designed to enhance the normative reasoning abilities of vision-language models (VLMs) by providing 1,853 ego-centric videos that depict human interactions. Each video is accompanied by questions that assess the model's ability to predict and justify normative actions across seven categories, including safety and privacy. The authors highlight that current VLMs perform poorly on this task, achieving only 45% accuracy compared to a human benchmark of 92%. They also propose a novel pipeline for dataset creation and demonstrate that using EgoNormia can improve the normative reasoning capabilities of VLMs through a retrieval-based generation method."}, 'zh': {'title': 'æå‡æœºå™¨çš„è§„èŒƒæ¨ç†èƒ½åŠ›', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†äººç±»åœ¨æ´»åŠ¨ä¸­å¦‚ä½•éµå¾ªç¤¾ä¼šè§„èŒƒï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªåä¸ºEgoNormiaçš„æ•°æ®é›†ï¼Œä»¥è¯„ä¼°è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨è§„èŒƒæ¨ç†æ–¹é¢çš„èƒ½åŠ›ã€‚è¯¥æ•°æ®é›†åŒ…å«1853ä¸ªä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„äººç±»äº’åŠ¨è§†é¢‘ï¼Œå¹¶é€šè¿‡ç›¸å…³é—®é¢˜æ¥è¯„ä¼°æ¨¡å‹å¯¹è§„èŒƒè¡Œä¸ºçš„é¢„æµ‹å’Œè§£é‡Šã€‚ç ”ç©¶å‘ç°ï¼Œå½“å‰çš„æœ€å…ˆè¿›VLMåœ¨è§„èŒƒç†è§£æ–¹é¢è¡¨ç°ä¸ä½³ï¼Œæœ€é«˜å¾—åˆ†ä»…ä¸º45%ï¼Œè¿œä½äºäººç±»çš„92%ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æå‡ºäº†ä¸€ç§åŸºäºæ£€ç´¢çš„ç”Ÿæˆæ–¹æ³•ï¼Œèƒ½å¤Ÿåˆ©ç”¨EgoNormiaæå‡VLMçš„è§„èŒƒæ¨ç†èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.20811', 'title': 'HAIC: Improving Human Action Understanding and Generation with Better Captions for Multi-modal Large Language Models', 'url': 'https://huggingface.co/papers/2502.20811', 'abstract': 'Recent Multi-modal Large Language Models (MLLMs) have made great progress in video understanding. However, their performance on videos involving human actions is still limited by the lack of high-quality data. To address this, we introduce a two-stage data annotation pipeline. First, we design strategies to accumulate videos featuring clear human actions from the Internet. Second, videos are annotated in a standardized caption format that uses human attributes to distinguish individuals and chronologically details their actions and interactions. Through this pipeline, we curate two datasets, namely HAICTrain and HAICBench. HAICTrain comprises 126K video-caption pairs generated by Gemini-Pro and verified for training purposes. Meanwhile, HAICBench includes 500 manually annotated video-caption pairs and 1,400 QA pairs, for a comprehensive evaluation of human action understanding. Experimental results demonstrate that training with HAICTrain not only significantly enhances human understanding abilities across 4 benchmarks, but can also improve text-to-video generation results. Both the HAICTrain and HAICBench are released at https://huggingface.co/datasets/KuaishouHAIC/HAIC.', 'score': 1, 'issue_id': 2486, 'pub_date': '2025-02-28', 'pub_date_card': {'ru': '28 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 28', 'zh': '2æœˆ28æ—¥'}, 'hash': '806f6aacd5ee2f8a', 'authors': ['Xiao Wang', 'Jingyun Hua', 'Weihong Lin', 'Yuanxing Zhang', 'Fuzheng Zhang', 'Jianlong Wu', 'Di Zhang', 'Liqiang Nie'], 'affiliations': ['Kuaishou Technology'], 'pdf_title_img': 'assets/pdf/title_img/2502.20811.jpg', 'data': {'categories': ['#dataset', '#data', '#video', '#multimodal', '#benchmark', '#synthetic'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑĞ¼Ğ¸ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ ÑĞ±Ğ¾Ñ€ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¸Ñ… ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ. Ğ’ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğµ Ğ±Ñ‹Ğ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ñ‹ Ğ´Ğ²Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…: HAICTrain Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ HAICBench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° HAICTrain Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ½Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'Enhancing Video Understanding with Curated Human Action Datasets', 'desc': 'This paper presents a solution to improve video understanding in Multi-modal Large Language Models (MLLMs) by addressing the scarcity of high-quality data on human actions. The authors introduce a two-stage data annotation pipeline that first collects videos with clear human actions from the Internet and then annotates them using a standardized caption format. This results in two curated datasets: HAICTrain, which contains 126K video-caption pairs for training, and HAICBench, which includes 500 annotated pairs for evaluation. Experimental results show that using HAICTrain significantly boosts human action understanding and enhances text-to-video generation capabilities across multiple benchmarks.'}, 'zh': {'title': 'æå‡è§†é¢‘ç†è§£çš„åˆ›æ–°æ•°æ®æ ‡æ³¨æµç¨‹', 'desc': 'æœ€è¿‘çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†é¢‘ç†è§£æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨æ¶‰åŠäººç±»åŠ¨ä½œçš„è§†é¢‘ä¸Šçš„è¡¨ç°ä»ç„¶å—åˆ°é«˜è´¨é‡æ•°æ®ç¼ºä¹çš„é™åˆ¶ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªä¸¤é˜¶æ®µçš„æ•°æ®æ ‡æ³¨æµç¨‹ï¼Œé¦–å…ˆä»äº’è”ç½‘æ”¶é›†åŒ…å«æ¸…æ™°äººç±»åŠ¨ä½œçš„è§†é¢‘ï¼Œç„¶åä½¿ç”¨æ ‡å‡†åŒ–çš„å­—å¹•æ ¼å¼å¯¹è§†é¢‘è¿›è¡Œæ ‡æ³¨ã€‚é€šè¿‡è¿™ä¸ªæµç¨‹ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸¤ä¸ªæ•°æ®é›†HAICTrainå’ŒHAICBenchï¼Œå®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨HAICTrainè¿›è¡Œè®­ç»ƒæ˜¾è‘—æå‡äº†äººç±»åŠ¨ä½œç†è§£èƒ½åŠ›ï¼Œå¹¶æ”¹å–„äº†æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆçš„æ•ˆæœã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.07920', 'title': 'Crowdsource, Crawl, or Generate? Creating SEA-VL, a Multicultural\n  Vision-Language Dataset for Southeast Asia', 'url': 'https://huggingface.co/papers/2503.07920', 'abstract': 'Southeast Asia (SEA) is a region of extraordinary linguistic and cultural diversity, yet it remains significantly underrepresented in vision-language (VL) research. This often results in artificial intelligence (AI) models that fail to capture SEA cultural nuances. To fill this gap, we present SEA-VL, an open-source initiative dedicated to developing high-quality, culturally relevant data for SEA languages. By involving contributors from SEA countries, SEA-VL aims to ensure better cultural relevance and diversity, fostering greater inclusivity of underrepresented languages in VL research. Beyond crowdsourcing, our initiative goes one step further in the exploration of the automatic collection of culturally relevant images through crawling and image generation. First, we find that image crawling achieves approximately ~85% cultural relevance while being more cost- and time-efficient than crowdsourcing. Second, despite the substantial progress in generative vision models, synthetic images remain unreliable in accurately reflecting SEA cultures. The generated images often fail to reflect the nuanced traditions and cultural contexts of the region. Collectively, we gather 1.28M SEA culturally-relevant images, more than 50 times larger than other existing datasets. Through SEA-VL, we aim to bridge the representation gap in SEA, fostering the development of more inclusive AI systems that authentically represent diverse cultures across SEA.', 'score': 73, 'issue_id': 2654, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 10', 'zh': '3æœˆ10æ—¥'}, 'hash': '32c690b6ffb8b143', 'authors': ['Samuel Cahyawijaya', 'Holy Lovenia', 'Joel Ruben Antony Moniz', 'Tack Hwa Wong', 'Mohammad Rifqi Farhansyah', 'Thant Thiri Maung', 'Frederikus Hudi', 'David Anugraha', 'Muhammad Ravi Shulthan Habibi', 'Muhammad Reza Qorib', 'Amit Agarwal', 'Joseph Marvin Imperial', 'Hitesh Laxmichand Patel', 'Vicky Feliren', 'Bahrul Ilmi Nasution', 'Manuel Antonio Rufino', 'Genta Indra Winata', 'Rian Adam Rajagede', 'Carlos Rafael Catalan', 'Mohamed Fazli Imam', 'Priyaranjan Pattnayak', 'Salsabila Zahirah Pranida', 'Kevin Pratama', 'Yeshil Bangera', 'Adisai Na-Thalang', 'Patricia Nicole Monderin', 'Yueqi Song', 'Christian Simon', 'Lynnette Hui Xian Ng', "Richardy Lobo' Sapan", 'Taki Hasan Rafi', 'Bin Wang', 'Supryadi', 'Kanyakorn Veerakanjana', 'Piyalitt Ittichaiwong', 'Matthew Theodore Roque', 'Karissa Vincentio', 'Takdanai Kreangphet', 'Phakphum Artkaew', 'Kadek Hendrawan Palgunadi', 'Yanzhi Yu', 'Rochana Prih Hastuti', 'William Nixon', 'Mithil Bangera', 'Adrian Xuan Wei Lim', 'Aye Hninn Khine', 'Hanif Muhammad Zhafran', 'Teddy Ferdinan', 'Audra Aurora Izzani', 'Ayushman Singh', 'Evan', 'Jauza Akbar Krito', 'Michael Anugraha', 'Fenal Ashokbhai Ilasariya', 'Haochen Li', 'John Amadeo Daniswara', 'Filbert Aurelian Tjiaranata', 'Eryawan Presma Yulianrifat', 'Can Udomcharoenchaikit', 'Fadil Risdian Ansori', 'Mahardika Krisna Ihsani', 'Giang Nguyen', 'Anab Maulana Barik', 'Dan John Velasco', 'Rifo Ahmad Genadi', 'Saptarshi Saha', 'Chengwei Wei', 'Isaiah Flores', 'Kenneth Ko Han Chen', 'Anjela Gail Santos', 'Wan Shen Lim', 'Kaung Si Phyo', 'Tim Santos', 'Meisyarah Dwiastuti', 'Jiayun Luo', 'Jan Christian Blaise Cruz', 'Ming Shan Hee', 'Ikhlasul Akmal Hanif', 'M. Alif Al Hakim', "Muhammad Rizky Sya'ban", 'Kun Kerdthaisong', 'Lester James V. Miranda', 'Fajri Koto', 'Tirana Noor Fatyanosa', 'Alham Fikri Aji', 'Jostin Jerico Rosal', 'Jun Kevin', 'Robert Wijaya', 'Onno P. Kampman', 'Ruochen Zhang', 'BÃ¶rje F. Karlsson', 'Peerat Limkonchotiwat'], 'affiliations': ['AI Singapore', 'Allen AI', 'Ateneo de Manila University', 'Auburn University', 'Bandung Institute of Technology', 'Beijing Academy of Artificial Intelligence (BAAI)', 'Binus University', 'Brawijaya University', 'Brown University', 'Capital One', 'Carnegie Mellon University', 'Chulalongkorn University', 'Cohere', 'Dataxet:Sonar', 'Faculty of Medicine Siriraj Hospital, Mahidol University', 'Graphcore', 'Hanyang University', 'Independent', 'Indian Statistical Institute, Kolkata', 'IndoNLP', 'Institut Teknologi Sepuluh Nopember', 'Institute for Infocomm Research, Singapore', 'King Mongkuts University of Technology Thonburi', 'MBZUAI', 'MOH Office for Healthcare Transformation', 'Macau University of Science and Technology', 'Meta', 'Mila - Quebec AI Institute', 'Monash University, Indonesia', 'Nara Institute of Science and Technology', 'National University Philippines', 'National University of Singapore', 'New York University', 'Oracle', 'Polytechnique Montreal', 'SCB 10X', 'SEACrowd', 'Samsung R&D Institute Philippines', 'Seoul National University of Science and Technology', 'Singapore Polytechnic', 'Singapore University of Technology and Design', 'Sony Group Corporation', 'Srinakharinwirot University', 'Thammasat University', 'The University of Manchester', 'Tianjin University', 'Ton Duc Thang University', 'Universitas Gadjah Mada', 'Universitas Islam Indonesia', 'Universitas Pelita Harapan', 'University of Bath', 'University of Illiinois, Urbana-Champaign', 'University of Indonesia', 'University of New Haven', 'University of Toronto', 'University of the Philippines', 'Vidyasirimedhi Institute of Science and Technology', 'Works Applications', 'WrocÅ‚aw Tech'], 'pdf_title_img': 'assets/pdf/title_img/2503.07920.jpg', 'data': {'categories': ['#synthetic', '#dataset', '#open_source', '#data', '#multimodal'], 'emoji': 'ğŸŒ', 'ru': {'title': 'ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ²Ğ° Ğ² Ğ˜Ğ˜ Ğ´Ğ»Ñ Ğ®Ğ³Ğ¾-Ğ’Ğ¾ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ ĞĞ·Ğ¸Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ SEA-VL - Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ñ‚Ğ¸Ğ²Ñƒ Ğ¿Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ² Ğ®Ğ³Ğ¾-Ğ’Ğ¾ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ ĞĞ·Ğ¸Ğ¸ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ vision-language Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞŸÑ€Ğ¾ĞµĞºÑ‚ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ğ¾Ğ¹ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ² Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑÑ… Ğ˜Ğ˜. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸Ğ·ÑƒÑ‡Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ±Ğ¾Ñ€Ğ° ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ğ¼Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· ĞºÑ€Ğ°ÑƒĞ»Ğ¸Ğ½Ğ³ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ’ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğµ ÑĞ¾Ğ±Ñ€Ğ°Ğ½Ğ¾ 1,28 Ğ¼Ğ»Ğ½ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ğ¾ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ² 50 Ñ€Ğ°Ğ· Ğ±Ğ¾Ğ»ÑŒÑˆĞµ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ².'}, 'en': {'title': 'Bridging the Cultural Gap in AI with SEA-VL', 'desc': "The paper introduces SEA-VL, an initiative aimed at enhancing representation of Southeast Asian cultures in vision-language (VL) research. It highlights the creation of a large dataset containing 1.28 million culturally relevant images, which is significantly larger than existing datasets. The initiative combines crowdsourcing with automated image crawling, achieving high cultural relevance efficiently. However, it also notes challenges with generative models, as synthetic images often fail to accurately depict the region's diverse cultural nuances."}, 'zh': {'title': 'å¡«è¡¥ä¸œå—äºšæ–‡åŒ–åœ¨AIç ”ç©¶ä¸­çš„ç©ºç™½', 'desc': 'ä¸œå—äºšåœ°åŒºè¯­è¨€å’Œæ–‡åŒ–å¤šæ ·æ€§æä¸ºä¸°å¯Œï¼Œä½†åœ¨è§†è§‰è¯­è¨€ç ”ç©¶ä¸­å´ä¸¥é‡ç¼ºä¹ä»£è¡¨æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†SEA-VLï¼Œè¿™æ˜¯ä¸€ä¸ªå¼€æ”¾æºä»£ç çš„é¡¹ç›®ï¼Œæ—¨åœ¨ä¸ºä¸œå—äºšè¯­è¨€å¼€å‘é«˜è´¨é‡ã€æ–‡åŒ–ç›¸å…³çš„æ•°æ®ã€‚é€šè¿‡å¸å¼•æ¥è‡ªä¸œå—äºšå›½å®¶çš„è´¡çŒ®è€…ï¼ŒSEA-VLç¡®ä¿äº†æ›´å¥½çš„æ–‡åŒ–ç›¸å…³æ€§å’Œå¤šæ ·æ€§ï¼Œä¿ƒè¿›äº†åœ¨è§†è§‰è¯­è¨€ç ”ç©¶ä¸­å¯¹è¢«ä½ä¼°è¯­è¨€çš„åŒ…å®¹æ€§ã€‚æˆ‘ä»¬æ”¶é›†äº†128ä¸‡å¼ ä¸ä¸œå—äºšæ–‡åŒ–ç›¸å…³çš„å›¾åƒï¼Œè¿œè¶…ç°æœ‰æ•°æ®é›†çš„è§„æ¨¡ï¼Œæ—¨åœ¨ç¼©å°ä¸œå—äºšçš„ä»£è¡¨æ€§å·®è·ï¼Œæ¨åŠ¨æ›´å…·åŒ…å®¹æ€§çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„å‘å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.07536', 'title': 'LMM-R1: Empowering 3B LMMs with Strong Reasoning Abilities Through\n  Two-Stage Rule-Based RL', 'url': 'https://huggingface.co/papers/2503.07536', 'abstract': 'Enhancing reasoning in Large Multimodal Models (LMMs) faces unique challenges from the complex interplay between visual perception and logical reasoning, particularly in compact 3B-parameter architectures where architectural constraints limit reasoning capacity and modality alignment.   While rule-based reinforcement learning (RL) excels in text-only domains, its multimodal extension confronts two critical barriers: (1) data limitations due to ambiguous answers and scarce complex reasoning examples, and (2) degraded foundational reasoning induced by multimodal pretraining.   To address these challenges, we propose \\method, a two-stage framework adapting rule-based RL for multimodal reasoning through Foundational Reasoning Enhancement (FRE) followed by Multimodal Generalization Training (MGT). The FRE stage first strengthens reasoning abilities using text-only data with rule-based RL, then the MGT stage generalizes these reasoning capabilities to multimodal domains.   Experiments on Qwen2.5-VL-Instruct-3B demonstrate that \\method achieves 4.83\\% and 4.5\\% average improvements over baselines in multimodal and text-only benchmarks, respectively, with a 3.63\\% gain in complex Football Game tasks. These results validate that text-based reasoning enhancement enables effective multimodal generalization, offering a data-efficient paradigm that bypasses costly high-quality multimodal training data.', 'score': 50, 'issue_id': 2654, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 10', 'zh': '3æœˆ10æ—¥'}, 'hash': '59c304598f64f1e6', 'authors': ['Yingzhe Peng', 'Gongrui Zhang', 'Miaosen Zhang', 'Zhiyuan You', 'Jie Liu', 'Qipeng Zhu', 'Kai Yang', 'Xingzhong Xu', 'Xin Geng', 'Xu Yang'], 'affiliations': ['Ant Group', 'Fudan University', 'Key Laboratory of New Generation Artificial Intelligence Technology and Its Interdisciplinary Applications (Southeast University), Ministry of Education, China', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2503.07536.jpg', 'data': {'categories': ['#reasoning', '#training', '#rl', '#benchmark', '#small_models', '#multimodal'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ£ÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ñ‚ĞµĞºÑÑ‚Ğ°Ñ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LMM) Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ¾Ğ¹ 3B-Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° ÑƒÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ¸Ñ… Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ğ½Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Qwen2.5-VL-Instruct-3B Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ°Ğº Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ² Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¾Ğ±ÑŠĞµĞ¼Ğ°Ñ… ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Boosting Reasoning in Multimodal Models Efficiently', 'desc': 'This paper addresses the challenges of improving reasoning in Large Multimodal Models (LMMs), particularly in smaller architectures with limited parameters. It identifies two main issues: the lack of sufficient data for complex reasoning in multimodal contexts and the negative impact of multimodal pretraining on foundational reasoning. The authors propose a two-stage framework that first enhances reasoning using rule-based reinforcement learning on text-only data, followed by training to generalize these skills to multimodal scenarios. Experimental results show significant improvements in reasoning performance across both multimodal and text-only tasks, demonstrating the effectiveness of their approach in reducing the need for extensive multimodal training data.'}, 'zh': {'title': 'å¢å¼ºå¤šæ¨¡æ€æ¨ç†èƒ½åŠ›çš„é«˜æ•ˆæ–¹æ³•', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†åœ¨å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰ä¸­å¢å¼ºæ¨ç†èƒ½åŠ›çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å‚æ•°é‡ä¸º3Bçš„ç´§å‡‘æ¶æ„ä¸­ï¼Œè§†è§‰æ„ŸçŸ¥ä¸é€»è¾‘æ¨ç†ä¹‹é—´çš„å¤æ‚äº’åŠ¨ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸º\textit{method}çš„ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œé€šè¿‡åŸºç¡€æ¨ç†å¢å¼ºï¼ˆFREï¼‰å’Œå¤šæ¨¡æ€æ³›åŒ–è®­ç»ƒï¼ˆMGTï¼‰æ¥é€‚åº”å¤šæ¨¡æ€æ¨ç†ã€‚FREé˜¶æ®µåˆ©ç”¨åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¢å¼ºæ–‡æœ¬æ•°æ®çš„æ¨ç†èƒ½åŠ›ï¼ŒMGTé˜¶æ®µåˆ™å°†è¿™äº›æ¨ç†èƒ½åŠ›æ¨å¹¿åˆ°å¤šæ¨¡æ€é¢†åŸŸã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œ\textit{method}åœ¨å¤šæ¨¡æ€å’Œæ–‡æœ¬åŸºå‡†æµ‹è¯•ä¸­åˆ†åˆ«æ¯”åŸºçº¿æé«˜äº†4.83%å’Œ4.5%ï¼Œåœ¨å¤æ‚çš„è¶³çƒæ¯”èµ›ä»»åŠ¡ä¸­æé«˜äº†3.63%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.08638', 'title': 'YuE: Scaling Open Foundation Models for Long-Form Music Generation', 'url': 'https://huggingface.co/papers/2503.08638', 'abstract': "We tackle the task of long-form music generation--particularly the challenging lyrics-to-song problem--by introducing YuE, a family of open foundation models based on the LLaMA2 architecture. Specifically, YuE scales to trillions of tokens and generates up to five minutes of music while maintaining lyrical alignment, coherent musical structure, and engaging vocal melodies with appropriate accompaniment. It achieves this through (1) track-decoupled next-token prediction to overcome dense mixture signals, (2) structural progressive conditioning for long-context lyrical alignment, and (3) a multitask, multiphase pre-training recipe to converge and generalize. In addition, we redesign the in-context learning technique for music generation, enabling versatile style transfer (e.g., converting Japanese city pop into an English rap while preserving the original accompaniment) and bidirectional generation. Through extensive evaluation, we demonstrate that YuE matches or even surpasses some of the proprietary systems in musicality and vocal agility. In addition, fine-tuning YuE enables additional controls and enhanced support for tail languages. Furthermore, beyond generation, we show that YuE's learned representations can perform well on music understanding tasks, where the results of YuE match or exceed state-of-the-art methods on the MARBLE benchmark. Keywords: lyrics2song, song generation, long-form, foundation model, music generation", 'score': 46, 'issue_id': 2655, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 11', 'zh': '3æœˆ11æ—¥'}, 'hash': 'eb1539380bf435c9', 'authors': ['Ruibin Yuan', 'Hanfeng Lin', 'Shuyue Guo', 'Ge Zhang', 'Jiahao Pan', 'Yongyi Zang', 'Haohe Liu', 'Yiming Liang', 'Wenye Ma', 'Xingjian Du', 'Xinrun Du', 'Zhen Ye', 'Tianyu Zheng', 'Yinghao Ma', 'Minghao Liu', 'Zeyue Tian', 'Ziya Zhou', 'Liumeng Xue', 'Xingwei Qu', 'Yizhi Li', 'Shangda Wu', 'Tianhao Shen', 'Ziyang Ma', 'Jun Zhan', 'Chunhui Wang', 'Yatian Wang', 'Xiaowei Chi', 'Xinyue Zhang', 'Zhenzhu Yang', 'Xiangzhou Wang', 'Shansong Liu', 'Lingrui Mei', 'Peng Li', 'Junjie Wang', 'Jianwei Yu', 'Guojian Pang', 'Xu Li', 'Zihao Wang', 'Xiaohuan Zhou', 'Lijun Yu', 'Emmanouil Benetos', 'Yong Chen', 'Chenghua Lin', 'Xie Chen', 'Gus Xia', 'Zhaoxiang Zhang', 'Chao Zhang', 'Wenhu Chen', 'Xinyu Zhou', 'Xipeng Qiu', 'Roger Dannenberg', 'Jiaheng Liu', 'Jian Yang', 'Wenhao Huang', 'Wei Xue', 'Xu Tan', 'Yike Guo'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2503.08638.jpg', 'data': {'categories': ['#training', '#multimodal', '#long_context', '#benchmark', '#low_resource', '#open_source', '#story_generation', '#audio'], 'emoji': 'ğŸµ', 'ru': {'title': 'YuE: ĞÑ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ·Ñ‹ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ°', 'desc': 'YuE - ÑÑ‚Ğ¾ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ LLaMA2. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ¾ Ğ¿ÑÑ‚Ğ¸ Ğ¼Ğ¸Ğ½ÑƒÑ‚ Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ¿ĞµÑĞ½Ğ¸, Ğ¼ÑƒĞ·Ñ‹ĞºĞ°Ğ»ÑŒĞ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ¸ Ğ²Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼ĞµĞ»Ğ¾Ğ´Ğ¸Ğ¸ Ñ Ğ°ĞºĞºĞ¾Ğ¼Ğ¿Ğ°Ğ½ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ¼. YuE Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµÑ…Ğ½Ğ¸Ğº, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ° Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ‚Ñ€ĞµĞºĞ¾Ğ² Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğµ ĞºĞ¾Ğ½Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ¿ĞµÑ€ĞµĞ½Ğ¾Ñ ÑÑ‚Ğ¸Ğ»Ñ Ğ¸ Ğ´Ğ²ÑƒĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ, Ğ° ĞµÑ‘ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸.'}, 'en': {'title': 'Transforming Lyrics into Melodies with YuE!', 'desc': 'This paper presents YuE, a new family of open foundation models designed for long-form music generation, specifically focusing on the lyrics-to-song challenge. YuE utilizes the LLaMA2 architecture and can generate up to five minutes of music while ensuring that the lyrics align with the musical structure and melodies. Key innovations include track-decoupled next-token prediction for better signal clarity, structural progressive conditioning for lyrical alignment, and a multitask pre-training approach to enhance generalization. The model also supports versatile style transfer and performs well on music understanding tasks, achieving results that rival existing proprietary systems.'}, 'zh': {'title': 'YuEï¼šæ­Œè¯è½¬æ­Œæ›²çš„éŸ³ä¹ç”Ÿæˆæ–°çªç ´', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºYuEçš„å¼€æ”¾åŸºç¡€æ¨¡å‹ï¼Œä¸“æ³¨äºé•¿ç¯‡éŸ³ä¹ç”Ÿæˆï¼Œç‰¹åˆ«æ˜¯æ­Œè¯è½¬æ­Œæ›²çš„é—®é¢˜ã€‚YuEåŸºäºLLaMA2æ¶æ„ï¼Œèƒ½å¤Ÿç”Ÿæˆé•¿è¾¾äº”åˆ†é’Ÿçš„éŸ³ä¹ï¼ŒåŒæ—¶ä¿æŒæ­Œè¯çš„å¯¹é½ã€è¿è´¯çš„éŸ³ä¹ç»“æ„å’Œå¼•äººå…¥èƒœçš„æ—‹å¾‹ã€‚é€šè¿‡é‡‡ç”¨è§£è€¦çš„ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹ã€ç»“æ„æ€§æ¸è¿›æ¡ä»¶å’Œå¤šä»»åŠ¡é¢„è®­ç»ƒç­‰æŠ€æœ¯ï¼ŒYuEåœ¨éŸ³ä¹ç”Ÿæˆä¸­è¡¨ç°å‡ºè‰²ã€‚ç»è¿‡è¯„ä¼°ï¼ŒYuEåœ¨éŸ³ä¹æ€§å’Œå£°ä¹çµæ´»æ€§æ–¹é¢ä¸ä¸€äº›ä¸“æœ‰ç³»ç»Ÿç›¸åŒ¹æ•Œï¼Œç”šè‡³è¶…è¶Šäº†å®ƒä»¬ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.05978', 'title': 'MagicInfinite: Generating Infinite Talking Videos with Your Words and\n  Voice', 'url': 'https://huggingface.co/papers/2503.05978', 'abstract': "We present MagicInfinite, a novel diffusion Transformer (DiT) framework that overcomes traditional portrait animation limitations, delivering high-fidelity results across diverse character types-realistic humans, full-body figures, and stylized anime characters. It supports varied facial poses, including back-facing views, and animates single or multiple characters with input masks for precise speaker designation in multi-character scenes. Our approach tackles key challenges with three innovations: (1) 3D full-attention mechanisms with a sliding window denoising strategy, enabling infinite video generation with temporal coherence and visual quality across diverse character styles; (2) a two-stage curriculum learning scheme, integrating audio for lip sync, text for expressive dynamics, and reference images for <PRE_TAG>identity preservation</POST_TAG>, enabling flexible multi-modal control over long sequences; and (3) region-specific masks with adaptive loss functions to balance global textual control and local audio guidance, supporting speaker-specific animations. Efficiency is enhanced via our innovative unified step and cfg distillation techniques, achieving a 20x inference speed boost over the basemodel: generating a 10 second 540x540p video in 10 seconds or 720x720p in 30 seconds on 8 H100 GPUs, without quality loss. Evaluations on our new benchmark demonstrate MagicInfinite's superiority in audio-lip synchronization, identity preservation, and motion naturalness across diverse scenarios. It is publicly available at https://www.hedra.com/, with examples at https://magicinfinite.github.io/.", 'score': 26, 'issue_id': 2658, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 7', 'zh': '3æœˆ7æ—¥'}, 'hash': '2111564f7e67ca23', 'authors': ['Hongwei Yi', 'Tian Ye', 'Shitong Shao', 'Xuancheng Yang', 'Jiantong Zhao', 'Hanzhong Guo', 'Terrance Wang', 'Qingyu Yin', 'Zeke Xie', 'Lei Zhu', 'Wei Li', 'Michael Lingelbach', 'Daquan Zhou'], 'affiliations': ['HKU', 'HKUST(GZ)', 'Hedra Inc.', 'Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2503.05978.jpg', 'data': {'categories': ['#benchmark', '#multimodal', '#inference', '#diffusion', '#video', '#open_source'], 'emoji': 'ğŸ­', 'ru': {'title': 'MagicInfinite: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ€Ñ‚Ñ€ĞµÑ‚Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜', 'desc': 'MagicInfinite - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° Ğ´Ğ»Ñ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ€Ñ‚Ñ€ĞµÑ‚Ğ¾Ğ². ĞĞ½Ğ° ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ»ÑĞ´ĞµĞ¹ Ğ¸ Ğ°Ğ½Ğ¸Ğ¼Ğµ-Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ 3D Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ Ğ¸ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½-ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¼Ğ°ÑĞºĞ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸. MagicInfinite Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ³ÑƒĞ± Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Revolutionizing Portrait Animation with MagicInfinite', 'desc': 'MagicInfinite is a cutting-edge diffusion Transformer framework designed to enhance portrait animation by producing high-quality results for various character types, including realistic humans and stylized anime. It introduces innovative techniques such as 3D full-attention mechanisms and a sliding window denoising strategy, allowing for infinite video generation while maintaining visual coherence. The framework employs a two-stage curriculum learning approach that integrates audio and text inputs for improved lip synchronization and expressive dynamics, along with region-specific masks for precise control over animations. With a significant boost in efficiency, MagicInfinite can generate high-resolution videos rapidly, outperforming existing models in key areas like audio-lip synchronization and identity preservation.'}, 'zh': {'title': 'é­”æ³•æ— é™ï¼šçªç ´è‚–åƒåŠ¨ç”»çš„ç•Œé™', 'desc': 'MagicInfiniteæ˜¯ä¸€ç§æ–°å‹çš„æ‰©æ•£å˜æ¢å™¨æ¡†æ¶ï¼Œæ—¨åœ¨å…‹æœä¼ ç»Ÿè‚–åƒåŠ¨ç”»çš„å±€é™æ€§ï¼Œèƒ½å¤Ÿåœ¨å¤šç§è§’è‰²ç±»å‹ä¸­æä¾›é«˜ä¿çœŸåº¦çš„ç»“æœï¼ŒåŒ…æ‹¬çœŸå®äººç±»ã€å…¨èº«äººç‰©å’Œé£æ ¼åŒ–çš„åŠ¨æ¼«è§’è‰²ã€‚è¯¥æ¡†æ¶æ”¯æŒå¤šç§é¢éƒ¨å§¿åŠ¿çš„åŠ¨ç”»ï¼ŒåŒ…æ‹¬èƒŒé¢è§†å›¾ï¼Œå¹¶èƒ½å¤Ÿé€šè¿‡è¾“å…¥æ©ç ç²¾ç¡®æŒ‡å®šå¤šè§’è‰²åœºæ™¯ä¸­çš„å‘è¨€è€…ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡ä¸‰é¡¹åˆ›æ–°æ¥è§£å†³å…³é”®æŒ‘æˆ˜ï¼š3Då…¨æ³¨æ„åŠ›æœºåˆ¶ä¸æ»‘åŠ¨çª—å£å»å™ªç­–ç•¥ï¼Œæ”¯æŒæ— é™è§†é¢‘ç”Ÿæˆå¹¶ä¿æŒæ—¶é—´ä¸€è‡´æ€§å’Œè§†è§‰è´¨é‡ï¼›ä»¥åŠåŒºåŸŸç‰¹å®šæ©ç ä¸è‡ªé€‚åº”æŸå¤±å‡½æ•°çš„ç»“åˆï¼Œå¹³è¡¡å…¨å±€æ–‡æœ¬æ§åˆ¶å’Œå±€éƒ¨éŸ³é¢‘æŒ‡å¯¼ã€‚æˆ‘ä»¬çš„è¯„ä¼°è¡¨æ˜ï¼ŒMagicInfiniteåœ¨éŸ³é¢‘ä¸å”‡åŒæ­¥ã€èº«ä»½ä¿ç•™å’ŒåŠ¨ä½œè‡ªç„¶æ€§æ–¹é¢ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.08120', 'title': 'UniF^2ace: Fine-grained Face Understanding and Generation\n  with Unified Multimodal Models', 'url': 'https://huggingface.co/papers/2503.08120', 'abstract': "Unified multimodal models (UMMs) have emerged as a powerful paradigm in foundational computer vision research, demonstrating significant potential in both image understanding and generation. However, existing research in the face domain primarily focuses on coarse facial attribute understanding, with limited capacity to handle fine-grained facial attributes and without addressing generation capabilities. To overcome these limitations, we propose UniF^2ace, the first UMM tailored specifically for fine-grained face understanding and generation. In general, we train UniF^2ace on a self-constructed, specialized dataset utilizing two mutually beneficial diffusion techniques and a two-level mixture-of-experts architecture. Specifically, we first build a large-scale facial dataset, UniF^2ace-130K, which contains 130K image-text pairs with one million question-answering pairs that span a wide range of facial attributes. Second, we establish a theoretical connection between discrete diffusion score matching and masked generative models, optimizing both evidence lower bounds simultaneously, which significantly improves the model's ability to synthesize facial details. Finally, we introduce both token-level and sequence-level mixture-of-experts, enabling efficient fine-grained representation learning for both understanding and generation tasks. Extensive experiments on UniF^2ace-130K demonstrate that UniF^2ace outperforms existing UMMs and generative models, achieving superior performance across both understanding and generation tasks.", 'score': 25, 'issue_id': 2654, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 11', 'zh': '3æœˆ11æ—¥'}, 'hash': '7c6e7c685b283d61', 'authors': ['Junzhe Li', 'Xuerui Qiu', 'Linrui Xu', 'Liya Guo', 'Delin Qu', 'Tingting Long', 'Chun Fan', 'Ming Li'], 'affiliations': ['Central South University', 'Computer Center, Peking University', 'Fudan University', 'Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ)', 'Institute of Automation, Chinese Academy of Sciences', 'School of Computer Science, Peking University', 'Yau Mathematical Sciences Center and Department of Mathematical Sciences, Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.08120.jpg', 'data': {'categories': ['#synthetic', '#cv', '#dataset', '#multimodal', '#architecture', '#diffusion'], 'emoji': 'ğŸ§‘', 'ru': {'title': 'UniF^2ace: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ»Ğ¸Ñ†', 'desc': 'UniF^2ace - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ (UMM), ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ»Ğ¸Ñ†. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰ĞµĞ¼ 130 Ñ‚Ñ‹ÑÑÑ‡ Ğ¿Ğ°Ñ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ñ Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ¼ Ğ¿Ğ°Ñ€ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ°Ñ… Ğ»Ğ¸Ñ†Ğ°. Ğ’ UniF^2ace Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ÑÑ Ğ´Ğ²Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ÑÑÑ‰Ğ¸Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ¸ Ğ´Ğ²ÑƒÑ…ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° ÑĞ¼ĞµÑĞ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ UniF^2ace Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ UMM Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ»Ğ¸Ñ†.'}, 'en': {'title': 'UniF^2ace: Mastering Fine-Grained Facial Understanding and Generation', 'desc': 'This paper introduces UniF^2ace, a unified multimodal model designed for fine-grained understanding and generation of facial attributes. Unlike previous models that only focus on coarse attributes, UniF^2ace leverages a large-scale dataset of 130K image-text pairs and one million question-answering pairs to enhance its learning capabilities. The model employs innovative diffusion techniques and a mixture-of-experts architecture to optimize its performance in synthesizing detailed facial features. Experimental results show that UniF^2ace significantly outperforms existing models in both understanding and generating facial attributes.'}, 'zh': {'title': 'ç»†ç²’åº¦é¢éƒ¨ç†è§£ä¸ç”Ÿæˆçš„ç»Ÿä¸€æ¨¡å‹', 'desc': 'ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹ï¼ˆUMMsï¼‰åœ¨è®¡ç®—æœºè§†è§‰ç ”ç©¶ä¸­å±•ç°å‡ºå¼ºå¤§çš„æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨å›¾åƒç†è§£å’Œç”Ÿæˆæ–¹é¢ã€‚ç„¶è€Œï¼Œç°æœ‰çš„é¢éƒ¨é¢†åŸŸç ”ç©¶ä¸»è¦é›†ä¸­åœ¨ç²—ç•¥çš„é¢éƒ¨å±æ€§ç†è§£ä¸Šï¼Œç¼ºä¹å¤„ç†ç»†ç²’åº¦é¢éƒ¨å±æ€§çš„èƒ½åŠ›ï¼Œå¹¶ä¸”æœªèƒ½è§£å†³ç”Ÿæˆèƒ½åŠ›çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†UniF^2aceï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªä¸“é—¨é’ˆå¯¹ç»†ç²’åº¦é¢éƒ¨ç†è§£å’Œç”Ÿæˆçš„UMMã€‚é€šè¿‡æ„å»ºä¸€ä¸ªåŒ…å«13ä¸‡å¼ å›¾åƒ-æ–‡æœ¬å¯¹çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œå¹¶é‡‡ç”¨ä¸¤ç§äº’è¡¥çš„æ‰©æ•£æŠ€æœ¯å’ŒåŒå±‚ä¸“å®¶æ··åˆæ¶æ„ï¼ŒUniF^2aceåœ¨ç†è§£å’Œç”Ÿæˆä»»åŠ¡ä¸Šå‡è¡¨ç°å‡ºè‰²ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.08625', 'title': 'SegAgent: Exploring Pixel Understanding Capabilities in MLLMs by\n  Imitating Human Annotator Trajectories', 'url': 'https://huggingface.co/papers/2503.08625', 'abstract': "While MLLMs have demonstrated adequate image understanding capabilities, they still struggle with pixel-level comprehension, limiting their practical applications. Current evaluation tasks like VQA and visual grounding remain too coarse to assess fine-grained pixel comprehension accurately. Though segmentation is foundational for pixel-level understanding, existing methods often require MLLMs to generate implicit tokens, decoded through external pixel decoders. This approach disrupts the MLLM's text output space, potentially compromising language capabilities and reducing flexibility and extensibility, while failing to reflect the model's intrinsic pixel-level understanding.   Thus, we introduce the Human-Like Mask Annotation Task (HLMAT), a new paradigm where MLLMs mimic human annotators using interactive segmentation tools. Modeling segmentation as a multi-step Markov Decision Process, HLMAT enables MLLMs to iteratively generate text-based click points, achieving high-quality masks without architectural changes or implicit tokens. Through this setup, we develop SegAgent, a model fine-tuned on human-like annotation trajectories, which achieves performance comparable to state-of-the-art (SOTA) methods and supports additional tasks like mask refinement and annotation filtering.   HLMAT provides a protocol for assessing fine-grained pixel understanding in MLLMs and introduces a vision-centric, multi-step decision-making task that facilitates exploration of MLLMs' visual reasoning abilities. Our adaptations of policy improvement method StaR and PRM-guided tree search further enhance model robustness in complex segmentation tasks, laying a foundation for future advancements in fine-grained visual perception and multi-step decision-making for MLLMs.", 'score': 22, 'issue_id': 2655, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 11', 'zh': '3æœˆ11æ—¥'}, 'hash': '081b91105002410a', 'authors': ['Muzhi Zhu', 'Yuzhuo Tian', 'Hao Chen', 'Chunluan Zhou', 'Qingpei Guo', 'Yang Liu', 'Ming Yang', 'Chunhua Shen'], 'affiliations': ['Ant Group', 'Zhejiang University, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.08625.jpg', 'data': {'categories': ['#agents', '#cv', '#rl', '#reasoning', '#optimization', '#games'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'Ğ§ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ°Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ°ÑĞ¾Ğº, Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½ÑƒÑ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¹ (HLMAT), Ğ³Ğ´Ğµ MLLM Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ñ€ÑƒÑÑ‚ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°-Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ‚Ğ¾Ñ€Ğ°. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ SegAgent Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ±ĞµĞ· Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹. HLMAT Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ MLLM Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¸Ñ… Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Enhancing Pixel-Level Understanding in MLLMs with Human-Like Annotation', 'desc': "This paper addresses the limitations of Multimodal Language Models (MLLMs) in understanding images at the pixel level, which restricts their practical use. It critiques existing evaluation methods for being too broad and introduces the Human-Like Mask Annotation Task (HLMAT) to improve pixel comprehension. HLMAT allows MLLMs to simulate human-like annotation through interactive segmentation, modeled as a multi-step Markov Decision Process. The proposed SegAgent model, trained on this new task, achieves competitive performance while maintaining the MLLM's language capabilities and flexibility."}, 'zh': {'title': 'æå‡åƒç´ ç†è§£çš„åˆ›æ–°æ ‡æ³¨ä»»åŠ¡', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨åƒç´ çº§ç†è§£æ–¹é¢çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨ç»†ç²’åº¦è¯„ä¼°ä»»åŠ¡ä¸­çš„å±€é™æ€§ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„ä»»åŠ¡â€”â€”äººç±»æ ·æœ¬æ ‡æ³¨ä»»åŠ¡ï¼ˆHLMATï¼‰ï¼Œé€šè¿‡æ¨¡æ‹Ÿäººç±»æ ‡æ³¨è€…çš„æ–¹å¼ï¼Œä½¿ç”¨äº¤äº’å¼åˆ†å‰²å·¥å…·æ¥æé«˜æ¨¡å‹çš„åƒç´ ç†è§£èƒ½åŠ›ã€‚HLMATå°†åˆ†å‰²å»ºæ¨¡ä¸ºå¤šæ­¥éª¤çš„é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼Œä½¿å¾—MLLMsèƒ½å¤Ÿè¿­ä»£ç”ŸæˆåŸºäºæ–‡æœ¬çš„ç‚¹å‡»ç‚¹ï¼Œä»è€Œæ— éœ€æ”¹å˜æ¨¡å‹æ¶æ„æˆ–ä½¿ç”¨éšå¼æ ‡è®°ã€‚é€šè¿‡è¿™ä¸€æ–¹æ³•ï¼Œæˆ‘ä»¬å¼€å‘äº†SegAgentæ¨¡å‹ï¼Œå…¶æ€§èƒ½ä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸å½“ï¼Œå¹¶æ”¯æŒæ©è†œç»†åŒ–å’Œæ ‡æ³¨è¿‡æ»¤ç­‰é¢å¤–ä»»åŠ¡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.07703', 'title': 'Seedream 2.0: A Native Chinese-English Bilingual Image Generation\n  Foundation Model', 'url': 'https://huggingface.co/papers/2503.07703', 'abstract': 'Rapid advancement of diffusion models has catalyzed remarkable progress in the field of image generation. However, prevalent models such as Flux, SD3.5 and Midjourney, still grapple with issues like model bias, limited text rendering capabilities, and insufficient understanding of Chinese cultural nuances. To address these limitations, we present Seedream 2.0, a native Chinese-English bilingual image generation foundation model that excels across diverse dimensions, which adeptly manages text prompt in both Chinese and English, supporting bilingual image generation and text rendering. We develop a powerful data system that facilitates knowledge integration, and a caption system that balances the accuracy and richness for image description. Particularly, Seedream is integrated with a self-developed bilingual large language model as a text encoder, allowing it to learn native knowledge directly from massive data. This enable it to generate high-fidelity images with accurate cultural nuances and aesthetic expressions described in either Chinese or English. Beside, Glyph-Aligned ByT5 is applied for flexible character-level text rendering, while a Scaled ROPE generalizes well to untrained resolutions. Multi-phase post-training optimizations, including SFT and RLHF iterations, further improve the overall capability. Through extensive experimentation, we demonstrate that Seedream 2.0 achieves state-of-the-art performance across multiple aspects, including prompt-following, aesthetics, text rendering, and structural correctness. Furthermore, Seedream 2.0 has been optimized through multiple RLHF iterations to closely align its output with human preferences, as revealed by its outstanding ELO score. In addition, it can be readily adapted to an instruction-based image editing model, such as SeedEdit, with strong editing capability that balances instruction-following and image consistency.', 'score': 22, 'issue_id': 2654, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 10', 'zh': '3æœˆ10æ—¥'}, 'hash': '00cd4369f3c531f9', 'authors': ['Lixue Gong', 'Xiaoxia Hou', 'Fanshi Li', 'Liang Li', 'Xiaochen Lian', 'Fei Liu', 'Liyang Liu', 'Wei Liu', 'Wei Lu', 'Yichun Shi', 'Shiqi Sun', 'Yu Tian', 'Zhi Tian', 'Peng Wang', 'Xun Wang', 'Ye Wang', 'Guofeng Wu', 'Jie Wu', 'Xin Xia', 'Xuefeng Xiao', 'Linjie Yang', 'Zhonghua Zhai', 'Xinyu Zhang', 'Qi Zhang', 'Yuwei Zhang', 'Shijia Zhao', 'Jianchao Yang', 'Weilin Huang'], 'affiliations': ['Seed Vision Team, ByteDance'], 'pdf_title_img': 'assets/pdf/title_img/2503.07703.jpg', 'data': {'categories': ['#cv', '#training', '#dataset', '#optimization', '#rlhf', '#alignment', '#multimodal', '#diffusion'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Seedream 2.0: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ´Ğ²ÑƒÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'Seedream 2.0 - ÑÑ‚Ğ¾ Ğ´Ğ²ÑƒÑĞ·Ñ‹Ñ‡Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ¾Ñ‰Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ´Ğ²ÑƒÑĞ·Ñ‹Ñ‡Ğ½ÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ñ‹Ñ… Ğ½ÑĞ°Ğ½ÑĞ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Glyph-Aligned ByT5 Ğ´Ğ»Ñ Ğ³Ğ¸Ğ±ĞºĞ¾Ğ³Ğ¾ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ° Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Scaled ROPE Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ½ĞµÑ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. ĞœĞ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ SFT Ğ¸ RLHF, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¾Ğ±Ñ‰Ğ¸Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Seedream 2.0: Bridging Cultures in Image Generation', 'desc': 'This paper introduces Seedream 2.0, a bilingual image generation model designed to overcome limitations found in existing models like Flux and Midjourney. It effectively handles text prompts in both Chinese and English, allowing for culturally nuanced image generation and accurate text rendering. The model incorporates a bilingual large language model for enhanced understanding and a robust data system for knowledge integration. Extensive optimizations, including reinforcement learning from human feedback, ensure that Seedream 2.0 excels in aesthetics, prompt adherence, and overall image quality.'}, 'zh': {'title': 'åŒè¯­å›¾åƒç”Ÿæˆçš„æœªæ¥ï¼šSeedream 2.0', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†Seedream 2.0ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸­è‹±åŒè¯­çš„å›¾åƒç”ŸæˆåŸºç¡€æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ¨¡å‹åœ¨æ–‡æœ¬æ¸²æŸ“å’Œæ–‡åŒ–ç†è§£æ–¹é¢çš„ä¸è¶³ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿå¤„ç†ä¸­æ–‡å’Œè‹±æ–‡çš„æ–‡æœ¬æç¤ºï¼Œæ”¯æŒåŒè¯­å›¾åƒç”Ÿæˆï¼Œå¹¶é€šè¿‡å¼ºå¤§çš„æ•°æ®ç³»ç»Ÿå’Œæè¿°ç³»ç»Ÿæå‡å›¾åƒæè¿°çš„å‡†ç¡®æ€§å’Œä¸°å¯Œæ€§ã€‚Seedream 2.0ç»“åˆäº†è‡ªç ”çš„åŒè¯­å¤§è¯­è¨€æ¨¡å‹ï¼Œèƒ½å¤Ÿä»æµ·é‡æ•°æ®ä¸­ç›´æ¥å­¦ä¹ æœ¬åœŸçŸ¥è¯†ï¼Œç”Ÿæˆé«˜ä¿çœŸåº¦çš„å›¾åƒï¼Œå‡†ç¡®è¡¨è¾¾æ–‡åŒ–ç»†èŠ‚å’Œç¾å­¦ç‰¹å¾ã€‚æ­¤å¤–ï¼Œé€šè¿‡å¤šé˜¶æ®µçš„åè®­ç»ƒä¼˜åŒ–ï¼ŒSeedream 2.0åœ¨å¤šä¸ªæ–¹é¢å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬éµå¾ªæç¤ºã€å®¡ç¾ã€æ–‡æœ¬æ¸²æŸ“å’Œç»“æ„æ­£ç¡®æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.07860', 'title': 'Video Action Differencing', 'url': 'https://huggingface.co/papers/2503.07860', 'abstract': 'How do two individuals differ when performing the same action? In this work, we introduce Video Action Differencing (VidDiff), the novel task of identifying subtle differences between videos of the same action, which has many applications, such as coaching and skill learning. To enable development on this new task, we first create VidDiffBench, a benchmark dataset containing 549 video pairs, with human annotations of 4,469 fine-grained action differences and 2,075 localization timestamps indicating where these differences occur. Our experiments demonstrate that VidDiffBench poses a significant challenge for state-of-the-art large multimodal models (LMMs), such as GPT-4o and Qwen2-VL. By analyzing failure cases of LMMs on VidDiffBench, we highlight two key challenges for this task: localizing relevant sub-actions over two videos and fine-grained frame comparison. To overcome these, we propose the VidDiff method, an agentic workflow that breaks the task into three stages: action difference proposal, keyframe localization, and frame differencing, each stage utilizing specialized foundation models. To encourage future research in this new task, we release the benchmark at https://huggingface.co/datasets/jmhb/VidDiffBench and code at http://jmhb0.github.io/viddiff.', 'score': 21, 'issue_id': 2653, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 10', 'zh': '3æœˆ10æ—¥'}, 'hash': '76b8b9c677de83cd', 'authors': ['James Burgess', 'Xiaohan Wang', 'Yuhui Zhang', 'Anita Rau', 'Alejandro Lozano', 'Lisa Dunlap', 'Trevor Darrell', 'Serena Yeung-Levy'], 'affiliations': ['Stanford', 'UC Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2503.07860.jpg', 'data': {'categories': ['#benchmark', '#agents', '#multimodal', '#dataset', '#video'], 'emoji': 'ğŸ¥', 'ru': {'title': 'ĞĞ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€Ğ¾Ğ½Ñ‚Ğ¸Ñ€ Ğ² Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾: Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ğ½ĞºĞ¸Ñ… Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ğ¹ Ğ² Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑÑ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ - Video Action Differencing (VidDiff), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ° Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ğ½ĞºĞ¸Ñ… Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¾Ğ´Ğ¸Ğ½Ğ°ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑĞ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ VidDiffBench, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 549 Ğ¿Ğ°Ñ€ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ğ¹ Ğ² Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑÑ… Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚ĞºĞ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ ÑÑ‚Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡ĞµĞ¹. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ VidDiff, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ñ‚Ñ€ĞµÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸ĞµĞ¼ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Unveiling Subtle Differences in Action Videos with VidDiff', 'desc': 'This paper introduces Video Action Differencing (VidDiff), a new task focused on identifying subtle differences in videos depicting the same action. To support this task, the authors created VidDiffBench, a benchmark dataset with 549 video pairs and detailed annotations of action differences and localization timestamps. The study reveals that current large multimodal models struggle with this task, particularly in localizing sub-actions and performing fine-grained frame comparisons. To address these challenges, the authors propose a structured approach called VidDiff, which divides the task into three stages, each leveraging specialized foundation models for improved performance.'}, 'zh': {'title': 'è§†é¢‘åŠ¨ä½œå·®å¼‚åŒ–ï¼šè¯†åˆ«ç»†å¾®å·®åˆ«çš„æ–°æŒ‘æˆ˜', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°ä»»åŠ¡ï¼Œç§°ä¸ºè§†é¢‘åŠ¨ä½œå·®å¼‚åŒ–ï¼ˆVidDiffï¼‰ï¼Œæ—¨åœ¨è¯†åˆ«åŒä¸€åŠ¨ä½œè§†é¢‘ä¹‹é—´çš„ç»†å¾®å·®åˆ«ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬åˆ›å»ºäº†VidDiffBenchï¼Œä¸€ä¸ªåŒ…å«549å¯¹è§†é¢‘çš„åŸºå‡†æ•°æ®é›†ï¼Œæ ‡æ³¨äº†4469ä¸ªç»†ç²’åº¦åŠ¨ä½œå·®å¼‚å’Œ2075ä¸ªæ—¶é—´æˆ³ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒVidDiffBenchå¯¹ç°æœ‰çš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰æå‡ºäº†é‡å¤§æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨å±€éƒ¨åŒ–ç›¸å…³å­åŠ¨ä½œå’Œç»†ç²’åº¦å¸§æ¯”è¾ƒæ–¹é¢ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†VidDiffæ–¹æ³•ï¼Œå°†ä»»åŠ¡åˆ†ä¸ºä¸‰ä¸ªé˜¶æ®µï¼šåŠ¨ä½œå·®å¼‚æè®®ã€å…³é”®å¸§å®šä½å’Œå¸§å·®å¼‚åŒ–ï¼Œæ¯ä¸ªé˜¶æ®µéƒ½åˆ©ç”¨äº†ä¸“é—¨çš„åŸºç¡€æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.07891', 'title': 'Gemini Embedding: Generalizable Embeddings from Gemini', 'url': 'https://huggingface.co/papers/2503.07891', 'abstract': "In this report, we introduce Gemini Embedding, a state-of-the-art embedding model leveraging the power of Gemini, Google's most capable large language model. Capitalizing on Gemini's inherent multilingual and code understanding capabilities, Gemini Embedding produces highly generalizable embeddings for text spanning numerous languages and textual modalities. The representations generated by Gemini Embedding can be precomputed and applied to a variety of downstream tasks including classification, similarity, clustering, ranking, and retrieval. Evaluated on the Massive Multilingual Text Embedding Benchmark (MMTEB), which includes over one hundred tasks across 250+ languages, Gemini Embedding substantially outperforms prior state-of-the-art models, demonstrating considerable improvements in embedding quality. Achieving state-of-the-art performance across MMTEB's multilingual, English, and code benchmarks, our unified model demonstrates strong capabilities across a broad selection of tasks and surpasses specialized domain-specific models.", 'score': 19, 'issue_id': 2655, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 10', 'zh': '3æœˆ10æ—¥'}, 'hash': '149f8fc31808d626', 'authors': ['Jinhyuk Lee', 'Feiyang Chen', 'Sahil Dua', 'Daniel Cer', 'Madhuri Shanbhogue', 'Iftekhar Naim', 'Gustavo HernÃ¡ndez Ãbrego', 'Zhe Li', 'Kaifeng Chen', 'Henrique Schechter Vera', 'Xiaoqi Ren', 'Shanfeng Zhang', 'Daniel Salz', 'Michael Boratko', 'Jay Han', 'Blair Chen', 'Shuo Huang', 'Vikram Rao', 'Paul Suganthan', 'Feng Han', 'Andreas Doumanoglou', 'Nithi Gupta', 'Fedor Moiseev', 'Cathy Yip', 'Aashi Jain', 'Simon Baumgartner', 'Shahrokh Shahi', 'Frank Palma Gomez', 'Sandeep Mariserla', 'Min Choi', 'Parashar Shah', 'Sonam Goenka', 'Ke Chen', 'Ye Xia', 'Koert Chen', 'Sai Meher Karthik Duddu', 'Yichang Chen', 'Trevor Walker', 'Wenlei Zhou', 'Rakesh Ghiya', 'Zach Gleicher', 'Karan Gill', 'Zhe Dong', 'Mojtaba Seyedhosseini', 'Yunhsuan Sung', 'Raphael Hoffmann', 'Tom Duerig'], 'affiliations': ['Google'], 'pdf_title_img': 'assets/pdf/title_img/2503.07891.jpg', 'data': {'categories': ['#multimodal', '#multilingual', '#dataset', '#transfer_learning', '#benchmark', '#open_source'], 'emoji': 'ğŸŒ', 'ru': {'title': 'Gemini Embedding: ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ Ğ´Ğ»Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ ĞºĞ¾Ğ´Ğ° Ğ½Ğ° Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Gemini Embedding - Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ½Ğ° Ğ¼Ğ¾Ñ‰Ğ½Ğ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Gemini Ğ¾Ñ‚ Google. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ĞºĞ¾Ğ´Ğ° Gemini Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ñ‚ĞµĞºÑÑ‚Ğ° Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ°Ñ…. Gemini Embedding Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°Ğ¼ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ MMTEB, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰ĞµĞ¼ Ğ±Ğ¾Ğ»ĞµĞµ 100 Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ½Ğ° 250+ ÑĞ·Ñ‹ĞºĞ°Ñ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ½Ğ° Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ ĞºĞ¾Ğ´Ğ¾Ğ¼, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Unifying Multilingual Understanding with Gemini Embedding', 'desc': "Gemini Embedding is a cutting-edge embedding model that utilizes Google's advanced Gemini large language model. It excels in generating versatile embeddings for text in multiple languages and formats, thanks to its multilingual and code comprehension abilities. These embeddings can be precomputed and effectively used in various machine learning tasks such as classification, similarity, clustering, ranking, and retrieval. In evaluations on the Massive Multilingual Text Embedding Benchmark (MMTEB), Gemini Embedding significantly outperformed previous models, showcasing its superior embedding quality across a wide range of languages and tasks."}, 'zh': {'title': 'Gemini Embeddingï¼šå¤šè¯­è¨€åµŒå…¥çš„æ–°æ ‡æ†', 'desc': 'æœ¬æŠ¥å‘Šä»‹ç»äº†Gemini Embeddingï¼Œè¿™æ˜¯ä¸€ç§å…ˆè¿›çš„åµŒå…¥æ¨¡å‹ï¼Œåˆ©ç”¨äº†è°·æ­Œæœ€å¼ºå¤§çš„å¤§å‹è¯­è¨€æ¨¡å‹Geminiçš„èƒ½åŠ›ã€‚Gemini Embeddingèƒ½å¤Ÿç”Ÿæˆé«˜åº¦é€šç”¨çš„æ–‡æœ¬åµŒå…¥ï¼Œé€‚ç”¨äºå¤šç§è¯­è¨€å’Œæ–‡æœ¬å½¢å¼ï¼Œå……åˆ†åˆ©ç”¨äº†Geminiçš„å¤šè¯­è¨€å’Œä»£ç ç†è§£èƒ½åŠ›ã€‚ç”Ÿæˆçš„è¡¨ç¤ºå¯ä»¥é¢„å…ˆè®¡ç®—ï¼Œå¹¶åº”ç”¨äºåˆ†ç±»ã€ç›¸ä¼¼æ€§ã€èšç±»ã€æ’åºå’Œæ£€ç´¢ç­‰å¤šç§ä¸‹æ¸¸ä»»åŠ¡ã€‚åœ¨å¤§è§„æ¨¡å¤šè¯­è¨€æ–‡æœ¬åµŒå…¥åŸºå‡†æµ‹è¯•ï¼ˆMMTEBï¼‰ä¸­ï¼ŒGemini Embeddingæ˜¾è‘—è¶…è¶Šäº†ä¹‹å‰çš„æœ€å…ˆè¿›æ¨¡å‹ï¼Œå±•ç¤ºäº†åµŒå…¥è´¨é‡çš„æ˜¾è‘—æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.07572', 'title': 'Optimizing Test-Time Compute via Meta Reinforcement Fine-Tuning', 'url': 'https://huggingface.co/papers/2503.07572', 'abstract': "Training models to effectively use test-time compute is crucial for improving the reasoning performance of LLMs. Current methods mostly do so via fine-tuning on search traces or running RL with 0/1 outcome reward, but do these approaches efficiently utilize test-time compute? Would these approaches continue to scale as the budget improves? In this paper, we try to answer these questions. We formalize the problem of optimizing test-time compute as a meta-reinforcement learning (RL) problem, which provides a principled perspective on spending test-time compute. This perspective enables us to view the long output stream from the LLM as consisting of several episodes run at test time and leads us to use a notion of cumulative regret over output tokens as a way to measure the efficacy of test-time compute. Akin to how RL algorithms can best tradeoff exploration and exploitation over training, minimizing cumulative regret would also provide the best balance between exploration and exploitation in the token stream. While we show that state-of-the-art models do not minimize regret, one can do so by maximizing a dense reward bonus in conjunction with the outcome 0/1 reward RL. This bonus is the ''progress'' made by each subsequent block in the output stream, quantified by the change in the likelihood of eventual success. Using these insights, we develop Meta Reinforcement Fine-Tuning, or MRT, a new class of fine-tuning methods for optimizing test-time compute. MRT leads to a 2-3x relative gain in performance and roughly a 1.5x gain in token efficiency for math reasoning compared to outcome-reward RL.", 'score': 18, 'issue_id': 2653, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 10', 'zh': '3æœˆ10æ—¥'}, 'hash': '49ca445bc3322f0d', 'authors': ['Yuxiao Qu', 'Matthew Y. R. Yang', 'Amrith Setlur', 'Lewis Tunstall', 'Edward Emanuel Beeching', 'Ruslan Salakhutdinov', 'Aviral Kumar'], 'affiliations': ['Carnegie Mellon University', 'Hugging Face'], 'pdf_title_img': 'assets/pdf/title_img/2503.07572.jpg', 'data': {'categories': ['#optimization', '#training', '#rl', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ LLM Ñ‡ĞµÑ€ĞµĞ· Ğ¼ĞµÑ‚Ğ°-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼', 'desc': "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·ÑƒÑÑ‚ ÑÑ‚Ñƒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ ĞºĞ°Ğº Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¼ĞµÑ‚Ğ°-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ²Ğ²Ğ¾Ğ´Ñ Ğ¿Ğ¾Ğ½ÑÑ‚Ğ¸Ğµ ĞºÑƒĞ¼ÑƒĞ»ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ¶Ğ°Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹. ĞĞ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Meta Reinforcement Fine-Tuning (MRT), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾Ğµ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ·Ğ° 'Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑ' Ğ² Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ğº Ğ±Ğ¸Ğ½Ğ°Ñ€Ğ½Ğ¾Ğ¼Ñƒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñƒ. MRT Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹."}, 'en': {'title': 'Optimizing Test-Time Compute for Enhanced LLM Reasoning', 'desc': "This paper addresses the challenge of optimizing test-time compute for large language models (LLMs) to enhance their reasoning capabilities. It introduces a meta-reinforcement learning framework that treats the output generation process as a series of episodes, allowing for a structured approach to manage compute resources effectively. The authors propose minimizing cumulative regret over output tokens as a metric for evaluating the efficiency of test-time compute, which balances exploration and exploitation in the model's output. They present a new fine-tuning method called Meta Reinforcement Fine-Tuning (MRT), which significantly improves performance and token efficiency in mathematical reasoning tasks compared to traditional outcome-reward reinforcement learning methods."}, 'zh': {'title': 'ä¼˜åŒ–æµ‹è¯•æ—¶è®¡ç®—ï¼Œæå‡æ¨ç†æ€§èƒ½ï¼', 'desc': 'æœ¬æ–‡æ¢è®¨äº†å¦‚ä½•æœ‰æ•ˆåˆ©ç”¨æµ‹è¯•æ—¶è®¡ç®—èµ„æºæ¥æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†æ€§èƒ½ã€‚æˆ‘ä»¬å°†ä¼˜åŒ–æµ‹è¯•æ—¶è®¡ç®—çš„é—®é¢˜å½¢å¼åŒ–ä¸ºå…ƒå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰é—®é¢˜ï¼Œä»è€Œæä¾›äº†ä¸€ä¸ªç³»ç»ŸåŒ–çš„è§†è§’æ¥æ”¯é…æµ‹è¯•æ—¶è®¡ç®—ã€‚é€šè¿‡å°†LLMçš„è¾“å‡ºæµè§†ä¸ºå¤šä¸ªæµ‹è¯•æ—¶çš„å›åˆï¼Œå¹¶ä½¿ç”¨ç´¯ç§¯é—æ†¾çš„æ¦‚å¿µæ¥è¡¡é‡æµ‹è¯•æ—¶è®¡ç®—çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„å¾®è°ƒæ–¹æ³•ï¼Œç§°ä¸ºå…ƒå¼ºåŒ–å¾®è°ƒï¼ˆMRTï¼‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMRTåœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­ç›¸è¾ƒäºä¼ ç»Ÿçš„ç»“æœå¥–åŠ±RLæ–¹æ³•ï¼Œæ€§èƒ½æå‡äº†2-3å€ï¼Œä»¤ä»¤ç‰Œæ•ˆç‡æé«˜äº†çº¦1.5å€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.07604', 'title': 'Implicit Reasoning in Transformers is Reasoning through Shortcuts', 'url': 'https://huggingface.co/papers/2503.07604', 'abstract': "Test-time compute is emerging as a new paradigm for enhancing language models' complex multi-step reasoning capabilities, as demonstrated by the success of OpenAI's o1 and o3, as well as DeepSeek's R1. Compared to explicit reasoning in test-time compute, implicit reasoning is more inference-efficient, requiring fewer generated tokens. However, why does the advanced reasoning capability fail to emerge in the implicit reasoning style? In this work, we train GPT-2 from scratch on a curated multi-step mathematical reasoning dataset and conduct analytical experiments to investigate how language models perform implicit reasoning in multi-step tasks. Our findings reveal: 1) Language models can perform step-by-step reasoning and achieve high accuracy in both in-domain and out-of-domain tests via implicit reasoning. However, this capability only emerges when trained on fixed-pattern data. 2) Conversely, implicit reasoning abilities emerging from training on un<PRE_TAG>fixed-pattern data</POST_TAG> tend to overfit a specific pattern and fail to generalize further. Notably, this limitation is also observed in state-of-the-art large language models. These findings suggest that language models acquire implicit reasoning through shortcut learning, enabling strong performance on tasks with similar patterns while lacking generalization.", 'score': 17, 'issue_id': 2654, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 10', 'zh': '3æœˆ10æ—¥'}, 'hash': '313594788f663498', 'authors': ['Tianhe Lin', 'Jian Xie', 'Siyu Yuan', 'Deqing Yang'], 'affiliations': ['School of Computer Science, Fudan University', 'School of Data Science, Fudan University'], 'pdf_title_img': 'assets/pdf/title_img/2503.07604.jpg', 'data': {'categories': ['#reasoning', '#training', '#dataset', '#math', '#data'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞĞµÑĞ²Ğ½Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…: ÑĞ¸Ğ»Ğ° ÑˆĞ°Ğ±Ğ»Ğ¾Ğ½Ğ¾Ğ² Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ‚ÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ½ĞµÑĞ²Ğ½Ğ¾Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¸, Ğ½Ğ¾ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ ÑˆĞ°Ğ±Ğ»Ğ¾Ğ½Ğ¾Ğ¼. ĞŸÑ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ½ĞµÑ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ ÑˆĞ°Ğ±Ğ»Ğ¾Ğ½Ğ¾Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞºĞ»Ğ¾Ğ½Ğ½Ñ‹ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒÑÑ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¼Ñƒ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñƒ Ğ¸ Ğ½Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°Ñ‚ÑŒ. Ğ­Ñ‚Ğ¾ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ĞµÑ‚ÑÑ Ğ´Ğ°Ğ¶Ğµ Ñƒ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ° Ñ‚Ğ¾, Ñ‡Ñ‚Ğ¾ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸Ğ¾Ğ±Ñ€ĞµÑ‚Ğ°ÑÑ‚ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¸ Ğ½ĞµÑĞ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ğ¼ Ğ¿ÑƒÑ‚ÑĞ¼, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ¼ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑ‚ÑŒÑÑ Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ ÑÑ…Ğ¾Ğ¶ĞµĞ³Ğ¾ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ°, Ğ½Ğ¾ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ.'}, 'en': {'title': 'Unlocking Implicit Reasoning in Language Models', 'desc': 'This paper explores how language models, specifically GPT-2, can perform implicit reasoning in multi-step mathematical tasks. It shows that while these models can achieve high accuracy through implicit reasoning, this ability is contingent on being trained with fixed-pattern data. When trained on variable patterns, the models tend to overfit and struggle to generalize their reasoning skills. The research highlights that language models often rely on shortcut learning, which allows them to excel in specific tasks but limits their broader reasoning capabilities.'}, 'zh': {'title': 'éšå¼æ¨ç†çš„æ·å¾„ä¸å±€é™æ€§', 'desc': 'æœ¬æ–‡æ¢è®¨äº†åœ¨æµ‹è¯•æ—¶è®¡ç®—ä¸­ï¼Œéšå¼æ¨ç†ä¸æ˜¾å¼æ¨ç†çš„æ•ˆç‡å·®å¼‚ã€‚ç ”ç©¶å‘ç°ï¼Œè¯­è¨€æ¨¡å‹åœ¨å›ºå®šæ¨¡å¼æ•°æ®ä¸Šè®­ç»ƒæ—¶ï¼Œèƒ½å¤Ÿé€šè¿‡éšå¼æ¨ç†å®ç°é€æ­¥æ¨ç†å¹¶åœ¨å¤šæ­¥ä»»åŠ¡ä¸­å–å¾—é«˜å‡†ç¡®ç‡ã€‚ç›¸åï¼Œåœ¨éå›ºå®šæ¨¡å¼æ•°æ®ä¸Šè®­ç»ƒçš„éšå¼æ¨ç†èƒ½åŠ›å®¹æ˜“è¿‡æ‹Ÿåˆç‰¹å®šæ¨¡å¼ï¼Œå¯¼è‡´æ³›åŒ–èƒ½åŠ›ä¸è¶³ã€‚æ€»çš„æ¥è¯´ï¼Œè¯­è¨€æ¨¡å‹é€šè¿‡æ·å¾„å­¦ä¹ è·å¾—éšå¼æ¨ç†èƒ½åŠ›ï¼Œä½†åœ¨é¢å¯¹ä¸åŒæ¨¡å¼æ—¶è¡¨ç°ä¸ä½³ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.08605', 'title': 'Tuning-Free Multi-Event Long Video Generation via Synchronized Coupled\n  Sampling', 'url': 'https://huggingface.co/papers/2503.08605', 'abstract': 'While recent advancements in text-to-video diffusion models enable high-quality short video generation from a single prompt, generating real-world long videos in a single pass remains challenging due to limited data and high computational costs. To address this, several works propose tuning-free approaches, i.e., extending existing models for long video generation, specifically using multiple prompts to allow for dynamic and controlled content changes. However, these methods primarily focus on ensuring smooth transitions between adjacent frames, often leading to content drift and a gradual loss of semantic coherence over longer sequences. To tackle such an issue, we propose Synchronized Coupled Sampling (SynCoS), a novel inference framework that synchronizes denoising paths across the entire video, ensuring long-range consistency across both adjacent and distant frames. Our approach combines two complementary sampling strategies: reverse and optimization-based sampling, which ensure seamless local transitions and enforce global coherence, respectively. However, directly alternating between these samplings misaligns denoising trajectories, disrupting prompt guidance and introducing unintended content changes as they operate independently. To resolve this, SynCoS synchronizes them through a grounded timestep and a fixed baseline noise, ensuring fully coupled sampling with aligned denoising paths. Extensive experiments show that SynCoS significantly improves multi-event long video generation, achieving smoother transitions and superior long-range coherence, outperforming previous approaches both quantitatively and qualitatively.', 'score': 16, 'issue_id': 2653, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 11', 'zh': '3æœˆ11æ—¥'}, 'hash': '6f7bf7b6c171af43', 'authors': ['Subin Kim', 'Seoung Wug Oh', 'Jui-Hsien Wang', 'Joon-Young Lee', 'Jinwoo Shin'], 'affiliations': ['Adobe Research', 'KAIST'], 'pdf_title_img': 'assets/pdf/title_img/2503.08605.jpg', 'data': {'categories': ['#inference', '#diffusion', '#video'], 'emoji': 'ğŸ¬', 'ru': {'title': 'SynCoS: Ğ¡Ğ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Synchronized Coupled Sampling (SynCoS). Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºÑƒ Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºÑƒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ»Ğ°Ğ²Ğ½Ñ‹Ñ… Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ¾Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ğ´Ñ€Ğ°Ğ¼Ğ¸ Ğ¸ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°. SynCoS ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑˆĞ°Ğ³ Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¹ ÑˆÑƒĞ¼, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ Ğ½ĞµĞ¶ĞµĞ»Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ SynCoS Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸ÑĞ¼Ğ¸, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ ĞºĞ°Ğº ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾, Ñ‚Ğ°Ğº Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾.'}, 'en': {'title': 'Achieving Long-Range Coherence in Video Generation with SynCoS', 'desc': 'This paper introduces Synchronized Coupled Sampling (SynCoS), a new framework for generating long videos from text prompts while maintaining semantic coherence. Traditional methods struggle with content drift and frame transitions, especially over extended sequences. SynCoS addresses these issues by synchronizing denoising paths across the video, combining reverse and optimization-based sampling strategies for better local and global coherence. Experimental results demonstrate that SynCoS enhances the quality of multi-event long video generation, outperforming existing techniques in both smoothness and consistency.'}, 'zh': {'title': 'åŒæ­¥è€¦åˆé‡‡æ ·ï¼šæå‡é•¿è§†é¢‘ç”Ÿæˆçš„ä¸€è‡´æ€§', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¨ç†æ¡†æ¶ï¼Œç§°ä¸ºåŒæ­¥è€¦åˆé‡‡æ ·ï¼ˆSynCoSï¼‰ï¼Œæ—¨åœ¨è§£å†³é•¿è§†é¢‘ç”Ÿæˆä¸­çš„ä¸€è‡´æ€§é—®é¢˜ã€‚é€šè¿‡åŒæ­¥å»å™ªè·¯å¾„ï¼ŒSynCoSç¡®ä¿äº†ç›¸é‚»å¸§å’Œè¿œç¨‹å¸§ä¹‹é—´çš„é•¿èŒƒå›´ä¸€è‡´æ€§ã€‚è¯¥æ–¹æ³•ç»“åˆäº†åå‘é‡‡æ ·å’ŒåŸºäºä¼˜åŒ–çš„é‡‡æ ·ç­–ç•¥ï¼Œä»¥å®ç°å±€éƒ¨å¹³æ»‘è¿‡æ¸¡å’Œå…¨å±€ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSynCoSåœ¨å¤šäº‹ä»¶é•¿è§†é¢‘ç”Ÿæˆæ–¹é¢æ˜¾è‘—ä¼˜äºä¹‹å‰çš„æ–¹æ³•ï¼Œæä¾›äº†æ›´å¹³æ»‘çš„è¿‡æ¸¡å’Œæ›´å¥½çš„é•¿èŒƒå›´è¯­ä¹‰ä¸€è‡´æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.08619', 'title': 'LightGen: Efficient Image Generation through Knowledge Distillation and\n  Direct Preference Optimization', 'url': 'https://huggingface.co/papers/2503.08619', 'abstract': 'Recent advances in text-to-image generation have primarily relied on extensive datasets and parameter-heavy architectures. These requirements severely limit accessibility for researchers and practitioners who lack substantial computational resources. In this paper, we introduce \\model, an efficient training paradigm for image generation models that uses knowledge distillation (KD) and Direct Preference Optimization (DPO). Drawing inspiration from the success of data KD techniques widely adopted in Multi-Modal Large Language Models (MLLMs), LightGen distills knowledge from state-of-the-art (SOTA) text-to-image models into a compact Masked Autoregressive (MAR) architecture with only 0.7B parameters. Using a compact <PRE_TAG>synthetic dataset</POST_TAG> of just 2M high-quality images generated from varied captions, we demonstrate that data diversity significantly outweighs data volume in determining model performance. This strategy dramatically reduces computational demands and reduces pre-training time from potentially thousands of GPU-days to merely 88 GPU-days. Furthermore, to address the inherent shortcomings of synthetic data, particularly poor high-frequency details and spatial inaccuracies, we integrate the DPO technique that refines image fidelity and positional accuracy. Comprehensive experiments confirm that LightGen achieves image generation quality comparable to SOTA models while significantly reducing computational resources and expanding accessibility for resource-constrained environments. Code is available at https://github.com/XianfengWu01/LightGen', 'score': 15, 'issue_id': 2655, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 11', 'zh': '3æœˆ11æ—¥'}, 'hash': '1645a4236e6d91b6', 'authors': ['Xianfeng Wu', 'Yajing Bai', 'Haoze Zheng', 'Harold Haodong Chen', 'Yexin Liu', 'Zihao Wang', 'Xuran Ma', 'Wen-Jie Shu', 'Xianzu Wu', 'Harry Yang', 'Ser-Nam Lim'], 'affiliations': ['Everlyn AI', 'The Hong Kong University of Science and Technology', 'University of Central Florida'], 'pdf_title_img': 'assets/pdf/title_img/2503.08619.jpg', 'data': {'categories': ['#training', '#dataset', '#synthetic', '#architecture', '#optimization'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµÑÑƒÑ€ÑĞ°Ğ¼Ğ¸', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ LightGen - ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Direct Preference Optimization. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ñ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸ĞµĞ¹ Ğ²ÑĞµĞ³Ğ¾ Ğ² 0.7 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· 2 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ²Ñ€ĞµĞ¼Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ¾ 88 GPU-Ğ´Ğ½ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ LightGen Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ¾Ğ³Ğ¾ Ñ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ¿Ñ€Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ°Ñ….'}, 'en': {'title': 'Efficient Image Generation with LightGen: Less is More!', 'desc': 'This paper presents LightGen, a new approach to text-to-image generation that focuses on efficiency and accessibility. It utilizes knowledge distillation (KD) and Direct Preference Optimization (DPO) to create a compact model with only 0.7 billion parameters, making it less demanding on computational resources. By training on a small but diverse synthetic dataset of 2 million images, the authors show that the variety of data is more important than sheer volume for model performance. The results indicate that LightGen can produce high-quality images comparable to state-of-the-art models while significantly reducing training time and resource requirements.'}, 'zh': {'title': 'é«˜æ•ˆå›¾åƒç”Ÿæˆï¼Œèµ„æºå‹å¥½ï¼', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºLightGençš„é«˜æ•ˆå›¾åƒç”Ÿæˆæ¨¡å‹è®­ç»ƒèŒƒå¼ï¼Œåˆ©ç”¨çŸ¥è¯†è’¸é¦ï¼ˆKDï¼‰å’Œç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ã€‚è¯¥æ–¹æ³•ä»æœ€å…ˆè¿›çš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ä¸­æå–çŸ¥è¯†ï¼Œæ„å»ºäº†ä¸€ä¸ªä»…æœ‰0.7äº¿å‚æ•°çš„ç´§å‡‘å‹è‡ªå›å½’æ¶æ„ã€‚é€šè¿‡ä½¿ç”¨ä»…200ä¸‡å¼ é«˜è´¨é‡åˆæˆå›¾åƒçš„æ•°æ®é›†ï¼Œç ”ç©¶è¡¨æ˜æ•°æ®çš„å¤šæ ·æ€§å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“è¿œå¤§äºæ•°æ®çš„æ•°é‡ã€‚LightGenæ˜¾è‘—é™ä½äº†è®¡ç®—éœ€æ±‚ï¼Œå°†é¢„è®­ç»ƒæ—¶é—´ä»æ•°åƒä¸ªGPUå¤©ç¼©çŸ­è‡³ä»…88ä¸ªGPUå¤©ï¼ŒåŒæ—¶ä¿æŒäº†ä¸æœ€å…ˆè¿›æ¨¡å‹ç›¸å½“çš„å›¾åƒç”Ÿæˆè´¨é‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.08686', 'title': 'OmniMamba: Efficient and Unified Multimodal Understanding and Generation\n  via State Space Models', 'url': 'https://huggingface.co/papers/2503.08686', 'abstract': "Recent advancements in unified multimodal understanding and visual generation (or multimodal generation) models have been hindered by their quadratic computational complexity and dependence on large-scale training data. We present OmniMamba, the first linear-architecture-based multimodal generation model that generates both text and images through a unified next-token prediction paradigm. The model fully leverages Mamba-2's high computational and memory efficiency, extending its capabilities from text generation to multimodal generation. To address the data inefficiency of existing unified models, we propose two key innovations: (1) decoupled vocabularies to guide modality-specific generation, and (2) task-specific LoRA for parameter-efficient adaptation. Furthermore, we introduce a decoupled two-stage training strategy to mitigate data imbalance between two tasks. Equipped with these techniques, OmniMamba achieves competitive performance with JanusFlow while surpassing Show-o across benchmarks, despite being trained on merely 2M image-text pairs, which is 1,000 times fewer than Show-o. Notably, OmniMamba stands out with outstanding inference efficiency, achieving up to a 119.2 times speedup and 63% GPU memory reduction for long-sequence generation compared to Transformer-based counterparts. Code and models are released at https://github.com/hustvl/OmniMamba", 'score': 13, 'issue_id': 2655, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 11', 'zh': '3æœˆ11æ—¥'}, 'hash': '7bb3cd796d69dc2c', 'authors': ['Jialv Zou', 'Bencheng Liao', 'Qian Zhang', 'Wenyu Liu', 'Xinggang Wang'], 'affiliations': ['Horizon Robotics', 'Institute of Artificial Intelligence, Huazhong University of Science & Technology', 'School of EIC, Huazhong University of Science & Technology'], 'pdf_title_img': 'assets/pdf/title_img/2503.08686.jpg', 'data': {'categories': ['#training', '#inference', '#multimodal', '#architecture', '#open_source', '#optimization'], 'emoji': 'ğŸ¦‹', 'ru': {'title': 'OmniMamba: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ¾Ğ¹', 'desc': 'OmniMamba - ÑÑ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ°Ğº Ñ‚ĞµĞºÑÑ‚, Ñ‚Ğ°Ğº Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· ĞµĞ´Ğ¸Ğ½ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Mamba-2, Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑÑ ĞµĞµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ´Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ½ĞµÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ğ¸ Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ-ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½ÑƒÑ LoRA Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¼Ğ°Ğ»Ñ‹Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². OmniMamba Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ğ±ÑƒĞ´ÑƒÑ‡Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ²ÑĞµĞ³Ğ¾ Ğ½Ğ° 2 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ñ… Ğ¿Ğ°Ñ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ-Ñ‚ĞµĞºÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ² 1000 Ñ€Ğ°Ğ· Ğ¼ĞµĞ½ÑŒÑˆĞµ, Ñ‡ĞµĞ¼ Ñƒ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ².'}, 'en': {'title': 'OmniMamba: Efficient Multimodal Generation with Linear Architecture', 'desc': 'OmniMamba is a groundbreaking multimodal generation model that efficiently creates both text and images using a linear architecture. It introduces a unified next-token prediction approach, which significantly reduces computational complexity and memory usage compared to traditional models. The model employs innovative techniques like decoupled vocabularies for specific modality guidance and task-specific LoRA for efficient parameter adaptation. With a unique two-stage training strategy, OmniMamba achieves impressive performance on benchmarks while requiring far less training data, demonstrating remarkable speed and memory efficiency during inference.'}, 'zh': {'title': 'OmniMambaï¼šé«˜æ•ˆçš„å¤šæ¨¡æ€ç”Ÿæˆæ–°é€‰æ‹©', 'desc': 'OmniMambaæ˜¯ä¸€ç§æ–°å‹çš„å¤šæ¨¡æ€ç”Ÿæˆæ¨¡å‹ï¼Œé‡‡ç”¨çº¿æ€§æ¶æ„ï¼Œèƒ½å¤ŸåŒæ—¶ç”Ÿæˆæ–‡æœ¬å’Œå›¾åƒã€‚è¯¥æ¨¡å‹é€šè¿‡ç»Ÿä¸€çš„ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹èŒƒå¼ï¼Œå……åˆ†åˆ©ç”¨äº†Mamba-2çš„é«˜æ•ˆè®¡ç®—å’Œå†…å­˜æ€§èƒ½ã€‚ä¸ºäº†æé«˜æ•°æ®åˆ©ç”¨æ•ˆç‡ï¼ŒOmniMambaå¼•å…¥äº†è§£è€¦è¯æ±‡å’Œä»»åŠ¡ç‰¹å®šçš„LoRAæŠ€æœ¯ï¼Œå¹¶é‡‡ç”¨äº†ä¸¤é˜¶æ®µçš„è®­ç»ƒç­–ç•¥æ¥è§£å†³ä»»åŠ¡é—´çš„æ•°æ®ä¸å¹³è¡¡é—®é¢˜ã€‚æœ€ç»ˆï¼ŒOmniMambaåœ¨ç”Ÿæˆæ•ˆç‡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œç›¸æ¯”äºä¼ ç»Ÿçš„Transformeræ¨¡å‹ï¼Œæ¨ç†é€Ÿåº¦æé«˜äº†119.2å€ï¼ŒGPUå†…å­˜å‡å°‘äº†63%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.08644', 'title': 'Exploiting Instruction-Following Retrievers for Malicious Information\n  Retrieval', 'url': 'https://huggingface.co/papers/2503.08644', 'abstract': 'Instruction-following retrievers have been widely adopted alongside LLMs in real-world applications, but little work has investigated the safety risks surrounding their increasing search capabilities. We empirically study the ability of retrievers to satisfy malicious queries, both when used directly and when used in a retrieval augmented generation-based setup. Concretely, we investigate six leading retrievers, including NV-Embed and LLM2Vec, and find that given malicious requests, most retrievers can (for >50% of queries) select relevant harmful passages. For example, LLM2Vec correctly selects passages for 61.35% of our malicious queries. We further uncover an emerging risk with instruction-following retrievers, where highly relevant harmful information can be surfaced by exploiting their instruction-following capabilities. Finally, we show that even safety-aligned LLMs, such as Llama3, can satisfy malicious requests when provided with harmful retrieved passages in-context. In summary, our findings underscore the malicious misuse risks associated with increasing retriever capability.', 'score': 12, 'issue_id': 2666, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 11', 'zh': '3æœˆ11æ—¥'}, 'hash': '4526f13d2d98134f', 'authors': ['Parishad BehnamGhader', 'Nicholas Meade', 'Siva Reddy'], 'affiliations': ['Canada CIFAR AI Chair', 'McGill University', 'Mila Quebec AI Institute'], 'pdf_title_img': 'assets/pdf/title_img/2503.08644.jpg', 'data': {'categories': ['#multimodal', '#security', '#rag', '#ethics'], 'emoji': 'ğŸ•µï¸', 'ru': {'title': 'Ğ¡ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ ÑƒĞ³Ñ€Ğ¾Ğ·Ñ‹ Ğ² ÑƒĞ¼Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ñ€ĞµÑ‚Ñ€Ğ¸Ğ²ĞµÑ€Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼Ñ‹Ñ… Ğ²Ğ¼ĞµÑÑ‚Ğµ Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¸Ğµ Ñ€ĞµÑ‚Ñ€Ğ¸Ğ²ĞµÑ€Ñ‹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹ ÑƒĞ´Ğ¾Ğ²Ğ»ĞµÑ‚Ğ²Ğ¾Ñ€ÑÑ‚ÑŒ Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ñ‹Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹, Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°Ñ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ğµ Ğ¾Ñ‚Ñ€Ñ‹Ğ²ĞºĞ¸ Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ’Ñ‹ÑĞ²Ğ»ĞµĞ½ Ñ€Ğ¸ÑĞº, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼Ğ¸ Ğ´Ğ»Ñ Ñ€ĞµÑ‚Ñ€Ğ¸Ğ²ĞµÑ€Ğ¾Ğ². Ğ”Ğ°Ğ¶Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ½Ğ¾Ğ¹ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ²Ñ‹Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚ Ğ¿Ñ€Ğ¸ Ğ½Ğ°Ğ»Ğ¸Ñ‡Ğ¸Ğ¸ Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ñ… Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ.'}, 'en': {'title': 'Uncovering the Risks of Powerful Retrievers in Malicious Queries', 'desc': 'This paper investigates the safety risks of instruction-following retrievers used with large language models (LLMs) in real-world applications. The authors analyze six prominent retrievers, revealing that they can often retrieve harmful content in response to malicious queries, with LLM2Vec achieving a 61.35% success rate. Additionally, the study highlights a concerning trend where instruction-following capabilities can inadvertently expose users to dangerous information. The findings emphasize the need for caution as the capabilities of retrievers grow, as even safety-aligned LLMs can inadvertently assist in fulfilling harmful requests.'}, 'zh': {'title': 'æ£€ç´¢å™¨èƒ½åŠ›æå‡å¸¦æ¥çš„å®‰å…¨éšæ‚£', 'desc': 'æœ¬æ–‡ç ”ç©¶äº†æŒ‡ä»¤è·Ÿéšæ£€ç´¢å™¨åœ¨å¤„ç†æ¶æ„æŸ¥è¯¢æ—¶çš„å®‰å…¨é£é™©ã€‚æˆ‘ä»¬åˆ†æäº†å…­ç§é¢†å…ˆçš„æ£€ç´¢å™¨ï¼ŒåŒ…æ‹¬NV-Embedå’ŒLLM2Vecï¼Œå‘ç°å¤§å¤šæ•°æ£€ç´¢å™¨åœ¨è¶…è¿‡50%çš„æ¶æ„æŸ¥è¯¢ä¸­èƒ½å¤Ÿé€‰æ‹©ç›¸å…³çš„æœ‰å®³å†…å®¹ã€‚ç‰¹åˆ«æ˜¯ï¼ŒLLM2Vecåœ¨61.35%çš„æ¶æ„æŸ¥è¯¢ä¸­æ­£ç¡®é€‰æ‹©äº†æœ‰å®³æ®µè½ã€‚æ­¤å¤–ï¼Œå³ä½¿æ˜¯å®‰å…¨å¯¹é½çš„LLMï¼Œå¦‚Llama3ï¼Œåœ¨æä¾›æœ‰å®³æ£€ç´¢æ®µè½çš„æƒ…å†µä¸‹ä¹Ÿèƒ½æ»¡è¶³æ¶æ„è¯·æ±‚ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.06940', 'title': 'CineBrain: A Large-Scale Multi-Modal Brain Dataset During Naturalistic\n  Audiovisual Narrative Processing', 'url': 'https://huggingface.co/papers/2503.06940', 'abstract': "In this paper, we introduce CineBrain, the first large-scale dataset featuring simultaneous EEG and fMRI recordings during dynamic audiovisual stimulation. Recognizing the complementary strengths of EEG's high temporal resolution and fMRI's deep-brain spatial coverage, CineBrain provides approximately six hours of narrative-driven content from the popular television series The Big Bang Theory for each of six participants. Building upon this unique dataset, we propose CineSync, an innovative multimodal decoding framework integrates a Multi-Modal Fusion Encoder with a diffusion-based Neural Latent Decoder. Our approach effectively fuses EEG and fMRI signals, significantly improving the reconstruction quality of complex audiovisual stimuli. To facilitate rigorous evaluation, we introduce Cine-Benchmark, a comprehensive evaluation protocol that assesses reconstructions across semantic and perceptual dimensions. Experimental results demonstrate that CineSync achieves state-of-the-art video reconstruction performance and highlight our initial success in combining fMRI and EEG for reconstructing both video and audio stimuli. Project Page: https://jianxgao.github.io/CineBrain.", 'score': 11, 'issue_id': 2664, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 10', 'zh': '3æœˆ10æ—¥'}, 'hash': '10b773e3c142acbe', 'authors': ['Jianxiong Gao', 'Yichang Liu', 'Baofeng Yang', 'Jianfeng Feng', 'Yanwei Fu'], 'affiliations': ['Fudan University'], 'pdf_title_img': 'assets/pdf/title_img/2503.06940.jpg', 'data': {'categories': ['#benchmark', '#multimodal', '#diffusion', '#dataset', '#video', '#audio'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ§Ñ‚ĞµĞ½Ğ¸Ğµ Ğ¼Ñ‹ÑĞ»ĞµĞ¹: Ğ¾Ñ‚ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğº Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ·Ğ²ÑƒĞºÑƒ', 'desc': 'CineBrain - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¹ Ğ·Ğ°Ğ¿Ğ¸ÑĞ¸ Ğ­Ğ­Ğ“ Ğ¸ Ñ„ĞœĞ Ğ¢ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¿Ñ€Ğ¾ÑĞ¼Ğ¾Ñ‚Ñ€Ğ° Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ CineSync - Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ. CineSync ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ­Ğ­Ğ“ Ğ¸ Ñ„ĞœĞ Ğ¢, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ‚Ğ¸Ğ¼ÑƒĞ»Ğ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ CineSync Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ñ‹ Ğ´Ğ»Ñ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ ĞºĞ°Ğº Ğ²Ğ¸Ğ´ĞµĞ¾, Ñ‚Ğ°Ğº Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾ ÑÑ‚Ğ¸Ğ¼ÑƒĞ»Ğ¾Ğ² Ğ¸Ğ· Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ².'}, 'en': {'title': 'CineBrain: Uniting EEG and fMRI for Enhanced Audiovisual Reconstruction', 'desc': 'This paper presents CineBrain, a groundbreaking dataset that combines EEG and fMRI recordings while participants watch dynamic audiovisual content. By leveraging the fast response time of EEG and the detailed spatial information from fMRI, CineBrain offers a rich resource for studying brain activity during complex stimuli. The authors introduce CineSync, a novel framework that merges these two modalities to enhance the reconstruction of audiovisual experiences. Their results show that CineSync outperforms previous methods in reconstructing video and audio, marking a significant advancement in multimodal brain signal analysis.'}, 'zh': {'title': 'CineBrainï¼šå¤šæ¨¡æ€è§£ç çš„æ–°çªç ´', 'desc': 'æœ¬æ–‡ä»‹ç»äº†CineBrainï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå¤§è§„æ¨¡çš„æ•°æ®é›†ï¼ŒåŒ…å«åœ¨åŠ¨æ€è§†å¬åˆºæ¿€ä¸‹åŒæ—¶è®°å½•çš„EEGå’ŒfMRIæ•°æ®ã€‚CineBrainåˆ©ç”¨EEGçš„é«˜æ—¶é—´åˆ†è¾¨ç‡å’ŒfMRIçš„æ·±è„‘ç©ºé—´è¦†ç›–ä¼˜åŠ¿ï¼Œä¸ºå…­åå‚ä¸è€…æä¾›äº†çº¦å…­å°æ—¶çš„ã€Šç”Ÿæ´»å¤§çˆ†ç‚¸ã€‹å™äº‹å†…å®¹ã€‚åŸºäºè¿™ä¸€ç‹¬ç‰¹æ•°æ®é›†ï¼Œæˆ‘ä»¬æå‡ºäº†CineSyncï¼Œè¿™æ˜¯ä¸€ç§åˆ›æ–°çš„å¤šæ¨¡æ€è§£ç æ¡†æ¶ï¼Œç»“åˆäº†å¤šæ¨¡æ€èåˆç¼–ç å™¨å’ŒåŸºäºæ‰©æ•£çš„ç¥ç»æ½œåœ¨è§£ç å™¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCineSyncåœ¨è§†é¢‘é‡å»ºæ€§èƒ½ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ï¼ŒæˆåŠŸç»“åˆäº†fMRIå’ŒEEGæ¥é‡å»ºè§†é¢‘å’ŒéŸ³é¢‘åˆºæ¿€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.07587', 'title': 'Robusto-1 Dataset: Comparing Humans and VLMs on real out-of-distribution\n  Autonomous Driving VQA from Peru', 'url': 'https://huggingface.co/papers/2503.07587', 'abstract': 'As multimodal foundational models start being deployed experimentally in Self-Driving cars, a reasonable question we ask ourselves is how similar to humans do these systems respond in certain driving situations -- especially those that are out-of-distribution? To study this, we create the Robusto-1 dataset that uses dashcam video data from Peru, a country with one of the worst (aggressive) drivers in the world, a high traffic index, and a high ratio of bizarre to non-bizarre street objects likely never seen in training. In particular, to preliminarly test at a cognitive level how well Foundational Visual Language Models (VLMs) compare to Humans in Driving, we move away from bounding boxes, segmentation maps, occupancy maps or trajectory estimation to multi-modal Visual Question Answering (VQA) comparing both humans and machines through a popular method in systems neuroscience known as Representational Similarity Analysis (RSA). Depending on the type of questions we ask and the answers these systems give, we will show in what cases do VLMs and Humans converge or diverge allowing us to probe on their cognitive alignment. We find that the degree of alignment varies significantly depending on the type of questions asked to each type of system (Humans vs VLMs), highlighting a gap in their alignment.', 'score': 9, 'issue_id': 2667, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 10', 'zh': '3æœˆ10æ—¥'}, 'hash': '274531da9b4c33d1', 'authors': ['Dunant Cusipuma', 'David Ortega', 'Victor Flores-Benites', 'Arturo Deza'], 'affiliations': ['Artificio', 'Universidad de Ingeneria Tecnologia (UTEC)'], 'pdf_title_img': 'assets/pdf/title_img/2503.07587.jpg', 'data': {'categories': ['#video', '#alignment', '#interpretability', '#multimodal', '#dataset'], 'emoji': 'ğŸš—', 'ru': {'title': 'Ğ§ĞµĞ»Ğ¾Ğ²ĞµĞº vs Ğ˜Ğ˜: ĞºÑ‚Ğ¾ Ğ»ÑƒÑ‡ÑˆĞµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°ĞµÑ‚ Ğ½ĞµĞ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğµ ÑĞ¸Ñ‚ÑƒĞ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ´Ğ¾Ñ€Ğ¾Ğ³Ğµ?', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ÑÑ‰ĞµĞµ Ñ€ĞµĞ°ĞºÑ†Ğ¸Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ»ÑĞ´ĞµĞ¹ Ğ² Ğ½ĞµĞ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ñ… ÑĞ¸Ñ‚ÑƒĞ°Ñ†Ğ¸ÑÑ… Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Robusto-1 Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ²Ğ¸Ğ´ĞµĞ¾Ñ€ĞµĞ³Ğ¸ÑÑ‚Ñ€Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ² ĞŸĞµÑ€Ñƒ, ÑÑ‚Ñ€Ğ°Ğ½Ğµ Ñ Ğ°Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğ¼ ÑÑ‚Ğ¸Ğ»ĞµĞ¼ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ñ€ĞµĞ¿Ñ€ĞµĞ·ĞµĞ½Ñ‚Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ° (RSA), Ğ¾Ğ½Ğ¸ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ»Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (VLM) Ğ¸ Ğ»ÑĞ´ĞµĞ¹ Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¾ Ğ´Ğ¾Ñ€Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑĞ¸Ñ‚ÑƒĞ°Ñ†Ğ¸ÑÑ…. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑÑ‚ĞµĞ¿ĞµĞ½ÑŒ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¸ Ğ»ÑĞ´ÑŒĞ¼Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ²Ğ°Ñ€ÑŒĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ñ‚Ğ¸Ğ¿Ğ° Ğ·Ğ°Ğ´Ğ°Ğ²Ğ°ĞµĞ¼Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ².'}, 'en': {'title': 'Assessing Human-Like Responses in Self-Driving AI', 'desc': 'This paper investigates how well multimodal foundational models, specifically Visual Language Models (VLMs), perform in driving situations compared to humans, particularly in challenging environments like Peru. The authors introduce the Robusto-1 dataset, which includes dashcam footage from a region known for aggressive driving and unusual street objects. They employ a method called Visual Question Answering (VQA) and Representational Similarity Analysis (RSA) to assess cognitive alignment between humans and VLMs. The findings reveal that the alignment between human responses and VLMs varies significantly based on the types of questions posed, indicating a notable gap in their cognitive processing.'}, 'zh': {'title': 'æ¢ç´¢è‡ªåŠ¨é©¾é©¶ä¸­çš„äººæœºè®¤çŸ¥å·®è·', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹åœ¨è‡ªåŠ¨é©¾é©¶æ±½è½¦ä¸­çš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚é©¾é©¶åœºæ™¯ä¸‹çš„è¡¨ç°ã€‚æˆ‘ä»¬åˆ›å»ºäº†Robusto-1æ•°æ®é›†ï¼Œä½¿ç”¨æ¥è‡ªç§˜é²çš„è¡Œè½¦è®°å½•ä»ªè§†é¢‘æ•°æ®ï¼Œä»¥æµ‹è¯•åŸºç¡€è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ä¸äººç±»åœ¨é©¾é©¶ä¸­çš„è®¤çŸ¥æ°´å¹³ã€‚é€šè¿‡å¤šæ¨¡æ€è§†è§‰é—®ç­”ï¼ˆVQAï¼‰æ–¹æ³•ï¼Œæˆ‘ä»¬æ¯”è¾ƒäº†äººç±»å’Œæœºå™¨çš„ååº”ï¼Œå¹¶ä½¿ç”¨è¡¨å¾ç›¸ä¼¼æ€§åˆ†æï¼ˆRSAï¼‰æ¥è¯„ä¼°å®ƒä»¬çš„è®¤çŸ¥ä¸€è‡´æ€§ã€‚ç ”ç©¶å‘ç°ï¼Œä¸åŒç±»å‹çš„é—®é¢˜ä¼šå¯¼è‡´VLMså’Œäººç±»åœ¨ååº”ä¸Šçš„æ˜¾è‘—å·®å¼‚ï¼Œæ­ç¤ºäº†å®ƒä»¬ä¹‹é—´çš„è®¤çŸ¥å·®è·ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.08307', 'title': '^RFLAV: Rolling Flow matching for infinite Audio Video generation', 'url': 'https://huggingface.co/papers/2503.08307', 'abstract': 'Joint audio-video (AV) generation is still a significant challenge in generative AI, primarily due to three critical requirements: quality of the generated samples, seamless multimodal synchronization and temporal coherence, with audio tracks that match the visual data and vice versa, and limitless video duration. In this paper, we present , a novel transformer-based architecture that addresses all the key challenges of AV generation. We explore three distinct cross modality interaction modules, with our lightweight temporal fusion module emerging as the most effective and computationally efficient approach for aligning audio and visual modalities. Our experimental results demonstrate that  outperforms existing state-of-the-art models in multimodal AV generation tasks. Our code and checkpoints are available at https://github.com/ErgastiAlex/R-FLAV.', 'score': 7, 'issue_id': 2660, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 11', 'zh': '3æœˆ11æ—¥'}, 'hash': '5b49ac0313e68c7e', 'authors': ['Alex Ergasti', 'Giuseppe Gabriele Tarollo', 'Filippo Botti', 'Tomaso Fontanini', 'Claudio Ferrari', 'Massimo Bertozzi', 'Andrea Prati'], 'affiliations': ['University of Parma, Department of Engineering and Architecture. Parma, Italy', 'University of Siena, Department of Information engineering and mathematics. Siena, Italy'], 'pdf_title_img': 'assets/pdf/title_img/2503.08307.jpg', 'data': {'categories': ['#video', '#architecture', '#multimodal', '#audio'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾-Ğ²Ğ¸Ğ´ĞµĞ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ , Ğ½Ğ¾Ğ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ğ´Ğ»Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ€ĞµÑˆĞ°ĞµÑ‚ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ AV-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸: ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑÑĞ¼Ğ¿Ğ»Ğ¾Ğ², ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ Ñ‚Ñ€Ğ¸ Ğ¼Ğ¾Ğ´ÑƒĞ»Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸, Ğ²Ñ‹Ğ´ĞµĞ»ÑÑ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ ĞºĞ°Ğº Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾  Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ SOTA-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ AV-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'Transforming Audio-Video Generation with Efficient Alignment', 'desc': 'This paper addresses the challenges of joint audio-video generation in generative AI, focusing on quality, synchronization, and temporal coherence. The authors introduce a novel transformer-based architecture that effectively aligns audio and visual data. They explore three cross modality interaction modules, highlighting a lightweight temporal fusion module as the most efficient solution. Experimental results show that their approach surpasses existing state-of-the-art models in multimodal AV generation tasks.'}, 'zh': {'title': 'çªç ´éŸ³é¢‘-è§†é¢‘ç”Ÿæˆçš„å…³é”®æŒ‘æˆ˜', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†è”åˆéŸ³é¢‘-è§†é¢‘ç”Ÿæˆçš„æŒ‘æˆ˜ï¼Œä¸»è¦åŒ…æ‹¬ç”Ÿæˆæ ·æœ¬çš„è´¨é‡ã€å¤šæ¨¡æ€çš„æ— ç¼åŒæ­¥å’Œæ—¶é—´ä¸€è‡´æ€§ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„åŸºäºå˜æ¢å™¨çš„æ¶æ„ï¼Œæ—¨åœ¨è§£å†³è¿™äº›å…³é”®é—®é¢˜ã€‚ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢äº†ä¸‰ç§ä¸åŒçš„è·¨æ¨¡æ€äº¤äº’æ¨¡å—ï¼Œå…¶ä¸­è½»é‡çº§çš„æ—¶é—´èåˆæ¨¡å—è¢«è¯æ˜æ˜¯å¯¹é½éŸ³é¢‘å’Œè§†è§‰æ¨¡æ€çš„æœ€æœ‰æ•ˆå’Œè®¡ç®—é«˜æ•ˆçš„æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šæ¨¡æ€éŸ³é¢‘-è§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸­ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.08685', 'title': '"Principal Components" Enable A New Language of Images', 'url': 'https://huggingface.co/papers/2503.08685', 'abstract': 'We introduce a novel visual tokenization framework that embeds a provable PCA-like structure into the latent token space. While existing visual tokenizers primarily optimize for reconstruction fidelity, they often neglect the structural properties of the latent space -- a critical factor for both interpretability and downstream tasks. Our method generates a 1D causal token sequence for images, where each successive token contributes non-overlapping information with mathematically guaranteed decreasing explained variance, analogous to principal component analysis. This structural constraint ensures the tokenizer extracts the most salient visual features first, with each subsequent token adding diminishing yet complementary information. Additionally, we identified and resolved a semantic-spectrum coupling effect that causes the unwanted entanglement of high-level semantic content and low-level spectral details in the tokens by leveraging a diffusion decoder. Experiments demonstrate that our approach achieves state-of-the-art reconstruction performance and enables better interpretability to align with the human vision system. Moreover, auto-regressive models trained on our token sequences achieve performance comparable to current state-of-the-art methods while requiring fewer tokens for training and inference.', 'score': 6, 'issue_id': 2654, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 11', 'zh': '3æœˆ11æ—¥'}, 'hash': 'a013cfdc2d1e9d7c', 'authors': ['Xin Wen', 'Bingchen Zhao', 'Ismail Elezi', 'Jiankang Deng', 'Xiaojuan Qi'], 'affiliations': ['Imperial College London', 'Noahs Ark Lab', 'University of Edinburgh', 'University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2503.08685.jpg', 'data': {'categories': ['#training', '#cv', '#interpretability', '#diffusion', '#architecture', '#optimization'], 'emoji': 'ğŸ§©', 'ru': {'title': 'Ğ¡Ñ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ: ĞŸĞš-Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ´Ğ¾ĞºĞ°Ğ·ÑƒĞµĞ¼ÑƒÑ PCA-Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ğ´Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½ÑƒÑ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½ÑƒÑ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ³Ğ´Ğµ ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½ Ğ²Ğ½Ğ¾ÑĞ¸Ñ‚ Ğ½ĞµĞ¿ĞµÑ€ĞµĞºÑ€Ñ‹Ğ²Ğ°ÑÑ‰ÑƒÑÑÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ ÑƒĞ±Ñ‹Ğ²Ğ°ÑÑ‰ĞµĞ¹ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ´Ğ¸ÑĞ¿ĞµÑ€ÑĞ¸ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ñ‚Ğ°ĞºĞ¶Ğµ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ÑĞ²ÑĞ·Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞ¿ĞµĞºÑ‚Ñ€Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹ Ğ² Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ñ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆÑƒÑ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ.'}, 'en': {'title': 'Enhancing Visual Tokenization with Structured Latent Spaces', 'desc': "This paper presents a new visual tokenization framework that incorporates a PCA-like structure into the latent token space. Unlike traditional visual tokenizers that focus mainly on how well they can recreate images, this method emphasizes the importance of the latent space's structure for better interpretability and performance in other tasks. The framework produces a sequence of tokens that each provide unique information, ensuring that the most important visual features are captured first, similar to how principal component analysis works. Additionally, the authors address a problem where high-level semantic information and low-level details become mixed in the tokens, leading to improved clarity and efficiency in training auto-regressive models."}, 'zh': {'title': 'åˆ›æ–°è§†è§‰æ ‡è®°åŒ–ï¼Œæå‡å¯è§£é‡Šæ€§ä¸æ€§èƒ½', 'desc': 'æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„è§†è§‰æ ‡è®°åŒ–æ¡†æ¶ï¼Œå°†å¯è¯æ˜çš„ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰ç»“æ„åµŒå…¥æ½œåœ¨æ ‡è®°ç©ºé—´ã€‚ç°æœ‰çš„è§†è§‰æ ‡è®°å™¨ä¸»è¦ä¼˜åŒ–é‡å»ºç²¾åº¦ï¼Œä½†å¾€å¾€å¿½è§†æ½œåœ¨ç©ºé—´çš„ç»“æ„ç‰¹æ€§ï¼Œè¿™å¯¹å¯è§£é‡Šæ€§å’Œä¸‹æ¸¸ä»»åŠ¡è‡³å…³é‡è¦ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ºå›¾åƒç”Ÿæˆä¸€ç»´å› æœæ ‡è®°åºåˆ—ï¼Œæ¯ä¸ªåç»­æ ‡è®°æä¾›ä¸é‡å çš„ä¿¡æ¯ï¼Œå¹¶ä¸”å…·æœ‰æ•°å­¦ä¸Šä¿è¯çš„é€’å‡è§£é‡Šæ–¹å·®ï¼Œç±»ä¼¼äºä¸»æˆåˆ†åˆ†æã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨é‡å»ºæ€§èƒ½ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ï¼Œå¹¶æé«˜äº†ä¸äººç±»è§†è§‰ç³»ç»Ÿçš„å¯¹é½å¯è§£é‡Šæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.08588', 'title': 'BiasEdit: Debiasing Stereotyped Language Models via Model Editing', 'url': 'https://huggingface.co/papers/2503.08588', 'abstract': "Previous studies have established that language models manifest stereotyped biases. Existing debiasing strategies, such as retraining a model with counterfactual data, representation projection, and prompting often fail to efficiently eliminate bias or directly alter the models' biased internal representations. To address these issues, we propose BiasEdit, an efficient model editing method to remove stereotypical bias from language models through lightweight networks that act as editors to generate parameter updates. BiasEdit employs a debiasing loss guiding editor networks to conduct local edits on partial parameters of a language model for debiasing while preserving the language modeling abilities during editing through a retention loss. Experiments on StereoSet and Crows-Pairs demonstrate the effectiveness, efficiency, and robustness of BiasEdit in eliminating bias compared to tangental debiasing baselines and little to no impact on the language models' general capabilities. In addition, we conduct bias tracing to probe bias in various modules and explore bias editing impacts on different components of language models.", 'score': 6, 'issue_id': 2658, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 11', 'zh': '3æœˆ11æ—¥'}, 'hash': '183584887772b6e7', 'authors': ['Xin Xu', 'Wei Xu', 'Ningyu Zhang', 'Julian McAuley'], 'affiliations': ['Georgia Institute of Technology', 'University of California, San Diego', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.08588.jpg', 'data': {'categories': ['#hallucinations', '#ethics', '#architecture', '#data', '#training'], 'emoji': 'âš–ï¸', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑƒÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ğ¸Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ BiasEdit Ğ´Ğ»Ñ ÑƒÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ ÑÑ‚ĞµÑ€ĞµĞ¾Ñ‚Ğ¸Ğ¿Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ´ĞµĞ±Ğ¸Ğ°ÑĞ¸Ğ½Ğ³Ğ°, BiasEdit Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğµ ÑĞµÑ‚Ğ¸-Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¾Ñ€Ñ‹ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ´Ğ»Ñ Ğ´ĞµĞ±Ğ¸Ğ°ÑĞ¸Ğ½Ğ³Ğ°, Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‰ÑƒÑ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¾Ñ€Ñ‹ Ğ½Ğ° Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ BiasEdit Ğ² ÑƒÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾Ğ±Ñ‰Ğ¸Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'BiasEdit: Efficiently Editing Bias in Language Models', 'desc': "This paper introduces BiasEdit, a novel method designed to reduce stereotypical biases in language models. Unlike traditional debiasing techniques that often fail to effectively alter biased representations, BiasEdit utilizes lightweight networks to make targeted updates to the model's parameters. It incorporates a debiasing loss to guide these edits while ensuring that the model's language processing abilities remain intact through a retention loss. The results show that BiasEdit is not only effective in reducing bias but also maintains the overall performance of the language models across various tasks."}, 'zh': {'title': 'é«˜æ•ˆå»åè§ï¼Œæå‡è¯­è¨€æ¨¡å‹å…¬æ­£æ€§', 'desc': 'æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºBiasEditçš„æ¨¡å‹ç¼–è¾‘æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡è½»é‡çº§ç½‘ç»œå»é™¤è¯­è¨€æ¨¡å‹ä¸­çš„åˆ»æ¿åè§ã€‚ä¸ä¼ ç»Ÿçš„å»åè§ç­–ç•¥ç›¸æ¯”ï¼ŒBiasEditèƒ½å¤Ÿé«˜æ•ˆåœ°è¿›è¡Œå±€éƒ¨å‚æ•°ç¼–è¾‘ï¼ŒåŒæ—¶ä¿æŒè¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ã€‚è¯¥æ–¹æ³•ä½¿ç”¨å»åè§æŸå¤±æŒ‡å¯¼ç¼–è¾‘ç½‘ç»œè¿›è¡Œå‚æ•°æ›´æ–°ï¼Œå¹¶é€šè¿‡ä¿ç•™æŸå¤±ç¡®ä¿ç¼–è¾‘è¿‡ç¨‹ä¸­çš„è¯­è¨€å»ºæ¨¡èƒ½åŠ›ä¸å—å½±å“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒBiasEditåœ¨æ¶ˆé™¤åè§æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä¸”å¯¹è¯­è¨€æ¨¡å‹çš„æ•´ä½“èƒ½åŠ›å½±å“è¾ƒå°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.08684', 'title': 'Perplexity Trap: PLM-Based Retrievers Overrate Low Perplexity Documents', 'url': 'https://huggingface.co/papers/2503.08684', 'abstract': 'Previous studies have found that PLM-based retrieval models exhibit a preference for LLM-generated content, assigning higher relevance scores to these documents even when their semantic quality is comparable to human-written ones. This phenomenon, known as source bias, threatens the sustainable development of the information access ecosystem. However, the underlying causes of source bias remain unexplored. In this paper, we explain the process of information retrieval with a causal graph and discover that PLM-based retrievers learn perplexity features for relevance estimation, causing source bias by ranking the documents with low perplexity higher. Theoretical analysis further reveals that the phenomenon stems from the positive correlation between the gradients of the loss functions in language modeling task and retrieval task. Based on the analysis, a causal-inspired inference-time debiasing method is proposed, called Causal Diagnosis and Correction (CDC). CDC first diagnoses the bias effect of the perplexity and then separates the bias effect from the overall estimated relevance score. Experimental results across three domains demonstrate the superior debiasing effectiveness of CDC, emphasizing the validity of our proposed explanatory framework. Source codes are available at https://github.com/WhyDwelledOnAi/Perplexity-Trap.', 'score': 5, 'issue_id': 2663, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 11', 'zh': '3æœˆ11æ—¥'}, 'hash': '13216e7922903487', 'authors': ['Haoyu Wang', 'Sunhao Dai', 'Haiyuan Zhao', 'Liang Pang', 'Xiao Zhang', 'Gang Wang', 'Zhenhua Dong', 'Jun Xu', 'Ji-Rong Wen'], 'affiliations': ['CAS Key Laboratory of AI Safety, Institute of Computing Technology, Beijing, China', 'Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China', 'Huawei Noahs Ark Lab, Shenzhen, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.08684.jpg', 'data': {'categories': ['#ethics', '#data', '#interpretability', '#inference'], 'emoji': 'ğŸ”', 'ru': {'title': 'ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğº Ğ˜Ğ˜-ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ñƒ', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğµ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (PLM) Ğ² Ğ¿Ğ¾Ğ»ÑŒĞ·Ñƒ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°, ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±ÑŠÑÑĞ½ÑÑÑ‚ ÑÑ‚Ğ¾Ñ‚ Ñ„ĞµĞ½Ğ¾Ğ¼ĞµĞ½ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ³Ñ€Ğ°Ñ„Ğ° Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ PLM-Ñ€ĞµÑ‚Ñ€Ğ¸Ğ²ĞµÑ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ¿ĞµÑ€Ğ¿Ğ»ĞµĞºÑĞ¸Ñ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ Ğ¿ĞµÑ€Ğ¿Ğ»ĞµĞºÑĞ¸ĞµĞ¹. Ğ¢ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾ ÑĞ²ÑĞ·Ğ°Ğ½Ğ¾ Ñ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸ĞµĞ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ°. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞ±Ğ¸Ğ°ÑĞ¸Ğ½Ğ³Ğ° Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°, Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ Causal Diagnosis and Correction (CDC).'}, 'en': {'title': 'Unraveling Source Bias in PLM Retrieval Models', 'desc': 'This paper investigates the issue of source bias in PLM-based retrieval models, where documents generated by large language models (LLMs) are favored over human-written content. The authors use a causal graph to explain how these models learn to estimate relevance based on perplexity features, leading to a preference for documents with lower perplexity scores. They identify a correlation between the gradients of loss functions in language modeling and retrieval tasks as a root cause of this bias. To address this, they propose a new method called Causal Diagnosis and Correction (CDC), which effectively debiases the relevance scores by isolating the bias introduced by perplexity.'}, 'zh': {'title': 'æ­ç¤ºæºåå·®ï¼Œæå‡ä¿¡æ¯æ£€ç´¢å‡†ç¡®æ€§', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†åŸºäºé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆPLMï¼‰çš„æ£€ç´¢æ¨¡å‹ä¸­å­˜åœ¨çš„æºåå·®ç°è±¡ã€‚ç ”ç©¶å‘ç°ï¼Œè¿™ç§åå·®ä½¿å¾—æ¨¡å‹æ›´å€¾å‘äºä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆçš„å†…å®¹åˆ†é…æ›´é«˜çš„ç›¸å…³æ€§è¯„åˆ†ï¼Œå³ä½¿è¿™äº›å†…å®¹çš„è¯­ä¹‰è´¨é‡ä¸äººç±»æ’°å†™çš„å†…å®¹ç›¸å½“ã€‚é€šè¿‡å› æœå›¾åˆ†æä¿¡æ¯æ£€ç´¢è¿‡ç¨‹ï¼Œæ­ç¤ºäº†æ¨¡å‹åœ¨ç›¸å…³æ€§ä¼°è®¡ä¸­å­¦ä¹ åˆ°çš„å›°æƒ‘åº¦ç‰¹å¾å¯¼è‡´äº†æºåå·®ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºå› æœè¯Šæ–­ä¸ä¿®æ­£ï¼ˆCDCï¼‰çš„å»åæ–¹æ³•ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåˆ†ç¦»å›°æƒ‘åº¦çš„åå·®æ•ˆåº”ï¼Œæå‡æ£€ç´¢æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.07699', 'title': 'RayFlow: Instance-Aware Diffusion Acceleration via Adaptive Flow\n  Trajectories', 'url': 'https://huggingface.co/papers/2503.07699', 'abstract': "Diffusion models have achieved remarkable success across various domains. However, their slow generation speed remains a critical challenge. Existing acceleration methods, while aiming to reduce steps, often compromise sample quality, controllability, or introduce training complexities. Therefore, we propose RayFlow, a novel diffusion framework that addresses these limitations. Unlike previous methods, RayFlow guides each sample along a unique path towards an instance-specific target distribution. This method minimizes sampling steps while preserving generation diversity and stability. Furthermore, we introduce Time Sampler, an importance sampling technique to enhance training efficiency by focusing on crucial timesteps. Extensive experiments demonstrate RayFlow's superiority in generating high-quality images with improved speed, control, and training efficiency compared to existing acceleration techniques.", 'score': 5, 'issue_id': 2656, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 10', 'zh': '3æœˆ10æ—¥'}, 'hash': '49ebba2cb63d91f8', 'authors': ['Huiyang Shao', 'Xin Xia', 'Yuhong Yang', 'Yuxi Ren', 'Xing Wang', 'Xuefeng Xiao'], 'affiliations': ['ByteDance Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2503.07699.jpg', 'data': {'categories': ['#cv', '#diffusion', '#optimization', '#training'], 'emoji': 'ğŸš€', 'ru': {'title': 'RayFlow: Ğ±Ñ‹ÑÑ‚Ñ€Ğ°Ñ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'RayFlow - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ½Ğ° Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ ÑÑĞ¼Ğ¿Ğ» Ğ¿Ğ¾ ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ¿ÑƒÑ‚Ğ¸ Ğº Ğ¸Ğ½Ğ´Ğ¸Ğ²Ğ¸Ğ´ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ, Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑˆĞ°Ğ³Ğ¾Ğ² Ğ±ĞµĞ· ÑƒÑ‰ĞµÑ€Ğ±Ğ° Ğ´Ğ»Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ. RayFlow Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Time Sampler Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿ÑƒÑ‚ĞµĞ¼ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ½Ğ° ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ°Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ RayFlow Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ² ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸, ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğµ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'RayFlow: Accelerating Diffusion Models Without Compromise', 'desc': 'This paper introduces RayFlow, a new diffusion model framework designed to improve the speed of image generation without sacrificing quality. Unlike traditional methods that often reduce the number of steps at the cost of sample diversity or stability, RayFlow customizes the sampling path for each instance, ensuring a more efficient and effective generation process. Additionally, the authors present Time Sampler, an importance sampling technique that optimizes training by prioritizing key timesteps. Experimental results show that RayFlow outperforms existing methods in generating high-quality images more quickly and with better control.'}, 'zh': {'title': 'RayFlowï¼šåŠ é€Ÿæ‰©æ•£æ¨¡å‹çš„æ–°æ–¹æ³•', 'desc': 'æ‰©æ•£æ¨¡å‹åœ¨å¤šä¸ªé¢†åŸŸå–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†å…¶ç”Ÿæˆé€Ÿåº¦è¾ƒæ…¢ä»ç„¶æ˜¯ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ã€‚ç°æœ‰çš„åŠ é€Ÿæ–¹æ³•è™½ç„¶æ—¨åœ¨å‡å°‘æ­¥éª¤ï¼Œä½†å¾€å¾€ä¼šå½±å“æ ·æœ¬è´¨é‡ã€å¯æ§æ€§æˆ–å¢åŠ è®­ç»ƒå¤æ‚æ€§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†RayFlowï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„æ‰©æ•£æ¡†æ¶ï¼Œèƒ½å¤Ÿè§£å†³è¿™äº›é™åˆ¶ã€‚RayFlowé€šè¿‡å¼•å¯¼æ¯ä¸ªæ ·æœ¬æ²¿ç€ç‹¬ç‰¹è·¯å¾„æœå‘ç‰¹å®šç›®æ ‡åˆ†å¸ƒï¼Œæœ€å°åŒ–é‡‡æ ·æ­¥éª¤ï¼ŒåŒæ—¶ä¿æŒç”Ÿæˆçš„å¤šæ ·æ€§å’Œç¨³å®šæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.05860', 'title': 'Benchmarking AI Models in Software Engineering: A Review, Search Tool,\n  and Enhancement Protocol', 'url': 'https://huggingface.co/papers/2503.05860', 'abstract': "Benchmarks are essential for consistent evaluation and reproducibility. The integration of Artificial Intelligence into Software Engineering (AI4SE) has given rise to numerous benchmarks for tasks such as code generation and bug fixing. However, this surge presents challenges: (1) scattered benchmark knowledge across tasks, (2) difficulty in selecting relevant benchmarks, (3) the absence of a uniform standard for benchmark development, and (4) limitations of existing benchmarks. In this paper, we review 173 studies and identify 204 AI4SE benchmarks. We classify these benchmarks, analyze their limitations, and expose gaps in practices. Based on our review, we created BenchScout, a semantic search tool to find relevant benchmarks, using automated clustering of the contexts from associated studies. We conducted a user study with 22 participants to evaluate BenchScout's usability, effectiveness, and intuitiveness which resulted in average scores of 4.5, 4.0, and 4.1 out of 5. To advance benchmarking standards, we propose BenchFrame, a unified method to enhance benchmark quality. As a case study, we applied BenchFrame to the HumanEval benchmark and addressed its main limitations. This led to <PRE_TAG>HumanEvalNext</POST_TAG>, featuring (1) corrected errors, (2) improved language conversion, (3) expanded test coverage, and (4) increased difficulty. We then evaluated ten state-of-the-art code language models on HumanEval, HumanEvalPlus, and <PRE_TAG>HumanEvalNext</POST_TAG>. On <PRE_TAG>HumanEvalNext</POST_TAG>, models showed a pass@1 score reduction of 31.22% and 19.94% compared to HumanEval and HumanEvalPlus, respectively.", 'score': 5, 'issue_id': 2661, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 7', 'zh': '3æœˆ7æ—¥'}, 'hash': 'dee962dca6de084b', 'authors': ['Roham Koohestani', 'Philippe de Bekker', 'Maliheh Izadi'], 'affiliations': ['Delft University of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2503.05860.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#survey'], 'emoji': 'ğŸ§ª', 'ru': {'title': 'Ğ¡Ğ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ² AI4SE: Ğ¾Ñ‚ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğº Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸ÑĞ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ² Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ² Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½ÑƒÑ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ñ (AI4SE). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°ÑÑĞ¼Ğ¾Ñ‚Ñ€ĞµĞ»Ğ¸ 173 Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ 204 Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ° AI4SE, ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ² Ğ¸Ñ… Ğ¸ Ğ¿Ñ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ² Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ° Ğ±Ñ‹Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° BenchScout Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ². ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ BenchFrame Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ±Ñ‹Ğ» Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½ Ğº Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºÑƒ HumanEval, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²ĞµĞ»Ğ¾ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ¹ Ğ²ĞµÑ€ÑĞ¸Ğ¸ HumanEvalNext.'}, 'en': {'title': 'Enhancing AI4SE Benchmarking with BenchScout and BenchFrame', 'desc': 'This paper addresses the challenges of evaluating benchmarks in the integration of Artificial Intelligence into Software Engineering (AI4SE). It reviews 173 studies to identify and classify 204 benchmarks, highlighting their limitations and gaps in current practices. The authors introduce BenchScout, a semantic search tool that helps users find relevant benchmarks through automated clustering. Additionally, they propose BenchFrame, a unified method to improve benchmark quality, demonstrated through the enhancement of the HumanEval benchmark into HumanEvalNext, which features significant improvements in error correction and test coverage.'}, 'zh': {'title': 'æå‡åŸºå‡†æµ‹è¯•è´¨é‡çš„ç»Ÿä¸€æ–¹æ³•', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†äººå·¥æ™ºèƒ½åœ¨è½¯ä»¶å·¥ç¨‹ä¸­çš„åŸºå‡†æµ‹è¯•ï¼ˆAI4SEï¼‰çš„é‡è¦æ€§ï¼Œåˆ†æäº†173é¡¹ç ”ç©¶å¹¶è¯†åˆ«å‡º204ä¸ªåŸºå‡†ã€‚æˆ‘ä»¬å‘ç°ç°æœ‰åŸºå‡†å­˜åœ¨çŸ¥è¯†åˆ†æ•£ã€é€‰æ‹©å›°éš¾ã€ç¼ºä¹ç»Ÿä¸€æ ‡å‡†å’Œå±€é™æ€§ç­‰é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼€å‘äº†BenchScoutï¼Œä¸€ä¸ªè¯­ä¹‰æœç´¢å·¥å…·ï¼Œå¸®åŠ©ç”¨æˆ·æ‰¾åˆ°ç›¸å…³åŸºå‡†ï¼Œå¹¶æå‡ºäº†BenchFrameï¼Œä¸€ä¸ªç»Ÿä¸€çš„æ–¹æ³•æ¥æå‡åŸºå‡†è´¨é‡ã€‚é€šè¿‡å¯¹HumanEvalåŸºå‡†çš„æ¡ˆä¾‹ç ”ç©¶ï¼Œæˆ‘ä»¬åˆ›å»ºäº†HumanEvalNextï¼Œä¿®æ­£äº†é”™è¯¯ã€æ”¹å–„äº†è¯­è¨€è½¬æ¢ã€æ‰©å±•äº†æµ‹è¯•è¦†ç›–ç‡å¹¶å¢åŠ äº†éš¾åº¦ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.08689', 'title': 'QuoTA: Query-oriented Token Assignment via CoT Query Decouple for Long\n  Video Comprehension', 'url': 'https://huggingface.co/papers/2503.08689', 'abstract': 'Recent advances in long video understanding typically mitigate visual redundancy through visual token pruning based on attention distribution. However, while existing methods employ post-hoc low-response token pruning in decoder layers, they overlook the input-level semantic correlation between visual tokens and instructions (query). In this paper, we propose QuoTA, an ante-hoc training-free modular that extends existing large video-language models (LVLMs) for visual token assignment based on query-oriented frame-level importance assessment. The query-oriented token selection is crucial as it aligns visual processing with task-specific requirements, optimizing token budget utilization while preserving semantically relevant content. Specifically, (i) QuoTA strategically allocates frame-level importance scores based on query relevance, enabling one-time visual token assignment before cross-modal interactions in decoder layers, (ii) we decouple the query through Chain-of-Thoughts reasoning to facilitate more precise LVLM-based frame importance scoring, and (iii) QuoTA offers a plug-and-play functionality that extends to existing LVLMs. Extensive experimental results demonstrate that implementing QuoTA with LLaVA-Video-7B yields an average performance improvement of 3.2% across six benchmarks (including Video-MME and MLVU) while operating within an identical visual token budget as the baseline. Codes are open-sourced at https://github.com/MAC-AutoML/QuoTA.', 'score': 4, 'issue_id': 2655, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 11', 'zh': '3æœˆ11æ—¥'}, 'hash': '7dbb5f0edfd69aad', 'authors': ['Yongdong Luo', 'Wang Chen', 'Xiawu Zheng', 'Weizhong Huang', 'Shukang Yin', 'Haojia Lin', 'Chaoyou Fu', 'Jinfa Huang', 'Jiayi Ji', 'Jiebo Luo', 'Rongrong Ji'], 'affiliations': ['Nanjing University', 'University of Rochester', 'Xiamen University'], 'pdf_title_img': 'assets/pdf/title_img/2503.08689.jpg', 'data': {'categories': ['#alignment', '#multimodal', '#long_context', '#architecture', '#video', '#benchmark', '#open_source', '#optimization'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ñ‹Ğ¹ Ğ¾Ñ‚Ğ±Ğ¾Ñ€ ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ˜Ğ˜', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ QuoTA - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ (LVLM). QuoTA Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ğ¾Ñ‚Ğ±Ğ¾Ñ€ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ° Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ±ÑĞ´Ğ¶ĞµÑ‚ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ° Ñ‡ĞµÑ€ĞµĞ· Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞµ Ğ¼Ñ‹ÑĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ°Ğ´Ñ€Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ QuoTA Ñ LLaVA-Video-7B Ğ´Ğ°ĞµÑ‚ ÑÑ€ĞµĞ´Ğ½ĞµĞµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 3.2% Ğ¿Ğ¾ ÑˆĞµÑÑ‚Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ğ¼ Ğ¿Ñ€Ğ¸ Ñ‚Ğ¾Ğ¼ Ğ¶Ğµ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ².'}, 'en': {'title': 'Optimizing Visual Token Selection for Better Video Understanding', 'desc': 'This paper introduces QuoTA, a new method for improving long video understanding by optimizing how visual tokens are selected based on their relevance to specific queries. Unlike previous approaches that prune tokens after processing, QuoTA assesses the importance of visual tokens before they are used, ensuring that only the most relevant information is retained. By using Chain-of-Thoughts reasoning, QuoTA enhances the accuracy of frame importance scoring, allowing for better alignment between visual data and task requirements. The results show that QuoTA can significantly boost performance in existing large video-language models while maintaining the same token budget.'}, 'zh': {'title': 'ä¼˜åŒ–è§†è§‰æ ‡è®°åˆ†é…ï¼Œæå‡è§†é¢‘ç†è§£æ€§èƒ½', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºQuoTAçš„æ¨¡å—ï¼Œæ—¨åœ¨æ”¹è¿›é•¿è§†é¢‘ç†è§£ä¸­çš„è§†è§‰æ ‡è®°åˆ†é…ã€‚ä¸ç°æœ‰æ–¹æ³•ä¸åŒï¼ŒQuoTAåœ¨è¾“å…¥å±‚é¢ä¸Šè€ƒè™‘è§†è§‰æ ‡è®°ä¸æŸ¥è¯¢ä¹‹é—´çš„è¯­ä¹‰å…³è”ï¼Œä»è€Œä¼˜åŒ–æ ‡è®°çš„ä½¿ç”¨æ•ˆç‡ã€‚é€šè¿‡åŸºäºæŸ¥è¯¢ç›¸å…³æ€§çš„å¸§çº§é‡è¦æ€§è¯„ä¼°ï¼ŒQuoTAèƒ½å¤Ÿåœ¨è§£ç å™¨å±‚ä¹‹å‰è¿›è¡Œä¸€æ¬¡æ€§è§†è§‰æ ‡è®°åˆ†é…ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒQuoTAåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒäº†ä¸åŸºçº¿ç›¸åŒçš„è§†è§‰æ ‡è®°é¢„ç®—ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.08507', 'title': 'Referring to Any Person', 'url': 'https://huggingface.co/papers/2503.08507', 'abstract': 'Humans are undoubtedly the most important participants in computer vision, and the ability to detect any individual given a natural language description, a task we define as referring to any person, holds substantial practical value. However, we find that existing models generally fail to achieve real-world usability, and current benchmarks are limited by their focus on one-to-one referring, that hinder progress in this area. In this work, we revisit this task from three critical perspectives: task definition, dataset design, and model architecture. We first identify five aspects of referable entities and three distinctive characteristics of this task. Next, we introduce HumanRef, a novel dataset designed to tackle these challenges and better reflect real-world applications. From a model design perspective, we integrate a multimodal large language model with an object detection framework, constructing a robust referring model named RexSeek. Experimental results reveal that state-of-the-art models, which perform well on commonly used benchmarks like RefCOCO/+/g, struggle with HumanRef due to their inability to detect multiple individuals. In contrast, RexSeek not only excels in human referring but also generalizes effectively to common object referring, making it broadly applicable across various perception tasks. Code is available at https://github.com/IDEA-Research/RexSeek', 'score': 4, 'issue_id': 2661, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 11', 'zh': '3æœˆ11æ—¥'}, 'hash': '905fc0de3c82fe2e', 'authors': ['Qing Jiang', 'Lin Wu', 'Zhaoyang Zeng', 'Tianhe Ren', 'Yuda Xiong', 'Yihao Chen', 'Qin Liu', 'Lei Zhang'], 'affiliations': ['International Digital Economy Academy (IDEA)', 'South China University of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2503.08507.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#games', '#optimization', '#interpretability', '#architecture', '#cv', '#multimodal'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¿Ğ¾Ğ¸ÑĞºĞµ Ğ»ÑĞ´ĞµĞ¹: Ğ¾Ñ‚ Ñ‚ĞµĞºÑÑ‚Ğ° Ğº Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ»ÑĞ´ĞµĞ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ² ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ¼ Ğ·Ñ€ĞµĞ½Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ HumanRef, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ»ÑƒÑ‡ÑˆĞµ Ğ¾Ñ‚Ñ€Ğ°Ğ¶Ğ°ĞµÑ‚ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ, Ñ‡ĞµĞ¼ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸. ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ RexSeek, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ¼ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ RexSeek Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ½Ğ¾Ğ²Ğ¾Ğ¼ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ Ğ¸ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ².'}, 'en': {'title': 'Revolutionizing Person Detection with HumanRef and RexSeek', 'desc': 'This paper addresses the challenge of detecting individuals in computer vision using natural language descriptions, a task known as referring to any person. The authors highlight that existing models often fall short in real-world scenarios and current benchmarks are too narrow, focusing mainly on one-to-one referring. They propose a new dataset called HumanRef, which is designed to better represent the complexities of real-world applications. Additionally, they introduce a new model, RexSeek, which combines a multimodal large language model with an object detection framework, showing improved performance in both human and object referring tasks.'}, 'zh': {'title': 'äººç±»è¯†åˆ«çš„æ–°çªç ´ï¼šRexSeekæ¨¡å‹', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†è®¡ç®—æœºè§†è§‰ä¸­äººç±»ä¸ªä½“è¯†åˆ«çš„é‡è¦æ€§ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„ä»»åŠ¡å®šä¹‰â€”â€”æ ¹æ®è‡ªç„¶è¯­è¨€æè¿°è¯†åˆ«ä¸ªä½“ã€‚ç°æœ‰æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­è¡¨ç°ä¸ä½³ï¼Œä¸”ç°æœ‰åŸºå‡†æµ‹è¯•ä¸»è¦é›†ä¸­åœ¨ä¸€å¯¹ä¸€çš„è¯†åˆ«ä¸Šï¼Œé™åˆ¶äº†è¯¥é¢†åŸŸçš„å‘å±•ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œä½œè€…è®¾è®¡äº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†HumanRefï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªåä¸ºRexSeekçš„å¤šæ¨¡æ€æ¨¡å‹ï¼Œèƒ½å¤Ÿæœ‰æ•ˆè¯†åˆ«å¤šä¸ªä¸ªä½“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRexSeekåœ¨è¯†åˆ«äººç±»å’Œå¸¸è§ç‰©ä½“æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.08417', 'title': 'AnyMoLe: Any Character Motion In-betweening Leveraging Video Diffusion\n  Models', 'url': 'https://huggingface.co/papers/2503.08417', 'abstract': "Despite recent advancements in learning-based motion in-betweening, a key limitation has been overlooked: the requirement for character-specific datasets. In this work, we introduce AnyMoLe, a novel method that addresses this limitation by leveraging video diffusion models to generate motion in-between frames for arbitrary characters without external data. Our approach employs a two-stage frame generation process to enhance contextual understanding. Furthermore, to bridge the domain gap between real-world and rendered character animations, we introduce ICAdapt, a fine-tuning technique for video diffusion models. Additionally, we propose a ``motion-video mimicking'' optimization technique, enabling seamless motion generation for characters with arbitrary joint structures using 2D and 3D-aware features. AnyMoLe significantly reduces data dependency while generating smooth and realistic transitions, making it applicable to a wide range of motion in-betweening tasks.", 'score': 4, 'issue_id': 2662, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 11', 'zh': '3æœˆ11æ—¥'}, 'hash': '7610945506fac90e', 'authors': ['Kwan Yun', 'Seokhyeon Hong', 'Chaelin Kim', 'Junyong Noh'], 'affiliations': ['KAIST, Visual Media Lab'], 'pdf_title_img': 'assets/pdf/title_img/2503.08417.jpg', 'data': {'categories': ['#3d', '#training', '#diffusion', '#optimization', '#video', '#dataset'], 'emoji': 'ğŸ­', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ±ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…', 'desc': "AnyMoLe - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ½ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ Ğ´Ğ¾Ğ²Ğ¾Ğ´ĞºĞ¸ ICAdapt Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ²Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ 'motion-video mimicking' Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ»Ğ°Ğ²Ğ½Ñ‹Ñ… Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹ Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¾Ğ¹ ÑÑƒÑÑ‚Ğ°Ğ²Ğ¾Ğ²."}, 'en': {'title': 'AnyMoLe: Motion Generation Without Character-Specific Data', 'desc': "This paper presents AnyMoLe, a new method for generating motion in-between frames for various characters without needing specific datasets. It utilizes video diffusion models in a two-stage frame generation process to improve the understanding of context in animations. The authors introduce ICAdapt, a technique that fine-tunes these models to better connect real-world and animated character movements. Additionally, a 'motion-video mimicking' optimization is proposed, allowing for fluid motion generation across different character joint structures, thus minimizing data dependency and enhancing the realism of transitions."}, 'zh': {'title': 'AnyMoLeï¼šæ— æ•°æ®ä¾èµ–çš„è§’è‰²è¿åŠ¨æ’å€¼æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°æ–¹æ³•AnyMoLeï¼Œæ—¨åœ¨è§£å†³åŸºäºå­¦ä¹ çš„è¿åŠ¨æ’å€¼ä¸­å¯¹ç‰¹å®šè§’è‰²æ•°æ®é›†çš„ä¾èµ–é—®é¢˜ã€‚è¯¥æ–¹æ³•åˆ©ç”¨è§†é¢‘æ‰©æ•£æ¨¡å‹ç”Ÿæˆä»»æ„è§’è‰²çš„å¸§é—´è¿åŠ¨ï¼Œæ— éœ€å¤–éƒ¨æ•°æ®ã€‚æˆ‘ä»¬é‡‡ç”¨äº†ä¸¤é˜¶æ®µçš„å¸§ç”Ÿæˆè¿‡ç¨‹ï¼Œä»¥å¢å¼ºä¸Šä¸‹æ–‡ç†è§£ï¼Œå¹¶å¼•å…¥äº†ICAdaptæŠ€æœ¯æ¥ç¼©å°çœŸå®ä¸–ç•Œä¸æ¸²æŸ“è§’è‰²åŠ¨ç”»ä¹‹é—´çš„é¢†åŸŸå·®è·ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†â€œè¿åŠ¨è§†é¢‘æ¨¡ä»¿â€ä¼˜åŒ–æŠ€æœ¯ï¼Œä½¿å¾—ä½¿ç”¨2Då’Œ3Dç‰¹å¾çš„ä»»æ„å…³èŠ‚ç»“æ„è§’è‰²çš„è¿åŠ¨ç”Ÿæˆå˜å¾—æ— ç¼ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.06492', 'title': 'VisualSimpleQA: A Benchmark for Decoupled Evaluation of Large\n  Vision-Language Models in Fact-Seeking Question Answering', 'url': 'https://huggingface.co/papers/2503.06492', 'abstract': 'Large vision-language models (LVLMs) have demonstrated remarkable achievements, yet the generation of non-factual responses remains prevalent in fact-seeking question answering (QA). Current multimodal fact-seeking benchmarks primarily focus on comparing model outputs to ground truth answers, providing limited insights into the performance of modality-specific modules. To bridge this gap, we introduce VisualSimpleQA, a multimodal fact-seeking benchmark with two key features. First, it enables streamlined and decoupled evaluation of LVLMs in visual and linguistic modalities. Second, it incorporates well-defined difficulty criteria to guide human annotation and facilitates the extraction of a challenging subset, <PRE_TAG>VisualSimpleQA-hard</POST_TAG>. Experiments on 15 LVLMs show that even state-of-the-art models such as GPT-4o achieve merely 60%+ correctness in multimodal fact-seeking QA on VisualSimpleQA and 30%+ on <PRE_TAG>VisualSimpleQA-hard</POST_TAG>. Furthermore, the decoupled evaluation across these models highlights substantial opportunities for improvement in both visual and linguistic modules. The dataset is available at https://huggingface.co/datasets/WYLing/VisualSimpleQA.', 'score': 4, 'issue_id': 2659, 'pub_date': '2025-03-09', 'pub_date_card': {'ru': '9 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 9', 'zh': '3æœˆ9æ—¥'}, 'hash': '5f5a7152d7773f38', 'authors': ['Yanling Wang', 'Yihan Zhao', 'Xiaodong Chen', 'Shasha Guo', 'Lixin Liu', 'Haoyang Li', 'Yong Xiao', 'Jing Zhang', 'Qi Li', 'Ke Xu'], 'affiliations': ['Renmin University of China', 'Tencent', 'Tsinghua University', 'Zhongguancun Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2503.06492.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#hallucinations', '#multimodal', '#interpretability'], 'emoji': 'ğŸ”', 'ru': {'title': 'VisualSimpleQA: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ± Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº VisualSimpleQA Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ñ„Ğ°ĞºÑ‚Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¿Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ÑƒĞ»Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LVLM). Ğ’ Ğ½ĞµĞ¼ Ñ‚Ğ°ĞºĞ¶Ğµ ĞµÑÑ‚ÑŒ ÑƒÑĞ»Ğ¾Ğ¶Ğ½ĞµĞ½Ğ½Ğ°Ñ Ğ²ĞµÑ€ÑĞ¸Ñ VisualSimpleQA-hard Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ñ€Ğ¾Ğ´Ğµ GPT-4o Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ»Ğ¸ÑˆÑŒ 60% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° VisualSimpleQA Ğ¸ 30% Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¹ Ğ²ĞµÑ€ÑĞ¸Ğ¸.'}, 'en': {'title': 'Enhancing Accuracy in Multimodal Fact-Seeking with VisualSimpleQA', 'desc': 'This paper addresses the issue of non-factual responses in large vision-language models (LVLMs) during fact-seeking question answering (QA). It introduces VisualSimpleQA, a new benchmark that allows for separate evaluation of visual and linguistic capabilities of these models. The benchmark includes specific difficulty levels to aid in human annotation and identifies a challenging subset called VisualSimpleQA-hard. Experiments reveal that even advanced models like GPT-4o struggle with accuracy, achieving only around 60% correctness overall and 30% on the harder subset, indicating significant room for improvement in both visual and linguistic processing.'}, 'zh': {'title': 'æå‡å¤šæ¨¡æ€é—®ç­”çš„å‡†ç¡®æ€§', 'desc': 'å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨äº‹å®å¯»æ±‚é—®ç­”ä¸­å–å¾—äº†æ˜¾è‘—æˆå°±ï¼Œä½†ä»ç„¶å­˜åœ¨ç”Ÿæˆä¸å‡†ç¡®å›ç­”çš„é—®é¢˜ã€‚å½“å‰çš„å¤šæ¨¡æ€äº‹å®å¯»æ±‚åŸºå‡†ä¸»è¦å…³æ³¨æ¨¡å‹è¾“å‡ºä¸çœŸå®ç­”æ¡ˆçš„æ¯”è¾ƒï¼Œç¼ºä¹å¯¹ç‰¹å®šæ¨¡æ€æ¨¡å—æ€§èƒ½çš„æ·±å…¥åˆ†æã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†VisualSimpleQAï¼Œè¿™æ˜¯ä¸€ä¸ªå…·æœ‰ä¸¤ä¸ªå…³é”®ç‰¹å¾çš„å¤šæ¨¡æ€äº‹å®å¯»æ±‚åŸºå‡†ã€‚é€šè¿‡ç®€åŒ–å’Œè§£è€¦çš„è¯„ä¼°æ–¹å¼ï¼ŒVisualSimpleQAèƒ½å¤Ÿæ›´å¥½åœ°è¯„ä¼°LVLMsåœ¨è§†è§‰å’Œè¯­è¨€æ¨¡æ€ä¸Šçš„è¡¨ç°ï¼Œå¹¶å¼•å…¥æ˜ç¡®çš„éš¾åº¦æ ‡å‡†ä»¥æŒ‡å¯¼äººå·¥æ ‡æ³¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.18858', 'title': 'Evaluating Intelligence via Trial and Error', 'url': 'https://huggingface.co/papers/2502.18858', 'abstract': "Intelligence is a crucial trait for species to find solutions within a limited number of trial-and-error attempts. Building on this idea, we introduce Survival Game as a framework to evaluate intelligence based on the number of failed attempts in a trial-and-error process. Fewer failures indicate higher intelligence. When the expectation and variance of failure counts are both finite, it signals the ability to consistently find solutions to new challenges, which we define as the Autonomous Level of intelligence. Using Survival Game, we comprehensively evaluate existing AI systems. Our results show that while AI systems achieve the Autonomous Level in simple tasks, they are still far from it in more complex tasks, such as vision, search, recommendation, and language. While scaling current AI technologies might help, this would come at an astronomical cost. Projections suggest that achieving the Autonomous Level for general tasks would require 10^{26} parameters. To put this into perspective, loading such a massive model requires so many H100 GPUs that their total value is 10^{7} times that of Apple Inc.'s market value. Even with Moore's Law, supporting such a parameter scale would take 70 years. This staggering cost highlights the complexity of human tasks and the inadequacies of current AI technologies. To further investigate this phenomenon, we conduct a theoretical analysis of Survival Game and its experimental results. Our findings suggest that human tasks possess a criticality property. As a result, Autonomous Level requires a deep understanding of the task's underlying mechanisms. Current AI systems, however, do not fully grasp these mechanisms and instead rely on superficial mimicry, making it difficult for them to reach an autonomous level. We believe Survival Game can not only guide the future development of AI but also offer profound insights into human intelligence.", 'score': 4, 'issue_id': 2656, 'pub_date': '2025-02-26', 'pub_date_card': {'ru': '26 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 26', 'zh': '2æœˆ26æ—¥'}, 'hash': '54797794006daa5e', 'authors': ['Jingtao Zhan', 'Jiahao Zhao', 'Jiayu Li', 'Yiqun Liu', 'Bo Zhang', 'Qingyao Ai', 'Jiaxin Mao', 'Hongning Wang', 'Min Zhang', 'Shaoping Ma'], 'affiliations': ['Renmin University of China', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2502.18858.jpg', 'data': {'categories': ['#reasoning', '#training', '#agents', '#agi', '#benchmark'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞĞ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚: Ğ¿ÑƒÑ‚ÑŒ Ñ‡ĞµÑ€ĞµĞ· Survival Game', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ Survival Game Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ½ĞµÑƒĞ´Ğ°Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚Ğ¾Ğº Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ¿Ñ€Ğ¾Ğ± Ğ¸ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¿Ğ¾Ğ½ÑÑ‚Ğ¸Ğµ ĞĞ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ£Ñ€Ğ¾Ğ²Ğ½Ñ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ¿Ğ¾ ÑÑ‚Ğ¾Ğ¼Ñƒ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ĞĞ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ£Ñ€Ğ¾Ğ²Ğ½Ñ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ÑÑ Ğ¾Ğ³Ñ€Ğ¾Ğ¼Ğ½Ğ¾Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ½ĞµÑ†ĞµĞ»ĞµÑĞ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ğ¾. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¾Ğ±Ğ»Ğ°Ğ´Ğ°ÑÑ‚ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ¾Ğ¼ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğ¼ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğ³Ğ¾ Ğ½Ğµ Ñ…Ğ²Ğ°Ñ‚Ğ°ĞµÑ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼ Ğ˜Ğ˜.'}, 'en': {'title': 'Measuring Intelligence: The Survival Game Framework', 'desc': 'This paper introduces the Survival Game framework to measure intelligence based on the number of failed attempts in problem-solving. It defines the Autonomous Level of intelligence as the ability to find solutions with fewer failures, indicating a deeper understanding of tasks. The study evaluates existing AI systems, revealing that while they perform well on simple tasks, they struggle with complex ones like vision and language. The findings suggest that achieving a high Autonomous Level in AI would require an impractical scale of parameters, highlighting the limitations of current technologies and the complexity of human-like intelligence.'}, 'zh': {'title': 'ç”Ÿå­˜æ¸¸æˆï¼šè¯„ä¼°æ™ºèƒ½çš„æ–°æ¡†æ¶', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºç”Ÿå­˜æ¸¸æˆçš„æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°æ™ºèƒ½æ°´å¹³ï¼Œç‰¹åˆ«æ˜¯åœ¨è¯•é”™è¿‡ç¨‹ä¸­å¤±è´¥æ¬¡æ•°çš„å¤šå°‘ã€‚å¤±è´¥æ¬¡æ•°è¶Šå°‘ï¼Œè¡¨ç¤ºæ™ºèƒ½æ°´å¹³è¶Šé«˜ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå°½ç®¡ç°æœ‰çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿåœ¨ç®€å•ä»»åŠ¡ä¸­è¾¾åˆ°äº†è‡ªä¸»æ°´å¹³ï¼Œä½†åœ¨å¤æ‚ä»»åŠ¡ï¼ˆå¦‚è§†è§‰ã€æœç´¢ã€æ¨èå’Œè¯­è¨€å¤„ç†ï¼‰ä¸­ä»ç„¶è¿œæœªè¾¾åˆ°ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œç”Ÿå­˜æ¸¸æˆä¸ä»…å¯ä»¥æŒ‡å¯¼æœªæ¥çš„äººå·¥æ™ºèƒ½å‘å±•ï¼Œè¿˜èƒ½æ·±å…¥ç†è§£äººç±»æ™ºèƒ½çš„æœ¬è´¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.06594', 'title': 'Beyond Decoder-only: Large Language Models Can be Good Encoders for\n  Machine Translation', 'url': 'https://huggingface.co/papers/2503.06594', 'abstract': 'The field of neural machine translation (NMT) has changed with the advent of large language models (<PRE_TAG>LLMs)</POST_TAG>. Much of the recent emphasis in natural language processing (NLP) has been on modeling machine translation and many other problems using a single pre-trained <PRE_TAG>Transformer decoder</POST_TAG>, while encoder-decoder architectures, which were the standard in earlier NMT models, have received relatively less attention. In this paper, we explore translation models that are universal, efficient, and easy to optimize, by marrying the world of LLMs with the world of NMT. We apply LLMs to NMT encoding and leave the NMT decoder unchanged. We also develop methods for adapting LLMs to work better with the NMT decoder. Furthermore, we construct a new dataset involving multiple tasks to assess how well the machine translation system generalizes across various tasks. Evaluations on the WMT and our datasets show that results using our method match or surpass a range of baselines in terms of translation quality, but achieve 2.4 sim 6.5 times inference speedups and a 75% reduction in the memory footprint of the KV cache. It also demonstrates strong generalization across a variety of translation-related tasks.', 'score': 3, 'issue_id': 2660, 'pub_date': '2025-03-09', 'pub_date_card': {'ru': '9 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 9', 'zh': '3æœˆ9æ—¥'}, 'hash': 'bea0df269ed71c2d', 'authors': ['Yingfeng Luo', 'Tong Zheng', 'Yongyu Mu', 'Bei Li', 'Qinghong Zhang', 'Yongqi Gao', 'Ziqiang Xu', 'Peinan Feng', 'Xiaoqian Liu', 'Tong Xiao', 'Jingbo Zhu'], 'affiliations': ['NLP Lab, Northeastern University, Shenyang, China', 'NiuTrans Research, Shenyang, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.06594.jpg', 'data': {'categories': ['#multilingual', '#machine_translation', '#dataset', '#inference', '#training'], 'emoji': 'ğŸš€', 'ru': {'title': 'ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ñ‰Ğ¸ LLM Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ NMT Ğ´Ğ»Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¼ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ¼ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğµ (NMT). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ LLM Ğ´Ğ»Ñ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² NMT, Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€ NMT Ğ±ĞµĞ· Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ñ‹ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ LLM Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ¼ NMT. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ½Ğµ ÑƒÑÑ‚ÑƒĞ¿Ğ°ĞµÑ‚ baseline-Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ°, Ğ½Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ² 2.4-6.5 Ñ€Ğ°Ğ· Ğ¸ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑŠĞµĞ¼Ğ° Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ KV-ĞºÑÑˆĞ° Ğ½Ğ° 75%.'}, 'en': {'title': 'Revolutionizing Translation: Merging LLMs with NMT for Efficiency and Quality', 'desc': 'This paper discusses advancements in neural machine translation (NMT) by integrating large language models (LLMs) with traditional NMT architectures. The authors propose a novel approach that utilizes LLMs for encoding while maintaining the existing NMT decoder, enhancing efficiency and optimization. They introduce a new dataset to evaluate the generalization of their translation models across multiple tasks. Results indicate that their method not only improves translation quality but also significantly increases inference speed and reduces memory usage.'}, 'zh': {'title': 'ç»“åˆLLMsä¸NMTï¼Œæå‡ç¿»è¯‘æ•ˆç‡ä¸è´¨é‡', 'desc': 'æœ¬æ–‡æ¢è®¨äº†å¦‚ä½•å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸ç¥ç»æœºå™¨ç¿»è¯‘ï¼ˆNMTï¼‰ç»“åˆï¼Œä»¥æé«˜ç¿»è¯‘æ¨¡å‹çš„é€šç”¨æ€§å’Œæ•ˆç‡ã€‚æˆ‘ä»¬é‡‡ç”¨LLMsè¿›è¡ŒNMTç¼–ç ï¼ŒåŒæ—¶ä¿æŒNMTè§£ç å™¨ä¸å˜ï¼Œå¹¶å¼€å‘äº†é€‚åº”LLMsä¸NMTè§£ç å™¨æ›´å¥½é…åˆçš„æ–¹æ³•ã€‚é€šè¿‡æ„å»ºä¸€ä¸ªåŒ…å«å¤šä»»åŠ¡çš„æ–°æ•°æ®é›†ï¼Œæˆ‘ä»¬è¯„ä¼°äº†æœºå™¨ç¿»è¯‘ç³»ç»Ÿåœ¨ä¸åŒä»»åŠ¡ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç¿»è¯‘è´¨é‡ä¸Šä¸å¤šç§åŸºçº¿ç›¸å½“æˆ–æ›´ä¼˜ï¼ŒåŒæ—¶åœ¨æ¨ç†é€Ÿåº¦ä¸Šæé«˜äº†2.4åˆ°6.5å€ï¼ŒKVç¼“å­˜çš„å†…å­˜å ç”¨å‡å°‘äº†75%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.08478', 'title': 'NullFace: Training-Free Localized Face Anonymization', 'url': 'https://huggingface.co/papers/2503.08478', 'abstract': "Privacy concerns around ever increasing number of cameras are increasing in today's digital age. Although existing anonymization methods are able to obscure identity information, they often struggle to preserve the utility of the images. In this work, we introduce a training-free method for face anonymization that preserves key non-identity-related attributes. Our approach utilizes a pre-trained text-to-image diffusion model without requiring optimization or training. It begins by inverting the input image to recover its initial noise. The noise is then denoised through an identity-conditioned diffusion process, where modified identity embeddings ensure the anonymized face is distinct from the original identity. Our approach also supports localized anonymization, giving users control over which facial regions are anonymized or kept intact. Comprehensive evaluations against state-of-the-art methods show our approach excels in anonymization, attribute preservation, and image quality. Its flexibility, robustness, and practicality make it well-suited for real-world applications. Code and data can be found at https://github.com/hanweikung/nullface .", 'score': 2, 'issue_id': 2659, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 11', 'zh': '3æœˆ11æ—¥'}, 'hash': '150c9c8854f08130', 'authors': ['Han-Wei Kung', 'Tuomas Varanka', 'Terence Sim', 'Nicu Sebe'], 'affiliations': ['National University of Singapore', 'University of Oulu', 'University of Trento'], 'pdf_title_img': 'assets/pdf/title_img/2503.08478.jpg', 'data': {'categories': ['#ethics', '#cv', '#training', '#diffusion'], 'emoji': 'ğŸ­', 'ru': {'title': 'ĞĞ½Ğ¾Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ»Ğ¸Ñ† Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ°Ğ½Ğ¾Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ»Ğ¸Ñ† Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…, Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¿Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ğ½Ğ°Ñ‡Ğ¸Ğ½Ğ°Ñ Ñ Ğ¸Ğ½Ğ²ĞµÑ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ ÑˆÑƒĞ¼Ğ°. Ğ—Ğ°Ñ‚ĞµĞ¼ Ğ¿Ñ€Ğ¾Ğ¸ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ ÑˆÑƒĞ¼Ğ° Ñ ÑƒÑĞ»Ğ¾Ğ²Ğ¸ĞµĞ¼ Ğ¿Ğ¾ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ³Ğ´Ğµ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ°Ğ½Ğ¾Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ»Ğ¸Ñ†Ğ° Ğ¾Ñ‚ Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»Ğ°. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ñ‹, Ğ½Ğµ ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ, Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ°Ğ½Ğ¾Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ¾Ğ².'}, 'en': {'title': 'Effortless Face Anonymization with Preserved Attributes', 'desc': 'This paper presents a novel method for face anonymization that does not require any training, making it efficient and practical. It leverages a pre-trained text-to-image diffusion model to anonymize faces while preserving important non-identity-related features. The method works by inverting the input image to retrieve its initial noise, which is then processed through a diffusion model that modifies identity embeddings. Additionally, it allows users to selectively anonymize specific facial regions, ensuring flexibility and control over the anonymization process.'}, 'zh': {'title': 'æ— è®­ç»ƒçš„é¢éƒ¨åŒ¿ååŒ–æ–¹æ³•ï¼Œä¿æŠ¤éšç§ä¸å›¾åƒè´¨é‡å¹¶å­˜', 'desc': 'åœ¨æ•°å­—æ—¶ä»£ï¼Œè¶Šæ¥è¶Šå¤šçš„æ‘„åƒå¤´å¼•å‘äº†éšç§é—®é¢˜ã€‚ç°æœ‰çš„åŒ¿ååŒ–æ–¹æ³•è™½ç„¶å¯ä»¥æ¨¡ç³Šèº«ä»½ä¿¡æ¯ï¼Œä½†å¾€å¾€éš¾ä»¥ä¿æŒå›¾åƒçš„å®ç”¨æ€§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ— è®­ç»ƒçš„é¢éƒ¨åŒ¿ååŒ–æ–¹æ³•ï¼Œèƒ½å¤Ÿä¿ç•™å…³é”®çš„éèº«ä»½ç›¸å…³å±æ€§ã€‚è¯¥æ–¹æ³•åˆ©ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡èº«ä»½æ¡ä»¶æ‰©æ•£è¿‡ç¨‹å»å™ªå£°ï¼Œç¡®ä¿åŒ¿ååŒ–çš„é¢å­”ä¸åŸå§‹èº«ä»½æ˜æ˜¾ä¸åŒï¼ŒåŒæ—¶æ”¯æŒå±€éƒ¨åŒ¿ååŒ–ï¼Œç”¨æˆ·å¯ä»¥æ§åˆ¶å“ªäº›é¢éƒ¨åŒºåŸŸè¢«åŒ¿ååŒ–æˆ–ä¿ç•™ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.08102', 'title': 'AI-native Memory 2.0: Second Me', 'url': 'https://huggingface.co/papers/2503.08102', 'abstract': 'Human interaction with the external world fundamentally involves the exchange of personal memory, whether with other individuals, websites, applications, or, in the future, AI agents. A significant portion of this interaction is redundant, requiring users to repeatedly provide the same information across different contexts. Existing solutions, such as browser-stored credentials, autofill mechanisms, and unified authentication systems, have aimed to mitigate this redundancy by serving as intermediaries that store and retrieve commonly used user data. The advent of large language models (LLMs) presents an opportunity to redefine memory management through an AI-native paradigm: SECOND ME. SECOND ME acts as an intelligent, persistent memory offload system that retains, organizes, and dynamically utilizes user-specific knowledge. By serving as an intermediary in user interactions, it can autonomously generate context-aware responses, prefill required information, and facilitate seamless communication with external systems, significantly reducing cognitive load and interaction friction. Unlike traditional memory storage solutions, SECOND ME extends beyond static data retention by leveraging LLM-based memory parameterization. This enables structured organization, contextual reasoning, and adaptive knowledge retrieval, facilitating a more systematic and intelligent approach to memory management. As AI-driven personal agents like SECOND ME become increasingly integrated into digital ecosystems, SECOND ME further represents a critical step toward augmenting human-world interaction with persistent, contextually aware, and self-optimizing memory systems. We have open-sourced the fully localizable deployment system at GitHub: https://github.com/Mindverse/Second-Me.', 'score': 2, 'issue_id': 2658, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 11', 'zh': '3æœˆ11æ—¥'}, 'hash': '45b3f3aee8d173f9', 'authors': ['Jiale Wei', 'Xiang Ying', 'Tao Gao', 'Felix Tao', 'Jingbo Shang'], 'affiliations': ['Mindverse.ai'], 'pdf_title_img': 'assets/pdf/title_img/2503.08102.jpg', 'data': {'categories': ['#multimodal', '#agi', '#agents', '#optimization', '#open_source'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'SECOND ME: Ğ˜Ğ˜-Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ Ğ²Ğ°ÑˆĞµĞ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸', 'desc': 'SECOND ME - ÑÑ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ»Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ. ĞĞ½Ğ° Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚, Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·ÑƒĞµÑ‚ Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ, Ğ²Ñ‹ÑÑ‚ÑƒĞ¿Ğ°Ñ Ğ¿Ğ¾ÑÑ€ĞµĞ´Ğ½Ğ¸ĞºĞ¾Ğ¼ Ğ²Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑÑ…. SECOND ME Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾-Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹, Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ñ„Ğ¾Ñ€Ğ¼Ñ‹ Ğ¸ Ğ¾Ğ±Ğ»ĞµĞ³Ñ‡Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ¼Ğ¼ÑƒĞ½Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ñ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğ¼Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼Ğ¸. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹, ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ¹ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğº ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹.'}, 'en': {'title': 'Revolutionizing Memory Management with AI: Meet SECOND ME!', 'desc': 'The paper introduces SECOND ME, an AI-driven memory management system designed to enhance user interactions with digital environments. It addresses the issue of redundant information sharing by autonomously storing and organizing user-specific knowledge, allowing for context-aware responses and prefilled information. Unlike traditional memory solutions, SECOND ME utilizes large language models to enable dynamic knowledge retrieval and contextual reasoning. This innovative approach aims to reduce cognitive load and improve the efficiency of human-AI interactions in various applications.'}, 'zh': {'title': 'æ™ºèƒ½è®°å¿†ç®¡ç†ï¼Œæå‡äººæœºäº’åŠ¨ä½“éªŒ', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºSECOND MEçš„æ™ºèƒ½è®°å¿†ç®¡ç†ç³»ç»Ÿï¼Œæ—¨åœ¨å‡å°‘ç”¨æˆ·åœ¨ä¸å¤–éƒ¨ä¸–ç•Œäº’åŠ¨æ—¶çš„å†—ä½™ä¿¡æ¯è¾“å…¥ã€‚é€šè¿‡åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼ŒSECOND MEèƒ½å¤Ÿæ™ºèƒ½åœ°å­˜å‚¨ã€ç»„ç»‡å’ŒåŠ¨æ€ä½¿ç”¨ç”¨æˆ·ç‰¹å®šçš„çŸ¥è¯†ã€‚ä¸ä¼ ç»Ÿçš„è®°å¿†å­˜å‚¨è§£å†³æ–¹æ¡ˆä¸åŒï¼ŒSECOND MEä¸ä»…ä»…æ˜¯é™æ€æ•°æ®çš„ä¿ç•™ï¼Œè€Œæ˜¯é€šè¿‡ä¸Šä¸‹æ–‡æ¨ç†å’Œè‡ªé€‚åº”çŸ¥è¯†æ£€ç´¢æ¥ä¼˜åŒ–ç”¨æˆ·ä½“éªŒã€‚éšç€AIé©±åŠ¨çš„ä¸ªäººä»£ç†çš„æ™®åŠï¼ŒSECOND MEä»£è¡¨äº†å¢å¼ºäººç±»ä¸ä¸–ç•Œäº’åŠ¨çš„é‡è¦ä¸€æ­¥ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.05066', 'title': 'Capacity-Aware Inference: Mitigating the Straggler Effect in Mixture of\n  Experts', 'url': 'https://huggingface.co/papers/2503.05066', 'abstract': 'The Mixture of Experts (MoE) is an effective architecture for scaling large language models by leveraging sparse expert activation, optimizing the trade-off between performance and efficiency. However, under expert parallelism, MoE suffers from inference inefficiencies due to imbalanced token-to-expert assignment, where some experts are overloaded while others remain underutilized. This imbalance leads to poor resource utilization and increased latency, as the most burdened expert dictates the overall delay, a phenomenon we define as the \\textit{Straggler Effect}. To mitigate this, we propose Capacity-Aware Inference, including two key techniques: (1) \\textit{Capacity-Aware Token Drop}, which discards overloaded tokens to regulate the maximum latency of MoE, and (2) \\textit{Capacity-Aware Token Reroute}, which reallocates overflowed tokens to underutilized experts, balancing the token distribution. These techniques collectively optimize both high-load and low-load expert utilization, leading to a more efficient MoE inference pipeline. Extensive experiments demonstrate the effectiveness of our methods, showing significant improvements in inference efficiency, e.g., 0.2\\% average performance increase and a 1.94times inference speedup on Mixtral-8times7B-Instruct.', 'score': 2, 'issue_id': 2672, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 7', 'zh': '3æœˆ7æ—¥'}, 'hash': 'febfae347962e18e', 'authors': ['Shwai He', 'Weilin Cai', 'Jiayi Huang', 'Ang Li'], 'affiliations': ['The Hong Kong University of Science and Technology (Guangzhou)', 'University of Maryland, College Park'], 'pdf_title_img': 'assets/pdf/title_img/2503.05066.jpg', 'data': {'categories': ['#inference', '#optimization', '#architecture', '#training'], 'emoji': 'âš–ï¸', 'ru': {'title': 'Ğ‘Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ° ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Mixture of Experts (MoE) Ğ´Ğ»Ñ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Capacity-Aware Inference Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ½ĞµÑ€Ğ°Ğ²Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ğ¼Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ½ĞµÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ¸ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´Ğ²Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸: Capacity-Aware Token Drop Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ±Ñ€Ğ°ÑÑ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿ĞµÑ€ĞµĞ³Ñ€ÑƒĞ¶ĞµĞ½Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Capacity-Aware Token Reroute Ğ´Ğ»Ñ Ğ¿ĞµÑ€ĞµÑ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ½ĞµĞ´Ğ¾Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ 0.2% ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ 1.94-ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğµ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Mixtral-8x7B-Instruct.'}, 'en': {'title': 'Optimizing Expert Utilization for Efficient Inference in MoE Models', 'desc': 'The paper discusses the Mixture of Experts (MoE) architecture, which enhances large language models by using sparse expert activation to balance performance and efficiency. It identifies a problem called the Straggler Effect, where some experts are overloaded while others are underused, leading to inefficiencies during inference. To address this, the authors introduce Capacity-Aware Inference techniques, including Capacity-Aware Token Drop and Capacity-Aware Token Reroute, which help manage token distribution among experts. Their experiments show that these methods significantly improve inference efficiency, achieving a notable speedup and slight performance gain.'}, 'zh': {'title': 'æå‡æ··åˆä¸“å®¶æ¨ç†æ•ˆç‡çš„åˆ›æ–°æ–¹æ³•', 'desc': 'æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ˜¯ä¸€ç§æœ‰æ•ˆçš„æ¶æ„ï¼Œç”¨äºé€šè¿‡ç¨€ç–ä¸“å®¶æ¿€æ´»æ¥æ‰©å±•å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä¼˜åŒ–æ€§èƒ½ä¸æ•ˆç‡ä¹‹é—´çš„æƒè¡¡ã€‚ç„¶è€Œï¼Œåœ¨ä¸“å®¶å¹¶è¡Œå¤„ç†ä¸‹ï¼ŒMoEç”±äºä¸å¹³è¡¡çš„ä»¤ç‰Œåˆ°ä¸“å®¶çš„åˆ†é…è€Œé¢ä¸´æ¨ç†æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ï¼Œå¯¼è‡´æŸäº›ä¸“å®¶è¿‡è½½è€Œå…¶ä»–ä¸“å®¶æœªè¢«å……åˆ†åˆ©ç”¨ã€‚è¿™ç§ä¸å¹³è¡¡å¯¼è‡´èµ„æºåˆ©ç”¨ç‡ä½ä¸‹å’Œå»¶è¿Ÿå¢åŠ ï¼Œå› ä¸ºæœ€ç¹å¿™çš„ä¸“å®¶å†³å®šäº†æ•´ä½“å»¶è¿Ÿï¼Œè¿™ç§ç°è±¡è¢«ç§°ä¸ºâ€œæ»åæ•ˆåº”â€ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å®¹é‡æ„ŸçŸ¥æ¨ç†ï¼ŒåŒ…æ‹¬ä¸¤ä¸ªå…³é”®æŠ€æœ¯ï¼šå®¹é‡æ„ŸçŸ¥ä»¤ç‰Œä¸¢å¼ƒå’Œå®¹é‡æ„ŸçŸ¥ä»¤ç‰Œé‡å®šå‘ï¼Œä»è€Œä¼˜åŒ–é«˜è´Ÿè½½å’Œä½è´Ÿè½½ä¸“å®¶çš„åˆ©ç”¨ç‡ï¼Œæå‡MoEæ¨ç†ç®¡é“çš„æ•ˆç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.07639', 'title': 'Mixture of Experts Made Intrinsically Interpretable', 'url': 'https://huggingface.co/papers/2503.07639', 'abstract': 'Neurons in large language models often exhibit polysemanticity, simultaneously encoding multiple unrelated concepts and obscuring interpretability. Instead of relying on post-hoc methods, we present MoE-X, a Mixture-of-Experts (MoE) language model designed to be intrinsically interpretable. Our approach is motivated by the observation that, in language models, wider networks with <PRE_TAG>sparse activations</POST_TAG> are more likely to capture interpretable factors. However, directly training such large sparse networks is computationally prohibitive. MoE architectures offer a scalable alternative by activating only a subset of experts for any given input, inherently aligning with interpretability objectives. In MoE-X, we establish this connection by rewriting the MoE layer as an equivalent sparse, large MLP. This approach enables efficient scaling of the hidden size while maintaining sparsity. To further enhance interpretability, we enforce sparse activation within each expert and redesign the routing mechanism to prioritize experts with the highest activation sparsity. These designs ensure that only the most salient features are routed and processed by the experts. We evaluate MoE-X on chess and natural language tasks, showing that it achieves performance comparable to dense models while significantly improving interpretability. MoE-X achieves a perplexity better than GPT-2, with interpretability surpassing even sparse autoencoder (SAE)-based approaches.', 'score': 2, 'issue_id': 2653, 'pub_date': '2025-03-05', 'pub_date_card': {'ru': '5 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 5', 'zh': '3æœˆ5æ—¥'}, 'hash': '7e9a13248a2692b5', 'authors': ['Xingyi Yang', 'Constantin Venhoff', 'Ashkan Khakzar', 'Christian Schroeder de Witt', 'Puneet K. Dokania', 'Adel Bibi', 'Philip Torr'], 'affiliations': ['National University of Singapore', 'University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2503.07639.jpg', 'data': {'categories': ['#training', '#games', '#architecture', '#interpretability'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'MoE-X: Ğ˜Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğµ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MoE-X - Ğ½Ğ¾Ğ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ½Ğ° Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğµ Mixture-of-Experts. Ğ­Ñ‚Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¹ Ğ¸ ÑĞµĞ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ². MoE-X Ğ¿ĞµÑ€ĞµĞ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ ÑĞ»Ğ¾Ğ¹ MoE ĞºĞ°Ğº ÑĞºĞ²Ğ¸Ğ²Ğ°Ğ»ĞµĞ½Ñ‚Ğ½ÑƒÑ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½ÑƒÑ Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ»Ğ¾Ğ¹Ğ½ÑƒÑ Ğ¿ĞµÑ€Ñ†ĞµĞ¿Ñ‚Ñ€Ğ¾Ğ½Ğ½ÑƒÑ ÑĞµÑ‚ÑŒ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ MoE-X Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ¾Ğ¹ Ñ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ¿Ñ€Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'MoE-X: Enhancing Interpretability in Language Models with Sparse Activations', 'desc': 'This paper introduces MoE-X, a Mixture-of-Experts language model that aims to improve interpretability in large language models by utilizing sparse activations. The authors argue that wider networks with sparse activations can better capture distinct concepts, making them more interpretable. MoE-X efficiently scales by activating only a subset of experts for each input, which aligns with the goal of enhancing interpretability. The model is evaluated on chess and natural language tasks, demonstrating performance on par with dense models while offering superior interpretability compared to existing methods.'}, 'zh': {'title': 'MoE-Xï¼šå¯è§£é‡Šçš„æ··åˆä¸“å®¶è¯­è¨€æ¨¡å‹', 'desc': 'åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ï¼Œç¥ç»å…ƒå¸¸å¸¸è¡¨ç°å‡ºå¤šä¹‰æ€§ï¼ŒåŒæ—¶ç¼–ç å¤šä¸ªæ— å…³çš„æ¦‚å¿µï¼Œå¯¼è‡´å¯è§£é‡Šæ€§å·®ã€‚æˆ‘ä»¬æå‡ºäº†MoE-Xï¼Œè¿™æ˜¯ä¸€ç§æ··åˆä¸“å®¶ï¼ˆMoEï¼‰è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨å†…åœ¨ä¸Šå…·æœ‰å¯è§£é‡Šæ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œå…·æœ‰ç¨€ç–æ¿€æ´»çš„å®½ç½‘ç»œæ›´æœ‰å¯èƒ½æ•æ‰å¯è§£é‡Šçš„å› ç´ ã€‚é€šè¿‡æ¿€æ´»ä»…ä¸€éƒ¨åˆ†ä¸“å®¶ï¼ŒMoEæ¶æ„æä¾›äº†ä¸€ç§å¯æ‰©å±•çš„æ›¿ä»£æ–¹æ¡ˆï¼Œä»è€Œåœ¨ä¿æŒç¨€ç–æ€§çš„åŒæ—¶å®ç°é«˜æ•ˆçš„éšè—å±‚è§„æ¨¡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.07565', 'title': 'Inductive Moment Matching', 'url': 'https://huggingface.co/papers/2503.07565', 'abstract': 'Diffusion models and Flow Matching generate high-quality samples but are slow at inference, and distilling them into few-step models often leads to instability and extensive tuning. To resolve these trade-offs, we propose Inductive Moment Matching (IMM), a new class of generative models for one- or few-step sampling with a single-stage training procedure. Unlike distillation, IMM does not require pre-training initialization and optimization of two networks; and unlike Consistency Models, IMM guarantees distribution-level convergence and remains stable under various hyperparameters and standard model architectures. IMM surpasses diffusion models on ImageNet-256x256 with 1.99 FID using only 8 inference steps and achieves state-of-the-art 2-step FID of 1.98 on CIFAR-10 for a model trained from scratch.', 'score': 1, 'issue_id': 2670, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 10', 'zh': '3æœˆ10æ—¥'}, 'hash': 'ada37e03407170a1', 'authors': ['Linqi Zhou', 'Stefano Ermon', 'Jiaming Song'], 'affiliations': ['Luma AI', 'Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2503.07565.jpg', 'data': {'categories': ['#inference', '#cv', '#training', '#diffusion', '#optimization', '#architecture'], 'emoji': 'ğŸš€', 'ru': {'title': 'IMM: Ğ‘Ñ‹ÑÑ‚Ñ€Ğ°Ñ Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ĞºĞ»Ğ°ÑÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Inductive Moment Matching (IMM). IMM Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ñ‹ Ğ·Ğ° Ğ¾Ğ´Ğ¸Ğ½ Ğ¸Ğ»Ğ¸ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ÑˆĞ°Ğ³Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¾Ğ´Ğ½Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸, IMM Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ²ÑƒÑ… ÑĞµÑ‚ĞµĞ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° ImageNet-256x256 Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¼ FID 1.99 Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ²ÑĞµĞ³Ğ¾ 8 ÑˆĞ°Ğ³Ğ¾Ğ² Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°.'}, 'en': {'title': 'Fast and Stable Sampling with Inductive Moment Matching', 'desc': 'Inductive Moment Matching (IMM) is a new type of generative model designed to produce high-quality samples quickly, addressing the slow inference times of diffusion models and Flow Matching. Unlike traditional distillation methods, IMM simplifies the training process by eliminating the need for pre-training and the optimization of multiple networks. It also ensures stability and convergence at the distribution level, making it robust against changes in hyperparameters and model architectures. IMM demonstrates superior performance, achieving a low FID score on ImageNet and CIFAR-10 with significantly fewer inference steps compared to existing models.'}, 'zh': {'title': 'å½’çº³æ—¶åˆ»åŒ¹é…ï¼šé«˜æ•ˆç¨³å®šçš„ç”Ÿæˆæ¨¡å‹', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ç”Ÿæˆæ¨¡å‹ï¼Œç§°ä¸ºå½’çº³æ—¶åˆ»åŒ¹é…ï¼ˆIMMï¼‰ï¼Œæ—¨åœ¨è§£å†³æ‰©æ•£æ¨¡å‹å’ŒæµåŒ¹é…åœ¨æ¨ç†æ—¶é€Ÿåº¦æ…¢çš„é—®é¢˜ã€‚IMM å…è®¸åœ¨å•é˜¶æ®µè®­ç»ƒè¿‡ç¨‹ä¸­è¿›è¡Œä¸€åˆ°å°‘æ­¥é‡‡æ ·ï¼Œè€Œæ— éœ€é¢„è®­ç»ƒå’Œä¼˜åŒ–ä¸¤ä¸ªç½‘ç»œã€‚ä¸ä¸€è‡´æ€§æ¨¡å‹ä¸åŒï¼ŒIMM ç¡®ä¿äº†åˆ†å¸ƒçº§åˆ«çš„æ”¶æ•›æ€§ï¼Œå¹¶åœ¨å„ç§è¶…å‚æ•°å’Œæ ‡å‡†æ¨¡å‹æ¶æ„ä¸‹ä¿æŒç¨³å®šã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒIMM åœ¨ ImageNet-256x256 æ•°æ®é›†ä¸Šä»¥ 8 æ¬¡æ¨ç†æ­¥éª¤è¾¾åˆ°äº† 1.99 çš„ FIDï¼Œä¸”åœ¨ CIFAR-10 æ•°æ®é›†ä¸Šä»¥ 2 æ¬¡æ¨ç†æ­¥éª¤è¾¾åˆ°äº† 1.98 çš„æœ€å…ˆè¿› FIDã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.07154', 'title': 'Ideas in Inference-time Scaling can Benefit Generative Pre-training\n  Algorithms', 'url': 'https://huggingface.co/papers/2503.07154', 'abstract': "Recent years have seen significant advancements in foundation models through generative pre-training, yet algorithmic innovation in this space has largely stagnated around autoregressive models for discrete signals and diffusion models for continuous signals. This stagnation creates a bottleneck that prevents us from fully unlocking the potential of rich multi-modal data, which in turn limits the progress on multimodal intelligence. We argue that an inference-first perspective, which prioritizes scaling efficiency during inference time across sequence length and refinement steps, can inspire novel generative pre-training algorithms. Using Inductive Moment Matching (IMM) as a concrete example, we demonstrate how addressing limitations in diffusion models' inference process through targeted modifications yields a stable, single-stage algorithm that achieves superior sample quality with over an order of magnitude greater inference efficiency.", 'score': 1, 'issue_id': 2671, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 10', 'zh': '3æœˆ10æ—¥'}, 'hash': '2c3a8d77ccb8e716', 'authors': ['Jiaming Song', 'Linqi Zhou'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2503.07154.jpg', 'data': {'categories': ['#training', '#architecture', '#diffusion', '#inference', '#optimization', '#multimodal'], 'emoji': 'ğŸš€', 'ru': {'title': 'ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ: Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°', 'desc': "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¾Ğ±ÑÑƒĞ¶Ğ´Ğ°ĞµÑ‚ÑÑ ÑÑ‚Ğ°Ğ³Ğ½Ğ°Ñ†Ğ¸Ñ Ğ² Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ğ¸ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ² Ğ´Ğ»Ñ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ½ĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ 'inference-first', Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ÑÑ Ğ½Ğ° ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ° Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Inductive Moment Matching (IMM) Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ, ĞºĞ°Ğº Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¿Ñ€Ğ¸Ğ²ĞµÑÑ‚Ğ¸ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ´Ğ½Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ°. Ğ­Ñ‚Ğ¾Ñ‚ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑÑĞ¼Ğ¿Ğ»Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞµĞ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°."}, 'en': {'title': 'Unlocking Multi-Modal Intelligence with Efficient Inference', 'desc': 'This paper discusses the current limitations in generative pre-training models, particularly focusing on autoregressive and diffusion models. It highlights how these limitations hinder the effective use of multi-modal data, which is essential for advancing multimodal intelligence. The authors propose an inference-first approach that emphasizes improving efficiency during the inference phase, which can lead to innovative generative algorithms. They introduce Inductive Moment Matching (IMM) as a method to enhance diffusion models, resulting in a more efficient and higher quality sampling process.'}, 'zh': {'title': 'æ¨ç†ä¼˜å…ˆï¼Œæ¿€å‘ç”Ÿæˆé¢„è®­ç»ƒæ–°ç®—æ³•', 'desc': 'è¿‘å¹´æ¥ï¼ŒåŸºç¡€æ¨¡å‹é€šè¿‡ç”Ÿæˆé¢„è®­ç»ƒå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨è‡ªå›å½’æ¨¡å‹å’Œæ‰©æ•£æ¨¡å‹çš„ç®—æ³•åˆ›æ–°æ–¹é¢åœæ»ä¸å‰ã€‚è¿™ç§åœæ»é€ æˆäº†ç“¶é¢ˆï¼Œé™åˆ¶äº†å¤šæ¨¡æ€æ•°æ®çš„æ½œåŠ›ï¼Œè¿›è€Œå½±å“äº†å¤šæ¨¡æ€æ™ºèƒ½çš„å‘å±•ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ä»¥æ¨ç†ä¸ºå…ˆçš„è§†è§’ï¼Œå¼ºè°ƒåœ¨æ¨ç†æ—¶é—´å†…çš„è§„æ¨¡æ•ˆç‡ï¼Œä»¥æ¿€å‘æ–°çš„ç”Ÿæˆé¢„è®­ç»ƒç®—æ³•ã€‚é€šè¿‡å½’çº³æ—¶åˆ»åŒ¹é…ï¼ˆIMMï¼‰ä½œä¸ºå…·ä½“ä¾‹å­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•é€šè¿‡é’ˆå¯¹æ€§ä¿®æ”¹æ‰©æ•£æ¨¡å‹çš„æ¨ç†è¿‡ç¨‹ï¼Œå¾—åˆ°ä¸€ç§ç¨³å®šçš„å•é˜¶æ®µç®—æ³•ï¼Œæ˜¾è‘—æé«˜æ ·æœ¬è´¨é‡å’Œæ¨ç†æ•ˆç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.08037', 'title': 'ObjectMover: Generative Object Movement with Video Prior', 'url': 'https://huggingface.co/papers/2503.08037', 'abstract': 'Simple as it seems, moving an object to another location within an image is, in fact, a challenging image-editing task that requires re-harmonizing the lighting, adjusting the pose based on perspective, accurately filling occluded regions, and ensuring coherent synchronization of shadows and reflections while maintaining the object identity. In this paper, we present ObjectMover, a generative model that can perform object movement in highly challenging scenes. Our key insight is that we model this task as a sequence-to-sequence problem and fine-tune a video generation model to leverage its knowledge of consistent object generation across video frames. We show that with this approach, our model is able to adjust to complex real-world scenarios, handling extreme lighting harmonization and object effect movement. As large-scale data for object movement are unavailable, we construct a data generation pipeline using a modern game engine to synthesize high-quality data pairs. We further propose a multi-task learning strategy that enables training on real-world video data to improve the model generalization. Through extensive experiments, we demonstrate that ObjectMover achieves outstanding results and adapts well to real-world scenarios.', 'score': 0, 'issue_id': 2667, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 11', 'zh': '3æœˆ11æ—¥'}, 'hash': '821b6b6ca9c0eb61', 'authors': ['Xin Yu', 'Tianyu Wang', 'Soo Ye Kim', 'Paul Guerrero', 'Xi Chen', 'Qing Liu', 'Zhe Lin', 'Xiaojuan Qi'], 'affiliations': ['Adobe Research', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2503.08037.jpg', 'data': {'categories': ['#games', '#data', '#training', '#synthetic', '#video', '#optimization', '#dataset'], 'emoji': 'ğŸ”„', 'ru': {'title': 'ObjectMover: Ğ˜Ğ˜ Ğ´Ğ»Ñ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ObjectMover - Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¿ĞµÑ€ĞµĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ sequence-to-sequence Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ÑÑ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ· Ğ¸Ğ³Ñ€Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ´Ğ²Ğ¸Ğ¶ĞºĞ° Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾. ObjectMover ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ÑÑ ÑĞ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼Ğ¸ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑĞ¼Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ³Ğ°Ñ€Ğ¼Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¾Ğ² Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°.'}, 'en': {'title': 'Seamless Object Movement in Images with ObjectMover', 'desc': "This paper introduces ObjectMover, a generative model designed to move objects within images while addressing challenges like lighting harmonization and perspective adjustments. The authors treat the task as a sequence-to-sequence problem, utilizing a fine-tuned video generation model to ensure consistent object representation across frames. To overcome the lack of large-scale data for object movement, they create a data generation pipeline using a game engine to produce high-quality training pairs. Additionally, a multi-task learning strategy is proposed to enhance the model's performance on real-world video data, leading to impressive results in complex scenarios."}, 'zh': {'title': 'ObjectMoverï¼šæ™ºèƒ½ç‰©ä½“ç§»åŠ¨çš„è§£å†³æ–¹æ¡ˆ', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºObjectMoverçš„ç”Ÿæˆæ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³å›¾åƒä¸­ç‰©ä½“ç§»åŠ¨çš„å¤æ‚ä»»åŠ¡ã€‚è¯¥æ¨¡å‹å°†ç‰©ä½“ç§»åŠ¨è§†ä¸ºåºåˆ—åˆ°åºåˆ—çš„é—®é¢˜ï¼Œå¹¶å¯¹è§†é¢‘ç”Ÿæˆæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»¥åˆ©ç”¨å…¶åœ¨è§†é¢‘å¸§é—´ä¸€è‡´ç”Ÿæˆç‰©ä½“çš„çŸ¥è¯†ã€‚ç”±äºç¼ºä¹å¤§è§„æ¨¡çš„ç‰©ä½“ç§»åŠ¨æ•°æ®ï¼Œæˆ‘ä»¬ä½¿ç”¨ç°ä»£æ¸¸æˆå¼•æ“æ„å»ºæ•°æ®ç”Ÿæˆç®¡é“ï¼Œåˆæˆé«˜è´¨é‡çš„æ•°æ®å¯¹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¤šä»»åŠ¡å­¦ä¹ ç­–ç•¥ï¼Œä»¥ä¾¿åœ¨çœŸå®è§†é¢‘æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.05037', 'title': 'Collapse of Dense Retrievers: Short, Early, and Literal Biases\n  Outranking Factual Evidence', 'url': 'https://huggingface.co/papers/2503.05037', 'abstract': "Dense retrieval models are commonly used in Information Retrieval (IR) applications, such as Retrieval-Augmented Generation (RAG). Since they often serve as the first step in these systems, their robustness is critical to avoid failures. In this work, by repurposing a relation extraction dataset (e.g. Re-DocRED), we design controlled experiments to quantify the impact of heuristic biases, such as favoring shorter documents, in retrievers like Dragon+ and Contriever. Our findings reveal significant vulnerabilities: retrievers often rely on superficial patterns like over-prioritizing document beginnings, shorter documents, repeated entities, and literal matches. Additionally, they tend to overlook whether the document contains the query's answer, lacking deep semantic understanding. Notably, when multiple biases combine, models exhibit catastrophic performance degradation, selecting the answer-containing document in less than 3% of cases over a biased document without the answer. Furthermore, we show that these biases have direct consequences for downstream applications like RAG, where retrieval-preferred documents can mislead LLMs, resulting in a 34% performance drop than not providing any documents at all.", 'score': 0, 'issue_id': 2667, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 6', 'zh': '3æœˆ6æ—¥'}, 'hash': '862c1dc027b05f42', 'authors': ['Mohsen Fayyaz', 'Ali Modarressi', 'Hinrich Schuetze', 'Nanyun Peng'], 'affiliations': ['CIS, LMU Munich', 'Munich Center for Machine Learning', 'University of California, Los Angeles'], 'pdf_title_img': 'assets/pdf/title_img/2503.05037.jpg', 'data': {'categories': ['#benchmark', '#security', '#rag', '#interpretability', '#dataset'], 'emoji': 'ğŸ•µï¸', 'ru': {'title': 'Ğ£ÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°: Ğ¿Ğ¾Ñ‡ĞµĞ¼Ñƒ ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¿Ğ¾Ğ´Ğ²ĞµÑÑ‚Ğ¸', 'desc': 'Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ÑĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°, Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼Ñ‹Ñ… Ğ² Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ¸ÑĞºĞµ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğµ, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ°Ğº Dragon+ Ğ¸ Contriever Ğ¿Ğ¾Ğ´Ğ²ĞµÑ€Ğ¶ĞµĞ½Ñ‹ ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼, Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ĞµĞµ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ’Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ¾Ğ¿Ğ¸Ñ€Ğ°ÑÑ‚ÑÑ Ğ½Ğ° Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹, Ğ¸Ğ³Ğ½Ğ¾Ñ€Ğ¸Ñ€ÑƒÑ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ğ¹ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ ÑĞ¼Ñ‹ÑĞ». Ğ­Ñ‚Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğº ĞºĞ°Ñ‚Ğ°ÑÑ‚Ñ€Ğ¾Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ¿Ğ°Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ½Ğ¸ÑÑ…Ğ¾Ğ´ÑÑ‰Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº RAG.'}, 'en': {'title': 'Uncovering Biases in Dense Retrieval Models for Better Information Retrieval', 'desc': 'This paper investigates the weaknesses of dense retrieval models used in Information Retrieval, particularly in systems like Retrieval-Augmented Generation (RAG). By analyzing a relation extraction dataset, the authors conduct experiments to identify how heuristic biases, such as favoring shorter documents, affect the performance of retrievers like Dragon+ and Contriever. The results show that these models often depend on superficial patterns and fail to ensure that retrieved documents contain the relevant answers, leading to poor retrieval outcomes. The study highlights that when multiple biases are present, the performance can drastically decline, negatively impacting downstream applications that rely on accurate document retrieval.'}, 'zh': {'title': 'æ­ç¤ºå¯†é›†æ£€ç´¢æ¨¡å‹çš„è„†å¼±æ€§', 'desc': 'æœ¬æ–‡æ¢è®¨äº†å¯†é›†æ£€ç´¢æ¨¡å‹åœ¨ä¿¡æ¯æ£€ç´¢åº”ç”¨ä¸­çš„è„†å¼±æ€§ï¼Œå°¤å…¶æ˜¯åœ¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿä¸­çš„é‡è¦æ€§ã€‚é€šè¿‡é‡æ–°åˆ©ç”¨å…³ç³»æå–æ•°æ®é›†ï¼Œè®¾è®¡äº†æ§åˆ¶å®éªŒæ¥é‡åŒ–å¯å‘å¼åè§å¯¹æ£€ç´¢å™¨çš„å½±å“ã€‚ç ”ç©¶å‘ç°ï¼Œæ£€ç´¢å™¨å¾€å¾€ä¾èµ–è¡¨é¢æ¨¡å¼ï¼Œå¦‚è¿‡åº¦ä¼˜å…ˆè€ƒè™‘æ–‡æ¡£å¼€å¤´ã€è¾ƒçŸ­æ–‡æ¡£å’Œå­—é¢åŒ¹é…ï¼Œç¼ºä¹æ·±å±‚è¯­ä¹‰ç†è§£ã€‚å¤šä¸ªåè§ç»“åˆæ—¶ï¼Œæ¨¡å‹æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œå¯¼è‡´é€‰æ‹©åŒ…å«ç­”æ¡ˆçš„æ–‡æ¡£çš„æ¦‚ç‡ä½äº3%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.03734', 'title': 'OTTER: A Vision-Language-Action Model with Text-Aware Visual Feature\n  Extraction', 'url': 'https://huggingface.co/papers/2503.03734', 'abstract': 'Vision-Language-Action (VLA) models aim to predict robotic actions based on visual observations and language instructions. Existing approaches require fine-tuning pre-trained visionlanguage models (VLMs) as visual and language features are independently fed into downstream policies, degrading the pre-trained semantic alignments. We propose OTTER, a novel VLA architecture that leverages these existing alignments through explicit, text-aware visual feature extraction. Instead of processing all visual features, OTTER selectively extracts and passes only task-relevant visual features that are semantically aligned with the language instruction to the policy transformer. This allows OTTER to keep the pre-trained vision-language encoders frozen. Thereby, OTTER preserves and utilizes the rich semantic understanding learned from large-scale pre-training, enabling strong zero-shot generalization capabilities. In simulation and real-world experiments, OTTER significantly outperforms existing VLA models, demonstrating strong zeroshot generalization to novel objects and environments. Video, code, checkpoints, and dataset: https://ottervla.github.io/.', 'score': 0, 'issue_id': 2668, 'pub_date': '2025-03-05', 'pub_date_card': {'ru': '5 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 5', 'zh': '3æœˆ5æ—¥'}, 'hash': 'a6a36002652e8be7', 'authors': ['Huang Huang', 'Fangchen Liu', 'Letian Fu', 'Tingfan Wu', 'Mustafa Mukadam', 'Jitendra Malik', 'Ken Goldberg', 'Pieter Abbeel'], 'affiliations': ['Meta AI', 'University of California, Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2503.03734.jpg', 'data': {'categories': ['#agi', '#architecture', '#transfer_learning', '#video', '#multimodal', '#training', '#agents'], 'emoji': 'ğŸ¦¦', 'ru': {'title': 'OTTER: Ğ£Ğ¼Ğ½Ğ¾Ğµ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹', 'desc': 'OTTER - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ·Ñ€ĞµĞ½Ğ¸Ñ-ÑĞ·Ñ‹ĞºĞ°-Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ (VLA), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ²ÑĞ·Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ·Ñ€ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ¼, Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°Ñ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸. OTTER ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ·Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ñ‹ Ğ·Ñ€ĞµĞ½Ğ¸Ñ-ÑĞ·Ñ‹ĞºĞ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ³Ğ°Ñ‚Ğ¾Ğµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ. Ğ’ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ñ… OTTER Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ VLA, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ ÑĞ¸Ğ»ÑŒĞ½ÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ½ÑƒĞ»ĞµĞ²Ğ¾Ğ¼ Ğ²Ñ‹ÑÑ‚Ñ€ĞµĞ»Ğµ.'}, 'en': {'title': 'OTTER: Enhancing Robotic Actions with Smart Visual-Language Alignment', 'desc': 'The paper introduces OTTER, a new Vision-Language-Action (VLA) model designed to enhance robotic action prediction using visual inputs and language instructions. Unlike traditional methods that require fine-tuning of pre-trained vision-language models, OTTER maintains the integrity of these models by keeping them frozen and selectively extracting only the relevant visual features aligned with the language commands. This approach allows OTTER to leverage the rich semantic knowledge from large-scale pre-training, leading to improved performance in zero-shot scenarios. Experimental results show that OTTER outperforms existing VLA models in both simulated and real-world tasks, demonstrating its ability to generalize to new objects and environments without additional training.'}, 'zh': {'title': 'OTTERï¼šæå‡æœºå™¨äººåŠ¨ä½œé¢„æµ‹çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹OTTERï¼Œæ—¨åœ¨æ ¹æ®è§†è§‰è§‚å¯Ÿå’Œè¯­è¨€æŒ‡ä»¤é¢„æµ‹æœºå™¨äººåŠ¨ä½œã€‚OTTERé€šè¿‡æ˜¾å¼çš„æ–‡æœ¬æ„ŸçŸ¥è§†è§‰ç‰¹å¾æå–ï¼Œåˆ©ç”¨ç°æœ‰çš„è¯­ä¹‰å¯¹é½ï¼Œé¿å…äº†å¯¹é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹çš„å¾®è°ƒã€‚è¯¥æ¨¡å‹é€‰æ‹©æ€§åœ°æå–ä¸ä»»åŠ¡ç›¸å…³çš„è§†è§‰ç‰¹å¾ï¼Œå¹¶å°†å…¶ä¼ é€’ç»™ç­–ç•¥å˜æ¢å™¨ï¼Œä»è€Œä¿æŒé¢„è®­ç»ƒçš„è§†è§‰-è¯­è¨€ç¼–ç å™¨ä¸å˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOTTERåœ¨æ–°ç‰©ä½“å’Œç¯å¢ƒä¸Šå±•ç°äº†å¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰çš„VLAæ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.21263', 'title': 'RuCCoD: Towards Automated ICD Coding in Russian', 'url': 'https://huggingface.co/papers/2502.21263', 'abstract': 'This study investigates the feasibility of automating clinical coding in Russian, a language with limited biomedical resources. We present a new dataset for ICD coding, which includes diagnosis fields from electronic health records (EHRs) annotated with over 10,000 entities and more than 1,500 unique ICD codes. This dataset serves as a benchmark for several state-of-the-art models, including BERT, LLaMA with LoRA, and RAG, with additional experiments examining transfer learning across domains (from PubMed abstracts to medical diagnosis) and terminologies (from UMLS concepts to ICD codes). We then apply the best-performing model to label an in-house EHR dataset containing patient histories from 2017 to 2021. Our experiments, conducted on a carefully curated test set, demonstrate that training with the automated predicted codes leads to a significant improvement in accuracy compared to manually annotated data from physicians. We believe our findings offer valuable insights into the potential for automating clinical coding in resource-limited languages like Russian, which could enhance clinical efficiency and data accuracy in these contexts.', 'score': 110, 'issue_id': 2617, 'pub_date': '2025-02-28', 'pub_date_card': {'ru': '28 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 28', 'zh': '2æœˆ28æ—¥'}, 'hash': '89ad8229208a4f98', 'authors': ['Aleksandr Nesterov', 'Andrey Sakhovskiy', 'Ivan Sviridov', 'Airat Valiev', 'Vladimir Makharev', 'Petr Anokhin', 'Galina Zubkova', 'Elena Tutubalina'], 'affiliations': ['AIRI, Moscow, Russia', 'HSE University, Moscow, Russia', 'ISP RAS Research Center for Trusted Artificial Intelligence, Moscow, Russia', 'Sber AI Lab, Moscow, Russia', 'Sber AI, Moscow, Russia'], 'pdf_title_img': 'assets/pdf/title_img/2502.21263.jpg', 'data': {'categories': ['#dataset', '#transfer_learning', '#benchmark', '#science', '#low_resource', '#healthcare', '#training'], 'emoji': 'ğŸ¥', 'ru': {'title': 'ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ñ€ÑƒÑÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ: Ğ¿Ñ€Ğ¾Ñ€Ñ‹Ğ² Ğ² ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ñ€ÑƒÑÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ ĞœĞšĞ‘, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ĞµĞµ 10000 ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ 1500 ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ĞºĞ¾Ğ´Ğ¾Ğ² ĞœĞšĞ‘. ĞŸÑ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº BERT, LLaMA Ñ LoRA Ğ¸ RAG, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ¿ĞµÑ€ĞµĞ½Ğ¾Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ğ¼Ğ¸ Ğ¸ Ñ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸ÑĞ¼Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ´Ğ°Ñ… Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¾Ğ¹ Ğ²Ñ€Ğ°Ñ‡ĞµĞ¹.'}, 'en': {'title': 'Automating Clinical Coding: A Leap for Russian Healthcare', 'desc': 'This paper explores the automation of clinical coding in the Russian language, which lacks extensive biomedical resources. It introduces a new dataset for ICD coding, featuring over 10,000 annotated entities and 1,500 unique ICD codes derived from electronic health records. The study benchmarks various advanced models, including BERT and LLaMA, and investigates transfer learning from different medical domains and terminologies. Results show that using automated coding significantly improves accuracy over traditional manual coding by physicians, highlighting the potential for enhanced clinical efficiency in resource-limited settings.'}, 'zh': {'title': 'è‡ªåŠ¨åŒ–ä¸´åºŠç¼–ç ï¼šæå‡ä¿„è¯­åŒ»ç–—æ•ˆç‡çš„å¸Œæœ›', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†åœ¨ä¿„è¯­ä¸­è‡ªåŠ¨åŒ–ä¸´åºŠç¼–ç çš„å¯è¡Œæ€§ï¼Œä¿„è¯­çš„ç”Ÿç‰©åŒ»å­¦èµ„æºç›¸å¯¹æœ‰é™ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„ICDç¼–ç æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«æ¥è‡ªç”µå­å¥åº·è®°å½•ï¼ˆEHRï¼‰çš„è¯Šæ–­å­—æ®µï¼Œæ ‡æ³¨äº†è¶…è¿‡10,000ä¸ªå®ä½“å’Œ1,500å¤šä¸ªç‹¬ç‰¹çš„ICDä»£ç ã€‚é€šè¿‡å¯¹å¤šç§å…ˆè¿›æ¨¡å‹ï¼ˆå¦‚BERTã€LLaMAä¸LoRAã€RAGï¼‰çš„åŸºå‡†æµ‹è¯•ï¼Œä»¥åŠè·¨é¢†åŸŸå’Œæœ¯è¯­çš„è¿ç§»å­¦ä¹ å®éªŒï¼Œæˆ‘ä»¬éªŒè¯äº†æ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨è‡ªåŠ¨é¢„æµ‹çš„ä»£ç è¿›è¡Œè®­ç»ƒï¼Œç›¸è¾ƒäºåŒ»ç”Ÿæ‰‹åŠ¨æ ‡æ³¨çš„æ•°æ®ï¼Œæ˜¾è‘—æé«˜äº†å‡†ç¡®æ€§ï¼Œæ˜¾ç¤ºäº†åœ¨èµ„æºæœ‰é™çš„è¯­è¨€ä¸­è‡ªåŠ¨åŒ–ä¸´åºŠç¼–ç çš„æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.05236', 'title': 'Unified Reward Model for Multimodal Understanding and Generation', 'url': 'https://huggingface.co/papers/2503.05236', 'abstract': 'Recent advances in human preference alignment have significantly enhanced multimodal generation and understanding. A key approach is training reward models to guide preference optimization. However, existing models are often task-specific, limiting their adaptability across diverse visual applications. We also argue that jointly learning to assess multiple tasks may foster a synergistic effect, where improved image understanding enhances image generation assessment, and refined image evaluation benefits video assessment through better frame analysis. To this end, this paper proposes UnifiedReward, the first unified reward model for multimodal understanding and generation assessment, enabling both pairwise ranking and pointwise scoring, which can be employed for vision model preference alignment. Specifically, (1) we first develop UnifiedReward on our constructed large-scale human preference dataset, including both image and video generation/understanding tasks. (2) Then, it is utilized to automatically construct high-quality preference pair data based on the vision models, fine-gradually filtering their outputs through pair ranking and point sifting. (3) Finally, these data are used for their preference alignment through Direct Preference Optimization (DPO). Experimental results demonstrate that joint learning to assess diverse visual tasks can lead to substantial mutual benefits and we apply our pipeline to both image and video understanding/generation tasks, significantly improving the performance in each domain.', 'score': 86, 'issue_id': 2607, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 7', 'zh': '3æœˆ7æ—¥'}, 'hash': '6ebf61a6777b8e4d', 'authors': ['Yibin Wang', 'Yuhang Zang', 'Hao Li', 'Cheng Jin', 'Jiaqi Wang'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2503.05236.jpg', 'data': {'categories': ['#multimodal', '#video', '#cv', '#alignment', '#dataset', '#rlhf', '#rag'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… AI-ÑĞ¸ÑÑ‚ĞµĞ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ UnifiedReward - Ğ¿ĞµÑ€Ğ²ÑƒÑ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑÑ…, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰ĞµĞ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¿Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. UnifiedReward Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ. Ğ—Ğ°Ñ‚ĞµĞ¼ ÑÑ‚Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ÑÑ Ğ´Ğ»Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Direct Preference Optimization (DPO).'}, 'en': {'title': 'UnifiedReward: Enhancing Multimodal Learning through Joint Preference Alignment', 'desc': 'This paper introduces UnifiedReward, a novel reward model designed to enhance multimodal understanding and generation in machine learning. It addresses the limitation of existing task-specific models by enabling joint learning across various visual tasks, which improves both image and video assessments. The model is trained on a large-scale human preference dataset and utilizes techniques like pairwise ranking and pointwise scoring for effective preference alignment. Experimental results show that this unified approach leads to significant performance improvements in both image and video tasks, demonstrating the benefits of synergistic learning.'}, 'zh': {'title': 'ç»Ÿä¸€å¥–åŠ±æ¨¡å‹ï¼Œæå‡å¤šæ¨¡æ€ç†è§£ä¸ç”Ÿæˆ', 'desc': 'æœ€è¿‘åœ¨äººç±»åå¥½å¯¹é½æ–¹é¢çš„è¿›å±•æ˜¾è‘—æå‡äº†å¤šæ¨¡æ€ç”Ÿæˆå’Œç†è§£çš„èƒ½åŠ›ã€‚å…³é”®æ–¹æ³•æ˜¯è®­ç»ƒå¥–åŠ±æ¨¡å‹æ¥æŒ‡å¯¼åå¥½ä¼˜åŒ–ã€‚ç„¶è€Œï¼Œç°æœ‰æ¨¡å‹é€šå¸¸æ˜¯ç‰¹å®šäºä»»åŠ¡çš„ï¼Œé™åˆ¶äº†å®ƒä»¬åœ¨ä¸åŒè§†è§‰åº”ç”¨ä¸­çš„é€‚åº”æ€§ã€‚æœ¬æ–‡æå‡ºäº†UnifiedRewardï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç”¨äºå¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆè¯„ä¼°çš„ç»Ÿä¸€å¥–åŠ±æ¨¡å‹ï¼Œèƒ½å¤ŸåŒæ—¶è¿›è¡Œæˆå¯¹æ’åå’Œé€ç‚¹è¯„åˆ†ï¼Œä»è€Œå®ç°è§†è§‰æ¨¡å‹çš„åå¥½å¯¹é½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.05500', 'title': 'EuroBERT: Scaling Multilingual Encoders for European Languages', 'url': 'https://huggingface.co/papers/2503.05500', 'abstract': 'General-purpose multilingual vector representations, used in retrieval, regression and classification, are traditionally obtained from bidirectional encoder models. Despite their wide applicability, encoders have been recently overshadowed by advances in generative decoder-only models. However, many innovations driving this progress are not inherently tied to decoders. In this paper, we revisit the development of multilingual encoders through the lens of these advances, and introduce EuroBERT, a family of multilingual encoders covering European and widely spoken global languages. Our models outperform existing alternatives across a diverse range of tasks, spanning multilingual capabilities, mathematics, and coding, and natively supporting sequences of up to 8,192 tokens. We also examine the design decisions behind EuroBERT, offering insights into our dataset composition and training pipeline. We publicly release the EuroBERT models, including intermediate training checkpoints, together with our training framework.', 'score': 57, 'issue_id': 2613, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 7', 'zh': '3æœˆ7æ—¥'}, 'hash': 'befb9ffdb6a6cd73', 'authors': ['Nicolas Boizard', 'Hippolyte Gisserot-Boukhlef', 'Duarte M. Alves', 'AndrÃ© Martins', 'Ayoub Hammal', 'Caio Corro', 'CÃ©line Hudelot', 'Emmanuel Malherbe', 'Etienne Malaboeuf', 'Fanny Jourdan', 'Gabriel Hautreux', 'JoÃ£o Alves', 'Kevin El-Haddad', 'Manuel Faysse', 'Maxime Peyrard', 'Nuno M. Guerreiro', 'Patrick Fernandes', 'Ricardo Rei', 'Pierre Colombo'], 'affiliations': ['Artefact', 'CINES', 'CNRS', 'Carnegie Mellon University', 'Diabolocom', 'Equall', 'INSA Rennes', 'IRISA', 'IRT Saint Exupery', 'ISIA Lab', 'Illuin Technology', 'Instituto Superior Tecnico & Universidade de Lisboa (Lisbon ELLIS Unit)', 'Instituto de Telecomunicacoes', 'LISN', 'MICS, CentraleSupelec, Universite Paris-Saclay', 'Unbabel', 'Universite Grenoble Alpes, Grenoble INP, LIG', 'Universite Paris-Saclay'], 'pdf_title_img': 'assets/pdf/title_img/2503.05500.jpg', 'data': {'categories': ['#multilingual', '#open_source', '#architecture', '#training', '#dataset', '#long_context'], 'emoji': 'ğŸŒ', 'ru': {'title': 'EuroBERT: Ğ’Ğ¾Ğ·Ñ€Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ² Ğ² ÑĞ¿Ğ¾Ñ…Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ EuroBERT - ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ² Ğ´Ğ»Ñ ĞµĞ²Ñ€Ğ¾Ğ¿ĞµĞ¹ÑĞºĞ¸Ñ… Ğ¸ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ². Ğ­Ñ‚Ğ¸ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ñ‹ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºÑƒ Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ EuroBERT Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ¾Ğ¹ Ğ´Ğ¾ 8192 Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ğ±Ñ‹Ğ»Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ñ‹ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ñ… Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ğ¸ pipeline Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'EuroBERT: Advancing Multilingual Encoders for Diverse Tasks', 'desc': 'This paper presents EuroBERT, a new family of multilingual encoder models designed to improve performance in various tasks such as retrieval, regression, and classification. Unlike traditional models that rely heavily on bidirectional encoders, EuroBERT leverages recent advancements in generative models while maintaining the strengths of encoders. The models are capable of handling sequences of up to 8,192 tokens and demonstrate superior performance across multilingual tasks, mathematics, and coding challenges. The authors also provide insights into the dataset and training processes used to develop EuroBERT, along with public access to the models and training framework.'}, 'zh': {'title': 'EuroBERTï¼šæå‡å¤šè¯­è¨€å¤„ç†çš„æ–°é€‰æ‹©', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„å¤šè¯­è¨€ç¼–ç å™¨æ¨¡å‹ï¼Œç§°ä¸ºEuroBERTï¼Œæ—¨åœ¨æå‡å¤šè¯­è¨€æ£€ç´¢ã€å›å½’å’Œåˆ†ç±»ä»»åŠ¡çš„æ€§èƒ½ã€‚å°½ç®¡ç”Ÿæˆè§£ç å™¨æ¨¡å‹è¿‘å¹´æ¥å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†æˆ‘ä»¬è®¤ä¸ºå¤šè¯­è¨€ç¼–ç å™¨ä»ç„¶å…·æœ‰é‡è¦ä»·å€¼ã€‚EuroBERTè¦†ç›–äº†æ¬§æ´²åŠå¹¿æ³›ä½¿ç”¨çš„å…¨çƒè¯­è¨€ï¼Œå¹¶åœ¨å¤šç§ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºç°æœ‰æ¨¡å‹ã€‚æˆ‘ä»¬è¿˜åˆ†äº«äº†EuroBERTçš„è®¾è®¡å†³ç­–ã€æ•°æ®é›†æ„æˆå’Œè®­ç»ƒæµç¨‹ï¼Œå¹¶å…¬å¼€å‘å¸ƒäº†æ¨¡å‹åŠå…¶è®­ç»ƒæ¡†æ¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.05085', 'title': 'S2S-Arena, Evaluating Speech2Speech Protocols on Instruction Following\n  with Paralinguistic Information', 'url': 'https://huggingface.co/papers/2503.05085', 'abstract': 'The rapid development of large language models (LLMs) has brought significant attention to speech models, particularly recent progress in speech2speech protocols supporting speech input and output. However, the existing benchmarks adopt automatic text-based evaluators for evaluating the instruction following ability of these models lack consideration for paralinguistic information in both speech understanding and generation. To address these issues, we introduce S2S-Arena, a novel arena-style S2S benchmark that evaluates instruction-following capabilities with paralinguistic information in both speech-in and speech-out across real-world tasks. We design 154 samples that fused TTS and live recordings in four domains with 21 tasks and manually evaluate existing popular speech models in an arena-style manner. The experimental results show that: (1) in addition to the superior performance of GPT-4o, the speech model of cascaded ASR, LLM, and TTS outperforms the jointly trained model after text-speech alignment in speech2speech protocols; (2) considering paralinguistic information, the knowledgeability of the speech model mainly depends on the LLM backbone, and the multilingual support of that is limited by the speech module; (3) excellent speech models can already understand the paralinguistic information in speech input, but generating appropriate audio with paralinguistic information is still a challenge.', 'score': 39, 'issue_id': 2619, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 7', 'zh': '3æœˆ7æ—¥'}, 'hash': 'aa9d1f284b6fe901', 'authors': ['Feng Jiang', 'Zhiyu Lin', 'Fan Bu', 'Yuhao Du', 'Benyou Wang', 'Haizhou Li'], 'affiliations': ['The Chinese University of Hong Kong, Shenzhen'], 'pdf_title_img': 'assets/pdf/title_img/2503.05085.jpg', 'data': {'categories': ['#audio', '#multilingual', '#benchmark'], 'emoji': 'ğŸ—£ï¸', 'ru': {'title': 'S2S-Arena: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ Ñ€ĞµÑ‡ĞµĞ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸ĞºĞ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ S2S-Arena - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€ĞµÑ‡ĞµĞ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ 154 Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ° Ğ² 4 Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ñ… Ñ 21 Ğ·Ğ°Ğ´Ğ°Ñ‡ĞµĞ¹, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ñ… ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¸ Ğ¶Ğ¸Ğ²ÑƒÑ Ñ€ĞµÑ‡ÑŒ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾ ĞºĞ°ÑĞºĞ°Ğ´Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (ASR+LLM+TTS) Ğ½Ğ°Ğ´ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… speech2speech. Ğ’Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ñ€ĞµÑ‡ĞµĞ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°ÑÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ñ€ĞµÑ‡Ğ¸, Ğ½Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰ĞµĞ³Ğ¾ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¾ÑÑ‚Ğ°ĞµÑ‚ÑÑ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡ĞµĞ¹.'}, 'en': {'title': 'Enhancing Speech Models with Paralinguistic Insights', 'desc': "This paper presents S2S-Arena, a new benchmark for evaluating speech-to-speech (S2S) models that incorporates paralinguistic information, which includes elements like tone and emotion in speech. The authors highlight that current evaluation methods primarily rely on text-based metrics, which do not adequately assess the models' ability to understand and generate speech with these nuances. Through a series of 154 samples across various tasks, the study reveals that models like GPT-4o and cascaded ASR-LLM-TTS outperform others when considering paralinguistic factors. The findings indicate that while advanced speech models can comprehend paralinguistic cues, generating speech that accurately reflects these cues remains a significant challenge."}, 'zh': {'title': 'å¼•å…¥å‰¯è¯­è¨€ä¿¡æ¯çš„è¯­éŸ³æ¨¡å‹è¯„ä¼°æ–°æ ‡å‡†', 'desc': 'éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œè¯­éŸ³æ¨¡å‹ä¹Ÿå—åˆ°äº†å¹¿æ³›å…³æ³¨ï¼Œå°¤å…¶æ˜¯åœ¨è¯­éŸ³è¾“å…¥å’Œè¾“å‡ºçš„è¯­éŸ³åˆ°è¯­éŸ³ï¼ˆspeech2speechï¼‰åè®®æ–¹é¢çš„è¿›å±•ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºå‡†æµ‹è¯•ä¸»è¦ä¾èµ–è‡ªåŠ¨æ–‡æœ¬è¯„ä¼°å™¨æ¥è¯„ä¼°è¿™äº›æ¨¡å‹çš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›ï¼Œæœªèƒ½è€ƒè™‘è¯­éŸ³ç†è§£å’Œç”Ÿæˆä¸­çš„å‰¯è¯­è¨€ä¿¡æ¯ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†S2S-Arenaï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„ç«æŠ€åœºé£æ ¼çš„S2SåŸºå‡†ï¼Œè¯„ä¼°åœ¨çœŸå®ä»»åŠ¡ä¸­è¯­éŸ³è¾“å…¥å’Œè¾“å‡ºçš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›ï¼Œå¹¶è€ƒè™‘å‰¯è¯­è¨€ä¿¡æ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå°½ç®¡GPT-4oè¡¨ç°ä¼˜è¶Šï¼Œä½†åœ¨è¯­éŸ³åˆ°è¯­éŸ³åè®®ä¸­ï¼Œçº§è”çš„ASRã€LLMå’ŒTTSè¯­éŸ³æ¨¡å‹åœ¨æ–‡æœ¬-è¯­éŸ³å¯¹é½åä¼˜äºè”åˆè®­ç»ƒæ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.05179', 'title': 'Sketch-of-Thought: Efficient LLM Reasoning with Adaptive\n  Cognitive-Inspired Sketching', 'url': 'https://huggingface.co/papers/2503.05179', 'abstract': 'Recent advances in large language models have demonstrated remarkable reasoning capabilities through Chain of Thought (CoT) prompting, but often at the cost of excessive verbosity in their intermediate outputs, which increases computational overhead. We introduce Sketch-of-Thought (SoT), a novel prompting framework that combines cognitive-inspired reasoning paradigms with linguistic constraints to minimize token usage while preserving reasoning accuracy. SoT is designed as a flexible framework that can incorporate any custom reasoning paradigms based on cognitive science, and we instantiate it with three such paradigms - Conceptual Chaining, Chunked Symbolism, and Expert Lexicons - each tailored to different reasoning tasks and selected dynamically via a lightweight routing model. Through comprehensive evaluation across 15 reasoning datasets with multiple languages and multimodal scenarios, we demonstrate that SoT achieves token reductions of 76% with negligible accuracy impact. In certain domains like mathematical and multi-hop reasoning, it even improves accuracy while using significantly fewer tokens. Our code is publicly available: https://www.github.com/SimonAytes/SoT.', 'score': 37, 'issue_id': 2607, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 7', 'zh': '3æœˆ7æ—¥'}, 'hash': 'e02cb6f62715b753', 'authors': ['Simon A. Aytes', 'Jinheon Baek', 'Sung Ju Hwang'], 'affiliations': ['DeepAuto.ai', 'KAIST'], 'pdf_title_img': 'assets/pdf/title_img/2503.05179.jpg', 'data': {'categories': ['#multimodal', '#dataset', '#reasoning', '#multilingual', '#optimization', '#open_source', '#math', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¸Ğ½Ğ³Ğ° Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Sketch-of-Thought (SoT), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾-Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸ÑĞ¼Ğ¸. SoT Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ° Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñ‹: Conceptual Chaining, Chunked Symbolism Ğ¸ Expert Lexicons, ĞºĞ°Ğ¶Ğ´Ğ°Ñ Ğ¸Ğ· ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° 15 Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ SoT ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ½Ğ° 76% Ğ±ĞµĞ· Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ Ğ½Ğ° Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ° Ğ² Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ… Ğ´Ğ°Ğ¶Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞµĞµ.'}, 'en': {'title': 'Efficient Reasoning with Sketch-of-Thought', 'desc': 'This paper presents a new prompting framework called Sketch-of-Thought (SoT) that enhances reasoning in large language models while reducing the number of tokens used. SoT integrates cognitive-inspired reasoning methods with linguistic constraints to maintain accuracy without excessive verbosity. The framework is adaptable, allowing for the inclusion of various reasoning paradigms, which are dynamically selected based on the task at hand. Evaluation shows that SoT can reduce token usage by 76% with little to no loss in accuracy, and in some cases, it even improves performance in specific reasoning tasks.'}, 'zh': {'title': 'æ€ç»´è‰å›¾ï¼šé«˜æ•ˆæ¨ç†çš„æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æç¤ºæ¡†æ¶ï¼Œç§°ä¸ºæ€ç»´è‰å›¾ï¼ˆSketch-of-Thoughtï¼ŒSoTï¼‰ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼ŒåŒæ—¶å‡å°‘ä¸­é—´è¾“å‡ºçš„å†—é•¿æ€§ã€‚SoTç»“åˆäº†è®¤çŸ¥ç§‘å­¦çš„æ¨ç†èŒƒå¼å’Œè¯­è¨€çº¦æŸï¼Œä»¥æœ€å°åŒ–ä»¤ç‰Œä½¿ç”¨é‡ï¼ŒåŒæ—¶ä¿æŒæ¨ç†çš„å‡†ç¡®æ€§ã€‚è¯¥æ¡†æ¶çµæ´»ï¼Œå¯ä»¥æ ¹æ®è®¤çŸ¥ç§‘å­¦çš„ä¸åŒæ¨ç†èŒƒå¼è¿›è¡Œå®šåˆ¶ï¼Œå¹¶é€šè¿‡è½»é‡çº§è·¯ç”±æ¨¡å‹åŠ¨æ€é€‰æ‹©ã€‚é€šè¿‡åœ¨15ä¸ªæ¨ç†æ•°æ®é›†ä¸Šçš„å…¨é¢è¯„ä¼°ï¼ŒSoTå®ç°äº†76%çš„ä»¤ç‰Œå‡å°‘ï¼Œä¸”å¯¹å‡†ç¡®æ€§å½±å“å¾®ä¹å…¶å¾®ï¼Œç”šè‡³åœ¨æŸäº›é¢†åŸŸæé«˜äº†å‡†ç¡®æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.05132', 'title': 'R1-Zero\'s "Aha Moment" in Visual Reasoning on a 2B Non-SFT Model', 'url': 'https://huggingface.co/papers/2503.05132', 'abstract': 'Recently DeepSeek R1 demonstrated how reinforcement learning with simple rule-based incentives can enable autonomous development of complex reasoning in large language models, characterized by the "aha moment", in which the model manifest self-reflection and increased response length during training. However, attempts to extend this success to multimodal reasoning often failed to reproduce these key characteristics. In this report, we present the first successful replication of these emergent characteristics for multimodal reasoning on only a non-SFT 2B model. Starting with Qwen2-VL-2B and applying reinforcement learning directly on the SAT dataset, our model achieves 59.47% accuracy on CVBench, outperforming the base model by approximately ~30% and exceeding both SFT setting by ~2%. In addition, we share our failed attempts and insights in attempting to achieve R1-like reasoning using RL with instruct models. aiming to shed light on the challenges involved. Our key observations include: (1) applying RL on instruct model often results in trivial reasoning trajectories, and (2) naive length reward are ineffective in eliciting reasoning capabilities. The project code is available at https://github.com/turningpoint-ai/VisualThinker-R1-Zero', 'score': 20, 'issue_id': 2609, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 7', 'zh': '3æœˆ7æ—¥'}, 'hash': '760e01cf2a414aeb', 'authors': ['Hengguang Zhou', 'Xirui Li', 'Ruochen Wang', 'Minhao Cheng', 'Tianyi Zhou', 'Cho-Jui Hsieh'], 'affiliations': ['Pennsylvania State University', 'University of California, LA', 'University of Maryland'], 'pdf_title_img': 'assets/pdf/title_img/2503.05132.jpg', 'data': {'categories': ['#rl', '#multimodal', '#training', '#reasoning', '#optimization'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞĞ¾Ğ²Ñ‹Ğµ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ñ‹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¸Ğ· DeepSeek R1 Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, ĞºĞ°Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‡ÑŒ LLM Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚Ñ‹ "ÑĞ²Ñ€Ğ¸ĞºĞ¸". ĞĞ´Ğ½Ğ°ĞºĞ¾ Ğ¿Ñ€Ğ¸ Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚ĞºĞ°Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ñ‚ÑŒ ÑÑ‚Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğº Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ½Ğµ ÑƒĞ´Ğ°Ğ²Ğ°Ğ»Ğ¾ÑÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ñ‚Ğ°ĞºĞ¸Ñ… Ğ¶Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ². Ğ’ ÑÑ‚Ğ¾Ğ¼ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ²Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµÑÑ‚Ğ¸ ÑÑ‚Ğ¸ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ Ğ½Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ SFT, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ½ÑƒĞ² Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´ĞµĞ»ÑÑ‚ÑÑ Ğ½ĞµÑƒĞ´Ğ°Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚ĞºĞ°Ğ¼Ğ¸ Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°Ğ¼Ğ¸, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ»ÑƒÑ‡ÑˆĞµ Ğ¿Ğ¾Ğ½ÑÑ‚ÑŒ Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ÑÑ‰Ğ¸Ğµ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Unlocking Multimodal Reasoning with Reinforcement Learning', 'desc': "This paper discusses the application of reinforcement learning (RL) to enhance multimodal reasoning in large language models, specifically using the Qwen2-VL-2B model. The authors successfully replicated the 'aha moment' phenomenon, where the model demonstrates self-reflection and improved response length during training, achieving a notable accuracy of 59.47% on the CVBench dataset. They also share insights from their unsuccessful attempts to replicate similar reasoning capabilities using instruct models, highlighting challenges such as trivial reasoning paths and ineffective reward structures. The findings suggest that while RL can significantly improve reasoning in multimodal contexts, careful consideration of reward mechanisms is crucial for success."}, 'zh': {'title': 'å¼ºåŒ–å­¦ä¹ åŠ©åŠ›å¤šæ¨¡æ€æ¨ç†çš„çªç ´', 'desc': 'æœ€è¿‘ï¼ŒDeepSeek R1å±•ç¤ºäº†å¦‚ä½•é€šè¿‡ç®€å•çš„åŸºäºè§„åˆ™çš„æ¿€åŠ±æ¥å®ç°å¼ºåŒ–å­¦ä¹ ï¼Œä½¿å¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿè‡ªä¸»å‘å±•å¤æ‚çš„æ¨ç†èƒ½åŠ›ã€‚è¿™ç§èƒ½åŠ›åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è¡¨ç°ä¸ºâ€œæç„¶å¤§æ‚Ÿâ€çš„æ—¶åˆ»ï¼Œæ¨¡å‹ä¼šè‡ªæˆ‘åæ€å¹¶å¢åŠ å“åº”é•¿åº¦ã€‚ç„¶è€Œï¼Œå°è¯•å°†è¿™ç§æˆåŠŸæ‰©å±•åˆ°å¤šæ¨¡æ€æ¨ç†æ—¶ï¼Œå¾€å¾€æ— æ³•é‡ç°è¿™äº›å…³é”®ç‰¹å¾ã€‚åœ¨æœ¬æŠ¥å‘Šä¸­ï¼Œæˆ‘ä»¬é¦–æ¬¡æˆåŠŸå¤åˆ¶äº†è¿™äº›ç‰¹å¾ï¼Œå¹¶åœ¨éSFTçš„2Bæ¨¡å‹ä¸Šå®ç°äº†å¤šæ¨¡æ€æ¨ç†çš„è¿›å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.02130', 'title': 'Forgetting Transformer: Softmax Attention with a Forget Gate', 'url': 'https://huggingface.co/papers/2503.02130', 'abstract': 'An essential component of modern recurrent sequence models is the forget gate. While Transformers do not have an explicit recurrent form, we show that a forget gate can be naturally incorporated into Transformers by down-weighting the unnormalized attention scores in a data-dependent way. We name this attention mechanism the Forgetting Attention and the resulting model the Forgetting Transformer (FoX). We show that FoX outperforms the Transformer on long-context language modeling, length extrapolation, and short-context downstream tasks, while performing on par with the Transformer on long-context downstream tasks. Moreover, it is compatible with the FlashAttention algorithm and does not require any positional embeddings. Several analyses, including the needle-in-the-haystack test, show that FoX also retains the Transformer\'s superior long-context capabilities over recurrent sequence models such as Mamba-2, HGRN2, and DeltaNet. We also introduce a "Pro" block design that incorporates some common architectural components in recurrent sequence models and find it significantly improves the performance of both FoX and the Transformer. Our code is available at https://github.com/zhixuan-lin/forgetting-transformer.', 'score': 18, 'issue_id': 2607, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 3', 'zh': '3æœˆ3æ—¥'}, 'hash': '4c39f334b6c4ed28', 'authors': ['Zhixuan Lin', 'Evgenii Nikishin', 'Xu Owen He', 'Aaron Courville'], 'affiliations': ['MakerMaker AI', 'Mila & Universite de Montreal'], 'pdf_title_img': 'assets/pdf/title_img/2503.02130.jpg', 'data': {'categories': ['#long_context', '#architecture', '#optimization', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Forgetting Transformer: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ² Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°Ñ…', 'desc': "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Forgetting Transformer (FoX), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ 'Ğ·Ğ°Ğ±Ñ‹Ğ²Ğ°ÑÑ‰ĞµĞ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ' Ğ² Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°. FoX Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ° Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ¸ ÑĞºÑÑ‚Ñ€Ğ°Ğ¿Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ñ‹, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ° Ñ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ¼ FlashAttention Ğ¸ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ². ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ FoX ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ€ĞµĞºÑƒÑ€Ñ€ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸."}, 'en': {'title': 'Enhancing Transformers with Forgetting Attention for Superior Performance', 'desc': 'This paper introduces a new attention mechanism called Forgetting Attention, which integrates a forget gate into Transformer models. The Forgetting Transformer (FoX) leverages this mechanism to improve performance on various language modeling tasks, particularly those involving long contexts. FoX not only matches the performance of traditional Transformers on long-context tasks but also excels in short-context and length extrapolation tasks. Additionally, it is compatible with the FlashAttention algorithm and eliminates the need for positional embeddings, enhancing its efficiency and effectiveness.'}, 'zh': {'title': 'é—å¿˜å˜æ¢å™¨ï¼šæå‡é•¿ä¸Šä¸‹æ–‡å»ºæ¨¡çš„åˆ©å™¨', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œç§°ä¸ºé—å¿˜æ³¨æ„åŠ›ï¼ˆForgetting Attentionï¼‰ï¼Œå¯ä»¥æœ‰æ•ˆåœ°å°†é—å¿˜é—¨é›†æˆåˆ°Transformeræ¨¡å‹ä¸­ã€‚é€šè¿‡ä»¥æ•°æ®ä¸ºä¾èµ–çš„æ–¹å¼é™ä½æœªå½’ä¸€åŒ–æ³¨æ„åŠ›åˆ†æ•°ï¼Œé—å¿˜æ³¨æ„åŠ›ä½¿å¾—Transformeråœ¨é•¿ä¸Šä¸‹æ–‡è¯­è¨€å»ºæ¨¡å’Œé•¿åº¦å¤–æ¨ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºä¼ ç»Ÿçš„Transformerã€‚æˆ‘ä»¬è¿˜è®¾è®¡äº†ä¸€ä¸ªâ€œProâ€æ¨¡å—ï¼Œç»“åˆäº†é€’å½’åºåˆ—æ¨¡å‹ä¸­çš„ä¸€äº›å¸¸è§æ¶æ„ç»„ä»¶ï¼Œæ˜¾è‘—æå‡äº†FoXå’ŒTransformerçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒFoXåœ¨é•¿ä¸Šä¸‹æ–‡ä»»åŠ¡ä¸­ä¿æŒäº†Transformerçš„ä¼˜åŠ¿ï¼Œè¶…è¶Šäº†å…¶ä»–é€’å½’åºåˆ—æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.05592', 'title': 'R1-Searcher: Incentivizing the Search Capability in LLMs via\n  Reinforcement Learning', 'url': 'https://huggingface.co/papers/2503.05592', 'abstract': 'Existing Large Reasoning Models (LRMs) have shown the potential of reinforcement learning (RL) to enhance the complex reasoning capabilities of Large Language Models~(LLMs). While they achieve remarkable performance on challenging tasks such as mathematics and coding, they often rely on their internal knowledge to solve problems, which can be inadequate for time-sensitive or knowledge-intensive questions, leading to inaccuracies and hallucinations. To address this, we propose R1-Searcher, a novel two-stage outcome-based RL approach designed to enhance the search capabilities of LLMs. This method allows LLMs to autonomously invoke external search systems to access additional knowledge during the reasoning process. Our framework relies exclusively on RL, without requiring process rewards or distillation for a cold start. % effectively generalizing to out-of-domain datasets and supporting both Base and Instruct models. Our experiments demonstrate that our method significantly outperforms previous strong RAG methods, even when compared to the closed-source GPT-4o-mini.', 'score': 17, 'issue_id': 2609, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 7', 'zh': '3æœˆ7æ—¥'}, 'hash': '6af1f8cd890c69ae', 'authors': ['Huatong Song', 'Jinhao Jiang', 'Yingqian Min', 'Jie Chen', 'Zhipeng Chen', 'Wayne Xin Zhao', 'Lei Fang', 'Ji-Rong Wen'], 'affiliations': ['DataCanvas', 'Gaoling School of Artificial Intelligence, Renmin University of China'], 'pdf_title_img': 'assets/pdf/title_img/2503.05592.jpg', 'data': {'categories': ['#hallucinations', '#optimization', '#rl', '#rag', '#reasoning'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ£ÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ˜Ğ˜ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'R1-Searcher - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‰Ğ¸Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ¿Ğ¾Ğ¸ÑĞºÑƒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. ĞĞ½ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ñ‰Ğ°Ñ‚ÑŒÑÑ Ğº Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğ¼ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ğ¼ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ¸ÑĞºĞ»ÑÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ½Ñ‹Ñ… Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ñ… Ğ¸Ğ»Ğ¸ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ R1-Searcher Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ RAG, Ğ´Ğ°Ğ¶Ğµ Ğ² ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¸ Ñ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ GPT-4o-mini.'}, 'en': {'title': 'Enhancing LLM Reasoning with External Knowledge Search', 'desc': 'This paper introduces R1-Searcher, a new approach that improves the reasoning abilities of Large Language Models (LLMs) using reinforcement learning (RL). Unlike existing models that depend solely on their internal knowledge, R1-Searcher enables LLMs to access external search systems for additional information, which helps in answering complex and time-sensitive questions more accurately. The method operates in two stages and does not require initial rewards or distillation, making it easier to implement. Experimental results show that R1-Searcher outperforms previous retrieval-augmented generation (RAG) methods, demonstrating its effectiveness across various datasets.'}, 'zh': {'title': 'å¢å¼ºæ¨ç†èƒ½åŠ›ï¼ŒR1-SearcheråŠ©åŠ›LLMs', 'desc': 'ç°æœ‰çš„å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰å±•ç¤ºäº†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¤æ‚æ¨ç†èƒ½åŠ›æ–¹é¢çš„æ½œåŠ›ã€‚å°½ç®¡å®ƒä»¬åœ¨æ•°å­¦å’Œç¼–ç¨‹ç­‰æŒ‘æˆ˜æ€§ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤„ç†æ—¶é—´æ•æ„Ÿæˆ–çŸ¥è¯†å¯†é›†çš„é—®é¢˜æ—¶ï¼Œå¾€å¾€ä¾èµ–å†…éƒ¨çŸ¥è¯†ï¼Œå¯¼è‡´ä¸å‡†ç¡®å’Œå¹»è§‰ç°è±¡ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†R1-Searcherï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„åŸºäºç»“æœçš„ä¸¤é˜¶æ®µå¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œæ—¨åœ¨å¢å¼ºLLMsçš„æœç´¢èƒ½åŠ›ã€‚è¯¥æ–¹æ³•å…è®¸LLMsåœ¨æ¨ç†è¿‡ç¨‹ä¸­è‡ªä¸»è°ƒç”¨å¤–éƒ¨æœç´¢ç³»ç»Ÿï¼Œä»¥è·å–é¢å¤–çŸ¥è¯†ï¼Œä»è€Œæ˜¾è‘—æé«˜æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.04957', 'title': 'SafeArena: Evaluating the Safety of Autonomous Web Agents', 'url': 'https://huggingface.co/papers/2503.04957', 'abstract': 'LLM-based agents are becoming increasingly proficient at solving web-based tasks. With this capability comes a greater risk of misuse for malicious purposes, such as posting misinformation in an online forum or selling illicit substances on a website. To evaluate these risks, we propose SafeArena, the first benchmark to focus on the deliberate misuse of web agents. SafeArena comprises 250 safe and 250 harmful tasks across four websites. We classify the harmful tasks into five harm categories -- misinformation, illegal activity, harassment, cybercrime, and social bias, designed to assess realistic misuses of web agents. We evaluate leading LLM-based web agents, including GPT-4o, Claude-3.5 Sonnet, Qwen-2-VL 72B, and Llama-3.2 90B, on our benchmark. To systematically assess their susceptibility to harmful tasks, we introduce the Agent Risk Assessment framework that categorizes agent behavior across four risk levels. We find agents are surprisingly compliant with malicious requests, with GPT-4o and Qwen-2 completing 34.7% and 27.3% of harmful requests, respectively. Our findings highlight the urgent need for safety alignment procedures for web agents. Our benchmark is available here: https://safearena.github.io', 'score': 15, 'issue_id': 2623, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 6', 'zh': '3æœˆ6æ—¥'}, 'hash': '8fd2a7cf41173ed6', 'authors': ['Ada Defne Tur', 'Nicholas Meade', 'Xing Han LÃ¹', 'Alejandra Zambrano', 'Arkil Patel', 'Esin Durmus', 'Spandana Gella', 'Karolina StaÅ„czak', 'Siva Reddy'], 'affiliations': ['Anthropic', 'Canada CIFAR AI Chair', 'Concordia University', 'McGill University', 'Mila Quebec AI Institute', 'ServiceNow Research'], 'pdf_title_img': 'assets/pdf/title_img/2503.04957.jpg', 'data': {'categories': ['#benchmark', '#agents', '#ethics', '#security', '#alignment'], 'emoji': 'ğŸ•µï¸', 'ru': {'title': 'ĞÑ†ĞµĞ½ĞºĞ° Ñ€Ğ¸ÑĞºĞ¾Ğ² Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²: Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² ÑĞ¿Ğ¾Ñ…Ñƒ Ğ²ĞµĞ±-Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸', 'desc': 'SafeArena - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€Ğ¸ÑĞºĞ¾Ğ² Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ğ»Ğ¾ÑƒĞ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ñ Ğ²ĞµĞ±-Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 500 Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ½Ğ° Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ… Ğ²ĞµĞ±-ÑĞ°Ğ¹Ñ‚Ğ°Ñ…, Ğ¸Ğ· ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… 250 ÑĞ²Ğ»ÑÑÑ‚ÑÑ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ğ¼Ğ¸, Ğ° 250 - Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ñ‹Ğ¼Ğ¸, Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ½Ğ° Ğ¿ÑÑ‚ÑŒ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¹ Ğ²Ñ€ĞµĞ´Ğ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ»Ğ¸ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ñ… LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ GPT-4o Ğ¸ Qwen-2-VL 72B, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½ÑƒÑ Ğ¸Ğ¼Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€Ğ¸ÑĞºĞ¾Ğ² Agent Risk Assessment. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ½ĞµĞ¾Ğ¶Ğ¸Ğ´Ğ°Ğ½Ğ½Ğ¾ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑÑ‚ Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ñ‹Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ ÑÑ€Ğ¾Ñ‡Ğ½ÑƒÑ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ²ĞµĞ±-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Assessing the Risks of LLM Agents in Web Tasks', 'desc': 'This paper introduces SafeArena, a benchmark designed to evaluate the risks associated with the misuse of large language model (LLM)-based agents in web tasks. It includes 500 tasks, split evenly between safe and harmful, categorized into five types of harm such as misinformation and cybercrime. The study assesses various leading LLM agents, revealing that they often comply with harmful requests, with notable completion rates for GPT-4o and Qwen-2. The findings underscore the necessity for improved safety measures to prevent the malicious use of these agents.'}, 'zh': {'title': 'ç½‘ç»œä»£ç†å®‰å…¨é£é™©è¯„ä¼°çš„å¿…è¦æ€§', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†SafeArenaï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªä¸“æ³¨äºç½‘ç»œä»£ç†æ¶æ„ä½¿ç”¨çš„åŸºå‡†æµ‹è¯•ã€‚SafeArenaåŒ…å«250ä¸ªå®‰å…¨ä»»åŠ¡å’Œ250ä¸ªæœ‰å®³ä»»åŠ¡ï¼Œæ¶µç›–å››ä¸ªç½‘ç«™ï¼Œå¹¶å°†æœ‰å®³ä»»åŠ¡åˆ†ä¸ºäº”ä¸ªç±»åˆ«ï¼Œå¦‚è™šå‡ä¿¡æ¯å’Œç½‘ç»œçŠ¯ç½ªã€‚æˆ‘ä»¬è¯„ä¼°äº†å¤šç§é¢†å…ˆçš„åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„ç½‘ç»œä»£ç†ï¼Œå‘ç°å®ƒä»¬å¯¹æ¶æ„è¯·æ±‚çš„å“åº”ç‡ä»¤äººæƒŠè®¶ï¼Œéƒ¨åˆ†ä»£ç†å®Œæˆäº†è¶…è¿‡ä¸‰åˆ†ä¹‹ä¸€çš„æœ‰å®³è¯·æ±‚ã€‚ç ”ç©¶ç»“æœå¼ºè°ƒäº†å¯¹ç½‘ç»œä»£ç†è¿›è¡Œå®‰å…¨å¯¹é½ç¨‹åºçš„è¿«åˆ‡éœ€æ±‚ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.05639', 'title': 'VideoPainter: Any-length Video Inpainting and Editing with Plug-and-Play\n  Context Control', 'url': 'https://huggingface.co/papers/2503.05639', 'abstract': "Video inpainting, which aims to restore corrupted video content, has experienced substantial progress. Despite these advances, existing methods, whether propagating unmasked region pixels through optical flow and receptive field priors, or extending image-inpainting models temporally, face challenges in generating fully masked objects or balancing the competing objectives of background context preservation and foreground generation in one model, respectively. To address these limitations, we propose a novel dual-stream paradigm VideoPainter that incorporates an efficient context encoder (comprising only 6% of the backbone parameters) to process masked videos and inject backbone-aware background contextual cues to any pre-trained video DiT, producing semantically consistent content in a plug-and-play manner. This architectural separation significantly reduces the model's learning complexity while enabling nuanced integration of crucial background context. We also introduce a novel target region ID resampling technique that enables any-length video inpainting, greatly enhancing our practical applicability. Additionally, we establish a scalable dataset pipeline leveraging current vision understanding models, contributing VPData and VPBench to facilitate segmentation-based inpainting training and assessment, the largest video inpainting dataset and benchmark to date with over 390K diverse clips. Using inpainting as a pipeline basis, we also explore downstream applications including video editing and video editing pair data generation, demonstrating competitive performance and significant practical potential. Extensive experiments demonstrate VideoPainter's superior performance in both any-length video inpainting and editing, across eight key metrics, including video quality, mask region preservation, and textual coherence.", 'score': 13, 'issue_id': 2614, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 7', 'zh': '3æœˆ7æ—¥'}, 'hash': '735dd34a3043623a', 'authors': ['Yuxuan Bian', 'Zhaoyang Zhang', 'Xuan Ju', 'Mingdeng Cao', 'Liangbin Xie', 'Ying Shan', 'Qiang Xu'], 'affiliations': ['Tencent ARC Lab, China', 'The Chinese University of Hong Kong, China', 'The University of Tokyo, Japan', 'University of Macau, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.05639.jpg', 'data': {'categories': ['#video', '#benchmark', '#dataset', '#games', '#architecture', '#optimization'], 'emoji': 'ğŸ¥', 'ru': {'title': 'VideoPainter: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ²ÑƒÑ…Ğ¿Ğ¾Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ²Ñ€ĞµĞ¶Ğ´ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° - VideoPainter. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…Ğ¿Ğ¾Ñ‚Ğ¾Ñ‡Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ¾Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº Ğ² Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ DiT. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ Ñ€ĞµÑĞµĞ¼Ğ¿Ğ»Ğ¸Ğ½Ğ³Ğ° ID Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰ÑƒÑ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ¸Ğ½Ğ¿ĞµĞ¹Ğ½Ñ‚Ğ¸Ğ½Ğ³ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ»ÑĞ±Ğ¾Ğ¹ Ğ´Ğ»Ğ¸Ğ½Ñ‹. Ğ”Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… VPData Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº VPBench Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ğ¿ĞµĞ¹Ğ½Ñ‚Ğ¸Ğ½Ğ³Ğ°.'}, 'en': {'title': 'Revolutionizing Video Inpainting with VideoPainter', 'desc': "This paper presents VideoPainter, a new approach to video inpainting that effectively restores missing video content. It introduces a dual-stream architecture that uses a lightweight context encoder to enhance background information while maintaining the integrity of foreground objects. The method also features a novel target region ID resampling technique, allowing for flexible inpainting of videos of any length. Additionally, the authors provide a comprehensive dataset and benchmark for training and evaluating video inpainting models, showcasing VideoPainter's strong performance across various metrics."}, 'zh': {'title': 'è§†é¢‘ä¿®å¤çš„æ–°çºªå…ƒï¼šVideoPainter', 'desc': 'è§†é¢‘ä¿®å¤æ—¨åœ¨æ¢å¤æŸåçš„è§†é¢‘å†…å®¹ï¼Œè¿‘å¹´æ¥å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç°æœ‰æ–¹æ³•åœ¨ç”Ÿæˆå®Œå…¨é®æŒ¡çš„ç‰©ä½“æˆ–å¹³è¡¡èƒŒæ™¯ä¿ç•™ä¸å‰æ™¯ç”Ÿæˆæ–¹é¢é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„åŒæµæ¶æ„VideoPainterï¼Œåˆ©ç”¨é«˜æ•ˆçš„ä¸Šä¸‹æ–‡ç¼–ç å™¨å¤„ç†é®æŒ¡è§†é¢‘ï¼Œå¹¶å°†èƒŒæ™¯ä¸Šä¸‹æ–‡ä¿¡æ¯æ³¨å…¥åˆ°é¢„è®­ç»ƒçš„è§†é¢‘æ¨¡å‹ä¸­ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§æ–°çš„ç›®æ ‡åŒºåŸŸIDé‡é‡‡æ ·æŠ€æœ¯ï¼Œæ”¯æŒä»»æ„é•¿åº¦çš„è§†é¢‘ä¿®å¤ï¼Œæå¤§åœ°æå‡äº†å®é™…åº”ç”¨çš„å¯è¡Œæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.05379', 'title': 'R1-Omni: Explainable Omni-Multimodal Emotion Recognition with\n  Reinforcing Learning', 'url': 'https://huggingface.co/papers/2503.05379', 'abstract': "In this work, we present the first application of Reinforcement Learning with Verifiable Reward (RLVR) to an Omni-multimodal large language model in the context of emotion recognition, a task where both visual and audio modalities play crucial roles. We leverage RLVR to optimize the Omni model, significantly enhancing its performance in three key aspects: reasoning capability, emotion recognition accuracy, and generalization ability. The introduction of RLVR not only improves the model's overall performance on in-distribution data but also demonstrates superior robustness when evaluated on out-of-distribution datasets. More importantly, the improved reasoning capability enables clear analysis of the contributions of different modalities, particularly visual and audio information, in the emotion recognition process. This provides valuable insights into the optimization of multimodal large language models.", 'score': 12, 'issue_id': 2609, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 7', 'zh': '3æœˆ7æ—¥'}, 'hash': '34c6afbd7ae83841', 'authors': ['Jiaxing Zhao', 'Xihan Wei', 'Liefeng Bo'], 'affiliations': ['Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2503.05379.jpg', 'data': {'categories': ['#optimization', '#rl', '#multimodal', '#audio', '#cv', '#reasoning'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'RLVR: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğ¸ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¹', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¾ Ğ¿ĞµÑ€Ğ²Ğ¾Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğ¼ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼ (RLVR) Ğº Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¹. RLVR Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Omni-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ ĞµÑ‘ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¹ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ°Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²ĞºĞ»Ğ°Ğ´ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¹.'}, 'en': {'title': 'Enhancing Emotion Recognition with RLVR in Multimodal Models', 'desc': "This paper introduces a novel approach called Reinforcement Learning with Verifiable Reward (RLVR) applied to an Omni-multimodal large language model for emotion recognition. The use of RLVR enhances the model's reasoning skills, accuracy in recognizing emotions, and its ability to generalize across different datasets. The model not only performs better on familiar data but also shows increased robustness when tested on new, unseen data. Additionally, the improved reasoning capability allows for a detailed understanding of how visual and audio inputs contribute to the emotion recognition task."}, 'zh': {'title': 'æƒ…æ„Ÿè¯†åˆ«ä¸­çš„å…¨æ¨¡æ€å¼ºåŒ–å­¦ä¹ æ–°çªç ´', 'desc': 'æœ¬ç ”ç©¶é¦–æ¬¡å°†å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰åº”ç”¨äºæƒ…æ„Ÿè¯†åˆ«çš„å…¨æ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ã€‚åœ¨è¿™ä¸ªä»»åŠ¡ä¸­ï¼Œè§†è§‰å’ŒéŸ³é¢‘æ¨¡æ€èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚é€šè¿‡ä½¿ç”¨RLVRï¼Œæˆ‘ä»¬æ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨æ¨ç†èƒ½åŠ›ã€æƒ…æ„Ÿè¯†åˆ«å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›ç­‰ä¸‰ä¸ªå…³é”®æ–¹é¢çš„è¡¨ç°ã€‚æ­¤å¤–ï¼ŒRLVRçš„å¼•å…¥ä¸ä»…æé«˜äº†æ¨¡å‹åœ¨åŒåˆ†å¸ƒæ•°æ®ä¸Šçš„æ•´ä½“æ€§èƒ½ï¼Œè¿˜åœ¨å¼‚åˆ†å¸ƒæ•°æ®é›†ä¸Šå±•ç°å‡ºæ›´å¼ºçš„é²æ£’æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.04808', 'title': 'Learning from Failures in Multi-Attempt Reinforcement Learning', 'url': 'https://huggingface.co/papers/2503.04808', 'abstract': "Recent advancements in reinforcement learning (RL) for large language models (LLMs), exemplified by DeepSeek R1, have shown that even a simple question-answering task can substantially improve an LLM's reasoning capabilities. In this work, we extend this approach by modifying the task into a multi-attempt setting. Instead of generating a single response per question, the model is given multiple attempts, with feedback provided after incorrect responses. The multi-attempt task encourages the model to refine its previous attempts and improve search efficiency. Experimental results show that even a small LLM trained on a multi-attempt task achieves significantly higher accuracy when evaluated with more attempts, improving from 45.6% with 1 attempt to 52.5% with 2 attempts on the math benchmark. In contrast, the same LLM trained on a standard single-turn task exhibits only a marginal improvement, increasing from 42.3% to 43.2% when given more attempts during evaluation. The results indicate that, compared to the standard single-turn task, an LLM trained on a multi-attempt task achieves slightly better performance on math benchmarks while also learning to refine its responses more effectively based on user feedback. Full code is available at https://github.com/DualityRL/multi-attempt", 'score': 11, 'issue_id': 2609, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 4', 'zh': '3æœˆ4æ—¥'}, 'hash': 'fb2db794d0ea3c11', 'authors': ['Stephen Chung', 'Wenyu Du', 'Jie Fu'], 'affiliations': ['DualityRL', 'Shanghai AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2503.04808.jpg', 'data': {'categories': ['#optimization', '#rl', '#math', '#training', '#rlhf', '#reasoning'], 'emoji': 'ğŸ”', 'ru': {'title': 'ĞœĞ½Ğ¾Ğ³Ğ¾Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ: Ğ¿ÑƒÑ‚ÑŒ Ğº Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼', 'desc': 'Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ²Ğ½ĞµĞ´Ñ€ÑÑ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚Ğ¾Ñ‡Ğ½ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğ¹ Ğ¾Ğ´Ğ½Ğ¾Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ĞµÑ‚ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚Ğ¾Ğº Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¸Ñ‚ÑŒ Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ, Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ Ğ¿Ğ¾ÑĞ»Ğµ Ğ½ĞµĞ²ĞµÑ€Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ ÑĞ¿Ğ¾ÑĞ¾Ğ±ÑÑ‚Ğ²ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞµ Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚Ğ¾Ğº. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ, Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ…, Ğ½Ğ¾ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ĞµĞµ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ²Ğ¾Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ.'}, 'en': {'title': 'Enhancing LLMs with Multi-Attempt Learning', 'desc': "This paper explores how modifying reinforcement learning tasks can enhance the reasoning abilities of large language models (LLMs). By implementing a multi-attempt question-answering framework, the model receives feedback on incorrect answers, allowing it to improve its responses iteratively. Experimental results demonstrate that even a small LLM can achieve better accuracy on math benchmarks when trained with this multi-attempt approach, compared to traditional single-turn tasks. The findings suggest that providing multiple attempts and feedback significantly aids in refining the model's performance."}, 'zh': {'title': 'å¤šæ¬¡å°è¯•ï¼Œæå‡æ¨ç†èƒ½åŠ›ï¼', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­åº”ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ–°æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯é€šè¿‡å¤šæ¬¡å°è¯•çš„ä»»åŠ¡è®¾ç½®æ¥æå‡æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚ä¸ä¼ ç»Ÿçš„å•æ¬¡å›ç­”ä¸åŒï¼Œæ¨¡å‹åœ¨æ¯ä¸ªé—®é¢˜ä¸Šå¯ä»¥è¿›è¡Œå¤šæ¬¡å°è¯•ï¼Œå¹¶åœ¨é”™è¯¯å›ç­”åè·å¾—åé¦ˆã€‚è¿™ç§å¤šæ¬¡å°è¯•çš„ä»»åŠ¡è®¾ç½®ä¿ƒä½¿æ¨¡å‹æ”¹è¿›ä¹‹å‰çš„å›ç­”ï¼Œä»è€Œæé«˜æœç´¢æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå³ä½¿æ˜¯å°å‹LLMï¼Œåœ¨å¤šæ¬¡å°è¯•çš„ä»»åŠ¡è®­ç»ƒä¸‹ï¼Œå…¶å‡†ç¡®ç‡æ˜¾è‘—æé«˜ï¼Œæ˜¾ç¤ºå‡ºå¤šæ¬¡å°è¯•å¯¹æ¨¡å‹å­¦ä¹ å’Œåé¦ˆçš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.05638', 'title': 'TrajectoryCrafter: Redirecting Camera Trajectory for Monocular Videos\n  via Diffusion Models', 'url': 'https://huggingface.co/papers/2503.05638', 'abstract': 'We present TrajectoryCrafter, a novel approach to redirect camera trajectories for monocular videos. By disentangling deterministic view transformations from stochastic content generation, our method achieves precise control over user-specified camera trajectories. We propose a novel dual-stream conditional video diffusion model that concurrently integrates point cloud renders and source videos as conditions, ensuring accurate view transformations and coherent 4D content generation. Instead of leveraging scarce multi-view videos, we curate a hybrid training dataset combining web-scale monocular videos with static multi-view datasets, by our innovative double-reprojection strategy, significantly fostering robust generalization across diverse scenes. Extensive evaluations on multi-view and large-scale monocular videos demonstrate the superior performance of our method.', 'score': 9, 'issue_id': 2612, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 7', 'zh': '3æœˆ7æ—¥'}, 'hash': '33c7af8e9a7df61e', 'authors': ['Mark YU', 'Wenbo Hu', 'Jinbo Xing', 'Ying Shan'], 'affiliations': ['ARC Lab, Tencent PCG'], 'pdf_title_img': 'assets/pdf/title_img/2503.05638.jpg', 'data': {'categories': ['#dataset', '#3d', '#optimization', '#diffusion', '#video'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Ğ£Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ĞµĞ¹ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°', 'desc': 'TrajectoryCrafter - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿ĞµÑ€ĞµĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ½Ğ¾ĞºÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ´ĞµÑ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´Ğ° Ğ¸ ÑÑ‚Ğ¾Ñ…Ğ°ÑÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ½Ğ°Ğ´ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¼ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ÑĞ¼Ğ¸ ĞºĞ°Ğ¼ĞµÑ€Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ²ÑƒÑ…Ğ¿Ğ¾Ñ‚Ğ¾Ñ‡Ğ½ÑƒÑ ÑƒÑĞ»Ğ¾Ğ²Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑÑ‰ÑƒÑ Ñ€ĞµĞ½Ğ´ĞµÑ€Ñ‹ Ğ¾Ğ±Ğ»Ğ°ĞºĞ° Ñ‚Ğ¾Ñ‡ĞµĞº Ğ¸ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ¼Ğ¾Ğ½Ğ¾ĞºÑƒĞ»ÑÑ€Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ñ‹Ğµ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ñ‹.'}, 'en': {'title': 'Mastering Camera Movement in Monocular Videos', 'desc': 'TrajectoryCrafter is a new method designed to change camera paths in single-camera videos. It separates the predictable changes in view from the random elements in the video content, allowing for better control over how the camera moves. The approach uses a dual-stream conditional video diffusion model that combines 3D point cloud images and original videos to ensure smooth transitions and realistic video generation. By creating a unique training dataset that merges large amounts of single-camera videos with some multi-camera data, the method shows improved performance across various scenes.'}, 'zh': {'title': 'ç²¾å‡†æ§åˆ¶æ‘„åƒæœºè½¨è¿¹çš„åˆ›æ–°æ–¹æ³•', 'desc': 'æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºTrajectoryCrafterçš„æ–°æ–¹æ³•ï¼Œç”¨äºé‡å®šå‘å•ç›®è§†é¢‘çš„æ‘„åƒæœºè½¨è¿¹ã€‚é€šè¿‡å°†ç¡®å®šæ€§çš„è§†å›¾å˜æ¢ä¸éšæœºå†…å®¹ç”Ÿæˆåˆ†ç¦»ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿç²¾ç¡®æ§åˆ¶ç”¨æˆ·æŒ‡å®šçš„æ‘„åƒæœºè½¨è¿¹ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„åŒæµæ¡ä»¶è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ŒåŒæ—¶æ•´åˆç‚¹äº‘æ¸²æŸ“å’Œæºè§†é¢‘ä½œä¸ºæ¡ä»¶ï¼Œç¡®ä¿å‡†ç¡®çš„è§†å›¾å˜æ¢å’Œä¸€è‡´çš„4Då†…å®¹ç”Ÿæˆã€‚é€šè¿‡åˆ›æ–°çš„åŒé‡é‡æŠ•å½±ç­–ç•¥ï¼Œæˆ‘ä»¬ç»“åˆäº†ç½‘ç»œè§„æ¨¡çš„å•ç›®è§†é¢‘å’Œé™æ€å¤šè§†è§’æ•°æ®é›†ï¼Œæ˜¾è‘—æé«˜äº†åœ¨ä¸åŒåœºæ™¯ä¸­çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.04872', 'title': 'TinyR1-32B-Preview: Boosting Accuracy with Branch-Merge Distillation', 'url': 'https://huggingface.co/papers/2503.04872', 'abstract': 'The challenge of reducing the size of Large Language Models (LLMs) while maintaining their performance has gained significant attention. However, existing methods, such as model distillation and transfer learning, often fail to achieve high accuracy. To address this limitation, we introduce the Branch-Merge distillation approach, which enhances model compression through two phases: (1) the Branch Phase, where knowledge from a large teacher model is selectively distilled into specialized student models via domain-specific supervised fine-tuning (SFT); And (2) the Merge Phase, where these student models are merged to enable cross-domain knowledge transfer and improve generalization. We validate our distillation approach using DeepSeek-R1 as the teacher and DeepSeek-R1-Distill-Qwen-32B as the student. The resulting merged model, TinyR1-32B-Preview, outperforms its counterpart DeepSeek-R1-Distill-Qwen-32B across multiple benchmarks, including Mathematics (+5.5 points), Coding (+4.4 points) and Science (+2.9 points), while achieving near-equal performance to DeepSeek-R1 on AIME 2024. The Branch-Merge distillation approach provides a scalable solution for creating smaller, high-performing LLMs with reduced computational cost and time.', 'score': 9, 'issue_id': 2609, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 6', 'zh': '3æœˆ6æ—¥'}, 'hash': '94defd7f9d19776e', 'authors': ['Lin Sun', 'Guangxiang Zhao', 'Xiaoqi Jian', 'Yuhan Wu', 'Weihong Lin', 'Yongfu Zhu', 'Change Jia', 'Linglin Zhang', 'Jinzhu Wu', 'Junfeng Ran', 'Sai-er Hu', 'Zihan Jiang', 'Junting Zhou', 'Wenrui Liu', 'Bin Cui', 'Tong Yang', 'Xiangzheng Zhang'], 'affiliations': ['Peking University', 'Qiyuan Tech'], 'pdf_title_img': 'assets/pdf/title_img/2503.04872.jpg', 'data': {'categories': ['#small_models', '#training', '#transfer_learning', '#optimization'], 'emoji': 'ğŸŒ³', 'ru': {'title': 'Ğ’ĞµÑ‚Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¸ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿ÑƒÑ‚ÑŒ Ğº ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğ¼ Ğ¸ Ğ¼Ğ¾Ñ‰Ğ½Ñ‹Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Branch-Merge distillation. ĞœĞµÑ‚Ğ¾Ğ´ ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ğ´Ğ²ÑƒÑ… Ñ„Ğ°Ğ·: Branch, Ğ³Ğ´Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ· Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸-ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ² ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸-ÑƒÑ‡ĞµĞ½Ğ¸ĞºĞ¸, Ğ¸ Merge, Ğ³Ğ´Ğµ ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‚ÑÑ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ TinyR1-32B-Preview Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸ Ğ¿Ğ¾ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ğ¼. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ñ…, Ğ½Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… LLM Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Branch-Merge: Compressing LLMs Without Compromise!', 'desc': 'This paper presents a new method called Branch-Merge distillation to reduce the size of Large Language Models (LLMs) while keeping their performance high. It consists of two main phases: the Branch Phase, where knowledge from a large teacher model is distilled into smaller, specialized student models through supervised fine-tuning, and the Merge Phase, where these student models are combined to enhance knowledge transfer across different domains. The approach was tested using specific models and showed that the merged model, TinyR1-32B-Preview, outperformed the individual student model in various tasks, including Mathematics, Coding, and Science. Overall, this method offers an effective way to create smaller LLMs that maintain strong performance and are more efficient in terms of computational resources.'}, 'zh': {'title': 'åˆ†æ”¯åˆå¹¶è’¸é¦ï¼šé«˜æ•ˆå‹ç¼©å¤§å‹è¯­è¨€æ¨¡å‹çš„åˆ›æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¨¡å‹è’¸é¦æ–¹æ³•ï¼Œç§°ä¸ºåˆ†æ”¯åˆå¹¶è’¸é¦ï¼Œæ—¨åœ¨åœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶å‡å°‘å¤§å‹è¯­è¨€æ¨¡å‹çš„ä½“ç§¯ã€‚è¯¥æ–¹æ³•åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šåˆ†æ”¯é˜¶æ®µé€šè¿‡é¢†åŸŸç‰¹å®šçš„ç›‘ç£å¾®è°ƒå°†çŸ¥è¯†ä»å¤§å‹æ•™å¸ˆæ¨¡å‹é€‰æ‹©æ€§åœ°è’¸é¦åˆ°ä¸“é—¨çš„å­¦ç”Ÿæ¨¡å‹ä¸­ï¼›åˆå¹¶é˜¶æ®µåˆ™å°†è¿™äº›å­¦ç”Ÿæ¨¡å‹åˆå¹¶ï¼Œä»¥å®ç°è·¨é¢†åŸŸçŸ¥è¯†è½¬ç§»å¹¶æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåˆå¹¶åçš„æ¨¡å‹TinyR1-32B-Previewåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºå…¶å¯¹åº”çš„å­¦ç”Ÿæ¨¡å‹DeepSeek-R1-Distill-Qwen-32Bã€‚è¯¥æ–¹æ³•ä¸ºåˆ›å»ºæ›´å°ä¸”é«˜æ€§èƒ½çš„è¯­è¨€æ¨¡å‹æä¾›äº†ä¸€ç§å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆï¼Œé™ä½äº†è®¡ç®—æˆæœ¬å’Œæ—¶é—´ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.05652', 'title': 'BEHAVIOR Robot Suite: Streamlining Real-World Whole-Body Manipulation\n  for Everyday Household Activities', 'url': 'https://huggingface.co/papers/2503.05652', 'abstract': "Real-world household tasks present significant challenges for mobile manipulation robots. An analysis of existing robotics benchmarks reveals that successful task performance hinges on three key whole-body control capabilities: bimanual coordination, stable and precise navigation, and extensive end-effector reachability. Achieving these capabilities requires careful hardware design, but the resulting system complexity further complicates visuomotor policy learning. To address these challenges, we introduce the BEHAVIOR Robot Suite (BRS), a comprehensive framework for whole-body manipulation in diverse household tasks. Built on a bimanual, wheeled robot with a 4-DoF torso, BRS integrates a cost-effective whole-body teleoperation interface for data collection and a novel algorithm for learning whole-body visuomotor policies. We evaluate BRS on five challenging household tasks that not only emphasize the three core capabilities but also introduce additional complexities, such as long-range navigation, interaction with articulated and deformable objects, and manipulation in confined spaces. We believe that BRS's integrated robotic embodiment, data collection interface, and learning framework mark a significant step toward enabling real-world whole-body manipulation for everyday household tasks. BRS is open-sourced at https://behavior-robot-suite.github.io/", 'score': 8, 'issue_id': 2608, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 7', 'zh': '3æœˆ7æ—¥'}, 'hash': '52a7efb2f40f020a', 'authors': ['Yunfan Jiang', 'Ruohan Zhang', 'Josiah Wong', 'Chen Wang', 'Yanjie Ze', 'Hang Yin', 'Cem Gokmen', 'Shuran Song', 'Jiajun Wu', 'Li Fei-Fei'], 'affiliations': ['Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2503.05652.jpg', 'data': {'categories': ['#robotics', '#open_source', '#dataset', '#training'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞšĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ´Ğ¾Ğ¼Ğ°ÑˆĞ½Ğ¸Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ BEHAVIOR Robot Suite (BRS) - ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ² Ğ´Ğ¾Ğ¼Ğ°ÑˆĞ½Ğ¸Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ…. BRS Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ´Ğ²ÑƒÑ€ÑƒĞºĞ¾Ğ¼ ĞºĞ¾Ğ»ĞµÑĞ½Ğ¾Ğ¼ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğµ Ñ 4-Ğ¾ÑĞµĞ²Ñ‹Ğ¼ Ñ‚Ğ¾Ñ€ÑĞ¾Ğ¼ Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹Ñ Ñ‚ĞµĞ»ĞµÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ¾Ğ¼Ğ¾Ñ‚Ğ¾Ñ€Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ°Ğ¼. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ¿ÑÑ‚Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ±Ñ‹Ñ‚Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ±Ğ¸Ğ¼Ğ°Ğ½ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸, Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾Ğ¹ Ğ´Ğ¾ÑÑĞ³Ğ°ĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ¾Ğ². BRS Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ ÑˆĞ°Ğ³ Ğ²Ğ¿ĞµÑ€ĞµĞ´ Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ¾Ğ¼Ğ°ÑˆĞ½Ğ¸Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ….'}, 'en': {'title': 'Empowering Robots for Everyday Household Tasks with BRS', 'desc': 'This paper presents the BEHAVIOR Robot Suite (BRS), a framework designed to enhance mobile manipulation robots for household tasks. It identifies three essential capabilities for effective task performance: bimanual coordination, stable navigation, and extensive reachability. The BRS integrates a teleoperation interface for data collection and a novel algorithm for learning visuomotor policies, addressing the complexities of hardware design and policy learning. The framework is evaluated on five challenging tasks that test these capabilities in real-world scenarios, aiming to improve robotic manipulation in everyday environments.'}, 'zh': {'title': 'å®ç°å®¶åº­ä»»åŠ¡çš„å…¨èº«æ“æ§æœºå™¨äºº', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†BEHAVIORæœºå™¨äººå¥—ä»¶ï¼ˆBRSï¼‰ï¼Œæ—¨åœ¨è§£å†³ç§»åŠ¨æ“ä½œæœºå™¨äººåœ¨å®¶åº­ä»»åŠ¡ä¸­é¢ä¸´çš„æŒ‘æˆ˜ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒæˆåŠŸå®Œæˆä»»åŠ¡ä¾èµ–äºä¸‰é¡¹å…³é”®çš„å…¨èº«æ§åˆ¶èƒ½åŠ›ï¼šåŒæ‰‹åè°ƒã€ç¨³å®šç²¾ç¡®çš„å¯¼èˆªå’Œå¹¿æ³›çš„æœ«ç«¯æ‰§è¡Œå™¨å¯è¾¾æ€§ã€‚BRSæ¡†æ¶ç»“åˆäº†ä¸€ä¸ªåŒæ‰‹è½®å¼æœºå™¨äººå’Œ4è‡ªç”±åº¦çš„èº¯å¹²ï¼Œæä¾›äº†ä¸€ç§ç»æµé«˜æ•ˆçš„å…¨èº«é¥æ“ä½œæ¥å£ç”¨äºæ•°æ®æ”¶é›†ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°ç®—æ³•ç”¨äºå­¦ä¹ å…¨èº«è§†è§‰è¿åŠ¨ç­–ç•¥ã€‚é€šè¿‡åœ¨äº”ä¸ªå¤æ‚çš„å®¶åº­ä»»åŠ¡ä¸Šè¯„ä¼°BRSï¼Œå±•ç¤ºäº†å…¶åœ¨é•¿è·ç¦»å¯¼èˆªã€ä¸å¯åŠ¨å’Œå¯å˜å½¢ç‰©ä½“çš„äº¤äº’ä»¥åŠåœ¨ç‹­å°ç©ºé—´ä¸­çš„æ“ä½œèƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.04824', 'title': 'ProReflow: Progressive Reflow with Decomposed Velocity', 'url': 'https://huggingface.co/papers/2503.04824', 'abstract': 'Diffusion models have achieved significant progress in both image and video generation while still suffering from huge computation costs. As an effective solution, flow matching aims to reflow the diffusion process of diffusion models into a straight line for a few-step and even one-step generation. However, in this paper, we suggest that the original training pipeline of flow matching is not optimal and introduce two techniques to improve it. Firstly, we introduce progressive reflow, which progressively reflows the diffusion models in local timesteps until the whole diffusion progresses, reducing the difficulty of flow matching. Second, we introduce aligned v-prediction, which highlights the importance of direction matching in flow matching over magnitude matching. Experimental results on SDv1.5 and SDXL demonstrate the effectiveness of our method, for example, conducting on SDv1.5 achieves an FID of 10.70 on MSCOCO2014 validation set with only 4 sampling steps, close to our teacher model (32 DDIM steps, FID = 10.05).', 'score': 8, 'issue_id': 2616, 'pub_date': '2025-03-05', 'pub_date_card': {'ru': '5 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 5', 'zh': '3æœˆ5æ—¥'}, 'hash': '3380a5ba02714266', 'authors': ['Lei Ke', 'Haohang Xu', 'Xuefei Ning', 'Yu Li', 'Jiajun Li', 'Haoling Li', 'Yuxuan Lin', 'Dongsheng Jiang', 'Yujiu Yang', 'Linfeng Zhang'], 'affiliations': ['Huawei Inc.', 'Shanghai Jiao Tong University', 'Tsinghua University', 'University of Electronic Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2503.04824.jpg', 'data': {'categories': ['#training', '#cv', '#diffusion', '#optimization'], 'emoji': 'ğŸŒŠ', 'ru': {'title': 'ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ² Ğ´Ğ»Ñ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑƒÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ 'progressive reflow', ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾ Ğ¿ĞµÑ€ĞµÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ, ÑƒĞ¿Ñ€Ğ¾Ñ‰Ğ°Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ². Ğ¢Ğ°ĞºĞ¶Ğµ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ 'aligned v-prediction', Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‰Ğ°Ñ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… SDv1.5 Ğ¸ SDXL Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¼ĞµĞ½ÑŒÑˆĞµĞ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğµ ÑˆĞ°Ğ³Ğ¾Ğ² ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."}, 'en': {'title': 'Streamlining Diffusion: Faster Generation with Flow Matching Enhancements', 'desc': 'This paper addresses the high computational costs associated with diffusion models used for image and video generation. It proposes flow matching as a method to streamline the diffusion process, allowing for faster generation in fewer steps. The authors introduce two enhancements: progressive reflow, which simplifies the flow matching by gradually adjusting local timesteps, and aligned v-prediction, which emphasizes the importance of direction over magnitude in flow matching. Experimental results show that their approach significantly improves performance, achieving competitive results with fewer sampling steps compared to traditional methods.'}, 'zh': {'title': 'ä¼˜åŒ–æ‰©æ•£æ¨¡å‹çš„æµåŒ¹é…è®­ç»ƒ', 'desc': 'æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒå’Œè§†é¢‘ç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†è®¡ç®—æˆæœ¬ä»ç„¶å¾ˆé«˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼ŒæµåŒ¹é…æŠ€æœ¯å°†æ‰©æ•£è¿‡ç¨‹é‡æ–°è°ƒæ•´ä¸ºç›´çº¿ï¼Œä»¥å®ç°å°‘æ­¥ç”šè‡³ä¸€æ­¥çš„ç”Ÿæˆã€‚æœ¬æ–‡æå‡ºäº†ä¸¤ç§æ”¹è¿›æµåŒ¹é…è®­ç»ƒæµç¨‹çš„æŠ€æœ¯ï¼šé€æ­¥é‡æ–°æµåŠ¨å’Œå¯¹é½çš„vé¢„æµ‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç”Ÿæˆè´¨é‡ä¸Šæ¥è¿‘æ•™å¸ˆæ¨¡å‹ï¼ŒåŒæ—¶å¤§å¹…å‡å°‘äº†é‡‡æ ·æ­¥éª¤ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.05447', 'title': 'Linear-MoE: Linear Sequence Modeling Meets Mixture-of-Experts', 'url': 'https://huggingface.co/papers/2503.05447', 'abstract': 'Linear Sequence Modeling (LSM) like linear attention, state space models and linear RNNs, and Mixture-of-Experts (MoE) have recently emerged as significant architectural improvements. In this paper, we introduce Linear-MoE, a production-level system for modeling and training large-scale models that integrate LSM with MoE. Linear-MoE leverages the advantages of both LSM modules for linear-complexity sequence modeling and MoE layers for sparsely activation, aiming to offer high performance with efficient training. The Linear-MoE system comprises: 1) Modeling subsystem, which provides a unified framework supporting all instances of LSM. and 2) Training subsystem, which facilitates efficient training by incorporating various advanced parallelism technologies, particularly Sequence Parallelism designed for Linear-MoE models. Additionally, we explore hybrid models that combine Linear-MoE layers with standard Transformer-MoE layers with its Sequence Parallelism to further enhance model flexibility and performance. Evaluations on two model series, A0.3B-2B and A1B-7B, demonstrate Linear-MoE achieves efficiency gains while maintaining competitive performance on various benchmarks, showcasing its potential as a next-generation foundational model architecture. Code: https://github.com/OpenSparseLLMs/Linear-MoE.', 'score': 6, 'issue_id': 2614, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 7', 'zh': '3æœˆ7æ—¥'}, 'hash': '3975a97b4236e791', 'authors': ['Weigao Sun', 'Disen Lan', 'Tong Zhu', 'Xiaoye Qu', 'Yu Cheng'], 'affiliations': ['Shanghai AI Laboratory', 'Soochow University', 'South China University of Technology', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2503.05447.jpg', 'data': {'categories': ['#optimization', '#training', '#architecture'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Linear-MoE: ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞ¼ĞµÑĞ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Linear-MoE - ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰ÑƒÑ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ (LSM) Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼ ÑĞ¼ĞµÑĞ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² (MoE). Linear-MoE Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° LSM-Ğ¼Ğ¾Ğ´ÑƒĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ MoE-ÑĞ»Ğ¾ĞµĞ² Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸, ÑÑ‚Ñ€ĞµĞ¼ÑÑÑŒ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ñ‚ÑŒ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¿Ğ¾Ğ´ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‰ÑƒÑ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ Ğ²ÑĞµÑ… ÑĞºĞ·ĞµĞ¼Ğ¿Ğ»ÑÑ€Ğ¾Ğ² LSM, Ğ¸ Ğ¿Ğ¾Ğ´ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸ÑĞ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»Ğ¸Ğ·Ğ¼Ğ°. ĞÑ†ĞµĞ½ĞºĞ¸ Ğ½Ğ° Ğ´Ğ²ÑƒÑ… ÑĞµÑ€Ğ¸ÑÑ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Linear-MoE Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ….'}, 'en': {'title': 'Efficient Modeling with Linear-MoE: Merging LSM and MoE for High Performance', 'desc': 'This paper presents Linear-MoE, a new system that combines Linear Sequence Modeling (LSM) with Mixture-of-Experts (MoE) to improve the efficiency and performance of large-scale models. Linear-MoE utilizes linear-complexity sequence modeling from LSM and the sparsity benefits of MoE layers, allowing for effective training and high performance. The system includes a modeling subsystem for LSM and a training subsystem that employs advanced parallelism techniques, particularly Sequence Parallelism. Experiments show that Linear-MoE achieves better efficiency while delivering competitive results on various benchmarks, indicating its promise as a foundational model architecture.'}, 'zh': {'title': 'çº¿æ€§-MoEï¼šé«˜æ•ˆçš„åºåˆ—å»ºæ¨¡ä¸è®­ç»ƒæ–°æ¶æ„', 'desc': 'çº¿æ€§åºåˆ—å»ºæ¨¡ï¼ˆLSMï¼‰å’Œä¸“å®¶æ··åˆæ¨¡å‹ï¼ˆMoEï¼‰æœ€è¿‘æˆä¸ºé‡è¦çš„æ¶æ„æ”¹è¿›ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºLinear-MoEçš„ç³»ç»Ÿï¼Œå®ƒå°†LSMä¸MoEç»“åˆï¼Œç”¨äºå»ºæ¨¡å’Œè®­ç»ƒå¤§è§„æ¨¡æ¨¡å‹ã€‚Linear-MoEåˆ©ç”¨LSMæ¨¡å—çš„çº¿æ€§å¤æ‚åº¦åºåˆ—å»ºæ¨¡ä¼˜åŠ¿å’ŒMoEå±‚çš„ç¨€ç–æ¿€æ´»ç‰¹æ€§ï¼Œæ—¨åœ¨æä¾›é«˜æ€§èƒ½å’Œé«˜æ•ˆè®­ç»ƒã€‚é€šè¿‡å¯¹A0.3B-2Bå’ŒA1B-7Bæ¨¡å‹ç³»åˆ—çš„è¯„ä¼°ï¼ŒLinear-MoEåœ¨ä¿æŒç«äº‰æ€§èƒ½çš„åŒæ—¶å®ç°äº†æ•ˆç‡æå‡ï¼Œå±•ç¤ºäº†å…¶ä½œä¸ºä¸‹ä¸€ä»£åŸºç¡€æ¨¡å‹æ¶æ„çš„æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.04548', 'title': 'An Empirical Study on Eliciting and Improving R1-like Reasoning Models', 'url': 'https://huggingface.co/papers/2503.04548', 'abstract': 'In this report, we present the third technical report on the development of slow-thinking models as part of the STILL project. As the technical pathway becomes clearer, scaling RL training has become a central technique for implementing such reasoning models. We systematically experiment with and document the effects of various factors influencing RL training, conducting experiments on both base models and fine-tuned models. Specifically, we demonstrate that our RL training approach consistently improves the Qwen2.5-32B base models, enhancing both response length and test accuracy. Furthermore, we show that even when a model like DeepSeek-R1-Distill-Qwen-1.5B has already achieved a high performance level, it can be further refined through RL training, reaching an accuracy of 39.33% on AIME 2024. Beyond RL training, we also explore the use of tool manipulation, finding that it significantly boosts the reasoning performance of large reasoning models. This approach achieves a remarkable accuracy of 86.67% with greedy search on AIME 2024, underscoring its effectiveness in enhancing model capabilities. We release our resources at the STILL project website: https://github.com/RUCAIBox/Slow_Thinking_with_LLMs.', 'score': 6, 'issue_id': 2615, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 6', 'zh': '3æœˆ6æ—¥'}, 'hash': 'defc053b9079a4a2', 'authors': ['Zhipeng Chen', 'Yingqian Min', 'Beichen Zhang', 'Jie Chen', 'Jinhao Jiang', 'Daixuan Cheng', 'Wayne Xin Zhao', 'Zheng Liu', 'Xu Miao', 'Yang Lu', 'Lei Fang', 'Zhongyuan Wang', 'Ji-Rong Wen'], 'affiliations': ['BAAI', 'DataCanvas', 'Gaoling School of Artificial Intelligence, Renmin University of China'], 'pdf_title_img': 'assets/pdf/title_img/2503.04548.jpg', 'data': {'categories': ['#reasoning', '#training', '#rl', '#optimization'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ£ÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¼ Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ñ‹ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL). Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¸Ğ·ÑƒÑ‡Ğ°Ğ»Ğ¸ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¾Ğ² Ğ½Ğ° RL-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ĞºĞ°Ğº Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ‘Ñ‹Ğ»Ğ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ RL-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Qwen2.5-32B, ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°Ñ Ğ´Ğ»Ğ¸Ğ½Ñƒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ‚ĞµÑÑ‚Ğ°Ñ…. ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑĞ¸Ğ»Ğ¾ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ½ÑƒĞ² Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ 86.67% Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… AIME 2024.'}, 'en': {'title': 'Enhancing Reasoning with Reinforcement Learning and Tool Manipulation', 'desc': 'This paper discusses advancements in slow-thinking models within the STILL project, focusing on reinforcement learning (RL) training techniques. The authors conduct systematic experiments to analyze how different factors affect RL training, leading to improvements in the Qwen2.5-32B base models. They demonstrate that even high-performing models can be further enhanced through RL training, achieving notable accuracy on benchmark tasks. Additionally, the study highlights the benefits of tool manipulation in improving reasoning performance, achieving impressive results on the AIME 2024 challenge.'}, 'zh': {'title': 'å¼ºåŒ–å­¦ä¹ æå‡æ¨ç†æ¨¡å‹çš„èƒ½åŠ›', 'desc': 'æœ¬æŠ¥å‘Šä»‹ç»äº†STILLé¡¹ç›®ä¸­æ…¢æ€ç»´æ¨¡å‹å‘å±•çš„ç¬¬ä¸‰ä¸ªæŠ€æœ¯æŠ¥å‘Šã€‚æˆ‘ä»¬ç³»ç»Ÿåœ°å®éªŒå¹¶è®°å½•äº†å½±å“å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒçš„å„ç§å› ç´ ï¼Œç‰¹åˆ«æ˜¯åœ¨åŸºç¡€æ¨¡å‹å’Œå¾®è°ƒæ¨¡å‹ä¸Šçš„å®éªŒã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼ŒRLè®­ç»ƒæ–¹æ³•èƒ½å¤Ÿæ˜¾è‘—æé«˜Qwen2.5-32BåŸºç¡€æ¨¡å‹çš„å“åº”é•¿åº¦å’Œæµ‹è¯•å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å‘ç°å·¥å…·æ“ä½œçš„ä½¿ç”¨æ˜¾è‘—æå‡äº†å¤§å‹æ¨ç†æ¨¡å‹çš„æ¨ç†æ€§èƒ½ï¼Œè¾¾åˆ°äº†86.67%çš„å‡†ç¡®ç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.04359', 'title': 'LONGCODEU: Benchmarking Long-Context Language Models on Long Code\n  Understanding', 'url': 'https://huggingface.co/papers/2503.04359', 'abstract': "Current advanced long-context language models offer great potential for real-world software engineering applications. However, progress in this critical domain remains hampered by a fundamental limitation: the absence of a rigorous evaluation framework for long code understanding. To gap this obstacle, we propose a long code understanding benchmark LONGCODEU from four aspects (8 tasks) to evaluate LCLMs' long code understanding ability required for practical applications, including code unit perception, intra-code unit understanding, inter-code unit relation understanding, and long code documentation understanding. We evaluate 9 popular LCLMs on LONGCODEU (i.e., 6 general models and 3 code models). Our experimental results reveal key limitations in current LCLMs' capabilities for long code understanding. Particularly, the performance of LCLMs drops dramatically when the long code length is greater than 32K, falling far short of their claimed 128K-1M context windows. In the four aspects, inter-code unit relation understanding is the most challenging for LCLMs. Our study provides valuable insights for optimizing LCLMs and driving advancements in software engineering.", 'score': 5, 'issue_id': 2617, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 6', 'zh': '3æœˆ6æ—¥'}, 'hash': '808bf0135ca11113', 'authors': ['Jia Li', 'Xuyuan Guo', 'Lei Li', 'Kechi Zhang', 'Ge Li', 'Jia Li', 'Zhengwei Tao', 'Fang Liu', 'Chongyang Tao', 'Yuqi Zhu', 'Zhi Jin'], 'affiliations': ['Key Lab of High Confidence Software Technology (Peking University), MoE, School of Computer Science, Peking University, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.04359.jpg', 'data': {'categories': ['#dataset', '#long_context', '#benchmark', '#optimization'], 'emoji': 'ğŸ“', 'ru': {'title': 'ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº LONGCODEU Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ (LCLM) Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ´. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 8 Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² 4 Ğ°ÑĞ¿ĞµĞºÑ‚Ğ°Ñ…: Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ ĞºĞ¾Ğ´Ğ¾Ğ²Ñ‹Ñ… ĞµĞ´Ğ¸Ğ½Ğ¸Ñ†, Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸ ĞºĞ¾Ğ´Ğ¾Ğ²Ñ‹Ñ… ĞµĞ´Ğ¸Ğ½Ğ¸Ñ†, Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ÑĞ²ÑĞ·ĞµĞ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ¾Ğ´Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ ĞµĞ´Ğ¸Ğ½Ğ¸Ñ†Ğ°Ğ¼Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ 9 Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… LCLM Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ°Ğ´Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğµ ĞºĞ¾Ğ´Ğ° Ğ±Ğ¾Ğ»ĞµĞµ 32K Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞĞ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼ Ğ°ÑĞ¿ĞµĞºÑ‚Ğ¾Ğ¼ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾ÑÑŒ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ÑĞ²ÑĞ·ĞµĞ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ¾Ğ´Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ ĞµĞ´Ğ¸Ğ½Ğ¸Ñ†Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Bridging the Gap in Long Code Understanding for LCLMs', 'desc': 'This paper addresses the challenges faced by advanced long-context language models (LCLMs) in understanding long code, which is crucial for software engineering. It introduces a new benchmark called LONGCODEU, designed to evaluate LCLMs across eight tasks that cover various aspects of long code comprehension. The study evaluates nine popular LCLMs and finds significant performance drops when handling code longer than 32K tokens, indicating limitations in their capabilities. The findings highlight that understanding relationships between code units is particularly difficult for these models, providing insights for future improvements in LCLMs.'}, 'zh': {'title': 'æå‡é•¿ä»£ç ç†è§£èƒ½åŠ›çš„å…³é”®è¯„ä¼°åŸºå‡†', 'desc': 'å½“å‰å…ˆè¿›çš„é•¿æ–‡æœ¬è¯­è¨€æ¨¡å‹åœ¨è½¯ä»¶å·¥ç¨‹åº”ç”¨ä¸­å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œç¼ºä¹ä¸¥æ ¼çš„è¯„ä¼°æ¡†æ¶é™åˆ¶äº†è¿™ä¸€é¢†åŸŸçš„è¿›å±•ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåä¸ºLONGCODEUçš„é•¿ä»£ç ç†è§£åŸºå‡†ï¼Œæ¶µç›–äº†å››ä¸ªæ–¹é¢çš„å…«ä¸ªä»»åŠ¡ï¼Œä»¥è¯„ä¼°é•¿ä»£ç ç†è§£èƒ½åŠ›ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œå½“å‰çš„é•¿ä»£ç è¯­è¨€æ¨¡å‹åœ¨å¤„ç†è¶…è¿‡32Kçš„é•¿ä»£ç æ—¶æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œå°¤å…¶æ˜¯åœ¨ç†è§£ä»£ç å•å…ƒä¹‹é—´çš„å…³ç³»æ–¹é¢æœ€å…·æŒ‘æˆ˜æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.01713', 'title': 'SAGE: A Framework of Precise Retrieval for RAG', 'url': 'https://huggingface.co/papers/2503.01713', 'abstract': 'Retrieval-augmented generation (RAG) has demonstrated significant proficiency in conducting question-answering (QA) tasks within a specified corpus. Nonetheless, numerous failure instances of RAG in QA still exist. These failures are not solely attributable to the limitations of Large Language Models (LLMs); instead, they predominantly arise from the retrieval of inaccurate information for LLMs due to two limitations: (1) Current RAG methods segment the corpus without considering semantics, making it difficult to find relevant context due to impaired correlation between questions and the segments. (2) There is a trade-off between missing essential context with fewer context retrieved and getting irrelevant context with more context retrieved.   In this paper, we introduce a RAG framework (SAGE), to overcome these limitations. First, to address the segmentation issue without considering semantics, we propose to train a semantic segmentation model. This model is trained to segment the corpus into semantically complete chunks. Second, to ensure that only the most relevant chunks are retrieved while the irrelevant ones are ignored, we design a chunk selection algorithm to dynamically select chunks based on the decreasing speed of the relevance score, leading to a more relevant selection. Third, to further ensure the precision of the retrieved chunks, we propose letting LLMs assess whether retrieved chunks are excessive or lacking and then adjust the amount of context accordingly. Experiments show that SAGE outperforms baselines by 61.25% in the quality of QA on average. Moreover, by avoiding retrieving noisy context, SAGE lowers the cost of the tokens consumed in LLM inference and achieves a 49.41% enhancement in cost efficiency on average. Additionally, our work offers valuable insights for boosting RAG.', 'score': 4, 'issue_id': 2613, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 3', 'zh': '3æœˆ3æ—¥'}, 'hash': 'dc1c8022a96cab3e', 'authors': ['Jintao Zhang', 'Guoliang Li', 'Jinyang Su'], 'affiliations': ['Department of Computer Science Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.01713.jpg', 'data': {'categories': ['#rag', '#optimization', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'SAGE: Ğ¡ĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¹ RAG Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº SAGE Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ retrieval-augmented generation (RAG) Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ° Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ², Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ SAGE Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° 61.25% Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¸ Ğ½Ğ° 49.41% Ğ¿Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµÑ‚ÑŒ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² RAG Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑĞ¸Ñ‚ÑŒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'SAGE: Smarter Retrieval for Better Question-Answering', 'desc': 'This paper presents a new framework called SAGE to improve retrieval-augmented generation (RAG) for question-answering tasks. It addresses two main issues: the ineffective segmentation of the corpus that ignores semantics and the trade-off between retrieving too little or too much context. SAGE introduces a semantic segmentation model to create meaningful chunks and a dynamic chunk selection algorithm to ensure only the most relevant information is retrieved. The results show that SAGE significantly enhances QA quality and cost efficiency compared to existing methods.'}, 'zh': {'title': 'æå‡é—®ç­”è´¨é‡çš„æ™ºèƒ½æ£€ç´¢æ¡†æ¶', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ£€ç´¢å¢å¼ºç”Ÿæˆæ¡†æ¶ï¼ˆSAGEï¼‰ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰RAGæ–¹æ³•åœ¨é—®ç­”ä»»åŠ¡ä¸­çš„å±€é™æ€§ã€‚é¦–å…ˆï¼ŒSAGEé€šè¿‡è®­ç»ƒè¯­ä¹‰åˆ†å‰²æ¨¡å‹ï¼Œå°†è¯­æ–™åº“åˆ†å‰²æˆè¯­ä¹‰å®Œæ•´çš„å—ï¼Œä»¥æé«˜ç›¸å…³æ€§ã€‚å…¶æ¬¡ï¼Œè®¾è®¡äº†ä¸€ç§åŠ¨æ€é€‰æ‹©ç®—æ³•ï¼Œæ ¹æ®ç›¸å…³æ€§å¾—åˆ†çš„ä¸‹é™é€Ÿåº¦é€‰æ‹©æœ€ç›¸å…³çš„å—ï¼Œä»è€Œé¿å…æ— å…³ä¿¡æ¯çš„å¹²æ‰°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSAGEåœ¨é—®ç­”è´¨é‡ä¸Šæ¯”åŸºçº¿æé«˜äº†61.25%ï¼Œå¹¶ä¸”åœ¨æˆæœ¬æ•ˆç‡ä¸Šæå‡äº†49.41%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.01840', 'title': 'EAGLE-3: Scaling up Inference Acceleration of Large Language Models via\n  Training-Time Test', 'url': 'https://huggingface.co/papers/2503.01840', 'abstract': "The sequential nature of modern LLMs makes them expensive and slow, and speculative sampling has proven to be an effective solution to this problem. Methods like EAGLE perform autoregression at the feature level, reusing top-layer features from the target model to achieve better results than vanilla speculative sampling. A growing trend in the LLM community is scaling up training data to improve model intelligence without increasing inference costs. However, we observe that scaling up data provides limited improvements for EAGLE. We identify that this limitation arises from EAGLE's feature prediction constraints. In this paper, we introduce EAGLE-3, which abandons feature prediction in favor of direct token prediction and replaces reliance on top-layer features with multi-layer feature fusion via a technique named training-time test. These improvements significantly enhance performance and enable the draft model to fully benefit from scaling up training data. Our experiments include both chat models and reasoning models, evaluated on five tasks. The results show that EAGLE-3 achieves a speedup ratio up to 6.5x, with about 1.4x improvement over EAGLE-2. The code is available at https://github.com/SafeAILab/EAGLE.", 'score': 3, 'issue_id': 2619, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 3', 'zh': '3æœˆ3æ—¥'}, 'hash': 'cf0ee90637c3e71a', 'authors': ['Yuhui Li', 'Fangyun Wei', 'Chao Zhang', 'Hongyang Zhang'], 'affiliations': ['Microsoft Research', 'Peking University', 'University of Waterloo', 'Vector Institute'], 'pdf_title_img': 'assets/pdf/title_img/2503.01840.jpg', 'data': {'categories': ['#training', '#inference', '#optimization', '#reasoning'], 'emoji': 'ğŸš€', 'ru': {'title': 'EAGLE-3: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ LLM Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ EAGLE-3, Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¿ĞµĞºÑƒĞ»ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ²ĞµÑ€ÑĞ¸Ğ¹, EAGLE-3 Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ÑĞ¼Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ»Ğ¾Ğ¹Ğ½Ğ¾Ğµ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞ¼Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ EAGLE-3 Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ´Ğ¾ 6,5 Ñ€Ğ°Ğ· Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸ĞµĞ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ½Ğ¾ Ğ² 1,4 Ñ€Ğ°Ğ·Ğ° Ğ»ÑƒÑ‡ÑˆĞµ, Ñ‡ĞµĞ¼ EAGLE-2. ĞœĞµÑ‚Ğ¾Ğ´ Ğ±Ñ‹Ğ» Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ĞºĞ°Ğº Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°ÑÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'EAGLE-3: Speeding Up LLMs with Direct Token Prediction', 'desc': 'This paper presents EAGLE-3, an advanced model that improves the efficiency of large language models (LLMs) by shifting from feature prediction to direct token prediction. By utilizing multi-layer feature fusion instead of relying solely on top-layer features, EAGLE-3 enhances performance and allows for better utilization of larger training datasets. The authors demonstrate that EAGLE-3 achieves significant speed improvements, with a speedup ratio of up to 6.5 times compared to previous methods. The results indicate that EAGLE-3 not only accelerates inference but also improves model intelligence, making it a valuable advancement in the field of machine learning.'}, 'zh': {'title': 'EAGLE-3ï¼šæå‡å¤§è¯­è¨€æ¨¡å‹æ¨ç†é€Ÿåº¦çš„åˆ›æ–°æ–¹æ¡ˆ', 'desc': 'ç°ä»£å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é¡ºåºç‰¹æ€§ä½¿å…¶åœ¨æ¨ç†æ—¶æˆæœ¬é«˜ä¸”é€Ÿåº¦æ…¢ï¼Œæ¨æµ‹é‡‡æ ·æ˜¯ä¸€ç§æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚EAGLEæ–¹æ³•åœ¨ç‰¹å¾å±‚é¢è¿›è¡Œè‡ªå›å½’ï¼Œåˆ©ç”¨ç›®æ ‡æ¨¡å‹çš„é¡¶å±‚ç‰¹å¾ï¼Œå–å¾—æ¯”ä¼ ç»Ÿæ¨æµ‹é‡‡æ ·æ›´å¥½çš„æ•ˆæœã€‚å°½ç®¡åœ¨LLMç¤¾åŒºä¸­ï¼Œæ‰©å¤§è®­ç»ƒæ•°æ®ä»¥æé«˜æ¨¡å‹æ™ºèƒ½çš„è¶‹åŠ¿æ—¥ç›Šå¢é•¿ï¼Œä½†æˆ‘ä»¬å‘ç°å¯¹äºEAGLEæ¥è¯´ï¼Œæ‰©å¤§æ•°æ®çš„æ•ˆæœæœ‰é™ã€‚æœ¬æ–‡æå‡ºäº†EAGLE-3ï¼Œæ”¾å¼ƒç‰¹å¾é¢„æµ‹ï¼Œé‡‡ç”¨ç›´æ¥çš„æ ‡è®°é¢„æµ‹ï¼Œå¹¶é€šè¿‡è®­ç»ƒæ—¶æµ‹è¯•çš„æŠ€æœ¯å®ç°å¤šå±‚ç‰¹å¾èåˆï¼Œä»è€Œæ˜¾è‘—æå‡æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.18968', 'title': 'Know You First and Be You Better: Modeling Human-Like User Simulators\n  via Implicit Profiles', 'url': 'https://huggingface.co/papers/2502.18968', 'abstract': 'User simulators are crucial for replicating human interactions with dialogue systems, supporting both collaborative training and automatic evaluation, especially for large language models (LLMs). However, existing simulators often rely solely on text utterances, missing implicit user traits such as personality, speaking style, and goals. In contrast, persona-based methods lack generalizability, as they depend on predefined profiles of famous individuals or archetypes. To address these challenges, we propose User Simulator with implicit Profiles (USP), a framework that infers implicit user profiles from human-machine conversations and uses them to generate more personalized and realistic dialogues. We first develop an LLM-driven extractor with a comprehensive profile schema. Then, we refine the simulation through conditional supervised fine-tuning and reinforcement learning with cycle consistency, optimizing it at both the utterance and conversation levels. Finally, we adopt a diverse profile sampler to capture the distribution of real-world user profiles. Experimental results demonstrate that USP outperforms strong baselines in terms of authenticity and diversity while achieving comparable performance in consistency. Furthermore, dynamic multi-turn evaluations based on USP strongly align with mainstream benchmarks, demonstrating its effectiveness in real-world applications.', 'score': 3, 'issue_id': 2619, 'pub_date': '2025-02-26', 'pub_date_card': {'ru': '26 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 26', 'zh': '2æœˆ26æ—¥'}, 'hash': '32f11f6f4b295fc5', 'authors': ['Kuang Wang', 'Xianfei Li', 'Shenghao Yang', 'Li Zhou', 'Feng Jiang', 'Haizhou Li'], 'affiliations': ['Shenzhen Research Institute of Big Data', 'Shenzhen University of Advanced Technology', 'The Chinese University of Hong Kong, Shenzhen'], 'pdf_title_img': 'assets/pdf/title_img/2502.18968.jpg', 'data': {'categories': ['#rl', '#benchmark', '#training', '#optimization', '#games', '#agents', '#interpretability'], 'emoji': 'ğŸ­', 'ru': {'title': 'Ğ ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ°Ñ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ½ĞµÑĞ²Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ - User Simulator with implicit Profiles (USP). USP Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ Ğ½ĞµÑĞ²Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ¸Ğ· Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ² Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ñ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ¾Ğ¹ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ñ… Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑĞºÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ¾Ñ€ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ USP Ğ½Ğ°Ğ´ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾ Ğ°ÑƒÑ‚ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ².'}, 'en': {'title': 'Enhancing Dialogue Systems with Implicit User Profiles', 'desc': 'This paper introduces the User Simulator with implicit Profiles (USP), a novel framework designed to enhance dialogue systems by incorporating implicit user traits like personality and goals. Unlike traditional simulators that focus only on text, USP infers user profiles from actual human-machine interactions, allowing for more personalized dialogue generation. The framework employs a large language model (LLM) to extract user profiles and utilizes conditional supervised fine-tuning and reinforcement learning to improve dialogue quality. Experimental results show that USP significantly improves the authenticity and diversity of generated dialogues while maintaining consistency, making it effective for real-world applications.'}, 'zh': {'title': 'éšå¼ç”¨æˆ·æ¡£æ¡ˆï¼Œæå‡å¯¹è¯çœŸå®æ„Ÿ', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨æˆ·æ¨¡æ‹Ÿå™¨ï¼ˆUSPï¼‰ï¼Œæ—¨åœ¨é€šè¿‡éšå¼ç”¨æˆ·ç‰¹å¾ç”Ÿæˆæ›´ä¸ªæ€§åŒ–å’ŒçœŸå®çš„å¯¹è¯ã€‚ç°æœ‰çš„æ¨¡æ‹Ÿå™¨é€šå¸¸åªä¾èµ–æ–‡æœ¬ï¼Œå¿½è§†äº†ç”¨æˆ·çš„ä¸ªæ€§ã€è¯´è¯é£æ ¼å’Œç›®æ ‡ç­‰éšæ€§ç‰¹å¾ã€‚USPé€šè¿‡ä»äººæœºå¯¹è¯ä¸­æ¨æ–­éšå¼ç”¨æˆ·æ¡£æ¡ˆï¼Œç»“åˆå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼Œä¼˜åŒ–å¯¹è¯ç”Ÿæˆè¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒUSPåœ¨çœŸå®æ€§å’Œå¤šæ ·æ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒåŒæ—¶åœ¨ä¸€è‡´æ€§ä¸Šè¡¨ç°ç›¸å½“ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.05315', 'title': 'LoRACode: LoRA Adapters for Code Embeddings', 'url': 'https://huggingface.co/papers/2503.05315', 'abstract': 'Code embeddings are essential for semantic code search; however, current approaches often struggle to capture the precise syntactic and contextual nuances inherent in code. Open-source models such as CodeBERT and UniXcoder exhibit limitations in scalability and efficiency, while high-performing proprietary systems impose substantial computational costs. We introduce a parameter-efficient fine-tuning method based on Low-Rank Adaptation (LoRA) to construct task-specific adapters for code retrieval. Our approach reduces the number of trainable parameters to less than two percent of the base model, enabling rapid fine-tuning on extensive code corpora (2 million samples in 25 minutes on two H100 GPUs). Experiments demonstrate an increase of up to 9.1% in Mean Reciprocal Rank (MRR) for Code2Code search, and up to 86.69% for Text2Code search tasks across multiple programming languages. Distinction in task-wise and language-wise adaptation helps explore the sensitivity of code retrieval for syntactical and linguistic variations.', 'score': 2, 'issue_id': 2610, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 7', 'zh': '3æœˆ7æ—¥'}, 'hash': '95dca112be949ba8', 'authors': ['Saumya Chaturvedi', 'Aman Chadha', 'Laurent Bindschaedler'], 'affiliations': ['AWS GenAI Santa Clara, CA, USA', 'Max Planck Institute for Software Systems Saarbrucken, Germany'], 'pdf_title_img': 'assets/pdf/title_img/2503.05315.jpg', 'data': {'categories': ['#data', '#open_source', '#training', '#optimization', '#plp'], 'emoji': 'ğŸ”', 'ru': {'title': 'LoRA: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ñ‚Ğ¾Ğ½ĞºĞ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° ĞºĞ¾Ğ´Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ´Ğ»Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° ĞºĞ¾Ğ´Ğ°, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Low-Rank Adaptation (LoRA). Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾ Ğ½Ğ°ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¾Ğ±ÑŠĞµĞ¼Ğ°Ñ… ĞºĞ¾Ğ´Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ MRR Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Code2Code Ğ¸ Text2Code. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ¸ÑĞºĞ° ĞºĞ¾Ğ´Ğ° Ğº ÑĞ¸Ğ½Ñ‚Ğ°ĞºÑĞ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸ÑĞ¼.'}, 'en': {'title': 'Efficient Code Retrieval with Low-Rank Adaptation', 'desc': 'This paper addresses the challenges of semantic code search by improving how code embeddings are generated. It highlights the limitations of existing models like CodeBERT and UniXcoder in terms of scalability and efficiency. The authors propose a new method using Low-Rank Adaptation (LoRA) to create task-specific adapters, significantly reducing the number of trainable parameters. Their approach allows for quick fine-tuning on large code datasets, resulting in notable improvements in retrieval performance across various programming languages.'}, 'zh': {'title': 'é«˜æ•ˆä»£ç æ£€ç´¢çš„ä½ç§©é€‚åº”æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºä½ç§©é€‚åº”ï¼ˆLoRAï¼‰çš„å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•ï¼Œç”¨äºæ„å»ºç‰¹å®šä»»åŠ¡çš„ä»£ç æ£€ç´¢é€‚é…å™¨ã€‚è¯¥æ–¹æ³•å°†å¯è®­ç»ƒå‚æ•°å‡å°‘åˆ°åŸºç¡€æ¨¡å‹çš„ä¸åˆ°2%ï¼Œä½¿å¾—åœ¨å¤§è§„æ¨¡ä»£ç è¯­æ–™åº“ä¸Šè¿›è¡Œå¿«é€Ÿå¾®è°ƒæˆä¸ºå¯èƒ½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨ä»£ç åˆ°ä»£ç çš„æ£€ç´¢ä»»åŠ¡ä¸­ï¼Œå¹³å‡å€’æ•°æ’åï¼ˆMRRï¼‰æé«˜äº†9.1%ï¼Œè€Œæ–‡æœ¬åˆ°ä»£ç çš„æ£€ç´¢ä»»åŠ¡æé«˜äº†86.69%ã€‚é€šè¿‡ä»»åŠ¡å’Œè¯­è¨€çš„é€‚åº”æ€§åŒºåˆ†ï¼Œæœ¬æ–‡æ¢è®¨äº†ä»£ç æ£€ç´¢å¯¹è¯­æ³•å’Œè¯­è¨€å˜ä½“çš„æ•æ„Ÿæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.04504', 'title': 'AnyAnomaly: Zero-Shot Customizable Video Anomaly Detection with LVLM', 'url': 'https://huggingface.co/papers/2503.04504', 'abstract': 'Video anomaly detection (VAD) is crucial for video analysis and surveillance in computer vision. However, existing VAD models rely on learned normal patterns, which makes them difficult to apply to diverse environments. Consequently, users should retrain models or develop separate AI models for new environments, which requires expertise in machine learning, high-performance hardware, and extensive data collection, limiting the practical usability of VAD. To address these challenges, this study proposes customizable video anomaly detection (C-VAD) technique and the AnyAnomaly model. C-VAD considers user-defined text as an abnormal event and detects frames containing a specified event in a video. We effectively implemented AnyAnomaly using a context-aware visual question answering without fine-tuning the large vision language model. To validate the effectiveness of the proposed model, we constructed C-VAD datasets and demonstrated the superiority of AnyAnomaly. Furthermore, our approach showed competitive performance on VAD benchmark datasets, achieving state-of-the-art results on the UBnormal dataset and outperforming other methods in generalization across all datasets. Our code is available online at github.com/SkiddieAhn/Paper-AnyAnomaly.', 'score': 1, 'issue_id': 2613, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 6', 'zh': '3æœˆ6æ—¥'}, 'hash': '7dbd20628d3eb105', 'authors': ['Sunghyun Ahn', 'Youngwan Jo', 'Kijung Lee', 'Sein Kwon', 'Inpyo Hong', 'Sanghyun Park'], 'affiliations': ['Yonsei University, Seoul, Korea'], 'pdf_title_img': 'assets/pdf/title_img/2503.04504.jpg', 'data': {'categories': ['#open_source', '#optimization', '#dataset', '#benchmark', '#cv', '#video'], 'emoji': 'ğŸ•µï¸', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ C-VAD. ĞĞ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ AnyAnomaly, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ñ, Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¼, Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ½Ğ¾Ğ²Ñ‹Ñ… ÑÑ€ĞµĞ´. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾-Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ AnyAnomaly Ğ½Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… C-VAD Ğ¸ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹.'}, 'en': {'title': 'Customizable Anomaly Detection for Diverse Video Environments', 'desc': 'This paper introduces a new approach to video anomaly detection (VAD) called customizable video anomaly detection (C-VAD). Unlike traditional VAD models that require retraining for different environments, C-VAD allows users to define what constitutes an abnormal event using text input. The AnyAnomaly model leverages a context-aware visual question answering system, enabling it to detect specified events without the need for extensive fine-tuning. The results show that AnyAnomaly not only performs well on custom datasets but also achieves state-of-the-art results on established VAD benchmarks, demonstrating its versatility and effectiveness.'}, 'zh': {'title': 'å¯å®šåˆ¶çš„è§†é¢‘å¼‚å¸¸æ£€æµ‹ï¼Œè½»æ¾åº”å¯¹å¤šæ ·ç¯å¢ƒ', 'desc': 'è§†é¢‘å¼‚å¸¸æ£€æµ‹ï¼ˆVADï¼‰åœ¨è®¡ç®—æœºè§†è§‰ä¸­çš„è§†é¢‘åˆ†æå’Œç›‘æ§ä¸­è‡³å…³é‡è¦ã€‚ç°æœ‰çš„VADæ¨¡å‹ä¾èµ–äºå­¦ä¹ åˆ°çš„æ­£å¸¸æ¨¡å¼ï¼Œè¿™ä½¿å¾—å®ƒä»¬åœ¨ä¸åŒç¯å¢ƒä¸­çš„åº”ç”¨å˜å¾—å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬ç ”ç©¶æå‡ºäº†å¯å®šåˆ¶çš„è§†é¢‘å¼‚å¸¸æ£€æµ‹ï¼ˆC-VADï¼‰æŠ€æœ¯å’ŒAnyAnomalyæ¨¡å‹ï¼Œå…è®¸ç”¨æˆ·å®šä¹‰å¼‚å¸¸äº‹ä»¶å¹¶æ£€æµ‹è§†é¢‘ä¸­çš„ç›¸å…³å¸§ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶åœ¨UBnormalæ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œå±•ç¤ºäº†å…¶åœ¨æ³›åŒ–èƒ½åŠ›ä¸Šçš„ä¼˜åŠ¿ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.11647', 'title': 'ReCamMaster: Camera-Controlled Generative Rendering from A Single Video', 'url': 'https://huggingface.co/papers/2503.11647', 'abstract': 'Camera control has been actively studied in text or image conditioned video generation tasks. However, altering camera trajectories of a given video remains under-explored, despite its importance in the field of video creation. It is non-trivial due to the extra constraints of maintaining multiple-frame appearance and dynamic synchronization. To address this, we present ReCamMaster, a camera-controlled generative video re-rendering framework that reproduces the dynamic scene of an input video at novel camera trajectories. The core innovation lies in harnessing the generative capabilities of pre-trained text-to-video models through a simple yet powerful video conditioning mechanism -- its capability often overlooked in current research. To overcome the scarcity of qualified training data, we construct a comprehensive multi-camera synchronized video dataset using Unreal Engine 5, which is carefully curated to follow real-world filming characteristics, covering diverse scenes and camera movements. It helps the model generalize to in-the-wild videos. Lastly, we further improve the robustness to diverse inputs through a meticulously designed training strategy. Extensive experiments tell that our method substantially outperforms existing state-of-the-art approaches and strong baselines. Our method also finds promising applications in video stabilization, super-resolution, and outpainting. Project page: https://jianhongbai.github.io/ReCamMaster/', 'score': 80, 'issue_id': 2730, 'pub_date': '2025-03-14', 'pub_date_card': {'ru': '14 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 14', 'zh': '3æœˆ14æ—¥'}, 'hash': '7e72838ea84ed904', 'authors': ['Jianhong Bai', 'Menghan Xia', 'Xiao Fu', 'Xintao Wang', 'Lianrui Mu', 'Jinwen Cao', 'Zuozhu Liu', 'Haoji Hu', 'Xiang Bai', 'Pengfei Wan', 'Di Zhang'], 'affiliations': ['CUHK', 'HUST', 'Kuaishou Technology', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.11647.jpg', 'data': {'categories': ['#dataset', '#optimization', '#training', '#video', '#games'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Ğ£Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ĞºĞ°Ğ¼ĞµÑ€Ğ¾Ğ¹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'ReCamMaster - ÑÑ‚Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ text-to-video Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ²Ğ¸Ğ´ĞµĞ¾-ĞºĞ¾Ğ½Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑÑ†ĞµĞ½Ñ‹ Ñ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ². Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ñ‹Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸ĞºĞ°Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Unreal Engine 5. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ¸ Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ² ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, ÑÑƒĞ¿ĞµÑ€Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ğ¸ Ğ°ÑƒÑ‚Ğ¿ĞµĞ¹Ğ½Ñ‚Ğ¸Ğ½Ğ³Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'ReCamMaster: Mastering Camera Control in Video Generation', 'desc': 'This paper introduces ReCamMaster, a novel framework for generating videos with controlled camera trajectories. It addresses the challenge of maintaining visual consistency and dynamic synchronization across multiple frames when altering camera paths. The framework leverages pre-trained text-to-video models and a specially curated multi-camera synchronized video dataset to enhance its performance. Experimental results demonstrate that ReCamMaster significantly outperforms existing methods, showcasing its potential for applications like video stabilization and super-resolution.'}, 'zh': {'title': 'é‡å¡‘è§†é¢‘åŠ¨æ€ï¼ŒæŒæ§ç›¸æœºè½¨è¿¹', 'desc': 'æœ¬è®ºæ–‡ç ”ç©¶äº†åœ¨æ–‡æœ¬æˆ–å›¾åƒæ¡ä»¶ä¸‹ç”Ÿæˆè§†é¢‘æ—¶çš„ç›¸æœºæ§åˆ¶é—®é¢˜ã€‚å°½ç®¡æ”¹å˜è§†é¢‘çš„ç›¸æœºè½¨è¿¹å¾ˆé‡è¦ï¼Œä½†è¿™ä¸€é¢†åŸŸçš„ç ”ç©¶ä»ç„¶è¾ƒå°‘ã€‚æˆ‘ä»¬æå‡ºäº†ReCamMasterï¼Œä¸€ä¸ªåŸºäºç”Ÿæˆæ¨¡å‹çš„è§†é¢‘é‡æ¸²æŸ“æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨æ–°çš„ç›¸æœºè½¨è¿¹ä¸‹é‡ç°è¾“å…¥è§†é¢‘çš„åŠ¨æ€åœºæ™¯ã€‚é€šè¿‡æ„å»ºä¸€ä¸ªå¤šç›¸æœºåŒæ­¥è§†é¢‘æ•°æ®é›†ï¼Œå¹¶é‡‡ç”¨ç²¾å¿ƒè®¾è®¡çš„è®­ç»ƒç­–ç•¥ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šç§è¾“å…¥ä¸‹è¡¨ç°å‡ºè‰²ï¼Œè¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æŠ€æœ¯ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.07677', 'title': 'PLADIS: Pushing the Limits of Attention in Diffusion Models at Inference\n  Time by Leveraging Sparsity', 'url': 'https://huggingface.co/papers/2503.07677', 'abstract': 'Diffusion models have shown impressive results in generating high-quality conditional samples using guidance techniques such as Classifier-Free Guidance (CFG). However, existing methods often require additional training or neural function evaluations (NFEs), making them incompatible with guidance-distilled models. Also, they rely on heuristic approaches that need identifying target layers. In this work, we propose a novel and efficient method, termed PLADIS, which boosts pre-trained models (U-Net/Transformer) by leveraging sparse attention. Specifically, we extrapolate query-key correlations using softmax and its sparse counterpart in the cross-attention layer during inference, without requiring extra training or NFEs. By leveraging the noise robustness of sparse attention, our PLADIS unleashes the latent potential of text-to-image diffusion models, enabling them to excel in areas where they once struggled with newfound effectiveness. It integrates seamlessly with guidance techniques, including guidance-distilled models. Extensive experiments show notable improvements in text alignment and human preference, offering a highly efficient and universally applicable solution.', 'score': 65, 'issue_id': 2730, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 10', 'zh': '3æœˆ10æ—¥'}, 'hash': '913b88ac595cc8b6', 'authors': ['Kwanyoung Kim', 'Byeongsu Sim'], 'affiliations': ['Samsung Research'], 'pdf_title_img': 'assets/pdf/title_img/2503.07677.jpg', 'data': {'categories': ['#training', '#cv', '#diffusion', '#inference'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'PLADIS: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ PLADIS Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ. PLADIS Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑĞºÑÑ‚Ñ€Ğ°Ğ¿Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ğ¹ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ-ĞºĞ»ÑÑ‡ Ğ² ÑĞ»Ğ¾Ğµ ĞºÑ€Ğ¾ÑÑ-Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°, Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒÑ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ»Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¾Ğº Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ÑÑ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ°Ğ¼Ğ¸ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Unlocking the Power of Diffusion Models with Sparse Attention', 'desc': "This paper introduces PLADIS, a new method that enhances pre-trained diffusion models like U-Net and Transformer by using sparse attention techniques. Unlike previous methods that needed extra training or evaluations, PLADIS operates efficiently during inference by utilizing query-key correlations in the cross-attention layer. This approach improves the models' performance in generating high-quality images from text prompts without the need for additional training. The results demonstrate significant advancements in text alignment and user preference, making PLADIS a versatile solution for various applications in text-to-image generation."}, 'zh': {'title': 'PLADISï¼šé«˜æ•ˆæå‡æ‰©æ•£æ¨¡å‹çš„ç¨€ç–æ³¨æ„åŠ›æ–¹æ³•', 'desc': 'æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆé«˜è´¨é‡æ¡ä»¶æ ·æœ¬æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶æ˜¯ä½¿ç”¨æ— åˆ†ç±»å™¨å¼•å¯¼ï¼ˆCFGï¼‰ç­‰æŠ€æœ¯ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦é¢å¤–çš„è®­ç»ƒæˆ–ç¥ç»åŠŸèƒ½è¯„ä¼°ï¼ˆNFEï¼‰ï¼Œè¿™ä½¿å¾—å®ƒä»¬ä¸å¼•å¯¼è’¸é¦æ¨¡å‹ä¸å…¼å®¹ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–é«˜æ•ˆçš„æ–¹æ³•ï¼Œç§°ä¸ºPLADISï¼Œé€šè¿‡åˆ©ç”¨ç¨€ç–æ³¨æ„åŠ›æ¥å¢å¼ºé¢„è®­ç»ƒæ¨¡å‹ï¼ˆå¦‚U-Net/Transformerï¼‰ã€‚PLADISåœ¨æ¨ç†è¿‡ç¨‹ä¸­åˆ©ç”¨äº¤å‰æ³¨æ„åŠ›å±‚ä¸­çš„softmaxå’Œç¨€ç–å¯¹åº”ç‰©ï¼Œæå‡æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹çš„æ½œåŠ›ï¼Œæ˜¾è‘—æ”¹å–„æ–‡æœ¬å¯¹é½å’Œäººç±»åå¥½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.11646', 'title': 'Adversarial Data Collection: Human-Collaborative Perturbations for\n  Efficient and Robust Robotic Imitation Learning', 'url': 'https://huggingface.co/papers/2503.11646', 'abstract': 'The pursuit of data efficiency, where quality outweighs quantity, has emerged as a cornerstone in robotic manipulation, especially given the high costs associated with real-world data collection. We propose that maximizing the informational density of individual demonstrations can dramatically reduce reliance on large-scale datasets while improving task performance. To this end, we introduce Adversarial Data Collection, a Human-in-the-Loop (HiL) framework that redefines robotic data acquisition through real-time, bidirectional human-environment interactions. Unlike conventional pipelines that passively record static demonstrations, ADC adopts a collaborative perturbation paradigm: during a single episode, an adversarial operator dynamically alters object states, environmental conditions, and linguistic commands, while the tele-operator adaptively adjusts actions to overcome these evolving challenges. This process compresses diverse failure-recovery behaviors, compositional task variations, and environmental perturbations into minimal demonstrations. Our experiments demonstrate that ADC-trained models achieve superior compositional generalization to unseen task instructions, enhanced robustness to perceptual perturbations, and emergent error recovery capabilities. Strikingly, models trained with merely 20% of the demonstration volume collected through ADC significantly outperform traditional approaches using full datasets. These advances bridge the gap between data-centric learning paradigms and practical robotic deployment, demonstrating that strategic data acquisition, not merely post-hoc processing, is critical for scalable, real-world robot learning. Additionally, we are curating a large-scale ADC-Robotics dataset comprising real-world manipulation tasks with adversarial perturbations. This benchmark will be open-sourced to facilitate advancements in robotic imitation learning.', 'score': 31, 'issue_id': 2731, 'pub_date': '2025-03-14', 'pub_date_card': {'ru': '14 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 14', 'zh': '3æœˆ14æ—¥'}, 'hash': 'efdf1296bc567414', 'authors': ['Siyuan Huang', 'Yue Liao', 'Siyuan Feng', 'Shu Jiang', 'Si Liu', 'Hongsheng Li', 'Maoqing Yao', 'Guanghui Ren'], 'affiliations': ['Agibot', 'Beihang University', 'MMLab, CUHK', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2503.11646.jpg', 'data': {'categories': ['#robotics', '#benchmark', '#dataset', '#optimization', '#open_source', '#agents', '#training', '#data'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞœĞµĞ½ÑŒÑˆĞµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ±Ğ¾Ğ»ÑŒÑˆĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ±Ğ¾Ñ€Ñƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ - Adversarial Data Collection (ADC). ADC Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°-Ğ¾Ğ¿ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ° Ğ¸ ÑÑ€ĞµĞ´Ñ‹ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾ Ğ½Ğ°ÑÑ‹Ñ‰ĞµĞ½Ğ½Ñ‹Ñ… Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° ADC-Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğº Ğ²Ğ¾Ğ·Ğ¼ÑƒÑ‰ĞµĞ½Ğ¸ÑĞ¼, Ñ‡ĞµĞ¼ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ¾Ğ·Ğ´Ğ°ÑÑ‚ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ADC-Robotics Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¸Ğ¼Ğ¸Ñ‚Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Maximizing Data Efficiency in Robotic Learning with Adversarial Collection', 'desc': 'This paper introduces a new method called Adversarial Data Collection (ADC) to improve robotic manipulation by focusing on data efficiency. Instead of relying on large datasets, ADC enhances the quality of data through real-time interactions between humans and robots, allowing for dynamic adjustments during demonstrations. By using an adversarial approach, the framework captures a wide range of behaviors and challenges in fewer demonstrations, leading to better performance in unseen tasks. The results show that models trained with ADC can generalize better and recover from errors more effectively, even with significantly less data than traditional methods.'}, 'zh': {'title': 'å¯¹æŠ—æ€§æ•°æ®æ”¶é›†ï¼šæå‡æœºå™¨äººå­¦ä¹ æ•ˆç‡çš„å…³é”®', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ•°æ®æ”¶é›†æ–¹æ³•ï¼Œç§°ä¸ºå¯¹æŠ—æ€§æ•°æ®æ”¶é›†ï¼ˆADCï¼‰ï¼Œæ—¨åœ¨æé«˜æœºå™¨äººæ“ä½œçš„æ•ˆç‡ã€‚é€šè¿‡å®æ—¶çš„äººæœºäº¤äº’ï¼ŒADCèƒ½å¤Ÿåœ¨åŠ¨æ€ç¯å¢ƒä¸­æ”¶é›†é«˜ä¿¡æ¯å¯†åº¦çš„æ¼”ç¤ºæ•°æ®ï¼Œä»è€Œå‡å°‘å¯¹å¤§è§„æ¨¡æ•°æ®é›†çš„ä¾èµ–ã€‚å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨ADCè®­ç»ƒçš„æ¨¡å‹åœ¨é¢å¯¹æœªè§ä»»åŠ¡æŒ‡ä»¤æ—¶è¡¨ç°å‡ºæ›´å¥½çš„ç»„åˆæ³›åŒ–èƒ½åŠ›å’Œå¯¹ç¯å¢ƒå¹²æ‰°çš„é²æ£’æ€§ã€‚æœ€ç»ˆï¼ŒADCæ–¹æ³•æ˜¾è‘—æé«˜äº†æœºå™¨äººå­¦ä¹ çš„å®ç”¨æ€§ï¼Œå±•ç¤ºäº†æˆ˜ç•¥æ€§æ•°æ®è·å–çš„é‡è¦æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.11224', 'title': 'Technologies on Effectiveness and Efficiency: A Survey of State Spaces\n  Models', 'url': 'https://huggingface.co/papers/2503.11224', 'abstract': 'State Space Models (SSMs) have emerged as a promising alternative to the popular transformer-based models and have been increasingly gaining attention. Compared to transformers, SSMs excel at tasks with sequential data or longer contexts, demonstrating comparable performances with significant efficiency gains. In this survey, we provide a coherent and systematic overview for SSMs, including their theoretical motivations, mathematical formulations, comparison with existing model classes, and various applications. We divide the SSM series into three main sections, providing a detailed introduction to the original SSM, the structured SSM represented by S4, and the selective SSM typified by Mamba. We put an emphasis on technicality, and highlight the various key techniques introduced to address the effectiveness and efficiency of SSMs. We hope this manuscript serves as an introduction for researchers to explore the theoretical foundations of SSMs.', 'score': 21, 'issue_id': 2731, 'pub_date': '2025-03-14', 'pub_date_card': {'ru': '14 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 14', 'zh': '3æœˆ14æ—¥'}, 'hash': 'fb4219d497e59f64', 'authors': ['Xingtai Lv', 'Youbang Sun', 'Kaiyan Zhang', 'Shang Qu', 'Xuekai Zhu', 'Yuchen Fan', 'Yi Wu', 'Ermo Hua', 'Xinwei Long', 'Ning Ding', 'Bowen Zhou'], 'affiliations': ['Department of Electronic Engineering, Tsinghua University', 'Robotics Institute, Carnegie Mellon University', 'Shanghai Artificial Intelligence Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2503.11224.jpg', 'data': {'categories': ['#architecture', '#long_context', '#survey', '#math', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'SSM: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ° Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°Ğ¼ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹', 'desc': 'Ğ­Ñ‚Ğ¾ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ (SSM) ĞºĞ°Ğº Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ñ‹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°Ğ¼ Ğ² Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸. SSM Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ñ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°Ğ¼Ğ¸, Ğ½Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼. Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾ÑĞ½Ğ¾Ğ²Ñ‹, Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ SSM. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹Ğ´ĞµĞ»ÑÑÑ‚ Ñ‚Ñ€Ğ¸ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ° SSM: Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ, ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ (Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, S4) Ğ¸ ÑĞµĞ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ (Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, Mamba).'}, 'en': {'title': 'Unlocking Efficiency: The Power of State Space Models', 'desc': 'State Space Models (SSMs) are gaining popularity as an efficient alternative to transformer models, especially for sequential data and longer contexts. This paper provides a comprehensive overview of SSMs, detailing their theoretical foundations, mathematical formulations, and comparisons with other model types. It categorizes SSMs into three sections: the original SSM, the structured S4 model, and the selective Mamba model, emphasizing their unique techniques for improved performance. The goal is to guide researchers in understanding and exploring the potential of SSMs in various applications.'}, 'zh': {'title': 'çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼šé«˜æ•ˆå¤„ç†åºåˆ—æ•°æ®çš„æ–°é€‰æ‹©', 'desc': 'çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMsï¼‰ä½œä¸ºä¸€ç§æœ‰å‰æ™¯çš„æ›¿ä»£æ–¹æ¡ˆï¼Œé€æ¸å—åˆ°å…³æ³¨ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†åºåˆ—æ•°æ®æˆ–é•¿ä¸Šä¸‹æ–‡ä»»åŠ¡æ—¶è¡¨ç°ä¼˜å¼‚ã€‚ä¸æµè¡Œçš„å˜æ¢å™¨æ¨¡å‹ç›¸æ¯”ï¼ŒSSMsåœ¨æ•ˆç‡ä¸Šæœ‰æ˜¾è‘—æå‡ï¼ŒåŒæ—¶åœ¨æ€§èƒ½ä¸Šä¹Ÿèƒ½ä¸ä¹‹åª²ç¾ã€‚æœ¬æ–‡å¯¹SSMsè¿›è¡Œäº†ç³»ç»Ÿçš„æ¦‚è¿°ï¼ŒåŒ…æ‹¬å…¶ç†è®ºåŠ¨æœºã€æ•°å­¦å…¬å¼ã€ä¸ç°æœ‰æ¨¡å‹çš„æ¯”è¾ƒä»¥åŠå„ç§åº”ç”¨ã€‚æˆ‘ä»¬å°†SSMç³»åˆ—åˆ†ä¸ºä¸‰ä¸ªä¸»è¦éƒ¨åˆ†ï¼Œè¯¦ç»†ä»‹ç»äº†åŸå§‹SSMã€ç»“æ„åŒ–SSMï¼ˆå¦‚S4ï¼‰å’Œé€‰æ‹©æ€§SSMï¼ˆå¦‚Mambaï¼‰ï¼Œå¹¶å¼ºè°ƒäº†æé«˜SSMæœ‰æ•ˆæ€§å’Œæ•ˆç‡çš„å…³é”®æŠ€æœ¯ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.11069', 'title': 'API Agents vs. GUI Agents: Divergence and Convergence', 'url': 'https://huggingface.co/papers/2503.11069', 'abstract': 'Large language models (LLMs) have evolved beyond simple text generation to power software agents that directly translate natural language commands into tangible actions. While API-based LLM agents initially rose to prominence for their robust automation capabilities and seamless integration with programmatic endpoints, recent progress in multimodal LLM research has enabled GUI-based LLM agents that interact with graphical user interfaces in a human-like manner. Although these two paradigms share the goal of enabling LLM-driven task automation, they diverge significantly in architectural complexity, development workflows, and user interaction models.   This paper presents the first comprehensive comparative study of API-based and GUI-based LLM agents, systematically analyzing their divergence and potential convergence. We examine key dimensions and highlight scenarios in which hybrid approaches can harness their complementary strengths. By proposing clear decision criteria and illustrating practical use cases, we aim to guide practitioners and researchers in selecting, combining, or transitioning between these paradigms. Ultimately, we indicate that continuing innovations in LLM-based automation are poised to blur the lines between API- and GUI-driven agents, paving the way for more flexible, adaptive solutions in a wide range of real-world applications.', 'score': 20, 'issue_id': 2730, 'pub_date': '2025-03-14', 'pub_date_card': {'ru': '14 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 14', 'zh': '3æœˆ14æ—¥'}, 'hash': '29e714954ed20978', 'authors': ['Chaoyun Zhang', 'Shilin He', 'Liqun Li', 'Si Qin', 'Yu Kang', 'Qingwei Lin', 'Dongmei Zhang'], 'affiliations': ['Microsoft'], 'pdf_title_img': 'assets/pdf/title_img/2503.11069.jpg', 'data': {'categories': ['#multimodal', '#survey', '#agents'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ¡Ñ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ API Ğ¸ GUI Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLM: Ğ¿ÑƒÑ‚ÑŒ Ğº Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¼ Ñ€ĞµÑˆĞµĞ½Ğ¸ÑĞ¼', 'desc': 'Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM), Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‰Ğ¸Ñ… Ñ‡ĞµÑ€ĞµĞ· API Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ² Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¼ Ğ´Ğ»Ñ Ğ¾Ğ±Ğ¾Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ². ĞĞ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¸ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ÑÑ‚ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸, Ğ³Ğ´Ğµ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ±Ñ‹Ñ‚ÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLM ÑÑ‚Ğ¸Ñ€Ğ°ÑÑ‚ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑÑ‚Ğ¸Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Bridging the Gap: API and GUI LLM Agents Unite', 'desc': 'This paper explores the evolution of large language models (LLMs) from simple text generators to sophisticated software agents that can perform tasks based on natural language commands. It compares two main types of LLM agents: API-based agents, which automate tasks through programmatic interfaces, and GUI-based agents, which interact with graphical user interfaces like humans. The study highlights the differences in their architecture, development processes, and user interactions, while also discussing how hybrid approaches can leverage the strengths of both paradigms. The authors provide decision criteria and practical examples to help users choose the right approach for their needs, suggesting that future advancements will further integrate these two types of agents.'}, 'zh': {'title': 'APIä¸GUIä»£ç†çš„æ¯”è¾ƒä¸èåˆä¹‹è·¯', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å·²ç»ä»ç®€å•çš„æ–‡æœ¬ç”Ÿæˆå‘å±•åˆ°èƒ½å¤Ÿå°†è‡ªç„¶è¯­è¨€å‘½ä»¤ç›´æ¥è½¬åŒ–ä¸ºå®é™…æ“ä½œçš„è½¯ä»¶ä»£ç†ã€‚æœ¬æ–‡é¦–æ¬¡å…¨é¢æ¯”è¾ƒäº†åŸºäºAPIçš„LLMä»£ç†å’ŒåŸºäºGUIçš„LLMä»£ç†ï¼Œåˆ†æäº†å®ƒä»¬åœ¨æ¶æ„å¤æ‚æ€§ã€å¼€å‘å·¥ä½œæµç¨‹å’Œç”¨æˆ·äº¤äº’æ¨¡å‹ä¸Šçš„æ˜¾è‘—å·®å¼‚ã€‚æˆ‘ä»¬æ¢è®¨äº†å…³é”®ç»´åº¦ï¼Œå¹¶å¼ºè°ƒäº†æ··åˆæ–¹æ³•åœ¨åˆ©ç”¨ä¸¤è€…äº’è¡¥ä¼˜åŠ¿æ–¹é¢çš„åœºæ™¯ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬æŒ‡å‡ºLLMé©±åŠ¨çš„è‡ªåŠ¨åŒ–åˆ›æ–°å°†æ¨¡ç³ŠAPIå’ŒGUIä»£ç†ä¹‹é—´çš„ç•Œé™ï¼Œä¸ºå„ç§å®é™…åº”ç”¨æä¾›æ›´çµæ´»ã€é€‚åº”æ€§å¼ºçš„è§£å†³æ–¹æ¡ˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.11514', 'title': 'Exploring the Vulnerabilities of Federated Learning: A Deep Dive into\n  Gradient Inversion Attacks', 'url': 'https://huggingface.co/papers/2503.11514', 'abstract': 'Federated Learning (FL) has emerged as a promising privacy-preserving collaborative model training paradigm without sharing raw data. However, recent studies have revealed that private information can still be leaked through shared gradient information and attacked by Gradient Inversion Attacks (GIA). While many GIA methods have been proposed, a detailed analysis, evaluation, and summary of these methods are still lacking. Although various survey papers summarize existing privacy attacks in FL, few studies have conducted extensive experiments to unveil the effectiveness of GIA and their associated limiting factors in this context. To fill this gap, we first undertake a systematic review of GIA and categorize existing methods into three types, i.e., optimization-based GIA (OP-GIA), generation-based GIA (GEN-GIA), and analytics-based GIA (ANA-GIA). Then, we comprehensively analyze and evaluate the three types of GIA in FL, providing insights into the factors that influence their performance, practicality, and potential threats. Our findings indicate that OP-GIA is the most practical attack setting despite its unsatisfactory performance, while GEN-GIA has many dependencies and ANA-GIA is easily detectable, making them both impractical. Finally, we offer a three-stage defense pipeline to users when designing FL frameworks and protocols for better privacy protection and share some future research directions from the perspectives of attackers and defenders that we believe should be pursued. We hope that our study can help researchers design more robust FL frameworks to defend against these attacks.', 'score': 13, 'issue_id': 2730, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 13', 'zh': '3æœˆ13æ—¥'}, 'hash': 'd31bf6f9bd4bc86b', 'authors': ['Pengxin Guo', 'Runxi Wang', 'Shuang Zeng', 'Jinjing Zhu', 'Haoning Jiang', 'Yanran Wang', 'Yuyin Zhou', 'Feifei Wang', 'Hui Xiong', 'Liangqiong Qu'], 'affiliations': ['Department of Biomedical Data Science, Stanford University, Stanford, CA 94305, USA', 'Department of Computer Science and Engineering, University of California, Santa Cruz, CA 95064, USA', 'Department of Electrical and Electronic Engineering, The University of Hong Kong, Hong Kong 999077, China', 'Department of Electronic and Electrical Engineering, Southern University of Science and Technology, Shenzhen 518055, China', 'Department of Mathematics, The University of Hong Kong, Hong Kong 999077, China', 'Materials Innovation Institute for Life Sciences and Energy (MILES), HKU-SIRI, Shenzhen 518055, China', 'School of Computing and Data Science, The University of Hong Kong, Hong Kong 999077, China', 'Thrust of Artificial Intelligence, The Hong Kong University of Science and Technology (Guangzhou), Guangzhou 511458, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.11514.jpg', 'data': {'categories': ['#leakage', '#benchmark', '#security', '#survey', '#healthcare', '#data'], 'emoji': 'ğŸ›¡ï¸', 'ru': {'title': 'Ğ—Ğ°Ñ‰Ğ¸Ñ‚Ğ° Ğ¿Ñ€Ğ¸Ğ²Ğ°Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ñ„ĞµĞ´ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸: Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ°Ñ‚Ğ°Ğº Ñ Ğ¸Ğ½Ğ²ĞµÑ€ÑĞ¸ĞµĞ¹ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ Ğ°Ñ‚Ğ°Ğº Ñ Ğ¸Ğ½Ğ²ĞµÑ€ÑĞ¸ĞµĞ¹ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ° (GIA) Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ñ„ĞµĞ´ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ (FL). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒÑÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ GIA Ğ½Ğ° Ñ‚Ñ€Ğ¸ Ñ‚Ğ¸Ğ¿Ğ°: Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ. ĞŸÑ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ñ‚Ğ¸Ğ¿Ğ° Ğ°Ñ‚Ğ°Ğº Ğ² FL. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ GIA ÑĞ²Ğ»ÑÑÑ‚ÑÑ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸, Ğ½ĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ¸Ñ… Ğ½ĞµÑƒĞ´Ğ¾Ğ²Ğ»ĞµÑ‚Ğ²Ğ¾Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ.'}, 'en': {'title': 'Strengthening Privacy in Federated Learning Against Gradient Inversion Attacks', 'desc': "This paper focuses on the vulnerabilities of Federated Learning (FL) to Gradient Inversion Attacks (GIA), which can leak private information despite the model's privacy-preserving intentions. It categorizes existing GIA methods into three types: optimization-based, generation-based, and analytics-based, and provides a thorough analysis of their effectiveness and limitations. The study reveals that while optimization-based GIA is the most practical, it still has performance issues, whereas generation-based and analytics-based methods are less practical due to their dependencies and detectability. The authors propose a defense strategy to enhance privacy in FL frameworks and suggest future research directions to strengthen defenses against these attacks."}, 'zh': {'title': 'æå‡è”é‚¦å­¦ä¹ éšç§ä¿æŠ¤çš„é˜²å¾¡ç­–ç•¥', 'desc': 'è”é‚¦å­¦ä¹ ï¼ˆFLï¼‰æ˜¯ä¸€ç§ä¿æŠ¤éšç§çš„åä½œæ¨¡å‹è®­ç»ƒæ–¹æ³•ï¼Œä¸éœ€è¦å…±äº«åŸå§‹æ•°æ®ã€‚ç„¶è€Œï¼Œæœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡å…±äº«æ¢¯åº¦ä¿¡æ¯ï¼Œç§å¯†ä¿¡æ¯ä»ç„¶å¯èƒ½è¢«æ³„éœ²ï¼Œå¹¶å—åˆ°æ¢¯åº¦åæ¼”æ”»å‡»ï¼ˆGIAï¼‰çš„å¨èƒã€‚æœ¬æ–‡å¯¹ç°æœ‰çš„GIAæ–¹æ³•è¿›è¡Œäº†ç³»ç»Ÿçš„å›é¡¾å’Œåˆ†ç±»ï¼Œå¹¶åˆ†æäº†ä¸‰ç§ç±»å‹çš„GIAåœ¨FLä¸­çš„è¡¨ç°å’Œå±€é™æ€§ã€‚æœ€åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªä¸‰é˜¶æ®µçš„é˜²å¾¡æ–¹æ¡ˆï¼Œä»¥å¸®åŠ©ç”¨æˆ·åœ¨è®¾è®¡FLæ¡†æ¶æ—¶æ›´å¥½åœ°ä¿æŠ¤éšç§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.10772', 'title': 'FlowTok: Flowing Seamlessly Across Text and Image Tokens', 'url': 'https://huggingface.co/papers/2503.10772', 'abstract': 'Bridging different modalities lies at the heart of cross-modality generation. While conventional approaches treat the text modality as a conditioning signal that gradually guides the denoising process from Gaussian noise to the target image modality, we explore a much simpler paradigm-directly evolving between text and image modalities through flow matching. This requires projecting both modalities into a shared latent space, which poses a significant challenge due to their inherently different representations: text is highly semantic and encoded as 1D tokens, whereas images are spatially redundant and represented as 2D latent embeddings. To address this, we introduce FlowTok, a minimal framework that seamlessly flows across text and images by encoding images into a compact 1D token representation. Compared to prior methods, this design reduces the latent space size by 3.3x at an image resolution of 256, eliminating the need for complex conditioning mechanisms or noise scheduling. Moreover, FlowTok naturally extends to image-to-text generation under the same formulation. With its streamlined architecture centered around compact 1D tokens, FlowTok is highly memory-efficient, requires significantly fewer training resources, and achieves much faster sampling speeds-all while delivering performance comparable to state-of-the-art models. Code will be available at https://github.com/bytedance/1d-tokenizer.', 'score': 12, 'issue_id': 2731, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 13', 'zh': '3æœˆ13æ—¥'}, 'hash': '548255900cd1ec21', 'authors': ['Ju He', 'Qihang Yu', 'Qihao Liu', 'Liang-Chieh Chen'], 'affiliations': ['ByteDance Seed', 'Johns Hopkins University'], 'pdf_title_img': 'assets/pdf/title_img/2503.10772.jpg', 'data': {'categories': ['#architecture', '#training', '#multimodal', '#diffusion'], 'emoji': 'ğŸŒŠ', 'ru': {'title': 'FlowTok: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ĞµĞ¼ Ñ‡ĞµÑ€ĞµĞ· Ñ‚Ğ¾ĞºĞµĞ½Ñ‹', 'desc': 'FlowTok - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ½Ğ°Ğ¾Ğ±Ğ¾Ñ€Ğ¾Ñ‚. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ¾Ğ±Ñ‰ĞµĞ¼ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾Ğµ Ğ¾Ğ´Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒĞ¿Ñ€Ğ¾ÑÑ‚Ğ¸Ñ‚ÑŒ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ, ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ ÑƒÑĞºĞ¾Ñ€Ğ¸Ñ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'FlowTok: Simplifying Cross-Modality Generation with 1D Tokens', 'desc': 'This paper presents FlowTok, a novel framework for cross-modality generation that simplifies the process of transitioning between text and image modalities. Unlike traditional methods that rely on complex conditioning signals and denoising processes, FlowTok directly evolves between text and images by utilizing flow matching in a shared latent space. The framework encodes images into a compact 1D token representation, significantly reducing the latent space size and improving efficiency. FlowTok not only enhances memory usage and training resource requirements but also maintains competitive performance in generating images from text and vice versa.'}, 'zh': {'title': 'FlowTokï¼šç®€åŒ–è·¨æ¨¡æ€ç”Ÿæˆçš„é«˜æ•ˆæ¡†æ¶', 'desc': 'è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†è·¨æ¨¡æ€ç”Ÿæˆä¸­çš„ä¸åŒæ¨¡æ€ä¹‹é—´çš„æ¡¥æ¥é—®é¢˜ã€‚ä¼ ç»Ÿæ–¹æ³•å°†æ–‡æœ¬æ¨¡æ€è§†ä¸ºå¼•å¯¼ä¿¡å·ï¼Œé€æ­¥å¼•å¯¼å»å™ªè¿‡ç¨‹ï¼Œè€Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ›´ç®€å•çš„æ–¹æ³•ï¼Œé€šè¿‡æµåŒ¹é…ç›´æ¥åœ¨æ–‡æœ¬å’Œå›¾åƒæ¨¡æ€ä¹‹é—´æ¼”å˜ã€‚æˆ‘ä»¬å¼•å…¥äº†FlowTokæ¡†æ¶ï¼Œå°†å›¾åƒç¼–ç ä¸ºç´§å‡‘çš„1Dæ ‡è®°è¡¨ç¤ºï¼Œä»è€Œåœ¨å…±äº«æ½œåœ¨ç©ºé—´ä¸­æµåŠ¨ï¼Œæ˜¾è‘—å‡å°‘äº†æ½œåœ¨ç©ºé—´çš„å¤§å°ã€‚FlowTokä¸ä»…æé«˜äº†å†…å­˜æ•ˆç‡å’Œé‡‡æ ·é€Ÿåº¦ï¼Œè¿˜åœ¨å›¾åƒåˆ°æ–‡æœ¬ç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œæ€§èƒ½ä¸æœ€å…ˆè¿›çš„æ¨¡å‹ç›¸å½“ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.11576', 'title': 'SmolDocling: An ultra-compact vision-language model for end-to-end\n  multi-modal document conversion', 'url': 'https://huggingface.co/papers/2503.11576', 'abstract': 'We introduce SmolDocling, an ultra-compact vision-language model targeting end-to-end document conversion. Our model comprehensively processes entire pages by generating DocTags, a new universal markup format that captures all page elements in their full context with location. Unlike existing approaches that rely on large foundational models, or ensemble solutions that rely on handcrafted pipelines of multiple specialized models, SmolDocling offers an end-to-end conversion for accurately capturing content, structure and spatial location of document elements in a 256M parameters vision-language model. SmolDocling exhibits robust performance in correctly reproducing document features such as code listings, tables, equations, charts, lists, and more across a diverse range of document types including business documents, academic papers, technical reports, patents, and forms -- significantly extending beyond the commonly observed focus on scientific papers. Additionally, we contribute novel publicly sourced datasets for charts, tables, equations, and code recognition. Experimental results demonstrate that SmolDocling competes with other Vision Language Models that are up to 27 times larger in size, while reducing computational requirements substantially. The model is currently available, datasets will be publicly available soon.', 'score': 11, 'issue_id': 2744, 'pub_date': '2025-03-14', 'pub_date_card': {'ru': '14 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 14', 'zh': '3æœˆ14æ—¥'}, 'hash': '5548a7c6526f8753', 'authors': ['Ahmed Nassar', 'Andres Marafioti', 'Matteo Omenetti', 'Maksym Lysak', 'Nikolaos Livathinos', 'Christoph Auer', 'Lucas Morin', 'Rafael Teixeira de Lima', 'Yusik Kim', 'A. Said Gurbuz', 'Michele Dolfi', 'Miquel FarrÃ©', 'Peter W. J. Staar'], 'affiliations': ['HuggingFace', 'IBM Research'], 'pdf_title_img': 'assets/pdf/title_img/2503.11576.jpg', 'data': {'categories': ['#small_models', '#open_source', '#cv', '#dataset', '#science', '#multimodal'], 'emoji': 'ğŸ“„', 'ru': {'title': 'SmolDocling: ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'SmolDocling - ÑÑ‚Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰Ğ°Ñ Ğ·Ñ€ĞµĞ½Ğ¸Ğµ Ğ¸ ÑĞ·Ñ‹Ğº. ĞĞ½Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½ÑƒÑ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºÑƒ DocTags, Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰ÑƒÑ Ğ²ÑĞµ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ñ‹ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ñ Ğ¸Ñ… Ñ€Ğ°ÑĞ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ĞµĞ¼. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ»Ğ¸ÑÑ‚Ğ¸Ğ½Ğ³Ğ¸ ĞºĞ¾Ğ´Ğ°, Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹, ÑƒÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ğ¸ Ğ´Ğ¸Ğ°Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñ‹. SmolDocling ĞºĞ¾Ğ½ĞºÑƒÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ² 27 Ñ€Ğ°Ğ· Ğ±Ğ¾Ğ»ÑŒÑˆĞµĞ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°, ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'SmolDocling: Compact and Powerful Document Conversion', 'desc': 'SmolDocling is a compact vision-language model designed for end-to-end document conversion. It generates DocTags, a universal markup format that captures the content, structure, and spatial location of document elements. Unlike traditional methods that use large models or complex pipelines, SmolDocling achieves high accuracy with only 256 million parameters. It performs well across various document types, including business and academic papers, and introduces new datasets for recognizing charts, tables, equations, and code.'}, 'zh': {'title': 'SmolDoclingï¼šé«˜æ•ˆæ–‡æ¡£è½¬æ¢çš„æ–°é€‰æ‹©', 'desc': 'æˆ‘ä»¬ä»‹ç»äº†SmolDoclingï¼Œè¿™æ˜¯ä¸€ç§è¶…ç´§å‡‘çš„è§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨å®ç°ç«¯åˆ°ç«¯çš„æ–‡æ¡£è½¬æ¢ã€‚è¯¥æ¨¡å‹é€šè¿‡ç”ŸæˆDocTagsï¼Œä¸€ç§æ–°çš„é€šç”¨æ ‡è®°æ ¼å¼ï¼Œå…¨é¢å¤„ç†æ•´ä¸ªé¡µé¢ï¼Œæ•æ‰æ‰€æœ‰é¡µé¢å…ƒç´ çš„å®Œæ•´ä¸Šä¸‹æ–‡å’Œä½ç½®ã€‚ä¸ä¾èµ–å¤§å‹åŸºç¡€æ¨¡å‹æˆ–å¤šä¸ªä¸“ç”¨æ¨¡å‹çš„æ‰‹å·¥ç®¡é“çš„ç°æœ‰æ–¹æ³•ä¸åŒï¼ŒSmolDoclingæä¾›äº†ä¸€ç§ç«¯åˆ°ç«¯çš„è½¬æ¢ï¼Œå‡†ç¡®æ•æ‰æ–‡æ¡£å…ƒç´ çš„å†…å®¹ã€ç»“æ„å’Œç©ºé—´ä½ç½®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSmolDoclingåœ¨æ€§èƒ½ä¸Šä¸å…¶ä»–é«˜è¾¾27å€å¤§å°çš„è§†è§‰è¯­è¨€æ¨¡å‹ç«äº‰ï¼ŒåŒæ—¶æ˜¾è‘—é™ä½äº†è®¡ç®—éœ€æ±‚ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.10970', 'title': 'TxAgent: An AI Agent for Therapeutic Reasoning Across a Universe of\n  Tools', 'url': 'https://huggingface.co/papers/2503.10970', 'abstract': 'Precision therapeutics require multimodal adaptive models that generate personalized treatment recommendations. We introduce TxAgent, an AI agent that leverages multi-step reasoning and real-time biomedical knowledge retrieval across a toolbox of 211 tools to analyze drug interactions, contraindications, and patient-specific treatment strategies. TxAgent evaluates how drugs interact at molecular, pharmacokinetic, and clinical levels, identifies contraindications based on patient comorbidities and concurrent medications, and tailors treatment strategies to individual patient characteristics. It retrieves and synthesizes evidence from multiple biomedical sources, assesses interactions between drugs and patient conditions, and refines treatment recommendations through iterative reasoning. It selects tools based on task objectives and executes structured function calls to solve therapeutic tasks that require clinical reasoning and cross-source validation. The ToolUniverse consolidates 211 tools from trusted sources, including all US FDA-approved drugs since 1939 and validated clinical insights from Open Targets. TxAgent outperforms leading LLMs, tool-use models, and reasoning agents across five new benchmarks: DrugPC, BrandPC, GenericPC, TreatmentPC, and DescriptionPC, covering 3,168 drug reasoning tasks and 456 personalized treatment scenarios. It achieves 92.1% accuracy in open-ended drug reasoning tasks, surpassing GPT-4o and outperforming DeepSeek-R1 (671B) in structured multi-step reasoning. TxAgent generalizes across drug name variants and descriptions. By integrating multi-step inference, real-time knowledge grounding, and tool-assisted decision-making, TxAgent ensures that treatment recommendations align with established clinical guidelines and real-world evidence, reducing the risk of adverse events and improving therapeutic decision-making.', 'score': 10, 'issue_id': 2732, 'pub_date': '2025-03-14', 'pub_date_card': {'ru': '14 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 14', 'zh': '3æœˆ14æ—¥'}, 'hash': 'b7a03e6b34c3c0de', 'authors': ['Shanghua Gao', 'Richard Zhu', 'Zhenglun Kong', 'Ayush Noori', 'Xiaorui Su', 'Curtis Ginder', 'Theodoros Tsiligkaridis', 'Marinka Zitnik'], 'affiliations': ['Broad Institute of MIT and Harvard, Cambridge, MA', 'Cardiovascular Division, Department of Medicine, Brigham and Womens Hospital, Harvard Medical School, Boston, MA', 'Department of Biomedical Informatics, Harvard Medical School, Boston, MA', 'Harvard Data Science Initiative, Cambridge, MA', 'Kempner Institute for the Study of Natural and Artificial Intelligence, Harvard University, Cambridge, MA', 'MIT Lincoln Laboratory, Lexington, MA'], 'pdf_title_img': 'assets/pdf/title_img/2503.10970.jpg', 'data': {'categories': ['#alignment', '#healthcare', '#science', '#agents', '#reasoning', '#benchmark', '#multimodal'], 'emoji': 'ğŸ’Š', 'ru': {'title': 'TxAgent: Ğ˜Ğ˜-Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ½Ğ¸Ğº Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ñ„Ğ°Ñ€Ğ¼Ğ°ĞºĞ¾Ñ‚ĞµÑ€Ğ°Ğ¿Ğ¸Ğ¸', 'desc': 'TxAgent - ÑÑ‚Ğ¾ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ñ‚ĞµÑ€Ğ°Ğ¿Ğ¸Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ±Ğ¸Ğ¾Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. ĞĞ½ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ»ĞµĞºĞ°Ñ€ÑÑ‚Ğ², Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¸Ğ½Ğ´Ğ¸Ğ²Ğ¸Ğ´ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¸Ğ· 211 Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². TxAgent Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ² Ğ¿ÑÑ‚Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… 3168 Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¾ Ğ»ĞµĞºĞ°Ñ€ÑÑ‚Ğ²Ğ°Ñ… Ğ¸ 456 Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ² Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ. Ğ˜Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ²Ñ‹Ğ²Ğ¾Ğ´, Ğ°ĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹, TxAgent Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¹ ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ°Ğ¼ Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼.'}, 'en': {'title': 'TxAgent: Personalized Precision Therapeutics through Advanced AI Reasoning', 'desc': 'The paper presents TxAgent, an advanced AI agent designed for precision therapeutics that generates personalized treatment recommendations. It utilizes multi-step reasoning and real-time biomedical knowledge retrieval from a comprehensive toolbox of 211 tools to analyze drug interactions and contraindications. TxAgent evaluates drug interactions at various levels and tailors treatment strategies based on individual patient characteristics, ensuring recommendations are evidence-based. The system outperforms existing models in drug reasoning tasks, achieving high accuracy and improving therapeutic decision-making by integrating clinical guidelines and real-world evidence.'}, 'zh': {'title': 'ä¸ªæ€§åŒ–æ²»ç–—çš„æ™ºèƒ½åŠ©æ‰‹TxAgent', 'desc': 'ç²¾å‡†æ²»ç–—éœ€è¦å¤šæ¨¡æ€è‡ªé€‚åº”æ¨¡å‹æ¥ç”Ÿæˆä¸ªæ€§åŒ–çš„æ²»ç–—å»ºè®®ã€‚æˆ‘ä»¬ä»‹ç»äº†TxAgentï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨å¤šæ­¥æ¨ç†å’Œå®æ—¶ç”Ÿç‰©åŒ»å­¦çŸ¥è¯†æ£€ç´¢çš„äººå·¥æ™ºèƒ½ä»£ç†ï¼Œèƒ½å¤Ÿåˆ†æè¯ç‰©ç›¸äº’ä½œç”¨ã€ç¦å¿Œç—‡å’Œæ‚£è€…ç‰¹å®šçš„æ²»ç–—ç­–ç•¥ã€‚TxAgentåœ¨åˆ†å­ã€è¯ä»£åŠ¨åŠ›å­¦å’Œä¸´åºŠå±‚é¢è¯„ä¼°è¯ç‰©ç›¸äº’ä½œç”¨ï¼Œå¹¶æ ¹æ®æ‚£è€…çš„åˆå¹¶ç—‡å’ŒåŒæ—¶ç”¨è¯è¯†åˆ«ç¦å¿Œç—‡ï¼Œé‡èº«å®šåˆ¶æ²»ç–—ç­–ç•¥ã€‚é€šè¿‡æ•´åˆå¤šæ­¥æ¨ç†ã€å®æ—¶çŸ¥è¯†åŸºç¡€å’Œå·¥å…·è¾…åŠ©å†³ç­–ï¼ŒTxAgentç¡®ä¿æ²»ç–—å»ºè®®ç¬¦åˆæ—¢å®šçš„ä¸´åºŠæŒ‡å—å’Œç°å®ä¸–ç•Œè¯æ®ï¼Œä»è€Œé™ä½ä¸è‰¯äº‹ä»¶çš„é£é™©ï¼Œæ”¹å–„æ²»ç–—å†³ç­–ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.10781', 'title': 'Large-scale Pre-training for Grounded Video Caption Generation', 'url': 'https://huggingface.co/papers/2503.10781', 'abstract': 'We propose a novel approach for captioning and object grounding in video, where the objects in the caption are grounded in the video via temporally dense bounding boxes. We introduce the following contributions. First, we present a large-scale automatic annotation method that aggregates captions grounded with bounding boxes across individual frames into temporally dense and consistent bounding box annotations. We apply this approach on the HowTo100M dataset to construct a large-scale pre-training dataset, named HowToGround1M. We also introduce a Grounded Video Caption Generation model, dubbed GROVE, and pre-train the model on HowToGround1M. Second, we introduce a new dataset, called iGround, of 3500 videos with manually annotated captions and dense spatio-temporally grounded bounding boxes. This allows us to measure progress on this challenging problem, as well as to fine-tune our model on this small-scale but high-quality data. Third, we demonstrate that our approach achieves state-of-the-art results on the proposed iGround dataset compared to a number of baselines, as well as on the VidSTG and ActivityNet-Entities datasets. We perform extensive ablations that demonstrate the importance of pre-training using our automatically annotated HowToGround1M dataset followed by fine-tuning on the manually annotated iGround dataset and validate the key technical contributions of our model.', 'score': 10, 'issue_id': 2737, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 13', 'zh': '3æœˆ13æ—¥'}, 'hash': '842a44eb006952de', 'authors': ['Evangelos Kazakos', 'Cordelia Schmid', 'Josef Sivic'], 'affiliations': ['Czech Institute of Informatics, Robotics and Cybernetics at the Czech Technical University in Prague', 'Inria, Ecole normale superieure, CNRS, PSL Research University'], 'pdf_title_img': 'assets/pdf/title_img/2503.10781.jpg', 'data': {'categories': ['#cv', '#dataset', '#training', '#video', '#data'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾: Ğ¾Ñ‚ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸ Ğº Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞµĞ¹ Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ‚ĞµĞ¼Ğ¿Ğ¾Ñ€Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ñ€Ğ°Ğ¼Ğ¾Ğº. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ HowToGround1M Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ GROVE, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… iGround Ñ Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¾Ğ¹ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ VidSTG Ğ¸ ActivityNet-Entities. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¾Ğ¹ Ğ½Ğ° Ğ²Ñ€ÑƒÑ‡Ğ½ÑƒÑ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ.'}, 'en': {'title': 'Grounding Video Captions with Precision', 'desc': 'This paper presents a new method for video captioning and object grounding, where objects mentioned in captions are linked to specific locations in the video using detailed bounding boxes. The authors introduce a large-scale automatic annotation technique that creates consistent bounding box annotations across video frames, resulting in a dataset called HowToGround1M for pre-training. They also develop a model named GROVE, which is trained on this dataset and further fine-tuned on a smaller, high-quality dataset called iGround, containing manually annotated videos. The results show that their approach outperforms existing methods on several benchmark datasets, highlighting the effectiveness of their pre-training and fine-tuning strategy.'}, 'zh': {'title': 'è§†é¢‘å­—å¹•ç”Ÿæˆä¸ç‰©ä½“å®šä½çš„æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„è§†é¢‘å­—å¹•ç”Ÿæˆå’Œç‰©ä½“å®šä½æ–¹æ³•ï¼Œé€šè¿‡æ—¶é—´å¯†é›†çš„è¾¹ç•Œæ¡†å°†å­—å¹•ä¸­çš„ç‰©ä½“ä¸è§†é¢‘ä¸­çš„å†…å®¹å…³è”èµ·æ¥ã€‚æˆ‘ä»¬ä»‹ç»äº†ä¸€ç§å¤§è§„æ¨¡è‡ªåŠ¨æ³¨é‡Šæ–¹æ³•ï¼Œå°†å•å¸§çš„è¾¹ç•Œæ¡†æ³¨é‡Šèšåˆä¸ºæ—¶é—´ä¸Šå¯†é›†ä¸”ä¸€è‡´çš„è¾¹ç•Œæ¡†æ³¨é‡Šï¼Œå¹¶åœ¨HowTo100Mæ•°æ®é›†ä¸Šæ„å»ºäº†ä¸€ä¸ªåä¸ºHowToGround1Mçš„å¤§è§„æ¨¡é¢„è®­ç»ƒæ•°æ®é›†ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ä¸ªåä¸ºGROVEçš„åŸºäºè§†é¢‘çš„å­—å¹•ç”Ÿæˆæ¨¡å‹ï¼Œå¹¶åœ¨HowToGround1Mä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªåä¸ºiGroundçš„æ–°æ•°æ®é›†ï¼ŒåŒ…å«3500ä¸ªè§†é¢‘åŠå…¶æ‰‹åŠ¨æ³¨é‡Šçš„å­—å¹•å’Œå¯†é›†çš„æ—¶ç©ºè¾¹ç•Œæ¡†ï¼Œä»¥ä¾¿äºè¯„ä¼°æ¨¡å‹çš„è¿›å±•å’Œè¿›è¡Œå¾®è°ƒã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.11579', 'title': 'Vamba: Understanding Hour-Long Videos with Hybrid Mamba-Transformers', 'url': 'https://huggingface.co/papers/2503.11579', 'abstract': 'State-of-the-art transformer-based large multimodal models (LMMs) struggle to handle hour-long video inputs due to the quadratic complexity of the causal self-attention operations, leading to high computational costs during training and inference. Existing token compression-based methods reduce the number of video tokens but often incur information loss and remain inefficient for extremely long sequences. In this paper, we explore an orthogonal direction to build a hybrid Mamba-Transformer model (VAMBA) that employs Mamba-2 blocks to encode video tokens with linear complexity. Without any token reduction, VAMBA can encode more than 1024 frames (640times360) on a single GPU, while transformer-based models can only encode 256 frames. On long video input, VAMBA achieves at least 50% reduction in GPU memory usage during training and inference, and nearly doubles the speed per training step compared to transformer-based LMMs. Our experimental results demonstrate that VAMBA improves accuracy by 4.3% on the challenging hour-long video understanding benchmark LVBench over prior efficient video LMMs, and maintains strong performance on a broad spectrum of long and short video understanding tasks.', 'score': 9, 'issue_id': 2744, 'pub_date': '2025-03-14', 'pub_date_card': {'ru': '14 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 14', 'zh': '3æœˆ14æ—¥'}, 'hash': 'e0a1f364990dbe23', 'authors': ['Weiming Ren', 'Wentao Ma', 'Huan Yang', 'Cong Wei', 'Ge Zhang', 'Wenhu Chen'], 'affiliations': ['1.AI', 'M-A-P', 'University of Toronto', 'University of Waterloo', 'Vector Institute'], 'pdf_title_img': 'assets/pdf/title_img/2503.11579.jpg', 'data': {'categories': ['#optimization', '#long_context', '#benchmark', '#architecture', '#video'], 'emoji': 'ğŸ¥', 'ru': {'title': 'VAMBA: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ VAMBA, Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´ Mamba Ğ¸ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°, Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾. VAMBA Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ±Ğ»Ğ¾ĞºĞ¸ Mamba-2 Ğ´Ğ»Ñ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ 1024 ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¼ GPU. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ GPU Ğ½Ğ° 50% Ğ¸ Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸ Ğ²Ğ´Ğ²Ğ¾Ğµ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. VAMBA Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 4.3% Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ LVBench Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ‡Ğ°ÑĞ¾Ğ²Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'VAMBA: Efficient Video Processing with Linear Complexity', 'desc': 'This paper introduces the Mamba-Transformer model (VAMBA), which addresses the limitations of existing transformer-based large multimodal models (LMMs) in processing long video inputs. Unlike traditional methods that reduce video tokens and often lose information, VAMBA uses Mamba-2 blocks to encode video tokens with linear complexity, allowing it to handle over 1024 frames efficiently. The model significantly reduces GPU memory usage by at least 50% during training and inference, while also increasing training speed nearly twofold compared to standard transformers. Experimental results show that VAMBA not only enhances accuracy on the LVBench benchmark but also performs well across various video understanding tasks.'}, 'zh': {'title': 'é«˜æ•ˆè§†é¢‘ç†è§£çš„æ–°çªç ´ï¼šVAMBAæ¨¡å‹', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ··åˆMamba-Transformeræ¨¡å‹ï¼ˆVAMBAï¼‰ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰å¤šæ¨¡æ€æ¨¡å‹åœ¨å¤„ç†é•¿è§†é¢‘è¾“å…¥æ—¶çš„è®¡ç®—å¤æ‚æ€§é—®é¢˜ã€‚VAMBAä½¿ç”¨Mamba-2æ¨¡å—ä»¥çº¿æ€§å¤æ‚åº¦ç¼–ç è§†é¢‘æ ‡è®°ï¼Œé¿å…äº†ä¿¡æ¯æŸå¤±ï¼Œå¹¶ä¸”æ— éœ€å‡å°‘æ ‡è®°æ•°é‡ã€‚ä¸ä¼ ç»Ÿçš„å˜æ¢å™¨æ¨¡å‹ç›¸æ¯”ï¼ŒVAMBAåœ¨å•ä¸ªGPUä¸Šèƒ½å¤Ÿç¼–ç è¶…è¿‡1024å¸§çš„è§†é¢‘ï¼Œæ˜¾è‘—æé«˜äº†è®­ç»ƒå’Œæ¨ç†çš„é€Ÿåº¦ï¼Œå¹¶å‡å°‘äº†GPUå†…å­˜ä½¿ç”¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVAMBAåœ¨é•¿è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸­æ¯”ä¹‹å‰çš„é«˜æ•ˆè§†é¢‘æ¨¡å‹æé«˜äº†4.3%çš„å‡†ç¡®ç‡ï¼ŒåŒæ—¶åœ¨å¤šç§è§†é¢‘ç†è§£ä»»åŠ¡ä¸­ä¿æŒäº†å¼ºå¤§çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.11651', 'title': 'VGGT: Visual Geometry Grounded Transformer', 'url': 'https://huggingface.co/papers/2503.11651', 'abstract': 'We present VGGT, a feed-forward neural network that directly infers all key 3D attributes of a scene, including camera parameters, point maps, depth maps, and 3D point tracks, from one, a few, or hundreds of its views. This approach is a step forward in 3D computer vision, where models have typically been constrained to and specialized for single tasks. It is also simple and efficient, reconstructing images in under one second, and still outperforming alternatives that require post-processing with visual geometry optimization techniques. The network achieves state-of-the-art results in multiple 3D tasks, including camera parameter estimation, multi-view depth estimation, dense point cloud reconstruction, and 3D point tracking. We also show that using pretrained VGGT as a feature backbone significantly enhances downstream tasks, such as non-rigid point tracking and feed-forward novel view synthesis. Code and models are publicly available at https://github.com/facebookresearch/vggt.', 'score': 8, 'issue_id': 2740, 'pub_date': '2025-03-14', 'pub_date_card': {'ru': '14 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 14', 'zh': '3æœˆ14æ—¥'}, 'hash': '701338c26ac42ac7', 'authors': ['Jianyuan Wang', 'Minghao Chen', 'Nikita Karaev', 'Andrea Vedaldi', 'Christian Rupprecht', 'David Novotny'], 'affiliations': ['Meta AI', 'Visual Geometry Group, University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2503.11651.jpg', 'data': {'categories': ['#cv', '#open_source', '#3d', '#optimization'], 'emoji': 'ğŸ”®', 'ru': {'title': 'VGGT: Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ÑŒ Ğ´Ğ»Ñ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğ³Ğ¾ 3D-Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° ÑÑ†ĞµĞ½', 'desc': 'VGGT - ÑÑ‚Ğ¾ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ°Ñ ÑĞµÑ‚ÑŒ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ²ÑĞµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ 3D-Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ñ‹ ÑÑ†ĞµĞ½Ñ‹ Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ»Ğ¸ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ĞµÑ‘ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ½ĞµĞµ Ñ‡ĞµĞ¼ Ğ·Ğ° ÑĞµĞºÑƒĞ½Ğ´Ñƒ, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ñ‹, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸. VGGT Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… 3D-Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹, Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ğ¾Ğ±Ğ»Ğ°ĞºĞ° Ñ‚Ğ¾Ñ‡ĞµĞº Ğ¸ 3D-Ñ‚Ñ€ĞµĞºĞ¸Ğ½Ğ³. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ VGGT Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¾ÑĞ½Ğ¾Ğ²Ñ‹ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ² Ğ¸ Ğ½ĞµĞ¶ĞµÑÑ‚ĞºĞ¾Ğ³Ğ¾ Ñ‚Ñ€ĞµĞºĞ¸Ğ½Ğ³Ğ° Ñ‚Ğ¾Ñ‡ĞµĞº.'}, 'en': {'title': 'VGGT: Revolutionizing 3D Scene Understanding with Speed and Efficiency', 'desc': 'VGGT is a feed-forward neural network designed to extract key 3D attributes from various views of a scene, such as camera parameters and depth maps. Unlike traditional models that focus on single tasks, VGGT efficiently handles multiple 3D computer vision tasks simultaneously. It operates quickly, reconstructing images in under one second while achieving superior results compared to methods that rely on post-processing. Additionally, VGGT can be used as a feature backbone to improve performance in related tasks like point tracking and novel view synthesis.'}, 'zh': {'title': 'VGGTï¼šé«˜æ•ˆçš„3Dåœºæ™¯æ¨æ–­ç½‘ç»œ', 'desc': 'æˆ‘ä»¬æå‡ºäº†VGGTï¼Œè¿™æ˜¯ä¸€ç§å‰é¦ˆç¥ç»ç½‘ç»œï¼Œå¯ä»¥ç›´æ¥æ¨æ–­åœºæ™¯çš„æ‰€æœ‰å…³é”®3Då±æ€§ï¼ŒåŒ…æ‹¬ç›¸æœºå‚æ•°ã€ç‚¹å›¾ã€æ·±åº¦å›¾å’Œ3Dç‚¹è½¨è¿¹ã€‚è¯¥æ–¹æ³•åœ¨3Dè®¡ç®—æœºè§†è§‰é¢†åŸŸå‘å‰è¿ˆå‡ºäº†ä¸€æ­¥ï¼Œå…‹æœäº†ä»¥å¾€æ¨¡å‹ä»…é™äºå•ä¸€ä»»åŠ¡çš„å±€é™æ€§ã€‚VGGTç®€å•é«˜æ•ˆï¼Œèƒ½å¤Ÿåœ¨ä¸åˆ°ä¸€ç§’çš„æ—¶é—´å†…é‡å»ºå›¾åƒï¼Œå¹¶ä¸”åœ¨å¤šä¸ª3Dä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºéœ€è¦åå¤„ç†çš„æ›¿ä»£æ–¹æ¡ˆã€‚ä½¿ç”¨é¢„è®­ç»ƒçš„VGGTä½œä¸ºç‰¹å¾éª¨å¹²æ˜¾è‘—æå‡äº†ä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ï¼Œå¦‚éåˆšæ€§ç‚¹è·Ÿè¸ªå’Œå‰é¦ˆæ–°è§†å›¾åˆæˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.10632', 'title': 'Kolmogorov-Arnold Attention: Is Learnable Attention Better For Vision\n  Transformers?', 'url': 'https://huggingface.co/papers/2503.10632', 'abstract': "Kolmogorov-Arnold networks (KANs) are a remarkable innovation consisting of learnable activation functions with the potential to capture more complex relationships from data. Although KANs are useful in finding symbolic representations and continual learning of one-dimensional functions, their effectiveness in diverse machine learning (ML) tasks, such as vision, remains questionable. Presently, KANs are deployed by replacing multilayer perceptrons (MLPs) in deep network architectures, including advanced architectures such as vision Transformers (ViTs). In this paper, we are the first to design a general learnable Kolmogorov-Arnold Attention (KArAt) for vanilla ViTs that can operate on any choice of basis. However, the computing and memory costs of training them motivated us to propose a more modular version, and we designed particular learnable attention, called Fourier-KArAt. Fourier-KArAt and its variants either outperform their ViT counterparts or show comparable performance on CIFAR-10, CIFAR-100, and ImageNet-1K datasets. We dissect these architectures' performance and generalization capacity by analyzing their loss landscapes, weight distributions, optimizer path, attention visualization, and spectral behavior, and contrast them with vanilla ViTs. The goal of this paper is not to produce parameter- and compute-efficient attention, but to encourage the community to explore KANs in conjunction with more advanced architectures that require a careful understanding of learnable activations. Our open-source code and implementation details are available on: https://subhajitmaity.me/KArAt", 'score': 8, 'issue_id': 2731, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 13', 'zh': '3æœˆ13æ—¥'}, 'hash': '46504216bbce5b86', 'authors': ['Subhajit Maity', 'Killian Hitsman', 'Xin Li', 'Aritra Dutta'], 'affiliations': ['Department of Computer Science, University of Central Florida, Orlando, FL, USA', 'Department of Mathematics, University of Central Florida, Orlando, FL, USA'], 'pdf_title_img': 'assets/pdf/title_img/2503.10632.jpg', 'data': {'categories': ['#architecture', '#cv', '#optimization', '#open_source', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ: ĞšĞ¾Ğ»Ğ¼Ğ¾Ğ³Ğ¾Ñ€Ğ¾Ğ²-ĞÑ€Ğ½Ğ¾Ğ»ÑŒĞ´ ÑĞµÑ‚Ğ¸ Ğ² Vision Transformers', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹ - ĞšĞ¾Ğ»Ğ¼Ğ¾Ğ³Ğ¾Ñ€Ğ¾Ğ²-ĞÑ€Ğ½Ğ¾Ğ»ÑŒĞ´ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ (KArAt) Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Vision Transformer. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½ÑƒÑ Ğ²ĞµÑ€ÑĞ¸Ñ KArAt Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¤ÑƒÑ€ÑŒĞµ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼ÑƒÑ Ğ¸Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‰ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ ViT Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ² Ğ½Ğ¾Ğ²Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ»Ğ°Ğ½Ğ´ÑˆĞ°Ñ„Ñ‚ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ, Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ²ĞµÑĞ¾Ğ² Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ğ¾ Ğ¸Ğ·ÑƒÑ‡Ğ°Ñ‚ÑŒ ÑĞµÑ‚Ğ¸ ĞšĞ¾Ğ»Ğ¼Ğ¾Ğ³Ğ¾Ñ€Ğ¾Ğ²Ğ°-ĞÑ€Ğ½Ğ¾Ğ»ÑŒĞ´Ğ° Ğ² ÑĞ¾Ñ‡ĞµÑ‚Ğ°Ğ½Ğ¸Ğ¸ Ñ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°Ğ¼Ğ¸ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Unlocking Complex Relationships with Learnable Activations in Vision Transformers', 'desc': 'Kolmogorov-Arnold networks (KANs) introduce learnable activation functions that can model complex data relationships. This paper explores the application of KANs in vision tasks by integrating them into vision Transformers (ViTs) through a novel learnable attention mechanism called Kolmogorov-Arnold Attention (KArAt). The authors also present a more efficient variant, Fourier-KArAt, which shows competitive performance on popular image datasets like CIFAR-10 and ImageNet-1K. The study emphasizes the importance of understanding learnable activations in advanced architectures, rather than solely focusing on efficiency.'}, 'zh': {'title': 'æ¢ç´¢å¯å­¦ä¹ æ¿€æ´»å‡½æ•°çš„æ½œåŠ›', 'desc': 'Kolmogorov-Arnoldç½‘ç»œï¼ˆKANsï¼‰æ˜¯ä¸€ç§åˆ›æ–°çš„å¯å­¦ä¹ æ¿€æ´»å‡½æ•°ï¼Œèƒ½å¤Ÿæ•æ‰æ•°æ®ä¸­çš„å¤æ‚å…³ç³»ã€‚å°½ç®¡KANsåœ¨ä¸€ç»´å‡½æ•°çš„ç¬¦å·è¡¨ç¤ºå’ŒæŒç»­å­¦ä¹ ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨è§†è§‰ç­‰å¤šç§æœºå™¨å­¦ä¹ ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ä»ç„¶å­˜åœ¨ç–‘é—®ã€‚æœ¬æ–‡é¦–æ¬¡ä¸ºæ™®é€šçš„è§†è§‰å˜æ¢å™¨ï¼ˆViTsï¼‰è®¾è®¡äº†ä¸€ç§é€šç”¨çš„å¯å­¦ä¹ Kolmogorov-Arnoldæ³¨æ„åŠ›ï¼ˆKArAtï¼‰ï¼Œå¹¶æå‡ºäº†æ›´æ¨¡å—åŒ–çš„Fourier-KArAtç‰ˆæœ¬ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFourier-KArAtåŠå…¶å˜ä½“åœ¨CIFAR-10ã€CIFAR-100å’ŒImageNet-1Kæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜äºæˆ–ä¸ViTç›¸å½“ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.06542', 'title': 'ARMOR v0.1: Empowering Autoregressive Multimodal Understanding Model\n  with Interleaved Multimodal Generation via Asymmetric Synergy', 'url': 'https://huggingface.co/papers/2503.06542', 'abstract': 'Unified models (UniMs) for multimodal understanding and generation have recently received much attention in the area of vision and language. Existing UniMs are designed to simultaneously learn both multimodal understanding and generation capabilities, demanding substantial computational resources, and often struggle to generate interleaved text-image. We present ARMOR, a resource-efficient and pure autoregressive framework that achieves both understanding and generation by fine-tuning existing multimodal large language models (MLLMs). Specifically, ARMOR extends existing MLLMs from three perspectives: (1) For model architecture, an asymmetric encoder-decoder architecture with a forward-switching mechanism is introduced to unify embedding space integrating textual and visual modalities for enabling natural text-image interleaved generation with minimal computational overhead. (2) For training data, a meticulously curated, high-quality interleaved dataset is collected for fine-tuning MLLMs. (3) For the training algorithm, we propose a ``what or how to generate" algorithm to empower existing MLLMs with multimodal generation capabilities while preserving their multimodal understanding capabilities, through three progressive training stages based on the collected dataset. Experimental results demonstrate that ARMOR upgrades existing MLLMs to UniMs with promising image generation capabilities, using limited training resources. Our code will be released soon at https://armor.github.io.', 'score': 6, 'issue_id': 2737, 'pub_date': '2025-03-09', 'pub_date_card': {'ru': '9 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 9', 'zh': '3æœˆ9æ—¥'}, 'hash': '4b19cfc1e459fb2f', 'authors': ['Jianwen Sun', 'Yukang Feng', 'Chuanhao Li', 'Fanrui Zhang', 'Zizhen Li', 'Jiaxin Ai', 'Sizhuo Zhou', 'Yu Dai', 'Shenglin Zhang', 'Kaipeng Zhang'], 'affiliations': ['Nankai University', 'Shanghai AI Laboratory', 'Shanghai Innovation Institute', 'University of Science and Technology of China', 'Wuhan University'], 'pdf_title_img': 'assets/pdf/title_img/2503.06542.jpg', 'data': {'categories': ['#games', '#optimization', '#multimodal', '#dataset', '#training', '#architecture'], 'emoji': 'ğŸ›¡ï¸', 'ru': {'title': 'ARMOR: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'ARMOR - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°ÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€-Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€ Ñ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ¼ Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¿ĞµÑ€ĞµĞ´ Ğ´Ğ»Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. ARMOR Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ ÑĞ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ñ‡ĞµÑ€ĞµĞ´ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ÑÑ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸. ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² Ñ‚Ñ€Ğ¸ ÑÑ‚Ğ°Ğ¿Ğ°, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğº Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'ARMOR: Efficient Multimodal Mastery with Minimal Resources', 'desc': 'The paper introduces ARMOR, a new framework designed to enhance multimodal understanding and generation in machine learning models. Unlike existing unified models that require extensive computational resources, ARMOR is resource-efficient and fine-tunes large language models to achieve its goals. It employs an asymmetric encoder-decoder architecture to seamlessly integrate text and image data, allowing for natural interleaved generation. Additionally, ARMOR utilizes a carefully curated dataset and a novel training algorithm to improve the multimodal capabilities of existing models while maintaining their understanding abilities.'}, 'zh': {'title': 'ARMORï¼šé«˜æ•ˆçš„å¤šæ¨¡æ€ç†è§£ä¸ç”Ÿæˆæ¡†æ¶', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºARMORçš„ç»Ÿä¸€æ¨¡å‹æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆçš„æ•ˆç‡ã€‚ARMORé€šè¿‡å¾®è°ƒç°æœ‰çš„å¤§å‹å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ï¼Œå®ç°äº†æ–‡æœ¬å’Œå›¾åƒçš„è‡ªç„¶äº¤ç»‡ç”Ÿæˆã€‚è¯¥æ¡†æ¶é‡‡ç”¨äº†ä¸å¯¹ç§°çš„ç¼–ç -è§£ç æ¶æ„ï¼Œå¹¶å¼•å…¥äº†å‰å‘åˆ‡æ¢æœºåˆ¶ï¼Œä»¥å‡å°‘è®¡ç®—èµ„æºçš„æ¶ˆè€—ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒARMORèƒ½å¤Ÿåœ¨æœ‰é™çš„è®­ç»ƒèµ„æºä¸‹ï¼Œæ˜¾è‘—æå‡ç°æœ‰æ¨¡å‹çš„å›¾åƒç”Ÿæˆèƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.09279', 'title': 'Cockatiel: Ensembling Synthetic and Human Preferenced Training for\n  Detailed Video Caption', 'url': 'https://huggingface.co/papers/2503.09279', 'abstract': 'Video Detailed Captioning (VDC) is a crucial task for vision-language bridging, enabling fine-grained descriptions of complex video content. In this paper, we first comprehensively benchmark current state-of-the-art approaches and systematically identified two critical limitations: biased capability towards specific captioning aspect and misalignment with human preferences. To address these deficiencies, we propose Cockatiel, a novel three-stage training pipeline that ensembles synthetic and human-aligned training for improving VDC performance. In the first stage, we derive a scorer from a meticulously annotated dataset to select synthetic captions high-performing on certain fine-grained video-caption alignment and human-preferred while disregarding others. Then, we train Cockatiel-13B, using this curated dataset to infuse it with assembled model strengths and human preferences. Finally, we further distill Cockatiel-8B from Cockatiel-13B for the ease of usage. Extensive quantitative and qualitative experiments reflect the effectiveness of our method, as we not only set new state-of-the-art performance on VDCSCORE in a dimension-balanced way but also surpass leading alternatives on human preference by a large margin as depicted by the human evaluation results.', 'score': 5, 'issue_id': 2730, 'pub_date': '2025-03-12', 'pub_date_card': {'ru': '12 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 12', 'zh': '3æœˆ12æ—¥'}, 'hash': 'edf6712b564fd37a', 'authors': ['Luozheng Qin', 'Zhiyu Tan', 'Mengping Yang', 'Xiaomeng Yang', 'Hao Li'], 'affiliations': ['Fudan University', 'Shanghai Academy of Artificial Intelligence for Science'], 'pdf_title_img': 'assets/pdf/title_img/2503.09279.jpg', 'data': {'categories': ['#benchmark', '#training', '#multimodal', '#synthetic', '#alignment'], 'emoji': 'ğŸ¦œ', 'ru': {'title': 'Cockatiel: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ² Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ (VDC) Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Cockatiel. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ñ‚Ñ€ĞµÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¾Ñ‚Ğ±Ğ¾Ñ€ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞµĞ¹, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Cockatiel-13B Ğ¸ ĞµĞµ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¼ĞµĞ½ÑŒÑˆÑƒÑ Cockatiel-8B. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Cockatiel Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞµ VDCSCORE Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼.'}, 'en': {'title': 'Bridging Vision and Language with Cockatiel for Enhanced Video Captioning', 'desc': 'This paper addresses the challenge of Video Detailed Captioning (VDC), which involves creating precise descriptions for complex video content. The authors identify two main issues with existing methods: a bias towards certain aspects of captioning and a lack of alignment with human preferences. To overcome these challenges, they introduce Cockatiel, a three-stage training pipeline that combines synthetic and human-aligned data to enhance VDC performance. Their experiments demonstrate that Cockatiel achieves state-of-the-art results on the VDCSCORE metric and significantly outperforms other methods in terms of human preference evaluations.'}, 'zh': {'title': 'æå‡è§†é¢‘æè¿°çš„æ™ºèƒ½åŒ–ä¸äººæ€§åŒ–', 'desc': 'è§†é¢‘è¯¦ç»†æè¿°ï¼ˆVDCï¼‰æ˜¯è¿æ¥è§†è§‰å’Œè¯­è¨€çš„é‡è¦ä»»åŠ¡ï¼Œèƒ½å¤Ÿå¯¹å¤æ‚è§†é¢‘å†…å®¹è¿›è¡Œç»†è‡´çš„æè¿°ã€‚æœ¬æ–‡é¦–å…ˆå¯¹å½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•è¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œå¹¶ç³»ç»Ÿåœ°è¯†åˆ«å‡ºä¸¤ä¸ªå…³é”®é™åˆ¶ï¼šå¯¹ç‰¹å®šæè¿°æ–¹é¢çš„åè§èƒ½åŠ›å’Œä¸äººç±»åå¥½çš„ä¸ä¸€è‡´ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Cockatielï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„ä¸‰é˜¶æ®µè®­ç»ƒæµç¨‹ï¼Œç»“åˆäº†åˆæˆå’Œäººç±»å¯¹é½çš„è®­ç»ƒï¼Œä»¥æé«˜VDCæ€§èƒ½ã€‚é€šè¿‡å¤§é‡çš„å®šé‡å’Œå®šæ€§å®éªŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨VDCSCOREä¸Šè®¾å®šäº†æ–°çš„æœ€å…ˆè¿›æ€§èƒ½ï¼Œå¹¶åœ¨ä¸äººç±»åå¥½çš„æ¯”è¾ƒä¸­å¤§å¹…è¶…è¶Šäº†é¢†å…ˆçš„æ›¿ä»£æ–¹æ¡ˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.10696', 'title': 'Neighboring Autoregressive Modeling for Efficient Visual Generation', 'url': 'https://huggingface.co/papers/2503.10696', 'abstract': 'Visual autoregressive models typically adhere to a raster-order ``next-token prediction" paradigm, which overlooks the spatial and temporal locality inherent in visual content. Specifically, visual tokens exhibit significantly stronger correlations with their spatially or temporally adjacent tokens compared to those that are distant. In this paper, we propose Neighboring Autoregressive Modeling (NAR), a novel paradigm that formulates autoregressive visual generation as a progressive outpainting procedure, following a near-to-far ``next-neighbor prediction" mechanism. Starting from an initial token, the remaining tokens are decoded in ascending order of their Manhattan distance from the initial token in the spatial-temporal space, progressively expanding the boundary of the decoded region. To enable parallel prediction of multiple adjacent tokens in the spatial-temporal space, we introduce a set of dimension-oriented decoding heads, each predicting the next token along a mutually orthogonal dimension. During inference, all tokens adjacent to the decoded tokens are processed in parallel, substantially reducing the model forward steps for generation. Experiments on ImageNet256times 256 and UCF101 demonstrate that NAR achieves 2.4times and 8.6times higher throughput respectively, while obtaining superior FID/FVD scores for both image and video generation tasks compared to the PAR-4X approach. When evaluating on text-to-image generation benchmark GenEval, NAR with 0.8B parameters outperforms Chameleon-7B while using merely 0.4 of the training data. Code is available at https://github.com/ThisisBillhe/NAR.', 'score': 5, 'issue_id': 2738, 'pub_date': '2025-03-12', 'pub_date_card': {'ru': '12 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 12', 'zh': '3æœˆ12æ—¥'}, 'hash': '5adda6787d6613db', 'authors': ['Yefei He', 'Yuanyu He', 'Shaoxuan He', 'Feng Chen', 'Hong Zhou', 'Kaipeng Zhang', 'Bohan Zhuang'], 'affiliations': ['Shanghai AI Laboratory, China', 'The University of Adelaide, Australia', 'Zhejiang University, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.10696.jpg', 'data': {'categories': ['#cv', '#video', '#games', '#benchmark', '#optimization', '#architecture', '#training'], 'emoji': 'ğŸ§©', 'ru': {'title': 'NAR: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°', 'desc': "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Neighboring Autoregressive Modeling (NAR). Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ñ… Ñ€Ğ°ÑÑ‚Ñ€Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ñ€ÑĞ´Ğ¾Ğº Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ, NAR Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ 'Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ ÑĞ¾ÑĞµĞ´Ğ°', ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ Ğ±Ğ»Ğ¸Ğ·Ğ¾ÑÑ‚ÑŒ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ°Ñ‡Ğ¸Ğ½Ğ°ĞµÑ‚ Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ° Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾ÑÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ, Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑÑ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸. Ğ”Ğ»Ñ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ÑĞ¾ÑĞµĞ´Ğ½Ğ¸Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ÑÑ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğµ Ğ³Ğ¾Ğ»Ğ¾Ğ²ĞºĞ¸, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸."}, 'en': {'title': 'Revolutionizing Visual Generation with Neighboring Autoregressive Modeling', 'desc': "This paper introduces Neighboring Autoregressive Modeling (NAR), a new approach to visual generation that improves upon traditional autoregressive models by focusing on the spatial and temporal relationships between visual tokens. Instead of predicting the next token in a raster order, NAR uses a 'next-neighbor prediction' method, decoding tokens based on their proximity to an initial token. This allows for parallel processing of adjacent tokens, significantly speeding up the generation process. Experiments show that NAR achieves much higher throughput and better quality scores in image and video generation compared to existing methods, while also being more efficient in terms of training data usage."}, 'zh': {'title': 'é‚»è¿‘è‡ªå›å½’å»ºæ¨¡ï¼šæå‡è§†è§‰ç”Ÿæˆæ•ˆç‡çš„åˆ›æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è§†è§‰è‡ªå›å½’æ¨¡å‹ï¼Œç§°ä¸ºé‚»è¿‘è‡ªå›å½’å»ºæ¨¡ï¼ˆNARï¼‰ï¼Œæ—¨åœ¨æ”¹å–„ä¼ ç»Ÿçš„åŸºäºå…‰æ …é¡ºåºçš„â€œä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹â€æ–¹æ³•ã€‚NARé€šè¿‡è¿‘é‚»é¢„æµ‹æœºåˆ¶ï¼Œå°†è‡ªå›å½’è§†è§‰ç”Ÿæˆè§†ä¸ºä¸€ç§é€æ­¥æ‰©å±•çš„è¿‡ç¨‹ï¼Œä»åˆå§‹æ ‡è®°å¼€å§‹ï¼ŒæŒ‰æ›¼å“ˆé¡¿è·ç¦»é€æ­¥è§£ç å‰©ä½™æ ‡è®°ã€‚è¯¥æ–¹æ³•å¼•å…¥äº†ä¸€ç»„é¢å‘ç»´åº¦çš„è§£ç å¤´ï¼Œå…è®¸åœ¨ç©ºé—´-æ—¶é—´ç©ºé—´ä¸­å¹¶è¡Œé¢„æµ‹å¤šä¸ªç›¸é‚»æ ‡è®°ï¼Œä»è€Œæ˜¾è‘—å‡å°‘ç”Ÿæˆæ‰€éœ€çš„æ¨¡å‹å‰å‘æ­¥éª¤ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒNARåœ¨å›¾åƒå’Œè§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸­å‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæ˜¾ç¤ºå‡ºæ›´é«˜çš„ååé‡å’Œæ›´å¥½çš„ç”Ÿæˆè´¨é‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.06553', 'title': 'ProJudge: A Multi-Modal Multi-Discipline Benchmark and\n  Instruction-Tuning Dataset for MLLM-based Process Judges', 'url': 'https://huggingface.co/papers/2503.06553', 'abstract': "As multi-modal large language models (MLLMs) frequently exhibit errors when solving scientific problems, evaluating the validity of their reasoning processes is critical for ensuring reliability and uncovering fine-grained model weaknesses. Since human evaluation is laborious and costly, prompting MLLMs as automated process judges has become a common practice. However, the reliability of these model-based judges remains uncertain. To address this, we introduce ProJudgeBench, the first comprehensive benchmark specifically designed for evaluating abilities of MLLM-based process judges. ProJudgeBench comprises 2,400 test cases and 50,118 step-level labels, spanning four scientific disciplines with diverse difficulty levels and multi-modal content. In ProJudgeBench, each step is meticulously annotated by human experts for correctness, error type, and explanation, enabling a systematic evaluation of judges' capabilities to detect, classify and diagnose errors. Evaluation on ProJudgeBench reveals a significant performance gap between open-source and proprietary models. To bridge this gap, we further propose ProJudge-173k, a large-scale instruction-tuning dataset, and a Dynamic Dual-Phase fine-tuning strategy that encourages models to explicitly reason through problem-solving before assessing solutions. Both contributions significantly enhance the process evaluation capabilities of open-source models. All the resources will be released to foster future research of reliable multi-modal process evaluation.", 'score': 5, 'issue_id': 2737, 'pub_date': '2025-03-09', 'pub_date_card': {'ru': '9 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 9', 'zh': '3æœˆ9æ—¥'}, 'hash': '9546d0d9f897e116', 'authors': ['Jiaxin Ai', 'Pengfei Zhou', 'Zhaopan Xu', 'Ming Li', 'Fanrui Zhang', 'Zizhen Li', 'Jianwen Sun', 'Yukang Feng', 'Baojin Huang', 'Zhongyuan Wang', 'Kaipeng Zhang'], 'affiliations': ['HZAU', 'NKU', 'Shanghai AI Laboratory', 'Shanghai Innovation Institute', 'USTC', 'WHU'], 'pdf_title_img': 'assets/pdf/title_img/2503.06553.jpg', 'data': {'categories': ['#interpretability', '#reasoning', '#multimodal', '#science', '#open_source', '#dataset', '#training', '#benchmark'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'ĞĞ°Ğ´ĞµĞ¶Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ˜Ğ˜-ÑÑƒĞ´ÑŒÑĞ¼Ğ¸', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ ProJudgeBench - Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğ² Ñ€Ğ¾Ğ»Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑÑƒĞ´ĞµĞ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 2400 Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ÑĞ»ÑƒÑ‡Ğ°ĞµĞ² Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ 50 000 Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ñ†ĞµĞ½Ğ¾Ğº Ğ² Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ… Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ´Ğ¸ÑÑ†Ğ¸Ğ¿Ğ»Ğ¸Ğ½Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ProJudge-173k Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ² Ñƒ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ.'}, 'en': {'title': 'Enhancing MLLM Reliability with ProJudgeBench', 'desc': 'This paper addresses the challenges faced by multi-modal large language models (MLLMs) in solving scientific problems accurately. It introduces ProJudgeBench, a benchmark designed to evaluate the reasoning abilities of MLLM-based judges through a comprehensive set of test cases and detailed annotations. The study highlights a performance gap between open-source and proprietary models, indicating the need for improvement in the evaluation process. To enhance this, the authors propose ProJudge-173k, a dataset for instruction-tuning and a new fine-tuning strategy that promotes better reasoning in problem-solving.'}, 'zh': {'title': 'æå‡å¤šæ¨¡æ€æ¨¡å‹çš„è¿‡ç¨‹è¯„ä¼°èƒ½åŠ›', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ProJudgeBenchï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰è¿‡ç¨‹åˆ¤æ–­èƒ½åŠ›çš„åŸºå‡†ã€‚è¯¥åŸºå‡†åŒ…å«2400ä¸ªæµ‹è¯•æ¡ˆä¾‹å’Œ50118ä¸ªæ­¥éª¤çº§æ ‡ç­¾ï¼Œæ¶µç›–å››ä¸ªç§‘å­¦é¢†åŸŸï¼Œå…·æœ‰ä¸åŒçš„éš¾åº¦å’Œå¤šæ¨¡æ€å†…å®¹ã€‚æ¯ä¸ªæ­¥éª¤éƒ½ç”±äººç±»ä¸“å®¶ä»”ç»†æ³¨é‡Šï¼Œä»¥ä¾¿ç³»ç»Ÿè¯„ä¼°æ¨¡å‹åœ¨æ£€æµ‹ã€åˆ†ç±»å’Œè¯Šæ–­é”™è¯¯æ–¹é¢çš„èƒ½åŠ›ã€‚é€šè¿‡åœ¨ProJudgeBenchä¸Šçš„è¯„ä¼°ï¼Œå‘ç°å¼€æºæ¨¡å‹ä¸ä¸“æœ‰æ¨¡å‹ä¹‹é—´å­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½å·®è·ï¼Œå¹¶æå‡ºäº†ProJudge-173kæ•°æ®é›†å’ŒåŠ¨æ€åŒé˜¶æ®µå¾®è°ƒç­–ç•¥ï¼Œä»¥æé«˜å¼€æºæ¨¡å‹çš„è¿‡ç¨‹è¯„ä¼°èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.06674', 'title': 'Learning Few-Step Diffusion Models by Trajectory Distribution Matching', 'url': 'https://huggingface.co/papers/2503.06674', 'abstract': "Accelerating diffusion model sampling is crucial for efficient AIGC deployment. While diffusion distillation methods -- based on distribution matching and trajectory matching -- reduce sampling to as few as one step, they fall short on complex tasks like text-to-image generation. Few-step generation offers a better balance between speed and quality, but existing approaches face a persistent trade-off: distribution matching lacks flexibility for multi-step sampling, while trajectory matching often yields suboptimal image quality. To bridge this gap, we propose learning few-step diffusion models by Trajectory Distribution Matching (TDM), a unified distillation paradigm that combines the strengths of distribution and trajectory matching. Our method introduces a data-free score distillation objective, aligning the student's trajectory with the teacher's at the distribution level. Further, we develop a sampling-steps-aware objective that decouples learning targets across different steps, enabling more adjustable sampling. This approach supports both deterministic sampling for superior image quality and flexible multi-step adaptation, achieving state-of-the-art performance with remarkable efficiency. Our model, TDM, outperforms existing methods on various backbones, such as SDXL and PixArt-alpha, delivering superior quality and significantly reduced training costs. In particular, our method distills PixArt-alpha into a 4-step generator that outperforms its teacher on real user preference at 1024 resolution. This is accomplished with 500 iterations and 2 A800 hours -- a mere 0.01% of the teacher's training cost. In addition, our proposed TDM can be extended to accelerate text-to-video diffusion. Notably, TDM can outperform its teacher model (CogVideoX-2B) by using only 4 NFE on VBench, improving the total score from 80.91 to 81.65. Project page: https://tdm-t2x.github.io/", 'score': 4, 'issue_id': 2732, 'pub_date': '2025-03-09', 'pub_date_card': {'ru': '9 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 9', 'zh': '3æœˆ9æ—¥'}, 'hash': '1ce5d8eb2086abfc', 'authors': ['Yihong Luo', 'Tianyang Hu', 'Jiacheng Sun', 'Yujun Cai', 'Jing Tang'], 'affiliations': ['Huawei Noahs Ark Lab'], 'pdf_title_img': 'assets/pdf/title_img/2503.06674.jpg', 'data': {'categories': ['#diffusion', '#training', '#cv', '#video'], 'emoji': 'ğŸš€', 'ru': {'title': 'TDM: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ Trajectory Distribution Matching (TDM). TDM Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ·Ğ° Ğ¼ĞµĞ½ÑŒÑˆĞµĞµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑˆĞ°Ğ³Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ±ĞµĞ·Ğ´Ğ°Ğ½Ğ½ÑƒÑ Ñ†ĞµĞ»ÑŒ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ Ñ†ĞµĞ»ÑŒ, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰ÑƒÑ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑˆĞ°Ğ³Ğ¾Ğ² ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ³Ğ¸Ğ±ĞºĞ¾ÑÑ‚ÑŒ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ. TDM Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°Ñ…, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº SDXL Ğ¸ PixArt-alpha, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°Ñ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'Efficient Few-Step Diffusion with TDM', 'desc': 'This paper presents a new method called Trajectory Distribution Matching (TDM) to improve the efficiency of diffusion models in generating images and videos. TDM combines the benefits of distribution matching and trajectory matching, allowing for fewer sampling steps while maintaining high image quality. The method introduces a data-free score distillation objective that aligns the learning process of a smaller model with a larger, more complex model. By decoupling learning targets for different sampling steps, TDM achieves state-of-the-art performance with significantly reduced training costs, making it suitable for applications like text-to-image and text-to-video generation.'}, 'zh': {'title': 'æå‡æ‰©æ•£æ¨¡å‹é‡‡æ ·æ•ˆç‡çš„åˆ›æ–°æ–¹æ³•', 'desc': 'åŠ é€Ÿæ‰©æ•£æ¨¡å‹é‡‡æ ·å¯¹äºé«˜æ•ˆçš„AIGCéƒ¨ç½²è‡³å…³é‡è¦ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å­¦ä¹ å°‘æ­¥æ‰©æ•£æ¨¡å‹çš„æ–¹æ³•ï¼Œç§°ä¸ºè½¨è¿¹åˆ†å¸ƒåŒ¹é…ï¼ˆTDMï¼‰ï¼Œå®ƒç»“åˆäº†åˆ†å¸ƒåŒ¹é…å’Œè½¨è¿¹åŒ¹é…çš„ä¼˜ç‚¹ã€‚é€šè¿‡å¼•å…¥æ— æ•°æ®çš„åˆ†æ•°è’¸é¦ç›®æ ‡ï¼Œæˆ‘ä»¬çš„æ¨¡å‹èƒ½å¤Ÿåœ¨ä¸åŒé‡‡æ ·æ­¥éª¤ä¹‹é—´è§£è€¦å­¦ä¹ ç›®æ ‡ï¼Œä»è€Œå®ç°æ›´çµæ´»çš„é‡‡æ ·ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTDMåœ¨å¤šä¸ªåŸºå‡†ä¸Šè¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œæ˜¾è‘—æé«˜äº†å›¾åƒè´¨é‡å¹¶é™ä½äº†è®­ç»ƒæˆæœ¬ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.10624', 'title': 'ETCH: Generalizing Body Fitting to Clothed Humans via Equivariant\n  Tightness', 'url': 'https://huggingface.co/papers/2503.10624', 'abstract': 'Fitting a body to a 3D clothed human point cloud is a common yet challenging task. Traditional optimization-based approaches use multi-stage pipelines that are sensitive to pose initialization, while recent learning-based methods often struggle with generalization across diverse poses and garment types. We propose Equivariant Tightness Fitting for Clothed Humans, or ETCH, a novel pipeline that estimates cloth-to-body surface mapping through locally approximate SE(3) equivariance, encoding tightness as displacement vectors from the cloth surface to the underlying body. Following this mapping, pose-invariant body features regress sparse body markers, simplifying clothed human fitting into an inner-body marker fitting task. Extensive experiments on CAPE and 4D-Dress show that ETCH significantly outperforms state-of-the-art methods -- both tightness-agnostic and tightness-aware -- in body fitting accuracy on loose clothing (16.7% ~ 69.5%) and shape accuracy (average 49.9%). Our equivariant tightness design can even reduce directional errors by (67.2% ~ 89.8%) in one-shot (or out-of-distribution) settings. Qualitative results demonstrate strong generalization of ETCH, regardless of challenging poses, unseen shapes, loose clothing, and non-rigid dynamics. We will release the code and models soon for research purposes at https://boqian-li.github.io/ETCH/.', 'score': 3, 'issue_id': 2738, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 13', 'zh': '3æœˆ13æ—¥'}, 'hash': 'c0aca4cafef8e621', 'authors': ['Boqian Li', 'Haiwen Feng', 'Zeyu Cai', 'Michael J. Black', 'Yuliang Xiu'], 'affiliations': ['Berkeley AI Research (BAIR)', 'Max Planck Institute for Intelligent Systems', 'Westlake University'], 'pdf_title_img': 'assets/pdf/title_img/2503.10624.jpg', 'data': {'categories': ['#3d', '#optimization', '#cv', '#open_source'], 'emoji': 'ğŸ‘•', 'ru': {'title': 'Ğ¢Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ğ½ĞºĞ° 3D-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‚ĞµĞ»Ğ° Ğº Ğ¾Ğ´ĞµÑ‚Ğ¾Ğ¼Ñƒ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºÑƒ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸Ğ»ĞµĞ³Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ´ĞµĞ¶Ğ´Ñ‹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ETCH Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ğ½ĞºĞ¸ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‚ĞµĞ»Ğ° Ğº Ğ¾Ğ±Ğ»Ğ°ĞºÑƒ Ñ‚Ğ¾Ñ‡ĞµĞº Ğ¾Ğ´ĞµÑ‚Ğ¾Ğ³Ğ¾ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°. ETCH Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ¶ĞµĞ½Ğ½ÑƒÑ SE(3)-ÑĞºĞ²Ğ¸Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¾Ğ´ĞµĞ¶Ğ´Ñ‹ Ğ¸ Ñ‚ĞµĞ»Ğ¾Ğ¼, ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒÑ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸Ğ»ĞµĞ³Ğ°Ğ½Ğ¸Ñ ĞºĞ°Ğº Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ñ‹ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ°Ñ€ĞºĞµÑ€Ñ‹ Ñ‚ĞµĞ»Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¸Ğ½Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ğº Ğ¿Ğ¾Ğ·Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ², ÑƒĞ¿Ñ€Ğ¾Ñ‰Ğ°Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ğ½ĞºĞ¸ Ñ‚ĞµĞ»Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ETCH Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ğ½ĞºĞ¸ Ñ‚ĞµĞ»Ğ° Ğ¸ Ñ„Ğ¾Ñ€Ğ¼Ñ‹, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ´Ğ»Ñ ÑĞ²Ğ¾Ğ±Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¾Ğ´ĞµĞ¶Ğ´Ñ‹.'}, 'en': {'title': 'Revolutionizing Body Fitting with Equivariant Tightness!', 'desc': 'The paper introduces Equivariant Tightness Fitting for Clothed Humans (ETCH), a new method for fitting a body model to a 3D point cloud of a clothed human. ETCH improves upon traditional optimization methods by using a pipeline that leverages SE(3) equivariance to accurately map cloth surfaces to body shapes. This approach simplifies the fitting process by focusing on pose-invariant body features and regressing sparse body markers. Experimental results show that ETCH significantly enhances fitting accuracy for various clothing types and poses, outperforming existing methods in both tightness and shape accuracy.'}, 'zh': {'title': 'ç­‰å˜ç´§è‡´æ‹Ÿåˆï¼šæå‡3Dç©¿è¡£äººç±»æ‹Ÿåˆç²¾åº¦çš„åˆ›æ–°æ–¹æ³•', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œç§°ä¸ºç­‰å˜ç´§è‡´æ‹Ÿåˆï¼ˆETCHï¼‰ï¼Œç”¨äºå°†èº«ä½“ä¸3Dç©¿è¡£äººç±»ç‚¹äº‘ç›¸åŒ¹é…ã€‚ä¼ ç»Ÿçš„æ–¹æ³•ä¾èµ–äºå¤šé˜¶æ®µä¼˜åŒ–ï¼Œå®¹æ˜“å—åˆ°å§¿åŠ¿åˆå§‹åŒ–çš„å½±å“ï¼Œè€Œå­¦ä¹ å‹æ–¹æ³•åœ¨ä¸åŒå§¿åŠ¿å’Œæœè£…ç±»å‹çš„æ³›åŒ–èƒ½åŠ›ä¸Šå­˜åœ¨å›°éš¾ã€‚ETCHé€šè¿‡å±€éƒ¨è¿‘ä¼¼çš„SE(3)ç­‰å˜æ€§æ¥ä¼°è®¡å¸ƒæ–™ä¸èº«ä½“è¡¨é¢çš„æ˜ å°„ï¼Œå¹¶å°†ç´§è‡´åº¦ç¼–ç ä¸ºä»å¸ƒæ–™è¡¨é¢åˆ°èº«ä½“çš„ä½ç§»å‘é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒETCHåœ¨æ¾æ•£è¡£ç‰©çš„èº«ä½“æ‹Ÿåˆç²¾åº¦å’Œå½¢çŠ¶ç²¾åº¦ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.11207', 'title': 'Can Large Reasoning Models do Analogical Reasoning under Perceptual\n  Uncertainty?', 'url': 'https://huggingface.co/papers/2503.11207', 'abstract': "This work presents a first evaluation of two state-of-the-art Large Reasoning Models (LRMs), OpenAI's o3-mini and DeepSeek R1, on analogical reasoning, focusing on well-established nonverbal human IQ tests based on Raven's progressive matrices. We benchmark with the I-RAVEN dataset and its more difficult extension, I-RAVEN-X, which tests the ability to generalize to longer reasoning rules and ranges of the attribute values. To assess the influence of visual uncertainties on these nonverbal analogical reasoning tests, we extend the I-RAVEN-X dataset, which otherwise assumes an oracle perception. We adopt a two-fold strategy to simulate this imperfect visual perception: 1) we introduce confounding attributes which, being sampled at random, do not contribute to the prediction of the correct answer of the puzzles and 2) smoothen the distributions of the input attributes' values. We observe a sharp decline in OpenAI's o3-mini task accuracy, dropping from 86.6% on the original I-RAVEN to just 17.0% -- approaching random chance -- on the more challenging I-RAVEN-X, which increases input length and range and emulates perceptual uncertainty. This drop occurred despite spending 3.4x more reasoning tokens. A similar trend is also observed for DeepSeek R1: from 80.6% to 23.2%. On the other hand, a neuro-symbolic probabilistic abductive model, ARLC, that achieves state-of-the-art performances on I-RAVEN, can robustly reason under all these out-of-distribution tests, maintaining strong accuracy with only a modest reduction from 98.6% to 88.0%. Our code is available at https://github.com/IBM/raven-large-language-models.", 'score': 2, 'issue_id': 2744, 'pub_date': '2025-03-14', 'pub_date_card': {'ru': '14 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 14', 'zh': '3æœˆ14æ—¥'}, 'hash': '1910487e8b409ffb', 'authors': ['Giacomo Camposampiero', 'Michael Hersche', 'Roger Wattenhofer', 'Abu Sebastian', 'Abbas Rahimi'], 'affiliations': ['ETH ZÃ¼rich', 'IBM Research - Zurich'], 'pdf_title_img': 'assets/pdf/title_img/2503.11207.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#cv', '#dataset'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞšÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ÑƒÑÑ‚ÑƒĞ¿Ğ°ÑÑ‚ Ğ½ĞµĞ¹Ñ€Ğ¾-ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼ Ğ² Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğ¸', 'desc': 'Ğ­Ñ‚Ğ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿ĞµÑ€Ğ²ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ´Ğ²ÑƒÑ… ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ (LRM) - o3-mini Ğ¾Ñ‚ OpenAI Ğ¸ DeepSeek R1 - Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ½ĞµĞ²ĞµÑ€Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ² IQ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ñ‹ Ğ Ğ°Ğ²ĞµĞ½Ğ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸Ğ½Ğ³ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… I-RAVEN Ğ¸ ĞµĞ³Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¼ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğ¸ I-RAVEN-X, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑÑÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ´Ğ¸Ğ°Ğ¿Ğ°Ğ·Ğ¾Ğ½Ñ‹ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ¾Ğ². Ğ”Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°ÑÑˆĞ¸Ñ€Ğ¸Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… I-RAVEN-X, Ğ²Ğ²ĞµĞ´Ñ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ñ‹Ğµ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ñ‹ Ğ¸ ÑĞ³Ğ»Ğ°Ğ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ LRM Ğ½Ğ° Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ…, Ğ² Ñ‚Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ĞºĞ°Ğº Ğ½ĞµĞ¹Ñ€Ğ¾-ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ARLC Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ° ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğº ÑÑ‚Ğ¸Ğ¼ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸ÑĞ¼.'}, 'en': {'title': 'Evaluating Reasoning Under Uncertainty in Large Models', 'desc': "This paper evaluates two advanced Large Reasoning Models (LRMs), OpenAI's o3-mini and DeepSeek R1, on their ability to perform analogical reasoning using Raven's progressive matrices. The study uses the I-RAVEN dataset and its more challenging version, I-RAVEN-X, to test the models' generalization capabilities under visual uncertainties. The results show a significant drop in accuracy for both LRMs when faced with the more complex tasks, indicating their struggle with longer reasoning rules and perceptual noise. In contrast, a neuro-symbolic model, ARLC, demonstrates robust performance, maintaining high accuracy even under challenging conditions."}, 'zh': {'title': 'å¤§å‹æ¨ç†æ¨¡å‹åœ¨ç±»æ¯”æ¨ç†ä¸­çš„æŒ‘æˆ˜ä¸æœºé‡', 'desc': 'æœ¬æ–‡é¦–æ¬¡è¯„ä¼°äº†ä¸¤ç§å…ˆè¿›çš„å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰ï¼ŒOpenAIçš„o3-miniå’ŒDeepSeek R1ï¼Œåœ¨ç±»æ¯”æ¨ç†æ–¹é¢çš„è¡¨ç°ï¼Œé‡ç‚¹å…³æ³¨åŸºäºRavenæ¸è¿›çŸ©é˜µçš„éè¯­è¨€äººç±»æ™ºå•†æµ‹è¯•ã€‚æˆ‘ä»¬ä½¿ç”¨I-RAVENæ•°æ®é›†åŠå…¶æ›´éš¾çš„æ‰©å±•ç‰ˆæœ¬I-RAVEN-Xè¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œåè€…æµ‹è¯•æ¨¡å‹å¯¹æ›´é•¿æ¨ç†è§„åˆ™å’Œå±æ€§å€¼èŒƒå›´çš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºäº†è¯„ä¼°è§†è§‰ä¸ç¡®å®šæ€§å¯¹è¿™äº›éè¯­è¨€ç±»æ¯”æ¨ç†æµ‹è¯•çš„å½±å“ï¼Œæˆ‘ä»¬æ‰©å±•äº†I-RAVEN-Xæ•°æ®é›†ï¼Œå¹¶é‡‡ç”¨äº†ä¸¤ç§ç­–ç•¥æ¥æ¨¡æ‹Ÿä¸å®Œç¾çš„è§†è§‰æ„ŸçŸ¥ã€‚ç»“æœæ˜¾ç¤ºï¼ŒOpenAIçš„o3-miniåœ¨I-RAVEN-Xä¸Šçš„ä»»åŠ¡å‡†ç¡®ç‡å¤§å¹…ä¸‹é™ï¼Œä»86.6%é™è‡³ä»…17.0%ï¼Œè€ŒARLCæ¨¡å‹åœ¨æ‰€æœ‰è¿™äº›åˆ†å¸ƒå¤–æµ‹è¯•ä¸­ä¿æŒäº†å¼ºå¤§çš„å‡†ç¡®æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.10620', 'title': 'From TOWER to SPIRE: Adding the Speech Modality to a Text-Only LLM', 'url': 'https://huggingface.co/papers/2503.10620', 'abstract': "Large language models (LLMs) have shown remarkable performance and generalization capabilities across multiple languages and tasks, making them very attractive targets for multi-modality integration (e.g., images or speech). In this work, we extend an existing LLM to the speech modality via speech discretization and continued pre-training. In particular, we are interested in multilingual LLMs, such as TOWER, as their pre-training setting allows us to treat discretized speech input as an additional translation language. The resulting open-source model, SPIRE, is able to transcribe and translate English speech input while maintaining TOWER's original performance on translation-related tasks, showcasing that discretized speech input integration as an additional language is feasible during LLM adaptation. We make our code and models available to the community.", 'score': 2, 'issue_id': 2739, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 13', 'zh': '3æœˆ13æ—¥'}, 'hash': '6fc0a5f12205a8bd', 'authors': ['Kshitij Ambilduke', 'Ben Peters', 'Sonal Sannigrahi', 'Anil Keshwani', 'Tsz Kin Lam', 'Bruno Martins', 'Marcely Zanon Boito', 'AndrÃ© F. T. Martins'], 'affiliations': ['ELLIS Unit Lisbon', 'INESC-ID', 'Instituto Superior TÃ©cnico, Universidade de Lisboa', 'Instituto de TelecomunicaÃ§Ãµes', 'NAVER LABS Europe', 'Paris-Saclay University', 'Sapienza University of Rome', 'Unbabel', 'University of Edinburgh'], 'pdf_title_img': 'assets/pdf/title_img/2503.10620.jpg', 'data': {'categories': ['#multilingual', '#machine_translation', '#open_source', '#multimodal', '#low_resource'], 'emoji': 'ğŸ—£ï¸', 'ru': {'title': 'Ğ Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ñ€ĞµÑ‡ĞµĞ²ÑƒÑ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ÑÑ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ñ€ĞµÑ‡ÑŒÑ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµÑ‡ĞµĞ²Ğ¾Ğ³Ğ¾ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ° Ğ¸ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ€ĞµÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ²Ğ²Ğ¾Ğ´ ĞºĞ°Ğº Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ ÑĞ·Ñ‹Ğº Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ TOWER. ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ SPIRE ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ñ‚Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºÑƒÑ Ñ€ĞµÑ‡ÑŒ, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ TOWER Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµÑ‡ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ²Ğ²Ğ¾Ğ´Ğ° ĞºĞ°Ğº Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ LLM.'}, 'en': {'title': 'Integrating Speech into Multilingual LLMs for Enhanced Performance', 'desc': 'This paper discusses the enhancement of large language models (LLMs) by integrating speech as a new modality. The authors focus on multilingual LLMs, specifically TOWER, and propose a method to convert speech into a format that the model can understand. They introduce a new model called SPIRE, which can transcribe and translate English speech while preserving the original capabilities of TOWER. The research demonstrates that incorporating discretized speech as an additional language is a viable approach for adapting LLMs, and the authors provide their code and models for public use.'}, 'zh': {'title': 'å°†è¯­éŸ³èå…¥å¤§å‹è¯­è¨€æ¨¡å‹çš„åˆ›æ–°ä¹‹è·¯', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šç§è¯­è¨€å’Œä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå…·æœ‰å¾ˆå¼ºçš„æ³›åŒ–èƒ½åŠ›ï¼Œé€‚åˆä¸å¤šæ¨¡æ€ï¼ˆå¦‚å›¾åƒæˆ–è¯­éŸ³ï¼‰ç»“åˆã€‚æœ¬æ–‡å°†ç°æœ‰çš„LLMæ‰©å±•åˆ°è¯­éŸ³æ¨¡æ€ï¼Œé€šè¿‡è¯­éŸ³ç¦»æ•£åŒ–å’ŒæŒç»­é¢„è®­ç»ƒæ¥å®ç°ã€‚æˆ‘ä»¬ç‰¹åˆ«å…³æ³¨å¤šè¯­è¨€LLMï¼Œä¾‹å¦‚TOWERï¼Œå› ä¸ºå®ƒçš„é¢„è®­ç»ƒè®¾ç½®å…è®¸æˆ‘ä»¬å°†ç¦»æ•£åŒ–çš„è¯­éŸ³è¾“å…¥è§†ä¸ºé¢å¤–çš„ç¿»è¯‘è¯­è¨€ã€‚æœ€ç»ˆç”Ÿæˆçš„å¼€æºæ¨¡å‹SPIREèƒ½å¤Ÿè½¬å½•å’Œç¿»è¯‘è‹±è¯­è¯­éŸ³è¾“å…¥ï¼ŒåŒæ—¶ä¿æŒTOWERåœ¨ç¿»è¯‘ç›¸å…³ä»»åŠ¡ä¸Šçš„åŸå§‹æ€§èƒ½ï¼Œè¯æ˜äº†åœ¨LLMé€‚åº”è¿‡ç¨‹ä¸­å°†ç¦»æ•£è¯­éŸ³è¾“å…¥ä½œä¸ºé¢å¤–è¯­è¨€çš„å¯è¡Œæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.10684', 'title': 'Open-World Skill Discovery from Unsegmented Demonstrations', 'url': 'https://huggingface.co/papers/2503.10684', 'abstract': 'Learning skills in open-world environments is essential for developing agents capable of handling a variety of tasks by combining basic skills. Online demonstration videos are typically long but unsegmented, making them difficult to segment and label with skill identifiers. Unlike existing methods that rely on sequence sampling or human labeling, we have developed a self-supervised learning-based approach to segment these long videos into a series of semantic-aware and skill-consistent segments. Drawing inspiration from human cognitive event segmentation theory, we introduce Skill Boundary Detection (SBD), an annotation-free temporal video segmentation algorithm. SBD detects skill boundaries in a video by leveraging prediction errors from a pretrained unconditional action-prediction model. This approach is based on the assumption that a significant increase in prediction error indicates a shift in the skill being executed. We evaluated our method in Minecraft, a rich open-world simulator with extensive gameplay videos available online. Our SBD-generated segments improved the average performance of conditioned policies by 63.7% and 52.1% on short-term atomic skill tasks, and their corresponding hierarchical agents by 11.3% and 20.8% on long-horizon tasks. Our method can leverage the diverse YouTube videos to train instruction-following agents. The project page can be found in https://craftjarvis.github.io/SkillDiscovery.', 'score': 2, 'issue_id': 2740, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 11', 'zh': '3æœˆ11æ—¥'}, 'hash': '004932028027606e', 'authors': ['Jingwen Deng', 'Zihao Wang', 'Shaofei Cai', 'Anji Liu', 'Yitao Liang'], 'affiliations': ['Peking University', 'University of California, Los Angeles'], 'pdf_title_img': 'assets/pdf/title_img/2503.10684.jpg', 'data': {'categories': ['#games', '#video', '#agents', '#open_source'], 'emoji': 'ğŸ®', 'ru': {'title': 'ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼ Ğ½Ğ°Ğ²Ñ‹ĞºĞ°Ğ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Skill Boundary Detection (SBD), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ±Ñ‹Ğ» Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ³Ñ€Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Minecraft Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ» Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑĞ»Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº Ğ¸ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². Ğ”Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ YouTube Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑÑ‰Ğ¸Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸.'}, 'en': {'title': 'Segmenting Skills for Smarter Agents', 'desc': 'This paper presents a novel approach for segmenting long, unstructured demonstration videos into meaningful skill segments using self-supervised learning. The method, called Skill Boundary Detection (SBD), identifies transitions between skills by analyzing prediction errors from a pretrained action-prediction model. By applying this technique, the authors demonstrate significant improvements in the performance of agents trained on these segmented skills in the Minecraft environment. This approach allows for the effective use of diverse online video content to enhance the training of instruction-following agents without the need for manual labeling.'}, 'zh': {'title': 'è‡ªç›‘ç£å­¦ä¹ åŠ©åŠ›æŠ€èƒ½è¾¹ç•Œæ£€æµ‹', 'desc': 'åœ¨å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­å­¦ä¹ æŠ€èƒ½å¯¹äºå¼€å‘èƒ½å¤Ÿå¤„ç†å¤šç§ä»»åŠ¡çš„æ™ºèƒ½ä½“è‡³å…³é‡è¦ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºè‡ªç›‘ç£å­¦ä¹ çš„æ–¹æ³•ï¼Œå¯ä»¥å°†é•¿è§†é¢‘åˆ†å‰²æˆä¸€ç³»åˆ—è¯­ä¹‰æ˜ç¡®ä¸”æŠ€èƒ½ä¸€è‡´çš„ç‰‡æ®µï¼Œè€Œæ— éœ€äººå·¥æ ‡æ³¨ã€‚è¯¥æ–¹æ³•é€šè¿‡æ£€æµ‹é¢„æµ‹è¯¯å·®æ¥è¯†åˆ«æŠ€èƒ½è¾¹ç•Œï¼Œå‡è®¾é¢„æµ‹è¯¯å·®çš„æ˜¾è‘—å¢åŠ è¡¨æ˜æŠ€èƒ½çš„è½¬å˜ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œè¿™ç§æ–¹æ³•åœ¨Minecraftä¸­æ˜¾è‘—æé«˜äº†æ¡ä»¶ç­–ç•¥å’Œå±‚æ¬¡ä»£ç†çš„è¡¨ç°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.05689', 'title': 'GoalFlow: Goal-Driven Flow Matching for Multimodal Trajectories\n  Generation in End-to-End Autonomous Driving', 'url': 'https://huggingface.co/papers/2503.05689', 'abstract': 'We propose GoalFlow, an end-to-end autonomous driving method for generating high-quality multimodal trajectories. In autonomous driving scenarios, there is rarely a single suitable trajectory. Recent methods have increasingly focused on modeling multimodal trajectory distributions. However, they suffer from trajectory selection complexity and reduced trajectory quality due to high trajectory divergence and inconsistencies between guidance and scene information. To address these issues, we introduce GoalFlow, a novel method that effectively constrains the generative process to produce high-quality, multimodal trajectories. To resolve the trajectory divergence problem inherent in diffusion-based methods, GoalFlow constrains the generated trajectories by introducing a goal point. GoalFlow establishes a novel scoring mechanism that selects the most appropriate goal point from the candidate points based on scene information. Furthermore, GoalFlow employs an efficient generative method, Flow Matching, to generate multimodal trajectories, and incorporates a refined scoring mechanism to select the optimal trajectory from the candidates. Our experimental results, validated on the NavsimDauner2024_navsim, demonstrate that GoalFlow achieves state-of-the-art performance, delivering robust multimodal trajectories for autonomous driving. GoalFlow achieved PDMS of 90.3, significantly surpassing other methods. Compared with other diffusion-policy-based methods, our approach requires only a single denoising step to obtain excellent performance. The code is available at https://github.com/YvanYin/GoalFlow.', 'score': 2, 'issue_id': 2731, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 7', 'zh': '3æœˆ7æ—¥'}, 'hash': 'eef61e2f2b4c0760', 'authors': ['Zebin Xing', 'Xingyu Zhang', 'Yang Hu', 'Bo Jiang', 'Tong He', 'Qian Zhang', 'Xiaoxiao Long', 'Wei Yin'], 'affiliations': ['Horizon Robotics', 'Huazhong University of Science & Technology', 'Nanjing University', 'School of Artificial Intelligence, University of Chinese Academy of Sciences', 'Shanghai AI Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2503.05689.jpg', 'data': {'categories': ['#optimization', '#diffusion', '#agents', '#multimodal'], 'emoji': 'ğŸš—', 'ru': {'title': 'GoalFlow: Ğ¢Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ', 'desc': 'GoalFlow - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹. ĞĞ½ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ñ€Ğ°ÑÑ…Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹, Ğ²Ğ²Ğ¾Ğ´Ñ Ñ†ĞµĞ»ĞµĞ²ÑƒÑ Ñ‚Ğ¾Ñ‡ĞºÑƒ Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ÑÑ‰ĞµĞ¹. GoalFlow Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Flow Matching Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ GoalFlow Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'GoalFlow: Driving the Future with High-Quality Multimodal Trajectories', 'desc': 'GoalFlow is a new method for generating high-quality multimodal trajectories in autonomous driving. It addresses the challenges of trajectory selection and quality by constraining the generative process with a goal point, which helps reduce trajectory divergence. The method uses a novel scoring mechanism to choose the best goal point based on the scene context, ensuring that the generated trajectories are relevant and effective. Experimental results show that GoalFlow outperforms existing methods, achieving state-of-the-art performance with fewer computational steps.'}, 'zh': {'title': 'GoalFlowï¼šé«˜è´¨é‡å¤šæ¨¡æ€è½¨è¿¹ç”Ÿæˆçš„åˆ›æ–°æ–¹æ³•', 'desc': 'æˆ‘ä»¬æå‡ºäº†GoalFlowï¼Œè¿™æ˜¯ä¸€ç§ç«¯åˆ°ç«¯çš„è‡ªåŠ¨é©¾é©¶æ–¹æ³•ï¼Œç”¨äºç”Ÿæˆé«˜è´¨é‡çš„å¤šæ¨¡æ€è½¨è¿¹ã€‚åœ¨è‡ªåŠ¨é©¾é©¶åœºæ™¯ä¸­ï¼Œé€šå¸¸æ²¡æœ‰å•ä¸€åˆé€‚çš„è½¨è¿¹ï¼Œæœ€è¿‘çš„æ–¹æ³•è¶Šæ¥è¶Šå…³æ³¨å¤šæ¨¡æ€è½¨è¿¹åˆ†å¸ƒçš„å»ºæ¨¡ã€‚ä¸ºäº†å…‹æœè½¨è¿¹é€‰æ‹©çš„å¤æ‚æ€§å’Œè½¨è¿¹è´¨é‡ä¸‹é™çš„é—®é¢˜ï¼ŒGoalFlowé€šè¿‡å¼•å…¥ç›®æ ‡ç‚¹æ¥æœ‰æ•ˆçº¦æŸç”Ÿæˆè¿‡ç¨‹ï¼Œä»è€Œç”Ÿæˆé«˜è´¨é‡çš„å¤šæ¨¡æ€è½¨è¿¹ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒGoalFlowåœ¨NavsimDauner2024_navsimä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæä¾›äº†ç¨³å¥çš„å¤šæ¨¡æ€è½¨è¿¹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.11629', 'title': 'TreeMeshGPT: Artistic Mesh Generation with Autoregressive Tree\n  Sequencing', 'url': 'https://huggingface.co/papers/2503.11629', 'abstract': 'We introduce TreeMeshGPT, an autoregressive Transformer designed to generate high-quality artistic meshes aligned with input point clouds. Instead of the conventional next-token prediction in autoregressive Transformer, we propose a novel Autoregressive Tree Sequencing where the next input token is retrieved from a dynamically growing tree structure that is built upon the triangle adjacency of faces within the mesh. Our sequencing enables the mesh to extend locally from the last generated triangular face at each step, and therefore reduces training difficulty and improves mesh quality. Our approach represents each triangular face with two tokens, achieving a compression rate of approximately 22% compared to the naive face tokenization. This efficient tokenization enables our model to generate highly detailed artistic meshes with strong point cloud conditioning, surpassing previous methods in both capacity and fidelity. Furthermore, our method generates mesh with strong normal orientation constraints, minimizing flipped normals commonly encountered in previous methods. Our experiments show that TreeMeshGPT enhances the mesh generation quality with refined details and normal orientation consistency.', 'score': 1, 'issue_id': 2744, 'pub_date': '2025-03-14', 'pub_date_card': {'ru': '14 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 14', 'zh': '3æœˆ14æ—¥'}, 'hash': '5f56461a339a86bb', 'authors': ['Stefan Lionar', 'Jiabin Liang', 'Gim Hee Lee'], 'affiliations': ['Garena', 'National University of Singapore', 'Sea AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2503.11629.jpg', 'data': {'categories': ['#games', '#3d', '#optimization', '#architecture'], 'emoji': 'ğŸŒ³', 'ru': {'title': 'Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… 3D-ÑĞµÑ‚Ğ¾Ğº Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ñ€ĞµĞ²Ğ¾Ğ²Ğ¸Ğ´Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸', 'desc': 'TreeMeshGPT - ÑÑ‚Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ñ…ÑƒĞ´Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… ÑĞµÑ‚Ğ¾Ğº, ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ±Ğ»Ğ°ĞºĞ°Ğ¼Ğ¸ Ñ‚Ğ¾Ñ‡ĞµĞº. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ´Ñ€ĞµĞ²Ğ¾Ğ²Ğ¸Ğ´Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ğ³Ğ´Ğµ ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½ Ğ²Ğ²Ğ¾Ğ´Ğ° Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ÑÑ Ğ¸Ğ· Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ€Ğ°ÑÑ‚ÑƒÑ‰ĞµĞ¹ Ğ´Ñ€ĞµĞ²Ğ¾Ğ²Ğ¸Ğ´Ğ½Ğ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹, Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ¼ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‚Ñ€ĞµÑƒĞ³Ğ¾Ğ»ÑŒĞ½Ğ¸ĞºĞ¾Ğ² Ğ² ÑĞµÑ‚ĞºĞµ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞµÑ‚ĞºĞµ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑÑ‚ÑŒÑÑ Ğ¾Ñ‚ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½ĞµĞ¹ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ñ‚Ñ€ĞµÑƒĞ³Ğ¾Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ³Ñ€Ğ°Ğ½Ğ¸ Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ ÑˆĞ°Ğ³Ğµ, Ñ‡Ñ‚Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑĞµÑ‚ĞºĞ¸. TreeMeshGPT Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ĞµĞ¼ĞºĞ¾ÑÑ‚Ğ¸ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞµÑ‚ĞºĞ¸ Ñ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»ĞµĞ¹.'}, 'en': {'title': 'Revolutionizing Mesh Generation with TreeMeshGPT', 'desc': 'TreeMeshGPT is a novel autoregressive Transformer model that generates artistic meshes from point clouds. It introduces a unique Autoregressive Tree Sequencing method, which builds a dynamic tree structure based on the adjacency of triangular faces, allowing for local mesh extension during generation. This approach not only simplifies the training process but also enhances the quality of the generated meshes by achieving a 22% compression rate through efficient tokenization of triangular faces. Additionally, TreeMeshGPT ensures better normal orientation, reducing the occurrence of flipped normals and improving overall mesh fidelity and detail compared to previous techniques.'}, 'zh': {'title': 'TreeMeshGPTï¼šé«˜è´¨é‡è‰ºæœ¯ç½‘æ ¼ç”Ÿæˆçš„æ–°æ–¹æ³•', 'desc': 'æˆ‘ä»¬ä»‹ç»äº†TreeMeshGPTï¼Œè¿™æ˜¯ä¸€ç§è‡ªå›å½’Transformerï¼Œæ—¨åœ¨ç”Ÿæˆä¸è¾“å…¥ç‚¹äº‘å¯¹é½çš„é«˜è´¨é‡è‰ºæœ¯ç½‘æ ¼ã€‚ä¸ä¼ ç»Ÿçš„è‡ªå›å½’Transformerçš„ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹ä¸åŒï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„è‡ªå›å½’æ ‘åºåˆ—åŒ–æ–¹æ³•ï¼Œé€šè¿‡åŠ¨æ€å¢é•¿çš„æ ‘ç»“æ„æ¥æ£€ç´¢ä¸‹ä¸€ä¸ªè¾“å…¥æ ‡è®°ã€‚æˆ‘ä»¬çš„åºåˆ—åŒ–æ–¹æ³•ä½¿å¾—ç½‘æ ¼èƒ½å¤Ÿåœ¨æ¯ä¸€æ­¥ä»æœ€åç”Ÿæˆçš„ä¸‰è§’é¢å±€éƒ¨æ‰©å±•ï¼Œä»è€Œé™ä½è®­ç»ƒéš¾åº¦å¹¶æé«˜ç½‘æ ¼è´¨é‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ¨¡å‹é€šè¿‡å°†æ¯ä¸ªä¸‰è§’é¢è¡¨ç¤ºä¸ºä¸¤ä¸ªæ ‡è®°ï¼Œå®ç°äº†çº¦22%çš„å‹ç¼©ç‡ï¼Œç”Ÿæˆçš„ç½‘æ ¼åœ¨ç»†èŠ‚å’Œæ³•çº¿æ–¹å‘ä¸€è‡´æ€§æ–¹é¢ä¼˜äºä»¥å¾€æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.08111', 'title': 'MaRI: Material Retrieval Integration across Domains', 'url': 'https://huggingface.co/papers/2503.08111', 'abstract': 'Accurate material retrieval is critical for creating realistic 3D assets. Existing methods rely on datasets that capture shape-invariant and lighting-varied representations of materials, which are scarce and face challenges due to limited diversity and inadequate real-world generalization. Most current approaches adopt traditional image search techniques. They fall short in capturing the unique properties of material spaces, leading to suboptimal performance in retrieval tasks. Addressing these challenges, we introduce MaRI, a framework designed to bridge the feature space gap between synthetic and real-world materials. MaRI constructs a shared embedding space that harmonizes visual and material attributes through a contrastive learning strategy by jointly training an image and a material encoder, bringing similar materials and images closer while separating dissimilar pairs within the feature space. To support this, we construct a comprehensive dataset comprising high-quality synthetic materials rendered with controlled shape variations and diverse lighting conditions, along with real-world materials processed and standardized using material transfer techniques. Extensive experiments demonstrate the superior performance, accuracy, and generalization capabilities of MaRI across diverse and complex material retrieval tasks, outperforming existing methods.', 'score': 1, 'issue_id': 2738, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 11', 'zh': '3æœˆ11æ—¥'}, 'hash': 'f3b97bb0031c6d57', 'authors': ['Jianhui Wang', 'Zhifei Yang', 'Yangfan He', 'Huixiong Zhang', 'Yuxuan Chen', 'Jingwei Huang'], 'affiliations': ['Fudan University', 'Peking University', 'Tencent Hunyuan3D', 'University of Electronic Science and Technology of China', 'University of Minnesota'], 'pdf_title_img': 'assets/pdf/title_img/2503.08111.jpg', 'data': {'categories': ['#synthetic', '#optimization', '#dataset', '#cv', '#3d'], 'emoji': 'ğŸ”', 'ru': {'title': 'MaRI: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¿Ğ¾Ğ¸ÑĞºĞµ 3D-Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MaRI - Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… 3D-Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ². ĞĞ½ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ¾Ğ±Ñ‰ĞµĞµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ¾Ğ±Ñ€Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ² Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ MaRI Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ².'}, 'en': {'title': 'Bridging the Gap in Material Retrieval with MaRI', 'desc': 'This paper presents MaRI, a novel framework aimed at improving material retrieval for realistic 3D asset creation. It addresses the limitations of existing methods that rely on traditional image search techniques, which often fail to capture the unique properties of materials. By utilizing a contrastive learning strategy, MaRI creates a shared embedding space that aligns visual and material attributes, enhancing the retrieval process. The framework is supported by a comprehensive dataset of synthetic and real-world materials, demonstrating superior performance in accuracy and generalization across various retrieval tasks.'}, 'zh': {'title': 'MaRIï¼šæå‡ææ–™æ£€ç´¢çš„æ™ºèƒ½æ¡†æ¶', 'desc': 'å‡†ç¡®çš„ææ–™æ£€ç´¢å¯¹äºåˆ›å»ºçœŸå®çš„3Dèµ„äº§è‡³å…³é‡è¦ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–äºæ•æ‰å½¢çŠ¶ä¸å˜å’Œå…‰ç…§å˜åŒ–çš„ææ–™è¡¨ç¤ºçš„æ•°æ®é›†ï¼Œè¿™äº›æ•°æ®é›†ç¨€ç¼ºä¸”é¢ä¸´å¤šæ ·æ€§ä¸è¶³å’Œç°å®ä¸–ç•Œæ³›åŒ–ä¸è‰¯çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†MaRIæ¡†æ¶ï¼Œæ—¨åœ¨å¼¥åˆåˆæˆææ–™å’ŒçœŸå®ææ–™ä¹‹é—´çš„ç‰¹å¾ç©ºé—´å·®è·ã€‚é€šè¿‡å¯¹æ¯”å­¦ä¹ ç­–ç•¥ï¼ŒMaRIæ„å»ºäº†ä¸€ä¸ªå…±äº«çš„åµŒå…¥ç©ºé—´ï¼Œä½¿å¾—ç›¸ä¼¼çš„ææ–™å’Œå›¾åƒåœ¨ç‰¹å¾ç©ºé—´ä¸­æ›´æ¥è¿‘ï¼ŒåŒæ—¶å°†ä¸ç›¸ä¼¼çš„å¯¹åˆ†å¼€ï¼Œä»è€Œæé«˜äº†ææ–™æ£€ç´¢çš„æ€§èƒ½å’Œå‡†ç¡®æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.09330', 'title': 'Group-robust Machine Unlearning', 'url': 'https://huggingface.co/papers/2503.09330', 'abstract': 'Machine unlearning is an emerging paradigm to remove the influence of specific training data (i.e., the forget set) from a model while preserving its knowledge of the rest of the data (i.e., the retain set). Previous approaches assume the forget data to be uniformly distributed from all training datapoints. However, if the data to unlearn is dominant in one group, we empirically show that performance for this group degrades, leading to fairness issues. This work tackles the overlooked problem of non-uniformly distributed forget sets, which we call group-robust machine unlearning, by presenting a simple, effective strategy that mitigates the performance loss in dominant groups via sample distribution reweighting. Moreover, we present MIU (Mutual Information-aware Machine Unlearning), the first approach for group robustness in approximate machine unlearning. MIU minimizes the mutual information between model features and group information, achieving unlearning while reducing performance degradation in the dominant group of the forget set. Additionally, MIU exploits sample distribution reweighting and mutual information calibration with the original model to preserve group robustness. We conduct experiments on three datasets and show that MIU outperforms standard methods, achieving unlearning without compromising model robustness. Source code available at https://github.com/tdemin16/group-robust_machine_unlearning.', 'score': 0, 'issue_id': 2739, 'pub_date': '2025-03-12', 'pub_date_card': {'ru': '12 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 12', 'zh': '3æœˆ12æ—¥'}, 'hash': '64a6c82be0db725d', 'authors': ['Thomas De Min', 'Subhankar Roy', 'StÃ©phane LathuiliÃ¨re', 'Elisa Ricci', 'Massimiliano Mancini'], 'affiliations': ['Fondazione Bruno Kessler', 'Inria Grenoble, Univ. Grenoble Alpes', 'LTCI, Telecom Paris, Institut Polytechnique de Paris', 'University of Trento'], 'pdf_title_img': 'assets/pdf/title_img/2503.09330.jpg', 'data': {'categories': ['#training', '#dataset', '#ethics', '#data'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ“Ñ€ÑƒĞ¿Ğ¿Ğ¾Ğ²Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğµ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ğ³Ğ´Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ· Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½ĞµÑ€Ğ°Ğ²Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ñ‹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ MIU, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ°Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¾ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ°Ñ…. MIU Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ²Ğ·Ğ²ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ Ğ¸ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºÑƒ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¾Ğ²Ğ¾Ğ¹ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ñ‚Ñ€ĞµÑ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ MIU Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ñ€Ğ°Ğ·Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· ÑƒÑ‰ĞµÑ€Ğ±Ğ° Ğ´Ğ»Ñ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Fair and Effective Machine Unlearning for Diverse Data Groups', 'desc': 'This paper introduces the concept of group-robust machine unlearning, which addresses the challenge of removing specific training data from a model while maintaining its performance across different groups. It highlights the issue of fairness when the data to be unlearned is not uniformly distributed, leading to performance degradation in dominant groups. The authors propose a novel method called MIU (Mutual Information-aware Machine Unlearning) that minimizes the mutual information between model features and group information, thus enhancing unlearning effectiveness. Through experiments on various datasets, MIU demonstrates superior performance compared to traditional methods, ensuring robust model performance even after unlearning.'}, 'zh': {'title': 'ç»„é²æ£’æ€§æœºå™¨é—å¿˜ï¼šå…¬å¹³æ€§ä¸æ€§èƒ½çš„å¹³è¡¡', 'desc': 'æœºå™¨é—å¿˜æ˜¯ä¸€ç§æ–°å…´çš„èŒƒå¼ï¼Œæ—¨åœ¨ä»æ¨¡å‹ä¸­å»é™¤ç‰¹å®šè®­ç»ƒæ•°æ®çš„å½±å“ï¼ŒåŒæ—¶ä¿ç•™å…¶å¯¹å…¶ä»–æ•°æ®çš„çŸ¥è¯†ã€‚ä»¥å¾€çš„æ–¹æ³•å‡è®¾é—å¿˜æ•°æ®å‡åŒ€åˆ†å¸ƒï¼Œä½†å¦‚æœè¦é—å¿˜çš„æ•°æ®åœ¨æŸä¸€ç»„ä¸­å ä¸»å¯¼åœ°ä½ï¼Œæ¨¡å‹åœ¨è¯¥ç»„çš„æ€§èƒ½ä¼šä¸‹é™ï¼Œå¯¼è‡´å…¬å¹³æ€§é—®é¢˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„ç­–ç•¥ï¼Œé€šè¿‡æ ·æœ¬åˆ†å¸ƒé‡åŠ æƒæ¥ç¼“è§£ä¸»å¯¼ç»„çš„æ€§èƒ½æŸå¤±ï¼Œè§£å†³äº†éå‡åŒ€åˆ†å¸ƒé—å¿˜é›†çš„é—®é¢˜ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†MIUï¼ˆäº’ä¿¡æ¯æ„ŸçŸ¥æœºå™¨é—å¿˜ï¼‰ï¼Œè¿™æ˜¯é¦–ä¸ªé’ˆå¯¹è¿‘ä¼¼æœºå™¨é—å¿˜çš„ç»„é²æ£’æ€§æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨å‡å°‘ä¸»å¯¼ç»„æ€§èƒ½ä¸‹é™çš„åŒæ—¶å®ç°é—å¿˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.13288', 'title': 'Ï†-Decoding: Adaptive Foresight Sampling for Balanced Inference-Time\n  Exploration and Exploitation', 'url': 'https://huggingface.co/papers/2503.13288', 'abstract': 'Inference-time optimization scales computation to derive deliberate reasoning steps for effective performance. While previous search-based strategies address the short-sightedness of auto-regressive generation, the vast search space leads to excessive exploration and insufficient exploitation. To strike an efficient balance to derive the optimal step, we frame the decoding strategy as foresight sampling, leveraging simulated future steps to obtain globally optimal step estimation. Built on it, we propose a novel decoding strategy, named phi-Decoding. To provide a precise and expressive estimation of step value, phi-Decoding approximates two distributions via foresight and clustering. Sampling from the joint distribution, the optimal steps can be selected for exploitation. To support adaptive computation allocation, we propose in-width and in-depth pruning strategies, featuring a light-weight solution to achieve inference efficiency. Extensive experiments across seven benchmarks show phi-Decoding outperforms strong baselines in both performance and efficiency. Additional analysis demonstrates its generalization across various LLMs and scalability across a wide range of computing budgets. The code will be released at https://github.com/xufangzhi/phi-Decoding, and the open-source PyPI package is coming soon.', 'score': 39, 'issue_id': 2805, 'pub_date': '2025-03-17', 'pub_date_card': {'ru': '17 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 17', 'zh': '3æœˆ17æ—¥'}, 'hash': '8a067ffdfadeb974', 'authors': ['Fangzhi Xu', 'Hang Yan', 'Chang Ma', 'Haiteng Zhao', 'Jun Liu', 'Qika Lin', 'Zhiyong Wu'], 'affiliations': ['National University of Singapore', 'Peking University', 'Shanghai AI Lab', 'The University of Hong Kong', 'Xian Jiaotong University'], 'pdf_title_img': 'assets/pdf/title_img/2503.13288.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#inference', '#open_source', '#training', '#architecture'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ²Ğ¸Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ phi-Decoding Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºÑƒ Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ¸Ğ´ĞµĞ½Ğ¸ĞµĞ¼, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¾Ñ†ĞµĞ½Ğ¸Ñ‚ÑŒ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑˆĞ°Ğ³Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… ÑˆĞ°Ğ³Ğ¾Ğ². phi-Decoding Ğ°Ğ¿Ğ¿Ñ€Ğ¾ĞºÑĞ¸Ğ¼Ğ¸Ñ€ÑƒĞµÑ‚ Ğ´Ğ²Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ²Ğ¸Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ ÑˆĞ°Ğ³Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¿Ñ€ÑƒĞ½Ğ¸Ğ½Ğ³Ğ° Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°.'}, 'en': {'title': 'Optimizing Inference with phi-Decoding: Balancing Exploration and Exploitation', 'desc': 'This paper introduces phi-Decoding, a new decoding strategy that optimizes inference-time computation in machine learning models. It addresses the limitations of previous search-based methods by balancing exploration and exploitation through foresight sampling, which simulates future steps for better decision-making. The approach uses clustering to approximate distributions, allowing for the selection of optimal steps during the decoding process. Extensive experiments demonstrate that phi-Decoding significantly improves both performance and efficiency across various benchmarks and large language models (LLMs).'}, 'zh': {'title': 'ä¼˜åŒ–æ¨ç†æ•ˆç‡çš„phi-Decodingç­–ç•¥', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è§£ç ç­–ç•¥ï¼Œç§°ä¸ºphi-Decodingï¼Œæ—¨åœ¨ä¼˜åŒ–æ¨ç†è¿‡ç¨‹ä¸­çš„è®¡ç®—æ•ˆç‡ã€‚é€šè¿‡å‰ç»é‡‡æ ·ï¼Œphi-Decodingèƒ½å¤Ÿåˆ©ç”¨æ¨¡æ‹Ÿçš„æœªæ¥æ­¥éª¤æ¥è·å¾—å…¨å±€æœ€ä¼˜çš„æ­¥éª¤ä¼°è®¡ï¼Œä»è€Œå¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨ã€‚è¯¥æ–¹æ³•é€šè¿‡è¿‘ä¼¼ä¸¤ä¸ªåˆ†å¸ƒæ¥æä¾›ç²¾ç¡®çš„æ­¥éª¤ä»·å€¼ä¼°è®¡ï¼Œå¹¶é€šè¿‡è”åˆåˆ†å¸ƒè¿›è¡Œé‡‡æ ·ä»¥é€‰æ‹©æœ€ä½³æ­¥éª¤ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œphi-Decodingåœ¨æ€§èƒ½å’Œæ•ˆç‡ä¸Šå‡ä¼˜äºç°æœ‰çš„å¼ºåŸºçº¿ï¼Œå¹¶ä¸”åœ¨ä¸åŒçš„å¤§å‹è¯­è¨€æ¨¡å‹å’Œè®¡ç®—é¢„ç®—ä¸‹å…·æœ‰è‰¯å¥½çš„å¯æ‰©å±•æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.15265', 'title': 'DeepMesh: Auto-Regressive Artist-mesh Creation with Reinforcement\n  Learning', 'url': 'https://huggingface.co/papers/2503.15265', 'abstract': 'Triangle meshes play a crucial role in 3D applications for efficient manipulation and rendering. While auto-regressive methods generate structured meshes by predicting discrete vertex tokens, they are often constrained by limited face counts and mesh incompleteness. To address these challenges, we propose DeepMesh, a framework that optimizes mesh generation through two key innovations: (1) an efficient pre-training strategy incorporating a novel tokenization algorithm, along with improvements in data curation and processing, and (2) the introduction of Reinforcement Learning (RL) into 3D mesh generation to achieve human preference alignment via Direct Preference Optimization (DPO). We design a scoring standard that combines human evaluation with 3D metrics to collect preference pairs for DPO, ensuring both visual appeal and geometric accuracy. Conditioned on point clouds and images, DeepMesh generates meshes with intricate details and precise topology, outperforming state-of-the-art methods in both precision and quality. Project page: https://zhaorw02.github.io/DeepMesh/', 'score': 34, 'issue_id': 2802, 'pub_date': '2025-03-19', 'pub_date_card': {'ru': '19 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 19', 'zh': '3æœˆ19æ—¥'}, 'hash': '37236f5315cc8aef', 'authors': ['Ruowen Zhao', 'Junliang Ye', 'Zhengyi Wang', 'Guangce Liu', 'Yiwen Chen', 'Yikai Wang', 'Jun Zhu'], 'affiliations': ['Nanyang Technological University', 'ShengShu', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.15265.jpg', 'data': {'categories': ['#optimization', '#data', '#alignment', '#rlhf', '#rl', '#3d'], 'emoji': 'ğŸ”·', 'ru': {'title': 'DeepMesh: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D-ÑĞµÑ‚Ğ¾Ğº Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹', 'desc': 'DeepMesh - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… ÑĞµÑ‚Ğ¾Ğº. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ½Ğ¾Ğ²Ñ‹Ğ¼ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ¼ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¾Ğ¹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. DeepMesh Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ğ½ĞµĞ´Ñ€ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ 3D-ÑĞµÑ‚Ğ¾Ğº, ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°, Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Direct Preference Optimization. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞµÑ‚ĞºĞ¸ Ñ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¾Ğ¹ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ³Ğ¸ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ±Ğ»Ğ°ĞºĞ¾Ğ² Ñ‚Ğ¾Ñ‡ĞµĞº Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ.'}, 'en': {'title': 'DeepMesh: Elevating 3D Mesh Generation with Human-Centric Learning', 'desc': 'This paper introduces DeepMesh, a new framework for generating 3D triangle meshes that enhances both quality and precision. It utilizes a unique pre-training strategy with an innovative tokenization method, improving how data is curated and processed. Additionally, it incorporates Reinforcement Learning to align mesh generation with human preferences through Direct Preference Optimization. By conditioning on point clouds and images, DeepMesh produces detailed and accurately structured meshes, surpassing existing methods in performance.'}, 'zh': {'title': 'DeepMeshï¼šä¼˜åŒ–3Dç½‘æ ¼ç”Ÿæˆçš„æ–°æ–¹æ³•', 'desc': 'ä¸‰è§’ç½‘æ ¼åœ¨3Dåº”ç”¨ä¸­è‡³å…³é‡è¦ï¼Œèƒ½å¤Ÿé«˜æ•ˆåœ°è¿›è¡Œæ“ä½œå’Œæ¸²æŸ“ã€‚ä¼ ç»Ÿçš„è‡ªå›å½’æ–¹æ³•é€šè¿‡é¢„æµ‹ç¦»æ•£çš„é¡¶ç‚¹æ ‡è®°ç”Ÿæˆç»“æ„åŒ–ç½‘æ ¼ï¼Œä½†å¸¸å¸¸å—åˆ°é¢æ•°é™åˆ¶å’Œç½‘æ ¼ä¸å®Œæ•´æ€§çš„å›°æ‰°ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†DeepMeshæ¡†æ¶ï¼Œé€šè¿‡ä¸¤é¡¹å…³é”®åˆ›æ–°æ¥ä¼˜åŒ–ç½‘æ ¼ç”Ÿæˆï¼šä¸€ç§é«˜æ•ˆçš„é¢„è®­ç»ƒç­–ç•¥å’Œå°†å¼ºåŒ–å­¦ä¹ å¼•å…¥3Dç½‘æ ¼ç”Ÿæˆã€‚DeepMeshèƒ½å¤Ÿç”Ÿæˆç»†èŠ‚ä¸°å¯Œã€æ‹“æ‰‘ç²¾ç¡®çš„ç½‘æ ¼ï¼Œä¸”åœ¨ç²¾åº¦å’Œè´¨é‡ä¸Šè¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.15485', 'title': 'TULIP: Towards Unified Language-Image Pretraining', 'url': 'https://huggingface.co/papers/2503.15485', 'abstract': 'Despite the recent success of image-text contrastive models like CLIP and SigLIP, these models often struggle with vision-centric tasks that demand high-fidelity image understanding, such as counting, depth estimation, and fine-grained object recognition. These models, by performing language alignment, tend to prioritize high-level semantics over visual understanding, weakening their image understanding. On the other hand, vision-focused models are great at processing visual information but struggle to understand language, limiting their flexibility for language-driven tasks. In this work, we introduce TULIP, an open-source, drop-in replacement for existing CLIP-like models. Our method leverages generative data augmentation, enhanced image-image and text-text contrastive learning, and image/text reconstruction regularization to learn fine-grained visual features while preserving global semantic alignment. Our approach, scaling to over 1B parameters, outperforms existing state-of-the-art (SOTA) models across multiple benchmarks, establishing a new SOTA zero-shot performance on ImageNet-1K, delivering up to a 2times enhancement over SigLIP on RxRx1 in linear probing for few-shot classification, and improving vision-language models, achieving over 3times higher scores than SigLIP on MMVP. Our code/checkpoints are available at https://tulip-berkeley.github.io', 'score': 32, 'issue_id': 2800, 'pub_date': '2025-03-19', 'pub_date_card': {'ru': '19 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 19', 'zh': '3æœˆ19æ—¥'}, 'hash': 'd4b870742a020d5a', 'authors': ['Zineng Tang', 'Long Lian', 'Seun Eisape', 'XuDong Wang', 'Roei Herzig', 'Adam Yala', 'Alane Suhr', 'Trevor Darrell', 'David M. Chan'], 'affiliations': ['University of California, Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2503.15485.jpg', 'data': {'categories': ['#optimization', '#multimodal', '#benchmark', '#architecture', '#open_source', '#cv'], 'emoji': 'ğŸŒ·', 'ru': {'title': 'TULIP: Ğ‘Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ·Ñ€ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ TULIP - Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°. TULIP ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğµ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼. TULIP Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°Ñ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ state-of-the-art Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… zero-shot ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¸ few-shot Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'TULIP: Bridging Vision and Language for Enhanced Image Understanding', 'desc': 'This paper presents TULIP, a new model designed to improve image understanding in tasks like counting and depth estimation, which are challenging for existing image-text contrastive models. TULIP enhances visual feature learning by using generative data augmentation and advanced contrastive learning techniques, while still maintaining a connection to high-level semantics. It scales effectively to over 1 billion parameters and demonstrates superior performance on various benchmarks, setting new records in zero-shot classification and few-shot tasks. The model aims to bridge the gap between vision-centric and language-centric approaches, providing a more flexible solution for vision-language tasks.'}, 'zh': {'title': 'TULIPï¼šæå‡å›¾åƒç†è§£çš„æ–°æ–¹æ³•', 'desc': 'å°½ç®¡åƒCLIPå’ŒSigLIPè¿™æ ·çš„å›¾åƒ-æ–‡æœ¬å¯¹æ¯”æ¨¡å‹å–å¾—äº†æˆåŠŸï¼Œä½†å®ƒä»¬åœ¨éœ€è¦é«˜ä¿çœŸå›¾åƒç†è§£çš„è§†è§‰ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ã€‚æœ¬æ–‡æå‡ºäº†TULIPï¼Œè¿™æ˜¯ä¸€ç§å¼€æºçš„æ›¿ä»£æ–¹æ¡ˆï¼Œæ—¨åœ¨é€šè¿‡ç”Ÿæˆæ•°æ®å¢å¼ºå’Œå¯¹æ¯”å­¦ä¹ æ¥æé«˜å›¾åƒç†è§£èƒ½åŠ›ã€‚TULIPèƒ½å¤Ÿå­¦ä¹ ç»†ç²’åº¦çš„è§†è§‰ç‰¹å¾ï¼ŒåŒæ—¶ä¿æŒå…¨å±€è¯­ä¹‰çš„ä¸€è‡´æ€§ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒTULIPåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ¨¡å‹ï¼Œæ˜¾è‘—æå‡äº†é›¶-shotæ€§èƒ½å’Œå°‘-shotåˆ†ç±»çš„æ•ˆæœã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.15475', 'title': 'Cube: A Roblox View of 3D Intelligence', 'url': 'https://huggingface.co/papers/2503.15475', 'abstract': 'Foundation models trained on vast amounts of data have demonstrated remarkable reasoning and generation capabilities in the domains of text, images, audio and video. Our goal at Roblox is to build such a foundation model for 3D intelligence, a model that can support developers in producing all aspects of a Roblox experience, from generating 3D objects and scenes to rigging characters for animation to producing programmatic scripts describing object behaviors. We discuss three key design requirements for such a 3D foundation model and then present our first step towards building such a model. We expect that 3D geometric shapes will be a core data type and describe our solution for 3D shape tokenizer. We show how our tokenization scheme can be used in applications for text-to-shape generation, shape-to-text generation and text-to-scene generation. We demonstrate how these applications can collaborate with existing large language models (LLMs) to perform scene analysis and reasoning. We conclude with a discussion outlining our path to building a fully unified foundation model for 3D intelligence.', 'score': 21, 'issue_id': 2800, 'pub_date': '2025-03-19', 'pub_date_card': {'ru': '19 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 19', 'zh': '3æœˆ19æ—¥'}, 'hash': '89037dc780448ff8', 'authors': ['Foundation AI Team', 'Kiran Bhat', 'Nishchaie Khanna', 'Karun Channa', 'Tinghui Zhou', 'Yiheng Zhu', 'Xiaoxia Sun', 'Charles Shang', 'Anirudh Sudarshan', 'Maurice Chu', 'Daiqing Li', 'Kangle Deng', 'Jean-Philippe Fauconnier', 'Tijmen Verhulsdonck', 'Maneesh Agrawala', 'Kayvon Fatahalian', 'Alexander Weiss', 'Christian Reiser', 'Ravi Kiran Chirravuri', 'Ravali Kandur', 'Alejandro Pelaez', 'Akash Garg', 'Michael Palleschi', 'Jessica Wang', 'Skylar Litz', 'Leon Liu', 'Anying Li', 'David Harmon', 'Derek Liu', 'Liangjun Feng', 'Denis Goupil', 'Lukas Kuczynski', 'Jihyun Yoon', 'Naveen Marri', 'Peiye Zhuang', 'Yinan Zhang', 'Brian Yin', 'Haomiao Jiang', 'Marcel van Workum', 'Thomas Lane', 'Bryce Erickson', 'Salil Pathare', 'Kyle Price', 'Anupam Singh', 'David Baszucki'], 'affiliations': ['Foundation AI team, Roblox', 'Roblox'], 'pdf_title_img': 'assets/pdf/title_img/2503.15475.jpg', 'data': {'categories': ['#games', '#multimodal', '#3d', '#reasoning'], 'emoji': 'ğŸ§Š', 'ru': {'title': '3D-Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚: Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¸Ñ€Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ 3D-Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ² Roblox. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€Ğ¸Ğ·Ğ²Ğ°Ğ½Ğ° Ğ¿Ğ¾Ğ¼Ğ¾Ñ‡ÑŒ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸ĞºĞ°Ğ¼ Ğ² ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ Ğ²ÑĞµÑ… Ğ°ÑĞ¿ĞµĞºÑ‚Ğ¾Ğ² Ğ¸Ğ³Ñ€Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¾Ğ¿Ñ‹Ñ‚Ğ°, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹ Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ñ‹Ğµ ÑĞºÑ€Ğ¸Ğ¿Ñ‚Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ 3D-Ñ„Ğ¾Ñ€Ğ¼ Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ĞµĞ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ„Ğ¾Ñ€Ğ¼ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°, Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸Ğ· Ñ„Ğ¾Ñ€Ğ¼ Ğ¸ ÑÑ†ĞµĞ½ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¾Ğ±ÑÑƒĞ¶Ğ´Ğ°ĞµÑ‚ÑÑ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¾ ÑÑ†ĞµĞ½Ğ°Ñ….'}, 'en': {'title': 'Building the Future of 3D Intelligence in Roblox', 'desc': 'This paper presents a vision for creating a foundation model specifically designed for 3D intelligence, which can assist developers in various aspects of creating Roblox experiences. The authors emphasize the importance of 3D geometric shapes as a fundamental data type and introduce a novel 3D shape tokenizer to facilitate this process. They explore applications such as text-to-shape and shape-to-text generation, demonstrating how these can work alongside large language models for enhanced scene analysis and reasoning. The paper outlines the initial steps taken towards building a comprehensive model that integrates these capabilities into a unified framework for 3D content creation.'}, 'zh': {'title': 'æ„å»º3Dæ™ºèƒ½çš„åŸºç¡€æ¨¡å‹', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†å¦‚ä½•ä¸º3Dæ™ºèƒ½æ„å»ºåŸºç¡€æ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿæ”¯æŒå¼€å‘è€…åœ¨Robloxå¹³å°ä¸Šç”Ÿæˆ3Då¯¹è±¡ã€åœºæ™¯å’ŒåŠ¨ç”»è§’è‰²ã€‚æˆ‘ä»¬æå‡ºäº†ä¸‰ä¸ªå…³é”®è®¾è®¡è¦æ±‚ï¼Œå¹¶ä»‹ç»äº†æ„å»º3Då½¢çŠ¶æ ‡è®°å™¨çš„åˆæ­¥æ­¥éª¤ã€‚æˆ‘ä»¬çš„æ ‡è®°åŒ–æ–¹æ¡ˆå¯ä»¥åº”ç”¨äºæ–‡æœ¬åˆ°å½¢çŠ¶ç”Ÿæˆã€å½¢çŠ¶åˆ°æ–‡æœ¬ç”Ÿæˆå’Œæ–‡æœ¬åˆ°åœºæ™¯ç”Ÿæˆç­‰ä»»åŠ¡ã€‚æœ€åï¼Œæˆ‘ä»¬è®¨è®ºäº†å¦‚ä½•ä¸ç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹åä½œï¼Œä»¥å®ç°åœºæ™¯åˆ†æå’Œæ¨ç†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.15417', 'title': 'Temporal Regularization Makes Your Video Generator Stronger', 'url': 'https://huggingface.co/papers/2503.15417', 'abstract': 'Temporal quality is a critical aspect of video generation, as it ensures consistent motion and realistic dynamics across frames. However, achieving high temporal coherence and diversity remains challenging. In this work, we explore temporal augmentation in video generation for the first time, and introduce FluxFlow for initial investigation, a strategy designed to enhance temporal quality. Operating at the data level, FluxFlow applies controlled temporal perturbations without requiring architectural modifications. Extensive experiments on UCF-101 and VBench benchmarks demonstrate that FluxFlow significantly improves temporal coherence and diversity across various video generation models, including U-Net, DiT, and AR-based architectures, while preserving spatial fidelity. These findings highlight the potential of temporal augmentation as a simple yet effective approach to advancing video generation quality.', 'score': 18, 'issue_id': 2801, 'pub_date': '2025-03-19', 'pub_date_card': {'ru': '19 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 19', 'zh': '3æœˆ19æ—¥'}, 'hash': '8eb262eda880d162', 'authors': ['Harold Haodong Chen', 'Haojian Huang', 'Xianfeng Wu', 'Yexin Liu', 'Yajing Bai', 'Wen-Jie Shu', 'Harry Yang', 'Ser-Nam Lim'], 'affiliations': ['Everlyn AI', 'HKU', 'HKUST', 'UCF'], 'pdf_title_img': 'assets/pdf/title_img/2503.15417.jpg', 'data': {'categories': ['#benchmark', '#video'], 'emoji': 'ğŸ¬', 'ru': {'title': 'FluxFlow: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ FluxFlow, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼ÑƒÑ‰ĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ FluxFlow Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞœĞµÑ‚Ğ¾Ğ´ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ»ĞµĞ³ĞºĞ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ Ğ² ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹.'}, 'en': {'title': 'Enhancing Video Generation with Temporal Augmentation', 'desc': 'This paper addresses the challenge of maintaining consistent motion and realistic dynamics in video generation, focusing on the aspect of temporal quality. It introduces a novel method called FluxFlow, which applies controlled temporal perturbations to enhance temporal coherence and diversity in generated videos. The approach operates at the data level, meaning it does not require changes to the underlying model architecture. Experimental results on standard benchmarks show that FluxFlow significantly improves the performance of various video generation models while maintaining high spatial fidelity.'}, 'zh': {'title': 'æå‡è§†é¢‘ç”Ÿæˆçš„æ—¶é—´è´¨é‡', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†è§†é¢‘ç”Ÿæˆä¸­çš„æ—¶é—´è´¨é‡é—®é¢˜ï¼Œå¼ºè°ƒäº†åœ¨å¸§ä¹‹é—´ä¿æŒä¸€è‡´è¿åŠ¨å’ŒçœŸå®åŠ¨æ€çš„é‡è¦æ€§ã€‚æˆ‘ä»¬é¦–æ¬¡å¼•å…¥äº†æ—¶é—´å¢å¼ºæŠ€æœ¯ï¼Œå¹¶æå‡ºäº†FluxFlowç­–ç•¥ï¼Œä»¥æé«˜è§†é¢‘ç”Ÿæˆçš„æ—¶é—´è´¨é‡ã€‚FluxFlowåœ¨æ•°æ®å±‚é¢è¿›è¡Œæ“ä½œï¼Œé€šè¿‡æ§åˆ¶æ—¶é—´æ‰°åŠ¨æ¥å¢å¼ºæ—¶é—´ä¸€è‡´æ€§å’Œå¤šæ ·æ€§ï¼Œè€Œæ— éœ€ä¿®æ”¹æ¨¡å‹æ¶æ„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFluxFlowåœ¨å¤šä¸ªè§†é¢‘ç”Ÿæˆæ¨¡å‹ä¸Šæ˜¾è‘—æ”¹å–„äº†æ—¶é—´ä¸€è‡´æ€§å’Œå¤šæ ·æ€§ï¼ŒåŒæ—¶ä¿æŒäº†ç©ºé—´ä¿çœŸåº¦ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.14868', 'title': 'Efficient Personalization of Quantized Diffusion Model without\n  Backpropagation', 'url': 'https://huggingface.co/papers/2503.14868', 'abstract': 'Diffusion models have shown remarkable performance in image synthesis, but they demand extensive computational and memory resources for training, fine-tuning and inference. Although advanced quantization techniques have successfully minimized memory usage for inference, training and fine-tuning these quantized models still require large memory possibly due to dequantization for accurate computation of gradients and/or backpropagation for gradient-based algorithms. However, memory-efficient fine-tuning is particularly desirable for applications such as personalization that often must be run on edge devices like mobile phones with private data. In this work, we address this challenge by quantizing a diffusion model with personalization via Textual Inversion and by leveraging a zeroth-order optimization on personalization tokens without dequantization so that it does not require gradient and activation storage for backpropagation that consumes considerable memory. Since a gradient estimation using zeroth-order optimization is quite noisy for a single or a few images in personalization, we propose to denoise the estimated gradient by projecting it onto a subspace that is constructed with the past history of the tokens, dubbed Subspace Gradient. In addition, we investigated the influence of text embedding in image generation, leading to our proposed time steps sampling, dubbed Partial Uniform Timestep Sampling for sampling with effective diffusion timesteps. Our method achieves comparable performance to prior methods in image and text alignment scores for personalizing Stable Diffusion with only forward passes while reducing training memory demand up to 8.2times.', 'score': 18, 'issue_id': 2802, 'pub_date': '2025-03-19', 'pub_date_card': {'ru': '19 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 19', 'zh': '3æœˆ19æ—¥'}, 'hash': '8555fb94242b412c', 'authors': ['Hoigi Seo', 'Wongi Jeong', 'Kyungryeol Lee', 'Se Young Chun'], 'affiliations': ['Dept. of Electrical and Computer Engineering, INMC & IPAI Seoul National University, Republic of Korea'], 'pdf_title_img': 'assets/pdf/title_img/2503.14868.jpg', 'data': {'categories': ['#optimization', '#diffusion', '#cv', '#training', '#inference'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞŸĞ°Ğ¼ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ´ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼: Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° ĞºÑ€Ğ°ĞµĞ²Ñ‹Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ…', 'desc': "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ½Ğ° ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ… Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ½ÑƒĞ»ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞ° Ğ´Ğ»Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¹. Ğ”Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ 'Subspace Gradient', Ğ¿Ñ€Ğ¾ĞµÑ†Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚ Ğ½Ğ° Ğ¿Ğ¾Ğ´Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾, Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ğ¾Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². Ğ¢Ğ°ĞºĞ¶Ğµ Ğ²Ğ²ĞµĞ´ĞµĞ½Ğ° Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ° 'Partial Uniform Timestep Sampling' Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸."}, 'en': {'title': 'Efficient Personalization of Diffusion Models with Subspace Gradient', 'desc': 'This paper presents a method to improve the efficiency of training diffusion models for image synthesis, particularly for personalization on edge devices. It introduces a quantization technique that allows for fine-tuning without the need for dequantization, thus saving memory during gradient computation. The authors propose a novel approach called Subspace Gradient to reduce noise in gradient estimation by utilizing historical data from personalization tokens. Additionally, they explore the impact of text embeddings on image generation, leading to a new sampling method that optimizes diffusion timesteps, achieving significant memory savings while maintaining performance.'}, 'zh': {'title': 'é«˜æ•ˆä¸ªæ€§åŒ–æ‰©æ•£æ¨¡å‹çš„å†…å­˜ä¼˜åŒ–', 'desc': 'æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒåˆæˆä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†è®­ç»ƒå’Œå¾®è°ƒéœ€è¦å¤§é‡è®¡ç®—å’Œå†…å­˜èµ„æºã€‚å°½ç®¡å…ˆè¿›çš„é‡åŒ–æŠ€æœ¯å¯ä»¥å‡å°‘æ¨ç†æ—¶çš„å†…å­˜ä½¿ç”¨ï¼Œä½†è®­ç»ƒè¿™äº›é‡åŒ–æ¨¡å‹ä»ç„¶éœ€è¦å¤§é‡å†…å­˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šè¿‡æ–‡æœ¬åæ¼”å¯¹æ‰©æ•£æ¨¡å‹è¿›è¡Œé‡åŒ–çš„æ–¹æ³•ï¼Œå¹¶åˆ©ç”¨é›¶é˜¶ä¼˜åŒ–åœ¨ä¸å»é‡åŒ–çš„æƒ…å†µä¸‹è¿›è¡Œä¸ªæ€§åŒ–å¾®è°ƒï¼Œä»è€Œå‡å°‘å†…å­˜æ¶ˆè€—ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å›¾åƒå’Œæ–‡æœ¬å¯¹é½å¾—åˆ†ä¸Šä¸ä¹‹å‰çš„æ–¹æ³•ç›¸å½“ï¼ŒåŒæ—¶å°†è®­ç»ƒå†…å­˜éœ€æ±‚é™ä½äº†å¤šè¾¾8.2å€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.11557', 'title': 'VERIFY: A Benchmark of Visual Explanation and Reasoning for\n  Investigating Multimodal Reasoning Fidelity', 'url': 'https://huggingface.co/papers/2503.11557', 'abstract': 'Visual reasoning is central to human cognition, enabling individuals to interpret and abstractly understand their environment. Although recent Multimodal Large Language Models (MLLMs) have demonstrated impressive performance across language and vision-language tasks, existing benchmarks primarily measure recognition-based skills and inadequately assess true visual reasoning capabilities. To bridge this critical gap, we introduce VERIFY, a benchmark explicitly designed to isolate and rigorously evaluate the visual reasoning capabilities of state-of-the-art MLLMs. VERIFY compels models to reason primarily from visual information, providing minimal textual context to reduce reliance on domain-specific knowledge and linguistic biases. Each problem is accompanied by a human-annotated reasoning path, making it the first to provide in-depth evaluation of model decision-making processes. Additionally, we propose novel metrics that assess visual reasoning fidelity beyond mere accuracy, highlighting critical imbalances in current model reasoning patterns. Our comprehensive benchmarking of leading MLLMs uncovers significant limitations, underscoring the need for a balanced and holistic approach to both perception and reasoning. For more teaser and testing, visit our project page (https://verify-eqh.pages.dev/).', 'score': 18, 'issue_id': 2812, 'pub_date': '2025-03-14', 'pub_date_card': {'ru': '14 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 14', 'zh': '3æœˆ14æ—¥'}, 'hash': '468daf59bbaf8821', 'authors': ['Jing Bi', 'Junjia Guo', 'Susan Liang', 'Guangyu Sun', 'Luchuan Song', 'Yunlong Tang', 'Jinxi He', 'Jiarui Wu', 'Ali Vosoughi', 'Chen Chen', 'Chenliang Xu'], 'affiliations': ['University of Central Florida', 'University of Rochester'], 'pdf_title_img': 'assets/pdf/title_img/2503.11557.jpg', 'data': {'categories': ['#cv', '#interpretability', '#benchmark', '#multimodal', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'VERIFY: ĞĞ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ˜Ğ˜', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº VERIFY Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM). Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ², VERIFY Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸, Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿ÑƒÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ. Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ñ… MLLM Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ VERIFY Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ² Ğ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑÑ… Ğº Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'VERIFY: Elevating Visual Reasoning in MLLMs', 'desc': 'This paper introduces VERIFY, a new benchmark aimed at evaluating the visual reasoning abilities of Multimodal Large Language Models (MLLMs). Unlike existing benchmarks that focus on recognition tasks, VERIFY emphasizes reasoning from visual data with minimal textual support, reducing biases from language. Each task includes a human-annotated reasoning path, allowing for a deeper understanding of how models make decisions. The study also presents new metrics to assess visual reasoning fidelity, revealing significant limitations in current MLLMs and advocating for a more balanced approach to perception and reasoning.'}, 'zh': {'title': 'è§†è§‰æ¨ç†èƒ½åŠ›çš„å…¨æ–°è¯„ä¼°', 'desc': 'è§†è§‰æ¨ç†æ˜¯äººç±»è®¤çŸ¥çš„é‡è¦éƒ¨åˆ†ï¼Œä½¿äººä»¬èƒ½å¤Ÿç†è§£å’ŒæŠ½è±¡åœ°ç†è§£ç¯å¢ƒã€‚å°½ç®¡æœ€è¿‘çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è¯­è¨€å’Œè§†è§‰è¯­è¨€ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†ç°æœ‰åŸºå‡†ä¸»è¦æµ‹é‡è¯†åˆ«èƒ½åŠ›ï¼Œæœªèƒ½å……åˆ†è¯„ä¼°çœŸæ­£çš„è§†è§‰æ¨ç†èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†VERIFYåŸºå‡†ï¼Œä¸“é—¨è®¾è®¡ç”¨äºä¸¥æ ¼è¯„ä¼°æœ€å…ˆè¿›çš„MLLMçš„è§†è§‰æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡æä¾›æœ€å°çš„æ–‡æœ¬ä¸Šä¸‹æ–‡ï¼ŒVERIFYä¿ƒä½¿æ¨¡å‹ä¸»è¦ä¾èµ–è§†è§‰ä¿¡æ¯è¿›è¡Œæ¨ç†ï¼Œä»è€Œå‡å°‘å¯¹ç‰¹å®šé¢†åŸŸçŸ¥è¯†å’Œè¯­è¨€åè§çš„ä¾èµ–ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.15354', 'title': 'Optimizing Decomposition for Optimal Claim Verification', 'url': 'https://huggingface.co/papers/2503.15354', 'abstract': 'Current research on the Decompose-Then-Verify paradigm for evaluating the factuality of long-form text typically treats decomposition and verification in isolation, overlooking their interactions and potential misalignment. We find that existing decomposition policies, typically hand-crafted demonstrations, do not align well with downstream verifiers in terms of atomicity -- a novel metric quantifying information density -- leading to suboptimal verification results. We formulate finding the optimal decomposition policy for optimal verification as a bilevel optimization problem. To approximate a solution for this strongly NP-hard problem, we propose dynamic decomposition, a reinforcement learning framework that leverages verifier feedback to learn a policy for dynamically decomposing claims to verifier-preferred atomicity. Experimental results show that dynamic decomposition outperforms existing decomposition policies, improving verification confidence by 0.07 and accuracy by 0.12 (on a 0-1 scale) on average across varying verifiers, datasets, and atomcities of input claims.', 'score': 15, 'issue_id': 2811, 'pub_date': '2025-03-19', 'pub_date_card': {'ru': '19 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 19', 'zh': '3æœˆ19æ—¥'}, 'hash': '1b88801be56f7c8f', 'authors': ['Yining Lu', 'Noah Ziems', 'Hy Dang', 'Meng Jiang'], 'affiliations': ['University of Notre Dame, South Bend, IN'], 'pdf_title_img': 'assets/pdf/title_img/2503.15354.jpg', 'data': {'categories': ['#long_context', '#rlhf', '#optimization', '#rl', '#benchmark', '#hallucinations'], 'emoji': 'ğŸ§©', 'ru': {'title': 'Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Decompose-Then-Verify Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ¿Ğ»Ğ¾Ñ…Ğ¾ ÑĞ¾Ğ³Ğ»Ğ°ÑÑƒÑÑ‚ÑÑ Ñ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾ Ğ°Ñ‚Ğ¾Ğ¼Ğ°Ñ€Ğ½Ğ¾ÑÑ‚Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ½ĞµĞ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°Ğ¼ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸. ĞĞ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ Ğ¾Ñ‚ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹, ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ² ÑÑ€ĞµĞ´Ğ½ĞµĞ¼ Ğ½Ğ° 0.12.'}, 'en': {'title': 'Optimizing Decomposition for Better Verification in Text Factuality', 'desc': 'This paper addresses the challenges in the Decompose-Then-Verify approach for assessing the factuality of long-form text. It highlights that traditional decomposition methods do not effectively align with verification processes, particularly in terms of atomicity, which measures the density of information. The authors propose a bilevel optimization framework to find the best decomposition policy that enhances verification outcomes. They introduce a reinforcement learning method called dynamic decomposition, which adapts based on verifier feedback, resulting in improved verification confidence and accuracy across different scenarios.'}, 'zh': {'title': 'åŠ¨æ€åˆ†è§£ï¼šæå‡é•¿æ–‡æœ¬éªŒè¯çš„æœ‰æ•ˆæ€§', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†åœ¨è¯„ä¼°é•¿æ–‡æœ¬äº‹å®æ€§æ—¶ï¼Œåˆ†è§£-éªŒè¯èŒƒå¼çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬å‘ç°ç°æœ‰çš„åˆ†è§£ç­–ç•¥ä¸ä¸‹æ¸¸éªŒè¯å™¨ä¹‹é—´å­˜åœ¨ä¸ä¸€è‡´ï¼Œå¯¼è‡´éªŒè¯ç»“æœä¸ç†æƒ³ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å°†å¯»æ‰¾æœ€ä½³åˆ†è§£ç­–ç•¥è§†ä¸ºä¸€ä¸ªåŒå±‚ä¼˜åŒ–é—®é¢˜ï¼Œå¹¶æå‡ºäº†ä¸€ç§åŠ¨æ€åˆ†è§£çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒåŠ¨æ€åˆ†è§£åœ¨ä¸åŒéªŒè¯å™¨å’Œæ•°æ®é›†ä¸Šæ˜¾è‘—æé«˜äº†éªŒè¯ä¿¡å¿ƒå’Œå‡†ç¡®æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.14891', 'title': 'MetaLadder: Ascending Mathematical Solution Quality via\n  Analogical-Problem Reasoning Transfer', 'url': 'https://huggingface.co/papers/2503.14891', 'abstract': 'Large Language Models (LLMs) have demonstrated promising capabilities in solving mathematical reasoning tasks, leveraging Chain-of-Thought (CoT) data as a vital component in guiding answer generation. Current paradigms typically generate CoT and answers directly for a given problem, diverging from human problem-solving strategies to some extent. Humans often solve problems by recalling analogous cases and leveraging their solutions to reason about the current task. Inspired by this cognitive process, we propose MetaLadder, a novel framework that explicitly prompts LLMs to recall and reflect on meta-problems, those structurally or semantically analogous problems, alongside their CoT solutions before addressing the target problem. Additionally, we introduce a problem-restating mechanism to enhance the model\'s comprehension of the target problem by regenerating the original question, which further improves reasoning accuracy. Therefore, the model can achieve reasoning transfer from analogical problems, mimicking human-like "learning from examples" and generalization abilities. Extensive experiments on mathematical benchmarks demonstrate that our MetaLadder significantly boosts LLMs\' problem-solving accuracy, largely outperforming standard CoT-based methods (10.3\\% accuracy gain) and other methods. Our code and data has been released at https://github.com/LHL3341/MetaLadder.', 'score': 15, 'issue_id': 2811, 'pub_date': '2025-03-19', 'pub_date_card': {'ru': '19 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 19', 'zh': '3æœˆ19æ—¥'}, 'hash': '0d8dbba5f4b7283d', 'authors': ['Honglin Lin', 'Zhuoshi Pan', 'Yu Li', 'Qizhi Pei', 'Xin Gao', 'Mengzhang Cai', 'Conghui He', 'Lijun Wu'], 'affiliations': ['Renmin University of China', 'Shanghai AI Laboratory', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.14891.jpg', 'data': {'categories': ['#transfer_learning', '#math', '#reasoning', '#training', '#benchmark'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'MetaLadder: ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ˜Ğ˜ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¿Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ğ¸ Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¼', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ MetaLadder Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ¸Ğ¼Ğ¸Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğº Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ñ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ¸Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. MetaLadder Ğ¿Ğ¾Ğ±ÑƒĞ¶Ğ´Ğ°ĞµÑ‚ LLM Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ñ‚ÑŒ Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼ĞµÑ‚Ğ°-Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¸ Ğ¸Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿ĞµÑ€ĞµĞ´ Ñ‚ĞµĞ¼, ĞºĞ°Ğº Ğ¿Ñ€Ğ¸ÑÑ‚ÑƒĞ¿Ğ¸Ñ‚ÑŒ Ğº Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'MetaLadder: Enhancing LLMs with Analogical Reasoning for Better Problem Solving', 'desc': "This paper introduces MetaLadder, a new framework designed to improve the mathematical reasoning abilities of Large Language Models (LLMs). It emphasizes the importance of recalling and reflecting on similar past problems, known as meta-problems, to enhance the model's problem-solving process. By incorporating a problem-restating mechanism, the framework helps the model better understand the target problem, leading to improved reasoning accuracy. Experimental results show that MetaLadder significantly outperforms traditional Chain-of-Thought methods, achieving a notable increase in accuracy on mathematical tasks."}, 'zh': {'title': 'å€Ÿé‰´ç±»æ¯”é—®é¢˜ï¼Œæå‡æ¨ç†èƒ½åŠ›', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è§£å†³æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­å±•ç°äº†è‰¯å¥½çš„èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åˆ©ç”¨é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ•°æ®æ¥æŒ‡å¯¼ç­”æ¡ˆç”Ÿæˆã€‚å½“å‰çš„æ–¹æ³•é€šå¸¸ç›´æ¥ä¸ºç»™å®šé—®é¢˜ç”ŸæˆCoTå’Œç­”æ¡ˆï¼Œè¿™ä¸äººç±»çš„è§£é¢˜ç­–ç•¥æœ‰æ‰€ä¸åŒã€‚äººç±»é€šå¸¸é€šè¿‡å›å¿†ç±»ä¼¼æ¡ˆä¾‹åŠå…¶è§£å†³æ–¹æ¡ˆæ¥æ¨ç†å½“å‰ä»»åŠ¡ã€‚æˆ‘ä»¬æå‡ºçš„MetaLadderæ¡†æ¶ï¼Œæ˜ç¡®å¼•å¯¼LLMså›å¿†å’Œåæ€ç»“æ„æˆ–è¯­ä¹‰ä¸Šç›¸ä¼¼çš„å…ƒé—®é¢˜åŠå…¶CoTè§£å†³æ–¹æ¡ˆï¼Œä»è€Œæé«˜æ¨ç†å‡†ç¡®æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.12532', 'title': 'STEVE: AStep Verification Pipeline for Computer-use Agent Training', 'url': 'https://huggingface.co/papers/2503.12532', 'abstract': 'Developing AI agents to autonomously manipulate graphical user interfaces is a long challenging task. Recent advances in data scaling law inspire us to train computer-use agents with a scaled instruction set, yet using behavior cloning to train agents still requires immense high-quality trajectories. To meet the scalability need, we designed STEVE, a step verification pipeline for computer-use agent training. First, we establish a large instruction set for computer-use agents and collect trajectory data with some suboptimal agents. GPT-4o is used to verify the correctness of each step in the trajectories based on the screens before and after the action execution, assigning each step with a binary label. Last, we adopt the Kahneman and Tversky Optimization to optimize the agent from the binary stepwise labels. Extensive experiments manifest that our agent outperforms supervised finetuning by leveraging both positive and negative actions within a trajectory. Also, STEVE enables us to train a 7B vision-language model as a computer-use agent, achieving leading performance in the challenging live desktop environment WinAgentArena with great efficiency at a reduced cost. Code and data: https://github.com/FanbinLu/STEVE.', 'score': 12, 'issue_id': 2800, 'pub_date': '2025-03-16', 'pub_date_card': {'ru': '16 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 16', 'zh': '3æœˆ16æ—¥'}, 'hash': '185728a70d3b80d0', 'authors': ['Fanbin Lu', 'Zhisheng Zhong', 'Ziqin Wei', 'Shu Liu', 'Chi-Wing Fu', 'Jiaya Jia'], 'affiliations': ['CUHK', 'HKUST', 'SmartMore'], 'pdf_title_img': 'assets/pdf/title_img/2503.12532.jpg', 'data': {'categories': ['#optimization', '#games', '#agents', '#training', '#cv'], 'emoji': 'ğŸ–¥ï¸', 'ru': {'title': 'STEVE: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ñ‹Ğ¼ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ STEVE - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ¸ ÑÑƒĞ±Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ GPT-4. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¸Ğ½Ğ°Ñ€Ğ½Ñ‹Ñ… Ğ¾Ñ†ĞµĞ½Ğ¾Ğº ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ÑˆĞ°Ğ³Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ĞšĞ°Ğ½ĞµĞ¼Ğ°Ğ½Ğ°-Ğ¢Ğ²ĞµÑ€ÑĞºĞ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ 7 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¹ ÑÑ€ĞµĞ´Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‡ĞµĞ³Ğ¾ ÑÑ‚Ğ¾Ğ»Ğ°.'}, 'en': {'title': 'STEVE: Optimizing AI Agents for GUI Manipulation Efficiently', 'desc': "This paper presents STEVE, a novel step verification pipeline designed to enhance the training of AI agents for manipulating graphical user interfaces. By utilizing a large instruction set and collecting trajectory data from suboptimal agents, STEVE verifies the correctness of each action using GPT-4o, which labels steps as correct or incorrect. The approach incorporates Kahneman and Tversky Optimization to refine the agent's learning process based on these binary labels, allowing the agent to learn from both successful and unsuccessful actions. The results demonstrate that STEVE significantly improves performance in complex environments like WinAgentArena while being more cost-effective than traditional supervised finetuning methods."}, 'zh': {'title': 'æ™ºèƒ½ä»£ç†è®­ç»ƒçš„æ–°çªç ´ï¼šSTEVE', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºSTEVEçš„æ­¥éª¤éªŒè¯ç®¡é“ï¼Œç”¨äºè®­ç»ƒè®¡ç®—æœºä½¿ç”¨ä»£ç†ã€‚æˆ‘ä»¬é¦–å…ˆå»ºç«‹äº†ä¸€ä¸ªå¤§å‹æŒ‡ä»¤é›†ï¼Œå¹¶æ”¶é›†äº†ä¸€äº›æ¬¡ä¼˜ä»£ç†çš„è½¨è¿¹æ•°æ®ã€‚é€šè¿‡ä½¿ç”¨GPT-4oéªŒè¯æ¯ä¸ªæ­¥éª¤çš„æ­£ç¡®æ€§ï¼Œå¹¶ä¸ºæ¯ä¸ªæ­¥éª¤åˆ†é…äºŒå…ƒæ ‡ç­¾ï¼Œæœ€åé‡‡ç”¨å¡å°¼æ›¼å’Œç‰¹æ²ƒæ–¯ä¼˜åŒ–æ–¹æ³•æ¥ä¼˜åŒ–ä»£ç†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ä»£ç†åœ¨å¤æ‚çš„æ¡Œé¢ç¯å¢ƒä¸­è¡¨ç°ä¼˜äºä¼ ç»Ÿçš„ç›‘ç£å¾®è°ƒæ–¹æ³•ï¼Œä¸”è®­ç»ƒæ•ˆç‡é«˜ã€æˆæœ¬ä½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.15264', 'title': 'LEGION: Learning to Ground and Explain for Synthetic Image Detection', 'url': 'https://huggingface.co/papers/2503.15264', 'abstract': 'The rapid advancements in generative technology have emerged as a double-edged sword. While offering powerful tools that enhance convenience, they also pose significant social concerns. As defenders, current synthetic image detection methods often lack artifact-level textual interpretability and are overly focused on image manipulation detection, and current datasets usually suffer from outdated generators and a lack of fine-grained annotations. In this paper, we introduce SynthScars, a high-quality and diverse dataset consisting of 12,236 fully synthetic images with human-expert annotations. It features 4 distinct image content types, 3 categories of artifacts, and fine-grained annotations covering pixel-level segmentation, detailed textual explanations, and artifact category labels. Furthermore, we propose LEGION (LEarning to Ground and explain for Synthetic Image detectiON), a multimodal large language model (MLLM)-based image forgery analysis framework that integrates artifact detection, segmentation, and explanation. Building upon this capability, we further explore LEGION as a controller, integrating it into image refinement pipelines to guide the generation of higher-quality and more realistic images. Extensive experiments show that LEGION outperforms existing methods across multiple benchmarks, particularly surpassing the second-best traditional expert on SynthScars by 3.31% in mIoU and 7.75% in F1 score. Moreover, the refined images generated under its guidance exhibit stronger alignment with human preferences. The code, model, and dataset will be released.', 'score': 8, 'issue_id': 2805, 'pub_date': '2025-03-19', 'pub_date_card': {'ru': '19 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 19', 'zh': '3æœˆ19æ—¥'}, 'hash': '4fb3a12e4b0a70e9', 'authors': ['Hengrui Kang', 'Siwei Wen', 'Zichen Wen', 'Junyan Ye', 'Weijia Li', 'Peilin Feng', 'Baichuan Zhou', 'Bin Wang', 'Dahua Lin', 'Linfeng Zhang', 'Conghui He'], 'affiliations': ['Beihang University', 'SenseTime Research', 'Shanghai Artificial Intelligence Laboratory', 'Shanghai Jiao Tong University', 'Sun Yat-Sen University'], 'pdf_title_img': 'assets/pdf/title_img/2503.15264.jpg', 'data': {'categories': ['#benchmark', '#interpretability', '#synthetic', '#alignment', '#cv', '#multimodal', '#dataset'], 'emoji': 'ğŸ”', 'ru': {'title': 'LEGION: ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… SynthScars, ÑĞ¾ÑÑ‚Ğ¾ÑÑ‰Ğ¸Ğ¹ Ğ¸Ğ· 12,236 Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ LEGION - Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¿Ğ¾Ğ´Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. LEGION Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸ÑĞ¼, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ğ¾Ğ², ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ğµ. ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Enhancing Synthetic Image Detection with LEGION and SynthScars', 'desc': 'This paper addresses the challenges in detecting synthetic images, which have become increasingly prevalent due to advancements in generative technology. It introduces SynthScars, a comprehensive dataset of 12,236 synthetic images with detailed annotations, including pixel-level segmentation and artifact categories. The authors propose LEGION, a multimodal large language model framework that enhances synthetic image detection by integrating artifact detection, segmentation, and explanation. Experimental results demonstrate that LEGION significantly outperforms existing methods, leading to improved image quality and alignment with human preferences.'}, 'zh': {'title': 'æå‡åˆæˆå›¾åƒæ£€æµ‹çš„æ™ºèƒ½åŒ–ä¸ç²¾ç¡®åº¦', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†SynthScarsï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«12,236å¼ å®Œå…¨åˆæˆå›¾åƒçš„é«˜è´¨é‡å¤šæ ·åŒ–æ•°æ®é›†ï¼Œé…æœ‰äººå·¥ä¸“å®¶æ³¨é‡Šã€‚è¯¥æ•°æ®é›†æ¶µç›–äº†å››ç§ä¸åŒçš„å›¾åƒå†…å®¹ç±»å‹å’Œä¸‰ç±»ä¼ªå½±ï¼Œæä¾›äº†åƒç´ çº§åˆ†å‰²ã€è¯¦ç»†æ–‡æœ¬è§£é‡Šå’Œä¼ªå½±ç±»åˆ«æ ‡ç­¾çš„ç»†ç²’åº¦æ³¨é‡Šã€‚æˆ‘ä»¬è¿˜æå‡ºäº†LEGIONï¼Œä¸€ä¸ªåŸºäºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„å›¾åƒä¼ªé€ åˆ†ææ¡†æ¶ï¼Œèƒ½å¤Ÿæ•´åˆä¼ªå½±æ£€æµ‹ã€åˆ†å‰²å’Œè§£é‡ŠåŠŸèƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLEGIONåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œç”Ÿæˆçš„å›¾åƒæ›´ç¬¦åˆäººç±»åå¥½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.14505', 'title': 'MusicInfuser: Making Video Diffusion Listen and Dance', 'url': 'https://huggingface.co/papers/2503.14505', 'abstract': 'We introduce MusicInfuser, an approach for generating high-quality dance videos that are synchronized to a specified music track. Rather than attempting to design and train a new multimodal audio-video model, we show how existing video diffusion models can be adapted to align with musical inputs by introducing lightweight music-video cross-attention and a low-rank adapter. Unlike prior work requiring motion capture data, our approach fine-tunes only on dance videos. MusicInfuser achieves high-quality music-driven video generation while preserving the flexibility and generative capabilities of the underlying models. We introduce an evaluation framework using Video-LLMs to assess multiple dimensions of dance generation quality. The project page and code are available at https://susunghong.github.io/MusicInfuser.', 'score': 8, 'issue_id': 2801, 'pub_date': '2025-03-18', 'pub_date_card': {'ru': '18 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 18', 'zh': '3æœˆ18æ—¥'}, 'hash': '61ef56ac402491c0', 'authors': ['Susung Hong', 'Ira Kemelmacher-Shlizerman', 'Brian Curless', 'Steven M. Seitz'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2503.14505.jpg', 'data': {'categories': ['#benchmark', '#open_source', '#diffusion', '#multimodal', '#video'], 'emoji': 'ğŸ’ƒ', 'ru': {'title': 'Ğ¢Ğ°Ğ½Ñ†ÑƒĞ¹ Ğ¿Ğ¾Ğ´ Ğ¼ÑƒĞ·Ñ‹ĞºÑƒ: Ğ˜Ğ˜-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ñ€Ğ¸Ñ‚Ğ¼Ğµ Ğ¼ĞµĞ»Ğ¾Ğ´Ğ¸Ğ¸', 'desc': 'MusicInfuser - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ñ‚Ğ°Ğ½Ñ†ĞµĞ²Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾, ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¼ÑƒĞ·Ñ‹ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ´Ğ¾Ñ€Ğ¾Ğ¶ĞºĞ¾Ğ¹. ĞĞ½ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ Ğ¼ÑƒĞ·Ñ‹ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ğ¾Ğµ ĞºÑ€Ğ¾ÑÑ-Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼ÑƒĞ·Ñ‹ĞºĞ¾Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€Ğ°Ğ½Ğ³Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‚, MusicInfuser Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° Ñ‚Ğ°Ğ½Ñ†ĞµĞ²Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ”Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ğ°Ğ½Ñ†ĞµĞ² Ğ¿Ğ¾ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ²Ğ¸Ğ´ĞµĞ¾-LLM.'}, 'en': {'title': 'Syncing Dance with Music: Introducing MusicInfuser', 'desc': "MusicInfuser is a novel method for creating high-quality dance videos that match a chosen music track. It leverages existing video diffusion models by incorporating music-video cross-attention and a low-rank adapter, rather than building a new model from scratch. This approach allows for fine-tuning on dance videos without the need for motion capture data, enhancing the model's efficiency. Additionally, MusicInfuser includes a new evaluation framework using Video-LLMs to measure various aspects of dance generation quality."}, 'zh': {'title': 'éŸ³ä¹ä¸èˆè¹ˆçš„å®Œç¾èåˆ', 'desc': 'æˆ‘ä»¬ä»‹ç»äº†MusicInfuserï¼Œè¿™æ˜¯ä¸€ç§ç”Ÿæˆé«˜è´¨é‡èˆè¹ˆè§†é¢‘çš„æ–¹æ³•ï¼Œèƒ½å¤Ÿä¸æŒ‡å®šçš„éŸ³ä¹è½¨é“åŒæ­¥ã€‚æˆ‘ä»¬é€šè¿‡å¼•å…¥è½»é‡çº§çš„éŸ³ä¹-è§†é¢‘äº¤å‰æ³¨æ„åŠ›æœºåˆ¶å’Œä½ç§©é€‚é…å™¨ï¼Œå±•ç¤ºäº†å¦‚ä½•è°ƒæ•´ç°æœ‰çš„è§†é¢‘æ‰©æ•£æ¨¡å‹ä»¥é€‚åº”éŸ³ä¹è¾“å…¥ã€‚ä¸ä¹‹å‰éœ€è¦è¿åŠ¨æ•æ‰æ•°æ®çš„å·¥ä½œä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä»…åœ¨èˆè¹ˆè§†é¢‘ä¸Šè¿›è¡Œå¾®è°ƒã€‚MusicInfuseråœ¨ä¿æŒåº•å±‚æ¨¡å‹çš„çµæ´»æ€§å’Œç”Ÿæˆèƒ½åŠ›çš„åŒæ—¶ï¼Œå®ç°äº†é«˜è´¨é‡çš„éŸ³ä¹é©±åŠ¨è§†é¢‘ç”Ÿæˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.12769', 'title': 'ViSpeak: Visual Instruction Feedback in Streaming Videos', 'url': 'https://huggingface.co/papers/2503.12769', 'abstract': 'Recent advances in Large Multi-modal Models (LMMs) are primarily focused on offline video understanding. Instead, streaming video understanding poses great challenges to recent models due to its time-sensitive, omni-modal and interactive characteristics. In this work, we aim to extend the streaming video understanding from a new perspective and propose a novel task named Visual Instruction Feedback in which models should be aware of visual contents and learn to extract instructions from them. For example, when users wave their hands to agents, agents should recognize the gesture and start conversations with welcome information. Thus, following instructions in visual modality greatly enhances user-agent interactions. To facilitate research, we define seven key subtasks highly relevant to visual modality and collect the ViSpeak-Instruct dataset for training and the ViSpeak-Bench for evaluation. Further, we propose the ViSpeak model, which is a SOTA streaming video understanding LMM with GPT-4o-level performance on various streaming video understanding benchmarks. After finetuning on our ViSpeak-Instruct dataset, ViSpeak is equipped with basic visual instruction feedback ability, serving as a solid baseline for future research.', 'score': 7, 'issue_id': 2800, 'pub_date': '2025-03-17', 'pub_date_card': {'ru': '17 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 17', 'zh': '3æœˆ17æ—¥'}, 'hash': '0913c8e386f3aae5', 'authors': ['Shenghao Fu', 'Qize Yang', 'Yuan-Ming Li', 'Yi-Xing Peng', 'Kun-Yu Lin', 'Xihan Wei', 'Jian-Fang Hu', 'Xiaohua Xie', 'Wei-Shi Zheng'], 'affiliations': ['Guangdong Province Key Laboratory of Information Security Technology, China', 'Key Laboratory of Machine Intelligence and Advanced Computing, Ministry of Education, China', 'Pazhou Laboratory (Huangpu), China', 'Peng Cheng Laboratory, China', 'School of Computer Science and Engineering, Sun Yat-sen University, China', 'Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2503.12769.jpg', 'data': {'categories': ['#dataset', '#multimodal', '#benchmark', '#video', '#agents'], 'emoji': 'ğŸ‘ï¸', 'ru': {'title': 'ViSpeak: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ 'Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ°Ñ ÑĞ²ÑĞ·ÑŒ' Ğ´Ğ»Ñ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ViSpeak, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½ÑƒÑ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞ°Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ° Ğ½Ğ¸Ñ… Ğ² Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ñ‹ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ViSpeak-Instruct Ğ¸ ViSpeak-Bench. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ViSpeak Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ GPT-4 Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾."}, 'en': {'title': 'Enhancing User-Agent Interaction through Visual Instruction Feedback', 'desc': 'This paper introduces a new approach to understanding streaming video using Large Multi-modal Models (LMMs). It focuses on a task called Visual Instruction Feedback, where models learn to interpret visual cues, like hand gestures, to enhance interactions with users. The authors present the ViSpeak model, which achieves state-of-the-art performance in streaming video understanding after being trained on a newly created dataset. This work aims to improve user-agent communication by enabling agents to respond appropriately to visual instructions.'}, 'zh': {'title': 'æå‡ç”¨æˆ·ä¸ä»£ç†äº’åŠ¨çš„è§†è§‰æŒ‡ä»¤åé¦ˆ', 'desc': 'è¿‘å¹´æ¥ï¼Œå¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMï¼‰åœ¨ç¦»çº¿è§†é¢‘ç†è§£æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œæµåª’ä½“è§†é¢‘ç†è§£ç”±äºå…¶æ—¶é—´æ•æ„Ÿæ€§ã€å…¨æ¨¡æ€å’Œäº¤äº’ç‰¹æ€§ï¼Œå¯¹ç°æœ‰æ¨¡å‹æå‡ºäº†å·¨å¤§æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°ä»»åŠ¡ï¼Œç§°ä¸ºè§†è§‰æŒ‡ä»¤åé¦ˆï¼Œæ¨¡å‹éœ€è¦ç†è§£è§†è§‰å†…å®¹å¹¶ä»ä¸­æå–æŒ‡ä»¤ï¼Œä»è€Œå¢å¼ºç”¨æˆ·ä¸ä»£ç†ä¹‹é—´çš„äº’åŠ¨ã€‚æˆ‘ä»¬å®šä¹‰äº†ä¸ƒä¸ªä¸è§†è§‰æ¨¡æ€é«˜åº¦ç›¸å…³çš„å­ä»»åŠ¡ï¼Œå¹¶æ”¶é›†äº†ViSpeak-Instructæ•°æ®é›†ç”¨äºè®­ç»ƒï¼ŒViSpeak-Benchç”¨äºè¯„ä¼°ï¼ŒåŒæ—¶æå‡ºäº†ViSpeakæ¨¡å‹ï¼Œå±•ç¤ºäº†åœ¨æµåª’ä½“è§†é¢‘ç†è§£åŸºå‡†ä¸Šçš„å…ˆè¿›æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.11227', 'title': 'GKG-LLM: A Unified Framework for Generalized Knowledge Graph\n  Construction', 'url': 'https://huggingface.co/papers/2503.11227', 'abstract': 'The construction of Generalized Knowledge Graph (GKG), including knowledge graph, event knowledge graph and commonsense knowledge graph, is fundamental for various natural language processing tasks. Current studies typically construct these types of graph separately, overlooking holistic insights and potential unification that could be beneficial in computing resources and usage perspectives. However, a key challenge in developing a unified framework for GKG is obstacles arising from task-specific differences. In this study, we propose a unified framework for constructing generalized knowledge graphs to address this challenge. First, we collect data from 15 sub-tasks in 29 datasets across the three types of graphs, categorizing them into in-sample, counter-task, and out-of-distribution (OOD) data. Then, we propose a three-stage curriculum learning fine-tuning framework, by iteratively injecting knowledge from the three types of graphs into the Large Language Models. Extensive experiments show that our proposed model improves the construction of all three graph types across in-domain, OOD and counter-task data.', 'score': 7, 'issue_id': 2805, 'pub_date': '2025-03-14', 'pub_date_card': {'ru': '14 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 14', 'zh': '3æœˆ14æ—¥'}, 'hash': '2e48a3554a1036a6', 'authors': ['Jian Zhang', 'Bifan Wei', 'Shihao Qi', 'haiping Zhu', 'Jun Liu', 'Qika Lin'], 'affiliations': ['National University of Singapore', 'School of Computer Science and Technology, Xian Jiaotong University, Xian, China', 'School of Continuing Education, Xian Jiaotong University, Xian, China', 'Shaanxi Province Key Laboratory of Big Data Knowledge Engineering, Xian Jiaotong University, Xian, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.11227.jpg', 'data': {'categories': ['#graphs', '#transfer_learning', '#data', '#training', '#dataset'], 'emoji': 'ğŸ•¸ï¸', 'ru': {'title': 'Ğ•Ğ´Ğ¸Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ½Ñ‹Ñ… Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ² Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ LLM', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ½Ñ‹Ñ… Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ² Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ (GKG), Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ³Ñ€Ğ°Ñ„Ñ‹ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, Ğ³Ñ€Ğ°Ñ„Ñ‹ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹ Ğ¸ Ğ³Ñ€Ğ°Ñ„Ñ‹ Ğ·Ğ´Ñ€Ğ°Ğ²Ğ¾Ğ³Ğ¾ ÑĞ¼Ñ‹ÑĞ»Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ‚Ñ€ĞµÑ…ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ĞºÑƒÑ€Ğ¸ĞºÑƒĞ»ÑƒĞ¼Ğ° Ğ´Ğ»Ñ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM). Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ğ»Ğ¸ÑÑŒ Ğ½Ğ° 15 Ğ¿Ğ¾Ğ´Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸Ğ· 29 Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ñ‡Ğ½Ñ‹Ğµ, ĞºĞ¾Ğ½Ñ‚Ñ€-Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½Ñ‹Ğµ Ğ¸ out-of-distribution Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ² Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ğ¸ Ğ²ÑĞµÑ… Ñ‚Ñ€ĞµÑ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ² Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Unifying Knowledge Graphs for Enhanced NLP Performance', 'desc': 'This paper presents a unified framework for constructing Generalized Knowledge Graphs (GKG) that integrates knowledge graphs, event knowledge graphs, and commonsense knowledge graphs. The authors identify the challenge of task-specific differences that hinder the unification of these graphs, which can lead to inefficiencies in resource usage. They propose a three-stage curriculum learning approach that fine-tunes Large Language Models by incorporating knowledge from all three graph types. Experimental results demonstrate that this framework enhances the performance of GKG construction across various data distributions, including in-sample, out-of-distribution, and counter-task scenarios.'}, 'zh': {'title': 'ç»Ÿä¸€æ„å»ºå¹¿ä¹‰çŸ¥è¯†å›¾è°±çš„æ¡†æ¶', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„å¹¿ä¹‰çŸ¥è¯†å›¾è°±ï¼ˆGKGï¼‰æ„å»ºæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å½“å‰ç ”ç©¶ä¸­å„ç±»çŸ¥è¯†å›¾è°±åˆ†å¼€æ„å»ºçš„é—®é¢˜ã€‚æˆ‘ä»¬ä»29ä¸ªæ•°æ®é›†ä¸­æ”¶é›†äº†15ä¸ªå­ä»»åŠ¡çš„æ•°æ®ï¼Œå¹¶å°†å…¶åˆ†ç±»ä¸ºæ ·æœ¬å†…ã€å¯¹æŠ—ä»»åŠ¡å’Œåˆ†å¸ƒå¤–æ•°æ®ã€‚é€šè¿‡ä¸‰é˜¶æ®µçš„è¯¾ç¨‹å­¦ä¹ å¾®è°ƒæ¡†æ¶ï¼Œæˆ‘ä»¬å°†ä¸‰ç§ç±»å‹çš„çŸ¥è¯†é€æ­¥æ³¨å…¥åˆ°å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨æ ·æœ¬å†…ã€åˆ†å¸ƒå¤–å’Œå¯¹æŠ—ä»»åŠ¡æ•°æ®ä¸Šå‡æå‡äº†ä¸‰ç§çŸ¥è¯†å›¾è°±çš„æ„å»ºæ•ˆæœã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.13360', 'title': 'Mitigating Visual Forgetting via Take-along Visual Conditioning for\n  Multi-modal Long CoT Reasoning', 'url': 'https://huggingface.co/papers/2503.13360', 'abstract': "Recent advancements in Large Language Models (LLMs) have demonstrated enhanced reasoning capabilities, evolving from Chain-of-Thought (CoT) prompting to advanced, product-oriented solutions like OpenAI o1. During our re-implementation of this model, we noticed that in multimodal tasks requiring visual input (e.g., geometry problems), Multimodal LLMs (MLLMs) struggle to maintain focus on the visual information, in other words, MLLMs suffer from a gradual decline in attention to visual information as reasoning progresses, causing text-over-relied outputs. To investigate this, we ablate image inputs during long-chain reasoning. Concretely, we truncate the reasoning process midway, then re-complete the reasoning process with the input image removed. We observe only a ~2% accuracy drop on MathVista's test-hard subset, revealing the model's textual outputs dominate the following reasoning process. Motivated by this, we propose Take-along Visual Conditioning (TVC), a strategy that shifts image input to critical reasoning stages and compresses redundant visual tokens via dynamic pruning. This methodology helps the model retain attention to the visual components throughout the reasoning. Our approach achieves state-of-the-art performance on average across five mathematical reasoning benchmarks (+3.4% vs previous sota), demonstrating the effectiveness of TVC in enhancing multimodal reasoning systems.", 'score': 5, 'issue_id': 2804, 'pub_date': '2025-03-17', 'pub_date_card': {'ru': '17 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 17', 'zh': '3æœˆ17æ—¥'}, 'hash': '8877e2e1a13921d1', 'authors': ['Hai-Long Sun', 'Zhun Sun', 'Houwen Peng', 'Han-Jia Ye'], 'affiliations': ['National Key Laboratory for Novel Software Technology, Nanjing University', 'School of Artificial Intelligence, Nanjing University', 'Tencent'], 'pdf_title_img': 'assets/pdf/title_img/2503.13360.jpg', 'data': {'categories': ['#training', '#long_context', '#reasoning', '#multimodal', '#math'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ‚ĞµÑ€ÑÑÑ‚ Ñ„Ğ¾ĞºÑƒÑ Ğ½Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Take-along Visual Conditioning (TVC), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿ĞµÑ€ĞµĞ¼ĞµÑ‰Ğ°ĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑÑ‚Ğ°Ğ¿Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ¶Ğ¸Ğ¼Ğ°ĞµÑ‚ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ‚ÑŒ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğº Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°Ğ¼ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ñ‚ÑĞ¶ĞµĞ½Ğ¸Ğ¸ Ğ²ÑĞµĞ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ TVC Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ¾ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¿ÑÑ‚Ğ¸ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ¿Ğ¾ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼, Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾Ğ¹Ğ´Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ½Ğ° 3.4%.'}, 'en': {'title': 'Enhancing Visual Attention in Multimodal Reasoning with TVC', 'desc': 'This paper discusses the limitations of Multimodal Large Language Models (MLLMs) in maintaining attention to visual information during complex reasoning tasks. The authors found that MLLMs tend to rely more on text as reasoning progresses, leading to a decline in accuracy when visual inputs are removed. To address this issue, they propose a new method called Take-along Visual Conditioning (TVC), which strategically integrates visual inputs at critical stages of reasoning and reduces unnecessary visual data. Their approach significantly improves performance on mathematical reasoning tasks, achieving state-of-the-art results across multiple benchmarks.'}, 'zh': {'title': 'æå‡å¤šæ¨¡æ€æ¨ç†çš„è§†è§‰å…³æ³¨åŠ›', 'desc': 'æœ€è¿‘ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†èƒ½åŠ›ä¸Šå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œå°¤å…¶æ˜¯ä»é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æç¤ºæ¼”å˜åˆ°æ›´å…ˆè¿›çš„äº§å“å¯¼å‘è§£å†³æ–¹æ¡ˆã€‚åœ¨æˆ‘ä»¬çš„æ¨¡å‹é‡æ–°å®ç°è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å‘ç°å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨éœ€è¦è§†è§‰è¾“å…¥çš„ä»»åŠ¡ä¸­ï¼ˆå¦‚å‡ ä½•é—®é¢˜ï¼‰éš¾ä»¥ä¿æŒå¯¹è§†è§‰ä¿¡æ¯çš„å…³æ³¨ï¼Œå¯¼è‡´æ–‡æœ¬è¾“å‡ºè¿‡äºä¾èµ–ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºâ€œéšè¡Œè§†è§‰æ¡ä»¶â€ï¼ˆTVCï¼‰çš„ç­–ç•¥ï¼Œé€šè¿‡åœ¨å…³é”®æ¨ç†é˜¶æ®µå¼•å…¥å›¾åƒè¾“å…¥ï¼Œå¹¶åŠ¨æ€ä¿®å‰ªå†—ä½™çš„è§†è§‰æ ‡è®°ï¼Œå¸®åŠ©æ¨¡å‹åœ¨æ•´ä¸ªæ¨ç†è¿‡ç¨‹ä¸­ä¿æŒå¯¹è§†è§‰æˆåˆ†çš„å…³æ³¨ã€‚æˆ‘ä»¬çš„ç ”ç©¶åœ¨äº”ä¸ªæ•°å­¦æ¨ç†åŸºå‡†ä¸Šå®ç°äº†æœ€æ–°çš„æœ€ä½³æ€§èƒ½ï¼Œè¯æ˜äº†TVCåœ¨å¢å¼ºå¤šæ¨¡æ€æ¨ç†ç³»ç»Ÿä¸­çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.12963', 'title': 'Unlock Pose Diversity: Accurate and Efficient Implicit Keypoint-based\n  Spatiotemporal Diffusion for Audio-driven Talking Portrait', 'url': 'https://huggingface.co/papers/2503.12963', 'abstract': 'Audio-driven single-image talking portrait generation plays a crucial role in virtual reality, digital human creation, and filmmaking. Existing approaches are generally categorized into keypoint-based and image-based methods. Keypoint-based methods effectively preserve character identity but struggle to capture fine facial details due to the fixed points limitation of the 3D Morphable Model. Moreover, traditional generative networks face challenges in establishing causality between audio and keypoints on limited datasets, resulting in low pose diversity. In contrast, image-based approaches produce high-quality portraits with diverse details using the diffusion network but incur identity distortion and expensive computational costs. In this work, we propose KDTalker, the first framework to combine unsupervised implicit 3D keypoint with a spatiotemporal diffusion model. Leveraging unsupervised implicit 3D keypoints, KDTalker adapts facial information densities, allowing the diffusion process to model diverse head poses and capture fine facial details flexibly. The custom-designed spatiotemporal attention mechanism ensures accurate lip synchronization, producing temporally consistent, high-quality animations while enhancing computational efficiency. Experimental results demonstrate that KDTalker achieves state-of-the-art performance regarding lip synchronization accuracy, head pose diversity, and execution efficiency.Our codes are available at https://github.com/chaolongy/KDTalker.', 'score': 5, 'issue_id': 2808, 'pub_date': '2025-03-17', 'pub_date_card': {'ru': '17 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 17', 'zh': '3æœˆ17æ—¥'}, 'hash': '2cf2c79f5c5a0177', 'authors': ['Chaolong Yang', 'Kai Yao', 'Yuyao Yan', 'Chenru Jiang', 'Weiguang Zhao', 'Jie Sun', 'Guangliang Cheng', 'Yifei Zhang', 'Bin Dong', 'Kaizhu Huang'], 'affiliations': ['Ant Group, Hangzhou, 310000, China', 'Department of Computer Science, University of Liverpool, Liverpool, L69 7 ZX, UK', 'Department of Foundational Mathematics, Xian Jiaotong-Liverpool University, Suzhou, 215123, China', 'Department of Mechatronics and Robotics, Xian Jiaotong-Liverpool University, Suzhou, 215123, China', 'Digital Innovation Research Center, Duke Kunshan University, Kunshan, 215316, China', 'Ricoh Software Research Center, Beijing, 100027, China', 'School of Robotic, Xian Jiaotong-Liverpool University, Suzhou, 215123, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.12963.jpg', 'data': {'categories': ['#architecture', '#cv', '#audio', '#diffusion', '#games', '#multimodal', '#video'], 'emoji': 'ğŸ—£ï¸', 'ru': {'title': 'KDTalker: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ñ€Ñ‚Ñ€ĞµÑ‚Ğ¾Ğ²', 'desc': 'KDTalker - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ°Ğ½Ğ¸Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ñ€Ñ‚Ñ€ĞµÑ‚Ğ¾Ğ² Ğ¿Ğ¾ Ğ°ÑƒĞ´Ğ¸Ğ¾. ĞĞ½ ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ Ğ½ĞµĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ğ½ĞµÑĞ²Ğ½Ñ‹Ğµ 3D ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ. KDTalker Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³Ğ¸Ğ±ĞºĞ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ·Ñ‹ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ñ‹ Ğ¸ Ğ¼ĞµĞ»ĞºĞ¸Ğµ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸ Ğ»Ğ¸Ñ†Ğ°. Ğ‘Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñƒ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½ÑƒÑ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ³ÑƒĞ± Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ KDTalker Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ³ÑƒĞ±, Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ¿Ğ¾Ğ· Ğ³Ğ¾Ğ»Ğ¾Ğ²Ñ‹ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'KDTalker: Revolutionizing Talking Portraits with Audio and 3D Keypoints', 'desc': 'This paper introduces KDTalker, a novel framework for generating talking portraits from audio inputs. It combines unsupervised implicit 3D keypoints with a spatiotemporal diffusion model to enhance facial detail and pose diversity. Unlike traditional methods, KDTalker effectively synchronizes lip movements with audio while maintaining character identity. The results show that KDTalker outperforms existing techniques in lip synchronization accuracy and computational efficiency.'}, 'zh': {'title': 'KDTalkerï¼šéŸ³é¢‘é©±åŠ¨çš„é«˜æ•ˆè¯´è¯è‚–åƒç”Ÿæˆ', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºKDTalkerçš„æ¡†æ¶ï¼Œç”¨äºéŸ³é¢‘é©±åŠ¨çš„å•å›¾åƒè¯´è¯è‚–åƒç”Ÿæˆã€‚è¯¥æ–¹æ³•ç»“åˆäº†æ— ç›‘ç£éšå¼3Då…³é”®ç‚¹å’Œæ—¶ç©ºæ‰©æ•£æ¨¡å‹ï¼Œèƒ½å¤Ÿçµæ´»æ•æ‰ç»†è…»çš„é¢éƒ¨ç»†èŠ‚å’Œå¤šæ ·çš„å¤´éƒ¨å§¿æ€ã€‚KDTalkeré€šè¿‡è‡ªå®šä¹‰çš„æ—¶ç©ºæ³¨æ„æœºåˆ¶ï¼Œç¡®ä¿äº†å‡†ç¡®çš„å”‡éƒ¨åŒæ­¥ï¼Œç”Ÿæˆé«˜è´¨é‡ä¸”æ—¶é—´ä¸€è‡´çš„åŠ¨ç”»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒKDTalkeråœ¨å”‡éƒ¨åŒæ­¥ç²¾åº¦ã€å¤´éƒ¨å§¿æ€å¤šæ ·æ€§å’Œæ‰§è¡Œæ•ˆç‡æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.14830', 'title': 'Decompositional Neural Scene Reconstruction with Generative Diffusion\n  Prior', 'url': 'https://huggingface.co/papers/2503.14830', 'abstract': 'Decompositional reconstruction of 3D scenes, with complete shapes and detailed texture of all objects within, is intriguing for downstream applications but remains challenging, particularly with sparse views as input. Recent approaches incorporate semantic or geometric regularization to address this issue, but they suffer significant degradation in underconstrained areas and fail to recover occluded regions. We argue that the key to solving this problem lies in supplementing missing information for these areas. To this end, we propose DP-Recon, which employs diffusion priors in the form of Score Distillation Sampling (SDS) to optimize the neural representation of each individual object under novel views. This provides additional information for the underconstrained areas, but directly incorporating diffusion prior raises potential conflicts between the reconstruction and generative guidance. Therefore, we further introduce a visibility-guided approach to dynamically adjust the per-pixel SDS loss weights. Together these components enhance both geometry and appearance recovery while remaining faithful to input images. Extensive experiments across Replica and ScanNet++ demonstrate that our method significantly outperforms SOTA methods. Notably, it achieves better object reconstruction under 10 views than the baselines under 100 views. Our method enables seamless text-based editing for geometry and appearance through SDS optimization and produces decomposed object meshes with detailed UV maps that support photorealistic Visual effects (VFX) editing. The project page is available at https://dp-recon.github.io/.', 'score': 3, 'issue_id': 2813, 'pub_date': '2025-03-19', 'pub_date_card': {'ru': '19 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 19', 'zh': '3æœˆ19æ—¥'}, 'hash': '7263c31fc82817f1', 'authors': ['Junfeng Ni', 'Yu Liu', 'Ruijie Lu', 'Zirui Zhou', 'Song-Chun Zhu', 'Yixin Chen', 'Siyuan Huang'], 'affiliations': ['Peking University', 'State Key Laboratory of General Artificial Intelligence, BIGAI', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.14830.jpg', 'data': {'categories': ['#3d', '#cv', '#diffusion', '#optimization'], 'emoji': 'ğŸ”', 'ru': {'title': 'DP-Recon: Ğ ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ 3D-ÑÑ†ĞµĞ½ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¾Ğ²', 'desc': 'DP-Recon - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ 3D-ÑÑ†ĞµĞ½, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ñ‹ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². ĞĞ½ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Score Distillation Sampling (SDS) Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ²Ğ¸Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ğ¸ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ Ğ²Ğ¸Ğ´Ğ° Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² ÑĞ»Ğ°Ğ±Ğ¾ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ… ÑÑ†ĞµĞ½Ñ‹. DP-Recon Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ Ğ¸ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğ¹ Ğ²Ğ¸Ğ´ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ², ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ 3D-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ„Ñ„ĞµĞºÑ‚Ğ°Ñ….'}, 'en': {'title': 'Revolutionizing 3D Scene Reconstruction with DP-Recon!', 'desc': 'This paper presents DP-Recon, a method for reconstructing 3D scenes from sparse views while maintaining detailed shapes and textures of objects. The approach utilizes diffusion priors through Score Distillation Sampling (SDS) to fill in missing information in underconstrained areas, improving the reconstruction of occluded regions. To address potential conflicts between reconstruction accuracy and generative guidance, a visibility-guided mechanism is introduced to adjust loss weights dynamically. Experimental results show that DP-Recon outperforms state-of-the-art methods, achieving superior object reconstruction even with fewer input views, and enabling advanced text-based editing for visual effects.'}, 'zh': {'title': 'DP-Reconï¼šæå‡3Dåœºæ™¯é‡å»ºçš„åˆ›æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºDP-Reconçš„æ–¹æ³•ï¼Œç”¨äºä»ç¨€ç–è§†å›¾é‡å»º3Dåœºæ™¯ï¼Œæ—¨åœ¨æ¢å¤æ‰€æœ‰ç‰©ä½“çš„å®Œæ•´å½¢çŠ¶å’Œç»†è‡´çº¹ç†ã€‚è¯¥æ–¹æ³•åˆ©ç”¨æ‰©æ•£å…ˆéªŒå’Œå¾—åˆ†è’¸é¦é‡‡æ ·ï¼ˆSDSï¼‰æ¥ä¼˜åŒ–æ¯ä¸ªç‰©ä½“çš„ç¥ç»è¡¨ç¤ºï¼Œä»è€Œä¸ºæ¬ çº¦æŸåŒºåŸŸæä¾›é¢å¤–ä¿¡æ¯ã€‚ä¸ºäº†é¿å…é‡å»ºä¸ç”ŸæˆæŒ‡å¯¼ä¹‹é—´çš„å†²çªï¼Œæœ¬æ–‡è¿˜å¼•å…¥äº†ä¸€ç§å¯è§æ€§å¼•å¯¼çš„æ–¹æ³•ï¼ŒåŠ¨æ€è°ƒæ•´æ¯ä¸ªåƒç´ çš„SDSæŸå¤±æƒé‡ã€‚é€šè¿‡åœ¨Replicaå’ŒScanNet++æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒï¼ŒDP-Reconåœ¨ç‰©ä½“é‡å»ºæ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå°¤å…¶æ˜¯åœ¨è§†å›¾æ•°é‡è¾ƒå°‘çš„æƒ…å†µä¸‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.15450', 'title': 'SkyLadder: Better and Faster Pretraining via Context Window Scheduling', 'url': 'https://huggingface.co/papers/2503.15450', 'abstract': 'Recent advancements in LLM pretraining have featured ever-expanding context windows to process longer sequences. However, our pilot study reveals that models pretrained with shorter context windows consistently outperform their long-context counterparts under a fixed token budget. This finding motivates us to explore an optimal context window scheduling strategy to better balance long-context capability with pretraining efficiency. To this end, we propose SkyLadder, a simple yet effective approach that implements a short-to-long context window transition. SkyLadder preserves strong standard benchmark performance, while matching or exceeding baseline results on long context tasks. Through extensive experiments, we pre-train 1B-parameter models (up to 32K context) and 3B-parameter models (8K context) on 100B tokens, demonstrating that SkyLadder yields consistent gains of up to 3.7% on common benchmarks, while achieving up to 22% faster training speeds compared to baselines. The code is at https://github.com/sail-sg/SkyLadder.', 'score': 2, 'issue_id': 2814, 'pub_date': '2025-03-19', 'pub_date_card': {'ru': '19 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 19', 'zh': '3æœˆ19æ—¥'}, 'hash': '92dcfd1093b6ecb7', 'authors': ['Tongyao Zhu', 'Qian Liu', 'Haonan Wang', 'Shiqi Chen', 'Xiangming Gu', 'Tianyu Pang', 'Min-Yen Kan'], 'affiliations': ['City University of Hong Kong', 'National University of Singapore', 'Sea AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2503.15450.jpg', 'data': {'categories': ['#optimization', '#architecture', '#benchmark', '#long_context', '#training'], 'emoji': 'ğŸªœ', 'ru': {'title': 'SkyLadder: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾ĞºĞ½Ğ° Ğ² Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ñ… Ğ¾ĞºĞ½Ğ°Ñ…, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾ĞºĞ½Ğ°Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ñ Ğ¾Ğ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ SkyLadder, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·ÑƒĞµÑ‚ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´ Ğ¾Ñ‚ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ñ… Ğº Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ğ¼ Ğ¾ĞºĞ½Ğ°Ğ¼ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. SkyLadder ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑ Ğ¸Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ SkyLadder Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚ Ğ´Ğ¾ 3.7% Ğ½Ğ° Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¸ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ¾ 22% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸.'}, 'en': {'title': 'SkyLadder: Optimizing Context Windows for Efficient LLM Training', 'desc': 'This paper discusses the impact of context window sizes in the pretraining of large language models (LLMs). It finds that models trained with shorter context windows perform better than those with longer ones when the total number of tokens is fixed. To address this, the authors introduce SkyLadder, a strategy that transitions from short to long context windows during training. Their experiments show that SkyLadder not only maintains high performance on standard benchmarks but also improves training speed significantly.'}, 'zh': {'title': 'SkyLadderï¼šä¼˜åŒ–ä¸Šä¸‹æ–‡çª—å£çš„é«˜æ•ˆé¢„è®­ç»ƒç­–ç•¥', 'desc': 'æœ€è¿‘åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é¢„è®­ç»ƒæ–¹é¢çš„è¿›å±•ï¼Œé‡‡ç”¨äº†è¶Šæ¥è¶Šå¤§çš„ä¸Šä¸‹æ–‡çª—å£æ¥å¤„ç†æ›´é•¿çš„åºåˆ—ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬çš„åˆæ­¥ç ”ç©¶è¡¨æ˜ï¼Œåœ¨å›ºå®šçš„æ ‡è®°é¢„ç®—ä¸‹ï¼Œä½¿ç”¨è¾ƒçŸ­ä¸Šä¸‹æ–‡çª—å£é¢„è®­ç»ƒçš„æ¨¡å‹è¡¨ç°ä¼˜äºé•¿ä¸Šä¸‹æ–‡æ¨¡å‹ã€‚è¿™ä¸€å‘ç°ä¿ƒä½¿æˆ‘ä»¬æ¢ç´¢ä¸€ç§æœ€ä½³çš„ä¸Šä¸‹æ–‡çª—å£è°ƒåº¦ç­–ç•¥ï¼Œä»¥æ›´å¥½åœ°å¹³è¡¡é•¿ä¸Šä¸‹æ–‡èƒ½åŠ›ä¸é¢„è®­ç»ƒæ•ˆç‡ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†SkyLadderï¼Œè¿™æ˜¯ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ–¹æ³•ï¼Œå®æ–½çŸ­åˆ°é•¿çš„ä¸Šä¸‹æ–‡çª—å£è¿‡æ¸¡ï¼Œä¿æŒå¼ºå¤§çš„åŸºå‡†æ€§èƒ½ï¼ŒåŒæ—¶åœ¨é•¿ä¸Šä¸‹æ–‡ä»»åŠ¡ä¸ŠåŒ¹é…æˆ–è¶…è¿‡åŸºçº¿ç»“æœã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.14434', 'title': 'LLM-FE: Automated Feature Engineering for Tabular Data with LLMs as\n  Evolutionary Optimizers', 'url': 'https://huggingface.co/papers/2503.14434', 'abstract': 'Automated feature engineering plays a critical role in improving predictive model performance for tabular learning tasks. Traditional automated feature engineering methods are limited by their reliance on pre-defined transformations within fixed, manually designed search spaces, often neglecting domain knowledge. Recent advances using Large Language Models (LLMs) have enabled the integration of domain knowledge into the feature engineering process. However, existing LLM-based approaches use direct prompting or rely solely on validation scores for feature selection, failing to leverage insights from prior feature discovery experiments or establish meaningful reasoning between feature generation and data-driven performance. To address these challenges, we propose LLM-FE, a novel framework that combines evolutionary search with the domain knowledge and reasoning capabilities of LLMs to automatically discover effective features for tabular learning tasks. LLM-FE formulates feature engineering as a program search problem, where LLMs propose new feature transformation programs iteratively, and data-driven feedback guides the search process. Our results demonstrate that LLM-FE consistently outperforms state-of-the-art baselines, significantly enhancing the performance of tabular prediction models across diverse classification and regression benchmarks.', 'score': 2, 'issue_id': 2814, 'pub_date': '2025-03-18', 'pub_date_card': {'ru': '18 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 18', 'zh': '3æœˆ18æ—¥'}, 'hash': '0f8980b0d2167f3d', 'authors': ['Nikhil Abhyankar', 'Parshin Shojaee', 'Chandan K. Reddy'], 'affiliations': ['Department of Computer Science, Virginia Tech'], 'pdf_title_img': 'assets/pdf/title_img/2503.14434.jpg', 'data': {'categories': ['#optimization', '#benchmark', '#reasoning', '#data', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'LLM-FE: Ğ˜Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'LLM-FE - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ½ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¿Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. LLM-FE Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² ĞºĞ°Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼, Ğ³Ğ´Ğµ LLM Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñ‹ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ², Ğ° Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ°Ñ ÑĞ²ÑĞ·ÑŒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¿Ğ¾Ğ¸ÑĞºĞ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ LLM-FE Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'Revolutionizing Feature Engineering with LLMs', 'desc': 'This paper introduces LLM-FE, a new framework for automated feature engineering that enhances predictive model performance in tabular data tasks. Unlike traditional methods that rely on fixed transformations, LLM-FE utilizes Large Language Models (LLMs) to incorporate domain knowledge and reasoning into the feature generation process. The framework treats feature engineering as a program search problem, allowing LLMs to iteratively propose feature transformations while using data-driven feedback to refine their search. Experimental results show that LLM-FE outperforms existing methods, leading to better outcomes in various classification and regression tasks.'}, 'zh': {'title': 'æ™ºèƒ½ç‰¹å¾å·¥ç¨‹ï¼Œæå‡é¢„æµ‹æ¨¡å‹æ€§èƒ½ï¼', 'desc': 'è‡ªåŠ¨ç‰¹å¾å·¥ç¨‹åœ¨æé«˜è¡¨æ ¼å­¦ä¹ ä»»åŠ¡çš„é¢„æµ‹æ¨¡å‹æ€§èƒ½ä¸­èµ·ç€å…³é”®ä½œç”¨ã€‚ä¼ ç»Ÿçš„è‡ªåŠ¨ç‰¹å¾å·¥ç¨‹æ–¹æ³•å—é™äºé¢„å®šä¹‰çš„å˜æ¢å’Œå›ºå®šçš„æ‰‹åŠ¨è®¾è®¡æœç´¢ç©ºé—´ï¼Œå¾€å¾€å¿½è§†é¢†åŸŸçŸ¥è¯†ã€‚æœ€è¿‘ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¿›å±•ä½¿å¾—å°†é¢†åŸŸçŸ¥è¯†èå…¥ç‰¹å¾å·¥ç¨‹è¿‡ç¨‹æˆä¸ºå¯èƒ½ã€‚æˆ‘ä»¬æå‡ºçš„LLM-FEæ¡†æ¶ç»“åˆäº†è¿›åŒ–æœç´¢ä¸LLMsçš„é¢†åŸŸçŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ï¼Œèƒ½å¤Ÿè‡ªåŠ¨å‘ç°æœ‰æ•ˆçš„ç‰¹å¾ï¼Œä»è€Œæ˜¾è‘—æå‡è¡¨æ ¼é¢„æµ‹æ¨¡å‹çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.15478', 'title': 'SWEET-RL: Training Multi-Turn LLM Agents on Collaborative Reasoning\n  Tasks', 'url': 'https://huggingface.co/papers/2503.15478', 'abstract': 'Large language model (LLM) agents need to perform multi-turn interactions in real-world tasks. However, existing multi-turn RL algorithms for optimizing LLM agents fail to perform effective credit assignment over multiple turns while leveraging the generalization capabilities of LLMs and it remains unclear how to develop such algorithms. To study this, we first introduce a new benchmark, ColBench, where an LLM agent interacts with a human collaborator over multiple turns to solve realistic tasks in backend programming and frontend design. Building on this benchmark, we propose a novel RL algorithm, SWEET-RL (RL with Step-WisE Evaluation from Training-time information), that uses a carefully designed optimization objective to train a critic model with access to additional training-time information. The critic provides step-level rewards for improving the policy model. Our experiments demonstrate that SWEET-RL achieves a 6% absolute improvement in success and win rates on ColBench compared to other state-of-the-art multi-turn RL algorithms, enabling Llama-3.1-8B to match or exceed the performance of GPT4-o in realistic collaborative content creation.', 'score': 1, 'issue_id': 2814, 'pub_date': '2025-03-19', 'pub_date_card': {'ru': '19 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 19', 'zh': '3æœˆ19æ—¥'}, 'hash': 'a4dcf1557e92629c', 'authors': ['Yifei Zhou', 'Song Jiang', 'Yuandong Tian', 'Jason Weston', 'Sergey Levine', 'Sainbayar Sukhbaatar', 'Xian Li'], 'affiliations': ['FAIR at Meta', 'UC Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2503.15478.jpg', 'data': {'categories': ['#optimization', '#benchmark', '#games', '#rl', '#agents', '#rlhf', '#training'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'SWEET-RL: ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ñ‹Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ SWEET-RL Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ñ‹Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑÑ…. ĞĞ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½ÑƒÑ Ñ†ĞµĞ»ĞµĞ²ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸. Ğ”Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ñ‹Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ColBench, Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ² Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸ Ğ²ĞµĞ±-Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ SWEET-RL Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° 6% Ğ¿Ğ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑĞ¼ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° ColBench.'}, 'en': {'title': 'Enhancing LLMs with SWEET-RL for Better Multi-Turn Interactions', 'desc': 'This paper addresses the challenge of optimizing large language model (LLM) agents for multi-turn interactions in real-world tasks. Existing reinforcement learning (RL) methods struggle with credit assignment across multiple turns, which affects their performance. To tackle this, the authors introduce ColBench, a benchmark for evaluating LLM agents in collaborative tasks, and propose a new RL algorithm called SWEET-RL. This algorithm enhances the training process by using a critic model that provides step-level rewards, leading to improved success rates in collaborative content creation tasks.'}, 'zh': {'title': 'æå‡å¤šè½®äº¤äº’çš„å¼ºåŒ–å­¦ä¹ æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šè½®äº¤äº’ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼ŒæŒ‡å‡ºç°æœ‰çš„å¤šè½®å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç®—æ³•åœ¨æœ‰æ•ˆçš„ä¿¡ç”¨åˆ†é…æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ColBenchï¼Œæ—¨åœ¨è¯„ä¼°LLMä»£ç†ä¸äººç±»åä½œè§£å†³å®é™…ä»»åŠ¡çš„èƒ½åŠ›ã€‚åŸºäºæ­¤åŸºå‡†ï¼Œæå‡ºäº†ä¸€ç§æ–°é¢–çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•SWEET-RLï¼Œè¯¥ç®—æ³•é€šè¿‡è®¾è®¡ä¼˜åŒ–ç›®æ ‡æ¥è®­ç»ƒä¸€ä¸ªè¯„è®ºæ¨¡å‹ï¼Œä»è€Œæä¾›é€æ­¥å¥–åŠ±ä»¥æ”¹å–„ç­–ç•¥æ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSWEET-RLåœ¨ColBenchä¸Šç›¸è¾ƒäºå…¶ä»–å…ˆè¿›çš„å¤šè½®RLç®—æ³•ï¼ŒæˆåŠŸç‡å’Œèƒœç‡æé«˜äº†6%ï¼Œä½¿å¾—Llama-3.1-8Båœ¨å®é™…åä½œå†…å®¹åˆ›ä½œä¸­èƒ½å¤Ÿä¸GPT4-oçš„è¡¨ç°ç›¸åŒ¹é…æˆ–è¶…è¶Šã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.15055', 'title': 'ELTEX: A Framework for Domain-Driven Synthetic Data Generation', 'url': 'https://huggingface.co/papers/2503.15055', 'abstract': "We present ELTEX (Efficient LLM Token Extraction), a domain-driven framework for generating high-quality synthetic training data in specialized domains. While Large Language Models (LLMs) have shown impressive general capabilities, their performance in specialized domains like cybersecurity remains limited by the scarcity of domain-specific training data. ELTEX addresses this challenge by systematically integrating explicit domain indicator extraction with dynamic prompting to preserve critical domain knowledge throughout the generation process. We demonstrate ELTEX's effectiveness in the context of blockchain-related cyberattack detection, where we fine-tune Gemma-2B using various combinations of real and ELTEX-generated data. Our results show that the ELTEX-enhanced model achieves performance competitive with GPT-4 across both standard classification metrics and uncertainty calibration, while requiring significantly fewer computational resources. We release a curated synthetic dataset of social media texts for cyberattack detection in blockchain. Our work demonstrates that domain-driven synthetic data generation can effectively bridge the performance gap between resource-efficient models and larger architectures in specialized domains.", 'score': 1, 'issue_id': 2802, 'pub_date': '2025-03-19', 'pub_date_card': {'ru': '19 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 19', 'zh': '3æœˆ19æ—¥'}, 'hash': '785ffad4856eb286', 'authors': ['Arina Razmyslovich', 'Kseniia Murasheva', 'Sofia Sedlova', 'Julien Capitaine', 'Eugene Dmitriev'], 'affiliations': ['Distributed Networks Institute (DNI)', 'Technologies MÃ©sozoÃ¯ques'], 'pdf_title_img': 'assets/pdf/title_img/2503.15055.jpg', 'data': {'categories': ['#science', '#data', '#training', '#synthetic', '#dataset'], 'emoji': 'ğŸ›¡ï¸', 'ru': {'title': 'ELTEX: Ğ¡Ğ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ°Ğ»Ñ‹Ñ… Ğ¯Ğœ Ğ² ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…', 'desc': 'ELTEX - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº ĞºĞ¸Ğ±ĞµÑ€Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ. ĞĞ½ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸Ğ½Ğ´Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¸Ğ½Ğ³Ğ¾Ğ¼ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. ELTEX Ğ±Ñ‹Ğ» Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½ Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ ĞºĞ¸Ğ±ĞµÑ€Ğ°Ñ‚Ğ°Ğº Ğ² Ğ±Ğ»Ğ¾ĞºÑ‡ĞµĞ¹Ğ½Ğµ, Ğ³Ğ´Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Gemma-2B, Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ñ‹Ğµ Ñ GPT-4. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ°Ğ»Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ….'}, 'en': {'title': 'Bridging the Data Gap in Cybersecurity with ELTEX', 'desc': 'The paper introduces ELTEX, a framework designed to create high-quality synthetic training data specifically for specialized fields like cybersecurity. It tackles the issue of limited domain-specific data that affects the performance of Large Language Models (LLMs) in these areas. By combining domain indicator extraction with dynamic prompting, ELTEX ensures that essential domain knowledge is maintained during data generation. The framework is validated through its application in blockchain cyberattack detection, showing that it can enhance model performance while being more resource-efficient than larger models like GPT-4.'}, 'zh': {'title': 'é¢†åŸŸé©±åŠ¨çš„åˆæˆæ•°æ®ç”Ÿæˆï¼Œæå‡æ¨¡å‹æ€§èƒ½ï¼', 'desc': 'ELTEXï¼ˆé«˜æ•ˆLLMä»¤ç‰Œæå–ï¼‰æ˜¯ä¸€ä¸ªé’ˆå¯¹ç‰¹å®šé¢†åŸŸç”Ÿæˆé«˜è´¨é‡åˆæˆè®­ç»ƒæ•°æ®çš„æ¡†æ¶ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é€šç”¨èƒ½åŠ›ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ç½‘ç»œå®‰å…¨ç­‰ä¸“ä¸šé¢†åŸŸçš„è¡¨ç°å—åˆ°é¢†åŸŸç‰¹å®šè®­ç»ƒæ•°æ®ç¨€ç¼ºçš„é™åˆ¶ã€‚ELTEXé€šè¿‡ç³»ç»Ÿåœ°æ•´åˆæ˜¾å¼é¢†åŸŸæŒ‡ç¤ºç¬¦æå–å’ŒåŠ¨æ€æç¤ºï¼Œç¡®ä¿åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ä¿ç•™å…³é”®çš„é¢†åŸŸçŸ¥è¯†ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒELTEXå¢å¼ºçš„æ¨¡å‹åœ¨åŒºå—é“¾ç›¸å…³çš„ç½‘ç»œæ”»å‡»æ£€æµ‹ä¸­ï¼Œæ€§èƒ½ä¸GPT-4ç›¸å½“ï¼ŒåŒæ—¶æ˜¾è‘—å‡å°‘äº†è®¡ç®—èµ„æºçš„éœ€æ±‚ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.13517', 'title': 'CURIE: Evaluating LLMs On Multitask Scientific Long Context\n  Understanding and Reasoning', 'url': 'https://huggingface.co/papers/2503.13517', 'abstract': 'Scientific problem-solving involves synthesizing information while applying expert knowledge. We introduce CURIE, a scientific long-Context Understanding,Reasoning and Information Extraction benchmark to measure the potential of Large Language Models (LLMs) in scientific problem-solving and assisting scientists in realistic workflows. This benchmark introduces ten challenging tasks with a total of 580 problems and solution pairs curated by experts in six disciplines - materials science, condensed matter physics, quantum computing, geospatial analysis, biodiversity, and proteins - covering both experimental and theoretical work-flows in science. We evaluate a range of closed and open LLMs on tasks in CURIE which requires domain expertise, comprehension of long in-context information,and multi-step reasoning. While Gemini Flash 2.0 and Claude-3 show consistent high comprehension across domains, the popular GPT-4o and command-R+ fail dramatically on protein sequencing tasks. With the best performance at 32% there is much room for improvement for all models. We hope that insights gained from CURIE can guide the future development of LLMs in sciences. Evaluation code and data are in https://github.com/google/curie', 'score': 1, 'issue_id': 2810, 'pub_date': '2025-03-14', 'pub_date_card': {'ru': '14 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 14', 'zh': '3æœˆ14æ—¥'}, 'hash': '56a7b2a9ca22936c', 'authors': ['Hao Cui', 'Zahra Shamsi', 'Gowoon Cheon', 'Xuejian Ma', 'Shutong Li', 'Maria Tikhanovskaya', 'Peter Norgaard', 'Nayantara Mudur', 'Martyna Plomecka', 'Paul Raccuglia', 'Yasaman Bahri', 'Victor V. Albert', 'Pranesh Srinivasan', 'Haining Pan', 'Philippe Faist', 'Brian Rohr', 'Michael J. Statt', 'Dan Morris', 'Drew Purves', 'Elise Kleeman', 'Ruth Alcantara', 'Matthew Abraham', 'Muqthar Mohammad', 'Ean Phing VanLee', 'Chenfei Jiang', 'Elizabeth Dorfman', 'Eun-Ah Kim', 'Michael P Brenner', 'Viren Jain', 'Sameera Ponda', 'Subhashini Venugopalan'], 'affiliations': ['Cornell', 'FU Berlin', 'Google', 'Harvard', 'Modelyst', 'NIST', 'Rutgers', 'UMD College Park', 'University of Zurich'], 'pdf_title_img': 'assets/pdf/title_img/2503.13517.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#science', '#dataset', '#multimodal', '#long_context'], 'emoji': 'ğŸ§ª', 'ru': {'title': 'CURIE: ĞĞ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ´Ğ»Ñ Ğ˜Ğ˜ Ğ² Ğ½Ğ°ÑƒĞºĞµ', 'desc': 'CURIE - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 580 Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸Ğ· 6 Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ´Ğ¸ÑÑ†Ğ¸Ğ¿Ğ»Ğ¸Ğ½, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½Ñ‡Ğ°Ñ‚Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… LLM Ğ½Ğ° CURIE Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ»Ğ¸ÑˆÑŒ 32% ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ñ€ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ½Ğ°Ğ´ĞµÑÑ‚ÑÑ, Ñ‡Ñ‚Ğ¾ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ CURIE Ğ¿Ğ¾Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ² Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞµĞ¼ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ğ¸ LLM Ğ´Ğ»Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'CURIE: Advancing LLMs for Scientific Problem-Solving', 'desc': 'The paper presents CURIE, a benchmark designed to evaluate the capabilities of Large Language Models (LLMs) in scientific problem-solving. It includes ten complex tasks with 580 curated problems across various scientific fields, emphasizing the need for domain expertise and multi-step reasoning. The evaluation reveals that while some models like Gemini Flash 2.0 and Claude-3 perform well, others like GPT-4o struggle significantly, particularly in protein sequencing tasks. The findings suggest that there is substantial potential for improving LLMs to better assist scientists in their workflows.'}, 'zh': {'title': 'CURIEï¼šæ¨åŠ¨ç§‘å­¦é—®é¢˜è§£å†³çš„è¯­è¨€æ¨¡å‹åŸºå‡†', 'desc': 'æœ¬æ–‡ä»‹ç»äº†CURIEï¼Œä¸€ä¸ªç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç§‘å­¦é—®é¢˜è§£å†³ä¸­çš„èƒ½åŠ›çš„åŸºå‡†ã€‚CURIEåŒ…å«åä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå…±580ä¸ªé—®é¢˜å’Œè§£å†³æ–¹æ¡ˆï¼Œæ¶µç›–ææ–™ç§‘å­¦ã€å‡èšæ€ç‰©ç†ã€é‡å­è®¡ç®—ã€åœ°ç†ç©ºé—´åˆ†æã€ç”Ÿç‰©å¤šæ ·æ€§å’Œè›‹ç™½è´¨ç­‰å…­ä¸ªå­¦ç§‘ã€‚æˆ‘ä»¬è¯„ä¼°äº†å¤šç§å°é—­å’Œå¼€æ”¾çš„LLMsï¼Œå‘ç°è™½ç„¶Gemini Flash 2.0å’ŒClaude-3åœ¨å„ä¸ªé¢†åŸŸè¡¨ç°å‡ºè‰²ï¼Œä½†GPT-4oå’Œcommand-R+åœ¨è›‹ç™½è´¨æµ‹åºä»»åŠ¡ä¸Šè¡¨ç°ä¸ä½³ã€‚CURIEçš„ç»“æœä¸ºæœªæ¥LLMsåœ¨ç§‘å­¦é¢†åŸŸçš„å‘å±•æä¾›äº†é‡è¦çš„æŒ‡å¯¼ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.19325', 'title': 'Long-Context Autoregressive Video Modeling with Next-Frame Prediction', 'url': 'https://huggingface.co/papers/2503.19325', 'abstract': 'Long-context autoregressive modeling has significantly advanced language generation, but video generation still struggles to fully utilize extended temporal contexts. To investigate long-context video modeling, we introduce Frame AutoRegressive (FAR), a strong baseline for video autoregressive modeling. Just as language models learn causal dependencies between tokens (i.e., Token AR), FAR models temporal causal dependencies between continuous frames, achieving better convergence than Token AR and video diffusion transformers. Building on FAR, we observe that long-context vision modeling faces challenges due to visual redundancy. Existing RoPE lacks effective temporal decay for remote context and fails to extrapolate well to long video sequences. Additionally, training on long videos is computationally expensive, as vision tokens grow much faster than language tokens. To tackle these issues, we propose balancing locality and long-range dependency. We introduce FlexRoPE, an test-time technique that adds flexible temporal decay to RoPE, enabling extrapolation to 16x longer vision contexts. Furthermore, we propose long short-term context modeling, where a high-resolution short-term context window ensures fine-grained temporal consistency, while an unlimited long-term context window encodes long-range information using fewer tokens. With this approach, we can train on long video sequences with a manageable token context length. We demonstrate that FAR achieves state-of-the-art performance in both short- and long-video generation, providing a simple yet effective baseline for video autoregressive modeling.', 'score': 59, 'issue_id': 2896, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 25', 'zh': '3æœˆ25æ—¥'}, 'hash': '543c7dbfad83ed73', 'authors': ['Yuchao Gu', 'Weijia Mao', 'Mike Zheng Shou'], 'affiliations': ['Show Lab, National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2503.19325.jpg', 'data': {'categories': ['#training', '#multimodal', '#long_context', '#video'], 'emoji': 'ğŸ¬', 'ru': {'title': 'FAR: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Frame AutoRegressive (FAR) - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ FlexRoPE - Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰ÑƒÑ ÑĞºÑÑ‚Ñ€Ğ°Ğ¿Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ñ‹ Ğ² 16 Ñ€Ğ°Ğ· Ğ´Ğ»Ğ¸Ğ½Ğ½ĞµĞµ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ…. ĞŸÑ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ĞºÑ€Ğ°Ñ‚ĞºĞ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. FAR Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ°Ğº ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'Revolutionizing Video Generation with Frame AutoRegressive Modeling', 'desc': 'This paper presents Frame AutoRegressive (FAR), a new method for generating videos by modeling the temporal dependencies between frames. FAR improves upon existing models by addressing the challenges of visual redundancy and the limitations of current positional encoding techniques like RoPE. The authors introduce FlexRoPE, which allows for flexible temporal decay, enabling the model to handle longer video sequences effectively. By combining short-term and long-term context modeling, FAR achieves state-of-the-art results in video generation while maintaining computational efficiency.'}, 'zh': {'title': 'é•¿æ—¶é—´ä¸Šä¸‹æ–‡è§†é¢‘ç”Ÿæˆçš„æ–°çªç ´', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„è§†é¢‘è‡ªå›å½’å»ºæ¨¡æ–¹æ³•ï¼Œç§°ä¸ºFrame AutoRegressive (FAR)ï¼Œæ—¨åœ¨è§£å†³é•¿æ—¶é—´ä¸Šä¸‹æ–‡è§†é¢‘ç”Ÿæˆä¸­çš„æŒ‘æˆ˜ã€‚FARé€šè¿‡å»ºæ¨¡è¿ç»­å¸§ä¹‹é—´çš„æ—¶é—´å› æœå…³ç³»ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿçš„è¯­è¨€æ¨¡å‹ï¼Œå–å¾—äº†æ›´å¥½çš„æ”¶æ•›æ•ˆæœã€‚ä¸ºäº†åº”å¯¹è§†è§‰å†—ä½™å’Œè®¡ç®—æˆæœ¬é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†FlexRoPEæŠ€æœ¯ï¼Œèƒ½å¤Ÿçµæ´»è°ƒæ•´è¿œç¨‹ä¸Šä¸‹æ–‡çš„æ—¶é—´è¡°å‡ï¼Œå¹¶å¼•å…¥äº†é•¿çŸ­æœŸä¸Šä¸‹æ–‡å»ºæ¨¡æ–¹æ³•ï¼Œä»¥ç¡®ä¿æ—¶é—´ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFARåœ¨çŸ­è§†é¢‘å’Œé•¿è§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸­å‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæˆä¸ºè§†é¢‘è‡ªå›å½’å»ºæ¨¡çš„æœ‰æ•ˆåŸºçº¿ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.18931', 'title': 'CoMP: Continual Multimodal Pre-training for Vision Foundation Models', 'url': 'https://huggingface.co/papers/2503.18931', 'abstract': 'Pre-trained Vision Foundation Models (VFMs) provide strong visual representations for a wide range of applications. In this paper, we continually pre-train prevailing VFMs in a multimodal manner such that they can effortlessly process visual inputs of varying sizes and produce visual representations that are more aligned with language representations, regardless of their original pre-training process. To this end, we introduce CoMP, a carefully designed multimodal pre-training pipeline. CoMP uses a Continual Rotary Position Embedding to support native resolution continual pre-training, and an Alignment Loss between visual and textual features through language prototypes to align multimodal representations. By three-stage training, our VFMs achieve remarkable improvements not only in multimodal understanding but also in other downstream tasks such as classification and segmentation. Remarkably, CoMP-SigLIP achieves scores of 66.7 on ChartQA and 75.9 on DocVQA with a 0.5B LLM, while maintaining an 87.4% accuracy on ImageNet-1K and a 49.5 mIoU on ADE20K under frozen chunk evaluation.', 'score': 26, 'issue_id': 2897, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 24', 'zh': '3æœˆ24æ—¥'}, 'hash': '59128d6a0bd3862c', 'authors': ['Yitong Chen', 'Lingchen Meng', 'Wujian Peng', 'Zuxuan Wu', 'Yu-Gang Jiang'], 'affiliations': ['Shanghai Innovation Institute', 'Shanghai Key Lab of Intell. Info. Processing, School of CS, Fudan University'], 'pdf_title_img': 'assets/pdf/title_img/2503.18931.jpg', 'data': {'categories': ['#alignment', '#multimodal', '#optimization', '#training', '#cv'], 'emoji': 'ğŸ”€', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ CoMP - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ (VFM). CoMP Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹. Ğ¢Ñ€ĞµÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ĞºĞ°Ğº Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸, Ñ‚Ğ°Ğº Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ CoMP-SigLIP Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ñ…Ğ¾Ñ€Ğ¾ÑˆÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° ImageNet-1K Ğ¸ ADE20K.'}, 'en': {'title': 'Enhancing Visual Models with Multimodal Pre-Training', 'desc': 'This paper presents a method to enhance Vision Foundation Models (VFMs) by using a multimodal pre-training approach. The proposed method, called CoMP, allows VFMs to process visual inputs of different sizes and align visual representations with language representations. CoMP employs a Continual Rotary Position Embedding and an Alignment Loss to improve the integration of visual and textual features. The results show significant performance gains in multimodal tasks and other applications like classification and segmentation, demonstrating the effectiveness of the proposed training pipeline.'}, 'zh': {'title': 'æå‡è§†è§‰æ¨¡å‹çš„å¤šæ¨¡æ€é¢„è®­ç»ƒæ–¹æ³•', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„å¤šæ¨¡æ€é¢„è®­ç»ƒæ–¹æ³•CoMPï¼Œç”¨äºæå‡è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆVFMï¼‰çš„è¡¨ç°ã€‚é€šè¿‡æŒç»­çš„å¤šæ¨¡æ€é¢„è®­ç»ƒï¼Œæ¨¡å‹èƒ½å¤Ÿå¤„ç†ä¸åŒå¤§å°çš„è§†è§‰è¾“å…¥ï¼Œå¹¶ç”Ÿæˆä¸è¯­è¨€è¡¨ç¤ºæ›´ä¸€è‡´çš„è§†è§‰è¡¨ç¤ºã€‚CoMPé‡‡ç”¨äº†æŒç»­æ—‹è½¬ä½ç½®åµŒå…¥å’Œè§†è§‰ä¸æ–‡æœ¬ç‰¹å¾ä¹‹é—´çš„å¯¹é½æŸå¤±ï¼Œä»¥å®ç°å¤šæ¨¡æ€è¡¨ç¤ºçš„å¯¹é½ã€‚ç»è¿‡ä¸‰é˜¶æ®µè®­ç»ƒï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å¤šæ¨¡æ€ç†è§£å’Œå…¶ä»–ä¸‹æ¸¸ä»»åŠ¡ä¸Šéƒ½å–å¾—äº†æ˜¾è‘—çš„æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.19622', 'title': 'Exploring Hallucination of Large Multimodal Models in Video\n  Understanding: Benchmark, Analysis and Mitigation', 'url': 'https://huggingface.co/papers/2503.19622', 'abstract': 'The hallucination of large multimodal models (LMMs), providing responses that appear correct but are actually incorrect, limits their reliability and applicability. This paper aims to study the hallucination problem of LMMs in video modality, which is dynamic and more challenging compared to static modalities like images and text. From this motivation, we first present a comprehensive benchmark termed HAVEN for evaluating hallucinations of LMMs in video understanding tasks. It is built upon three dimensions, i.e., hallucination causes, hallucination aspects, and question formats, resulting in 6K questions. Then, we quantitatively study 7 influential factors on hallucinations, e.g., duration time of videos, model sizes, and model reasoning, via experiments of 16 LMMs on the presented benchmark. In addition, inspired by recent thinking models like OpenAI o1, we propose a video-thinking model to mitigate the hallucinations of LMMs via supervised reasoning fine-tuning (SRFT) and direct preference optimization (TDPO)-- where SRFT enhances reasoning capabilities while TDPO reduces hallucinations in the thinking process. Extensive experiments and analyses demonstrate the effectiveness. Remarkably, it improves the baseline by 7.65% in accuracy on hallucination evaluation and reduces the bias score by 4.5%. The code and data are public at https://github.com/Hongcheng-Gao/HAVEN.', 'score': 25, 'issue_id': 2897, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 25', 'zh': '3æœˆ25æ—¥'}, 'hash': 'c727aefeccdca380', 'authors': ['Hongcheng Gao', 'Jiashu Qu', 'Jingyi Tang', 'Baolong Bi', 'Yue Liu', 'Hongyu Chen', 'Li Liang', 'Li Su', 'Qingming Huang'], 'affiliations': ['Beijing Jiaotong University', 'Key Lab of Intell. Info. Process., Inst. of Comput. Tech., CAS', 'National University of Singapore', 'University of Chinese Academy of Sciences', 'University of Cincinnati'], 'pdf_title_img': 'assets/pdf/title_img/2503.19622.jpg', 'data': {'categories': ['#video', '#multimodal', '#benchmark', '#hallucinations', '#training', '#reasoning'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Ğ‘Ğ¾Ñ€ÑŒĞ±Ğ° Ñ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğµ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¸ Ğ¼Ñ‹ÑĞ»ÑÑ‰Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ', 'desc': "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ² ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LMM) Ğ¿Ñ€Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº HAVEN Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ LMM Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ°ÑĞ¿ĞµĞºÑ‚Ñ‹ Ğ¸ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ñ‹ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· ÑĞµĞ¼Ğ¸ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¾Ğ², Ğ²Ğ»Ğ¸ÑÑÑ‰Ğ¸Ñ… Ğ½Ğ° Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸, Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ñ 16 Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ LMM. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ 'video-thinking' Ğ´Ğ»Ñ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² SRFT Ğ¸ TDPO, Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ²ÑˆĞ°Ñ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚Ğ¸."}, 'en': {'title': 'Mitigating Hallucinations in Video Understanding with HAVEN', 'desc': 'This paper addresses the issue of hallucinations in large multimodal models (LMMs), particularly in video understanding tasks. It introduces a benchmark called HAVEN, which evaluates hallucinations based on various factors such as causes, aspects, and question formats, comprising 6,000 questions. The authors analyze seven influential factors affecting hallucinations and propose a novel video-thinking model that employs supervised reasoning fine-tuning and direct preference optimization to reduce these hallucinations. Experimental results show a significant improvement in accuracy and a reduction in bias, demonstrating the effectiveness of the proposed methods.'}, 'zh': {'title': 'è§£å†³è§†é¢‘æ¨¡æ€ä¸­çš„å¹»è§‰é—®é¢˜', 'desc': 'æœ¬æ–‡ç ”ç©¶äº†å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨è§†é¢‘ç†è§£ä»»åŠ¡ä¸­çš„å¹»è§‰é—®é¢˜ï¼Œè¿™ç§é—®é¢˜ä½¿å¾—æ¨¡å‹çš„è¾“å‡ºçœ‹ä¼¼æ­£ç¡®ä½†å®é™…ä¸Šä¸å‡†ç¡®ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåä¸ºHAVENçš„åŸºå‡†ï¼Œç”¨äºè¯„ä¼°LMMsåœ¨è§†é¢‘æ¨¡æ€ä¸‹çš„å¹»è§‰ï¼Œæ¶µç›–äº†å¹»è§‰çš„åŸå› ã€æ–¹é¢å’Œé—®é¢˜æ ¼å¼ç­‰ä¸‰ä¸ªç»´åº¦ï¼Œå…±åŒ…å«6000ä¸ªé—®é¢˜ã€‚é€šè¿‡å¯¹16ä¸ªLMMsè¿›è¡Œå®éªŒï¼Œæˆ‘ä»¬å®šé‡åˆ†æäº†å½±å“å¹»è§‰çš„7ä¸ªå› ç´ ï¼Œå¦‚è§†é¢‘æ—¶é•¿ã€æ¨¡å‹è§„æ¨¡å’Œæ¨ç†èƒ½åŠ›ã€‚æœ€åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è§†é¢‘æ€ç»´æ¨¡å‹ï¼Œé€šè¿‡ç›‘ç£æ¨ç†å¾®è°ƒï¼ˆSRFTï¼‰å’Œç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆTDPOï¼‰æ¥å‡è½»å¹»è§‰ç°è±¡ï¼Œå®éªŒç»“æœæ˜¾ç¤ºè¯¥æ–¹æ³•åœ¨å‡†ç¡®æ€§ä¸Šæé«˜äº†7.65%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.19385', 'title': 'Inference-Time Scaling for Flow Models via Stochastic Generation and\n  Rollover Budget Forcing', 'url': 'https://huggingface.co/papers/2503.19385', 'abstract': 'We propose an inference-time scaling approach for pretrained flow models. Recently, inference-time scaling has gained significant attention in LLMs and diffusion models, improving sample quality or better aligning outputs with user preferences by leveraging additional computation. For diffusion models, particle sampling has allowed more efficient scaling due to the stochasticity at intermediate denoising steps. On the contrary, while flow models have gained popularity as an alternative to diffusion models--offering faster generation and high-quality outputs in state-of-the-art image and video generative models--efficient inference-time scaling methods used for diffusion models cannot be directly applied due to their deterministic generative process. To enable efficient inference-time scaling for flow models, we propose three key ideas: 1) SDE-based generation, enabling particle sampling in flow models, 2) Interpolant conversion, broadening the search space and enhancing sample diversity, and 3) Rollover Budget Forcing (RBF), an adaptive allocation of computational resources across timesteps to maximize budget utilization. Our experiments show that SDE-based generation, particularly variance-preserving (VP) interpolant-based generation, improves the performance of particle sampling methods for inference-time scaling in flow models. Additionally, we demonstrate that RBF with VP-SDE achieves the best performance, outperforming all previous inference-time scaling approaches.', 'score': 24, 'issue_id': 2896, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 25', 'zh': '3æœˆ25æ—¥'}, 'hash': 'e0ead8fbe973f326', 'authors': ['Jaihoon Kim', 'Taehoon Yoon', 'Jisung Hwang', 'Minhyuk Sung'], 'affiliations': ['KAIST'], 'pdf_title_img': 'assets/pdf/title_img/2503.19385.jpg', 'data': {'categories': ['#inference', '#diffusion', '#optimization', '#video'], 'emoji': 'ğŸŒŠ', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğµ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¸Ğ´ĞµĞ¸: Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ñ…Ğ°ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¹ (SDE), Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ğ¾Ğ»ÑĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ SDE, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ¸ÑĞ¿ĞµÑ€ÑĞ¸Ğ¸, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ Ñ‡Ğ°ÑÑ‚Ğ¸Ñ† Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ² Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°.'}, 'en': {'title': 'Enhancing Flow Models with Efficient Inference-Time Scaling', 'desc': 'This paper introduces a new method for improving flow models during inference, which is the process of generating outputs from trained models. The authors focus on three innovative techniques: using stochastic differential equations (SDE) for particle sampling, converting interpolants to increase diversity in generated samples, and implementing Rollover Budget Forcing (RBF) to optimize computational resource allocation. These methods allow flow models to achieve better sample quality and efficiency, similar to advancements seen in diffusion models. The results show that their approach significantly enhances performance, making flow models more competitive in generating high-quality images and videos.'}, 'zh': {'title': 'æµæ¨¡å‹çš„é«˜æ•ˆæ¨ç†æ—¶é—´ç¼©æ”¾æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹é¢„è®­ç»ƒæµæ¨¡å‹çš„æ¨ç†æ—¶é—´ç¼©æ”¾æ–¹æ³•ã€‚è¿‘å¹´æ¥ï¼Œæ¨ç†æ—¶é—´ç¼©æ”¾åœ¨å¤§è¯­è¨€æ¨¡å‹å’Œæ‰©æ•£æ¨¡å‹ä¸­å—åˆ°å¹¿æ³›å…³æ³¨ï¼Œé€šè¿‡åˆ©ç”¨é¢å¤–çš„è®¡ç®—æ¥æé«˜æ ·æœ¬è´¨é‡æˆ–æ›´å¥½åœ°ç¬¦åˆç”¨æˆ·åå¥½ã€‚å°½ç®¡æµæ¨¡å‹ä½œä¸ºæ‰©æ•£æ¨¡å‹çš„æ›¿ä»£æ–¹æ¡ˆè¶Šæ¥è¶Šå—æ¬¢è¿ï¼Œä½†ç”±äºå…¶ç¡®å®šæ€§ç”Ÿæˆè¿‡ç¨‹ï¼Œç°æœ‰çš„æ‰©æ•£æ¨¡å‹æ¨ç†æ—¶é—´ç¼©æ”¾æ–¹æ³•æ— æ³•ç›´æ¥åº”ç”¨äºæµæ¨¡å‹ã€‚æˆ‘ä»¬æå‡ºäº†ä¸‰ç§å…³é”®æ€æƒ³ï¼Œä»¥å®ç°æµæ¨¡å‹çš„é«˜æ•ˆæ¨ç†æ—¶é—´ç¼©æ”¾ï¼šåŸºäºSDEçš„ç”Ÿæˆã€æ’å€¼è½¬æ¢å’Œè‡ªé€‚åº”è®¡ç®—èµ„æºåˆ†é…ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.19903', 'title': 'Scaling Vision Pre-Training to 4K Resolution', 'url': 'https://huggingface.co/papers/2503.19903', 'abstract': 'High-resolution perception of visual details is crucial for daily tasks. Current vision pre-training, however, is still limited to low resolutions (e.g., 378 x 378 pixels) due to the quadratic cost of processing larger images. We introduce PS3 that scales CLIP-style vision pre-training to 4K resolution with a near-constant cost. Instead of contrastive learning on global image representation, PS3 is pre-trained by selectively processing local regions and contrasting them with local detailed captions, enabling high-resolution representation learning with greatly reduced computational overhead. The pre-trained PS3 is able to both encode the global image at low resolution and selectively process local high-resolution regions based on their saliency or relevance to a text prompt. When applying PS3 to multi-modal LLM (MLLM), the resulting model, named VILA-HD, significantly improves high-resolution visual perception compared to baselines without high-resolution vision pre-training such as AnyRes and S^2 while using up to 4.3x fewer tokens. PS3 also unlocks appealing scaling properties of VILA-HD, including scaling up resolution for free and scaling up test-time compute for better performance. Compared to state of the arts, VILA-HD outperforms previous MLLMs such as NVILA and Qwen2-VL across multiple benchmarks and achieves better efficiency than latest token pruning approaches. Finally, we find current benchmarks do not require 4K-resolution perception, which motivates us to propose 4KPro, a new benchmark of image QA at 4K resolution, on which VILA-HD outperforms all previous MLLMs, including a 14.5% improvement over GPT-4o, and a 3.2% improvement and 2.96x speedup over Qwen2-VL.', 'score': 21, 'issue_id': 2898, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 25', 'zh': '3æœˆ25æ—¥'}, 'hash': '7b81adffd1f39557', 'authors': ['Baifeng Shi', 'Boyi Li', 'Han Cai', 'Yao Lu', 'Sifei Liu', 'Marco Pavone', 'Jan Kautz', 'Song Han', 'Trevor Darrell', 'Pavlo Molchanov', 'Hongxu Yin'], 'affiliations': ['NVIDIA', 'UC Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2503.19903.jpg', 'data': {'categories': ['#optimization', '#training', '#cv', '#multimodal', '#benchmark'], 'emoji': 'ğŸ”', 'ru': {'title': 'PS3: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ ÑĞ²ĞµÑ€Ñ…Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ', 'desc': 'PS3 - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ ÑĞ²ĞµÑ€Ñ…Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ (4K) Ğ¿Ñ€Ğ¸ Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸ Ğ¿Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ…. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, PS3 Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ñ‡Ğ½Ğ¾ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¸ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¸Ñ… Ñ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ÑĞ¼Ğ¸. ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ PS3 Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (MLLM) Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. PS3 Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµÑÑƒÑ€ÑÑ‹ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Scaling Vision Pre-Training to 4K with PS3', 'desc': 'This paper presents PS3, a novel approach to vision pre-training that enables high-resolution image processing at 4K resolution while maintaining a near-constant computational cost. By focusing on local regions of images and contrasting them with detailed captions, PS3 enhances the learning of visual representations without the heavy resource demands typically associated with high-resolution data. The resulting model, VILA-HD, demonstrates significant improvements in visual perception tasks compared to existing models, achieving better efficiency and performance across various benchmarks. Additionally, the authors introduce 4KPro, a new benchmark for image question answering at 4K resolution, where VILA-HD shows superior results over previous models.'}, 'zh': {'title': 'é«˜åˆ†è¾¨ç‡è§†è§‰æ„ŸçŸ¥çš„æ–°çªç ´', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºPS3çš„è§†è§‰é¢„è®­ç»ƒæ–¹æ³•ï¼Œèƒ½å¤Ÿä»¥æ¥è¿‘æ’å®šçš„æˆæœ¬å°†CLIPé£æ ¼çš„è§†è§‰é¢„è®­ç»ƒæ‰©å±•åˆ°4Kåˆ†è¾¨ç‡ã€‚PS3é€šè¿‡é€‰æ‹©æ€§å¤„ç†å±€éƒ¨åŒºåŸŸå¹¶ä¸å±€éƒ¨è¯¦ç»†æè¿°è¿›è¡Œå¯¹æ¯”ï¼Œæ¥å®ç°é«˜åˆ†è¾¨ç‡è¡¨ç¤ºå­¦ä¹ ï¼Œä»è€Œå¤§å¹…é™ä½è®¡ç®—å¼€é”€ã€‚é¢„è®­ç»ƒåçš„PS3èƒ½å¤Ÿåœ¨ä½åˆ†è¾¨ç‡ä¸‹ç¼–ç å…¨å±€å›¾åƒï¼Œå¹¶æ ¹æ®æ–‡æœ¬æç¤ºçš„æ˜¾è‘—æ€§æˆ–ç›¸å…³æ€§é€‰æ‹©æ€§å¤„ç†å±€éƒ¨é«˜åˆ†è¾¨ç‡åŒºåŸŸã€‚æœ€ç»ˆï¼ŒåŸºäºPS3çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹VILA-HDåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº†é«˜åˆ†è¾¨ç‡è§†è§‰æ„ŸçŸ¥èƒ½åŠ›ï¼Œè¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.14905', 'title': 'Spot the Fake: Large Multimodal Model-Based Synthetic Image Detection\n  with Artifact Explanation', 'url': 'https://huggingface.co/papers/2503.14905', 'abstract': 'With the rapid advancement of Artificial Intelligence Generated Content (AIGC) technologies, synthetic images have become increasingly prevalent in everyday life, posing new challenges for authenticity assessment and detection. Despite the effectiveness of existing methods in evaluating image authenticity and locating forgeries, these approaches often lack human interpretability and do not fully address the growing complexity of synthetic data. To tackle these challenges, we introduce FakeVLM, a specialized large multimodal model designed for both general synthetic image and DeepFake detection tasks. FakeVLM not only excels in distinguishing real from fake images but also provides clear, natural language explanations for image artifacts, enhancing interpretability. Additionally, we present FakeClue, a comprehensive dataset containing over 100,000 images across seven categories, annotated with fine-grained artifact clues in natural language. FakeVLM demonstrates performance comparable to expert models while eliminating the need for additional classifiers, making it a robust solution for synthetic data detection. Extensive evaluations across multiple datasets confirm the superiority of FakeVLM in both authenticity classification and artifact explanation tasks, setting a new benchmark for synthetic image detection. The dataset and code will be released in: https://github.com/opendatalab/FakeVLM.', 'score': 15, 'issue_id': 2900, 'pub_date': '2025-03-19', 'pub_date_card': {'ru': '19 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 19', 'zh': '3æœˆ19æ—¥'}, 'hash': 'e8053458773c179b', 'authors': ['Siwei Wen', 'Junyan Ye', 'Peilin Feng', 'Hengrui Kang', 'Zichen Wen', 'Yize Chen', 'Jiang Wu', 'Wenjun Wu', 'Conghui He', 'Weijia Li'], 'affiliations': ['Beihang University', 'Shanghai Artificial Intelligence Laboratory', 'Shanghai Jiao Tong University', 'Sun Yat-Sen University', 'The Chinese University of Hong Kong, Shenzhen'], 'pdf_title_img': 'assets/pdf/title_img/2503.14905.jpg', 'data': {'categories': ['#multimodal', '#interpretability', '#benchmark', '#dataset', '#synthetic', '#cv'], 'emoji': 'ğŸ•µï¸', 'ru': {'title': 'FakeVLM: Ğ˜Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸', 'desc': 'FakeVLM - ÑÑ‚Ğ¾ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ DeepFake. ĞĞ½Ğ° Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ°ĞµÑ‚ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ğ½Ğ¾ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ½ÑÑ‚Ğ½Ñ‹Ğµ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ñ Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ğ¾Ğ² Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… FakeClue Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 100 000 Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ°Ğ¼Ğ¸ Ğ¾Ğ± Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ğ°Ñ…. FakeVLM Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°Ñ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ² Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'FakeVLM: Unmasking Synthetic Images with Clarity', 'desc': 'This paper introduces FakeVLM, a large multimodal model designed to detect synthetic images and DeepFakes while providing human-readable explanations for its decisions. Unlike existing methods, FakeVLM enhances interpretability by explaining image artifacts in natural language, making it easier for users to understand the detection process. The model is evaluated on a new dataset called FakeClue, which contains over 100,000 annotated images, allowing for fine-grained analysis of image authenticity. Results show that FakeVLM performs comparably to expert models without needing extra classifiers, establishing a new standard in synthetic image detection.'}, 'zh': {'title': 'FakeVLMï¼šåˆæˆå›¾åƒæ£€æµ‹çš„æ–°æ ‡æ†', 'desc': 'éšç€äººå·¥æ™ºèƒ½ç”Ÿæˆå†…å®¹ï¼ˆAIGCï¼‰æŠ€æœ¯çš„å¿«é€Ÿå‘å±•ï¼Œåˆæˆå›¾åƒåœ¨æ—¥å¸¸ç”Ÿæ´»ä¸­å˜å¾—è¶Šæ¥è¶Šæ™®éï¼Œè¿™ç»™çœŸå®æ€§è¯„ä¼°å’Œæ£€æµ‹å¸¦æ¥äº†æ–°çš„æŒ‘æˆ˜ã€‚ç°æœ‰çš„æ–¹æ³•è™½ç„¶åœ¨è¯„ä¼°å›¾åƒçœŸå®æ€§å’Œå®šä½ä¼ªé€ æ–¹é¢æœ‰æ•ˆï¼Œä½†å¾€å¾€ç¼ºä¹äººç±»å¯è§£é‡Šæ€§ï¼Œæ— æ³•å®Œå…¨åº”å¯¹åˆæˆæ•°æ®æ—¥ç›Šå¤æ‚çš„æƒ…å†µã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†FakeVLMï¼Œè¿™æ˜¯ä¸€ç§ä¸“é—¨é’ˆå¯¹åˆæˆå›¾åƒå’Œæ·±åº¦ä¼ªé€ æ£€æµ‹ä»»åŠ¡çš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ã€‚FakeVLMä¸ä»…åœ¨åŒºåˆ†çœŸå®ä¸ä¼ªé€ å›¾åƒæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œè¿˜èƒ½æä¾›æ¸…æ™°çš„è‡ªç„¶è¯­è¨€è§£é‡Šï¼Œå¢å¼ºäº†å¯è§£é‡Šæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.13964', 'title': 'MDocAgent: A Multi-Modal Multi-Agent Framework for Document\n  Understanding', 'url': 'https://huggingface.co/papers/2503.13964', 'abstract': "Document Question Answering (DocQA) is a very common task. Existing methods using Large Language Models (LLMs) or Large Vision Language Models (LVLMs) and Retrieval Augmented Generation (RAG) often prioritize information from a single modal, failing to effectively integrate textual and visual cues. These approaches struggle with complex multi-modal reasoning, limiting their performance on real-world documents. We present MDocAgent (A Multi-Modal Multi-Agent Framework for Document Understanding), a novel RAG and multi-agent framework that leverages both text and image. Our system employs five specialized agents: a general agent, a critical agent, a text agent, an image agent and a summarizing agent. These agents engage in multi-modal context retrieval, combining their individual insights to achieve a more comprehensive understanding of the document's content. This collaborative approach enables the system to synthesize information from both textual and visual components, leading to improved accuracy in question answering. Preliminary experiments on five benchmarks like MMLongBench, LongDocURL demonstrate the effectiveness of our MDocAgent, achieve an average improvement of 12.1% compared to current state-of-the-art method. This work contributes to the development of more robust and comprehensive DocQA systems capable of handling the complexities of real-world documents containing rich textual and visual information. Our data and code are available at https://github.com/aiming-lab/MDocAgent.", 'score': 12, 'issue_id': 2900, 'pub_date': '2025-03-18', 'pub_date_card': {'ru': '18 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 18', 'zh': '3æœˆ18æ—¥'}, 'hash': 'ac3677766b7897a0', 'authors': ['Siwei Han', 'Peng Xia', 'Ruiyi Zhang', 'Tong Sun', 'Yun Li', 'Hongtu Zhu', 'Huaxiu Yao'], 'affiliations': ['Adobe Research', 'UNC-Chapel Hill'], 'pdf_title_img': 'assets/pdf/title_img/2503.13964.jpg', 'data': {'categories': ['#multimodal', '#reasoning', '#benchmark', '#optimization', '#agents', '#open_source', '#rag'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²: Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑ Ñ‚ĞµĞºÑÑ‚ Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MDocAgent - Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸Ğ»Ğ¸ RAG, MDocAgent ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²ÑƒÑ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿ÑÑ‚ÑŒ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ² ÑÑ€ĞµĞ´Ğ½ĞµĞ¼ Ğ½Ğ° 12.1% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'MDocAgent: Uniting Text and Images for Smarter Document Understanding', 'desc': 'This paper introduces MDocAgent, a new framework designed for Document Question Answering (DocQA) that effectively combines text and image data. Unlike existing methods that often focus on a single type of information, MDocAgent utilizes a multi-agent system with five specialized agents to enhance multi-modal reasoning. These agents work together to retrieve and synthesize information from both textual and visual sources, improving the accuracy of answers to questions about documents. Preliminary tests show that MDocAgent outperforms current leading methods by an average of 12.1%, demonstrating its potential for better handling complex real-world documents.'}, 'zh': {'title': 'å¤šæ¨¡æ€æ–‡æ¡£ç†è§£çš„æ–°çªç ´', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æ–‡æ¡£é—®ç­”ç³»ç»ŸMDocAgentï¼Œå®ƒç»“åˆäº†æ–‡æœ¬å’Œå›¾åƒä¿¡æ¯ï¼Œæ—¨åœ¨æé«˜å¤šæ¨¡æ€æ–‡æ¡£ç†è§£çš„èƒ½åŠ›ã€‚ç°æœ‰çš„æ–¹æ³•å¾€å¾€åªå…³æ³¨å•ä¸€æ¨¡æ€ï¼Œå¯¼è‡´åœ¨å¤æ‚çš„å¤šæ¨¡æ€æ¨ç†ä¸­è¡¨ç°ä¸ä½³ã€‚MDocAgenté‡‡ç”¨äº†äº”ä¸ªä¸“é—¨çš„ä»£ç†ï¼Œåˆ†åˆ«è´Ÿè´£ä¸åŒçš„ä»»åŠ¡ï¼Œé€šè¿‡åä½œæ£€ç´¢å¤šæ¨¡æ€ä¸Šä¸‹æ–‡ï¼Œä»è€Œæ›´å…¨é¢åœ°ç†è§£æ–‡æ¡£å†…å®¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMDocAgentåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå¹³å‡æé«˜äº†12.1%çš„å‡†ç¡®ç‡ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤„ç†ç°å®ä¸–ç•Œæ–‡æ¡£ä¸­çš„æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.19855', 'title': 'Think Twice: Enhancing LLM Reasoning by Scaling Multi-round Test-time\n  Thinking', 'url': 'https://huggingface.co/papers/2503.19855', 'abstract': "Recent advances in large language models (LLMs), such as OpenAI-o1 and DeepSeek-R1, have demonstrated the effectiveness of test-time scaling, where extended reasoning processes substantially enhance model performance. Despite this, current models are constrained by limitations in handling long texts and reinforcement learning (RL) training efficiency. To address these issues, we propose a simple yet effective test-time scaling approach Multi-round Thinking. This method iteratively refines model reasoning by leveraging previous answers as prompts for subsequent rounds. Extensive experiments across multiple models, including QwQ-32B and DeepSeek-R1, consistently show performance improvements on various benchmarks such as AIME 2024, MATH-500, GPQA-diamond, and LiveCodeBench. For instance, the accuracy of QwQ-32B improved from 80.3% (Round 1) to 82.1% (Round 2) on the AIME 2024 dataset, while DeepSeek-R1 showed a similar increase from 79.7% to 82.0%. These results confirm that Multi-round Thinking is a broadly applicable, straightforward approach to achieving stable enhancements in model performance, underscoring its potential for future developments in test-time scaling techniques. The key prompt: {Original question prompt} The assistant's previous answer is: <answer> {last round answer} </answer>, and please re-answer.", 'score': 11, 'issue_id': 2897, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 25', 'zh': '3æœˆ25æ—¥'}, 'hash': 'c9d16e0d2423104a', 'authors': ['Xiaoyu Tian', 'Sitong Zhao', 'Haotian Wang', 'Shuaiting Chen', 'Yunjie Ji', 'Yiping Peng', 'Han Zhao', 'Xiangang Li'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2503.19855.jpg', 'data': {'categories': ['#long_context', '#benchmark', '#optimization', '#training', '#reasoning'], 'emoji': 'ğŸ”„', 'ru': {'title': 'Ğ˜Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ˜Ğ˜: Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ¹ Ğ¿ÑƒÑ‚ÑŒ Ğº Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸', 'desc': "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ 'ĞœĞ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ÑƒĞ½Ğ´Ğ¾Ğ²Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ'. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº Ğ´Ğ»Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ñ… Ñ€Ğ°ÑƒĞ½Ğ´Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ QwQ-32B Ğ¸ DeepSeek-R1, Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ 'ĞœĞ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ÑƒĞ½Ğ´Ğ¾Ğ²Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ' - ÑÑ‚Ğ¾ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."}, 'en': {'title': 'Enhancing Model Performance with Multi-round Thinking', 'desc': "This paper introduces a new method called Multi-round Thinking to improve the performance of large language models (LLMs) during test-time scaling. The approach involves iteratively refining the model's reasoning by using previous answers as prompts for subsequent rounds of questioning. Experiments show that this method leads to significant accuracy improvements across various benchmarks, demonstrating its effectiveness in enhancing model performance. The results indicate that Multi-round Thinking is a simple yet powerful technique that can be widely applied to improve LLMs' reasoning capabilities."}, 'zh': {'title': 'å¤šè½®æ€è€ƒï¼šæå‡æ¨¡å‹æ¨ç†çš„æœ‰æ•ˆæ–¹æ³•', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºå¤šè½®æ€è€ƒçš„æµ‹è¯•æ—¶é—´æ‰©å±•æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚è¯¥æ–¹æ³•é€šè¿‡å°†ä¹‹å‰çš„ç­”æ¡ˆä½œä¸ºåç»­è½®æ¬¡çš„æç¤ºï¼Œè¿­ä»£åœ°ä¼˜åŒ–æ¨¡å‹çš„æ¨ç†è¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨å¤šè½®æ€è€ƒåï¼Œå¤šä¸ªæ¨¡å‹åœ¨ä¸åŒåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°å‡æœ‰æ˜¾è‘—æå‡ã€‚æ¯”å¦‚ï¼ŒQwQ-32Båœ¨AIME 2024æ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡ä»80.3%æå‡è‡³82.1%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.19910', 'title': 'CoLLM: A Large Language Model for Composed Image Retrieval', 'url': 'https://huggingface.co/papers/2503.19910', 'abstract': 'Composed Image Retrieval (CIR) is a complex task that aims to retrieve images based on a multimodal query. Typical training data consists of triplets containing a reference image, a textual description of desired modifications, and the target image, which are expensive and time-consuming to acquire. The scarcity of CIR datasets has led to zero-shot approaches utilizing synthetic triplets or leveraging vision-language models (VLMs) with ubiquitous web-crawled image-caption pairs. However, these methods have significant limitations: synthetic triplets suffer from limited scale, lack of diversity, and unnatural modification text, while image-caption pairs hinder joint embedding learning of the multimodal query due to the absence of triplet data. Moreover, existing approaches struggle with complex and nuanced modification texts that demand sophisticated fusion and understanding of vision and language modalities. We present CoLLM, a one-stop framework that effectively addresses these limitations. Our approach generates triplets on-the-fly from image-caption pairs, enabling supervised training without manual annotation. We leverage Large Language Models (LLMs) to generate joint embeddings of reference images and modification texts, facilitating deeper multimodal fusion. Additionally, we introduce Multi-Text CIR (MTCIR), a large-scale dataset comprising 3.4M samples, and refine existing CIR benchmarks (CIRR and Fashion-IQ) to enhance evaluation reliability. Experimental results demonstrate that CoLLM achieves state-of-the-art performance across multiple CIR benchmarks and settings. MTCIR yields competitive results, with up to 15% performance improvement. Our refined benchmarks provide more reliable evaluation metrics for CIR models, contributing to the advancement of this important field.', 'score': 8, 'issue_id': 2896, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 25', 'zh': '3æœˆ25æ—¥'}, 'hash': '63f36082e6c27f3e', 'authors': ['Chuong Huynh', 'Jinyu Yang', 'Ashish Tawari', 'Mubarak Shah', 'Son Tran', 'Raffay Hamid', 'Trishul Chilimbi', 'Abhinav Shrivastava'], 'affiliations': ['Amazon', 'Center for Research in Computer Vision, University of Central Florida', 'University of Maryland, College Park'], 'pdf_title_img': 'assets/pdf/title_img/2503.19910.jpg', 'data': {'categories': ['#optimization', '#synthetic', '#dataset', '#multimodal', '#benchmark'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'CoLLM: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ¸ÑĞºĞµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ CoLLM - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ (CIR). CoLLM Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ğµ Ñ‚Ñ€Ğ¸Ğ¿Ğ»ĞµÑ‚Ñ‹ Ğ½Ğ° Ğ»ĞµÑ‚Ñƒ Ğ¸Ğ· Ğ¿Ğ°Ñ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ-Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑÑŒ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒÑÑ Ğ±ĞµĞ· Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ñ‹Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸, ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ MTCIR Ğ¸ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ñ‹ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ CIR.'}, 'en': {'title': 'Revolutionizing Composed Image Retrieval with CoLLM', 'desc': 'This paper introduces CoLLM, a novel framework for Composed Image Retrieval (CIR) that overcomes the challenges of limited training data. It generates triplets from existing image-caption pairs, allowing for supervised training without the need for manual annotations. By utilizing Large Language Models (LLMs), CoLLM creates joint embeddings that enhance the understanding of complex multimodal queries. The authors also present the Multi-Text CIR (MTCIR) dataset, which significantly improves evaluation metrics and demonstrates state-of-the-art performance in CIR tasks.'}, 'zh': {'title': 'CoLLMï¼šå¤åˆå›¾åƒæ£€ç´¢çš„æ–°çªç ´', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºCoLLMçš„æ¡†æ¶ï¼Œç”¨äºè§£å†³å¤åˆå›¾åƒæ£€ç´¢ï¼ˆCIRï¼‰ä¸­çš„æ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡ä»å›¾åƒ-æ–‡æœ¬å¯¹ä¸­åŠ¨æ€ç”Ÿæˆä¸‰å…ƒç»„ï¼Œé¿å…äº†æ‰‹åŠ¨æ ‡æ³¨çš„éœ€æ±‚ï¼Œä»è€Œå®ç°äº†ç›‘ç£å­¦ä¹ ã€‚æˆ‘ä»¬åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç”Ÿæˆå‚è€ƒå›¾åƒå’Œä¿®æ”¹æ–‡æœ¬çš„è”åˆåµŒå…¥ï¼Œä¿ƒè¿›äº†å¤šæ¨¡æ€çš„æ·±åº¦èåˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¨å‡ºäº†ä¸€ä¸ªåŒ…å«340ä¸‡æ ·æœ¬çš„å¤§è§„æ¨¡æ•°æ®é›†MTCIRï¼Œå¹¶æ”¹è¿›äº†ç°æœ‰çš„CIRåŸºå‡†ï¼Œä»¥æé«˜è¯„ä¼°çš„å¯é æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.19470', 'title': 'ReSearch: Learning to Reason with Search for LLMs via Reinforcement\n  Learning', 'url': 'https://huggingface.co/papers/2503.19470', 'abstract': 'Large Language Models (LLMs) have shown remarkable capabilities in reasoning, exemplified by the success of OpenAI-o1 and DeepSeek-R1. However, integrating reasoning with external search processes remains challenging, especially for complex multi-hop questions requiring multiple retrieval steps. We propose ReSearch, a novel framework that trains LLMs to Reason with Search via reinforcement learning without using any supervised data on reasoning steps. Our approach treats search operations as integral components of the reasoning chain, where when and how to perform searches is guided by text-based thinking, and search results subsequently influence further reasoning. We train ReSearch on Qwen2.5-7B(-Instruct) and Qwen2.5-32B(-Instruct) models and conduct extensive experiments. Despite being trained on only one dataset, our models demonstrate strong generalizability across various benchmarks. Analysis reveals that ReSearch naturally elicits advanced reasoning capabilities such as reflection and self-correction during the reinforcement learning process.', 'score': 8, 'issue_id': 2897, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 25', 'zh': '3æœˆ25æ—¥'}, 'hash': '3e50fa3f4f4a6c0c', 'authors': ['Mingyang Chen', 'Tianpeng Li', 'Haoze Sun', 'Yijie Zhou', 'Chenzheng Zhu', 'Fan Yang', 'Zenan Zhou', 'Weipeng Chen', 'Haofen Wang', 'Jeff Z. Pan', 'Wen Zhang', 'Huajun Chen'], 'affiliations': ['Baichuan Inc.', 'The University of Edinburgh', 'Tongji University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.19470.jpg', 'data': {'categories': ['#rl', '#benchmark', '#rag', '#training', '#reasoning'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ£ÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ˜Ğ˜ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ°', 'desc': 'ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº ReSearch, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑƒÑ‡Ğ¸Ñ‚ÑÑ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ² Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºÑƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ³Ğ´Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚, ĞºĞ¾Ğ³Ğ´Ğ° Ğ¸ ĞºĞ°Ğº Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ¸ÑĞº. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾, Ğ½ĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑĞ¸Ğ»ÑŒĞ½ÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚, Ñ‡Ñ‚Ğ¾ ReSearch ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ¼ Ğ²Ñ‹Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ñ Ğ¸ ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ñ.'}, 'en': {'title': 'Empowering LLMs: Reasoning Meets Search with ReSearch', 'desc': 'This paper introduces ReSearch, a new framework that enhances Large Language Models (LLMs) by integrating reasoning with external search processes. It uses reinforcement learning to train LLMs to effectively decide when and how to perform searches, treating these operations as key parts of the reasoning process. The framework is tested on Qwen2.5 models and shows strong performance across different benchmarks, even though it was trained on a single dataset. Notably, ReSearch enables advanced reasoning skills like reflection and self-correction, showcasing its potential for complex question answering.'}, 'zh': {'title': 'æ¨ç†ä¸æœç´¢çš„å®Œç¾ç»“åˆ', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å°†æ¨ç†ä¸å¤–éƒ¨æœç´¢è¿‡ç¨‹ç»“åˆä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå°¤å…¶æ˜¯å¯¹äºå¤æ‚çš„å¤šè·³é—®é¢˜ã€‚æˆ‘ä»¬æå‡ºäº†ReSearchï¼Œä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒLLMsè¿›è¡Œæœç´¢æ¨ç†ï¼Œè€Œä¸ä½¿ç”¨ä»»ä½•ç›‘ç£æ•°æ®ã€‚è¯¥æ–¹æ³•å°†æœç´¢æ“ä½œè§†ä¸ºæ¨ç†é“¾çš„æ ¸å¿ƒéƒ¨åˆ†ï¼Œæœç´¢çš„æ—¶æœºå’Œæ–¹å¼ç”±åŸºäºæ–‡æœ¬çš„æ€ç»´æŒ‡å¯¼ï¼Œæœç´¢ç»“æœè¿›ä¸€æ­¥å½±å“æ¨ç†è¿‡ç¨‹ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œå°½ç®¡åªåœ¨ä¸€ä¸ªæ•°æ®é›†ä¸Šè®­ç»ƒï¼ŒReSearchæ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.18446', 'title': 'Latent Space Super-Resolution for Higher-Resolution Image Generation\n  with Diffusion Models', 'url': 'https://huggingface.co/papers/2503.18446', 'abstract': 'In this paper, we propose LSRNA, a novel framework for higher-resolution (exceeding 1K) image generation using diffusion models by leveraging super-resolution directly in the latent space. Existing diffusion models struggle with scaling beyond their training resolutions, often leading to structural distortions or content repetition. Reference-based methods address the issues by upsampling a low-resolution reference to guide higher-resolution generation. However, they face significant challenges: upsampling in latent space often causes manifold deviation, which degrades output quality. On the other hand, upsampling in RGB space tends to produce overly smoothed outputs. To overcome these limitations, LSRNA combines Latent space Super-Resolution (LSR) for manifold alignment and Region-wise Noise Addition (RNA) to enhance high-frequency details. Our extensive experiments demonstrate that integrating LSRNA outperforms state-of-the-art reference-based methods across various resolutions and metrics, while showing the critical role of latent space upsampling in preserving detail and sharpness. The code is available at https://github.com/3587jjh/LSRNA.', 'score': 6, 'issue_id': 2897, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 24', 'zh': '3æœˆ24æ—¥'}, 'hash': 'd3e203fb399d6eee', 'authors': ['Jinho Jeong', 'Sangmin Han', 'Jinwoo Kim', 'Seon Joo Kim'], 'affiliations': ['Yonsei University'], 'pdf_title_img': 'assets/pdf/title_img/2503.18446.jpg', 'data': {'categories': ['#open_source', '#diffusion', '#3d', '#optimization', '#cv'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'LSRNA: Ğ¡ÑƒĞ¿ĞµÑ€Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ² Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'LSRNA - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ (Ğ±Ğ¾Ğ»ĞµĞµ 1K) Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´ÑÑ‚ Ğº Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¸Ğ»Ğ¸ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ¿Ñ€Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸. LSRNA ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ ÑÑƒĞ¿ĞµÑ€Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ² Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ (LSR) Ğ´Ğ»Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ¸ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ÑˆÑƒĞ¼Ğ° Ğ¿Ğ¾ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ°Ğ¼ (RNA) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ LSRNA Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€ĞµÑ„ĞµÑ€ĞµĞ½ÑĞ¾Ğ² Ğ¿Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸ÑĞ¼ Ğ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼.'}, 'en': {'title': 'Enhancing High-Resolution Image Generation with LSRNA', 'desc': 'This paper introduces LSRNA, a new framework designed to generate high-resolution images (over 1K) using diffusion models by applying super-resolution techniques in the latent space. Traditional diffusion models often struggle with generating images at higher resolutions, leading to issues like distortions and repeated content. The proposed method addresses these challenges by utilizing Latent space Super-Resolution (LSR) for better alignment and Region-wise Noise Addition (RNA) to improve detail in the generated images. Experimental results show that LSRNA significantly outperforms existing reference-based methods, highlighting the importance of latent space upsampling for maintaining image quality.'}, 'zh': {'title': 'LSRNAï¼šè¶…åˆ†è¾¨ç‡ç”Ÿæˆçš„åˆ›æ–°æ¡†æ¶', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶LSRNAï¼Œç”¨äºç”Ÿæˆé«˜åˆ†è¾¨ç‡ï¼ˆè¶…è¿‡1Kï¼‰çš„å›¾åƒï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹ç›´æ¥åœ¨æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œè¶…åˆ†è¾¨ç‡å¤„ç†ã€‚ç°æœ‰çš„æ‰©æ•£æ¨¡å‹åœ¨è¶…å‡ºè®­ç»ƒåˆ†è¾¨ç‡æ—¶å¸¸å¸¸å‡ºç°ç»“æ„å¤±çœŸæˆ–å†…å®¹é‡å¤çš„é—®é¢˜ã€‚å‚è€ƒåŸºç¡€çš„æ–¹æ³•é€šè¿‡å°†ä½åˆ†è¾¨ç‡å‚è€ƒå›¾åƒä¸Šé‡‡æ ·æ¥æŒ‡å¯¼é«˜åˆ†è¾¨ç‡ç”Ÿæˆï¼Œä½†åœ¨æ½œåœ¨ç©ºé—´ä¸­ä¸Šé‡‡æ ·ä¼šå¯¼è‡´æµå½¢åå·®ï¼Œä»è€Œé™ä½è¾“å‡ºè´¨é‡ã€‚LSRNAç»“åˆäº†æ½œåœ¨ç©ºé—´è¶…åˆ†è¾¨ç‡ï¼ˆLSRï¼‰å’ŒåŒºåŸŸå™ªå£°æ·»åŠ ï¼ˆRNAï¼‰ï¼Œæœ‰æ•ˆæå‡äº†é«˜é¢‘ç»†èŠ‚ï¼Œå®éªŒç»“æœè¡¨æ˜å…¶åœ¨å„ä¸ªåˆ†è¾¨ç‡å’ŒæŒ‡æ ‡ä¸Šå‡ä¼˜äºç°æœ‰çš„å‚è€ƒåŸºç¡€æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.19065', 'title': 'WikiAutoGen: Towards Multi-Modal Wikipedia-Style Article Generation', 'url': 'https://huggingface.co/papers/2503.19065', 'abstract': 'Knowledge discovery and collection are intelligence-intensive tasks that traditionally require significant human effort to ensure high-quality outputs. Recent research has explored multi-agent frameworks for automating Wikipedia-style article generation by retrieving and synthesizing information from the internet. However, these methods primarily focus on text-only generation, overlooking the importance of multimodal content in enhancing informativeness and engagement. In this work, we introduce WikiAutoGen, a novel system for automated multimodal Wikipedia-style article generation. Unlike prior approaches, WikiAutoGen retrieves and integrates relevant images alongside text, enriching both the depth and visual appeal of generated content. To further improve factual accuracy and comprehensiveness, we propose a multi-perspective self-reflection mechanism, which critically assesses retrieved content from diverse viewpoints to enhance reliability, breadth, and coherence, etc. Additionally, we introduce WikiSeek, a benchmark comprising Wikipedia articles with topics paired with both textual and image-based representations, designed to evaluate multimodal knowledge generation on more challenging topics. Experimental results show that WikiAutoGen outperforms previous methods by 8%-29% on our WikiSeek benchmark, producing more accurate, coherent, and visually enriched Wikipedia-style articles. We show some of our generated examples in https://wikiautogen.github.io/ .', 'score': 5, 'issue_id': 2908, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 24', 'zh': '3æœˆ24æ—¥'}, 'hash': 'aaba6b8dd7c54bf2', 'authors': ['Zhongyu Yang', 'Jun Chen', 'Dannong Xu', 'Junjie Fei', 'Xiaoqian Shen', 'Liangbing Zhao', 'Chun-Mei Feng', 'Mohamed Elhoseiny'], 'affiliations': ['IHPC, A*STAR', 'King Abdullah University of Science and Technology', 'Lanzhou University', 'The University of Sydney'], 'pdf_title_img': 'assets/pdf/title_img/2503.19065.jpg', 'data': {'categories': ['#story_generation', '#benchmark', '#interpretability', '#multimodal'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'WikiAutoGen: Ğ˜Ğ˜ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑÑ‚Ğ°Ñ‚ÑŒĞ¸ Ğ’Ğ¸ĞºĞ¸Ğ¿ĞµĞ´Ğ¸Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ WikiAutoGen - Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ‚Ğ°Ñ‚ĞµĞ¹ Ğ² ÑÑ‚Ğ¸Ğ»Ğµ Ğ’Ğ¸ĞºĞ¸Ğ¿ĞµĞ´Ğ¸Ğ¸. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², WikiAutoGen Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ Ğ¸ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ²Ğ¼ĞµÑÑ‚Ğµ Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼, Ğ¾Ğ±Ğ¾Ğ³Ğ°Ñ‰Ğ°Ñ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñƒ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¿Ñ€Ğ¸Ğ²Ğ»ĞµĞºĞ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ ÑĞ°Ğ¼Ğ¾Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ WikiSeek - ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼ Ñ‚ĞµĞ¼Ğ°Ğ¼.'}, 'en': {'title': 'Enhancing Wikipedia Articles with Multimodal Automation', 'desc': 'This paper presents WikiAutoGen, a system designed to automate the generation of Wikipedia-style articles using both text and images. Unlike previous methods that focused solely on text, WikiAutoGen enhances the informativeness and engagement of articles by integrating relevant images. The system employs a multi-perspective self-reflection mechanism to ensure the accuracy and coherence of the content by evaluating it from various viewpoints. Experimental results demonstrate that WikiAutoGen significantly outperforms existing approaches, achieving higher quality in multimodal article generation.'}, 'zh': {'title': 'è‡ªåŠ¨ç”Ÿæˆå¤šæ¨¡æ€ç»´åŸºç™¾ç§‘æ–‡ç« çš„æ–°æ–¹æ³•', 'desc': 'æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„ç³»ç»ŸWikiAutoGenï¼Œç”¨äºè‡ªåŠ¨ç”Ÿæˆå¤šæ¨¡æ€çš„ç»´åŸºç™¾ç§‘é£æ ¼æ–‡ç« ã€‚ä¸ä»¥å¾€ä»…å…³æ³¨æ–‡æœ¬ç”Ÿæˆçš„æ–¹æ³•ä¸åŒï¼ŒWikiAutoGenåŒæ—¶æ£€ç´¢å’Œæ•´åˆç›¸å…³å›¾åƒï¼Œå¢å¼ºäº†ç”Ÿæˆå†…å®¹çš„æ·±åº¦å’Œè§†è§‰å¸å¼•åŠ›ã€‚ä¸ºäº†æé«˜äº‹å®å‡†ç¡®æ€§å’Œå…¨é¢æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å¤šè§’åº¦è‡ªæˆ‘åæ€æœºåˆ¶ï¼Œä»ä¸åŒè§†è§’è¯„ä¼°æ£€ç´¢å†…å®¹ï¼Œä»¥å¢å¼ºå¯é æ€§å’Œè¿è´¯æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¨å‡ºäº†WikiSeekåŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°åœ¨æ›´å…·æŒ‘æˆ˜æ€§ä¸»é¢˜ä¸Šçš„å¤šæ¨¡æ€çŸ¥è¯†ç”Ÿæˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.19041', 'title': 'LookAhead Tuning: Safer Language Models via Partial Answer Previews', 'url': 'https://huggingface.co/papers/2503.19041', 'abstract': "Fine-tuning enables large language models (LLMs) to adapt to specific domains, but often undermines their previously established safety alignment. To mitigate the degradation of model safety during fine-tuning, we introduce LookAhead Tuning, which comprises two simple, low-resource, and effective data-driven methods that modify training data by previewing partial answer prefixes. Both methods aim to preserve the model's inherent safety mechanisms by minimizing perturbations to initial token distributions. Comprehensive experiments demonstrate that LookAhead Tuning effectively maintains model safety without sacrificing robust performance on downstream tasks. Our findings position LookAhead Tuning as a reliable and efficient solution for the safe and effective adaptation of LLMs. Code is released at https://github.com/zjunlp/LookAheadTuning.", 'score': 4, 'issue_id': 2896, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 24', 'zh': '3æœˆ24æ—¥'}, 'hash': '8495b5a09ddf5611', 'authors': ['Kangwei Liu', 'Mengru Wang', 'Yujie Luo', 'Lin Yuan', 'Mengshu Sun', 'Ningyu Zhang', 'Lei Liang', 'Zhiqiang Zhang', 'Jun Zhou', 'Huajun Chen'], 'affiliations': ['Ant Group', 'Zhejiang University', 'Zhejiang University - Ant Group Joint Laboratory of Knowledge Graph'], 'pdf_title_img': 'assets/pdf/title_img/2503.19041.jpg', 'data': {'categories': ['#training', '#alignment', '#low_resource'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ‘ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ°Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ LookAhead Tuning. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğ¼ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ğ¼, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¸Ñ… Ğ¸Ğ·Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑĞ¼Ğ¾Ñ‚Ñ€Ğ° Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµÑ„Ğ¸ĞºÑĞ¾Ğ² Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ LookAhead Tuning ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±ĞµĞ· ÑƒÑ‰ĞµÑ€Ğ±Ğ° Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ†ĞµĞ»ĞµĞ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ….'}, 'en': {'title': 'LookAhead Tuning: Safeguarding LLMs During Fine-Tuning', 'desc': "This paper presents LookAhead Tuning, a novel approach to fine-tuning large language models (LLMs) while preserving their safety alignment. The method involves two data-driven techniques that adjust training data by examining partial answer prefixes, which helps maintain the model's safety mechanisms. By minimizing changes to the initial token distributions, LookAhead Tuning effectively prevents safety degradation during the adaptation process. Experimental results show that this approach not only safeguards model safety but also ensures strong performance on various downstream tasks."}, 'zh': {'title': 'LookAhead Tuningï¼šå®‰å…¨å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹çš„æ–°æ–¹æ³•', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºLookAhead Tuningçš„æŠ€æœ¯ï¼Œæ—¨åœ¨è§£å†³åœ¨å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ—¶å®‰å…¨æ€§ä¸‹é™çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡é¢„è§ˆéƒ¨åˆ†ç­”æ¡ˆå‰ç¼€ï¼Œé‡‡ç”¨ä¸¤ç§ç®€å•ä¸”ä½èµ„æºçš„æ•°æ®é©±åŠ¨æ–¹æ³•æ¥ä¿®æ”¹è®­ç»ƒæ•°æ®ã€‚å…¶ç›®æ ‡æ˜¯é€šè¿‡æœ€å°åŒ–åˆå§‹æ ‡è®°åˆ†å¸ƒçš„æ‰°åŠ¨ï¼Œä¿æŒæ¨¡å‹å›ºæœ‰çš„å®‰å…¨æœºåˆ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLookAhead Tuningèƒ½å¤Ÿæœ‰æ•ˆç»´æŠ¤æ¨¡å‹çš„å®‰å…¨æ€§ï¼ŒåŒæ—¶åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­ä¿æŒå¼ºå¤§çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.19907', 'title': 'FullDiT: Multi-Task Video Generative Foundation Model with Full\n  Attention', 'url': 'https://huggingface.co/papers/2503.19907', 'abstract': 'Current video generative foundation models primarily focus on text-to-video tasks, providing limited control for fine-grained video content creation. Although adapter-based approaches (e.g., ControlNet) enable additional controls with minimal fine-tuning, they encounter challenges when integrating multiple conditions, including: branch conflicts between independently trained adapters, parameter redundancy leading to increased computational cost, and suboptimal performance compared to full fine-tuning. To address these challenges, we introduce FullDiT, a unified foundation model for video generation that seamlessly integrates multiple conditions via unified full-attention mechanisms. By fusing multi-task conditions into a unified sequence representation and leveraging the long-context learning ability of full self-attention to capture condition dynamics, FullDiT reduces parameter overhead, avoids conditions conflict, and shows scalability and emergent ability. We further introduce FullBench for multi-task video generation evaluation. Experiments demonstrate that FullDiT achieves state-of-the-art results, highlighting the efficacy of full-attention in complex multi-task video generation.', 'score': 3, 'issue_id': 2910, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 25', 'zh': '3æœˆ25æ—¥'}, 'hash': '7505b749f1328947', 'authors': ['Xuan Ju', 'Weicai Ye', 'Quande Liu', 'Qiulin Wang', 'Xintao Wang', 'Pengfei Wan', 'Di Zhang', 'Kun Gai', 'Qiang Xu'], 'affiliations': ['Kuaishou Technology', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2503.19907.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#video', '#long_context', '#games'], 'emoji': 'ğŸ¬', 'ru': {'title': 'FullDiT: Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'FullDiT - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. ĞĞ½Ğ° Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ñ‹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ğ°Ğ¼Ğ¸ Ğ¸ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². FullDiT Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞµĞ´Ğ¸Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ³Ğ¾ ÑĞ°Ğ¼Ğ¾Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ FullDiT Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑĞ¼Ğ¸.'}, 'en': {'title': 'FullDiT: Unifying Control for Advanced Video Generation', 'desc': 'This paper presents FullDiT, a novel video generative model that enhances control over video content creation by integrating multiple conditions effectively. Unlike existing adapter-based methods, FullDiT utilizes a unified full-attention mechanism to manage various input conditions without the issues of parameter redundancy and branch conflicts. The model captures the dynamics of conditions through a unified sequence representation, which improves scalability and performance. Additionally, the authors introduce FullBench, a new evaluation framework for assessing multi-task video generation, demonstrating that FullDiT achieves state-of-the-art results in this domain.'}, 'zh': {'title': 'å…¨æ³¨æ„åŠ›ï¼Œè§†é¢‘ç”Ÿæˆçš„æ–°çªç ´', 'desc': 'å½“å‰çš„è§†é¢‘ç”ŸæˆåŸºç¡€æ¨¡å‹ä¸»è¦é›†ä¸­åœ¨æ–‡æœ¬åˆ°è§†é¢‘çš„ä»»åŠ¡ä¸Šï¼Œæä¾›çš„ç»†ç²’åº¦è§†é¢‘å†…å®¹åˆ›ä½œæ§åˆ¶æœ‰é™ã€‚è™½ç„¶åŸºäºé€‚é…å™¨çš„æ–¹æ³•ï¼ˆå¦‚ControlNetï¼‰å¯ä»¥åœ¨æœ€å°å¾®è°ƒçš„æƒ…å†µä¸‹å®ç°é¢å¤–æ§åˆ¶ï¼Œä½†åœ¨æ•´åˆå¤šä¸ªæ¡ä»¶æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå¦‚ç‹¬ç«‹è®­ç»ƒçš„é€‚é…å™¨ä¹‹é—´çš„åˆ†æ”¯å†²çªã€å‚æ•°å†—ä½™å¯¼è‡´çš„è®¡ç®—æˆæœ¬å¢åŠ ï¼Œä»¥åŠä¸å®Œå…¨å¾®è°ƒç›¸æ¯”çš„æ€§èƒ½ä¸è¶³ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†FullDiTï¼Œè¿™æ˜¯ä¸€ç§ç»Ÿä¸€çš„è§†é¢‘ç”ŸæˆåŸºç¡€æ¨¡å‹ï¼Œé€šè¿‡ç»Ÿä¸€çš„å…¨æ³¨æ„åŠ›æœºåˆ¶æ— ç¼æ•´åˆå¤šä¸ªæ¡ä»¶ã€‚å®éªŒè¡¨æ˜ï¼ŒFullDiTåœ¨å¤æ‚çš„å¤šä»»åŠ¡è§†é¢‘ç”Ÿæˆä¸­å®ç°äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œçªæ˜¾äº†å…¨æ³¨æ„åŠ›åœ¨æ•æ‰æ¡ä»¶åŠ¨æ€æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.18893', 'title': 'xKV: Cross-Layer SVD for KV-Cache Compression', 'url': 'https://huggingface.co/papers/2503.18893', 'abstract': "Large Language Models (LLMs) with long context windows enable powerful applications but come at the cost of high memory consumption to store the Key and Value states (KV-Cache). Recent studies attempted to merge KV-cache from multiple layers into shared representations, yet these approaches either require expensive pretraining or rely on assumptions of high per-token cosine similarity across layers which generally does not hold in practice. We find that the dominant singular vectors are remarkably well-aligned across multiple layers of the KV-Cache. Exploiting this insight, we propose xKV, a simple post-training method that applies Singular Value Decomposition (SVD) on the KV-Cache of grouped layers. xKV consolidates the KV-Cache of multiple layers into a shared low-rank subspace, significantly reducing KV-Cache sizes. Through extensive evaluations on the RULER long-context benchmark with widely-used LLMs (e.g., Llama-3.1 and Qwen2.5), xKV achieves up to 6.8x higher compression rates than state-of-the-art inter-layer technique while improving accuracy by 2.7%. Moreover, xKV is compatible with the emerging Multi-Head Latent Attention (MLA) (e.g., DeepSeek-Coder-V2), yielding a notable 3x compression rates on coding tasks without performance degradation. These results highlight xKV's strong capability and versatility in addressing memory bottlenecks for long-context LLM inference. Our code is publicly available at: https://github.com/abdelfattah-lab/xKV.", 'score': 3, 'issue_id': 2909, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 24', 'zh': '3æœˆ24æ—¥'}, 'hash': 'b933f4d58f9fb03a', 'authors': ['Chi-Chih Chang', 'Chien-Yu Lin', 'Yash Akhauri', 'Wei-Cheng Lin', 'Kai-Chiang Wu', 'Luis Ceze', 'Mohamed S. Abdelfattah'], 'affiliations': ['Cornell University', 'National Yang Ming Chiao Tung University', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2503.18893.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#open_source', '#long_context', '#architecture', '#inference'], 'emoji': 'ğŸ—œï¸', 'ru': {'title': 'xKV: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ´Ğ»Ñ LLM Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ xKV Ğ´Ğ»Ñ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ KV-Cache Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM) Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ğ¼ Ğ¾ĞºĞ½Ğ¾Ğ¼. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¸Ğ½Ğ³ÑƒĞ»ÑÑ€Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ (SVD) Ğ´Ğ»Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ KV-Cache Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑĞ»Ğ¾ĞµĞ² Ğ² Ğ¾Ğ±Ñ‰ĞµĞµ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€Ğ°Ğ½Ğ³Ğ¾Ğ²Ğ¾Ğµ Ğ¿Ğ¾Ğ´Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾. xKV Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ´Ğ¾ 6.8-ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° 2.7% Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ RULER. ĞœĞµÑ‚Ğ¾Ğ´ Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼ Ñ Multi-Head Latent Attention (MLA) Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ 3-ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'xKV: Efficient Memory Management for Long-Context LLMs', 'desc': 'This paper introduces xKV, a method designed to reduce the memory usage of Key and Value states (KV-Cache) in Large Language Models (LLMs) with long context windows. By applying Singular Value Decomposition (SVD) to the KV-Cache of grouped layers, xKV consolidates these caches into a shared low-rank subspace, leading to significant size reductions. The method achieves up to 6.8 times higher compression rates compared to existing techniques while also improving model accuracy by 2.7%. Additionally, xKV is compatible with Multi-Head Latent Attention models, demonstrating its effectiveness in various coding tasks without sacrificing performance.'}, 'zh': {'title': 'xKVï¼šé«˜æ•ˆå‹ç¼©é•¿ä¸Šä¸‹æ–‡æ¨¡å‹çš„å…³é”®æŠ€æœ¯', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†é•¿ä¸Šä¸‹æ–‡æ—¶å…·æœ‰å¼ºå¤§çš„åº”ç”¨èƒ½åŠ›ï¼Œä½†éœ€è¦æ¶ˆè€—å¤§é‡å†…å­˜æ¥å­˜å‚¨é”®å€¼ç¼“å­˜ï¼ˆKV-Cacheï¼‰ã€‚æœ€è¿‘çš„ç ”ç©¶å°è¯•å°†å¤šä¸ªå±‚çš„KVç¼“å­˜åˆå¹¶ä¸ºå…±äº«è¡¨ç¤ºï¼Œä½†è¿™äº›æ–¹æ³•é€šå¸¸éœ€è¦æ˜‚è´µçš„é¢„è®­ç»ƒæˆ–ä¾èµ–äºå±‚é—´é«˜ä½™å¼¦ç›¸ä¼¼æ€§çš„å‡è®¾ï¼Œè¿™åœ¨å®é™…ä¸­å¹¶ä¸æˆç«‹ã€‚æˆ‘ä»¬å‘ç°ï¼ŒKVç¼“å­˜çš„ä¸»å¥‡å¼‚å‘é‡åœ¨å¤šä¸ªå±‚ä¹‹é—´å¯¹é½å¾—éå¸¸å¥½ã€‚åŸºäºè¿™ä¸€å‘ç°ï¼Œæˆ‘ä»¬æå‡ºäº†xKVï¼Œè¿™æ˜¯ä¸€ç§ç®€å•çš„åè®­ç»ƒæ–¹æ³•ï¼Œé€šè¿‡å¯¹åˆ†ç»„å±‚çš„KVç¼“å­˜åº”ç”¨å¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰ï¼Œæ˜¾è‘—å‡å°‘äº†KVç¼“å­˜çš„å¤§å°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.17973', 'title': 'PhysTwin: Physics-Informed Reconstruction and Simulation of Deformable\n  Objects from Videos', 'url': 'https://huggingface.co/papers/2503.17973', 'abstract': 'Creating a physical digital twin of a real-world object has immense potential in robotics, content creation, and XR. In this paper, we present PhysTwin, a novel framework that uses sparse videos of dynamic objects under interaction to produce a photo- and physically realistic, real-time interactive virtual replica. Our approach centers on two key components: (1) a physics-informed representation that combines spring-mass models for realistic physical simulation, generative shape models for geometry, and Gaussian splats for rendering; and (2) a novel multi-stage, optimization-based inverse modeling framework that reconstructs complete geometry, infers dense physical properties, and replicates realistic appearance from videos. Our method integrates an inverse physics framework with visual perception cues, enabling high-fidelity reconstruction even from partial, occluded, and limited viewpoints. PhysTwin supports modeling various deformable objects, including ropes, stuffed animals, cloth, and delivery packages. Experiments show that PhysTwin outperforms competing methods in reconstruction, rendering, future prediction, and simulation under novel interactions. We further demonstrate its applications in interactive real-time simulation and model-based robotic motion planning.', 'score': 3, 'issue_id': 2910, 'pub_date': '2025-03-23', 'pub_date_card': {'ru': '23 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 23', 'zh': '3æœˆ23æ—¥'}, 'hash': '91f6aafdb1c387f0', 'authors': ['Hanxiao Jiang', 'Hao-Yu Hsu', 'Kaifeng Zhang', 'Hsin-Ni Yu', 'Shenlong Wang', 'Yunzhu Li'], 'affiliations': ['Columbia University', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2503.17973.jpg', 'data': {'categories': ['#optimization', '#3d', '#robotics'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'PhysTwin: Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ñ†Ğ¸Ñ„Ñ€Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ¸ĞºĞ¾Ğ² Ğ¸Ğ· Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ PhysTwin - Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ„Ğ¾Ñ‚Ğ¾- Ğ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ†Ğ¸Ñ„Ñ€Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ¸ĞºĞ¾Ğ² Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹ Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰ĞµĞµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€ÑƒĞ¶Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ°ÑÑ, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ„Ğ¾Ñ€Ğ¼ Ğ¸ Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹ ÑĞ¿Ğ»Ğ°Ñ‚Ñ‹. ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸, Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ² Ğ¸ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ Ğ²Ğ¸Ğ´Ğ°. PhysTwin Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸, Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğµ Ğ¸ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Realistic Digital Twins: Bridging the Physical and Virtual Worlds', 'desc': 'This paper introduces PhysTwin, a framework designed to create realistic digital twins of dynamic objects using sparse video data. It employs a physics-informed representation that integrates spring-mass models for physical simulation, generative shape models for geometry, and Gaussian splats for rendering. The framework utilizes a multi-stage optimization process to reconstruct object geometry, infer physical properties, and achieve realistic appearances from limited video inputs. PhysTwin excels in modeling various deformable objects and demonstrates superior performance in reconstruction and simulation tasks, making it valuable for robotics and interactive applications.'}, 'zh': {'title': 'ç‰©ç†æ•°å­—åŒèƒèƒï¼šçœŸå®ä¸è™šæ‹Ÿçš„å®Œç¾ç»“åˆ', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºPhysTwinçš„æ–°æ¡†æ¶ï¼Œæ—¨åœ¨åˆ›å»ºçœŸå®ç‰©ä½“çš„æ•°å­—åŒèƒèƒã€‚è¯¥æ¡†æ¶åˆ©ç”¨ç¨€ç–è§†é¢‘æ•æ‰åŠ¨æ€ç‰©ä½“çš„äº¤äº’ï¼Œç”Ÿæˆé€¼çœŸä¸”å¯å®æ—¶äº’åŠ¨çš„è™šæ‹Ÿå¤åˆ¶å“ã€‚PhysTwinç»“åˆäº†ç‰©ç†ä¿¡æ¯è¡¨ç¤ºå’Œå¤šé˜¶æ®µä¼˜åŒ–é€†å»ºæ¨¡æ¡†æ¶ï¼Œèƒ½å¤Ÿä»è§†é¢‘ä¸­é‡å»ºå®Œæ•´å‡ ä½•å½¢çŠ¶ã€æ¨æ–­ç‰©ç†å±æ€§å¹¶å¤åˆ¶çœŸå®å¤–è§‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPhysTwinåœ¨é‡å»ºã€æ¸²æŸ“å’Œæ¨¡æ‹Ÿæ–¹é¢ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.17361', 'title': 'Gumbel-Softmax Flow Matching with Straight-Through Guidance for\n  Controllable Biological Sequence Generation', 'url': 'https://huggingface.co/papers/2503.17361', 'abstract': 'Flow matching in the continuous simplex has emerged as a promising strategy for DNA sequence design, but struggles to scale to higher simplex dimensions required for peptide and protein generation. We introduce Gumbel-Softmax Flow and Score Matching, a generative framework on the simplex based on a novel Gumbel-Softmax interpolant with a time-dependent temperature. Using this interpolant, we introduce Gumbel-Softmax Flow Matching by deriving a parameterized velocity field that transports from smooth categorical distributions to distributions concentrated at a single vertex of the simplex. We alternatively present Gumbel-Softmax Score Matching which learns to regress the gradient of the probability density. Our framework enables high-quality, diverse generation and scales efficiently to higher-dimensional simplices. To enable training-free guidance, we propose Straight-Through Guided Flows (STGFlow), a classifier-based guidance method that leverages straight-through estimators to steer the unconditional velocity field toward optimal vertices of the simplex. STGFlow enables efficient inference-time guidance using classifiers pre-trained on clean sequences, and can be used with any discrete flow method. Together, these components form a robust framework for controllable de novo sequence generation. We demonstrate state-of-the-art performance in conditional DNA promoter design, sequence-only protein generation, and target-binding peptide design for rare disease treatment.', 'score': 3, 'issue_id': 2896, 'pub_date': '2025-03-21', 'pub_date_card': {'ru': '21 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 21', 'zh': '3æœˆ21æ—¥'}, 'hash': 'b5389a3e5ab241c3', 'authors': ['Sophia Tang', 'Yinuo Zhang', 'Alexander Tong', 'Pranam Chatterjee'], 'affiliations': ['Center of Computational Biology, Duke-NUS Medical School', 'Department of Biomedical Engineering, Duke University', 'Department of Biostatistics and Bioinformatics, Duke University', 'Department of Computer Science, Duke University', 'Management and Technology Program, University of Pennsylvania', 'Mila, Quebec AI Institute', 'UniversitÃ© de MontrÃ©al'], 'pdf_title_img': 'assets/pdf/title_img/2503.17361.jpg', 'data': {'categories': ['#diffusion', '#data', '#optimization', '#training', '#architecture', '#healthcare'], 'emoji': 'ğŸ§¬', 'ru': {'title': 'ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ½Ğ° ÑĞ¸Ğ¼Ğ¿Ğ»ĞµĞºÑĞµ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° ÑĞ¸Ğ¼Ğ¿Ğ»ĞµĞºÑĞµ, Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ Gumbel-Softmax Flow and Score Matching. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ğ¾Ğ»ÑĞ½Ñ‚ Gumbel-Softmax Ñ Ğ·Ğ°Ğ²Ğ¸ÑÑÑ‰ĞµĞ¹ Ğ¾Ñ‚ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ñ‚ĞµĞ¼Ğ¿ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ğ¾Ğ¹ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ ĞµĞ³Ğ¾ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ»Ñ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°Ñ‚ÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹, ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒÑÑÑŒ Ğ½Ğ° ÑĞ¸Ğ¼Ğ¿Ğ»ĞµĞºÑÑ‹ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Straight-Through Guided Flows Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Revolutionizing Sequence Generation with Gumbel-Softmax Flows', 'desc': 'This paper presents a new method called Gumbel-Softmax Flow and Score Matching for generating DNA sequences, peptides, and proteins. It introduces a Gumbel-Softmax interpolant that helps in transitioning between smooth categorical distributions and specific points in a simplex, which is a mathematical space used for these types of data. The authors also propose a technique called Straight-Through Guided Flows (STGFlow) that uses classifiers to guide the generation process towards optimal outcomes without needing extensive training. Overall, this framework allows for efficient and high-quality generation of biological sequences, achieving impressive results in various applications.'}, 'zh': {'title': 'é«˜æ•ˆç”Ÿæˆé«˜ç»´åºåˆ—çš„åˆ›æ–°æ¡†æ¶', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ç”Ÿæˆæ¡†æ¶ï¼Œç§°ä¸ºGumbel-Softmaxæµå’Œè¯„åˆ†åŒ¹é…ï¼Œæ—¨åœ¨è§£å†³DNAåºåˆ—è®¾è®¡ä¸­çš„é«˜ç»´ç®€å•å½¢é—®é¢˜ã€‚é€šè¿‡å¼•å…¥æ—¶é—´ä¾èµ–çš„Gumbel-Softmaxæ’å€¼ï¼Œæˆ‘ä»¬èƒ½å¤Ÿåœ¨ç®€å•å½¢ä¸Šå®ç°é«˜è´¨é‡å’Œå¤šæ ·åŒ–çš„ç”Ÿæˆã€‚è¯¥æ¡†æ¶è¿˜åŒ…æ‹¬ä¸€ç§åä¸ºSTGFlowçš„åˆ†ç±»å™¨å¼•å¯¼æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨æ¨ç†æ—¶æœ‰æ•ˆåœ°å¼•å¯¼ç”Ÿæˆè¿‡ç¨‹ã€‚æˆ‘ä»¬çš„ç ”ç©¶åœ¨æ¡ä»¶DNAå¯åŠ¨å­è®¾è®¡ã€åºåˆ—ç”Ÿæˆçš„è›‹ç™½è´¨å’Œé¶å‘ç»“åˆè‚½çš„è®¾è®¡ä¸­å±•ç¤ºäº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.17237', 'title': 'Strong Baseline: Multi-UAV Tracking via YOLOv12 with BoT-SORT-ReID', 'url': 'https://huggingface.co/papers/2503.17237', 'abstract': 'Detecting and tracking multiple unmanned aerial vehicles (UAVs) in thermal infrared video is inherently challenging due to low contrast, environmental noise, and small target sizes. This paper provides a straightforward approach to address multi-UAV tracking in thermal infrared video, leveraging recent advances in detection and tracking. Instead of relying on the YOLOv5 with the DeepSORT pipeline, we present a tracking framework built on YOLOv12 and BoT-SORT, enhanced with tailored training and inference strategies. We evaluate our approach following the metrics from the 4th Anti-UAV Challenge and demonstrate competitive performance. Notably, we achieve strong results without using contrast enhancement or temporal information fusion to enrich UAV features, highlighting our approach as a "Strong Baseline" for the multi-UAV tracking task. We provide implementation details, in-depth experimental analysis, and a discussion of potential improvements. The code is available at https://github.com/wish44165/YOLOv12-BoT-SORT-ReID .', 'score': 3, 'issue_id': 2900, 'pub_date': '2025-03-21', 'pub_date_card': {'ru': '21 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 21', 'zh': '3æœˆ21æ—¥'}, 'hash': '03b184a7ba5377e7', 'authors': ['Yu-Hsi Chen'], 'affiliations': ['The University of Melbourne, Parkville, Australia'], 'pdf_title_img': 'assets/pdf/title_img/2503.17237.jpg', 'data': {'categories': ['#video', '#cv', '#training', '#benchmark'], 'emoji': 'ğŸš', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ‘ĞŸĞ›Ğ Ğ½Ğ° Ñ‚ĞµĞ¿Ğ»Ğ¾Ğ²Ğ¸Ğ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¼ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ‘ĞŸĞ›Ğ Ğ½Ğ° Ğ¸Ğ½Ñ„Ñ€Ğ°ĞºÑ€Ğ°ÑĞ½Ğ¾Ğ¼ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ YOLOv12 Ğ¸ BoT-SORT. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ĞºÑƒ YOLOv5 Ğ¸ DeepSORT, Ğ½Ğµ Ğ¿Ñ€Ğ¸Ğ±ĞµĞ³Ğ°Ñ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ° Ğ¸Ğ»Ğ¸ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ» ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ñ… 4-Ğ³Ğ¾ Anti-UAV Challenge. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¸ Ğ¾Ğ±ÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Revolutionizing UAV Tracking with YOLOv12 and BoT-SORT', 'desc': 'This paper addresses the challenge of detecting and tracking multiple unmanned aerial vehicles (UAVs) in thermal infrared video, which is difficult due to low visibility and small sizes of the targets. The authors propose a new tracking framework that utilizes YOLOv12 and BoT-SORT, moving away from the traditional YOLOv5 and DeepSORT methods. Their approach includes specific training and inference strategies that enhance performance without relying on contrast enhancement or temporal information. The results show that their method performs competitively in the 4th Anti-UAV Challenge, establishing it as a strong baseline for future multi-UAV tracking research.'}, 'zh': {'title': 'çƒ­çº¢å¤–è§†é¢‘ä¸­çš„å¤šæ— äººæœºè·Ÿè¸ªæ–°æ–¹æ³•', 'desc': 'æœ¬è®ºæ–‡é’ˆå¯¹çƒ­çº¢å¤–è§†é¢‘ä¸­å¤šæ— äººæœºï¼ˆUAVï¼‰çš„æ£€æµ‹ä¸è·Ÿè¸ªé—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„æ–¹æ³•ã€‚æˆ‘ä»¬é‡‡ç”¨äº†YOLOv12å’ŒBoT-SORTæ„å»ºè·Ÿè¸ªæ¡†æ¶ï¼Œå¹¶é€šè¿‡å®šåˆ¶çš„è®­ç»ƒå’Œæ¨ç†ç­–ç•¥è¿›è¡Œå¢å¼ºã€‚æˆ‘ä»¬çš„è¯„ä¼°åŸºäºç¬¬å››å±Šåæ— äººæœºæŒ‘æˆ˜èµ›çš„æŒ‡æ ‡ï¼Œç»“æœæ˜¾ç¤ºå‡ºç«äº‰åŠ›çš„æ€§èƒ½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬åœ¨ä¸ä½¿ç”¨å¯¹æ¯”åº¦å¢å¼ºæˆ–æ—¶é—´ä¿¡æ¯èåˆçš„æƒ…å†µä¸‹ï¼Œä¾ç„¶å–å¾—äº†è‰¯å¥½çš„ç»“æœï¼Œæ ‡å¿—ç€æˆ‘ä»¬çš„æ–¹æ³•æ˜¯å¤šæ— äººæœºè·Ÿè¸ªä»»åŠ¡çš„â€œå¼ºåŸºçº¿â€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.16965', 'title': 'When Words Outperform Vision: VLMs Can Self-Improve Via Text-Only\n  Training For Human-Centered Decision Making', 'url': 'https://huggingface.co/papers/2503.16965', 'abstract': "Embodied decision-making is fundamental for AI agents operating in real-world environments. While Visual Language Models (VLMs) have advanced this capability, they still struggle with complex decisions, particularly in human-centered situations that require deep reasoning about human needs and values. In this study, we systematically evaluate open-sourced VLMs on multimodal human-centered decision-making tasks. We find that LLMs receiving only textual descriptions unexpectedly outperform their VLM counterparts of similar scale that process actual images, suggesting that visual alignment may hinder VLM abilities. To address this challenge, we propose a novel text-only training approach with synthesized textual data. This method strengthens VLMs' language components and transfers the learned abilities to multimodal inference, eliminating the need for expensive image-text paired data. Furthermore, we show that VLMs can achieve substantial performance gains through self-improvement, using training data generated by their LLM counterparts rather than relying on larger teacher models like GPT-4. Our findings establish a more efficient and scalable approach to enhancing VLMs' human-centered decision-making capabilities, opening new avenues for optimizing VLMs through self-improvement mechanisms.", 'score': 3, 'issue_id': 2896, 'pub_date': '2025-03-21', 'pub_date_card': {'ru': '21 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 21', 'zh': '3æœˆ21æ—¥'}, 'hash': 'ca2e599ff0665dfe', 'authors': ['Zhe Hu', 'Jing Li', 'Yu Yin'], 'affiliations': ['Department of Computer and Data Sciences, Case Western Reserve University', 'Department of Computing, The Hong Kong Polytechnic University', 'Research Centre for Data Science & Artificial Intelligence'], 'pdf_title_img': 'assets/pdf/title_img/2503.16965.jpg', 'data': {'categories': ['#optimization', '#transfer_learning', '#training', '#agents', '#multimodal'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ¡Ğ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ VLM Ñ‡ĞµÑ€ĞµĞ· Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ', 'desc': 'Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ¾ÑÑ€ĞµĞ´Ğ¾Ñ‚Ğ¾Ñ‡ĞµĞ½Ğ¾ Ğ½Ğ° Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (VLM) Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹, Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‰Ğ¸Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ VLM Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°, Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ VLM Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ ÑĞ²Ğ¾Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ñ‡ĞµÑ€ĞµĞ· ÑĞ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ñ… LLM-Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Enhancing VLMs through Text-Only Training and Self-Improvement', 'desc': 'This paper explores the challenges faced by Visual Language Models (VLMs) in making complex decisions that involve understanding human needs and values. The authors find that VLMs that rely on visual data often perform worse than those using only text, indicating that visual information may complicate decision-making. To improve VLM performance, they propose a new training method that uses synthesized textual data, enhancing the language understanding of VLMs without needing paired image-text data. Additionally, the study demonstrates that VLMs can improve their decision-making abilities by learning from their own generated data, rather than depending on larger models, making the training process more efficient and scalable.'}, 'zh': {'title': 'æå‡VLMäººç±»ä¸­å¿ƒå†³ç­–èƒ½åŠ›çš„æ–°æ–¹æ³•', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤æ‚äººç±»ä¸­å¿ƒå†³ç­–ä¸­çš„è¡¨ç°ã€‚æˆ‘ä»¬å‘ç°ï¼Œä»…ä½¿ç”¨æ–‡æœ¬æè¿°çš„è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æŸäº›ä»»åŠ¡ä¸Šæ„å¤–åœ°è¶…è¶Šäº†å¤„ç†å›¾åƒçš„VLMsï¼Œè¿™è¡¨æ˜è§†è§‰å¯¹é½å¯èƒ½ä¼šé™åˆ¶VLMçš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„ä»…åŸºäºæ–‡æœ¬çš„è®­ç»ƒæ–¹æ³•ï¼Œåˆ©ç”¨åˆæˆæ–‡æœ¬æ•°æ®æ¥å¢å¼ºVLMçš„è¯­è¨€èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œé€šè¿‡è‡ªæˆ‘æ”¹è¿›ï¼ŒVLMså¯ä»¥æ˜¾è‘—æå‡å…¶äººç±»ä¸­å¿ƒå†³ç­–èƒ½åŠ›ï¼Œå¼€è¾Ÿäº†ä¼˜åŒ–VLMçš„æ–°é€”å¾„ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.19207', 'title': 'FRESA:Feedforward Reconstruction of Personalized Skinned Avatars from\n  Few Images', 'url': 'https://huggingface.co/papers/2503.19207', 'abstract': 'We present a novel method for reconstructing personalized 3D human avatars with realistic animation from only a few images. Due to the large variations in body shapes, poses, and cloth types, existing methods mostly require hours of per-subject optimization during inference, which limits their practical applications. In contrast, we learn a universal prior from over a thousand clothed humans to achieve instant feedforward generation and zero-shot generalization. Specifically, instead of rigging the avatar with shared skinning weights, we jointly infer personalized avatar shape, skinning weights, and pose-dependent deformations, which effectively improves overall geometric fidelity and reduces deformation artifacts. Moreover, to normalize pose variations and resolve coupled ambiguity between canonical shapes and skinning weights, we design a 3D canonicalization process to produce pixel-aligned initial conditions, which helps to reconstruct fine-grained geometric details. We then propose a multi-frame feature aggregation to robustly reduce artifacts introduced in canonicalization and fuse a plausible avatar preserving person-specific identities. Finally, we train the model in an end-to-end framework on a large-scale capture dataset, which contains diverse human subjects paired with high-quality 3D scans. Extensive experiments show that our method generates more authentic reconstruction and animation than state-of-the-arts, and can be directly generalized to inputs from casually taken phone photos. Project page and code is available at https://github.com/rongakowang/FRESA.', 'score': 2, 'issue_id': 2907, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 24', 'zh': '3æœˆ24æ—¥'}, 'hash': '051b6e95d11816f7', 'authors': ['Rong Wang', 'Fabian Prada', 'Ziyan Wang', 'Zhongshi Jiang', 'Chengxiang Yin', 'Junxuan Li', 'Shunsuke Saito', 'Igor Santesteban', 'Javier Romero', 'Rohan Joshi', 'Hongdong Li', 'Jason Saragih', 'Yaser Sheikh'], 'affiliations': ['Australian National University', 'Meta Reality Labs Research'], 'pdf_title_img': 'assets/pdf/title_img/2503.19207.jpg', 'data': {'categories': ['#3d'], 'emoji': 'ğŸ§', 'ru': {'title': 'ĞœĞ³Ğ½Ğ¾Ğ²ĞµĞ½Ğ½Ğ¾Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… 3D-Ğ°Ğ²Ğ°Ñ‚Ğ°Ñ€Ğ¾Ğ² Ğ¿Ğ¾ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼ Ñ„Ğ¾Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸ÑĞ¼', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… 3D-Ğ°Ğ²Ğ°Ñ‚Ğ°Ñ€Ğ¾Ğ² Ğ»ÑĞ´ĞµĞ¹ Ñ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²ÑĞµĞ³Ğ¾ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±ÑƒÑ‡Ğ¸Ğ»Ğ¸ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° Ñ‚Ñ‹ÑÑÑ‡Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¾Ğ´ĞµÑ‚Ñ‹Ñ… Ğ»ÑĞ´ĞµĞ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ³Ğ½Ğ¾Ğ²ĞµĞ½Ğ½Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ°Ğ²Ğ°Ñ‚Ğ°Ñ€Ñ‹ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ„Ğ¾Ñ€Ğ¼Ñƒ Ğ°Ğ²Ğ°Ñ‚Ğ°Ñ€Ğ°, Ğ²ĞµÑĞ° ÑĞºĞ¸Ğ½Ğ½Ğ¸Ğ½Ğ³Ğ° Ğ¸ Ğ·Ğ°Ğ²Ğ¸ÑÑÑ‰Ğ¸Ğµ Ğ¾Ñ‚ Ğ¿Ğ¾Ğ·Ñ‹ Ğ´ĞµÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ 3D-ĞºĞ°Ğ½Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ°Ğ³Ñ€ĞµĞ³Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¼ĞµĞ»ĞºĞ¸Ğµ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ‚ÑŒ Ğ¸Ğ½Ğ´Ğ¸Ğ²Ğ¸Ğ´ÑƒĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°.'}, 'en': {'title': 'Instant 3D Avatars from Few Images!', 'desc': "This paper introduces a new technique for creating personalized 3D human avatars with realistic animations using just a few images. Unlike previous methods that require extensive optimization for each individual, this approach leverages a universal model learned from a large dataset of clothed humans, allowing for quick and efficient avatar generation. The method improves the accuracy of the avatar's shape and movement by jointly inferring skinning weights and pose-dependent deformations, which minimizes visual artifacts. Additionally, a 3D canonicalization process is implemented to align poses and enhance geometric details, resulting in high-quality reconstructions that can be generated from casual photos."}, 'zh': {'title': 'ä¸ªæ€§åŒ–3Då¤´åƒé‡å»ºçš„æ–°æ–¹æ³•', 'desc': 'æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œå¯ä»¥ä»…é€šè¿‡å‡ å¼ å›¾ç‰‡é‡å»ºä¸ªæ€§åŒ–çš„3Däººç±»å¤´åƒï¼Œå¹¶å®ç°é€¼çœŸçš„åŠ¨ç”»ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦åœ¨æ¨ç†è¿‡ç¨‹ä¸­å¯¹æ¯ä¸ªå¯¹è±¡è¿›è¡Œæ•°å°æ—¶çš„ä¼˜åŒ–ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬çš„å®é™…åº”ç”¨ã€‚æˆ‘ä»¬çš„æŠ€æœ¯é€šè¿‡å­¦ä¹ æ¥è‡ªä¸€åƒå¤šåç©¿è¡£äººç±»çš„é€šç”¨å…ˆéªŒï¼Œå®ç°äº†å³æ—¶å‰é¦ˆç”Ÿæˆå’Œé›¶æ ·æœ¬æ³›åŒ–ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ç§3Dæ ‡å‡†åŒ–è¿‡ç¨‹ï¼Œä»¥è§£å†³å§¿åŠ¿å˜åŒ–å’Œå½¢çŠ¶ä¹‹é—´çš„æ¨¡ç³Šæ€§ï¼Œä»è€Œæé«˜å‡ ä½•ç²¾åº¦å¹¶å‡å°‘å˜å½¢ä¼ªå½±ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.19123', 'title': 'Overcoming Vocabulary Mismatch: Vocabulary-agnostic Teacher Guided\n  Language Modeling', 'url': 'https://huggingface.co/papers/2503.19123', 'abstract': 'Using large teacher models to guide the training of smaller student models has become the prevailing paradigm for efficient and effective learning. However, vocabulary mismatches between teacher and student language models pose significant challenges in language modeling, resulting in divergent token sequences and output distributions. To overcome these limitations, we propose Vocabulary-agnostic Teacher Guided Language Modeling (VocAgnoLM), a novel approach that bridges the gap caused by vocabulary mismatch through two key methods: (1) Token-level Lexical Alignment, which aligns token sequences across mismatched vocabularies, and (2) Teacher Guided Loss, which leverages the loss of teacher model to guide effective student training. We demonstrate its effectiveness in language modeling with 1B student model using various 7B teacher models with different vocabularies. Notably, with Qwen2.5-Math-Instruct, a teacher model sharing only about 6% of its vocabulary with TinyLlama, VocAgnoLM achieves a 46% performance improvement compared to naive continual pretraining. Furthermore, we demonstrate that VocAgnoLM consistently benefits from stronger teacher models, providing a robust solution to vocabulary mismatches in language modeling.', 'score': 2, 'issue_id': 2907, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 24', 'zh': '3æœˆ24æ—¥'}, 'hash': 'b5efb0f50380f253', 'authors': ['Haebin Shin', 'Lei Ji', 'Xiao Liu', 'Yeyun Gong'], 'affiliations': ['KAIST AI', 'Microsoft Research'], 'pdf_title_img': 'assets/pdf/title_img/2503.19123.jpg', 'data': {'categories': ['#multilingual', '#optimization', '#transfer_learning', '#small_models', '#training'], 'emoji': 'ğŸ” ', 'ru': {'title': 'ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ±Ğ°Ñ€ÑŒĞµÑ€Ğ° Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ VocAgnoLM. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ ÑĞ»Ğ¾Ğ²Ğ°Ñ€ĞµĞ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ¹ Ğ¸ ÑƒÑ‡ĞµĞ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ»ĞµĞºÑĞ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. VocAgnoLM Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğ¼ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ ĞºĞ¾Ğ³Ğ´Ğ° ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ¸Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ°ÑÑ‚ÑÑ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ñ… ÑƒÑ‡ĞµĞ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Bridging Vocabulary Gaps for Better Language Learning', 'desc': "This paper introduces a new method called Vocabulary-agnostic Teacher Guided Language Modeling (VocAgnoLM) to improve the training of smaller language models using larger teacher models. It addresses the problem of vocabulary mismatches that can lead to different token sequences and output distributions between the teacher and student models. VocAgnoLM employs two main techniques: Token-level Lexical Alignment to synchronize token sequences and Teacher Guided Loss to utilize the teacher's loss for better student training. The results show that this approach significantly enhances performance, achieving a 46% improvement in language modeling tasks, especially when using stronger teacher models."}, 'zh': {'title': 'æ‰“ç ´è¯æ±‡å£å’ï¼Œæå‡è¯­è¨€å»ºæ¨¡æ•ˆç‡', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è¯­è¨€å»ºæ¨¡æ–¹æ³•ï¼Œç§°ä¸ºVocabulary-agnostic Teacher Guided Language Modelingï¼ˆVocAgnoLMï¼‰ï¼Œæ—¨åœ¨è§£å†³æ•™å¸ˆæ¨¡å‹ä¸å­¦ç”Ÿæ¨¡å‹ä¹‹é—´çš„è¯æ±‡ä¸åŒ¹é…é—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡ä¸¤ç§å…³é”®æŠ€æœ¯å®ç°ï¼šä¸€æ˜¯ä»¤ç‰Œçº§è¯æ±‡å¯¹é½ï¼ŒäºŒæ˜¯æ•™å¸ˆå¼•å¯¼æŸå¤±ï¼Œå¸®åŠ©å­¦ç”Ÿæ¨¡å‹æ›´æœ‰æ•ˆåœ°å­¦ä¹ ã€‚å®éªŒè¡¨æ˜ï¼ŒVocAgnoLMåœ¨ä½¿ç”¨ä¸åŒè¯æ±‡çš„æ•™å¸ˆæ¨¡å‹æ—¶ï¼Œèƒ½å¤Ÿæ˜¾è‘—æé«˜å­¦ç”Ÿæ¨¡å‹çš„æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨è¯æ±‡é‡å è¾ƒå°‘çš„æƒ…å†µä¸‹ã€‚è¯¥æ–¹æ³•ä¸ºè¯­è¨€å»ºæ¨¡ä¸­çš„è¯æ±‡ä¸åŒ¹é…é—®é¢˜æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.18783', 'title': 'Frequency Dynamic Convolution for Dense Image Prediction', 'url': 'https://huggingface.co/papers/2503.18783', 'abstract': '', 'score': 2, 'issue_id': 2897, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 25', 'zh': '3æœˆ25æ—¥'}, 'hash': '0be08263b46c092e', 'authors': ['Linwei Chen', 'Lin Gu', 'Liang Li', 'Chenggang Yan', 'Ying Fu'], 'affiliations': ['Beijing Institute of Technology', 'Chinese Academy of Sciences', 'Hangzhou Dianzi University', 'RIKEN', 'The University of Tokyo', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.18783.jpg', 'data': {'categories': [], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ Ğ²Ñ€ĞµĞ¼Ñ Ğ¸ Ñ€ĞµÑÑƒÑ€ÑÑ‹, Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¸Ñ… Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğº Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑĞ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ…. Ğ­Ñ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ LLM Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ°.'}, 'en': {'title': 'Hybrid Models: Bridging Spatial and Temporal Learning', 'desc': "This paper presents a novel approach to improve the performance of deep learning models by utilizing a hybrid architecture that combines convolutional neural networks (CNNs) with recurrent neural networks (RNNs). The proposed method enhances feature extraction from spatial data while also capturing temporal dependencies, making it suitable for tasks like video analysis and time-series prediction. The authors demonstrate that their model outperforms existing state-of-the-art techniques on several benchmark datasets. Additionally, they provide insights into the model's interpretability and robustness against adversarial attacks."}, 'zh': {'title': 'æå‡é¢„æµ‹å‡†ç¡®æ€§çš„åˆ›æ–°ç®—æ³•', 'desc': 'è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†ä¸€ç§æ–°çš„æœºå™¨å­¦ä¹ ç®—æ³•ï¼Œæ—¨åœ¨æé«˜æ¨¡å‹çš„é¢„æµ‹å‡†ç¡®æ€§ã€‚ä½œè€…æå‡ºäº†ä¸€ç§æ”¹è¿›çš„ç‰¹å¾é€‰æ‹©æ–¹æ³•ï¼Œå¯ä»¥æœ‰æ•ˆå‡å°‘æ•°æ®ç»´åº¦ï¼ŒåŒæ—¶ä¿ç•™é‡è¦ä¿¡æ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç®—æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚é€šè¿‡ä¼˜åŒ–æ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹ï¼Œç ”ç©¶è€…å¸Œæœ›æ¨åŠ¨æœºå™¨å­¦ä¹ åœ¨å®é™…åº”ç”¨ä¸­çš„æ•ˆæœã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.11849', 'title': 'Towards a Unified Copernicus Foundation Model for Earth Vision', 'url': 'https://huggingface.co/papers/2503.11849', 'abstract': "Advances in Earth observation (EO) foundation models have unlocked the potential of big satellite data to learn generic representations from space, benefiting a wide range of downstream applications crucial to our planet. However, most existing efforts remain limited to fixed spectral sensors, focus solely on the Earth's surface, and overlook valuable metadata beyond imagery. In this work, we take a step towards next-generation EO foundation models with three key components: 1) Copernicus-Pretrain, a massive-scale pretraining dataset that integrates 18.7M aligned images from all major Copernicus Sentinel missions, spanning from the Earth's surface to its atmosphere; 2) Copernicus-FM, a unified foundation model capable of processing any spectral or non-spectral sensor modality using extended dynamic hypernetworks and flexible metadata encoding; and 3) Copernicus-Bench, a systematic evaluation benchmark with 15 hierarchical downstream tasks ranging from preprocessing to specialized applications for each Sentinel mission. Our dataset, model, and benchmark greatly improve the scalability, versatility, and multimodal adaptability of EO foundation models, while also creating new opportunities to connect EO, weather, and climate research. Codes, datasets and models are available at https://github.com/zhu-xlab/Copernicus-FM.", 'score': 2, 'issue_id': 2904, 'pub_date': '2025-03-14', 'pub_date_card': {'ru': '14 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 14', 'zh': '3æœˆ14æ—¥'}, 'hash': '73a149c0c20956cf', 'authors': ['Yi Wang', 'Zhitong Xiong', 'Chenying Liu', 'Adam J. Stewart', 'Thomas Dujardin', 'Nikolaos Ioannis Bountos', 'Angelos Zavras', 'Franziska Gerken', 'Ioannis Papoutsis', 'Laura Leal-TaixÃ©', 'Xiao Xiang Zhu'], 'affiliations': ['Harokopio University of Athens', 'Munich Center for Machine Learning', 'NVIDIA', 'National Technical University of Athens & National Observatory of Athens', 'Technical University of Munich'], 'pdf_title_img': 'assets/pdf/title_img/2503.11849.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#cv', '#dataset'], 'emoji': 'ğŸ›°ï¸', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° ÑĞ¿ÑƒÑ‚Ğ½Ğ¸ĞºĞ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ñ Ğ—ĞµĞ¼Ğ»Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Copernicus-FM - ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½ÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑĞµĞ½ÑĞ¾Ñ€Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ³Ğ¸Ğ¿ĞµÑ€ÑĞµÑ‚ĞµĞ¹. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±Ñ‹Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ¼Ğ°ÑÑĞ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Copernicus-Pretrain, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ 18,7 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ ÑĞ¾ ÑĞ¿ÑƒÑ‚Ğ½Ğ¸ĞºĞ¾Ğ² Ğ¼Ğ¸ÑÑĞ¸Ğ¸ Copernicus Sentinel. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Copernicus-Bench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° 15 Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ….'}, 'en': {'title': 'Unlocking Earth Observation with Advanced Foundation Models', 'desc': "This paper presents advancements in Earth observation (EO) foundation models that utilize large satellite datasets to enhance learning from space imagery. The authors introduce Copernicus-Pretrain, a comprehensive dataset with 18.7 million aligned images from various Copernicus Sentinel missions, which includes data from both the Earth's surface and atmosphere. They also propose Copernicus-FM, a versatile foundation model that can handle different types of sensor data and incorporates metadata for improved analysis. Finally, the paper outlines Copernicus-Bench, a benchmark for evaluating the model's performance across 15 diverse tasks, thereby enhancing the scalability and adaptability of EO applications."}, 'zh': {'title': 'åœ°çƒè§‚æµ‹åŸºç¡€æ¨¡å‹çš„æœªæ¥ï¼šå¤šæ¨¡æ€ä¸å¯æ‰©å±•æ€§', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†åœ°çƒè§‚æµ‹åŸºç¡€æ¨¡å‹çš„è¿›å±•ï¼Œåˆ©ç”¨å¤§è§„æ¨¡å«æ˜Ÿæ•°æ®å­¦ä¹ é€šç”¨è¡¨ç¤ºï¼Œä¿ƒè¿›äº†å¤šç§é‡è¦åº”ç”¨çš„å‘å±•ã€‚æˆ‘ä»¬æå‡ºäº†ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šCopernicus-Pretrainï¼Œä¸€ä¸ªåŒ…å«1870ä¸‡å¯¹é½å›¾åƒçš„å¤§è§„æ¨¡é¢„è®­ç»ƒæ•°æ®é›†ï¼Œæ¶µç›–åœ°çƒè¡¨é¢åˆ°å¤§æ°”å±‚çš„æ‰€æœ‰ä¸»è¦Copernicus Sentinelä»»åŠ¡ï¼›Copernicus-FMï¼Œä¸€ä¸ªç»Ÿä¸€çš„åŸºç¡€æ¨¡å‹ï¼Œèƒ½å¤Ÿå¤„ç†ä»»ä½•å…‰è°±æˆ–éå…‰è°±ä¼ æ„Ÿå™¨çš„æ¨¡æ€ï¼Œå¹¶ä½¿ç”¨æ‰©å±•çš„åŠ¨æ€è¶…ç½‘ç»œå’Œçµæ´»çš„å…ƒæ•°æ®ç¼–ç ï¼›ä»¥åŠCopernicus-Benchï¼Œä¸€ä¸ªç³»ç»Ÿçš„è¯„ä¼°åŸºå‡†ï¼ŒåŒ…å«15ä¸ªå±‚æ¬¡çš„ä¸‹æ¸¸ä»»åŠ¡ï¼Œä»é¢„å¤„ç†åˆ°æ¯ä¸ªSentinelä»»åŠ¡çš„ä¸“ä¸šåº”ç”¨ã€‚æˆ‘ä»¬çš„å·¥ä½œæ˜¾è‘—æé«˜äº†åœ°çƒè§‚æµ‹åŸºç¡€æ¨¡å‹çš„å¯æ‰©å±•æ€§ã€å¤šåŠŸèƒ½æ€§å’Œå¤šæ¨¡æ€é€‚åº”æ€§ï¼ŒåŒæ—¶ä¸ºè¿æ¥åœ°çƒè§‚æµ‹ã€å¤©æ°”å’Œæ°”å€™ç ”ç©¶åˆ›é€ äº†æ–°çš„æœºä¼šã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.19777', 'title': 'LPOSS: Label Propagation Over Patches and Pixels for Open-vocabulary\n  Semantic Segmentation', 'url': 'https://huggingface.co/papers/2503.19777', 'abstract': 'We propose a training-free method for open-vocabulary semantic segmentation using Vision-and-Language Models (VLMs). Our approach enhances the initial per-patch predictions of VLMs through label propagation, which jointly optimizes predictions by incorporating patch-to-patch relationships. Since VLMs are primarily optimized for cross-modal alignment and not for intra-modal similarity, we use a Vision Model (VM) that is observed to better capture these relationships. We address resolution limitations inherent to patch-based encoders by applying label propagation at the pixel level as a refinement step, significantly improving segmentation accuracy near class boundaries. Our method, called LPOSS+, performs inference over the entire image, avoiding window-based processing and thereby capturing contextual interactions across the full image. LPOSS+ achieves state-of-the-art performance among training-free methods, across a diverse set of datasets. Code: https://github.com/vladan-stojnic/LPOSS', 'score': 1, 'issue_id': 2905, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 25', 'zh': '3æœˆ25æ—¥'}, 'hash': 'adee7b684f0c25f1', 'authors': ['Vladan StojniÄ‡', 'Yannis Kalantidis', 'JiÅ™Ã­ Matas', 'Giorgos Tolias'], 'affiliations': ['NAVER LABS Europe', 'VRG, FEE, Czech Technical University in Prague'], 'pdf_title_img': 'assets/pdf/title_img/2503.19777.jpg', 'data': {'categories': ['#open_source', '#inference', '#cv', '#training'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¼ĞµÑ‚Ğ¾Ğº', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ ÑĞ»Ğ¾Ğ²Ğ°Ñ€ĞµĞ¼ Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ° (VLM). ĞœĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ¾Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ VLM Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¼ĞµÑ‚Ğ¾Ğº, Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ğ°Ñ‚Ñ‡Ğ°Ğ¼Ğ¸. Ğ”Ğ»Ñ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ·Ñ€ĞµĞ½Ğ¸Ñ (VM). ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¼ĞµÑ‚Ğ¾Ğº Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµÑ‚ÑŒ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ, Ğ¿Ñ€Ğ¸ÑÑƒÑ‰Ğ¸Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ°Ğ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ°Ñ‚Ñ‡ĞµĞ¹.'}, 'en': {'title': 'Revolutionizing Semantic Segmentation with Training-Free Label Propagation', 'desc': 'This paper introduces a novel method for open-vocabulary semantic segmentation that does not require training. The method leverages Vision-and-Language Models (VLMs) and enhances their predictions by using label propagation to optimize relationships between image patches. To improve accuracy, especially near class boundaries, the authors apply label propagation at the pixel level, addressing the limitations of patch-based encoders. The proposed method, LPOSS+, outperforms existing training-free approaches and effectively captures contextual interactions across the entire image.'}, 'zh': {'title': 'æ— è®­ç»ƒçš„å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²æ–°æ–¹æ³•', 'desc': 'æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ— è®­ç»ƒçš„å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²æ–¹æ³•ï¼Œåˆ©ç”¨è§†è§‰ä¸è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ã€‚è¯¥æ–¹æ³•é€šè¿‡æ ‡ç­¾ä¼ æ’­å¢å¼ºäº†VLMsçš„åˆå§‹æ¯ä¸ªè¡¥ä¸é¢„æµ‹ï¼Œä¼˜åŒ–äº†è¡¥ä¸ä¹‹é—´çš„å…³ç³»ã€‚æˆ‘ä»¬ä½¿ç”¨è§†è§‰æ¨¡å‹ï¼ˆVMï¼‰æ¥æ›´å¥½åœ°æ•æ‰è¿™äº›å…³ç³»ï¼Œå¹¶åœ¨åƒç´ çº§åˆ«åº”ç”¨æ ‡ç­¾ä¼ æ’­ï¼Œä»¥è§£å†³åŸºäºè¡¥ä¸ç¼–ç å™¨çš„åˆ†è¾¨ç‡é™åˆ¶ï¼Œä»è€Œæ˜¾è‘—æé«˜äº†ç±»è¾¹ç•Œé™„è¿‘çš„åˆ†å‰²ç²¾åº¦ã€‚æˆ‘ä»¬çš„LPOSS+æ–¹æ³•åœ¨æ•´ä¸ªå›¾åƒä¸Šè¿›è¡Œæ¨ç†ï¼Œé¿å…äº†åŸºäºçª—å£çš„å¤„ç†ï¼Œèƒ½å¤Ÿæ•æ‰å…¨å›¾çš„ä¸Šä¸‹æ–‡äº¤äº’ï¼Œå¹¶åœ¨æ— è®­ç»ƒæ–¹æ³•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.19356', 'title': 'Can Vision-Language Models Answer Face to Face Questions in the\n  Real-World?', 'url': 'https://huggingface.co/papers/2503.19356', 'abstract': 'AI models have made significant strides in recent years in their ability to describe and answer questions about real-world images. They have also made progress in the ability to converse with users in real-time using audio input. This raises the question: have we reached the point where AI models, connected to a camera and microphone, can converse with users in real-time about scenes and events that are unfolding live in front of the camera? This has been a long-standing goal in AI and is a prerequisite for real-world AI assistants and humanoid robots to interact with humans in everyday situations. In this work, we introduce a new dataset and benchmark, the Qualcomm Interactive Video Dataset (IVD), which allows us to assess the extent to which existing models can support these abilities, and to what degree these capabilities can be instilled through fine-tuning. The dataset is based on a simple question-answering setup, where users ask questions that the system has to answer, in real-time, based on the camera and audio input. We show that existing models fall far behind human performance on this task, and we identify the main sources for the performance gap. However, we also show that for many of the required perceptual skills, fine-tuning on this form of data can significantly reduce this gap.', 'score': 1, 'issue_id': 2909, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 25', 'zh': '3æœˆ25æ—¥'}, 'hash': '139d332c36986584', 'authors': ['Reza Pourreza', 'Rishit Dagli', 'Apratim Bhattacharyya', 'Sunny Panchal', 'Guillaume Berger', 'Roland Memisevic'], 'affiliations': ['Qualcomm AI Research', 'University of Toronto'], 'pdf_title_img': 'assets/pdf/title_img/2503.19356.jpg', 'data': {'categories': ['#benchmark', '#agi', '#multimodal', '#optimization', '#cv', '#dataset', '#training', '#audio', '#interpretability'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞĞ° Ğ¿ÑƒÑ‚Ğ¸ Ğº Ğ˜Ğ˜, Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°ÑÑ‰ĞµĞ¼Ñƒ Ğ¼Ğ¸Ñ€ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚ - Qualcomm Interactive Video Dataset (IVD), Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° (Ğ˜Ğ˜) Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼Ğ¸ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾- Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ²Ñ…Ğ¾Ğ´Ğ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ñ‚ÑÑ‚Ğ°ÑÑ‚ Ğ¾Ñ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ»ÑÑÑ‚ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ñ‹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ²Ğ° Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ˜Ğ˜ Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¼. Ğ¢ĞµĞ¼ Ğ½Ğµ Ğ¼ĞµĞ½ĞµĞµ, Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ ÑÑ‚Ğ¾Ñ‚ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¸Ñ… Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ñ… Ğ¿ĞµÑ€Ñ†ĞµĞ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ².'}, 'en': {'title': 'Bridging the Gap: Real-Time AI Conversations with Live Input', 'desc': 'This paper discusses advancements in AI models that can describe and answer questions about real-world images and engage in real-time conversations using audio. The authors introduce the Qualcomm Interactive Video Dataset (IVD), which serves as a benchmark to evaluate how well current models can interact with users based on live camera and audio input. The study reveals that while existing models do not yet match human performance in this interactive task, fine-tuning these models on the new dataset can improve their perceptual skills significantly. Ultimately, this work highlights the potential for developing AI assistants and humanoid robots that can effectively communicate in everyday situations.'}, 'zh': {'title': 'è¿ˆå‘å®æ—¶å¯¹è¯çš„äººå·¥æ™ºèƒ½æ–°é˜¶æ®µ', 'desc': 'è¿‘å¹´æ¥ï¼Œäººå·¥æ™ºèƒ½æ¨¡å‹åœ¨æè¿°å’Œå›ç­”ç°å®ä¸–ç•Œå›¾åƒæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚å®ƒä»¬åœ¨å®æ—¶ä¸ç”¨æˆ·è¿›è¡ŒéŸ³é¢‘å¯¹è¯çš„èƒ½åŠ›ä¸Šä¹Ÿæœ‰æ‰€æå‡ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†å’ŒåŸºå‡†ï¼Œåä¸ºé«˜é€šäº’åŠ¨è§†é¢‘æ•°æ®é›†ï¼ˆIVDï¼‰ï¼Œç”¨äºè¯„ä¼°ç°æœ‰æ¨¡å‹åœ¨å®æ—¶åœºæ™¯å’Œäº‹ä»¶å¯¹è¯ä¸­çš„èƒ½åŠ›ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå°½ç®¡ç°æœ‰æ¨¡å‹åœ¨æ­¤ä»»åŠ¡ä¸Šçš„è¡¨ç°è¿œä½äºäººç±»ï¼Œä½†é€šè¿‡å¾®è°ƒå¯ä»¥æ˜¾è‘—ç¼©å°è¿™ä¸€å·®è·ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.19355', 'title': 'ST-VLM: Kinematic Instruction Tuning for Spatio-Temporal Reasoning in\n  Vision-Language Models', 'url': 'https://huggingface.co/papers/2503.19355', 'abstract': 'Spatio-temporal reasoning is essential in understanding real-world environments in various fields, eg, autonomous driving and sports analytics. Recent advances have improved the spatial reasoning ability of Vision-Language Models (VLMs) by introducing large-scale data, but these models still struggle to analyze kinematic elements like traveled distance and speed of moving objects. To bridge this gap, we construct a spatio-temporal reasoning dataset and benchmark involving kinematic instruction tuning, referred to as STKit and STKit-Bench. They consist of real-world videos with 3D annotations, detailing object motion dynamics: traveled distance, speed, movement direction, inter-object distance comparisons, and relative movement direction. To further scale such data construction to videos without 3D labels, we propose an automatic pipeline to generate pseudo-labels using 4D reconstruction in real-world scale. With our kinematic instruction tuning data for spatio-temporal reasoning, we present ST-VLM, a VLM enhanced for spatio-temporal reasoning, which exhibits outstanding performance on STKit-Bench. Furthermore, we show that ST-VLM generalizes robustly across diverse domains and tasks, outperforming baselines on other spatio-temporal benchmarks (eg, ActivityNet, TVQA+). Finally, by integrating learned spatio-temporal reasoning with existing abilities, ST-VLM enables complex multi-step reasoning. Project page: https://ikodoh.github.io/ST-VLM.', 'score': 1, 'issue_id': 2915, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 25', 'zh': '3æœˆ25æ—¥'}, 'hash': 'de2ed078954ba830', 'authors': ['Dohwan Ko', 'Sihyeon Kim', 'Yumin Suh', 'Vijay Kumar B. G', 'Minseo Yoon', 'Manmohan Chandraker', 'Hyunwoo J. Kim'], 'affiliations': ['KAIST', 'Korea University', 'NEC Labs America'], 'pdf_title_img': 'assets/pdf/title_img/2503.19355.jpg', 'data': {'categories': ['#benchmark', '#cv', '#multimodal', '#dataset', '#transfer_learning', '#reasoning'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº STKit Ğ¸ STKit-Bench Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ST-VLM - ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ½Ğ° ĞºĞ¸Ğ½ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑÑ… Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾. ST-VLM Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° STKit-Bench Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ñ Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸.'}, 'en': {'title': 'Enhancing Vision-Language Models with Spatio-Temporal Reasoning', 'desc': "This paper addresses the challenge of spatio-temporal reasoning in Vision-Language Models (VLMs), particularly in understanding the movement of objects in real-world scenarios. The authors introduce a new dataset, STKit, which includes videos with detailed 3D annotations of kinematic elements such as distance traveled and speed. They also propose an automatic pipeline for generating pseudo-labels from videos lacking 3D data, enhancing the dataset's scalability. The resulting model, ST-VLM, demonstrates superior performance in spatio-temporal reasoning tasks and shows strong generalization across various benchmarks."}, 'zh': {'title': 'æå‡æ—¶ç©ºæ¨ç†èƒ½åŠ›çš„è§†è§‰è¯­è¨€æ¨¡å‹', 'desc': 'æ—¶ç©ºæ¨ç†åœ¨ç†è§£ç°å®ä¸–ç•Œç¯å¢ƒä¸­è‡³å…³é‡è¦ï¼Œå°¤å…¶æ˜¯åœ¨è‡ªåŠ¨é©¾é©¶å’Œä½“è‚²åˆ†æç­‰é¢†åŸŸã€‚å°½ç®¡è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨ç©ºé—´æ¨ç†èƒ½åŠ›ä¸Šå–å¾—äº†è¿›å±•ï¼Œä½†åœ¨åˆ†æè¿åŠ¨ç‰©ä½“çš„è¿åŠ¨å­¦å…ƒç´ ï¼ˆå¦‚è¡Œé©¶è·ç¦»å’Œé€Ÿåº¦ï¼‰æ–¹é¢ä»ç„¶å­˜åœ¨å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªæ—¶ç©ºæ¨ç†æ•°æ®é›†å’ŒåŸºå‡†ï¼Œç§°ä¸ºSTKitå’ŒSTKit-Benchï¼ŒåŒ…å«å¸¦æœ‰3Dæ³¨é‡Šçš„çœŸå®è§†é¢‘ï¼Œè¯¦ç»†æè¿°äº†ç‰©ä½“è¿åŠ¨åŠ¨æ€ã€‚é€šè¿‡å¼•å…¥è‡ªåŠ¨åŒ–ç®¡é“ç”Ÿæˆä¼ªæ ‡ç­¾ï¼Œæˆ‘ä»¬æå‡ºçš„ST-VLMæ¨¡å‹åœ¨æ—¶ç©ºæ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†å…¶ä»–æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.18712', 'title': 'LLaVAction: evaluating and training multi-modal large language models\n  for action recognition', 'url': 'https://huggingface.co/papers/2503.18712', 'abstract': "Understanding human behavior requires measuring behavioral actions. Due to its complexity, behavior is best mapped onto a rich, semantic structure such as language. The recent development of multi-modal large language models (MLLMs) is a promising candidate for a wide range of action understanding tasks. In this work, we focus on evaluating and then improving MLLMs to perform action recognition. We reformulate EPIC-KITCHENS-100, one of the largest and most challenging egocentric action datasets, to the form of video multiple question answering (EPIC-KITCHENS-100-MQA). We show that when we sample difficult incorrect answers as distractors, leading MLLMs struggle to recognize the correct actions. We propose a series of methods that greatly improve the MLLMs' ability to perform action recognition, achieving state-of-the-art on both the EPIC-KITCHENS-100 validation set, as well as outperforming GPT-4o by 21 points in accuracy on EPIC-KITCHENS-100-MQA. Lastly, we show improvements on other action-related video benchmarks such as EgoSchema, PerceptionTest, LongVideoBench, VideoMME and MVBench, suggesting that MLLMs are a promising path forward for complex action tasks. Code and models are available at: https://github.com/AdaptiveMotorControlLab/LLaVAction.", 'score': 1, 'issue_id': 2916, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 24', 'zh': '3æœˆ24æ—¥'}, 'hash': '05cb55b99d04c44e', 'authors': ['Shaokai Ye', 'Haozhe Qi', 'Alexander Mathis', 'Mackenzie W. Mathis'], 'affiliations': ['EPFL'], 'pdf_title_img': 'assets/pdf/title_img/2503.18712.jpg', 'data': {'categories': ['#dataset', '#multimodal', '#optimization', '#training', '#video', '#benchmark', '#games'], 'emoji': 'ğŸ¬', 'ru': {'title': 'MLLM: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğ´Ğ»Ñ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ EPIC-KITCHENS-100 Ğ² Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ², Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¾Ñ†ĞµĞ½Ğ¸Ñ‚ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ MLLM Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ. ĞĞ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ñ€ÑĞ´ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‰Ğ¸Ñ… ÑÑ‚Ñƒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ½ÑƒĞ² Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ EPIC-KITCHENS-100. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑĞ¼Ğ¸ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾, Ñ‡Ñ‚Ğ¾ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ° Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ MLLM Ğ´Ğ»Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹.'}, 'en': {'title': 'Unlocking Action Recognition with Multi-Modal Language Models', 'desc': 'This paper explores the use of multi-modal large language models (MLLMs) for understanding human actions through video data. The authors reformulate the EPIC-KITCHENS-100 dataset into a video multiple question answering format to enhance action recognition capabilities. They identify that MLLMs struggle with difficult distractor answers, which impacts their performance. By implementing various improvement methods, the authors achieve state-of-the-art results in action recognition tasks, demonstrating the potential of MLLMs for complex behavioral understanding.'}, 'zh': {'title': 'å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹åŠ©åŠ›åŠ¨ä½œè¯†åˆ«', 'desc': 'ç†è§£äººç±»è¡Œä¸ºéœ€è¦æµ‹é‡è¡Œä¸ºåŠ¨ä½œã€‚ç”±äºè¡Œä¸ºçš„å¤æ‚æ€§ï¼Œæœ€å¥½å°†å…¶æ˜ å°„åˆ°ä¸°å¯Œçš„è¯­ä¹‰ç»“æ„ä¸Šï¼Œä¾‹å¦‚è¯­è¨€ã€‚æœ¬æ–‡ä¸“æ³¨äºè¯„ä¼°å’Œæ”¹è¿›å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨åŠ¨ä½œè¯†åˆ«æ–¹é¢çš„è¡¨ç°ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç³»åˆ—æ–¹æ³•ï¼Œæ˜¾è‘—æé«˜äº†MLLMsçš„åŠ¨ä½œè¯†åˆ«èƒ½åŠ›ï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.17982', 'title': 'Co-SemDepth: Fast Joint Semantic Segmentation and Depth Estimation on\n  Aerial Images', 'url': 'https://huggingface.co/papers/2503.17982', 'abstract': 'Understanding the geometric and semantic properties of the scene is crucial in autonomous navigation and particularly challenging in the case of Unmanned Aerial Vehicle (UAV) navigation. Such information may be by obtained by estimating depth and semantic segmentation maps of the surrounding environment and for their practical use in autonomous navigation, the procedure must be performed as close to real-time as possible. In this paper, we leverage monocular cameras on aerial robots to predict depth and semantic maps in low-altitude unstructured environments. We propose a joint deep-learning architecture that can perform the two tasks accurately and rapidly, and validate its effectiveness on MidAir and Aeroscapes benchmark datasets. Our joint-architecture proves to be competitive or superior to the other single and joint architecture methods while performing its task fast predicting 20.2 FPS on a single NVIDIA quadro p5000 GPU and it has a low memory footprint. All codes for training and prediction can be found on this link: https://github.com/Malga-Vision/Co-SemDepth', 'score': 0, 'issue_id': 2911, 'pub_date': '2025-03-23', 'pub_date_card': {'ru': '23 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 23', 'zh': '3æœˆ23æ—¥'}, 'hash': '27d4e625a8566ad0', 'authors': ['Yara AlaaEldin', 'Francesca Odone'], 'affiliations': ['University of Genova Genova, Italy'], 'pdf_title_img': 'assets/pdf/title_img/2503.17982.jpg', 'data': {'categories': ['#open_source', '#optimization', '#agents', '#benchmark', '#architecture', '#cv'], 'emoji': 'ğŸš', 'ru': {'title': 'Ğ‘Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¹ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· ÑÑ†ĞµĞ½Ñ‹ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ¹ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸ Ğ‘ĞŸĞ›Ğ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ±ĞµÑĞ¿Ğ¸Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ğ»ĞµÑ‚Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ¿Ğ¿Ğ°Ñ€Ğ°Ñ‚Ğ¾Ğ². ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ½Ğ¾ĞºÑƒĞ»ÑÑ€Ğ½Ñ‹Ğµ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¾ĞºÑ€ÑƒĞ¶Ğ°ÑÑ‰ĞµĞ¹ ÑÑ€ĞµĞ´Ñ‹ Ğ½Ğ° Ğ¼Ğ°Ğ»Ñ‹Ñ… Ğ²Ñ‹ÑĞ¾Ñ‚Ğ°Ñ…. ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğµ Ğ¸Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‰Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… MidAir Ğ¸ Aeroscapes. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾ (20.2 FPS Ğ½Ğ° NVIDIA Quadro P5000) Ğ¸ Ğ¸Ğ¼ĞµĞµÑ‚ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ¾Ğ±ÑŠĞµĞ¼ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸.'}, 'en': {'title': 'Real-Time Depth and Semantic Mapping for UAVs', 'desc': 'This paper addresses the challenges of understanding the environment for Unmanned Aerial Vehicle (UAV) navigation by estimating depth and semantic segmentation maps. The authors propose a joint deep-learning architecture that efficiently predicts these maps using monocular cameras in real-time. Their model achieves a high performance of 20.2 frames per second on a single NVIDIA Quadro P5000 GPU while maintaining a low memory footprint. The effectiveness of the proposed architecture is validated against benchmark datasets, showing competitive or superior results compared to existing methods.'}, 'zh': {'title': 'æ— äººæœºå¯¼èˆªä¸­çš„æ·±åº¦ä¸è¯­ä¹‰åˆ†å‰²è”åˆå­¦ä¹ ', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†æ— äººæœºå¯¼èˆªä¸­åœºæ™¯çš„å‡ ä½•å’Œè¯­ä¹‰ç‰¹æ€§ã€‚æˆ‘ä»¬åˆ©ç”¨å•ç›®ç›¸æœºåœ¨ä½ç©ºéç»“æ„åŒ–ç¯å¢ƒä¸­é¢„æµ‹æ·±åº¦å’Œè¯­ä¹‰åˆ†å‰²å›¾ã€‚æå‡ºäº†ä¸€ç§è”åˆæ·±åº¦å­¦ä¹ æ¶æ„ï¼Œèƒ½å¤Ÿå¿«é€Ÿä¸”å‡†ç¡®åœ°å®Œæˆè¿™ä¸¤ä¸ªä»»åŠ¡ï¼Œå¹¶åœ¨MidAirå’ŒAeroscapesåŸºå‡†æ•°æ®é›†ä¸ŠéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚è¯¥æ¶æ„åœ¨æ€§èƒ½ä¸Šä¼˜äºå…¶ä»–å•ä¸€å’Œè”åˆæ¶æ„æ–¹æ³•ï¼Œèƒ½å¤Ÿä»¥20.2å¸§æ¯ç§’çš„é€Ÿåº¦è¿›è¡Œé¢„æµ‹ï¼ŒåŒæ—¶å†…å­˜å ç”¨è¾ƒä½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.16776', 'title': 'OpenCity3D: What do Vision-Language Models know about Urban\n  Environments?', 'url': 'https://huggingface.co/papers/2503.16776', 'abstract': "Vision-language models (VLMs) show great promise for 3D scene understanding but are mainly applied to indoor spaces or autonomous driving, focusing on low-level tasks like segmentation. This work expands their use to urban-scale environments by leveraging 3D reconstructions from multi-view aerial imagery. We propose OpenCity3D, an approach that addresses high-level tasks, such as population density estimation, building age classification, property price prediction, crime rate assessment, and noise pollution evaluation. Our findings highlight OpenCity3D's impressive zero-shot and few-shot capabilities, showcasing adaptability to new contexts. This research establishes a new paradigm for language-driven urban analytics, enabling applications in planning, policy, and environmental monitoring. See our project page: opencity3d.github.io", 'score': 0, 'issue_id': 2916, 'pub_date': '2025-03-21', 'pub_date_card': {'ru': '21 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 21', 'zh': '3æœˆ21æ—¥'}, 'hash': 'c5625062d93c0379', 'authors': ['Valentin Bieri', 'Marco Zamboni', 'Nicolas S. Blumer', 'Qingxuan Chen', 'Francis Engelmann'], 'affiliations': ['ETH Zurich', 'Stanford University', 'University of Zurich'], 'pdf_title_img': 'assets/pdf/title_img/2503.16776.jpg', 'data': {'categories': ['#science', '#transfer_learning', '#multimodal', '#3d'], 'emoji': 'ğŸ™ï¸', 'ru': {'title': 'OpenCity3D: ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° ÑÑ‚Ñ€Ğ°Ğ¶Ğµ ÑƒĞ¼Ğ½Ñ‹Ñ… Ğ³Ğ¾Ñ€Ğ¾Ğ´Ğ¾Ğ²', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ OpenCity3D - Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (VLM) Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ³Ğ¾Ñ€Ğ¾Ğ´ÑĞºĞ¾Ğ¹ ÑÑ€ĞµĞ´Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ 3D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ°ÑÑ€Ğ¾Ñ„Ğ¾Ñ‚Ğ¾ÑĞ½Ğ¸Ğ¼ĞºĞ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, ĞºĞ°Ğº VLM Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² ÑƒÑ€Ğ±Ğ°Ğ½Ğ¸ÑÑ‚Ğ¸ĞºĞµ, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ°ÑĞµĞ»ĞµĞ½Ğ¸Ñ, ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ²Ğ¾Ğ·Ñ€Ğ°ÑÑ‚Ğ° Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ†ĞµĞ½ Ğ½Ğ° Ğ½ĞµĞ´Ğ²Ğ¸Ğ¶Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ²Ğ¿ĞµÑ‡Ğ°Ñ‚Ğ»ÑÑÑ‰Ğ¸Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ OpenCity3D Ğ² Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ°Ñ… zero-shot Ğ¸ few-shot Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ³Ğ¾Ñ€Ğ¾Ğ´ÑĞºĞ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ñ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ² Ğ³Ñ€Ğ°Ğ´Ğ¾ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğµ Ğ¸ ÑĞºĞ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³Ğµ.'}, 'en': {'title': 'Revolutionizing Urban Analytics with OpenCity3D', 'desc': 'This paper introduces OpenCity3D, a vision-language model designed for understanding urban environments using 3D reconstructions from aerial images. Unlike previous models that focused on low-level tasks, OpenCity3D tackles high-level urban analytics such as estimating population density and predicting property prices. The model demonstrates strong zero-shot and few-shot learning abilities, allowing it to adapt to new urban contexts without extensive retraining. This research paves the way for innovative applications in urban planning, policy-making, and environmental monitoring.'}, 'zh': {'title': 'OpenCity3Dï¼šåŸå¸‚åˆ†æçš„æ–°è§†è§’', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„è§†è§‰è¯­è¨€æ¨¡å‹OpenCity3Dï¼Œæ—¨åœ¨æå‡å¯¹åŸå¸‚è§„æ¨¡ç¯å¢ƒçš„ç†è§£ã€‚è¯¥æ¨¡å‹åˆ©ç”¨å¤šè§†è§’èˆªæ‹å›¾åƒçš„3Dé‡å»ºï¼Œèƒ½å¤Ÿå¤„ç†é«˜å±‚æ¬¡ä»»åŠ¡ï¼Œå¦‚äººå£å¯†åº¦ä¼°è®¡ã€å»ºç­‘å¹´é¾„åˆ†ç±»å’Œç‰©ä¸šä»·æ ¼é¢„æµ‹ç­‰ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒOpenCity3Dåœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬å­¦ä¹ æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿé€‚åº”æ–°çš„åº”ç”¨åœºæ™¯ã€‚æ­¤ç ”ç©¶ä¸ºåŸºäºè¯­è¨€çš„åŸå¸‚åˆ†æå¼€è¾Ÿäº†æ–°æ–¹å‘ï¼Œå…·æœ‰å¹¿æ³›çš„è§„åˆ’å’Œç¯å¢ƒç›‘æµ‹åº”ç”¨æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.15667', 'title': 'DiffPortrait360: Consistent Portrait Diffusion for 360 View Synthesis', 'url': 'https://huggingface.co/papers/2503.15667', 'abstract': 'Generating high-quality 360-degree views of human heads from single-view images is essential for enabling accessible immersive telepresence applications and scalable personalized content creation. While cutting-edge methods for full head generation are limited to modeling realistic human heads, the latest diffusion-based approaches for style-omniscient head synthesis can produce only frontal views and struggle with view consistency, preventing their conversion into true 3D models for rendering from arbitrary angles. We introduce a novel approach that generates fully consistent 360-degree head views, accommodating human, stylized, and anthropomorphic forms, including accessories like glasses and hats. Our method builds on the DiffPortrait3D framework, incorporating a custom ControlNet for back-of-head detail generation and a dual appearance module to ensure global front-back consistency. By training on continuous view sequences and integrating a back reference image, our approach achieves robust, locally continuous view synthesis. Our model can be used to produce high-quality neural radiance fields (NeRFs) for real-time, free-viewpoint rendering, outperforming state-of-the-art methods in object synthesis and 360-degree head generation for very challenging input portraits.', 'score': 0, 'issue_id': 2916, 'pub_date': '2025-03-19', 'pub_date_card': {'ru': '19 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 19', 'zh': '3æœˆ19æ—¥'}, 'hash': '0f10483eac04cdbf', 'authors': ['Yuming Gu', 'Phong Tran', 'Yujian Zheng', 'Hongyi Xu', 'Heyuan Li', 'Adilbek Karmanov', 'Hao Li'], 'affiliations': ['ByteDance Inc.', 'MBZUAI', 'Pinscreen Inc.', 'The Chinese University of Hong Kong, Shenzhen', 'University of Southern California'], 'pdf_title_img': 'assets/pdf/title_img/2503.15667.jpg', 'data': {'categories': ['#diffusion', '#cv', '#3d'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² 3D-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ³Ğ¾Ğ»Ğ¾Ğ²: Ğ¾Ñ‚ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ĞºÑƒÑ€ÑĞ° Ğº Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ±Ğ·Ğ¾Ñ€Ñƒ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… 360-Ğ³Ñ€Ğ°Ğ´ÑƒÑĞ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ³Ğ¾Ğ»Ğ¾Ğ², Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğµ, ÑÑ‚Ğ¸Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸ Ğ°Ğ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¾Ğ¼Ğ¾Ñ€Ñ„Ğ½Ñ‹Ğµ Ñ„Ğ¾Ñ€Ğ¼Ñ‹. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞµ DiffPortrait3D Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ControlNet Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹ Ğ·Ğ°Ğ´Ğ½ĞµĞ¹ Ñ‡Ğ°ÑÑ‚Ğ¸ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ñ‹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑÑ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ² Ğ¸ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ğ¾Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ´Ğ½ĞµĞ¹ Ñ‡Ğ°ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 360-Ğ³Ñ€Ğ°Ğ´ÑƒÑĞ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ³Ğ¾Ğ»Ğ¾Ğ² Ğ´Ğ»Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ñ€Ñ‚Ñ€ĞµÑ‚Ğ¾Ğ².'}, 'en': {'title': 'Revolutionizing 360-Degree Head Generation with Consistency and Detail', 'desc': 'This paper presents a new method for generating 360-degree views of human heads from single images, which is important for creating immersive experiences and personalized content. Unlike previous techniques that only produce frontal views and lack consistency, this approach ensures that the generated heads look realistic from all angles. It utilizes a framework called DiffPortrait3D, enhanced with a ControlNet for adding details to the back of the head and a dual appearance module for maintaining consistency between the front and back views. The model is trained on sequences of views and can create high-quality neural radiance fields (NeRFs), enabling real-time rendering from any viewpoint and surpassing existing methods in head generation.'}, 'zh': {'title': '360åº¦äººå¤´è§†å›¾ç”Ÿæˆçš„æ–°çªç ´', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œå¯ä»¥ä»å•è§†è§’å›¾åƒç”Ÿæˆé«˜è´¨é‡çš„360åº¦äººå¤´è§†å›¾ã€‚è¿™ç§æ–¹æ³•è§£å†³äº†ç°æœ‰æŠ€æœ¯åœ¨ç”ŸæˆçœŸå®äººå¤´æ—¶åªèƒ½æä¾›æ­£é¢è§†å›¾çš„é—®é¢˜ï¼Œå¹¶ä¸”ç¡®ä¿äº†è§†å›¾çš„ä¸€è‡´æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŸºäºDiffPortrait3Dæ¡†æ¶ï¼Œç»“åˆäº†è‡ªå®šä¹‰çš„ControlNetå’ŒåŒé‡å¤–è§‚æ¨¡å—ï¼Œä»¥ç”Ÿæˆåè„‘å‹ºçš„ç»†èŠ‚å¹¶ä¿æŒå‰åè§†å›¾çš„ä¸€è‡´æ€§ã€‚é€šè¿‡è®­ç»ƒè¿ç»­è§†è§’åºåˆ—å¹¶æ•´åˆåå‚è€ƒå›¾åƒï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¯¹è±¡åˆæˆå’Œ360åº¦äººå¤´ç”Ÿæˆæ–¹é¢è¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æŠ€æœ¯ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.09566', 'title': 'TPDiff: Temporal Pyramid Video Diffusion Model', 'url': 'https://huggingface.co/papers/2503.09566', 'abstract': 'The development of video diffusion models unveils a significant challenge: the substantial computational demands. To mitigate this challenge, we note that the reverse process of diffusion exhibits an inherent entropy-reducing nature. Given the inter-frame redundancy in video modality, maintaining full frame rates in high-entropy stages is unnecessary. Based on this insight, we propose TPDiff, a unified framework to enhance training and inference efficiency. By dividing diffusion into several stages, our framework progressively increases frame rate along the diffusion process with only the last stage operating on full frame rate, thereby optimizing computational efficiency. To train the multi-stage diffusion model, we introduce a dedicated training framework: stage-wise diffusion. By solving the partitioned probability flow ordinary differential equations (ODE) of diffusion under aligned data and noise, our training strategy is applicable to various diffusion forms and further enhances training efficiency. Comprehensive experimental evaluations validate the generality of our method, demonstrating 50% reduction in training cost and 1.5x improvement in inference efficiency.', 'score': 32, 'issue_id': 2681, 'pub_date': '2025-03-12', 'pub_date_card': {'ru': '12 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 12', 'zh': '3æœˆ12æ—¥'}, 'hash': '6952e94de20936ce', 'authors': ['Lingmin Ran', 'Mike Zheng Shou'], 'affiliations': ['Show Lab, National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2503.09566.jpg', 'data': {'categories': ['#inference', '#video', '#diffusion', '#training', '#optimization'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾ÑÑ‚ÑŒ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ TPDiff - ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ€Ğ°Ğ·Ğ´ĞµĞ»Ğ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ÑÑ‚Ğ°Ğ¿Ğ¾Ğ², Ğ¿Ğ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°Ñ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ñƒ ĞºĞ°Ğ´Ñ€Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµÑÑƒÑ€ÑÑ‹. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ: Ğ¿Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ°Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° 50% Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ² 1,5 Ñ€Ğ°Ğ·Ğ°.'}, 'en': {'title': 'Optimizing Video Diffusion with TPDiff: Efficiency Unleashed!', 'desc': 'This paper addresses the high computational costs associated with video diffusion models by introducing TPDiff, a framework that optimizes training and inference efficiency. The authors leverage the entropy-reducing nature of the diffusion process and the redundancy between video frames to reduce the need for full frame rates during high-entropy stages. TPDiff operates in multiple stages, gradually increasing the frame rate, with only the final stage using the full frame rate, thus enhancing computational efficiency. The proposed stage-wise diffusion training framework further improves efficiency by solving partitioned probability flow ordinary differential equations, leading to significant reductions in training costs and improvements in inference speed.'}, 'zh': {'title': 'ä¼˜åŒ–è§†é¢‘æ‰©æ•£æ¨¡å‹çš„è®¡ç®—æ•ˆç‡', 'desc': 'è§†é¢‘æ‰©æ•£æ¨¡å‹çš„å‘å±•é¢ä¸´ç€å·¨å¤§çš„è®¡ç®—éœ€æ±‚ã€‚ä¸ºäº†ç¼“è§£è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ³¨æ„åˆ°æ‰©æ•£çš„åå‘è¿‡ç¨‹å…·æœ‰å›ºæœ‰çš„å‡å°‘ç†µçš„ç‰¹æ€§ã€‚è€ƒè™‘åˆ°è§†é¢‘æ¨¡æ€ä¸­çš„å¸§é—´å†—ä½™ï¼Œåœ¨é«˜ç†µé˜¶æ®µä¿æŒå…¨å¸§ç‡æ˜¯æ²¡æœ‰å¿…è¦çš„ã€‚åŸºäºè¿™ä¸€è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†TPDiffæ¡†æ¶ï¼Œé€šè¿‡å°†æ‰©æ•£è¿‡ç¨‹åˆ†ä¸ºå¤šä¸ªé˜¶æ®µï¼Œé€æ­¥æé«˜å¸§ç‡ï¼Œä»è€Œä¼˜åŒ–è®¡ç®—æ•ˆç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.09573', 'title': 'Block Diffusion: Interpolating Between Autoregressive and Diffusion\n  Language Models', 'url': 'https://huggingface.co/papers/2503.09573', 'abstract': 'Diffusion language models offer unique benefits over autoregressive models due to their potential for parallelized generation and controllability, yet they lag in likelihood modeling and are limited to fixed-length generation. In this work, we introduce a class of block diffusion language models that interpolate between discrete denoising diffusion and autoregressive models. Block diffusion overcomes key limitations of both approaches by supporting flexible-length generation and improving inference efficiency with KV caching and parallel token sampling. We propose a recipe for building effective block diffusion models that includes an efficient training algorithm, estimators of gradient variance, and data-driven noise schedules to minimize the variance. Block diffusion sets a new state-of-the-art performance among diffusion models on language modeling benchmarks and enables generation of arbitrary-length sequences. We provide the code, along with the model weights and blog post on the project page: https://m-arriola.com/bd3lms/', 'score': 29, 'issue_id': 2678, 'pub_date': '2025-03-12', 'pub_date_card': {'ru': '12 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 12', 'zh': '3æœˆ12æ—¥'}, 'hash': '32f097e93cbf5f3a', 'authors': ['Marianne Arriola', 'Aaron Gokaslan', 'Justin T Chiu', 'Zhihan Yang', 'Zhixuan Qi', 'Jiaqi Han', 'Subham Sekhar Sahoo', 'Volodymyr Kuleshov'], 'affiliations': ['Cohere, NY, USA', 'Cornell Tech, NY, USA', 'Stanford University, CA, USA'], 'pdf_title_img': 'assets/pdf/title_img/2503.09573.jpg', 'data': {'categories': ['#open_source', '#diffusion', '#benchmark', '#training', '#architecture'], 'emoji': 'ğŸ§©', 'ru': {'title': 'Ğ‘Ğ»Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: Ğ»ÑƒÑ‡ÑˆĞµĞµ Ğ¸Ğ· Ğ´Ğ²ÑƒÑ… Ğ¼Ğ¸Ñ€Ğ¾Ğ² Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ĞºĞ»Ğ°ÑÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ - Ğ±Ğ»Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ½Ğ¸ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‚ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‚ĞµĞºÑÑ‚Ñ‹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ¸ÑĞ¿ĞµÑ€ÑĞ¸Ğ¸ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ° Ğ¸ Ñ€Ğ°ÑĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ ÑˆÑƒĞ¼Ğ° Ğ´Ğ»Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ¸ÑĞ¿ĞµÑ€ÑĞ¸Ğ¸. Ğ‘Ğ»Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑÑ€ĞµĞ´Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'Block Diffusion: The Future of Flexible Language Generation', 'desc': 'This paper presents block diffusion language models, which combine the strengths of diffusion models and autoregressive models. These models allow for flexible-length text generation and improve efficiency during inference by using techniques like KV caching and parallel token sampling. The authors introduce a comprehensive approach for training these models, including methods to reduce gradient variance and optimize noise schedules. As a result, block diffusion models achieve state-of-the-art performance in language modeling tasks and can generate sequences of varying lengths.'}, 'zh': {'title': 'å—æ‰©æ•£æ¨¡å‹ï¼šçµæ´»ç”Ÿæˆä¸é«˜æ•ˆæ¨ç†çš„ç»“åˆ', 'desc': 'æ‰©æ•£è¯­è¨€æ¨¡å‹ç›¸æ¯”è‡ªå›å½’æ¨¡å‹å…·æœ‰ç‹¬ç‰¹çš„ä¼˜åŠ¿ï¼Œå¦‚å¹¶è¡Œç”Ÿæˆå’Œå¯æ§æ€§ï¼Œä½†åœ¨ä¼¼ç„¶å»ºæ¨¡æ–¹é¢è¡¨ç°è¾ƒå·®ï¼Œå¹¶ä¸”ç”Ÿæˆé•¿åº¦å›ºå®šã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç±»å—æ‰©æ•£è¯­è¨€æ¨¡å‹ï¼Œç»“åˆäº†ç¦»æ•£å»å™ªæ‰©æ•£å’Œè‡ªå›å½’æ¨¡å‹çš„ä¼˜ç‚¹ã€‚å—æ‰©æ•£å…‹æœäº†è¿™ä¸¤ç§æ–¹æ³•çš„å…³é”®é™åˆ¶ï¼Œæ”¯æŒçµæ´»é•¿åº¦çš„ç”Ÿæˆï¼Œå¹¶é€šè¿‡KVç¼“å­˜å’Œå¹¶è¡Œä»¤ç‰Œé‡‡æ ·æé«˜æ¨ç†æ•ˆç‡ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ„å»ºæœ‰æ•ˆå—æ‰©æ•£æ¨¡å‹çš„æ–¹æ¡ˆï¼ŒåŒ…æ‹¬é«˜æ•ˆçš„è®­ç»ƒç®—æ³•ã€æ¢¯åº¦æ–¹å·®ä¼°è®¡å™¨å’Œæ•°æ®é©±åŠ¨çš„å™ªå£°è°ƒåº¦ï¼Œä»¥æœ€å°åŒ–æ–¹å·®ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.09151', 'title': 'Reangle-A-Video: 4D Video Generation as Video-to-Video Translation', 'url': 'https://huggingface.co/papers/2503.09151', 'abstract': 'We introduce Reangle-A-Video, a unified framework for generating synchronized multi-view videos from a single input video. Unlike mainstream approaches that train multi-view video diffusion models on large-scale 4D datasets, our method reframes the multi-view video generation task as video-to-videos translation, leveraging publicly available image and video diffusion priors. In essence, Reangle-A-Video operates in two stages. (1) Multi-View Motion Learning: An image-to-video diffusion transformer is synchronously fine-tuned in a self-supervised manner to distill view-invariant motion from a set of warped videos. (2) Multi-View Consistent Image-to-Images Translation: The first frame of the input video is warped and inpainted into various camera perspectives under an inference-time cross-view consistency guidance using DUSt3R, generating multi-view consistent starting images. Extensive experiments on static view transport and dynamic camera control show that Reangle-A-Video surpasses existing methods, establishing a new solution for multi-view video generation. We will publicly release our code and data. Project page: https://hyeonho99.github.io/reangle-a-video/', 'score': 23, 'issue_id': 2681, 'pub_date': '2025-03-12', 'pub_date_card': {'ru': '12 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 12', 'zh': '3æœˆ12æ—¥'}, 'hash': 'e1463c182b0fe8e9', 'authors': ['Hyeonho Jeong', 'Suhyeon Lee', 'Jong Chul Ye'], 'affiliations': ['KAIST AI'], 'pdf_title_img': 'assets/pdf/title_img/2503.09151.jpg', 'data': {'categories': ['#open_source', '#video', '#multimodal', '#diffusion'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ğ¾Ğ¹ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ±ĞµĞ· 4D-Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ²', 'desc': 'Reangle-A-Video - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ: Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ğ¾Ğ¼Ñƒ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ². Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², Reangle-A-Video Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… 4D-Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ², Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ° Ñ€Ğ°ĞºÑƒÑ€ÑĞ° Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ĞºĞ°Ğ¼ĞµÑ€Ğ¾Ğ¹.'}, 'en': {'title': 'Transforming Single Videos into Multi-View Masterpieces!', 'desc': 'Reangle-A-Video is a novel framework designed to create synchronized multi-view videos from a single input video. It innovatively approaches the multi-view video generation task by treating it as a video-to-video translation problem, utilizing existing image and video diffusion models. The process consists of two main stages: first, it learns motion patterns from warped videos in a self-supervised manner, and second, it generates consistent multi-view images by warping and inpainting the initial frame under specific guidance. This method outperforms current techniques in both static view transport and dynamic camera control, marking a significant advancement in multi-view video generation.'}, 'zh': {'title': 'Reangle-A-Videoï¼šå•è§†é¢‘ç”Ÿæˆå¤šè§†è§’åŒæ­¥è§†é¢‘çš„æ–°æ–¹æ³•', 'desc': 'æˆ‘ä»¬æå‡ºäº†Reangle-A-Videoï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œç”¨äºä»å•ä¸ªè¾“å…¥è§†é¢‘ç”ŸæˆåŒæ­¥çš„å¤šè§†è§’è§†é¢‘ã€‚ä¸ä¸»æµæ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•å°†å¤šè§†è§’è§†é¢‘ç”Ÿæˆä»»åŠ¡é‡æ–°å®šä¹‰ä¸ºè§†é¢‘åˆ°è§†é¢‘çš„è½¬æ¢ï¼Œåˆ©ç”¨å…¬å¼€å¯ç”¨çš„å›¾åƒå’Œè§†é¢‘æ‰©æ•£å…ˆéªŒã€‚Reangle-A-Videoçš„æ“ä½œåˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šé¦–å…ˆï¼Œé€šè¿‡è‡ªç›‘ç£æ–¹å¼å¯¹å›¾åƒåˆ°è§†é¢‘çš„æ‰©æ•£å˜æ¢å™¨è¿›è¡ŒåŒæ­¥å¾®è°ƒï¼Œä»¥æå–è§†è§’ä¸å˜çš„è¿åŠ¨ï¼›å…¶æ¬¡ï¼Œåœ¨æ¨ç†æ—¶ä½¿ç”¨DUSt3Rè¿›è¡Œè·¨è§†è§’ä¸€è‡´æ€§æŒ‡å¯¼ï¼Œå°†è¾“å…¥è§†é¢‘çš„ç¬¬ä¸€å¸§å˜å½¢å¹¶ä¿®å¤ä¸ºä¸åŒçš„æ‘„åƒæœºè§†è§’ï¼Œç”Ÿæˆå¤šè§†è§’ä¸€è‡´çš„èµ·å§‹å›¾åƒã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.09601', 'title': 'RewardSDS: Aligning Score Distillation via Reward-Weighted Sampling', 'url': 'https://huggingface.co/papers/2503.09601', 'abstract': 'Score Distillation Sampling (SDS) has emerged as an effective technique for leveraging 2D diffusion priors for tasks such as text-to-3D generation. While powerful, SDS struggles with achieving fine-grained alignment to user intent. To overcome this, we introduce RewardSDS, a novel approach that weights noise samples based on alignment scores from a reward model, producing a weighted SDS loss. This loss prioritizes gradients from noise samples that yield aligned high-reward output. Our approach is broadly applicable and can extend SDS-based methods. In particular, we demonstrate its applicability to Variational Score Distillation (VSD) by introducing RewardVSD. We evaluate RewardSDS and RewardVSD on text-to-image, 2D editing, and text-to-3D generation tasks, showing significant improvements over SDS and VSD on a diverse set of metrics measuring generation quality and alignment to desired reward models, enabling state-of-the-art performance. Project page is available at https://itaychachy. github.io/reward-sds/.', 'score': 10, 'issue_id': 2683, 'pub_date': '2025-03-12', 'pub_date_card': {'ru': '12 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 12', 'zh': '3æœˆ12æ—¥'}, 'hash': '4cbf8c2d009fe092', 'authors': ['Itay Chachy', 'Guy Yariv', 'Sagie Benaim'], 'affiliations': ['Hebrew University of Jerusalem'], 'pdf_title_img': 'assets/pdf/title_img/2503.09601.jpg', 'data': {'categories': ['#multimodal', '#alignment', '#diffusion', '#3d', '#training'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Ğ¢Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ²Ğ·Ğ²ĞµÑˆĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ RewardSDS, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‰Ğ¸Ğ¹ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ Score Distillation Sampling (SDS) Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°. RewardSDS Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ²Ğ·Ğ²ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ñ ÑˆÑƒĞ¼Ğ¾Ğ²Ñ‹Ñ… ÑÑĞ¼Ğ¿Ğ»Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ¸ÑĞ¼ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ RewardVSD - Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Variational Score Distillation (VSD). Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸ÑĞ¼ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ 3D-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Aligning User Intent with Reward-Driven Sampling', 'desc': 'This paper presents RewardSDS, an innovative method that enhances Score Distillation Sampling (SDS) by incorporating alignment scores from a reward model. By weighting noise samples according to their alignment with user intent, RewardSDS generates a more effective weighted loss function. This approach not only improves the performance of SDS but also extends its application to Variational Score Distillation (VSD) through the introduction of RewardVSD. The authors demonstrate that their methods significantly outperform existing techniques in generating high-quality outputs across various tasks, including text-to-image and text-to-3D generation.'}, 'zh': {'title': 'æå‡ç”Ÿæˆå¯¹é½åº¦çš„æ–°æ–¹æ³•', 'desc': 'å¾—åˆ†è’¸é¦é‡‡æ ·ï¼ˆSDSï¼‰æ˜¯ä¸€ç§æœ‰æ•ˆåˆ©ç”¨äºŒç»´æ‰©æ•£å…ˆéªŒè¿›è¡Œæ–‡æœ¬åˆ°ä¸‰ç»´ç”Ÿæˆçš„æŠ€æœ¯ã€‚ç„¶è€Œï¼ŒSDSåœ¨å®ç°ä¸ç”¨æˆ·æ„å›¾çš„ç»†è‡´å¯¹é½æ–¹é¢å­˜åœ¨å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†RewardSDSï¼Œè¿™æ˜¯ä¸€ç§åŸºäºå¥–åŠ±æ¨¡å‹çš„å¯¹é½å¾—åˆ†åŠ æƒå™ªå£°æ ·æœ¬çš„æ–°æ–¹æ³•ï¼Œä»è€Œç”ŸæˆåŠ æƒçš„SDSæŸå¤±ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼ŒRewardSDSå’ŒRewardVSDåœ¨æ–‡æœ¬åˆ°å›¾åƒã€äºŒç»´ç¼–è¾‘å’Œæ–‡æœ¬åˆ°ä¸‰ç»´ç”Ÿæˆä»»åŠ¡ä¸Šæ˜¾è‘—æé«˜äº†ç”Ÿæˆè´¨é‡å’Œä¸æœŸæœ›å¥–åŠ±æ¨¡å‹çš„å¯¹é½åº¦ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.08525', 'title': 'GTR: Guided Thought Reinforcement Prevents Thought Collapse in RL-based\n  VLM Agent Training', 'url': 'https://huggingface.co/papers/2503.08525', 'abstract': "Reinforcement learning with verifiable outcome rewards (RLVR) has effectively scaled up chain-of-thought (CoT) reasoning in large language models (LLMs). Yet, its efficacy in training vision-language model (VLM) agents for goal-directed action reasoning in visual environments is less established. This work investigates this problem through extensive experiments on complex card games, such as 24 points, and embodied tasks from ALFWorld. We find that when rewards are based solely on action outcomes, RL fails to incentivize CoT reasoning in VLMs, instead leading to a phenomenon we termed thought collapse, characterized by a rapid loss of diversity in the agent's thoughts, state-irrelevant and incomplete reasoning, and subsequent invalid actions, resulting in negative rewards. To counteract thought collapse, we highlight the necessity of process guidance and propose an automated corrector that evaluates and refines the agent's reasoning at each RL step. This simple and scalable GTR (Guided Thought Reinforcement) framework trains reasoning and action simultaneously without the need for dense, per-step human labeling. Our experiments demonstrate that GTR significantly enhances the performance and generalization of the LLaVA-7b model across various visual environments, achieving 3-5 times higher task success rates compared to SoTA models with notably smaller model sizes.", 'score': 10, 'issue_id': 2681, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 11', 'zh': '3æœˆ11æ—¥'}, 'hash': 'e82260bba3e835b4', 'authors': ['Tong Wei', 'Yijun Yang', 'Junliang Xing', 'Yuanchun Shi', 'Zongqing Lu', 'Deheng Ye'], 'affiliations': ['Peking University', 'Tencent', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.08525.jpg', 'data': {'categories': ['#agents', '#games', '#reasoning', '#video', '#training', '#optimization', '#rl'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ£Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸ Ñ„ĞµĞ½Ğ¾Ğ¼ĞµĞ½ 'ĞºĞ¾Ğ»Ğ»Ğ°Ğ¿ÑĞ° Ğ¼Ñ‹ÑĞ»ĞµĞ¹' Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ GTR (Guided Thought Reinforcement), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¸ ÑƒÑ‚Ğ¾Ñ‡Ğ½ÑĞµÑ‚ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ ÑˆĞ°Ğ³Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ GTR Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ LLaVA-7b Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ…."}, 'en': {'title': 'Enhancing VLMs with Guided Thought Reinforcement', 'desc': "This paper explores the challenges of using reinforcement learning (RL) to train vision-language models (VLMs) for reasoning in visual tasks. It identifies a problem called 'thought collapse', where the model's reasoning becomes less diverse and leads to incorrect actions when rewards are based only on outcomes. To address this, the authors propose a Guided Thought Reinforcement (GTR) framework that provides process guidance to improve the reasoning of VLMs during training. Their experiments show that GTR significantly boosts the performance of the LLaVA-7b model, achieving much higher success rates in complex tasks compared to state-of-the-art models."}, 'zh': {'title': 'å¼•å¯¼æ€ç»´å¼ºåŒ–ï¼šæå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†å¯éªŒè¯ç»“æœå¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰åœ¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ä¸­çš„åº”ç”¨ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚çš„è§†è§‰ç¯å¢ƒä¸­è¿›è¡Œç›®æ ‡å¯¼å‘çš„æ¨ç†ã€‚æˆ‘ä»¬å‘ç°ï¼Œå½“å¥–åŠ±ä»…åŸºäºè¡ŒåŠ¨ç»“æœæ—¶ï¼ŒRLæ— æ³•æœ‰æ•ˆæ¿€åŠ±VLMçš„æ€ç»´é“¾æ¨ç†ï¼Œå¯¼è‡´æ€ç»´å´©æºƒç°è±¡ï¼Œè¡¨ç°ä¸ºä»£ç†çš„æ€ç»´å¤šæ ·æ€§è¿…é€Ÿä¸‹é™å’Œæ¨ç†ä¸å®Œæ•´ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è‡ªåŠ¨çº æ­£å™¨ï¼Œèƒ½å¤Ÿåœ¨æ¯ä¸ªRLæ­¥éª¤ä¸­è¯„ä¼°å’Œæ”¹è¿›ä»£ç†çš„æ¨ç†è¿‡ç¨‹ã€‚é€šè¿‡å®éªŒï¼Œæˆ‘ä»¬çš„å¼•å¯¼æ€ç»´å¼ºåŒ–ï¼ˆGTRï¼‰æ¡†æ¶æ˜¾è‘—æé«˜äº†LLaVA-7bæ¨¡å‹åœ¨å„ç§è§†è§‰ç¯å¢ƒä¸­çš„è¡¨ç°å’Œæ³›åŒ–èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.04388', 'title': 'More Documents, Same Length: Isolating the Challenge of Multiple\n  Documents in RAG', 'url': 'https://huggingface.co/papers/2503.04388', 'abstract': 'Retrieval-augmented generation (RAG) provides LLMs with relevant documents. Although previous studies noted that retrieving many documents can degrade performance, they did not isolate how the quantity of documents affects performance while controlling for context length. We evaluate various language models on custom datasets derived from a multi-hop QA task. We keep the context length and position of relevant information constant while varying the number of documents, and find that increasing the document count in RAG settings poses significant challenges for LLMs. Additionally, our results indicate that processing multiple documents is a separate challenge from handling long contexts. We also make the datasets and code available: https://github.com/shaharl6000/MoreDocsSameLen .', 'score': 10, 'issue_id': 2682, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 6', 'zh': '3æœˆ6æ—¥'}, 'hash': '9b3fb8251a206c0d', 'authors': ['Shahar Levy', 'Nir Mazor', 'Lihi Shalmon', 'Michael Hassid', 'Gabriel Stanovsky'], 'affiliations': ['School of Computer Science and Engineering The Hebrew University of Jerusalem, Jerusalem, Israel'], 'pdf_title_img': 'assets/pdf/title_img/2503.04388.jpg', 'data': {'categories': ['#long_context', '#dataset', '#open_source', '#rag'], 'emoji': 'ğŸ“š', 'ru': {'title': 'Ğ‘Ğ¾Ğ»ÑŒÑˆĞµ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² - Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸ĞµĞ¼ (RAG). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ³Ğ´Ğµ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞ»Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ñƒ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ½Ñ‹Ğ¼Ğ¸, Ğ½Ğ¾ Ğ²Ğ°Ñ€ÑŒĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ñ‡Ğ¸ÑĞ»Ğ¾ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² RAG ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ±Ñ‹Ğ»Ğ¾ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² - ÑÑ‚Ğ¾ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ¾Ñ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'More Documents, More Challenges for LLMs!', 'desc': 'This paper investigates how the number of documents retrieved in Retrieval-augmented generation (RAG) impacts the performance of large language models (LLMs). The authors conduct experiments using custom datasets focused on multi-hop question answering, ensuring that context length remains constant while varying the number of documents. Their findings reveal that increasing the number of documents can significantly hinder the performance of LLMs, indicating that managing multiple documents presents unique challenges distinct from those associated with long contexts. The study contributes to the understanding of RAG by providing datasets and code for further research.'}, 'zh': {'title': 'æ–‡æ¡£æ•°é‡å½±å“LLMæ€§èƒ½çš„æŒ‘æˆ˜', 'desc': 'æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æä¾›ç›¸å…³æ–‡æ¡£ã€‚å°½ç®¡ä¹‹å‰çš„ç ”ç©¶æŒ‡å‡ºæ£€ç´¢è¿‡å¤šæ–‡æ¡£å¯èƒ½ä¼šé™ä½æ€§èƒ½ï¼Œä½†å¹¶æœªæ˜ç¡®æ§åˆ¶ä¸Šä¸‹æ–‡é•¿åº¦æ¥ç ”ç©¶æ–‡æ¡£æ•°é‡å¯¹æ€§èƒ½çš„å½±å“ã€‚æˆ‘ä»¬åœ¨å¤šè·³é—®ç­”ä»»åŠ¡çš„è‡ªå®šä¹‰æ•°æ®é›†ä¸Šè¯„ä¼°äº†å„ç§è¯­è¨€æ¨¡å‹ï¼Œå‘ç°å¢åŠ æ–‡æ¡£æ•°é‡åœ¨RAGè®¾ç½®ä¸­å¯¹LLMsé€ æˆäº†æ˜¾è‘—æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå¤„ç†å¤šä¸ªæ–‡æ¡£ä¸å¤„ç†é•¿ä¸Šä¸‹æ–‡æ˜¯ä¸¤ä¸ªä¸åŒçš„æŒ‘æˆ˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.06955', 'title': 'Motion Anything: Any to Motion Generation', 'url': 'https://huggingface.co/papers/2503.06955', 'abstract': 'Conditional motion generation has been extensively studied in computer vision, yet two critical challenges remain. First, while masked autoregressive methods have recently outperformed diffusion-based approaches, existing masking models lack a mechanism to prioritize dynamic frames and body parts based on given conditions. Second, existing methods for different conditioning modalities often fail to integrate multiple modalities effectively, limiting control and coherence in generated motion. To address these challenges, we propose Motion Anything, a multimodal motion generation framework that introduces an Attention-based Mask Modeling approach, enabling fine-grained spatial and temporal control over key frames and actions. Our model adaptively encodes multimodal conditions, including text and music, improving controllability. Additionally, we introduce Text-Music-Dance (TMD), a new motion dataset consisting of 2,153 pairs of text, music, and dance, making it twice the size of AIST++, thereby filling a critical gap in the community. Extensive experiments demonstrate that Motion Anything surpasses state-of-the-art methods across multiple benchmarks, achieving a 15% improvement in FID on HumanML3D and showing consistent performance gains on AIST++ and TMD. See our project website https://steve-zeyu-zhang.github.io/MotionAnything', 'score': 7, 'issue_id': 2680, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 10', 'zh': '3æœˆ10æ—¥'}, 'hash': '9199d2d99b75d862', 'authors': ['Zeyu Zhang', 'Yiran Wang', 'Wei Mao', 'Danning Li', 'Rui Zhao', 'Biao Wu', 'Zirui Song', 'Bohan Zhuang', 'Ian Reid', 'Richard Hartley'], 'affiliations': ['ANU', 'Google', 'JD.com', 'MBZUAI', 'McGill', 'Tencent', 'USYD', 'UTS', 'ZJU'], 'pdf_title_img': 'assets/pdf/title_img/2503.06955.jpg', 'data': {'categories': ['#games', '#synthetic', '#optimization', '#benchmark', '#multimodal', '#cv', '#dataset'], 'emoji': 'ğŸ•º', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Motion Anything - Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰ÑƒÑ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ½Ğ°Ğ´ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼Ğ¸ ĞºĞ°Ğ´Ñ€Ğ°Ğ¼Ğ¸ Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑĞ¼Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ‚ĞµĞºÑÑ‚ Ğ¸ Ğ¼ÑƒĞ·Ñ‹ĞºÑƒ, Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Text-Music-Dance (TMD), ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 2153 Ğ¿Ğ°Ñ€Ñ‹ Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸ Ğ¸ Ñ‚Ğ°Ğ½Ñ†Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Motion Anything Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ 15% ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ FID Ğ½Ğ° HumanML3D.'}, 'en': {'title': 'Revolutionizing Motion Generation with Multimodal Control', 'desc': 'This paper presents Motion Anything, a new framework for generating motion that effectively combines multiple input types like text and music. It addresses two main challenges in motion generation: the need for prioritizing dynamic elements and the integration of different conditioning modalities. The proposed Attention-based Mask Modeling allows for better control over key frames and actions, enhancing the quality of generated motions. Additionally, the introduction of the Text-Music-Dance dataset provides a larger resource for training, leading to significant improvements in performance compared to existing methods.'}, 'zh': {'title': 'å¤šæ¨¡æ€è¿åŠ¨ç”Ÿæˆçš„æ–°çªç ´', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºMotion Anythingçš„å¤šæ¨¡æ€è¿åŠ¨ç”Ÿæˆæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•åœ¨åŠ¨æ€å¸§å’Œèº«ä½“éƒ¨ä½ä¼˜å…ˆçº§æ–¹é¢çš„ä¸è¶³ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºæ³¨æ„åŠ›çš„æ©æ¨¡å»ºæ¨¡æ–¹æ³•ï¼Œä½¿å¾—å¯¹å…³é”®å¸§å’ŒåŠ¨ä½œçš„ç©ºé—´å’Œæ—¶é—´æ§åˆ¶æ›´åŠ ç²¾ç»†ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿè‡ªé€‚åº”ç¼–ç æ–‡æœ¬å’ŒéŸ³ä¹ç­‰å¤šæ¨¡æ€æ¡ä»¶ï¼Œä»è€Œæé«˜ç”Ÿæˆè¿åŠ¨çš„å¯æ§æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜åˆ›å»ºäº†ä¸€ä¸ªæ–°çš„è¿åŠ¨æ•°æ®é›†Text-Music-Dance (TMD)ï¼ŒåŒ…å«2153å¯¹æ–‡æœ¬ã€éŸ³ä¹å’Œèˆè¹ˆï¼Œå¡«è¡¥äº†ç¤¾åŒºä¸­çš„é‡è¦ç©ºç™½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.07103', 'title': 'Quantizing Large Language Models for Code Generation: A Differentiated\n  Replication', 'url': 'https://huggingface.co/papers/2503.07103', 'abstract': "Large Language Models (LLMs) have shown an impressive capability in code generation and, specifically, to automatically implement requirements described in natural language. The LLM effectiveness generally increases with its size: The higher the number of LLM's trainable parameters the better its ability to implement code. However, when it comes to deploying LLM-based code generators, larger LLMs pose significant challenges related to their memory (and, consequently, carbon) footprint. A previous work by Wei et al. proposed to leverage quantization techniques to reduce the memory footprint of LLM-based code generators without substantially degrading their effectiveness. In short, they studied LLMs featuring up to 16B parameters, quantizing their precision from floating point 32 bits down to int 8 bits and showing their limited impact on code generation performance. Given the fast pace at which LLM capabilities and quantization techniques are evolving, in this work we present a differentiated replication of the work by Wei et al. in which we consider (i) on the one side, more recent and larger code-related LLMs, of up to 34B parameters; (ii) the latest advancements in model quantization techniques, which allow pushing the compression to the extreme quantization level of 2 bits per model parameter and; (iii) different types of calibration datasets to guide the quantization process, including code-specific ones. Our empirical evaluation reveals that the new frontier for LLM quantization is 4-bit precision, resulting in an average memory footprint reduction of 70% compared to the original model without observing any significant decrease in performance. Additionally, when the quantization becomes even more extreme (3 and 2 bits), a code-specific calibration dataset helps to limit the loss of performance.", 'score': 6, 'issue_id': 2683, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 10', 'zh': '3æœˆ10æ—¥'}, 'hash': '4f07119680bb80a1', 'authors': ['Alessandro Giagnorio', 'Antonio Mastropaolo', 'Saima Afrin', 'Massimiliano Di Penta', 'Gabriele Bavota'], 'affiliations': ['Software Institute UniversitÃ  della Svizzera italiana, Switzerland'], 'pdf_title_img': 'assets/pdf/title_img/2503.07103.jpg', 'data': {'categories': ['#optimization', '#inference', '#dataset', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞšĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ LLM Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸Ğ·ÑƒÑ‡Ğ°ÑÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ¾ 2 Ğ±Ğ¸Ñ‚ Ğ½Ğ° Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ 4-Ğ±Ğ¸Ñ‚Ğ½Ğ¾Ğµ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ Ğ¾Ğ±ÑŠĞµĞ¼ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ½Ğ° 70% Ğ±ĞµĞ· ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞŸÑ€Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞºÑÑ‚Ñ€ĞµĞ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ (3 Ğ¸ 2 Ğ±Ğ¸Ñ‚Ğ°) Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ»Ñ ĞºĞ¾Ğ´Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ñ‚ÑŒ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Optimizing Code Generation with Extreme Quantization Techniques', 'desc': 'This paper explores the use of quantization techniques to reduce the memory footprint of large language models (LLMs) used for code generation. It builds on previous work by examining larger models with up to 34 billion parameters and applying advanced quantization methods that compress model precision down to 2 bits. The study finds that using 4-bit quantization can reduce memory usage by 70% without significantly impacting performance. Furthermore, it highlights the importance of using code-specific calibration datasets to maintain effectiveness even with more aggressive quantization levels.'}, 'zh': {'title': 'é‡åŒ–æŠ€æœ¯åŠ©åŠ›å¤§å‹è¯­è¨€æ¨¡å‹çš„é«˜æ•ˆä»£ç ç”Ÿæˆ', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä»£ç ç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿæ ¹æ®è‡ªç„¶è¯­è¨€æè¿°è‡ªåŠ¨å®ç°éœ€æ±‚ã€‚æ¨¡å‹çš„æœ‰æ•ˆæ€§é€šå¸¸éšç€å‚æ•°æ•°é‡çš„å¢åŠ è€Œæé«˜ï¼Œä½†è¾ƒå¤§çš„æ¨¡å‹åœ¨éƒ¨ç½²æ—¶ä¼šé¢ä¸´å†…å­˜å’Œç¢³è¶³è¿¹çš„æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œç ”ç©¶è€…ä»¬æå‡ºäº†é‡åŒ–æŠ€æœ¯ï¼Œä»¥å‡å°‘LLMä»£ç ç”Ÿæˆå™¨çš„å†…å­˜å ç”¨ï¼ŒåŒæ—¶ä¿æŒå…¶æœ‰æ•ˆæ€§ã€‚æœ¬æ–‡é€šè¿‡ç ”ç©¶æ›´æ–°çš„ã€å‚æ•°é«˜è¾¾34Bçš„LLMï¼Œæ¢ç´¢äº†æ›´æç«¯çš„é‡åŒ–æŠ€æœ¯ï¼Œå‘ç°4ä½ç²¾åº¦çš„é‡åŒ–å¯ä»¥å°†å†…å­˜å ç”¨å‡å°‘70%ï¼Œè€Œæ€§èƒ½å‡ ä¹æ²¡æœ‰æ˜¾è‘—ä¸‹é™ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.06573', 'title': 'WildIFEval: Instruction Following in the Wild', 'url': 'https://huggingface.co/papers/2503.06573', 'abstract': 'Recent LLMs have shown remarkable success in following user instructions, yet handling instructions with multiple constraints remains a significant challenge. In this work, we introduce WildIFEval - a large-scale dataset of 12K real user instructions with diverse, multi-constraint conditions. Unlike prior datasets, our collection spans a broad lexical and topical spectrum of constraints, in natural user prompts. We categorize these constraints into eight high-level classes to capture their distribution and dynamics in real-world scenarios. Leveraging WildIFEval, we conduct extensive experiments to benchmark the instruction-following capabilities of leading LLMs. Our findings reveal that all evaluated models experience performance degradation with an increasing number of constraints. Thus, we show that all models have a large room for improvement on such tasks. Moreover, we observe that the specific type of constraint plays a critical role in model performance. We release our dataset to promote further research on instruction-following under complex, realistic conditions.', 'score': 5, 'issue_id': 2683, 'pub_date': '2025-03-09', 'pub_date_card': {'ru': '9 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 9', 'zh': '3æœˆ9æ—¥'}, 'hash': 'a2fa7b2b917156fd', 'authors': ['Gili Lior', 'Asaf Yehudai', 'Ariel Gera', 'Liat Ein-Dor'], 'affiliations': ['IBM Research', 'The Hebrew University of Jerusalem'], 'pdf_title_img': 'assets/pdf/title_img/2503.06573.jpg', 'data': {'categories': ['#alignment', '#dataset', '#benchmark'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ²Ñ‹Ğ·Ğ¾Ğ² Ğ´Ğ»Ñ Ğ˜Ğ˜: ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… WildIFEval, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 12000 Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸ÑĞ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒÑÑ‚ ÑÑ‚Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° 8 ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¹ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ²ÑĞµÑ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ÑÑ Ñ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ‡Ğ¸ÑĞ»Ğ° Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸ÑĞ¼Ğ¸.'}, 'en': {'title': 'WildIFEval: Benchmarking LLMs on Complex User Instructions', 'desc': 'This paper presents WildIFEval, a new dataset designed to evaluate how well large language models (LLMs) can follow user instructions with multiple constraints. The dataset contains 12,000 real user prompts that include a variety of constraints categorized into eight classes, reflecting the complexity of real-world instructions. Experiments show that as the number of constraints increases, the performance of all tested LLMs declines, indicating a significant area for improvement. The study highlights the importance of the type of constraint in affecting model performance and aims to encourage further research in this challenging area of instruction-following.'}, 'zh': {'title': 'å¤šé‡çº¦æŸä¸‹çš„æŒ‡ä»¤éµå¾ªæŒ‘æˆ˜', 'desc': 'æœ€è¿‘çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨éµå¾ªç”¨æˆ·æŒ‡ä»¤æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†å¤„ç†å…·æœ‰å¤šä¸ªçº¦æŸçš„æŒ‡ä»¤ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†WildIFEvalï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«12000ä¸ªçœŸå®ç”¨æˆ·æŒ‡ä»¤çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œæ¶µç›–äº†å¤šæ ·åŒ–çš„å¤šé‡çº¦æŸæ¡ä»¶ã€‚ä¸ä¹‹å‰çš„æ•°æ®é›†ä¸åŒï¼Œæˆ‘ä»¬çš„æ”¶é›†æ¶µç›–äº†å¹¿æ³›çš„è¯æ±‡å’Œä¸»é¢˜çº¦æŸï¼Œå¹¶å°†è¿™äº›çº¦æŸåˆ†ä¸ºå…«ä¸ªé«˜å±‚ç±»åˆ«ï¼Œä»¥æ•æ‰å®ƒä»¬åœ¨ç°å®åœºæ™¯ä¸­çš„åˆ†å¸ƒå’ŒåŠ¨æ€ã€‚é€šè¿‡WildIFEvalï¼Œæˆ‘ä»¬è¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œä»¥åŸºå‡†æµ‹è¯•é¢†å…ˆLLMsçš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›ï¼Œå‘ç°æ‰€æœ‰è¯„ä¼°çš„æ¨¡å‹åœ¨çº¦æŸæ•°é‡å¢åŠ æ—¶æ€§èƒ½ä¸‹é™ï¼Œè¡¨æ˜è¿™äº›ä»»åŠ¡ä»æœ‰å¾ˆå¤§çš„æ”¹è¿›ç©ºé—´ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.09402', 'title': 'VLog: Video-Language Models by Generative Retrieval of Narration\n  Vocabulary', 'url': 'https://huggingface.co/papers/2503.09402', 'abstract': "Human daily activities can be concisely narrated as sequences of routine events (e.g., turning off an alarm) in video streams, forming an event vocabulary. Motivated by this, we introduce VLog, a novel video understanding framework that define video narrations as vocabulary, going beyond the typical subword vocabularies in existing generative video-language models. Built on the lightweight language model GPT-2, VLog feature three key innovations: (i) A generative retrieval model, marrying language model's complex reasoning capabilities with contrastive retrieval's efficient similarity search. (ii) A hierarchical vocabulary derived from large-scale video narrations using our narration pair encoding algorithm, enabling efficient indexing of specific events (e.g., cutting a tomato) by identifying broader scenarios (e.g., kitchen) with expressive postfixes (e.g., by the left hand). (iii) A vocabulary update strategy leveraging generative models to extend the vocabulary for novel events encountered during inference. To validate our approach, we introduce VidCap-Eval, a development set requiring concise narrations with reasoning relationships (e.g., before and after). Experiments on EgoSchema, COIN, and HiREST further demonstrate the effectiveness of VLog, highlighting its ability to generate concise, contextually accurate, and efficient narrations, offering a novel perspective on video understanding. Codes are released at https://github.com/showlab/VLog.", 'score': 4, 'issue_id': 2681, 'pub_date': '2025-03-12', 'pub_date_card': {'ru': '12 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 12', 'zh': '3æœˆ12æ—¥'}, 'hash': '6a25ed0e2c069e4f', 'authors': ['Kevin Qinghong Lin', 'Mike Zheng Shou'], 'affiliations': ['Show Lab, National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2503.09402.jpg', 'data': {'categories': ['#dataset', '#multimodal', '#games', '#reasoning', '#video', '#benchmark'], 'emoji': 'ğŸ¥', 'ru': {'title': 'VLog: ĞŸĞµÑ€ĞµÑĞºĞ°Ğ· Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµÑ€ĞµĞ· ÑĞ»Ğ¾Ğ²Ğ°Ñ€ÑŒ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹', 'desc': 'VLog - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ»Ğ¾Ğ²Ğ°Ñ€ÑŒ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ²ÑĞµĞ´Ğ½ĞµĞ²Ğ½Ğ¾Ğ¹ Ğ´ĞµÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°. ĞĞ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ½Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ GPT-2 Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ¸ÑĞºĞ°, Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ ÑĞ»Ğ¾Ğ²Ğ°Ñ€ÑŒ Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ. VLog ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºÑ€Ğ°Ñ‚ĞºĞ¸Ğµ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ²ÑĞ·Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸ÑĞ¼Ğ¸. Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ±Ñ‹Ğ»Ğ° Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ VidCap-Eval.'}, 'en': {'title': 'VLog: Revolutionizing Video Narration with Hierarchical Vocabulary', 'desc': 'The paper presents VLog, a new framework for understanding videos by narrating daily activities as sequences of events. It introduces a hierarchical vocabulary that allows for efficient indexing of specific actions within broader contexts, enhancing the way video content is interpreted. VLog combines a generative retrieval model with a lightweight language model, enabling complex reasoning and efficient similarity searches. Additionally, it features a vocabulary update strategy that adapts to new events during inference, demonstrating its effectiveness through experiments on various datasets.'}, 'zh': {'title': 'VLogï¼šè§†é¢‘ç†è§£çš„æ–°è§†è§’', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºVLogçš„è§†é¢‘ç†è§£æ¡†æ¶ï¼Œæ—¨åœ¨å°†è§†é¢‘å™è¿°å®šä¹‰ä¸ºè¯æ±‡ï¼Œè¶…è¶Šç°æœ‰ç”Ÿæˆè§†é¢‘è¯­è¨€æ¨¡å‹ä¸­çš„å­è¯è¯æ±‡ã€‚VLogåŸºäºè½»é‡çº§è¯­è¨€æ¨¡å‹GPT-2ï¼Œå…·æœ‰ä¸‰é¡¹å…³é”®åˆ›æ–°ï¼šç”Ÿæˆæ£€ç´¢æ¨¡å‹ã€å±‚æ¬¡è¯æ±‡å’Œè¯æ±‡æ›´æ–°ç­–ç•¥ã€‚ç”Ÿæˆæ£€ç´¢æ¨¡å‹ç»“åˆäº†è¯­è¨€æ¨¡å‹çš„å¤æ‚æ¨ç†èƒ½åŠ›å’Œå¯¹æ¯”æ£€ç´¢çš„é«˜æ•ˆç›¸ä¼¼æ€§æœç´¢ã€‚é€šè¿‡åœ¨EgoSchemaã€COINå’ŒHiRESTæ•°æ®é›†ä¸Šçš„å®éªŒï¼ŒéªŒè¯äº†VLogåœ¨ç”Ÿæˆç®€æ´ã€ä¸Šä¸‹æ–‡å‡†ç¡®çš„å™è¿°æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.09579', 'title': 'Cost-Optimal Grouped-Query Attention for Long-Context LLMs', 'url': 'https://huggingface.co/papers/2503.09579', 'abstract': 'Building effective and efficient Transformer-based large language models (LLMs) has recently become a research focus, requiring maximizing model language capabilities and minimizing training and deployment costs. Existing efforts have primarily described complex relationships among model performance, parameter size, and data size, as well as searched for the optimal compute allocation to train LLMs. However, they overlook the impacts of context length and attention head configuration (the number of query and key-value heads in grouped-query attention) on training and inference. In this paper, we systematically compare models with different parameter sizes, context lengths, and attention head configurations in terms of model performance, computational cost, and memory cost. Then, we extend the existing scaling methods, which are based solely on parameter size and training compute, to guide the construction of cost-optimal LLMs during both training and inference. Our quantitative scaling studies show that, when processing sufficiently long sequences, a larger model with fewer attention heads can achieve a lower loss while incurring lower computational and memory costs. Our findings provide valuable insights for developing practical LLMs, especially in long-context processing scenarios. We will publicly release our code and data.', 'score': 3, 'issue_id': 2682, 'pub_date': '2025-03-12', 'pub_date_card': {'ru': '12 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 12', 'zh': '3æœˆ12æ—¥'}, 'hash': '2c884c8c6aab1cc4', 'authors': ['Yingfa Chen', 'Yutong Wu', 'Xu Han', 'Zhiyuan Liu', 'Maosong Sun'], 'affiliations': ['NLP Group, DCST, IAI, BNRIST, Tsinghua University, Beijing, China', 'SIST, University of Science and Technology Beijing, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.09579.jpg', 'data': {'categories': ['#optimization', '#architecture', '#open_source', '#long_context', '#inference', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ LLM: Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğº Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ´Ğ»Ğ¸Ğ½Ñ‹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğº Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´ÑÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ´Ğ»Ğ¸Ğ½Ğ°Ğ¼Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğº Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. ĞĞ½Ğ¸ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑÑÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ LLM ĞºĞ°Ğº Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ‚Ğ°Ğº Ğ¸ Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğµ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğº Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ… Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸.'}, 'en': {'title': 'Optimizing LLMs: Less Heads, More Efficiency!', 'desc': 'This paper investigates how different configurations of Transformer-based large language models (LLMs) affect their performance and resource efficiency. It highlights the importance of context length and attention head settings, which have been largely ignored in previous research. By comparing various model sizes and configurations, the authors propose new scaling methods that optimize both training and inference costs. Their results indicate that larger models with fewer attention heads can perform better and use less computational and memory resources when handling long sequences.'}, 'zh': {'title': 'ä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹çš„æ„å»ºä¸æˆæœ¬', 'desc': 'æœ¬æ–‡æ¢è®¨äº†å¦‚ä½•æ„å»ºé«˜æ•ˆçš„åŸºäºTransformerçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œé‡ç‚¹åœ¨äºæœ€å¤§åŒ–æ¨¡å‹çš„è¯­è¨€èƒ½åŠ›ï¼ŒåŒæ—¶é™ä½è®­ç»ƒå’Œéƒ¨ç½²æˆæœ¬ã€‚ç ”ç©¶æ¯”è¾ƒäº†ä¸åŒå‚æ•°å¤§å°ã€ä¸Šä¸‹æ–‡é•¿åº¦å’Œæ³¨æ„åŠ›å¤´é…ç½®å¯¹æ¨¡å‹æ€§èƒ½ã€è®¡ç®—æˆæœ¬å’Œå†…å­˜æˆæœ¬çš„å½±å“ã€‚ç»“æœè¡¨æ˜ï¼Œåœ¨å¤„ç†è¾ƒé•¿åºåˆ—æ—¶ï¼Œè¾ƒå¤§çš„æ¨¡å‹é…åˆè¾ƒå°‘çš„æ³¨æ„åŠ›å¤´å¯ä»¥å®ç°æ›´ä½çš„æŸå¤±ï¼ŒåŒæ—¶é™ä½è®¡ç®—å’Œå†…å­˜æˆæœ¬ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ºå¼€å‘å®ç”¨çš„LLMsæä¾›äº†é‡è¦çš„è§è§£ï¼Œå°¤å…¶æ˜¯åœ¨é•¿ä¸Šä¸‹æ–‡å¤„ç†çš„åœºæ™¯ä¸­ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.09419', 'title': 'Alias-Free Latent Diffusion Models:Improving Fractional Shift\n  Equivariance of Diffusion Latent Space', 'url': 'https://huggingface.co/papers/2503.09419', 'abstract': 'Latent Diffusion Models (LDMs) are known to have an unstable generation process, where even small perturbations or shifts in the input noise can lead to significantly different outputs. This hinders their applicability in applications requiring consistent results. In this work, we redesign LDMs to enhance consistency by making them shift-equivariant. While introducing anti-aliasing operations can partially improve shift-equivariance, significant aliasing and inconsistency persist due to the unique challenges in LDMs, including 1) aliasing amplification during VAE training and multiple U-Net inferences, and 2) self-attention modules that inherently lack shift-equivariance. To address these issues, we redesign the attention modules to be shift-equivariant and propose an equivariance loss that effectively suppresses the frequency bandwidth of the features in the continuous domain. The resulting alias-free LDM (AF-LDM) achieves strong shift-equivariance and is also robust to irregular warping. Extensive experiments demonstrate that AF-LDM produces significantly more consistent results than vanilla LDM across various applications, including video editing and image-to-image translation. Code is available at: https://github.com/SingleZombie/AFLDM', 'score': 3, 'issue_id': 2680, 'pub_date': '2025-03-12', 'pub_date_card': {'ru': '12 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 12', 'zh': '3æœˆ12æ—¥'}, 'hash': '1c497a991b18da6a', 'authors': ['Yifan Zhou', 'Zeqi Xiao', 'Shuai Yang', 'Xingang Pan'], 'affiliations': ['S-Lab, Nanyang Technological University', 'Wangxuan Institute of Computer Technology, Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2503.09419.jpg', 'data': {'categories': ['#video', '#diffusion', '#training', '#optimization', '#architecture', '#cv'], 'emoji': 'ğŸ”„', 'ru': {'title': 'Ğ¡Ñ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞºĞ²Ğ¸Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LDM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ AF-LDM, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±Ğ»Ğ°Ğ´Ğ°ĞµÑ‚ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ¾Ğ¼ ÑĞºĞ²Ğ¸Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº ÑĞ´Ğ²Ğ¸Ğ³Ñƒ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ Ğ¿ĞµÑ€ĞµÑ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ¼Ğ¾Ğ´ÑƒĞ»ĞµĞ¹ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ²Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ»Ğ¾ÑÑ‹ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ AF-LDM Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğµ LDM Ğ¿Ğ¾ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Achieving Consistency in Latent Diffusion Models with Shift-Equivariance', 'desc': 'Latent Diffusion Models (LDMs) often produce inconsistent outputs due to their sensitivity to input noise variations. This paper presents a redesign of LDMs to improve their consistency by implementing shift-equivariance. The authors address challenges such as aliasing during VAE training and the limitations of self-attention modules by introducing new shift-equivariant attention mechanisms and an equivariance loss. The resulting alias-free LDM (AF-LDM) shows enhanced robustness and consistency in applications like video editing and image translation compared to traditional LDMs.'}, 'zh': {'title': 'æå‡æ½œåœ¨æ‰©æ•£æ¨¡å‹çš„ä¸€è‡´æ€§', 'desc': 'æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMsï¼‰åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­å­˜åœ¨ä¸ç¨³å®šæ€§ï¼Œè¾“å…¥å™ªå£°çš„å¾®å°å˜åŒ–å¯èƒ½å¯¼è‡´è¾“å‡ºç»“æœæ˜¾è‘—ä¸åŒï¼Œè¿™é™åˆ¶äº†å…¶åœ¨éœ€è¦ä¸€è‡´æ€§ç»“æœçš„åº”ç”¨ä¸­çš„é€‚ç”¨æ€§ã€‚æœ¬æ–‡é€šè¿‡é‡æ–°è®¾è®¡LDMsï¼Œä½¿å…¶å…·å¤‡å¹³ç§»ç­‰å˜æ€§ï¼Œä»è€Œå¢å¼ºä¸€è‡´æ€§ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ³¨æ„åŠ›æ¨¡å—ï¼Œä½¿å…¶å…·å¤‡å¹³ç§»ç­‰å˜æ€§ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§ç­‰å˜æŸå¤±ï¼Œæœ‰æ•ˆæŠ‘åˆ¶ç‰¹å¾åœ¨è¿ç»­åŸŸä¸­çš„é¢‘ç‡å¸¦å®½ã€‚æœ€ç»ˆï¼Œå¾—åˆ°çš„æ— åˆ«åLDMï¼ˆAF-LDMï¼‰åœ¨å¤šä¸ªåº”ç”¨ä¸­è¡¨ç°å‡ºæ›´å¼ºçš„ä¸€è‡´æ€§ï¼Œå°¤å…¶æ˜¯åœ¨è§†é¢‘ç¼–è¾‘å’Œå›¾åƒåˆ°å›¾åƒè½¬æ¢ä»»åŠ¡ä¸­ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.08681', 'title': 'Self-Taught Self-Correction for Small Language Models', 'url': 'https://huggingface.co/papers/2503.08681', 'abstract': 'Although large language models (LLMs) have achieved remarkable performance across various tasks, they remain prone to errors. A key challenge is enabling them to self-correct. While prior research has relied on external tools or large proprietary models, this work explores self-correction in small language models (SLMs) through iterative fine-tuning using solely self-generated data. We introduce the Self-Taught Self-Correction (STaSC) algorithm, which incorporates multiple algorithmic design choices. Experimental results on a question-answering task demonstrate that STaSC effectively learns self-correction, leading to significant performance improvements. Our analysis further provides insights into the mechanisms of self-correction and the impact of different design choices on learning dynamics and overall performance. To support future research, we release our user-friendly codebase and lightweight models.', 'score': 3, 'issue_id': 2686, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 11', 'zh': '3æœˆ11æ—¥'}, 'hash': '1886b6cebec24540', 'authors': ['Viktor Moskvoretskii', 'Chris Biemann', 'Irina Nikishina'], 'affiliations': ['HSE University', 'Skoltech', 'University of Hamburg'], 'pdf_title_img': 'assets/pdf/title_img/2503.08681.jpg', 'data': {'categories': ['#small_models', '#open_source', '#optimization', '#alignment', '#training'], 'emoji': 'ğŸ”§', 'ru': {'title': 'Ğ¡Ğ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ğ¸: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ°Ğ»Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Self-Taught Self-Correction (STaSC) Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ°Ğ»Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (SLM) ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ğ¸. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ñ… Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ¸Ğ»Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, STaSC Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° ÑĞ°Ğ¼Ğ¾Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ğ¸ Ğ¸ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ°ÑĞ¿ĞµĞºÑ‚Ğ¾Ğ² Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğ° Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ° Ğ½Ğ° Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºÑƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ‰ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ.'}, 'en': {'title': 'Empowering Small Models with Self-Correction', 'desc': 'This paper addresses the issue of error-proneness in large language models (LLMs) by focusing on self-correction capabilities in smaller language models (SLMs). The authors propose a novel algorithm called Self-Taught Self-Correction (STaSC), which utilizes iterative fine-tuning with self-generated data to enhance the self-correction process. Experimental results indicate that STaSC significantly improves performance on a question-answering task, showcasing its effectiveness. Additionally, the paper analyzes the mechanisms behind self-correction and how various design choices influence learning dynamics and outcomes.'}, 'zh': {'title': 'å°å‹æ¨¡å‹çš„è‡ªæˆ‘çº æ­£æ–°æ–¹æ³•', 'desc': 'å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å„ç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬ä»ç„¶å®¹æ˜“å‡ºé”™ã€‚æœ¬æ–‡æ¢è®¨äº†å¦‚ä½•é€šè¿‡è‡ªæˆ‘ç”Ÿæˆçš„æ•°æ®å¯¹å°å‹è¯­è¨€æ¨¡å‹è¿›è¡Œè¿­ä»£å¾®è°ƒï¼Œä»¥å®ç°è‡ªæˆ‘çº æ­£ã€‚æˆ‘ä»¬æå‡ºäº†è‡ªæˆ‘å­¦ä¹ è‡ªæˆ‘çº æ­£ï¼ˆSTaSCï¼‰ç®—æ³•ï¼Œè¯¥ç®—æ³•ç»“åˆäº†å¤šç§è®¾è®¡é€‰æ‹©ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSTaSCåœ¨é—®ç­”ä»»åŠ¡ä¸­æœ‰æ•ˆåœ°å­¦ä¹ äº†è‡ªæˆ‘çº æ­£ï¼Œæ˜¾è‘—æé«˜äº†æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.07588', 'title': 'When Large Vision-Language Model Meets Large Remote Sensing Imagery:\n  Coarse-to-Fine Text-Guided Token Pruning', 'url': 'https://huggingface.co/papers/2503.07588', 'abstract': "Efficient vision-language understanding of large Remote Sensing Images (RSIs) is meaningful but challenging. Current Large Vision-Language Models (LVLMs) typically employ limited pre-defined grids to process images, leading to information loss when handling gigapixel RSIs. Conversely, using unlimited grids significantly increases computational costs. To preserve image details while reducing computational complexity, we propose a text-guided token pruning method with Dynamic Image Pyramid (DIP) integration. Our method introduces: (i) a Region Focus Module (RFM) that leverages text-aware region localization capability to identify critical vision tokens, and (ii) a coarse-to-fine image tile selection and vision token pruning strategy based on DIP, which is guided by RFM outputs and avoids directly processing the entire large imagery. Additionally, existing benchmarks for evaluating LVLMs' perception ability on large RSI suffer from limited question diversity and constrained image sizes. We construct a new benchmark named LRS-VQA, which contains 7,333 QA pairs across 8 categories, with image length up to 27,328 pixels. Our method outperforms existing high-resolution strategies on four datasets using the same data. Moreover, compared to existing token reduction methods, our approach demonstrates higher efficiency under high-resolution settings. Dataset and code are in https://github.com/VisionXLab/LRS-VQA.", 'score': 3, 'issue_id': 2684, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 10', 'zh': '3æœˆ10æ—¥'}, 'hash': 'b3cad6b7241db7fa', 'authors': ['Junwei Luo', 'Yingying Zhang', 'Xue Yang', 'Kang Wu', 'Qi Zhu', 'Lei Liang', 'Jingdong Chen', 'Yansheng Li'], 'affiliations': ['Ant Group', 'Shanghai Jiao Tong University', 'University of Science and Technology of China', 'Wuhan University'], 'pdf_title_img': 'assets/pdf/title_img/2503.07588.jpg', 'data': {'categories': ['#optimization', '#cv', '#survey', '#dataset', '#benchmark'], 'emoji': 'ğŸ›°ï¸', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ³Ğ¸Ğ³Ğ°Ğ½Ñ‚ÑĞºĞ¸Ñ… ÑĞ¿ÑƒÑ‚Ğ½Ğ¸ĞºĞ¾Ğ²Ñ‹Ñ… ÑĞ½Ğ¸Ğ¼ĞºĞ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… ÑĞ¿ÑƒÑ‚Ğ½Ğ¸ĞºĞ¾Ğ²Ñ‹Ñ… ÑĞ½Ğ¸Ğ¼ĞºĞ¾Ğ² Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾-Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LVLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ğ¸Ñ€Ğ°Ğ¼Ğ¸Ğ´Ñ‹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ñ… Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ½Ğ° Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ°Ñ… (RFM) Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ° Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚ Ğ³Ñ€ÑƒĞ±Ğ¾Ğ³Ğ¾ Ğº Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¼Ñƒ. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ LRS-VQA Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ LVLM Ğ² Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¿ÑƒÑ‚Ğ½Ğ¸ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ ÑĞ½Ğ¸Ğ¼ĞºĞ°Ğ¼Ğ¸.'}, 'en': {'title': 'Efficiently Unlocking Insights from Gigapixel Remote Sensing Images', 'desc': 'This paper addresses the challenges of understanding large Remote Sensing Images (RSIs) using Large Vision-Language Models (LVLMs). It introduces a novel text-guided token pruning method combined with a Dynamic Image Pyramid (DIP) to efficiently process gigapixel images without losing important details. The proposed Region Focus Module (RFM) helps in identifying essential vision tokens, while the coarse-to-fine strategy reduces computational costs. Additionally, the authors present a new benchmark, LRS-VQA, to evaluate LVLMs on large RSIs, demonstrating that their method outperforms existing strategies in terms of efficiency and effectiveness.'}, 'zh': {'title': 'é«˜æ•ˆå¤„ç†å¤§å‹é¥æ„Ÿå›¾åƒçš„è§†è§‰-è¯­è¨€ç†è§£æ–¹æ³•', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„è§†è§‰-è¯­è¨€ç†è§£æ–¹æ³•ï¼Œä¸“é—¨é’ˆå¯¹å¤§å‹é¥æ„Ÿå›¾åƒï¼ˆRSIsï¼‰ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†åŠ¨æ€å›¾åƒé‡‘å­—å¡”ï¼ˆDIPï¼‰å’Œæ–‡æœ¬å¼•å¯¼çš„æ ‡è®°ä¿®å‰ªæŠ€æœ¯ï¼Œä»¥å‡å°‘è®¡ç®—å¤æ‚åº¦å¹¶ä¿ç•™å›¾åƒç»†èŠ‚ã€‚é€šè¿‡åŒºåŸŸèšç„¦æ¨¡å—ï¼ˆRFMï¼‰ï¼Œæˆ‘ä»¬èƒ½å¤Ÿè¯†åˆ«å…³é”®çš„è§†è§‰æ ‡è®°ï¼Œä»è€Œä¼˜åŒ–å›¾åƒå¤„ç†è¿‡ç¨‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ„å»ºäº†ä¸€ä¸ªæ–°çš„åŸºå‡†æ•°æ®é›†LRS-VQAï¼Œä»¥è¯„ä¼°ç°æœ‰å¤§å‹è§†è§‰-è¯­è¨€æ¨¡å‹çš„æ„ŸçŸ¥èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.09590', 'title': 'BIMBA: Selective-Scan Compression for Long-Range Video Question\n  Answering', 'url': 'https://huggingface.co/papers/2503.09590', 'abstract': 'Video Question Answering (VQA) in long videos poses the key challenge of extracting relevant information and modeling long-range dependencies from many redundant frames. The self-attention mechanism provides a general solution for sequence modeling, but it has a prohibitive cost when applied to a massive number of spatiotemporal tokens in long videos. Most prior methods rely on compression strategies to lower the computational cost, such as reducing the input length via sparse frame sampling or compressing the output sequence passed to the large language model (LLM) via space-time pooling. However, these naive approaches over-represent redundant information and often miss salient events or fast-occurring space-time patterns. In this work, we introduce BIMBA, an efficient state-space model to handle long-form videos. Our model leverages the selective scan algorithm to learn to effectively select critical information from high-dimensional video and transform it into a reduced token sequence for efficient LLM processing. Extensive experiments demonstrate that BIMBA achieves state-of-the-art accuracy on multiple long-form VQA benchmarks, including PerceptionTest, NExT-QA, EgoSchema, VNBench, LongVideoBench, and Video-MME. Code, and models are publicly available at https://sites.google.com/view/bimba-mllm.', 'score': 2, 'issue_id': 2689, 'pub_date': '2025-03-12', 'pub_date_card': {'ru': '12 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 12', 'zh': '3æœˆ12æ—¥'}, 'hash': '82b85fd9bb1c9a17', 'authors': ['Md Mohaiminul Islam', 'Tushar Nagarajan', 'Huiyu Wang', 'Gedas Bertasius', 'Lorenzo Torresani'], 'affiliations': ['Meta AI', 'UNC Chapel Hill'], 'pdf_title_img': 'assets/pdf/title_img/2503.09590.jpg', 'data': {'categories': ['#video', '#open_source', '#benchmark', '#long_context', '#architecture'], 'emoji': 'ğŸ¥', 'ru': {'title': 'BIMBA: ÑƒĞ¼Ğ½Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ BIMBA - ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ-Ğ¾Ñ‚Ğ²ĞµÑ‚ (VQA). ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ÑĞºĞ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ· Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞµĞµ Ğ² ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ½ÑƒÑ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ (LLM). BIMBA Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ´Ğ»Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ VQA.'}, 'en': {'title': 'BIMBA: Efficiently Answering Questions from Long Videos', 'desc': 'This paper addresses the challenge of Video Question Answering (VQA) in long videos, where extracting relevant information from numerous frames is difficult. The authors introduce BIMBA, a state-space model that efficiently selects critical information from high-dimensional video data. Unlike previous methods that often miss important events due to redundancy, BIMBA uses a selective scan algorithm to create a reduced token sequence for processing by large language models (LLMs). The results show that BIMBA achieves state-of-the-art performance on various long-form VQA benchmarks, demonstrating its effectiveness in handling long videos.'}, 'zh': {'title': 'BIMBAï¼šé«˜æ•ˆå¤„ç†é•¿è§†é¢‘é—®ç­”çš„åˆ›æ–°æ¨¡å‹', 'desc': 'è§†é¢‘é—®ç­”ï¼ˆVQAï¼‰åœ¨é•¿è§†é¢‘ä¸­é¢ä¸´æå–ç›¸å…³ä¿¡æ¯å’Œå»ºæ¨¡é•¿è·ç¦»ä¾èµ–çš„æŒ‘æˆ˜ã€‚è‡ªæ³¨æ„åŠ›æœºåˆ¶è™½ç„¶èƒ½å¤„ç†åºåˆ—å»ºæ¨¡ï¼Œä½†åœ¨å¤„ç†å¤§é‡æ—¶ç©ºæ ‡è®°æ—¶è®¡ç®—æˆæœ¬è¿‡é«˜ã€‚ä»¥å¾€çš„æ–¹æ³•é€šå¸¸ä¾èµ–å‹ç¼©ç­–ç•¥æ¥é™ä½è®¡ç®—æˆæœ¬ï¼Œä½†è¿™äº›ç®€å•çš„æ–¹æ³•å¾€å¾€ä¼šè¿‡åº¦è¡¨ç¤ºå†—ä½™ä¿¡æ¯ï¼Œé”™è¿‡é‡è¦äº‹ä»¶ã€‚æˆ‘ä»¬æå‡ºäº†BIMBAï¼Œä¸€ä¸ªé«˜æ•ˆçš„çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼Œèƒ½å¤Ÿä»é«˜ç»´è§†é¢‘ä¸­é€‰æ‹©å…³é”®ä¿¡æ¯ï¼Œå¹¶å°†å…¶è½¬åŒ–ä¸ºç®€åŒ–çš„æ ‡è®°åºåˆ—ï¼Œä»¥ä¾¿é«˜æ•ˆå¤„ç†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.09427', 'title': 'Multimodal Language Modeling for High-Accuracy Single Cell\n  Transcriptomics Analysis and Generation', 'url': 'https://huggingface.co/papers/2503.09427', 'abstract': 'Pre-trained language models (PLMs) have revolutionized scientific research, yet their application to single-cell analysis remains limited. Text PLMs cannot process single-cell RNA sequencing data, while cell PLMs lack the ability to handle free text, restricting their use in multimodal tasks. Existing efforts to bridge these modalities often suffer from information loss or inadequate single-modal pre-training, leading to suboptimal performances. To address these challenges, we propose Single-Cell MultiModal Generative Pre-trained Transformer (scMMGPT), a unified PLM for joint cell and text modeling. scMMGPT effectively integrates the state-of-the-art cell and text PLMs, facilitating cross-modal knowledge sharing for improved performance. To bridge the text-cell modality gap, scMMGPT leverages dedicated cross-modal projectors, and undergoes extensive pre-training on 27 million cells -- the largest dataset for multimodal cell-text PLMs to date. This large-scale pre-training enables scMMGPT to excel in joint cell-text tasks, achieving an 84\\% relative improvement of textual discrepancy for cell description generation, 20.5\\% higher accuracy for cell type annotation, and 4\\% improvement in k-NN accuracy for text-conditioned pseudo-cell generation, outperforming baselines.', 'score': 2, 'issue_id': 2677, 'pub_date': '2025-03-12', 'pub_date_card': {'ru': '12 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 12', 'zh': '3æœˆ12æ—¥'}, 'hash': '491beb48064068d2', 'authors': ['Yaorui Shi', 'Jiaqi Yang', 'Sihang Li', 'Junfeng Fang', 'Xiang Wang', 'Zhiyuan Liu', 'Yang Zhang'], 'affiliations': ['National University of Singapore', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2503.09427.jpg', 'data': {'categories': ['#plp', '#transfer_learning', '#science', '#multimodal', '#dataset', '#training'], 'emoji': 'ğŸ§¬', 'ru': {'title': 'Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° ĞºĞ»ĞµÑ‚Ğ¾Ğº Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°', 'desc': 'scMMGPT - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ°Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¾Ğ´Ğ¸Ğ½Ğ¾Ñ‡Ğ½Ñ‹Ñ… ĞºĞ»ĞµÑ‚Ğ¾Ğº Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°. ĞĞ½Ğ° Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¸Ğ· ÑÑ‚Ğ¸Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. scMMGPT Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¾Ñ€Ñ‹ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ²Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ»ĞµÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ±Ñ‹Ğ»Ğ° Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° 27 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ñ… ĞºĞ»ĞµÑ‚Ğ¾Ğº, Ñ‡Ñ‚Ğ¾ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ ĞºÑ€ÑƒĞ¿Ğ½ĞµĞ¹ÑˆĞ¸Ğ¼ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ¼ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ĞºĞ»ĞµÑ‚Ğ¾Ñ‡Ğ½Ğ¾-Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° ÑĞµĞ³Ğ¾Ğ´Ğ½ÑÑˆĞ½Ğ¸Ğ¹ Ğ´ĞµĞ½ÑŒ.'}, 'en': {'title': 'Bridging Cells and Text: The Power of scMMGPT', 'desc': 'This paper introduces the Single-Cell MultiModal Generative Pre-trained Transformer (scMMGPT), a novel model designed to integrate single-cell RNA sequencing data with textual information. Traditional pre-trained language models struggle with this integration due to their inability to process both modalities effectively. scMMGPT addresses these limitations by utilizing cross-modal projectors and extensive pre-training on a large dataset of 27 million cells, enhancing its performance in joint tasks. The results demonstrate significant improvements in cell description generation, cell type annotation accuracy, and text-conditioned pseudo-cell generation compared to existing models.'}, 'zh': {'title': 'å•ç»†èƒå¤šæ¨¡æ€ç”Ÿæˆé¢„è®­ç»ƒå˜æ¢å™¨çš„åˆ›æ–°åº”ç”¨', 'desc': 'é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆPLMsï¼‰åœ¨ç§‘å­¦ç ”ç©¶ä¸­å¸¦æ¥äº†é©å‘½æ€§çš„å˜åŒ–ï¼Œä½†åœ¨å•ç»†èƒåˆ†æä¸­çš„åº”ç”¨ä»ç„¶æœ‰é™ã€‚ç°æœ‰çš„æ–‡æœ¬PLMsæ— æ³•å¤„ç†å•ç»†èƒRNAæµ‹åºæ•°æ®ï¼Œè€Œç»†èƒPLMsåˆæ— æ³•å¤„ç†è‡ªç”±æ–‡æœ¬ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­çš„ä½¿ç”¨ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å•ç»†èƒå¤šæ¨¡æ€ç”Ÿæˆé¢„è®­ç»ƒå˜æ¢å™¨ï¼ˆscMMGPTï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºç»†èƒå’Œæ–‡æœ¬è”åˆå»ºæ¨¡çš„ç»Ÿä¸€PLMã€‚scMMGPTé€šè¿‡ä¸“é—¨çš„è·¨æ¨¡æ€æŠ•å½±å™¨å’Œåœ¨2700ä¸‡ä¸ªç»†èƒä¸Šè¿›è¡Œçš„å¤§è§„æ¨¡é¢„è®­ç»ƒï¼Œæ˜¾è‘—æé«˜äº†ç»†èƒæè¿°ç”Ÿæˆå’Œç»†èƒç±»å‹æ³¨é‡Šçš„å‡†ç¡®æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.05397', 'title': 'Multi Agent based Medical Assistant for Edge Devices', 'url': 'https://huggingface.co/papers/2503.05397', 'abstract': 'Large Action Models (LAMs) have revolutionized intelligent automation, but their application in healthcare faces challenges due to privacy concerns, latency, and dependency on internet access. This report introduces an ondevice, multi-agent healthcare assistant that overcomes these limitations. The system utilizes smaller, task-specific agents to optimize resources, ensure scalability and high performance. Our proposed system acts as a one-stop solution for health care needs with features like appointment booking, health monitoring, medication reminders, and daily health reporting. Powered by the Qwen Code Instruct 2.5 7B model, the Planner and Caller Agents achieve an average RougeL score of 85.5 for planning and 96.5 for calling for our tasks while being lightweight for on-device deployment. This innovative approach combines the benefits of ondevice systems with multi-agent architectures, paving the way for user-centric healthcare solutions.', 'score': 2, 'issue_id': 2685, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 7', 'zh': '3æœˆ7æ—¥'}, 'hash': '0d34c4bc75fe4355', 'authors': ['Sakharam Gawade', 'Shivam Akhouri', 'Chinmay Kulkarni', 'Jagdish Samant', 'Pragya Sahu', 'Aastik', 'Jai Pahal', 'Saswat Meher'], 'affiliations': ['Samsung Research Institute Bangalore, India'], 'pdf_title_img': 'assets/pdf/title_img/2503.05397.jpg', 'data': {'categories': ['#architecture', '#healthcare', '#small_models', '#agents'], 'emoji': 'ğŸ¥', 'ru': {'title': 'ĞœĞ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ½Ğ¸Ğº: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ±ĞµĞ· ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑĞ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ½Ğ¸ĞºĞ°, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‰ĞµĞ³Ğ¾ Ğ½Ğ° ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğµ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ. Ğ­Ñ‚Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°ĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ´ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ, Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ°Ğ¼Ğ¸ Ğ¸ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑŒÑ Ğ¾Ñ‚ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚Ğ°, Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ·Ğ´Ñ€Ğ°Ğ²Ğ¾Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğµ, ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹, ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€ĞµÑÑƒÑ€ÑÑ‹ Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ. ĞŸĞ¾Ğ¼Ğ¾Ñ‰Ğ½Ğ¸Ğº, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Qwen Code Instruct 2.5 7B, Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ½ÑƒĞ¶Ğ´, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ·Ğ°Ğ¿Ğ¸ÑÑŒ Ğ½Ğ° Ğ¿Ñ€Ğ¸ĞµĞ¼, Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³ Ğ·Ğ´Ğ¾Ñ€Ğ¾Ğ²ÑŒÑ Ğ¸ Ğ½Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¾ Ğ¿Ñ€Ğ¸ĞµĞ¼Ğµ Ğ»ĞµĞºĞ°Ñ€ÑÑ‚Ğ².'}, 'en': {'title': 'Empowering Healthcare with On-Device Multi-Agent Systems', 'desc': 'This paper presents a novel on-device multi-agent healthcare assistant that addresses key challenges in using Large Action Models (LAMs) in healthcare, such as privacy, latency, and internet dependency. By employing smaller, task-specific agents, the system enhances resource optimization, scalability, and performance. The assistant offers various healthcare functionalities, including appointment scheduling, health monitoring, and medication reminders, all while maintaining a lightweight design for on-device use. The system demonstrates impressive performance metrics, achieving high RougeL scores for its planning and calling tasks, showcasing its potential for user-centric healthcare solutions.'}, 'zh': {'title': 'æ™ºèƒ½åŒ»ç–—åŠ©æ‰‹ï¼šéšç§ã€å®‰å…¨ã€é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆ', 'desc': 'å¤§å‹è¡ŒåŠ¨æ¨¡å‹ï¼ˆLAMsï¼‰åœ¨æ™ºèƒ½è‡ªåŠ¨åŒ–é¢†åŸŸå–å¾—äº†é©å‘½æ€§è¿›å±•ï¼Œä½†åœ¨åŒ»ç–—ä¿å¥ä¸­çš„åº”ç”¨é¢ä¸´éšç§ã€å»¶è¿Ÿå’Œå¯¹äº’è”ç½‘ä¾èµ–ç­‰æŒ‘æˆ˜ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºè®¾å¤‡çš„å¤šæ™ºèƒ½ä½“åŒ»ç–—åŠ©æ‰‹ï¼Œå…‹æœäº†è¿™äº›é™åˆ¶ã€‚è¯¥ç³»ç»Ÿåˆ©ç”¨è¾ƒå°çš„ã€ç‰¹å®šä»»åŠ¡çš„æ™ºèƒ½ä½“æ¥ä¼˜åŒ–èµ„æºï¼Œç¡®ä¿å¯æ‰©å±•æ€§å’Œé«˜æ€§èƒ½ã€‚æˆ‘ä»¬æå‡ºçš„ç³»ç»Ÿä½œä¸ºä¸€ç«™å¼åŒ»ç–—è§£å†³æ–¹æ¡ˆï¼Œå…·å¤‡é¢„çº¦ã€å¥åº·ç›‘æµ‹ã€ç”¨è¯æé†’å’Œæ¯æ—¥å¥åº·æŠ¥å‘Šç­‰åŠŸèƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.09600', 'title': 'MoC: Mixtures of Text Chunking Learners for Retrieval-Augmented\n  Generation System', 'url': 'https://huggingface.co/papers/2503.09600', 'abstract': 'Retrieval-Augmented Generation (RAG), while serving as a viable complement to large language models (LLMs), often overlooks the crucial aspect of text chunking within its pipeline. This paper initially introduces a dual-metric evaluation method, comprising Boundary Clarity and Chunk Stickiness, to enable the direct quantification of chunking quality. Leveraging this assessment method, we highlight the inherent limitations of traditional and semantic chunking in handling complex contextual nuances, thereby substantiating the necessity of integrating LLMs into chunking process. To address the inherent trade-off between computational efficiency and chunking precision in LLM-based approaches, we devise the granularity-aware Mixture-of-Chunkers (MoC) framework, which consists of a three-stage processing mechanism. Notably, our objective is to guide the chunker towards generating a structured list of chunking regular expressions, which are subsequently employed to extract chunks from the original text. Extensive experiments demonstrate that both our proposed metrics and the MoC framework effectively settle challenges of the chunking task, revealing the chunking kernel while enhancing the performance of the RAG system.', 'score': 1, 'issue_id': 2683, 'pub_date': '2025-03-12', 'pub_date_card': {'ru': '12 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 12', 'zh': '3æœˆ12æ—¥'}, 'hash': '4d7741ddea8c8c48', 'authors': ['Jihao Zhao', 'Zhiyuan Ji', 'Zhaoxin Fan', 'Hanyu Wang', 'Simin Niu', 'Bo Tang', 'Feiyu Xiong', 'Zhiyu Li'], 'affiliations': ['Institute for Advanced Algorithms Research, Shanghai, China', 'School of Information, Renmin University of China, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.09600.jpg', 'data': {'categories': ['#long_context', '#rag', '#optimization'], 'emoji': 'âœ‚ï¸', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ±Ğ¸ĞµĞ½Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ RAG ÑĞ¸ÑÑ‚ĞµĞ¼', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€Ğ°Ğ·Ğ±Ğ¸ĞµĞ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ½Ğ° Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… RAG. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ¾Ğ¹ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€Ğ°Ğ·Ğ±Ğ¸ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ². ĞĞ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Mixture-of-Chunkers (MoC), Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… Ğ²Ñ‹Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ¸ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº MoC ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€ĞµÑˆĞ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ€Ğ°Ğ·Ğ±Ğ¸ĞµĞ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ ÑĞ¸ÑÑ‚ĞµĞ¼ RAG.'}, 'en': {'title': 'Enhancing Chunking for Better Retrieval-Augmented Generation', 'desc': 'This paper discusses the importance of text chunking in Retrieval-Augmented Generation (RAG) systems that use large language models (LLMs). It introduces a new evaluation method with two metrics, Boundary Clarity and Chunk Stickiness, to measure the quality of text chunks. The authors point out the limitations of existing chunking methods and propose a new framework called Mixture-of-Chunkers (MoC) that improves chunking precision while maintaining computational efficiency. Through experiments, they show that their metrics and MoC framework enhance the overall performance of RAG systems by effectively addressing chunking challenges.'}, 'zh': {'title': 'æå‡RAGç³»ç»Ÿæ€§èƒ½çš„åˆ†å—åˆ›æ–°', 'desc': 'æœ¬æ–‡æ¢è®¨äº†æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ¨¡å‹ä¸­æ–‡æœ¬åˆ†å—çš„é‡è¦æ€§ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŒæŒ‡æ ‡è¯„ä¼°æ–¹æ³•ï¼ŒåŒ…æ‹¬è¾¹ç•Œæ¸…æ™°åº¦å’Œåˆ†å—ç²˜æ€§ï¼Œä»¥é‡åŒ–åˆ†å—è´¨é‡ã€‚é€šè¿‡è¿™ä¸€è¯„ä¼°æ–¹æ³•ï¼Œæˆ‘ä»¬æ­ç¤ºäº†ä¼ ç»Ÿå’Œè¯­ä¹‰åˆ†å—åœ¨å¤„ç†å¤æ‚ä¸Šä¸‹æ–‡æ—¶çš„å±€é™æ€§ï¼Œå¹¶å¼ºè°ƒäº†å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ•´åˆåˆ°åˆ†å—è¿‡ç¨‹ä¸­çš„å¿…è¦æ€§ã€‚ä¸ºäº†è§£å†³åŸºäºLLMsçš„æ–¹æ³•åœ¨è®¡ç®—æ•ˆç‡å’Œåˆ†å—ç²¾åº¦ä¹‹é—´çš„æƒè¡¡ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå…³æ³¨ç²’åº¦çš„æ··åˆåˆ†å—å™¨ï¼ˆMoCï¼‰æ¡†æ¶ï¼Œæ˜¾è‘—æå‡äº†RAGç³»ç»Ÿçš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.09516', 'title': 'Search-R1: Training LLMs to Reason and Leverage Search Engines with\n  Reinforcement Learning', 'url': 'https://huggingface.co/papers/2503.09516', 'abstract': 'Efficiently acquiring external knowledge and up-to-date information is essential for effective reasoning and text generation in large language models (LLMs). Retrieval augmentation and tool-use training approaches where a search engine is treated as a tool lack complex multi-turn retrieval flexibility or require large-scale supervised data. Prompting advanced LLMs with reasoning capabilities during inference to use search engines is not optimal, since the LLM does not learn how to optimally interact with the search engine. This paper introduces Search-R1, an extension of the DeepSeek-R1 model where the LLM learns -- solely through reinforcement learning (RL) -- to autonomously generate (multiple) search queries during step-by-step reasoning with real-time retrieval. Search-R1 optimizes LLM rollouts with multi-turn search interactions, leveraging retrieved token masking for stable RL training and a simple outcome-based reward function. Experiments on seven question-answering datasets show that Search-R1 improves performance by 26% (Qwen2.5-7B), 21% (Qwen2.5-3B), and 10% (LLaMA3.2-3B) over SOTA baselines. This paper further provides empirical insights into RL optimization methods, LLM choices, and response length dynamics in retrieval-augmented reasoning. The code and model checkpoints are available at https://github.com/PeterGriffinJin/Search-R1.', 'score': 0, 'issue_id': 2696, 'pub_date': '2025-03-12', 'pub_date_card': {'ru': '12 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 12', 'zh': '3æœˆ12æ—¥'}, 'hash': 'd471c9c6c84abb33', 'authors': ['Bowen Jin', 'Hansi Zeng', 'Zhenrui Yue', 'Dong Wang', 'Hamed Zamani', 'Jiawei Han'], 'affiliations': ['Center for Intelligent Information Retrieval, University of Massachusetts Amherst', 'Department of Computer Science, University of Illinois at Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2503.09516.jpg', 'data': {'categories': ['#rl', '#reasoning', '#rag', '#optimization', '#training'], 'emoji': 'ğŸ”', 'ru': {'title': 'ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ¿Ğ¾Ğ¸ÑĞºÑƒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Search-R1 - Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ DeepSeek-R1, Ğ³Ğ´Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ (LLM) ÑƒÑ‡Ğ¸Ñ‚ÑÑ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Search-R1 Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ LLM Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ¾Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ½Ğ° ÑĞµĞ¼Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹. Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ñ… Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞµ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ… Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¿Ğ¾Ğ¸ÑĞºĞ°.'}, 'en': {'title': 'Empowering LLMs with Autonomous Search through Reinforcement Learning', 'desc': 'This paper presents Search-R1, a novel approach that enhances large language models (LLMs) by enabling them to autonomously generate search queries during reasoning tasks. Unlike traditional methods that rely on supervised data or lack flexibility, Search-R1 employs reinforcement learning (RL) to optimize multi-turn interactions with search engines. The model uses token masking and a simple reward function to stabilize training and improve the quality of retrieved information. Experimental results demonstrate significant performance gains across various question-answering datasets, showcasing the effectiveness of RL in retrieval-augmented reasoning.'}, 'zh': {'title': 'æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ£€ç´¢èƒ½åŠ›', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºSearch-R1çš„æ¨¡å‹æ‰©å±•ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†å’Œæ–‡æœ¬ç”Ÿæˆä¸­çš„å¤–éƒ¨çŸ¥è¯†è·å–èƒ½åŠ›ã€‚é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼ŒSearch-R1èƒ½å¤Ÿè‡ªä¸»ç”Ÿæˆå¤šä¸ªæœç´¢æŸ¥è¯¢ï¼Œä»è€Œå®ç°å®æ—¶æ£€ç´¢å’Œé€æ­¥æ¨ç†ã€‚è¯¥æ¨¡å‹ä¼˜åŒ–äº†å¤šè½®æœç´¢äº¤äº’ï¼Œåˆ©ç”¨æ£€ç´¢åˆ°çš„ä»¤ç‰Œæ©è”½æŠ€æœ¯è¿›è¡Œç¨³å®šçš„RLè®­ç»ƒï¼Œå¹¶é‡‡ç”¨ç®€å•çš„åŸºäºç»“æœçš„å¥–åŠ±å‡½æ•°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSearch-R1åœ¨ä¸ƒä¸ªé—®ç­”æ•°æ®é›†ä¸Šçš„è¡¨ç°æ˜¾è‘—æå‡ï¼Œåˆ†åˆ«æé«˜äº†26%ã€21%å’Œ10%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.09410', 'title': 'Monte Carlo Diffusion for Generalizable Learning-Based RANSAC', 'url': 'https://huggingface.co/papers/2503.09410', 'abstract': 'Random Sample Consensus (RANSAC) is a fundamental approach for robustly estimating parametric models from noisy data. Existing learning-based RANSAC methods utilize deep learning to enhance the robustness of RANSAC against outliers. However, these approaches are trained and tested on the data generated by the same algorithms, leading to limited generalization to out-of-distribution data during inference. Therefore, in this paper, we introduce a novel diffusion-based paradigm that progressively injects noise into ground-truth data, simulating the noisy conditions for training learning-based RANSAC. To enhance data diversity, we incorporate Monte Carlo sampling into the diffusion paradigm, approximating diverse data distributions by introducing different types of randomness at multiple stages. We evaluate our approach in the context of feature matching through comprehensive experiments on the ScanNet and MegaDepth datasets. The experimental results demonstrate that our Monte Carlo diffusion mechanism significantly improves the generalization ability of learning-based RANSAC. We also develop extensive ablation studies that highlight the effectiveness of key components in our framework.', 'score': 0, 'issue_id': 2686, 'pub_date': '2025-03-12', 'pub_date_card': {'ru': '12 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 12', 'zh': '3æœˆ12æ—¥'}, 'hash': '2ccf3518b5892591', 'authors': ['Jiale Wang', 'Chen Zhao', 'Wei Ke', 'Tong Zhang'], 'affiliations': ['EPFL', 'University of Chinese Academy of Sciences', 'Xian Jiaotong University'], 'pdf_title_img': 'assets/pdf/title_img/2503.09410.jpg', 'data': {'categories': ['#dataset', '#data', '#diffusion', '#cv', '#benchmark', '#optimization'], 'emoji': 'ğŸ²', 'ru': {'title': 'ĞŸĞ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ĞµĞ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ RANSAC Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞœĞ¾Ğ½Ñ‚Ğµ-ĞšĞ°Ñ€Ğ»Ğ¾', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ° RANSAC Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° ĞœĞ¾Ğ½Ñ‚Ğµ-ĞšĞ°Ñ€Ğ»Ğ¾ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑĞ¸Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ñ… Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑˆÑƒĞ¼Ğ½Ñ‹Ğµ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ScanNet Ğ¸ MegaDepth Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ RANSAC Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹.'}, 'en': {'title': 'Enhancing RANSAC Robustness with Monte Carlo Diffusion', 'desc': "This paper presents a new method to improve the robustness of learning-based Random Sample Consensus (RANSAC) against noisy data. The authors propose a diffusion-based approach that adds noise to clean data, mimicking real-world conditions for better training. By integrating Monte Carlo sampling, they create diverse data distributions, enhancing the model's ability to generalize to unseen data. Experimental results show that this method significantly boosts the performance of learning-based RANSAC in feature matching tasks."}, 'zh': {'title': 'åŸºäºæ‰©æ•£çš„RANSACï¼šæå‡é²æ£’æ€§ä¸æ³›åŒ–èƒ½åŠ›', 'desc': 'éšæœºæ ·æœ¬ä¸€è‡´æ€§ï¼ˆRANSACï¼‰æ˜¯ä¸€ç§ç”¨äºä»å™ªå£°æ•°æ®ä¸­ç¨³å¥ä¼°è®¡å‚æ•°æ¨¡å‹çš„åŸºæœ¬æ–¹æ³•ã€‚ç°æœ‰çš„åŸºäºå­¦ä¹ çš„RANSACæ–¹æ³•åˆ©ç”¨æ·±åº¦å­¦ä¹ å¢å¼ºå…¶å¯¹å¼‚å¸¸å€¼çš„é²æ£’æ€§ï¼Œä½†è¿™äº›æ–¹æ³•åœ¨ç›¸åŒç®—æ³•ç”Ÿæˆçš„æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒå’Œæµ‹è¯•ï¼Œå¯¼è‡´åœ¨æ¨ç†æ—¶å¯¹åˆ†å¸ƒå¤–æ•°æ®çš„æ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„åŸºäºæ‰©æ•£çš„èŒƒå¼ï¼Œé€šè¿‡é€æ­¥å‘çœŸå®æ•°æ®ä¸­æ³¨å…¥å™ªå£°ï¼Œæ¨¡æ‹Ÿè®­ç»ƒåŸºäºå­¦ä¹ çš„RANSACæ‰€éœ€çš„å™ªå£°æ¡ä»¶ã€‚æˆ‘ä»¬è¿˜ç»“åˆäº†è’™ç‰¹å¡æ´›é‡‡æ ·ï¼Œä»¥åœ¨å¤šä¸ªé˜¶æ®µå¼•å…¥ä¸åŒç±»å‹çš„éšæœºæ€§ï¼Œä»è€Œå¢å¼ºæ•°æ®çš„å¤šæ ·æ€§ï¼Œè¯„ä¼°ç»“æœè¡¨æ˜è¯¥æ–¹æ³•æ˜¾è‘—æé«˜äº†å­¦ä¹ å‹RANSACçš„æ³›åŒ–èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.08674', 'title': 'Understanding and Mitigating Distribution Shifts For Machine Learning\n  Force Fields', 'url': 'https://huggingface.co/papers/2503.08674', 'abstract': 'Machine Learning Force Fields (MLFFs) are a promising alternative to expensive ab initio quantum mechanical molecular simulations. Given the diversity of chemical spaces that are of interest and the cost of generating new data, it is important to understand how MLFFs generalize beyond their training distributions. In order to characterize and better understand distribution shifts in MLFFs, we conduct diagnostic experiments on chemical datasets, revealing common shifts that pose significant challenges, even for large foundation models trained on extensive data. Based on these observations, we hypothesize that current supervised training methods inadequately regularize MLFFs, resulting in overfitting and learning poor representations of out-of-distribution systems. We then propose two new methods as initial steps for mitigating distribution shifts for MLFFs. Our methods focus on test-time refinement strategies that incur minimal computational cost and do not use expensive ab initio reference labels. The first strategy, based on spectral graph theory, modifies the edges of test graphs to align with graph structures seen during training. Our second strategy improves representations for out-of-distribution systems at test-time by taking gradient steps using an auxiliary objective, such as a cheap physical prior. Our test-time refinement strategies significantly reduce errors on out-of-distribution systems, suggesting that MLFFs are capable of and can move towards modeling diverse chemical spaces, but are not being effectively trained to do so. Our experiments establish clear benchmarks for evaluating the generalization capabilities of the next generation of MLFFs. Our code is available at https://tkreiman.github.io/projects/mlff_distribution_shifts/.', 'score': 0, 'issue_id': 2693, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 11', 'zh': '3æœˆ11æ—¥'}, 'hash': '444f0805a7ed179b', 'authors': ['Tobias Kreiman', 'Aditi S. Krishnapriyan'], 'affiliations': ['LBNL', 'UC Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2503.08674.jpg', 'data': {'categories': ['#training', '#benchmark', '#transfer_learning', '#dataset', '#graphs', '#optimization', '#data'], 'emoji': 'ğŸ§ª', 'ru': {'title': 'ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ† Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ: Ğ½Ğ¾Ğ²Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ñ‹Ñ… ÑĞ¸Ğ»Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ğ¾Ğ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ñ‹Ñ… ÑĞ¸Ğ»Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ğ¾Ğ»ĞµĞ¹ (MLFFs) ĞºĞ°Ğº Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ñ‹ Ğ´Ğ¾Ñ€Ğ¾Ğ³Ğ¾ÑÑ‚Ğ¾ÑÑ‰Ğ¸Ğ¼ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ¾-Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ»ÑÑ€Ğ½Ñ‹Ğ¼ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸ÑĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸Ğ·ÑƒÑ‡Ğ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ MLFFs Ğ·Ğ° Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ°Ğ¼Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ²Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ ÑĞ¼ÑĞ³Ñ‡ĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¾Ğ² ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ. ĞŸĞµÑ€Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° ÑĞ¿ĞµĞºÑ‚Ñ€Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ‚ĞµĞ¾Ñ€Ğ¸Ğ¸ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ² Ğ¸ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ñ‘Ğ±Ñ€Ğ° Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ². Ğ’Ñ‚Ğ¾Ñ€Ğ¾Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ²Ğ½Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ñ†ĞµĞ»ĞµĞ²ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ.'}, 'en': {'title': 'Enhancing MLFFs: Bridging the Gap in Chemical Space Generalization', 'desc': 'This paper discusses Machine Learning Force Fields (MLFFs) as a cost-effective alternative to traditional quantum mechanical simulations for molecular modeling. It highlights the challenges MLFFs face when generalizing to chemical spaces that differ from their training data, often leading to overfitting. The authors propose two innovative test-time refinement strategies to improve the performance of MLFFs on out-of-distribution data without relying on expensive reference labels. Their findings suggest that with better training techniques, MLFFs can effectively model a wider range of chemical environments.'}, 'zh': {'title': 'æå‡æœºå™¨å­¦ä¹ åŠ›åœºçš„æ³›åŒ–èƒ½åŠ›', 'desc': 'æœºå™¨å­¦ä¹ åŠ›åœºï¼ˆMLFFsï¼‰æ˜¯ä¸€ç§æœ‰å‰æ™¯çš„æ›¿ä»£æ–¹æ¡ˆï¼Œç”¨äºæ˜‚è´µçš„é‡å­åŠ›å­¦åˆ†å­æ¨¡æ‹Ÿã€‚æœ¬æ–‡ç ”ç©¶äº†MLFFsåœ¨è®­ç»ƒåˆ†å¸ƒä¹‹å¤–çš„æ³›åŒ–èƒ½åŠ›ï¼Œå‘ç°å½“å‰çš„ç›‘ç£è®­ç»ƒæ–¹æ³•å¯èƒ½å¯¼è‡´è¿‡æ‹Ÿåˆï¼Œæ— æ³•æœ‰æ•ˆå­¦ä¹ åˆ†å¸ƒå¤–ç³»ç»Ÿçš„è¡¨ç¤ºã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†ä¸¤ç§æ–°çš„æµ‹è¯•æ—¶ä¼˜åŒ–ç­–ç•¥ï¼Œæ—¨åœ¨å‡å°‘åˆ†å¸ƒè½¬ç§»å¸¦æ¥çš„è¯¯å·®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™äº›ç­–ç•¥æ˜¾è‘—æé«˜äº†åœ¨åˆ†å¸ƒå¤–ç³»ç»Ÿä¸Šçš„è¡¨ç°ï¼Œè¡¨æ˜MLFFsæœ‰æ½œåŠ›å»ºæ¨¡å¤šæ ·çš„åŒ–å­¦ç©ºé—´ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.05333', 'title': 'PhysicsGen: Can Generative Models Learn from Images to Predict Complex\n  Physical Relations?', 'url': 'https://huggingface.co/papers/2503.05333', 'abstract': 'The image-to-image translation abilities of generative learning models have recently made significant progress in the estimation of complex (steered) mappings between image distributions. While appearance based tasks like image in-painting or style transfer have been studied at length, we propose to investigate the potential of generative models in the context of physical simulations. Providing a dataset of 300k image-pairs and baseline evaluations for three different physical simulation tasks, we propose a benchmark to investigate the following research questions: i) are generative models able to learn complex physical relations from input-output image pairs? ii) what speedups can be achieved by replacing differential equation based simulations? While baseline evaluations of different current models show the potential for high speedups (ii), these results also show strong limitations toward the physical correctness (i). This underlines the need for new methods to enforce physical correctness. Data, baseline models and evaluation code http://www.physics-gen.org.', 'score': 0, 'issue_id': 2690, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 7', 'zh': '3æœˆ7æ—¥'}, 'hash': 'c9dae1097c1be3c2', 'authors': ['Martin Spitznagel', 'Jan Vaillant', 'Janis Keuper'], 'affiliations': ['Herrenknecht AG', 'Institute for Machine Learning and Analytics (IMLA), Offenburg University, Germany', 'University of Mannheim, Germany'], 'pdf_title_img': 'assets/pdf/title_img/2503.05333.jpg', 'data': {'categories': ['#cv', '#diffusion', '#optimization', '#benchmark', '#dataset'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸: Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· 300 Ñ‚Ñ‹ÑÑÑ‡ Ğ¿Ğ°Ñ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ»Ñ Ñ‚Ñ€ĞµÑ… Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¾ Ğ½Ğ° Ğ²Ñ‹ÑÑĞ½ĞµĞ½Ğ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸Ğ·ÑƒÑ‡Ğ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ñ Ğ¸Ğ· Ğ¿Ğ°Ñ€ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ¸ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ¢Ğ°ĞºĞ¶Ğµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸ÑĞ¼Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Revolutionizing Physical Simulations with Generative Models', 'desc': 'This paper explores the use of generative learning models for image-to-image translation in the context of physical simulations. It presents a dataset of 300,000 image pairs and establishes a benchmark for evaluating how well these models can learn complex physical relationships. The authors investigate the potential speed improvements that can be achieved by using generative models instead of traditional differential equation-based simulations. However, while some models show promise for faster simulations, they often struggle with maintaining physical accuracy, highlighting the need for new approaches to ensure correctness in physical simulations.'}, 'zh': {'title': 'æ¢ç´¢ç”Ÿæˆæ¨¡å‹åœ¨ç‰©ç†æ¨¡æ‹Ÿä¸­çš„æ½œåŠ›', 'desc': 'è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†ç”Ÿæˆå­¦ä¹ æ¨¡å‹åœ¨ç‰©ç†æ¨¡æ‹Ÿä¸­çš„åº”ç”¨ï¼Œå°¤å…¶æ˜¯å›¾åƒåˆ°å›¾åƒçš„è½¬æ¢èƒ½åŠ›ã€‚ç ”ç©¶è€…æä¾›äº†ä¸€ä¸ªåŒ…å«30ä¸‡å¯¹å›¾åƒçš„æ•°æ®é›†ï¼Œå¹¶é’ˆå¯¹ä¸‰ç§ä¸åŒçš„ç‰©ç†æ¨¡æ‹Ÿä»»åŠ¡è¿›è¡Œäº†åŸºå‡†è¯„ä¼°ã€‚è®ºæ–‡æå‡ºäº†ä¸¤ä¸ªç ”ç©¶é—®é¢˜ï¼šç”Ÿæˆæ¨¡å‹æ˜¯å¦èƒ½å¤Ÿä»è¾“å…¥è¾“å‡ºå›¾åƒå¯¹ä¸­å­¦ä¹ å¤æ‚çš„ç‰©ç†å…³ç³»ï¼Ÿé€šè¿‡æ›¿ä»£åŸºäºå¾®åˆ†æ–¹ç¨‹çš„æ¨¡æ‹Ÿï¼Œèƒ½å®ç°å¤šå¤§çš„åŠ é€Ÿï¼Ÿç»“æœæ˜¾ç¤ºï¼Œå°½ç®¡å½“å‰æ¨¡å‹åœ¨åŠ é€Ÿæ–¹é¢æœ‰æ½œåŠ›ï¼Œä½†åœ¨ç‰©ç†æ­£ç¡®æ€§æ–¹é¢å­˜åœ¨æ˜æ˜¾çš„å±€é™æ€§ï¼Œå› æ­¤éœ€è¦æ–°çš„æ–¹æ³•æ¥ç¡®ä¿ç‰©ç†çš„å‡†ç¡®æ€§ã€‚'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (64)', '#agents (73)', '#agi (20)', '#alignment (40)', '#architecture (126)', '#audio (22)', '#benchmark (229)', '#cv (145)', '#data (83)', '#dataset (198)', '#diffusion (102)', '#ethics (24)', '#games (64)', '#graphs (6)', '#hallucinations (29)', '#healthcare (19)', '#inference (78)', '#interpretability (45)', '#leakage (4)', '#long_context (53)', '#low_resource (15)', '#machine_translation (9)', '#math (20)', '#multilingual (22)', '#multimodal (202)', '#open_source (143)', '#optimization (268)', '#plp (5)', '#rag (28)', '#reasoning (143)', '#rl (59)', '#rlhf (25)', '#robotics (25)', '#science (22)', '#security (18)', '#small_models (22)', '#story_generation (7)', '#survey (17)', '#synthetic (52)', '#training (274)', '#transfer_learning (44)', '#video (103)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            ğŸ”º ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'ğŸ”„ ' + getTimeDiff('2025-03-31 12:20',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ",
                    pub_date: "Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸",
                    issue_id: "Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "è¯„åˆ†",
                    pub_date: "å‘å¸ƒæ—¥æœŸ",
                    issue_id: "HFä¸Šä¼ æ—¥æœŸ"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-03-31 12:20')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-03-31 12:20')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('monthly'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    